{
    "thinkt4nk": "Hrrm... it seems bundle wasn't required in the command at all?  When I executed 'npm bundle install...', I got the usage output, so 'bundle' is obviously not a valid command.  However, when I just execute 'npm install https://github.com/squaremo/node-amqp/tarball/master -g', it installs just fine.  What was intended?\n. Hrrm... it seems bundle wasn't required in the command at all?  When I executed 'npm bundle install...', I got the usage output, so 'bundle' is obviously not a valid command.  However, when I just execute 'npm install https://github.com/squaremo/node-amqp/tarball/master -g', it installs just fine.  What was intended?\n. ",
    "squaremo": "It's probably a bit out of date with respect to npm. The idea is that 'bundle install' ('install bundle', 'indle bunstall') downloads Socket.IO and node-amqp to ./node_modules/.\n. Ah yes, npm 1.0 doesn't use 'bundle' as a command, it's just 'install' (and it defaults to installing locally, that is in node_modules in the same directory)\n. The examples work for me. What doesn't work for you?\n. One gotcha is the version of socket.io. v0.7 has gone off in a weird direction rather than solving some of the problems in 0.6, so rabbit.js sticks with 0.6. It's not down as a dependency in the package.json, because it's not strictly necessary, so to install it you will need to do\n\n```\n$ npm install socket.io@0.6.17\n```\n. I'm not sure this is a good idea; they zeromq guys decided to get rid of durable subscriptions (see http://article.gmane.org/gmane.network.zeromq.devel/10091/match=durable+identities). It's an easy way to leak resources.\nAlso, there's no notion of acknowledging messages sent to the browser, so this wouldn't actually stop messages getting dropped.\n\nIf you can think of a sensible semantics for durable subscriptions from browsers that avoids those problems, I am happy to consider it. Possibly connections should just have some more in-built reliability.\n\nYour implementation is a bit odd. Firstly, you use the same queue name for all the subscribers, which would mean that they compete as consumers (i.e., each message would go to only one subscriber). Secondly, you create a direct exchange and always bind the queue using an empty string, so it's effectively a fanout exchange anyway.\n. This pattern is encoded in rabbit.js as 'req' and 'rep' sockets: see the req/rep example.\n. Ohhh, snap! Is there a standard way of dealing with this backwards incompatibility?\n. I've updated to Node.JS 0.6 series, and improved the packaging a bit. The README has fresh details on how to run the examples (you'll need to use NODE_PATH now ..).\n. That's pretty much the use case for rabbit.js, making it easy to do that; example/sockjs.js is a good example (though it uses a request socket, whereas you probably want a subscribe socket).\n\nThat error is kind of mysterious, I don't see it if I run the example myself. I see \"rep socket opened\" as the next line after \"SockJS bound to ..\". Can you isolate the problem?\n. All the AMQP is done for you by rabbit.js. That's basically the point of it. You can always just use node-amqp, if you want to use AMQP directly -- as you can see from rabbit.js, it's not too hard.\n\nsockjs.js isn't requiring itself, it's requiring the library sockjs from node_modules. It ought to be named differently to avoid that confusion.\n\nTo connect to a RabbitMQ somewhere else, supply a URL to sockets.listen,as in this code:\n\n```\n$ sockets.listen(svr, {url: \"amqp://example.com\"})\n```\n. @fostercraig: To be honest I am not completely sure of the reconnect semantics of SockJS. Just opening another ought to work however.\nOne possible hitch is that SockJS (and Socket.IO, and HTTP for that matter) is limited to two HTTP open connections to any given host; in practice this means that in general one can only maintain one SockJS connection to a host, since the fallback needs one connection for sending and one for long polling.\n. You can always multiplex over the SockJS connection (in fact this is an explicit decision on the part of SockJS, to leave that sort of thing up to applications). That's not how rabbit.js works right now, but as it happens, I am changing things around to make that much easier.\n\nSpecifically, I'm giving making socket() the top-level abstraction, rather than listen(), and making the sockets implement the Stream interface so you can just pipe from whatever to whatever; e.g., in your scenario you can pipe a sub socket into the SockJS connection and the SockJS connection into a pub socket. If that doesn't make any sense wait a bit for the readme changes -- all will become clear! Well, fingers crossed.\n. @soichih By the way, did you get this to work, or did you give up? (the latter would make me sad of course)\n. Hi! Yes, you are right; in my haste to write a pithy example I elided the callbacks. The README should now have a properly working example.\n(The examples in the directories under example don't make this mistake, by the way.)\n. When would you want messages to be acknowledged?\n. OK so this is one-way, as in, you're not posting replies to the work items? And I presume the reason for using acknowledgements is for some kind of reliability: if the task processing fails and takes down the socket or connection, the message will be requeued and given to another worker if there is one, or possibly the crashed and restarted worker.\n\nIf you need this kind of reliability, then have workers reply to tasks, and have processes that post tasks resend them if they don't get a reply within some timeout. If you use replies, rabbit.js could (though it doesn't currently) use the lack of an acknowledgement to short-circuit this and resend itself if the worker process dies. Replies may also be useful for rate-limiting task-producers so that tasks don't back up.\n\nIf you don't want to send replies, there's no place for rabbit.js to say \"ok this message is handled\"; I guess it could acknowledge each message after emitting a data event, but I don't think this really buys a lot.\n\nIf you're attached to using (one-hop) acknowledgements, you may want to just use the AMQP client library.\n\nI may look at adding sockopts for blocking and unblocking the receive buffer, but by that stage it is getting as complicated as AMQP and that's something I definitely don't want. If you've another idea I'd be happy to discuss it.\n. Joseph, did you try any of the above out or think of a better solution (or switch to using something else)?\n. @jgornick As above I'm not convinced that acknowledgements give you more reliability without creating other problems. For example, what if you do the important bit of the work, then crash before you can call `ack()`. Then the item will be sent again, possibly to someone else, and the work will be done again. (AMQP is no better in this respect, -- you have a bit more control over what happens when, but no more guarantees)\n\nMy resistance to this additional complication is based on the assumption that people use rabbit.js as its own abstraction -- simple messaging that happens to use RabbitMQ -- whereas _maybe_ people use it only because the node AMQP library is a bit of a shambles. @josephholsten, @jgornick, is that latter the case for you?\n. @josephholsten, @jgornick Thanks guys. It sounds like I ought to treat rabbit.js more like a reformation of AMQP -- correcting some odd defaults and papering over some other things. (Being less concerned about mimicking ZeroMQ's API accurately may also be worthwhile, though it's a good model of simplicity)\n\n> I pulled out regular amqp to get acks, but it's a much more complicated but of code. I worry about other people \n> maintaining it because they have to grok amqp semantics before they can do anything.\n\nIn any case they would have to understand that `ack()` _must_ be called in some situations and not in others. Perhaps it's not too much of a stretch though. Friends don't let friends learn AMQP.\n\n> Anyway, I'm off to go learn zmq better.\n\nPieter once expressed it to me this way: RabbitMQ is good if you want to _use_ a message broker, ZeroMQ is good if you want to _make_ a message broker. Make of that what you will ...\n. > It would mean that unless they explicitly ACK the 'data' event, the message itself will not be de-queued?\n\nYes, pretty much. The idea being that if the client crashes before acking, the message gets given to someone else.\n\n> In this case, will the message be re-queued at the back of the queue by rabbitMQ or left in the same position at \n> the front and resend immediately to another consumer?\n\nBear in mind that RabbitMQ can't react to something NOT happening: either the consumer has to go away (by cancelling, or closing the channel), or the message has to be nacked (it's a RabbitMQ extension of AMQP).\n\nPrior to RabbitMQ v2.7.0, such message went to the back of the queue. Now they go back where they were; which is to say, usually to the front of the queue.\n. @alexfernandez Confirms (server to publishers) or Acknowledgments (from consumers to server)? I can see maybe one would want both; I don't know if node-amqp supports confirms though.\n. @scriby rabbit.js doesn't claim to be a fully-featured AMQP library, though you should be able to do most of what's in the tutorials. You might want to take a look at https://github.com/squaremo/amqp.node if you want finer control.\n\nHaving said that, I ought to get back to thinking about this one, clearly it's a common need.\n. I've implemented a WORKER socket type:\nhttps://github.com/squaremo/rabbit.js/tree/worker_socket#readme\n(scroll down a bit, to \"Socket types\")\n\nSome rationale for the API is in the commit message: https://github.com/squaremo/rabbit.js/commit/bc2bef1dfe91f89929311dc30c31b5aad394c9d4\n\nThis also adds the setsockopt option `'prefetch'`, for limiting the number of messages RabbitMQ will send you before you have to acknowledge some. It's described in the README.\n\nI'd be grateful if anyone who's still interested would give it a whirl.\n. > Is it possible that you update in npm\n\nDone, just published v0.4.0 of rabbit.js to npm.\nAaaand I'm actually going to close this. Thanks to everyone that chipped in their thoughts.\n. I'll just change to use an explicit loop-by-index everywhere -- it's clearer and (apparently) faster anyway.\n. (If you fancy preparing a pull request that would also be welcome!)\n. Looks like that was the only place a `for(..in..)` was used instead of a `for (;;)`. Fixed in master thanks to pull request #12\n. Thanks!\n. While I would like rabbit.js, among other things, to be an easy way to use RabbitMQ, I don't like the idea of allowing arbitrary AMQP to leak into the rabbit.js API.\n\nWhat is it specifically you want to accomplish? Is node-amqp more suitable, if you need fine control via AMQP?\n. OK, yeah if you want to be specific about exchange and queue properties it may be better to use node-amqp -- the API isn't that bad is it? (I do some work on that, including an in-progress API cleanup, so I would be interested to hear your views).\n\nOn the error event: yep, I agree rabbit.js should expose errors rather than swallow them or splode. Let me think about that one a bit.\n\nOn the `opts` parameters: hem. haw. hrrm. Yar, use your fork for the time being. I'll see if the idea grows on me.\n\nCheers, thanks for taking an interest in rabbit.js,\n-M\n. (I've closed this but I may merge yet ...)\n. Hi\n\nThere are races if you don't use callbacks/event handlers; specifically, the context 'ready' event, and the callback argument to connect, which both do I/O. How to arrange them is spelt out in the initial example in the readme (the other code snippets miss them out -- I ought to fix that).\n\nWould you try using those, and see if it helps?\n. Which version of the AMQP client are you using? `npm ls`\n. I ask because there are problems in the last couple of node-amqp (one introduced by me -- oops). Version 0.1.0 is the last known good.\n. Off the top of my head:\n\n```\nrm -rf node_modules/amqp\nnpm install amqp@0.1.0\n```\n\nIt's worth reading the man pages for `npm`: http://npmjs.org/doc/npm.html\n. You could try 0.1.1, which I'm pretty sure works with node 0.6.\n. Do you have rabbitmq running locally?\n. Hi,\n\nThe idea isn't really to think about queues (or exchanges), but in terms of the messaging patterns (PUB/SUB, REQUEST/REPLY, etc.). All of those that read from a socket have the messages buffered in a queue.\n\nIf you're trying to interoperate with AMQP clients, though, it helps to know that PUSH sockets and REQ sockets both publish to the queue that they are connected to. E.g., in\n\n```\n$ var push = context.socket('PUSH');\n$ push.connect('foobar');\n$ push.write(\"here is some stuff\", 'utf8');\n```\n\nthe message gets sent to a shared queue called 'foobar'.\n. Good point, and good idea!\n\nI would like it to still be able to be run without arguments; i.e., using a default port. I may start a feature branch for this and hack on it a bit.\n. I updated the examples, and as a part of that, made the port a command-line option. For the SockJS example too.\n. Do you have the management UI available for the production rabbit? If so, you could check, while the listener and send are running,\n1. there's at least two connections\n2. there's an exchange called \"foobar\"\n3. there's at least one anonymous queue (i.e., called amq-gen.<random-number-here>)\n4. the queue is bound to the exchange -- there's a little diagram which says the bindings\n\nor since you have rabbitmqctl, you could try\n\n```\nrabbitmqctl list_connections\nrabbitmqctl list_exchanges\nrabbitmqctl list_queues\nrabbitmqctl list_bindings\n```\n\nto get the same information.\n. The exchanges, queues, and bindings all look fine from rabbit.js's point of view.\n\nAll those blocked connections though -- not sure what's going on there. RabbitMQ will block connections when it thinks it's going to run out of space to put messages; but, you have only one queue, and it has no messages. So it can hardly have run out. Do you mind if I post this to the rabbitmq mailing list to see what people make of it? Also not sure why so many connections -- are there other vhosts set up?\n\nIn the meantime, one way to check that it's the blocked connection that's keeping messages from getting through is to look at whether Node has to buffer things itself; an increasing `context._connection.bufferSize` will tell you that.\n. Ah, looking at the `rabbitmqctl status`, it appears that RabbitMQ thinks there is about 200MB memory available to it. Does that sound right for the host you're using? I don't think RabbitMQ will handle running in that little memory gracefully; the memory alarm would constantly trigger, which would explain the blocked connections.\n. You'll be able to tell if that's the case by looking at the logs (default locations given in http://www.rabbitmq.com/relocate.html).\n\nThe memory limit will appear in a line similar to\n\n```\n=INFO REPORT==== 29-Oct-2009::15:43:27 ===\nMemory limit set to 2048MB.\n```\n\nand the alarm triggering will look like\n\n```\n=INFO REPORT==== 15-Sep-2010::14:53:44 ===\nvm_memory_high_watermark set.\nMemory used:53650536 allowed:52556595\n=INFO REPORT==== 15-Sep-2010::14:53:44 ===\nalarm_handler: {set,{vm_memory_high_watermark,[]}} \n```\n. OK no worries -- sounds like the right thing to do. Hey do you think I should make rabbit.js work with Redis too? I'd have to change the name I guess ...\n. RabbitMQ will put messages on disk if they are marked persistent (for recovery purposes), and it'll page queues out to disk when it starts running out of memory. The message index is in memory though, so there is a hard limit, unless you use the tokyo cabinet plugin to page the index out too. Mostly it likes to have things in memory, for sure.\n\nI might try a redis context just to see what it's like.\n. Good call, thanks!\n. Which version of node-amqp is present? `npm ls`\n. Ah good point, I pinned it to 0.1.1 because I had broken 0.1.2. There's a 0.1.3 now so I should update the dep.\n\nNow constructing test case for > 100kB messages.\n. Oh boy, I am a doofus. That's the second time I have got the arguments to Buffer.copy the wrong way around. Yeah I know -- testing.\n\nThanks for spotting this! I'll have a look at your changesets and push a new release to npm.\n. The fix is in https://github.com/squaremo/rabbit.js/commit/58bb1cc62aa4ac98cbea9ffe88f06cf1c79de02b so should be in master. But it won't be in any npm-available releases, since I am a bad person and I've not made any such for some time now.\n\nAre you definitely using a git checkout?\n. Yes, looks like the published AMQP package is failing the 'engine' test. https://github.com/postwait/node-amqp/issues/111 has a workaround, until Theo publishes a new release:\n\n```\nDownload the [node-amqp] tgz and use npm with the option to install from tgz to install it.\n```\n. There's now a release of node-amqp that supports Node.JS v0.8 (node-amqp v0.1.6; possibly prior releases).\n\nI've updated the dep in package.json to refer to a published node-amqp release again (in 6924d96dda50531863ff5d8032ae87e968d3f9af)\n. Hi, sorry I missed this for a couple of weeks -- bad library author!\n\nI'm not sure why you need two steps -- is server2 doing something to the request before it forwards it?\nIf server2 and the worker are doing independent jobs, you could dispatch to both in parallel and wait for both replies.\n. Hi Luc,\n\nI suppose it really depends on the nature of the analysis it's doing -- is it something the express handler process can do just by looking at the request? If not, then yes perhaps an extra step is warranted.\n\nYep, REQ/REP is an appropriate abstraction -- you can indeed make requests from different modules or processes and they'll be routed back to the right place.\n. (Housekeeping)\n. Augh, yeah looks like the next node-amqp release is <undefined> months away.\nThanks!\n. Are you using only rabbit.js? This error probably means testNODE has been declared from elsewhere, e.g., by using the Node AMQP client directly at some point. rabbit.js more or less assumes that won't be the case; i.e., that it has free range to declare what it likes (all AMQP programs have to assume this, in practice).\n. Ah I see. It's quite tricky to handle AMQP errors, since any error tends to invalidate the AMQP channel, meaning you have to construct a new one and start things again. But you are right that rabbit.js could probably do a lot better than let exceptions be unhandled all the way up.\n. @DuarteMadueno master branch now has error reporting on sockets; basically, if an AMQP channel reports an error, any socket using it will emit an error and close. If you've not moved on from rabbit.js, would you try it out to see if it works like you'd expect? Extra test cases very welcome. Thanks!\n. No worries! I understand -- it's more flexible using node-amqp directly, if a bit more work too.\nThanks for pointing out the problem in the first place.\n. This seems OK (i.e., I would merge it) -- why did you close it?\n. BTW you can supply username and password in the connection URL -- see https://github.com/postwait/node-amqp, section \"Connection\".\n\nNonetheless it may be useful in some circumstances to be able to supply an object rather than trying to assemble a URL string.\n. What is the 'durable' option supposed to make durable?\n. So, you want to be able to connect another SUB socket and get any messages you missed while disconnected?\n\nYes, you'll need `exclusive: false` and `autoDelete: false` (`durable` doesn't have that much bearing, it just means the queue won't go away if the broker restart -- but if you set the others to false you may as well set durable to true too).\n\nYou will also have to name the queue, because otherwise you won't be able to find it again -- a SUB socket lets the broker give its queue a random name. You'll also have to ask the PUB socket to send persistent messages.\n\nI can see there's a use for persistent subscriptions (not missing messages); although, ZeroMQ dropped this feature, possibly because it's too easy to leak queues if you don't reconnect. But as I've said elsewhere, I don't think the way to extend rabbit.js is to just expose AMQP in all its wartiness. If you truly want all the options, just use the AMQP client.\n. @jamuhl Just to be clear, I believe I understand the need, I just don't want to make rabbit.js as complicated as AMQP (because what would be the point of it). I've created issue 39 to deal with durable subscriptions.\n\nIs your AMQP wrapper on github?\n. Yeah, I was thinking of that, though I would probably put the optional argument (the topic) second.\n. In the amqplib-based implementation, there's no 'topic' option at all yet. There is an undocumented feature whereby you can pass an object to `#connect`, specifying the exchange name, type and routing key -- but this doesn't really help much if you want to change the topic for each write.\n\nOf course, `#write` already has a second argument, that is the encoding. So I think this would have to be another method, just for PUB sockets, perhaps `#publish(topic, message, [encoding])`.\n. OK I have implemented `#publish` as above. There are implications.\n\nOne is that it doesn't make much sense unless a SUB socket can subscribe, in some way, to different topics. Here I've followed ZeroMQ, in that subscribing is distinct from connecting:\n\n``` js\nsub.connect('events');\nsub.subscribe('critical.*');\n```\n\nBut it's a breaking change, and the semantics are not obvious (do all subscriptions apply to all connections?), and it's all a bit confusing. So I may return to the structured-connection-address mentioned above -- that is, `sub.connect({address: 'events', topic: 'critical.*'})` or even `sub.connect('events', 'critical.*', callback)`.\n. > But it's a breaking change, and the semantics are not obvious (do all subscriptions apply to all \n> connections?), and it's all a bit confusing. So I may return to the structured-connection-address\n\nActually, I have added an optional argument to `#connect`, just for SUB sockets, for the topic. The kind of exchange is determined by the `routing` option given when creating the socket.\n\nSo, in its entirely, it looks like this:\n\n``` js\nvar sub = ctx.socket('SUBSCRIBE', {routing: 'topic'});\nvar pub = ctx.socket('PUBLISH', {routing: 'topic'}); // must agree on routing if they are to talk\nsub.connect('events', 'critical.*'); // the third argument would be a completion callback\npub.connect('events'); // ditto second argument\npub.publish('critical.alarm', \"Emergency!\", 'utf8');\n```\n. Noted. It's a shame JS is so weak on iteration, usually this sort of thing would be trivial without needing to expand the API. But you can do\n\n```\nfor (key in {'A': true, 'B': true, 'C': true})\n  conn.subscribe({address: 'users', pattern: key, routing: 'direct'});\n```\n. Or\n\n``` js\n['A','B','C'].forEach(function(x) {\n  sub.connect({address: 'users', pattern: x, routing: 'direct'}); });\n```\n\nalthough callbacks are a bit tricky.\n. Closing this to take it off radar; I think using forEach is adequate.\n. It would be really handy to know what exactly is the frame that trips it up.\n\nIf it happens after ten minutes then running it with debug logging on might generate too much output, or even prevent the bug. (But you can try: run the program with the env variable NODE_DEBUG_AMQP set to something; e.g., `NODE_DEBUG_AMQP=true node myprogram.js).\n\nAny chance you can hack `node_modules/amqp/index.js` to log the offending frame when it throws an error in `Connection._onMethod`?\n. One problem I can spot in the sample code is that it calls `connect` with every post -- thereby doing the same work over and over again. It's intended that `connect` is called once for each resource (e.g., `'pre'`) to which you want to connect.\n. > IMHO the simplest way to go about writing a library interface API to rabbit.js is to expose all the functionality of\n> rabbitMQ in an easy to use way and give users access to provisioning and controlling all the features rabbitMQ \n> has to offer. \n> Hence letting users write their own abstraction levels via their application layer if they so desire. Of course users \n> can use AMQP directly, but that is too low level and not necessarily rabbitMQ-centric.\n\nWell, I didn't start rabbit.js with the intention of exposing all the functionality of RabbitMQ. A better route to that is an AMQP library with RabbitMQ's extensions added into the protocol (e.g., exchange to exchange bindings, publisher acknowledgments). My intention was to have a simple messaging model that aligned with the Node.JS way of things fairly well, that happened to use RabbitMQ.\n\nIt's become apparent since that some people are using rabbit.js because node-amqp is too complicated (which it inherits from AMQP) and inconsistent in some places (e.g., some methods return promises and some don't). Some of those people find that some things they need, that they know RabbitMQ does, are missing from rabbit.js; for example, acknowledgements. I'm willing to grow rabbit.js in those directions if that will make life easier, since that's after all the point of the exercise -- on a case-by-case basis, and addressing the actual problem people are having.\n\n> By making certain assumptions (and thus hardcoding options - ie \"the handful of permutations\"), you probably fit \n> the bill for a percentage of use-cases, but as it so happens, most people (including myself) need more \n> flexibility/control over lifetime of messages, queues, exchanges, etc.\n> By exposing all options, users 1) know exactly what they can change w/o having to guess what the option \n> nomenclature is 2) can see exactly what each default setting is 3) can change each setting as per their own \n> application needs. I also took care in segregating exchange, queue and message options. I believe your API \n> muddles the lines a bit. Your library is very well written, so it's a bit of a shame IMHO not to use it to its full \n> extent! \n\nI don't see any moral difference between e.g., node-amqp and rabbit.js-plus-options-for-everything.\n\nThere are some (potential) parameters to rabbit.js's behaviour that it may make sense to codify in some kind of policy. For example, message lifetime, or dead-lettering. But again I'd want to look at these on a case-by-case basis, and be very deliberate about how they are exposed.\n\nSo, all in all, I would rather look at your development requests individually than accept this pull request. I appreciate you likely spent a fair bit of time on it. Perhaps with a bit more consideration we can come up with a mechanism that allows flexibility you need, without sacrificing the simplicity of rabbit.js.\n. What is your example demonstrating? (I think it's incorrect behaviour)\n. > I tried rabbit.js/example/socketio on Node.js v0.11.0\n> However, chat message has been replicated by the number of clients connected.\n\nI see what you're getting at.\n\nThis seems to have been broken, by coincidence, between Node.JS v0.8 and Node.JS v0.10. To be fair, it was working by coincidence in the first place.\n\nIn v0.8, an EventEmitter's event listener map is created lazily in a method, while in v0.10 it's created in the constructor. Using util#inherits avoids this by not invoking the EventEmitter constructor (https://github.com/joyent/node/blob/master/lib/util.js), referring instead to the prototype.\n. Fixed in master, which now uses `util.inherits` everywhere.\nNB in general, it's also necessary to call the \"super\" constructor to make sure any fields that need to be initialised are initialised (with EventEmitter it happens to work if you don't.)\n. Thanks!\n. I think this is caused by the same breakage as #35. To wit, it\n- works OK in Node v0.8 (and probably some v0.10.x, x < 4)\n- works OK in rabbit.js master\n- doesn't work OK in Node.JS v0.10.4 prior to commit 8b431bf753480ade572fb245bb235071c62fa8c7\n. OK I think that second thing is fixed in #35. Specifically, with \"works\" meaning it prints out things just once, it\n- always works with Node.JS v0.8\n- works with Node.JS and rabbit.js master\n- does not work with Node.JS v0.10.4 and rabbit.js after `git checkout f493e78`\n\nI'll look into the first thing ..\n. > I'll look into the first thing ..\n\nThe first test case above also appears to be due to EventEmitter not working the way it used to. (It used to work by coincidence; now it's broken by coincidence).\n\nIf you have a moment would you check with a master checkout? Thanks!\n. Ha, yes good point, well made. Yeah I ought to do a trawl for any other breakages, and post a new release. It's been a while.\n. I guess one might put a long TTL on a queue to give yourself plenty of time to reconnect, so yeah it might be useful to be able to say \"OK I am definitely done with this socket, destroy it forever.\"\n. Presently it's a toss-up between adding things in, and reimplementing rabbit.js on https://github.com/squaremo/amqp.node (in the expectation that it would then be much less painful to add and fix things).\n\nI hear you on the merging; sorry about that.\n. > Presently it's a toss-up between adding things in, and reimplementing rabbit.js on https://github.com/squaremo/amqp.node (in the expectation that it would then be much less painful to add and fix things).\n\nOn that point, I've reimplemented rabbit.js on amqplib in the branch `amqplib`; this is available in npm as 0.2.1-amqplib. My plan is to keep developing that and retire the node-amqp implementation.\n. This is implemented in master now, as the socket creation options `resume_name` and `resume_grace_period`.\n. Thinking aloud: is `Socket#destroy()` appropriate for this?\n. > Among other things this would delete the appropriate resources (ifUnused).\n\nUnfortunately this won't work, for a couple of reasons. Firstly, AMQP is stupid (who knew?): ifUnused will cause the channel to break should the queue or exchange be in fact used. Secondly, it wouldn't help anyway, because \"used\" means the wrong thing from our point of view. In the case of exchanges, it means bindings from the exchange, which won't always be in place (e.g., if there are no sub sockets at the time). In the case of queues, it means consumers, which again may not be in place.\n\n> Thinking aloud: is Socket#destroy() appropriate for this?\n\nNo, of course not, me. That's for invalidating the whole socket.\n. > Among other things this would delete the appropriate resources (ifUnused).\n\nI can't think of a way to make sure exchanges are garbage collected.\n. Good idea (though it'll only work with fairly new RabbitMQ versions).\nThanks!\n. You will still have a problem if you simply open more than 10 sockets without closing any. Rather than removing the event listener, I've charged the Context with keeping track of its sockets, and passing on errors. This means there's one error handler, registered by the context.\nCommit 6075a48ea7b1ea7d334b9cfe86958240f98562cd, on master, if you'd like to try it. It also introduces a Context#close, which was missing.\n\nThanks!\n. Yes it probably is best to close exchanges when done with them -- except they may be used from e.g., another socket. So it really needs to have a kind of garbage collection, rather than outright close.\n\nThat aside, can you explain a bit where you think the race condition is?\n. Thanks @wolfeidau, yes that makes it clearer.\n\n> When i went to write a test case I could NOT get the damn channels to go away without a setTimeout it was like \n> they took time to clean up, NOTE I am calling exchange.close() to based on my reading only close the exchange \n> not destroy it on the server.\n\nWhen you do `#close`, there's an RPC to the server, so there's some intervening I/O. You're quite correct, it doesn't destroy the exchange on the server; it just closes the channel that represents the exchange in the client. Having a channel for each exchange is kind of weird but that's node-amqp for you.\n\nOne thing I don't quite get: are you publishing to a new exchange each time? Because I thought node-amqp cached open exchanges, so if you are always publishing to the same place, you'll get back the same exchange object -- so no channel leak.\n\nWhat I am worried about is this scenario:\n\n```\n1. socket#1 connect to 'foo'\n2. socket#2 connect to 'foo'\n3. socket#1 publish and close\n4. socket#2 publish -- but 'foo' is closed\n```\n. Hi, sorry I have left this sitting here for a bit -- I've been working on https://github.com/squaremo/amqp.node.\n\nI will have a look myself at what amqp-node is up to. In general though, you are quite right, that rabbit.js isn't tidying up, so I need to fix that one way or another.\n. OK so one thing from that test case is that `Socket#end` doesn't cancel subscriptions or close the socket, meaning it won't result in the socket emitting 'end'. Arguably it should, at least for duplex streams: it's not in the \"specification\" for streams, such as it is, but it's certainly expected that ending a stream will provoke the other side into also ending (`#end()` is a writable stream thing; `on('end', ...)` is a readable stream thing).\n. I took a look at the node-amqp code. Oh dear.\n\nnode-amqp will always create a new channel every time you use `Connection#exchange()` or `Connection#queue()`. It also records them, indexed by the name. So, if you call `Connection#exchange()` twice with the same name, it will forget about the first one.\n\n\"Why does it record them?\" you might ask. Well, there appears to be two reasons. One is that it tries to keep track of bindings, so if I use queue.bind it'll increment a counter on the exchange mentioned (never mind that bindings are idempotent, or that it overwrites its records); the second is that there's some kind of reconnect mechanism whereby exchanges and queues are reinstated. It doesn't look like that could ever work, but maybe I'm missing something there.\n\nThe good news is that since it's not (or no longer?) caching them, always closing things once you're done is totally fine.\n. OK, after something of a longcut I think I've adequately tidied up the erm, cleanup. Each socket now registers any channels it opens (which is sometimes a few, since in node-amqp even checking for the existence of an exchange or queue involves opening a channel. Actually I could probably mitigate that a bit by closing the ones I don't need ..).\n\nAnyway, you should no longer see an ever-growing list of channels, provided you close sockets once you're done with them. You may want to cache sockets yourself if you can, to avoid all the channel opening.\n\nDo you have a test system you can try it in? There's an 'open lots of sockets' test but it doesn't exercise the code over a long period.\n. > OK I built a rather primitive example of using rabbit.js with express.\n> \n> This is located at https://github.com/wolfeidau/node-rabbitjs-test\n\nBrill, I will take a look.\n\n> I am interested to hear what you think, for the application I am developing I still have the issue that I am using a range of endpoints so pooling as I am doing here is probably not going to work.\n\nYeah, it sounds like you may want to use a \"lower level\" lib for more message-by-message control; or, maybe, to encode extra information into the messages, for dispatching by the receivers.\n. Just closing this for housekeeping, but feel free to revive the discussion.\n. You are right, that sounds very like the breakage due to me not using util#inherits (or the like), as reported in #35, #37 and #38, and supposedly fixed in https://github.com/squaremo/rabbit.js/commit/8b431bf753480ade572fb245bb235071c62fa8c7 and released in v0.2.1. \n\nI'll have a closer look at your code and try it out.\n\nJust to check:\n- are you using rabbit.js 0.2.1?\n- does it work in, e.g., Node.JS 10.3? (that's 10-series, but before the problems in those issues)\n. I'm surprised that runs as-is, since it doesn't synchronise on the context being open (emitting 'ready').\nIf I stitch that in, the tests appear to run OK. That's with rabbit.js v0.2.1, and Node.JS 0.10.4 and 0.10.12.\n\n``` javascript\n  it('should open a publish stream', function(done){\n\n    log('open')\n\n    bus.rabbitContext.on('ready', function() {\n      bus.publish('sometestpub', function(err, stream){\n        log('stream', 'publish');\n        stream.should.exist;\n        stream.write(\"TEST TEST\", 'utf8');\n        done()\n      })\n    })\n  })\n```\n. Events and callbacks are almost no use for synchronisation, it is true. My adjustment was pretty crude -- just enough to delay the test run, given some assumptions (specifically that the test harness serialises tests).\n\nI am now seeing some \"send 'channel.open' seen\" errors, in a different scenario. So I'm not sure it's entirely down to the problems fixed already. Stay tuned ...\n. Ops, yes you are quite right, the README is misleading. I've just pushed 21d21c8fc01f21ef0f0b6e8001bfaef04700171e, which converts expiration to a string. (It's a string in AMQP, to admit different ways of specifying a time, e.g., '2013-07-18T15:13:00Z', though RabbitMQ only accepts stringified numbers.)\n. I should say, until the next release to npm, it's best to stringify the expiration yourself.\n. Nice. Thanks!\n. > It only applies to Streams which are pull-style. For streams like network streams, your _read implementation is usually a noop, and you simply push when you get data.\n\nWell, for a network socket, one could stop reading from the socket. My comment is just observing that there's no reasonable equivalent in AMQP. Given that the raison d'\u00eatre of streams is to propagate backpressure, it's a shame.\n. > The data would continue to pile up regardless, just in the buffers of the Streams2 API, or perhaps in the network buffers.\n\nIn the kernel's buffers, I would _very much hope_, and at some point this would result in backpressure upstream (for TCP at least).\n\n> The Streams2 API just makes it so you don't have to worry about it as a producer, whereas before pause and resume were advisory.\n\nUnbounded buffers anywhere are seldom a good idea ...\n. > > Unbounded buffers anywhere are seldom a good idea ...\n> \n> Very true. That's why the high-water mark is important.\n\nThe point is, `highWaterMark` is advisory, the same as pause/resume. So if one does things like `.push` whenever data comes in, the buffers are effectively unbounded.\n. (Just closing this for housekeeping)\n. > Did I get this completely wrong?\n\nNot at all, that's a good analysis.\n\nThe synchronicity requirement comes from ZeroMQ's REQ/REP design, which admits intermediaries. The intermediaries can pass things along as fast as they get them, _provided_ the end points are synchronous. If they aren't, then as you've identified, answers could come back out of order. In the case of Rabbit.JS, RabbitMQ is the intermediary, of course.\n\nYou could keep a window of correlationIds at the requester, and only yield a message when the head correlationId has a reply. It's a bit like the moving window used for confirmations in amqplib.\n. > how to ensure the responder is orderly...\n\nIt doesn't matter, so long as it puts the right replyTo and correlationId on the response.\n. Ah I see. Yes, the responders have to process just one request at a time, in that case; tricky if you are sending the request off to somewhere else.\n\nYou can always have a number of responders (you have to pick a number, and that's always fishy); or have a pool where you create a new responder every time they are all busy. But this is getting away from the supposed simplicity of rabbit.js.\n. The other other side -- the gripping side --  of the coin, is that this means it's relatively easy to overwhelm Node.JS programs, since they don't (tend to) bound the number of in-progress requests. No customer has to wait to give their coffee order, but the cashier runs out of counter-room on which to put graffitied paper cups ..\n. That'll do it, nice.\n\nDare I say it, futures handle this kind of situation very nicely, now that I think about it. Have a response queue, which gets a future for each reply; then loop through the queue, dereferencing the futures serially.\n. By the way, I implemented the correlationId windowing in 0a946044e91d292ed6c8a8335b226312df1cf652\n. It's generated and added to the AMQP message when you write to the socket, and kept in a buffer to match replies against. The idea being that you read the replies back in the order that the requests were written.\n. In this case, once the rabbit.js connections are down, your SockJS connections are no good either. So I think you'll have to close those as well, and force the clients (i.e., the code running in browsers) to reconnect -- their messages are going nowhere in the meantime anyway.\n\nThe alternative is to buffer messages (from the browser) while RabbitMQ is offline, but I think that's more complicated for no particular gain. If you want reliability, you can't really mask the fact that the browser is going to have to retry things (whether it's replaying messages, or reconnecting, or whatever).\n. > Might just be best to let the node server crash and have forever restart it then i think.\n\nYes, that's reasonable -- that way you can be confident all the resources will be freed up (although, it's not an excuse not to _try_ and clean up ...).\n. There's some non-determinism working against you here. Specifically, the server hasn't necessarily finished closing the first sub socket by the time you're opening the second. This means two things: it hasn't released its \"exclusive\" lock on the queue (that which is named by `\"subs.*\"`), and it hasn't necessarily stopped delivering messages to the first socket.\n\nWith regard to that latter: I think, though I'll have to check, that amqplib stops processing any deliveries once you've closed the channel (quite reasonably, and in fact it's a requirement of the AMQP specification), which is why the `'data'` event doesn't fire. It's also the case that amqplib multiplexes \"fairly\" among channels -- more apparent non-determinism -- which might be how the PUB socket's message can seemingly overtake the SUB socket's close.\n\nYou can see it working by introducing a delay -- it can be small delay, but perhaps not as small as setImmediate -- after `sub.close()`. It's enough time for the server to release the lock on the subscription queue and for the first socket's 'consume' to be torn down.\n\nOh, and there's another thing at work here, which is a breaking change I've introduced in master (but not in a release, yet): you have to call e.g., `sub.subscribe()` on a SUB socket to receive messages. Technically it's `sub.subscribe(topic)` for some `topic`(s), but a falsey value works fine in the default case.\n\nTo be honest I am still working on these features, and actually I'm a bit undecided about them, or at least how they work at present.\n. By the way, the SUB socket uses 'exclusive' to avoid (by means of loudly exploding) the situation in which you have two or more live sockets using the same subscription -- which, for admittedly implementation reasons, would lead to each socket getting only some of the messages.\n\nThe feature as a whole is more for the scenario in which your whole program exits, exceptionally or otherwise, and you want to pick up where you left off. There is still some message loss possible -- in fact probable, if there's a fairly constant message flow. I could use acks to work around that, but it does make things rather slower (it could be an option I suppose).\n. (NB I've taken \"resume*\" onto its own branch for the time being -- I wasn't very happy with the implementation or semantics, and it might interact poorly with other options. In other words I want to think it over again)\n. > I've seen some interesting and very useful changes in the master branch and I'm wondering when \n> they will be released, are you close to a new release?\n> Or otherwise, would you say it's safe to point npm to the master branch directly in the meantime?\n\nI don't think it's safe to point npm at a git repo in general. Lately I've changed my mind back and forth in the master branch, which I ought to have done in a feature branch, so there's an illustration of why not!\n\nI'm not sure about the \"resume\" stuff yet, so I'll probably not release that for the time being. I may make a release without it, though.\n. I should say that while I don't recommend using git master in your production system, I would be grateful for any reports of using the new features in development\n. > So now we are switching into creating different sockets for the different topics, and closing them after we don't need them anymore.\n\nRight, yes that is just the thing `#publish` and the `'topic'` option are designed for.\n\n``` js\nvar pub = ctx.socket('PUBLISH', { routing: topic });\npub.connect('events');\n// ...\npub.publish('component.warning', JSON.stringify({foo: \"bar\"}), 'utf8');\n```\n\n> Another thing is that we need our published messages to be stored in a queue so that they are not lost if the SUB socket hasn't connected yet\n\nYou would have to create a queue per subscriber, before starting any publishers, which is effectively the same as opening the sub sockets before you start any publishers. There's synchronisation required whatever you do.\n\nRabbitMQ and AMQP won't help you with this, or not much. There's no provision for consumers catching up with old messages. If you cannot miss messages, better to stick them in a database and replay them for new subscribers, although you still have a kind of synchronisation problem (that is, figuring out when to switch from the replayed messages to the freshly-published messages).\n. > For now we switched to creating and closing sockets for different topics until there's a new release. Do you think this is going to be a problem in terms of performance or anything like that?\n\nYou are right that the connection is created just once; but, each socket still uses its own channel and may require some round-trips to declare various queues and what-have-you, so creating sockets is a little expensive too.\n\nPerhaps I should roll a release with the topic stuff, and put the resume stuff on a branch for later.\n. v0.4.1 is published to npm. It has the topic stuff and the `persistent` option, but not the subscription resume stuff.\n. Yes, that would be complementary. Magic strings and magic properties are of an ilk in JavaScript, but it does feel nicer, I agree.\n. If you close a socket, it won't actually be closed until its outgoing buffer is flushed. So you should be able to synchronise on getting the 'close' event.\n. > Are you saying to disconnect the worker when I get a close event on a rabbit.js socket?\n\nYes; the idea being that the 'close' event means everything on that socket up to and including the close signal has been sent, and no more events will be forthcoming.\n\nI took 'connection' above to mean socket. Did it mean 'context'? That would be slightly different, since closing the underlying connection discards anything not yet sent. You'd want to synchronise on closing any sockets you have, first. If there's more than one to a context, you could use a countdown latch, say.\n. So, broadly, your worry is that there may be outstanding I/O at the time you want to shut the worker down?\n\nThere are some things you can do (not necessarily available in this library, though) to make sure there's no outstanding AMQP messages, or to put them back on the queue if you don't handle them, but in general, if you're killing workers in order to reload code, that you'll have to be prepared for this, and have clients retry.\n. > I would prefer something with a callback: \n\nThe trouble is that this callback would have to be passed all the way down to the socket, and the underlying library just doesn't support that.\n\nIn any case, there are two things working in your favour here. One is that writes don't get buffered at the (rabbit.js) socket, they are given straight to the AMQP client. The second is that Socket#close closes the AMQP client channel, which won't take effect until any writes are flushed. In other words, things should happen in the order that you do them.\n\nThat's the theory anyhow. Are you doing the `setImmediate()` because you ran into a problem with just closing the socket after writing?\n. The other thing that occurs to me is that I don't support the Writable event 'finish', which is supposed to be called after 1. the stream has been ended and 2. all writes have been flushed. Consider it on the TODO list.\n. > i'm doing that because without it, the message never gets sent. I don't get any errors or anything\n\nHrm, so it doesn't work i the way I claim. That may be a problem in amqplib that I need to investigate.\n\nI'm not completely sure I can get 'finish' to work, or not in the expected way. There are buffers in-between a rabbit.js socket and the network socket, and 'finish' does not compose in the sense that 'finish' from the buffer you are dealing with does not mean 'finish' in the operating system buffer.\n\nJust to be clear, does `done()` exit the program?\n. > Just to be clear, does done() exit the program?\n\nI ask because exiting, or closing the context, will drop any frames in sockets on the ground.\nThe way to do it is to wait until the socket has closed cleanly:\n\n``` js\ntriggerSocket.close();\ntriggerSocket.once('close', done);\n```\n\n(This doesn't mean I won't support `'finish'`, which might be more convenient in some circumstances)\n. Hrm. It's an ugly conclusion, but I suspect you may be right about that last. Emitting 'error' without any listeners present is supposed to cause the program to exit, but somewhere this mechanism is being sabotaged (I suspect it's inside promise machinery).\n\nI will try to sprinkle some setImmediate dust through the code.\n. Great minds -- that is what I've done in 347c73b4722ce0038233216d6026ad6ad34ed754, for Context.\n\nThe sockets themselves pass through the underlying channels errors (OK I need to test that), but I will probably need to give some of the socket setup and connect code the same treatment.\n. > I will probably need to give some of the socket setup and connect code the same treatment.\n\nDone. Fancy giving it a run?\n. Grand, thanks Derick. Expect it to make its way into a release soon.\n. Part of the contract of push or req is that the message goes to a single consumer. Routing (i.e., sending to an exchange rather than a queue) breaks the contract by allowing a message to go to more than one consumer, so one might get several replies back for a request, for example.\n\nDo you want to round-robin messages among connections, like push and request do? Or just have pub/sub but with replies; there is some precedent for that pattern, in nanomsg: http://nanomsg.org/v0.3/nn_survey.7.html\n\nWhat's the high-level requirement -- maybe there's another way to accomplish it.\n. I think I see: you have a bunch of different kinds of job, and you want to be able to partition them into a smaller number of queues, for workers to pick up.\n\nYou could partition things statically, by choosing your partitions ahead of time and naming the queues for them. In effect the producers and consumers then have to know the routing -- e.g., \"I have a stripe:invoice.created, so it goes in the 'stripe:invoice' queue, then the consumer can look at it in more detail\". But I can see why your mind returns to exchanges and routing -- it seems like this ought to be dynamic.\n\nA bit of ancient history: In JMS there's the notion of a 'selector', which is like a queue query. You can consume from a queue while supplying a selector, to filter the messages that you're interested in.\n\nAMQP, which derives in large part from JMS, doesn't have selectors. They are supposed to be subsumed by exchanges and routing (which also supposedly cover JMS Topics), but sadly, the designers of AMQP missed a trick: selectors do something like routing _while also_ guaranteeing that 1. messages won't be dropped on the floor if no-one's there to listen, and 2. only one consumer will see each message. There's just no way to do those in AMQP.\n\nNow back to the present. You can probably see where problems will arise: there's no way to guarantee that messages won't be dropped on the floor, if the _producers_ don't know how they will get routed, since each message will go through an exchange and there may not be a binding in place to route it to a queue.\n\nThere's ways to mitigate this in AMQP -- using the mandatory (\"return if unroutable\") flag, among others -- but they break the simplicity of the rabbit.js model, and I'd like to avoid that.\n\nThere's no inherent problem with having tens of queues, by the way -- the messages would get buffered somewhere, and partitioning them into queues gives the right semantics (each message to one worker). Maybe there's some way the API can be more convenient though? For workers it's not too bad, you just `#connect` to each queue you care about. For producers though, well you'd have to have a socket per type of job, and pick the right one. On the other hand, something like `#publish`, where you mention the job type when you send the message, wouldn't guarantee that there's a queue at the other end (or at least, it would be expensive to do so). Perhaps:\n\n``` js\nvar jobs = ctx.socket('TASK');\n// Connect to all the task queues we will want to send to\njobs.connect('stripe:customer.created');\njobs.connect('stripe:customer.updated');\n// ...\n\n// Send to a specific task queue\njobs.send('stripe:customer.created', JSON.stringify(job), 'utf8');\n```\n\nNote there's absolutely no wildcard trickiness possible, it's all still sending straight to queues.\n. > there are hundreds of jobs and dozens of job types that will be built, changed, maintained and updated over the years. It's not something that can be set up front and have hard coded.\n\nRight, I understand. It would be nice to have the best of both worlds -- that is, to be able to verify that you won't miss messages, _and_ to be able to have a rolling evolution of the system. But that's probably outside the scope of rabbit.js, if not the system you're building.\n\nIn any case, alternate exchanges and dead lettering will help you keep track of exceptions at runtime -- messages that don't get processed either because no-one's listening, or because whoever got it was unable to process it. In recent versions of RabbitMQ you can set these with a \"policy\", i.e., apply them to exchanges and queues as matched by a regexp you give. (In less recent versions they are an argument to exchange.declare or queue.declare.)\n\nSetting an \"alternate exchange\" will give you catch-all route for messages that would otherwise get dropped. Dead lettering and per-queue expiration will give you unprocessed messages. I think these are out-of-scope for rabbit.js, but you can certainly use rabbit.js to get at the messages, by e.g., routing dead letters to a queue and having a worker socket in its own process to log them all, blink a red LED, email an operator or whatever.\n\nSo with all that in mind, what do you think of the TASK socket?\n. > So with all that in mind, what do you think of the TASK socket?\n\nI think this would survive dozens of job types (after all, every sub socket has its own queue, as does every request socket), and online repartitioning of the workers, and so on. The question is, do you need wildcard matching?\n\nI _think_ you're suggesting that you will, so you can send to `\"job.foo.frobnicate\"` and have a worker that's dealing with `\"job.foo.*\"`, rather than mention frobnication specifically. I would point out that you have to change code _somewhere_ to send new kinds of job, and that you can start extra workers to pick up the new kinds of job while you change the old workers to handle it. In the wildcard case, the old worker would just start getting jobs it didn't know how to handle.\nBut perhaps I'm off on a tangent.\n. I've implemented the TASK socket in master. (If it turns out I'm off track, we can think of it as an hilarious April Fools prank.)\n. @derickbailey I'm interested to hear where this ended up for you -- did you get something working in a fork, try the TASK socket, do something completely different ..?\n. I knew I'd run into this before: https://github.com/squaremo/amqp.node/commit/e1bcc0edc313e70fb7972f525ef7a0c273ef215c\n\nYep, `nextTick` ought to be `process.nextTick` -- and, I should actually use `delay` everywhere if I'm going to be tricky about assigning it.\n. `'routing'` is only accepted as an option to the socket constructor. It appears in the code e.g., https://github.com/squaremo/rabbit.js/blob/v0.4.1/lib/sockets.js#L222\n\nThis is not made especially clear, looking at the documentation as it stands. I'll fix that.\n. > 'routing' is only accepted as an option to the socket constructor.\n\n``` js\nvar sub = ctx.socket('SUBSCRIBE', {routing: 'topic'});\n```\n. > Why is routing only supported on the instantiation of a socket?\n\nBecause it doesn't make much sense to have different routing for successive connections, since the topic for any publication will then have a variety of meanings. I'm sure one can contrive an example where this is useful, but it's easier to just have different sockets.\n\n> EDIT: I'm getting an error:\n\nIn all likelihood it's because you've connected to the an address as you've used before, but with different routing. The underlying protocol (AMQP) doesn't like things changing in this way. The way to fix it is to go into RabbitMQ (the management console, ideally) and delete the exchanges named for the addresses.\n\nThis is a bit of a pain, I know. I've not thought of a good way of garbage collecting things created by connections (or even deleting them explicitly). It's a problem with AMQP, too.\n. Both of those things are possible. What's not possible is to have a single socket that uses two different kinds of routing.\n\n\"connection\" above refers to calling \"connect\" on a socket, so, i.e., a socket can be connected to a number of addresses.\n. ``` js\nvar sock = ctx.socket('SUBSCRIBE', {routing: 'topic'});\nsock.connect('address', 'routing.key');\nsock.connect('address', 'other.routing.key');\n```\n\n> There documentation isn't very clear.\n\nDo you think examples for each method/option/whatever, along the lines of the above, would help?\n. > Does sock.connect create a new connection, or does it just modify the existing connection on the sock\n\nIt creates a new connection; the socket retains all its other connections. It's must useful for SUBSCRIBE, to get messages for more than one routing key, and WORKER, to listen for more than one kind of job. Otherwise, the benefit of multiple connections is largely moot.\n\nThere's no \"disconnect\", but not for any principled reason.\n. > Are there any potential leak issues to be aware of?\n\nThere is one, and I'm not sure how it can be avoided. Each connection creates resources in the RabbitMQ broker (queues, exchanges), and not all of these can be garbage collected.\n\nFor most programs, it's not a big deal, since they will usually connect to the same addresses again and again. But say a program generated random addresses to connect to, and connected and disconnected repeatedly -- then it would leave junk around.\n\nThe reason it's difficult is that deleting the resources when the socket is closed is the wrong thing to do -- what if another socket is connected to the same resources (which after all is the whole point!). AMQP has some means by which unused resources will be removed, but they are not adequate.\n. > This is not made especially clear, looking at the documentation as it stands. I'll fix that.\n\nI've removed `routing` from the socket options section and mentioned it elsewhere instead. I've also put an example of topics and routing in.\n. > In #59 (comment), you suggest that you can create one socket, and handle different routing keys separately\n\nThat example shows a single socket being subscribed to two keys -- it will get messages from _either_ key. There's no suggestion that the messages are handled differently.\n. The problem is that the `#connect` hasn't completed -- it has to do a round-trip to the server to assert the presence of the queue (`'backend'`, in this case), and it doesn't consider itself connected until that has returned, so writes go nowhere.\n\nThe way to synchronise is to supply a callback to `#connect`:\n\n``` js\ncontext.on('ready', function(){\n    console.log('ready')\n    req.connect('backend', function() {\n        req.write(JSON.stringify({\n                method: 'users.login',\n                info: {}\n            }), 'utf8');\n    });\n});\n```\n. Oh -- I'm sure it used to be. I have a growing pile of documentation editing to do!\n. OK, I have mentioned this in the docs. Thanks for pointing these out, it's made me realise I've been letting the docs fall behind and get messy.\n. rabbit.js doesn't do anything special for `setEncoding`, it leaves it to the readable stream implementation. If I put\n\n``` js\nvar ctx = require('rabbit.js').createContext();\nvar sub = ctx.socket('SUB');\nsub.setEncoding('wrong');\n```\n\nin a file and run it, it crashes with an \"Unknown encoding\" exception. This makes me think, maybe you are calling that from a callback or promise continuation, and the exception is being swallowed. Could that be the case?\n\nHence or otherwise, would you supply a full, minimal example that freezes -- ta.\n. The answers will come back in the order that you sent them, so one thing you could do is keep a queue (array) of the callbacks, and dequeue (`shift`) every time a message comes in.\n. Yes, an ID system would work fine. The discussion here is apposite: https://github.com/squaremo/rabbit.js/issues/48\n. @samholmes How did you get on?\n. @samholmes, just checking that this is working for you. Do you think it's worth me including something in rabbit.js to make this easier? It doesn't fit in the socket-style API, but perhaps it could be a utility.\n. Is that because you are using it from another library?\n. Right. And those other applications need it to be durable, so e.g., bindings don't go away.\nI suppose it would make sense if a \"persistent\" PUBLISH socket declared its exchanges as durable. Does it belong in the socket constructor or `connect` though? It could be both.\n. The constructor arguments tend to just set the default for that socket, so I guess it can be in both places.\n. OK. Sorry for the long silence. I have pushed a change to master which lets you tell a socket not to try and create objects when connecting; this means you can use exchanges and queues created elsewhere with properties different to those rabbit.js would give them.\n\n``` js\nvar ctx = require('rabbit.js').createContext();\nvar pub = ctx.socket('PUB', {noCreate: true});\npub.connect(existingExchange);\n```\n\nIf anyone is game to try this out, would you let me know if it works for you? It's pretty sketchy but I'd like to build on it. Thank you!\n. I am thinking that a 'persistent' socket, either set in the constructor or by setsockopt, should declare its exchange to be durable. Does that break anything? (except backwards compatibility)\n. > Does it break backwards compatibility?\n\nIn the sense that someone already using a \"persistent\" PUB or SUB socket will encounter breakages, since rabbit.js will try to re-declare the exchanges involved with a different 'durable' property (AMQP doesn't like this).\n. It's not usually a good idea to leave messages unacknowledged. Do you mean instead that one should be able to _reject_ a message?\n. If you fail to acknowledge a message, it will remain in limbo until you close the connection to the broker. I wouldn't think that's what you really want -- or at least, I don't think it's what most people would want.\n\nAs well as discarding messages, there is an option to requeue messages. Requeueing a message means other workers might get it, or you might get it again. (When I say \"option\", I mean it is possible in AMQP, the protocol for talking to RabbitMQ). There's no option to say \"redeliver later\", where later is some specified delay, although it could probably be arranged with some extra complication.\n\nI'm not really sure which, if any, of these you really want.\n. Yes, and the library I'm using supports `nack` (and its simpler friend, `reject`), with variations for requeuing or discarding. There is a chance -- a high chance, if there's only one worker -- that the same worker will just get the message again, since messages are re-enqueued at the head of the queue. But if this will be workable for you, how about,\n\n``` js\nvar msg = worksock.read();\nif (msg != null) {\n  if (canProcess(msg)) {\n    doSo(msg);\n    worksock.ack();\n  }\n  else if (maybeSomeoneCanProcess(msg)) {\n    worksock.requeue();\n  }\n  else {\n    worksock.discard();\n  }\n}\n```\n. I'm going to close this, since the pull request was dealt with.\n. Changeset looks good. What is the motivation -- is it to make up for backpressure not quite working?\n. OK merged. Any brave soul want to try it out?\n. Bueller? I can make a new release I think -- one might be queued up anyway. Votes?\n. Roger that @awashbrook, I'll give it a go over the weekend.\n. > Is it as simple as adding another param to ReqSocket.connect\n\nDo you want replies to go back via an exchange as well, or just the requests?\n. The complications are:\n1. The return path must be encoded in each request. The conventional way to do this is to put a queue name in the 'replyTo' header. Having some other encoding (to include an exchange name) would break this convention, and compatibility with any libraries that follow it. There's also limited space -- the header has up to 255 chars, and both queue names and exchange names are up to 255 chars.\n2. Having routing in the middle breaks the REQ/REP contract. Requests can be dropped if there's no-one currently listening, or go to more than one replier, and the requesting socket won't know whether or not to expect an answer, or more than one answer. See the discussion under #57.\n. Another (weaker) objection is that the point of rabbit.js is to not have to think about exchanges and queues.\n\nWhat is your motivation for sending via an exchange?\n. > Also, setting a different exchange than the default allows us to set durability and auto-delete and all that.\n\nThe default exchange is durable, and doesn't need auto-deletion; but I take your point that you can't redeclare it with different properties.\n\nAnother thing that might be useful is being able to \"tap\" service requests by using a direct exchange and listening in with an additional queue.\n\n> I'm not 100% clear what you meant by, \"Having routing in the middle breaks the REQ/REP contract\"\n\nI suppose the library could maintain exchanges and bindings as well as queues. My concern is trying to keep things simple. After a few extensions of the model, the various special cases and interactions between them get hard to document and maintain. Even the topic routing, added recently, is quite awkward. So I prefer to stick with simple rules and not expose much of AMQP.\n. Hi there, thank you for bug report.\n\nIt is reasonable to expect your code to work. The reason it's not -- I suspect (I'll investigate further) -- is that the request socket wants to acknowledge the message after the handler has run, and since you're closing it, it tries to do the ack on an invalid channel.\n\nUsing nextTick or setImmediate is a good work-around for now. Probably `#end` and `#close` ought to do this themselves, to avoid this situation.\n. Good stuff! I'll take a look v. soon.\n. Just from the description above: how come requeue uses `nack` and discard uses `reject`? The difference is admittedly slim, since nack has been supported in RabbitMQ for a long time.\n. Ops, what I meant was that `reject` may as well be used in both places, since it's equivalent (for single messages) to `nack`, and is in the AMQP specification rather than an extension (as `nack` is).\n\nNever mind though, I changed it and merged the previous commit. Thanks!\n. Which headers/properties? You might be better off using squaremo/amqp.node directly -- it's not much more complicated that rabbit.js.\n. > I'm not seeing anything in this repo that would allow me to authenticate a connection.\n\nThe `createContext` procedure allows you to supply a connection URL, and this takes the form given in http://www.rabbitmq.com/uri-spec.html. If you need to supply a username and password, use something like `\"amqp://user:pass@host\"`.\n. > Any thoughts as to why this would not be a good idea?\n\n'auto-delete' means the queue will be deleted once there are no consumers; for example, if you connect a REP socket to `'foo'` with auto-delete then close the socket, `'foo'` will get deleted.\n\nThe trouble is we want this to work for both REP _and_ REQ sockets; that is, we want the queue to be deleted only when there are no REP sockets and no REQ sockets connected. Otherwise, we could have a REQ socket sending its requests into the void with no chance of a reply, even if a REP socket comes along later.\n\nHowever, because REQ sockets _don't_ consume from the queue, there's no way for them to prevent the queue from being deleted.\n. Hello, sorry to miss this one for a while.\n\nThe RabbitMQ docs you quote are saying \"**don't** create a queue per request, create a queue per client\". It's the latter which rabbit.js does.\n. I see, you want to multiplex requests from different sources over the socket (or create multiple sockets). \n\nThe way rabbit.js works, you would have to know the order in which you requested things; internally, it assigns each a correlationId (as you mention), then it emits the answers in the order the requests came in.\n\nSo, as a sketch, you could push each request (as a promise, if you want to keep the scheme in the example above) into a queue (= array), and shift from the queue when the answer came back.\n. Yes, garbage-collecting exchanges is a pain. I haven't come up with a good solution -- it's a gap in AMQP, really.\n\nThe flaw in this change is that it only takes into account subscribers -- when the last subscriber goes, the exchange will be deleted, and any publishers to that exchange will get an error and crash.\n\nSee #49 re garbage collection in general.\n. > The flaw in this change is that it only takes into account subscribers\n\nTo explain the problem a bit more: if an exchange is marked as auto-delete, once no queues are bound to it, it will be deleted. In rabbit.js, queues are owned by subscribers, so in effect what you get is that once there are no subscribers, there are no queues, and once there are no queues, the exchange is deleted.\n\nHowever, there's no analogue to those queues for publishers, so there's no way (that I've come up with) for a publisher to prevent an auto-delete exchange from being deleted. Without that, while this might help keep things tidy, it also causes problems.\n. Ah yes, I can do another release! One sec ..\n. Since you asked nicely, v0.4.2 is up on npmjs.org. It has TASK and WORKER sockets, with `#requeue` and `#discard` on the latter.\n. Just to check I understand what you're after: instead of\n\n```\nvar req = ctx.socket('REQUEST');\nreq.connect(\"questions\");\nreq.write(\"foobar?\");\nvar answer = req.read();\n```\n\nyou'd rather have (something like):\n\n```\nvar req = ctx.socket('REQUEST');\nreq.connect('questions')\nreq.ask('foobar?', function(answer) {\n  // ... something\n});\n```\n\nIs that right? I can see why that's somewhat easier, given that read isn't blocking.\n. I don't think this would be overly difficult but I might not get to it very soon -- anyone fancy giving it a go?\n. > when you read from socket, you should expect to get messages in random order.\n\nI don't think that would help -- you wouldn't know what is an answer to what. It really has to be done in the machinery _behind_ a socket.\n\nIt's easy enough to do without altering the machinery, if you don't mind the callbacks being called in order (i.e., later callbacks waiting for earlier ones, even if they arrive first). Otherwise, it is a bit more difficult than I indicated above.\n. Right, the WORKER socket assumes you're doing just one thing at a time, and scaling up by having more workers.\n\nThis discussion may be apposite: https://github.com/squaremo/rabbit.js/issues/48\n\nThe solution discussed there is to construct a future for each task, and pass that along with the payload to the next stage. Then you can iterate through the promises and ack things in order.\n. > I would love to have a extended ack method that can accept the message payload (or a uniqueId) for acking that single message.\n\nBoth of these are problematic: the first assumes no two payloads will ever be the same (uh-oh); the second requires some means of giving a buffer a unique ID, which is then required when acking it. I suppose it could be a property set on the buffer, but that seems kind of gross -- also, whatever you're passing it on to would have to know to tell you once it's done -- so you may as well give it a future or callback or whatever, rather than an ID.\n. > I think that you can send, along the message, the deliveryTag that message has. As I see [1] the readable.push method, I think this change will break the current API, as it allows only a Buffer or a String\n\nOne can always set a property on the buffer or string -- it's JavaScript after all. But I don't like this solution, because the receiver has to know a very specific behaviour, which is to get the delivery tag from the buffer it's responding to, and somehow attach that to its answer. I'd rather there was a callback.\n\nThe library could help you out: maybe something like\n\n``` js\nvar worker = rabbit.socker('WORKER');\nworker.connect('jobs');\nworker.on('data', function(job) {\n  doSomethingAsync(job, worker.lastMessageAcker());\n});\n```\n. You want to get all the order updates but be able to tell which specific order each message pertains to, is that right?\n\nI would just put the order number in the payload, and take it out of the routing key. If there's something that needs to subscribe to a specific order number (i.e., no wildcard), put it in both the payload and the routing key.\n. Could the callback you provide to processFn possibly be called more than once for a particular message? Otherwise, looking at that code, I can't see how ack could be called more times than there are messages.\n\nBy the way, since `#ack` acknowledges the least recent message, if processFn calls its callback asynchronously, you could ack the wrong messages. It might be better to do it this way (sorry, JS syntax):\n\n``` js\n// ...\nworker.connect(provider);\nfunction doJob() {\n  var job = worker.read();\n  if (!job) worker.once('readable', doJob);\n  else {\n    processFn(JSON.parse(job), function(err) {\n      if (err) return console.log(err);\n      worker.ack();\n      doJob();\n    });\n  }\n}\n```\n\nIn other words, read the next message after processing the last message. (See #79 about acknowledging messages out of order)\n. Nice and simple improvement -- thank you.\n. Hi! Yes there's no particular reason it can't be updated, just me being busy. I ought to take a look at the other outstanding PRs and issues too!\n. I'm not quite sure what you're asking for -- can you sketch out how it would look? (In code, or pseudo-code, perhaps)\n. Ah, I see! OK, that is a worthy aim. Let me take a look at it ..\n. Sorry to take a while to get to this\n. Hello, sorry I didn't look into this. Are you closing the issue because io.js and node.js converged again?\n. OK, I've added node.js 0.12 and ^1 (i.e., io.js) to the engines, and tested them (admittedly the tests are not numerous).\n. Hello,\nDo you mean \"how do I pop (get) messages from a queue\"? If so, you want a `PULL` socket, and to connect with `socket.connect(queueName)`.\n\nIf you mean \"how do I get a _single_ message from a queue\", you may have to use https://github.com/squaremo/amqp.node instead.\n. Node.JS can have pretty unhelpful behaviour under load, so it's difficult to narrow down what might be the problem. As a guess though, it's easy to ignore back-pressure from publishing, and this may mean you are shovelling lots of messages into buffers that are already full -- Node.JS won't prevent you from doing this.\n\nI would have a look at:\n- What RabbitMQ thinks is happening (connection state, message rates, memory use of the server, ...)\n- The memory use of Node.JS as load increases\n\nand check that you are responding to `write` returning `false`.\n. That .. looks fine and works fine for me. Odd indeed.\n\nWhat versions of Node.JS and rabbit.js are you using?\n. RabbitMQ in general can be used as an \"engine\" in the terms of the discussion you point at, and rabbit.js may thereby be suitable as a convenient way of using RabbitMQ.\n\nServing millions of users at a time will take careful design whatever you use, and I would expect there to be more fundamental decisions to make before you look at whether to use rabbit.js or not -- for example, will a RabbitMQ cluster or federation be able to handle the load you expect? How many web servers with SockJS will you need to account for that load? And so on.\n. This is not really encouraged, by design -- PUB/SUB sockets are meant to only collect messages while your application is running.\n\nYou may find it easier to encode what you want in amqplib (squaremo/amqp.node), which has full control over the lifetime of exchanges and queues and so on, at the cost of having to do things at a \"lower level\".\n. The problem here is that the code doesn't yield to I/O at any point. You could write the same thing with a regular net.Socket and get the same result; if you don't return from your handler (in this case, the one you register for 'ready'), no I/O can happen, and the queue into which you're putting bytes cannot clear.\n. Ah I see, you created the queue with a dead-letter exchange already (through the management UI perhaps?), then because rabbit.js doesn't know this and can't be told, it falls over. What a pain!\n\nI would really like to keep the interface to rabbit.js as simple as possible, which is why I don't expose those kinds of options. But I can see this is a major pain for you. Maybe I could check for the queue first, and leave it alone if it exists already.\n\nAs a workaround for now, you could use a policy to set the dead letter exchange for the queue; I've tried this, and it seems that queue attributes that come from policies don't count when it comes to checking for inequivalent args (as the error above has it).\n. I have pushed a change to master which lets you tell a socket not to try and create objects when connecting; this means you can use exchanges and queues created elsewhere with properties different to those rabbit.js would give them.\n\nvar ctx = require('rabbit.js').createContext();\nvar push = ctx.socket('PUSH', {noCreate: true});\npush.connect(existingQueue);\n\nI figure this is a solution to the general problem of using existing objects. Would you let me know if it works for you? If so I will tidy it up and make it a lasting change to the API. Thanks!\n. Rogerio, you can tell the context a heartbeat interval by using a querystring parameter when connecting. Something like\n\n```\nvar ctx = rabbitjs.createContext('amqp://localhost?heartbeat=380')\n```\n\nThe connection URL specification is at http://www.rabbitmq.com/uri-spec.html. amqplib, on which rabbit.js is based, also supports `frame_max` and `channel_max`.\n. > Some more detail here: https://www.rabbitmq.com/uri-query-parameters.html\n\nThose are the parameters accepted by RabbitMQ's Erlang client. The descriptions of `heartbeat`, `frame_max` and `channel_max` are accurate though. amqplib does support SSL configuration, but through a different means.\n. @rogeriobiondi Did the answer above work for you? Please close if so -- thanks!\n. Hi, sorry for not being responsive on this one -- did you 1. give up or 2. figure it out?\n. > Error: connect ECONNREFUSED\n\nThis suggests to me that there's no RabbitMQ running; or that it's not running where the example expects it to be (localhost 5675). You can adapt the example to use a RabbitMQ server somewhere else, by providing a connection URL in the `#createContext` call.\n. That sentence is supposed to indicate that the technique in the ordering example can also be used for worker sockets. Would it be better to say,\n\n> It is important to acknowledge messages in the right order. A way to maintain ordering for REP sockets, which can be adapted to WORKER sockets, is shown in the \"ordering\" example.\n\n(Is it reasonably clear how to adapt the example, do you think?)\n. > It appears that this flexibility exists in Ruby's RabbitMQ lib, so I'm not sure regarding the reason for the handicap.\n\nThis flexibility also exists in https://github.com/squaremo/amqp.node. Calling it a handicap here is not very fair: the idea of rabbit.js is to make using RabbitMQ transparently easy, and that means eliding options that don't matter for the socket-like abstractions given.\n\nI am starting to think that the socket-like abstractions are the less useful bit. Would it be better to just have ways, in amqplib, of publishing and consuming via streams?\n. rabbit.js is fairly portable code in itself (although, inevitably, written in callback/promises style); however, since it relies on amqp.node (aka amqplib), I don't think it's usable outside a Node.JS environment.\n. > (And sorry to squaremo if this feels disrespectful to have this discussion on a rabbit.js issue.)\n\nYeah that is a bit cheeky, but since you had the grace to apologise, I'll allow it ;)\n. Yes, that ought to do it. Beware of relying on RabbitMQ for locking though, as noted in the edits at the end of that blog post. Something like zookeeper may be more reliable, if you need strong consistency guarantees.\n. It's probably a bit out of date with respect to npm. The idea is that 'bundle install' ('install bundle', 'indle bunstall') downloads Socket.IO and node-amqp to ./node_modules/.\n. Ah yes, npm 1.0 doesn't use 'bundle' as a command, it's just 'install' (and it defaults to installing locally, that is in node_modules in the same directory)\n. The examples work for me. What doesn't work for you?\n. One gotcha is the version of socket.io. v0.7 has gone off in a weird direction rather than solving some of the problems in 0.6, so rabbit.js sticks with 0.6. It's not down as a dependency in the package.json, because it's not strictly necessary, so to install it you will need to do\n\n```\n$ npm install socket.io@0.6.17\n```\n. I'm not sure this is a good idea; they zeromq guys decided to get rid of durable subscriptions (see http://article.gmane.org/gmane.network.zeromq.devel/10091/match=durable+identities). It's an easy way to leak resources.\nAlso, there's no notion of acknowledging messages sent to the browser, so this wouldn't actually stop messages getting dropped.\n\nIf you can think of a sensible semantics for durable subscriptions from browsers that avoids those problems, I am happy to consider it. Possibly connections should just have some more in-built reliability.\n\nYour implementation is a bit odd. Firstly, you use the same queue name for all the subscribers, which would mean that they compete as consumers (i.e., each message would go to only one subscriber). Secondly, you create a direct exchange and always bind the queue using an empty string, so it's effectively a fanout exchange anyway.\n. This pattern is encoded in rabbit.js as 'req' and 'rep' sockets: see the req/rep example.\n. Ohhh, snap! Is there a standard way of dealing with this backwards incompatibility?\n. I've updated to Node.JS 0.6 series, and improved the packaging a bit. The README has fresh details on how to run the examples (you'll need to use NODE_PATH now ..).\n. That's pretty much the use case for rabbit.js, making it easy to do that; example/sockjs.js is a good example (though it uses a request socket, whereas you probably want a subscribe socket).\n\nThat error is kind of mysterious, I don't see it if I run the example myself. I see \"rep socket opened\" as the next line after \"SockJS bound to ..\". Can you isolate the problem?\n. All the AMQP is done for you by rabbit.js. That's basically the point of it. You can always just use node-amqp, if you want to use AMQP directly -- as you can see from rabbit.js, it's not too hard.\n\nsockjs.js isn't requiring itself, it's requiring the library sockjs from node_modules. It ought to be named differently to avoid that confusion.\n\nTo connect to a RabbitMQ somewhere else, supply a URL to sockets.listen,as in this code:\n\n```\n$ sockets.listen(svr, {url: \"amqp://example.com\"})\n```\n. @fostercraig: To be honest I am not completely sure of the reconnect semantics of SockJS. Just opening another ought to work however.\nOne possible hitch is that SockJS (and Socket.IO, and HTTP for that matter) is limited to two HTTP open connections to any given host; in practice this means that in general one can only maintain one SockJS connection to a host, since the fallback needs one connection for sending and one for long polling.\n. You can always multiplex over the SockJS connection (in fact this is an explicit decision on the part of SockJS, to leave that sort of thing up to applications). That's not how rabbit.js works right now, but as it happens, I am changing things around to make that much easier.\n\nSpecifically, I'm giving making socket() the top-level abstraction, rather than listen(), and making the sockets implement the Stream interface so you can just pipe from whatever to whatever; e.g., in your scenario you can pipe a sub socket into the SockJS connection and the SockJS connection into a pub socket. If that doesn't make any sense wait a bit for the readme changes -- all will become clear! Well, fingers crossed.\n. @soichih By the way, did you get this to work, or did you give up? (the latter would make me sad of course)\n. Hi! Yes, you are right; in my haste to write a pithy example I elided the callbacks. The README should now have a properly working example.\n(The examples in the directories under example don't make this mistake, by the way.)\n. When would you want messages to be acknowledged?\n. OK so this is one-way, as in, you're not posting replies to the work items? And I presume the reason for using acknowledgements is for some kind of reliability: if the task processing fails and takes down the socket or connection, the message will be requeued and given to another worker if there is one, or possibly the crashed and restarted worker.\n\nIf you need this kind of reliability, then have workers reply to tasks, and have processes that post tasks resend them if they don't get a reply within some timeout. If you use replies, rabbit.js could (though it doesn't currently) use the lack of an acknowledgement to short-circuit this and resend itself if the worker process dies. Replies may also be useful for rate-limiting task-producers so that tasks don't back up.\n\nIf you don't want to send replies, there's no place for rabbit.js to say \"ok this message is handled\"; I guess it could acknowledge each message after emitting a data event, but I don't think this really buys a lot.\n\nIf you're attached to using (one-hop) acknowledgements, you may want to just use the AMQP client library.\n\nI may look at adding sockopts for blocking and unblocking the receive buffer, but by that stage it is getting as complicated as AMQP and that's something I definitely don't want. If you've another idea I'd be happy to discuss it.\n. Joseph, did you try any of the above out or think of a better solution (or switch to using something else)?\n. @jgornick As above I'm not convinced that acknowledgements give you more reliability without creating other problems. For example, what if you do the important bit of the work, then crash before you can call `ack()`. Then the item will be sent again, possibly to someone else, and the work will be done again. (AMQP is no better in this respect, -- you have a bit more control over what happens when, but no more guarantees)\n\nMy resistance to this additional complication is based on the assumption that people use rabbit.js as its own abstraction -- simple messaging that happens to use RabbitMQ -- whereas _maybe_ people use it only because the node AMQP library is a bit of a shambles. @josephholsten, @jgornick, is that latter the case for you?\n. @josephholsten, @jgornick Thanks guys. It sounds like I ought to treat rabbit.js more like a reformation of AMQP -- correcting some odd defaults and papering over some other things. (Being less concerned about mimicking ZeroMQ's API accurately may also be worthwhile, though it's a good model of simplicity)\n\n> I pulled out regular amqp to get acks, but it's a much more complicated but of code. I worry about other people \n> maintaining it because they have to grok amqp semantics before they can do anything.\n\nIn any case they would have to understand that `ack()` _must_ be called in some situations and not in others. Perhaps it's not too much of a stretch though. Friends don't let friends learn AMQP.\n\n> Anyway, I'm off to go learn zmq better.\n\nPieter once expressed it to me this way: RabbitMQ is good if you want to _use_ a message broker, ZeroMQ is good if you want to _make_ a message broker. Make of that what you will ...\n. > It would mean that unless they explicitly ACK the 'data' event, the message itself will not be de-queued?\n\nYes, pretty much. The idea being that if the client crashes before acking, the message gets given to someone else.\n\n> In this case, will the message be re-queued at the back of the queue by rabbitMQ or left in the same position at \n> the front and resend immediately to another consumer?\n\nBear in mind that RabbitMQ can't react to something NOT happening: either the consumer has to go away (by cancelling, or closing the channel), or the message has to be nacked (it's a RabbitMQ extension of AMQP).\n\nPrior to RabbitMQ v2.7.0, such message went to the back of the queue. Now they go back where they were; which is to say, usually to the front of the queue.\n. @alexfernandez Confirms (server to publishers) or Acknowledgments (from consumers to server)? I can see maybe one would want both; I don't know if node-amqp supports confirms though.\n. @scriby rabbit.js doesn't claim to be a fully-featured AMQP library, though you should be able to do most of what's in the tutorials. You might want to take a look at https://github.com/squaremo/amqp.node if you want finer control.\n\nHaving said that, I ought to get back to thinking about this one, clearly it's a common need.\n. I've implemented a WORKER socket type:\nhttps://github.com/squaremo/rabbit.js/tree/worker_socket#readme\n(scroll down a bit, to \"Socket types\")\n\nSome rationale for the API is in the commit message: https://github.com/squaremo/rabbit.js/commit/bc2bef1dfe91f89929311dc30c31b5aad394c9d4\n\nThis also adds the setsockopt option `'prefetch'`, for limiting the number of messages RabbitMQ will send you before you have to acknowledge some. It's described in the README.\n\nI'd be grateful if anyone who's still interested would give it a whirl.\n. > Is it possible that you update in npm\n\nDone, just published v0.4.0 of rabbit.js to npm.\nAaaand I'm actually going to close this. Thanks to everyone that chipped in their thoughts.\n. I'll just change to use an explicit loop-by-index everywhere -- it's clearer and (apparently) faster anyway.\n. (If you fancy preparing a pull request that would also be welcome!)\n. Looks like that was the only place a `for(..in..)` was used instead of a `for (;;)`. Fixed in master thanks to pull request #12\n. Thanks!\n. While I would like rabbit.js, among other things, to be an easy way to use RabbitMQ, I don't like the idea of allowing arbitrary AMQP to leak into the rabbit.js API.\n\nWhat is it specifically you want to accomplish? Is node-amqp more suitable, if you need fine control via AMQP?\n. OK, yeah if you want to be specific about exchange and queue properties it may be better to use node-amqp -- the API isn't that bad is it? (I do some work on that, including an in-progress API cleanup, so I would be interested to hear your views).\n\nOn the error event: yep, I agree rabbit.js should expose errors rather than swallow them or splode. Let me think about that one a bit.\n\nOn the `opts` parameters: hem. haw. hrrm. Yar, use your fork for the time being. I'll see if the idea grows on me.\n\nCheers, thanks for taking an interest in rabbit.js,\n-M\n. (I've closed this but I may merge yet ...)\n. Hi\n\nThere are races if you don't use callbacks/event handlers; specifically, the context 'ready' event, and the callback argument to connect, which both do I/O. How to arrange them is spelt out in the initial example in the readme (the other code snippets miss them out -- I ought to fix that).\n\nWould you try using those, and see if it helps?\n. Which version of the AMQP client are you using? `npm ls`\n. I ask because there are problems in the last couple of node-amqp (one introduced by me -- oops). Version 0.1.0 is the last known good.\n. Off the top of my head:\n\n```\nrm -rf node_modules/amqp\nnpm install amqp@0.1.0\n```\n\nIt's worth reading the man pages for `npm`: http://npmjs.org/doc/npm.html\n. You could try 0.1.1, which I'm pretty sure works with node 0.6.\n. Do you have rabbitmq running locally?\n. Hi,\n\nThe idea isn't really to think about queues (or exchanges), but in terms of the messaging patterns (PUB/SUB, REQUEST/REPLY, etc.). All of those that read from a socket have the messages buffered in a queue.\n\nIf you're trying to interoperate with AMQP clients, though, it helps to know that PUSH sockets and REQ sockets both publish to the queue that they are connected to. E.g., in\n\n```\n$ var push = context.socket('PUSH');\n$ push.connect('foobar');\n$ push.write(\"here is some stuff\", 'utf8');\n```\n\nthe message gets sent to a shared queue called 'foobar'.\n. Good point, and good idea!\n\nI would like it to still be able to be run without arguments; i.e., using a default port. I may start a feature branch for this and hack on it a bit.\n. I updated the examples, and as a part of that, made the port a command-line option. For the SockJS example too.\n. Do you have the management UI available for the production rabbit? If so, you could check, while the listener and send are running,\n1. there's at least two connections\n2. there's an exchange called \"foobar\"\n3. there's at least one anonymous queue (i.e., called amq-gen.<random-number-here>)\n4. the queue is bound to the exchange -- there's a little diagram which says the bindings\n\nor since you have rabbitmqctl, you could try\n\n```\nrabbitmqctl list_connections\nrabbitmqctl list_exchanges\nrabbitmqctl list_queues\nrabbitmqctl list_bindings\n```\n\nto get the same information.\n. The exchanges, queues, and bindings all look fine from rabbit.js's point of view.\n\nAll those blocked connections though -- not sure what's going on there. RabbitMQ will block connections when it thinks it's going to run out of space to put messages; but, you have only one queue, and it has no messages. So it can hardly have run out. Do you mind if I post this to the rabbitmq mailing list to see what people make of it? Also not sure why so many connections -- are there other vhosts set up?\n\nIn the meantime, one way to check that it's the blocked connection that's keeping messages from getting through is to look at whether Node has to buffer things itself; an increasing `context._connection.bufferSize` will tell you that.\n. Ah, looking at the `rabbitmqctl status`, it appears that RabbitMQ thinks there is about 200MB memory available to it. Does that sound right for the host you're using? I don't think RabbitMQ will handle running in that little memory gracefully; the memory alarm would constantly trigger, which would explain the blocked connections.\n. You'll be able to tell if that's the case by looking at the logs (default locations given in http://www.rabbitmq.com/relocate.html).\n\nThe memory limit will appear in a line similar to\n\n```\n=INFO REPORT==== 29-Oct-2009::15:43:27 ===\nMemory limit set to 2048MB.\n```\n\nand the alarm triggering will look like\n\n```\n=INFO REPORT==== 15-Sep-2010::14:53:44 ===\nvm_memory_high_watermark set.\nMemory used:53650536 allowed:52556595\n=INFO REPORT==== 15-Sep-2010::14:53:44 ===\nalarm_handler: {set,{vm_memory_high_watermark,[]}} \n```\n. OK no worries -- sounds like the right thing to do. Hey do you think I should make rabbit.js work with Redis too? I'd have to change the name I guess ...\n. RabbitMQ will put messages on disk if they are marked persistent (for recovery purposes), and it'll page queues out to disk when it starts running out of memory. The message index is in memory though, so there is a hard limit, unless you use the tokyo cabinet plugin to page the index out too. Mostly it likes to have things in memory, for sure.\n\nI might try a redis context just to see what it's like.\n. Good call, thanks!\n. Which version of node-amqp is present? `npm ls`\n. Ah good point, I pinned it to 0.1.1 because I had broken 0.1.2. There's a 0.1.3 now so I should update the dep.\n\nNow constructing test case for > 100kB messages.\n. Oh boy, I am a doofus. That's the second time I have got the arguments to Buffer.copy the wrong way around. Yeah I know -- testing.\n\nThanks for spotting this! I'll have a look at your changesets and push a new release to npm.\n. The fix is in https://github.com/squaremo/rabbit.js/commit/58bb1cc62aa4ac98cbea9ffe88f06cf1c79de02b so should be in master. But it won't be in any npm-available releases, since I am a bad person and I've not made any such for some time now.\n\nAre you definitely using a git checkout?\n. Yes, looks like the published AMQP package is failing the 'engine' test. https://github.com/postwait/node-amqp/issues/111 has a workaround, until Theo publishes a new release:\n\n```\nDownload the [node-amqp] tgz and use npm with the option to install from tgz to install it.\n```\n. There's now a release of node-amqp that supports Node.JS v0.8 (node-amqp v0.1.6; possibly prior releases).\n\nI've updated the dep in package.json to refer to a published node-amqp release again (in 6924d96dda50531863ff5d8032ae87e968d3f9af)\n. Hi, sorry I missed this for a couple of weeks -- bad library author!\n\nI'm not sure why you need two steps -- is server2 doing something to the request before it forwards it?\nIf server2 and the worker are doing independent jobs, you could dispatch to both in parallel and wait for both replies.\n. Hi Luc,\n\nI suppose it really depends on the nature of the analysis it's doing -- is it something the express handler process can do just by looking at the request? If not, then yes perhaps an extra step is warranted.\n\nYep, REQ/REP is an appropriate abstraction -- you can indeed make requests from different modules or processes and they'll be routed back to the right place.\n. (Housekeeping)\n. Augh, yeah looks like the next node-amqp release is <undefined> months away.\nThanks!\n. Are you using only rabbit.js? This error probably means testNODE has been declared from elsewhere, e.g., by using the Node AMQP client directly at some point. rabbit.js more or less assumes that won't be the case; i.e., that it has free range to declare what it likes (all AMQP programs have to assume this, in practice).\n. Ah I see. It's quite tricky to handle AMQP errors, since any error tends to invalidate the AMQP channel, meaning you have to construct a new one and start things again. But you are right that rabbit.js could probably do a lot better than let exceptions be unhandled all the way up.\n. @DuarteMadueno master branch now has error reporting on sockets; basically, if an AMQP channel reports an error, any socket using it will emit an error and close. If you've not moved on from rabbit.js, would you try it out to see if it works like you'd expect? Extra test cases very welcome. Thanks!\n. No worries! I understand -- it's more flexible using node-amqp directly, if a bit more work too.\nThanks for pointing out the problem in the first place.\n. This seems OK (i.e., I would merge it) -- why did you close it?\n. BTW you can supply username and password in the connection URL -- see https://github.com/postwait/node-amqp, section \"Connection\".\n\nNonetheless it may be useful in some circumstances to be able to supply an object rather than trying to assemble a URL string.\n. What is the 'durable' option supposed to make durable?\n. So, you want to be able to connect another SUB socket and get any messages you missed while disconnected?\n\nYes, you'll need `exclusive: false` and `autoDelete: false` (`durable` doesn't have that much bearing, it just means the queue won't go away if the broker restart -- but if you set the others to false you may as well set durable to true too).\n\nYou will also have to name the queue, because otherwise you won't be able to find it again -- a SUB socket lets the broker give its queue a random name. You'll also have to ask the PUB socket to send persistent messages.\n\nI can see there's a use for persistent subscriptions (not missing messages); although, ZeroMQ dropped this feature, possibly because it's too easy to leak queues if you don't reconnect. But as I've said elsewhere, I don't think the way to extend rabbit.js is to just expose AMQP in all its wartiness. If you truly want all the options, just use the AMQP client.\n. @jamuhl Just to be clear, I believe I understand the need, I just don't want to make rabbit.js as complicated as AMQP (because what would be the point of it). I've created issue 39 to deal with durable subscriptions.\n\nIs your AMQP wrapper on github?\n. Yeah, I was thinking of that, though I would probably put the optional argument (the topic) second.\n. In the amqplib-based implementation, there's no 'topic' option at all yet. There is an undocumented feature whereby you can pass an object to `#connect`, specifying the exchange name, type and routing key -- but this doesn't really help much if you want to change the topic for each write.\n\nOf course, `#write` already has a second argument, that is the encoding. So I think this would have to be another method, just for PUB sockets, perhaps `#publish(topic, message, [encoding])`.\n. OK I have implemented `#publish` as above. There are implications.\n\nOne is that it doesn't make much sense unless a SUB socket can subscribe, in some way, to different topics. Here I've followed ZeroMQ, in that subscribing is distinct from connecting:\n\n``` js\nsub.connect('events');\nsub.subscribe('critical.*');\n```\n\nBut it's a breaking change, and the semantics are not obvious (do all subscriptions apply to all connections?), and it's all a bit confusing. So I may return to the structured-connection-address mentioned above -- that is, `sub.connect({address: 'events', topic: 'critical.*'})` or even `sub.connect('events', 'critical.*', callback)`.\n. > But it's a breaking change, and the semantics are not obvious (do all subscriptions apply to all \n> connections?), and it's all a bit confusing. So I may return to the structured-connection-address\n\nActually, I have added an optional argument to `#connect`, just for SUB sockets, for the topic. The kind of exchange is determined by the `routing` option given when creating the socket.\n\nSo, in its entirely, it looks like this:\n\n``` js\nvar sub = ctx.socket('SUBSCRIBE', {routing: 'topic'});\nvar pub = ctx.socket('PUBLISH', {routing: 'topic'}); // must agree on routing if they are to talk\nsub.connect('events', 'critical.*'); // the third argument would be a completion callback\npub.connect('events'); // ditto second argument\npub.publish('critical.alarm', \"Emergency!\", 'utf8');\n```\n. Noted. It's a shame JS is so weak on iteration, usually this sort of thing would be trivial without needing to expand the API. But you can do\n\n```\nfor (key in {'A': true, 'B': true, 'C': true})\n  conn.subscribe({address: 'users', pattern: key, routing: 'direct'});\n```\n. Or\n\n``` js\n['A','B','C'].forEach(function(x) {\n  sub.connect({address: 'users', pattern: x, routing: 'direct'}); });\n```\n\nalthough callbacks are a bit tricky.\n. Closing this to take it off radar; I think using forEach is adequate.\n. It would be really handy to know what exactly is the frame that trips it up.\n\nIf it happens after ten minutes then running it with debug logging on might generate too much output, or even prevent the bug. (But you can try: run the program with the env variable NODE_DEBUG_AMQP set to something; e.g., `NODE_DEBUG_AMQP=true node myprogram.js).\n\nAny chance you can hack `node_modules/amqp/index.js` to log the offending frame when it throws an error in `Connection._onMethod`?\n. One problem I can spot in the sample code is that it calls `connect` with every post -- thereby doing the same work over and over again. It's intended that `connect` is called once for each resource (e.g., `'pre'`) to which you want to connect.\n. > IMHO the simplest way to go about writing a library interface API to rabbit.js is to expose all the functionality of\n> rabbitMQ in an easy to use way and give users access to provisioning and controlling all the features rabbitMQ \n> has to offer. \n> Hence letting users write their own abstraction levels via their application layer if they so desire. Of course users \n> can use AMQP directly, but that is too low level and not necessarily rabbitMQ-centric.\n\nWell, I didn't start rabbit.js with the intention of exposing all the functionality of RabbitMQ. A better route to that is an AMQP library with RabbitMQ's extensions added into the protocol (e.g., exchange to exchange bindings, publisher acknowledgments). My intention was to have a simple messaging model that aligned with the Node.JS way of things fairly well, that happened to use RabbitMQ.\n\nIt's become apparent since that some people are using rabbit.js because node-amqp is too complicated (which it inherits from AMQP) and inconsistent in some places (e.g., some methods return promises and some don't). Some of those people find that some things they need, that they know RabbitMQ does, are missing from rabbit.js; for example, acknowledgements. I'm willing to grow rabbit.js in those directions if that will make life easier, since that's after all the point of the exercise -- on a case-by-case basis, and addressing the actual problem people are having.\n\n> By making certain assumptions (and thus hardcoding options - ie \"the handful of permutations\"), you probably fit \n> the bill for a percentage of use-cases, but as it so happens, most people (including myself) need more \n> flexibility/control over lifetime of messages, queues, exchanges, etc.\n> By exposing all options, users 1) know exactly what they can change w/o having to guess what the option \n> nomenclature is 2) can see exactly what each default setting is 3) can change each setting as per their own \n> application needs. I also took care in segregating exchange, queue and message options. I believe your API \n> muddles the lines a bit. Your library is very well written, so it's a bit of a shame IMHO not to use it to its full \n> extent! \n\nI don't see any moral difference between e.g., node-amqp and rabbit.js-plus-options-for-everything.\n\nThere are some (potential) parameters to rabbit.js's behaviour that it may make sense to codify in some kind of policy. For example, message lifetime, or dead-lettering. But again I'd want to look at these on a case-by-case basis, and be very deliberate about how they are exposed.\n\nSo, all in all, I would rather look at your development requests individually than accept this pull request. I appreciate you likely spent a fair bit of time on it. Perhaps with a bit more consideration we can come up with a mechanism that allows flexibility you need, without sacrificing the simplicity of rabbit.js.\n. What is your example demonstrating? (I think it's incorrect behaviour)\n. > I tried rabbit.js/example/socketio on Node.js v0.11.0\n> However, chat message has been replicated by the number of clients connected.\n\nI see what you're getting at.\n\nThis seems to have been broken, by coincidence, between Node.JS v0.8 and Node.JS v0.10. To be fair, it was working by coincidence in the first place.\n\nIn v0.8, an EventEmitter's event listener map is created lazily in a method, while in v0.10 it's created in the constructor. Using util#inherits avoids this by not invoking the EventEmitter constructor (https://github.com/joyent/node/blob/master/lib/util.js), referring instead to the prototype.\n. Fixed in master, which now uses `util.inherits` everywhere.\nNB in general, it's also necessary to call the \"super\" constructor to make sure any fields that need to be initialised are initialised (with EventEmitter it happens to work if you don't.)\n. Thanks!\n. I think this is caused by the same breakage as #35. To wit, it\n- works OK in Node v0.8 (and probably some v0.10.x, x < 4)\n- works OK in rabbit.js master\n- doesn't work OK in Node.JS v0.10.4 prior to commit 8b431bf753480ade572fb245bb235071c62fa8c7\n. OK I think that second thing is fixed in #35. Specifically, with \"works\" meaning it prints out things just once, it\n- always works with Node.JS v0.8\n- works with Node.JS and rabbit.js master\n- does not work with Node.JS v0.10.4 and rabbit.js after `git checkout f493e78`\n\nI'll look into the first thing ..\n. > I'll look into the first thing ..\n\nThe first test case above also appears to be due to EventEmitter not working the way it used to. (It used to work by coincidence; now it's broken by coincidence).\n\nIf you have a moment would you check with a master checkout? Thanks!\n. Ha, yes good point, well made. Yeah I ought to do a trawl for any other breakages, and post a new release. It's been a while.\n. I guess one might put a long TTL on a queue to give yourself plenty of time to reconnect, so yeah it might be useful to be able to say \"OK I am definitely done with this socket, destroy it forever.\"\n. Presently it's a toss-up between adding things in, and reimplementing rabbit.js on https://github.com/squaremo/amqp.node (in the expectation that it would then be much less painful to add and fix things).\n\nI hear you on the merging; sorry about that.\n. > Presently it's a toss-up between adding things in, and reimplementing rabbit.js on https://github.com/squaremo/amqp.node (in the expectation that it would then be much less painful to add and fix things).\n\nOn that point, I've reimplemented rabbit.js on amqplib in the branch `amqplib`; this is available in npm as 0.2.1-amqplib. My plan is to keep developing that and retire the node-amqp implementation.\n. This is implemented in master now, as the socket creation options `resume_name` and `resume_grace_period`.\n. Thinking aloud: is `Socket#destroy()` appropriate for this?\n. > Among other things this would delete the appropriate resources (ifUnused).\n\nUnfortunately this won't work, for a couple of reasons. Firstly, AMQP is stupid (who knew?): ifUnused will cause the channel to break should the queue or exchange be in fact used. Secondly, it wouldn't help anyway, because \"used\" means the wrong thing from our point of view. In the case of exchanges, it means bindings from the exchange, which won't always be in place (e.g., if there are no sub sockets at the time). In the case of queues, it means consumers, which again may not be in place.\n\n> Thinking aloud: is Socket#destroy() appropriate for this?\n\nNo, of course not, me. That's for invalidating the whole socket.\n. > Among other things this would delete the appropriate resources (ifUnused).\n\nI can't think of a way to make sure exchanges are garbage collected.\n. Good idea (though it'll only work with fairly new RabbitMQ versions).\nThanks!\n. You will still have a problem if you simply open more than 10 sockets without closing any. Rather than removing the event listener, I've charged the Context with keeping track of its sockets, and passing on errors. This means there's one error handler, registered by the context.\nCommit 6075a48ea7b1ea7d334b9cfe86958240f98562cd, on master, if you'd like to try it. It also introduces a Context#close, which was missing.\n\nThanks!\n. Yes it probably is best to close exchanges when done with them -- except they may be used from e.g., another socket. So it really needs to have a kind of garbage collection, rather than outright close.\n\nThat aside, can you explain a bit where you think the race condition is?\n. Thanks @wolfeidau, yes that makes it clearer.\n\n> When i went to write a test case I could NOT get the damn channels to go away without a setTimeout it was like \n> they took time to clean up, NOTE I am calling exchange.close() to based on my reading only close the exchange \n> not destroy it on the server.\n\nWhen you do `#close`, there's an RPC to the server, so there's some intervening I/O. You're quite correct, it doesn't destroy the exchange on the server; it just closes the channel that represents the exchange in the client. Having a channel for each exchange is kind of weird but that's node-amqp for you.\n\nOne thing I don't quite get: are you publishing to a new exchange each time? Because I thought node-amqp cached open exchanges, so if you are always publishing to the same place, you'll get back the same exchange object -- so no channel leak.\n\nWhat I am worried about is this scenario:\n\n```\n1. socket#1 connect to 'foo'\n2. socket#2 connect to 'foo'\n3. socket#1 publish and close\n4. socket#2 publish -- but 'foo' is closed\n```\n. Hi, sorry I have left this sitting here for a bit -- I've been working on https://github.com/squaremo/amqp.node.\n\nI will have a look myself at what amqp-node is up to. In general though, you are quite right, that rabbit.js isn't tidying up, so I need to fix that one way or another.\n. OK so one thing from that test case is that `Socket#end` doesn't cancel subscriptions or close the socket, meaning it won't result in the socket emitting 'end'. Arguably it should, at least for duplex streams: it's not in the \"specification\" for streams, such as it is, but it's certainly expected that ending a stream will provoke the other side into also ending (`#end()` is a writable stream thing; `on('end', ...)` is a readable stream thing).\n. I took a look at the node-amqp code. Oh dear.\n\nnode-amqp will always create a new channel every time you use `Connection#exchange()` or `Connection#queue()`. It also records them, indexed by the name. So, if you call `Connection#exchange()` twice with the same name, it will forget about the first one.\n\n\"Why does it record them?\" you might ask. Well, there appears to be two reasons. One is that it tries to keep track of bindings, so if I use queue.bind it'll increment a counter on the exchange mentioned (never mind that bindings are idempotent, or that it overwrites its records); the second is that there's some kind of reconnect mechanism whereby exchanges and queues are reinstated. It doesn't look like that could ever work, but maybe I'm missing something there.\n\nThe good news is that since it's not (or no longer?) caching them, always closing things once you're done is totally fine.\n. OK, after something of a longcut I think I've adequately tidied up the erm, cleanup. Each socket now registers any channels it opens (which is sometimes a few, since in node-amqp even checking for the existence of an exchange or queue involves opening a channel. Actually I could probably mitigate that a bit by closing the ones I don't need ..).\n\nAnyway, you should no longer see an ever-growing list of channels, provided you close sockets once you're done with them. You may want to cache sockets yourself if you can, to avoid all the channel opening.\n\nDo you have a test system you can try it in? There's an 'open lots of sockets' test but it doesn't exercise the code over a long period.\n. > OK I built a rather primitive example of using rabbit.js with express.\n> \n> This is located at https://github.com/wolfeidau/node-rabbitjs-test\n\nBrill, I will take a look.\n\n> I am interested to hear what you think, for the application I am developing I still have the issue that I am using a range of endpoints so pooling as I am doing here is probably not going to work.\n\nYeah, it sounds like you may want to use a \"lower level\" lib for more message-by-message control; or, maybe, to encode extra information into the messages, for dispatching by the receivers.\n. Just closing this for housekeeping, but feel free to revive the discussion.\n. You are right, that sounds very like the breakage due to me not using util#inherits (or the like), as reported in #35, #37 and #38, and supposedly fixed in https://github.com/squaremo/rabbit.js/commit/8b431bf753480ade572fb245bb235071c62fa8c7 and released in v0.2.1. \n\nI'll have a closer look at your code and try it out.\n\nJust to check:\n- are you using rabbit.js 0.2.1?\n- does it work in, e.g., Node.JS 10.3? (that's 10-series, but before the problems in those issues)\n. I'm surprised that runs as-is, since it doesn't synchronise on the context being open (emitting 'ready').\nIf I stitch that in, the tests appear to run OK. That's with rabbit.js v0.2.1, and Node.JS 0.10.4 and 0.10.12.\n\n``` javascript\n  it('should open a publish stream', function(done){\n\n    log('open')\n\n    bus.rabbitContext.on('ready', function() {\n      bus.publish('sometestpub', function(err, stream){\n        log('stream', 'publish');\n        stream.should.exist;\n        stream.write(\"TEST TEST\", 'utf8');\n        done()\n      })\n    })\n  })\n```\n. Events and callbacks are almost no use for synchronisation, it is true. My adjustment was pretty crude -- just enough to delay the test run, given some assumptions (specifically that the test harness serialises tests).\n\nI am now seeing some \"send 'channel.open' seen\" errors, in a different scenario. So I'm not sure it's entirely down to the problems fixed already. Stay tuned ...\n. Ops, yes you are quite right, the README is misleading. I've just pushed 21d21c8fc01f21ef0f0b6e8001bfaef04700171e, which converts expiration to a string. (It's a string in AMQP, to admit different ways of specifying a time, e.g., '2013-07-18T15:13:00Z', though RabbitMQ only accepts stringified numbers.)\n. I should say, until the next release to npm, it's best to stringify the expiration yourself.\n. Nice. Thanks!\n. > It only applies to Streams which are pull-style. For streams like network streams, your _read implementation is usually a noop, and you simply push when you get data.\n\nWell, for a network socket, one could stop reading from the socket. My comment is just observing that there's no reasonable equivalent in AMQP. Given that the raison d'\u00eatre of streams is to propagate backpressure, it's a shame.\n. > The data would continue to pile up regardless, just in the buffers of the Streams2 API, or perhaps in the network buffers.\n\nIn the kernel's buffers, I would _very much hope_, and at some point this would result in backpressure upstream (for TCP at least).\n\n> The Streams2 API just makes it so you don't have to worry about it as a producer, whereas before pause and resume were advisory.\n\nUnbounded buffers anywhere are seldom a good idea ...\n. > > Unbounded buffers anywhere are seldom a good idea ...\n> \n> Very true. That's why the high-water mark is important.\n\nThe point is, `highWaterMark` is advisory, the same as pause/resume. So if one does things like `.push` whenever data comes in, the buffers are effectively unbounded.\n. (Just closing this for housekeeping)\n. > Did I get this completely wrong?\n\nNot at all, that's a good analysis.\n\nThe synchronicity requirement comes from ZeroMQ's REQ/REP design, which admits intermediaries. The intermediaries can pass things along as fast as they get them, _provided_ the end points are synchronous. If they aren't, then as you've identified, answers could come back out of order. In the case of Rabbit.JS, RabbitMQ is the intermediary, of course.\n\nYou could keep a window of correlationIds at the requester, and only yield a message when the head correlationId has a reply. It's a bit like the moving window used for confirmations in amqplib.\n. > how to ensure the responder is orderly...\n\nIt doesn't matter, so long as it puts the right replyTo and correlationId on the response.\n. Ah I see. Yes, the responders have to process just one request at a time, in that case; tricky if you are sending the request off to somewhere else.\n\nYou can always have a number of responders (you have to pick a number, and that's always fishy); or have a pool where you create a new responder every time they are all busy. But this is getting away from the supposed simplicity of rabbit.js.\n. The other other side -- the gripping side --  of the coin, is that this means it's relatively easy to overwhelm Node.JS programs, since they don't (tend to) bound the number of in-progress requests. No customer has to wait to give their coffee order, but the cashier runs out of counter-room on which to put graffitied paper cups ..\n. That'll do it, nice.\n\nDare I say it, futures handle this kind of situation very nicely, now that I think about it. Have a response queue, which gets a future for each reply; then loop through the queue, dereferencing the futures serially.\n. By the way, I implemented the correlationId windowing in 0a946044e91d292ed6c8a8335b226312df1cf652\n. It's generated and added to the AMQP message when you write to the socket, and kept in a buffer to match replies against. The idea being that you read the replies back in the order that the requests were written.\n. In this case, once the rabbit.js connections are down, your SockJS connections are no good either. So I think you'll have to close those as well, and force the clients (i.e., the code running in browsers) to reconnect -- their messages are going nowhere in the meantime anyway.\n\nThe alternative is to buffer messages (from the browser) while RabbitMQ is offline, but I think that's more complicated for no particular gain. If you want reliability, you can't really mask the fact that the browser is going to have to retry things (whether it's replaying messages, or reconnecting, or whatever).\n. > Might just be best to let the node server crash and have forever restart it then i think.\n\nYes, that's reasonable -- that way you can be confident all the resources will be freed up (although, it's not an excuse not to _try_ and clean up ...).\n. There's some non-determinism working against you here. Specifically, the server hasn't necessarily finished closing the first sub socket by the time you're opening the second. This means two things: it hasn't released its \"exclusive\" lock on the queue (that which is named by `\"subs.*\"`), and it hasn't necessarily stopped delivering messages to the first socket.\n\nWith regard to that latter: I think, though I'll have to check, that amqplib stops processing any deliveries once you've closed the channel (quite reasonably, and in fact it's a requirement of the AMQP specification), which is why the `'data'` event doesn't fire. It's also the case that amqplib multiplexes \"fairly\" among channels -- more apparent non-determinism -- which might be how the PUB socket's message can seemingly overtake the SUB socket's close.\n\nYou can see it working by introducing a delay -- it can be small delay, but perhaps not as small as setImmediate -- after `sub.close()`. It's enough time for the server to release the lock on the subscription queue and for the first socket's 'consume' to be torn down.\n\nOh, and there's another thing at work here, which is a breaking change I've introduced in master (but not in a release, yet): you have to call e.g., `sub.subscribe()` on a SUB socket to receive messages. Technically it's `sub.subscribe(topic)` for some `topic`(s), but a falsey value works fine in the default case.\n\nTo be honest I am still working on these features, and actually I'm a bit undecided about them, or at least how they work at present.\n. By the way, the SUB socket uses 'exclusive' to avoid (by means of loudly exploding) the situation in which you have two or more live sockets using the same subscription -- which, for admittedly implementation reasons, would lead to each socket getting only some of the messages.\n\nThe feature as a whole is more for the scenario in which your whole program exits, exceptionally or otherwise, and you want to pick up where you left off. There is still some message loss possible -- in fact probable, if there's a fairly constant message flow. I could use acks to work around that, but it does make things rather slower (it could be an option I suppose).\n. (NB I've taken \"resume*\" onto its own branch for the time being -- I wasn't very happy with the implementation or semantics, and it might interact poorly with other options. In other words I want to think it over again)\n. > I've seen some interesting and very useful changes in the master branch and I'm wondering when \n> they will be released, are you close to a new release?\n> Or otherwise, would you say it's safe to point npm to the master branch directly in the meantime?\n\nI don't think it's safe to point npm at a git repo in general. Lately I've changed my mind back and forth in the master branch, which I ought to have done in a feature branch, so there's an illustration of why not!\n\nI'm not sure about the \"resume\" stuff yet, so I'll probably not release that for the time being. I may make a release without it, though.\n. I should say that while I don't recommend using git master in your production system, I would be grateful for any reports of using the new features in development\n. > So now we are switching into creating different sockets for the different topics, and closing them after we don't need them anymore.\n\nRight, yes that is just the thing `#publish` and the `'topic'` option are designed for.\n\n``` js\nvar pub = ctx.socket('PUBLISH', { routing: topic });\npub.connect('events');\n// ...\npub.publish('component.warning', JSON.stringify({foo: \"bar\"}), 'utf8');\n```\n\n> Another thing is that we need our published messages to be stored in a queue so that they are not lost if the SUB socket hasn't connected yet\n\nYou would have to create a queue per subscriber, before starting any publishers, which is effectively the same as opening the sub sockets before you start any publishers. There's synchronisation required whatever you do.\n\nRabbitMQ and AMQP won't help you with this, or not much. There's no provision for consumers catching up with old messages. If you cannot miss messages, better to stick them in a database and replay them for new subscribers, although you still have a kind of synchronisation problem (that is, figuring out when to switch from the replayed messages to the freshly-published messages).\n. > For now we switched to creating and closing sockets for different topics until there's a new release. Do you think this is going to be a problem in terms of performance or anything like that?\n\nYou are right that the connection is created just once; but, each socket still uses its own channel and may require some round-trips to declare various queues and what-have-you, so creating sockets is a little expensive too.\n\nPerhaps I should roll a release with the topic stuff, and put the resume stuff on a branch for later.\n. v0.4.1 is published to npm. It has the topic stuff and the `persistent` option, but not the subscription resume stuff.\n. Yes, that would be complementary. Magic strings and magic properties are of an ilk in JavaScript, but it does feel nicer, I agree.\n. If you close a socket, it won't actually be closed until its outgoing buffer is flushed. So you should be able to synchronise on getting the 'close' event.\n. > Are you saying to disconnect the worker when I get a close event on a rabbit.js socket?\n\nYes; the idea being that the 'close' event means everything on that socket up to and including the close signal has been sent, and no more events will be forthcoming.\n\nI took 'connection' above to mean socket. Did it mean 'context'? That would be slightly different, since closing the underlying connection discards anything not yet sent. You'd want to synchronise on closing any sockets you have, first. If there's more than one to a context, you could use a countdown latch, say.\n. So, broadly, your worry is that there may be outstanding I/O at the time you want to shut the worker down?\n\nThere are some things you can do (not necessarily available in this library, though) to make sure there's no outstanding AMQP messages, or to put them back on the queue if you don't handle them, but in general, if you're killing workers in order to reload code, that you'll have to be prepared for this, and have clients retry.\n. > I would prefer something with a callback: \n\nThe trouble is that this callback would have to be passed all the way down to the socket, and the underlying library just doesn't support that.\n\nIn any case, there are two things working in your favour here. One is that writes don't get buffered at the (rabbit.js) socket, they are given straight to the AMQP client. The second is that Socket#close closes the AMQP client channel, which won't take effect until any writes are flushed. In other words, things should happen in the order that you do them.\n\nThat's the theory anyhow. Are you doing the `setImmediate()` because you ran into a problem with just closing the socket after writing?\n. The other thing that occurs to me is that I don't support the Writable event 'finish', which is supposed to be called after 1. the stream has been ended and 2. all writes have been flushed. Consider it on the TODO list.\n. > i'm doing that because without it, the message never gets sent. I don't get any errors or anything\n\nHrm, so it doesn't work i the way I claim. That may be a problem in amqplib that I need to investigate.\n\nI'm not completely sure I can get 'finish' to work, or not in the expected way. There are buffers in-between a rabbit.js socket and the network socket, and 'finish' does not compose in the sense that 'finish' from the buffer you are dealing with does not mean 'finish' in the operating system buffer.\n\nJust to be clear, does `done()` exit the program?\n. > Just to be clear, does done() exit the program?\n\nI ask because exiting, or closing the context, will drop any frames in sockets on the ground.\nThe way to do it is to wait until the socket has closed cleanly:\n\n``` js\ntriggerSocket.close();\ntriggerSocket.once('close', done);\n```\n\n(This doesn't mean I won't support `'finish'`, which might be more convenient in some circumstances)\n. Hrm. It's an ugly conclusion, but I suspect you may be right about that last. Emitting 'error' without any listeners present is supposed to cause the program to exit, but somewhere this mechanism is being sabotaged (I suspect it's inside promise machinery).\n\nI will try to sprinkle some setImmediate dust through the code.\n. Great minds -- that is what I've done in 347c73b4722ce0038233216d6026ad6ad34ed754, for Context.\n\nThe sockets themselves pass through the underlying channels errors (OK I need to test that), but I will probably need to give some of the socket setup and connect code the same treatment.\n. > I will probably need to give some of the socket setup and connect code the same treatment.\n\nDone. Fancy giving it a run?\n. Grand, thanks Derick. Expect it to make its way into a release soon.\n. Part of the contract of push or req is that the message goes to a single consumer. Routing (i.e., sending to an exchange rather than a queue) breaks the contract by allowing a message to go to more than one consumer, so one might get several replies back for a request, for example.\n\nDo you want to round-robin messages among connections, like push and request do? Or just have pub/sub but with replies; there is some precedent for that pattern, in nanomsg: http://nanomsg.org/v0.3/nn_survey.7.html\n\nWhat's the high-level requirement -- maybe there's another way to accomplish it.\n. I think I see: you have a bunch of different kinds of job, and you want to be able to partition them into a smaller number of queues, for workers to pick up.\n\nYou could partition things statically, by choosing your partitions ahead of time and naming the queues for them. In effect the producers and consumers then have to know the routing -- e.g., \"I have a stripe:invoice.created, so it goes in the 'stripe:invoice' queue, then the consumer can look at it in more detail\". But I can see why your mind returns to exchanges and routing -- it seems like this ought to be dynamic.\n\nA bit of ancient history: In JMS there's the notion of a 'selector', which is like a queue query. You can consume from a queue while supplying a selector, to filter the messages that you're interested in.\n\nAMQP, which derives in large part from JMS, doesn't have selectors. They are supposed to be subsumed by exchanges and routing (which also supposedly cover JMS Topics), but sadly, the designers of AMQP missed a trick: selectors do something like routing _while also_ guaranteeing that 1. messages won't be dropped on the floor if no-one's there to listen, and 2. only one consumer will see each message. There's just no way to do those in AMQP.\n\nNow back to the present. You can probably see where problems will arise: there's no way to guarantee that messages won't be dropped on the floor, if the _producers_ don't know how they will get routed, since each message will go through an exchange and there may not be a binding in place to route it to a queue.\n\nThere's ways to mitigate this in AMQP -- using the mandatory (\"return if unroutable\") flag, among others -- but they break the simplicity of the rabbit.js model, and I'd like to avoid that.\n\nThere's no inherent problem with having tens of queues, by the way -- the messages would get buffered somewhere, and partitioning them into queues gives the right semantics (each message to one worker). Maybe there's some way the API can be more convenient though? For workers it's not too bad, you just `#connect` to each queue you care about. For producers though, well you'd have to have a socket per type of job, and pick the right one. On the other hand, something like `#publish`, where you mention the job type when you send the message, wouldn't guarantee that there's a queue at the other end (or at least, it would be expensive to do so). Perhaps:\n\n``` js\nvar jobs = ctx.socket('TASK');\n// Connect to all the task queues we will want to send to\njobs.connect('stripe:customer.created');\njobs.connect('stripe:customer.updated');\n// ...\n\n// Send to a specific task queue\njobs.send('stripe:customer.created', JSON.stringify(job), 'utf8');\n```\n\nNote there's absolutely no wildcard trickiness possible, it's all still sending straight to queues.\n. > there are hundreds of jobs and dozens of job types that will be built, changed, maintained and updated over the years. It's not something that can be set up front and have hard coded.\n\nRight, I understand. It would be nice to have the best of both worlds -- that is, to be able to verify that you won't miss messages, _and_ to be able to have a rolling evolution of the system. But that's probably outside the scope of rabbit.js, if not the system you're building.\n\nIn any case, alternate exchanges and dead lettering will help you keep track of exceptions at runtime -- messages that don't get processed either because no-one's listening, or because whoever got it was unable to process it. In recent versions of RabbitMQ you can set these with a \"policy\", i.e., apply them to exchanges and queues as matched by a regexp you give. (In less recent versions they are an argument to exchange.declare or queue.declare.)\n\nSetting an \"alternate exchange\" will give you catch-all route for messages that would otherwise get dropped. Dead lettering and per-queue expiration will give you unprocessed messages. I think these are out-of-scope for rabbit.js, but you can certainly use rabbit.js to get at the messages, by e.g., routing dead letters to a queue and having a worker socket in its own process to log them all, blink a red LED, email an operator or whatever.\n\nSo with all that in mind, what do you think of the TASK socket?\n. > So with all that in mind, what do you think of the TASK socket?\n\nI think this would survive dozens of job types (after all, every sub socket has its own queue, as does every request socket), and online repartitioning of the workers, and so on. The question is, do you need wildcard matching?\n\nI _think_ you're suggesting that you will, so you can send to `\"job.foo.frobnicate\"` and have a worker that's dealing with `\"job.foo.*\"`, rather than mention frobnication specifically. I would point out that you have to change code _somewhere_ to send new kinds of job, and that you can start extra workers to pick up the new kinds of job while you change the old workers to handle it. In the wildcard case, the old worker would just start getting jobs it didn't know how to handle.\nBut perhaps I'm off on a tangent.\n. I've implemented the TASK socket in master. (If it turns out I'm off track, we can think of it as an hilarious April Fools prank.)\n. @derickbailey I'm interested to hear where this ended up for you -- did you get something working in a fork, try the TASK socket, do something completely different ..?\n. I knew I'd run into this before: https://github.com/squaremo/amqp.node/commit/e1bcc0edc313e70fb7972f525ef7a0c273ef215c\n\nYep, `nextTick` ought to be `process.nextTick` -- and, I should actually use `delay` everywhere if I'm going to be tricky about assigning it.\n. `'routing'` is only accepted as an option to the socket constructor. It appears in the code e.g., https://github.com/squaremo/rabbit.js/blob/v0.4.1/lib/sockets.js#L222\n\nThis is not made especially clear, looking at the documentation as it stands. I'll fix that.\n. > 'routing' is only accepted as an option to the socket constructor.\n\n``` js\nvar sub = ctx.socket('SUBSCRIBE', {routing: 'topic'});\n```\n. > Why is routing only supported on the instantiation of a socket?\n\nBecause it doesn't make much sense to have different routing for successive connections, since the topic for any publication will then have a variety of meanings. I'm sure one can contrive an example where this is useful, but it's easier to just have different sockets.\n\n> EDIT: I'm getting an error:\n\nIn all likelihood it's because you've connected to the an address as you've used before, but with different routing. The underlying protocol (AMQP) doesn't like things changing in this way. The way to fix it is to go into RabbitMQ (the management console, ideally) and delete the exchanges named for the addresses.\n\nThis is a bit of a pain, I know. I've not thought of a good way of garbage collecting things created by connections (or even deleting them explicitly). It's a problem with AMQP, too.\n. Both of those things are possible. What's not possible is to have a single socket that uses two different kinds of routing.\n\n\"connection\" above refers to calling \"connect\" on a socket, so, i.e., a socket can be connected to a number of addresses.\n. ``` js\nvar sock = ctx.socket('SUBSCRIBE', {routing: 'topic'});\nsock.connect('address', 'routing.key');\nsock.connect('address', 'other.routing.key');\n```\n\n> There documentation isn't very clear.\n\nDo you think examples for each method/option/whatever, along the lines of the above, would help?\n. > Does sock.connect create a new connection, or does it just modify the existing connection on the sock\n\nIt creates a new connection; the socket retains all its other connections. It's must useful for SUBSCRIBE, to get messages for more than one routing key, and WORKER, to listen for more than one kind of job. Otherwise, the benefit of multiple connections is largely moot.\n\nThere's no \"disconnect\", but not for any principled reason.\n. > Are there any potential leak issues to be aware of?\n\nThere is one, and I'm not sure how it can be avoided. Each connection creates resources in the RabbitMQ broker (queues, exchanges), and not all of these can be garbage collected.\n\nFor most programs, it's not a big deal, since they will usually connect to the same addresses again and again. But say a program generated random addresses to connect to, and connected and disconnected repeatedly -- then it would leave junk around.\n\nThe reason it's difficult is that deleting the resources when the socket is closed is the wrong thing to do -- what if another socket is connected to the same resources (which after all is the whole point!). AMQP has some means by which unused resources will be removed, but they are not adequate.\n. > This is not made especially clear, looking at the documentation as it stands. I'll fix that.\n\nI've removed `routing` from the socket options section and mentioned it elsewhere instead. I've also put an example of topics and routing in.\n. > In #59 (comment), you suggest that you can create one socket, and handle different routing keys separately\n\nThat example shows a single socket being subscribed to two keys -- it will get messages from _either_ key. There's no suggestion that the messages are handled differently.\n. The problem is that the `#connect` hasn't completed -- it has to do a round-trip to the server to assert the presence of the queue (`'backend'`, in this case), and it doesn't consider itself connected until that has returned, so writes go nowhere.\n\nThe way to synchronise is to supply a callback to `#connect`:\n\n``` js\ncontext.on('ready', function(){\n    console.log('ready')\n    req.connect('backend', function() {\n        req.write(JSON.stringify({\n                method: 'users.login',\n                info: {}\n            }), 'utf8');\n    });\n});\n```\n. Oh -- I'm sure it used to be. I have a growing pile of documentation editing to do!\n. OK, I have mentioned this in the docs. Thanks for pointing these out, it's made me realise I've been letting the docs fall behind and get messy.\n. rabbit.js doesn't do anything special for `setEncoding`, it leaves it to the readable stream implementation. If I put\n\n``` js\nvar ctx = require('rabbit.js').createContext();\nvar sub = ctx.socket('SUB');\nsub.setEncoding('wrong');\n```\n\nin a file and run it, it crashes with an \"Unknown encoding\" exception. This makes me think, maybe you are calling that from a callback or promise continuation, and the exception is being swallowed. Could that be the case?\n\nHence or otherwise, would you supply a full, minimal example that freezes -- ta.\n. The answers will come back in the order that you sent them, so one thing you could do is keep a queue (array) of the callbacks, and dequeue (`shift`) every time a message comes in.\n. Yes, an ID system would work fine. The discussion here is apposite: https://github.com/squaremo/rabbit.js/issues/48\n. @samholmes How did you get on?\n. @samholmes, just checking that this is working for you. Do you think it's worth me including something in rabbit.js to make this easier? It doesn't fit in the socket-style API, but perhaps it could be a utility.\n. Is that because you are using it from another library?\n. Right. And those other applications need it to be durable, so e.g., bindings don't go away.\nI suppose it would make sense if a \"persistent\" PUBLISH socket declared its exchanges as durable. Does it belong in the socket constructor or `connect` though? It could be both.\n. The constructor arguments tend to just set the default for that socket, so I guess it can be in both places.\n. OK. Sorry for the long silence. I have pushed a change to master which lets you tell a socket not to try and create objects when connecting; this means you can use exchanges and queues created elsewhere with properties different to those rabbit.js would give them.\n\n``` js\nvar ctx = require('rabbit.js').createContext();\nvar pub = ctx.socket('PUB', {noCreate: true});\npub.connect(existingExchange);\n```\n\nIf anyone is game to try this out, would you let me know if it works for you? It's pretty sketchy but I'd like to build on it. Thank you!\n. I am thinking that a 'persistent' socket, either set in the constructor or by setsockopt, should declare its exchange to be durable. Does that break anything? (except backwards compatibility)\n. > Does it break backwards compatibility?\n\nIn the sense that someone already using a \"persistent\" PUB or SUB socket will encounter breakages, since rabbit.js will try to re-declare the exchanges involved with a different 'durable' property (AMQP doesn't like this).\n. It's not usually a good idea to leave messages unacknowledged. Do you mean instead that one should be able to _reject_ a message?\n. If you fail to acknowledge a message, it will remain in limbo until you close the connection to the broker. I wouldn't think that's what you really want -- or at least, I don't think it's what most people would want.\n\nAs well as discarding messages, there is an option to requeue messages. Requeueing a message means other workers might get it, or you might get it again. (When I say \"option\", I mean it is possible in AMQP, the protocol for talking to RabbitMQ). There's no option to say \"redeliver later\", where later is some specified delay, although it could probably be arranged with some extra complication.\n\nI'm not really sure which, if any, of these you really want.\n. Yes, and the library I'm using supports `nack` (and its simpler friend, `reject`), with variations for requeuing or discarding. There is a chance -- a high chance, if there's only one worker -- that the same worker will just get the message again, since messages are re-enqueued at the head of the queue. But if this will be workable for you, how about,\n\n``` js\nvar msg = worksock.read();\nif (msg != null) {\n  if (canProcess(msg)) {\n    doSo(msg);\n    worksock.ack();\n  }\n  else if (maybeSomeoneCanProcess(msg)) {\n    worksock.requeue();\n  }\n  else {\n    worksock.discard();\n  }\n}\n```\n. I'm going to close this, since the pull request was dealt with.\n. Changeset looks good. What is the motivation -- is it to make up for backpressure not quite working?\n. OK merged. Any brave soul want to try it out?\n. Bueller? I can make a new release I think -- one might be queued up anyway. Votes?\n. Roger that @awashbrook, I'll give it a go over the weekend.\n. > Is it as simple as adding another param to ReqSocket.connect\n\nDo you want replies to go back via an exchange as well, or just the requests?\n. The complications are:\n1. The return path must be encoded in each request. The conventional way to do this is to put a queue name in the 'replyTo' header. Having some other encoding (to include an exchange name) would break this convention, and compatibility with any libraries that follow it. There's also limited space -- the header has up to 255 chars, and both queue names and exchange names are up to 255 chars.\n2. Having routing in the middle breaks the REQ/REP contract. Requests can be dropped if there's no-one currently listening, or go to more than one replier, and the requesting socket won't know whether or not to expect an answer, or more than one answer. See the discussion under #57.\n. Another (weaker) objection is that the point of rabbit.js is to not have to think about exchanges and queues.\n\nWhat is your motivation for sending via an exchange?\n. > Also, setting a different exchange than the default allows us to set durability and auto-delete and all that.\n\nThe default exchange is durable, and doesn't need auto-deletion; but I take your point that you can't redeclare it with different properties.\n\nAnother thing that might be useful is being able to \"tap\" service requests by using a direct exchange and listening in with an additional queue.\n\n> I'm not 100% clear what you meant by, \"Having routing in the middle breaks the REQ/REP contract\"\n\nI suppose the library could maintain exchanges and bindings as well as queues. My concern is trying to keep things simple. After a few extensions of the model, the various special cases and interactions between them get hard to document and maintain. Even the topic routing, added recently, is quite awkward. So I prefer to stick with simple rules and not expose much of AMQP.\n. Hi there, thank you for bug report.\n\nIt is reasonable to expect your code to work. The reason it's not -- I suspect (I'll investigate further) -- is that the request socket wants to acknowledge the message after the handler has run, and since you're closing it, it tries to do the ack on an invalid channel.\n\nUsing nextTick or setImmediate is a good work-around for now. Probably `#end` and `#close` ought to do this themselves, to avoid this situation.\n. Good stuff! I'll take a look v. soon.\n. Just from the description above: how come requeue uses `nack` and discard uses `reject`? The difference is admittedly slim, since nack has been supported in RabbitMQ for a long time.\n. Ops, what I meant was that `reject` may as well be used in both places, since it's equivalent (for single messages) to `nack`, and is in the AMQP specification rather than an extension (as `nack` is).\n\nNever mind though, I changed it and merged the previous commit. Thanks!\n. Which headers/properties? You might be better off using squaremo/amqp.node directly -- it's not much more complicated that rabbit.js.\n. > I'm not seeing anything in this repo that would allow me to authenticate a connection.\n\nThe `createContext` procedure allows you to supply a connection URL, and this takes the form given in http://www.rabbitmq.com/uri-spec.html. If you need to supply a username and password, use something like `\"amqp://user:pass@host\"`.\n. > Any thoughts as to why this would not be a good idea?\n\n'auto-delete' means the queue will be deleted once there are no consumers; for example, if you connect a REP socket to `'foo'` with auto-delete then close the socket, `'foo'` will get deleted.\n\nThe trouble is we want this to work for both REP _and_ REQ sockets; that is, we want the queue to be deleted only when there are no REP sockets and no REQ sockets connected. Otherwise, we could have a REQ socket sending its requests into the void with no chance of a reply, even if a REP socket comes along later.\n\nHowever, because REQ sockets _don't_ consume from the queue, there's no way for them to prevent the queue from being deleted.\n. Hello, sorry to miss this one for a while.\n\nThe RabbitMQ docs you quote are saying \"**don't** create a queue per request, create a queue per client\". It's the latter which rabbit.js does.\n. I see, you want to multiplex requests from different sources over the socket (or create multiple sockets). \n\nThe way rabbit.js works, you would have to know the order in which you requested things; internally, it assigns each a correlationId (as you mention), then it emits the answers in the order the requests came in.\n\nSo, as a sketch, you could push each request (as a promise, if you want to keep the scheme in the example above) into a queue (= array), and shift from the queue when the answer came back.\n. Yes, garbage-collecting exchanges is a pain. I haven't come up with a good solution -- it's a gap in AMQP, really.\n\nThe flaw in this change is that it only takes into account subscribers -- when the last subscriber goes, the exchange will be deleted, and any publishers to that exchange will get an error and crash.\n\nSee #49 re garbage collection in general.\n. > The flaw in this change is that it only takes into account subscribers\n\nTo explain the problem a bit more: if an exchange is marked as auto-delete, once no queues are bound to it, it will be deleted. In rabbit.js, queues are owned by subscribers, so in effect what you get is that once there are no subscribers, there are no queues, and once there are no queues, the exchange is deleted.\n\nHowever, there's no analogue to those queues for publishers, so there's no way (that I've come up with) for a publisher to prevent an auto-delete exchange from being deleted. Without that, while this might help keep things tidy, it also causes problems.\n. Ah yes, I can do another release! One sec ..\n. Since you asked nicely, v0.4.2 is up on npmjs.org. It has TASK and WORKER sockets, with `#requeue` and `#discard` on the latter.\n. Just to check I understand what you're after: instead of\n\n```\nvar req = ctx.socket('REQUEST');\nreq.connect(\"questions\");\nreq.write(\"foobar?\");\nvar answer = req.read();\n```\n\nyou'd rather have (something like):\n\n```\nvar req = ctx.socket('REQUEST');\nreq.connect('questions')\nreq.ask('foobar?', function(answer) {\n  // ... something\n});\n```\n\nIs that right? I can see why that's somewhat easier, given that read isn't blocking.\n. I don't think this would be overly difficult but I might not get to it very soon -- anyone fancy giving it a go?\n. > when you read from socket, you should expect to get messages in random order.\n\nI don't think that would help -- you wouldn't know what is an answer to what. It really has to be done in the machinery _behind_ a socket.\n\nIt's easy enough to do without altering the machinery, if you don't mind the callbacks being called in order (i.e., later callbacks waiting for earlier ones, even if they arrive first). Otherwise, it is a bit more difficult than I indicated above.\n. Right, the WORKER socket assumes you're doing just one thing at a time, and scaling up by having more workers.\n\nThis discussion may be apposite: https://github.com/squaremo/rabbit.js/issues/48\n\nThe solution discussed there is to construct a future for each task, and pass that along with the payload to the next stage. Then you can iterate through the promises and ack things in order.\n. > I would love to have a extended ack method that can accept the message payload (or a uniqueId) for acking that single message.\n\nBoth of these are problematic: the first assumes no two payloads will ever be the same (uh-oh); the second requires some means of giving a buffer a unique ID, which is then required when acking it. I suppose it could be a property set on the buffer, but that seems kind of gross -- also, whatever you're passing it on to would have to know to tell you once it's done -- so you may as well give it a future or callback or whatever, rather than an ID.\n. > I think that you can send, along the message, the deliveryTag that message has. As I see [1] the readable.push method, I think this change will break the current API, as it allows only a Buffer or a String\n\nOne can always set a property on the buffer or string -- it's JavaScript after all. But I don't like this solution, because the receiver has to know a very specific behaviour, which is to get the delivery tag from the buffer it's responding to, and somehow attach that to its answer. I'd rather there was a callback.\n\nThe library could help you out: maybe something like\n\n``` js\nvar worker = rabbit.socker('WORKER');\nworker.connect('jobs');\nworker.on('data', function(job) {\n  doSomethingAsync(job, worker.lastMessageAcker());\n});\n```\n. You want to get all the order updates but be able to tell which specific order each message pertains to, is that right?\n\nI would just put the order number in the payload, and take it out of the routing key. If there's something that needs to subscribe to a specific order number (i.e., no wildcard), put it in both the payload and the routing key.\n. Could the callback you provide to processFn possibly be called more than once for a particular message? Otherwise, looking at that code, I can't see how ack could be called more times than there are messages.\n\nBy the way, since `#ack` acknowledges the least recent message, if processFn calls its callback asynchronously, you could ack the wrong messages. It might be better to do it this way (sorry, JS syntax):\n\n``` js\n// ...\nworker.connect(provider);\nfunction doJob() {\n  var job = worker.read();\n  if (!job) worker.once('readable', doJob);\n  else {\n    processFn(JSON.parse(job), function(err) {\n      if (err) return console.log(err);\n      worker.ack();\n      doJob();\n    });\n  }\n}\n```\n\nIn other words, read the next message after processing the last message. (See #79 about acknowledging messages out of order)\n. Nice and simple improvement -- thank you.\n. Hi! Yes there's no particular reason it can't be updated, just me being busy. I ought to take a look at the other outstanding PRs and issues too!\n. I'm not quite sure what you're asking for -- can you sketch out how it would look? (In code, or pseudo-code, perhaps)\n. Ah, I see! OK, that is a worthy aim. Let me take a look at it ..\n. Sorry to take a while to get to this\n. Hello, sorry I didn't look into this. Are you closing the issue because io.js and node.js converged again?\n. OK, I've added node.js 0.12 and ^1 (i.e., io.js) to the engines, and tested them (admittedly the tests are not numerous).\n. Hello,\nDo you mean \"how do I pop (get) messages from a queue\"? If so, you want a `PULL` socket, and to connect with `socket.connect(queueName)`.\n\nIf you mean \"how do I get a _single_ message from a queue\", you may have to use https://github.com/squaremo/amqp.node instead.\n. Node.JS can have pretty unhelpful behaviour under load, so it's difficult to narrow down what might be the problem. As a guess though, it's easy to ignore back-pressure from publishing, and this may mean you are shovelling lots of messages into buffers that are already full -- Node.JS won't prevent you from doing this.\n\nI would have a look at:\n- What RabbitMQ thinks is happening (connection state, message rates, memory use of the server, ...)\n- The memory use of Node.JS as load increases\n\nand check that you are responding to `write` returning `false`.\n. That .. looks fine and works fine for me. Odd indeed.\n\nWhat versions of Node.JS and rabbit.js are you using?\n. RabbitMQ in general can be used as an \"engine\" in the terms of the discussion you point at, and rabbit.js may thereby be suitable as a convenient way of using RabbitMQ.\n\nServing millions of users at a time will take careful design whatever you use, and I would expect there to be more fundamental decisions to make before you look at whether to use rabbit.js or not -- for example, will a RabbitMQ cluster or federation be able to handle the load you expect? How many web servers with SockJS will you need to account for that load? And so on.\n. This is not really encouraged, by design -- PUB/SUB sockets are meant to only collect messages while your application is running.\n\nYou may find it easier to encode what you want in amqplib (squaremo/amqp.node), which has full control over the lifetime of exchanges and queues and so on, at the cost of having to do things at a \"lower level\".\n. The problem here is that the code doesn't yield to I/O at any point. You could write the same thing with a regular net.Socket and get the same result; if you don't return from your handler (in this case, the one you register for 'ready'), no I/O can happen, and the queue into which you're putting bytes cannot clear.\n. Ah I see, you created the queue with a dead-letter exchange already (through the management UI perhaps?), then because rabbit.js doesn't know this and can't be told, it falls over. What a pain!\n\nI would really like to keep the interface to rabbit.js as simple as possible, which is why I don't expose those kinds of options. But I can see this is a major pain for you. Maybe I could check for the queue first, and leave it alone if it exists already.\n\nAs a workaround for now, you could use a policy to set the dead letter exchange for the queue; I've tried this, and it seems that queue attributes that come from policies don't count when it comes to checking for inequivalent args (as the error above has it).\n. I have pushed a change to master which lets you tell a socket not to try and create objects when connecting; this means you can use exchanges and queues created elsewhere with properties different to those rabbit.js would give them.\n\nvar ctx = require('rabbit.js').createContext();\nvar push = ctx.socket('PUSH', {noCreate: true});\npush.connect(existingQueue);\n\nI figure this is a solution to the general problem of using existing objects. Would you let me know if it works for you? If so I will tidy it up and make it a lasting change to the API. Thanks!\n. Rogerio, you can tell the context a heartbeat interval by using a querystring parameter when connecting. Something like\n\n```\nvar ctx = rabbitjs.createContext('amqp://localhost?heartbeat=380')\n```\n\nThe connection URL specification is at http://www.rabbitmq.com/uri-spec.html. amqplib, on which rabbit.js is based, also supports `frame_max` and `channel_max`.\n. > Some more detail here: https://www.rabbitmq.com/uri-query-parameters.html\n\nThose are the parameters accepted by RabbitMQ's Erlang client. The descriptions of `heartbeat`, `frame_max` and `channel_max` are accurate though. amqplib does support SSL configuration, but through a different means.\n. @rogeriobiondi Did the answer above work for you? Please close if so -- thanks!\n. Hi, sorry for not being responsive on this one -- did you 1. give up or 2. figure it out?\n. > Error: connect ECONNREFUSED\n\nThis suggests to me that there's no RabbitMQ running; or that it's not running where the example expects it to be (localhost 5675). You can adapt the example to use a RabbitMQ server somewhere else, by providing a connection URL in the `#createContext` call.\n. That sentence is supposed to indicate that the technique in the ordering example can also be used for worker sockets. Would it be better to say,\n\n> It is important to acknowledge messages in the right order. A way to maintain ordering for REP sockets, which can be adapted to WORKER sockets, is shown in the \"ordering\" example.\n\n(Is it reasonably clear how to adapt the example, do you think?)\n. > It appears that this flexibility exists in Ruby's RabbitMQ lib, so I'm not sure regarding the reason for the handicap.\n\nThis flexibility also exists in https://github.com/squaremo/amqp.node. Calling it a handicap here is not very fair: the idea of rabbit.js is to make using RabbitMQ transparently easy, and that means eliding options that don't matter for the socket-like abstractions given.\n\nI am starting to think that the socket-like abstractions are the less useful bit. Would it be better to just have ways, in amqplib, of publishing and consuming via streams?\n. rabbit.js is fairly portable code in itself (although, inevitably, written in callback/promises style); however, since it relies on amqp.node (aka amqplib), I don't think it's usable outside a Node.JS environment.\n. > (And sorry to squaremo if this feels disrespectful to have this discussion on a rabbit.js issue.)\n\nYeah that is a bit cheeky, but since you had the grace to apologise, I'll allow it ;)\n. Yes, that ought to do it. Beware of relying on RabbitMQ for locking though, as noted in the edits at the end of that blog post. Something like zookeeper may be more reliable, if you need strong consistency guarantees.\n. ",
    "mateodelnorte": "Was just running the examples wrong. All looks good. Thanks. \n. Was just running the examples wrong. All looks good. Thanks. \n. ",
    "adrai": "Thanks for the fast reply. I just looked to this routing example of rabbitmq http://www.rabbitmq.com/tutorials/tutorial-four-python.html\n\nAt the end I need something like pub/sub that provides all messages even when a consumer is temporary down.\n\nPS. Sorry I'm new to node and message queue stuff...\n. Thanks for the fast reply. I just looked to this routing example of rabbitmq http://www.rabbitmq.com/tutorials/tutorial-four-python.html\n\nAt the end I need something like pub/sub that provides all messages even when a consumer is temporary down.\n\nPS. Sorry I'm new to node and message queue stuff...\n. ",
    "soichih": "I am seeing several browser error messages.. Please try http://soichi2.grid.iu.edu:8080/\n\nWhat I don't understand is that, there are no reference to AMQP inside example/sockjs.js. Also, why is sockjs.js requiring itself by doing \"var sockjs = require('sockjs');\" at the top? Where do I configure inside sockjs.js to point to my RabbitMQ server?\n. Sorry, I gave up.. :) I am sure it would have worked if I continued working on it, but I found it much easier to just setup a cometD server with RabbitMQ listener which will 1) queue last messages 2) post to CometD channel. This allows client to 1) receive all recent events when user opens a page 2) receive events in real time via CometD javascript client. \n. I am seeing several browser error messages.. Please try http://soichi2.grid.iu.edu:8080/\n\nWhat I don't understand is that, there are no reference to AMQP inside example/sockjs.js. Also, why is sockjs.js requiring itself by doing \"var sockjs = require('sockjs');\" at the top? Where do I configure inside sockjs.js to point to my RabbitMQ server?\n. Sorry, I gave up.. :) I am sure it would have worked if I continued working on it, but I found it much easier to just setup a cometD server with RabbitMQ listener which will 1) queue last messages 2) post to CometD channel. This allows client to 1) receive all recent events when user opens a page 2) receive events in real time via CometD javascript client. \n. ",
    "fostercraig": "Sorry to chime in here, but I have a question regarding sockets and SockJS in general. I am doing a simple pub/sub using rabbit. I want my browser to publish a message but also subscribe to the same exchange such that it receives the message back. When I first start node and load up the page, everything works swimmingly. However, when I refresh the pub works from the browser but it stops receiving any messages. So my real question is how do you have both a pub and sub SockJS connection open with the browser? Am I perhaps not properly closing the connection upon refresh? \n\nI've tried one method, which might not be good (as I am guessing these are considered separate sockjs connections):\n\nvar sock = new SockJS(\"http://mydomain:5918/socks\");\nsock.onopen = function() {\n    sock.send('pub comments');\n}\n\nvar sock_rec=new SockJS(\"http://mydomain:5918/socks\");\nsock_rec.onopen=function() {\n  sock_rec.send('sub comments');\n}\n\nsock_rec.onmessage = function(msg) {\n    console.log('got a message');\n    console.log(msg);\n    var raw_data=JSON.parse(msg.data);\n    var data=raw_data['comment'] + ' posted by ' + raw_data['username'] + ' ' + raw_data['time'] + '<br>';\n\n```\n   var childContent= new dijit.layout.ContentPane({\n                       content:data\n        });\n    childContent.placeAt(commentPane.containerNode,'last');\n```\n\n};\n. Ah, ok. I'm such an ignoramus with this stuff...\n\nBut I think maybe a better way is to just have the browser do a pub\nand send back the message from node back over the Sockjs connection.\nThat seems simpler!\n\nOn Thu, Jan 5, 2012 at 10:10 AM, Michael Bridgen\nreply@reply.github.com\nwrote:\n\n> @fostercraig: To be honest I am not completely sure of the reconnect semantics of SockJS. Just opening another ought to work however.\n> One possible hitch is that SockJS (and Socket.IO, and HTTP for that matter) is limited to two HTTP open connections to any given host; in practice this means that in general one can only maintain one SockJS connection to a host, since the fallback needs one connection for sending and one for long polling.\n> \n> ---\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/squaremo/rabbit.js/issues/6#issuecomment-3373708\n. Sorry to chime in here, but I have a question regarding sockets and SockJS in general. I am doing a simple pub/sub using rabbit. I want my browser to publish a message but also subscribe to the same exchange such that it receives the message back. When I first start node and load up the page, everything works swimmingly. However, when I refresh the pub works from the browser but it stops receiving any messages. So my real question is how do you have both a pub and sub SockJS connection open with the browser? Am I perhaps not properly closing the connection upon refresh? \n\nI've tried one method, which might not be good (as I am guessing these are considered separate sockjs connections):\n\nvar sock = new SockJS(\"http://mydomain:5918/socks\");\nsock.onopen = function() {\n    sock.send('pub comments');\n}\n\nvar sock_rec=new SockJS(\"http://mydomain:5918/socks\");\nsock_rec.onopen=function() {\n  sock_rec.send('sub comments');\n}\n\nsock_rec.onmessage = function(msg) {\n    console.log('got a message');\n    console.log(msg);\n    var raw_data=JSON.parse(msg.data);\n    var data=raw_data['comment'] + ' posted by ' + raw_data['username'] + ' ' + raw_data['time'] + '<br>';\n\n```\n   var childContent= new dijit.layout.ContentPane({\n                       content:data\n        });\n    childContent.placeAt(commentPane.containerNode,'last');\n```\n\n};\n. Ah, ok. I'm such an ignoramus with this stuff...\n\nBut I think maybe a better way is to just have the browser do a pub\nand send back the message from node back over the Sockjs connection.\nThat seems simpler!\n\nOn Thu, Jan 5, 2012 at 10:10 AM, Michael Bridgen\nreply@reply.github.com\nwrote:\n\n> @fostercraig: To be honest I am not completely sure of the reconnect semantics of SockJS. Just opening another ought to work however.\n> One possible hitch is that SockJS (and Socket.IO, and HTTP for that matter) is limited to two HTTP open connections to any given host; in practice this means that in general one can only maintain one SockJS connection to a host, since the fallback needs one connection for sending and one for long polling.\n> \n> ---\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/squaremo/rabbit.js/issues/6#issuecomment-3373708\n. ",
    "majek": "Hmm, not against master. Closing. See #8\n. Now better.\n. Hmm, not against master. Closing. See #8\n. Now better.\n. ",
    "josephholsten": "I've got a work queue, each client should only pop off a single message at a time. I typically use ack to confirm that the message was processed.\n\nIs there a way to accomplish this using the existing socket patterns?\n. I used rabbit.js because it had simpler API semantics than full AMQP, and I was working with folks that only understood resque style queues.\n\nI pulled out regular amqp to get acks, but it's a much more complicated but of code.  I worry about other people maintaining it because they have to grok amqp semantics before they can do anything. \n\nThe reason I'm not having the posting process retry after a timeout is because that process is almost certainly finished in every work-queue style context. If I was happy with retry/timeout I would probably implement that via an HTTP service that respond with status 202 Accepted while it works.\n\nI imagine a great deal of my resistance is due to not groking zmq and brokerless semantics. \n. I should mention that your reliability concern: that work might be done 1.5ish times, is not a concern for folks who've dealt with clustered brokers. A common guarantee is that a message will be delivered at least once. You just learn to make processing a message have only idempotent side effects.\n\nAnyway, I'm off to go learn zmq better. \n. I've got a work queue, each client should only pop off a single message at a time. I typically use ack to confirm that the message was processed.\n\nIs there a way to accomplish this using the existing socket patterns?\n. I used rabbit.js because it had simpler API semantics than full AMQP, and I was working with folks that only understood resque style queues.\n\nI pulled out regular amqp to get acks, but it's a much more complicated but of code.  I worry about other people maintaining it because they have to grok amqp semantics before they can do anything. \n\nThe reason I'm not having the posting process retry after a timeout is because that process is almost certainly finished in every work-queue style context. If I was happy with retry/timeout I would probably implement that via an HTTP service that respond with status 202 Accepted while it works.\n\nI imagine a great deal of my resistance is due to not groking zmq and brokerless semantics. \n. I should mention that your reliability concern: that work might be done 1.5ish times, is not a concern for folks who've dealt with clustered brokers. A common guarantee is that a message will be delivered at least once. You just learn to make processing a message have only idempotent side effects.\n\nAnyway, I'm off to go learn zmq better. \n. ",
    "jgornick": "@squaremo I'm +1 for adding an ack into the PULL socket connection. \n\nMaybe something like:\n\n```\nvar socket = conn.socket('PULL', { ack: true });\nsocket.on('data', function(data, ack) {\n  // ... do async processing on the data\n\n  // call ack when done processing\n  ack();\n});\nsocket.connect('some_queue');\n```\n\nThoughts?\n. @squaremo Yeah, the latter is the case for me. I chose rabbit.js because it was the more popular and most updated RabbitMQ library, even though it's an abstraction on node-ampq. I guess if ack is really important to me, I'll use the node-ampq library directly.\n\nI totally get where you're coming from and the resistance to adding this type of feature. The rabbit.js library is meant to be the most simple abstraction of the node AMPQ library. It's simple because it's modeled after ZeroMQ, which is also very cool!\n\nThanks for your input!\n. @squaremo I'm +1 for adding an ack into the PULL socket connection. \n\nMaybe something like:\n\n```\nvar socket = conn.socket('PULL', { ack: true });\nsocket.on('data', function(data, ack) {\n  // ... do async processing on the data\n\n  // call ack when done processing\n  ack();\n});\nsocket.connect('some_queue');\n```\n\nThoughts?\n. @squaremo Yeah, the latter is the case for me. I chose rabbit.js because it was the more popular and most updated RabbitMQ library, even though it's an abstraction on node-ampq. I guess if ack is really important to me, I'll use the node-ampq library directly.\n\nI totally get where you're coming from and the resistance to adding this type of feature. The rabbit.js library is meant to be the most simple abstraction of the node AMPQ library. It's simple because it's modeled after ZeroMQ, which is also very cool!\n\nThanks for your input!\n. ",
    "alexfernandez": "Yes, please! Adding ack, which is [present in RabbitMQ](http://www.rabbitmq.com/tutorials/tutorial-two-python.html), would make my life insanely easier, and rabbit.js the best RabbitMQ library by far.\n\nMy workflow is a work queue, set up as follows: many processes write to a channel, and one process reads from it; if the database is up it stores the required values, otherwise it should wait for a bit until it is online.\n\nRight now I have to:\n- either check in the receiver that the database is fine, otherwise store in memory (and lose persistence),\n- or check in the sender that there is a response and otherwise resend, placing the burden on the client.\n  I have opted to deal with error conditions by rewriting the message to the queue if there is any problem, but this is costly and unreliable.\n\nHaving two special types of socket: \"PRODUCE\", \"CONSUME\", where the second accepts an ack() to actually consume the element in the queue, would perhaps be a nice solution.\n. We had to use amqp that supports confirms, but we would happily go back to rabbit.js if support was added.\n. We want acknowledgements; sorry for the confusion.\n\nOur use case is as follows: we want to ensure that messages are dealt with once and only once. After taking the message out of the queue the client can crash before storing everything into the database, and in that case we would like the message to be dealt with after a restart. This is solved nicely with what is referred to as consumer acknowledgements in RabbitMQ. We do not need those acknowledgements to travel back to the publishers.\n. Yes, please! Adding ack, which is [present in RabbitMQ](http://www.rabbitmq.com/tutorials/tutorial-two-python.html), would make my life insanely easier, and rabbit.js the best RabbitMQ library by far.\n\nMy workflow is a work queue, set up as follows: many processes write to a channel, and one process reads from it; if the database is up it stores the required values, otherwise it should wait for a bit until it is online.\n\nRight now I have to:\n- either check in the receiver that the database is fine, otherwise store in memory (and lose persistence),\n- or check in the sender that there is a response and otherwise resend, placing the burden on the client.\n  I have opted to deal with error conditions by rewriting the message to the queue if there is any problem, but this is costly and unreliable.\n\nHaving two special types of socket: \"PRODUCE\", \"CONSUME\", where the second accepts an ack() to actually consume the element in the queue, would perhaps be a nice solution.\n. We had to use amqp that supports confirms, but we would happily go back to rabbit.js if support was added.\n. We want acknowledgements; sorry for the confusion.\n\nOur use case is as follows: we want to ensure that messages are dealt with once and only once. After taking the message out of the queue the client can crash before storing everything into the database, and in that case we would like the message to be dealt with after a restart. This is solved nicely with what is referred to as consumer acknowledgements in RabbitMQ. We do not need those acknowledgements to travel back to the publishers.\n. ",
    "accelerated": "So essentially if one is enabling this ACK feature in their code (which hopefully can be turned on/off at will via a config option), it would mean that unless they explicitly ACK the 'data' event, the message itself will not be de-queued? In this case, will the message be re-queued at the back of the queue by rabbitMQ or left in the same position at the front and resend immediately to another consumer?  \n. Chaoser, you can clone my branch, I have made all the changes to rabbit.js to support everything from queue/message/exchange settings. \n. @jamuhl I've branched out and made the changes already. Feel free to use my codebase if you like. \n. persistent messages, durable queues, etc which survive the disconnect from a client. I believe it's there in the pull request i sent a while back. This was one of the changes I added because I needed clients to be able to publish messages to queues where nobody is connected yet so that they will get them next time they login. Also messages should survive a rabbitMQ server crash...\n. I believe the simplicity of rabbit.js can be kept with the addition of a few options. Don't see how that can hurt anyone. That's the reason why I chose to slightly modify your library rather than have a wrapper on top of AMQP. I'm not claiming at all that my solution is the best approach but it's worth considering IMHO. \n. Hi Michael, \nIMHO the simplest way to go about writing a library interface API to rabbit.js is to expose all the functionality of rabbitMQ in an easy to use way and give users access to provisioning and controlling all the features rabbitMQ has to offer. Hence letting users write their own abstraction levels via their application layer if they so desire. Of course users can use AMQP directly, but that is too low level and not necessarily rabbitMQ-centric.\nBy making certain assumptions (and thus hardcoding options - ie \"the handful of permutations\"), you probably fit the bill for a percentage of use-cases, but as it so happens, most people (including myself) need more flexibility/control over lifetime of messages, queues, exchanges, etc. Also, as you can see, most forked projects from your codebase, have been branched off especially for this reason - more control over certain options. There is even a new project https://github.com/adrai/rabbitmq-nodejs-client which was redesigned for the ground up, because this library was too restrictive. I had to struggle a lot to understand the use of options in this library and took an equal amount of time to figure out what I can and cannot do to leverage rabbitMQ. By exposing all options, users 1) know exactly what they can change w/o having to guess what the option nomenclature is 2) can see exactly what each default setting is 3) can change each setting as per their own application needs. I also took care in segregating exchange, queue and message options. I believe your API muddles the lines a bit. Your library is very well written, so it's a bit of a shame IMHO not to use it to its full extent! \nAlso my development requests which I opened were addressed in this pull request. \n. Hi Michael, thanks for the reply.\nHowever I would like to add one more thing. My proposed API is exactly equal in all respects to your API, except that you can just pass more options. You do allow some options to be passed (so it's not a total abstraction), and so my version just allows a few more (we're talking additional properties of the same JSON object here). The simplicity is still there. Users can still use the \"same\" subset of options as before. Their REQ/REP or PUB/SUB will behave in exactly the same way. I can send you code snippets which i'm using in production and you will see that creating a PUB/SUB is just as easy and straightforward as before. So users are not sacrificing anything and it is definitely NOT as complex as using AMQP directly. WIN-WIN in my opinion. But I'm willing to work with you on defining some other interface if you want...it's hard for me to keep sync-ing code and maintaining my branch all the time. I was hoping we could somehow merge both ideologies. \n. An API for explicitly deleting a queue would be good as well, besides the TTL. It's there in my cloned branch as well if you want to take a look.\n. Hi Sqaremo, after the socket cleanup issue which you just finished, is there a plan to add options for persistent/durable queues and persistent messages? as well as deleting an exchange/queues on demand? I am trying not to have my own branch if possible...and for me continuously merging with your master is getting too messy. Thanks!\n. So essentially if one is enabling this ACK feature in their code (which hopefully can be turned on/off at will via a config option), it would mean that unless they explicitly ACK the 'data' event, the message itself will not be de-queued? In this case, will the message be re-queued at the back of the queue by rabbitMQ or left in the same position at the front and resend immediately to another consumer?  \n. Chaoser, you can clone my branch, I have made all the changes to rabbit.js to support everything from queue/message/exchange settings. \n. @jamuhl I've branched out and made the changes already. Feel free to use my codebase if you like. \n. persistent messages, durable queues, etc which survive the disconnect from a client. I believe it's there in the pull request i sent a while back. This was one of the changes I added because I needed clients to be able to publish messages to queues where nobody is connected yet so that they will get them next time they login. Also messages should survive a rabbitMQ server crash...\n. I believe the simplicity of rabbit.js can be kept with the addition of a few options. Don't see how that can hurt anyone. That's the reason why I chose to slightly modify your library rather than have a wrapper on top of AMQP. I'm not claiming at all that my solution is the best approach but it's worth considering IMHO. \n. Hi Michael, \nIMHO the simplest way to go about writing a library interface API to rabbit.js is to expose all the functionality of rabbitMQ in an easy to use way and give users access to provisioning and controlling all the features rabbitMQ has to offer. Hence letting users write their own abstraction levels via their application layer if they so desire. Of course users can use AMQP directly, but that is too low level and not necessarily rabbitMQ-centric.\nBy making certain assumptions (and thus hardcoding options - ie \"the handful of permutations\"), you probably fit the bill for a percentage of use-cases, but as it so happens, most people (including myself) need more flexibility/control over lifetime of messages, queues, exchanges, etc. Also, as you can see, most forked projects from your codebase, have been branched off especially for this reason - more control over certain options. There is even a new project https://github.com/adrai/rabbitmq-nodejs-client which was redesigned for the ground up, because this library was too restrictive. I had to struggle a lot to understand the use of options in this library and took an equal amount of time to figure out what I can and cannot do to leverage rabbitMQ. By exposing all options, users 1) know exactly what they can change w/o having to guess what the option nomenclature is 2) can see exactly what each default setting is 3) can change each setting as per their own application needs. I also took care in segregating exchange, queue and message options. I believe your API muddles the lines a bit. Your library is very well written, so it's a bit of a shame IMHO not to use it to its full extent! \nAlso my development requests which I opened were addressed in this pull request. \n. Hi Michael, thanks for the reply.\nHowever I would like to add one more thing. My proposed API is exactly equal in all respects to your API, except that you can just pass more options. You do allow some options to be passed (so it's not a total abstraction), and so my version just allows a few more (we're talking additional properties of the same JSON object here). The simplicity is still there. Users can still use the \"same\" subset of options as before. Their REQ/REP or PUB/SUB will behave in exactly the same way. I can send you code snippets which i'm using in production and you will see that creating a PUB/SUB is just as easy and straightforward as before. So users are not sacrificing anything and it is definitely NOT as complex as using AMQP directly. WIN-WIN in my opinion. But I'm willing to work with you on defining some other interface if you want...it's hard for me to keep sync-ing code and maintaining my branch all the time. I was hoping we could somehow merge both ideologies. \n. An API for explicitly deleting a queue would be good as well, besides the TTL. It's there in my cloned branch as well if you want to take a look.\n. Hi Sqaremo, after the socket cleanup issue which you just finished, is there a plan to add options for persistent/durable queues and persistent messages? as well as deleting an exchange/queues on demand? I am trying not to have my own branch if possible...and for me continuously merging with your master is getting too messy. Thanks!\n. ",
    "RuslanZavacky": "So, how ended adding acknowledgement? RabbitMQ support confirms, and they are actually really needed :)\nhttp://www.rabbitmq.com/confirms.html\n\nIf you have no plans on adding them, I think I'll do it by myself, because this is really needed feature.\n. I've actually moved to amqp itself, need more control over queues settings\n. thanks, I'll take a look\n. So, how ended adding acknowledgement? RabbitMQ support confirms, and they are actually really needed :)\nhttp://www.rabbitmq.com/confirms.html\n\nIf you have no plans on adding them, I think I'll do it by myself, because this is really needed feature.\n. I've actually moved to amqp itself, need more control over queues settings\n. thanks, I'll take a look\n. ",
    "scriby": "Here's the rabbit tutorial that talks about work queues and acks: https://www.rabbitmq.com/tutorials/tutorial-two-python.html. Seems like a pattern that should be well supported by the node.js library as well.\n. Here's the rabbit tutorial that talks about work queues and acks: https://www.rabbitmq.com/tutorials/tutorial-two-python.html. Seems like a pattern that should be well supported by the node.js library as well.\n. ",
    "elcct": "@squaremo this is brilliant! I can confirm it works :) Exactly what I need. Thank you.\n\nIs it possible that you update in npm, so I could update project automatically?\n. @squaremo this is brilliant! I can confirm it works :) Exactly what I need. Thank you.\n\nIs it possible that you update in npm, so I could update project automatically?\n. ",
    "dokie": "Well I needed to set/check the AMQP message headers, to know if the AMQP connect fail and also soon we need to define queue and exchange attributes. I chose to use rabbit.js. because it was a cleaner API. For now we can just use my fork. \n. No worries. I can handle whatever I need to do. The bugs in node-amqp concerned me for a while but again I'll just fix em for my own stuff, eg no error emerges when trying to connect to non localhost server that doesn't exist or is not reachable gets swallowed  it if it's local and not available a connection refused error is thrown. \n. On the node-amqp API, it's not that bad but it's more involved initially to get a simple RPC message exchange going. \n. Well I needed to set/check the AMQP message headers, to know if the AMQP connect fail and also soon we need to define queue and exchange attributes. I chose to use rabbit.js. because it was a cleaner API. For now we can just use my fork. \n. No worries. I can handle whatever I need to do. The bugs in node-amqp concerned me for a while but again I'll just fix em for my own stuff, eg no error emerges when trying to connect to non localhost server that doesn't exist or is not reachable gets swallowed  it if it's local and not available a connection refused error is thrown. \n. On the node-amqp API, it's not that bad but it's more involved initially to get a simple RPC message exchange going. \n. ",
    "ArnaudT": " Hi,\n\nYes, It works now. I used the handlers when I connect to rabbitMQ the first time. I don't know why I change for the reconnect code!\n\nThanks for your help.\n\nI replaced the reconnect code with this:\n\n```\n   try {\n        pubStatSub.write(JSON.stringify(message), 'utf8');\n    } catch (err) {\n        context = require('rabbit.js').createContext(\"amqp://\" + CONFIG.amqpHost + \":\" + CONFIG.amqpPort);\n        context.on('ready', function () {\n            var pubStatSub = context.socket('PUSH');\n            pubStatSub.connect('statsub_queue', function() {\n                pubStatSub.write(JSON.stringify(message), 'utf8');\n            });\n        });\n    }\n```\n.  Hi,\n\nYes, It works now. I used the handlers when I connect to rabbitMQ the first time. I don't know why I change for the reconnect code!\n\nThanks for your help.\n\nI replaced the reconnect code with this:\n\n```\n   try {\n        pubStatSub.write(JSON.stringify(message), 'utf8');\n    } catch (err) {\n        context = require('rabbit.js').createContext(\"amqp://\" + CONFIG.amqpHost + \":\" + CONFIG.amqpPort);\n        context.on('ready', function () {\n            var pubStatSub = context.socket('PUSH');\n            pubStatSub.connect('statsub_queue', function() {\n                pubStatSub.write(JSON.stringify(message), 'utf8');\n            });\n        });\n    }\n```\n. ",
    "seallee": "I just down loaded it from https://github.com/squaremo/rabbit.js/downloads, I will check it later. Thanks for your help.\nHow could I down load Version 0.1.0 ?\n. I used npm to down load it, but it failed.\nHere is the information:\n\n[root@www npm]# npm install amqp@0.1.0 -g\nnpm http GET https://registry.npmjs.org/amqp/0.1.0\nnpm http 304 https://registry.npmjs.org/amqp/0.1.0\nnpm WARN amqp@0.1.0 package.json: bugs['web'] should probably be bugs['url']\n\nnpm ERR! Unsupported\nnpm ERR! Not compatible with your version of node/npm: amqp@0.1.0\nnpm ERR! Required: {\"node\":\"0.4 || 0.5\"}\nnpm ERR! Actual:   {\"npm\":\"1.1.9\",\"node\":\"0.6.13\"}\nnpm ERR! \nnpm ERR! System Linux 2.6.32-71.el6.x86_64\nnpm ERR! command \"node\" \"/usr/local/bin/npm\" \"install\" \"amqp@0.1.0\"\nnpm ERR! cwd /usr/local/lib/node_modules/npm\nnpm ERR! node -v v0.6.13\nnpm ERR! npm -v 1.1.9\nnpm ERR! code ENOTSUP\nnpm ERR! message Unsupported\nnpm ERR! errno {}\nnpm ERR! \nnpm ERR! Additional logging details can be found in:\nnpm ERR!     /usr/local/lib/node_modules/npm/npm-debug.log\nnpm not ok\n\nIs that to say, if I want to use amqp@0.1.0, I have to use the nodejs 0.4/0.5?\nOr could you send it to me?(lihongbo_9@163.com)\nThank you!\n. Yeah, it works now!\nI can go on testing, thanks.\n. I just down loaded it from https://github.com/squaremo/rabbit.js/downloads, I will check it later. Thanks for your help.\nHow could I down load Version 0.1.0 ?\n. I used npm to down load it, but it failed.\nHere is the information:\n\n[root@www npm]# npm install amqp@0.1.0 -g\nnpm http GET https://registry.npmjs.org/amqp/0.1.0\nnpm http 304 https://registry.npmjs.org/amqp/0.1.0\nnpm WARN amqp@0.1.0 package.json: bugs['web'] should probably be bugs['url']\n\nnpm ERR! Unsupported\nnpm ERR! Not compatible with your version of node/npm: amqp@0.1.0\nnpm ERR! Required: {\"node\":\"0.4 || 0.5\"}\nnpm ERR! Actual:   {\"npm\":\"1.1.9\",\"node\":\"0.6.13\"}\nnpm ERR! \nnpm ERR! System Linux 2.6.32-71.el6.x86_64\nnpm ERR! command \"node\" \"/usr/local/bin/npm\" \"install\" \"amqp@0.1.0\"\nnpm ERR! cwd /usr/local/lib/node_modules/npm\nnpm ERR! node -v v0.6.13\nnpm ERR! npm -v 1.1.9\nnpm ERR! code ENOTSUP\nnpm ERR! message Unsupported\nnpm ERR! errno {}\nnpm ERR! \nnpm ERR! Additional logging details can be found in:\nnpm ERR!     /usr/local/lib/node_modules/npm/npm-debug.log\nnpm not ok\n\nIs that to say, if I want to use amqp@0.1.0, I have to use the nodejs 0.4/0.5?\nOr could you send it to me?(lihongbo_9@163.com)\nThank you!\n. Yeah, it works now!\nI can go on testing, thanks.\n. ",
    "devikalpana": "im running nodeserver.js in terminal\n. i solved the problem\n. im running nodeserver.js in terminal\n. i solved the problem\n. ",
    "cpsubrian": "Cool.  If I get some time this afternoon I may do the same.\n. Cool.  If I get some time this afternoon I may do the same.\n. ",
    "fabdrol": "@squaremo Thanks for your quick reply.\nHere's the result. Seems like all connections are okay, exept maybe the \"blocking\" thing?\nListen.js still isn't receiving data... :(\n\n```\nroot@vps20841:~/node# rabbitmqctl list_connections\nListing connections ...\nguest   127.0.0.1   48329   blocked\nguest   127.0.0.1   56268   blocked\nguest   127.0.0.1   44985   blocked\nguest   127.0.0.1   48339   blocked\nguest   127.0.0.1   48335   blocked\nguest   127.0.0.1   59207   blocked\nguest   127.0.0.1   49693   blocked\nguest   127.0.0.1   38669   blocked\nguest   127.0.0.1   42090   blocked\nguest   127.0.0.1   59367   blocked\nguest   127.0.0.1   55873   blocked\nguest   127.0.0.1   48338   blocked\nguest   127.0.0.1   39871   blocked\nguest   127.0.0.1   56802   blocked\nguest   127.0.0.1   51745   blocked\nguest   127.0.0.1   32970   blocked\nguest   127.0.0.1   52902   blocked\nguest   127.0.0.1   44522   blocked\nguest   127.0.0.1   48351   blocked\nguest   127.0.0.1   45223   blocked\nguest   127.0.0.1   36377   blocked\nguest   127.0.0.1   41351   blocked\nguest   127.0.0.1   39465   blocked\nguest   127.0.0.1   48512   blocked\nguest   127.0.0.1   46911   blocked\nguest   127.0.0.1   43706   blocked\nguest   127.0.0.1   60939   blocked\nguest   127.0.0.1   44780   blocked\nguest   127.0.0.1   47939   blocked\nguest   127.0.0.1   48350   blocking\nguest   127.0.0.1   48330   blocked\nguest   127.0.0.1   58946   blocked\nguest   127.0.0.1   39132   blocked\nguest   127.0.0.1   57959   blocked\nguest   127.0.0.1   55610   blocked\nguest   127.0.0.1   44325   blocked\nguest   127.0.0.1   60704   blocked\nguest   127.0.0.1   39505   blocked\nguest   127.0.0.1   39560   blocked\nguest   127.0.0.1   59698   blocked\nguest   127.0.0.1   54807   blocked\nguest   127.0.0.1   53337   blocked\nguest   127.0.0.1   43231   blocked\nguest   127.0.0.1   58111   blocked\nguest   127.0.0.1   48336   blocked\nguest   127.0.0.1   56431   blocked\nguest   127.0.0.1   47084   blocked\nguest   127.0.0.1   38705   blocked\nguest   127.0.0.1   50742   blocked\nguest   127.0.0.1   50534   blocked\nguest   127.0.0.1   44785   blocked\nguest   127.0.0.1   48332   blocked\nguest   127.0.0.1   43799   blocked\nguest   127.0.0.1   52334   blocked\nguest   127.0.0.1   49425   blocked\nguest   127.0.0.1   45581   blocked\nguest   127.0.0.1   37266   blocked\nguest   127.0.0.1   35015   blocked\nguest   127.0.0.1   48204   blocked\nguest   127.0.0.1   54950   blocked\nguest   127.0.0.1   42833   blocked\nguest   127.0.0.1   41091   blocked\nguest   127.0.0.1   57612   blocked\nguest   127.0.0.1   39818   blocked\nguest   127.0.0.1   34384   blocked\nguest   127.0.0.1   44064   blocked\nguest   127.0.0.1   34022   blocked\nguest   127.0.0.1   47832   blocked\n...done.\nroot@vps20841:~/node# rabbitmqctl list_exchanges\nListing exchanges ...\nfoobar  fanout\namq.rabbitmq.trace  topic\namq.rabbitmq.log    topic\namq.match   headers\namq.headers headers\namq.topic   topic\namq.direct  direct\namq.fanout  fanout\n    direct\napp:events  fanout\n...done.\nroot@vps20841:~/node# rabbitmqctl list_queues\nListing queues ...\namq.gen-A8zGHUad-iEEOFhlbqAkcS  0\n...done.\nroot@vps20841:~/node# rabbitmqctl list_bindings\nListing bindings ...\n    exchange    amq.gen-A8zGHUad-iEEOFhlbqAkcS  queue   amq.gen-A8zGHUad-iEEOFhlbqAkcS  []\nfoobar  exchange    amq.gen-A8zGHUad-iEEOFhlbqAkcS  queue       []\n...done.\nroot@vps20841:~/node# \n```\n. hmm what would be a minimum of memory to run at? If RabbitMQ's memory footprint's too large, it might be wise to find another solution. My app is quite memory intensive...\n. I've moved to Redis pub/sub and increased my memory size. Thanks a lot for your help anyway. Even if I'd be able to get RabbitMQ working, I guess it's still overhead! (I'm already using Redis, and guaranteed delivery isn't necessery anyway)\n. Haha, I don't know, maybe guaranteed delivery and stuff? Redis is blazingly fast, but it is also rather memory intensive if used on large scale... I wrote this really simple messenger based on redis: https://github.com/fabdrol/simplemessenger\nbtw, I suppose RabbitMQ also operates exclusively in memory.. the main benefit would be when you already use Redis, so there's less overhead\n. @squaremo Thanks for your quick reply.\nHere's the result. Seems like all connections are okay, exept maybe the \"blocking\" thing?\nListen.js still isn't receiving data... :(\n\n```\nroot@vps20841:~/node# rabbitmqctl list_connections\nListing connections ...\nguest   127.0.0.1   48329   blocked\nguest   127.0.0.1   56268   blocked\nguest   127.0.0.1   44985   blocked\nguest   127.0.0.1   48339   blocked\nguest   127.0.0.1   48335   blocked\nguest   127.0.0.1   59207   blocked\nguest   127.0.0.1   49693   blocked\nguest   127.0.0.1   38669   blocked\nguest   127.0.0.1   42090   blocked\nguest   127.0.0.1   59367   blocked\nguest   127.0.0.1   55873   blocked\nguest   127.0.0.1   48338   blocked\nguest   127.0.0.1   39871   blocked\nguest   127.0.0.1   56802   blocked\nguest   127.0.0.1   51745   blocked\nguest   127.0.0.1   32970   blocked\nguest   127.0.0.1   52902   blocked\nguest   127.0.0.1   44522   blocked\nguest   127.0.0.1   48351   blocked\nguest   127.0.0.1   45223   blocked\nguest   127.0.0.1   36377   blocked\nguest   127.0.0.1   41351   blocked\nguest   127.0.0.1   39465   blocked\nguest   127.0.0.1   48512   blocked\nguest   127.0.0.1   46911   blocked\nguest   127.0.0.1   43706   blocked\nguest   127.0.0.1   60939   blocked\nguest   127.0.0.1   44780   blocked\nguest   127.0.0.1   47939   blocked\nguest   127.0.0.1   48350   blocking\nguest   127.0.0.1   48330   blocked\nguest   127.0.0.1   58946   blocked\nguest   127.0.0.1   39132   blocked\nguest   127.0.0.1   57959   blocked\nguest   127.0.0.1   55610   blocked\nguest   127.0.0.1   44325   blocked\nguest   127.0.0.1   60704   blocked\nguest   127.0.0.1   39505   blocked\nguest   127.0.0.1   39560   blocked\nguest   127.0.0.1   59698   blocked\nguest   127.0.0.1   54807   blocked\nguest   127.0.0.1   53337   blocked\nguest   127.0.0.1   43231   blocked\nguest   127.0.0.1   58111   blocked\nguest   127.0.0.1   48336   blocked\nguest   127.0.0.1   56431   blocked\nguest   127.0.0.1   47084   blocked\nguest   127.0.0.1   38705   blocked\nguest   127.0.0.1   50742   blocked\nguest   127.0.0.1   50534   blocked\nguest   127.0.0.1   44785   blocked\nguest   127.0.0.1   48332   blocked\nguest   127.0.0.1   43799   blocked\nguest   127.0.0.1   52334   blocked\nguest   127.0.0.1   49425   blocked\nguest   127.0.0.1   45581   blocked\nguest   127.0.0.1   37266   blocked\nguest   127.0.0.1   35015   blocked\nguest   127.0.0.1   48204   blocked\nguest   127.0.0.1   54950   blocked\nguest   127.0.0.1   42833   blocked\nguest   127.0.0.1   41091   blocked\nguest   127.0.0.1   57612   blocked\nguest   127.0.0.1   39818   blocked\nguest   127.0.0.1   34384   blocked\nguest   127.0.0.1   44064   blocked\nguest   127.0.0.1   34022   blocked\nguest   127.0.0.1   47832   blocked\n...done.\nroot@vps20841:~/node# rabbitmqctl list_exchanges\nListing exchanges ...\nfoobar  fanout\namq.rabbitmq.trace  topic\namq.rabbitmq.log    topic\namq.match   headers\namq.headers headers\namq.topic   topic\namq.direct  direct\namq.fanout  fanout\n    direct\napp:events  fanout\n...done.\nroot@vps20841:~/node# rabbitmqctl list_queues\nListing queues ...\namq.gen-A8zGHUad-iEEOFhlbqAkcS  0\n...done.\nroot@vps20841:~/node# rabbitmqctl list_bindings\nListing bindings ...\n    exchange    amq.gen-A8zGHUad-iEEOFhlbqAkcS  queue   amq.gen-A8zGHUad-iEEOFhlbqAkcS  []\nfoobar  exchange    amq.gen-A8zGHUad-iEEOFhlbqAkcS  queue       []\n...done.\nroot@vps20841:~/node# \n```\n. hmm what would be a minimum of memory to run at? If RabbitMQ's memory footprint's too large, it might be wise to find another solution. My app is quite memory intensive...\n. I've moved to Redis pub/sub and increased my memory size. Thanks a lot for your help anyway. Even if I'd be able to get RabbitMQ working, I guess it's still overhead! (I'm already using Redis, and guaranteed delivery isn't necessery anyway)\n. Haha, I don't know, maybe guaranteed delivery and stuff? Redis is blazingly fast, but it is also rather memory intensive if used on large scale... I wrote this really simple messenger based on redis: https://github.com/fabdrol/simplemessenger\nbtw, I suppose RabbitMQ also operates exclusively in memory.. the main benefit would be when you already use Redis, so there's less overhead\n. ",
    "vvo": "just saw global leak was fixed in another pull request.\n. ``` bash\n-> % npm ls\nrabbit.js@0.2.0 /home/vvo/Dropbox/documents/sites/Fasterize/rabbit.js\n\u2514\u2500\u2500 amqp@0.1.1\n```\n\nAlso this is the one provided by a fresh install of rabbit.js\n. Great! Thank you :)\n. just saw global leak was fixed in another pull request.\n. ``` bash\n-> % npm ls\nrabbit.js@0.2.0 /home/vvo/Dropbox/documents/sites/Fasterize/rabbit.js\n\u2514\u2500\u2500 amqp@0.1.1\n```\n\nAlso this is the one provided by a fresh install of rabbit.js\n. Great! Thank you :)\n. ",
    "sja": "And ignore node_modules was added in a3685b5e , too.\nMaybe a new pull request would be cool with just the req/rep examples?\n. And ignore node_modules was added in a3685b5e , too.\nMaybe a new pull request would be cool with just the req/rep examples?\n. ",
    "viable-hartman": "Hi squaremo,\n\nWhat was the fix for this bug?  \n\nI get the same error, but the copy call looks identical in the github code and my checked out code.\n. I thought i was, but apparently I was not.  I've checked out again and the\nproblem is resolved.  Thank you for your prompt response!\nOn Apr 9, 2013 6:10 AM, \"Michael Bridgen\" notifications@github.com wrote:\n\n> The fix is in 58bb1cchttps://github.com/squaremo/rabbit.js/commit/58bb1cc62aa4ac98cbea9ffe88f06cf1c79de02bso should be in master. But it won't be in any npm-available releases,\n> since I am a bad person and I've not made any such for some time now.\n> \n> Are you definitely using a git checkout?\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/squaremo/rabbit.js/issues/24#issuecomment-16111386\n> .\n. Hi squaremo,\n\nWhat was the fix for this bug?  \n\nI get the same error, but the copy call looks identical in the github code and my checked out code.\n. I thought i was, but apparently I was not.  I've checked out again and the\nproblem is resolved.  Thank you for your prompt response!\nOn Apr 9, 2013 6:10 AM, \"Michael Bridgen\" notifications@github.com wrote:\n\n> The fix is in 58bb1cchttps://github.com/squaremo/rabbit.js/commit/58bb1cc62aa4ac98cbea9ffe88f06cf1c79de02bso should be in master. But it won't be in any npm-available releases,\n> since I am a bad person and I've not made any such for some time now.\n> \n> Are you definitely using a git checkout?\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/squaremo/rabbit.js/issues/24#issuecomment-16111386\n> .\n. ",
    "lucj": "Hello,\n\nDon't worry :) And first of all thanks for your great work.\n\nI think you are right, for me server2 is the central system that will check to which module (worker) the message needs to be sent to . Server2 will only analyse the incoming message to determine the target but no real processing will be done there (except logging all incoming messages). Once it has forwarded the request, it should wait for the reply and forward the reply to server1.\n\nMultiple module should be able to send messages to server2 and wait for the worker's response. Is a simple REQ/REP the best approach in this case ?\n\nThanks,\nLuc\n. Hi,\n\nThanks for your clarification. I will need to study all of this much deeper but I'm convinced rabbitmq will be the good choice.\nRight now, I'm struggling to design the architecture I will need knowing that each module should be able to:\n- send a request on a given socket and receive a response on this same socket\n- receive a request on another socket it listens to, process it and send a response to the sender\n- send a request that should be broadcasted to all the other module (though rabbitmq (the server2 of my example)).\n\nThanks a lot for your help.\nLuc\n. Hello,\n\nDon't worry :) And first of all thanks for your great work.\n\nI think you are right, for me server2 is the central system that will check to which module (worker) the message needs to be sent to . Server2 will only analyse the incoming message to determine the target but no real processing will be done there (except logging all incoming messages). Once it has forwarded the request, it should wait for the reply and forward the reply to server1.\n\nMultiple module should be able to send messages to server2 and wait for the worker's response. Is a simple REQ/REP the best approach in this case ?\n\nThanks,\nLuc\n. Hi,\n\nThanks for your clarification. I will need to study all of this much deeper but I'm convinced rabbitmq will be the good choice.\nRight now, I'm struggling to design the architecture I will need knowing that each module should be able to:\n- send a request on a given socket and receive a response on this same socket\n- receive a request on another socket it listens to, process it and send a response to the sender\n- send a request that should be broadcasted to all the other module (though rabbitmq (the server2 of my example)).\n\nThanks a lot for your help.\nLuc\n. ",
    "duartemadueno": "Yes, i'm only using rabbit.js. I know why the error is being thrown but would like to handle that specific exception so i can return a custom message to the client.\n\nThe solution was:\n- process.on('uncaughtException', function (err) {});\n\nAnyway, this isn't the best solution, as you might see.\n. Since this is just a small project there's no problem working things this way but, it's true, would really handy if we could handle AMQP errors through rabbit.js\n\nAnother thing, hope you guys don't mind but i changed socket.js file so it can receive a RoutingKey on write. In my case, i don't know which RoutingKeys will be used by the client.\n. Hi @squaremo right now i'm using https://github.com/postwait/node-amqp and that solved my problem. Sorry i can't help you on this particular issue\n. Yes, i'm only using rabbit.js. I know why the error is being thrown but would like to handle that specific exception so i can return a custom message to the client.\n\nThe solution was:\n- process.on('uncaughtException', function (err) {});\n\nAnyway, this isn't the best solution, as you might see.\n. Since this is just a small project there's no problem working things this way but, it's true, would really handy if we could handle AMQP errors through rabbit.js\n\nAnother thing, hope you guys don't mind but i changed socket.js file so it can receive a RoutingKey on write. In my case, i don't know which RoutingKeys will be used by the client.\n. Hi @squaremo right now i'm using https://github.com/postwait/node-amqp and that solved my problem. Sorry i can't help you on this particular issue\n. ",
    "jamuhl": "+1\n. @accelerated thanks.... We actually use our own wrapper over node AMQP right now. Just thought it would be a great idea to have it here (so we could drop ours) - as it's in my opinion only a small addition to rabbit.js not hurting anyone - but i respect your decision don't get me wrong (@squaremo)\n\nSo we stay with our wrapper. To be honest i don't really see the point to use rabbitMQ without the additional options it gives. If i would have been able to just use ZeroMQ i would have used ZeroMQ - but i need the persistent queues.\n. yes https://github.com/adrai/rabbitmq-nodejs-client we just needed the durable sub/pub (with autodelete false) and the ack function - so nothing more there.\n\noh adrai even opened a issue for this long time ago https://github.com/squaremo/rabbit.js/issues/3\n. +1\n. @accelerated thanks.... We actually use our own wrapper over node AMQP right now. Just thought it would be a great idea to have it here (so we could drop ours) - as it's in my opinion only a small addition to rabbit.js not hurting anyone - but i respect your decision don't get me wrong (@squaremo)\n\nSo we stay with our wrapper. To be honest i don't really see the point to use rabbitMQ without the additional options it gives. If i would have been able to just use ZeroMQ i would have used ZeroMQ - but i need the persistent queues.\n. yes https://github.com/adrai/rabbitmq-nodejs-client we just needed the durable sub/pub (with autodelete false) and the ack function - so nothing more there.\n\noh adrai even opened a issue for this long time ago https://github.com/squaremo/rabbit.js/issues/3\n. ",
    "payomdousti": "Yep, that fixed the issue. Thanks!\n. Yep, that fixed the issue. Thanks!\n. ",
    "cooldaemon": "Thank you for your reply.\n\nI tried rabbit.js/example/socketio on Node.js v0.11.0\nHowever, chat message has been replicated by the number of clients connected.\nFirst code example is sending an event to SUB2, but SUB and SUB2 received event.\n\nAll Socket instance, has inherited only one Stream instance.\nSo, I modified Socket.prototype using the util.inherits.\n\nCheers.\n. Thank you for your support!\n. Thank you for your reply.\n\nI tried rabbit.js/example/socketio on Node.js v0.11.0\nHowever, chat message has been replicated by the number of clients connected.\nFirst code example is sending an event to SUB2, but SUB and SUB2 received event.\n\nAll Socket instance, has inherited only one Stream instance.\nSo, I modified Socket.prototype using the util.inherits.\n\nCheers.\n. Thank you for your support!\n. ",
    "dvberkel": "Thank you for rabbit.js\n. Thank you for rabbit.js\n. ",
    "pghalliday": "i'll take a look\n. Indeed this works with master :)\n. looking forward to the next release then :D\n. i'll take a look\n. Indeed this works with master :)\n. looking forward to the next release then :D\n. ",
    "ping86": "I totally agree, now I have a project I need to cancel this sentence \"this.client.connect\" when the user leaves the channel and I cant do.\n. I totally agree, now I have a project I need to cancel this sentence \"this.client.connect\" when the user leaves the channel and I cant do.\n. ",
    "wolfeidau": "Yeah damn good point, thanks for the explanation and even better fix.\n\nCheers\n. OK so firstly I will go over my current usage pattern to ensure i have the model correct.\n\nI open a SINGLE amqp context, then I have a SUB socket which is opened and attatched to a long lived TCP connection from a device, this socket is open waiting for incoming messages to send out the TCP connection, it's lifecycle is attached the to TCP connection.\n\nON the other side I have a publisher which is running within a web server, as requests come in I open and close sockets to push messages to the queue owned by the device being transmitted to.\n\nSo pretty much ONE amqp connection, and PUB sockets being opened and closed based on the requirements of the REST request.\n\nIn the case of publisher and subscriber I noticed that if i list the connections channels they are not being recycled or deleted they just hang around.\n\nI made the change in this push request and in both cases this cleared up the issues.\n\nWhen i went to write a test case I could NOT get the damn channels to go away without a setTimeout it was like they took time to clean up, NOTE I am calling exchange.close() to based on my reading only close the exchange not destroy it on the server.\n\nHopefully this gives you more background on the intent of my change.\n. I did a bunch of testing to try and trigger the failure you outlined.\n\nThere is something really odd going on when you share a connection between two publishers, it seems to me the connection ignores your close on the exchange, but does eventually close it.\n\nOn the channels side, which is the main thing I had issue with, all instances of a socket / queue combination use only the first one created, but still add a new one per request, however no messages seem to traverse it.\n\nI haven't had a chance to dig into the amqp code and discover the cause, but it looks like you have in the past :smile: \n\nThe test I built does implement what you outlined, however it seems to create more questions than it answers.\n\nIf you log the connection through different phases of the test you will see a single exchange does indeed get allocated for the two connections, but ending the first one doesn't close it.\n\nAlso to note is at termination of the test there are still 4 channels open within the connection, all i can think is that the amqp connection must be lazily clearing these up and either:\n1. It is keeping a reference counter for exchanges\n   or \n2. This test is executing too fast to see the diff\n   or \n3. I need to do more real world testing  \n. Aha\n\nYeah that sounds like what i observed, as i mentioned in the admin UI i just saw an ever growing list of channels with on the first one being used.\n\nOne of the concerns I have had with the underlying amqp driver is it is complicated and very difficult to trace.\n\nI am not sure there is much which can be done about it given the size of the code base, my current aim with rabbit.js is to make it as simple as possible to switch in and out messaging protocols. So far this has been pretty easy which is awesome.\n. Yes this is something I have planned to do, I was going to build a small web app which enabled users to login and interact with a queue, publishing and consuming events via a couple of end points, then hit it with a basic client which exercises them. \n\nOnce I have something working I will post it up here.\n\nIt was my objective to use this to understand the cost of channel open close operations, if these are expensive I may need to cache them as you highlighted.\n. OK I built a rather primitive example of using rabbit.js with express.\n\nThis is located at https://github.com/wolfeidau/node-rabbitjs-test \n\nUsing the method described in the readme I can crash the node process on a regular basis.\n\nIn addition to this, as you advised it uses a lot of CPU creating and destroying the channels, that said it doesn't leak, which is awesome.\n\nGoing to add another version which pools the sockets and see how that performs.\n\nNote this is the error I am getting.\n\n```\nevents.js:72\n        throw er; // Unhandled 'error' event\n              ^\nError: CHANNEL_ERROR - unexpected method in connection state running\n    at Connection._onMethod (/Users/markw/Code/Javascript/node-rabbitjs-test/node_modules/rabbit.js/node_modules/amqp/amqp.js:1155:15)\n    at AMQPParser.parser.onMethod (/Users/markw/Code/Javascript/node-rabbitjs-test/node_modules/rabbit.js/node_modules/amqp/amqp.js:897:12)\n    at AMQPParser._parseMethodFrame (/Users/markw/Code/Javascript/node-rabbitjs-test/node_modules/rabbit.js/node_modules/amqp/amqp.js:451:10)\n    at frameEnd (/Users/markw/Code/Javascript/node-rabbitjs-test/node_modules/rabbit.js/node_modules/amqp/amqp.js:192:16)\n    at frame (/Users/markw/Code/Javascript/node-rabbitjs-test/node_modules/rabbit.js/node_modules/amqp/amqp.js:177:14)\n    at AMQPParser.header [as parse] (/Users/markw/Code/Javascript/node-rabbitjs-test/node_modules/rabbit.js/node_modules/amqp/amqp.js:163:14)\n    at AMQPParser.execute (/Users/markw/Code/Javascript/node-rabbitjs-test/node_modules/rabbit.js/node_modules/amqp/amqp.js:236:21)\n    at Connection.<anonymous> (/Users/markw/Code/Javascript/node-rabbitjs-test/node_modules/rabbit.js/node_modules/amqp/amqp.js:935:14)\n    at Connection.EventEmitter.emit (events.js:95:17)\n    at Connection.<anonymous> (_stream_readable.js:736:14)\n    at Connection.EventEmitter.emit (events.js:92:17)\n```\n. Added pooling of the subscriptions in my example and it is pretty much solid as a rock.\n\nAlso CPU usage within the rabbitmq process has dropped from 50% to 4%, so I have come to the conclusion that reuse of subscriptions, and therefore channels, in this case is a must.\n\nI am interested to hear what you think, for the application I am developing I still have the issue that I am using a range of endpoints so pooling as I am doing here is probably not going to work.\n\nI am looking forward to trying out your new AMQP module.\n. Yeah, I have written something a lot smaller and simpler than rabbit.js which I will hopefully share soon.\n\nIntend using routing keys + event streams to limit the number of exchanges and subscriptions within my apps.\n\nWould love some critique on my work once i post it up as my knowledge of rabbitmq is still limited to what I learnt from the RabbitMQ in Action book. \n\nCheers.\n. Yeah I pretty much moved to using your amqplib and wrote a couple of abstractions for what i needed.\n\nStill getting accustomed to promises but all seems to be working fine.\n. This did indeed resolve my issue, that said I think the challenge here is that, and correct me if i am wrong this \"ready\" event is only emitted once, so I pretty much have it around the first test and then assume in the next one that  the context is in a ready state. I am not entirely keen on this, but having had a look at what you have to work with I understand with this limitations.\n\nOriginally I was trying to use the connect callback, but again this only fired on the first test, so the second would time out.\n\nCheers for taking the time to check it over for me, your guidance is much appreciated.\n. Yeah damn good point, thanks for the explanation and even better fix.\n\nCheers\n. OK so firstly I will go over my current usage pattern to ensure i have the model correct.\n\nI open a SINGLE amqp context, then I have a SUB socket which is opened and attatched to a long lived TCP connection from a device, this socket is open waiting for incoming messages to send out the TCP connection, it's lifecycle is attached the to TCP connection.\n\nON the other side I have a publisher which is running within a web server, as requests come in I open and close sockets to push messages to the queue owned by the device being transmitted to.\n\nSo pretty much ONE amqp connection, and PUB sockets being opened and closed based on the requirements of the REST request.\n\nIn the case of publisher and subscriber I noticed that if i list the connections channels they are not being recycled or deleted they just hang around.\n\nI made the change in this push request and in both cases this cleared up the issues.\n\nWhen i went to write a test case I could NOT get the damn channels to go away without a setTimeout it was like they took time to clean up, NOTE I am calling exchange.close() to based on my reading only close the exchange not destroy it on the server.\n\nHopefully this gives you more background on the intent of my change.\n. I did a bunch of testing to try and trigger the failure you outlined.\n\nThere is something really odd going on when you share a connection between two publishers, it seems to me the connection ignores your close on the exchange, but does eventually close it.\n\nOn the channels side, which is the main thing I had issue with, all instances of a socket / queue combination use only the first one created, but still add a new one per request, however no messages seem to traverse it.\n\nI haven't had a chance to dig into the amqp code and discover the cause, but it looks like you have in the past :smile: \n\nThe test I built does implement what you outlined, however it seems to create more questions than it answers.\n\nIf you log the connection through different phases of the test you will see a single exchange does indeed get allocated for the two connections, but ending the first one doesn't close it.\n\nAlso to note is at termination of the test there are still 4 channels open within the connection, all i can think is that the amqp connection must be lazily clearing these up and either:\n1. It is keeping a reference counter for exchanges\n   or \n2. This test is executing too fast to see the diff\n   or \n3. I need to do more real world testing  \n. Aha\n\nYeah that sounds like what i observed, as i mentioned in the admin UI i just saw an ever growing list of channels with on the first one being used.\n\nOne of the concerns I have had with the underlying amqp driver is it is complicated and very difficult to trace.\n\nI am not sure there is much which can be done about it given the size of the code base, my current aim with rabbit.js is to make it as simple as possible to switch in and out messaging protocols. So far this has been pretty easy which is awesome.\n. Yes this is something I have planned to do, I was going to build a small web app which enabled users to login and interact with a queue, publishing and consuming events via a couple of end points, then hit it with a basic client which exercises them. \n\nOnce I have something working I will post it up here.\n\nIt was my objective to use this to understand the cost of channel open close operations, if these are expensive I may need to cache them as you highlighted.\n. OK I built a rather primitive example of using rabbit.js with express.\n\nThis is located at https://github.com/wolfeidau/node-rabbitjs-test \n\nUsing the method described in the readme I can crash the node process on a regular basis.\n\nIn addition to this, as you advised it uses a lot of CPU creating and destroying the channels, that said it doesn't leak, which is awesome.\n\nGoing to add another version which pools the sockets and see how that performs.\n\nNote this is the error I am getting.\n\n```\nevents.js:72\n        throw er; // Unhandled 'error' event\n              ^\nError: CHANNEL_ERROR - unexpected method in connection state running\n    at Connection._onMethod (/Users/markw/Code/Javascript/node-rabbitjs-test/node_modules/rabbit.js/node_modules/amqp/amqp.js:1155:15)\n    at AMQPParser.parser.onMethod (/Users/markw/Code/Javascript/node-rabbitjs-test/node_modules/rabbit.js/node_modules/amqp/amqp.js:897:12)\n    at AMQPParser._parseMethodFrame (/Users/markw/Code/Javascript/node-rabbitjs-test/node_modules/rabbit.js/node_modules/amqp/amqp.js:451:10)\n    at frameEnd (/Users/markw/Code/Javascript/node-rabbitjs-test/node_modules/rabbit.js/node_modules/amqp/amqp.js:192:16)\n    at frame (/Users/markw/Code/Javascript/node-rabbitjs-test/node_modules/rabbit.js/node_modules/amqp/amqp.js:177:14)\n    at AMQPParser.header [as parse] (/Users/markw/Code/Javascript/node-rabbitjs-test/node_modules/rabbit.js/node_modules/amqp/amqp.js:163:14)\n    at AMQPParser.execute (/Users/markw/Code/Javascript/node-rabbitjs-test/node_modules/rabbit.js/node_modules/amqp/amqp.js:236:21)\n    at Connection.<anonymous> (/Users/markw/Code/Javascript/node-rabbitjs-test/node_modules/rabbit.js/node_modules/amqp/amqp.js:935:14)\n    at Connection.EventEmitter.emit (events.js:95:17)\n    at Connection.<anonymous> (_stream_readable.js:736:14)\n    at Connection.EventEmitter.emit (events.js:92:17)\n```\n. Added pooling of the subscriptions in my example and it is pretty much solid as a rock.\n\nAlso CPU usage within the rabbitmq process has dropped from 50% to 4%, so I have come to the conclusion that reuse of subscriptions, and therefore channels, in this case is a must.\n\nI am interested to hear what you think, for the application I am developing I still have the issue that I am using a range of endpoints so pooling as I am doing here is probably not going to work.\n\nI am looking forward to trying out your new AMQP module.\n. Yeah, I have written something a lot smaller and simpler than rabbit.js which I will hopefully share soon.\n\nIntend using routing keys + event streams to limit the number of exchanges and subscriptions within my apps.\n\nWould love some critique on my work once i post it up as my knowledge of rabbitmq is still limited to what I learnt from the RabbitMQ in Action book. \n\nCheers.\n. Yeah I pretty much moved to using your amqplib and wrote a couple of abstractions for what i needed.\n\nStill getting accustomed to promises but all seems to be working fine.\n. This did indeed resolve my issue, that said I think the challenge here is that, and correct me if i am wrong this \"ready\" event is only emitted once, so I pretty much have it around the first test and then assume in the next one that  the context is in a ready state. I am not entirely keen on this, but having had a look at what you have to work with I understand with this limitations.\n\nOriginally I was trying to use the connect callback, but again this only fired on the first test, so the second would time out.\n\nCheers for taking the time to check it over for me, your guidance is much appreciated.\n. ",
    "skeggse": "> Well, for a network socket, one could stop reading from the socket.\n\nThe data would continue to pile up regardless, just in the buffers of the Streams2 API, or perhaps in the network buffers.\n\n> My comment is just observing that there's no reasonable equivalent in AMQP.\n\nSimilarly, you can always just push and the incoming data will pile up in somebody's local buffers. The Streams2 API just makes it so you don't have to worry about it as a producer, whereas before `pause` and `resume` were advisory.\n. > Unbounded buffers anywhere are seldom a good idea ...\n\nVery true. That's why the high-water mark is important.\n\n> In the kernel's buffers, I would very much hope, and at some point this would result in backpressure upstream (for TCP at least).\n\nThat's actually a really interesting idea. I'm gonna go do some research.\n. > > In the kernel's buffers, I would very much hope, and at some point this would result in backpressure upstream (for TCP at least).\n> \n> That's actually a really interesting idea. I'm gonna go do some research.\n\nLooks like backpressure is applied over a tcp stream. So that's good.\n\nhttps://gist.github.com/skeggse/6802812\n. > You could keep a window of correlationIds at the requester, and only yield a message when the head correlationId has a reply. It's a bit like the moving window used for confirmations in amqplib.\n\nThat's exactly where I've ended up for the requester, but how to ensure the responder is orderly...\n\nIt seems like asynchronicity is where Node shines, and while making something like a socket pool would work, it also seems wasteful.\n. > > how to ensure the responder is orderly...\n> \n> It doesn't matter, so long as it puts the right replyTo and correlationId on the response.\n\nWhat I meant by that was that if you include metadata about the message necessary to include the `correlationId` for the response, it ceases to be the same kind of stream. One can no longer pipe it through streams that deal with raw binary data. Similarly, you'd have to `write` your binary data with a `correlationId`, removing the ability to pipe binary streams back as a response.\n\nThat's why I have a problem with my proposed approach: the streams aren't as general-purpose anymore.\n. > Yes, the responders have to process just one request at a time\n\nWhich is the other side of the coin. The purpose of asynchronous RPC is to enable efficiency, but if the responder is limited to a single request at a time, it's not very awesome.\n\nTake, for example, a responder that forwards its data on to a payment processor. Assume that the processor takes 800ms including RTT for the entire transaction, pass or fail. If you need to handle hundreds/second, you'll need to pool--as you said--or create far more responders. Neither of these are ideal. However, if you take advantage of Node's event-loop and asynchronous paradigms, you can handle lots of transactions/second for each socket.\n. True, but I have yet to find a practical limit to Node's context switching and asynchronicity. We've been running in production with Node instances that handle thousands of simultaneous requests/process and virtually no slowdown.\n. My solution is [streamdispatch](http://npmjs.org/package/streamdispatch). Tell the user that, as part of these asynchronous rpc sockets, you must ensure that responses are in-order. Then handle appropriately. If they have asynchronous tasks on the other end, they should use streamdispatch (or something similar)!\n. Made me wonder if it would actually be simpler. It was! A level of abstraction does have its uses! [streamdispatch promises](https://github.com/skeggse/node-streamdispatch/tree/promises).\n. > Well, for a network socket, one could stop reading from the socket.\n\nThe data would continue to pile up regardless, just in the buffers of the Streams2 API, or perhaps in the network buffers.\n\n> My comment is just observing that there's no reasonable equivalent in AMQP.\n\nSimilarly, you can always just push and the incoming data will pile up in somebody's local buffers. The Streams2 API just makes it so you don't have to worry about it as a producer, whereas before `pause` and `resume` were advisory.\n. > Unbounded buffers anywhere are seldom a good idea ...\n\nVery true. That's why the high-water mark is important.\n\n> In the kernel's buffers, I would very much hope, and at some point this would result in backpressure upstream (for TCP at least).\n\nThat's actually a really interesting idea. I'm gonna go do some research.\n. > > In the kernel's buffers, I would very much hope, and at some point this would result in backpressure upstream (for TCP at least).\n> \n> That's actually a really interesting idea. I'm gonna go do some research.\n\nLooks like backpressure is applied over a tcp stream. So that's good.\n\nhttps://gist.github.com/skeggse/6802812\n. > You could keep a window of correlationIds at the requester, and only yield a message when the head correlationId has a reply. It's a bit like the moving window used for confirmations in amqplib.\n\nThat's exactly where I've ended up for the requester, but how to ensure the responder is orderly...\n\nIt seems like asynchronicity is where Node shines, and while making something like a socket pool would work, it also seems wasteful.\n. > > how to ensure the responder is orderly...\n> \n> It doesn't matter, so long as it puts the right replyTo and correlationId on the response.\n\nWhat I meant by that was that if you include metadata about the message necessary to include the `correlationId` for the response, it ceases to be the same kind of stream. One can no longer pipe it through streams that deal with raw binary data. Similarly, you'd have to `write` your binary data with a `correlationId`, removing the ability to pipe binary streams back as a response.\n\nThat's why I have a problem with my proposed approach: the streams aren't as general-purpose anymore.\n. > Yes, the responders have to process just one request at a time\n\nWhich is the other side of the coin. The purpose of asynchronous RPC is to enable efficiency, but if the responder is limited to a single request at a time, it's not very awesome.\n\nTake, for example, a responder that forwards its data on to a payment processor. Assume that the processor takes 800ms including RTT for the entire transaction, pass or fail. If you need to handle hundreds/second, you'll need to pool--as you said--or create far more responders. Neither of these are ideal. However, if you take advantage of Node's event-loop and asynchronous paradigms, you can handle lots of transactions/second for each socket.\n. True, but I have yet to find a practical limit to Node's context switching and asynchronicity. We've been running in production with Node instances that handle thousands of simultaneous requests/process and virtually no slowdown.\n. My solution is [streamdispatch](http://npmjs.org/package/streamdispatch). Tell the user that, as part of these asynchronous rpc sockets, you must ensure that responses are in-order. Then handle appropriately. If they have asynchronous tasks on the other end, they should use streamdispatch (or something similar)!\n. Made me wonder if it would actually be simpler. It was! A level of abstraction does have its uses! [streamdispatch promises](https://github.com/skeggse/node-streamdispatch/tree/promises).\n. ",
    "dehru": "Hello, so in a REQ/REP model, is the correlationId automatically handled for me, or do I need to somehow add it to my message before I send it?\n. Thank you.\n. Hello, so in a REQ/REP model, is the correlationId automatically handled for me, or do I need to somehow add it to my message before I send it?\n. Thank you.\n. ",
    "imboxswedenab": "Might just be best to let the node server crash and have forever restart it then i think.\n\nThanks for you reply squaremo and thank you for a fantastic lib!\n. Might just be best to let the node server crash and have forever restart it then i think.\n\nThanks for you reply squaremo and thank you for a fantastic lib!\n. ",
    "andresberrios": "Great, thanks!\n. Well, for example, we wanted to use one PUB socket to publish messages to different topics according to the current action our application is performing. We were reading the readme in the repo and tried it but later realised that in 0.4.0 it wasn't possible to use them this way. So now we are switching into creating different sockets for the different topics, and closing them after we don't need them anymore. I'm not sure but I think it shouldn't be that bad, since the real connection to the server is actually established when creating the context, right?\nAny advice on this?\n. Another thing is that we need our published messages to be stored in a queue so that they are not lost if the SUB socket hasn't connected yet before the PUB socket starts publishing. For now, we are using the HTTP API of RabbitMQ to create the queue (or make sure it's there) when the application starts. Is there a way we can manage queues and/or bindings from rabbit.js? It's not critical, just want to know haha\nThanks!\n. I see. For now we switched to creating and closing sockets for different topics until there's a new release. Do you think this is going to be a problem in terms of performance or anything like that?\nThanks a lot for the help.\n. I see. Well that would be really awesome :D let me know if you do it, since we would change our code accordingly as soon as possible.\n. Amazing! Thanks a lot!\n. Great, thanks!\n. Well, for example, we wanted to use one PUB socket to publish messages to different topics according to the current action our application is performing. We were reading the readme in the repo and tried it but later realised that in 0.4.0 it wasn't possible to use them this way. So now we are switching into creating different sockets for the different topics, and closing them after we don't need them anymore. I'm not sure but I think it shouldn't be that bad, since the real connection to the server is actually established when creating the context, right?\nAny advice on this?\n. Another thing is that we need our published messages to be stored in a queue so that they are not lost if the SUB socket hasn't connected yet before the PUB socket starts publishing. For now, we are using the HTTP API of RabbitMQ to create the queue (or make sure it's there) when the application starts. Is there a way we can manage queues and/or bindings from rabbit.js? It's not critical, just want to know haha\nThanks!\n. I see. For now we switched to creating and closing sockets for different topics until there's a new release. Do you think this is going to be a problem in terms of performance or anything like that?\nThanks a lot for the help.\n. I see. Well that would be really awesome :D let me know if you do it, since we would change our code accordingly as soon as possible.\n. Amazing! Thanks a lot!\n. ",
    "dstibrany": "I think I've got a solution for this.\n\nWhen the worker is disconnected, I also disconnect the RabbitMQ connection.\n\nThen, in the function where I actually send data, I first check to see if a connection is open, if it is I use it, if it's not I create a new connection, send a message and close right away.\n. I'm trying to understand your response. Are you saying to disconnect the worker when I get a close event on a rabbit.js socket? Is this also assuming that there only exists 1 socket per worker at all times?\n. I did mean 'context', yep.\n\nI think I get what you're saying, but you seem to be referring to situations specific to when a socket has unsent data.\n\nI was referring to a situation where the socket hasn't even been written to, because maybe it's waiting for a DB call.  No matter where I do the cluster disconnect, it seems like new requests could already have come in that are connected to the old worker, and will eventually try to connect to RabbitMQ via the old worker.  There doesn't seem like a good place to close Rabbit without potentially losing messages.\n\nI may be missing something, but I do not see how this can be done without explicitly checking the context connection status before creating and/or pushing to the socket.\n. Yea, exactly. It was a general I/O worry. :)\n\nThanks for the feedback; I think I have a lot to go on.\n. I think I've got a solution for this.\n\nWhen the worker is disconnected, I also disconnect the RabbitMQ connection.\n\nThen, in the function where I actually send data, I first check to see if a connection is open, if it is I use it, if it's not I create a new connection, send a message and close right away.\n. I'm trying to understand your response. Are you saying to disconnect the worker when I get a close event on a rabbit.js socket? Is this also assuming that there only exists 1 socket per worker at all times?\n. I did mean 'context', yep.\n\nI think I get what you're saying, but you seem to be referring to situations specific to when a socket has unsent data.\n\nI was referring to a situation where the socket hasn't even been written to, because maybe it's waiting for a DB call.  No matter where I do the cluster disconnect, it seems like new requests could already have come in that are connected to the old worker, and will eventually try to connect to RabbitMQ via the old worker.  There doesn't seem like a good place to close Rabbit without potentially losing messages.\n\nI may be missing something, but I do not see how this can be done without explicitly checking the context connection status before creating and/or pushing to the socket.\n. Yea, exactly. It was a general I/O worry. :)\n\nThanks for the feedback; I think I have a lot to go on.\n. ",
    "derickbailey": "regarding your previous question on setImmediate: i'm doing that because without it, the message never gets sent. I don't get any errors or anything, it just doesn't send the message. Adding the `setImmediate` call works for now... would love to see the `finish` event available :)\n. I've run in to this problem with promises a lot, as well. the two things i typically do to work around this are:\n\n1) always use a `.then(null, function(err){});` hander as the very last step in the promises. \n\n2) `setImmediate` like i mentioned above.\n\nfor the first point, i find code like this can swallow errors:\n\n``` js\nwhen(foo).then(bar, errorHandler)\n```\n\nif the `bar` method throws an error, then the `errorHandler` method will never be called. Therefore, I always write this as\n\n``` js\nwhen(foo).then(bar).then(null, errorHandler)\n```\n\nthat way, the error handler is the last part and if `bar` throws an error, it gets handled. \n. awesome. will do! :)\n. :+1: \n\njust tried this version on my app, and i was able to get rid of the `setImmediate` in my `context.on(\"ready\", ...` callback. \n. found myself hitting a brick wall today, needing this. imma go ahead and implement it and attach the changes as a PR, for this ticket.\n. i have multiple scenarios with multiple projects that see a need for this, all of which come down to either doing more work on the receiving end, or having a ton of queues names for specific message types. \n\nin one project, i am sending job requests - things that need to be run. i had originally set up the job queues with their own name, like `job:job-a` and `job:job-b`. I was using push / worker for this, because I need jobs that will wait in the queue until something is available to handle them, and I want to `.ack()` the message. \n\nin another project, i have Stripe integration for payment processing. Stripe sends me a webhook for events related to customers, subscriptions, etc. I don't want to process the webhook in the http thread, so I'm sending the messages across my queues. I need a different handler for each of the events that I need to handle. i also need to make sure these events are handled. These need to be persistent messages. i was originally going to do the same as the previous project where I have named queues for each message type and use push / worker for this. i would end up with 5 or 10 names queues for this, to start with... `stripe:invoice.updated`, `stripe:customer.created`, etc.\n\ni just keep coming back to the need for named queues, with persistence, to send messages based on routing.\n\nam i thinking about this wrong? does it matter that i end up with 20 or 30 queues, named after the message type? that seems a bit backward to me... and every time i try to go down this path just one more time, i find myself thinking that the problem i'm facing would be better solved with exchanges and routing slips. \n. in the case of my stripe integration, i will end up doing 1 of 3 things, I think:\n1. using pub/sub and not having a guarantee of the message being handled (no thanks)\n2. using a single queue, and having a switch statement on the other end to determine which code to run for the message type\n3. using a worker queue with routing for the type of event from stripe, sending each event to the correct handler\n\noption 2 seems the most viable at this point, but also seems like it's reproducing in code what should be done at a queue level (option 3) \n. Thanks for the detailed response - the comparison to JMS is especially helpful (spent 2 years in that world, previously) \n\nIn the case of Stripe, it's pretty easy for me to know up-front which queues I need. I only have a few specific webhooks to handle.\n\nBut with the job system... there are hundreds of jobs and dozens of job types that will be built, changed, maintained and updated over the years. It's not something that can be set up front and have hard coded. I could have a configuration file that I read as needed, but this would be an additional process added to the job configuration process - which is already a complex system and prone to human error. It's technically possible, but is very much a usability problem with all the maintenance and work that happens on the jobs. \n\n... what about dead letter queues? i haven't done anything with these in rabbitmq, yet, but i would wonder if an exchange can be configured to catch missed messages in the dead letter queue?\n. i would like to see something along these lines, as well. i'm just now getting to the point where i realize the need for request/response, and having this callback based API would be nice.\n. regarding your previous question on setImmediate: i'm doing that because without it, the message never gets sent. I don't get any errors or anything, it just doesn't send the message. Adding the `setImmediate` call works for now... would love to see the `finish` event available :)\n. I've run in to this problem with promises a lot, as well. the two things i typically do to work around this are:\n\n1) always use a `.then(null, function(err){});` hander as the very last step in the promises. \n\n2) `setImmediate` like i mentioned above.\n\nfor the first point, i find code like this can swallow errors:\n\n``` js\nwhen(foo).then(bar, errorHandler)\n```\n\nif the `bar` method throws an error, then the `errorHandler` method will never be called. Therefore, I always write this as\n\n``` js\nwhen(foo).then(bar).then(null, errorHandler)\n```\n\nthat way, the error handler is the last part and if `bar` throws an error, it gets handled. \n. awesome. will do! :)\n. :+1: \n\njust tried this version on my app, and i was able to get rid of the `setImmediate` in my `context.on(\"ready\", ...` callback. \n. found myself hitting a brick wall today, needing this. imma go ahead and implement it and attach the changes as a PR, for this ticket.\n. i have multiple scenarios with multiple projects that see a need for this, all of which come down to either doing more work on the receiving end, or having a ton of queues names for specific message types. \n\nin one project, i am sending job requests - things that need to be run. i had originally set up the job queues with their own name, like `job:job-a` and `job:job-b`. I was using push / worker for this, because I need jobs that will wait in the queue until something is available to handle them, and I want to `.ack()` the message. \n\nin another project, i have Stripe integration for payment processing. Stripe sends me a webhook for events related to customers, subscriptions, etc. I don't want to process the webhook in the http thread, so I'm sending the messages across my queues. I need a different handler for each of the events that I need to handle. i also need to make sure these events are handled. These need to be persistent messages. i was originally going to do the same as the previous project where I have named queues for each message type and use push / worker for this. i would end up with 5 or 10 names queues for this, to start with... `stripe:invoice.updated`, `stripe:customer.created`, etc.\n\ni just keep coming back to the need for named queues, with persistence, to send messages based on routing.\n\nam i thinking about this wrong? does it matter that i end up with 20 or 30 queues, named after the message type? that seems a bit backward to me... and every time i try to go down this path just one more time, i find myself thinking that the problem i'm facing would be better solved with exchanges and routing slips. \n. in the case of my stripe integration, i will end up doing 1 of 3 things, I think:\n1. using pub/sub and not having a guarantee of the message being handled (no thanks)\n2. using a single queue, and having a switch statement on the other end to determine which code to run for the message type\n3. using a worker queue with routing for the type of event from stripe, sending each event to the correct handler\n\noption 2 seems the most viable at this point, but also seems like it's reproducing in code what should be done at a queue level (option 3) \n. Thanks for the detailed response - the comparison to JMS is especially helpful (spent 2 years in that world, previously) \n\nIn the case of Stripe, it's pretty easy for me to know up-front which queues I need. I only have a few specific webhooks to handle.\n\nBut with the job system... there are hundreds of jobs and dozens of job types that will be built, changed, maintained and updated over the years. It's not something that can be set up front and have hard coded. I could have a configuration file that I read as needed, but this would be an additional process added to the job configuration process - which is already a complex system and prone to human error. It's technically possible, but is very much a usability problem with all the maintenance and work that happens on the jobs. \n\n... what about dead letter queues? i haven't done anything with these in rabbitmq, yet, but i would wonder if an exchange can be configured to catch missed messages in the dead letter queue?\n. i would like to see something along these lines, as well. i'm just now getting to the point where i realize the need for request/response, and having this callback based API would be nice.\n. ",
    "samholmes": "How does one set `this.options.routing`? There is no place where you can set this.\n. Didn't know that. Definitely document this better. I'd also make a suggestion to use examples instead of the #method documentation syntax, IMO. It's clearer this way.\n\nWhy is routing only supported on the instantiation of a socket?\n\nEDIT: I'm getting an error:\n\n```\n[Error: Channel closed by server: 406 (PRECONDITION-FAILED) with message \"PRECONDITION_FAILED - cannot redeclare exchange 'foo' in vhost '/' with different type, durable, internal or autodelete value\"]\n```\n\n``` js\nvar context = require('rabbit.js').createContext('amqp://localhost');\n\ncontext.on('error', function(err){\n    console.log(err);\n})\n\nvar pub = context.socket('PUB', {routing: 'topic'});\n\npub.connect('foo');\n\nsetInterval(function(){\n    pub.publish('one', \"Message from topic 'one' producer!\\n\");\n    pub.publish('two', \"Message from topic 'two' producer!\\n\");\n}, 1000)\n```\n. So, is it not possible to have pub and rep sockets on the same server process? Or is it just not possible to have two pubs with different routing?\n. How do I set the routing key for a SUB socket? There documentation isn't very clear.\n. It might help to see more code.\n\nQuick question. Does sock.connect create a new connection, or does it just modify the existing connection on the sock; does it return the connection? You're example shows you using connect twice. The first time with 'routing.key' as the routing key, and the second time with 'other.routing.key'. Does this change the current connection/socket, closing the connection to the other exchange with the routing key 'routing.key'?\n. Are there any potential leak issues to be aware of?\n. Why wasn't this documented, or was it? haha\n. I just realized immediately after writing this issue what the problem is and what exactly is going on with my code.\n\nThe problem is that the event handlers for both 'error' and 'data' events on the socket are being registered to the socket before any response from the socket. Therefore, when the first response from 'method1' comes back, both of the functions attach to the socket get invoked (and removed because they're attached with the .once EventEmitter method). When the response from 'method2' comes around, there are no event handlers to handle it, thus it passes by silently.\n\nSo the issue is apparent. However, the solution isn't so clear to me yet. It seems like what I would need to do is generate a unique ID for each payload, and then invoke the appropriate callback using this ID. \n\nOr, maybe I should just create a new socket object for every request. My only concern here is with efficiency and memory leaks. Also, would this even solve the issue; wouldn't each new socket get the same 'data' events written to it?\n. That doesn't work because the order doesn't come back the same. If a process on the exchange takes longer than others, then it will come in after other messages. I'll need an ID system.\n\n``` js\n/**\n * Message Queue (RabbitMQ)\n */\n\nglobal.mqContext = require('rabbit.js').createContext('amqp://localhost');\nglobal.mqSockets = { // Cache for all the sockets\n    'backend': 'REQ'\n};\n\nglobal.mqContext.on('ready', function(){\n    for (var key in global.mqSockets)\n    {\n        if (global.mqSockets.hasOwnProperty(key));\n        {\n            if (['REQ', 'SUB'].indexOf(global.mqSockets[key]) !== -1)\n            {\n                var mqsock = global.mqContext.socket(global.mqSockets[key]);\n\n                mqsock.mqCallbacks = [];\n\n                mqsock.connect(key, function(){\n                    global.mqSockets[key] = mqsock;\n                });\n\n                mqsock.on('error', function(err){\n                    var cb = mqsock.mqCallbacks.shift();\n                    cb(err);\n                })\n\n                mqsock.on('data', function(data){\n                    var cb = mqsock.mqCallbacks.shift();\n                    var json = JSON.parse(data);\n\n                    if (json.error)\n                        return cb(json.error);\n\n                    cb(null, json);\n                });\n            }\n        }\n    }\n})\n\nglobal.MQ = function(queue){\n    var mqsock = global.mqSockets[queue];\n\n    function rtnFun(method, info, cb){\n        if (typeof info === 'function')\n        {\n            var cb = info;\n            var info = {};\n        }\n\n        mqsock.mqCallbacks.push(cb);\n\n        var payload = JSON.stringify({\n                method: method,\n                info: info\n            });\n\n        mqsock.write(payload, 'utf8');\n\n        return rtnFun;\n    };\n\n    return rtnFun;\n}\n```\n\nThat's my implementation. When I do something like this:\n\n``` js\nmqBackend('exchange.getPriceHighByDate', wait(getPriceHighByDateHandler));\nmqBackend('ads.getAdViewsByDate', wait(getAdViewsByDateHandle));\nmqBackend('banks.getCreditsCreatedByDate', wait(getCreditsCreatedByDateHandler));\nmqBackend('banks.getCreditsConsumedByDate', wait(getCreditsConsumedByDateHandler));\n```\n\nThe second one triggers after all of them. This causes it to get the wrong handler, `wait(getCreditsConsumedByDateHandler)`, instead of the right handler, `wait(getAdViewsByDateHandle)`.\n. I went with a ID system.\n\n``` js\n/**\n * Message Queue (RabbitMQ)\n */\n\nglobal.mqContext = require('rabbit.js').createContext('amqp://localhost');\nglobal.mqSockets = { // Cache for all the sockets\n    'backend': 'REQ'\n};\n\nglobal.mqContext.on('ready', function(){\n    for (var key in global.mqSockets)\n    {\n        if (global.mqSockets.hasOwnProperty(key));\n        {\n            if (['REQ', 'SUB'].indexOf(global.mqSockets[key]) !== -1)\n            {\n                var mqsock = global.mqContext.socket(global.mqSockets[key]);\n\n                mqsock.mqCallbacks = {};\n\n                mqsock.connect(key, function(){\n                    global.mqSockets[key] = mqsock;\n                });\n\n                mqsock.on('data', function(payload){\n                    var json = JSON.parse(payload);\n\n                    var cb = mqsock.mqCallbacks[json.callbackId];\n\n                    if (typeof cb !== 'function') return;\n\n                    delete mqsock.mqCallbacks[json.callbackId];\n\n                    if (json.error)\n                        return cb(json.error);\n\n                    cb(null, json.info);\n                });\n            }\n        }\n    }\n})\n\nglobal.MQ = function(queue){\n    var mqsock = global.mqSockets[queue];\n\n    function rtnFun(method, info, cb){\n        if (typeof info === 'function')\n        {\n            var cb = info;\n            var info = {};\n        }\n\n        var callbackId = Math.random();\n\n        mqsock.mqCallbacks[callbackId] = cb;\n\n        var payload = JSON.stringify({\n                callbackId: callbackId,\n                method: method,\n                info: info\n            });\n\n        mqsock.write(payload, 'utf8');\n\n        return rtnFun;\n    };\n\n    return rtnFun;\n}\n```\n. This is working fine for me. I'm not sure how it fits with the style of rabbit.js.\n. One thing that needs modifying with my code is that it'll fail if I try to use the MQ function before the socket connects. I might modify this into a module that will queue up method invocations in that case.\n. How does one set `this.options.routing`? There is no place where you can set this.\n. Didn't know that. Definitely document this better. I'd also make a suggestion to use examples instead of the #method documentation syntax, IMO. It's clearer this way.\n\nWhy is routing only supported on the instantiation of a socket?\n\nEDIT: I'm getting an error:\n\n```\n[Error: Channel closed by server: 406 (PRECONDITION-FAILED) with message \"PRECONDITION_FAILED - cannot redeclare exchange 'foo' in vhost '/' with different type, durable, internal or autodelete value\"]\n```\n\n``` js\nvar context = require('rabbit.js').createContext('amqp://localhost');\n\ncontext.on('error', function(err){\n    console.log(err);\n})\n\nvar pub = context.socket('PUB', {routing: 'topic'});\n\npub.connect('foo');\n\nsetInterval(function(){\n    pub.publish('one', \"Message from topic 'one' producer!\\n\");\n    pub.publish('two', \"Message from topic 'two' producer!\\n\");\n}, 1000)\n```\n. So, is it not possible to have pub and rep sockets on the same server process? Or is it just not possible to have two pubs with different routing?\n. How do I set the routing key for a SUB socket? There documentation isn't very clear.\n. It might help to see more code.\n\nQuick question. Does sock.connect create a new connection, or does it just modify the existing connection on the sock; does it return the connection? You're example shows you using connect twice. The first time with 'routing.key' as the routing key, and the second time with 'other.routing.key'. Does this change the current connection/socket, closing the connection to the other exchange with the routing key 'routing.key'?\n. Are there any potential leak issues to be aware of?\n. Why wasn't this documented, or was it? haha\n. I just realized immediately after writing this issue what the problem is and what exactly is going on with my code.\n\nThe problem is that the event handlers for both 'error' and 'data' events on the socket are being registered to the socket before any response from the socket. Therefore, when the first response from 'method1' comes back, both of the functions attach to the socket get invoked (and removed because they're attached with the .once EventEmitter method). When the response from 'method2' comes around, there are no event handlers to handle it, thus it passes by silently.\n\nSo the issue is apparent. However, the solution isn't so clear to me yet. It seems like what I would need to do is generate a unique ID for each payload, and then invoke the appropriate callback using this ID. \n\nOr, maybe I should just create a new socket object for every request. My only concern here is with efficiency and memory leaks. Also, would this even solve the issue; wouldn't each new socket get the same 'data' events written to it?\n. That doesn't work because the order doesn't come back the same. If a process on the exchange takes longer than others, then it will come in after other messages. I'll need an ID system.\n\n``` js\n/**\n * Message Queue (RabbitMQ)\n */\n\nglobal.mqContext = require('rabbit.js').createContext('amqp://localhost');\nglobal.mqSockets = { // Cache for all the sockets\n    'backend': 'REQ'\n};\n\nglobal.mqContext.on('ready', function(){\n    for (var key in global.mqSockets)\n    {\n        if (global.mqSockets.hasOwnProperty(key));\n        {\n            if (['REQ', 'SUB'].indexOf(global.mqSockets[key]) !== -1)\n            {\n                var mqsock = global.mqContext.socket(global.mqSockets[key]);\n\n                mqsock.mqCallbacks = [];\n\n                mqsock.connect(key, function(){\n                    global.mqSockets[key] = mqsock;\n                });\n\n                mqsock.on('error', function(err){\n                    var cb = mqsock.mqCallbacks.shift();\n                    cb(err);\n                })\n\n                mqsock.on('data', function(data){\n                    var cb = mqsock.mqCallbacks.shift();\n                    var json = JSON.parse(data);\n\n                    if (json.error)\n                        return cb(json.error);\n\n                    cb(null, json);\n                });\n            }\n        }\n    }\n})\n\nglobal.MQ = function(queue){\n    var mqsock = global.mqSockets[queue];\n\n    function rtnFun(method, info, cb){\n        if (typeof info === 'function')\n        {\n            var cb = info;\n            var info = {};\n        }\n\n        mqsock.mqCallbacks.push(cb);\n\n        var payload = JSON.stringify({\n                method: method,\n                info: info\n            });\n\n        mqsock.write(payload, 'utf8');\n\n        return rtnFun;\n    };\n\n    return rtnFun;\n}\n```\n\nThat's my implementation. When I do something like this:\n\n``` js\nmqBackend('exchange.getPriceHighByDate', wait(getPriceHighByDateHandler));\nmqBackend('ads.getAdViewsByDate', wait(getAdViewsByDateHandle));\nmqBackend('banks.getCreditsCreatedByDate', wait(getCreditsCreatedByDateHandler));\nmqBackend('banks.getCreditsConsumedByDate', wait(getCreditsConsumedByDateHandler));\n```\n\nThe second one triggers after all of them. This causes it to get the wrong handler, `wait(getCreditsConsumedByDateHandler)`, instead of the right handler, `wait(getAdViewsByDateHandle)`.\n. I went with a ID system.\n\n``` js\n/**\n * Message Queue (RabbitMQ)\n */\n\nglobal.mqContext = require('rabbit.js').createContext('amqp://localhost');\nglobal.mqSockets = { // Cache for all the sockets\n    'backend': 'REQ'\n};\n\nglobal.mqContext.on('ready', function(){\n    for (var key in global.mqSockets)\n    {\n        if (global.mqSockets.hasOwnProperty(key));\n        {\n            if (['REQ', 'SUB'].indexOf(global.mqSockets[key]) !== -1)\n            {\n                var mqsock = global.mqContext.socket(global.mqSockets[key]);\n\n                mqsock.mqCallbacks = {};\n\n                mqsock.connect(key, function(){\n                    global.mqSockets[key] = mqsock;\n                });\n\n                mqsock.on('data', function(payload){\n                    var json = JSON.parse(payload);\n\n                    var cb = mqsock.mqCallbacks[json.callbackId];\n\n                    if (typeof cb !== 'function') return;\n\n                    delete mqsock.mqCallbacks[json.callbackId];\n\n                    if (json.error)\n                        return cb(json.error);\n\n                    cb(null, json.info);\n                });\n            }\n        }\n    }\n})\n\nglobal.MQ = function(queue){\n    var mqsock = global.mqSockets[queue];\n\n    function rtnFun(method, info, cb){\n        if (typeof info === 'function')\n        {\n            var cb = info;\n            var info = {};\n        }\n\n        var callbackId = Math.random();\n\n        mqsock.mqCallbacks[callbackId] = cb;\n\n        var payload = JSON.stringify({\n                callbackId: callbackId,\n                method: method,\n                info: info\n            });\n\n        mqsock.write(payload, 'utf8');\n\n        return rtnFun;\n    };\n\n    return rtnFun;\n}\n```\n. This is working fine for me. I'm not sure how it fits with the style of rabbit.js.\n. One thing that needs modifying with my code is that it'll fail if I try to use the MQ function before the socket connects. I might modify this into a module that will queue up method invocations in that case.\n. ",
    "IanVaughan": "In https://github.com/squaremo/rabbit.js/issues/59#issuecomment-44533354, you suggest that you can create one socket, and handle different routing keys separately :\n\n``` js\nvar sock = ctx.socket('SUBSCRIBE', {routing: 'topic'});\nsock.connect('address', 'routing.key');\nsock.connect('address', 'other.routing.key');\n```\n\nBut this is not working for me, a message sent with one routing key is showing up in both subscribers :\n\n``` coffee\n    sub = rabbit.socket('SUBSCRIBE', {routing: 'topic'})\n\n    sub.connect 'events', 'rt.job', ->\n      winston.log('info', 'rabbit connected on rt.job')\n      sub.on 'data', (data) -> \n        winston.log('info', 'rt.job : ' + data)\n\n    sub.connect 'events', 'rt.user', ->\n      winston.log('info', 'rabbit connected on rt.user')\n      sub.on 'data', (data) -> \n        winston.log('info', 'rt.user : ' + data)\n```\n\nAnd you can see why, the two queues are the same :\n![screen shot 2015-05-13 at 22 14 05](https://cloud.githubusercontent.com/assets/158600/7621279/80cc7d92-f9bd-11e4-8d7a-9ae43100e41d.png)\n\nWhereas creating two sockets does work, but this does not seem right nor match your example?\n\n``` coffee\n  rabbit.on 'ready', ->\n    winston.log('info', 'rabbit ready on : ' + config.rabbit.host)\n    sub = rabbit.socket('SUBSCRIBE', {routing: 'topic'})\n    sub2 = rabbit.socket('SUBSCRIBE', {routing: 'topic'})\n\n    sub.connect 'events', 'rt.job', ->\n      winston.log('info', 'rabbit connected on rt.job')\n      sub.on 'data', (data) -> \n        winston.log('info', 'rt.job : ' + data)\n\n    sub2.connect 'events', 'rt.user', ->\n      winston.log('info', 'rabbit connected on rt.user')\n      sub2.on 'data', (data) -> \n        winston.log('info', 'rt.user : ' + data)\n```\n\nCan see the two queues nicely :\n![screen shot 2015-05-13 at 22 14 43](https://cloud.githubusercontent.com/assets/158600/7621278/80c9a342-f9bd-11e4-83fb-afd0760521e2.png)\n. In https://github.com/squaremo/rabbit.js/issues/59#issuecomment-44533354, you suggest that you can create one socket, and handle different routing keys separately :\n\n``` js\nvar sock = ctx.socket('SUBSCRIBE', {routing: 'topic'});\nsock.connect('address', 'routing.key');\nsock.connect('address', 'other.routing.key');\n```\n\nBut this is not working for me, a message sent with one routing key is showing up in both subscribers :\n\n``` coffee\n    sub = rabbit.socket('SUBSCRIBE', {routing: 'topic'})\n\n    sub.connect 'events', 'rt.job', ->\n      winston.log('info', 'rabbit connected on rt.job')\n      sub.on 'data', (data) -> \n        winston.log('info', 'rt.job : ' + data)\n\n    sub.connect 'events', 'rt.user', ->\n      winston.log('info', 'rabbit connected on rt.user')\n      sub.on 'data', (data) -> \n        winston.log('info', 'rt.user : ' + data)\n```\n\nAnd you can see why, the two queues are the same :\n![screen shot 2015-05-13 at 22 14 05](https://cloud.githubusercontent.com/assets/158600/7621279/80cc7d92-f9bd-11e4-8d7a-9ae43100e41d.png)\n\nWhereas creating two sockets does work, but this does not seem right nor match your example?\n\n``` coffee\n  rabbit.on 'ready', ->\n    winston.log('info', 'rabbit ready on : ' + config.rabbit.host)\n    sub = rabbit.socket('SUBSCRIBE', {routing: 'topic'})\n    sub2 = rabbit.socket('SUBSCRIBE', {routing: 'topic'})\n\n    sub.connect 'events', 'rt.job', ->\n      winston.log('info', 'rabbit connected on rt.job')\n      sub.on 'data', (data) -> \n        winston.log('info', 'rt.job : ' + data)\n\n    sub2.connect 'events', 'rt.user', ->\n      winston.log('info', 'rabbit connected on rt.user')\n      sub2.on 'data', (data) -> \n        winston.log('info', 'rt.user : ' + data)\n```\n\nCan see the two queues nicely :\n![screen shot 2015-05-13 at 22 14 43](https://cloud.githubusercontent.com/assets/158600/7621278/80c9a342-f9bd-11e4-83fb-afd0760521e2.png)\n. ",
    "carolineBda": "Yes you are right I can see the exception. That was because I was using it inside a mocha test which froze.\nThanks for your help \n. Yes you are right I can see the exception. That was because I was using it inside a mocha test which froze.\nThanks for your help \n. ",
    "stephenmuss": "@squaremo it's because the consumers are actually different applications/services.\n. I thought it might make sense to appear in `connect` but noticed that in other places you allowed for the `persistent` option in the constructor. I guess it's really a matter of whether you conform to the current method of passing an option in the constructor or whether it makes more sense to allow for a durable flag in `connect`.\n. Does it break backwards compatibility? From what I can tell it should be backwards compatible.\n\nI'm happy to make any changes you see fit. This was more to get your thoughts.\n. Fair enough. I hadn't considered that people may already be passing `persistent` through as an option to the `PubSocket` and `SubSocket` constructors.\n. @squaremo it's because the consumers are actually different applications/services.\n. I thought it might make sense to appear in `connect` but noticed that in other places you allowed for the `persistent` option in the constructor. I guess it's really a matter of whether you conform to the current method of passing an option in the constructor or whether it makes more sense to allow for a durable flag in `connect`.\n. Does it break backwards compatibility? From what I can tell it should be backwards compatible.\n\nI'm happy to make any changes you see fit. This was more to get your thoughts.\n. Fair enough. I hadn't considered that people may already be passing `persistent` through as an option to the `PubSocket` and `SubSocket` constructors.\n. ",
    "mathiaske": "+1\nFor the current version of our application we will simply leave everything 'transient' \nbut this could be of interest in the future.\n\nTo my knowledge connecting/re-asserting an exchange with different properties than the already existing one fails (in this case durable and transient). \nIs it possible to read out the properties of an exchange before connecting to it?\nWouldn't this allow some kind of auto-fix/step-up-down option/function or even auto-configuration of the socket, or is my imagination simply going wild again ..\n. +1\nFor the current version of our application we will simply leave everything 'transient' \nbut this could be of interest in the future.\n\nTo my knowledge connecting/re-asserting an exchange with different properties than the already existing one fails (in this case durable and transient). \nIs it possible to read out the properties of an exchange before connecting to it?\nWouldn't this allow some kind of auto-fix/step-up-down option/function or even auto-configuration of the socket, or is my imagination simply going wild again ..\n. ",
    "disintegrator": "+1\nI am currently trying to work around this but I would definitely appreciate control over the exchange's properties.\n. +1\nI am currently trying to work around this but I would definitely appreciate control over the exchange's properties.\n. ",
    "JSteunou": "+1\nCurrently using default \"amq.topic\" exchange from RabbitMQ which is durable.\n. +1\nCurrently using default \"amq.topic\" exchange from RabbitMQ which is durable.\n. ",
    "JoelBennett": "+1 here as well.\n. +1 here as well.\n. ",
    "mikeatlas": "Thanks! This works for me with `\"rabbit.js\": \"^0.4.4\",` \n\n`context.socket('PUB', {noCreate: true});` for a durable exchange that already existed on my server:\n\n![Publishing to an existing durable exchange screenshot](http://i.imgur.com/vvJqpD4.png)\n. Thanks! This works for me with `\"rabbit.js\": \"^0.4.4\",` \n\n`context.socket('PUB', {noCreate: true});` for a durable exchange that already existed on my server:\n\n![Publishing to an existing durable exchange screenshot](http://i.imgur.com/vvJqpD4.png)\n. ",
    "jcrugzz": "This seems sound to me. A major version bump should be reasonable for this change as it would not affect anyone using it in this way. Semver to the rescue :). Thoughts @squaremo ?\n. This seems reasonable :+1: \n. this looks super useful! @squaremo would appreciate a look when you have the chance :)\n. @squaremo ill take a shot at it as I'm using this lib with a project. I just ran the tests and it just had a bunch of logs on an unhandled promise reject but the tests still mostly passed. \n. This seems sound to me. A major version bump should be reasonable for this change as it would not affect anyone using it in this way. Semver to the rescue :). Thoughts @squaremo ?\n. This seems reasonable :+1: \n. this looks super useful! @squaremo would appreciate a look when you have the chance :)\n. @squaremo ill take a shot at it as I'm using this lib with a project. I just ran the tests and it just had a bunch of logs on an unhandled promise reject but the tests still mostly passed. \n. ",
    "dev-tim": "Hello! Do we have any updates on this PR? Is there some changes needed to make it possible to configure durable queue?\n. Update with durable queue issue. You can't just make queue durable in this case. \nThe reason behind it is that in PubSocket you are not supposed to manage queues at all, here you should use topics instead. \n\nMoreover in rabbit.js while creating SUB socket amqp.node's _assertQueue_ method is used which creates new random queue on each subscribe: \n\n```\nch.assertQueue('', {\n      exclusive: true, autoDelete: true\n  })\n```\n\nwhich can be potentially a leak if you make you queues durable.\n\nSo if you want to have durable queues you also want to specify queue name explicitly, event if it breaks concept PUB/SUB managed by topic. \n\nI'll propose another PR. \n. Hello! Do we have any updates on this PR? Is there some changes needed to make it possible to configure durable queue?\n. Update with durable queue issue. You can't just make queue durable in this case. \nThe reason behind it is that in PubSocket you are not supposed to manage queues at all, here you should use topics instead. \n\nMoreover in rabbit.js while creating SUB socket amqp.node's _assertQueue_ method is used which creates new random queue on each subscribe: \n\n```\nch.assertQueue('', {\n      exclusive: true, autoDelete: true\n  })\n```\n\nwhich can be potentially a leak if you make you queues durable.\n\nSo if you want to have durable queues you also want to specify queue name explicitly, event if it breaks concept PUB/SUB managed by topic. \n\nI'll propose another PR. \n. ",
    "johanzander": "We have an existing AMQP exchange with python/Java, etc bindings that is setup with durable=True.\nI want to interface this using rabbit.js / PubSocket but cannot since durable is hardcoded to False.\nI found the following to fix my problem: \nIn SubSocket.prototype.connect, change:\n  ch.assertExchange(source,\n                    this.options.routing || 'fanout',\n                    {durable: false, autoDelete: false})\nto \n  ch.assertExchange(source,\n                    this.options.routing || 'fanout',\n                    {durable: this.options.persistent, autoDelete: false})\nShould I create a pull request or will this have any negative side-effects?\n. We have an existing AMQP exchange with python/Java, etc bindings that is setup with durable=True.\nI want to interface this using rabbit.js / PubSocket but cannot since durable is hardcoded to False.\nI found the following to fix my problem: \nIn SubSocket.prototype.connect, change:\n  ch.assertExchange(source,\n                    this.options.routing || 'fanout',\n                    {durable: false, autoDelete: false})\nto \n  ch.assertExchange(source,\n                    this.options.routing || 'fanout',\n                    {durable: this.options.persistent, autoDelete: false})\nShould I create a pull request or will this have any negative side-effects?\n. ",
    "jcollum": "I may be running into this as well: \n\n```\nError: Channel closed by server: 406 (PRECONDITION-FAILED) with message \"PRECONDITION_FAILED - inequivalent arg 'durable' for exchange 'PullData.Response' in vhost '/': received 'false' but current is 'true'\"\n  at Channel.C.accept (/Users/collumj/work/alpha/node_modules/rabbit.js/node_modules/amqplib/lib/channel.js:398:24)\n\n```\n. I agree, I'm trying to connect to an existing RabbitMQ server and getting a 403: \n\n```\nError: Handshake terminated by server: 403 (ACCESS-REFUSED) with message \"ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.\"\n```\n\nI'm not seeing anything in this repo that would allow me to authenticate a connection. I'll look at amqp.node. \n. That did it thanks \n. Funny, I'm back to this question; if I want to add a header like \"message_id\" (with a GUID value) should I upgrade to the AMQP.Node library? I'll give it a shot... \n. Yes, I think you're right; a coworker pointed that out shortly after I posted. I should've asked around before posting this. I'll close it. \n\nI do think that the examples would be better if they gave some sort of message about checking if RabbitMQ is running.  \n. Yes that makes more sense. Maybe you could add that a sample of working\nwith REP/WORKER sockets can be found in the test directory. As it is, I was\nexpecting the ordering example to be a REP/WORKER sample.\n\nOn Wed, Jun 17, 2015 at 11:29 AM Michael Bridgen notifications@github.com\nwrote:\n\n> That sentence is supposed to indicate that the technique in the ordering\n> example can also be used for worker sockets. Would it be better to say,\n> \n> It is important to acknowledge messages in the right order. A way to\n> maintain ordering for REP sockets, which can be adapted to WORKER sockets,\n> is shown in the \"ordering\" example.\n> \n> (Is it reasonably clear how to adapt the example, do you think?)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/squaremo/rabbit.js/issues/102#issuecomment-112905422.\n. :+1:\n\nCurrent warning is a bit different \n\n`npm WARN engine rabbit.js@0.4.4: wanted: {\"node\":\">=0.8 <0.13 || ^1\"} (current: {\"node\":\"4.0.0\",\"npm\":\"2.14.2\"})` \n. Hey @squaremo did you know that `amqplib` got a version bump around Sep 16th? \n. I may be running into this as well: \n\n```\nError: Channel closed by server: 406 (PRECONDITION-FAILED) with message \"PRECONDITION_FAILED - inequivalent arg 'durable' for exchange 'PullData.Response' in vhost '/': received 'false' but current is 'true'\"\n  at Channel.C.accept (/Users/collumj/work/alpha/node_modules/rabbit.js/node_modules/amqplib/lib/channel.js:398:24)\n\n```\n. I agree, I'm trying to connect to an existing RabbitMQ server and getting a 403: \n\n```\nError: Handshake terminated by server: 403 (ACCESS-REFUSED) with message \"ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.\"\n```\n\nI'm not seeing anything in this repo that would allow me to authenticate a connection. I'll look at amqp.node. \n. That did it thanks \n. Funny, I'm back to this question; if I want to add a header like \"message_id\" (with a GUID value) should I upgrade to the AMQP.Node library? I'll give it a shot... \n. Yes, I think you're right; a coworker pointed that out shortly after I posted. I should've asked around before posting this. I'll close it. \n\nI do think that the examples would be better if they gave some sort of message about checking if RabbitMQ is running.  \n. Yes that makes more sense. Maybe you could add that a sample of working\nwith REP/WORKER sockets can be found in the test directory. As it is, I was\nexpecting the ordering example to be a REP/WORKER sample.\n\nOn Wed, Jun 17, 2015 at 11:29 AM Michael Bridgen notifications@github.com\nwrote:\n\n> That sentence is supposed to indicate that the technique in the ordering\n> example can also be used for worker sockets. Would it be better to say,\n> \n> It is important to acknowledge messages in the right order. A way to\n> maintain ordering for REP sockets, which can be adapted to WORKER sockets,\n> is shown in the \"ordering\" example.\n> \n> (Is it reasonably clear how to adapt the example, do you think?)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/squaremo/rabbit.js/issues/102#issuecomment-112905422.\n. :+1:\n\nCurrent warning is a bit different \n\n`npm WARN engine rabbit.js@0.4.4: wanted: {\"node\":\">=0.8 <0.13 || ^1\"} (current: {\"node\":\"4.0.0\",\"npm\":\"2.14.2\"})` \n. Hey @squaremo did you know that `amqplib` got a version bump around Sep 16th? \n. ",
    "christophermina": "Hi @squaremo, It's not so much to reject a message, but in case the system is too busy to process that particular request, it may want the request to be delivered again later, or to a different machine / process that is also connected to the message queue.  Without a way to ignore a message, the machine that receives it MUST process that message and has no ability to defer processing for later, or as mentioned, to another machine. \n. Thanks for the comment. I was under the impression that if the broker did not receive an acknowledgement within a period of time, it would automatically requeue the message, hence the notion of simply ignoring it.  \n\nThe purpose of my request is indeed to requeue the message.  It looks like RabbitMQ support #nack, with the option to requeue or not.  Maybe this would be the appropriate way to handle this?  (see: http://www.rabbitmq.com/nack.html)\n. Thanks a lot for the information.  I didn't realize this, obviously.  I'll check it out. \n. @squaremo , please see Pull request mentioned above, regarding this issue. \n. I'm not very familiar with the AMQP library, so semantically, it\nlooked like the most appropriate usage. However, if nack() with requeue =\nfalse is more appropriate, I can make the change.\n\nOn Tuesday, August 12, 2014, Michael Bridgen notifications@github.com\nwrote:\n\n> Just from the description above: how come requeue uses nack and discard\n> uses reject? The difference is admittedly slim, since nack has been\n> supported in RabbitMQ for a long time.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/squaremo/rabbit.js/pull/69#issuecomment-51992919.\n\n## \n\nChristopher Mina\n\nLive Healthy. Work Hard. Be Kind. Gain Success.\n. I've modified the code to use nack instead of reject, per comment above. \n. Hi @squaremo, It's not so much to reject a message, but in case the system is too busy to process that particular request, it may want the request to be delivered again later, or to a different machine / process that is also connected to the message queue.  Without a way to ignore a message, the machine that receives it MUST process that message and has no ability to defer processing for later, or as mentioned, to another machine. \n. Thanks for the comment. I was under the impression that if the broker did not receive an acknowledgement within a period of time, it would automatically requeue the message, hence the notion of simply ignoring it.  \n\nThe purpose of my request is indeed to requeue the message.  It looks like RabbitMQ support #nack, with the option to requeue or not.  Maybe this would be the appropriate way to handle this?  (see: http://www.rabbitmq.com/nack.html)\n. Thanks a lot for the information.  I didn't realize this, obviously.  I'll check it out. \n. @squaremo , please see Pull request mentioned above, regarding this issue. \n. I'm not very familiar with the AMQP library, so semantically, it\nlooked like the most appropriate usage. However, if nack() with requeue =\nfalse is more appropriate, I can make the change.\n\nOn Tuesday, August 12, 2014, Michael Bridgen notifications@github.com\nwrote:\n\n> Just from the description above: how come requeue uses nack and discard\n> uses reject? The difference is admittedly slim, since nack has been\n> supported in RabbitMQ for a long time.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/squaremo/rabbit.js/pull/69#issuecomment-51992919.\n\n## \n\nChristopher Mina\n\nLive Healthy. Work Hard. Be Kind. Gain Success.\n. I've modified the code to use nack instead of reject, per comment above. \n. ",
    "jan-molak": "Hey @squaremo and thanks for your awesome work on rabbit.js!\n\n+1 on this pull request. It would be very useful to have both 'blocked' and 'unblocked' events emitted in order to add some more intelligent error handling in our applications. Thanks!\n\nJ\n. Hey @squaremo and thanks for your awesome work on rabbit.js!\n\n+1 on this pull request. It would be very useful to have both 'blocked' and 'unblocked' events emitted in order to add some more intelligent error handling in our applications. Thanks!\n\nJ\n. ",
    "awashbrook": "+1 we really need this pull request for a very important application we are developing...\n. We've been using this change for two weeks and are fine. So we would love to see new release.\n. +1 we really need this pull request for a very important application we are developing...\n. We've been using this change for two weeks and are fine. So we would love to see new release.\n. ",
    "zachgoldstein": "+1 for this over here too. Rabbitmq blocking connections has caused a major outage for us in the past. We need to have checks to deal with this better when the backpressure isn't quite enough.\n. Yup, replies as well. Ideally like control over the exchange via the connect for all sockets really. It's not clear to me what socket.connect does off the bat. It either creates a queue or an exchange based on the socket type. Best would be an options object I could pass in there with queueName, exchangeName, etc.\n\nThis is what I've done to patch it for now:\n\n```\nself.dummySoc.constructor.prototype.connect = function(queueName, exchangeName, callback) {\n    var socket = this;\n    var ch = this.ch;\n\n    if (this.consumers[queueName]) {\n        delay(callback);\n        return;\n    }\n    ch.assertExchange(exchangeName, \n        socket.options.routing || 'direct',\n        { durable: socket.options.persistent }\n    ).then(function() {\n        return ch.assertQueue(queueName, {\n            durable: socket.options.persistent\n        });\n    }).then(function(ok) {\n        return ch.consume(queueName, function(msg) {\n            if (msg !== null) {\n                socket.requests.push(msg);\n                socket.push(msg.content);\n            } else socket.push(null);\n        }, {\n            noAck: false\n        }).then(function(ok) {\n            socket.consumers[queueName] = ok.consumerTag;\n        });\n    }).then(callback || ignore);\n};\n\n```\n. Basically if we've got a variety of different services, each one doing rpc, we can do something like namespacing via the exchanges and not have to worry about something going really wrong and binding to another service's queues.\nPoint 1. makes total sense.\nBit confused about point 2. It still goes to the default exchange, what about that exchange prevents those problems?\n. Also, setting a different exchange than the default allows us to set durability and auto-delete and all that. Those choices can be a bit application-specific and having access to that flexibility via the socket abstraction would be cool.\n. Just to follow up, I went with the defaults you've laid out here in rabbit.js instead of all this patching to get exchanges. We're going to put the basic namespacing we want in our queue names. Those complications are pretty good reasons for us not to bother with the patching. So thanks for that response, much appreciated.\n\nI don't think that sticking the reply queues on a specific exchange is really that important. Putting the request queues on a topic exchange with a routing key scheme that matches something like \"service.subsystem.method\" would still be nice though. I'm not 100% clear what you meant by, \"Having routing in the middle breaks the REQ/REP contract\". If it's just requests on another exchange, I don't think that would break it right? The point is just to have a direct exchange for the message going back from the server node. Even if it wasn't the default exchange, as long as the reply_to queue is on a direct exchange it's no biggie right?\n. +1 for this over here too. Rabbitmq blocking connections has caused a major outage for us in the past. We need to have checks to deal with this better when the backpressure isn't quite enough.\n. Yup, replies as well. Ideally like control over the exchange via the connect for all sockets really. It's not clear to me what socket.connect does off the bat. It either creates a queue or an exchange based on the socket type. Best would be an options object I could pass in there with queueName, exchangeName, etc.\n\nThis is what I've done to patch it for now:\n\n```\nself.dummySoc.constructor.prototype.connect = function(queueName, exchangeName, callback) {\n    var socket = this;\n    var ch = this.ch;\n\n    if (this.consumers[queueName]) {\n        delay(callback);\n        return;\n    }\n    ch.assertExchange(exchangeName, \n        socket.options.routing || 'direct',\n        { durable: socket.options.persistent }\n    ).then(function() {\n        return ch.assertQueue(queueName, {\n            durable: socket.options.persistent\n        });\n    }).then(function(ok) {\n        return ch.consume(queueName, function(msg) {\n            if (msg !== null) {\n                socket.requests.push(msg);\n                socket.push(msg.content);\n            } else socket.push(null);\n        }, {\n            noAck: false\n        }).then(function(ok) {\n            socket.consumers[queueName] = ok.consumerTag;\n        });\n    }).then(callback || ignore);\n};\n\n```\n. Basically if we've got a variety of different services, each one doing rpc, we can do something like namespacing via the exchanges and not have to worry about something going really wrong and binding to another service's queues.\nPoint 1. makes total sense.\nBit confused about point 2. It still goes to the default exchange, what about that exchange prevents those problems?\n. Also, setting a different exchange than the default allows us to set durability and auto-delete and all that. Those choices can be a bit application-specific and having access to that flexibility via the socket abstraction would be cool.\n. Just to follow up, I went with the defaults you've laid out here in rabbit.js instead of all this patching to get exchanges. We're going to put the basic namespacing we want in our queue names. Those complications are pretty good reasons for us not to bother with the patching. So thanks for that response, much appreciated.\n\nI don't think that sticking the reply queues on a specific exchange is really that important. Putting the request queues on a topic exchange with a routing key scheme that matches something like \"service.subsystem.method\" would still be nice though. I'm not 100% clear what you meant by, \"Having routing in the middle breaks the REQ/REP contract\". If it's just requests on another exchange, I don't think that would break it right? The point is just to have a direct exchange for the message going back from the server node. Even if it wasn't the default exchange, as long as the reply_to queue is on a direct exchange it's no biggie right?\n. ",
    "AlPatsino": "Seems that variant:\n\n```\nrequest.on(\"data\", function(message) {\n    console.log('%s-st Handler received message - %j', i, JSON.parse(message));\n    process.nextTick(function() {\n        request.close();\n    });\n});\n```\n\nIs more appropriate. Could you please explain whether this approach is correct?\n. Seems that variant:\n\n```\nrequest.on(\"data\", function(message) {\n    console.log('%s-st Handler received message - %j', i, JSON.parse(message));\n    process.nextTick(function() {\n        request.close();\n    });\n});\n```\n\nIs more appropriate. Could you please explain whether this approach is correct?\n. ",
    "vespakoen": "subscribing\n\n+1 for automatically closing the socket\n. subscribing\n\n+1 for automatically closing the socket\n. ",
    "5an1ty": "Seems like this is still an issue?\n. Seems like this is still an issue?\n. ",
    "mamingcao": "Thanks @AlPatsino  your solution works well. \n. Thanks @AlPatsino  your solution works well. \n. ",
    "CalebMorris": "Example uses that I would want header support for:\n1) User authentication\n2) Cross-Queue Auth\n3) Cross-Service Auth\nIt seems like something that an abstraction would want considering it is a common feature for most message queues.\n. Example uses that I would want header support for:\n1) User authentication\n2) Cross-Queue Auth\n3) Cross-Service Auth\nIt seems like something that an abstraction would want considering it is a common feature for most message queues.\n. ",
    "utestmig": "Right, I believe we are saying the same thing. I basically copied/pasted what the docs said \"let's create a single callback queue per client\".\n\nI'm still wondering about:\n- Should I only create one 'REQ' socket?\n- Would I continue to listen to 'data'? If so, how will the correlationId get resolved? This is being generated in the 'write' function.\n. Right, I believe we are saying the same thing. I basically copied/pasted what the docs said \"let's create a single callback queue per client\".\n\nI'm still wondering about:\n- Should I only create one 'REQ' socket?\n- Would I continue to listen to 'data'? If so, how will the correlationId get resolved? This is being generated in the 'write' function.\n. ",
    "goloroden": "Sorry, I had not seen https://github.com/squaremo/rabbit.js/issues/57#issuecomment-39115765 before. I think that this answers my question.\n. Sorry, I had not seen https://github.com/squaremo/rabbit.js/issues/57#issuecomment-39115765 before. I think that this answers my question.\n. ",
    "xaka": "That's right. I think as it's RPC, it should look like a normal asynchronous function call and handling where client/server don't care about the order.\n. I might, but my concern is that it doesn't fit well into current behave-like-a-Socket idea. It goes kind of against the flow. What we could do though, as a first step, it to make sequential behavior to be optional i.e. when you read from socket, you should expect to get messages in random order. That would unblock the ability to build RPC-like servers/clients on top of it. What about adding `sequential` to list of options with default value being `true` for backward compatibility?\n. That's right. I think as it's RPC, it should look like a normal asynchronous function call and handling where client/server don't care about the order.\n. I might, but my concern is that it doesn't fit well into current behave-like-a-Socket idea. It goes kind of against the flow. What we could do though, as a first step, it to make sequential behavior to be optional i.e. when you read from socket, you should expect to get messages in random order. That would unblock the ability to build RPC-like servers/clients on top of it. What about adding `sequential` to list of options with default value being `true` for backward compatibility?\n. ",
    "willyaranda": "@squaremo I think messages received from your lib to Rabbit.JS internals has a `deliveryTag` (that is what you are using to ACK messages, based on https://github.com/squaremo/amqp.node/blob/master/lib/channel_model.js#L220-224 )\n\nI think that you can send, along the message, the `deliveryTag` that message has. As I see [1] the `readable.push` method, I think this change will break the current API, as it allows only a `Buffer` or a `String`, and not an `object`, or two parameters.\n\n[1] http://nodejs.org/api/stream.html#stream_readable_push_chunk_encoding\n. @squaremo I think messages received from your lib to Rabbit.JS internals has a `deliveryTag` (that is what you are using to ACK messages, based on https://github.com/squaremo/amqp.node/blob/master/lib/channel_model.js#L220-224 )\n\nI think that you can send, along the message, the `deliveryTag` that message has. As I see [1] the `readable.push` method, I think this change will break the current API, as it allows only a `Buffer` or a `String`, and not an `object`, or two parameters.\n\n[1] http://nodejs.org/api/stream.html#stream_readable_push_chunk_encoding\n. ",
    "Neil-UWA": "@squaremo , thanks for your reply, It does make sense. the #ack must have been called twice somewhere. I didn't realise that I should check previous closed issues for answers. Many thanks and apologies.\n. @squaremo , thanks for your reply, It does make sense. the #ack must have been called twice somewhere. I didn't realise that I should check previous closed issues for answers. Many thanks and apologies.\n. ",
    "corpulent": "I had a bug in my code that I resolved and everything works fine now.\n. @NikosEfthias I am sorry, but I honestly cannot find the specific line of code we fixed.  All it was is that we where not using rabbit.js the way it was supposed to be used.  It was a very obvious bug on our part.  But we eventually we switched to https://github.com/squaremo/amqp.node,  this library is much easier to work with.. @squaremo how would you subscribe and consume an existing queue?\n. I had a bug in my code that I resolved and everything works fine now.\n. @NikosEfthias I am sorry, but I honestly cannot find the specific line of code we fixed.  All it was is that we where not using rabbit.js the way it was supposed to be used.  It was a very obvious bug on our part.  But we eventually we switched to https://github.com/squaremo/amqp.node,  this library is much easier to work with.. @squaremo how would you subscribe and consume an existing queue?\n. ",
    "NikosEfthias": "how did you exactly fix this? With rabbitmq i get no error but when i use activemq i get this error @corpulent . @corpulent I got the same result and after some research, i found the issue amqp.node does not implement the amqp 1.0, however, ActiveMQ, unlike rabbitmq, uses the latest protocol standard. how did you exactly fix this? With rabbitmq i get no error but when i use activemq i get this error @corpulent . @corpulent I got the same result and after some research, i found the issue amqp.node does not implement the amqp 1.0, however, ActiveMQ, unlike rabbitmq, uses the latest protocol standard. ",
    "realistschuckle": "@tobias-neubert I would change your API, a little.\n\n``` javascript\nfunction Foo(context) {\n  this.context = context;\n}\n\nFoo.prototype.bar = function () {\n  var self = this;\n  this.context.on('ready', function () {\n    var socket = self.context.socket('PUBLISH');\n    socket.end('hello-rabbit', 'Here I am');\n  });\n};\n\nmodule.exports = Foo;\n```\n\nThen, I could test it with (shameless plug) something like [nodemock](http://curtis.schlak.com/nodemock/).\n\n``` javascript\nvar nodemock = require('nodemock');\nvar ctrl = {};\nvar frabbit = nodemock.mock('on').takes('ready', function () {}).ctrl(1, ctrl);\nfrabbit.mock('socket').takes('PUBLISH');\nfrabbit.mock('end').takes('hello-rabbit', 'Here I am');\n\nvar Foo = require('./foo'); // Your file\nvar foo = new Foo(frabbit);\nfoo.bar();\n\nctrl.trigger(10, 20); // triggers the callback of the \"on\" method\nfrabbit.assert();\n```\n. @tobias-neubert I would change your API, a little.\n\n``` javascript\nfunction Foo(context) {\n  this.context = context;\n}\n\nFoo.prototype.bar = function () {\n  var self = this;\n  this.context.on('ready', function () {\n    var socket = self.context.socket('PUBLISH');\n    socket.end('hello-rabbit', 'Here I am');\n  });\n};\n\nmodule.exports = Foo;\n```\n\nThen, I could test it with (shameless plug) something like [nodemock](http://curtis.schlak.com/nodemock/).\n\n``` javascript\nvar nodemock = require('nodemock');\nvar ctrl = {};\nvar frabbit = nodemock.mock('on').takes('ready', function () {}).ctrl(1, ctrl);\nfrabbit.mock('socket').takes('PUBLISH');\nfrabbit.mock('end').takes('hello-rabbit', 'Here I am');\n\nvar Foo = require('./foo'); // Your file\nvar foo = new Foo(frabbit);\nfoo.bar();\n\nctrl.trigger(10, 20); // triggers the callback of the \"on\" method\nfrabbit.assert();\n```\n. ",
    "tjwebb": "https://github.com/squaremo/rabbit.js/blob/master/package.json#L23\n\nYes please\n. https://github.com/squaremo/rabbit.js/blob/master/package.json#L23\n\nYes please\n. ",
    "sydcanem": "It would be great if I could just do:\n\n``` js\nvar ReqSocket = require('rabbit.js').ReqSocket;\n// Manage the connection and channel\n...\nvar requester = new ReqSocket(channel, options);\n```\n. Looks like this PR #88 seems to work ok. I can now do:\n\n``` js\nvar ReqSocket = require('rabbit.js').REQ;\n```\n. It would be great if I could just do:\n\n``` js\nvar ReqSocket = require('rabbit.js').ReqSocket;\n// Manage the connection and channel\n...\nvar requester = new ReqSocket(channel, options);\n```\n. Looks like this PR #88 seems to work ok. I can now do:\n\n``` js\nvar ReqSocket = require('rabbit.js').REQ;\n```\n. ",
    "zladuric": "Kind of, although this is probably not going to happen soon. And since there's not even the node 0.12 support (in the \"engines\"), I'll have to do my manual adjustments anyway locally.\n. Kind of, although this is probably not going to happen soon. And since there's not even the node 0.12 support (in the \"engines\"), I'll have to do my manual adjustments anyway locally.\n. ",
    "mwaarna": "Maybe try nulling the buffer after each write. or move the buffer outside of the loop since the buffer never changes.\n. Maybe try nulling the buffer after each write. or move the buffer outside of the loop since the buffer never changes.\n. ",
    "wickedsheep": "Already tried both combinations with the same results, so I decided to keep the example as simple as possible. Here is the code that generates a random message and clears the buffer after each write.\n\n``` javascript\nvar rabbit = require('rabbit.js'),\n  util = require('util'),\n  context = rabbit.createContext('amqp://localhost'),\n  producer,\n  buffer,\n  i,\n  set = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789';\n\nfunction getRandomMessage() {\n  var msg = '',\n    i;\n\n  for (i = 0; i < 20; ++i)\n    msg += set.charAt(Math.floor(Math.random() * set.length));\n\n  return msg;\n}\n\ncontext.on('ready', function () {\n  producer = context.socket('PUSH');\n  producer.connect('test', function () {\n    for (i = 0; i < 100000; i++) {\n      buffer = new Buffer(getRandomMessage(), 'utf8');\n      producer.write(buffer);\n      buffer = null;\n\n      if (i % 1000 === 0)\n        console.log('usage @ '+i+': ' + util.inspect(process.memoryUsage()));\n    }\n\n    console.log('usage @ ' + i + ': ' + util.inspect(process.memoryUsage()));\n    buffer = null;\n  });\n});\n```\n. Already tried both combinations with the same results, so I decided to keep the example as simple as possible. Here is the code that generates a random message and clears the buffer after each write.\n\n``` javascript\nvar rabbit = require('rabbit.js'),\n  util = require('util'),\n  context = rabbit.createContext('amqp://localhost'),\n  producer,\n  buffer,\n  i,\n  set = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789';\n\nfunction getRandomMessage() {\n  var msg = '',\n    i;\n\n  for (i = 0; i < 20; ++i)\n    msg += set.charAt(Math.floor(Math.random() * set.length));\n\n  return msg;\n}\n\ncontext.on('ready', function () {\n  producer = context.socket('PUSH');\n  producer.connect('test', function () {\n    for (i = 0; i < 100000; i++) {\n      buffer = new Buffer(getRandomMessage(), 'utf8');\n      producer.write(buffer);\n      buffer = null;\n\n      if (i % 1000 === 0)\n        console.log('usage @ '+i+': ' + util.inspect(process.memoryUsage()));\n    }\n\n    console.log('usage @ ' + i + ': ' + util.inspect(process.memoryUsage()));\n    buffer = null;\n  });\n});\n```\n. ",
    "sberryman": "I thought about this as well, I need contentEncoding.\n\nI was thinking of specifying an array of allowed options for publish and merging those. Some just don't make sense such as correlationId, messageId and timestamp as they are message specific. Unless those options were defined as a function then we could pass the chunk and encoding into the function and set the option value to the result of that function.\n. It would be really nice to have an example of a streaming worker which handles ack's in order. I'm okay with items down the stream to wait sending an ack until the low water mark catches up. So if you had a prefetch of 100, messages 2-100 were complete but still waiting on message 1 to finish, I'm okay holding 2-100 in the buffer until 1 is acked at which point 2-100 will very quickly be acked as well. Hopefully that made sense.\n. I thought about this as well, I need contentEncoding.\n\nI was thinking of specifying an array of allowed options for publish and merging those. Some just don't make sense such as correlationId, messageId and timestamp as they are message specific. Unless those options were defined as a function then we could pass the chunk and encoding into the function and set the option value to the result of that function.\n. It would be really nice to have an example of a streaming worker which handles ack's in order. I'm okay with items down the stream to wait sending an ack until the low water mark catches up. So if you had a prefetch of 100, messages 2-100 were complete but still waiting on message 1 to finish, I'm okay holding 2-100 in the buffer until 1 is acked at which point 2-100 will very quickly be acked as well. Hopefully that made sense.\n. ",
    "ar4mirez": "@sberryman Agree with you :+1: I try to help with this PR but has been a year already so I'm not sure if will be merged some day or if there any other alternative ?\n. @sberryman Agree with you :+1: I try to help with this PR but has been a year already so I'm not sure if will be merged some day or if there any other alternative ?\n. ",
    "etyrrill": "Checking for the queue first would be a great solution.  I considered making that change instead of exposing the deadLetterExchange option (see pull request #98), but being lazy, I made the easy change instead.  The queue is created by the consumer, and will always be there before our node server starts.\n\nI appreciate you taking a look at this.\n\nThanks,\nEd\n. Checking for the queue first would be a great solution.  I considered making that change instead of exposing the deadLetterExchange option (see pull request #98), but being lazy, I made the easy change instead.  The queue is created by the consumer, and will always be there before our node server starts.\n\nI appreciate you taking a look at this.\n\nThanks,\nEd\n. ",
    "darrellodonnell": "Some more detail here: https://www.rabbitmq.com/uri-query-parameters.html\n. Some more detail here: https://www.rabbitmq.com/uri-query-parameters.html\n. ",
    "rogeriobiondi": "Hello, thank you very much for your information and clarification about the issue. It answered and helped a lot!\nBest regards\n. Hello, thank you very much for your information and clarification about the issue. It answered and helped a lot!\nBest regards\n. ",
    "pleerock": "hmmm now after probably 20nth launch it worked.. I cant get why there are delays (or what) because my queue was empty\n. hmmm now after probably 20nth launch it worked.. I cant get why there are delays (or what) because my queue was empty\n. ",
    "DennisAhaus": "Branch may fail because waht is if no options are defined?\n. Branch may fail because waht is if no options are defined?\n. ",
    "evensense": "It was just mistake with RabbitMQ server permissions. You can add them by running following command on server machine : \n\n```\nrabbitmqctl set_permissions username \".*\" \".*\" \".*\" \n```\n\nwhich basically gives all permissions to user, in my case **tomek**\n. It was just mistake with RabbitMQ server permissions. You can add them by running following command on server machine : \n\n```\nrabbitmqctl set_permissions username \".*\" \".*\" \".*\" \n```\n\nwhich basically gives all permissions to user, in my case **tomek**\n. ",
    "charyorde": "If not, how can I retrieve the autogenerated Queue name (if any)?\n. Okay, I understand that the concept of 'persistence' handles durability and exclusivity only partains to Queues and not Exchanges.\n\nNow where consumerTag would be useful is when you have non-nodejs apps as consumers of a pub/sub socket. Embedding the identifier in the message body is near impossible because Java/Scala libraries receive messages as byte array.\n. If not, how can I retrieve the autogenerated Queue name (if any)?\n. Okay, I understand that the concept of 'persistence' handles durability and exclusivity only partains to Queues and not Exchanges.\n\nNow where consumerTag would be useful is when you have non-nodejs apps as consumers of a pub/sub socket. Embedding the identifier in the message body is near impossible because Java/Scala libraries receive messages as byte array.\n. ",
    "battlesnake": "`sub.queue` may be what you're looking for, to get the auto-generated queue name.  I'm new to this library (and to RabbitMQ), and am having difficulty getting SUB to work (I need to RTFM a couple more times I think).\n. `sub.queue` may be what you're looking for, to get the auto-generated queue name.  I'm new to this library (and to RabbitMQ), and am having difficulty getting SUB to work (I need to RTFM a couple more times I think).\n. ",
    "floatingLomas": "Hey, just following up - can this be merged in?\n. Hey, just following up - can this be merged in?\n. ",
    "mkozjak": "@squaremo: What's up with this?\n. @squaremo: What's up with this?\n. ",
    "zamnuts": "+1 Could really use this feature. `noCreate` either puts a burden on the sysadmin role, or creates a manual step for the application owner. It appears that this flexibility exists in Ruby's RabbitMQ lib, so I'm not sure regarding the reason for the handicap.\n. @squaremo I am new to RabbitMQ (and AMQP in general), and am under the impression that amqp.node is for _any_ service that supports AMQP, while rabbit.js is specific to the RabbitMQ flavor and its individual paradigms. I could be wrong and incorrectly aligned with this project's goals, so I apologize for that.\n\nWith this in mind, considering [RabbitMQ's management interface (see `/api/exchanges/vhost/name`)](https://cdn.rawgit.com/rabbitmq/rabbitmq-management/rabbitmq_v3_5_4/priv/www/api/index.html) reveals AMQP's underlying e.g. `durable` option, I would expect it to be presented in rabbit.js.\n\nPerhaps omitting functionality (e.g. options) altogether is less favorable than having them available but with intrinsic defaults. There already exists a hint of this with `routing` being `fanout`.\n\nFrom a beginners perspective, the current socket-like abstractions are useful organizationally speaking, albeit I don't personally use them in a stream context (yet).\n. +1 Could really use this feature. `noCreate` either puts a burden on the sysadmin role, or creates a manual step for the application owner. It appears that this flexibility exists in Ruby's RabbitMQ lib, so I'm not sure regarding the reason for the handicap.\n. @squaremo I am new to RabbitMQ (and AMQP in general), and am under the impression that amqp.node is for _any_ service that supports AMQP, while rabbit.js is specific to the RabbitMQ flavor and its individual paradigms. I could be wrong and incorrectly aligned with this project's goals, so I apologize for that.\n\nWith this in mind, considering [RabbitMQ's management interface (see `/api/exchanges/vhost/name`)](https://cdn.rawgit.com/rabbitmq/rabbitmq-management/rabbitmq_v3_5_4/priv/www/api/index.html) reveals AMQP's underlying e.g. `durable` option, I would expect it to be presented in rabbit.js.\n\nPerhaps omitting functionality (e.g. options) altogether is less favorable than having them available but with intrinsic defaults. There already exists a hint of this with `routing` being `fanout`.\n\nFrom a beginners perspective, the current socket-like abstractions are useful organizationally speaking, albeit I don't personally use them in a stream context (yet).\n. ",
    "nahpeps": "I am also new in rabbit.js and the whole node.js context. For me, the abstraction of using sockets instead of streams was very helpful, because I don't need to deal with the pitfalls of streams, but use a concept that I know in general.\n\nBut of course I also read the RabbitMQ documentation and was irritated because I was not able to create an `autoDelete` exchange until I digged into the source.\n\nSo just adding the possibility to give, maybe optional, settings to the socket creation which then are merged with a default option set, would be a great thing.\n. I am also new in rabbit.js and the whole node.js context. For me, the abstraction of using sockets instead of streams was very helpful, because I don't need to deal with the pitfalls of streams, but use a concept that I know in general.\n\nBut of course I also read the RabbitMQ documentation and was irritated because I was not able to create an `autoDelete` exchange until I digged into the source.\n\nSo just adding the possibility to give, maybe optional, settings to the socket creation which then are merged with a default option set, would be a great thing.\n. ",
    "rtubio": "@squaremo : I know that I made you a question in the repository for AMQP that is pretty similar to this one, but I want to explore all the possibilities.\n\nIs there any pure JS or AngularJS library for RabbitMQ?\n. @analytik : my use case is a system composed of 2 different types of clients:\n- Web application developed with Angular JS that, right now, is using pusher.com and JSON/RPC over HTTPS for the communications with a Django server. \n- Python client that connects to a Twisted server to exchange messages with low latency (real time).\n\nI am trying to find an integrated communications system to use a single protocol for all the above mentioned 3 connections. RabbitMQ, together with Redis, Crossbar.io and gunicorn-websockets, match pretty well my requirements; however, each of them has a little drawback that I am trying to work around:\n1. RabbitMQ and Redis do not have an Angular/Javascript browser support, so I cannot directly communicate my first app (Angular JS based) with the server;\n   1. crossbar.io and gunicorn-websocket: libraries available for JS and even Angular (WAMP);\n2. RabbitMQ and Redis directly integrate with Django through Celery (2/3 steps configuration process) that is really fast and efficient;\n   1. crossbar.io and gunicorn-websocket: not a so-clear-integration process w/Django;\n   2. there is a project/example to integrate crossbar.io w/Django through Celery, but it is not very mature yet;\n3. crossbar.io and gunicorn-websocket implement WAMP, which is the correct protocol for this application (offers RPC and PUB/SUB, both for clients ant the web);\n   1. RabbitMQ offers STOMP, but you kind of have to run it as a slightly separate server on its own.\n\nAs you may have noticed, I am trying to find the most suitable option and that is why I am asking for clarifications on some of the issues above to understand correctly the actual limits of these options.\n. @squaremo : I know that I made you a question in the repository for AMQP that is pretty similar to this one, but I want to explore all the possibilities.\n\nIs there any pure JS or AngularJS library for RabbitMQ?\n. @analytik : my use case is a system composed of 2 different types of clients:\n- Web application developed with Angular JS that, right now, is using pusher.com and JSON/RPC over HTTPS for the communications with a Django server. \n- Python client that connects to a Twisted server to exchange messages with low latency (real time).\n\nI am trying to find an integrated communications system to use a single protocol for all the above mentioned 3 connections. RabbitMQ, together with Redis, Crossbar.io and gunicorn-websockets, match pretty well my requirements; however, each of them has a little drawback that I am trying to work around:\n1. RabbitMQ and Redis do not have an Angular/Javascript browser support, so I cannot directly communicate my first app (Angular JS based) with the server;\n   1. crossbar.io and gunicorn-websocket: libraries available for JS and even Angular (WAMP);\n2. RabbitMQ and Redis directly integrate with Django through Celery (2/3 steps configuration process) that is really fast and efficient;\n   1. crossbar.io and gunicorn-websocket: not a so-clear-integration process w/Django;\n   2. there is a project/example to integrate crossbar.io w/Django through Celery, but it is not very mature yet;\n3. crossbar.io and gunicorn-websocket implement WAMP, which is the correct protocol for this application (offers RPC and PUB/SUB, both for clients ant the web);\n   1. RabbitMQ offers STOMP, but you kind of have to run it as a slightly separate server on its own.\n\nAs you may have noticed, I am trying to find the most suitable option and that is why I am asking for clarifications on some of the issues above to understand correctly the actual limits of these options.\n. ",
    "analytik": "I don't think anyone really connects directly to RabbitMQ from a browser. What's your use case?\n. www.rethinkdb.com\n\nAlthough no single technology ever will be a silver bullet, I would say that RethinkDB has the potential to simplify your stack, although maybe not exactly as you imagine. It can replace Pusher.com with its realtime push, it's a full-featured database, and to some degree it can work as PUB/SUB, although it's not trying to compete with RabbitMQ.\n\nIn the next release (1-3 months) it will add support for direct connection from browsers, although I'm not sure if they plan any library-specific support on Day 1. However, hooking up RethinkDB -> Node.js + Socket.io -> Angular + Socket.io is very easy, and can be done in an hour or two.\n\nIf you're a fan of ORMs, you can use Angular + JS-Data-Angular -> Node.js + JS-Data-RethinkDB, although that doesn't have a direct support for websockets/push, but can get you up and running with basic filtering in place.\n\nYou could also try https://github.com/mikemintz/rethinkdb-websocket-client - but I have not tried that one.\n\n(And sorry to squaremo if this feels disrespectful to have this discussion on a rabbit.js issue.)\n. I don't think anyone really connects directly to RabbitMQ from a browser. What's your use case?\n. www.rethinkdb.com\n\nAlthough no single technology ever will be a silver bullet, I would say that RethinkDB has the potential to simplify your stack, although maybe not exactly as you imagine. It can replace Pusher.com with its realtime push, it's a full-featured database, and to some degree it can work as PUB/SUB, although it's not trying to compete with RabbitMQ.\n\nIn the next release (1-3 months) it will add support for direct connection from browsers, although I'm not sure if they plan any library-specific support on Day 1. However, hooking up RethinkDB -> Node.js + Socket.io -> Angular + Socket.io is very easy, and can be done in an hour or two.\n\nIf you're a fan of ORMs, you can use Angular + JS-Data-Angular -> Node.js + JS-Data-RethinkDB, although that doesn't have a direct support for websockets/push, but can get you up and running with basic filtering in place.\n\nYou could also try https://github.com/mikemintz/rethinkdb-websocket-client - but I have not tried that one.\n\n(And sorry to squaremo if this feels disrespectful to have this discussion on a rabbit.js issue.)\n. ",
    "gRoberts84": "Ok, sorry I should have RTFM...\n\nPUSH / WORKER: a WORKER socket is similar to a PULL socket, but requires that you call #ack on it to acknowledge that you have processed each message. Any messages left unacknowledged when the socket closes, or crashes, will be requeued and delivered to another connected socket (should there be one).\n\nD'oh. Sorry all.\n. Hey Michael,\n\nThanks for getting back to me and sorry for the delay, I've been trying to find the time to work on this around all of my other work. Fun huh.\n\nStill not sure how best to tackle this... At first I thought PUSH/WORKER was the best solution, however now looking at it, it \"may\" be REQ/REP.\n\nUltimately I need a non-blocking way of ensuring that only one recipient processes a message at a time and should that recipient fail, the message needs requeuing to allow another recipient to pick it up.\n\nJust to explain how I've done things so far:\n1. REST Server sends PUSH message to Address 1 to initiate data stream.\n2. DATA Connector receives PUSH message on Address 1 and creates a new data stream.\n3. DATA Connector receives data from data stream and sends PUSH message to Address 2 with data.\n4. QUEUE Processor receives PUSH message on Address 2, processes and acknowledges message.\n\nCurrently 3 won't receive any messages until 1 is acknowledged, however by acknowledging it, should the process fail after acknowledging it, it won't be requeued.\n\nI still think message queuing is the best solution, it's just more how to implement it. \n\nDo you have any suggestions on how best to use RabbitMQ/Rabbit.js to support this, or anything else for that matter?\n\nThanks again\n\nGavin\n. Ok, sorry I should have RTFM...\n\nPUSH / WORKER: a WORKER socket is similar to a PULL socket, but requires that you call #ack on it to acknowledge that you have processed each message. Any messages left unacknowledged when the socket closes, or crashes, will be requeued and delivered to another connected socket (should there be one).\n\nD'oh. Sorry all.\n. Hey Michael,\n\nThanks for getting back to me and sorry for the delay, I've been trying to find the time to work on this around all of my other work. Fun huh.\n\nStill not sure how best to tackle this... At first I thought PUSH/WORKER was the best solution, however now looking at it, it \"may\" be REQ/REP.\n\nUltimately I need a non-blocking way of ensuring that only one recipient processes a message at a time and should that recipient fail, the message needs requeuing to allow another recipient to pick it up.\n\nJust to explain how I've done things so far:\n1. REST Server sends PUSH message to Address 1 to initiate data stream.\n2. DATA Connector receives PUSH message on Address 1 and creates a new data stream.\n3. DATA Connector receives data from data stream and sends PUSH message to Address 2 with data.\n4. QUEUE Processor receives PUSH message on Address 2, processes and acknowledges message.\n\nCurrently 3 won't receive any messages until 1 is acknowledged, however by acknowledging it, should the process fail after acknowledging it, it won't be requeued.\n\nI still think message queuing is the best solution, it's just more how to implement it. \n\nDo you have any suggestions on how best to use RabbitMQ/Rabbit.js to support this, or anything else for that matter?\n\nThanks again\n\nGavin\n. ",
    "vicneanschi": "Please take a look at documentation http://www.squaremobius.net/rabbit.js/\nThere is a section at the end of the document.\n. Please take a look at documentation http://www.squaremobius.net/rabbit.js/\nThere is a section at the end of the document.\n. ",
    "davesag": "I'd like to see this PR get merged please.  . I'd like to see this PR get merged please.  . ",
    "liu11hao11": "the example is correctly\r\nbelow is the source\r\n\r\n`PubSocket.prototype.write = function(chunk, encoding) {\r\n  return this.publish(false, chunk, encoding);\r\n};`\r\n\r\n\r\n. the example is correctly\r\nbelow is the source\r\n\r\n`PubSocket.prototype.write = function(chunk, encoding) {\r\n  return this.publish(false, chunk, encoding);\r\n};`\r\n\r\n\r\n. "
}
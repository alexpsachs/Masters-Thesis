{
    "ashtuchkin": "Looks good to me, thanks!\nAs for the method names - I'll make aliases, thanks for the feedback.\n. Looks good. Thank you! \n. Thanks! Could you add a test for the new encoding?\n. Nice work!\n. Just did, thanks for your efforts!\n. Thanks a lot! Could you also make some tests, like its done for GBK? Maybe several web pages using Big5?\n. Technically this 'global' is local to the module and never exposed, so its really equivalent. Still, I agree that its better from code culture point of view.\n. You're right. Total surprise for me, thanks! I think I've read about it somewhere, but don't remember where. I'll need to be more careful (and I have one more reason to use CoffeeScript, which I use in most of my projects).\n. Great! Looking forward for your pull request.\n. Hi! After digging into code, it seems to me that it would be better to do it externally, along the lines of:\n\n```\nvar str = \"Test html escaping.. \u0422\u0435\u0441\u0442\u0438\u0440\u0443\u0435\u043c \u0425\u0422\u041c\u041b.\";\nvar html_str = iconv.decode(iconv.encode(str, \"latin1\"), \"latin1\");\nhtml_str = html_str.replace(/[?]/g, function(c, idx) {\n    return \"&#\"+str.charCodeAt(idx)+\";\";\n});\n```\n\nWhat do you think?\n. Javascript stores strings internally as UTF-16, i.e. 2 bytes per character. And it happens that latin1 encoding is exactly first 256 characters of Unicode, or first byte of javascript string characters, stripping the second one. It also happens that this is exactly what 'binary' encoding in Node.js does. So all encoding/decoding is done by Node.js itself using 'binary' encoding.\nHope this helps.\n\nAbout the deprecation and replacement - it would need some research on performance. Native solutions are usually much faster. I don't have spare time right now to do that, but I'll be happy if you are willing to participate on it.\n. Nice, thank you!\n\nI was thinking about making automatic generation of most codepages using f.ex. original iconv, but don't have time for it right now. Maybe you could help with it? Should be not complex and it would prevent human errors. Or, if you can wait, I'll do it eventually.\n. Thanks a lot!\n. So cool, thanks a lot, Mike!\n. I've added even more encodings and published it as version 0.2.0. Thanks again for your time!\n. Fixed in 0.2.1, thanks!\n. Merged & released version 0.2.3. Thanks for the collaboration and greetings from a yacht in Adriatic sea! ^_^\n. I guess it depends on the machine. Just re-checked and got following results:\n\n``` console\nashtuchkin@ubuntu:~$ node -v\nv0.8.7\nashtuchkin@ubuntu:~$ cd iconv-lite/\nashtuchkin@ubuntu:~/iconv-lite$ node test/perfomance.js \n\nEncoding 262144 chars 1000 times:\niconv: 9886ms, 25.90 Mb/s.\niconv-lite: 5364ms, 47.73 Mb/s.\n\nDecoding 262144 bytes 1000 times:\niconv: 6957ms, 36.80 Mb/s.\niconv-lite: 9356ms, 27.36 Mb/s.\nashtuchkin@ubuntu:~/iconv-lite$ \n```\n. Actually, I just re-checked several times one-after-another, and it may be that my notebook has a dynamic cpu frequency and it changes significantly.\n\n``` console\n\nashtuchkin@ubuntu:~/iconv-lite$ node test/perfomance.js \n\nEncoding 262144 chars 1000 times:\niconv: 8125ms, 31.51 Mb/s.\niconv-lite: 5078ms, 50.41 Mb/s.\n\nDecoding 262144 bytes 1000 times:\niconv: 6729ms, 38.04 Mb/s.\niconv-lite: 8867ms, 28.87 Mb/s.\nashtuchkin@ubuntu:~/iconv-lite$ node test/perfomance.js \n\nEncoding 262144 chars 1000 times:\niconv: 6955ms, 36.81 Mb/s.\niconv-lite: 5077ms, 50.42 Mb/s.\n\nDecoding 262144 bytes 1000 times:\niconv: 6631ms, 38.61 Mb/s.\niconv-lite: 8656ms, 29.57 Mb/s.\nashtuchkin@ubuntu:~/iconv-lite$ node test/perfomance.js \n\nEncoding 262144 chars 1000 times:\niconv: 6913ms, 37.03 Mb/s.\niconv-lite: 5008ms, 51.12 Mb/s.\n\nDecoding 262144 bytes 1000 times:\niconv: 6464ms, 39.60 Mb/s.\niconv-lite: 8578ms, 29.84 Mb/s.\nashtuchkin@ubuntu:~/iconv-lite$ node test/perfomance.js \n\nEncoding 262144 chars 1000 times:\niconv: 6917ms, 37.01 Mb/s.\niconv-lite: 5123ms, 49.97 Mb/s.\n\nDecoding 262144 bytes 1000 times:\niconv: 6692ms, 38.25 Mb/s.\niconv-lite: 9123ms, 28.06 Mb/s.\n```\n. Ok, I took some time to let V8 optimize the encoding/decoding functions and here's what I got:\n\n``` console\nashtuchkin@ubuntu:~/iconv-lite$ node test/perfomance.js\nEncoding 262144 chars 1000 times:\niconv: 7122ms, 35.94 Mb/s.\niconv-lite: 1139ms, 224.76 Mb/s.\n\nDecoding 262144 bytes 1000 times:\niconv: 6716ms, 38.12 Mb/s.\niconv-lite: 1975ms, 129.62 Mb/s.\nashtuchkin@ubuntu:~/iconv-lite$ node test/perfomance.js\n\nEncoding 262144 chars 1000 times:\niconv: 7041ms, 36.36 Mb/s.\niconv-lite: 1115ms, 229.60 Mb/s.\n\nDecoding 262144 bytes 1000 times:\niconv: 6607ms, 38.75 Mb/s.\niconv-lite: 1946ms, 131.55 Mb/s.\nashtuchkin@ubuntu:~/iconv-lite$ node test/perfomance.js\n\nEncoding 262144 chars 1000 times:\niconv: 6958ms, 36.79 Mb/s.\niconv-lite: 1112ms, 230.22 Mb/s.\n\nDecoding 262144 bytes 1000 times:\niconv: 6657ms, 38.46 Mb/s.\niconv-lite: 1944ms, 131.69 Mb/s.\n```\n\nCould you please confirm you have similar performance increase with latest v0.2.4?\n. Just now\n. Thats great) And thank you for raising this issue!\n. You could make it similar to GBK encoding. You only need to get a table of\nUnicode <-> GB18030 characters from somewhere. And also, please make a test\nfor it, all encodings are tested in iconv-lite.\n\n## \n\nAlexander Shtuchkin\n\nOn Sat, Aug 25, 2012 at 1:28 AM, fengmk2 notifications@github.com wrote:\n\n> :) Thanks for your excellent work!\n> \n> I will keep on using this module on my project, and replace all `iconv`\n> with `iconv-lite`.\n> \n> One more question: How can I add 'GB18030' encodings on iconv-lite?\n> \n> ---\n> \n> MK2, FaWave, Net4Team \u263anodejs\n> TEL: 18668079069\n> Github: https://github.com/fengmk2\n> \u5fae\u535a: @Python\u53d1\u70e7\u53cb\n> Twitter: @fengmk2\n> CNode: http://cnodejs.org/user/suqian\n> Blog: http://fengmk2.github.com/\n> \n> On Aug 25, 2012, at 5:23 AM, Alexander Shtuchkin notifications@github.com\n> wrote:\n> \n> > Thats great) And thank you for raising this issue!\n> > \n> > \u2014\n> > Reply to this email directly or view it on GitHub.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/ashtuchkin/iconv-lite/issues/15#issuecomment-8015349.\n. Also, to make it compatible with browsers (started in asutherland/iconv-lite@e8273cbc4cb321f300c3530b4feda71383e72f39) We will probably use toots/buffer-browserify or native arrays\n. Fixed in 0.4\n. Yup, thanks!\n. I optimized it a little more if you don't mind :)\n\n```\ngbk charset performance test:\n\nEncoding 131072 chars 10000 times:\niconv: 110634ms, 20.25 Mb/s.\niconv-lite: 12094ms, 185.22 Mb/s.\n\nDecoding 229376 bytes 10000 times:\niconv: 113290ms, 19.77 Mb/s.\niconv-lite: 13567ms, 165.11 Mb/s.\n```\n\nAlso, published v0.2.5.\n. Thanks! Will release it with the next version (change is rather minor).\n. Thanks! Should be fixed in 83c09be246fba24bdd6e13c1302af1e050face19\nI've published new version 0.2.6 on npm with this fix, please check.\n. Hi Julien and thank you!\nGood streaming support will require handling edge issues and will require review all current encoding schemes, which is somewhat time-consuming. If you are interested in doing it, I'll be happy to elaborate more about how to do it.\n\nMeanwhile, you're right that single-byte encodings can be streamed without handling edge cases, just use it buffer-by-buffer.\n. Julien, sorry for a long delay.\n\nI've reviewed the code and it seems it would take more than I anticipated.\nCurrent architecture is not really suited for streaming, so I'll need to\nrevise it as a whole. Basic problem here is that we need to keep state\nbetween decoding buffers, and we can't do that easily with current arch.\n\nI'm a bit busy right now with my daywork, but I'm planning to do the\noverhaul maybe in several months (I also have several more features to add\nlike browser support and more multibyte encondings).\n\n## \n\nAlexander Shtuchkin\n\nOn Thu, Nov 29, 2012 at 11:20 AM, Julien Genestoux <notifications@github.com\n\n> wrote:\n> \n> Thanks for the response!\n> I'll happily help. What do you see the algorithm to be?\n> \n> Thanks,\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/ashtuchkin/iconv-lite/issues/20#issuecomment-10837605.\n. Meanwhile, you can look at http://code.google.com/p/stringencoding/\nThere should be streaming support built in.\n. Although it uses ArrayBuffers, not node's Buffers, I believe you could try something like \n\n```\n var buffer = new Buffer(..) // node's buffer.\n var string = TextDecoder(encoding).decode(new Uint8Array(buf), {stream: true});\n```\n. Hi! Sorry for delay. At last, I've published a iconv-lite@0.4.0-pre2 / [branch v0.4](https://github.com/ashtuchkin/iconv-lite/tree/v0.4) with stream support (see [README](https://github.com/ashtuchkin/iconv-lite/blob/v0.4/README.md) for syntax). Any comments appreciated before I release it.\n. @tomas any issues? I want to release the 0.4 version soon, so your feedback is very welcome.\n. So, you need browser-like behavior, that's a bit more complex than what iconv-lite is designed to do. \n\nWhen there's no indication of charset in HTTP headers, you could probably peek the first ~1 kb for a `<meta http-equiv=\"Content-Type\">` or `<meta charset=\"...\">` tags and use them (using ascii encoding for this should be enough).\n\nOtherwise, you could skip this logic and give your user ability to just set the encoding manually. IMHO its a rare case.\n\nAlso, please be aware that iconv-lite in streaming decoding mode produces a stream of **strings**, not buffers.\n. All right, it seems that the discussion moved away from streams. I'm closing the issue for now, as the functionality is there. Report new bug if you notice any problems.\n. Jon, I wasn't planning to work on it in the nearest future (I have lots to do on my regular work). Do you need something specific from the feature list?\n. no problem)\n. Moved to 0.4.\n. Glad you figured it out. Here's whats happening inside:\n\n```\n> iconv.encode( '\u00e9', 'latin1' )\n<Buffer e9>\n> iconv.encode( '\u00e9', 'latin1' ).toString()  # Here .toString() uses 'utf8' by default and E9 in an incorrect char in it.\n'\ufffd'\n> iconv.encode( '\u00e9', 'latin1' ).toString().charCodeAt(0)\n65533\n> new Buffer('\ufffd') # iconv.decode expects Buffer as a first arg and converts string to Buffer using utf8 if given a string.\n<Buffer ef bf bd>\n> iconv.decode(new Buffer('\ufffd'), 'latin1' )\n'\u00ef\u00bf\u00bd'\n\n```\n. Nice, thank you! It would be even better with tests.\n. Thanks, David!\n. Thank you! I've already merged the #24 (it was first and has more tests), but added your test case.\n. Interesting. I'm wondering what's your use case that you care about 120ms on start?\n. Ok, thank you and thanks for the contribution!\n. Published v0.2.9.\n. Hi! Just so I can understand you better - are you worrying about performance or code explicitness? Is it ok if I convert to Buffer inside the `decode` function?\n\nCurrently you can pass a string to `decode`, but it'll be handled as 'utf8', not 'binary'. Although I agree that 'binary' makes more sense.\n. That'd require writing 2 versions of algorithm for each different encoding scheme as far as I can say... What use case do you have for it?\n. I've had a lot of experiments to squeeze as much performance as I can out of this. The best I could do is to work with Buffers as much as possible and convert to/from strings on the boundaries - this turned out to be much faster than working with strings/chars directly, giving the byte-by-byte nature of encoding conversion.\n\nKeeping that in mind, I don't think there will be significant gains of decoding straight from the strings, especially in the light of needing to keep 2 versions of all decoding routines. Creating a small buffer is cheap nowadays, it's allocated from pools (I'm sure you knew that), so the only thing that we skip is writing a binary-encoded string to a buffer, which is also pretty cheap AFAIK.\n\nAlso, I believe a better way to do it in your project might be to feed decoder directly with slices of original buffer. This will require stateful codec approach, which is planned for v0.3, together with streaming.\n\nSo, I propose using current approach for now and optimize when v0.3 will be out (though no timeline promises here).\nBTW, I published a v0.2.10 to accept binary-encoded strings directly in `decode` function.\n. Yes, I agree about the boundary. Short test has shown that with string len=10, js routine is 5x faster than Buffer.write() with 'utf8' encoding. But on 100 char strings they are already equal in time. On 1000 char - js is 7.5x slower. Maybe this optimization could take better place in the node.js core.\n\n``` javascript\n(function() {\n    var buf = new Buffer(1000);\n    var str = '';\n    for (var i = 0; i < 10; i++)\n        str = str + 'helloworld'; // sample 10 char string.\n\n    var start = Date.now();\n    for (var i = 0; i < 1000000; i++) {\n        for (var j = 0, k = 0; j < str.length; j++) {\n            var c = str.charCodeAt(j);\n            if (c < 0x80) {\n                buf[k++] = c;\n            } else {\n                // Here we'll implement utf8.\n            }\n        }\n    }\n    console.log(\"JS only: \", Date.now() - start);\n\n    start = Date.now();\n    for (var i = 0; i < 1000000; i++) {\n        buf.write(str);\n    }\n    console.log(\"Cross-boundary: \", Date.now() - start);\n\n})() // Wrapped in function to let V8 optimize it.\n```\n\nAnyway I'll close this issue if you don't mind, because of the double-codebase concern. I just dont have enough time to support it, sorry.\n. Hi and thanks for your efforts!\n\nMy concerns:\n1. This needs proper testing. Please, use current testing infrastructure to write at least a couple of tests both for StreamEncoder and StreamDecoder.\n2. I believe StringDecoder.detectIncompleteChar is not a public Node.js API - not good. Are there any alternatives? Also it will not work for any multibyte encoding except utf8 & utf16.\n3. StreamEncoder will not work for [surrogate pairs](http://en.wikipedia.org/wiki/Surrogate_pair).\n\nI cannot accept this PR until these are addressed, and I believe items 2 and 3 cannot be addressed before a major rewrite of this library that I've started in a v0.3 branch awhile ago.\n. Hi and thanks for your efforts!\n\nMy concerns:\n1. This needs proper testing. Please, use current testing infrastructure to write at least a couple of tests both for StreamEncoder and StreamDecoder.\n2. I believe StringDecoder.detectIncompleteChar is not a public Node.js API - not good. Are there any alternatives? Also it will not work for any multibyte encoding except utf8 & utf16.\n3. StreamEncoder will not work for [surrogate pairs](http://en.wikipedia.org/wiki/Surrogate_pair).\n\nI cannot accept this PR until these are addressed, and I believe items 2 and 3 cannot be addressed before a major rewrite of this library that I've started in a v0.3 branch awhile ago.\n. Okay, I'll close this for now and will track the feature in #20.\n. Nice, thank you!\n. Published to npm as version 0.2.11.\n. Hi there! Added all v0.2.x tags. Thanks!\n. Hi Dan, thank you. Please make it compile (see travis error), add some tests and I'll be happy to merge.\n. Thats great, thank you! Can I ask you to add one more test? For a sufficiently large (~10kb) text in CP949, like a web page or some other text you might find relevant, could you check the encoding/decoding provides the same results as the iconv library? We did this with big5 and gbk for example. This is useful because the double-byte encodings are  a bit harder to check.\n. Thats a good work, thanks a lot Dan!\n. Yes, thank you Ralle, you're absolutely right.\n. Hi Ralle!\n\nDoing this the right way would require some non-trivial time, which I don't have right now (maybe in a month or two).\nWhat I would suggest for now is:\n1. Try to compare str == iconv.decode(iconv.encode(str, enc), enc). This should give you a good enough approximation.\n2. Or, use original iconv library.\n. I think the mechanism in #53 should help you. Closing this one.\n. Thanks Fabio! I'm keeping it in mind, but cant promise a timeline yet, sorry.\n. FYI: I've added support for Shift_JIS in V0.4 ( #51 ).\n. Hi! Can you post expected result and result given by iconv-lite?\n. ``` javascript\n> iconv.encode('\u05e9\u05dc\u05d5\u05dd \u05e2\u05d5\u05dc\u05dd!', 'win1255');\n<Buffer f9 ec e5 ed 20 f2 e5 ec ed 21>\n```\n\n``` bash\n~$ printf '\u05e9\u05dc\u05d5\u05dd \u05e2\u05d5\u05dc\u05dd!' | iconv -f utf-8 -t cp1255 | hexdump\n0000000 f9 ec e5 ed 20 f2 e5 ec ed 21                  \n```\n\nSeems ok for me. Closing.\n. Yes, sure. `iconv.decode(buf, encoding)` takes binary data and decodes it into a JS string. So, what you're need here is:\n\n``` javascript\noriginalData = new Buffer(\"base64 string with ISO-8859-1 encoding goes here\", \"base64\");  // Notice, no .toString().\njsStr = iconv.decode(originalData, \"ISO-8859-1\");\n\n// Here you can use jsStr as usual javascript String:\njsStr.replace(\"hello\", \"world\");\n\n// If you need a buffer with this string UTF-8 encoded, then you can use Buffer like that:\nutf8EncodedStringBuf = new Buffer(jsStr);  // utf8 is the default encoding.\n\n// Or, equivalent\nutf8EncodedStringBuf = iconv.encode(jsStr, \"utf8\");\n```\n. It depends on how that string was converted from the actual bytes. \n\nIf you have a JS string which was UTF-8-decoded from a buffer of ISO-8859-1-encoded data, then the recovery is possible only if the original string was all ASCII. UTF-8 is (obviously) not compatible with ISO-8859-1 and will produce all sorts of weird chars and decoding errors otherwise.\n\nIn general, you should attempt to get original bytes (in Buffer-s), that way it's easier and more robust.\nIf its not possible, then the best you can do is to try do encode it back as UTF8, then decode as ISO-8859-1, but it'll surely lose some information.\n\n``` javascript\nstr = iconv.decode(new Buffer(str), \"ISO-8859-1\");\n```\n. You're inadvertently converting the `chunk` to string by doing `buffer += chunk`. You should keep it as a buffer, something like this:\n\n``` javascript\n        var buffer = new Buffer(0);\n        stream.on(\"data\", function(chunk) {\n            buffer = Buffer.concat([buffer, chunk]);\n        });\n\n        stream.once(\"end\", function(){\n            str = iconv.decode(buffer, \"ISO-8859-1\");\n        });\n\n```\n. Hi! You should try it like this:\n\n``` javascript\n$ = cheerio.load(iconv.decode(body, 'iso-8859-1'));\n```\n\niconv.decode converts a Buffer of encoded data to a JS string, this is exactly what you need here.\n. Added in v0.4 ( #51 )\n. Fixed in 0.4 ( #51 )\n. Hi! Could you be more specific? Specifically a couple of lines of code with expected results would be helpful.\n. Ok, you are trying to decode a JS string, which is incorrect. You need to get original Buffer-s. Please see https://github.com/ashtuchkin/iconv-lite/issues/36#issuecomment-25700690\n. No, sorry.\n. `iconvLite.decode(buf, 'gbk').indexOf('\ufffd') >= 0` ?\n. I think the mechanism in #53 will help in this situation - it will notify you if any incompatible character is found. Let me know if you need something else. Closing for now.\n. When you work with text data in javascript, you usually work in the following steps:\n1. Get some bytes with known encoding from external source (you get Buffer).\n2. Convert the source Buffer to a native js string (which is itself utf-16), using `iconv.decode` or `buf.toString('utf-8')`.\n3. Do something with native js strings. These are the only strings you can meaningfully work with. \n4. Convert output native js strings to an output Buffer encoded with destination encoding. (`iconv.encode` or `new Buffer(str, 'utf-8')`)\n5. Send output Buffer (bytes) to external party.\n\nSo, in your case I assume that the `str` is given to you as a native js string and so it's utf-16, not utf-8. If not, please ensure it's correctly decoded (just print it to console).\n\nSecond step, where you're converting it to an iso-8859-8 **Buffer**, is good.\n\nThe last line (`str = buf.toString()`) is meaningless because you're trying to read iso-8859-8 encoded buffer as a utf-8 encoded buffer -> you'll get garbage.\n\nAfter conversion to something other than native js strings, you should work with Buffers, not trying to convert it back. Usually you'll need some concatenation, use `Buffer.concat([buf1, buf2])`.\n. Why not?\n\n``` javascript\n\nxmlStr = \"<?xml version='1.0' encoding='ISO-8859-8'?>\" + originalXmlStr;\n\nbuf = iconv.encode(xmlStr, 'iso-8859-8');\n\nrequest({\n  url: \"..\",\n  method: \"POST\",\n  body: buf,\n  ...\n}, function(err, res, body) {\n  // do something.\n});\n```\n\nThe thing is, with `iso-8859` family of encodings, ASCII chars are kept as-is, so the xml header will be kept.\n. From request documentation: \n\n```\nbody - entity body for PATCH, POST and PUT requests. Must be a Buffer or String.\n```\n. We've been discussing it in #20. Browser support will require significant code refactor (f.ex. to make it possible to skip huge CJK tables), so I've no plans in nearest future to add this.\n\nYou can make a fork, of course, but I think for now its easier just to use http://code.google.com/p/stringencoding/\n. Thank you for your effort!\nWe took Big5 encoding from this website: http://moztw.org/docs/big5/  It seems that HKSCS is not compatible with encoding that we use ([Mozilla 1.8 b2u](http://moztw.org/docs/big5/table/moz18-b2u.txt)), so I'm not sure why would you want to create an alias for it if it doesn't work?\n\nThis aside, you can create this same alias in your program if you need it:\n\n``` javascript\nvar iconv = require('iconv-lite');\niconv.encodings['big5hkscs'] = 'big5';\n\n// use it\nstr = iconv.decode(buf, \"big5-hkscs\");\n```\n. Hi! FYI: In #51 I've added support for Big5-HKSCS, with astral characters and sequences. It is not released yet, but can already be tested.\nAlso, the documentation part will be tracked in #48.\nClosing this one.\n. Added howto/internals section to the [wiki](https://github.com/ashtuchkin/iconv-lite/wiki)\n. Thank you! I've used that to create a generic way to add double-byte encodings in V0.4 ( #51 ). Closing this one.\n. Thank you! Added aliases from your PR.\n. Oops, this bug already exists.\n. I agree, I don't really have time to do it now. The main obstacle is that we need to reimplement all the native Node encodings from scratch to support this feature and it's not easy.. @nleush Do you need one of them, or both?\n. Renamed issue to address only EUC-JP, as these are quite different. ISO-2022-JP moved to #60\n. Great, thanks for the report.\n. Hi! I think this character is in the extended, microsoft code page 932, not in the original Shift_JIS or JIS X 0208.\nI took the Shift_JIS table from [unicode.com](http://www.unicode.org/Public/MAPPINGS/OBSOLETE/EASTASIA/JIS/SHIFTJIS.TXT).\n\nLibiconv seems to agree:\n\n``` bash\n$ printf '\\x87\\x40' | iconv -f shift_jis -t utf-16be | hexdump\niconv: (stdin):1:0: cannot convert\n$ printf '\\x87\\x40' | iconv -f cp932 -t utf-16be | hexdump\n0000000 24 60\n```\n\n(U+2460 is \u2460)\n. http://www8.plala.or.jp/tkubota1/unicode-symbols-map2.html\n. Although I can probably use Encoding Standard's variation - they have these characters http://encoding.spec.whatwg.org/index-jis0208.txt\n. I moved to Encoding Standard as the base for Shift_JIS and CP932 encodings in b5a80d1160, so this issue should be fixed. Also, added test for this specific request, see below.\n. Seems that Encoding Standard is almost the same as CP932, extended for compatibility. The authors did a great job reviewing best practices from browsers there.\n. AFAIK, you just need to install node-iconv yourself for encoding module to\nuse it. Let me know if it helped.\n\nISO-2022-JP is a rare encoding nowadays and is quite different in structure\nthan most others (its stateful), so to support it I'd need to write\nseparate codec. I didn't want to make these efforts until at least a couple\nof people said they need it. Your vote is the first for now)\nOn Nov 7, 2014 5:05 AM, \"Max Ogden\" notifications@github.com wrote:\n\n> Ah actually I forgot to also mention that mailparser uses\n> https://www.npmjs.org/package/encoding which in turn uses iconv-lite, but\n> encoding is supposed to be normalization layer around iconv and\n> iconv-lite so it shouldn't have crashed here. Seems like a bug in encoding,\n> i'll file an issue there as well\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/60#issuecomment-62139268\n> .\n. Got it! Thanks\n\n## \n\nAlexander Shtuchkin\n\nOn Mon, Dec 1, 2014 at 8:24 AM, coyle notifications@github.com wrote:\n\n> I'll throw my vote in here. Just ran into the same error.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/60#issuecomment-65090106\n> .\n. Thank you!\n. Hi! You're probably using old version 0.2, I've yet to release a new version 0.4 (in a couple of days). For now you can use github master branch.\n. 0.4.0 is on the npm.\n. Profiling 0.11.12 (good):\n\n```\n   5582   93.8%  LazyCompile: *encoderSBCSWrite /Users/ashtuchkin/code/iconv-lite/encodings/sbcs-codec.js:47:26\n   5491   98.4%    LazyCompile: *encode /Users/ashtuchkin/code/iconv-lite/lib/index.js:13:31\n   5491  100.0%      Function: ~<anonymous> /Users/ashtuchkin/code/iconv-lite/test/performance.js:1:11\n   5491  100.0%        LazyCompile: ~Module._compile module.js:367:37\n   5491  100.0%          LazyCompile: ~Module._extensions..js module.js:465:37\n   5491  100.0%            LazyCompile: ~Module.load module.js:339:33\n\n    259    4.4%  node::smalloc::Alloc(v8::FunctionCallbackInfo<v8::Value> const&)\n    259  100.0%    LazyCompile: ~Buffer buffer.js:47:16\n    259  100.0%      LazyCompile: *encoderSBCSWrite /Users/ashtuchkin/code/iconv-lite/encodings/sbcs-codec.js:47:26\n    258   99.6%        LazyCompile: *encode /Users/ashtuchkin/code/iconv-lite/lib/index.js:13:31\n    258  100.0%          Function: ~<anonymous> /Users/ashtuchkin/code/iconv-lite/test/performance.js:1:11\n    258  100.0%            LazyCompile: ~Module._compile module.js:367:37\n```\n\nv0.11.13 (bad)\n\n```\n  21253   17.5%  v8::internal::JSObject::SetElement(v8::internal::Handle<v8::internal::JSObject>, unsigned int, v8::internal::Handle<v8::internal::Object>, PropertyAttributes, v8::internal::StrictMode, bool, v8::internal::SetPropertyMode)\n  21253  100.0%    LazyCompile: *encoderSBCSWrite /Users/ashtuchkin/code/iconv-lite/encodings/sbcs-codec.js:47:26\n  21003   98.8%      LazyCompile: *encode /Users/ashtuchkin/code/iconv-lite/lib/index.js:13:31\n  21003  100.0%        Function: ~<anonymous> /Users/ashtuchkin/code/iconv-lite/test/performance.js:1:11\n  21003  100.0%          LazyCompile: ~Module._compile module.js:367:37\n  21003  100.0%            LazyCompile: ~Module._extensions..js module.js:465:37\n\n  21043   17.4%  LazyCompile: *encoderSBCSWrite /Users/ashtuchkin/code/iconv-lite/encodings/sbcs-codec.js:47:26\n  20760   98.7%    LazyCompile: *encode /Users/ashtuchkin/code/iconv-lite/lib/index.js:13:31\n  20760  100.0%      Function: ~<anonymous> /Users/ashtuchkin/code/iconv-lite/test/performance.js:1:11\n  20760  100.0%        LazyCompile: ~Module._compile module.js:367:37\n  20760  100.0%          LazyCompile: ~Module._extensions..js module.js:465:37\n  20760  100.0%            LazyCompile: ~Module.load module.js:339:33\n\n  19456   16.1%  v8::internal::Runtime::SetObjectProperty(v8::internal::Isolate*, v8::internal::Handle<v8::internal::Object>, v8::internal::Handle<v8::internal::Object>, v8::internal::Handle<v8::internal::Object>, PropertyAttributes, v8::internal::StrictMode)\n  19456  100.0%    LazyCompile: *encoderSBCSWrite /Users/ashtuchkin/code/iconv-lite/encodings/sbcs-codec.js:47:26\n  19216   98.8%      LazyCompile: *encode /Users/ashtuchkin/code/iconv-lite/lib/index.js:13:31\n  19216  100.0%        Function: ~<anonymous> /Users/ashtuchkin/code/iconv-lite/test/performance.js:1:11\n  19216  100.0%          LazyCompile: ~Module._compile module.js:367:37\n  19216  100.0%            LazyCompile: ~Module._extensions..js module.js:465:37\n\n  17416   14.4%  v8::internal::JSObject::SetElementWithoutInterceptor(v8::internal::Handle<v8::internal::JSObject>, unsigned int, v8::internal::Handle<v8::internal::Object>, PropertyAttributes, v8::internal::StrictMode, bool, v8::internal::SetPropertyMode)\n  17416  100.0%    LazyCompile: *encoderSBCSWrite /Users/ashtuchkin/code/iconv-lite/encodings/sbcs-codec.js:47:26\n  17195   98.7%      LazyCompile: *encode /Users/ashtuchkin/code/iconv-lite/lib/index.js:13:31\n  17195  100.0%        Function: ~<anonymous> /Users/ashtuchkin/code/iconv-lite/test/performance.js:1:11\n  17195  100.0%          LazyCompile: ~Module._compile module.js:367:37\n  17195  100.0%            LazyCompile: ~Module._extensions..js module.js:465:37\n\n  15557   12.8%  v8::internal::ExternalUint8Array::SetValue(v8::internal::Handle<v8::internal::ExternalUint8Array>, unsigned int, v8::internal::Handle<v8::internal::Object>)\n  15557  100.0%    LazyCompile: *encoderSBCSWrite /Users/ashtuchkin/code/iconv-lite/encodings/sbcs-codec.js:47:26\n  15373   98.8%      LazyCompile: *encode /Users/ashtuchkin/code/iconv-lite/lib/index.js:13:31\n  15373  100.0%        Function: ~<anonymous> /Users/ashtuchkin/code/iconv-lite/test/performance.js:1:11\n  15373  100.0%          LazyCompile: ~Module._compile module.js:367:37\n  15373  100.0%            LazyCompile: ~Module._extensions..js module.js:465:37\n\n  10792    8.9%  v8::internal::Runtime_SetProperty(int, v8::internal::Object**, v8::internal::Isolate*)\n  10792  100.0%    LazyCompile: *encoderSBCSWrite /Users/ashtuchkin/code/iconv-lite/encodings/sbcs-codec.js:47:26\n  10657   98.7%      LazyCompile: *encode /Users/ashtuchkin/code/iconv-lite/lib/index.js:13:31\n  10657  100.0%        Function: ~<anonymous> /Users/ashtuchkin/code/iconv-lite/test/performance.js:1:11\n  10657  100.0%          LazyCompile: ~Module._compile module.js:367:37\n  10657  100.0%            LazyCompile: ~Module._extensions..js module.js:465:37\n\n   5862    4.8%  v8::internal::ExternalUint8Array::SetValue(unsigned int, v8::internal::Object*)\n   5862  100.0%    LazyCompile: *encoderSBCSWrite /Users/ashtuchkin/code/iconv-lite/encodings/sbcs-codec.js:47:26\n   5790   98.8%      LazyCompile: *encode /Users/ashtuchkin/code/iconv-lite/lib/index.js:13:31\n   5790  100.0%        Function: ~<anonymous> /Users/ashtuchkin/code/iconv-lite/test/performance.js:1:11\n   5790  100.0%          LazyCompile: ~Module._compile module.js:367:37\n   5790  100.0%            LazyCompile: ~Module._extensions..js module.js:465:37\n\n   4985    4.1%  Stub: CEntryStub\n   3602   72.3%    LazyCompile: *encoderSBCSWrite /Users/ashtuchkin/code/iconv-lite/encodings/sbcs-codec.js:47:26\n   3560   98.8%      LazyCompile: *encode /Users/ashtuchkin/code/iconv-lite/lib/index.js:13:31\n   3560  100.0%        Function: ~<anonymous> /Users/ashtuchkin/code/iconv-lite/test/performance.js:1:11\n   3560  100.0%          LazyCompile: ~Module._compile module.js:367:37\n   3560  100.0%            LazyCompile: ~Module._extensions..js module.js:465:37\n   1368   27.4%    LazyCompile: *encode /Users/ashtuchkin/code/iconv-lite/lib/index.js:13:31\n   1368  100.0%      Function: ~<anonymous> /Users/ashtuchkin/code/iconv-lite/test/performance.js:1:11\n   1368  100.0%        LazyCompile: ~Module._compile module.js:367:37\n   1368  100.0%          LazyCompile: ~Module._extensions..js module.js:465:37\n   1368  100.0%            LazyCompile: ~Module.load module.js:339:33\n\n   3063    2.5%  v8::internal::HandleScope::ZapRange(v8::internal::Object**, v8::internal::Object**)\n   3063  100.0%    LazyCompile: *encoderSBCSWrite /Users/ashtuchkin/code/iconv-lite/encodings/sbcs-codec.js:47:26\n   3020   98.6%      LazyCompile: *encode /Users/ashtuchkin/code/iconv-lite/lib/index.js:13:31\n   3020  100.0%        Function: ~<anonymous> /Users/ashtuchkin/code/iconv-lite/test/performance.js:1:11\n   3020  100.0%          LazyCompile: ~Module._compile module.js:367:37\n   3020  100.0%            LazyCompile: ~Module._extensions..js module.js:465:37\n```\n. @mithgol yes, I think its the same one. Thanks! It seems we'll have to wait for the fix.\n. Just tested the 0.11.14-pre (https://github.com/joyent/node/commit/9452ea2ef5bbaf4c0b03764c53899d97d0f210f3) and it works great, even faster than before. So, just waiting for 0.11.14 (or 0.12.0) release.\n\n```\nwindows-1251 charset performance test:\n\nEncoding 262144 chars 10000 times:\niconv: 24599ms, 104.07 Mb/s.\niconv-lite: 6106ms, 419.26 Mb/s.\n\nDecoding 262144 bytes 10000 times:\niconv: 23217ms, 110.26 Mb/s.\niconv-lite: 9683ms, 264.38 Mb/s.\n```\n. I've created a [wiki page](https://github.com/ashtuchkin/iconv-lite/wiki/Supported-Encodings) for it linked from readme.\n\nBasically, all encodings from your module are supported except cp808. I'll add it soon.\n. Added codepage 808 & released to npm in version 0.4.1.\n. Hi Carlos! Why do you think it is needed? What is your use case? I see something like this `str = iconv.decode(buf)` to be misleading.\n. Yes, I think explicitness here is better for the users. Thank you for your efforts!\n. That makes sense, thank you!\n. Published to npm in v0.4.2.\n. I was waiting for someone who needs it :) (see #56). I'll try to implement it in the next couple of days, don't bother. Just out of curiosity, where do you need that?\n. I've added both UTF-16 and UTF-16BE in new version 0.4.3 (on NPM).\n\n``` javascript\n// == UTF-16 codec =============================================================\n// Decoder chooses automatically from UTF-16LE and UTF-16BE using BOM and space-based heuristic.\n// Defaults to UTF-16BE, according to RFC 2781, although it is against some industry practices, see\n// http://en.wikipedia.org/wiki/UTF-16 and http://encoding.spec.whatwg.org/#utf-16le\n// Decoder default can be changed: iconv.decode(buf, 'utf16', {default: 'utf-16le'});\n\n// Encoder prepends BOM and uses UTF-16BE.\n// Endianness can also be changed: iconv.encode(str, 'utf16', {use: 'utf-16le'});\n```\n\nLet me know how it goes and if you need anything else.\nI can also create a UTF-32 encoding, please create another issue about it if you need that.\n. No problemo amigo)\n. Thanks!\n. Hi! \n\nI haven't checked it manually, but I think most names that are used in the real world will be interpreted correctly. First, names are not case sensitive in iconv-lite and all non-alphanumeric characters are stripped, as well as what seems like a year suffix (i.e. `ISO_8859-1:1987` = `ISO8859-1` = `iso88591`). Then, there are lots of aliases, for example `win1251` = `windows1251` = `1251` = `Windows-1251`.\n\nIf you have a limited set of encodings that you support, you can check them all with `iconv.encodingExists(name)` function. Then, you can either tell me to add an alias, or do it in your project with the following code:\n\n```\nvar iconv = require('iconv-lite');\niconv.encode('utf8', ''); // load encoding definitions\niconv.encodings['<new alias name>'] = '<alias to what>'; // both lowercase, alphanumeric.\n```\n\nIf, on the other hand, you want to support all IANA names and aliases, then, probably, iconv-lite is not an option because it doesn't support all the older encodings (neither do node-iconv AFAIK).\n. Ok, I see there are two UTF-7 formats - one original ([RFC2152](https://tools.ietf.org/html/rfc2152)) and the other modified, used in IMAP [RFC3501 sec5.1.3](http://tools.ietf.org/html/rfc3501#section-5.1.3).\n\nDo you mind if I name them UTF-7 and UTF-7-IMAP (as was done f.ex. in http://weldon.whipple.org/sendmail/utf7imap/)?\n. Published as v0.4.4 on npm.\n. I don't quite understand when it could be useful compared to a callback. Do you think several invalid characters could be handled in a smarter way if handled together?\n\nThe analogy to `String#split` seems a bit odd for me, especially when you need to check `index%2` to get if the string is valid or invalid. Moreover, invalid portions will need to be in Buffer-s, as we couldn't convert them to js string.\n\nIf there is really a smarter way to handle multiple invalid chars as opposed to single, then I'd suggest using the same callback, but passing multiple invalid bytes to it as a Buffer. What do you think?\n. Ok, now I got you, thanks for the explanation! \n\nWell, this would require additional code that does just this in all codecs. It's rather big time investment for me, plus burden to all future codecs, so I'm a bit reluctant to implement it without a hard use case.\n\nThe use case you described above is mostly solved by a callback, with the exception of single valid char surrounded by invalid, which I believe is a rare case (that the format supports and recommends it).\n. I'm still not convinced in the _usefulness_ of this functionality and it will not only increase code complexity/size and API surface, but also increase the burden of adding each new encoding, even if you write all the code for now. IMHO it's just not worth it until we find a compelling use case.\n. Ok, but why can't you do this using callbacks? Each range of non-encodable\ncharacters will be given to this callback and it can either throw or return\na string to replace them, similar to `.replace(regexp, function)` in\njavascript. This would also allow transliteration and checking that all\ncharacters are encodable. A much more flexible mechanism, which is also\nfits nice with streaming mode.\n\nWhen the scheme stabilizes, you can also make it into its own codec. Or I\ncan generalize it and make a meta-codec that will take basic encoding,\nescaping rules and escaping encoding as parameters.\n\nWhat do you think?\n\n## \n\nAlexander Shtuchkin\n\nOn Thu, Jul 17, 2014 at 5:08 AM, Mithgol notifications@github.com wrote:\n\n> In a nutshell this use case is a generalization of the UTF-7's use case:\n> the Unicode characters are forced to some 8-bit medium (defined by\n> a single-byte encoding) instead of UTF-7's original 7-bit medium.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/73#issuecomment-49298302\n> .\n. I'm thinking about the latter case, that would be more convenient and probably faster too. There is one issue though - as iconv-lite is stream-oriented and working chunk-by-chunk, I would not want to accumulate ranges of invalid characters, because we can run out of memory on very long invalid streams.\n\nSo I'm thinking about the following interface:\n\n``` javascript\nfunction unencodableHandler(str, offset, context, rangeStarted, rangeFinished) {\n  // str is a string of unencodable characters.\n  // offset - index of str in context.\n  // context is the whole string (chunk) currently encoded.\n  // rangeStarted flag is true when str starts a range of contiguous invalid characters.\n  // rangeFinished flag is true when str completes the range.\n  // Flags are always true in non-streaming usage.\n\n  // Return characters that can be translated thus far.\n  return (rangeStarted ? start_escape : \"\") + escape(str) + (rangeFinished ? end_escape : \"\");\n}\n\n// Convert a string - all unencodable strings will be complete.\nbuf = iconv.encode(str, \"cp866\", {unencodableHandler: unencodableHandler});\n\n// Or a stream\ninputStream.pipe(iconv.encodeStream(\"cp866\", {unencodableHandler: unencodableHandler})).pipe(outStream);\n```\n. Sorry, not much free time recently. I do remember about it and will fix eventually.\n. Hi Ribhararnus! We use core nodejs functions to convert to/from utf8, so this would be hard to implement. And I would think it's a bit out of scope of this project. Thanks for suggestion!\n. Hi, yes I did, thank you. Although even if it was defined, it would throw anyway because there's an \"unknown table value when decoding\".  Could you post the buffer where it happens?\n. No problem, I don't need any private code. Anyway, I've fixed reference error, could you test the master branch before I publish it to npm?  `npm install ashtuchkin/iconv-lite` will install it.\n. Yes, seems that you're giving it array of objects, not buffer. I'll release this change to npm later if you don't mind, as it's pretty minor issue. Thanks!\n. Hi! There is no limit. Moreover, iconv-lite is designed to be stream-first, so it can process gigabytes without choking.\n\nI tried to reproduce your error, but to no avail. I used this code, it worked correctly:\n\n``` javascript\n    grunt.registerTask('test', \"test\", function() {\n        var content = grunt.file.read('public/js/jquery.js')\n        console.log(content)\n    });\n```\n\nSize of jquery is ~200 kb. Node v0.10.31, iconv-lite v0.2.11, grunt v.0.4.5\n\nCan you provide a test case?\n. No problem, thank you for re-checking it)\n. Segfault in javascript-only module (which iconv-lite is) has a strong smell of core node or V8 problem. Oh! Here it is: joyent/node#8208\n\nI confirm the problem in Node v0.10.31 and that it's fixed in v0.10.32-pre, after joyent/node#8224. We just need to wait for this release. For now, the only option is to downgrade to 0.10.30. Thank you!\n. Could you post:\n- the contents of the file you convert\n- contents of 'data' variable in your first script?\n- version of node and iconv-lite you use\n. Although I probably know what the problem is. The file you read has a [BOM character](http://en.wikipedia.org/wiki/Byte_order_mark) in the beginning. Unfortunately, Node.js keeps it by default  (see joyent/node#1918). Then, GBK encoding has no code for the BOM character, so iconv-lite replaces it by '?'.\n\nSolutions:\n1. Add `data = data.replace(/^\\uFEFF/, '');` before encoding.\n2. Use ['strip-bom' module](https://github.com/sindresorhus/strip-bom) for more readability.\n\nLet me know if it helps.\n. Are you using node v0.10.31? If so, you should upgrade, see #77\n. Closing. Feel free to reopen if the issue persists.\n. Ok, then try to:\n1. Upgrade node to latest 0.10 and recheck.\n2. Replace all `iconv.decode(buf, 'utf-8')` with `buf.toString('utf8')` - is it still crashing?\n. Hi!\nMy initial guess is that although you convert content encoding to (probably) utf8, there is still a `<meta http-equiv=\"content-type\" content=\"text/html; charset=Shift_JIS\">` inside the html, which makes Chrome think that this utf8 is actually shift_jis and decode it one more time. Can you check if this is the case?\nIf yes, you'll need to filter out this line. If no, I'll need at least your input/output files and preferably a small separate test case that shows this behavior.\n. Maybe its an issue with concat-stream module? BTW, you could use iconv-lite's concating capability:\n\nsrc.pipe(iconv.decodeStream('win1251')).collect(function(err, body) {...})\n. Hi! This is one of the planned use cases for user callback feature (#53). \nFor now you can workaround by just replacing illegal chars (\ufffd) with empty string.\n. Thank you!\n. Well, iconv-lite doesn't decode utf8 itself, it uses Node core alg for performance reasons.\nI'm not sure its wise to reimplement it and likely lose performance just for this.\nAs a workaround, you can check for the presence of character \ufffd in result, although it could be a false positive. Another workaround is to use node-iconv.\n. Like I said above, at the moment, the only way to do that is to check for the presence of character \ufffd in result. The thing is, I don't control UTF-8 decoding - it's done by Node.js. I could reimplement it in javascript, but I think it'd be a lot slower.\n. \"\ufffd\" is used for decoding, \"?\" for encoding (as many encodings cannot\nrepresent \ufffd).\n\n## \n\nAlexander Shtuchkin\n\nOn Mon, May 25, 2015 at 10:57 PM, Benjamin Pasero notifications@github.com\nwrote:\n\n> @ashtuchkin https://github.com/ashtuchkin got it. But in the docs you\n> say \"Untranslatable characters are set to \ufffd or ?\", so it might also be \"?\"\n> which I cannot really check for because its a valid character.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/83#issuecomment-105406513\n> .\n. 1. Are you talking about encoding or decoding? I thought decoding, right?\n2. Uft8 is still outside my control.\n   On May 26, 2015 05:19, \"Benjamin Pasero\" notifications@github.com wrote:\n\n> Right, but \"?\" is ambiguos because it might also be a character used in\n> the original text. I think raising some flag or error would make more sense\n> to catch this case.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/83#issuecomment-105504072\n> .\n. Yeah, that's a completely separate issue from \"ability to validate UTF-8\nencoding\" :)\n\nI have a callback-like mechanism in mind (see #53), where you can either\nthrow exception or define replacements for characters that cannot be\nrepresented. Still haven't got a moment to implement it though.\u200b\n\n## \n\nAlexander Shtuchkin\n\nOn Tue, May 26, 2015 at 10:25 AM, Benjamin Pasero notifications@github.com\nwrote:\n\n> In our case, we use encode() to convert an existing file to another\n> encoding. E.g. we have a source UTF-16 file that we want to save as DOS\n> encoding. DOS can not present all encodings (Chinese et. al), so I would\n> like to show an error to the user. Similar to how in Sublime Text you will\n> not be able to save a file in an encoding that cannot represent all\n> characters you have before encoding.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/83#issuecomment-105611321\n> .\n. Upgrade node.js. See #77\n. There are a lot of interpretations of what exactly \"big5\" encoding is. I used the one of [Encoding Standard](https://encoding.spec.whatwg.org), it's basically cp950 with the HKSC and other common extensions. There, both A451 and A2CC are valid encodings of '\u5341'.   See [more explanations in comments](https://github.com/ashtuchkin/iconv-lite/blob/master/encodings/dbcs-data.js#L127).\n\nDoes it give you any problems?\n. Released a 0.4.6 with a fix for it. Thank you!\nIt was easier just to remove the dots in alias-defining file.\n. Interesting. Although I'm sure it can be done without additional generated file.\nCould you describe in more detail what you mean by \"ASCII-compatible\" charset? How that'll save you decoding? In my view, utf8 is not ASCII-compatible at all)\n. Got it. Makes sense.\nI'm thinking to reuse current caching architecture, but avoid making it part of public api yet (it'd require tests, docs, etc.). What do you think about `iconv.getCodec(encodingName).asciiCompatible`? It's cached, but must load the data for codec once.\n. Unfortunately this breaks the encoding search algorithm (as you can see in the travis report). Much easier to just pre-strip aliases. (see my commit https://github.com/ashtuchkin/iconv-lite/commit/b832d8fb3e5b335f89b6fe801ac26c0897818ad7)\n. This is so bad) You'd screw up any `for (in)` cycle with this. If the method is still needed, I'd recommend creating it as a non-enumerable property, like that:\n\n``` coffeescript\nObject.defineProperty Object.prototype, \"permits\", {enumerable: false, value: -> \"\"}\n```\n\nAnyway, I made it work even with enumerable prop with soon-to-be-released version 0.4.7.\n. I wasn't aware of that trick, thank you! We should probably add 0.12 there as well (I'll do that).\n. I agree that we should keep the tests as long as it's not a lot of hassle. Thank you Doug!\nThis is wonderful to have a supportive community)\n. Looks like iconv removed support for a couple of cp encodings. I tried upgrading before, but got the same errors.\n. Note, this is fixed in 14cf0cacb84e21a6d32efdda74e7ee90006dae34 after I changed tests to skip encodings that are not supported in node-iconv in ff06d8a4a4ad9ab66aa8560c789cabc5985dcaef.\n. Added bnoordhuis/node-iconv#118 to get these encodings back.\n. No, it's a bad idea. Please read https://github.com/ashtuchkin/iconv-lite/wiki for explanation about how to use iconv-lite correctly.\n. Published new version 0.4.8 with this alias added.\n. It should be.\nOn Apr 24, 2015 03:35, \"khalilTN\" notifications@github.com wrote:\n\n> Is this already available on npm ?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/94#issuecomment-95885314\n> .\n. Seems that the character table loading is screwed up, but I need more information to understand what's going on. Do you use browserify to package Chrome app? Can you isolate the bug in a separate mini-project so that I could reproduce it?\n. Thanks for figuring out what happened, that was weird)\n. Yes, it's used only when creating lookup tables, at the time of codec creation (first usage of encoding), for performance reasons.\n\nAs a temporary workaround you can clean the codec cache every time before changing defaultCharSingleByte:\n\n``` javascript\niconv._codecDataCache = {};\niconv.defaultCharSingleByte = <new value>;\n```\n\nI don't intend to change this behavior as this will be a non-issue once #73 is fixed, so closing for now. Feel free to reopen if you have other suggestions.\n. Hey Benjamin, thanks, and thanks for the suggestion. Do you have any interface ideas in mind? How do you see it in code (from your side, not internally)?\n\nAt the moment we have completely separate operations of decode vs encode, with only strings between them, so I don't really know how to pass this \"if file had a BOM\" information. We shouldn't keep the BOM in decoded strings, right?\n\nMoreover, right now for UTF8 and UTF16 we pass the data straight to native Node.js implementation, so it might be a bit tricky to handle, although probably possible.\n. Thanks, that makes sense. So basically decode will always remove BOM and all decoded strings would not have it, plus encode would add BOM if required by encoding name, right? I'm also thinking about adding a flag instead of adding encoding names, what do you think about that?\n\nAlso, can you describe your use case a bit more, so that I could understand context, if it's not secret of course? Are you working on a text editor?\n. Got ya. I love how Microsoft reinvents itself recently. You rock!\n\nGive me a couple of days, I'll add it.\nOn May 21, 2015 23:51, \"Benjamin Pasero\" notifications@github.com wrote:\n\n> Basically I would argue that a string should never ever include a BOM\n> because I cannot think of a use case where you would want the BOM in the\n> string. So, decode() looks at BOMs and removes them because it returns a\n> string.\n> \n> Encode should add the BOM if specified (either by option or encoding name).\n> \n> In many cases you would want a BOM for UTF16LE and UTF16BE anyways.\n> Because tools cannot know which encoding to use by looking at the bytes. I\n> believe UTF16 without BOM does not make a lot of sense.\n> \n> I am working for Visual Studio Code (https://code.visualstudio.com/) and\n> iconv-lite is being used for all encoding matters.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/97#issuecomment-104540648\n> .\n. Hey, I've published v0.4.9 today, would love to hear feedback)\n. Cool, nice to hear that!\n\n## \n\nAlexander Shtuchkin\n\nOn Wed, May 27, 2015 at 10:32 AM, Benjamin Pasero notifications@github.com\nwrote:\n\n> @ashtuchkin https://github.com/ashtuchkin added to our editor and\n> things are smooth so far. thanks for the prompt implementation and\n> responsiveness!\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/97#issuecomment-106006077\n> .\n. AFAIK it's more often used in practice (f.ex. Windows defaults to LE), plus\nit's native to Node.js and x86 architecture.\nOn May 26, 2015 7:49 PM, \"Douglas Christopher Wilson\" <\nnotifications@github.com> wrote:\n\n> Hi! I was looking around at the recent changes and recent closed issues,\n> but wasn't able to find the answer to this question:\n> \n> Why was the default UTF-16 encoding changed from BE to LE (when there is\n> no BOM, and of course, the content contains no space characters, which are\n> atypical in JSON content)?\n> \n> Yes, I know I can alter the default, but I was just seeking for the reason\n> behind the change so I can evaluate what I want to do when upgrading to the\n> latest version of this module.\n> \n> Thanks!\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/98.\n. BTW, You've got me thinking, why do we use only spaces? I think I'll add ASCII chars into consideration as well - that would be much more interesting.\n. Just released v0.4.10 with the improved heuristic (taking into account any ASCII chars, not just spaces). Now, JSON or not, the default encoding should be not really important.\n. It seems Node v0.12 doesn't support encoding as a second argument ([see doc](https://nodejs.org/api/fs.html#fs_fs_createreadstream_path_options)), only options dictionary. \n\nShould work fine when called like you wrote in the first post (`fs.createReadStream(fileName,{encoding:'gbk'})`). Let me know if it doesn't work when called that way.\n. Hmm, just checked and it works for me. Can you remove `iconv.undoExtendNodeEncodings()` from your function? The actual read is happening after you return from function, so you shouldn't do this until an 'end' or 'close' event.\n. Yeah, you shouldn't call `iconv.undoExtendNodeEncodings();` before the decoding starts :) Glad you worked it out.\n. Sorry I don't understand. Could you elaborate?\nOn Jun 8, 2015 8:56 PM, \"limeng\" notifications@github.com wrote:\n\n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/100.\n. A sample is on the home page of this project (\nhttps://github.com/ashtuchkin/iconv-lite). Basically you do `str =\niconv.decode(buf, '<encoding>');`, where `buf` is the raw data you read\nfrom TCP socket (type Buffer). Most popular Chinese encoding is 'gbk', but\nthere could be some others as well, you'll need to check it.\n\n## \n\nAlexander Shtuchkin\n\nOn Tue, Jun 9, 2015 at 12:08 AM, limeng notifications@github.com wrote:\n\n> How do I decode chinese?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/100#issuecomment-110255064\n> .\n. Yeah, I see you have a network error, then try to decode a 'null' buffer. You should check that the buffer is valid before calling `iconv.decode()`.\n. The nodejs utf8 decoder should support it afaik. Could you try it?\nOr do you need an encoder?\nOn Jul 3, 2015 11:22, \"Douglas Christopher Wilson\" notifications@github.com\nwrote:\n\n> Hi! So I really need to get CESU-8 implemented somewhere for use with old\n> crap like various databases. Would you be willing to add CESU-8 support to\n> this module :)?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/102.\n. I think I saw references to CESU-8 in the Node code, so I tend to think it\nwas intentional.\n\nAbout the encoder - if you need it quickly, you could use something like\nthis:\n\n```\nbuf = Buffer.concat(str.split('').map(function(s) {return new Buffer(s);}));\n```\n\nBasically, CESU-8 is just a UTF-8 with UTF-16 surrogates encoded as a\nseparate chars.\n\nI'll also add this to the encoding wish list and get to it when I have\ntime. Deal? :)\n\n## \n\nAlexander Shtuchkin\n\nOn Fri, Jul 3, 2015 at 12:23 PM, Douglas Christopher Wilson <\nnotifications@github.com> wrote:\n\n> I need both an encoder and a decoder (because I need to write to the\n> databases, not just read). The Node.js UTF-8 decoder does support CESU-8,\n> yes. This is more about an encoder (though I believe Node.js utf8 only\n> supports CESU-8 out of coincidence, rather than intention).\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/102#issuecomment-118410982\n> .\n. Works for me... Node v0.12.4\n\n```\n> str = \"\\uD801\\uDC37\"\n'\ud801\udc37'\n> buf = Buffer.concat(str.split('').map(function(s) {return new\nBuffer(s);}));\n<Buffer ef bf bd ef bf bd>\n```\n\n## \n\nAlexander Shtuchkin\n\nOn Fri, Jul 3, 2015 at 1:06 PM, Douglas Christopher Wilson <\nnotifications@github.com> wrote:\n\n> I tried that trick but it does not work. Node replaces all the surrogate\n> pairs with the Unicode replacement character...\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/102#issuecomment-118419191\n> .\n. Ahh, yeah, that's the different character, sorry.\n\n## \n\nAlexander Shtuchkin\n\nOn Fri, Jul 3, 2015 at 1:32 PM, Alexander Shtuchkin ashtuchkin@gmail.com\nwrote:\n\n> Works for me... Node v0.12.4\n> \n> ```\n> > str = \"\\uD801\\uDC37\"\n> '\ud801\udc37'\n> > buf = Buffer.concat(str.split('').map(function(s) {return new\n> Buffer(s);}));\n> <Buffer ef bf bd ef bf bd>\n> ```\n> \n> ## \n> \n> Alexander Shtuchkin\n> \n> On Fri, Jul 3, 2015 at 1:06 PM, Douglas Christopher Wilson <\n> notifications@github.com> wrote:\n> \n> > I tried that trick but it does not work. Node replaces all the surrogate\n> > pairs with the Unicode replacement character...\n> > \n> > \u2014\n> > Reply to this email directly or view it on GitHub\n> > https://github.com/ashtuchkin/iconv-lite/issues/102#issuecomment-118419191\n> > .\n. Fixed in e75dacaede131585e5e07268ae1467f40f02f990, published on npm as v0.4.11. Thanks for getting my attention to it!\n. Cool)\n. You could create an encoding definition JSON and add it to `iconv.encodings`, something like:\n\n``` javascript\niconv.encodings['big5uao'] = {\n  type: '_dbcs',\n  table: <your correspondence table>\n}\n```\n\nBefore matching encoding name, it's lowercased and stripped of all non-alphanum characters, so you should do that too when adding field to `iconv.encodings`.\n\nKey points in code:\n- https://github.com/ashtuchkin/iconv-lite/blob/master/lib/index.js#L61\n- https://github.com/ashtuchkin/iconv-lite/blob/master/encodings/dbcs-data.js#L160\n- https://github.com/ashtuchkin/iconv-lite/tree/master/encodings/tables\n- https://github.com/ashtuchkin/iconv-lite/blob/master/generation/utils.js#L59  (table format)\n. That's a large work, thank you so much for doing it! I'll look into it asap.\n. Just tried it and I have a different `too-large-conv.html` produced. What version of node do you use?\n. Tried node versions 0.12.4, 0.11.13, 0.12.7, io.js v3.3.0, all worked well (too-large.html and too-large-conv.html were the same). Maybe architecture is different? Are you using x86 or x64? Can you give more info on your machine specs?\n. Hmm.. All my test machines are mac/Ubuntu, so probably related to Windows.\n\nMy guess is that line endings are changed somewhere on load/save file, on\nNode level or on OS level.\n\nCan you write a function that checks line endings in Buffer-s before and\nafter iconv-lite conversion?\n\nAnother test - can you skip conversion for 'too-large', just load - copy\nbuffer - save?\n\nThird test - can you try io.js latest? They use newer version of V8 & libuv\nthat could have this issue resolved.\nOn Sep 4, 2015 4:27 AM, \"bit-seq\" notifications@github.com wrote:\n\n> The problem is still given, Windows 10 (updated from Windows 7), Node\n> v0.12.7 x64.\n> On a notebook I too wasn't able to reproduce the raised behaviour. I\n> attempted to run the decoding with Node v012.7 x64 on fresh installed\n> operating systems Ubuntu 14.04.3 and Windows 10.\n> \n> Any hints to look for?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/104#issuecomment-137710911\n> .\n. Ok, so I found the problem here. The issue is a node bug and documented as [node#1024](https://github.com/nodejs/node/issues/1024). Affected Node versions: v0.11.7 - v1.4.3. Fixing commit: [1640dedb3b2](https://github.com/nodejs/node/commit/1640dedb3b2a8d6e54ba7b22290d86d5984768be)\n\nA bit more context: when decoding, iconv-lite fills out ucs2-encoded Buffer that then is converted to JS string. The linked issue prevented correct conversion at this step.\n\nThe fix is to just upgrade node. Thanks for creating a reproducible test case.\n. Thanks @Mithgol! I fixed the CESU-8 and added warnings for extendNodeEncodings. Seems we cannot do anything at the moment with the latter issue.\n. Thanks for reporting it! Fixed in 15f2554c97c79f259ccbdc95a19209f25d13f217, will soon push to npm.\n. I guarantee :)\n\n## \n\nAlexander Shtuchkin\n\nOn Sun, Sep 27, 2015 at 8:12 PM, Douglas Christopher Wilson <\nnotifications@github.com> wrote:\n\n> Awesome, thank you, @ashtuchkin https://github.com/ashtuchkin !\n> \n> P.S. whatever happens in #107\n> https://github.com/ashtuchkin/iconv-lite/issues/107, please do not even\n> think about unpublishing 0.4.12 from npm, pretty please, as I am publishing\n> a module that depends on 0.4.12 :)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/106#issuecomment-143628598\n> .\n. What would you suggest as a solution/fix?\n\nIt was a hack from the beginning and we have no guarantees about something\nnot being broken with new Node changes. I also have no idea how to describe\nthat \"It doesn't work for `new Buffer`, but probably works for other\nencoding-related operations\".\n\nI'm open for discussion, but deprecation seemed the most valid option here.\nOn Sep 26, 2015 22:13, \"Mithgol\" notifications@github.com wrote:\n\n> The method .extendNodeEncodings from iconv-lite v0.4.11 cannot be used\n> in Node.js v4 to change the behaviour of its Buffer() constructor (\n> nodejs/node#2835 https://github.com/nodejs/node/issues/2835).\n> \n> The method .extendNodeEncodings from iconv-lite v0.4.12 does not work in\n> Node.js v4 at all (85938db\n> https://github.com/ashtuchkin/iconv-lite/commit/85938dbcb46ed9c98d712c363380fe41b7613c73\n> ).\n> \n> These changes are breaking for the modules that used .extendNodeEncodings\n> but refrained from using Buffer() in Node v4.\n> \n> The only workaround for these modules is to declare \"iconv\": \"0.4.11\"\n> as a dependency (without ~ or ^ before 0.4.11, because they cannot\n> rely on semantic versioning http://semver.org/) as soon as possible.\n> \n> (The dependency \"iconv\": \"\n> https://github.com/ashtuchkin/iconv-lite/tarball/15f2554c97c79f259ccbdc95a19209f25d13f217\n> \" might be useful for the modules that need CESU-8 decoder as well.)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/107.\n. Silly mistake.. Thanks!\n. This is by design. We must set this to utf8 to avoid Node interfering with decoding process. Actual encoding is set inside the converter (`conv` parameter) in [line 15](https://github.com/ashtuchkin/iconv-lite/blob/master/lib/streams.js#L15)\n. \u200bIf the stream you're piping to \u200bonly supports buffers (encoding=null),\nthen I'd suggest the following:\n\n```\nsource.pipe(iconv.decodeStream(encoding)).pipe(iconv.encodeStream('utf8')).pipe(aws_upload_whatever)\n\n```\n\nThe rule of thumb here is the following - decode converts encoded binary to\njs string; encode converts js string to encoded binary. To convert binary\n-> binary, you need to decode, then encode.\n\n## \n\nAlexander Shtuchkin\n\nOn Fri, Oct 30, 2015 at 2:02 AM, Dominik Lessel notifications@github.com\nwrote:\n\n> Ah okay.\n> \n> I tried a streaming conversion of S3 objects (CSV files) and the\n> AWS.S3.upload() method, which accepts a stream object, is blowing up, when\n> used with iconv-lite. After forking and editing streams.js#L79 to accept encoding:\n> null it worked as expected.\n> \n> TypeError: Object foo|bar|baz has no method 'copy'\n> at Function.Buffer.concat (buffer.js:499:9)\n> at ManagedUpload.fillStream (/var/task/node_modules/aws-sdk/lib/s3/managed_upload.js:386:21)\n> at IconvLiteDecoderStream.<anonymous> (/var/task/node_modules/aws-sdk/lib/s3/managed_upload.js:169:28)\n> at IconvLiteDecoderStream.emit (events.js:117:20)\n> at _stream_readable.js:944:16\n> at process._tickDomainCallback (node.js:486:13)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/110#issuecomment-152464893\n> .\n. I kinda tried to cover that in [wiki](https://github.com/ashtuchkin/iconv-lite/wiki), but any suggestions welcome!\n. Nope.. Will add if there's a need (a couple people ask)\nOn Nov 5, 2015 06:46, \"Mithgol\" notifications@github.com wrote:\n\n> Does iconv-lite support EBCDIC?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/111.\n. If your case is not very performance-sensitive and not streaming, you can do something like this:\n\n```\nvar bomUsed = false;\nvar res = iconv.decode(buf, \"utf8\", {stripBOM: function() { bomUsed = true });\nif (!bomUsed)\n  res = iconv.decode(buf, \"1255\");\n```\n\nstripBOM parameter acts both as flag and as a callback that is called when BOM exists.\n\nIf streaming is required, then you can write small wrapper that would do the detection, similar to [StripBOMWrapper](https://github.com/ashtuchkin/iconv-lite/blob/master/lib/bom-handling.js#L28).\n. Thanks Daniel! Added the your module to BOM handling section of readme.\n. Thanks for the report! I'll look into it soon. \n. Sorry for big delay, just pushed a fix.. Fixed in recent commit. I used `encodeAdd` and `encodeSkipVals` options for dbcs codec, they were introduced just for this purpose. Also, I skipped the option of euro encoding of gbk - it's much easier without it, hope it's fine.. Also, table caching is not needed because it's only loaded once to convert to internal representation.. There seems to be a module for it: https://www.npmjs.com/package/superagent-charset\n. You're welcome :)\n. Sorry for the huge delay.\r\n\r\nSo what happens here is `writeStream` is the file write stream created by `fs.createWriteStream('out-win1250.txt')`, not encoding stream. This is because the `pipe()` method returns it's argument and not `this`. To make it work, you need to get the correct writeStream:\r\n```\r\nconst writeStream = iconv.encodeStream('win1250');\r\nwriteStream.pipe(fs.createWriteStream('out-win1250.txt'));\r\n```\r\n\r\nAlso, it's not recommended to use `.collect()` method together with piping data to another stream - the backpressure mechanisms will probably screw up. Only use `.collect()` at the end of the pipe.. Still cannot reproduce it :(  Tried the upgrade method you described and it worked without any errors.. \n\nLet me know if you're able to consistently reproduce it.\n. Guys I need your help here. I don't have an environment to reproduce it, so I'm counting on you to isolate the exact requirements for this to happen.\r\n\r\nI'm also not fond of changing that line to `iconv.encodings = require(\"iconv-lite/encodings\");` because that is a workaround for `require()` bug - it should be fixed on their side. By specification, require should work relative to the caller .js file and it's a much larger issue if it doesn't.. Thanks for the extended explanation, @Marak.\r\n\r\n> Generally, it's not safe to do a lazy require of modules.\r\n\r\nIt looks to me that generally, it's not safe to do a chroot of a node process, because node's module caching has unexpected results for dynamic requires. Are you sure other libraries/modules don't use it? It's a perfectly valid mode of loading modules in node.js, as far as I know.\r\n\r\n> Is there a reason that ../encodings.js file is being loaded lazily in getCodec?\r\n\r\nYes, it's an explicit optimization of startup time. Encodings tree is large and can take several hundred ms to load. I don't want to impose this tax on all projects indirectly requiring iconv-lite because they might not use (or even know about) it at all.\r\n\r\nI still don't know why it works the second time for @dbussert - module caching should not depend on that.. Dear @Marak,\r\n\r\nI'm yet to find evidence that others are experiencing the same issue as you are. All the other people in this thread have a situation where a restart \"fixes\" the problem, which should not happen with a chroot. Also I haven't seen any mention of chroot other than from you. So, I'm still gathering information about that case and would prefer you stop interfering and going personal about it. There's no need to preach about best practices, tell me what I should do, or talk like you represent all the other people in this thread.\r\n\r\nAs for your particular problem - no, I won't fix it for you. The core reason you have this problem is you built a system based on a subtly incorrect assumption that the node module system works correctly after a chroot. This subtle incorrectness showed itself when you started using iconv-lite, but it has potential to show for other modules as well. You cannot assume that other modules won't use a valid system interface because you think it's \"not a best practice\". I'm glad you found a workaround for your particular system, but 1) it's not guaranteed to work for other people, and 2) you'll need to find similar workarounds for other libraries/modules. I would've recommended to stop relying on the assumption above and try to fix your system in a more general way, like using a separate process to do the chroot and only start your original node process after that, but I don't have enough context about your system.\r\n\r\nTo be clear, I'm not being stubborn about making the change you suggested. I'm not doing that because:\r\n 1. As I said earlier, I don't have evidence that this what's actually happening for the OP and other people in this thread.\r\n 2. Each part of the current solution is there for a good reason:\r\n    1. Dynamic require provides the dont-pay-if-you-dont-use guarantee for users with regards to startup time, as we discussed previously.\r\n    2. Relative require path guarantees that I'm requiring file from the same exact module. As you might already know, there can be several versions of a module installed in a node_modules folder and I have no idea which one of them will be resolved to a top-level 'iconv-lite'. I hope I don't have to explain why this requirement is important.. Hello Allen! Please refer to \"iconv\" package maintainer. This is a\ndifferent package named \"iconv-lite\".\n\nAlex\nOn May 4, 2016 02:32, \"allen.hu\" notifications@github.com wrote:\n\n> Before the node v6.0.0, the repository\n> https://github.com/hujb2000/easynode-ipc.git , Do npm install is always\n> ok , but when I upgrade node to the last version v6.0.0, when npm install\n> it console such logs,\n> Can you help me ?\n> \n> npm ERR! Darwin 15.2.0\n> npm ERR! argv \"/Users/hujiabao/.nvm/versions/node/v6.0.0/bin/node\"\n> \"/Users/hujiabao/.nvm/versions/node/v6.0.0/bin/npm\" \"update\"\n> npm ERR! node v6.0.0\n> npm ERR! npm v3.8.6\n> npm ERR! code ELIFECYCLE\n> \n> npm ERR! iconv@2.1.11 install: node-gyp rebuild\n> npm ERR! Exit status 1\n> npm ERR!\n> npm ERR! Failed at the iconv@2.1.11 install script 'node-gyp rebuild'.\n> npm ERR! Make sure you have the latest version of node.js and npm\n> installed.\n> npm ERR! If you do, this is most likely a problem with the iconv package,\n> npm ERR! not with npm itself.\n> npm ERR! Tell the author that this fails on your system:\n> npm ERR! node-gyp rebuild\n> npm ERR! You can get information on how to open an issue for this project\n> with:\n> npm ERR! npm bugs iconv\n> npm ERR! Or if that isn't available, you can get their info via:\n> npm ERR! npm owner ls iconv\n> npm ERR! There is likely additional logging output above.\n> \n> npm ERR! Please include the following file with any support request:\n> npm ERR! /Users/hujiabao/workspace_docker/icp/easynode-ipc/npm-debug.log\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/119\n. Glad you solved it :)\n\n## \n\nAlexander Shtuchkin\n\nOn Wed, Jun 8, 2016 at 5:00 PM, Andrey Sidorov notifications@github.com\nwrote:\n\n> Closed #120 https://github.com/ashtuchkin/iconv-lite/issues/120.\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/120#event-686636164, or mute\n> the thread\n> https://github.com/notifications/unsubscribe/AAmVHVTawTnsGqvcWai7u3jIUGmol_atks5qJ1epgaJpZM4IxgPU\n> .\n. Yep ok, good luck :)\n\n## \n\nAlexander Shtuchkin\n\nOn Wed, Jun 8, 2016 at 5:03 PM, Andrey Sidorov notifications@github.com\nwrote:\n\n> my main issue is not solved, but at least I know now that I don't need\n> iconv to convert from utf8mb4, it's just utf8 - so nothing to do with\n> iconv-lite :)\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/120#issuecomment-224767252,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAmVHe3QToprgMqU40_UEnrMeWzs6SsZks5qJ1hngaJpZM4IxgPU\n> .\n. async is only a dev dependency and only used for '.parallel' method, and I don't see any breaking changes there. Am I missing something?\n. Ok, thanks anyway :)\n. Thanks! And sorry for delay.. hmm.. just installed node v6.3 and run `npm test` and it looks fine.. (needed to update iconv to latest version) Are you having any problems with it?\n. Also see https://nodejs.org/api/buffer.html#buffer_buf_index\n. No problem :)\n\n## \n\nAlexander Shtuchkin\n\nOn Fri, Aug 5, 2016 at 1:37 AM, Alan Plum notifications@github.com wrote:\n\n> Okay, strange. I was just able to verify that this actually works in\n> Node.js 6 just fine. I was debugging buffer issues yesterday and was unable\n> to write to buffers using the subscript operator but it must have been\n> caused by something else. Sorry for the noise.\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/123#issuecomment-237789308,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAmVHb7rIni2-CkXAiBDFXvrqw95Q_STks5qcvZJgaJpZM4JcfhC\n> .\n. I needed to fix some tests to make it work, but now it's merged into master in #136. Hey! Are these completely new encodings or just aliases to existing ones?\nBtw, iconv-lite supports macroman directly.\n\n## \n\nAlexander Shtuchkin\n\nOn Wed, Aug 24, 2016 at 2:47 AM, Andrey Sidorov notifications@github.com\nwrote:\n\n> hi,\n> I'm adding support for non-utf encorings to mysql2 (\n> sidorares/node-mysql2#374\n> https://github.com/sidorares/node-mysql2/pull/374 ) and currently\n> trying to map between mysql charsets and iconv encoding names. Some\n> encodings seem to be missing ( macroman, keybcs2 ) for example.\n> \n> It seems that all of them are described here - https://github.com/twitter/\n> mysql/tree/master/sql/share/charsets\n> \n> Would you be interested in adding them? Perhaps script to import this data\n> automatically? Is there a documentation on how to add new encoding table to\n> iconv-lite?\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/125, or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAmVHc7P2tDjejdbVKNcoxQpnLgn7W_Nks5qjBMogaJpZM4Jr1CL\n> .\n. That's an obscure encoding :) https://en.wikipedia.org/wiki/Kamenick%C3%BD_encoding\nGoogling it shows mysql codebase and ~7000 old pages, which is pretty small, so I'm not sure it makes sense to add it to iconv-lite directly. \n\nHave no fear though! You can append encoding from outside of iconv-lite, in your own code, like this:\n\n``` js\nvar iconv = require('iconv-lite');\niconv.getCodec('ascii'); // Lazy-load existing encodings to fill out iconv.encodings dict.\n\n// Now add new encodings\n// key in this dictionary is lowercase name of encoding with all non-alphanumeric symbols stripped\n// (i.e. name might be \"abc-XYZ\", then the key is \"abcxyz\", see more examples in /encodings directory)\niconv.encodings[\"myencoding\"] = {\n        \"type\": \"_sbcs\",  // Single-byte encoding.\n        // \"chars\" is either 256 characters of encoding, or just top 128, if lower 128 is same as ascii.\n        \"chars\": \"\u00c4\u0100\u0101\u00c9\u0104\u00d6\u00dc\u00e1\u0105\u010c\u00e4\u010d\u0106\u0107\u00e9\u0179\u017a\u010e\u00ed\u010f\u0112\u0113\u0116\u00f3\u0117\u00f4\u00f6\u00f5\u00fa\u011a\u011b\u00fc\u2020\u00b0\u0118\u00a3\u00a7\u2022\u00b6\u00df\u00ae\u00a9\u2122\u0119\u00a8\u2260\u0123\u012e\u012f\u012a\u2264\u2265\u012b\u0136\u2202\u2211\u0142\u013b\u013c\u013d\u013e\u0139\u013a\u0145\u0146\u0143\u00ac\u221a\u0144\u0147\u2206\u00ab\u00bb\u2026 \u0148\u0150\u00d5\u0151\u014c\u2013\u2014\u201c\u201d\u2018\u2019\u00f7\u25ca\u014d\u0154\u0155\u0158\u2039\u203a\u0159\u0156\u0157\u0160\u201a\u201e\u0161\u015a\u015b\u00c1\u0164\u0165\u00cd\u017d\u017e\u016a\u00d3\u00d4\u016b\u016e\u00da\u016f\u0170\u0171\u0172\u0173\u00dd\u00fd\u0137\u017b\u0141\u017c\u0122\u02c7\"\n};\n\n// Also you can add encoding aliases\niconv.encodings[\"myawesomeencoding\"] = \"myencoding\";\n\n// Now you can use the new encodings to actually encode/decode stuff.\nbuf = iconv.encode(\"myencoding\", \"input string\");\n```\n\n(Alternatively, you can say that you don't support it as it's very obscure :) )\n. Hey Lucas,\n\nIt's a question to `request` library, not `iconv-lite`. I don't know the\ninternals of `request`, sorry.\n\n## \n\nAlexander Shtuchkin\n\nOn Fri, Aug 26, 2016 at 2:31 PM, Lucas Pelegrino notifications@github.com\nwrote:\n\n> Hey guys,\n> \n> This is more of a question. I'm pipeing one request and doing the decoding\n> myself like this:\n> \n>   // decodes\n>   .on('response', function(resp){\n>     var charset = extractCharsetFromStr(resp.headers['content-type']),\n>         encoding = resp.headers['content-encoding'],\n>         decodedRes;\n> \n> ```\n> // unzip\n> if(encoding == 'gzip' || encoding == 'deflate')\n>   decodedRes = resp.pipe(encoding == 'gzip' ? zlib.createGunzip() : zlib.createInflate())\n> else\n>   decodedRes = resp\n> \n> // charset\n> if(charset != 'utf-8' && iconv.encodingExists(charset)){\n>   return decodedRes.pipe(iconv.decodeStream(charset))\n> }\n> ```\n> \n>   })\n> \n> But this is not modifying the response body in the callback request({},\n> (err, resp) => {}), my question is, if I listen to the 'data' and\n> 'finish' event and buffer the body myself, will request still buffer the\n> body on it's on if I don't pass the callback? I don't want to hold two\n> objects like these in memory.\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/126, or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAmVHddWXb0eYTgLsqQKxuIbE93QRpZXks5qj1tEgaJpZM4JufBb\n> .\n. Hmm, I checked it and it looks like `gb18030-ranges.json` is a perfectly valid json file. I think it's either a file corruption on your drive, or a bug in babel plugin. Never seen this error before. Let me know if you need more help.. windows-1251 cannot represent kanji :) It's a single-byte encoding\nthat has codepoints only for latin and cyrillic letters.\n\nSee: https://en.wikipedia.org/wiki/Windows-1251\n\n## \n\nAlexander Shtuchkin\n\nOn Wed, Oct 12, 2016 at 3:56 AM, wanghaisheng notifications@github.com\nwrote:\n\n> node version 4.5\n>            console.log(   iconv.decode(iconv.encode(\"\u6d3b\u6d3d\u6d3e\u6d36\u6d1b\u6cf5\u6d39\u6d27\", \"windows-1251\"), \"windows-1251\"));\n>            console.log(   iconv.decode(iconv.encode(\"\u6d3b\u6d3d\u6d3e\u6d36\u6d1b\u6cf5\u6d39\u6d27\", \"windows-1251\"), \"utf8\"));\n> \n> output\n> \n> ??????\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/130, or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAmVHU-RqQfdzoLa57wbAGMo-azf4oYqks5qzLzVgaJpZM4KUoAA\n> .\n. Let me know if you have more questions.. That is a great idea. I'll need to fix a couple of issues and make it 1.0. How urgent do you need it?\n. Awesome, thanks! Will merge soon.\n. The API looks correct from the initial look.\n. Added small mention in readme. Let me know if you had something bigger in mind.. Thanks!\nThat's funny, why doesn't it use regular rules, i.e. if \"files\" field is not given, then everything except the contents of `.npmignore` is included?\n\nAnyway, the \"./generation\" is not needed for this module operation. You'll probably need to add \"./lib\" and \"./changelog.md\" explicitly as well.\n. Hey shanavas, thanks for your efforts.\r\n\r\nAfter giving it a bit more thought, I would prefer not to add this field. Having a blacklist of files that don't need to be packaged (like in .npmignore) is safer than having a whitelist (like the \"files\" prop) because I won't need to remember to add new files there. This is easy to forget.\r\n\r\nI'm open to suggestions, but in general I think it makes a lot more sense to adjust npm2deb to use the npm's rules of inclusion and support all modules without changes. Otherwise you'll need to ask a lot of projects to adjust stuff.. Thanks for the report, will look into it asap. Meanwhile, can you check\nthat removing that line actually fixes the build?\n\ncc @larssn\n\nOn Nov 21, 2016 2:11 AM, \"Felix Becker\" notifications@github.com wrote:\n\n> 0.4.14 includes TypeScript definition files that have a triple-slash\n> reference to the \"node\" typings. Triple slash references are a thing of the\n> past, they are only used inside DT to declare dependencies. It doesn't work\n> with included definition files:\n> \n> https://travis-ci.org/felixfbecker/vscode-php-debug/jobs/177567952#L877\n> \n> This line needs to be removed:\n> https://github.com/ashtuchkin/iconv-lite/blob/master/lib/index.d.ts#L6\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/137, or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAmVHYVFD8i-zbBIB9pEam_CT1ByShpqks5rAW5egaJpZM4K4A2p\n> .\n. I.e. is everything else working?\n\nOn Nov 21, 2016 10:01 AM, \"Alexander Shtuchkin\" ashtuchkin@gmail.com\nwrote:\n\n> Thanks for the report, will look into it asap. Meanwhile, can you check\n> that removing that line actually fixes the build?\n> \n> cc @larssn\n> \n> On Nov 21, 2016 2:11 AM, \"Felix Becker\" notifications@github.com wrote:\n> \n> > 0.4.14 includes TypeScript definition files that have a triple-slash\n> > reference to the \"node\" typings. Triple slash references are a thing of the\n> > past, they are only used inside DT to declare dependencies. It doesn't work\n> > with included definition files:\n> > \n> > https://travis-ci.org/felixfbecker/vscode-php-debug/jobs/177567952#L877\n> > \n> > This line needs to be removed:\n> > https://github.com/ashtuchkin/iconv-lite/blob/master/lib/index.d.ts#L6\n> > \n> > \u2014\n> > You are receiving this because you are subscribed to this thread.\n> > Reply to this email directly, view it on GitHub\n> > https://github.com/ashtuchkin/iconv-lite/issues/137, or mute the thread\n> > https://github.com/notifications/unsubscribe-auth/AAmVHYVFD8i-zbBIB9pEam_CT1ByShpqks5rAW5egaJpZM4K4A2p\n> > .\n. I published v0.4.15 with this fix. Could you check it? \r\n\r\nThanks again for quick response!. Just checked it myself on a small sample project and it did compile successfully after installation of \"@types/node\". Typescript 2.1, Node 6.5.. Thanks!. Hey, it looks like res.text() is already decoding the request body with\neither UTF8 or encoding provided in headers.\n\nTo fix it, you might want to use res.arrayBuffer() (\nhttps://developer.mozilla.org/en-US/docs/Web/API/Body/arrayBuffer), then\nconvert it to Buffer and provide to decode().\n\nLet me know if it works.\n\n\n\nOn Sun, 1 Jan 2017 at 03:41, Hellon Canella <notifications@github.com>\nwrote:\n\n> I'm fetching a xml encoded in ISO-8859-1.\n>\n>\n> As long as the fetching is completed I'm trying to convert it to UTF-8.\n>\n>\n> const iconv = require('iconv-lite');\n>\n>\n>\n> fetch('http://www.band.uol.com.br/rss/colunista_64.xml')\n>\n>   .then(res => res.text())\n>\n>   .then(text => {\n>\n>      const decodedText = iconv.decode(new Buffer(text), 'latin1')\n>\n>              , output = iconv.encode(decodedText, 'utf8')\n>\n>       console.log(output.toString())\n>\n> })\n>\n>\n>\n> The problem is: all especial characters of the body is being replaced by\n> \"\u00bf\u00bd\"\n>\n>\n>\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/139>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHaDClqP4eRox_qX_42rRYUb8NabNks5rNugUgaJpZM4LYo1b>\n> .\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n. Not really proficient in C++ node coding, but can't you create a `Buffer` in C++ land? or `Uint8Array`? That would be the most efficient. \r\n\r\nIf it's not possible or you really need to use json (and not just JS object), then your method should definitely work.\r\n\r\nTip: the `utf8String` you mention in step 5 is actually a regular JS String. It usually doesn't matter which encoding is used internally (utf16), so it's better to name it just `errorString`.. Great to hear it works!. The library itself has pretty reasonable memory footprint (several megabytes at most), so I assume that grunt is trying to process too large file. \r\n\r\nThere are several solutions I see:\r\n 1. Make sure all files in your project is utf8, so that grunt doesn't need to convert anything.\r\n 2. Change grunt or your Gruntfile to use streaming iconv-lite API.. I mean, if you use iconv-lite directly in Gruntfile, then you can change it to use streaming api. Otherwise, if it's some internal-to-grunt operation, then the only way is to change grunt itself.. Thanks for the detailed report Daniel!\r\n\r\nI just checked on my machine and it seem to work fine (iconv-lite@0.4.15, node@v7.4.0):\r\n```\r\n> const enc = iconv.encode(Buffer.from('test'), 'ISO-8859-1');\r\nundefined\r\n> console.log(Buffer.from('test'), enc, iconv.decode(enc, 'ISO-8859-1'));\r\n<Buffer 74 65 73 74> <Buffer 74 65 73 74> 'test'\r\nundefined\r\n```\r\n\r\nThe library is fully covered with tests for all encodings, plus ISO-8859-1 is a popular one and haven't changed for years, so I presume something might be wrong with your environment.\r\n\r\nSpecifically, I'm wondering why the buffers are printed like arrays, not the regular `<Buffer ...>`. Could it be that some other library is monkey-patching Buffer? Which version of node are you using? Could you try to reinstall the library (I've seen a partial/screwed installs before)?. Ok, makes sense. I'll try to reproduce it in a browser.\r\n\r\nMeanwhile, could you try again with a first argument to `encode` being a string, not buffer? When you encode something, you convert from string to buffer; when decoding - buffer to string.. yep, thanks. 63 is ascii for \"?\". I managed to reproduce it. Turns out, when Chrome loads javascript files, it defaults to latin1 encoding, but iconv-lite source files require utf-8. \r\n\r\nTo fix it, one of the following needs to be done:\r\n * add `<meta charset=\"utf-8\">` to the html file. \r\n * use `<script src=\"your-file.js\" charset=\"utf-8\"></script>` to load the script.\r\n * add `Content-type: application/javascript; charset=utf-8` header to the http request of js file.\r\n\r\nLet me know if it helped.. Yep I will add a note in readme and a runtime warning. Thanks!\n\n\nOn Sun, 8 Jan 2017 at 17:11, Daniel Huisman <notifications@github.com>\nwrote:\n\n> Closed #142 <https://github.com/ashtuchkin/iconv-lite/issues/142>.\n>\n>\n>\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/142#event-915222112>, or mute\n> the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHcR5xrZN1YFcD2YeqBfr5WGXHzEKks5rQOCWgaJpZM4Ldi99>\n> .\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n. Note: added a warning when this situation arises, with a link to relevant [wiki page](https://github.com/ashtuchkin/iconv-lite/wiki/Javascript-source-file-encodings). Hehe I like your way of thinking :)\r\n\r\nShould it be a filed to IntelliJ then? AFAIK cp950 is a valid utf-8 json file.. Cool cool :). Hey, this doesn't look like text encoding. Could it be gzipped? Could you\npost the whole response with headers?\n\n--\nAlexander Shtuchkin\n\nOn Tue, Jan 17, 2017 at 8:10 PM, lou <notifications@github.com> wrote:\n\n> Hi & hope you can help:/\n>\n> I have been having a hard time trying to work through some encoding issues.\n>\n> I am using Electron with Node.js https. The app Posts a request to a Soap\n> webservice.\n>\n> I receive a status 200, correct Headers and response. However the response\n> is a garbled twisted string of weird characters as in my screenshot.\n> [image: screenshot_characters]\n> <https://cloud.githubusercontent.com/assets/15900463/22050542/a48dc25e-dda0-11e6-9409-ddc5e391cfe3.png>\n>\n> .\n>\n> Am trying iconv-lite but with no success. I am using Node v6.9.2, linux\n> and npm 3.10.9.\n>\n> Really appreciate any help\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/144>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHX9YK0MqzJHLgs1L0ru5SbMeQiF7ks5rTZCZgaJpZM4Lmdam>\n> .\n>\n. Thanks! Can you try to remove line with \"Accept-Encoding\" from your request\nheaders and try again? I think server returns gzipped data now.\n\n--\nAlexander Shtuchkin\n\nOn Tue, Jan 17, 2017 at 11:58 PM, lou <notifications@github.com> wrote:\n\n> Here is the post request code that I forgot to include earlier. I also\n> realise iconv-lite may not work with my Node version (so I may upgrade) but\n> any help would do.\n> [image: post_req]\n> <https://cloud.githubusercontent.com/assets/15900463/22055338/92923cbe-ddbf-11e6-9418-d2088dbc419d.png>\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/144#issuecomment-273408051>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHWFVMyzGsfI337OKoKCklmsKfgB9ks5rTcY5gaJpZM4Lmdam>\n> .\n>\n. Hey raccy, thanks for filing this issue. \r\n\r\nIn multibyte encodings, iconv-lite tries its best to mirror the [WHATWG Encoding Standard](https://encoding.spec.whatwg.org). I just checked it out and it maps symbol 1-33 to U+FF5E, see [this](https://encoding.spec.whatwg.org/index-jis0208.txt) and [this](https://encoding.spec.whatwg.org/jis0208.html). \r\n\r\nDo you have other sources except libiconv that map 1-33 to U+301C? You might want to file an issue to [encoding standard issue tracker](https://github.com/whatwg/encoding/issues). I see there's some [minor discussion](https://github.com/whatwg/encoding/issues/47#issuecomment-251083712) there about it.\r\n\r\nI can probably add the encoding pair U+301C -> 81 60 for Shift_JIS and CP932 to be more flexible, but for the decoding part I currently aim to follow encoding standard.\r\n\r\nWhat do you think?. Thanks for chiming in, Ikedas. What do you think of discussion of the same issue at the encoding standard tracker: https://github.com/whatwg/encoding/issues/47 ?\r\n\r\nNote to self: Ambiguities can be see here: https://www.w3.org/TR/2000/NOTE-japanese-xml-20000414/#ambiguity_of_yen. Thanks Brian! Do I understand right that this only affects performance?\r\n. Thanks again!. That's a very good question. Although it seems react-native is not a full\nnode environment as it doesn't know streams or Buffer class, we still want\nto support it because it's in this project's mission to support all js\nenvironments.\n\nI'll look into making it trivially usable by switching to Uint8Array\ninstead of Buffer if the latter is not available. No promises on timeline,\nthough.\n\nMeanwhile for now you can try to build it into a single .js file using\nbrowserify, then use the resulting file like in a browser.\n\nAlex.\n\n--\nAlexander Shtuchkin\n\nOn Tue, Feb 14, 2017 at 9:08 PM, AdamChrist <notifications@github.com>\nwrote:\n\n> I want to use iconv-lite in react-native, but icon-lite require stream of\n> Node's stream.\n> The error message is \"Unable to resolve module stream\".\n> Is there any way I can use it in react-native? thanks\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/148>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHaKEyvTSHNWhKcqG_ptQO9dR0sGTks5rcogxgaJpZM4MBTzB>\n> .\n>\n. Tracking React Native support in #150.. Thanks for the report, Williams!\n\nUsually in Node environment Buffer is defined globally.. what environment\nare you using?\n\nAlex\n\nOn Apr 16, 2017 5:30 AM, \"williams-voon\" <notifications@github.com> wrote:\n\nHi,\n\nIn this file 'Buffer' is not defined.\n/iconv-lite/encodings/dbcs-codec.js\n\nI think the line should be added at head of the file.\nconst { Buffer } = require('buffer')\n\nor ES6: import { Buffer } from 'buffer'\n\nRegards,\nWilliams\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\n<https://github.com/ashtuchkin/iconv-lite/issues/150>, or mute the thread\n<https://github.com/notifications/unsubscribe-auth/AAmVHYRuQTf_VxtH7k3ad58fWvPba6rEks5rwgn8gaJpZM4M-ndp>\n.\n. @williams-voon I added explicit require()-s of buffer module, should now work, could you please check it?\r\n\r\nTo check, please install two new modules: `buffer` and `stream`, then install `iconv-lite` from master - use something like this in your `package.json`:\r\n```\r\n{\r\n  \"dependencies\": {\r\n    .. your deps ..\r\n    \"iconv-lite\": \"ashtuchkin/iconv-lite#master\",\r\n  }\r\n}\r\n```\r\n. Released iconv-lite v0.4.16 with the fix. Thanks!. Thanks @flyher!. You can use any guide to browserify on the internets, like the following:\r\nhttps://medium.com/@christopherphillips_88739/a-beginners-guide-to-browserify-1170a724ceb2\r\nhttps://writingjavascript.org/posts/introduction-to-browserify\r\nhttps://github.com/substack/browserify-handbook\r\n\r\nAnd then just require this library and use it.. Not sure how iconv-lite is involved here.. :)\n\nOn Apr 24, 2017 8:02 PM, \"Hsiu Hui\" <notifications@github.com> wrote:\n\n> When I test this page:\n> https://www.wired.com/2017/04/just-pair-11-radio-gadgets-can-steal-car/\n>\n> and my code is as below:\n>\n> request({url: result.url, encoding: null}, (error, response, html)=>{\n>     var enc = charset(res.headers, html)\n>     enc = enc || jschardet.detect(html).encoding\n>     console.log(\"encoding: \" + enc)\n> })\n>\n> It always return \"null\". But when I use document.charset in chrme dev\n> tool, it return UTF-8\n> Can somebody fix it?\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/152>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHVCPR8h8tHX-OlE1LGZFijesickNks5rzWJCgaJpZM4NG8r4>\n> .\n>\n. Thanks Lars!. Thanks for the report! I reproduced it and will fix soon.\r\n\r\nI'll need to replace the check here: https://github.com/ashtuchkin/iconv-lite/blob/master/encodings/internal.js#L38  to something like `new Buffer('eda0bdedb2a9', 'hex').toString() !== '\ud83d\udca9'`\r\n. Hmm, thanks for the info. I still need to support Node v0.10 and would like\nto avoid external dependencies.. Is there any other options?\n\n--\nAlexander Shtuchkin\n\nOn Mon, Jun 12, 2017 at 8:59 AM, Andrey Sidorov <notifications@github.com>\nwrote:\n\n> new Buffer('string', enc) is deprecated - should be Buffer.from('string',\n> enc) ( + safe-buffer <https://github.com/feross/safe-buffer> polyfill )\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/154#issuecomment-307834326>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHVlDTeJIJTK9HkvdAmvOHawTFRDHks5sDWBKgaJpZM4N3R01>\n> .\n>\n. Ok, so fixed in c7a8243 and published as v0.4.18.. Thanks for the report @sidorares!. Thanks guys, I think you're right, I'll add a fix soon.\n\n--\nAlexander Shtuchkin\n\nOn Wed, Jul 5, 2017 at 11:26 AM, Erik Kemperman <notifications@github.com>\nwrote:\n\n> Just noticed the same thing. Suggested fix:\n>\n> .toLowerCase().replace(/:\\d{4}$/, \"\").replace(/[^0-9a-z]/g, \"\");\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/156#issuecomment-313186544>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHYk30eR2uB93vvK3Kr06jEYV5yLcks5sK9VAgaJpZM4OMhuw>\n> .\n>\n. Interesting, is it much faster than `iconv.encode(..).length`? What's the use case here?. What is your use case, why do you need that api (vs using encode().length)?\nWhy do you need these milliseconds?  I'm still trying to wrap my head over\nwhy would you need the byte length without the content? You're not\nallocating memory for that are you?\n\nI'm wondering why there were no feature requests for that in the past 4\nyears.. did anything change recently?\n\nI'm in general reluctant to increase the API surface for something that is\na bit faster for a small subset of encodings, so we need to really\nunderstand what that gives us.\n\nOn Jul 10, 2017 4:34 AM, \"Dmitriy Tsvettsikh\" <notifications@github.com>\nwrote:\n\n> @ashtuchkin <https://github.com/ashtuchkin> It's faster for internal and\n> single-byte encodings. The use cases are the same as for Buffer.byteLength\n> - if you want to known size of string in bytes before encoding.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/pull/158#issuecomment-314080248>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHQ1G-FWuVRp1ByOHqhdfKjgdFsK5ks5sMgwygaJpZM4OSOvP>\n> .\n>\n. Yes, both Mac and Linux are definitely supported.\n\n--\nAlexander Shtuchkin\n\nOn Tue, Aug 8, 2017 at 6:26 AM, Jesse Laukkanen <notifications@github.com>\nwrote:\n\n> Readme says:\n>\n> ... Works on Windows and in sandboxed environments like Cloud9.\n>\n> and later\n>\n> Comparison with node-iconv module (1000x256kb, on MacBook Pro\n>\n> which sounds like this might work also on Mac. How is it, is Mac supported\n> or not?\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/159>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHSzcPdCUBL_owkceYIi_cPgSGpqfks5sWGIZgaJpZM4OwuwN>\n> .\n>\n. Looks like the buffer that is passed to iconv.decode() is undefined. Maybe\nHTTP request failed? You might want to add a check before calling\niconv.decode().\n\n--\nAlexander Shtuchkin\n\nOn Tue, Aug 22, 2017 at 4:16 PM, Nordes M\u00e9nard-Lamarre <\nnotifications@github.com> wrote:\n\n> Hello,\n>\n> I encountered an error that happen a bit randomly on one server I contact.\n> I do a simple get to receive the data (Japanese encoding). I was wondering\n> if it comes from the codec itself or if it was the 'request' module that\n> was failing.\n>\n> Here's the StackTrace\n>\n> C:\\my-app\\node_modules\\iconv-lite\\encodings\\dbcs-codec.js:453\n>     var newBuf = new Buffer(buf.length*2),\n>                                ^\n>\n> TypeError: Cannot read property 'length' of undefined\n>     at DBCSDecoder.write (C:\\my-app\\node_modules\\iconv-lite\\encodings\\dbcs-codec.js:453:32)\n>     at Object.decode (C:\\my-app\\node_modules\\iconv-lite\\lib\\index.js:42:23)\n>     at Request._callback (C:\\my-app\\src\\service\\myProxyService.js:45:22)\n>     at self.callback (C:\\my-app\\node_modules\\request\\request.js:188:22)\n>     at emitOne (events.js:115:13)\n>     at Request.emit (events.js:210:7)\n>     at Request.onRequestError (C:\\my-app\\node_modules\\request\\request.js:884:8)\n>     at emitOne (events.js:115:13)\n>     at ClientRequest.emit (events.js:210:7)\n>     at Socket.socketErrorListener (_http_client.js:401:9)\n>     at emitOne (events.js:115:13)\n>     at Socket.emit (events.js:210:7)\n>     at emitErrorNT (internal/streams/destroy.js:64:8)\n>     at _combinedTickCallback (internal/process/next_tick.js:138:11)\n>     at process._tickCallback (internal/process/next_tick.js:180:9)\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/161>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHdRWC4n1YJQldAz2ZP4XHwwILGjKks5sa2FOgaJpZM4O_TJv>\n> .\n>\n. Very interesting, thanks for the report! It looks like it was the unexpected consequence of #147. I'll revert that commit.. Okay this should be fixed in febb0b8e280de5de844151036e87cfa06b92d548. I'll publish new iconv version soon. Thanks!. This line might be the problem:\nvar a = btoa(s.toString())\n\n\"s\" here is actually a Buffer with encoded data. When you do .toString(),\nyou lose the encoding.\n\nInstead, I recommend either of these:\nvar a = s.toString(\"base64\");\nOr\nvar a = iconv.decode(s, \"base64\");\n\nAlso please make sure to read these wiki pages for more context:\nhttps://github.com/ashtuchkin/iconv-lite/wiki\nhttps://github.com/ashtuchkin/iconv-lite/wiki/Use-Buffers-when-decoding\n\n\nOn Sep 12, 2017 10:36 PM, \"Tharaka Pathirana\" <notifications@github.com>\nwrote:\n\n> I've created a base64 encoding string in the following format using C#. I\n> want to create the same encoded string using JavaScript.\n> C#\n> var isoEncoding = System.Text.Encoding.GetEncoding(\"ISO-8859-1\");\n> var a = isoEncoding.GetBytes(username + \":\" + password);\n> String encoded = System.Convert.ToBase64String(a);\n> request.Headers.Add(\"Authorization\", \"Basic \" + encoded);\n>\n> Here I used iconv and try to get encoded string and it gives a different\n> encoded string. what can be the reason?\n>\n> var str = this.username+':'+this.password;\n> var s = iconv.encode(str, 'ISO-8859-1');\n> var a = btoa(s.toString());\n> headers.append('Authorization', 'Basic ' + a);\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/163>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHR0_tlKPOSDu5zvCtF8wSF2U2Ob9ks5sh2nYgaJpZM4PVmnG>\n> .\n>\n. To be more specific, buffer.toString() decodes binary data in that buffer\nusing utf-8 encoding, so that messes up things.\n\nOn Sep 13, 2017 10:30 AM, \"Alexander Shtuchkin\" <ashtuchkin@gmail.com>\nwrote:\n\n> This line might be the problem:\n> var a = btoa(s.toString())\n>\n> \"s\" here is actually a Buffer with encoded data. When you do .toString(),\n> you lose the encoding.\n>\n> Instead, I recommend either of these:\n> var a = s.toString(\"base64\");\n> Or\n> var a = iconv.decode(s, \"base64\");\n>\n> Also please make sure to read these wiki pages for more context:\n> https://github.com/ashtuchkin/iconv-lite/wiki\n> https://github.com/ashtuchkin/iconv-lite/wiki/Use-Buffers-when-decoding\n>\n>\n> On Sep 12, 2017 10:36 PM, \"Tharaka Pathirana\" <notifications@github.com>\n> wrote:\n>\n>> I've created a base64 encoding string in the following format using C#. I\n>> want to create the same encoded string using JavaScript.\n>> C#\n>> var isoEncoding = System.Text.Encoding.GetEncoding(\"ISO-8859-1\");\n>> var a = isoEncoding.GetBytes(username + \":\" + password);\n>> String encoded = System.Convert.ToBase64String(a);\n>> request.Headers.Add(\"Authorization\", \"Basic \" + encoded);\n>>\n>> Here I used iconv and try to get encoded string and it gives a different\n>> encoded string. what can be the reason?\n>>\n>> var str = this.username+':'+this.password;\n>> var s = iconv.encode(str, 'ISO-8859-1');\n>> var a = btoa(s.toString());\n>> headers.append('Authorization', 'Basic ' + a);\n>>\n>> \u2014\n>> You are receiving this because you are subscribed to this thread.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/ashtuchkin/iconv-lite/issues/163>, or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AAmVHR0_tlKPOSDu5zvCtF8wSF2U2Ob9ks5sh2nYgaJpZM4PVmnG>\n>> .\n>>\n>\n. Maybe what trips you up is ISO-8859-1 is actually the same as UTF-8 for all\nASCII characters. It's only different in characters with accents etc.\n\nSo i.e. iconv.encode(\"Hello, world!\", \"ISO-8859-1\").toString() should be\nequal to \"Hello, world!\" and it's correct behavior.\n\nCan you provide the values you're testing with and what output values you\nexpect?\n\n\n--\nAlexander Shtuchkin\n\nOn Wed, Sep 13, 2017 at 11:13 AM, Tharaka Pathirana <\nnotifications@github.com> wrote:\n\n> I have tried both s.toString(\"base64\") and iconv.decode(s, \"base64\"). but\n> it gives same as btoa(s.toString())\n>\n> var s = iconv.encode(str, 'ISO-8859-1'); // Is this correct?\n> I debug the code and check, this is given Unit8Array same as byte array of\n> the string. This look\n> like utf-8 encoded array.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/163#issuecomment-329252008>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHT_c6_uOFkiTB5zFdqDdZRU0zh9Eks5siBtNgaJpZM4PVmnG>\n> .\n>\n. Yes.\n\nOn Oct 3, 2017 03:14, \"Idan Gozlan\" <notifications@github.com> wrote:\n\n> Hi,\n>\n> Is that possible use this convertor for windows-1252 to utf-8 conversion?\n>\n> Thanks\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/167>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHQzbDdrlYRgOETUs36evDw6schyXks5sogkggaJpZM4Pr4kQ>\n> .\n>\n. function convert(buffer_in_win1252_encoding) {\n  var str = iconv.decode(buffer_in_win1252_encoding, \"windows-1252\");\n  var output_buffer_in_utf8_encoding = iconv.encode(str, \"utf8\");\n  return output_buffer_in_utf8_encoding;\n}\n\nSee also wiki in this project for more information.\n\n--\nAlexander Shtuchkin\n\nOn Wed, Oct 4, 2017 at 12:34 AM, Idan Gozlan <notifications@github.com>\nwrote:\n\n> Can you please provide example? Didn't find the way..\n>\n> On Oct 3, 2017 20:53, \"Alexander Shtuchkin\" <notifications@github.com>\n> wrote:\n>\n> > Yes.\n> >\n> > On Oct 3, 2017 03:14, \"Idan Gozlan\" <notifications@github.com> wrote:\n> >\n> > > Hi,\n> > >\n> > > Is that possible use this convertor for windows-1252 to utf-8\n> conversion?\n> > >\n> > > Thanks\n> > >\n> > > \u2014\n> > > You are receiving this because you are subscribed to this thread.\n> > > Reply to this email directly, view it on GitHub\n> > > <https://github.com/ashtuchkin/iconv-lite/issues/167>, or mute the\n> > thread\n> > > <https://github.com/notifications/unsubscribe-auth/\n> > AAmVHQzbDdrlYRgOETUs36evDw6schyXks5sogkggaJpZM4Pr4kQ>\n> > > .\n> > >\n> >\n> > \u2014\n> > You are receiving this because you authored the thread.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/ashtuchkin/iconv-lite/issues/\n> 167#issuecomment-333925645>,\n> > or mute the thread\n> > <https://github.com/notifications/unsubscribe-auth/\n> AB5hbYaJYQXmMd24gpV613oJtMQQvayHks5sonSQgaJpZM4Pr4kQ>\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/167#issuecomment-334073152>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHd_9rX58Er2worWD_9fSnOqLCp0qks5sozUQgaJpZM4Pr4kQ>\n> .\n>\n. Works for me...\n\n--\nAlexander Shtuchkin\n\nOn Sun, Oct 8, 2017 at 2:52 AM, Idan Gozlan <notifications@github.com>\nwrote:\n\n> Well... not working.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/167#issuecomment-334995382>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHcXzNkog5pbFpfB36AJHq95v3mFKks5sqJtggaJpZM4Pr4kQ>\n> .\n>\n. Looks like cp720 is rather rare encoding and not supported by iconv library (from which I got the list of encodings). Would you want to add a pull request to add it? See cp808 as an example how it could look like: https://github.com/ashtuchkin/iconv-lite/blob/master/encodings/sbcs-data.js#L15. Interesting, I didn't know about this flag. Does it have anything other\nthan loading require()-d files?\n\nOn Nov 16, 2017 05:20, \"Rob Calcroft\" <notifications@github.com> wrote:\n\n> This lib appears to through lots of warnings when an app is run with\n> --trace-sync-io could this have an impact on performance?\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/169>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHdwEeJ2CpLHcO5QWYEm--HY8OO0uks5s3DaGgaJpZM4QgeiL>\n> .\n>\n. Hey, there's several things going on here:\r\n 1. Using `str = iconvlite.decode(buffer3, \"1255\");`, you're converting from a win1255-encoded buffer to a JS string. JS string doesn't have encoding, it's just a list of characters.\r\n 2. When you do `console.log(str)`, the string is converted to whatever encoding your terminal has and shows you the characters.\r\n 3. When you do `fs.writeFile(\"...\", str, ..)`, Node writes your JS string `str` to file using default encoding, which is utf-8. That's why you have utf-8 characters in that file.\r\n\r\nIf you want to save that string in win1255 encoding, just write the `buffer3` directly:\r\n`fs.writeFile(\"/home/USER/git/npm-test-1/windows-1255.txt\", buffer3, ..`\r\n\r\nSee more info in my wiki: https://github.com/ashtuchkin/iconv-lite/wiki and https://github.com/ashtuchkin/iconv-lite/wiki/Use-Buffers-when-decoding. I see. Can you send a buffer (i.e. actual bytes) to that server? that would\nbe the simple solution. If you can only send strings, then you'll need to\ndetermine where exactly encoding conversion is happening. As I said, JS\nstrings don't have encodings by themselves - it's a list of characters, not\nbytes.\n\nSometimes legacy code do use strings as lists of bytes, though. If you have\nthis case, then you might try using `str = buffer3.toString(\"binary\")`.\nThat might or might not work.\n\n\n--\nAlexander Shtuchkin\n\nOn Sun, Nov 19, 2017 at 11:08 PM, LionOpeter <notifications@github.com>\nwrote:\n\n> Hi @ashtuchkin <https://github.com/ashtuchkin> and thank you very much\n> for replying.\n> I get what you said. My main goal (as i mentioned in my question number 2)\n> is to create a string which has the right character(s). For me the right\n> characters are the characters of windows-1255 encoding because my IRC\n> server works only with this charset.If i send the server the wrong encoding\n> i get an error saying 'Nickname is unavailable: Illegal characters' Any\n> idea how i can pull this off?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/170#issuecomment-345609012>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHY6jPuXZR5hNFHHZ8CcBs4U0H3Z-ks5s4SVqgaJpZM4QjNHu>\n> .\n>\n. Nah, this problem need to be resolved on the irc module side. I noticed\nthere's an \"encoding\" option for irc client (\nhttps://github.com/martynsmith/node-irc/blob/master/lib/irc.js#L59), maybe\nyou set that to \"windows-1255\" and just send the strings?\n\nIf you still want to experiment with manual character manipulation in\nstrings, you can look here:\nhttps://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String/fromCharCode\n\n--\nAlexander Shtuchkin\n\nOn Sun, Nov 19, 2017 at 11:53 PM, LionOpeter <notifications@github.com>\nwrote:\n\n> If i try to send it as a buffer i get error:\n>\n> /home/USER/git/npm-test-1/node_modules/irc/lib/irc.js:930\n>     if (args[args.length - 1].match(/\\s/) || args[args.length - 1].match(/^:/) || args[args.length - 1] === '') {\n>                               ^\n>\n> TypeError: args[(args.length - 1)].match is not a function\n>     at Client.send (/home/USER/git/npm-test-1/node_modules/irc/lib/irc.js:930:31)\n>     at Client._connectionHandler (/home/USER/git/npm-test-1/node_modules/irc/lib/irc.js:725:10)\n>     at Object.onceWrapper (events.js:314:30)\n>     at emitNone (events.js:110:20)\n>     at Socket.emit (events.js:207:7)\n>     at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1125:10)\n>\n>\n> In-addition, an attempt to use str = buffer3.toString(\"binary\") ended up\n> with the same result of Illegal characters error. Any way to assign a\n> string with a hex value in javascript?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/170#issuecomment-345616839>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHeeMQn-rocIJuCFrcRqXFEBw-l2xks5s4S_7gaJpZM4QjNHu>\n> .\n>\n. Looks good to me, thanks!\nAs for the method names - I'll make aliases, thanks for the feedback.\n. Looks good. Thank you! \n. Thanks! Could you add a test for the new encoding?\n. Nice work!\n. Just did, thanks for your efforts!\n. Thanks a lot! Could you also make some tests, like its done for GBK? Maybe several web pages using Big5?\n. Technically this 'global' is local to the module and never exposed, so its really equivalent. Still, I agree that its better from code culture point of view.\n. You're right. Total surprise for me, thanks! I think I've read about it somewhere, but don't remember where. I'll need to be more careful (and I have one more reason to use CoffeeScript, which I use in most of my projects).\n. Great! Looking forward for your pull request.\n. Hi! After digging into code, it seems to me that it would be better to do it externally, along the lines of:\n\n```\nvar str = \"Test html escaping.. \u0422\u0435\u0441\u0442\u0438\u0440\u0443\u0435\u043c \u0425\u0422\u041c\u041b.\";\nvar html_str = iconv.decode(iconv.encode(str, \"latin1\"), \"latin1\");\nhtml_str = html_str.replace(/[?]/g, function(c, idx) {\n    return \"&#\"+str.charCodeAt(idx)+\";\";\n});\n```\n\nWhat do you think?\n. Javascript stores strings internally as UTF-16, i.e. 2 bytes per character. And it happens that latin1 encoding is exactly first 256 characters of Unicode, or first byte of javascript string characters, stripping the second one. It also happens that this is exactly what 'binary' encoding in Node.js does. So all encoding/decoding is done by Node.js itself using 'binary' encoding.\nHope this helps.\n\nAbout the deprecation and replacement - it would need some research on performance. Native solutions are usually much faster. I don't have spare time right now to do that, but I'll be happy if you are willing to participate on it.\n. Nice, thank you!\n\nI was thinking about making automatic generation of most codepages using f.ex. original iconv, but don't have time for it right now. Maybe you could help with it? Should be not complex and it would prevent human errors. Or, if you can wait, I'll do it eventually.\n. Thanks a lot!\n. So cool, thanks a lot, Mike!\n. I've added even more encodings and published it as version 0.2.0. Thanks again for your time!\n. Fixed in 0.2.1, thanks!\n. Merged & released version 0.2.3. Thanks for the collaboration and greetings from a yacht in Adriatic sea! ^_^\n. I guess it depends on the machine. Just re-checked and got following results:\n\n``` console\nashtuchkin@ubuntu:~$ node -v\nv0.8.7\nashtuchkin@ubuntu:~$ cd iconv-lite/\nashtuchkin@ubuntu:~/iconv-lite$ node test/perfomance.js \n\nEncoding 262144 chars 1000 times:\niconv: 9886ms, 25.90 Mb/s.\niconv-lite: 5364ms, 47.73 Mb/s.\n\nDecoding 262144 bytes 1000 times:\niconv: 6957ms, 36.80 Mb/s.\niconv-lite: 9356ms, 27.36 Mb/s.\nashtuchkin@ubuntu:~/iconv-lite$ \n```\n. Actually, I just re-checked several times one-after-another, and it may be that my notebook has a dynamic cpu frequency and it changes significantly.\n\n``` console\n\nashtuchkin@ubuntu:~/iconv-lite$ node test/perfomance.js \n\nEncoding 262144 chars 1000 times:\niconv: 8125ms, 31.51 Mb/s.\niconv-lite: 5078ms, 50.41 Mb/s.\n\nDecoding 262144 bytes 1000 times:\niconv: 6729ms, 38.04 Mb/s.\niconv-lite: 8867ms, 28.87 Mb/s.\nashtuchkin@ubuntu:~/iconv-lite$ node test/perfomance.js \n\nEncoding 262144 chars 1000 times:\niconv: 6955ms, 36.81 Mb/s.\niconv-lite: 5077ms, 50.42 Mb/s.\n\nDecoding 262144 bytes 1000 times:\niconv: 6631ms, 38.61 Mb/s.\niconv-lite: 8656ms, 29.57 Mb/s.\nashtuchkin@ubuntu:~/iconv-lite$ node test/perfomance.js \n\nEncoding 262144 chars 1000 times:\niconv: 6913ms, 37.03 Mb/s.\niconv-lite: 5008ms, 51.12 Mb/s.\n\nDecoding 262144 bytes 1000 times:\niconv: 6464ms, 39.60 Mb/s.\niconv-lite: 8578ms, 29.84 Mb/s.\nashtuchkin@ubuntu:~/iconv-lite$ node test/perfomance.js \n\nEncoding 262144 chars 1000 times:\niconv: 6917ms, 37.01 Mb/s.\niconv-lite: 5123ms, 49.97 Mb/s.\n\nDecoding 262144 bytes 1000 times:\niconv: 6692ms, 38.25 Mb/s.\niconv-lite: 9123ms, 28.06 Mb/s.\n```\n. Ok, I took some time to let V8 optimize the encoding/decoding functions and here's what I got:\n\n``` console\nashtuchkin@ubuntu:~/iconv-lite$ node test/perfomance.js\nEncoding 262144 chars 1000 times:\niconv: 7122ms, 35.94 Mb/s.\niconv-lite: 1139ms, 224.76 Mb/s.\n\nDecoding 262144 bytes 1000 times:\niconv: 6716ms, 38.12 Mb/s.\niconv-lite: 1975ms, 129.62 Mb/s.\nashtuchkin@ubuntu:~/iconv-lite$ node test/perfomance.js\n\nEncoding 262144 chars 1000 times:\niconv: 7041ms, 36.36 Mb/s.\niconv-lite: 1115ms, 229.60 Mb/s.\n\nDecoding 262144 bytes 1000 times:\niconv: 6607ms, 38.75 Mb/s.\niconv-lite: 1946ms, 131.55 Mb/s.\nashtuchkin@ubuntu:~/iconv-lite$ node test/perfomance.js\n\nEncoding 262144 chars 1000 times:\niconv: 6958ms, 36.79 Mb/s.\niconv-lite: 1112ms, 230.22 Mb/s.\n\nDecoding 262144 bytes 1000 times:\niconv: 6657ms, 38.46 Mb/s.\niconv-lite: 1944ms, 131.69 Mb/s.\n```\n\nCould you please confirm you have similar performance increase with latest v0.2.4?\n. Just now\n. Thats great) And thank you for raising this issue!\n. You could make it similar to GBK encoding. You only need to get a table of\nUnicode <-> GB18030 characters from somewhere. And also, please make a test\nfor it, all encodings are tested in iconv-lite.\n\n## \n\nAlexander Shtuchkin\n\nOn Sat, Aug 25, 2012 at 1:28 AM, fengmk2 notifications@github.com wrote:\n\n> :) Thanks for your excellent work!\n> \n> I will keep on using this module on my project, and replace all `iconv`\n> with `iconv-lite`.\n> \n> One more question: How can I add 'GB18030' encodings on iconv-lite?\n> \n> ---\n> \n> MK2, FaWave, Net4Team \u263anodejs\n> TEL: 18668079069\n> Github: https://github.com/fengmk2\n> \u5fae\u535a: @Python\u53d1\u70e7\u53cb\n> Twitter: @fengmk2\n> CNode: http://cnodejs.org/user/suqian\n> Blog: http://fengmk2.github.com/\n> \n> On Aug 25, 2012, at 5:23 AM, Alexander Shtuchkin notifications@github.com\n> wrote:\n> \n> > Thats great) And thank you for raising this issue!\n> > \n> > \u2014\n> > Reply to this email directly or view it on GitHub.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/ashtuchkin/iconv-lite/issues/15#issuecomment-8015349.\n. Also, to make it compatible with browsers (started in asutherland/iconv-lite@e8273cbc4cb321f300c3530b4feda71383e72f39) We will probably use toots/buffer-browserify or native arrays\n. Fixed in 0.4\n. Yup, thanks!\n. I optimized it a little more if you don't mind :)\n\n```\ngbk charset performance test:\n\nEncoding 131072 chars 10000 times:\niconv: 110634ms, 20.25 Mb/s.\niconv-lite: 12094ms, 185.22 Mb/s.\n\nDecoding 229376 bytes 10000 times:\niconv: 113290ms, 19.77 Mb/s.\niconv-lite: 13567ms, 165.11 Mb/s.\n```\n\nAlso, published v0.2.5.\n. Thanks! Will release it with the next version (change is rather minor).\n. Thanks! Should be fixed in 83c09be246fba24bdd6e13c1302af1e050face19\nI've published new version 0.2.6 on npm with this fix, please check.\n. Hi Julien and thank you!\nGood streaming support will require handling edge issues and will require review all current encoding schemes, which is somewhat time-consuming. If you are interested in doing it, I'll be happy to elaborate more about how to do it.\n\nMeanwhile, you're right that single-byte encodings can be streamed without handling edge cases, just use it buffer-by-buffer.\n. Julien, sorry for a long delay.\n\nI've reviewed the code and it seems it would take more than I anticipated.\nCurrent architecture is not really suited for streaming, so I'll need to\nrevise it as a whole. Basic problem here is that we need to keep state\nbetween decoding buffers, and we can't do that easily with current arch.\n\nI'm a bit busy right now with my daywork, but I'm planning to do the\noverhaul maybe in several months (I also have several more features to add\nlike browser support and more multibyte encondings).\n\n## \n\nAlexander Shtuchkin\n\nOn Thu, Nov 29, 2012 at 11:20 AM, Julien Genestoux <notifications@github.com\n\n> wrote:\n> \n> Thanks for the response!\n> I'll happily help. What do you see the algorithm to be?\n> \n> Thanks,\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/ashtuchkin/iconv-lite/issues/20#issuecomment-10837605.\n. Meanwhile, you can look at http://code.google.com/p/stringencoding/\nThere should be streaming support built in.\n. Although it uses ArrayBuffers, not node's Buffers, I believe you could try something like \n\n```\n var buffer = new Buffer(..) // node's buffer.\n var string = TextDecoder(encoding).decode(new Uint8Array(buf), {stream: true});\n```\n. Hi! Sorry for delay. At last, I've published a iconv-lite@0.4.0-pre2 / [branch v0.4](https://github.com/ashtuchkin/iconv-lite/tree/v0.4) with stream support (see [README](https://github.com/ashtuchkin/iconv-lite/blob/v0.4/README.md) for syntax). Any comments appreciated before I release it.\n. @tomas any issues? I want to release the 0.4 version soon, so your feedback is very welcome.\n. So, you need browser-like behavior, that's a bit more complex than what iconv-lite is designed to do. \n\nWhen there's no indication of charset in HTTP headers, you could probably peek the first ~1 kb for a `<meta http-equiv=\"Content-Type\">` or `<meta charset=\"...\">` tags and use them (using ascii encoding for this should be enough).\n\nOtherwise, you could skip this logic and give your user ability to just set the encoding manually. IMHO its a rare case.\n\nAlso, please be aware that iconv-lite in streaming decoding mode produces a stream of **strings**, not buffers.\n. All right, it seems that the discussion moved away from streams. I'm closing the issue for now, as the functionality is there. Report new bug if you notice any problems.\n. Jon, I wasn't planning to work on it in the nearest future (I have lots to do on my regular work). Do you need something specific from the feature list?\n. no problem)\n. Moved to 0.4.\n. Glad you figured it out. Here's whats happening inside:\n\n```\n> iconv.encode( '\u00e9', 'latin1' )\n<Buffer e9>\n> iconv.encode( '\u00e9', 'latin1' ).toString()  # Here .toString() uses 'utf8' by default and E9 in an incorrect char in it.\n'\ufffd'\n> iconv.encode( '\u00e9', 'latin1' ).toString().charCodeAt(0)\n65533\n> new Buffer('\ufffd') # iconv.decode expects Buffer as a first arg and converts string to Buffer using utf8 if given a string.\n<Buffer ef bf bd>\n> iconv.decode(new Buffer('\ufffd'), 'latin1' )\n'\u00ef\u00bf\u00bd'\n\n```\n. Nice, thank you! It would be even better with tests.\n. Thanks, David!\n. Thank you! I've already merged the #24 (it was first and has more tests), but added your test case.\n. Interesting. I'm wondering what's your use case that you care about 120ms on start?\n. Ok, thank you and thanks for the contribution!\n. Published v0.2.9.\n. Hi! Just so I can understand you better - are you worrying about performance or code explicitness? Is it ok if I convert to Buffer inside the `decode` function?\n\nCurrently you can pass a string to `decode`, but it'll be handled as 'utf8', not 'binary'. Although I agree that 'binary' makes more sense.\n. That'd require writing 2 versions of algorithm for each different encoding scheme as far as I can say... What use case do you have for it?\n. I've had a lot of experiments to squeeze as much performance as I can out of this. The best I could do is to work with Buffers as much as possible and convert to/from strings on the boundaries - this turned out to be much faster than working with strings/chars directly, giving the byte-by-byte nature of encoding conversion.\n\nKeeping that in mind, I don't think there will be significant gains of decoding straight from the strings, especially in the light of needing to keep 2 versions of all decoding routines. Creating a small buffer is cheap nowadays, it's allocated from pools (I'm sure you knew that), so the only thing that we skip is writing a binary-encoded string to a buffer, which is also pretty cheap AFAIK.\n\nAlso, I believe a better way to do it in your project might be to feed decoder directly with slices of original buffer. This will require stateful codec approach, which is planned for v0.3, together with streaming.\n\nSo, I propose using current approach for now and optimize when v0.3 will be out (though no timeline promises here).\nBTW, I published a v0.2.10 to accept binary-encoded strings directly in `decode` function.\n. Yes, I agree about the boundary. Short test has shown that with string len=10, js routine is 5x faster than Buffer.write() with 'utf8' encoding. But on 100 char strings they are already equal in time. On 1000 char - js is 7.5x slower. Maybe this optimization could take better place in the node.js core.\n\n``` javascript\n(function() {\n    var buf = new Buffer(1000);\n    var str = '';\n    for (var i = 0; i < 10; i++)\n        str = str + 'helloworld'; // sample 10 char string.\n\n    var start = Date.now();\n    for (var i = 0; i < 1000000; i++) {\n        for (var j = 0, k = 0; j < str.length; j++) {\n            var c = str.charCodeAt(j);\n            if (c < 0x80) {\n                buf[k++] = c;\n            } else {\n                // Here we'll implement utf8.\n            }\n        }\n    }\n    console.log(\"JS only: \", Date.now() - start);\n\n    start = Date.now();\n    for (var i = 0; i < 1000000; i++) {\n        buf.write(str);\n    }\n    console.log(\"Cross-boundary: \", Date.now() - start);\n\n})() // Wrapped in function to let V8 optimize it.\n```\n\nAnyway I'll close this issue if you don't mind, because of the double-codebase concern. I just dont have enough time to support it, sorry.\n. Hi and thanks for your efforts!\n\nMy concerns:\n1. This needs proper testing. Please, use current testing infrastructure to write at least a couple of tests both for StreamEncoder and StreamDecoder.\n2. I believe StringDecoder.detectIncompleteChar is not a public Node.js API - not good. Are there any alternatives? Also it will not work for any multibyte encoding except utf8 & utf16.\n3. StreamEncoder will not work for [surrogate pairs](http://en.wikipedia.org/wiki/Surrogate_pair).\n\nI cannot accept this PR until these are addressed, and I believe items 2 and 3 cannot be addressed before a major rewrite of this library that I've started in a v0.3 branch awhile ago.\n. Hi and thanks for your efforts!\n\nMy concerns:\n1. This needs proper testing. Please, use current testing infrastructure to write at least a couple of tests both for StreamEncoder and StreamDecoder.\n2. I believe StringDecoder.detectIncompleteChar is not a public Node.js API - not good. Are there any alternatives? Also it will not work for any multibyte encoding except utf8 & utf16.\n3. StreamEncoder will not work for [surrogate pairs](http://en.wikipedia.org/wiki/Surrogate_pair).\n\nI cannot accept this PR until these are addressed, and I believe items 2 and 3 cannot be addressed before a major rewrite of this library that I've started in a v0.3 branch awhile ago.\n. Okay, I'll close this for now and will track the feature in #20.\n. Nice, thank you!\n. Published to npm as version 0.2.11.\n. Hi there! Added all v0.2.x tags. Thanks!\n. Hi Dan, thank you. Please make it compile (see travis error), add some tests and I'll be happy to merge.\n. Thats great, thank you! Can I ask you to add one more test? For a sufficiently large (~10kb) text in CP949, like a web page or some other text you might find relevant, could you check the encoding/decoding provides the same results as the iconv library? We did this with big5 and gbk for example. This is useful because the double-byte encodings are  a bit harder to check.\n. Thats a good work, thanks a lot Dan!\n. Yes, thank you Ralle, you're absolutely right.\n. Hi Ralle!\n\nDoing this the right way would require some non-trivial time, which I don't have right now (maybe in a month or two).\nWhat I would suggest for now is:\n1. Try to compare str == iconv.decode(iconv.encode(str, enc), enc). This should give you a good enough approximation.\n2. Or, use original iconv library.\n. I think the mechanism in #53 should help you. Closing this one.\n. Thanks Fabio! I'm keeping it in mind, but cant promise a timeline yet, sorry.\n. FYI: I've added support for Shift_JIS in V0.4 ( #51 ).\n. Hi! Can you post expected result and result given by iconv-lite?\n. ``` javascript\n> iconv.encode('\u05e9\u05dc\u05d5\u05dd \u05e2\u05d5\u05dc\u05dd!', 'win1255');\n<Buffer f9 ec e5 ed 20 f2 e5 ec ed 21>\n```\n\n``` bash\n~$ printf '\u05e9\u05dc\u05d5\u05dd \u05e2\u05d5\u05dc\u05dd!' | iconv -f utf-8 -t cp1255 | hexdump\n0000000 f9 ec e5 ed 20 f2 e5 ec ed 21                  \n```\n\nSeems ok for me. Closing.\n. Yes, sure. `iconv.decode(buf, encoding)` takes binary data and decodes it into a JS string. So, what you're need here is:\n\n``` javascript\noriginalData = new Buffer(\"base64 string with ISO-8859-1 encoding goes here\", \"base64\");  // Notice, no .toString().\njsStr = iconv.decode(originalData, \"ISO-8859-1\");\n\n// Here you can use jsStr as usual javascript String:\njsStr.replace(\"hello\", \"world\");\n\n// If you need a buffer with this string UTF-8 encoded, then you can use Buffer like that:\nutf8EncodedStringBuf = new Buffer(jsStr);  // utf8 is the default encoding.\n\n// Or, equivalent\nutf8EncodedStringBuf = iconv.encode(jsStr, \"utf8\");\n```\n. It depends on how that string was converted from the actual bytes. \n\nIf you have a JS string which was UTF-8-decoded from a buffer of ISO-8859-1-encoded data, then the recovery is possible only if the original string was all ASCII. UTF-8 is (obviously) not compatible with ISO-8859-1 and will produce all sorts of weird chars and decoding errors otherwise.\n\nIn general, you should attempt to get original bytes (in Buffer-s), that way it's easier and more robust.\nIf its not possible, then the best you can do is to try do encode it back as UTF8, then decode as ISO-8859-1, but it'll surely lose some information.\n\n``` javascript\nstr = iconv.decode(new Buffer(str), \"ISO-8859-1\");\n```\n. You're inadvertently converting the `chunk` to string by doing `buffer += chunk`. You should keep it as a buffer, something like this:\n\n``` javascript\n        var buffer = new Buffer(0);\n        stream.on(\"data\", function(chunk) {\n            buffer = Buffer.concat([buffer, chunk]);\n        });\n\n        stream.once(\"end\", function(){\n            str = iconv.decode(buffer, \"ISO-8859-1\");\n        });\n\n```\n. Hi! You should try it like this:\n\n``` javascript\n$ = cheerio.load(iconv.decode(body, 'iso-8859-1'));\n```\n\niconv.decode converts a Buffer of encoded data to a JS string, this is exactly what you need here.\n. Added in v0.4 ( #51 )\n. Fixed in 0.4 ( #51 )\n. Hi! Could you be more specific? Specifically a couple of lines of code with expected results would be helpful.\n. Ok, you are trying to decode a JS string, which is incorrect. You need to get original Buffer-s. Please see https://github.com/ashtuchkin/iconv-lite/issues/36#issuecomment-25700690\n. No, sorry.\n. `iconvLite.decode(buf, 'gbk').indexOf('\ufffd') >= 0` ?\n. I think the mechanism in #53 will help in this situation - it will notify you if any incompatible character is found. Let me know if you need something else. Closing for now.\n. When you work with text data in javascript, you usually work in the following steps:\n1. Get some bytes with known encoding from external source (you get Buffer).\n2. Convert the source Buffer to a native js string (which is itself utf-16), using `iconv.decode` or `buf.toString('utf-8')`.\n3. Do something with native js strings. These are the only strings you can meaningfully work with. \n4. Convert output native js strings to an output Buffer encoded with destination encoding. (`iconv.encode` or `new Buffer(str, 'utf-8')`)\n5. Send output Buffer (bytes) to external party.\n\nSo, in your case I assume that the `str` is given to you as a native js string and so it's utf-16, not utf-8. If not, please ensure it's correctly decoded (just print it to console).\n\nSecond step, where you're converting it to an iso-8859-8 **Buffer**, is good.\n\nThe last line (`str = buf.toString()`) is meaningless because you're trying to read iso-8859-8 encoded buffer as a utf-8 encoded buffer -> you'll get garbage.\n\nAfter conversion to something other than native js strings, you should work with Buffers, not trying to convert it back. Usually you'll need some concatenation, use `Buffer.concat([buf1, buf2])`.\n. Why not?\n\n``` javascript\n\nxmlStr = \"<?xml version='1.0' encoding='ISO-8859-8'?>\" + originalXmlStr;\n\nbuf = iconv.encode(xmlStr, 'iso-8859-8');\n\nrequest({\n  url: \"..\",\n  method: \"POST\",\n  body: buf,\n  ...\n}, function(err, res, body) {\n  // do something.\n});\n```\n\nThe thing is, with `iso-8859` family of encodings, ASCII chars are kept as-is, so the xml header will be kept.\n. From request documentation: \n\n```\nbody - entity body for PATCH, POST and PUT requests. Must be a Buffer or String.\n```\n. We've been discussing it in #20. Browser support will require significant code refactor (f.ex. to make it possible to skip huge CJK tables), so I've no plans in nearest future to add this.\n\nYou can make a fork, of course, but I think for now its easier just to use http://code.google.com/p/stringencoding/\n. Thank you for your effort!\nWe took Big5 encoding from this website: http://moztw.org/docs/big5/  It seems that HKSCS is not compatible with encoding that we use ([Mozilla 1.8 b2u](http://moztw.org/docs/big5/table/moz18-b2u.txt)), so I'm not sure why would you want to create an alias for it if it doesn't work?\n\nThis aside, you can create this same alias in your program if you need it:\n\n``` javascript\nvar iconv = require('iconv-lite');\niconv.encodings['big5hkscs'] = 'big5';\n\n// use it\nstr = iconv.decode(buf, \"big5-hkscs\");\n```\n. Hi! FYI: In #51 I've added support for Big5-HKSCS, with astral characters and sequences. It is not released yet, but can already be tested.\nAlso, the documentation part will be tracked in #48.\nClosing this one.\n. Added howto/internals section to the [wiki](https://github.com/ashtuchkin/iconv-lite/wiki)\n. Thank you! I've used that to create a generic way to add double-byte encodings in V0.4 ( #51 ). Closing this one.\n. Thank you! Added aliases from your PR.\n. Oops, this bug already exists.\n. I agree, I don't really have time to do it now. The main obstacle is that we need to reimplement all the native Node encodings from scratch to support this feature and it's not easy.. @nleush Do you need one of them, or both?\n. Renamed issue to address only EUC-JP, as these are quite different. ISO-2022-JP moved to #60\n. Great, thanks for the report.\n. Hi! I think this character is in the extended, microsoft code page 932, not in the original Shift_JIS or JIS X 0208.\nI took the Shift_JIS table from [unicode.com](http://www.unicode.org/Public/MAPPINGS/OBSOLETE/EASTASIA/JIS/SHIFTJIS.TXT).\n\nLibiconv seems to agree:\n\n``` bash\n$ printf '\\x87\\x40' | iconv -f shift_jis -t utf-16be | hexdump\niconv: (stdin):1:0: cannot convert\n$ printf '\\x87\\x40' | iconv -f cp932 -t utf-16be | hexdump\n0000000 24 60\n```\n\n(U+2460 is \u2460)\n. http://www8.plala.or.jp/tkubota1/unicode-symbols-map2.html\n. Although I can probably use Encoding Standard's variation - they have these characters http://encoding.spec.whatwg.org/index-jis0208.txt\n. I moved to Encoding Standard as the base for Shift_JIS and CP932 encodings in b5a80d1160, so this issue should be fixed. Also, added test for this specific request, see below.\n. Seems that Encoding Standard is almost the same as CP932, extended for compatibility. The authors did a great job reviewing best practices from browsers there.\n. AFAIK, you just need to install node-iconv yourself for encoding module to\nuse it. Let me know if it helped.\n\nISO-2022-JP is a rare encoding nowadays and is quite different in structure\nthan most others (its stateful), so to support it I'd need to write\nseparate codec. I didn't want to make these efforts until at least a couple\nof people said they need it. Your vote is the first for now)\nOn Nov 7, 2014 5:05 AM, \"Max Ogden\" notifications@github.com wrote:\n\n> Ah actually I forgot to also mention that mailparser uses\n> https://www.npmjs.org/package/encoding which in turn uses iconv-lite, but\n> encoding is supposed to be normalization layer around iconv and\n> iconv-lite so it shouldn't have crashed here. Seems like a bug in encoding,\n> i'll file an issue there as well\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/60#issuecomment-62139268\n> .\n. Got it! Thanks\n\n## \n\nAlexander Shtuchkin\n\nOn Mon, Dec 1, 2014 at 8:24 AM, coyle notifications@github.com wrote:\n\n> I'll throw my vote in here. Just ran into the same error.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/60#issuecomment-65090106\n> .\n. Thank you!\n. Hi! You're probably using old version 0.2, I've yet to release a new version 0.4 (in a couple of days). For now you can use github master branch.\n. 0.4.0 is on the npm.\n. Profiling 0.11.12 (good):\n\n```\n   5582   93.8%  LazyCompile: *encoderSBCSWrite /Users/ashtuchkin/code/iconv-lite/encodings/sbcs-codec.js:47:26\n   5491   98.4%    LazyCompile: *encode /Users/ashtuchkin/code/iconv-lite/lib/index.js:13:31\n   5491  100.0%      Function: ~<anonymous> /Users/ashtuchkin/code/iconv-lite/test/performance.js:1:11\n   5491  100.0%        LazyCompile: ~Module._compile module.js:367:37\n   5491  100.0%          LazyCompile: ~Module._extensions..js module.js:465:37\n   5491  100.0%            LazyCompile: ~Module.load module.js:339:33\n\n    259    4.4%  node::smalloc::Alloc(v8::FunctionCallbackInfo<v8::Value> const&)\n    259  100.0%    LazyCompile: ~Buffer buffer.js:47:16\n    259  100.0%      LazyCompile: *encoderSBCSWrite /Users/ashtuchkin/code/iconv-lite/encodings/sbcs-codec.js:47:26\n    258   99.6%        LazyCompile: *encode /Users/ashtuchkin/code/iconv-lite/lib/index.js:13:31\n    258  100.0%          Function: ~<anonymous> /Users/ashtuchkin/code/iconv-lite/test/performance.js:1:11\n    258  100.0%            LazyCompile: ~Module._compile module.js:367:37\n```\n\nv0.11.13 (bad)\n\n```\n  21253   17.5%  v8::internal::JSObject::SetElement(v8::internal::Handle<v8::internal::JSObject>, unsigned int, v8::internal::Handle<v8::internal::Object>, PropertyAttributes, v8::internal::StrictMode, bool, v8::internal::SetPropertyMode)\n  21253  100.0%    LazyCompile: *encoderSBCSWrite /Users/ashtuchkin/code/iconv-lite/encodings/sbcs-codec.js:47:26\n  21003   98.8%      LazyCompile: *encode /Users/ashtuchkin/code/iconv-lite/lib/index.js:13:31\n  21003  100.0%        Function: ~<anonymous> /Users/ashtuchkin/code/iconv-lite/test/performance.js:1:11\n  21003  100.0%          LazyCompile: ~Module._compile module.js:367:37\n  21003  100.0%            LazyCompile: ~Module._extensions..js module.js:465:37\n\n  21043   17.4%  LazyCompile: *encoderSBCSWrite /Users/ashtuchkin/code/iconv-lite/encodings/sbcs-codec.js:47:26\n  20760   98.7%    LazyCompile: *encode /Users/ashtuchkin/code/iconv-lite/lib/index.js:13:31\n  20760  100.0%      Function: ~<anonymous> /Users/ashtuchkin/code/iconv-lite/test/performance.js:1:11\n  20760  100.0%        LazyCompile: ~Module._compile module.js:367:37\n  20760  100.0%          LazyCompile: ~Module._extensions..js module.js:465:37\n  20760  100.0%            LazyCompile: ~Module.load module.js:339:33\n\n  19456   16.1%  v8::internal::Runtime::SetObjectProperty(v8::internal::Isolate*, v8::internal::Handle<v8::internal::Object>, v8::internal::Handle<v8::internal::Object>, v8::internal::Handle<v8::internal::Object>, PropertyAttributes, v8::internal::StrictMode)\n  19456  100.0%    LazyCompile: *encoderSBCSWrite /Users/ashtuchkin/code/iconv-lite/encodings/sbcs-codec.js:47:26\n  19216   98.8%      LazyCompile: *encode /Users/ashtuchkin/code/iconv-lite/lib/index.js:13:31\n  19216  100.0%        Function: ~<anonymous> /Users/ashtuchkin/code/iconv-lite/test/performance.js:1:11\n  19216  100.0%          LazyCompile: ~Module._compile module.js:367:37\n  19216  100.0%            LazyCompile: ~Module._extensions..js module.js:465:37\n\n  17416   14.4%  v8::internal::JSObject::SetElementWithoutInterceptor(v8::internal::Handle<v8::internal::JSObject>, unsigned int, v8::internal::Handle<v8::internal::Object>, PropertyAttributes, v8::internal::StrictMode, bool, v8::internal::SetPropertyMode)\n  17416  100.0%    LazyCompile: *encoderSBCSWrite /Users/ashtuchkin/code/iconv-lite/encodings/sbcs-codec.js:47:26\n  17195   98.7%      LazyCompile: *encode /Users/ashtuchkin/code/iconv-lite/lib/index.js:13:31\n  17195  100.0%        Function: ~<anonymous> /Users/ashtuchkin/code/iconv-lite/test/performance.js:1:11\n  17195  100.0%          LazyCompile: ~Module._compile module.js:367:37\n  17195  100.0%            LazyCompile: ~Module._extensions..js module.js:465:37\n\n  15557   12.8%  v8::internal::ExternalUint8Array::SetValue(v8::internal::Handle<v8::internal::ExternalUint8Array>, unsigned int, v8::internal::Handle<v8::internal::Object>)\n  15557  100.0%    LazyCompile: *encoderSBCSWrite /Users/ashtuchkin/code/iconv-lite/encodings/sbcs-codec.js:47:26\n  15373   98.8%      LazyCompile: *encode /Users/ashtuchkin/code/iconv-lite/lib/index.js:13:31\n  15373  100.0%        Function: ~<anonymous> /Users/ashtuchkin/code/iconv-lite/test/performance.js:1:11\n  15373  100.0%          LazyCompile: ~Module._compile module.js:367:37\n  15373  100.0%            LazyCompile: ~Module._extensions..js module.js:465:37\n\n  10792    8.9%  v8::internal::Runtime_SetProperty(int, v8::internal::Object**, v8::internal::Isolate*)\n  10792  100.0%    LazyCompile: *encoderSBCSWrite /Users/ashtuchkin/code/iconv-lite/encodings/sbcs-codec.js:47:26\n  10657   98.7%      LazyCompile: *encode /Users/ashtuchkin/code/iconv-lite/lib/index.js:13:31\n  10657  100.0%        Function: ~<anonymous> /Users/ashtuchkin/code/iconv-lite/test/performance.js:1:11\n  10657  100.0%          LazyCompile: ~Module._compile module.js:367:37\n  10657  100.0%            LazyCompile: ~Module._extensions..js module.js:465:37\n\n   5862    4.8%  v8::internal::ExternalUint8Array::SetValue(unsigned int, v8::internal::Object*)\n   5862  100.0%    LazyCompile: *encoderSBCSWrite /Users/ashtuchkin/code/iconv-lite/encodings/sbcs-codec.js:47:26\n   5790   98.8%      LazyCompile: *encode /Users/ashtuchkin/code/iconv-lite/lib/index.js:13:31\n   5790  100.0%        Function: ~<anonymous> /Users/ashtuchkin/code/iconv-lite/test/performance.js:1:11\n   5790  100.0%          LazyCompile: ~Module._compile module.js:367:37\n   5790  100.0%            LazyCompile: ~Module._extensions..js module.js:465:37\n\n   4985    4.1%  Stub: CEntryStub\n   3602   72.3%    LazyCompile: *encoderSBCSWrite /Users/ashtuchkin/code/iconv-lite/encodings/sbcs-codec.js:47:26\n   3560   98.8%      LazyCompile: *encode /Users/ashtuchkin/code/iconv-lite/lib/index.js:13:31\n   3560  100.0%        Function: ~<anonymous> /Users/ashtuchkin/code/iconv-lite/test/performance.js:1:11\n   3560  100.0%          LazyCompile: ~Module._compile module.js:367:37\n   3560  100.0%            LazyCompile: ~Module._extensions..js module.js:465:37\n   1368   27.4%    LazyCompile: *encode /Users/ashtuchkin/code/iconv-lite/lib/index.js:13:31\n   1368  100.0%      Function: ~<anonymous> /Users/ashtuchkin/code/iconv-lite/test/performance.js:1:11\n   1368  100.0%        LazyCompile: ~Module._compile module.js:367:37\n   1368  100.0%          LazyCompile: ~Module._extensions..js module.js:465:37\n   1368  100.0%            LazyCompile: ~Module.load module.js:339:33\n\n   3063    2.5%  v8::internal::HandleScope::ZapRange(v8::internal::Object**, v8::internal::Object**)\n   3063  100.0%    LazyCompile: *encoderSBCSWrite /Users/ashtuchkin/code/iconv-lite/encodings/sbcs-codec.js:47:26\n   3020   98.6%      LazyCompile: *encode /Users/ashtuchkin/code/iconv-lite/lib/index.js:13:31\n   3020  100.0%        Function: ~<anonymous> /Users/ashtuchkin/code/iconv-lite/test/performance.js:1:11\n   3020  100.0%          LazyCompile: ~Module._compile module.js:367:37\n   3020  100.0%            LazyCompile: ~Module._extensions..js module.js:465:37\n```\n. @mithgol yes, I think its the same one. Thanks! It seems we'll have to wait for the fix.\n. Just tested the 0.11.14-pre (https://github.com/joyent/node/commit/9452ea2ef5bbaf4c0b03764c53899d97d0f210f3) and it works great, even faster than before. So, just waiting for 0.11.14 (or 0.12.0) release.\n\n```\nwindows-1251 charset performance test:\n\nEncoding 262144 chars 10000 times:\niconv: 24599ms, 104.07 Mb/s.\niconv-lite: 6106ms, 419.26 Mb/s.\n\nDecoding 262144 bytes 10000 times:\niconv: 23217ms, 110.26 Mb/s.\niconv-lite: 9683ms, 264.38 Mb/s.\n```\n. I've created a [wiki page](https://github.com/ashtuchkin/iconv-lite/wiki/Supported-Encodings) for it linked from readme.\n\nBasically, all encodings from your module are supported except cp808. I'll add it soon.\n. Added codepage 808 & released to npm in version 0.4.1.\n. Hi Carlos! Why do you think it is needed? What is your use case? I see something like this `str = iconv.decode(buf)` to be misleading.\n. Yes, I think explicitness here is better for the users. Thank you for your efforts!\n. That makes sense, thank you!\n. Published to npm in v0.4.2.\n. I was waiting for someone who needs it :) (see #56). I'll try to implement it in the next couple of days, don't bother. Just out of curiosity, where do you need that?\n. I've added both UTF-16 and UTF-16BE in new version 0.4.3 (on NPM).\n\n``` javascript\n// == UTF-16 codec =============================================================\n// Decoder chooses automatically from UTF-16LE and UTF-16BE using BOM and space-based heuristic.\n// Defaults to UTF-16BE, according to RFC 2781, although it is against some industry practices, see\n// http://en.wikipedia.org/wiki/UTF-16 and http://encoding.spec.whatwg.org/#utf-16le\n// Decoder default can be changed: iconv.decode(buf, 'utf16', {default: 'utf-16le'});\n\n// Encoder prepends BOM and uses UTF-16BE.\n// Endianness can also be changed: iconv.encode(str, 'utf16', {use: 'utf-16le'});\n```\n\nLet me know how it goes and if you need anything else.\nI can also create a UTF-32 encoding, please create another issue about it if you need that.\n. No problemo amigo)\n. Thanks!\n. Hi! \n\nI haven't checked it manually, but I think most names that are used in the real world will be interpreted correctly. First, names are not case sensitive in iconv-lite and all non-alphanumeric characters are stripped, as well as what seems like a year suffix (i.e. `ISO_8859-1:1987` = `ISO8859-1` = `iso88591`). Then, there are lots of aliases, for example `win1251` = `windows1251` = `1251` = `Windows-1251`.\n\nIf you have a limited set of encodings that you support, you can check them all with `iconv.encodingExists(name)` function. Then, you can either tell me to add an alias, or do it in your project with the following code:\n\n```\nvar iconv = require('iconv-lite');\niconv.encode('utf8', ''); // load encoding definitions\niconv.encodings['<new alias name>'] = '<alias to what>'; // both lowercase, alphanumeric.\n```\n\nIf, on the other hand, you want to support all IANA names and aliases, then, probably, iconv-lite is not an option because it doesn't support all the older encodings (neither do node-iconv AFAIK).\n. Ok, I see there are two UTF-7 formats - one original ([RFC2152](https://tools.ietf.org/html/rfc2152)) and the other modified, used in IMAP [RFC3501 sec5.1.3](http://tools.ietf.org/html/rfc3501#section-5.1.3).\n\nDo you mind if I name them UTF-7 and UTF-7-IMAP (as was done f.ex. in http://weldon.whipple.org/sendmail/utf7imap/)?\n. Published as v0.4.4 on npm.\n. I don't quite understand when it could be useful compared to a callback. Do you think several invalid characters could be handled in a smarter way if handled together?\n\nThe analogy to `String#split` seems a bit odd for me, especially when you need to check `index%2` to get if the string is valid or invalid. Moreover, invalid portions will need to be in Buffer-s, as we couldn't convert them to js string.\n\nIf there is really a smarter way to handle multiple invalid chars as opposed to single, then I'd suggest using the same callback, but passing multiple invalid bytes to it as a Buffer. What do you think?\n. Ok, now I got you, thanks for the explanation! \n\nWell, this would require additional code that does just this in all codecs. It's rather big time investment for me, plus burden to all future codecs, so I'm a bit reluctant to implement it without a hard use case.\n\nThe use case you described above is mostly solved by a callback, with the exception of single valid char surrounded by invalid, which I believe is a rare case (that the format supports and recommends it).\n. I'm still not convinced in the _usefulness_ of this functionality and it will not only increase code complexity/size and API surface, but also increase the burden of adding each new encoding, even if you write all the code for now. IMHO it's just not worth it until we find a compelling use case.\n. Ok, but why can't you do this using callbacks? Each range of non-encodable\ncharacters will be given to this callback and it can either throw or return\na string to replace them, similar to `.replace(regexp, function)` in\njavascript. This would also allow transliteration and checking that all\ncharacters are encodable. A much more flexible mechanism, which is also\nfits nice with streaming mode.\n\nWhen the scheme stabilizes, you can also make it into its own codec. Or I\ncan generalize it and make a meta-codec that will take basic encoding,\nescaping rules and escaping encoding as parameters.\n\nWhat do you think?\n\n## \n\nAlexander Shtuchkin\n\nOn Thu, Jul 17, 2014 at 5:08 AM, Mithgol notifications@github.com wrote:\n\n> In a nutshell this use case is a generalization of the UTF-7's use case:\n> the Unicode characters are forced to some 8-bit medium (defined by\n> a single-byte encoding) instead of UTF-7's original 7-bit medium.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/73#issuecomment-49298302\n> .\n. I'm thinking about the latter case, that would be more convenient and probably faster too. There is one issue though - as iconv-lite is stream-oriented and working chunk-by-chunk, I would not want to accumulate ranges of invalid characters, because we can run out of memory on very long invalid streams.\n\nSo I'm thinking about the following interface:\n\n``` javascript\nfunction unencodableHandler(str, offset, context, rangeStarted, rangeFinished) {\n  // str is a string of unencodable characters.\n  // offset - index of str in context.\n  // context is the whole string (chunk) currently encoded.\n  // rangeStarted flag is true when str starts a range of contiguous invalid characters.\n  // rangeFinished flag is true when str completes the range.\n  // Flags are always true in non-streaming usage.\n\n  // Return characters that can be translated thus far.\n  return (rangeStarted ? start_escape : \"\") + escape(str) + (rangeFinished ? end_escape : \"\");\n}\n\n// Convert a string - all unencodable strings will be complete.\nbuf = iconv.encode(str, \"cp866\", {unencodableHandler: unencodableHandler});\n\n// Or a stream\ninputStream.pipe(iconv.encodeStream(\"cp866\", {unencodableHandler: unencodableHandler})).pipe(outStream);\n```\n. Sorry, not much free time recently. I do remember about it and will fix eventually.\n. Hi Ribhararnus! We use core nodejs functions to convert to/from utf8, so this would be hard to implement. And I would think it's a bit out of scope of this project. Thanks for suggestion!\n. Hi, yes I did, thank you. Although even if it was defined, it would throw anyway because there's an \"unknown table value when decoding\".  Could you post the buffer where it happens?\n. No problem, I don't need any private code. Anyway, I've fixed reference error, could you test the master branch before I publish it to npm?  `npm install ashtuchkin/iconv-lite` will install it.\n. Yes, seems that you're giving it array of objects, not buffer. I'll release this change to npm later if you don't mind, as it's pretty minor issue. Thanks!\n. Hi! There is no limit. Moreover, iconv-lite is designed to be stream-first, so it can process gigabytes without choking.\n\nI tried to reproduce your error, but to no avail. I used this code, it worked correctly:\n\n``` javascript\n    grunt.registerTask('test', \"test\", function() {\n        var content = grunt.file.read('public/js/jquery.js')\n        console.log(content)\n    });\n```\n\nSize of jquery is ~200 kb. Node v0.10.31, iconv-lite v0.2.11, grunt v.0.4.5\n\nCan you provide a test case?\n. No problem, thank you for re-checking it)\n. Segfault in javascript-only module (which iconv-lite is) has a strong smell of core node or V8 problem. Oh! Here it is: joyent/node#8208\n\nI confirm the problem in Node v0.10.31 and that it's fixed in v0.10.32-pre, after joyent/node#8224. We just need to wait for this release. For now, the only option is to downgrade to 0.10.30. Thank you!\n. Could you post:\n- the contents of the file you convert\n- contents of 'data' variable in your first script?\n- version of node and iconv-lite you use\n. Although I probably know what the problem is. The file you read has a [BOM character](http://en.wikipedia.org/wiki/Byte_order_mark) in the beginning. Unfortunately, Node.js keeps it by default  (see joyent/node#1918). Then, GBK encoding has no code for the BOM character, so iconv-lite replaces it by '?'.\n\nSolutions:\n1. Add `data = data.replace(/^\\uFEFF/, '');` before encoding.\n2. Use ['strip-bom' module](https://github.com/sindresorhus/strip-bom) for more readability.\n\nLet me know if it helps.\n. Are you using node v0.10.31? If so, you should upgrade, see #77\n. Closing. Feel free to reopen if the issue persists.\n. Ok, then try to:\n1. Upgrade node to latest 0.10 and recheck.\n2. Replace all `iconv.decode(buf, 'utf-8')` with `buf.toString('utf8')` - is it still crashing?\n. Hi!\nMy initial guess is that although you convert content encoding to (probably) utf8, there is still a `<meta http-equiv=\"content-type\" content=\"text/html; charset=Shift_JIS\">` inside the html, which makes Chrome think that this utf8 is actually shift_jis and decode it one more time. Can you check if this is the case?\nIf yes, you'll need to filter out this line. If no, I'll need at least your input/output files and preferably a small separate test case that shows this behavior.\n. Maybe its an issue with concat-stream module? BTW, you could use iconv-lite's concating capability:\n\nsrc.pipe(iconv.decodeStream('win1251')).collect(function(err, body) {...})\n. Hi! This is one of the planned use cases for user callback feature (#53). \nFor now you can workaround by just replacing illegal chars (\ufffd) with empty string.\n. Thank you!\n. Well, iconv-lite doesn't decode utf8 itself, it uses Node core alg for performance reasons.\nI'm not sure its wise to reimplement it and likely lose performance just for this.\nAs a workaround, you can check for the presence of character \ufffd in result, although it could be a false positive. Another workaround is to use node-iconv.\n. Like I said above, at the moment, the only way to do that is to check for the presence of character \ufffd in result. The thing is, I don't control UTF-8 decoding - it's done by Node.js. I could reimplement it in javascript, but I think it'd be a lot slower.\n. \"\ufffd\" is used for decoding, \"?\" for encoding (as many encodings cannot\nrepresent \ufffd).\n\n## \n\nAlexander Shtuchkin\n\nOn Mon, May 25, 2015 at 10:57 PM, Benjamin Pasero notifications@github.com\nwrote:\n\n> @ashtuchkin https://github.com/ashtuchkin got it. But in the docs you\n> say \"Untranslatable characters are set to \ufffd or ?\", so it might also be \"?\"\n> which I cannot really check for because its a valid character.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/83#issuecomment-105406513\n> .\n. 1. Are you talking about encoding or decoding? I thought decoding, right?\n2. Uft8 is still outside my control.\n   On May 26, 2015 05:19, \"Benjamin Pasero\" notifications@github.com wrote:\n\n> Right, but \"?\" is ambiguos because it might also be a character used in\n> the original text. I think raising some flag or error would make more sense\n> to catch this case.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/83#issuecomment-105504072\n> .\n. Yeah, that's a completely separate issue from \"ability to validate UTF-8\nencoding\" :)\n\nI have a callback-like mechanism in mind (see #53), where you can either\nthrow exception or define replacements for characters that cannot be\nrepresented. Still haven't got a moment to implement it though.\u200b\n\n## \n\nAlexander Shtuchkin\n\nOn Tue, May 26, 2015 at 10:25 AM, Benjamin Pasero notifications@github.com\nwrote:\n\n> In our case, we use encode() to convert an existing file to another\n> encoding. E.g. we have a source UTF-16 file that we want to save as DOS\n> encoding. DOS can not present all encodings (Chinese et. al), so I would\n> like to show an error to the user. Similar to how in Sublime Text you will\n> not be able to save a file in an encoding that cannot represent all\n> characters you have before encoding.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/83#issuecomment-105611321\n> .\n. Upgrade node.js. See #77\n. There are a lot of interpretations of what exactly \"big5\" encoding is. I used the one of [Encoding Standard](https://encoding.spec.whatwg.org), it's basically cp950 with the HKSC and other common extensions. There, both A451 and A2CC are valid encodings of '\u5341'.   See [more explanations in comments](https://github.com/ashtuchkin/iconv-lite/blob/master/encodings/dbcs-data.js#L127).\n\nDoes it give you any problems?\n. Released a 0.4.6 with a fix for it. Thank you!\nIt was easier just to remove the dots in alias-defining file.\n. Interesting. Although I'm sure it can be done without additional generated file.\nCould you describe in more detail what you mean by \"ASCII-compatible\" charset? How that'll save you decoding? In my view, utf8 is not ASCII-compatible at all)\n. Got it. Makes sense.\nI'm thinking to reuse current caching architecture, but avoid making it part of public api yet (it'd require tests, docs, etc.). What do you think about `iconv.getCodec(encodingName).asciiCompatible`? It's cached, but must load the data for codec once.\n. Unfortunately this breaks the encoding search algorithm (as you can see in the travis report). Much easier to just pre-strip aliases. (see my commit https://github.com/ashtuchkin/iconv-lite/commit/b832d8fb3e5b335f89b6fe801ac26c0897818ad7)\n. This is so bad) You'd screw up any `for (in)` cycle with this. If the method is still needed, I'd recommend creating it as a non-enumerable property, like that:\n\n``` coffeescript\nObject.defineProperty Object.prototype, \"permits\", {enumerable: false, value: -> \"\"}\n```\n\nAnyway, I made it work even with enumerable prop with soon-to-be-released version 0.4.7.\n. I wasn't aware of that trick, thank you! We should probably add 0.12 there as well (I'll do that).\n. I agree that we should keep the tests as long as it's not a lot of hassle. Thank you Doug!\nThis is wonderful to have a supportive community)\n. Looks like iconv removed support for a couple of cp encodings. I tried upgrading before, but got the same errors.\n. Note, this is fixed in 14cf0cacb84e21a6d32efdda74e7ee90006dae34 after I changed tests to skip encodings that are not supported in node-iconv in ff06d8a4a4ad9ab66aa8560c789cabc5985dcaef.\n. Added bnoordhuis/node-iconv#118 to get these encodings back.\n. No, it's a bad idea. Please read https://github.com/ashtuchkin/iconv-lite/wiki for explanation about how to use iconv-lite correctly.\n. Published new version 0.4.8 with this alias added.\n. It should be.\nOn Apr 24, 2015 03:35, \"khalilTN\" notifications@github.com wrote:\n\n> Is this already available on npm ?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/94#issuecomment-95885314\n> .\n. Seems that the character table loading is screwed up, but I need more information to understand what's going on. Do you use browserify to package Chrome app? Can you isolate the bug in a separate mini-project so that I could reproduce it?\n. Thanks for figuring out what happened, that was weird)\n. Yes, it's used only when creating lookup tables, at the time of codec creation (first usage of encoding), for performance reasons.\n\nAs a temporary workaround you can clean the codec cache every time before changing defaultCharSingleByte:\n\n``` javascript\niconv._codecDataCache = {};\niconv.defaultCharSingleByte = <new value>;\n```\n\nI don't intend to change this behavior as this will be a non-issue once #73 is fixed, so closing for now. Feel free to reopen if you have other suggestions.\n. Hey Benjamin, thanks, and thanks for the suggestion. Do you have any interface ideas in mind? How do you see it in code (from your side, not internally)?\n\nAt the moment we have completely separate operations of decode vs encode, with only strings between them, so I don't really know how to pass this \"if file had a BOM\" information. We shouldn't keep the BOM in decoded strings, right?\n\nMoreover, right now for UTF8 and UTF16 we pass the data straight to native Node.js implementation, so it might be a bit tricky to handle, although probably possible.\n. Thanks, that makes sense. So basically decode will always remove BOM and all decoded strings would not have it, plus encode would add BOM if required by encoding name, right? I'm also thinking about adding a flag instead of adding encoding names, what do you think about that?\n\nAlso, can you describe your use case a bit more, so that I could understand context, if it's not secret of course? Are you working on a text editor?\n. Got ya. I love how Microsoft reinvents itself recently. You rock!\n\nGive me a couple of days, I'll add it.\nOn May 21, 2015 23:51, \"Benjamin Pasero\" notifications@github.com wrote:\n\n> Basically I would argue that a string should never ever include a BOM\n> because I cannot think of a use case where you would want the BOM in the\n> string. So, decode() looks at BOMs and removes them because it returns a\n> string.\n> \n> Encode should add the BOM if specified (either by option or encoding name).\n> \n> In many cases you would want a BOM for UTF16LE and UTF16BE anyways.\n> Because tools cannot know which encoding to use by looking at the bytes. I\n> believe UTF16 without BOM does not make a lot of sense.\n> \n> I am working for Visual Studio Code (https://code.visualstudio.com/) and\n> iconv-lite is being used for all encoding matters.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/97#issuecomment-104540648\n> .\n. Hey, I've published v0.4.9 today, would love to hear feedback)\n. Cool, nice to hear that!\n\n## \n\nAlexander Shtuchkin\n\nOn Wed, May 27, 2015 at 10:32 AM, Benjamin Pasero notifications@github.com\nwrote:\n\n> @ashtuchkin https://github.com/ashtuchkin added to our editor and\n> things are smooth so far. thanks for the prompt implementation and\n> responsiveness!\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/97#issuecomment-106006077\n> .\n. AFAIK it's more often used in practice (f.ex. Windows defaults to LE), plus\nit's native to Node.js and x86 architecture.\nOn May 26, 2015 7:49 PM, \"Douglas Christopher Wilson\" <\nnotifications@github.com> wrote:\n\n> Hi! I was looking around at the recent changes and recent closed issues,\n> but wasn't able to find the answer to this question:\n> \n> Why was the default UTF-16 encoding changed from BE to LE (when there is\n> no BOM, and of course, the content contains no space characters, which are\n> atypical in JSON content)?\n> \n> Yes, I know I can alter the default, but I was just seeking for the reason\n> behind the change so I can evaluate what I want to do when upgrading to the\n> latest version of this module.\n> \n> Thanks!\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/98.\n. BTW, You've got me thinking, why do we use only spaces? I think I'll add ASCII chars into consideration as well - that would be much more interesting.\n. Just released v0.4.10 with the improved heuristic (taking into account any ASCII chars, not just spaces). Now, JSON or not, the default encoding should be not really important.\n. It seems Node v0.12 doesn't support encoding as a second argument ([see doc](https://nodejs.org/api/fs.html#fs_fs_createreadstream_path_options)), only options dictionary. \n\nShould work fine when called like you wrote in the first post (`fs.createReadStream(fileName,{encoding:'gbk'})`). Let me know if it doesn't work when called that way.\n. Hmm, just checked and it works for me. Can you remove `iconv.undoExtendNodeEncodings()` from your function? The actual read is happening after you return from function, so you shouldn't do this until an 'end' or 'close' event.\n. Yeah, you shouldn't call `iconv.undoExtendNodeEncodings();` before the decoding starts :) Glad you worked it out.\n. Sorry I don't understand. Could you elaborate?\nOn Jun 8, 2015 8:56 PM, \"limeng\" notifications@github.com wrote:\n\n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/100.\n. A sample is on the home page of this project (\nhttps://github.com/ashtuchkin/iconv-lite). Basically you do `str =\niconv.decode(buf, '<encoding>');`, where `buf` is the raw data you read\nfrom TCP socket (type Buffer). Most popular Chinese encoding is 'gbk', but\nthere could be some others as well, you'll need to check it.\n\n## \n\nAlexander Shtuchkin\n\nOn Tue, Jun 9, 2015 at 12:08 AM, limeng notifications@github.com wrote:\n\n> How do I decode chinese?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/100#issuecomment-110255064\n> .\n. Yeah, I see you have a network error, then try to decode a 'null' buffer. You should check that the buffer is valid before calling `iconv.decode()`.\n. The nodejs utf8 decoder should support it afaik. Could you try it?\nOr do you need an encoder?\nOn Jul 3, 2015 11:22, \"Douglas Christopher Wilson\" notifications@github.com\nwrote:\n\n> Hi! So I really need to get CESU-8 implemented somewhere for use with old\n> crap like various databases. Would you be willing to add CESU-8 support to\n> this module :)?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/102.\n. I think I saw references to CESU-8 in the Node code, so I tend to think it\nwas intentional.\n\nAbout the encoder - if you need it quickly, you could use something like\nthis:\n\n```\nbuf = Buffer.concat(str.split('').map(function(s) {return new Buffer(s);}));\n```\n\nBasically, CESU-8 is just a UTF-8 with UTF-16 surrogates encoded as a\nseparate chars.\n\nI'll also add this to the encoding wish list and get to it when I have\ntime. Deal? :)\n\n## \n\nAlexander Shtuchkin\n\nOn Fri, Jul 3, 2015 at 12:23 PM, Douglas Christopher Wilson <\nnotifications@github.com> wrote:\n\n> I need both an encoder and a decoder (because I need to write to the\n> databases, not just read). The Node.js UTF-8 decoder does support CESU-8,\n> yes. This is more about an encoder (though I believe Node.js utf8 only\n> supports CESU-8 out of coincidence, rather than intention).\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/102#issuecomment-118410982\n> .\n. Works for me... Node v0.12.4\n\n```\n> str = \"\\uD801\\uDC37\"\n'\ud801\udc37'\n> buf = Buffer.concat(str.split('').map(function(s) {return new\nBuffer(s);}));\n<Buffer ef bf bd ef bf bd>\n```\n\n## \n\nAlexander Shtuchkin\n\nOn Fri, Jul 3, 2015 at 1:06 PM, Douglas Christopher Wilson <\nnotifications@github.com> wrote:\n\n> I tried that trick but it does not work. Node replaces all the surrogate\n> pairs with the Unicode replacement character...\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/102#issuecomment-118419191\n> .\n. Ahh, yeah, that's the different character, sorry.\n\n## \n\nAlexander Shtuchkin\n\nOn Fri, Jul 3, 2015 at 1:32 PM, Alexander Shtuchkin ashtuchkin@gmail.com\nwrote:\n\n> Works for me... Node v0.12.4\n> \n> ```\n> > str = \"\\uD801\\uDC37\"\n> '\ud801\udc37'\n> > buf = Buffer.concat(str.split('').map(function(s) {return new\n> Buffer(s);}));\n> <Buffer ef bf bd ef bf bd>\n> ```\n> \n> ## \n> \n> Alexander Shtuchkin\n> \n> On Fri, Jul 3, 2015 at 1:06 PM, Douglas Christopher Wilson <\n> notifications@github.com> wrote:\n> \n> > I tried that trick but it does not work. Node replaces all the surrogate\n> > pairs with the Unicode replacement character...\n> > \n> > \u2014\n> > Reply to this email directly or view it on GitHub\n> > https://github.com/ashtuchkin/iconv-lite/issues/102#issuecomment-118419191\n> > .\n. Fixed in e75dacaede131585e5e07268ae1467f40f02f990, published on npm as v0.4.11. Thanks for getting my attention to it!\n. Cool)\n. You could create an encoding definition JSON and add it to `iconv.encodings`, something like:\n\n``` javascript\niconv.encodings['big5uao'] = {\n  type: '_dbcs',\n  table: <your correspondence table>\n}\n```\n\nBefore matching encoding name, it's lowercased and stripped of all non-alphanum characters, so you should do that too when adding field to `iconv.encodings`.\n\nKey points in code:\n- https://github.com/ashtuchkin/iconv-lite/blob/master/lib/index.js#L61\n- https://github.com/ashtuchkin/iconv-lite/blob/master/encodings/dbcs-data.js#L160\n- https://github.com/ashtuchkin/iconv-lite/tree/master/encodings/tables\n- https://github.com/ashtuchkin/iconv-lite/blob/master/generation/utils.js#L59  (table format)\n. That's a large work, thank you so much for doing it! I'll look into it asap.\n. Just tried it and I have a different `too-large-conv.html` produced. What version of node do you use?\n. Tried node versions 0.12.4, 0.11.13, 0.12.7, io.js v3.3.0, all worked well (too-large.html and too-large-conv.html were the same). Maybe architecture is different? Are you using x86 or x64? Can you give more info on your machine specs?\n. Hmm.. All my test machines are mac/Ubuntu, so probably related to Windows.\n\nMy guess is that line endings are changed somewhere on load/save file, on\nNode level or on OS level.\n\nCan you write a function that checks line endings in Buffer-s before and\nafter iconv-lite conversion?\n\nAnother test - can you skip conversion for 'too-large', just load - copy\nbuffer - save?\n\nThird test - can you try io.js latest? They use newer version of V8 & libuv\nthat could have this issue resolved.\nOn Sep 4, 2015 4:27 AM, \"bit-seq\" notifications@github.com wrote:\n\n> The problem is still given, Windows 10 (updated from Windows 7), Node\n> v0.12.7 x64.\n> On a notebook I too wasn't able to reproduce the raised behaviour. I\n> attempted to run the decoding with Node v012.7 x64 on fresh installed\n> operating systems Ubuntu 14.04.3 and Windows 10.\n> \n> Any hints to look for?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/104#issuecomment-137710911\n> .\n. Ok, so I found the problem here. The issue is a node bug and documented as [node#1024](https://github.com/nodejs/node/issues/1024). Affected Node versions: v0.11.7 - v1.4.3. Fixing commit: [1640dedb3b2](https://github.com/nodejs/node/commit/1640dedb3b2a8d6e54ba7b22290d86d5984768be)\n\nA bit more context: when decoding, iconv-lite fills out ucs2-encoded Buffer that then is converted to JS string. The linked issue prevented correct conversion at this step.\n\nThe fix is to just upgrade node. Thanks for creating a reproducible test case.\n. Thanks @Mithgol! I fixed the CESU-8 and added warnings for extendNodeEncodings. Seems we cannot do anything at the moment with the latter issue.\n. Thanks for reporting it! Fixed in 15f2554c97c79f259ccbdc95a19209f25d13f217, will soon push to npm.\n. I guarantee :)\n\n## \n\nAlexander Shtuchkin\n\nOn Sun, Sep 27, 2015 at 8:12 PM, Douglas Christopher Wilson <\nnotifications@github.com> wrote:\n\n> Awesome, thank you, @ashtuchkin https://github.com/ashtuchkin !\n> \n> P.S. whatever happens in #107\n> https://github.com/ashtuchkin/iconv-lite/issues/107, please do not even\n> think about unpublishing 0.4.12 from npm, pretty please, as I am publishing\n> a module that depends on 0.4.12 :)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/106#issuecomment-143628598\n> .\n. What would you suggest as a solution/fix?\n\nIt was a hack from the beginning and we have no guarantees about something\nnot being broken with new Node changes. I also have no idea how to describe\nthat \"It doesn't work for `new Buffer`, but probably works for other\nencoding-related operations\".\n\nI'm open for discussion, but deprecation seemed the most valid option here.\nOn Sep 26, 2015 22:13, \"Mithgol\" notifications@github.com wrote:\n\n> The method .extendNodeEncodings from iconv-lite v0.4.11 cannot be used\n> in Node.js v4 to change the behaviour of its Buffer() constructor (\n> nodejs/node#2835 https://github.com/nodejs/node/issues/2835).\n> \n> The method .extendNodeEncodings from iconv-lite v0.4.12 does not work in\n> Node.js v4 at all (85938db\n> https://github.com/ashtuchkin/iconv-lite/commit/85938dbcb46ed9c98d712c363380fe41b7613c73\n> ).\n> \n> These changes are breaking for the modules that used .extendNodeEncodings\n> but refrained from using Buffer() in Node v4.\n> \n> The only workaround for these modules is to declare \"iconv\": \"0.4.11\"\n> as a dependency (without ~ or ^ before 0.4.11, because they cannot\n> rely on semantic versioning http://semver.org/) as soon as possible.\n> \n> (The dependency \"iconv\": \"\n> https://github.com/ashtuchkin/iconv-lite/tarball/15f2554c97c79f259ccbdc95a19209f25d13f217\n> \" might be useful for the modules that need CESU-8 decoder as well.)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/107.\n. Silly mistake.. Thanks!\n. This is by design. We must set this to utf8 to avoid Node interfering with decoding process. Actual encoding is set inside the converter (`conv` parameter) in [line 15](https://github.com/ashtuchkin/iconv-lite/blob/master/lib/streams.js#L15)\n. \u200bIf the stream you're piping to \u200bonly supports buffers (encoding=null),\nthen I'd suggest the following:\n\n```\nsource.pipe(iconv.decodeStream(encoding)).pipe(iconv.encodeStream('utf8')).pipe(aws_upload_whatever)\n\n```\n\nThe rule of thumb here is the following - decode converts encoded binary to\njs string; encode converts js string to encoded binary. To convert binary\n-> binary, you need to decode, then encode.\n\n## \n\nAlexander Shtuchkin\n\nOn Fri, Oct 30, 2015 at 2:02 AM, Dominik Lessel notifications@github.com\nwrote:\n\n> Ah okay.\n> \n> I tried a streaming conversion of S3 objects (CSV files) and the\n> AWS.S3.upload() method, which accepts a stream object, is blowing up, when\n> used with iconv-lite. After forking and editing streams.js#L79 to accept encoding:\n> null it worked as expected.\n> \n> TypeError: Object foo|bar|baz has no method 'copy'\n> at Function.Buffer.concat (buffer.js:499:9)\n> at ManagedUpload.fillStream (/var/task/node_modules/aws-sdk/lib/s3/managed_upload.js:386:21)\n> at IconvLiteDecoderStream.<anonymous> (/var/task/node_modules/aws-sdk/lib/s3/managed_upload.js:169:28)\n> at IconvLiteDecoderStream.emit (events.js:117:20)\n> at _stream_readable.js:944:16\n> at process._tickDomainCallback (node.js:486:13)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/110#issuecomment-152464893\n> .\n. I kinda tried to cover that in [wiki](https://github.com/ashtuchkin/iconv-lite/wiki), but any suggestions welcome!\n. Nope.. Will add if there's a need (a couple people ask)\nOn Nov 5, 2015 06:46, \"Mithgol\" notifications@github.com wrote:\n\n> Does iconv-lite support EBCDIC?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/111.\n. If your case is not very performance-sensitive and not streaming, you can do something like this:\n\n```\nvar bomUsed = false;\nvar res = iconv.decode(buf, \"utf8\", {stripBOM: function() { bomUsed = true });\nif (!bomUsed)\n  res = iconv.decode(buf, \"1255\");\n```\n\nstripBOM parameter acts both as flag and as a callback that is called when BOM exists.\n\nIf streaming is required, then you can write small wrapper that would do the detection, similar to [StripBOMWrapper](https://github.com/ashtuchkin/iconv-lite/blob/master/lib/bom-handling.js#L28).\n. Thanks Daniel! Added the your module to BOM handling section of readme.\n. Thanks for the report! I'll look into it soon. \n. Sorry for big delay, just pushed a fix.. Fixed in recent commit. I used `encodeAdd` and `encodeSkipVals` options for dbcs codec, they were introduced just for this purpose. Also, I skipped the option of euro encoding of gbk - it's much easier without it, hope it's fine.. Also, table caching is not needed because it's only loaded once to convert to internal representation.. There seems to be a module for it: https://www.npmjs.com/package/superagent-charset\n. You're welcome :)\n. Sorry for the huge delay.\r\n\r\nSo what happens here is `writeStream` is the file write stream created by `fs.createWriteStream('out-win1250.txt')`, not encoding stream. This is because the `pipe()` method returns it's argument and not `this`. To make it work, you need to get the correct writeStream:\r\n```\r\nconst writeStream = iconv.encodeStream('win1250');\r\nwriteStream.pipe(fs.createWriteStream('out-win1250.txt'));\r\n```\r\n\r\nAlso, it's not recommended to use `.collect()` method together with piping data to another stream - the backpressure mechanisms will probably screw up. Only use `.collect()` at the end of the pipe.. Still cannot reproduce it :(  Tried the upgrade method you described and it worked without any errors.. \n\nLet me know if you're able to consistently reproduce it.\n. Guys I need your help here. I don't have an environment to reproduce it, so I'm counting on you to isolate the exact requirements for this to happen.\r\n\r\nI'm also not fond of changing that line to `iconv.encodings = require(\"iconv-lite/encodings\");` because that is a workaround for `require()` bug - it should be fixed on their side. By specification, require should work relative to the caller .js file and it's a much larger issue if it doesn't.. Thanks for the extended explanation, @Marak.\r\n\r\n> Generally, it's not safe to do a lazy require of modules.\r\n\r\nIt looks to me that generally, it's not safe to do a chroot of a node process, because node's module caching has unexpected results for dynamic requires. Are you sure other libraries/modules don't use it? It's a perfectly valid mode of loading modules in node.js, as far as I know.\r\n\r\n> Is there a reason that ../encodings.js file is being loaded lazily in getCodec?\r\n\r\nYes, it's an explicit optimization of startup time. Encodings tree is large and can take several hundred ms to load. I don't want to impose this tax on all projects indirectly requiring iconv-lite because they might not use (or even know about) it at all.\r\n\r\nI still don't know why it works the second time for @dbussert - module caching should not depend on that.. Dear @Marak,\r\n\r\nI'm yet to find evidence that others are experiencing the same issue as you are. All the other people in this thread have a situation where a restart \"fixes\" the problem, which should not happen with a chroot. Also I haven't seen any mention of chroot other than from you. So, I'm still gathering information about that case and would prefer you stop interfering and going personal about it. There's no need to preach about best practices, tell me what I should do, or talk like you represent all the other people in this thread.\r\n\r\nAs for your particular problem - no, I won't fix it for you. The core reason you have this problem is you built a system based on a subtly incorrect assumption that the node module system works correctly after a chroot. This subtle incorrectness showed itself when you started using iconv-lite, but it has potential to show for other modules as well. You cannot assume that other modules won't use a valid system interface because you think it's \"not a best practice\". I'm glad you found a workaround for your particular system, but 1) it's not guaranteed to work for other people, and 2) you'll need to find similar workarounds for other libraries/modules. I would've recommended to stop relying on the assumption above and try to fix your system in a more general way, like using a separate process to do the chroot and only start your original node process after that, but I don't have enough context about your system.\r\n\r\nTo be clear, I'm not being stubborn about making the change you suggested. I'm not doing that because:\r\n 1. As I said earlier, I don't have evidence that this what's actually happening for the OP and other people in this thread.\r\n 2. Each part of the current solution is there for a good reason:\r\n    1. Dynamic require provides the dont-pay-if-you-dont-use guarantee for users with regards to startup time, as we discussed previously.\r\n    2. Relative require path guarantees that I'm requiring file from the same exact module. As you might already know, there can be several versions of a module installed in a node_modules folder and I have no idea which one of them will be resolved to a top-level 'iconv-lite'. I hope I don't have to explain why this requirement is important.. Hello Allen! Please refer to \"iconv\" package maintainer. This is a\ndifferent package named \"iconv-lite\".\n\nAlex\nOn May 4, 2016 02:32, \"allen.hu\" notifications@github.com wrote:\n\n> Before the node v6.0.0, the repository\n> https://github.com/hujb2000/easynode-ipc.git , Do npm install is always\n> ok , but when I upgrade node to the last version v6.0.0, when npm install\n> it console such logs,\n> Can you help me ?\n> \n> npm ERR! Darwin 15.2.0\n> npm ERR! argv \"/Users/hujiabao/.nvm/versions/node/v6.0.0/bin/node\"\n> \"/Users/hujiabao/.nvm/versions/node/v6.0.0/bin/npm\" \"update\"\n> npm ERR! node v6.0.0\n> npm ERR! npm v3.8.6\n> npm ERR! code ELIFECYCLE\n> \n> npm ERR! iconv@2.1.11 install: node-gyp rebuild\n> npm ERR! Exit status 1\n> npm ERR!\n> npm ERR! Failed at the iconv@2.1.11 install script 'node-gyp rebuild'.\n> npm ERR! Make sure you have the latest version of node.js and npm\n> installed.\n> npm ERR! If you do, this is most likely a problem with the iconv package,\n> npm ERR! not with npm itself.\n> npm ERR! Tell the author that this fails on your system:\n> npm ERR! node-gyp rebuild\n> npm ERR! You can get information on how to open an issue for this project\n> with:\n> npm ERR! npm bugs iconv\n> npm ERR! Or if that isn't available, you can get their info via:\n> npm ERR! npm owner ls iconv\n> npm ERR! There is likely additional logging output above.\n> \n> npm ERR! Please include the following file with any support request:\n> npm ERR! /Users/hujiabao/workspace_docker/icp/easynode-ipc/npm-debug.log\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly or view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/119\n. Glad you solved it :)\n\n## \n\nAlexander Shtuchkin\n\nOn Wed, Jun 8, 2016 at 5:00 PM, Andrey Sidorov notifications@github.com\nwrote:\n\n> Closed #120 https://github.com/ashtuchkin/iconv-lite/issues/120.\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/120#event-686636164, or mute\n> the thread\n> https://github.com/notifications/unsubscribe/AAmVHVTawTnsGqvcWai7u3jIUGmol_atks5qJ1epgaJpZM4IxgPU\n> .\n. Yep ok, good luck :)\n\n## \n\nAlexander Shtuchkin\n\nOn Wed, Jun 8, 2016 at 5:03 PM, Andrey Sidorov notifications@github.com\nwrote:\n\n> my main issue is not solved, but at least I know now that I don't need\n> iconv to convert from utf8mb4, it's just utf8 - so nothing to do with\n> iconv-lite :)\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/120#issuecomment-224767252,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAmVHe3QToprgMqU40_UEnrMeWzs6SsZks5qJ1hngaJpZM4IxgPU\n> .\n. async is only a dev dependency and only used for '.parallel' method, and I don't see any breaking changes there. Am I missing something?\n. Ok, thanks anyway :)\n. Thanks! And sorry for delay.. hmm.. just installed node v6.3 and run `npm test` and it looks fine.. (needed to update iconv to latest version) Are you having any problems with it?\n. Also see https://nodejs.org/api/buffer.html#buffer_buf_index\n. No problem :)\n\n## \n\nAlexander Shtuchkin\n\nOn Fri, Aug 5, 2016 at 1:37 AM, Alan Plum notifications@github.com wrote:\n\n> Okay, strange. I was just able to verify that this actually works in\n> Node.js 6 just fine. I was debugging buffer issues yesterday and was unable\n> to write to buffers using the subscript operator but it must have been\n> caused by something else. Sorry for the noise.\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/123#issuecomment-237789308,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAmVHb7rIni2-CkXAiBDFXvrqw95Q_STks5qcvZJgaJpZM4JcfhC\n> .\n. I needed to fix some tests to make it work, but now it's merged into master in #136. Hey! Are these completely new encodings or just aliases to existing ones?\nBtw, iconv-lite supports macroman directly.\n\n## \n\nAlexander Shtuchkin\n\nOn Wed, Aug 24, 2016 at 2:47 AM, Andrey Sidorov notifications@github.com\nwrote:\n\n> hi,\n> I'm adding support for non-utf encorings to mysql2 (\n> sidorares/node-mysql2#374\n> https://github.com/sidorares/node-mysql2/pull/374 ) and currently\n> trying to map between mysql charsets and iconv encoding names. Some\n> encodings seem to be missing ( macroman, keybcs2 ) for example.\n> \n> It seems that all of them are described here - https://github.com/twitter/\n> mysql/tree/master/sql/share/charsets\n> \n> Would you be interested in adding them? Perhaps script to import this data\n> automatically? Is there a documentation on how to add new encoding table to\n> iconv-lite?\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/125, or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAmVHc7P2tDjejdbVKNcoxQpnLgn7W_Nks5qjBMogaJpZM4Jr1CL\n> .\n. That's an obscure encoding :) https://en.wikipedia.org/wiki/Kamenick%C3%BD_encoding\nGoogling it shows mysql codebase and ~7000 old pages, which is pretty small, so I'm not sure it makes sense to add it to iconv-lite directly. \n\nHave no fear though! You can append encoding from outside of iconv-lite, in your own code, like this:\n\n``` js\nvar iconv = require('iconv-lite');\niconv.getCodec('ascii'); // Lazy-load existing encodings to fill out iconv.encodings dict.\n\n// Now add new encodings\n// key in this dictionary is lowercase name of encoding with all non-alphanumeric symbols stripped\n// (i.e. name might be \"abc-XYZ\", then the key is \"abcxyz\", see more examples in /encodings directory)\niconv.encodings[\"myencoding\"] = {\n        \"type\": \"_sbcs\",  // Single-byte encoding.\n        // \"chars\" is either 256 characters of encoding, or just top 128, if lower 128 is same as ascii.\n        \"chars\": \"\u00c4\u0100\u0101\u00c9\u0104\u00d6\u00dc\u00e1\u0105\u010c\u00e4\u010d\u0106\u0107\u00e9\u0179\u017a\u010e\u00ed\u010f\u0112\u0113\u0116\u00f3\u0117\u00f4\u00f6\u00f5\u00fa\u011a\u011b\u00fc\u2020\u00b0\u0118\u00a3\u00a7\u2022\u00b6\u00df\u00ae\u00a9\u2122\u0119\u00a8\u2260\u0123\u012e\u012f\u012a\u2264\u2265\u012b\u0136\u2202\u2211\u0142\u013b\u013c\u013d\u013e\u0139\u013a\u0145\u0146\u0143\u00ac\u221a\u0144\u0147\u2206\u00ab\u00bb\u2026 \u0148\u0150\u00d5\u0151\u014c\u2013\u2014\u201c\u201d\u2018\u2019\u00f7\u25ca\u014d\u0154\u0155\u0158\u2039\u203a\u0159\u0156\u0157\u0160\u201a\u201e\u0161\u015a\u015b\u00c1\u0164\u0165\u00cd\u017d\u017e\u016a\u00d3\u00d4\u016b\u016e\u00da\u016f\u0170\u0171\u0172\u0173\u00dd\u00fd\u0137\u017b\u0141\u017c\u0122\u02c7\"\n};\n\n// Also you can add encoding aliases\niconv.encodings[\"myawesomeencoding\"] = \"myencoding\";\n\n// Now you can use the new encodings to actually encode/decode stuff.\nbuf = iconv.encode(\"myencoding\", \"input string\");\n```\n\n(Alternatively, you can say that you don't support it as it's very obscure :) )\n. Hey Lucas,\n\nIt's a question to `request` library, not `iconv-lite`. I don't know the\ninternals of `request`, sorry.\n\n## \n\nAlexander Shtuchkin\n\nOn Fri, Aug 26, 2016 at 2:31 PM, Lucas Pelegrino notifications@github.com\nwrote:\n\n> Hey guys,\n> \n> This is more of a question. I'm pipeing one request and doing the decoding\n> myself like this:\n> \n>   // decodes\n>   .on('response', function(resp){\n>     var charset = extractCharsetFromStr(resp.headers['content-type']),\n>         encoding = resp.headers['content-encoding'],\n>         decodedRes;\n> \n> ```\n> // unzip\n> if(encoding == 'gzip' || encoding == 'deflate')\n>   decodedRes = resp.pipe(encoding == 'gzip' ? zlib.createGunzip() : zlib.createInflate())\n> else\n>   decodedRes = resp\n> \n> // charset\n> if(charset != 'utf-8' && iconv.encodingExists(charset)){\n>   return decodedRes.pipe(iconv.decodeStream(charset))\n> }\n> ```\n> \n>   })\n> \n> But this is not modifying the response body in the callback request({},\n> (err, resp) => {}), my question is, if I listen to the 'data' and\n> 'finish' event and buffer the body myself, will request still buffer the\n> body on it's on if I don't pass the callback? I don't want to hold two\n> objects like these in memory.\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/126, or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAmVHddWXb0eYTgLsqQKxuIbE93QRpZXks5qj1tEgaJpZM4JufBb\n> .\n. Hmm, I checked it and it looks like `gb18030-ranges.json` is a perfectly valid json file. I think it's either a file corruption on your drive, or a bug in babel plugin. Never seen this error before. Let me know if you need more help.. windows-1251 cannot represent kanji :) It's a single-byte encoding\nthat has codepoints only for latin and cyrillic letters.\n\nSee: https://en.wikipedia.org/wiki/Windows-1251\n\n## \n\nAlexander Shtuchkin\n\nOn Wed, Oct 12, 2016 at 3:56 AM, wanghaisheng notifications@github.com\nwrote:\n\n> node version 4.5\n>            console.log(   iconv.decode(iconv.encode(\"\u6d3b\u6d3d\u6d3e\u6d36\u6d1b\u6cf5\u6d39\u6d27\", \"windows-1251\"), \"windows-1251\"));\n>            console.log(   iconv.decode(iconv.encode(\"\u6d3b\u6d3d\u6d3e\u6d36\u6d1b\u6cf5\u6d39\u6d27\", \"windows-1251\"), \"utf8\"));\n> \n> output\n> \n> ??????\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/130, or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAmVHU-RqQfdzoLa57wbAGMo-azf4oYqks5qzLzVgaJpZM4KUoAA\n> .\n. Let me know if you have more questions.. That is a great idea. I'll need to fix a couple of issues and make it 1.0. How urgent do you need it?\n. Awesome, thanks! Will merge soon.\n. The API looks correct from the initial look.\n. Added small mention in readme. Let me know if you had something bigger in mind.. Thanks!\nThat's funny, why doesn't it use regular rules, i.e. if \"files\" field is not given, then everything except the contents of `.npmignore` is included?\n\nAnyway, the \"./generation\" is not needed for this module operation. You'll probably need to add \"./lib\" and \"./changelog.md\" explicitly as well.\n. Hey shanavas, thanks for your efforts.\r\n\r\nAfter giving it a bit more thought, I would prefer not to add this field. Having a blacklist of files that don't need to be packaged (like in .npmignore) is safer than having a whitelist (like the \"files\" prop) because I won't need to remember to add new files there. This is easy to forget.\r\n\r\nI'm open to suggestions, but in general I think it makes a lot more sense to adjust npm2deb to use the npm's rules of inclusion and support all modules without changes. Otherwise you'll need to ask a lot of projects to adjust stuff.. Thanks for the report, will look into it asap. Meanwhile, can you check\nthat removing that line actually fixes the build?\n\ncc @larssn\n\nOn Nov 21, 2016 2:11 AM, \"Felix Becker\" notifications@github.com wrote:\n\n> 0.4.14 includes TypeScript definition files that have a triple-slash\n> reference to the \"node\" typings. Triple slash references are a thing of the\n> past, they are only used inside DT to declare dependencies. It doesn't work\n> with included definition files:\n> \n> https://travis-ci.org/felixfbecker/vscode-php-debug/jobs/177567952#L877\n> \n> This line needs to be removed:\n> https://github.com/ashtuchkin/iconv-lite/blob/master/lib/index.d.ts#L6\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/ashtuchkin/iconv-lite/issues/137, or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAmVHYVFD8i-zbBIB9pEam_CT1ByShpqks5rAW5egaJpZM4K4A2p\n> .\n. I.e. is everything else working?\n\nOn Nov 21, 2016 10:01 AM, \"Alexander Shtuchkin\" ashtuchkin@gmail.com\nwrote:\n\n> Thanks for the report, will look into it asap. Meanwhile, can you check\n> that removing that line actually fixes the build?\n> \n> cc @larssn\n> \n> On Nov 21, 2016 2:11 AM, \"Felix Becker\" notifications@github.com wrote:\n> \n> > 0.4.14 includes TypeScript definition files that have a triple-slash\n> > reference to the \"node\" typings. Triple slash references are a thing of the\n> > past, they are only used inside DT to declare dependencies. It doesn't work\n> > with included definition files:\n> > \n> > https://travis-ci.org/felixfbecker/vscode-php-debug/jobs/177567952#L877\n> > \n> > This line needs to be removed:\n> > https://github.com/ashtuchkin/iconv-lite/blob/master/lib/index.d.ts#L6\n> > \n> > \u2014\n> > You are receiving this because you are subscribed to this thread.\n> > Reply to this email directly, view it on GitHub\n> > https://github.com/ashtuchkin/iconv-lite/issues/137, or mute the thread\n> > https://github.com/notifications/unsubscribe-auth/AAmVHYVFD8i-zbBIB9pEam_CT1ByShpqks5rAW5egaJpZM4K4A2p\n> > .\n. I published v0.4.15 with this fix. Could you check it? \r\n\r\nThanks again for quick response!. Just checked it myself on a small sample project and it did compile successfully after installation of \"@types/node\". Typescript 2.1, Node 6.5.. Thanks!. Hey, it looks like res.text() is already decoding the request body with\neither UTF8 or encoding provided in headers.\n\nTo fix it, you might want to use res.arrayBuffer() (\nhttps://developer.mozilla.org/en-US/docs/Web/API/Body/arrayBuffer), then\nconvert it to Buffer and provide to decode().\n\nLet me know if it works.\n\n\n\nOn Sun, 1 Jan 2017 at 03:41, Hellon Canella <notifications@github.com>\nwrote:\n\n> I'm fetching a xml encoded in ISO-8859-1.\n>\n>\n> As long as the fetching is completed I'm trying to convert it to UTF-8.\n>\n>\n> const iconv = require('iconv-lite');\n>\n>\n>\n> fetch('http://www.band.uol.com.br/rss/colunista_64.xml')\n>\n>   .then(res => res.text())\n>\n>   .then(text => {\n>\n>      const decodedText = iconv.decode(new Buffer(text), 'latin1')\n>\n>              , output = iconv.encode(decodedText, 'utf8')\n>\n>       console.log(output.toString())\n>\n> })\n>\n>\n>\n> The problem is: all especial characters of the body is being replaced by\n> \"\u00bf\u00bd\"\n>\n>\n>\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/139>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHaDClqP4eRox_qX_42rRYUb8NabNks5rNugUgaJpZM4LYo1b>\n> .\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n. Not really proficient in C++ node coding, but can't you create a `Buffer` in C++ land? or `Uint8Array`? That would be the most efficient. \r\n\r\nIf it's not possible or you really need to use json (and not just JS object), then your method should definitely work.\r\n\r\nTip: the `utf8String` you mention in step 5 is actually a regular JS String. It usually doesn't matter which encoding is used internally (utf16), so it's better to name it just `errorString`.. Great to hear it works!. The library itself has pretty reasonable memory footprint (several megabytes at most), so I assume that grunt is trying to process too large file. \r\n\r\nThere are several solutions I see:\r\n 1. Make sure all files in your project is utf8, so that grunt doesn't need to convert anything.\r\n 2. Change grunt or your Gruntfile to use streaming iconv-lite API.. I mean, if you use iconv-lite directly in Gruntfile, then you can change it to use streaming api. Otherwise, if it's some internal-to-grunt operation, then the only way is to change grunt itself.. Thanks for the detailed report Daniel!\r\n\r\nI just checked on my machine and it seem to work fine (iconv-lite@0.4.15, node@v7.4.0):\r\n```\r\n> const enc = iconv.encode(Buffer.from('test'), 'ISO-8859-1');\r\nundefined\r\n> console.log(Buffer.from('test'), enc, iconv.decode(enc, 'ISO-8859-1'));\r\n<Buffer 74 65 73 74> <Buffer 74 65 73 74> 'test'\r\nundefined\r\n```\r\n\r\nThe library is fully covered with tests for all encodings, plus ISO-8859-1 is a popular one and haven't changed for years, so I presume something might be wrong with your environment.\r\n\r\nSpecifically, I'm wondering why the buffers are printed like arrays, not the regular `<Buffer ...>`. Could it be that some other library is monkey-patching Buffer? Which version of node are you using? Could you try to reinstall the library (I've seen a partial/screwed installs before)?. Ok, makes sense. I'll try to reproduce it in a browser.\r\n\r\nMeanwhile, could you try again with a first argument to `encode` being a string, not buffer? When you encode something, you convert from string to buffer; when decoding - buffer to string.. yep, thanks. 63 is ascii for \"?\". I managed to reproduce it. Turns out, when Chrome loads javascript files, it defaults to latin1 encoding, but iconv-lite source files require utf-8. \r\n\r\nTo fix it, one of the following needs to be done:\r\n * add `<meta charset=\"utf-8\">` to the html file. \r\n * use `<script src=\"your-file.js\" charset=\"utf-8\"></script>` to load the script.\r\n * add `Content-type: application/javascript; charset=utf-8` header to the http request of js file.\r\n\r\nLet me know if it helped.. Yep I will add a note in readme and a runtime warning. Thanks!\n\n\nOn Sun, 8 Jan 2017 at 17:11, Daniel Huisman <notifications@github.com>\nwrote:\n\n> Closed #142 <https://github.com/ashtuchkin/iconv-lite/issues/142>.\n>\n>\n>\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/142#event-915222112>, or mute\n> the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHcR5xrZN1YFcD2YeqBfr5WGXHzEKks5rQOCWgaJpZM4Ldi99>\n> .\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n. Note: added a warning when this situation arises, with a link to relevant [wiki page](https://github.com/ashtuchkin/iconv-lite/wiki/Javascript-source-file-encodings). Hehe I like your way of thinking :)\r\n\r\nShould it be a filed to IntelliJ then? AFAIK cp950 is a valid utf-8 json file.. Cool cool :). Hey, this doesn't look like text encoding. Could it be gzipped? Could you\npost the whole response with headers?\n\n--\nAlexander Shtuchkin\n\nOn Tue, Jan 17, 2017 at 8:10 PM, lou <notifications@github.com> wrote:\n\n> Hi & hope you can help:/\n>\n> I have been having a hard time trying to work through some encoding issues.\n>\n> I am using Electron with Node.js https. The app Posts a request to a Soap\n> webservice.\n>\n> I receive a status 200, correct Headers and response. However the response\n> is a garbled twisted string of weird characters as in my screenshot.\n> [image: screenshot_characters]\n> <https://cloud.githubusercontent.com/assets/15900463/22050542/a48dc25e-dda0-11e6-9409-ddc5e391cfe3.png>\n>\n> .\n>\n> Am trying iconv-lite but with no success. I am using Node v6.9.2, linux\n> and npm 3.10.9.\n>\n> Really appreciate any help\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/144>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHX9YK0MqzJHLgs1L0ru5SbMeQiF7ks5rTZCZgaJpZM4Lmdam>\n> .\n>\n. Thanks! Can you try to remove line with \"Accept-Encoding\" from your request\nheaders and try again? I think server returns gzipped data now.\n\n--\nAlexander Shtuchkin\n\nOn Tue, Jan 17, 2017 at 11:58 PM, lou <notifications@github.com> wrote:\n\n> Here is the post request code that I forgot to include earlier. I also\n> realise iconv-lite may not work with my Node version (so I may upgrade) but\n> any help would do.\n> [image: post_req]\n> <https://cloud.githubusercontent.com/assets/15900463/22055338/92923cbe-ddbf-11e6-9418-d2088dbc419d.png>\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/144#issuecomment-273408051>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHWFVMyzGsfI337OKoKCklmsKfgB9ks5rTcY5gaJpZM4Lmdam>\n> .\n>\n. Hey raccy, thanks for filing this issue. \r\n\r\nIn multibyte encodings, iconv-lite tries its best to mirror the [WHATWG Encoding Standard](https://encoding.spec.whatwg.org). I just checked it out and it maps symbol 1-33 to U+FF5E, see [this](https://encoding.spec.whatwg.org/index-jis0208.txt) and [this](https://encoding.spec.whatwg.org/jis0208.html). \r\n\r\nDo you have other sources except libiconv that map 1-33 to U+301C? You might want to file an issue to [encoding standard issue tracker](https://github.com/whatwg/encoding/issues). I see there's some [minor discussion](https://github.com/whatwg/encoding/issues/47#issuecomment-251083712) there about it.\r\n\r\nI can probably add the encoding pair U+301C -> 81 60 for Shift_JIS and CP932 to be more flexible, but for the decoding part I currently aim to follow encoding standard.\r\n\r\nWhat do you think?. Thanks for chiming in, Ikedas. What do you think of discussion of the same issue at the encoding standard tracker: https://github.com/whatwg/encoding/issues/47 ?\r\n\r\nNote to self: Ambiguities can be see here: https://www.w3.org/TR/2000/NOTE-japanese-xml-20000414/#ambiguity_of_yen. Thanks Brian! Do I understand right that this only affects performance?\r\n. Thanks again!. That's a very good question. Although it seems react-native is not a full\nnode environment as it doesn't know streams or Buffer class, we still want\nto support it because it's in this project's mission to support all js\nenvironments.\n\nI'll look into making it trivially usable by switching to Uint8Array\ninstead of Buffer if the latter is not available. No promises on timeline,\nthough.\n\nMeanwhile for now you can try to build it into a single .js file using\nbrowserify, then use the resulting file like in a browser.\n\nAlex.\n\n--\nAlexander Shtuchkin\n\nOn Tue, Feb 14, 2017 at 9:08 PM, AdamChrist <notifications@github.com>\nwrote:\n\n> I want to use iconv-lite in react-native, but icon-lite require stream of\n> Node's stream.\n> The error message is \"Unable to resolve module stream\".\n> Is there any way I can use it in react-native? thanks\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/148>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHaKEyvTSHNWhKcqG_ptQO9dR0sGTks5rcogxgaJpZM4MBTzB>\n> .\n>\n. Tracking React Native support in #150.. Thanks for the report, Williams!\n\nUsually in Node environment Buffer is defined globally.. what environment\nare you using?\n\nAlex\n\nOn Apr 16, 2017 5:30 AM, \"williams-voon\" <notifications@github.com> wrote:\n\nHi,\n\nIn this file 'Buffer' is not defined.\n/iconv-lite/encodings/dbcs-codec.js\n\nI think the line should be added at head of the file.\nconst { Buffer } = require('buffer')\n\nor ES6: import { Buffer } from 'buffer'\n\nRegards,\nWilliams\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\n<https://github.com/ashtuchkin/iconv-lite/issues/150>, or mute the thread\n<https://github.com/notifications/unsubscribe-auth/AAmVHYRuQTf_VxtH7k3ad58fWvPba6rEks5rwgn8gaJpZM4M-ndp>\n.\n. @williams-voon I added explicit require()-s of buffer module, should now work, could you please check it?\r\n\r\nTo check, please install two new modules: `buffer` and `stream`, then install `iconv-lite` from master - use something like this in your `package.json`:\r\n```\r\n{\r\n  \"dependencies\": {\r\n    .. your deps ..\r\n    \"iconv-lite\": \"ashtuchkin/iconv-lite#master\",\r\n  }\r\n}\r\n```\r\n. Released iconv-lite v0.4.16 with the fix. Thanks!. Thanks @flyher!. You can use any guide to browserify on the internets, like the following:\r\nhttps://medium.com/@christopherphillips_88739/a-beginners-guide-to-browserify-1170a724ceb2\r\nhttps://writingjavascript.org/posts/introduction-to-browserify\r\nhttps://github.com/substack/browserify-handbook\r\n\r\nAnd then just require this library and use it.. Not sure how iconv-lite is involved here.. :)\n\nOn Apr 24, 2017 8:02 PM, \"Hsiu Hui\" <notifications@github.com> wrote:\n\n> When I test this page:\n> https://www.wired.com/2017/04/just-pair-11-radio-gadgets-can-steal-car/\n>\n> and my code is as below:\n>\n> request({url: result.url, encoding: null}, (error, response, html)=>{\n>     var enc = charset(res.headers, html)\n>     enc = enc || jschardet.detect(html).encoding\n>     console.log(\"encoding: \" + enc)\n> })\n>\n> It always return \"null\". But when I use document.charset in chrme dev\n> tool, it return UTF-8\n> Can somebody fix it?\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/152>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHVCPR8h8tHX-OlE1LGZFijesickNks5rzWJCgaJpZM4NG8r4>\n> .\n>\n. Thanks Lars!. Thanks for the report! I reproduced it and will fix soon.\r\n\r\nI'll need to replace the check here: https://github.com/ashtuchkin/iconv-lite/blob/master/encodings/internal.js#L38  to something like `new Buffer('eda0bdedb2a9', 'hex').toString() !== '\ud83d\udca9'`\r\n. Hmm, thanks for the info. I still need to support Node v0.10 and would like\nto avoid external dependencies.. Is there any other options?\n\n--\nAlexander Shtuchkin\n\nOn Mon, Jun 12, 2017 at 8:59 AM, Andrey Sidorov <notifications@github.com>\nwrote:\n\n> new Buffer('string', enc) is deprecated - should be Buffer.from('string',\n> enc) ( + safe-buffer <https://github.com/feross/safe-buffer> polyfill )\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/154#issuecomment-307834326>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHVlDTeJIJTK9HkvdAmvOHawTFRDHks5sDWBKgaJpZM4N3R01>\n> .\n>\n. Ok, so fixed in c7a8243 and published as v0.4.18.. Thanks for the report @sidorares!. Thanks guys, I think you're right, I'll add a fix soon.\n\n--\nAlexander Shtuchkin\n\nOn Wed, Jul 5, 2017 at 11:26 AM, Erik Kemperman <notifications@github.com>\nwrote:\n\n> Just noticed the same thing. Suggested fix:\n>\n> .toLowerCase().replace(/:\\d{4}$/, \"\").replace(/[^0-9a-z]/g, \"\");\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/156#issuecomment-313186544>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHYk30eR2uB93vvK3Kr06jEYV5yLcks5sK9VAgaJpZM4OMhuw>\n> .\n>\n. Interesting, is it much faster than `iconv.encode(..).length`? What's the use case here?. What is your use case, why do you need that api (vs using encode().length)?\nWhy do you need these milliseconds?  I'm still trying to wrap my head over\nwhy would you need the byte length without the content? You're not\nallocating memory for that are you?\n\nI'm wondering why there were no feature requests for that in the past 4\nyears.. did anything change recently?\n\nI'm in general reluctant to increase the API surface for something that is\na bit faster for a small subset of encodings, so we need to really\nunderstand what that gives us.\n\nOn Jul 10, 2017 4:34 AM, \"Dmitriy Tsvettsikh\" <notifications@github.com>\nwrote:\n\n> @ashtuchkin <https://github.com/ashtuchkin> It's faster for internal and\n> single-byte encodings. The use cases are the same as for Buffer.byteLength\n> - if you want to known size of string in bytes before encoding.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/pull/158#issuecomment-314080248>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHQ1G-FWuVRp1ByOHqhdfKjgdFsK5ks5sMgwygaJpZM4OSOvP>\n> .\n>\n. Yes, both Mac and Linux are definitely supported.\n\n--\nAlexander Shtuchkin\n\nOn Tue, Aug 8, 2017 at 6:26 AM, Jesse Laukkanen <notifications@github.com>\nwrote:\n\n> Readme says:\n>\n> ... Works on Windows and in sandboxed environments like Cloud9.\n>\n> and later\n>\n> Comparison with node-iconv module (1000x256kb, on MacBook Pro\n>\n> which sounds like this might work also on Mac. How is it, is Mac supported\n> or not?\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/159>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHSzcPdCUBL_owkceYIi_cPgSGpqfks5sWGIZgaJpZM4OwuwN>\n> .\n>\n. Looks like the buffer that is passed to iconv.decode() is undefined. Maybe\nHTTP request failed? You might want to add a check before calling\niconv.decode().\n\n--\nAlexander Shtuchkin\n\nOn Tue, Aug 22, 2017 at 4:16 PM, Nordes M\u00e9nard-Lamarre <\nnotifications@github.com> wrote:\n\n> Hello,\n>\n> I encountered an error that happen a bit randomly on one server I contact.\n> I do a simple get to receive the data (Japanese encoding). I was wondering\n> if it comes from the codec itself or if it was the 'request' module that\n> was failing.\n>\n> Here's the StackTrace\n>\n> C:\\my-app\\node_modules\\iconv-lite\\encodings\\dbcs-codec.js:453\n>     var newBuf = new Buffer(buf.length*2),\n>                                ^\n>\n> TypeError: Cannot read property 'length' of undefined\n>     at DBCSDecoder.write (C:\\my-app\\node_modules\\iconv-lite\\encodings\\dbcs-codec.js:453:32)\n>     at Object.decode (C:\\my-app\\node_modules\\iconv-lite\\lib\\index.js:42:23)\n>     at Request._callback (C:\\my-app\\src\\service\\myProxyService.js:45:22)\n>     at self.callback (C:\\my-app\\node_modules\\request\\request.js:188:22)\n>     at emitOne (events.js:115:13)\n>     at Request.emit (events.js:210:7)\n>     at Request.onRequestError (C:\\my-app\\node_modules\\request\\request.js:884:8)\n>     at emitOne (events.js:115:13)\n>     at ClientRequest.emit (events.js:210:7)\n>     at Socket.socketErrorListener (_http_client.js:401:9)\n>     at emitOne (events.js:115:13)\n>     at Socket.emit (events.js:210:7)\n>     at emitErrorNT (internal/streams/destroy.js:64:8)\n>     at _combinedTickCallback (internal/process/next_tick.js:138:11)\n>     at process._tickCallback (internal/process/next_tick.js:180:9)\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/161>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHdRWC4n1YJQldAz2ZP4XHwwILGjKks5sa2FOgaJpZM4O_TJv>\n> .\n>\n. Very interesting, thanks for the report! It looks like it was the unexpected consequence of #147. I'll revert that commit.. Okay this should be fixed in febb0b8e280de5de844151036e87cfa06b92d548. I'll publish new iconv version soon. Thanks!. This line might be the problem:\nvar a = btoa(s.toString())\n\n\"s\" here is actually a Buffer with encoded data. When you do .toString(),\nyou lose the encoding.\n\nInstead, I recommend either of these:\nvar a = s.toString(\"base64\");\nOr\nvar a = iconv.decode(s, \"base64\");\n\nAlso please make sure to read these wiki pages for more context:\nhttps://github.com/ashtuchkin/iconv-lite/wiki\nhttps://github.com/ashtuchkin/iconv-lite/wiki/Use-Buffers-when-decoding\n\n\nOn Sep 12, 2017 10:36 PM, \"Tharaka Pathirana\" <notifications@github.com>\nwrote:\n\n> I've created a base64 encoding string in the following format using C#. I\n> want to create the same encoded string using JavaScript.\n> C#\n> var isoEncoding = System.Text.Encoding.GetEncoding(\"ISO-8859-1\");\n> var a = isoEncoding.GetBytes(username + \":\" + password);\n> String encoded = System.Convert.ToBase64String(a);\n> request.Headers.Add(\"Authorization\", \"Basic \" + encoded);\n>\n> Here I used iconv and try to get encoded string and it gives a different\n> encoded string. what can be the reason?\n>\n> var str = this.username+':'+this.password;\n> var s = iconv.encode(str, 'ISO-8859-1');\n> var a = btoa(s.toString());\n> headers.append('Authorization', 'Basic ' + a);\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/163>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHR0_tlKPOSDu5zvCtF8wSF2U2Ob9ks5sh2nYgaJpZM4PVmnG>\n> .\n>\n. To be more specific, buffer.toString() decodes binary data in that buffer\nusing utf-8 encoding, so that messes up things.\n\nOn Sep 13, 2017 10:30 AM, \"Alexander Shtuchkin\" <ashtuchkin@gmail.com>\nwrote:\n\n> This line might be the problem:\n> var a = btoa(s.toString())\n>\n> \"s\" here is actually a Buffer with encoded data. When you do .toString(),\n> you lose the encoding.\n>\n> Instead, I recommend either of these:\n> var a = s.toString(\"base64\");\n> Or\n> var a = iconv.decode(s, \"base64\");\n>\n> Also please make sure to read these wiki pages for more context:\n> https://github.com/ashtuchkin/iconv-lite/wiki\n> https://github.com/ashtuchkin/iconv-lite/wiki/Use-Buffers-when-decoding\n>\n>\n> On Sep 12, 2017 10:36 PM, \"Tharaka Pathirana\" <notifications@github.com>\n> wrote:\n>\n>> I've created a base64 encoding string in the following format using C#. I\n>> want to create the same encoded string using JavaScript.\n>> C#\n>> var isoEncoding = System.Text.Encoding.GetEncoding(\"ISO-8859-1\");\n>> var a = isoEncoding.GetBytes(username + \":\" + password);\n>> String encoded = System.Convert.ToBase64String(a);\n>> request.Headers.Add(\"Authorization\", \"Basic \" + encoded);\n>>\n>> Here I used iconv and try to get encoded string and it gives a different\n>> encoded string. what can be the reason?\n>>\n>> var str = this.username+':'+this.password;\n>> var s = iconv.encode(str, 'ISO-8859-1');\n>> var a = btoa(s.toString());\n>> headers.append('Authorization', 'Basic ' + a);\n>>\n>> \u2014\n>> You are receiving this because you are subscribed to this thread.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/ashtuchkin/iconv-lite/issues/163>, or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AAmVHR0_tlKPOSDu5zvCtF8wSF2U2Ob9ks5sh2nYgaJpZM4PVmnG>\n>> .\n>>\n>\n. Maybe what trips you up is ISO-8859-1 is actually the same as UTF-8 for all\nASCII characters. It's only different in characters with accents etc.\n\nSo i.e. iconv.encode(\"Hello, world!\", \"ISO-8859-1\").toString() should be\nequal to \"Hello, world!\" and it's correct behavior.\n\nCan you provide the values you're testing with and what output values you\nexpect?\n\n\n--\nAlexander Shtuchkin\n\nOn Wed, Sep 13, 2017 at 11:13 AM, Tharaka Pathirana <\nnotifications@github.com> wrote:\n\n> I have tried both s.toString(\"base64\") and iconv.decode(s, \"base64\"). but\n> it gives same as btoa(s.toString())\n>\n> var s = iconv.encode(str, 'ISO-8859-1'); // Is this correct?\n> I debug the code and check, this is given Unit8Array same as byte array of\n> the string. This look\n> like utf-8 encoded array.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/163#issuecomment-329252008>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHT_c6_uOFkiTB5zFdqDdZRU0zh9Eks5siBtNgaJpZM4PVmnG>\n> .\n>\n. Yes.\n\nOn Oct 3, 2017 03:14, \"Idan Gozlan\" <notifications@github.com> wrote:\n\n> Hi,\n>\n> Is that possible use this convertor for windows-1252 to utf-8 conversion?\n>\n> Thanks\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/167>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHQzbDdrlYRgOETUs36evDw6schyXks5sogkggaJpZM4Pr4kQ>\n> .\n>\n. function convert(buffer_in_win1252_encoding) {\n  var str = iconv.decode(buffer_in_win1252_encoding, \"windows-1252\");\n  var output_buffer_in_utf8_encoding = iconv.encode(str, \"utf8\");\n  return output_buffer_in_utf8_encoding;\n}\n\nSee also wiki in this project for more information.\n\n--\nAlexander Shtuchkin\n\nOn Wed, Oct 4, 2017 at 12:34 AM, Idan Gozlan <notifications@github.com>\nwrote:\n\n> Can you please provide example? Didn't find the way..\n>\n> On Oct 3, 2017 20:53, \"Alexander Shtuchkin\" <notifications@github.com>\n> wrote:\n>\n> > Yes.\n> >\n> > On Oct 3, 2017 03:14, \"Idan Gozlan\" <notifications@github.com> wrote:\n> >\n> > > Hi,\n> > >\n> > > Is that possible use this convertor for windows-1252 to utf-8\n> conversion?\n> > >\n> > > Thanks\n> > >\n> > > \u2014\n> > > You are receiving this because you are subscribed to this thread.\n> > > Reply to this email directly, view it on GitHub\n> > > <https://github.com/ashtuchkin/iconv-lite/issues/167>, or mute the\n> > thread\n> > > <https://github.com/notifications/unsubscribe-auth/\n> > AAmVHQzbDdrlYRgOETUs36evDw6schyXks5sogkggaJpZM4Pr4kQ>\n> > > .\n> > >\n> >\n> > \u2014\n> > You are receiving this because you authored the thread.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/ashtuchkin/iconv-lite/issues/\n> 167#issuecomment-333925645>,\n> > or mute the thread\n> > <https://github.com/notifications/unsubscribe-auth/\n> AB5hbYaJYQXmMd24gpV613oJtMQQvayHks5sonSQgaJpZM4Pr4kQ>\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/167#issuecomment-334073152>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHd_9rX58Er2worWD_9fSnOqLCp0qks5sozUQgaJpZM4Pr4kQ>\n> .\n>\n. Works for me...\n\n--\nAlexander Shtuchkin\n\nOn Sun, Oct 8, 2017 at 2:52 AM, Idan Gozlan <notifications@github.com>\nwrote:\n\n> Well... not working.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/167#issuecomment-334995382>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHcXzNkog5pbFpfB36AJHq95v3mFKks5sqJtggaJpZM4Pr4kQ>\n> .\n>\n. Looks like cp720 is rather rare encoding and not supported by iconv library (from which I got the list of encodings). Would you want to add a pull request to add it? See cp808 as an example how it could look like: https://github.com/ashtuchkin/iconv-lite/blob/master/encodings/sbcs-data.js#L15. Interesting, I didn't know about this flag. Does it have anything other\nthan loading require()-d files?\n\nOn Nov 16, 2017 05:20, \"Rob Calcroft\" <notifications@github.com> wrote:\n\n> This lib appears to through lots of warnings when an app is run with\n> --trace-sync-io could this have an impact on performance?\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/169>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHdwEeJ2CpLHcO5QWYEm--HY8OO0uks5s3DaGgaJpZM4QgeiL>\n> .\n>\n. Hey, there's several things going on here:\r\n 1. Using `str = iconvlite.decode(buffer3, \"1255\");`, you're converting from a win1255-encoded buffer to a JS string. JS string doesn't have encoding, it's just a list of characters.\r\n 2. When you do `console.log(str)`, the string is converted to whatever encoding your terminal has and shows you the characters.\r\n 3. When you do `fs.writeFile(\"...\", str, ..)`, Node writes your JS string `str` to file using default encoding, which is utf-8. That's why you have utf-8 characters in that file.\r\n\r\nIf you want to save that string in win1255 encoding, just write the `buffer3` directly:\r\n`fs.writeFile(\"/home/USER/git/npm-test-1/windows-1255.txt\", buffer3, ..`\r\n\r\nSee more info in my wiki: https://github.com/ashtuchkin/iconv-lite/wiki and https://github.com/ashtuchkin/iconv-lite/wiki/Use-Buffers-when-decoding. I see. Can you send a buffer (i.e. actual bytes) to that server? that would\nbe the simple solution. If you can only send strings, then you'll need to\ndetermine where exactly encoding conversion is happening. As I said, JS\nstrings don't have encodings by themselves - it's a list of characters, not\nbytes.\n\nSometimes legacy code do use strings as lists of bytes, though. If you have\nthis case, then you might try using `str = buffer3.toString(\"binary\")`.\nThat might or might not work.\n\n\n--\nAlexander Shtuchkin\n\nOn Sun, Nov 19, 2017 at 11:08 PM, LionOpeter <notifications@github.com>\nwrote:\n\n> Hi @ashtuchkin <https://github.com/ashtuchkin> and thank you very much\n> for replying.\n> I get what you said. My main goal (as i mentioned in my question number 2)\n> is to create a string which has the right character(s). For me the right\n> characters are the characters of windows-1255 encoding because my IRC\n> server works only with this charset.If i send the server the wrong encoding\n> i get an error saying 'Nickname is unavailable: Illegal characters' Any\n> idea how i can pull this off?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/170#issuecomment-345609012>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHY6jPuXZR5hNFHHZ8CcBs4U0H3Z-ks5s4SVqgaJpZM4QjNHu>\n> .\n>\n. Nah, this problem need to be resolved on the irc module side. I noticed\nthere's an \"encoding\" option for irc client (\nhttps://github.com/martynsmith/node-irc/blob/master/lib/irc.js#L59), maybe\nyou set that to \"windows-1255\" and just send the strings?\n\nIf you still want to experiment with manual character manipulation in\nstrings, you can look here:\nhttps://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String/fromCharCode\n\n--\nAlexander Shtuchkin\n\nOn Sun, Nov 19, 2017 at 11:53 PM, LionOpeter <notifications@github.com>\nwrote:\n\n> If i try to send it as a buffer i get error:\n>\n> /home/USER/git/npm-test-1/node_modules/irc/lib/irc.js:930\n>     if (args[args.length - 1].match(/\\s/) || args[args.length - 1].match(/^:/) || args[args.length - 1] === '') {\n>                               ^\n>\n> TypeError: args[(args.length - 1)].match is not a function\n>     at Client.send (/home/USER/git/npm-test-1/node_modules/irc/lib/irc.js:930:31)\n>     at Client._connectionHandler (/home/USER/git/npm-test-1/node_modules/irc/lib/irc.js:725:10)\n>     at Object.onceWrapper (events.js:314:30)\n>     at emitNone (events.js:110:20)\n>     at Socket.emit (events.js:207:7)\n>     at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1125:10)\n>\n>\n> In-addition, an attempt to use str = buffer3.toString(\"binary\") ended up\n> with the same result of Illegal characters error. Any way to assign a\n> string with a hex value in javascript?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/170#issuecomment-345616839>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmVHeeMQn-rocIJuCFrcRqXFEBw-l2xks5s4S_7gaJpZM4QjNHu>\n> .\n>\n. ",
    "jenkinv": "It's a pleasure day when i find your merged work now.\n. Not at all.\n. It's a pleasure day when i find your merged work now.\n. Not at all.\n. ",
    "stagas": "Thanks! Can you publish to npm please?\n. You confused me for a second so I wrote a little test. It leaks alright :) https://gist.github.com/2058045\n. Thanks! Can you publish to npm please?\n. You confused me for a second so I wrote a little test. It leaks alright :) https://gist.github.com/2058045\n. ",
    "david50407": "it works! http://i.imgur.com/nUZZqUp.png\n. it works! http://i.imgur.com/nUZZqUp.png\n. ",
    "kapouer": "I think it's hard to find a proper way to do this.\n. I think it's hard to find a proper way to do this.\n. ",
    "pekim": "To generate the encoding files.\n\n```\ncd iconv-lite\nnode generation/generate-encoding.js\n```\n\nThe encoding files are written to the `encodings` directory.\n\nI wasn't sure where to put the `generate-encoding.js` file, so I put it in it's own source directory. That doesn't seem quite right, but it didn't seem to belong anywhere else.\n. I like what you've done. One file for all of the single-byte encodings makes a lot of sense now that they're all generated. Characters in the character strings (instead of unicode escaped) makes it a lot easier to see at a glance an encoding is plausible.\n. To generate the encoding files.\n\n```\ncd iconv-lite\nnode generation/generate-encoding.js\n```\n\nThe encoding files are written to the `encodings` directory.\n\nI wasn't sure where to put the `generate-encoding.js` file, so I put it in it's own source directory. That doesn't seem quite right, but it didn't seem to belong anywhere else.\n. I like what you've done. One file for all of the single-byte encodings makes a lot of sense now that they're all generated. Characters in the character strings (instead of unicode escaped) makes it a lot easier to see at a glance an encoding is plausible.\n. ",
    "jinze": "The bug is fixed so fast! Thank you very much!\n. The bug is fixed so fast! Thank you very much!\n. ",
    "lastonesky": "Thank you ! =.=\n\n2012/7/13 Alexander Shtuchkin <\nreply@reply.github.com\n\n> Merged & released version 0.2.3. Thanks for the collaboration and\n> greetings from a yacht in Adriatic sea! ^_^\n> \n> ---\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/ashtuchkin/iconv-lite/pull/14#issuecomment-6964013\n. Thank you ! =.=\n\n2012/7/13 Alexander Shtuchkin <\nreply@reply.github.com\n\n> Merged & released version 0.2.3. Thanks for the collaboration and\n> greetings from a yacht in Adriatic sea! ^_^\n> \n> ---\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/ashtuchkin/iconv-lite/pull/14#issuecomment-6964013\n. ",
    "fengmk2": "Have you push v0.2.4 to npm?\n. Awsome optimized! I got the same result now!\n\n``` bash\n$ node test/perfomance.js \n\nEncoding 262144 chars 1000 times:\niconv: 4648ms, 55.08 Mb/s.\niconv-lite: 1628ms, 157.25 Mb/s.\n\nDecoding 262144 bytes 1000 times:\niconv: 4438ms, 57.68 Mb/s.\niconv-lite: 2476ms, 103.39 Mb/s.\n```\n. :) Thanks for your excellent work! \n\nI will keep on using this module on my project, and replace all `iconv` with `iconv-lite`.\n\nOne more question: How can I add 'GB18030' encodings on iconv-lite?\n\n---\n\nMK2, FaWave, Net4Team \u263anodejs\nTEL: 18668079069\nGithub: https://github.com/fengmk2\n\u5fae\u535a: @Python\u53d1\u70e7\u53cb\nTwitter: @fengmk2\nCNode: http://cnodejs.org/user/suqian\nBlog: http://fengmk2.github.com/\n\nOn Aug 25, 2012, at 5:23 AM, Alexander Shtuchkin notifications@github.com wrote:\n\n> Thats great) And thank you for raising this issue!\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. OMG, your work is crazy! Great! Thanks @ashtuchkin .\n. @dangibson It should be great! I think not all project need so many encodings to convert. Just two or free encodings need to convert each other.\n. Too slow on 0.11.13 !!!!\n\n![image](https://cloud.githubusercontent.com/assets/156269/3267829/0cd5e60e-f2d3-11e3-99d0-8a874135b163.png)\n\nJust use 0.11.12 and it will be faster than 0.10.x:\n\n![image](https://cloud.githubusercontent.com/assets/156269/3267834/1cad8c8a-f2d3-11e3-840e-382d1a5a9fbe.png)\n\n![image](https://cloud.githubusercontent.com/assets/156269/3267839/2a7fb9aa-f2d3-11e3-970d-3b362439a951.png)\n. Have you push v0.2.4 to npm?\n. Awsome optimized! I got the same result now!\n\n``` bash\n$ node test/perfomance.js \n\nEncoding 262144 chars 1000 times:\niconv: 4648ms, 55.08 Mb/s.\niconv-lite: 1628ms, 157.25 Mb/s.\n\nDecoding 262144 bytes 1000 times:\niconv: 4438ms, 57.68 Mb/s.\niconv-lite: 2476ms, 103.39 Mb/s.\n```\n. :) Thanks for your excellent work! \n\nI will keep on using this module on my project, and replace all `iconv` with `iconv-lite`.\n\nOne more question: How can I add 'GB18030' encodings on iconv-lite?\n\n---\n\nMK2, FaWave, Net4Team \u263anodejs\nTEL: 18668079069\nGithub: https://github.com/fengmk2\n\u5fae\u535a: @Python\u53d1\u70e7\u53cb\nTwitter: @fengmk2\nCNode: http://cnodejs.org/user/suqian\nBlog: http://fengmk2.github.com/\n\nOn Aug 25, 2012, at 5:23 AM, Alexander Shtuchkin notifications@github.com wrote:\n\n> Thats great) And thank you for raising this issue!\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. OMG, your work is crazy! Great! Thanks @ashtuchkin .\n. @dangibson It should be great! I think not all project need so many encodings to convert. Just two or free encodings need to convert each other.\n. Too slow on 0.11.13 !!!!\n\n![image](https://cloud.githubusercontent.com/assets/156269/3267829/0cd5e60e-f2d3-11e3-99d0-8a874135b163.png)\n\nJust use 0.11.12 and it will be faster than 0.10.x:\n\n![image](https://cloud.githubusercontent.com/assets/156269/3267834/1cad8c8a-f2d3-11e3-840e-382d1a5a9fbe.png)\n\n![image](https://cloud.githubusercontent.com/assets/156269/3267839/2a7fb9aa-f2d3-11e3-970d-3b362439a951.png)\n. ",
    "travisbot": "This pull request [passes](http://travis-ci.org/ashtuchkin/iconv-lite/builds/2236266) (merged a03ad0e1 into d57c2719).\n. This pull request [passes](http://travis-ci.org/ashtuchkin/iconv-lite/builds/2236266) (merged a03ad0e1 into d57c2719).\n. ",
    "templth": "Works fine with 0.2.6! Thanks very much for the very quick fix!\nThierry\n. Works fine with 0.2.6! Thanks very much for the very quick fix!\nThierry\n. ",
    "julien51": "Thanks for the response!\nI'll happily help. What do you see the algorithm to be?\n\nThanks,\n. Ok, thanks for the response. Well, I guess I'll have to wait. Let me know if there is anything we can do to help go a bit faster :)\n. Very very interesting, indeed.\nMaybe it would actually be less work to port it from Array buffers to\nNode's Buffers?\nWhat do you think?\n\nThanks,\n\nOn Wed, Dec 5, 2012 at 6:45 AM, Alexander Shtuchkin <\nnotifications@github.com> wrote:\n\n> Although it uses ArrayBuffers, not node's Buffers, I believe you could try\n> something like\n> \n>  var buffer = new Buffer(..) // node's buffer.\n>  var string = TextDecoder(encoding).decode(new Uint8Array(buf), {stream: true});\n> \n>  \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/ashtuchkin/iconv-lite/issues/20#issuecomment-11030483.\n. Thanks for the response!\nI'll happily help. What do you see the algorithm to be?\n\nThanks,\n. Ok, thanks for the response. Well, I guess I'll have to wait. Let me know if there is anything we can do to help go a bit faster :)\n. Very very interesting, indeed.\nMaybe it would actually be less work to port it from Array buffers to\nNode's Buffers?\nWhat do you think?\n\nThanks,\n\nOn Wed, Dec 5, 2012 at 6:45 AM, Alexander Shtuchkin <\nnotifications@github.com> wrote:\n\n> Although it uses ArrayBuffers, not node's Buffers, I believe you could try\n> something like\n> \n>  var buffer = new Buffer(..) // node's buffer.\n>  var string = TextDecoder(encoding).decode(new Uint8Array(buf), {stream: true});\n> \n>  \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/ashtuchkin/iconv-lite/issues/20#issuecomment-11030483.\n. ",
    "mscdex": "Any update on this? I'd love to see streaming encoding/decoding in this module. It seems the stringencoding project is dead and has some issues with streaming (e.g. [issue 8](https://code.google.com/p/stringencoding/issues/detail?id=8)).\n. I'm thinking about performance.\n. On a project I'm working on, I'm parsing a Buffer and converting parts of it to binary strings because I want to avoid things like excessive Buffer creation and Buffer.concat() calls. Also the end result is going to be string(s) anyway, not actual binary data, so there is no need to convert it back to a Buffer for this particular case.\n. Well, it's not just excessive Buffer creation and such in and of itself, but doing toString() and things means crossing the JS<->C++ boundary also, which is not cheap if you're really concerned about performance.\n\nI can understand not wanting to maintain two codebases side-by-side, I just thought it might be worth it performance-wise to avoid crossing that boundary (not to mention also being more browser-friendly without the likes of Browserify).\n. ASCII-compatible means bytes 0x00-0x7F in an encoding are all ASCII characters/bytes and not some other characters. Many character sets are compatible in this way, but some are not.\n\nChecking whether a destination encoding is ASCII-compatible is useful if you are already traversing binary data and you can check whether each byte is <= 0x7F. If there are no bytes above 0x7F and the encoding is ASCII-compatible, you don't have to run the entire set of data through a decoder which will end up giving you back the same string anyway.\n\nThe reason I use a pre-generated file for this is that it's significantly faster to use an object (with fast properties) than to do a lookup on the fly.\n. As far as I know, yes.. Any update on this? I'd love to see streaming encoding/decoding in this module. It seems the stringencoding project is dead and has some issues with streaming (e.g. [issue 8](https://code.google.com/p/stringencoding/issues/detail?id=8)).\n. I'm thinking about performance.\n. On a project I'm working on, I'm parsing a Buffer and converting parts of it to binary strings because I want to avoid things like excessive Buffer creation and Buffer.concat() calls. Also the end result is going to be string(s) anyway, not actual binary data, so there is no need to convert it back to a Buffer for this particular case.\n. Well, it's not just excessive Buffer creation and such in and of itself, but doing toString() and things means crossing the JS<->C++ boundary also, which is not cheap if you're really concerned about performance.\n\nI can understand not wanting to maintain two codebases side-by-side, I just thought it might be worth it performance-wise to avoid crossing that boundary (not to mention also being more browser-friendly without the likes of Browserify).\n. ASCII-compatible means bytes 0x00-0x7F in an encoding are all ASCII characters/bytes and not some other characters. Many character sets are compatible in this way, but some are not.\n\nChecking whether a destination encoding is ASCII-compatible is useful if you are already traversing binary data and you can check whether each byte is <= 0x7F. If there are no bytes above 0x7F and the encoding is ASCII-compatible, you don't have to run the entire set of data through a decoder which will end up giving you back the same string anyway.\n\nThe reason I use a pre-generated file for this is that it's significantly faster to use an object (with fast properties) than to do a lookup on the fly.\n. As far as I know, yes.. ",
    "solatis": "Hey there, I was just discussing this with @tomas with regards to the iconv-lite depency and the lack of streams support for [needle](https://github.com/tomas/needle/commit/b452fab928e85c3792b505f67075799b3c00133a#commitcomment-5825832). I am capable and interested in looking into a possible implementation for this. I'm fully aware of the caveats, but before I dive into this would appreciate to know if you have something to say about this. Other than lack of time, any other reason why you didn't get around implementing this?\n. What @ashtuchkin describes is exactly how web browsers operate: if no charset is provided in the headers, assume a certain default/detected charset. When processing the document, as soon as you hit a HTML-based charset definition, restart processing of the document from the start.\n\nMaybe this would be a good thing for a HTML stream processing middleware library to handle, that could wrap iconv-lite. This sounds like an area where you do not want too many people reinventing the wheel.\n. Hey there, I was just discussing this with @tomas with regards to the iconv-lite depency and the lack of streams support for [needle](https://github.com/tomas/needle/commit/b452fab928e85c3792b505f67075799b3c00133a#commitcomment-5825832). I am capable and interested in looking into a possible implementation for this. I'm fully aware of the caveats, but before I dive into this would appreciate to know if you have something to say about this. Other than lack of time, any other reason why you didn't get around implementing this?\n. What @ashtuchkin describes is exactly how web browsers operate: if no charset is provided in the headers, assume a certain default/detected charset. When processing the document, as soon as you hit a HTML-based charset definition, restart processing of the document from the start.\n\nMaybe this would be a good thing for a HTML stream processing middleware library to handle, that could wrap iconv-lite. This sounds like an area where you do not want too many people reinventing the wheel.\n. ",
    "tomas": "Looks smooth. I'll take it for a spin with [needle](github.com/tomas/needle) and let you know if anything breaks.\n. No and yes.\n\nApparenty the streaming works, but I'm in the middle of a chicken vs. egg situation. I'm using iconv-lite to do streaming conversion on HTTP responses, but most sites don't actually return the \"real\" charset as part of the HTTP response, so I'm actually forced to get that string out of the <meta charset> HTML tag, after the stream is processed, which kinda defeats the purpose of using iconv-lite as part of the response pipeline. \n\nBasically what I need is to convert whatever stream of data I pass to iconv-lite, and make sure I get UTF-8. And the [tests that I'm doing](https://github.com/tomas/needle/blob/master/test/iconv_spec.js) are not working. With some websites I get 'windows-1252' as a result, with others I get 'ascii', and with others 'SHIFT_JIS'.\n\nMaybe the jschardet library is the actual culprit?\n. Looks smooth. I'll take it for a spin with [needle](github.com/tomas/needle) and let you know if anything breaks.\n. No and yes.\n\nApparenty the streaming works, but I'm in the middle of a chicken vs. egg situation. I'm using iconv-lite to do streaming conversion on HTTP responses, but most sites don't actually return the \"real\" charset as part of the HTTP response, so I'm actually forced to get that string out of the <meta charset> HTML tag, after the stream is processed, which kinda defeats the purpose of using iconv-lite as part of the response pipeline. \n\nBasically what I need is to convert whatever stream of data I pass to iconv-lite, and make sure I get UTF-8. And the [tests that I'm doing](https://github.com/tomas/needle/blob/master/test/iconv_spec.js) are not working. With some websites I get 'windows-1252' as a result, with others I get 'ascii', and with others 'SHIFT_JIS'.\n\nMaybe the jschardet library is the actual culprit?\n. ",
    "jonschlinkert": "Will this be merged in soon?\n. Sorry for the extremely late reply! no I think at the time I was working on a project and was adding iconv-lite as a dep, and something in the refactor caught my eye... anyway thanks for the hard work on this. It's been useful.\n. Will this be merged in soon?\n. Sorry for the extremely late reply! no I think at the time I was working on a project and was adding iconv-lite as a dep, and something in the refactor caught my eye... anyway thanks for the hard work on this. It's been useful.\n. ",
    "bfontaine": "Sorry, I just found that it works on buffers:\n\n```\n> iconv.decode( iconv.encode( '\u00e9', 'latin1' ), 'latin1' )\n'\u00e9'\n```\n. Thanks :)\n. Sorry, I just found that it works on buffers:\n\n```\n> iconv.decode( iconv.encode( '\u00e9', 'latin1' ), 'latin1' )\n'\u00e9'\n```\n. Thanks :)\n. ",
    "leetreveil": "My script may or may not need to use the iconv-lite library depending on the input. So for every file that I parse that doesn't need it I get a +120ms delay.\n. My script may or may not need to use the iconv-lite library depending on the input. So for every file that I parse that doesn't need it I get a +120ms delay.\n. ",
    "dangibson": "Tests added. Travis CI builds. Sorry about that, this is my first pull request on github - didn't know about travis or the tests.\n. Sure - I'll add one later tonight.\n. Done - the test tests that iconv and iconv-lite convert the same for every single character (not just a random snippet of text from a webpage).\n. This loads the mapping from a text file. There are heaps more similar text files for other mappings available at http://www.unicode.org/Public/MAPPINGS/\n\nI'm thinking of changing this to load all files it finds in a folder so that mappings can be added simply by copying a file in, but only loading the file when the mapping is actually required to reduce memory usage/start up speed.\n. All done.\n. You could add a mapping file in encodings/filemapping. The file has three columns - see one of the existing files for an example file. You don't need to change any code if you use the mapping file.\n. The simplest is simply adding a file like in encodings/filemapping/cp949 ks_c_56011987.txt\n\nRequires no source code changes and a lot of the encodings are already done - the above was download from http://www.unicode.org/Public/MAPPINGS/VENDORS/MICSFT/WINDOWS/CP949.TXT\n. Tests added. Travis CI builds. Sorry about that, this is my first pull request on github - didn't know about travis or the tests.\n. Sure - I'll add one later tonight.\n. Done - the test tests that iconv and iconv-lite convert the same for every single character (not just a random snippet of text from a webpage).\n. This loads the mapping from a text file. There are heaps more similar text files for other mappings available at http://www.unicode.org/Public/MAPPINGS/\n\nI'm thinking of changing this to load all files it finds in a folder so that mappings can be added simply by copying a file in, but only loading the file when the mapping is actually required to reduce memory usage/start up speed.\n. All done.\n. You could add a mapping file in encodings/filemapping. The file has three columns - see one of the existing files for an example file. You don't need to change any code if you use the mapping file.\n. The simplest is simply adding a file like in encodings/filemapping/cp949 ks_c_56011987.txt\n\nRequires no source code changes and a lot of the encodings are already done - the above was download from http://www.unicode.org/Public/MAPPINGS/VENDORS/MICSFT/WINDOWS/CP949.TXT\n. ",
    "Ralle": "I believe this is the way you should use it\n\n```\n// we have a buffer with bytes\nvar buf = fs.readFileSync('myWindowsFile.txt');\n// decode the bytes to a V8 string from win1251\nvar str = iconv.decode(buf, 'win1251');\n// encode this string into a buffer of the type latin1\nvar buf2 = iconv.encode(str, 'latin1');\n```\n. I believe this is the way you should use it\n\n```\n// we have a buffer with bytes\nvar buf = fs.readFileSync('myWindowsFile.txt');\n// decode the bytes to a V8 string from win1251\nvar str = iconv.decode(buf, 'win1251');\n// encode this string into a buffer of the type latin1\nvar buf2 = iconv.encode(str, 'latin1');\n```\n. ",
    "wu-yue": "I need a Windows unzip util which is able to extract SHIFT JIS filenames, so I cooked a SHIFT JIS encoding table for iconv-lite.\n\nSource table: ftp://ftp.unicode.org/Public/MAPPINGS/OBSOLETE/EASTASIA/JIS/SHIFTJIS.TXT\nConvert script: https://gist.github.com/wu-yue/6941291\nResult encoding table: https://gist.github.com/wu-yue/6941368\n\nTwo problems here:\n1 The source table is no longer maintained.\n2 SHIFT JIS has special mapping in ASCII part: The single-byte characters 0x00 to 0x7F match the ASCII encoding, except for a yen sign (U+00A5) at 0x5C and an overline (U+203E) at 0x7E in place of the ASCII character set's backslash and tilde respectively(http://en.wikipedia.org/wiki/Shiftjis). Currently, iconv-lite use \"gbkcode & 0x80\" to tell if it will use the encoding table, so these two special mapping won't work.\n. Thanks!\nWaiting for it on NPM~\n. Hi~(_^__^_)\n\nI didn't know there is an Encoding Standard for web. I'm kind of believing in the Unicode religion, and think the Unicode is the encoding standard for web. End users should be forced to generate new information in Unicode form if they are willing to communicate these information globally. So for now I just can't see a point for a standard converting API built into browsers. I think the Unicode is the answer, not a new converting API in every browsers.\n\nBut we do need some kind of way to convert legacy information into the new Unicode representation form. By we, I mean information vendors who are hosting a server. There are information who have difficulties to rebuild from scratch in Unicode form. We should leave these information in their original form if possible. Modern browsers have capability to view these legacy forms. If there ever be a converting is needed, it should be done only once, so it's better for server to do this job. What we need is a code mapping way able to upgrade most legacy information into Unicode form.\n\nAbout the upgrade mapping way, I vote for the Windows CP932 as the source form (I didn't know CP932 is a super set of SHIFTJIS). I think Windows has the major end user usage share in Japan. Well, I didn't conduct a surgery. Maybe add the Mac encoding is fine either.\n\nThe mapping table in http://encoding.spec.whatwg.org/#indexes is said comes from formerly proprietary extensions from IBM and NEC. So even IBM and NEC have abandoned these mapping, I think there is no need to write new code for them, and it looks like CP932 is also built from IBM and NEC 's idea, maybe they just share many common parts. I think if it's really needed, maybe could merge those two, but it's easier to just follow CP932: ftp://ftp.unicode.org/Public/MAPPINGS/VENDORS/MICSFT/WINDOWS/CP932.TXT\n\nBut if CP932 has some copyright issues, then I guess we have to follow the Encoding Standard mapping.\n\n(excuse me for \u2460)\n. Great! Thank you for working on this.\n\nWaiting for 0.4 on NPM~ (_^__^_) \n. I need a Windows unzip util which is able to extract SHIFT JIS filenames, so I cooked a SHIFT JIS encoding table for iconv-lite.\n\nSource table: ftp://ftp.unicode.org/Public/MAPPINGS/OBSOLETE/EASTASIA/JIS/SHIFTJIS.TXT\nConvert script: https://gist.github.com/wu-yue/6941291\nResult encoding table: https://gist.github.com/wu-yue/6941368\n\nTwo problems here:\n1 The source table is no longer maintained.\n2 SHIFT JIS has special mapping in ASCII part: The single-byte characters 0x00 to 0x7F match the ASCII encoding, except for a yen sign (U+00A5) at 0x5C and an overline (U+203E) at 0x7E in place of the ASCII character set's backslash and tilde respectively(http://en.wikipedia.org/wiki/Shiftjis). Currently, iconv-lite use \"gbkcode & 0x80\" to tell if it will use the encoding table, so these two special mapping won't work.\n. Thanks!\nWaiting for it on NPM~\n. Hi~(_^__^_)\n\nI didn't know there is an Encoding Standard for web. I'm kind of believing in the Unicode religion, and think the Unicode is the encoding standard for web. End users should be forced to generate new information in Unicode form if they are willing to communicate these information globally. So for now I just can't see a point for a standard converting API built into browsers. I think the Unicode is the answer, not a new converting API in every browsers.\n\nBut we do need some kind of way to convert legacy information into the new Unicode representation form. By we, I mean information vendors who are hosting a server. There are information who have difficulties to rebuild from scratch in Unicode form. We should leave these information in their original form if possible. Modern browsers have capability to view these legacy forms. If there ever be a converting is needed, it should be done only once, so it's better for server to do this job. What we need is a code mapping way able to upgrade most legacy information into Unicode form.\n\nAbout the upgrade mapping way, I vote for the Windows CP932 as the source form (I didn't know CP932 is a super set of SHIFTJIS). I think Windows has the major end user usage share in Japan. Well, I didn't conduct a surgery. Maybe add the Mac encoding is fine either.\n\nThe mapping table in http://encoding.spec.whatwg.org/#indexes is said comes from formerly proprietary extensions from IBM and NEC. So even IBM and NEC have abandoned these mapping, I think there is no need to write new code for them, and it looks like CP932 is also built from IBM and NEC 's idea, maybe they just share many common parts. I think if it's really needed, maybe could merge those two, but it's easier to just follow CP932: ftp://ftp.unicode.org/Public/MAPPINGS/VENDORS/MICSFT/WINDOWS/CP932.TXT\n\nBut if CP932 has some copyright issues, then I guess we have to follow the Encoding Standard mapping.\n\n(excuse me for \u2460)\n. Great! Thank you for working on this.\n\nWaiting for 0.4 on NPM~ (_^__^_) \n. ",
    "piuccio": "great, thank you!\n. great, thank you!\n. ",
    "alexandernst": "Ok, but what if I get a string in ISO-8859-1 instead of base64?\n. I'm getting the string from this library: https://github.com/mscdex/node-imap\n\nI fetch a mail part with:\n\n```\nf.on(\"message\", function(msg, seqno){\n\n    msg.on(\"body\", function(stream, info){\n        var buffer = \"\";\n        stream.on(\"data\", function(chunk){\n            buffer += chunk;\n        });\n\n        stream.once(\"end\", function(){\n            //convert buffer from whatever encoding has to utf8\n        });\n\n..........\n```\n\nI'll ask that there too :)\n. That seems reasonable. I'll try it monday morning as soon as I get to the office :)\n. I tried concat-ing the buffer instead of converting it to a string and now it works as expected, thank you! :)\n. Ok, but what if I get a string in ISO-8859-1 instead of base64?\n. I'm getting the string from this library: https://github.com/mscdex/node-imap\n\nI fetch a mail part with:\n\n```\nf.on(\"message\", function(msg, seqno){\n\n    msg.on(\"body\", function(stream, info){\n        var buffer = \"\";\n        stream.on(\"data\", function(chunk){\n            buffer += chunk;\n        });\n\n        stream.once(\"end\", function(){\n            //convert buffer from whatever encoding has to utf8\n        });\n\n..........\n```\n\nI'll ask that there too :)\n. That seems reasonable. I'll try it monday morning as soon as I get to the office :)\n. I tried concat-ing the buffer instead of converting it to a string and now it works as expected, thank you! :)\n. ",
    "c0b41": "example http://requirebin.com/?gist=7700953 \n. example http://requirebin.com/?gist=7700953 \n. ",
    "mderazon": "I need to send the `ISO-8859-8` encoded string as a request body to a 3rd party server that expects xml input in that encoding with the header\n\n``` xml\n<?xml version='1.0' encoding='ISO-8859-8'?>\n```\n\nI am doing it with [request](https://github.com/mikeal/request) node module. So are you saying that I need to feed response body with Buffer instead of string ? I'm not sure it's possible \n\nThanks !\n. I wasn't aware of this :)\n\nThanks a lot ! \n. I need to send the `ISO-8859-8` encoded string as a request body to a 3rd party server that expects xml input in that encoding with the header\n\n``` xml\n<?xml version='1.0' encoding='ISO-8859-8'?>\n```\n\nI am doing it with [request](https://github.com/mikeal/request) node module. So are you saying that I need to feed response body with Buffer instead of string ? I'm not sure it's possible \n\nThanks !\n. I wasn't aware of this :)\n\nThanks a lot ! \n. ",
    "kcauchy": "I've read #20 and http://code.google.com/p/stringencoding/\n\nAnd I find two important things:\n1.  The API is different. You're using iconv.decode(BufferOrString, encoding), while they use TextEncoder(encoding).encode(XX); Which means we might need use at least a wrapper to support both browser and Node.\n2.  They don't support Node.js buffer (Of course).\n\nSo I've already fork your project, and I think I will give a browser friendly version soon.\n. E.m. I found too many Node only features. I gave up.\n. I've read #20 and http://code.google.com/p/stringencoding/\n\nAnd I find two important things:\n1.  The API is different. You're using iconv.decode(BufferOrString, encoding), while they use TextEncoder(encoding).encode(XX); Which means we might need use at least a wrapper to support both browser and Node.\n2.  They don't support Node.js buffer (Of course).\n\nSo I've already fork your project, and I think I will give a browser friendly version soon.\n. E.m. I found too many Node only features. I gave up.\n. ",
    "leesei": "I came across the encoding at http://www.hko.gov.hk/contentc.htm\n. I see. I tried to parse the HKO website and find iconv-lite not supporting the encoding.\nI added the alias to iconv-lite and it works fine (maybe I did not came across the incompatibility yet).\n\nIn my use case I could as well defining the alias in my app.\nThanks for the pointer.\n\nP.S.:\nShould the snippets (and potentially the extension of encodings) be part of feature and documented in README?\n. Great.\nThanks for your effort.\n. I came across the encoding at http://www.hko.gov.hk/contentc.htm\n. I see. I tried to parse the HKO website and find iconv-lite not supporting the encoding.\nI added the alias to iconv-lite and it works fine (maybe I did not came across the incompatibility yet).\n\nIn my use case I could as well defining the alias in my app.\nThanks for the pointer.\n\nP.S.:\nShould the snippets (and potentially the extension of encodings) be part of feature and documented in README?\n. Great.\nThanks for your effort.\n. ",
    "mvdwalle": "There is no folder encodings/filemapping in the node module when installing using npm install iconv-lite.\n\nLooking at the code there is also a filemapping.js module missing in the version installed using npm. \n. There is no folder encodings/filemapping in the node module when installing using npm install iconv-lite.\n\nLooking at the code there is also a filemapping.js module missing in the version installed using npm. \n. ",
    "Kemichal": "Support for hp-roman-8 is in the V0.4 branch, which I missed, closing this.\n. Support for hp-roman-8 is in the V0.4 branch, which I missed, closing this.\n. ",
    "peakji": "Any news on this? This would be a great feature to have.. Any news on this? This would be a great feature to have.. ",
    "Dexus": "https://github.com/andyhu/node-transliteration\n\nMay help.\n. https://github.com/andyhu/node-transliteration\n\nMay help.\n. ",
    "nleush": "+1\n. EUC-JP\n\nI'm working on https://github.com/itteco/iframely \nEUC-JP often used on japan pages. And we have many users who use iframely to parse japan pages.\n. +1\n. EUC-JP\n\nI'm working on https://github.com/itteco/iframely \nEUC-JP often used on japan pages. And we have many users who use iframely to parse japan pages.\n. ",
    "htanjo": "+1\nStill a lot of Japanese web sites use EUC-JP.\n. Thank you so much! This works fine in my private repo.\n. +1\nStill a lot of Japanese web sites use EUC-JP.\n. Thank you so much! This works fine in my private repo.\n. ",
    "reinaldorauch": "+1 on this ^^. +1 on this ^^. ",
    "maxogden": "Today I ran into this when trying to parse my gmail `.mbox` (https://support.google.com/accounts/answer/3024195?hl=en) with the https://www.npmjs.org/package/mbox-stream module which uses https://www.npmjs.org/package/mailparser which uses iconv-lite\n\n```\n[Error: Encoding not recognized: 'ISO-2022-JP' (searched as: 'iso2022jp')]\n```\n. Ah actually I forgot to also mention that mailparser uses https://www.npmjs.org/package/encoding which in turn uses `iconv-lite`, but `encoding` is supposed to be normalization layer around `iconv` and `iconv-lite` so it shouldn't have crashed here. Seems like a bug in `encoding`, i'll file an issue there as well\n. Today I ran into this when trying to parse my gmail `.mbox` (https://support.google.com/accounts/answer/3024195?hl=en) with the https://www.npmjs.org/package/mbox-stream module which uses https://www.npmjs.org/package/mailparser which uses iconv-lite\n\n```\n[Error: Encoding not recognized: 'ISO-2022-JP' (searched as: 'iso2022jp')]\n```\n. Ah actually I forgot to also mention that mailparser uses https://www.npmjs.org/package/encoding which in turn uses `iconv-lite`, but `encoding` is supposed to be normalization layer around `iconv` and `iconv-lite` so it shouldn't have crashed here. Seems like a bug in `encoding`, i'll file an issue there as well\n. ",
    "coyle": "I'll throw my vote in here. Just ran into the same error.\n. I'll throw my vote in here. Just ran into the same error.\n. ",
    "AVVS": "Would love to get it supported as well :+1: \n. Would love to get it supported as well :+1: \n. ",
    "bitinn": "Casting my vote in support as well, our [node-fetch](https://github.com/bitinn/node-fetch) uses [encoding](https://github.com/andris9/encoding) which uses `iconv-lite` by default, I found out about this on production server today, installing `iconv` certainly helps resolving this issue :)\n. Casting my vote in support as well, our [node-fetch](https://github.com/bitinn/node-fetch) uses [encoding](https://github.com/andris9/encoding) which uses `iconv-lite` by default, I found out about this on production server today, installing `iconv` certainly helps resolving this issue :)\n. ",
    "asleepysamurai": "  Just adding my vote for this issue as well. This appears to be an encoding that's used widely enough in email that it has become a would-be really-really-really-nice-to-have for my application. \n\n   I will look into what writing a new codec for this encoding involves and if possible will write one myself and submit a pull request.\n\nEDIT: Scratch that, it looks like node-iconv already supports this so I'll go with that for now.\n.   Just adding my vote for this issue as well. This appears to be an encoding that's used widely enough in email that it has become a would-be really-really-really-nice-to-have for my application. \n\n   I will look into what writing a new codec for this encoding involves and if possible will write one myself and submit a pull request.\n\nEDIT: Scratch that, it looks like node-iconv already supports this so I'll go with that for now.\n. ",
    "lygstate": "vote for this.\n. In conclusion, \n    let buffer = iconv.encode('\u20ac', 'gb18030')\n    expect(buffer).to.eql(new Buffer([0x80]))\n\nThis is invalid.\n. vote for this.\n. In conclusion, \n    let buffer = iconv.encode('\u20ac', 'gb18030')\n    expect(buffer).to.eql(new Buffer([0x80]))\n\nThis is invalid.\n. ",
    "Edeph": "I also vote for this, it would be great to have since i run into this problem every time i deal with japanese emails.\r\n. I also vote for this, it would be great to have since i run into this problem every time i deal with japanese emails.\r\n. ",
    "Atm0sf3ar": "You get my vote. Just ran into this issue.. You get my vote. Just ran into this issue.. ",
    "nnabinh": "Also my vote for this, just ran into same situation.\r\n=> need to use both `iconv-lite` & `iconv` at the same time which I don't like! \ud83d\udc80 . Also my vote for this, just ran into same situation.\r\n=> need to use both `iconv-lite` & `iconv` at the same time which I don't like! \ud83d\udc80 . ",
    "vinayr": "Works with the master branch. Thanks.\n. Works with the master branch. Thanks.\n. ",
    "Mithgol": "Could joyent/node#7633 be the\u00a0cause of\u00a0this\u00a0behaviour?\n. The upstream issue is\u00a0fixed.\n. Very good. :clap:\n. Seems fair to\u00a0me too.\n. Thanks!\n. Examples of possible usage:\n- checking a\u00a0string for\u00a0compatibility with an\u00a0encoding (#33, #42):\n\n``` js\niconvLite.split(someString, someEncoding).length <= 1\n```\n- transliteration (#55):\n\n``` js\niconvLite.split(someString, someEncoding).map(function(substring, index){\n   if( index % 2 === 0 ){ // encodable substring: 0th, 2nd, 4th\u2026\n      return substring;\n   } else { // non-encodable substring: 1st, 3rd, 5th\u2026\n      // TODO: use a\u00a0real transliteration here instead\u00a0of a\u00a0mere slugification\n      return require('underscore.string').slugify(substring).\n   }\n}).join('');\n```\n. I have to admit I\u00a0could have misunderstood what a\u00a0callback in #53 actually meant.\n\nWhat I\u00a0proposed\u00a0here is a\u00a0method to\u00a0deal with non-encodable parts of\u00a0a\u00a0Unicode string\u00a0\u2014 not\u00a0with a\u00a0non-decodable parts of\u00a0a\u00a0Buffer.\n\nTherefore here the\u00a0even parts (`index % 2 === 0`) are\u00a0not the\u00a0decoded parts of\u00a0some Buffer, but\u00a0rather Unicode substrings that may\u00a0fit to\u00a0a\u00a0specified encoding (but they aren't touched\u00a0yet, just split\u00a0out of the\u00a0original string).\n\nAlso the odd parts (`index %2 !== 0`) are\u00a0not the\u00a0Buffers that could\u00a0not be\u00a0decoded, but\u00a0rather Unicode\u00a0substrings containing the\u00a0characters that are\u00a0not supported\u00a0by a\u00a0specified encoding and\u00a0thus they won't be encoded.\n\nDoes it explain the\u00a0absence of Buffers and the\u00a0analogy to `String#split`?\n. The thought of several invalid characters that could be handled in\u00a0a\u00a0smarter way if\u00a0handled together\u00a0\u2014 that\u00a0is a\u00a0thought that I\u00a0borrowed from Wikipedia's \u201c[UTF-7](http://en.wikipedia.org/wiki/UTF-7)\u201d article, from its\u00a0section \u201c[Encoding](http://en.wikipedia.org/wiki/UTF-7#Encoding)\u201d. That's what they\u00a0say:\n\n> A simple encoder may encode all\u00a0characters it\u00a0considers safe for\u00a0direct encoding directly. However, the\u00a0cost of ending a\u00a0Unicode sequence, outputing a\u00a0single character directly in\u00a0ASCII and\u00a0then starting another Unicode sequence is 3 to 3\u2154 bytes. This\u00a0is more than the 2\u2154 bytes needed to\u00a0represent the\u00a0character as a\u00a0part of a\u00a0Unicode sequence.\n\nImagine a\u00a0medium where most characters are encoded with some default encoding (probably one\u00a0byte per\u00a0character, such\u00a0as CP866 or\u00a0\u041aOI-8R), but the\u00a0rest are converted to\u00a0UTF-7. Such a\u00a0method (`.split`) would facilitate a\u00a0smarter handling of \u201cinvalid\u201d characters and\u00a0even a\u00a0smarter way of dealing with single \u201cvalid\u201d characters that\u00a0appear surrounded by the\u00a0\u201cinvalid\u201d.\n. What if I\u00a0wrote a\u00a0pull\u00a0request?\n\nI\u00a0may actually be willing to\u00a0write\u00a0it, but that depends\u00a0on how\u00a0many codecs iconv-lite has: [`sbcs-codec.js`](https://github.com/ashtuchkin/iconv-lite/blob/master/encodings/sbcs-codec.js), [`dbcs-codec.js`](https://github.com/ashtuchkin/iconv-lite/blob/master/encodings/dbcs-codec.js), [`internal.js`](https://github.com/ashtuchkin/iconv-lite/blob/master/encodings/internal.js), [`utf16.js`](https://github.com/ashtuchkin/iconv-lite/blob/master/encodings/utf16.js), any\u00a0else?\n\n**Preliminary note 1.** For\u00a0the\u00a0Unicode encodings (such\u00a0as UTF-16) any JavaScript character is\u00a0encodable and\u00a0thus `.split(inputString, encodingUnicode)` would simply return `[inputString]`. I\u00a0guess the\u00a0matter of implementing `.split()` for the\u00a0latter of the\u00a0above four codecs (for [`utf16.js`](https://github.com/ashtuchkin/iconv-lite/blob/master/encodings/utf16.js)) is a\u00a0piece of\u00a0cake.\n\n**Preliminary note 2.**  One of the\u00a0remaining codecs ([`internal.js`](https://github.com/ashtuchkin/iconv-lite/blob/master/encodings/internal.js)) supports eight encodings. Five of\u00a0them (`'utf8'`, `'cesu8'`, `'unicode11utf8'`, `'ucs2'`, `'utf16le'`) are Unicode encodings and\u00a0thus `.split(inputString, encodingUnicode)` would also simply return `[inputString]` for\u00a0them. The\u00a0remaining three encodings (`'binary'`, `'base64'`, `'hex'`) have simple regexp character classes that define the\u00a0supported characters (`/[\\x00-\\xFF]/`, `/[A-Za-z0-9+\\/]/`, `/[0-9A-Fa-f]/`) and\u00a0thus `String#split` could be used as `iconvLite#split` for\u00a0them (after these\u00a0classes are\u00a0negated to\u00a0produce encodable substrings in\u00a0even positions instead\u00a0of odd).\n. Well, I\u00a0hope I\u00a0have some (more or\u00a0less compelling) use\u00a0case.\n\nYou\u00a0know how the\u00a0UTF-7 was first invented, right? There\u00a0was (and\u00a0still somewhere is) a\u00a0medium (MIME\u00a0headers) where Unicode was\u00a0not permitted and\u00a0thus any characters of\u00a0a Unicode string either would fit\u00a0in the\u00a0other (ASCII) encoding or would have to\u00a0be encoded (using ASCII characters) and\u00a0escaped (not\u00a0to\u00a0be confused with\u00a0real ASCII characters).\n\nI have to\u00a0face another similar medium now. That\u00a0medium is [Fidonet](http://en.wikipedia.org/wiki/Fidonet) where the\u00a0design of the\u00a0most popular message\u00a0editor ([GoldED+](http://en.wikipedia.org/wiki/GoldED#GoldED.2B)) makes any\u00a0support of the multibyte encodings in\u00a0GoldED+ impossible. Therefore it\u00a0is also not possible to\u00a0simply write Unicode messages to\u00a0Fidonet and expect\u00a0them to\u00a0be ever read\u00a0by\u00a0the\u00a0users of GoldED+; however, if the\u00a0text is mostly in\u00a0Russian, it\u00a0becomes possible to\u00a0write most of\u00a0the\u00a0message in\u00a0a\u00a0single-byte encoding (such\u00a0as [CP866](http://en.wikipedia.org/wiki/Code_page_866)), split\u00a0out the\u00a0substrings that won't fit, encode\u00a0them differently and\u00a0escape\u00a0them (not to\u00a0be confused with the\u00a0rest of message).\n\nIf a\u00a0standard arises for\u00a0such encoding and\u00a0escaping, then the\u00a0other message\u00a0editors (and mere Fidonet browsers and\u00a0WebBBS) could collectively embrace and\u00a0extend GoldED+.\n\nSpeaking of the different encoding of\u00a0such substrings, at\u00a0first I\u00a0suggested ([there](http://ftn.su/m/RU.FTN.DEVELOP/2:50/88+53c38653), last paragraph) that it\u00a0could\u00a0be [Punycode](http://en.wikipedia.org/wiki/Punycode); then Serguei\u00a0E.\u00a0Leontiev [pointed\u00a0out](http://ftn.su/m/RU.FTN.DEVELOP/%3C1187490639@ddt.demos.su%3E+555d042e) that UTF-7 is more\u00a0compact.\n\nHowever, before they\u00a0are differently encoded, these\u00a0substrings (outside\u00a0of CP866\u00a0range) have\u00a0to\u00a0be isolated, split\u00a0out\u00a0of the\u00a0string containing the\u00a0original (Unicode) text of the\u00a0message. That's my use\u00a0case for the `.split` method suggested\u00a0above.\n. In\u00a0a\u00a0nutshell this use\u00a0case is a\u00a0generalization of the\u00a0UTF-7's use\u00a0case: the\u00a0Unicode characters are\u00a0forced into\u00a0some 8-bit medium (defined by one\u00a0of the\u00a0supported single-byte encodings) instead\u00a0of UTF-7's original 7-bit medium.\n\nThe whole implementation of the\u00a0use\u00a0case would look like\u00a0the\u00a0following:\n\n``` js\nvar iconvLite = require('iconv-lite');\n\niconvLite.extendNodeEncodings();\n\nvar UnicodeTo8bit = function(sourceString, targetEncoding){\n   var buffers = iconvLite.split(\n      sourceString, targetEncoding\n   ).map(function(substring, index){\n      if( index % 2 === 0 ){ // encodable substring: 0th, 2nd, 4th\u2026\n         return Buffer(substring, targetEncoding);\n      } else { // non-encodable substring: 1st, 3rd, 5th\u2026\n         // TODO: define an\u00a0escaping function\n         var escapedString = escapingFunction(\n            Buffer(substring, 'utf7').toString('utf8')\n         );\n         return Buffer(escapedString, targetEncoding);\n      }\n   });\n   return Buffer.concat(buffers);\n};\n```\n. Would such callback be [programmed to\u00a0receive](http://en.wikipedia.org/wiki/Hotel_California) only one character (the\u00a0next non-encodable character)? or\u00a0a\u00a0whole substring of such characters collected until the\u00a0next encodable character is encountered (or\u00a0original\u00a0string\u00a0ended)?\n- In the latter case, more similar to [`.replace(regexp, function)`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String/replace#Specifying_a_function_as_a_parameter), such\u00a0callback would indeed make `.split` unnecessary.\n- In\u00a0the\u00a0former case the\u00a0callback may collect such substrings internally; however, unfortunately, it\u00a0would\u00a0not be\u00a0given any\u00a0hint when the\u00a0last character in\u00a0a\u00a0substring is\u00a0encountered\u00a0\u2014 where it\u00a0has to\u00a0stop collecting such a\u00a0substring and\u00a0to\u00a0return an\u00a0escaped encoded result. (Hence the\u00a0above reference to\u00a0the \u201cHotel California\u201d implying its two final verses.)\n. LGTM.\n\nThat `context` thing would seem more helpful if\u00a0it\u00a0was guaranteed to\u00a0contain some\u00a0context both\u00a0before and\u00a0after the `srt` (e.g. to\u00a0decide whether the\u00a0default `start_escape` / `end_escape` values are appropriate and\u00a0efficient for\u00a0that\u00a0context).\n\nFor\u00a0example, as [Wikipedia\u00a0says](http://en.wikipedia.org/wiki/UTF-7#Description) about UTF-7,\n\n> The end is indicated by\u00a0any character not\u00a0in the\u00a0modified Base64\u00a0set. If\u00a0the\u00a0character after the\u00a0modified Base64 is a `-` (ASCII hyphen-minus) then it\u00a0is consumed by the\u00a0decoder and\u00a0decoding resumes with the\u00a0next character. Otherwise decoding resumes with the\u00a0character after the\u00a0base64.\n\nThus `end_escape` could be `''` by\u00a0default and `'-'` if `context` contains its\u00a0own `'-'` after the `str`.\n\nHowever, in\u00a0practice `end_escape` is\u00a0going to\u00a0be `'-'` by\u00a0default and\u00a0become `''` only\u00a0if `context` does\u00a0contain some\u00a0other character (not\u00a0a\u00a0minus) after the `str`. (If `context` just\u00a0ends there, it\u00a0might\u00a0mean a\u00a0mere\u00a0end of the\u00a0current chunk and\u00a0there's no\u00a0guarantees that the\u00a0next\u00a0chunk won't start with its own `'-'`.)\n\nHowever, that's still better than not having `context` at\u00a0all. Good\u00a0enough.\n. Just\u00a0a\u00a0nudge because \u2248nine\u00a0months have\u00a0passed.\n. The first of the\u00a0above problems seems\u00a0to\u00a0be a\u00a0problem of `iconv` and, as\u00a0such, there it\u00a0should be dealt\u00a0with. (bnoordhuis/node-iconv#132)\n. Thanks, the first\u00a0problem is gone.\n\nTravis CI job logs for [iojs](https://travis-ci.org/Mithgol/iconv-lite/jobs/79810607) and [Node.js v4.0.0](https://travis-ci.org/Mithgol/iconv-lite/jobs/79810608) now seem to\u00a0indicate that `iconv-lite` can\u00a0no\u00a0longer extend Node's native\u00a0set of\u00a0encodings correctly.\n. Created some Buffers:\n\n![(screenshot)](https://cloud.githubusercontent.com/assets/1088720/9831362/56911214-595c-11e5-9f9a-f00bc0c17d21.gif)\n\nTheir lengths are fine, but their contents seem quite\u00a0random.\n\nI suspect that the\u00a0new\u00a0version of Node's `Buffer` constructor (introduced by\u00a0a\u00a0rename in nodejs/node@1057d1186b0fedf87411a1cb56c67dc385ee1f9f and\u00a0developed elsewhere before\u00a0that) just\u00a0isn't as\u00a0extendable as\u00a0its\u00a0previous version. I've opened an\u00a0issue (nodejs/node#2835) about\u00a0that.\n. > What would you suggest as a solution/fix?\n\nI guess it\u00a0really depends\u00a0on the\u00a0Node's devs' decision about nodejs/node#2835.\n- If nodejs/node#2835 becomes wontfixed, then the\u00a0code behind `extendNodeEncodings` should\u00a0indeed be\u00a0deprecated.\n- If nodejs/node#2835 becomes solved and\u00a0some\u00a0API introduced that\u00a0can\u00a0create new\u00a0encodings, then `extendNodeEncodings` would\u00a0be re-created using that\u00a0new\u00a0API and\u00a0undeprecated and\u00a0thus its\u00a0current full-scale deprecation is\u00a0somewhat premature.\n\nI\u00a0suggest that we\u00a0do\u00a0nothing until nodejs/node#2835 is\u00a0resolved; and\u00a0keep ashtuchkin/iconv-lite#107 open.\n. > \u201cIt doesn't work for `new Buffer`, but probably works for\u00a0other encoding-related operations\u201d\n\nWell, it could be \u201cIt does\u00a0not work for `new Buffer`, but\u00a0we\u00a0have\u00a0tests that\u00a0ensure that\u00a0it\u00a0works for\u00a0the\u00a0two other encoding-related operations: for `Buffer.isEncoding()` and\u00a0for `someBuffer.toString()`\u201d.\n. Personally I\u00a0don't need much\u00a0of EBCDIC; only four most\u00a0common variations. I've requested a\u00a0pull (#112).\n. EBCDIC 500 mapping has\u00a0been taken from http://www.unicode.org/Public/MAPPINGS/VENDORS/MICSFT/EBCDIC/CP500.TXT and\u00a0automatically converted from `0xXXXX` to `\\uXXXX` format for\u00a0JavaScript.\r\n\r\nEBCDIC 1148 is [said](https://en.wikipedia.org/wiki/EBCDIC_500) to\u00a0be different only at\u00a0code\u00a0point `9F` (I\u00a0have manually retyped that\u00a0difference).\r\n. Currently Wikipedia says that EBCDIC NL is 0x15 in [EBCDIC\u00a0500](https://en.wikipedia.org/wiki/EBCDIC_500) (and\u00a0in its\u00a0variation EBCDIC\u00a01148) and in\u00a0[EBCDIC\u00a0037](https://en.wikipedia.org/wiki/EBCDIC_037) (and\u00a0in its\u00a0variation EBCDIC\u00a01140).\r\n\r\nThese\u00a0four are\u00a0mapped (by\u00a0the\u00a0Microsoft mappings, mentioned above) to\u00a0U+0085 ([officially said](http://www.unicode.org/charts/PDF/U0080.pdf) to\u00a0be \u201cNEXT\u00a0LINE\u201d or\u00a0\u201cNEL\u201d) which seems correct to\u00a0me.. Could joyent/node#7633 be the\u00a0cause of\u00a0this\u00a0behaviour?\n. The upstream issue is\u00a0fixed.\n. Very good. :clap:\n. Seems fair to\u00a0me too.\n. Thanks!\n. Examples of possible usage:\n- checking a\u00a0string for\u00a0compatibility with an\u00a0encoding (#33, #42):\n\n``` js\niconvLite.split(someString, someEncoding).length <= 1\n```\n- transliteration (#55):\n\n``` js\niconvLite.split(someString, someEncoding).map(function(substring, index){\n   if( index % 2 === 0 ){ // encodable substring: 0th, 2nd, 4th\u2026\n      return substring;\n   } else { // non-encodable substring: 1st, 3rd, 5th\u2026\n      // TODO: use a\u00a0real transliteration here instead\u00a0of a\u00a0mere slugification\n      return require('underscore.string').slugify(substring).\n   }\n}).join('');\n```\n. I have to admit I\u00a0could have misunderstood what a\u00a0callback in #53 actually meant.\n\nWhat I\u00a0proposed\u00a0here is a\u00a0method to\u00a0deal with non-encodable parts of\u00a0a\u00a0Unicode string\u00a0\u2014 not\u00a0with a\u00a0non-decodable parts of\u00a0a\u00a0Buffer.\n\nTherefore here the\u00a0even parts (`index % 2 === 0`) are\u00a0not the\u00a0decoded parts of\u00a0some Buffer, but\u00a0rather Unicode substrings that may\u00a0fit to\u00a0a\u00a0specified encoding (but they aren't touched\u00a0yet, just split\u00a0out of the\u00a0original string).\n\nAlso the odd parts (`index %2 !== 0`) are\u00a0not the\u00a0Buffers that could\u00a0not be\u00a0decoded, but\u00a0rather Unicode\u00a0substrings containing the\u00a0characters that are\u00a0not supported\u00a0by a\u00a0specified encoding and\u00a0thus they won't be encoded.\n\nDoes it explain the\u00a0absence of Buffers and the\u00a0analogy to `String#split`?\n. The thought of several invalid characters that could be handled in\u00a0a\u00a0smarter way if\u00a0handled together\u00a0\u2014 that\u00a0is a\u00a0thought that I\u00a0borrowed from Wikipedia's \u201c[UTF-7](http://en.wikipedia.org/wiki/UTF-7)\u201d article, from its\u00a0section \u201c[Encoding](http://en.wikipedia.org/wiki/UTF-7#Encoding)\u201d. That's what they\u00a0say:\n\n> A simple encoder may encode all\u00a0characters it\u00a0considers safe for\u00a0direct encoding directly. However, the\u00a0cost of ending a\u00a0Unicode sequence, outputing a\u00a0single character directly in\u00a0ASCII and\u00a0then starting another Unicode sequence is 3 to 3\u2154 bytes. This\u00a0is more than the 2\u2154 bytes needed to\u00a0represent the\u00a0character as a\u00a0part of a\u00a0Unicode sequence.\n\nImagine a\u00a0medium where most characters are encoded with some default encoding (probably one\u00a0byte per\u00a0character, such\u00a0as CP866 or\u00a0\u041aOI-8R), but the\u00a0rest are converted to\u00a0UTF-7. Such a\u00a0method (`.split`) would facilitate a\u00a0smarter handling of \u201cinvalid\u201d characters and\u00a0even a\u00a0smarter way of dealing with single \u201cvalid\u201d characters that\u00a0appear surrounded by the\u00a0\u201cinvalid\u201d.\n. What if I\u00a0wrote a\u00a0pull\u00a0request?\n\nI\u00a0may actually be willing to\u00a0write\u00a0it, but that depends\u00a0on how\u00a0many codecs iconv-lite has: [`sbcs-codec.js`](https://github.com/ashtuchkin/iconv-lite/blob/master/encodings/sbcs-codec.js), [`dbcs-codec.js`](https://github.com/ashtuchkin/iconv-lite/blob/master/encodings/dbcs-codec.js), [`internal.js`](https://github.com/ashtuchkin/iconv-lite/blob/master/encodings/internal.js), [`utf16.js`](https://github.com/ashtuchkin/iconv-lite/blob/master/encodings/utf16.js), any\u00a0else?\n\n**Preliminary note 1.** For\u00a0the\u00a0Unicode encodings (such\u00a0as UTF-16) any JavaScript character is\u00a0encodable and\u00a0thus `.split(inputString, encodingUnicode)` would simply return `[inputString]`. I\u00a0guess the\u00a0matter of implementing `.split()` for the\u00a0latter of the\u00a0above four codecs (for [`utf16.js`](https://github.com/ashtuchkin/iconv-lite/blob/master/encodings/utf16.js)) is a\u00a0piece of\u00a0cake.\n\n**Preliminary note 2.**  One of the\u00a0remaining codecs ([`internal.js`](https://github.com/ashtuchkin/iconv-lite/blob/master/encodings/internal.js)) supports eight encodings. Five of\u00a0them (`'utf8'`, `'cesu8'`, `'unicode11utf8'`, `'ucs2'`, `'utf16le'`) are Unicode encodings and\u00a0thus `.split(inputString, encodingUnicode)` would also simply return `[inputString]` for\u00a0them. The\u00a0remaining three encodings (`'binary'`, `'base64'`, `'hex'`) have simple regexp character classes that define the\u00a0supported characters (`/[\\x00-\\xFF]/`, `/[A-Za-z0-9+\\/]/`, `/[0-9A-Fa-f]/`) and\u00a0thus `String#split` could be used as `iconvLite#split` for\u00a0them (after these\u00a0classes are\u00a0negated to\u00a0produce encodable substrings in\u00a0even positions instead\u00a0of odd).\n. Well, I\u00a0hope I\u00a0have some (more or\u00a0less compelling) use\u00a0case.\n\nYou\u00a0know how the\u00a0UTF-7 was first invented, right? There\u00a0was (and\u00a0still somewhere is) a\u00a0medium (MIME\u00a0headers) where Unicode was\u00a0not permitted and\u00a0thus any characters of\u00a0a Unicode string either would fit\u00a0in the\u00a0other (ASCII) encoding or would have to\u00a0be encoded (using ASCII characters) and\u00a0escaped (not\u00a0to\u00a0be confused with\u00a0real ASCII characters).\n\nI have to\u00a0face another similar medium now. That\u00a0medium is [Fidonet](http://en.wikipedia.org/wiki/Fidonet) where the\u00a0design of the\u00a0most popular message\u00a0editor ([GoldED+](http://en.wikipedia.org/wiki/GoldED#GoldED.2B)) makes any\u00a0support of the multibyte encodings in\u00a0GoldED+ impossible. Therefore it\u00a0is also not possible to\u00a0simply write Unicode messages to\u00a0Fidonet and expect\u00a0them to\u00a0be ever read\u00a0by\u00a0the\u00a0users of GoldED+; however, if the\u00a0text is mostly in\u00a0Russian, it\u00a0becomes possible to\u00a0write most of\u00a0the\u00a0message in\u00a0a\u00a0single-byte encoding (such\u00a0as [CP866](http://en.wikipedia.org/wiki/Code_page_866)), split\u00a0out the\u00a0substrings that won't fit, encode\u00a0them differently and\u00a0escape\u00a0them (not to\u00a0be confused with the\u00a0rest of message).\n\nIf a\u00a0standard arises for\u00a0such encoding and\u00a0escaping, then the\u00a0other message\u00a0editors (and mere Fidonet browsers and\u00a0WebBBS) could collectively embrace and\u00a0extend GoldED+.\n\nSpeaking of the different encoding of\u00a0such substrings, at\u00a0first I\u00a0suggested ([there](http://ftn.su/m/RU.FTN.DEVELOP/2:50/88+53c38653), last paragraph) that it\u00a0could\u00a0be [Punycode](http://en.wikipedia.org/wiki/Punycode); then Serguei\u00a0E.\u00a0Leontiev [pointed\u00a0out](http://ftn.su/m/RU.FTN.DEVELOP/%3C1187490639@ddt.demos.su%3E+555d042e) that UTF-7 is more\u00a0compact.\n\nHowever, before they\u00a0are differently encoded, these\u00a0substrings (outside\u00a0of CP866\u00a0range) have\u00a0to\u00a0be isolated, split\u00a0out\u00a0of the\u00a0string containing the\u00a0original (Unicode) text of the\u00a0message. That's my use\u00a0case for the `.split` method suggested\u00a0above.\n. In\u00a0a\u00a0nutshell this use\u00a0case is a\u00a0generalization of the\u00a0UTF-7's use\u00a0case: the\u00a0Unicode characters are\u00a0forced into\u00a0some 8-bit medium (defined by one\u00a0of the\u00a0supported single-byte encodings) instead\u00a0of UTF-7's original 7-bit medium.\n\nThe whole implementation of the\u00a0use\u00a0case would look like\u00a0the\u00a0following:\n\n``` js\nvar iconvLite = require('iconv-lite');\n\niconvLite.extendNodeEncodings();\n\nvar UnicodeTo8bit = function(sourceString, targetEncoding){\n   var buffers = iconvLite.split(\n      sourceString, targetEncoding\n   ).map(function(substring, index){\n      if( index % 2 === 0 ){ // encodable substring: 0th, 2nd, 4th\u2026\n         return Buffer(substring, targetEncoding);\n      } else { // non-encodable substring: 1st, 3rd, 5th\u2026\n         // TODO: define an\u00a0escaping function\n         var escapedString = escapingFunction(\n            Buffer(substring, 'utf7').toString('utf8')\n         );\n         return Buffer(escapedString, targetEncoding);\n      }\n   });\n   return Buffer.concat(buffers);\n};\n```\n. Would such callback be [programmed to\u00a0receive](http://en.wikipedia.org/wiki/Hotel_California) only one character (the\u00a0next non-encodable character)? or\u00a0a\u00a0whole substring of such characters collected until the\u00a0next encodable character is encountered (or\u00a0original\u00a0string\u00a0ended)?\n- In the latter case, more similar to [`.replace(regexp, function)`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String/replace#Specifying_a_function_as_a_parameter), such\u00a0callback would indeed make `.split` unnecessary.\n- In\u00a0the\u00a0former case the\u00a0callback may collect such substrings internally; however, unfortunately, it\u00a0would\u00a0not be\u00a0given any\u00a0hint when the\u00a0last character in\u00a0a\u00a0substring is\u00a0encountered\u00a0\u2014 where it\u00a0has to\u00a0stop collecting such a\u00a0substring and\u00a0to\u00a0return an\u00a0escaped encoded result. (Hence the\u00a0above reference to\u00a0the \u201cHotel California\u201d implying its two final verses.)\n. LGTM.\n\nThat `context` thing would seem more helpful if\u00a0it\u00a0was guaranteed to\u00a0contain some\u00a0context both\u00a0before and\u00a0after the `srt` (e.g. to\u00a0decide whether the\u00a0default `start_escape` / `end_escape` values are appropriate and\u00a0efficient for\u00a0that\u00a0context).\n\nFor\u00a0example, as [Wikipedia\u00a0says](http://en.wikipedia.org/wiki/UTF-7#Description) about UTF-7,\n\n> The end is indicated by\u00a0any character not\u00a0in the\u00a0modified Base64\u00a0set. If\u00a0the\u00a0character after the\u00a0modified Base64 is a `-` (ASCII hyphen-minus) then it\u00a0is consumed by the\u00a0decoder and\u00a0decoding resumes with the\u00a0next character. Otherwise decoding resumes with the\u00a0character after the\u00a0base64.\n\nThus `end_escape` could be `''` by\u00a0default and `'-'` if `context` contains its\u00a0own `'-'` after the `str`.\n\nHowever, in\u00a0practice `end_escape` is\u00a0going to\u00a0be `'-'` by\u00a0default and\u00a0become `''` only\u00a0if `context` does\u00a0contain some\u00a0other character (not\u00a0a\u00a0minus) after the `str`. (If `context` just\u00a0ends there, it\u00a0might\u00a0mean a\u00a0mere\u00a0end of the\u00a0current chunk and\u00a0there's no\u00a0guarantees that the\u00a0next\u00a0chunk won't start with its own `'-'`.)\n\nHowever, that's still better than not having `context` at\u00a0all. Good\u00a0enough.\n. Just\u00a0a\u00a0nudge because \u2248nine\u00a0months have\u00a0passed.\n. The first of the\u00a0above problems seems\u00a0to\u00a0be a\u00a0problem of `iconv` and, as\u00a0such, there it\u00a0should be dealt\u00a0with. (bnoordhuis/node-iconv#132)\n. Thanks, the first\u00a0problem is gone.\n\nTravis CI job logs for [iojs](https://travis-ci.org/Mithgol/iconv-lite/jobs/79810607) and [Node.js v4.0.0](https://travis-ci.org/Mithgol/iconv-lite/jobs/79810608) now seem to\u00a0indicate that `iconv-lite` can\u00a0no\u00a0longer extend Node's native\u00a0set of\u00a0encodings correctly.\n. Created some Buffers:\n\n![(screenshot)](https://cloud.githubusercontent.com/assets/1088720/9831362/56911214-595c-11e5-9f9a-f00bc0c17d21.gif)\n\nTheir lengths are fine, but their contents seem quite\u00a0random.\n\nI suspect that the\u00a0new\u00a0version of Node's `Buffer` constructor (introduced by\u00a0a\u00a0rename in nodejs/node@1057d1186b0fedf87411a1cb56c67dc385ee1f9f and\u00a0developed elsewhere before\u00a0that) just\u00a0isn't as\u00a0extendable as\u00a0its\u00a0previous version. I've opened an\u00a0issue (nodejs/node#2835) about\u00a0that.\n. > What would you suggest as a solution/fix?\n\nI guess it\u00a0really depends\u00a0on the\u00a0Node's devs' decision about nodejs/node#2835.\n- If nodejs/node#2835 becomes wontfixed, then the\u00a0code behind `extendNodeEncodings` should\u00a0indeed be\u00a0deprecated.\n- If nodejs/node#2835 becomes solved and\u00a0some\u00a0API introduced that\u00a0can\u00a0create new\u00a0encodings, then `extendNodeEncodings` would\u00a0be re-created using that\u00a0new\u00a0API and\u00a0undeprecated and\u00a0thus its\u00a0current full-scale deprecation is\u00a0somewhat premature.\n\nI\u00a0suggest that we\u00a0do\u00a0nothing until nodejs/node#2835 is\u00a0resolved; and\u00a0keep ashtuchkin/iconv-lite#107 open.\n. > \u201cIt doesn't work for `new Buffer`, but probably works for\u00a0other encoding-related operations\u201d\n\nWell, it could be \u201cIt does\u00a0not work for `new Buffer`, but\u00a0we\u00a0have\u00a0tests that\u00a0ensure that\u00a0it\u00a0works for\u00a0the\u00a0two other encoding-related operations: for `Buffer.isEncoding()` and\u00a0for `someBuffer.toString()`\u201d.\n. Personally I\u00a0don't need much\u00a0of EBCDIC; only four most\u00a0common variations. I've requested a\u00a0pull (#112).\n. EBCDIC 500 mapping has\u00a0been taken from http://www.unicode.org/Public/MAPPINGS/VENDORS/MICSFT/EBCDIC/CP500.TXT and\u00a0automatically converted from `0xXXXX` to `\\uXXXX` format for\u00a0JavaScript.\r\n\r\nEBCDIC 1148 is [said](https://en.wikipedia.org/wiki/EBCDIC_500) to\u00a0be different only at\u00a0code\u00a0point `9F` (I\u00a0have manually retyped that\u00a0difference).\r\n. Currently Wikipedia says that EBCDIC NL is 0x15 in [EBCDIC\u00a0500](https://en.wikipedia.org/wiki/EBCDIC_500) (and\u00a0in its\u00a0variation EBCDIC\u00a01148) and in\u00a0[EBCDIC\u00a0037](https://en.wikipedia.org/wiki/EBCDIC_037) (and\u00a0in its\u00a0variation EBCDIC\u00a01140).\r\n\r\nThese\u00a0four are\u00a0mapped (by\u00a0the\u00a0Microsoft mappings, mentioned above) to\u00a0U+0085 ([officially said](http://www.unicode.org/charts/PDF/U0080.pdf) to\u00a0be \u201cNEXT\u00a0LINE\u201d or\u00a0\u201cNEL\u201d) which seems correct to\u00a0me.. ",
    "cgc": "the only compelling reason I've got is backwards compatability. in any case, I totally respect asking users to be explicit.\n. the only compelling reason I've got is backwards compatability. in any case, I totally respect asking users to be explicit.\n. ",
    "dougwilson": "Actually, this library does not even support UTF-16 or UTF-32. Would you mind if a made a PR adding support for these two encodings? (UTF-16 is not the same as UTF-16LE, specifically the behavior of the BOM)\n. > Just out of curiosity, where do you need that?\n\nWe have a SOAP service (which is XML bodies being sent) that we want to move into our routing layer using Node.js and route based on the SOAP header within the XML, so we want to decode the POSTed payloads. Because of the age of that technology, a lot of the POSTs are `text/html; charset=utf-16` (which UTF-16 includes the BOM so it can be determined if it is LE or BE). And yes, all the UTF-16 we are getting is BE, since that is what the Microsoft WCF accepts and it is network order.\n\nThere is already a UTF-16LE codec (same as in Node.js core). What would be nice would be a UTF-16BE codec, but what I think would be awesome if if there could also be a UTF-16 codec, which would wait for the first 2 bytes to be written to the decoder and then switch to LE or BE based on that or error is it's not a BOM (and perhaps the codec has an option to strip the BOM for the output?). The main problem is how such a \"UTF-16\" codec would work in encoding mode, since the name doesn't say the serialization order. Perhaps it can just be set to not allow serialization, or it takes an option to which order and if to automatically insert the appropriate BOM.\n\nAll of the BOM stuff can be solved without this library helping, but manually reading off the first 4 bytes and creating the appropriate codec here, so it's definitely not a must-have, but if there was a UTF-16 codec with BOM reading, it would mean we could directly pass in the stream and ask for the codec with the name of the `charset` in the `content-type`.\n. P.S. Thank you for a library that does not require complication for us Windows users :D\n. Awesome, I'll try it out tomorrow. You're amazing! The description you posted sounds like it'll cover everything we need :) We do not send or receive any UTF-32, so someone else will need to request that ;)\n. OK, I tried it out and it works great! Thanks soo much!\n. yay! :) :snowman: I don't expect you to keep it enabled if there are no simple ticks available in the future, btw, nor do I expect you to actually take back the official support note in the change log :)\n. This requires iconv dep to be at least 2.1.5\n. Weird. You wouldn't think a patch version would cause all that mess :S\n. Cool, that seems to work perfectly :)\n. I need both an encoder and a decoder (because I need to write to the databases, not just read). The Node.js UTF-8 decoder does support CESU-8, yes. This is more about an encoder (though I believe Node.js utf8 only supports CESU-8 out of coincidence, rather than intention).\n. I tried that trick but it does not work. Node replaces all the surrogate pairs with the Unicode replacement character...\n. Right, yea, you can see it changed both surrogates to ef bf bd which is UTF-8 encoding of U+FFFD so decoding that just gives you two question marks instead of the original character, lol.\n. ```\n$ node -pe 'require(\"iconv-lite\").encode(\"yes \\uD83D\\uDC4D\",\"cesu8\")'\n<Buffer 79 65 73 20 ed a0 bd ed b1 8d>\n```\n\n:joy:\n. It also looks like Node.js 4.0 can no longer natively decode CESU-8.\n. Awesome, thank you, @ashtuchkin !\n\nP.S. whatever happens in #107, please do not even think about unpublishing 0.4.12 from npm, pretty please, as I am publishing a module that depends on 0.4.12 :)\n. From http://semver.org/ :\n\n> Major version zero (0.y.z) is for initial development. Anything may change at any time. The public API should not be considered stable.\n\nYes, it sucks that `npm` allows people to use `~` and `^` with 0.y.z versions, even though that will likely cause a bunch of headaches.\n. Actually, this library does not even support UTF-16 or UTF-32. Would you mind if a made a PR adding support for these two encodings? (UTF-16 is not the same as UTF-16LE, specifically the behavior of the BOM)\n. > Just out of curiosity, where do you need that?\n\nWe have a SOAP service (which is XML bodies being sent) that we want to move into our routing layer using Node.js and route based on the SOAP header within the XML, so we want to decode the POSTed payloads. Because of the age of that technology, a lot of the POSTs are `text/html; charset=utf-16` (which UTF-16 includes the BOM so it can be determined if it is LE or BE). And yes, all the UTF-16 we are getting is BE, since that is what the Microsoft WCF accepts and it is network order.\n\nThere is already a UTF-16LE codec (same as in Node.js core). What would be nice would be a UTF-16BE codec, but what I think would be awesome if if there could also be a UTF-16 codec, which would wait for the first 2 bytes to be written to the decoder and then switch to LE or BE based on that or error is it's not a BOM (and perhaps the codec has an option to strip the BOM for the output?). The main problem is how such a \"UTF-16\" codec would work in encoding mode, since the name doesn't say the serialization order. Perhaps it can just be set to not allow serialization, or it takes an option to which order and if to automatically insert the appropriate BOM.\n\nAll of the BOM stuff can be solved without this library helping, but manually reading off the first 4 bytes and creating the appropriate codec here, so it's definitely not a must-have, but if there was a UTF-16 codec with BOM reading, it would mean we could directly pass in the stream and ask for the codec with the name of the `charset` in the `content-type`.\n. P.S. Thank you for a library that does not require complication for us Windows users :D\n. Awesome, I'll try it out tomorrow. You're amazing! The description you posted sounds like it'll cover everything we need :) We do not send or receive any UTF-32, so someone else will need to request that ;)\n. OK, I tried it out and it works great! Thanks soo much!\n. yay! :) :snowman: I don't expect you to keep it enabled if there are no simple ticks available in the future, btw, nor do I expect you to actually take back the official support note in the change log :)\n. This requires iconv dep to be at least 2.1.5\n. Weird. You wouldn't think a patch version would cause all that mess :S\n. Cool, that seems to work perfectly :)\n. I need both an encoder and a decoder (because I need to write to the databases, not just read). The Node.js UTF-8 decoder does support CESU-8, yes. This is more about an encoder (though I believe Node.js utf8 only supports CESU-8 out of coincidence, rather than intention).\n. I tried that trick but it does not work. Node replaces all the surrogate pairs with the Unicode replacement character...\n. Right, yea, you can see it changed both surrogates to ef bf bd which is UTF-8 encoding of U+FFFD so decoding that just gives you two question marks instead of the original character, lol.\n. ```\n$ node -pe 'require(\"iconv-lite\").encode(\"yes \\uD83D\\uDC4D\",\"cesu8\")'\n<Buffer 79 65 73 20 ed a0 bd ed b1 8d>\n```\n\n:joy:\n. It also looks like Node.js 4.0 can no longer natively decode CESU-8.\n. Awesome, thank you, @ashtuchkin !\n\nP.S. whatever happens in #107, please do not even think about unpublishing 0.4.12 from npm, pretty please, as I am publishing a module that depends on 0.4.12 :)\n. From http://semver.org/ :\n\n> Major version zero (0.y.z) is for initial development. Anything may change at any time. The public API should not be considered stable.\n\nYes, it sucks that `npm` allows people to use `~` and `^` with 0.y.z versions, even though that will likely cause a bunch of headaches.\n. ",
    "btd": "Thanks, for response. My purpose is getting some possibly text content via http requests, and from headers get charset and decode them to utf-8 (i.e. that is what browsers do =), as header Content-Type: mime; charset=IANA_charset name or alias, thus i was need to know that question. \nI do not want to support all encodings, it is enough most common one byte encoding and utf-8, utf-16*.\nThanks that point to `. encodingExists` it should be enough to see if will be any errors, just because it is missing, but i think with main encoding names, should no be problems (i have another module that resolve aliases).\n. Thanks, for response. My purpose is getting some possibly text content via http requests, and from headers get charset and decode them to utf-8 (i.e. that is what browsers do =), as header Content-Type: mime; charset=IANA_charset name or alias, thus i was need to know that question. \nI do not want to support all encodings, it is enough most common one byte encoding and utf-8, utf-16*.\nThanks that point to `. encodingExists` it should be enough to see if will be any errors, just because it is missing, but i think with main encoding names, should no be problems (i have another module that resolve aliases).\n. ",
    "andris9": "There are already some utf7 implementations for javascript (for example the [utf7 module for node](https://github.com/kkaefer/utf7) or [this gist](https://gist.github.com/peteroupc/08c5ecc8131a76062ffe)) but it would be great if it would be baked in to iconv-lite\n. 'UTF-7-IMAP' seems fair to me. I do not know if anyone uses the modified format outside IMAP folder naming and it is going to be deprecated in the long term by RFC6855 anyway.\n. There are already some utf7 implementations for javascript (for example the [utf7 module for node](https://github.com/kkaefer/utf7) or [this gist](https://gist.github.com/peteroupc/08c5ecc8131a76062ffe)) but it would be great if it would be baked in to iconv-lite\n. 'UTF-7-IMAP' seems fair to me. I do not know if anyone uses the modified format outside IMAP folder naming and it is going to be deprecated in the long term by RFC6855 anyway.\n. ",
    "benjycui": "Sorry, this project is a private project of my company, it maybe illegal if I show it here :(\n. I think it is OK now, THANKS a lot!\n\n```\npath-to/iconv-lite/encodings/dbcs-codec.js:506\n            throw new Error(\"iconv-lite internal error: invalid decoding table\n                  ^\nError: iconv-lite internal error: invalid decoding table value undefined at 0/{\"cache\":true,\"code\":\"OK\",\"content\":[],\"date\":[\"2014-08-22 08:09:00\",1408666140000],\"dymHeads\":[\"url\"],\"errorMessage\":\"\",\"fatalMsg\":\"\",\"uuid\":\"\"}\n    at Object.decoderDBCSWrite [as write] (path-to/iconv-lite/encodings/dbcs-codec.js:506:19)\n    at Object.decode (path-to/iconv-lite/lib/index.js:36:23)\n```\n. Haha... Sharp eyes :) I am just a green hand to nodejs, so a foolish mistake.\n. Sorry, this project is a private project of my company, it maybe illegal if I show it here :(\n. I think it is OK now, THANKS a lot!\n\n```\npath-to/iconv-lite/encodings/dbcs-codec.js:506\n            throw new Error(\"iconv-lite internal error: invalid decoding table\n                  ^\nError: iconv-lite internal error: invalid decoding table value undefined at 0/{\"cache\":true,\"code\":\"OK\",\"content\":[],\"date\":[\"2014-08-22 08:09:00\",1408666140000],\"dymHeads\":[\"url\"],\"errorMessage\":\"\",\"fatalMsg\":\"\",\"uuid\":\"\"}\n    at Object.decoderDBCSWrite [as write] (path-to/iconv-lite/encodings/dbcs-codec.js:506:19)\n    at Object.decode (path-to/iconv-lite/lib/index.js:36:23)\n```\n. Haha... Sharp eyes :) I am just a green hand to nodejs, so a foolish mistake.\n. ",
    "danielppereira": "Sorry, my bad.\n\nI was watching the variable content through the chrome console inspector.\nI think it's a limitation from the browser to show the entire content file.\n\nUsing the console.log, the content isn't truncated.\n\nSorry again, and thks.\n. Sorry, my bad.\n\nI was watching the variable content through the chrome console inspector.\nI think it's a limitation from the browser to show the entire content file.\n\nUsing the console.log, the content isn't truncated.\n\nSorry again, and thks.\n. ",
    "jincdream": "now  I use this way to solve:\nfs.readFile(path,{encoding:'utf-8'},function(err,data){\n        buf = iconv.encode(data,'gb2312');\n        console.log();\n        fs.writeFile(outName,buf,function(){\n            iconv.extendNodeEncodings();\n            var str = fs.readFileSync(outName,'gb2312');\n            str = str.substring(1);\n            fs.writeFileSync(outName,str,'gb2312');\n            iconv.undoExtendNodeEncodings();\n        });\n    });\n. \"version\": \"0.4.4\",\"GBK encoding has no code for the BOM character\";\nIt's helpful , thank u.\n. var fs = require('fs');\nvar ph = require('path');\nvar iconv = require('iconv-lite');\nmodule.exports = function(path,req,write){\n  iconv.extendNodeEncodings();\n  var read = fs.createReadStream(path,'gbk')\n  iconv.undoExtendNodeEncodings();\n  return read\n}\n. Oh,sorry~ It doesn't work --> fs.createReadStream(fileName,{encoding:'gbk'})\n\nand error:\n    throw new Error('Unknown encoding: ' + encoding);\n          ^\nError: Unknown encoding: gbk\n. It's helpful.\nI have two module file :server.js & readGbk.js.\nIn this problem,server.js require readGbk.js.\n\nreadGbk.js:\n\n``` javascript\nvar fs = require('fs');\nvar ph = require('path');\nvar iconv = require('iconv-lite');\nmodule.exports = function(path,req,write){\n  iconv.extendNodeEncodings();\n  var read = fs.createReadStream(path,{encoding:'gbk'})\n  iconv.undoExtendNodeEncodings();\n  return read\n}\n```\n\nserver.js:\n\n``` javascript\nreadGbk = require('../readGbk');\nserver.render = function(req,res){\n  ....\n  readGbk(file).pipe(res)\n}\n```\n\nRun and throw an error:\n\n```\nand error:\nthrow new Error('Unknown encoding: ' + encoding);\n^\nError: Unknown encoding: gbk\n```\n\nNow ,\nsolution A:\nreadGbk.js:\n\n``` javascript\nvar fs = require('fs');\nvar ph = require('path');\nvar iconv = require('iconv-lite');\nmodule.exports = function(path,req,write){\n  iconv.extendNodeEncodings();\n  var read = fs.createReadStream(path,{encoding:'gbk'})\n\n  return read\n}\n```\n\nsolution B:\nreadGbk.js:\n\n``` javascript\nvar fs = require('fs');\nvar ph = require('path');\nvar iconv = require('iconv-lite');\nmodule.exports = function(path,req,write){ \n  var read = fs.createReadStream(path,{encoding:'gbk'})\n  return read\n}\n```\n\nserver.js:\n\n``` javascript\niconv.extendNodeEncodings();\nreadGbk = require('../readGbk');\nserver.render = function(req,res){\n  ....\n  readGbk(file).pipe(res)\n}\n```\n\nThx.\n. now  I use this way to solve:\nfs.readFile(path,{encoding:'utf-8'},function(err,data){\n        buf = iconv.encode(data,'gb2312');\n        console.log();\n        fs.writeFile(outName,buf,function(){\n            iconv.extendNodeEncodings();\n            var str = fs.readFileSync(outName,'gb2312');\n            str = str.substring(1);\n            fs.writeFileSync(outName,str,'gb2312');\n            iconv.undoExtendNodeEncodings();\n        });\n    });\n. \"version\": \"0.4.4\",\"GBK encoding has no code for the BOM character\";\nIt's helpful , thank u.\n. var fs = require('fs');\nvar ph = require('path');\nvar iconv = require('iconv-lite');\nmodule.exports = function(path,req,write){\n  iconv.extendNodeEncodings();\n  var read = fs.createReadStream(path,'gbk')\n  iconv.undoExtendNodeEncodings();\n  return read\n}\n. Oh,sorry~ It doesn't work --> fs.createReadStream(fileName,{encoding:'gbk'})\n\nand error:\n    throw new Error('Unknown encoding: ' + encoding);\n          ^\nError: Unknown encoding: gbk\n. It's helpful.\nI have two module file :server.js & readGbk.js.\nIn this problem,server.js require readGbk.js.\n\nreadGbk.js:\n\n``` javascript\nvar fs = require('fs');\nvar ph = require('path');\nvar iconv = require('iconv-lite');\nmodule.exports = function(path,req,write){\n  iconv.extendNodeEncodings();\n  var read = fs.createReadStream(path,{encoding:'gbk'})\n  iconv.undoExtendNodeEncodings();\n  return read\n}\n```\n\nserver.js:\n\n``` javascript\nreadGbk = require('../readGbk');\nserver.render = function(req,res){\n  ....\n  readGbk(file).pipe(res)\n}\n```\n\nRun and throw an error:\n\n```\nand error:\nthrow new Error('Unknown encoding: ' + encoding);\n^\nError: Unknown encoding: gbk\n```\n\nNow ,\nsolution A:\nreadGbk.js:\n\n``` javascript\nvar fs = require('fs');\nvar ph = require('path');\nvar iconv = require('iconv-lite');\nmodule.exports = function(path,req,write){\n  iconv.extendNodeEncodings();\n  var read = fs.createReadStream(path,{encoding:'gbk'})\n\n  return read\n}\n```\n\nsolution B:\nreadGbk.js:\n\n``` javascript\nvar fs = require('fs');\nvar ph = require('path');\nvar iconv = require('iconv-lite');\nmodule.exports = function(path,req,write){ \n  var read = fs.createReadStream(path,{encoding:'gbk'})\n  return read\n}\n```\n\nserver.js:\n\n``` javascript\niconv.extendNodeEncodings();\nreadGbk = require('../readGbk');\nserver.render = function(req,res){\n  ....\n  readGbk(file).pipe(res)\n}\n```\n\nThx.\n. ",
    "arikw": "node v0.10.21\n. node v0.10.21\n. ",
    "sharmalalit": "@ashtuchkin Thanks for your response. I already had this change in place but I forgot to update the charset in header. \nYou can close this issue. \n\nWhile trying to convert the html content in different encoding , I sometime get response data empty and I am not able to modify it or see it in debug.\n\nConsider the case when charset is not present in header of request.\nvar concat = require('concat-stream');\n\nvar cstream = concat(function(data)) {\n   //data is empty string if it was non-utf8 encoded say shift_jis\n   // function to decode the data using iconv-lite \n   convertedData =  iconv.decode('shift_jis');\n   // function to do html parsing processedData\n  processedData = process(convertedData );\n\n  dest.write(processedData);\n  dest.end();\n}\n\nsrc.pipe(cStream);\n\nSee the data above is empty if html content was shift_jis encoded otherwise it works fine. As Node does not shift_jis by default this may be the case. But even after adding iconv.extendNodeEncodings(); does not help the case and buffer is still unreadable.\n. @ashtuchkin Thanks for your response. I already had this change in place but I forgot to update the charset in header. \nYou can close this issue. \n\nWhile trying to convert the html content in different encoding , I sometime get response data empty and I am not able to modify it or see it in debug.\n\nConsider the case when charset is not present in header of request.\nvar concat = require('concat-stream');\n\nvar cstream = concat(function(data)) {\n   //data is empty string if it was non-utf8 encoded say shift_jis\n   // function to decode the data using iconv-lite \n   convertedData =  iconv.decode('shift_jis');\n   // function to do html parsing processedData\n  processedData = process(convertedData );\n\n  dest.write(processedData);\n  dest.end();\n}\n\nsrc.pipe(cStream);\n\nSee the data above is empty if html content was shift_jis encoded otherwise it works fine. As Node does not shift_jis by default this may be the case. But even after adding iconv.extendNodeEncodings(); does not help the case and buffer is still unreadable.\n. ",
    "bpasero": "@ashtuchkin I was thinking of a similar request today. Is there any way to know (after a call to encode) that the output is bogus? I think iconv is more agressive on throwing an error if the encoding choice does not make sense for the input. See https://github.com/bnoordhuis/node-iconv#dealing-with-untranslatable-characters\n. @ashtuchkin got it. But in the docs you say \"Untranslatable characters are set to \ufffd or ?\", so it might also be \"?\" which I cannot really check for because its a valid character.\n. Right, but \"?\" is ambiguos because it might also be a character used in the original text. I think raising some flag or error would make more sense to catch this case.\n. In our case, we use encode() to convert an existing file to another encoding. E.g. we have a source UTF-16 file that we want to save as DOS encoding. DOS can not present all encodings (Chinese et. al), so I would like to show an error to the user. Similar to how in Sublime Text you will not be able to save a file in an encoding that cannot represent all characters you have before encoding.\n\nRe UTF8: understood. i also think that UTF 8 will not be an issue in any case. This is more about encoding from a rich character set to one that cannot represent all characters.\n. Makes sense!\n. Hi Alexander, thanks for the prompt reply. I was thinking a little bit about what API could work and came to the conclusion that ideally this would all be handled from the encoding name itself. Imagine alongside utf8, utf16le and utf16be we would have utf8_bom, utf16le_bom and utf16be_bom. This would make the choice of BOM very explicit and would also bind it to the actual encoding. In other words, it would not make sense to have an option to set the bom for an encoding that does not know any BOM (e.g. cp1252).\n\nI am currently only using the encode/decode API where imho for encode() this would work nicely:\n- encode() would add the BOM if the encoding indicates so\n- encode() would remove any BOM if the encoding does not indicate to add a BOM\n- encode() would remove any wrong BOMs if a node buffer is passed in in both cases\n\nFor decode() its a bit weird though, I have to say. I think decode() should _always_ remove the BOM from the string returned. It could still treat utf8_bom, utf16be_bom and utf16le_bom as utf8, utf16be and utf16le.\n. Basically I would argue that a string should never ever include a BOM because I cannot think of a use case where you would want the BOM in the string. So, decode() looks at BOMs and removes them because it returns a string.\n\nEncode should add the BOM if specified (either by option or encoding name).\n\nIn many cases you would want a BOM for UTF16LE and UTF16BE anyways. Because tools cannot know which encoding to use by looking at the bytes. I believe UTF16 without BOM does not make a lot of sense.\n\nI am working for Visual Studio Code (https://code.visualstudio.com/) and iconv-lite is being used for all encoding matters.\n. Nice and I will be happy to test it and give feedback!\n. @ashtuchkin great, I think the API is very good. I will give it a try next week and report back how it works.\n. @ashtuchkin added to our editor and things are smooth so far. thanks for the prompt implementation and responsiveness!\n. @ashtuchkin I was thinking of a similar request today. Is there any way to know (after a call to encode) that the output is bogus? I think iconv is more agressive on throwing an error if the encoding choice does not make sense for the input. See https://github.com/bnoordhuis/node-iconv#dealing-with-untranslatable-characters\n. @ashtuchkin got it. But in the docs you say \"Untranslatable characters are set to \ufffd or ?\", so it might also be \"?\" which I cannot really check for because its a valid character.\n. Right, but \"?\" is ambiguos because it might also be a character used in the original text. I think raising some flag or error would make more sense to catch this case.\n. In our case, we use encode() to convert an existing file to another encoding. E.g. we have a source UTF-16 file that we want to save as DOS encoding. DOS can not present all encodings (Chinese et. al), so I would like to show an error to the user. Similar to how in Sublime Text you will not be able to save a file in an encoding that cannot represent all characters you have before encoding.\n\nRe UTF8: understood. i also think that UTF 8 will not be an issue in any case. This is more about encoding from a rich character set to one that cannot represent all characters.\n. Makes sense!\n. Hi Alexander, thanks for the prompt reply. I was thinking a little bit about what API could work and came to the conclusion that ideally this would all be handled from the encoding name itself. Imagine alongside utf8, utf16le and utf16be we would have utf8_bom, utf16le_bom and utf16be_bom. This would make the choice of BOM very explicit and would also bind it to the actual encoding. In other words, it would not make sense to have an option to set the bom for an encoding that does not know any BOM (e.g. cp1252).\n\nI am currently only using the encode/decode API where imho for encode() this would work nicely:\n- encode() would add the BOM if the encoding indicates so\n- encode() would remove any BOM if the encoding does not indicate to add a BOM\n- encode() would remove any wrong BOMs if a node buffer is passed in in both cases\n\nFor decode() its a bit weird though, I have to say. I think decode() should _always_ remove the BOM from the string returned. It could still treat utf8_bom, utf16be_bom and utf16le_bom as utf8, utf16be and utf16le.\n. Basically I would argue that a string should never ever include a BOM because I cannot think of a use case where you would want the BOM in the string. So, decode() looks at BOMs and removes them because it returns a string.\n\nEncode should add the BOM if specified (either by option or encoding name).\n\nIn many cases you would want a BOM for UTF16LE and UTF16BE anyways. Because tools cannot know which encoding to use by looking at the bytes. I believe UTF16 without BOM does not make a lot of sense.\n\nI am working for Visual Studio Code (https://code.visualstudio.com/) and iconv-lite is being used for all encoding matters.\n. Nice and I will be happy to test it and give feedback!\n. @ashtuchkin great, I think the API is very good. I will give it a try next week and report back how it works.\n. @ashtuchkin added to our editor and things are smooth so far. thanks for the prompt implementation and responsiveness!\n. ",
    "sergmakov": "Thanks, it helps.\n. Thanks, it helps.\n. ",
    "YuhangGe": "Chrome and other browsers encode this char to a451 when the charset of html page is \"big5\". So, when I write a tool to crawl content of an Hong Kong website, I can't get correct response from the server if I post A2CC to the server.\n. Chrome and other browsers encode this char to a451 when the charset of html page is \"big5\". So, when I write a tool to crawl content of an Hong Kong website, I can't get correct response from the server if I post A2CC to the server.\n. ",
    "jonathanong": "no idea what's going on with those errors!\n. no idea what's going on with those errors!\n. ",
    "dcposch": "Wow, that was fast! :+1: \n. Wow, that was fast! :+1: \n. ",
    "jcppman": "Thanks @ashtuchkin , I figured it out when I was trying to create a minimal project to reproduce this problem. The solution is simple: add `<meta charset=\"UTF-8\">` to header to avoid those encoding tables to be screwed.\n\nBefore setting charset explicitly, the `mappingTable` inside `DBCSCodec` constructor looks like: \n\n![image](https://cloud.githubusercontent.com/assets/5248918/7214886/199bb64e-e5f2-11e4-8c20-5544d6e8275d.png)\n\nAfter:\n\n![image](https://cloud.githubusercontent.com/assets/5248918/7214896/5c9594a6-e5f2-11e4-9389-0c5c4e6610d0.png)\n\nOne thing I've no idea why is, the chars in those json tables will only be broken in some strange condition, in order to reproduce the problem, I have to be:\n- inside a Chrome App\n- use AngularJS and enable [ng-csp](https://docs.angularjs.org/api/ng/directive/ngCsp) mode\n- inside html, use mustache expression to render some variable `MyValue = {{ myvalue }}` (this step is neccessary, the variable can be anything inside the scope)\n\n**Thank you for creating this awesome module BTW!**\n. Thanks @ashtuchkin , I figured it out when I was trying to create a minimal project to reproduce this problem. The solution is simple: add `<meta charset=\"UTF-8\">` to header to avoid those encoding tables to be screwed.\n\nBefore setting charset explicitly, the `mappingTable` inside `DBCSCodec` constructor looks like: \n\n![image](https://cloud.githubusercontent.com/assets/5248918/7214886/199bb64e-e5f2-11e4-8c20-5544d6e8275d.png)\n\nAfter:\n\n![image](https://cloud.githubusercontent.com/assets/5248918/7214896/5c9594a6-e5f2-11e4-9389-0c5c4e6610d0.png)\n\nOne thing I've no idea why is, the chars in those json tables will only be broken in some strange condition, in order to reproduce the problem, I have to be:\n- inside a Chrome App\n- use AngularJS and enable [ng-csp](https://docs.angularjs.org/api/ng/directive/ngCsp) mode\n- inside html, use mustache expression to render some variable `MyValue = {{ myvalue }}` (this step is neccessary, the variable can be anything inside the scope)\n\n**Thank you for creating this awesome module BTW!**\n. ",
    "Messilimeng": "How do I decode chinese?\n. How do I decode chinese?\n. ",
    "Summerlve": "yes, it's the problem about network , sometime the network is bad , so  got the null buffer. thx.\n. yes, it's the problem about network , sometime the network is bad , so  got the null buffer. thx.\n. ",
    "kidwm": "Thanks, I'll try to make it out.\n. Thanks, I'll try to make it out.\n. ",
    "ghost": "The problem is still given, Windows 10 (updated from Windows 7), Node v0.12.7 x64. Also I tried Node versions 0.12.4, 0.11.13, both x86 and x64.\nOn a notebook I too wasn't able to reproduce the raised behaviour. I attempted to run the decoding with Node v012.7 x64 on fresh installed operating systems Ubuntu 14.04.3 and Windows 10.\n\nDo you have any other hints to look for?\n. A good hint to mention line-endings to cause additional trouble. However, they are just involved as they increased file-size by another character per line-break. On my Windows machine with option `autocrlf true`, _Git for Windows_ checks out Windows-style `CRLF` and commits Unix-style `LF` line-endings.\n\nWith an increased file-size [the issue is reproducible on machines treating line-endings Unix-style](/bit-seq/iconv-lite-mcve/tree/ac04865b8b787954a8dd660d4d435c3d32ba99e7).\nI added lines to the example files to again have [`too-large.html`](/bit-seq/iconv-lite-mcve/blob/ac04865b8b787954a8dd660d4d435c3d32ba99e7/too-large/too-large.html) (508kb) causing problems, whereby file [`large.html`](/bit-seq/iconv-lite-mcve/blob/ac04865b8b787954a8dd660d4d435c3d32ba99e7/large/large.html) (504kb) decodes and encodes correctly. There's some debug output to generate files highlighting the content of every step and the according line-endings. On a unix or ios system there won't be any `CRLF`s on a checkout, however, by unzipping the [archive](/bit-seq/iconv-lite-mcve/blob/ac04865b8b787954a8dd660d4d435c3d32ba99e7/iconv-lite-mcve.zip) the first files of the chain should contain `CRLF`s. Calling the function `eolUnix` forces `LF` line-endings even for `CRLF` files.\n\nNode v0.12.7 still shows the affected behaviour. The result is correct using io.js v3.0.0 and node v4.0.0.\n. Thank you for your effort!\n. The problem is still given, Windows 10 (updated from Windows 7), Node v0.12.7 x64. Also I tried Node versions 0.12.4, 0.11.13, both x86 and x64.\nOn a notebook I too wasn't able to reproduce the raised behaviour. I attempted to run the decoding with Node v012.7 x64 on fresh installed operating systems Ubuntu 14.04.3 and Windows 10.\n\nDo you have any other hints to look for?\n. A good hint to mention line-endings to cause additional trouble. However, they are just involved as they increased file-size by another character per line-break. On my Windows machine with option `autocrlf true`, _Git for Windows_ checks out Windows-style `CRLF` and commits Unix-style `LF` line-endings.\n\nWith an increased file-size [the issue is reproducible on machines treating line-endings Unix-style](/bit-seq/iconv-lite-mcve/tree/ac04865b8b787954a8dd660d4d435c3d32ba99e7).\nI added lines to the example files to again have [`too-large.html`](/bit-seq/iconv-lite-mcve/blob/ac04865b8b787954a8dd660d4d435c3d32ba99e7/too-large/too-large.html) (508kb) causing problems, whereby file [`large.html`](/bit-seq/iconv-lite-mcve/blob/ac04865b8b787954a8dd660d4d435c3d32ba99e7/large/large.html) (504kb) decodes and encodes correctly. There's some debug output to generate files highlighting the content of every step and the according line-endings. On a unix or ios system there won't be any `CRLF`s on a checkout, however, by unzipping the [archive](/bit-seq/iconv-lite-mcve/blob/ac04865b8b787954a8dd660d4d435c3d32ba99e7/iconv-lite-mcve.zip) the first files of the chain should contain `CRLF`s. Calling the function `eolUnix` forces `LF` line-endings even for `CRLF` files.\n\nNode v0.12.7 still shows the affected behaviour. The result is correct using io.js v3.0.0 and node v4.0.0.\n. Thank you for your effort!\n. ",
    "bnoordhuis": "See my comment [here](https://github.com/bnoordhuis/node-iconv/issues/132#issuecomment-139489356): the problem is with the default C++ compiler, it's too old to understand C++11.\n. Yes, that was removed in nodejs/node@70d1f32 (\"deps: update v8 to 4.4.63.9\"):\n\n> Notable backwards incompatible changes are the removal of the smalloc module and dropped support for CESU-8 decoding.  CESU-8 support can be brought back if necessary by doing UTF-8 decoding ourselves.\n\nLong story short: V8 string functions no longer accept CESU-8 as input.  Node would have to implement its own UTF-8-to-UTF-16 encoder.\n. See my comment [here](https://github.com/bnoordhuis/node-iconv/issues/132#issuecomment-139489356): the problem is with the default C++ compiler, it's too old to understand C++11.\n. Yes, that was removed in nodejs/node@70d1f32 (\"deps: update v8 to 4.4.63.9\"):\n\n> Notable backwards incompatible changes are the removal of the smalloc module and dropped support for CESU-8 decoding.  CESU-8 support can be brought back if necessary by doing UTF-8 decoding ourselves.\n\nLong story short: V8 string functions no longer accept CESU-8 as input.  Node would have to implement its own UTF-8-to-UTF-16 encoder.\n. ",
    "dominiklessel": "Ah okay.\n\nI tried a streaming conversion of S3 objects (CSV files) and the AWS.S3.upload() method, which accepts a stream object, is blowing up, when used with iconv-lite. After forking and editing streams.js#L79 to accept `encoding: null` it worked as expected.\n\n```\nTypeError: Object foo|bar|baz has no method 'copy'\nat Function.Buffer.concat (buffer.js:499:9)\nat ManagedUpload.fillStream (/var/task/node_modules/aws-sdk/lib/s3/managed_upload.js:386:21)\nat IconvLiteDecoderStream.<anonymous> (/var/task/node_modules/aws-sdk/lib/s3/managed_upload.js:169:28)\nat IconvLiteDecoderStream.emit (events.js:117:20)\nat _stream_readable.js:944:16\nat process._tickDomainCallback (node.js:486:13)\n```\n. Thank you for your clarification! Maybe this should be added to the readme :+1: \n. Ah okay.\n\nI tried a streaming conversion of S3 objects (CSV files) and the AWS.S3.upload() method, which accepts a stream object, is blowing up, when used with iconv-lite. After forking and editing streams.js#L79 to accept `encoding: null` it worked as expected.\n\n```\nTypeError: Object foo|bar|baz has no method 'copy'\nat Function.Buffer.concat (buffer.js:499:9)\nat ManagedUpload.fillStream (/var/task/node_modules/aws-sdk/lib/s3/managed_upload.js:386:21)\nat IconvLiteDecoderStream.<anonymous> (/var/task/node_modules/aws-sdk/lib/s3/managed_upload.js:169:28)\nat IconvLiteDecoderStream.emit (events.js:117:20)\nat _stream_readable.js:944:16\nat process._tickDomainCallback (node.js:486:13)\n```\n. Thank you for your clarification! Maybe this should be added to the readme :+1: \n. ",
    "devin122": "There is some problems here with some of the control mappings. The problem arises because EBCDIC has a Carriage Return, New Line, *and* Line Feed.  The problem with these mappings  is that control characters in EBCDIC which do not translate have been given arbitrary unicode values starting at 0x80. This includes the NL character (0x20 in EBCDIC), which is assigned U+0080. On the systems I've touched the EBCDIC NL character is used in place of the LF character for marking EOL. Im not really sure how many programs handle U+0085 properly. The other side is, when converting the other direction, with LF being the usual line terminator. means it gets converted to EBCDIC LF (0x25).  I need to double check, but on the EBCDIC machines I've had access to, they do not like this at all. They want NL line endings. . There is some problems here with some of the control mappings. The problem arises because EBCDIC has a Carriage Return, New Line, *and* Line Feed.  The problem with these mappings  is that control characters in EBCDIC which do not translate have been given arbitrary unicode values starting at 0x80. This includes the NL character (0x20 in EBCDIC), which is assigned U+0080. On the systems I've touched the EBCDIC NL character is used in place of the LF character for marking EOL. Im not really sure how many programs handle U+0085 properly. The other side is, when converting the other direction, with LF being the usual line terminator. means it gets converted to EBCDIC LF (0x25).  I need to double check, but on the EBCDIC machines I've had access to, they do not like this at all. They want NL line endings. . ",
    "danielgindi": "Thanks for your quick reply!\n\nI have already created a module that wraps iconv-lite and jschardet with a stream that autodetects and falls back.\n\nhttps://github.com/danielgindi/node-autodetect-decoder-stream\n. That's nice :)\nIt actually tries to detect any encoding using `jschardet` so BOM is the least of it\n. Thanks for your quick reply!\n\nI have already created a module that wraps iconv-lite and jschardet with a stream that autodetects and falls back.\n\nhttps://github.com/danielgindi/node-autodetect-decoder-stream\n. That's nice :)\nIt actually tries to detect any encoding using `jschardet` so BOM is the least of it\n. ",
    "lessfish": "thanks, it really works well ! Thanks again !\n. thanks, it really works well ! Thanks again !\n. ",
    "shortRound1911": "I just encountered this in an expressjs application after upgrading from node 4.2 to 6.6.0. After upgrading node, I removed my node_modules folder and ran a fresh 'npm install'. First startup seemed healthy until posting a form to the server where I saw this issue. As mentioned by @leanderlee restarting the service seemed to fix it.\n. I just encountered this in an expressjs application after upgrading from node 4.2 to 6.6.0. After upgrading node, I removed my node_modules folder and ran a fresh 'npm install'. First startup seemed healthy until posting a form to the server where I saw this issue. As mentioned by @leanderlee restarting the service seemed to fix it.\n. ",
    "Marak": "Issue is due to changing the current working directory of node.js process after process starts and then you attempt to perform method using `iconv-lite`. The lazy require breaks since it's no longer in the same relative path. Same if `node_modules` somehow disappears after process start.\r\n\r\nI think I found the solution.\r\n\r\nTry changing: \r\n\r\n` iconv.encodings = require(\"../encodings\");`\r\n\r\nto\r\n\r\n` iconv.encodings = require(\"iconv-lite/encodings\");`\r\n\r\n\r\nhttps://github.com/ashtuchkin/iconv-lite/blob/master/lib/index.js#L61\r\n. @dbussert Did you see my comment?. @ashtuchkin -\r\n\r\nThis isn't something which requires you setup a reproducible environment to isolate or fix the bug.\r\n\r\nIt seems to be simple. Looking at https://github.com/ashtuchkin/iconv-lite/blob/master/lib/index.js#L61 , we can see that we have a \"lazy require\" statement.\r\n\r\nIn other words, we have a require statement which isn't being executed until `getCodec` function is executed. This is not a bug in `require()`. There are many reasons and conditions which could cause a lazy require statement to fail.\r\n\r\nIn the case I hit ( a production service serving millions of requests ), the Node.js process was being `chroot` to a new directory after the process started. This means that the relative path of `../encodings` is no longer valid, hence the errors. Generally, it's not safe to do a lazy require of modules.\r\n\r\nIs there a reason that `../encodings.js` file is being loaded lazily in `getCodec`? Could it be required outside the function scope?\r\n\r\nAlso, you could probably replicate the bug using https://www.npmjs.com/package/chroot library.\r\n. @ashtuchkin -\r\n\r\nLazy loading of Node.js modules is not a best practice and can cause several issues. You should understand that `require` is a synchronous and blocking statement. The same 100ms you are saving on initial load time will now be 100ms of blocked process time when `getCodec` is first called. I could go on and on. I won't. You are wrong, sorry.\r\n\r\nRegardless, I understand the need for optimization on start time. It's a trade-off you have to make. Based on what I see here, the majority of users will never experience this require failing issue and will benefit from the optimized start time. HOWEVER, some users are experiencing this issue and I'm telling you exactly why it's happening.\r\n\r\nYou should either figure out a fix, or tell us you will not fix it. I already proposed the solution we used in production. What none of us need here is reasons why you don't think this is broken.\r\n\r\nCheers.. Have you considered what is going to happen if someone is running a webserver with 1000s of requests per second and then `getCodec` method is called for the first time? I believe all requests will block until `encodings` file completes load.\r\n\r\nI hit this problem ( of not being able to find encodings file ), found the solution, and posted it here. You want to lazy require a huge file with a relative path, go for it. Every-time anyone hits any environment issue with `express` where `node_modules` is not where it's suppose to be after process start is going to google and find this Github Issue. It's why you are seeing a variety of issues reported here with seemingly different sources and behaviors...\r\n. Issue is due to changing the current working directory of node.js process after process starts and then you attempt to perform method using `iconv-lite`. The lazy require breaks since it's no longer in the same relative path. Same if `node_modules` somehow disappears after process start.\r\n\r\nI think I found the solution.\r\n\r\nTry changing: \r\n\r\n` iconv.encodings = require(\"../encodings\");`\r\n\r\nto\r\n\r\n` iconv.encodings = require(\"iconv-lite/encodings\");`\r\n\r\n\r\nhttps://github.com/ashtuchkin/iconv-lite/blob/master/lib/index.js#L61\r\n. @dbussert Did you see my comment?. @ashtuchkin -\r\n\r\nThis isn't something which requires you setup a reproducible environment to isolate or fix the bug.\r\n\r\nIt seems to be simple. Looking at https://github.com/ashtuchkin/iconv-lite/blob/master/lib/index.js#L61 , we can see that we have a \"lazy require\" statement.\r\n\r\nIn other words, we have a require statement which isn't being executed until `getCodec` function is executed. This is not a bug in `require()`. There are many reasons and conditions which could cause a lazy require statement to fail.\r\n\r\nIn the case I hit ( a production service serving millions of requests ), the Node.js process was being `chroot` to a new directory after the process started. This means that the relative path of `../encodings` is no longer valid, hence the errors. Generally, it's not safe to do a lazy require of modules.\r\n\r\nIs there a reason that `../encodings.js` file is being loaded lazily in `getCodec`? Could it be required outside the function scope?\r\n\r\nAlso, you could probably replicate the bug using https://www.npmjs.com/package/chroot library.\r\n. @ashtuchkin -\r\n\r\nLazy loading of Node.js modules is not a best practice and can cause several issues. You should understand that `require` is a synchronous and blocking statement. The same 100ms you are saving on initial load time will now be 100ms of blocked process time when `getCodec` is first called. I could go on and on. I won't. You are wrong, sorry.\r\n\r\nRegardless, I understand the need for optimization on start time. It's a trade-off you have to make. Based on what I see here, the majority of users will never experience this require failing issue and will benefit from the optimized start time. HOWEVER, some users are experiencing this issue and I'm telling you exactly why it's happening.\r\n\r\nYou should either figure out a fix, or tell us you will not fix it. I already proposed the solution we used in production. What none of us need here is reasons why you don't think this is broken.\r\n\r\nCheers.. Have you considered what is going to happen if someone is running a webserver with 1000s of requests per second and then `getCodec` method is called for the first time? I believe all requests will block until `encodings` file completes load.\r\n\r\nI hit this problem ( of not being able to find encodings file ), found the solution, and posted it here. You want to lazy require a huge file with a relative path, go for it. Every-time anyone hits any environment issue with `express` where `node_modules` is not where it's suppose to be after process start is going to google and find this Github Issue. It's why you are seeing a variety of issues reported here with seemingly different sources and behaviors...\r\n. ",
    "dbussert": "I can reliably recreate this error in our test environment, we deploy node and mode_module updates packaged into an rpm, and in our latest version, our code is crashing on body parsing due to its use of this library.  restarting our node service fixes this issue, it only happens right after we deploy the rpm.. I can reliably recreate this error in our test environment, we deploy node and mode_module updates packaged into an rpm, and in our latest version, our code is crashing on body parsing due to its use of this library.  restarting our node service fixes this issue, it only happens right after we deploy the rpm.. ",
    "jcreamer898": "The lazy loading also seems to be causing an issue in other libraries... https://github.com/facebook/jest/issues/2605. The lazy loading also seems to be causing an issue in other libraries... https://github.com/facebook/jest/issues/2605. ",
    "kleggas": "I got the same problem as @Marak is describing, and it does not solve if I restart. His fix worked for me.\r\nAnd I agree with @Marak. Instead of excusing this behaviour, treat it as the bug it is and fix, please.. I got the same problem as @Marak is describing, and it does not solve if I restart. His fix worked for me.\r\nAnd I agree with @Marak. Instead of excusing this behaviour, treat it as the bug it is and fix, please.. ",
    "vernondegoede": "I'm experiencing the same issue when executing a script within my sandboxed test environment (Jest) which uses `iconv-lite` as a dependency. \r\n\r\nWhen I'm not lazy-loading `encodings/index.js` (but instead requiring everything directory at the top of the module) everything works fine for me.\r\n\r\nThe initial reason for lazy-loading this module was because of speed issues, which were fixed in PR https://github.com/ashtuchkin/iconv-lite/pull/26.\r\n\r\nI ran this script twice (both using lazy-loading and without) and got the following results:\r\n\r\nWith lazy-loading:\r\n```\r\n\u276f node -e \"require('./lib/index')\"  0.18s user 0.02s system 98% cpu 0.204 total\r\n```\r\n\r\nWithout lazy-loading (changed `iconv.encodings = require('../encodings')` at the top of the file):\r\n```\r\n\u276f node -e \"require('./lib/index')\"  0.18s user 0.02s system 98% cpu 0.204 total\r\n```\r\n\r\nThe results seem to be exactly the same. I'm not sure if something changed within Node itself, or if I'm doing something wrong, but if this those stats are correct there would be no reason to lazy-load the encodings anyway. . I'm experiencing the same issue when executing a script within my sandboxed test environment (Jest) which uses `iconv-lite` as a dependency. \r\n\r\nWhen I'm not lazy-loading `encodings/index.js` (but instead requiring everything directory at the top of the module) everything works fine for me.\r\n\r\nThe initial reason for lazy-loading this module was because of speed issues, which were fixed in PR https://github.com/ashtuchkin/iconv-lite/pull/26.\r\n\r\nI ran this script twice (both using lazy-loading and without) and got the following results:\r\n\r\nWith lazy-loading:\r\n```\r\n\u276f node -e \"require('./lib/index')\"  0.18s user 0.02s system 98% cpu 0.204 total\r\n```\r\n\r\nWithout lazy-loading (changed `iconv.encodings = require('../encodings')` at the top of the file):\r\n```\r\n\u276f node -e \"require('./lib/index')\"  0.18s user 0.02s system 98% cpu 0.204 total\r\n```\r\n\r\nThe results seem to be exactly the same. I'm not sure if something changed within Node itself, or if I'm doing something wrong, but if this those stats are correct there would be no reason to lazy-load the encodings anyway. . ",
    "hujb2000": "I see , Thanks\n. I see , Thanks\n. ",
    "sidorares": "never mind, that was mysql issue ( 4-byte chars used in field name and mysql returned '3f203f203f203f203f203f203f203f203f203f203f203f203f203f203f20' and I thought results of `toString('utf8')` are incorrect but in fact it was actually '? ? ? ? ? ? ? ? ? ? ? ? ? ? ?' in that string )\n. my main issue is not solved, but at least I know now that I don't need iconv to convert from utf8mb4, it's just utf8 - so nothing to do with iconv-lite :)\n. missed `macroman`, it's supported indeed. How about `keybcs2` ?\nThey all must be old encodings but I'm not sure is the names are canonical and you might have them under different name\n. yes, it's probably too mysql specific to be part of iconv-lite.\nI think I'll convert xml to iconv encodings and distribute them with mysql2 package and lazy load when requested\n\nThanks for example!\n. `new Buffer('string', enc)` is deprecated - should be `Buffer.from('string', enc)` ( + [safe-buffer](https://github.com/feross/safe-buffer) polyfill ). I'm afraid only option is to pull safe-buffer content, it's quite small - https://github.com/feross/safe-buffer/blob/master/index.js. @chalker how likely is hard deprecation soon?. @chalker I had 3 bug reports next day after I accidentally released node-mysql2 with one of the functions using new API without shim, there are still plenty of pre node 4.5 users. never mind, that was mysql issue ( 4-byte chars used in field name and mysql returned '3f203f203f203f203f203f203f203f203f203f203f203f203f203f203f20' and I thought results of `toString('utf8')` are incorrect but in fact it was actually '? ? ? ? ? ? ? ? ? ? ? ? ? ? ?' in that string )\n. my main issue is not solved, but at least I know now that I don't need iconv to convert from utf8mb4, it's just utf8 - so nothing to do with iconv-lite :)\n. missed `macroman`, it's supported indeed. How about `keybcs2` ?\nThey all must be old encodings but I'm not sure is the names are canonical and you might have them under different name\n. yes, it's probably too mysql specific to be part of iconv-lite.\nI think I'll convert xml to iconv encodings and distribute them with mysql2 package and lazy load when requested\n\nThanks for example!\n. `new Buffer('string', enc)` is deprecated - should be `Buffer.from('string', enc)` ( + [safe-buffer](https://github.com/feross/safe-buffer) polyfill ). I'm afraid only option is to pull safe-buffer content, it's quite small - https://github.com/feross/safe-buffer/blob/master/index.js. @chalker how likely is hard deprecation soon?. @chalker I had 3 bug reports next day after I accidentally released node-mysql2 with one of the functions using new API without shim, there are still plenty of pre node 4.5 users. ",
    "robertkeizer": "Nope. Feel free to close and ignore.\n\nWas going through my own private deps and noticed it wasn't version locked in this project.\n\nAdmittedly didn't check how `async` was being used internal to iconv-lite.\n. Nope. Feel free to close and ignore.\n\nWas going through my own private deps and noticed it wasn't version locked in this project.\n\nAdmittedly didn't check how `async` was being used internal to iconv-lite.\n. ",
    "pluma": "Okay, strange. I was just able to verify that this actually works in Node.js 6 just fine. I was debugging buffer issues yesterday and was unable to write to buffers using the subscript operator but it must have been caused by something else. Sorry for the noise.\n. Okay, strange. I was just able to verify that this actually works in Node.js 6 just fine. I was debugging buffer issues yesterday and was unable to write to buffers using the subscript operator but it must have been caused by something else. Sorry for the noise.\n. ",
    "lucaswxp": "Oh I'm so sorry, I thought I was in the right place.\n\nClosing this.\n. Oh I'm so sorry, I thought I was in the right place.\n\nClosing this.\n. ",
    "domenic": "I already published the package with a pinned dependency version, so there's no urgency. Just, whenever you have time to publish, then I can update my package and all my consumers can be sure to get new bug fixes automatically.\n. I already published the package with a pinned dependency version, so there's no urgency. Just, whenever you have time to publish, then I can update my package and all my consumers can be sure to get new bug fixes automatically.\n. ",
    "larssn": "And it seems to work just fine. Even works with Rollup.\n\n```\nimport iconv from 'iconv-lite';\n\nexport class test {\n   constructor() {\n      let dec = iconv.decode( ... );\n   }\n}\n```\n\nIf its ok, I might pull readme.md and include this minor typescript example. Probably elaborate on it a bit.\n. @ashtuchkin @felixfbecker \r\nThis change has broken my Angular 2 app build process.\r\n\r\nReading up on whether triple slashes are deprecated didn't reveal anything:\r\nhttps://www.typescriptlang.org/docs/handbook/triple-slash-directives.html\r\n\r\n> The /// <reference path=\"...\" /> directive is the most common of this group. It serves as a declaration of dependency between files.\r\n\r\nI know we (in the typescript community, as of TS2.0) have switched to the @types/module format in package.conf, but this is different.\r\n\r\nThe typescript compiler (v2.0.10) no longer knows about the node built-ins, like Buffer, stream etc, and thus refuses to transpile, in my case.\r\n\r\n```\r\n/node_modules/iconv-lite/lib/index.d.ts, line: 6\r\n           Cannot find module 'stream'.\r\n```. But this gist of this change boils down to me having to add a bunch of extra definitions manually then. Can't see how that is supposed to be an improvement, legacy or not.. You sure thats also true for node internals? I didn't install the .d.ts files for those before.\r\n. No, the definitions weren't there before, I checked.\r\n\r\nLook, seeing as typescript runs in a node environment, it makes sense that it would know about the built-in node modules, IF told to look for them. I'm no expert on the latest changes in typescript and how definitions should and shouldn't look, because the docs are in constant flux; all I do know is this change broke our build, and its the only change to it.\r\n\r\nI'll fork this, and go back to using triple-slashes until its 100% clear what the correct approach is, going forward.\r\n\r\nAdding that single line fixes my problems, I've just tested it.\r\nI'm sorry it caused problems for you.. Well then, I can't explain why the triple-slash fixes my problem then.. I've added your requirements. AoT compiling still works.. Added your requests. Tested locally as working.. Yeah `Buffer` is a node built-in, no need to import.\r\n\r\nDoesn't seem like you are using typescript, and I have no experience with react, otherwise I'd recommend doing:\r\n\r\n`npm install @types/node`. Thanks for making a useful module!. Sounds like you need to save your js files with UTF8 encoding. See https://stackoverflow.com/a/40365121/536485 (requires VSCode). And it seems to work just fine. Even works with Rollup.\n\n```\nimport iconv from 'iconv-lite';\n\nexport class test {\n   constructor() {\n      let dec = iconv.decode( ... );\n   }\n}\n```\n\nIf its ok, I might pull readme.md and include this minor typescript example. Probably elaborate on it a bit.\n. @ashtuchkin @felixfbecker \r\nThis change has broken my Angular 2 app build process.\r\n\r\nReading up on whether triple slashes are deprecated didn't reveal anything:\r\nhttps://www.typescriptlang.org/docs/handbook/triple-slash-directives.html\r\n\r\n> The /// <reference path=\"...\" /> directive is the most common of this group. It serves as a declaration of dependency between files.\r\n\r\nI know we (in the typescript community, as of TS2.0) have switched to the @types/module format in package.conf, but this is different.\r\n\r\nThe typescript compiler (v2.0.10) no longer knows about the node built-ins, like Buffer, stream etc, and thus refuses to transpile, in my case.\r\n\r\n```\r\n/node_modules/iconv-lite/lib/index.d.ts, line: 6\r\n           Cannot find module 'stream'.\r\n```. But this gist of this change boils down to me having to add a bunch of extra definitions manually then. Can't see how that is supposed to be an improvement, legacy or not.. You sure thats also true for node internals? I didn't install the .d.ts files for those before.\r\n. No, the definitions weren't there before, I checked.\r\n\r\nLook, seeing as typescript runs in a node environment, it makes sense that it would know about the built-in node modules, IF told to look for them. I'm no expert on the latest changes in typescript and how definitions should and shouldn't look, because the docs are in constant flux; all I do know is this change broke our build, and its the only change to it.\r\n\r\nI'll fork this, and go back to using triple-slashes until its 100% clear what the correct approach is, going forward.\r\n\r\nAdding that single line fixes my problems, I've just tested it.\r\nI'm sorry it caused problems for you.. Well then, I can't explain why the triple-slash fixes my problem then.. I've added your requirements. AoT compiling still works.. Added your requests. Tested locally as working.. Yeah `Buffer` is a node built-in, no need to import.\r\n\r\nDoesn't seem like you are using typescript, and I have no experience with react, otherwise I'd recommend doing:\r\n\r\n`npm install @types/node`. Thanks for making a useful module!. Sounds like you need to save your js files with UTF8 encoding. See https://stackoverflow.com/a/40365121/536485 (requires VSCode). ",
    "mmouterde": "sorry. iconv-lite is not guilty.\n. sorry. iconv-lite is not guilty.\n. ",
    "shanavas786": "> That's funny, why doesn't it use regular rules, i.e. if \"files\" field is not given, then everything except the contents of .npmignore is included?\n\nThanks for suggestion, npm2deb needs improvements. We will consider adding this soon.\n. > That's funny, why doesn't it use regular rules, i.e. if \"files\" field is not given, then everything except the contents of .npmignore is included?\n\nThanks for suggestion, npm2deb needs improvements. We will consider adding this soon.\n. ",
    "felixfbecker": "Yes, they are correct otherwise. Removing the line fixes everything.\r\n\r\n(Sidenote: Please do not make the mistake of adding `@types/node` as a dependency - this mistake has been done a lot of times already and broke a lot of builds too. It's not the right solution). Fixed https://travis-ci.org/felixfbecker/vscode-php-debug/builds/177751114. @larssn I did not say it is deprecated, but legacy, because it is not needed anymore. TS picks up files automatically. Node is a global definition, it cannot get added to dependencies. You need to install it in your project manually, through whatever mechanism (like `typings` or `npm`) and ensure it is included in your project by tsconfig.json settings. Adding a triple slash reference is assuming a location for the node typings, which is wrong.. No, you had to remove the definitions before too. Just because there is a /// reference doesn't make the node typings magically install themselves. You still need to install them.. Then you were probably lucky that some other module added them as a dependency (even though it shouldn't). Definitions need to be installed. _There are no declarations bundled with TypeScript_.. To be honest, the whole change was bad. Bundled definitions should be written in ES6 module format, not with `declare module`.. Yes, they are correct otherwise. Removing the line fixes everything.\r\n\r\n(Sidenote: Please do not make the mistake of adding `@types/node` as a dependency - this mistake has been done a lot of times already and broke a lot of builds too. It's not the right solution). Fixed https://travis-ci.org/felixfbecker/vscode-php-debug/builds/177751114. @larssn I did not say it is deprecated, but legacy, because it is not needed anymore. TS picks up files automatically. Node is a global definition, it cannot get added to dependencies. You need to install it in your project manually, through whatever mechanism (like `typings` or `npm`) and ensure it is included in your project by tsconfig.json settings. Adding a triple slash reference is assuming a location for the node typings, which is wrong.. No, you had to remove the definitions before too. Just because there is a /// reference doesn't make the node typings magically install themselves. You still need to install them.. Then you were probably lucky that some other module added them as a dependency (even though it shouldn't). Definitions need to be installed. _There are no declarations bundled with TypeScript_.. To be honest, the whole change was bad. Bundled definitions should be written in ES6 module format, not with `declare module`.. ",
    "helloncanella": "Thank you. This approach works.. Thank you. This approach works.. ",
    "sulliwane": "Ok, so I just tried and using base64 works fine ! Yay\r\n\r\nMy js string has a of null charachter at the end (\\u000) like 20. These are present in the base64 string as well (AAAAAAAA....) so I guess my c++ base64 conversion function needs some reworking, until then I just remove them in JS space with ```string.replace(/\\0/g, '')```.\r\n\r\nMany thanks! iconv-lite is really great.... Ok, so I just tried and using base64 works fine ! Yay\r\n\r\nMy js string has a of null charachter at the end (\\u000) like 20. These are present in the base64 string as well (AAAAAAAA....) so I guess my c++ base64 conversion function needs some reworking, until then I just remove them in JS space with ```string.replace(/\\0/g, '')```.\r\n\r\nMany thanks! iconv-lite is really great.... ",
    "jnfsmile": "Can you explain how to configure that in Gruntfile? Couldn't find a reference to that.. Can you explain how to configure that in Gruntfile? Couldn't find a reference to that.. ",
    "DanielHuisman": "I'm running this in the browser using Browserify (and Babelify for ES6 support). \r\nApparently `Buffer.from('test')` returns a `Uint8Array`, found using `console.log(Buffer.from('test')[Symbol.toStringTag])`.\r\n\r\nI also looked at the code generated by Browserify and found it uses the [buffer module](https://github.com/feross/buffer), which according to their \"how it works\" section returns `Uint8Array` instances with a modified prototype to support all Buffer functions. So I guess this is the expected behaviour and also the reason the browser prints them as plain arrays.. Same result when using a string (`test`), it's encoded as `[63, 63, 63, 63]`. Thanks, adding `<meta charset=\"utf-8\">` fixed it. Weird that UTF-8 still isn't the default encoding. Might be useful to include this somewhere in the documentation for browser support.. I'm running this in the browser using Browserify (and Babelify for ES6 support). \r\nApparently `Buffer.from('test')` returns a `Uint8Array`, found using `console.log(Buffer.from('test')[Symbol.toStringTag])`.\r\n\r\nI also looked at the code generated by Browserify and found it uses the [buffer module](https://github.com/feross/buffer), which according to their \"how it works\" section returns `Uint8Array` instances with a modified prototype to support all Buffer functions. So I guess this is the expected behaviour and also the reason the browser prints them as plain arrays.. Same result when using a string (`test`), it's encoded as `[63, 63, 63, 63]`. Thanks, adding `<meta charset=\"utf-8\">` fixed it. Weird that UTF-8 still isn't the default encoding. Might be useful to include this somewhere in the documentation for browser support.. ",
    "PaulBGD": "Yep, there's an issue for it already: https://youtrack.jetbrains.com/issue/IDEA-152656. Yep, there's an issue for it already: https://youtrack.jetbrains.com/issue/IDEA-152656. ",
    "louise-r-blue": "Here is the post request code that I forgot to include earlier. I also realise iconv-lite may not work with my Node version (so I may upgrade) but any help would do.\r\n![post_req](https://cloud.githubusercontent.com/assets/15900463/22055338/92923cbe-ddbf-11e6-9418-d2088dbc419d.png)\r\n\r\n. Oh gosh that is it! Thank you ashtuchkin  :). Here is the post request code that I forgot to include earlier. I also realise iconv-lite may not work with my Node version (so I may upgrade) but any help would do.\r\n![post_req](https://cloud.githubusercontent.com/assets/15900463/22055338/92923cbe-ddbf-11e6-9418-d2088dbc419d.png)\r\n\r\n. Oh gosh that is it! Thank you ashtuchkin  :). ",
    "ikedas": "Hi ashtuchkin,\r\n\r\nraccy is right.  U+FF5E is a mapping according to Microsoft Code Page (cp932) which is not authorized by public standards body.  U+301C is the mapping according to Japan Industrial Standard (JIS X 0208).\r\n\r\n- Shift_JIS would be better to conform to JIS X 0208: Detailed encoding scheme is defined in Annex 1 of this standard.\r\n- EUC-JP would be better to conform to [eucjp-ascii](http://web.archive.org/web/19990203115405/http://www.opengroup.or.jp/jvc/cde/ucs-conv-e.html) defined by OSF/JVC.  Though it is not a national standard, it is identical to x-eucjp-open-19970715-ascii listed in [XML Japanese Profile](https://www.w3.org/TR/2000/NOTE-japanese-xml-20000414/).\r\n\r\nEven more characters are also given imcompatible mappings over two mappings above.  It is quite a mess for japanese users.  If you prefer, I'd like to provide changes.\r\n. takahashim's suggestion looks reasonable for me.  Current index-jis0208.txt would be renamed to index-windows31j.txt or similar.  Appropriate names would be assigned to appropriate mappings.\r\n\r\n(Problem on indices beyond 8836 (94 \u00d7 94) would be separate matter.  They are simply beyond the domain of definition for CCS by ISO/IEC, i.e. domain of extension by vendors.)\r\n\r\nOn ambiguity, several implimentations adds one-way (Unicode to legacy) mappings for non-standard encoding, e.g. U+2015 HORIZONTAL BAR to \\xA1\\xBD EM DASH, therefore roundtrip conversion between cp932-based and JIS-based mappings is more or less satisfied.\r\n\r\n---\r\n(Addition) As takahashim pointed out, mapping defined by JIS X 0213 is rarely used in practice.  It's an extension to JIS X 0208 but not compatible.\r\n. Mappings on unicode.org may not be compatible to other implementation, e.g. 0x815C / 0x213D is mapped to U+2015 HORIZONTAL BAR.  Personally I believe mapping defined by JIS (it is only mapping publicly authorized by ISO/IEC 10646) should be referred, however, investigation on existing implimentations is useful.\r\n\r\nI suggest that at least 10 mappings mentioned above would be checked (both on forward and reverse mappings) to compare implementations.  Additionally, duplicate mappings such as U+2116 NUMERO SIGN (both JIS X 0208 and JIS X 0212 have it) would be considered.. I compiled tables to help comparing implementations.\r\n\r\n* **IMO**, \u201cCanonic\u201d in the tables below would provide bi-directional conversion (from and to Unicode), while others would provide only reverse (from Unicode) or forward (to Unicode) conversion.\r\n\r\n* Note that tables below focuses on EUC-JP implementations.  They are not necessarily applicable to Shift_JIS / cp932.\r\n\r\nFollowing table shows vendor-dependent mappings.  That is, beyond implementations, single code point on legacy character set can be mapped to multiple Unicode characters.\r\n\r\n| Canonic | Microsoft | JIS X 0208 Annex 5 | Code Point |\r\n| ------- | ------ | ------ | ---- |\r\n| U+203E  | U+FFE3 | U+FFE3 | A1B1 |\r\n| U+2014  | U+2015 |        | A1BD |\r\n| U+301C  | U+FF5E |        | A1C1 |\r\n| U+2016  | U+2225 |        | A1C2 |\r\n| U+2212  | U+FF0D |        | A1DD |\r\n| U+00A5  | U+FFE5 | U+FFE5 | A1EF |\r\n| U+00A2  | U+FFE0 |        | A1F1 |\r\n| U+00A3  | U+FFE1 |        | A1F2 |\r\n| U+00AC  | U+FFE2 |        | A2CC |\r\n| U+00A6  | U+FFE4 |        | 8FA2C3 |\r\n\r\nFollowing table shows non-injective mappings.  That is, beyond implementations, multiple code points on legacy character set will be mapped to single Unicode character.\r\n\r\n| Canonic | JIS X 0212 | IBM/NEC ext. | Unicode |\r\n| ------ | ------ | ------ | ------ |\r\n| ADE2   | 8FA2F1 | 8FF4AC | U+2116 |\r\n| ADE4   |        | 8FF4AD | U+2121 |\r\n| ADB5   |        | 8FF3FD | U+2160 |\r\n| ADB6   |        | 8FF3FE | U+2161 |\r\n| ADB7   |        | 8FF4A1 | U+2162 |\r\n| ADB8   |        | 8FF4A2 | U+2163 |\r\n| ADB9   |        | 8FF4A3 | U+2164 |\r\n| ADBA   |        | 8FF4A4 | U+2165 |\r\n| ADBB   |        | 8FF4A5 | U+2166 |\r\n| ADBC   |        | 8FF4A6 | U+2167 |\r\n| ADBD   |        | 8FF4A7 | U+2168 |\r\n| ADBE   |        | 8FF4A8 | U+2169 |\r\n| A2E5   |        | ADF5   | U+221A |\r\n| A2DC   |        | ADF7   | U+2220 |\r\n| A2C1   |        | ADFB   | U+2229 |\r\n| A2C0   |        | ADFC   | U+222A |\r\n| A2E9   |        | ADF2   | U+222B |\r\n| A2E8   |        | ADFA   | U+2235 |\r\n| A2E2   |        | ADF0   | U+2252 |\r\n| A2E1   |        | ADF1   | U+2261 |\r\n| A2DD   |        | ADF6   | U+22A5 |\r\n| ADEA   |        | 8FF4AB | U+3231 |\r\n\r\n* Note: ADxx, 8FF3xx and 8FF4xx are IBM/NEC extensions.\r\n* Current index-jis0208.txt by WHATWG lacks mapping for 8FF3xx and 8FF4xx defined by eucjp-open.\r\n\r\n\r\n. Hi ashtuchkin,\r\n\r\nraccy is right.  U+FF5E is a mapping according to Microsoft Code Page (cp932) which is not authorized by public standards body.  U+301C is the mapping according to Japan Industrial Standard (JIS X 0208).\r\n\r\n- Shift_JIS would be better to conform to JIS X 0208: Detailed encoding scheme is defined in Annex 1 of this standard.\r\n- EUC-JP would be better to conform to [eucjp-ascii](http://web.archive.org/web/19990203115405/http://www.opengroup.or.jp/jvc/cde/ucs-conv-e.html) defined by OSF/JVC.  Though it is not a national standard, it is identical to x-eucjp-open-19970715-ascii listed in [XML Japanese Profile](https://www.w3.org/TR/2000/NOTE-japanese-xml-20000414/).\r\n\r\nEven more characters are also given imcompatible mappings over two mappings above.  It is quite a mess for japanese users.  If you prefer, I'd like to provide changes.\r\n. takahashim's suggestion looks reasonable for me.  Current index-jis0208.txt would be renamed to index-windows31j.txt or similar.  Appropriate names would be assigned to appropriate mappings.\r\n\r\n(Problem on indices beyond 8836 (94 \u00d7 94) would be separate matter.  They are simply beyond the domain of definition for CCS by ISO/IEC, i.e. domain of extension by vendors.)\r\n\r\nOn ambiguity, several implimentations adds one-way (Unicode to legacy) mappings for non-standard encoding, e.g. U+2015 HORIZONTAL BAR to \\xA1\\xBD EM DASH, therefore roundtrip conversion between cp932-based and JIS-based mappings is more or less satisfied.\r\n\r\n---\r\n(Addition) As takahashim pointed out, mapping defined by JIS X 0213 is rarely used in practice.  It's an extension to JIS X 0208 but not compatible.\r\n. Mappings on unicode.org may not be compatible to other implementation, e.g. 0x815C / 0x213D is mapped to U+2015 HORIZONTAL BAR.  Personally I believe mapping defined by JIS (it is only mapping publicly authorized by ISO/IEC 10646) should be referred, however, investigation on existing implimentations is useful.\r\n\r\nI suggest that at least 10 mappings mentioned above would be checked (both on forward and reverse mappings) to compare implementations.  Additionally, duplicate mappings such as U+2116 NUMERO SIGN (both JIS X 0208 and JIS X 0212 have it) would be considered.. I compiled tables to help comparing implementations.\r\n\r\n* **IMO**, \u201cCanonic\u201d in the tables below would provide bi-directional conversion (from and to Unicode), while others would provide only reverse (from Unicode) or forward (to Unicode) conversion.\r\n\r\n* Note that tables below focuses on EUC-JP implementations.  They are not necessarily applicable to Shift_JIS / cp932.\r\n\r\nFollowing table shows vendor-dependent mappings.  That is, beyond implementations, single code point on legacy character set can be mapped to multiple Unicode characters.\r\n\r\n| Canonic | Microsoft | JIS X 0208 Annex 5 | Code Point |\r\n| ------- | ------ | ------ | ---- |\r\n| U+203E  | U+FFE3 | U+FFE3 | A1B1 |\r\n| U+2014  | U+2015 |        | A1BD |\r\n| U+301C  | U+FF5E |        | A1C1 |\r\n| U+2016  | U+2225 |        | A1C2 |\r\n| U+2212  | U+FF0D |        | A1DD |\r\n| U+00A5  | U+FFE5 | U+FFE5 | A1EF |\r\n| U+00A2  | U+FFE0 |        | A1F1 |\r\n| U+00A3  | U+FFE1 |        | A1F2 |\r\n| U+00AC  | U+FFE2 |        | A2CC |\r\n| U+00A6  | U+FFE4 |        | 8FA2C3 |\r\n\r\nFollowing table shows non-injective mappings.  That is, beyond implementations, multiple code points on legacy character set will be mapped to single Unicode character.\r\n\r\n| Canonic | JIS X 0212 | IBM/NEC ext. | Unicode |\r\n| ------ | ------ | ------ | ------ |\r\n| ADE2   | 8FA2F1 | 8FF4AC | U+2116 |\r\n| ADE4   |        | 8FF4AD | U+2121 |\r\n| ADB5   |        | 8FF3FD | U+2160 |\r\n| ADB6   |        | 8FF3FE | U+2161 |\r\n| ADB7   |        | 8FF4A1 | U+2162 |\r\n| ADB8   |        | 8FF4A2 | U+2163 |\r\n| ADB9   |        | 8FF4A3 | U+2164 |\r\n| ADBA   |        | 8FF4A4 | U+2165 |\r\n| ADBB   |        | 8FF4A5 | U+2166 |\r\n| ADBC   |        | 8FF4A6 | U+2167 |\r\n| ADBD   |        | 8FF4A7 | U+2168 |\r\n| ADBE   |        | 8FF4A8 | U+2169 |\r\n| A2E5   |        | ADF5   | U+221A |\r\n| A2DC   |        | ADF7   | U+2220 |\r\n| A2C1   |        | ADFB   | U+2229 |\r\n| A2C0   |        | ADFC   | U+222A |\r\n| A2E9   |        | ADF2   | U+222B |\r\n| A2E8   |        | ADFA   | U+2235 |\r\n| A2E2   |        | ADF0   | U+2252 |\r\n| A2E1   |        | ADF1   | U+2261 |\r\n| A2DD   |        | ADF6   | U+22A5 |\r\n| ADEA   |        | 8FF4AB | U+3231 |\r\n\r\n* Note: ADxx, 8FF3xx and 8FF4xx are IBM/NEC extensions.\r\n* Current index-jis0208.txt by WHATWG lacks mapping for 8FF3xx and 8FF4xx defined by eucjp-open.\r\n\r\n\r\n. ",
    "raccy": "Thank you for your reply, ashtuchkin.\r\n\r\nI found these files.\r\n\r\n* ftp://ftp.unicode.org/Public/MAPPINGS/OBSOLETE/EASTASIA/JIS/JIS0208.TXT\r\n* ftp://ftp.unicode.org/Public/MAPPINGS/OBSOLETE/EASTASIA/JIS/JIS0212.TXT\r\n\r\nThese are in the OBSOLETE directory, but libiconv probably used these maps.\r\n\r\nI don't object to iconv-lite being based on WHATWG Encoding Standard, and I think that is a good policy. But I think that there are two problems.\r\n\r\n1. iconv-lite is different from the behavior of node-iconv. This confuse us. (See the code below, and run)\r\n2. JIS X 0208 takes precedence over JIS X 0212 (beginnig with 8F), and depending on the implementation, JIS X 0212 may not be supported by EUC-JP.\r\n\r\nThis code is check enocde/decode with icnov-lite and node-iconv.\r\n\r\n```\r\nconst Iconv = require('iconv').Iconv;\r\nconst lite = require('iconv-lite');\r\n\r\nconst unicodePoint = s => 'U+' + s.charCodeAt(0).toString(16).toUpperCase();\r\nconst bufferString = buf => {\r\n  let s = '[ '\r\n  for (const b of buf) {\r\n    s += b.toString(16).toUpperCase();\r\n    s += ' ';\r\n  }\r\n  s += ']';\r\n  return s;\r\n};\r\nconst up = (m, s) => console.log(m + ' : ' + unicodePoint(s));\r\nconst bp = (m, b) => console.log(m + ' -> ' + bufferString(b));\r\nconst ubp = (m, b) => up(m, b.toString('utf8'));\r\n\r\nconst waveDash = '\\u301C';\r\nconst fwTilde = '\\uFF5E';\r\n\r\nconst eucjp_A1C1 = new Buffer([0xA1, 0xC1]); // JISX0208 1-33 on EUC-JP\r\nconst eucjp_8FA2B7 = new Buffer([0x8F, 0xA2, 0xB7]); // JISX0212 1-23 on EUC-JP\r\nconst sjis_8160 = new Buffer([0x81, 0x60]); // JISX0208 1-33 on Shift_JIS\r\n\r\nconsole.log('---- Unicode ----');\r\nup('WAVE DASH', waveDash);\r\nup('FULLWIDTH TILDE: ', fwTilde);\r\n\r\nconsole.log();\r\nconsole.log('---- iconv-lite ----');\r\nup('EUC-JP A1 C1', lite.decode(eucjp_A1C1, 'eucjp'));\r\nup('EUC-JP 8F A2 B7', lite.decode(eucjp_8FA2B7, 'eucjp'));\r\nbp('WAVE DASH to EUC-JP', lite.encode(waveDash, 'eucjp'));\r\nbp('FULLWIDTH TILDE to EUC-JP', lite.encode(fwTilde, 'eucjp'));\r\nconsole.log();\r\nup('Shift_JIS 81 60', lite.decode(sjis_8160, 'shift_jis'));\r\nbp('WAVE DASH to Shift_JIS', lite.encode(waveDash, 'shift_jis'));\r\nbp('FULLWIDTH TILDE to Shift_JIS', lite.encode(fwTilde, 'shift_jis'));\r\nconsole.log();\r\nup('CP932 81 60', lite.decode(sjis_8160, 'cp932'));\r\nbp('WAVE DASH to CP932', lite.encode(waveDash, 'cp932'));\r\nbp('FULLWIDTH TILDE to CP932', lite.encode(fwTilde, 'cp932'));\r\n\r\nconsole.log();\r\nconsole.log('---- node-iconv ----');\r\nconst utf8_waveDash = Buffer.from(waveDash, 'utf8');\r\nconst utf8_fwTilde = Buffer.from(fwTilde, 'utf8');\r\n\r\nconst e2u_iconv = new Iconv('EUC-JP', 'UTF-8');\r\nconst u2e_iconv = new Iconv('UTF-8', 'EUC-JP');\r\nubp('EUC-JP A1 C1', e2u_iconv.convert(eucjp_A1C1));\r\nubp('EUC-JP 8F A2 B7', e2u_iconv.convert(eucjp_8FA2B7));\r\nbp('WAVE DASH to EUC-JP', u2e_iconv.convert(utf8_waveDash));\r\nbp('FULLWIDTH TILDE to EUC-JP', u2e_iconv.convert(utf8_fwTilde));\r\nconsole.log();\r\nconst s2u_iconv = new Iconv('Shift_JIS', 'UTF-8');\r\nconst u2s_iconv = new Iconv('UTF-8', 'Shift_JIS');\r\nubp('Shift_JIS 81 60', s2u_iconv.convert(sjis_8160));\r\nbp('WAVE DASH to Shift_JIS', u2s_iconv.convert(utf8_waveDash));\r\ntry {\r\n  // Error: Illegal character sequence\r\n  bp('FULLWIDTH TILDE to Shift_JIS', u2s_iconv.convert(utf8_fwTilde));\r\n} catch (e) {\r\n  console.log('FULLWIDTH TILDE to Shift_JIS <ERROR> ' + e.message);\r\n}\r\nconsole.log();\r\nconst c2u_iconv = new Iconv('CP932', 'UTF-8');\r\nconst u2c_iconv = new Iconv('UTF-8', 'CP932');\r\nubp('CP932 81 60', c2u_iconv.convert(sjis_8160));\r\nbp('WAVE DASH to CP932', u2c_iconv.convert(utf8_waveDash));\r\nbp('FULLWIDTH TILDE to CP932', u2c_iconv.convert(utf8_fwTilde));\r\n```\r\n. Thank you for your reply, ashtuchkin.\r\n\r\nI found these files.\r\n\r\n* ftp://ftp.unicode.org/Public/MAPPINGS/OBSOLETE/EASTASIA/JIS/JIS0208.TXT\r\n* ftp://ftp.unicode.org/Public/MAPPINGS/OBSOLETE/EASTASIA/JIS/JIS0212.TXT\r\n\r\nThese are in the OBSOLETE directory, but libiconv probably used these maps.\r\n\r\nI don't object to iconv-lite being based on WHATWG Encoding Standard, and I think that is a good policy. But I think that there are two problems.\r\n\r\n1. iconv-lite is different from the behavior of node-iconv. This confuse us. (See the code below, and run)\r\n2. JIS X 0208 takes precedence over JIS X 0212 (beginnig with 8F), and depending on the implementation, JIS X 0212 may not be supported by EUC-JP.\r\n\r\nThis code is check enocde/decode with icnov-lite and node-iconv.\r\n\r\n```\r\nconst Iconv = require('iconv').Iconv;\r\nconst lite = require('iconv-lite');\r\n\r\nconst unicodePoint = s => 'U+' + s.charCodeAt(0).toString(16).toUpperCase();\r\nconst bufferString = buf => {\r\n  let s = '[ '\r\n  for (const b of buf) {\r\n    s += b.toString(16).toUpperCase();\r\n    s += ' ';\r\n  }\r\n  s += ']';\r\n  return s;\r\n};\r\nconst up = (m, s) => console.log(m + ' : ' + unicodePoint(s));\r\nconst bp = (m, b) => console.log(m + ' -> ' + bufferString(b));\r\nconst ubp = (m, b) => up(m, b.toString('utf8'));\r\n\r\nconst waveDash = '\\u301C';\r\nconst fwTilde = '\\uFF5E';\r\n\r\nconst eucjp_A1C1 = new Buffer([0xA1, 0xC1]); // JISX0208 1-33 on EUC-JP\r\nconst eucjp_8FA2B7 = new Buffer([0x8F, 0xA2, 0xB7]); // JISX0212 1-23 on EUC-JP\r\nconst sjis_8160 = new Buffer([0x81, 0x60]); // JISX0208 1-33 on Shift_JIS\r\n\r\nconsole.log('---- Unicode ----');\r\nup('WAVE DASH', waveDash);\r\nup('FULLWIDTH TILDE: ', fwTilde);\r\n\r\nconsole.log();\r\nconsole.log('---- iconv-lite ----');\r\nup('EUC-JP A1 C1', lite.decode(eucjp_A1C1, 'eucjp'));\r\nup('EUC-JP 8F A2 B7', lite.decode(eucjp_8FA2B7, 'eucjp'));\r\nbp('WAVE DASH to EUC-JP', lite.encode(waveDash, 'eucjp'));\r\nbp('FULLWIDTH TILDE to EUC-JP', lite.encode(fwTilde, 'eucjp'));\r\nconsole.log();\r\nup('Shift_JIS 81 60', lite.decode(sjis_8160, 'shift_jis'));\r\nbp('WAVE DASH to Shift_JIS', lite.encode(waveDash, 'shift_jis'));\r\nbp('FULLWIDTH TILDE to Shift_JIS', lite.encode(fwTilde, 'shift_jis'));\r\nconsole.log();\r\nup('CP932 81 60', lite.decode(sjis_8160, 'cp932'));\r\nbp('WAVE DASH to CP932', lite.encode(waveDash, 'cp932'));\r\nbp('FULLWIDTH TILDE to CP932', lite.encode(fwTilde, 'cp932'));\r\n\r\nconsole.log();\r\nconsole.log('---- node-iconv ----');\r\nconst utf8_waveDash = Buffer.from(waveDash, 'utf8');\r\nconst utf8_fwTilde = Buffer.from(fwTilde, 'utf8');\r\n\r\nconst e2u_iconv = new Iconv('EUC-JP', 'UTF-8');\r\nconst u2e_iconv = new Iconv('UTF-8', 'EUC-JP');\r\nubp('EUC-JP A1 C1', e2u_iconv.convert(eucjp_A1C1));\r\nubp('EUC-JP 8F A2 B7', e2u_iconv.convert(eucjp_8FA2B7));\r\nbp('WAVE DASH to EUC-JP', u2e_iconv.convert(utf8_waveDash));\r\nbp('FULLWIDTH TILDE to EUC-JP', u2e_iconv.convert(utf8_fwTilde));\r\nconsole.log();\r\nconst s2u_iconv = new Iconv('Shift_JIS', 'UTF-8');\r\nconst u2s_iconv = new Iconv('UTF-8', 'Shift_JIS');\r\nubp('Shift_JIS 81 60', s2u_iconv.convert(sjis_8160));\r\nbp('WAVE DASH to Shift_JIS', u2s_iconv.convert(utf8_waveDash));\r\ntry {\r\n  // Error: Illegal character sequence\r\n  bp('FULLWIDTH TILDE to Shift_JIS', u2s_iconv.convert(utf8_fwTilde));\r\n} catch (e) {\r\n  console.log('FULLWIDTH TILDE to Shift_JIS <ERROR> ' + e.message);\r\n}\r\nconsole.log();\r\nconst c2u_iconv = new Iconv('CP932', 'UTF-8');\r\nconst u2c_iconv = new Iconv('UTF-8', 'CP932');\r\nubp('CP932 81 60', c2u_iconv.convert(sjis_8160));\r\nbp('WAVE DASH to CP932', u2c_iconv.convert(utf8_waveDash));\r\nbp('FULLWIDTH TILDE to CP932', u2c_iconv.convert(utf8_fwTilde));\r\n```\r\n. ",
    "AdamChrist": "@ashtuchkin thanks. @ashtuchkin thanks. ",
    "williams-voon": "Hi Alex,\r\n\r\nI am using React Native.\r\n\r\nRegards,\r\nWilliams Voon. @ashtuchkin \r\n\r\nI tried following steps.\r\n\r\n1. Update package.json, and content is listed below:\r\n\r\n```son\r\n\"dependencies\": {\r\n    \"buffer\": \"^5.0.6\",\r\n    \"iconv-lite\": \"ashtuchkin/iconv-lite#master\",\r\n    \"react\": \"16.0.0-alpha.6\",\r\n    \"react-native\": \"0.43.3\",\r\n    \"stream\": \"0.0.2\"\r\n  },\r\n```\r\n\r\n2. Remove the folder: node_modules\r\n```\r\nrm -fr node_modules\r\n```\r\n3. Install again\r\n```\r\nnpm install\r\n```\r\n\r\n4. And then run my JS code in React Native project. The code listed below:\r\n```javascript\r\n    let buf =  iconv.encode('\u5b89\u5b89\u8981\u4e56\u4e56\\n', 'GBK' );\r\n    let str=iconv.decode(buf,'GBK');\r\n    console.log(str);\r\n```\r\n\r\nThe code works as expected, and no error found.\r\n\r\nSo I think the problem has been solved. Thank you very much!\r\n\r\n. Hi Alex,\r\n\r\nI am using React Native.\r\n\r\nRegards,\r\nWilliams Voon. @ashtuchkin \r\n\r\nI tried following steps.\r\n\r\n1. Update package.json, and content is listed below:\r\n\r\n```son\r\n\"dependencies\": {\r\n    \"buffer\": \"^5.0.6\",\r\n    \"iconv-lite\": \"ashtuchkin/iconv-lite#master\",\r\n    \"react\": \"16.0.0-alpha.6\",\r\n    \"react-native\": \"0.43.3\",\r\n    \"stream\": \"0.0.2\"\r\n  },\r\n```\r\n\r\n2. Remove the folder: node_modules\r\n```\r\nrm -fr node_modules\r\n```\r\n3. Install again\r\n```\r\nnpm install\r\n```\r\n\r\n4. And then run my JS code in React Native project. The code listed below:\r\n```javascript\r\n    let buf =  iconv.encode('\u5b89\u5b89\u8981\u4e56\u4e56\\n', 'GBK' );\r\n    let str=iconv.decode(buf,'GBK');\r\n    console.log(str);\r\n```\r\n\r\nThe code works as expected, and no error found.\r\n\r\nSo I think the problem has been solved. Thank you very much!\r\n\r\n. ",
    "flyher": "@ashtuchkin \r\nIn react-native , when i use axios(arraybuffer) to get html (gb2312) source, the iconv-lite helped me.\r\nThank you for your good job.\r\n\r\nshare my code , maybe it can help more people:\r\n```html\r\nimport axios from 'axios';\r\nimport iconv from 'iconv-lite';\r\nimport { Buffer } from 'buffer';\r\n\r\nfetchHtml(){\r\n    return axios({\r\n      method: 'get',\r\n      url: baseurl,\r\n      responseType: 'arraybuffer'\r\n    })\r\n      .then((response) => {\r\n        let html_ = iconv.decode(new Buffer(response.data), 'gbk');\r\n\r\n        this.setState({\r\n          html_: html_,\r\n        });\r\n      })\r\n      .catch((error) => {\r\n        console.error(error);\r\n      });\r\n}\r\n\r\n```. @ashtuchkin \r\nIn react-native , when i use axios(arraybuffer) to get html (gb2312) source, the iconv-lite helped me.\r\nThank you for your good job.\r\n\r\nshare my code , maybe it can help more people:\r\n```html\r\nimport axios from 'axios';\r\nimport iconv from 'iconv-lite';\r\nimport { Buffer } from 'buffer';\r\n\r\nfetchHtml(){\r\n    return axios({\r\n      method: 'get',\r\n      url: baseurl,\r\n      responseType: 'arraybuffer'\r\n    })\r\n      .then((response) => {\r\n        let html_ = iconv.decode(new Buffer(response.data), 'gbk');\r\n\r\n        this.setState({\r\n          html_: html_,\r\n        });\r\n      })\r\n      .catch((error) => {\r\n        console.error(error);\r\n      });\r\n}\r\n\r\n```. ",
    "phated": "@ashtuchkin I wouldn't worry about the deprecation currently. @ashtuchkin I wouldn't worry about the deprecation currently. ",
    "ChALkeR": "@sidorares At this moment, there is no ETA for the hard deprecation of `new Buffer`. I.e. at the last discussion of this in March, the CTC decided to not set a timeframe for runtime deprecation yet.\r\n\r\nThat doesn't mean that it can't happen , though. It definitely won't happen before 9.0 (October 2017) and I find it unlikely that it will happen in 9.0 (though personally I would prefer to see that in).\r\n\r\nThere are other resons why you shouldn't be using the old API, though.\r\n\r\nAlso note that the `safe-buffer` polyfill is only needed for Node.js versions 4.4.x and lower, 5.x branch, 0.10 branch and 0.12 branch. I don't think that's really needed nowdays.. @sidorares At this moment, there is no ETA for the hard deprecation of `new Buffer`. I.e. at the last discussion of this in March, the CTC decided to not set a timeframe for runtime deprecation yet.\r\n\r\nThat doesn't mean that it can't happen , though. It definitely won't happen before 9.0 (October 2017) and I find it unlikely that it will happen in 9.0 (though personally I would prefer to see that in).\r\n\r\nThere are other resons why you shouldn't be using the old API, though.\r\n\r\nAlso note that the `safe-buffer` polyfill is only needed for Node.js versions 4.4.x and lower, 5.x branch, 0.10 branch and 0.12 branch. I don't think that's really needed nowdays.. ",
    "erikkemperman": "Just noticed the same thing. Suggested fix:\r\n```js\r\n.toLowerCase().replace(/:\\d{4}$/, \"\").replace(/[^0-9a-z]/g, \"\");\r\n```. Actually, maybe it should be\r\n```js\r\n.toLowerCase().replace(/:\\d{4}[^0-9a-z]*$/, \"\").replace(/[^0-9a-z]/g, \"\");\r\n```. I was thinking you might want to get rid of trailing (after the year) non-alphanumerics as well?. Just noticed the same thing. Suggested fix:\r\n```js\r\n.toLowerCase().replace(/:\\d{4}$/, \"\").replace(/[^0-9a-z]/g, \"\");\r\n```. Actually, maybe it should be\r\n```js\r\n.toLowerCase().replace(/:\\d{4}[^0-9a-z]*$/, \"\").replace(/[^0-9a-z]/g, \"\");\r\n```. I was thinking you might want to get rid of trailing (after the year) non-alphanumerics as well?. ",
    "rossj": "FWIW I just swapped the ORs in my local copy\r\n```javascript\r\n/:\\d{4}$|[^0-9a-z]/g\r\n```. FWIW I just swapped the ORs in my local copy\r\n```javascript\r\n/:\\d{4}$|[^0-9a-z]/g\r\n```. ",
    "laoqiren": "I just misunderstood the document, the issue should be closed now.. I just misunderstood the document, the issue should be closed now.. ",
    "reklatsmasters": "@ashtuchkin It's faster for internal and single-byte encodings. The use cases are the same as for Buffer.byteLength - if you want to known size of string in bytes before encoding.. @ashtuchkin It's faster for internal and single-byte encodings. The use cases are the same as for Buffer.byteLength - if you want to known size of string in bytes before encoding.. ",
    "JsseL": "Then perhaps some rewording in Readme might make sense, at least for me\r\n> Works on Windows and in sandboxed environments like Cloud9.\r\n\r\nwas a bit confusing.\r\n\r\nBut in any case, thanks for the answer and the lib! :+1: . Then perhaps some rewording in Readme might make sense, at least for me\r\n> Works on Windows and in sandboxed environments like Cloud9.\r\n\r\nwas a bit confusing.\r\n\r\nBut in any case, thanks for the answer and the lib! :+1: . ",
    "Nordes": "Thank's, I'll do that.\r\n. Thank's, I'll do that.\r\n. ",
    "tharaka1": "I have tried both s.toString(\"base64\") and iconv.decode(s, \"base64\"). but it gives same as btoa(s.toString())\r\n\r\nvar s = iconv.encode(str, 'ISO-8859-1'); // Is this correct?\r\nI debug the code and check, this is given Unit8Array same as byte array of the string. This look \r\nlike utf-8 encoded array.\r\n\r\n\r\n. I sent you details of testing and what i am expecting.. After adding attribute node to the types field in to tsconfig.app.json it works.\r\n  {\r\n  \"extends\": \"../tsconfig.json\",\r\n  \"compilerOptions\": {\r\n    \"outDir\": \"../out-tsc/app\",\r\n    \"module\": \"es6\",\r\n    \"baseUrl\": \"\",\r\n    \"types\": [\"node\"] --> ADD THIS\r\n  },\r\n  \"exclude\": [\r\n    \"test.ts\",\r\n    \"**/*.spec.ts\"\r\n  ]\r\n}. I have tried both s.toString(\"base64\") and iconv.decode(s, \"base64\"). but it gives same as btoa(s.toString())\r\n\r\nvar s = iconv.encode(str, 'ISO-8859-1'); // Is this correct?\r\nI debug the code and check, this is given Unit8Array same as byte array of the string. This look \r\nlike utf-8 encoded array.\r\n\r\n\r\n. I sent you details of testing and what i am expecting.. After adding attribute node to the types field in to tsconfig.app.json it works.\r\n  {\r\n  \"extends\": \"../tsconfig.json\",\r\n  \"compilerOptions\": {\r\n    \"outDir\": \"../out-tsc/app\",\r\n    \"module\": \"es6\",\r\n    \"baseUrl\": \"\",\r\n    \"types\": [\"node\"] --> ADD THIS\r\n  },\r\n  \"exclude\": [\r\n    \"test.ts\",\r\n    \"**/*.spec.ts\"\r\n  ]\r\n}. ",
    "kevinptt0323": "I think I should suggest libiconv first.. I think I should suggest libiconv first.. ",
    "idangozlan": "Can you please provide example? Didn't find the way..\n\nOn Oct 3, 2017 20:53, \"Alexander Shtuchkin\" <notifications@github.com>\nwrote:\n\n> Yes.\n>\n> On Oct 3, 2017 03:14, \"Idan Gozlan\" <notifications@github.com> wrote:\n>\n> > Hi,\n> >\n> > Is that possible use this convertor for windows-1252 to utf-8 conversion?\n> >\n> > Thanks\n> >\n> > \u2014\n> > You are receiving this because you are subscribed to this thread.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/ashtuchkin/iconv-lite/issues/167>, or mute the\n> thread\n> > <https://github.com/notifications/unsubscribe-auth/\n> AAmVHQzbDdrlYRgOETUs36evDw6schyXks5sogkggaJpZM4Pr4kQ>\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/167#issuecomment-333925645>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AB5hbYaJYQXmMd24gpV613oJtMQQvayHks5sonSQgaJpZM4Pr4kQ>\n> .\n>\n. Well... not working.\r\n\r\nBut it's working on `iconv` module, by using:\r\n```\r\nconst iconv = new Iconv('UTF-8', 'windows-1252');\r\nfunction convert(string) {\r\nreturn iconv.convert(string).toString();\r\n}\r\n```. Can you please provide example? Didn't find the way..\n\nOn Oct 3, 2017 20:53, \"Alexander Shtuchkin\" <notifications@github.com>\nwrote:\n\n> Yes.\n>\n> On Oct 3, 2017 03:14, \"Idan Gozlan\" <notifications@github.com> wrote:\n>\n> > Hi,\n> >\n> > Is that possible use this convertor for windows-1252 to utf-8 conversion?\n> >\n> > Thanks\n> >\n> > \u2014\n> > You are receiving this because you are subscribed to this thread.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/ashtuchkin/iconv-lite/issues/167>, or mute the\n> thread\n> > <https://github.com/notifications/unsubscribe-auth/\n> AAmVHQzbDdrlYRgOETUs36evDw6schyXks5sogkggaJpZM4Pr4kQ>\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ashtuchkin/iconv-lite/issues/167#issuecomment-333925645>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AB5hbYaJYQXmMd24gpV613oJtMQQvayHks5sonSQgaJpZM4Pr4kQ>\n> .\n>\n. Well... not working.\r\n\r\nBut it's working on `iconv` module, by using:\r\n```\r\nconst iconv = new Iconv('UTF-8', 'windows-1252');\r\nfunction convert(string) {\r\nreturn iconv.convert(string).toString();\r\n}\r\n```. ",
    "LionOpeter": "Hi @ashtuchkin and thank you very much for replying.\r\nI get what you said. My main goal (as i mentioned in my question number 2) is to create a string which has the right character(s). For me the right characters are the characters of windows-1255 encoding because my IRC server works only with this charset.If i send the server the wrong encoding i get an error saying `'Nickname is unavailable: Illegal characters'` Any idea how i can pull this off?. If i try to send it as a buffer i get error:\r\n```\r\n/home/USER/git/npm-test-1/node_modules/irc/lib/irc.js:930\r\n    if (args[args.length - 1].match(/\\s/) || args[args.length - 1].match(/^:/) || args[args.length - 1] === '') {\r\n                              ^\r\n\r\nTypeError: args[(args.length - 1)].match is not a function\r\n    at Client.send (/home/USER/git/npm-test-1/node_modules/irc/lib/irc.js:930:31)\r\n    at Client._connectionHandler (/home/USER/git/npm-test-1/node_modules/irc/lib/irc.js:725:10)\r\n    at Object.onceWrapper (events.js:314:30)\r\n    at emitNone (events.js:110:20)\r\n    at Socket.emit (events.js:207:7)\r\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1125:10)\r\n\r\n```\r\nIn-addition, an attempt to use `str = buffer3.toString(\"binary\")` ended up with the same result of `Illegal characters` error. Any way to assign a string with a hex value in javascript?. Hi @ashtuchkin and thank you very much for replying.\r\nI get what you said. My main goal (as i mentioned in my question number 2) is to create a string which has the right character(s). For me the right characters are the characters of windows-1255 encoding because my IRC server works only with this charset.If i send the server the wrong encoding i get an error saying `'Nickname is unavailable: Illegal characters'` Any idea how i can pull this off?. If i try to send it as a buffer i get error:\r\n```\r\n/home/USER/git/npm-test-1/node_modules/irc/lib/irc.js:930\r\n    if (args[args.length - 1].match(/\\s/) || args[args.length - 1].match(/^:/) || args[args.length - 1] === '') {\r\n                              ^\r\n\r\nTypeError: args[(args.length - 1)].match is not a function\r\n    at Client.send (/home/USER/git/npm-test-1/node_modules/irc/lib/irc.js:930:31)\r\n    at Client._connectionHandler (/home/USER/git/npm-test-1/node_modules/irc/lib/irc.js:725:10)\r\n    at Object.onceWrapper (events.js:314:30)\r\n    at emitNone (events.js:110:20)\r\n    at Socket.emit (events.js:207:7)\r\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1125:10)\r\n\r\n```\r\nIn-addition, an attempt to use `str = buffer3.toString(\"binary\")` ended up with the same result of `Illegal characters` error. Any way to assign a string with a hex value in javascript?. "
}
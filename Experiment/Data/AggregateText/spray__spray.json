{
    "sirthias": "Excellent.\nMerged into develop branch.\nThanks for the contribution!\n. Maarten,\nthanks for the report.\nI see the problem and will get the fix out in the next days.\nCheers,\nMathias\n. Actually this behavior is intended. IMHO in the majority of cases path matching should \"just work\". Even when the user appended a slash the uri should still match.\nHowever, you can easily achieve the distinction you are looking for by reversing the order of the two sub routes.\nIf you try to match the \"a/\" route first it will only match if there really is a trailing slash...\n\nCheers,\nMathias\n. This was not a pedantic question at all!\nRather a perfectly valid and valuable one.\nSo please keep them coming... :)\n(You can of course also use the mailing list)\n. Thanks for the comment.\nYes, \"metrics\" is a great project.\nAs is https://github.com/twitter/ostrich ...\nUnfortunately both libs somewhat do \"too much\" and are not \"actor enabled\".\nSo currently we are planning to roll our own solution but turn to the existing ones for structure and ideas...\n. Alright, would you be able to provide a few examples on how you use Metrics in your actor code? (preferably on the mailing list)\n. Current thinking: provide a thin adapter layer to [Graphite](http://graphite.wikidot.com/), probably implemented as a singleton actor representing the \"gateway\" to Graphite.\n. Sound cool.\nI guess a small example project somewhere would be excellent!\n. Metrics really has come a long way and is nicely modular by now.\nNo point in trying to roll our own monitoring solution anymore.\nWe should provide `MetricsSupport` trait and a few simple directives to get Metrics support rolling...\n. Excellent, David!\nWe are incredibly busy with all the things we have planned for the 1.0 release, and unfortunately we had to leave this feature off the list. So if you'd like to pick it up, we'd likely be able to support it.\nOtherwise we'll have a go at this later...\n. I'd say we first try to get the basic metrics abstractions (gauges, counters, histograms, etc.) mirrored by directives and then build more complex things from the basics.\n. Well, I might want to log custom things to metrics from my directives, so something generic underneath is probably the best foundation.\n. Curtis, indeed, as far as I know there is noone working on this issue at the moment. If you have something worthwhile including we'd be more than happy to take a look.\n. There is somewhat longer-standing PR: https://github.com/spray/spray/pull/359\nthat\"s we hope to get to soon...\n. I just closed @derekwyatt's PR https://github.com/spray/spray/pull/359, which will not make it into spray anymore. Still, we should definitely use it as a basis for any future work on this feature in Akka HTTP.\n. We'll not do this since it would make things like\n\n```\nget {\n    someActor ! _\n}\n```\n\nimpossible (require wrapping)...\n. Fixed in develop and akka1.1 branches\n. Closed by https://github.com/spray/spray/commit/ca2a39713b4a9c8a134d6e208b138d10cf235688\n. Paul,\nthanks for the report and the praise (also the ones via twitter!).\nAre you seeing this initial delay with your own application or one of the examples?\nThe markdown-server example really does have a great initial delay due to the pegdown markdown-parser being created at the bytecode-level...\nApart from this we haven't experienced an especially slow startup so far.\nIs there something special about your use case?\nCheers,\nMathias \n. Thanks, Paul, for these details.\nWe'll look into this and whip something up for spray 0.6.0, which is not far at all after todays events...\n\nCheers,\nMathias\n\n---\n\nmathias@spray.cc\nhttp://www.spray.cc\n\nOn 12.05.2011, at 22:11, psnively wrote:\n\n> You're more than welcome!\n> \n> I don't believe there's anything particularly unusual about our use case so far, and once the initial request has been processed, successive operations are nice and zippy. On my 2.13 GHz, 4G RAM MacBook Air running either Tomcat 7.0.12 or Jetty 8.0.0.M2, I see the first operation go from ~1015ms to successive operations taking ~35ms. You can see the reason why very easily: please just launch your server JVM with the \"-verbose:class\" option and examine the logs. Upon handling the first request, you'll see an enormous number of classes loaded. Again, many are from the cc.spray package, but a much, much larger number are from parboiled. Anything you can do to preload these packages will be a big win for clients of that first request. :-)\n> \n> Thanks!\n> Paul\n> \n> ## \n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/spray/spray/issues/14#comment_1148968\n. of course, an corresponding Unmarshaller should be added as well\n. No considered that important any more.\nCurrently the \"preferred\" way of deployment is as one JAR using spray-can as container and the sbt-assembly plugin as packager.\n. closed by latest commits in develop\n. Use ClientConfig.requestTimeoutInMs\n. Streaming large responses is the first priority.\nBeing able to receive streamed requests asynchronously is a second priority.\n. Response streaming available in develop branch and current 0.9.0-SNAPSHOT build\n. I would strongly recommend using the 0.7.0-SNAPSHOT from the develop branch.\nCurrent artifacts for this version are available from scala-tools snapshots.\n\nBtw: All older SBT versions are still available (incl. 0.7.5.RC1 and 0.7.7).\nJust follow the installation instruction [here](http://code.google.com/p/simple-build-tool/wiki/Setup)\n. Excellent, thanks Steffen!\nI'll look into your branch as soon as I find the time...\n. Thanks for the report, Neil!\n. Yes, just confirmed that spray 0.7.0 cannot deal with quoted cookie-values due to a bug in the HTTP parser.\nThanks for the report!\n. I'll upgrade to Akka 1.2 as soon as the final is out...\n. Now on http://spray.io\n. The problem is not the '/' character but the trailing semicolon, which is not allowed according to the cookie spec (http://tools.ietf.org/html/rfc6265#section-4.1.1). So the server is indeed sending a bad header...\n. No applicable on the spray level as the underlying container/server suppresses sending of the response body for HEAD requests\n. Julien,\nthanks for your contribution.\nI'm looking forward to your multipart implementation.\nSomehow my initial feeling is that including the \"boundary\" parameter directly into the ContentType class is not the best solution. The boundary seems like an implementation detail of multipart messages that, IMHO, shouldn't be surfaced this high up in the API.\nWithout having though about it too much it seems to me that the natural solution would be to upgrade the HttpContent type to a union type with the implementations:\n- NoContent\n- SinglePartContent\n- MultiPartContent\n\nThe latter would have to be container for a List[BodyPart] as well as the boundary....\nThe BodyPart would encapsulate a List[HttpHeader] as well as an HttpContent (similar to what the HttpMessage already does). Whether or not \"NoContent\" makes sense depends also on whether body parts without content (e.g. only headers) are allowed by the spec.\nYou could also check out how the BlueEyes team has modelled this...\n\nThanks and cheers,\nMathias\n. One of our design principles is to start with thinking about how a new feature would be used rather than coming at it from an implementation perspective.\nI think the following way of consuming multipart content would be good:\n\n```\ncontent(asMultipart[Employee, Image]) { (employee, image) =>\n    ...\n}\n```\n\nSimilarly, marshalling several values into a multipart messages could be done like this:\n\n```\ncontext.complete(multipart(employee, image))\n```\n\nSo, after having pondered this a bit more, I agree that extending the HttpContent might not be best solution. A special multipart marshaller/unmarshaller pair would probably suffice, along with the \"asMultipart\" and \"multipart\" wrapper methods for unmarshalling/marshalling.\n\nCheers,\nMathias\n\n---\n\nmathias@spray.cc\nhttp://www.spray.cc\n\nOn 01.09.2011, at 19:30, shopolate wrote:\n\n> Please ignore my precedent pull request, I did not realized I was on the master branch.\n> \n> Thank you for your commentaries. I would say I am not sure subclassing HttpContent would simplify the framework. I like the way we can marshall things, especially with multipart, where we can reuse marshalling on individual parts.\n> \n> As for the \"boundary\" parameter, I agree it is quite ugly. BUT, it make sense to parse the headers just once. And if we want to parse the Content-Type just once, ContentType seems the right place to put parameter data, hence my modification. I added the same kind of processing for Content-Disposition header. I think parsing headers are an important feature of such a framework. I guess this is why you added the convenient data structures for parsing cookie headers for example. Why some headers would get the \"royal treatment\" by being fully parsed, and their data fully exposed to the end user? But I don't have a clear picture of what you are envisioning for the future of spray, so my view could be too simplistic.\n> \n> Anyway, I wanted to thank you for spray, which is one of the easiest, most powerfull framework I have ever tried. I love it and try to spread the word :)\n> \n> ## \n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/spray/spray/pull/45\n. Julien,\n\nI just realized now that your pull request as well as your points so far are addressing the \"form-data\" sub type of \"multipart\". Even though you prominently put this in the title I was somehow thinking we are talking about general \"multipart/mixed\" support (RFC 1341 sec. 7.2). Sorry for that!\nSo, please ignore the comments I made so far (as \"form-data\" of course carries names for the parts whereas the general \"multipart/mixed\" type does not).\n\nI agree with you in that form content, no matter whether `application/x-www-form-urlencoded` or `multipart/form-data`, is similar to query parameters in that it combines key/value pairs with a custom type that a user wants to unmarshal to. The difference is that query parameter values as well as the values of `application/x-www-form-urlencoded` form fields have a known encoding (url-encoded), whereas the values of `multipart/form-data` form fields can be present in any content-type. \nCurrently the approach for dealing with query parameter values is quite different from the one for message entities (bodies). The former uses implementations of the SimpleParser trait whereas the latter uses the Unmarshallers.\nTheoretically the both could be merged, but so far this seemed somewhat unnatural to me.\n\nNow, when we are thinking about something like a `formField`/`formFields` directive that extracts form field values in analogy to the `parameter`/`parameters` directive (which I think is a good approach!), merging SimpleParsers into Unmarshallers seems more attractive. Otherwise we would have to have two different directives for `application/x-www-form-urlencoded` and `multipart/form-data`, since these two would require differing type classes for deserialization.\n\nSo, in the moment, I think I'd favor this approach for deserialization:\n- somehow merge SimpleParsers into Unmarshallers\n- change the `parameters` directives to use Unmarshallers\n- introduce `formFields` directives that work with `application/x-www-form-urlencoded` as well as `multipart/form-data` form content in analogy to the `parameters` directives\n- remove the old (existing) Marshaller/Unmarshaller pair for FormContent\n\nFor marshalling, I'd probably go for something like this:\n\ncontext.complete(\n  MultiPartFormContent(\n    part(\"employee\", employee),\n    part(\"image\", image)\n  )\n)\n\nIt's basically what you suggest with two changes:\n1. An explicit MultiPartFormContent type, which would have its own Marshaller and does not require another `RequestContext.complete` overload. This marshaller also would give us the natural place for verifying acceptability of `multipart/form-data`.\n2. No way to explicitly set a media type for the part. The Marshaller for an `Employee` figures this out itself using the requests accept headers and its own capabilities.\n\nGetting all this implemented would certainly be a great step forward for spray and would significantly improve the existing (somewhat wimpy) support for `application/x-www-form-urlencoded` form content.\nThanks for pushing me to think about all this\u2026 :)\n\nCheers,\nMathias\n\n---\n\nmathias@spray.cc\nhttp://www.spray.cc\n\nOn 03.09.2011, at 16:20, shopolate wrote:\n\n> I forgot to add that relying on part order might not fit every use cases. Ideally, we would be able to map the part name with the type. This also could be achieved by a directive similar to the parameters one. The only difference is we could match a part either by its name or by its index.\n> \n> If you are interested, I could try to do it.\n> \n> As for marshalling, the problem here is you might want to specify names and content types to marshal to. It seems to me that:\n> \n> context.complete(multipart(employee, image))\n> \n> might be restrictive, especially since the default content type of a body part is text/plain. For example, something like:\n> \n> context.complete(\n>    bodyPart(employee, `application/json`, \"employee'), \n>    bodyPart(someStream, `image/png`, \"image\", fileName = \"icon.png\"), \n>    bodyPart(someStringValue, name = \"someFormKey\")) \n> \n> would allow for more control.\n> \n> cheers,\n> Julien\n> \n> ## \n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/spray/spray/pull/45#issuecomment-1986802\n. The type of `image` in my example is some custom type that a Marshaller must be available for.\nThe MultiPartFormContent Marshaller would simply delegate marshalling of the part content to the individual Marshallers.\n\n> As for the refactoring of SImpleParsers you are talking about, I have a hard time figuring it out. Unmarshallers are unmarshalling HttpContent, SImpleParsers are unmarshalling strings. It seems to me quite a stretch to merge them together, since the semantics looks quite different.\n\nI'll have to take a look at this in some more depth. You are right, currently they are quite different, but in principle they have the same responsibility. Will see what I can do...\n\n---\n\nmathias@spray.cc\nhttp://www.spray.cc\n\nOn 05.09.2011, at 16:32, shopolate wrote:\n\n> Everything makes sense. \n> \n> One last little dark area: what is the type of the image var in your unmarshalling example? It can not be InputStream or Array[Byte], otherwise, you could not deduce if the content type is application/octet-stream, image/png or image/jpeg. It could not be HttpContent neither, otherwise it would contradict your statement about the framework deducing the most fit content type from accept header itself.\n> \n> As for the refactoring of SImpleParsers you are talking about, I have a hard time figuring it out. Unmarshallers are unmarshalling HttpContent, SImpleParsers are unmarshalling strings. It seems to me quite a stretch to merge them together, since the semantics looks quite different.\n> \n> Cheers,\n> \n> Julien\n> \n> ## \n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/spray/spray/pull/45#issuecomment-2001021\n. Thanks Julien,\nI'll look into your pull request as soon as I find the time...\n\nCheers,\nMathias\n. Julien,\nform support is now fully available in the develop branch.\nThanks a lot again for this very high-quality pull request!\nEven though I haven't directly merged it in I used different parts in various places.\n(The reason that it wasn't directly merged is, that I discovered a whole series of things in the codebase I wanted to improve architecturally, and the implementation of form support was a good occasion to take this on...)\n\nThanks again and cheers,\nMathias\n. closed by [this commit](https://github.com/spray/spray/commit/780a24e0abc52043cb7b287c270777b8aa883a49)\n. Not relevant any more for 1.0-M3+\n. This was already done in the develop branch of spray-template.\nNow also in master...\n. For the time being the [DebuggingDirectives](https://github.com/spray/spray/blob/master/spray-server/src/main/scala/cc/spray/directives/DebuggingDirectives.scala) will do...\n. Could be provide some more details?\nWhere did this issue appear again?\nWhich branch are you looking at?\n. Alright, thanks for the clarification...\n. Just use the header\n\n```\n`Authorization`(BasicHttpCredentials(username = \"bob\", password = \"123xyz\"))\n```\n\nPlease use the [mailing list](http://groups.google.com/group/spray-user) for any follow-up and all other questions regarding spray that you might have..\n\nCheers and happy spraying,\nMathias\n\n---\n\nmathias@spray.cc\nhttp://www.spray.cc\n\nOn 04.11.2011, at 19:35, jamosa wrote:\n\n> Hello!\n> \n> I'm trying to do a test in my project that uses a Rest Interface with authentication. To do the test, I define some interfaces and then I try to connect and it fails because I don't introduce user and password in httpRequest() . \n> My code is the next: \n> \n> val mediaType =`text/javascript`\n> \n> val expected =\n>        \"\"\"\n>        {\n>          \"message\":\"It contains the collection of identifiers of the corpus in the system\"\n>        }\n>        \"\"\"\n> \n> ```\n>  testService(HttpRequest(GET, \"/corpus\", headers = List(`Accept`(mediaType),`Authorization`(httpCredentials)))) {\n>    mapService\n>  }.response.content.as[String] must be(Right(pretty(render(parse(expected)))))\n> ```\n> \n> So, I know that I have to specify httpCredentials but I don't know how.\n> If anyone can explain me how can I do it, I would be so grateful.\n> \n> Thank you!\n> \n> ---\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/spray/spray/issues/55\n. If I understand you right, onejigtwo, you want to use different sets of (un)marshallers with different URI patterns.\nThis is already perfectly possible:\n\n```\npath(\"v1\") {\n  import Version1Marshallers._\n  ...\n} ~\npath(\"v2\") {\n  import Version2Marshallers._\n  ..\n}\n```\n\nHTH,\nMathias\n. You can use the 0.8.0-RC3 on scala-tools snapshots or wait for the final, which will be released today.\n. Konstantin, thanks for the report.\nThis has been fixed with the documentation update for the 0.8.0 release.\n(Btw. the documentation is a wiki, you can simply apply a fix yourself...)\n. Thanks Gregg, for providing the starting point for this!\nI have added some more content, so we should be all set for now...\n. In 1.0-M3 chunked requests can be handled via a custom `receive` in the HttpService actor.\n. Thanks, Chris, I'll try to merge this ASAP ...\n. It appears you are referring to the spray-can README.\nI have added an issue there: https://github.com/spray/spray-can/issues/13\nStill, the artifact coordinates as well as info on what dependencies are marked _provided_ and need to be pulled in \"manually\" should be put into the documentation (in addition to the references to the template and examples projects)\n. Now on http://spray.io\n. actually a [bug in spray-can](https://github.com/spray/spray-can/issues/20)\n. actually a [bug in spray-can](https://github.com/spray/spray-can/issues/21)\n. Is that at all possible on the JVM? (w/o having to re-implement DNS ourselves)\n. FTR:\nnetty has already invested quite a bit of time/work into a proper solution,\ncheck out these two PRs:\n- https://github.com/netty/netty/pull/1354 which was later superceded by\n- https://github.com/netty/netty/pull/1622\n\n@normanmaurer merged the latter one into [this branch](https://github.com/netty/netty/tree/dns) just three days ago.\n\nI haven't looked at the code at all so far, but seeing the amount of discussion and review that the PRs have received I trust there is something interesting there.\n\nRelated tickets:\n- https://www.assembla.com/spaces/akka/tickets/2591 (akka)\n- https://github.com/playframework/playframework/issues/1997 (play)\n. Thanks, just updated the link.\nWill be online with the next site deployment...\nThanks!\n. This has already been fixed in the current development version (\"develop\" branch and current SNAPSHOT builds)...\n. I don't think so. ETags and `if-modified-since` are two different things that _spray_ would ideally support both. Maybe you wouldn't _use_ both at the same time because they have somewhat similar use-cases, but _spray_ should still give you the choice (and even work with both at the same time).\n. Ok, added another issue to track quality values: https://github.com/spray/spray/issues/167\n. In addition to the case described in the ML thread linked above there is a very simple test case provoking the problematic behavior: If a connection managed by spray-client times out spray-client currently doesn't properly recover and schedule the next request to another or a new connection. (See [this thread](http://groups.google.com/group/spray-user/browse_thread/thread/ac98b8ab8adec810)).\nThis issue needs to be fixed for 0.9.0.\n. Now on http://spray.io\n. Fixed in master (1.0-M3).\nNo more RootService, therefore no more \"two HttpService\" setup.\n. I'm not sure this will still required for 1.0.\nNote than you can already combine different \"handling strategies\" by concatenating routes \"blocks\" via the \u2018~\u2018 operator, which essentially achieve the same thing.\n. You could do something like this:\n\n```\ntrait ServiceA extends Directives {\n  val routeA = // ...\n}\n\ntrait ServiceB extends Directives {\n  val routeB = // ...                                                                                                             \n}\n\nclass ComplexService extends ServiceA with ServiceB {\n  val route = routeA ~ routeB\n}\n```\n\nIn this way you could build an arbitrary number of sub routing components that you plug together in any way you please.\nWouldn't that work?\n. We will heavily rework the basic RootService/HttpService architecture for 1.0.\nIn the process the rejection-to-response mapping logic will moved directly into the route structure, as will exception handling.\nThis should nicely take care of the problem you are seeing...\n\n---\n\nmathias@spray.cc\nhttp://www.spray.cc\n\nOn 12.04.2012, at 13:04, peltchu wrote:\n\n> Sorry to get back at this again.\n> \n> Nonetheless, after a night's sleep I feel there's one significant drawback with the route approach when compared to the service-level approach. \n> \n> Mappings of rejections to responses happens at the service level and this allows easy configuration of error responses from filters. The route approach seems to me problematic when you want eg. _AuthorizationFailedRejection_ to result in completely different response in route A than in route B. \n> \n> For custom filters it's of course easier but I think when utilizing the default filters (eg. _authorize_) provided by spray they would all have to be intercepted to provide custom rejections/responses. It seems that this would become an another rejection handling layer in addition to the one provided by spray and thus it feels like working against the grain. On the other hand the feature in question would allow this very easily.\n> \n> Any ideas/suggestions on this?\n> \n> ---\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/spray/spray/issues/83#issuecomment-5088713\n. Closing as not relevant anymore in master (1.0-M3).\nThe RootService is gone, the simplified architecture increases ease of use as well as flexibility.\n. Now on http://spray.io\n. Erdem,\nthanks a lot for this super-clean pull request!\n. Fixed in master (1.0-M3).\nThe implicit architecture has been much improved, the DefaultMarshallers trait is gone.\nHowever, in order to have the Marshallers be usable on the client-side as well as the server-side, i.e. for rendering to HttpRequests as well as HttpResponses they cannot directly produce redirects or rejections.\n. Hmm... I'm not sure that'd be something a lot of applications would use....\n. Thanks, Lanchlan, for this excellent bug report!\nTurned out that this was actually due to a problem in spray-can, which just got fixed with the [0.9.3 release](http://implicit.ly/spray-can-093).\n\nThanks again and cheers,\nMathias\n. The query parameters are available from the RequestContext:\n\n```\nval queryParams: Map[String, String] = ctx.request.queryParams\n```\n\nPlease direct all further questions to the [mailing list](http://groups.google.com/group/spray-user) as this space here is reserved for issues.\n. Fixed in master.\nAs of 1.0-M3 exception handling has been much improved via the new `ExceptionHandler` infrastructure.\n. Fixed in master (with the new test DSL).\n. Not in the current 1.0 milestones.\nIn spray-can 0.9 there was an extra field for the remote address in the RequestContext, but we'd like to not having to add an extra field just for this feature, which many applications don't really need.\n. Expect this issue to be closed soon, so the work-around with having nginx in front of spray-can will not be required for long. \n. Antony,\nunfortunately we are not able to develop, maintain and support a proper Java side API for spray, officially it's Scala only.\nIf you have specific recommendations as to what we might change or add in order to make life for usage from Java possible/easier we'd be happy to consider any suggestions, but we won't take on this task ourselves... sorry!\n. Thanks David for clarifying this and sharing your experiences!\n. Fixed in master.\n. Solved by new MergeStrategy in sbt-assembly\n. Thanks for the report!\nNote that this error only appears if the file you are serving is larger than the configured `file-chunking-threshold-size`.\n. Thanks for the report.\nShould be back now...\n. We currently do not publish api-docs.\nHowever, we will bring them back online with the next major cycle on the spray.io site...\n. Are you using SBT for building?\nNote that the master branch already holds the sources for spray 1.0-M1, which depends on Akka 2.0.\nIf you'd like to use spray with Akka 1.x you'll need to get release 0.9.0 from the respective tag.\n. We'll update the documentation soon.\nThe URL in your error message works for me, so it's likely a problem on your side...\n. No, not yet.\n. One note: This will only work for non-HTTPS connections!\nIn order to support proxied HTTPS we need a much more involved logic, which we currently do not want to get started with.\n. Fixed in master.\nAs of the 1.0-M3 the test DSL is completely redesigned and much improved.\n. Alex,\ngreat!\nThanks for this fix.\nHowever, could you apply your fix to the `develop` branch rather than the `master` and re-issue the pull request?\n(We follow the _git-flow_ branching model and do most of our work on the `develop` branch)...\n. Yeah, that might well be.\nWe haven't really been paying much attention to Windows compatibility.\nIf you'd like to provide a patch for fixing these things as well we'll happily merge it in.\nCheers,\nMathias\n. Thanks Alex!\nI had to cherry pick your EOL-fix commit, since you added the IPv6 Host header fix also to this pull request, which prevented it from merging cleanly...\n. Thanks, Age, for bringing this up.\nI've added a patch that makes unmarshalling a bit more lenient by allowing empty key=value pairs as in your example. They are now simply ignored. However, the fix you suggested is a bit too relaxed for me, since it silently discards content that someone might not immediately recognize as being invalid.\nIn these cases I am usually an advocate of \"fail early\" with a clear error message.\n. Hi Samira,\nthe reason that the IoWorker is not an actor is that it contains blocking code and needs to carefully manage exactly one dedicated thread, which is not directly possible with Akka 2.0 actors. Also see the Akka documentation stating that\n\n> Actors should not block (i.e. passively wait while occupying a Thread) on some external entity,\n> which might be a lock, a network socket, etc. The blocking operations should be done in some\n> special-cased thread which sends messages to the actors which shall act on them.\n\nAlso: I'd like to invite you to ask any questions on the [mailing list](http://groups.google.com/group/spray-user), so that others can benefit from the answers as well. This area here is purely reserved for issue management.\n. Yes, of course it'd be possible to block the current thread in an actor.\nHowever, Akka wasn't _designed_ for this use case. Actors in Akka 2.0 are scheduled via dispatchers that are entirely configured via the (external) configuration, so as to enable the easy fiddling with dispatching options.\n\nIn the case of the IoWorker we have very specific needs with regard to scheduling (one fixed thread) and do not require any of the usual actor features. Also, the current implementation, being extremely light-weight, performs better than a similar one implemented via an actor would.\n\nBy the way: the IO implementation in Akka 2.0 also uses a dedicated thread for managing a NIO selector.\n. Samira,\nas I suggested before, I am happy to answer any questions on the mailing list, not in this issue tracking area.\nCheers,\nMathias\n. Excellent, Mike!\nThank you for spotting and fixing this...\n\n(And congratulations to nailing you first pull request, I can't see anything wrong with it!)\n. Once thing: I cherry-picked your commit over to the `develop` branch, since thats the branch we are doing all of our developing work in (we follow the git-flow branching model).\n. Fixed in master.\nThe new spray-servlet module moves the responsibility for creation of the ActorSystem to the user, therefore she is free in naming the ActorSystem according to her needs.\n. Cherry-picked commit 1eca501da44087b0d6cef654139f79895d9f6162 into develop branch\n. #wontfix\n. Cherry-picked commit cbbb84d3038b39fcba71c4be98bc629e9d8e3f88 into develop branch\n. Jan,\nthanks for your pull request!\nDue to the much improved general architecture in 1.0-M3 (the \"master\" branch) I think you should be able to achieve what you are trying to do with your patch in a much simpler way.\nIf you have time to take a look I'd be interested in your opinion (we can also and gladly discuss things via Skype or email).\nCheers,\nMathias\n. spray already supports 3.0 compatible servlet containers.\n. Fixed in master.\nYou can now influence the error message rendered for such a parsing error with a custom ExceptionHandler.\n. Alex,\ngreat, thanks for finding and fixing the problem.\nI can't merge your pull request directly, since development has come a long way since 0.9.0.\nAre you hoping for a 0.9.1 release containing the fix or will you be able to upgrade to 1.0 anyway at some point?\n\nCheers,\nMathias\n. Thanks, Daniel,\nI'll take a look at your code as soon as I am ready to integrate it.\n. Daniel,\nthis is an interesting addition that I'd like to get into the \"master\" branch.\nHowever, since the new HttpConduit has slightly changed from the one you based your pull request on, you'd have to re-issue a pull request against the \"master\" branch.\nAlso, I'd like to have the \"cookie-jar\" functionality be part of the standard HttpConduit, not a sub-class. We should probably add a boolean setting \"keep-cookies\" to the ConduitSettings (default: on).\n\nThanks again for your contribution and cheers,\nMathias\n. Daniel,\nI think cookie-persistence across several connections of one HttpConduit would be a great feature to have.\nSo, If you'd like to contribute it, we'd sure find a way to bring it into the codebase...\n. Closing for the time being.\n. Please use [repo.spray.cc](http://repo.spray.cc).\nBtw: There is an [excellent mailing list](http://groups.google.com/group/spray-user) to which such questions are better targeted...\n. Ok, thanks for that comment.\nWe'll add a link to the README.\n. Thanks, Ismael, we'll merge ASAP....\n. Bj\u00f6rn,\nthanks for your pull request.\nUnfortunately it doesn't merge cleanly anymore, which is why I have applied your changes manually to the master branch. Could you try building the master once more in your environment and, potentially, resend a new pull request, if there are still tests that are red?\nCheers,\nMathias\n. Thanks, Bj\u00f6rn, for the confirmation.\nIncreasing the stack size is a good idea in any case with Scala....\n\nCheers,\nMathias\n\nOn 04.09.2012, at 20:38, tekener wrote:\n\n> Hi Mathias,\n> \n> every test is green now. By the way, i had some java.lang.StackOverflowError running \"sbt test\". The solution was to increase stack size to 2M.\n> \n> SBT_OPTS=\"-XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256m -Xmx1512M -Xss2M\"\n> \n> Greeting, Bj\u00f6rn\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. Thanks, Paul, for the report.\nIf the `content` directive matches it should \"cancel\" all rejections of the types that it produces itself, therefore avoiding the problem you were seeing in your use case. However, there was a small bug in the cancellation logic, that I just fixed.\n. feature/1.0-M3 is a private development branch.\nIt is not supposed to contain anything that it directly usable for you.\n. Right, thank you!\n. @pmlt This feature will not be part of the 1.0/1.1/1.2 final.\nIt will become available with the move of play onto the new akka-http sometime next year.\n. Fixed with latest rework of spray-can level parsing infrastructure.\n. Thanks for the report, I was able to reproduce the problem.\nWe'll look into it...\n. Thanks a lot for taking the time to report this issue!\n. Yes, I can reproduce the problem on my machine, but I'm not sure it's spray related.\nI'm seeing similar \"connection reset by peer\" errors with (the already patched version of) `ab` when not using persistent connections.\nHowever, as you suspected the chunking config is far from optimal in this modified example setup.\nWhen I increase the [chunk-size](https://github.com/spray/spray/blob/master/examples/spray-routing/simple-on-spray-can/src/main/resources/application.conf#L15) from 5K to 50K the test runs through smoothly, without dropped connections.\n. Alexandre,\nunfortunately we are bogged down with higher-prio work right now and therefore probably won't get to this in the next days. Sorry...\n. We'll soon upgrade to the new `SslTlsSupport` that has now been moved over to Akka 2.2 IO.\nThis will therefore have to be implemented in Akka.\n. Yes, looking at this again I also thing optional path elements will only add confusion and obfuscate what will actually be matched. Closing as \"not a good idea (any more)\"...\n. Great, thanks Age!\nI'll take a look at the conduit stopping as well...\n. Fixed in 1.0-M3\n. Age,\nwith 1.0-M3 out of the door I finally have time to look at your pull request.\nSeveral things:\n1. You are right, the scaladoc comment of the `headerValue` directive is wrong and needs to be fixed.\n2. I like your idea of providing an overload for the `headerValue` directive that simply takes the header name and extracts the corresponding header value. For this overload a `MissingHeaderRejection` makes sense.\n3. Your `HeaderUnmarshaller` has exactly the signature of the current `headerValue` directive. So, you are in fact trying to extract a custom value from a header. Since we deemed this the most common use case we shaped the `headerValue` directive into its current form. From looking at your code I see an improvement opportunity though: we can easily provide a `MalformedHeaderRejection`.\n\nSo, what I'd suggest we do is this:\n1. Add two predefined Rejection types:\n   \n   ```\n   MissingHeaderRejection(headerName: String)\n   MalformedHeaderRejection(headerName: String, error: Exception)\n   ```\n2. Add a `headerValue(headerName: String): Directive[String :: HNil]` directive, which rejects with a `MissingHeaderRejection` if the header is not present.\n3. Wrap the function execution of the other `headerValue` overload with `try/catch` logic producing a `MalformedHeaderRejection` upon any exceptions caught.\n\nThis leaves only one case to be taken care of: the `MissingHeaderRejection` in the non-String `headerValue` overload. Luckily there is an easy work-around that should solve the problem. With something like this:\n\n```\n`headerValue { case `Remote-Address`(ip) => ip } | reject(MissingHeaderRejection(\"Remote-Address\")\n```\n\nyou can easily produce a proper `MissingHeaderRejection`, if you really need it.\n\nWould all this work in your use case?\n\nCheers,\nMathias\n. > Then for point 3, could you perhaps show a use case example of what it would look like to use the non-String headerValue overload to create an instance of some custom class using the extracted header value? I'm having a hard time visualizing it.\n\nThe `cookie` directive you quoted is a good example. The [clientIP](https://github.com/spray/spray/blob/master/spray-routing/src/main/scala/spray/routing/directives/MiscDirectives.scala#L58) directive as well.\n\n> it would still be nice to have some kind of as[X] unmarshalling support to convert header (and cookie) values to some custom type, but that might not be needed by many people and it is easy to add yourself when needed.\n\nWell, I would generally encourage people to _not_ use the name-based overload, but rather the properly typed one for extracting value. It has the advantage of being type-safe in that the compiler will catch you, if you change the name or implementation of the header, whereas a regex-based approach is inherently unsafe.\n. Just to close this:\nThe ideas discussed in this issue are now implemented.\n\nCommits:\n- https://github.com/spray/spray/commit/d00fde58a6e54ee87fd37f206fb83c03ca496443 (master branch)\n- https://github.com/spray/spray/commit/dcfc944eec51eaceec39d2c870eb0aba8695c107 (release-1.0.x branch)\n. No, more something like this:\n\n```\npath(\"foo\") {\n  get {\n    proxyTo(\"http://oldapi.example.com\")\n  }\n}\n```\n. Probably not a good idea, as feedback will be too scattered.\n. Also add support for a \"confirmed\" close, i.e. delay the dispatching of a `Closed` event until the peer has ack'ed the FIN.\n. To quote Age:\n\n> I created a little alias for adding the proper gzip request-response behaviour to my own app:\n> \n> def withGzipSupport = (decodeRequest(NoEncoding) | decodeRequest(Gzip)) &\n>                      (encodeResponse(NoEncoding) | encodeResponse(Gzip))\n> \n> You could argue that this should be the default for all apps that don't explicitly specify an encoding because they would be more resilient to different clients. Any reason why this is not the case?\n. Thanks, Paul!\n. Thanks, Paul, for explaining your reasoning in detail. You are right, we'd like to have `routingSettings` be properly overridable in order to give users another way of supplying their own settings, in addition to simply putting them into the `application.conf` that is loaded by the default classloader. In that regard having the member be a `val` was a bug.\n\n> Now for a question: do you have an ETA for when an M5 might appear, and more generally, for when we might expect 1.0 and 1.1?\n\nWe'll set up nightly builds today, so that you should have access to a fixed build very shortly.\nWith regard to the ETA of 1.0/1.1 final: the feature set is pretty much complete (apart from smaller things like a few additional directives that we are going to add). Currently we are planning for a first RC in the next weeks, definitely before the end of the month.\n. now in master (as part of the akka.io backport from Akka 2.2)\n. According to the most recent WOFF spec recommendation the media-type should be `application/font-woff`.\nSee: http://www.w3.org/TR/WOFF/#appendix-b\n. Great, Jacobus!\nI think the community will certainly applaud the making public of all additional examples, so S4 stack example is going to be much appreciated, no doubt.\n\nI'm not sure we'll have to resources to maintain your example ourselves, but this shouldn't keep you from publishing it!\n. Great!\nThanks for the pull request!\n. Jan, let me know if this fix doesn't work for you...\n. cannot reproduce\n. Why?\nWhat was the idea here?\nUnclear due to crappy ticket label/description.\n. also see [this thread](https://groups.google.com/d/topic/spray-user/xmZQV5OT8Vg/discussion)\n. Sam,\nnote that that the `spray.can.client.ssl-encryption` setting is gone without replacement since M6.\nOne single HttpClient instance can now handle encrypted as well as unencrypted connections.\nThis should simplify a few things for you...\n\nCheers,\nMathias\n. sendReceive depends on an HttpConduit, which must be explicitly created.\nCan you show a snippet of what you'd like your code to look like from the user perspective?\n. > HttpConduits can be safely shared, right? As in, sharing them won't bottleneck how many requests we can make with the http client?\n\nCorrect. Sharing HttpConduits is no problem. However, since they are bound to a specific host and port in many cases sharing them will not be as useful as sharing HttpClients, which can handle thousands of connections to many different hosts.\n\n> From a user perspective, the conduit thing is just boilerplate \u2013 if you could add a sendReceive method onto the HTTPClient that would be perfect, or allow the HttpConduit to take in o aHttpClient\n\nNo, the HttpConduit is not just boilerplate. It manages a pool of connections to a specific host and thus constitutes another (different) layer of abstraction.\n. We are currently in the process of significantly refactoring the architecture of the client-side actor organization in all layers (spray-io, spray-can and spray-client). The current setup is built too much with symmetry to the server-side in mind and thereby presents a somewhat unnatural and slightly quirky solution.\n\n@Sam: Let's revisit your ideas once the new architecture is out, it might well be that your issue will have been solved in the process.\n. Basic message parsing has been updated.\nHowever, I'll leave the ticket open until we have revamped the HTTP header parser as well (probably with the advent of parboiled2).\n. This has happened with the port to Akka HTTP and will not be happening anymore (apart from bug fixes) in spray.\n. Please check out [this thread](https://groups.google.com/d/topic/spray-user/bFZ7YhWAB3I/discussion) on the mailing list for more information about this...\n. Thanks for the report.\nThis issue is already fixed in the `master` and `release-1.0.x` branches.\n. Ok, we'll probably add a dedicated rejection for rate limiting once it is implemented...\n. Well, on the client-side it's your application that triggers the establishment of new connections. So it should be easy to limit that purely in your application layer. Or would you like spray-can to deny opening a new connection if your application explicitly demands to do so?\n. Since it's not hard to queue new client-side connection requests in your application layer we are going to focus on the server-side with regard to this feature.\n. Now in master (via backport of Akka 2.2 akka.io functionality): config setting: `akka.io.tcp.max-channels`\n. Fixed with https://github.com/spray/spray/pull/351\n. Ok, thanks.\nChanging the `clientIP` directive would work but would also change the semantics.\nCurrently we have a strict ordering of preference with regard to the different source headers. Changing to `optionalHeaderValuePF` would take the first of _any_ of the three headers and therefore not be independent of header ordering.\n. Is this still a problem with 1.0-M8.1? We'll need to look into it...\n. Actually, the first fix is not good enough... we need something better.\nI'm on it...\n. `DateTime` is a model for \"HTTP times\" as defined by [RFC 2616 section 3.3.1](http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.3.1) and as such neither carries time zone information nor milliseconds. If you are searching for a DateTime implementation supporting milliseconds the one from `spray.http` is not the one you are looking for.\n. Great!\nThanks for these fixes!\n. Great. Thanks!\n. fixed in master (via new custom URI parser)\n. What kind of JDK are you compiling against?\n. I am on the exact same environment under OS/X, so I'm assuming you either have some ivy cache problem or some other \"litter\" from before.\nTry again after a \"git clean -xdf\" and possibly cleaning out everything spray related from your ivy2 cache. \n. Can you temporarily fix the collision by commenting out the offending import and insert a\n\n```\nprintln(Date.getClass.getName)\n```\n\n?\n. Another question:\nWhy do you have to build spray yourself?\nCan't you use either a milestone or nightly build?\n. Since this feature is not that hard to implement I don't think it is too far out.\n(If you send us a pull-request it might be available sooner... :) )\n. Thinking about it again I'd recommend to not start working on this issue as your first contribution to spray.\nWe are currently in the process of revamping this part of spray-can, so we might end up stepping on each others toes. We'll take on this issue for M8 soon ourselves...\nStill, thanks for offering! Much appreciated!\n. Yes, we'll get to this very soon.\n. Chris, this feature is important for several big users, so we have marked it with high priority.\nIf you consider your use case a \"big user\" as well, please shoot me a mail. We might be able to close this issue today or tomorrow if required...\n. Thanks, Mathias, for reporting!\n. When exactly is this happening?\nSimply setting `remote-address-header = on` doesn't trigger the problem for me with either the `simple-http-server` or the `on-spray-can` example.\n. Fixed in master\n. Thanks!\n. Thanks, Chap!\n. Jason,\nthis is the commit you are looking for:\nhttps://github.com/spray/spray/commit/ed23fc5dfffb8527c18303ea023fe5d56b8412dd\n\n(Please use the mailing list for questions like this!)\n. Thanks!\n. Available in master: https://github.com/spray/spray/blob/db2e32054c8c9b896f2c1864af143bc4960b78ef/spray-io/src/main/scala/spray/io/Pipelines.scala#L42\n. I agree. JSONP is incompatible with the proposed prevention of JSON hijacking.\nHowever, the question remains whether we should have the JSON marshallers produce the prefix for their outputs.\nHere I would argue that apparently modern browsers don't expose this JSON hijacking vulnerability anymore and the issue is therefore not worth it anymore.\n\n@analytically: would you therefore be ok with simply closing this ticket?\n. HTTP message rendering has be completely overhauled, this bug should not appear anymore.\n. Thanks!\n. Already fixed by commit ae61a01c03eb16932d52e5fb07d6c2fdee3aa73d\n. I think it'd make sense to fold this into [this PR/issue](https://github.com/spray/spray/pull/311) and devise a single, joined first cookie jar implementation, fulfilling all key requirements. WDYT?\n. Ok, sorry for taking so long in coming back to this.\nLooking at this patch again, couldn't we rely on the existing Cookie model for session cookies as well.\nIn what way do sessions cookies differ from \"ordinary\" cookies?\n. Magnus, how would you like to move on with this PR?\n. Ok, Magnus, thanks.\n\n@enetsee, how would you like to move on with this?\n. Closing for now.\n@enetsee, if you don't agree with folding this into #311 please let us know.\n. Fixed in master.\n. fixed in master\n. Fixed with latest rework of HTTP model in master branch.\n. Stefan, sorry for taking so long for reacting to your pull-request.\nI think it falls a bit short with regard to issue #216.\nThe HTTP spec requires that header values (and value parameters) be quoted if they contain reserved characters.\nTo simply never quote the cookie values opens up the risk of creating illegal HTTP messages.\n. [RFC 6265](http://tools.ietf.org/html/rfc6265#section-4.1.1) defines the cookie value as such:\n\n```\ncookie-value      = *cookie-octet / ( DQUOTE *cookie-octet DQUOTE )\ncookie-octet      = %x21 / %x23-2B / %x2D-3A / %x3C-5B / %x5D-7E\n                   ; US-ASCII characters excluding CTLs,\n                   ; whitespace DQUOTE, comma, semicolon,\n                   ; and backslash\n```\n\nSo, you are right in that the quoting doesn't actually expand the character class allowed in the cookie value, so we might as well not render it.\nWe are currently in the process of revamping spray's HTTP model and will change cookie rendering to omit the quotes altogether. Additionally, we'll enforce the requirement of cookie values not containing illegal characters, since there appears to be no standard quoting mechanism.\n\nUp until this is implemented I'll leave this ticket open.\nThanks for reporting!\n. Closed with latest rework of HTTP model in master branch.\n. fixed in master\n\nFrom now on all requests leaving the spray-can layer are already fully parsed.\n. Yep. These methods are no more...\n. fixed in master via new URI parser\n. Fixed in master\n. Fixed in current master\n. Sorry for taking so long in responding.\nI don't quite understand the problem you are having with the current implementation of the `optionalCookie` directive.\nCan you elaborate?\n. Ok, I understand now.\nThe `optionalCookie` directive leaking `MissingCookieRejection` is indeed a bug.\nI just pushed [this commit](https://github.com/spray/spray/commit/7e5dd8d798e0a38577a360794d010e6964db0629) with a fix. Thanks for finding and reporting!\n. Provided via `spray.can.server.remote-address-header` setting. Closing.\n. You \"got error message from google with BAD header\"?\nWhat error did you get exactly and from whom?\nCan you show the request that is going out and the response that is coming back?\nWhat exactly do you want to ignore?\n. Timothy,\nI am sorry, but we do not have the capacity to analyze and debug your code.\nIf you would like to report an issue for spray (which we'd be thankful for) we need to ask for a clear description of what exactly it is that you are experiencing, in what type of environments and how it differs from what you would expect.\nWe need to quite significantly prioritize our work on spray, vague bug reports therefore have only very slim chances of actually triggering any action from our side...\n. The warning can be disabled with this config setting:\nhttps://github.com/spray/spray/blob/master/spray-can/src/main/resources/reference.conf#L237\n. Sorry for taking so long in responding to your PR.\nIn the meantime we have support for `become` in PipelineStages ourselves (https://github.com/spray/spray/blob/db2e32054c8c9b896f2c1864af143bc4960b78ef/spray-io/src/main/scala/spray/io/Pipelines.scala#L42) and will probably be moving onto the new pipeline infrastructure coming with the new akka-io soon.\n\nSo this PR will not get merged.\nStill, thanks for contributing!\n. Ok, sorry for the delay in responding.\nWe would like to test the travis integration, so would you be able to update the travis config to Scala 2.10.1, squash your commits and rebase onto the current master?\n. We don't need `-Xss6M` for spray, `-Xss3M` is sufficient. Since the tests create quite a few threads this may already help. Can you try `-Xmx2536M` as well?\n. Interesting. The reason must be related to build process configuration.\nI just checked our Jenkins, we use these simple flags for our nightly builds (which run solidly under Debian Squeeze):\n\n```\n-Dfile.encoding=UTF8 -Xmx2048m -XX:MaxPermSize=512m -Xss2m clean test:compile test\n```\n\nOn my own box (OS/X 10.8.3) I run sbt with these flags:\n\n```\n-Dfile.encoding=UTF8 -XX:MaxPermSize=512m -Xms512m -Xmx3g -Xss3m -XX:+CMSClassUnloadingEnabled -XX:+UseConcMarkSweepGC\n```\n\nNo issues either.\nCan you find out where exactly the process hangs? (e.g. with `jps` / `jstack`)\n. Gilles, do you have any news on this?\n. Thanks a lot, Gilles!\n. Gilles, we just applied a fix (https://github.com/spray/spray/commit/20c4dfd5b54bee617b19a8ae0c3860d3b77fb68b) which seems to fix the seg-fault problem. We'll see how things go...\n. If you are not using PerConnectionHandlers (but a `SingletonHandler`) this is not an issue.\n. You can use the SingletonHandler with the `detachTo` directive or use some other way to offload route execution from the main handler actor. So, don't worry, I don't think PerConnectionHandlers really are required for this.\n. In M8 `PerConnectionHandler` handlers don't exist anymore. With the latest API it is completely up to the user to decide when and how application-level service actors are created and registered, per-connection service actors are one of several possible alternatives.\n. Fixed in M8.\n. Yeah, I think that's good enough.\n. Moved to https://github.com/spray/spray/issues/266\n. Sorry for the delay in looking at this.\nCould you rebase on top of the current master?\n. With the move to akka we'll \"automatically\" move to sonatype. Currently we don't plan to expedite this for the current spray release cycle.\n. Ok, this ticket has just been prioritized.\nWe'll have the spray 1.0.0 / 1.1.0 / 1.2.0 final artifacts on sonatype / maven central soon.\n. Ok, we have just pushed the 1.0.0, 1.1.0 and 1.2.0 artifacts from repo.spray.io to sonatype releases.\nIt'll take some time to sync up with maven central but in a few hours they should have arrived.\n. hmm... it's here:\nhttp://oss.sonatype.org/content/repositories/releases/io/spray/\n\nbut not (yet) here:\nhttp://central.maven.org/maven2/io/\n\nI'll wait another day before investigating the reasons for the missing maven central sync...\n. Yeah, just realized that after the first artifact promotion we need to explicitly request maven central sync (which I just did). Hang in there...\nSorry for the delay!\n. Ok, just got word that maven central sync has now been activated and will complete in at most 2 hours... yay!\n. Probably not a good idea.\nThe \"deeper\" model we have now somehow feels more structured.\n. Thanks, Johan, we'll look at your code as soon as our time permits...\n. Sorry, Johan, for taking so long to get to this.\nIt seems to me that this patch has now been superceded by Derek's (https://github.com/spray/spray/pull/359).\nWould you agree?\n. Ok, Johan, thanks for responding. I'll close this PR then.\n. Sorry for the delay in looking at this.\nWould you be able to squash your commits and rebase onto the current master?\n\nAlso: I'd prefer it if we disallowed the \"empty\" case like this:\n\n```\ndef setCookie(first: HttpCookie, more: HttpCookie*): Directive0 = ...\n```\n\nThis way we can simply extend the existing directive and don't even have to add a new one.\n. Note that you don't have to create a new pull request. Simply `push -f` into your fork and the existing pull request will be updated.\n\nClosing here and continuing with #295\n. already covered by `RespondWithDirectives`\n. Marek,\nthank you for submitting this PR and sorry for not reacting sooner. We've been quite swamped with high-priority stuff these last weeks.\nHowever, I'd like to merge your PR now. Would you be able to update it to the latest version of the master?\nThanks again!\n. Excellent. Keep us posted!\n. Can you simply squash your commits, rebase them into the curent master and `push -f` the resulting commit to your fork? Otherwise the merge is going to create a mess on our side...\n. Thanks, Marek!\n. Unclear.\nWhat is this supposed to mean exactly?\nPlease be more specific the next time you add a ticket!\n. Excellent observation, Nikolas.\nWe'll get that fixed ASAP.\nThanks for the report!\n. Thanks, Heiko!\n. `/api?query=\u0430\u0441\u043f\u0435\u0440\u0438\u043d` is not a valid URI according to [RFC 3986](/api?query=\u0430\u0441\u043f\u0435\u0440\u0438\u043d), which is why you are seeing the exception. Cyrillic characters are not allowed in URIs, they need to be percent encoded.\nIf you are trying to construct a URI its unencoded components you should use the respective `Uri.apply` overload.\n. Yes, we cannot move to Java7 yet.\nWe have to stick to Akka and Scala which are still Java6 compatible.\n. Sorry for the delay!\nCould you update this PR to the latest master?\nI have just merged in support for json4s and improved the tests. It should now be even simpler for you to integrate the support for play-json.\n. Luis,\ncould you sent us a signed [CLA](http://spray.io/project-info/contributing/#contributor-license-agreement-cla) and rewrite the commit message to\n\n```\n+ httpx: add support for play-json (un)marshalling\n```\n\nWe'd like to get this merged ASAP so we can start cutting RC1.\n. Great, thanks!\n. Thanks, Luis!\n. Yes. It'll be in RC1 which we'll release early next week.\n. Thanks, Heiko!\n. Fixed with latest rework of spray-can HTTP parsing layer\n. Will be solved via [this](https://github.com/jrudolph/spray-aspects).\n. Make sure to also cover the problem discussed here: https://github.com/spray/spray/issues/239\n. In order to do this properly the message protocol must be extended.\nAFAICS the best way would be to have the HostConnector accept incoming `ChunkedRequestStart` messages, send them out to a connection and spawn a per-request sub-actor (the `RequestChunkSender`), which handles the rest of the chunked request.\nThis `RequestChunkSender` would then reply back to the user (i.e. the sender of the `ChunkedRequestStart` message) in order to make its ActorRef known so that the user can then send `MessageChunks` to it, which will be dispatched across the connection.\n. Also add option for accepting `;` as separator in query.\n. As this is not a terribly hard thing to add it go in quite quickly.\nMaybe you'd like to take stab at a pull request ... :)\n. Endre, just out of curiosity, what do you use spray for? :)\n. Interesting! Let us know if you find anything you'd like to see improved (except for closing this ticket of course... :)\n. superceded by #365\n. I think you solution falls a bit short.\nAlso it'd be great if you could add some tests...\n. can you please squash your commits into one single one, so it's easier to review (no new PR necessary).\n\nAlso, I'd propose changing the approach and making `ASCII_PRINTABLE_CHARS` a compound mask that is built up from disjunct \"groups\". This way expressions like `ASCII_PRINTABLE_CHARS & ~(AMP | EQUAL)` will work again and be easier to read.\n\nAnd please rename `ASCII_PRINTABLE_CHARS` to `VCHAR` (I realized that I wasn't consistent before either, character groups should be names with singular name (so VCHAR rather than VCHARS), so maybe you can rename the preexisting groups `QUERY_FRAGMENT_CHARS` and `PATH_SEGMENT_CHARS` to their singular counterparts in the process).\n\nThanks for contributing! Much appreciated!\n. Why would you have to add 43 new groups?\nWouldn't we need just one more group that adds all visible charts we haven't yet marked and then \"or\" the bits of all the masks making up a VCHAR? (maybe there is something here I'm not seeing yet).\n. This looks quite nice already, thanks!!\nApart from the comments I think the only thing missing is a `strict: Boolean` parameter to the `UriParser` constructor and the respective `Uri.apply` overloads, so people can decide whether they want relaxed or strict URI parsing.\n. (And I think we are fine with non-strict mode being the default...)\n. This looks great now! Thanks a lot!\nDon't worry about the rest, I can easily finish up the integration of the `strict` flag myself...\nJust let me know, when you are ready.\nThen I'll merge...\n. Thanks, Njal, will merge ASAP...\n. Yes, it does. Sorry for the delay with merging...\nI was awfully busy this week.\nWe'll definitely get this in next week.\n. What about a config setting which effectively disables query parsing altogether?\nSo instead of a parsed query structure you simply get one single raw query string, which you can then subsequently parse yourself. spray would simply treat all characters after the `?` and before a potential `#` as belonging to the query string...\nWouldn't that solve the issue and still keep our architecture clean and lean without adding to much application-specific special-case-handling?\n. Ok, we'll include a new config setting `spray.can.parsing.uri-parsing-mode` with the following defined settings:\n- `strict` for what we have now\n- `relaxed` for what this PR provides\n- `relaxed-with-raw-query` for additionally disabling parsing of the internal query structure\n\nHopefully this is flexible enough for most applications.\n. In master now...\n. As of [this commit](https://github.com/spray/spray/commit/9bd96f345d9e6605decd4fcaa9c4187e0da528ae) the spray master branch (incl. the respective nightly builds) are already on Scala 2.10.1.\n. The `DateTime` defines an actual point in time, i.e. in calendar \"real-time\".\n`System.nanoTime` has no relation to the calendar and therefore cannot be used to determine an absolute point in time. It can only be used for measuring relative time periods.\nTherefore I don't see the `DateTime` type in scope of this ticket.\n. Currently this issue is not high priority for us, so I'd like to hold off with actually implementing it until the respective parts of the codebase have stabilized further. Especially the spray-can HttpServer frontend will undergo at least one more major refactoring in the next weeks...\n. Yes, the idea is to give you a way to still write low-level routing code, but make it more explicit.\nGenerally people should try work with the directives, which, in most cases, should give you all the flexibility you need.\n. It is fixed in the port to akka-http.\nhttps://github.com/akka/akka/blob/release-2.3-dev/akka-http/src/main/scala/akka/http/server/package.scala#L11\n. Wouldn't something like\n\n```\nparameters('view.as[View] ?) { view =>\n}\n```\n\nbe more readable?\n. All you need to do is to make your `fromString` on the `View` companion object implicit, and things should work.\n`View` doesn't have to be a case class for this to work.\n. [issue description replaced with this comment]\n. > I would need to implement 2 entry points doing the same thing except that one is auto-chunking mode and the other regular upload.\n\nWhat would you do in \"regular mode\" when a client sends a chunked request?\nAuto-chunking simplifies the application side logic because you only have to deal with one kind of request: chunked ones. So, I'm not sure that enabling auto-chunking per path would really buy you anything...\n. > will incoming-auto-chunking-threshold-size be supported by spray-servlet?\n\nI suspect not.\nThe only way for us to get to the request entity is via the `httpServletRequest.getInputStream`, which is a blocking abstraction. So I think it's hard for us to send a \"virtual chunk\" up to the application whenever a bunch of new bytes have arrived on the connection because we simply cannot know if and when this is the case.\nAll we see on the inputStream is EOF, which signals that we have read the complete request.\n. Yes.\nIn that regard my previous comment only applies to \"large\" requests, with \"large\" being defined by your application config. You could configure `incoming-auto-chunking-threshold-size` to `0` however to make it apply to all non-empty requests.\n. > I was thinking that if the Content-Length was over the auto-chunking threshold ...\n\nYes, but what about the cases where the incoming request is already a chunked one and there is no `Content-Length` header?\n. > Correct, that's what I have already done but I would also like the option to handle a different path's POST request in regular mode and that's no longer possible ...\n\nNote that the client might decide to send even \"small\" requests with `Transfer-Encoding: chunked`, so you really cannot make the distinction in the way you'd like to.\nCan you give us some more insight into why you'd like to handle small uploads differently from large ones?\n. > I would imagine that if the client enables Transfer-Encoding: chunked mode that it knows that that particular endpoint is capable of handling chunks, otherwise it would be a regular POST request.\n\nUnfortunately this is not the case. HTTP/1.1 _requires_ all servers to be able to handle chunked requests and all clients to accept chunked responses. Otherwise they cannot call themselves HTTP/1.1 compliant.\nSo, as a server you will have to be able to deal with incoming chunked requests no matter what.\n\nWe do realize that the current support for non-aggregated chunked requests is suboptimal in spray-routing. We are working on getting this improved. Ideally you should be able to use spray-routing on non-aggregated chunked requests pretty much as you would otherwise.\n. Agreed.\n. @acruise server-side chunkless streaming is already available for some time\n. Yes, use [this setting](https://github.com/spray/spray/blob/4d711d8e148a710c87f47cf73150cc9f15dcd98d/spray-can/src/main/resources/reference.conf#L91) to enable.\n. Mike, yes, that's good.\nNow, could you squash the three commits into one?\nThen we are ready to go...\n. Thanks!, Mike!\n. Thanks for reporting!\n. Thanks for finding and reporting, Ian!\n. Great pull-request with an excellent test-coverage...\nThank you very much, Adam!\n. After you have applied the minor tweaks I'd be happy to merge!\n. Thanks again, Adam!\n. Yes, this is the originating thread:\nhttps://groups.google.com/forum/#!searchin/spray-user/identity$20encoding/spray-user/0MjoHuFm8Jk/8eZ51MDjokMJ\n. Are you using keep-alive connections?\nWhat does \"benchmarking with ab (Apache Benchmark) and Siege fails\" mean?\nWhat error message are you getting?\nWhat exact command line arguments are you using?\n\nFrom your report it's really hard to reproduce the problem on our side.\nIf you are not sure that what you are seeing really is a bug please turn to the mailing list before filing a ticket here...\n. what happens when you add the `-k` switch to your `ab` command line?\n. You can also try https://github.com/lighttpd/weighttp or https://github.com/wg/wrk for load generation.\nThe problems concerning non-keep-alive connections with `ab` are known. I don't know whether siege suffers from similar symptoms.\n\nClosing until it becomes clearer that the issue really is spray-related.\n. This issue should be fixed in the last nightly build. Can you verify?\n. Great, thanks, we are looking forward to a test case.\nBy the way: don't worry about the server-side, we can easily supply one ourselves.\n\nClosing this issue now, since the \"illegal chunk termination\" appears to be unrelated.\n. Thanks, Eugeny, for this proposal.\nWe just pushed a similar but slightly extended solution into the master branch...\n. Thanks, Eric, for finding and reporting!\n. No, but it will be in tomorrows one.\n. I have an idea of how to greatly increase the power of Marshallers and Unmarshallers, which appears to be not that hard to implement and which should also allow us to close this ticket:\n\nCurrently Marshallers marshal only to `HttpEntity` instances and Unmarshallers unmarshal only from `HttpEntity` instances. Suppose we introduced this additional abstraction:\n\n```\ncase class HttpMessageEssentials(entity: HttpEntity, headers: List[HttpHeader] = Nil)\n```\n\nand these additional kinds of (Un)marshallers:\n- `FromMessageEssentialsUnmarshaller[T]`\n- `ToMessageEssentialsMarshaller[T]`\n- `FromRequestUnmarshaller[T]`\n- `FromResponseUnmarshaller[T]`\n- `ToRequestMarshaller[T]`\n- `ToResponseMarshaller[T]`\n\nThese type classes would form a logical chain in which more specific by default delegate to less specific ones.\nFor example, _spray-routing_ would require a `ToResponseMarshaller[T]` for completion with an object of type `T`. If there is one available all is good. If not the compiler would look for a `ToMessageEssentialsMarshaller[T]`. If none is directly available it would construct one from an implicitly available `ToEntityMarshaller[T]` (which would be identical to the existing `Marshaller[T]`. This way the user can pick the right level for writing custom Marshallers and Unmarshallers. The Entity and Essentials levels would work for the client- and server-side, whereas the most specific level (to and from requests/responses) only on one side.\n\nThis new system would nicely extend the existing infrastructure and existing code would continue to work. It would also allow for setting headers and even status codes from marshallers.\n\n@jrudolph and everyone else: WDYT? Does that sound reasonable?\n. > +1. Do you think this will make 1.1/1.2 final?\n\nYes. This is important enough and will be implemented very soon.\n. Yes, by implementing either a `FromMessageEssentialsUnmarshaller` or a `FromRequestUnmarshaller`/ `FromResponseUnmarshaller` your unmarshalling logic will have access to all message headers.\n. No worries, Age!\n. Addition:\nWhen reworking the marshalling infrastructure we should also think about how to best model composed marshallers, which delegate to sub-marshallers, e.g. because `Foo` can be rendered in three different representations (like `application/json`, `text/xml` or `text/plain`).\n\nSee also this thread: https://groups.google.com/forum/#!topic/spray-user/xtawBl2Ytx4\n. Thanks, Vincent!\nNote that you don't have to create a new pull-request when you'd like to make changes to an existing one.\nJust `push -f` into your fork and the pull-request will be updated.\n. Thanks, Andr\u00e9!\n. Please check out the mailing list for a discussion of this topic.\n(https://groups.google.com/forum/?fromgroups#!searchin/spray-user/swagger)\n. Great, thanks for finding and reporting this problem!\nWe'll get it fixed ASAP.\n. Yaroslav, thanks for this proposal.\nI think it's a good idea to allow the setting of the request timeout also for spray-servlet.\nHowever, since it's already supported for spray-can (https://github.com/spray/spray/blob/master/spray-can/src/main/scala/spray/can/Http.scala#L89) we should not introduce a separate command but rather move the `SetRequestTimeout` class from spray-can into spray-http and also use it from spray-servlet.\n\nAdditionally, while we are at it we might want to do the same for the `SetTimeoutTimeout` command.\n\nWould you like to take a stab at this as well?\n. Yaroslav,\nwe should not add any dependencies between modules for this. Adding dependencies is a major change that requires careful thought, such a small extension like the one we are doing here does not warrant a dependency addition.\n\nHow about we solve it like this:\n1. Move `SetRequestTimeout` and `SetTimeoutTimeout` to spray-http, drop the `extend Command`\n2. Introduce a `private[server] case class SetTimeoutCommand(underlying: AnyRef) extends Command` type in the `spray.can.server` package\n3. In the `HttpServerConnection` override the `running` method to wrap incoming `SetRequestTimeout` or `SetTimeoutTimeout` messages in a `SetTimeoutCommand` (look at the `HttpClientConnection` for an example, on the client-side something similar is already done)\n4. Make `spray-servlet` react to `SetRequestTimeout` and `SetTimeoutTimeout`\n\nI think this should work and keep dependencies clean.\nWDYT? \n. Could you squash your commits into a single one and `push -f` into your fork?\nThis would make the review a bit easier (and clean up the history).\nThanks!\n. Sorry for not having been clear enough before.\nI think all we need is a generic `case class CommandWrapper(cmd: Any) extends Command` type somewhere. It's probably best to put it right into the `spray.io` package (package.scala).\nThen we don't need to duplicate all the command types from spray-http in spray-can again and can simply match like this:\n\n```\ncase CommandWrapper(SetRequestTimeout(duration)) => ...\n```\n. Because we need to send e.g. a `SetRequestTimeout` instance through the command pipeline of the HttpServer or HttpClient. In order to do that we simply wrap it in a `CommandWrapper` instance (whose only member actually should be an `AnyRef` instance).\n. We'd like to uphold a certain degree of type-safety. Having `Command` and `Event` pipelines separated allows the compiler to detect basic \"wrong-message in wrong-pipeline\" situations.\n. Just match like this:\n\n```\ncase CommandWrapper(SetRequestTimeout(duration)) => ...\n```\n. The latter please, it allows us to control what events the pipeline will see. Also, it makes sure that misguided actor messages remain unhandled (and appear, for example, as an `UnhandledMessage` log entry).\n. Apart from the few small comments this looks ready to be merged.\nThanks, Yaroslav!\n. Thanks, Yaroslav!\n. And once more: Thanks, Mike! Great contribution from your side...\n. Nice catch! Thanks for finding and fixing.\nWould you be able to add a test for the this bug as well? (so as to make sure we don't see it again)\n. Gary, I just merged Mikes fix for issue #298, which I think also fixes this issues.\nCan you try with todays nightly build? (just triggered it: 1.1-20130523)\nClosing for now...\n. Thanks, Mike!\n. Gary, can you provide a simply failing test case that you'd like to see working?\nI think it's easier to properly understand the issue by looking at some example code...\n. The cache implementation is intentionally restricted to a maximum capacity of 255 headers (in total!), so it makes sense to not have it be flooded with headers on only one type.\nAdditionally the header cache is currently used on a _per-connection_ basis, for which it is unlikely to see more one single User-Agent header value.\nWe are still thinking about how to best enable sharing of header cache entries across connections, but so far this is not implemented. I'll therefore close this issue (and open issue #306 instead).\n. Since the new parboiled2-based header parser in Akka HTTP is now a lot faster than the one in spray I'd consider this optimization much less important. And since there is probably a lot more optimization potential in other parts of the stack I am closing this ticket for now.\n. In order to implement #365 we'll have to introduce Akka as a dependency for _spray-http_.\nIn a first step we'll probably just do that but leave http & httpx separate.\n. Ok, understood the issues. We'll get it fixed.\nThanks for reporting, Mathias!\n. Can you describe you current use case in some more detail? What exactly do you need?\n. I see, just wanted to make sure we fully understand your motivation.\nI think this is a valid request and should be implemented. Thanks for putting in this ticket!\n. Submitted a PR fixing this. I'll probably be reviewed (by @jrudolph) and merged today, so tomorrows nightly should allow you to access your version parameter.\n. We recently had some changes in related files, can you rebase onto the current master and `push -f` into your fork?\n. Could you clean up your commit history a bit? (If you want, you can also squash everything into a single commit).\nThis makes reviewing a bit easier.\n\n(Regarding the travis failures, let's ignore them for now...)\n. Since the CookieJar is essentially just a map we could have it extend `collection.immutable.Map` so we get the full functionality of the `Map` interface. Also, I'm not sure the map should be `Map[Uri, List[HttpCookie]]`.\nWouldn't `Map[Uri.Host, Set[HttpCookie]]` be more appropriate? (Also see: http://stackoverflow.com/questions/1612177/are-http-cookies-port-specific)\n. Johannes raised some very good points here, which lead me to think that an overly simple implementation might actually do more harm than good. It also shows that there is definitely value in solving this problem of client-side cookie management correctly for our users since it's not easy to come up with something proper oneself.\n\nSo, a \"cookie-jar\" implementation would be great to have but we probably shouldn't settle for anything too simplistic. Maybe the first step should be to agree on a first (minimal) set of requirements/features before thinking about how to best implement them?\n. We are currently in the process of moving all open spray tickets over to the Akka repository, as all future development (apart from bug fixes) will happen there.\nEven though this PR has never made it into spray we'd still like to keep it as a reference to turn to when attacking ticket https://github.com/spray/spray/issues/772.\n\nThanks to everyone, who has contributed to this ticket! Your work is not lost!\n. If we really need a dependency onto Akka 2.2 you should issue your pull-request against the `release-1.2.x` branch, which is already built against Akka 2.2. Upgrading Akka across incompatible versions (like from 2.1 to 2.2) is not something that can be done \"on the side\".\n. Brian, note that the `release-1.2.x` branch has been renamed to `release/1.2` in the meantime...\n. Brian,\nwould you say that this PR has now been superceded by @mpilquist's (https://github.com/spray/spray/pull/329) and can therefore be closed?\n. @topping: Brian, how would you like to move on with this PR?\n. @topping Would you agree that this PR can be closed for now?\n. Stepping back I'm not sure the issue you are trying to fix here is actually an issue.\nThe `DateTime` abstraction we have in `spray-http` intentionally has a precision of 1 sec, not 1 ms.\nSo `DateTime(clicks).clicks` will differ from `clicks` in 999 out of 1000 cases.\nWhy is this a problem in your use case?\n. Martijn,\nI'm sorry if my comments have offended you, I do apologize should that be the case.\n\n> DateTime(clicks) seems like it does support millisecond precision. If it doesn't, it would make more sense to accept seconds rather than milliseconds, and have the user do any rounding or truncating they may want to do themselves.\n\nI understand your point here. Having `DateTime.apply` take seconds instead of millis would be somewhat cleaner on the one hand but would also make the API more difficult to use and be error-prone in that people might just supply `System.currentTimeMillis` because that is the \"standard\" way of getting the current system time. So we have to trade off internal cleanliness against API usability.\n\nMy take now would be to improve the scaladocs of the `DateTime` type to make very clear what it is that it models: a Date + Time abstraction with second precision and without timezone support.\n\n> I figured by the way that those two heap allocations would not be a big deal.\n\nIn many cases they wouldn't be. However, in addition to the immutability we wrote this `DateTime` implementation mainly because we wanted something fast and lightweight. `DateTime` instances are created at least once for every HTTP request or response _spray_ instantiates and as such are somewhat in the \"hot path\". In these areas of the code base we tend to fight for every allocation, otherwise it'd be hard for _spray_ to reach/maintain its position of being among the very fastest of JVM-based HTTP stacks.\n\nWe happily accept pull-requests that make _spray_ better (and not just bigger or different). So, if you show what use cases would benefit from having `DateTime` provide milli-second precision (rather than the second-precision that is all that's required for HTTP) it'd be easier for us to follow you here.\n. > ... it was being outperformed a factor 300 by the original code ...\n\nInteresting. I wouldn't have expected _such_ a large difference. Do you still have your benchmarking code around somewhere?\n. Closing for now. We currently don't feel millisecond precision on `spray.http.DateTime` is required.\n. Agreed. We'll change the decoders to change/remove the `Content-Encoding` header.\nThanks for reporting!\n. I like the idea for a `removeHeaders` and `mapHeaders` transformer for the request side of the pipeline.\nThanks for the suggestion!\n. (discussion continued in PR https://github.com/spray/spray/issues/378)\n. It's already available with todays nightly builds.\n. There will probably be no M9, the next release is going to be RC1, but it will not be available before July.\nYou can simply upgrade to 1.1-20130614 and stick with it until RC1 if you want.\n. Stanislav,\nwhat version does your comment target?\nAre you implying that the fix to this issue is non-effective?\n(Just trying to understand you better...)\n. Just verified that the patch is indeed effective in fixing the bug for me.\nCould you distill a test case demonstrating the problem that you still appear to be having?\n. I see your screenshot but I cannot reproduce the issue. For example, if I change the `/ping` route of the `on-jetty` example to this:\n\n```\npath(\"ping\") { ctx =>\n  ctx.complete(ctx.request.headers.map(h => h.getClass.getName + \": \" + h) mkString \"\\n\")\n}\n```\n\nI get this output:\n\n```\nspray.http.HttpHeaders$Cookie: Cookie: __utma=96992031.823935018.1371557886.1371561377.1371565322.3; __utmc=96992031; __utmz=96992031.1371557886.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none)\nspray.http.HttpHeaders$Host: Host: 127.0.0.1:8080\nspray.http.HttpHeaders$Accept: Accept: text/html, application/xhtml+xml, application/xml, */*\nspray.http.HttpHeaders$Accept$minusLanguage: Accept-Language: en-US, en\nspray.http.HttpHeaders$RawHeader: Referer: http://127.0.0.1:8080/\nspray.http.HttpHeaders$User$minusAgent: User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.110 Safari/537.36\nspray.http.HttpHeaders$Connection: Connection: keep-alive\nspray.http.HttpHeaders$Accept$minusEncoding: Accept-Encoding: gzip, deflate, sdch\n```\n\nAs you can see all headers have been parsed into their respective model classes.\nCould you somehow provide us with a failing test-case?\nOr a failing example that we can run ourselves?\n. Stanislav,\ncan you check out this branch of the spray-template project:\nhttps://github.com/spray/spray-template/tree/wip/318\n\nWhen you start sbt, run `container:start` and browse to `http://localhost:8080` you should see that the request headers are correctly parsed into their high-level model classes.\n. Stanislav,\nyou should generally have all spray JARs on the same version. Mixing versions is not supported.\n. As I said, if you mix versions you are on your own. Why can't you simply upgrade to the 1.2-20130620 nightly?\n. The problem is that according to [the spec](http://tools.ietf.org/html/rfc6265#section-4.1.1) the `Expires` attribute of a cookie must be capitalized, so your cookie is actually invalid.\nHowever, we have relaxed the parser with [this commit](https://github.com/spray/spray/commit/1b84d752c6f4509c8d8ea16242100f5cce0c610b), so if you upgrade to todays nightly this issue should be fixed.\n. similar to what Apache provides with its [`MaxKeepAliveRequests`](http://httpd.apache.org/docs/2.4/mod/core.html#maxkeepaliverequests) setting.\n. Duplicate of #310 \n. Ian,\nhow are you accessing the JSON content?\nIf you use any of the predefined JSON Marshallers they should correctly decode the entity with UTF-8.\n(See implementation [here](https://github.com/spray/spray/blob/master/spray-httpx/src/main/scala/spray/httpx/SprayJsonSupport.scala#L34) and the test [here](https://github.com/spray/spray/blob/master/spray-httpx/src/test/scala/spray/httpx/JsonSupportSpec.scala#L44)\n. Kevin,\nit looks like for some reason the target actor of your \"ask\" has already been stopped by the time you perform the ask. As to why, I don't know.\n\nAnd: Please use the mailing list for questions like these!\n. Just triggered a \"special\" nightly build right off this PR (`1.1-20130618`). We'll do the actual merge into master and polishing (e.g. a few failing tests) tomorrow...\n. Thanks, Michael, for finding, reporting and fixing this bug.\nCould you as well add a test that fails without your fix?\n. Another thing: It's great that you are already following our new commit message policy. Could you add a blank char after the initial `=`?\n. > The normalizedSettings.sslEngineProvider has the default ClientSSLEngineProvider and hence, overrides the implicit ClientSSLEngineProvider in the HttpHostConnector constructor. \n\nHmm... the `normalizedSettings.sslEngineProvider` should also be the correct one. Let's find out why it isn't.\nAnd, from the looks of it, we can actually remove the implicit parameter on the `HttpHostConnector` constructor. The connector already receives the sslEngineProvider via the `HostConnectorSetup` parameter. The explicit parameter is therefore redundant.\n\n> I'd be happy to add a test. Could you point me to the best place to add one?\n\nIt think it'd be best to add a case [here](https://github.com/spray/spray/blob/master/spray-can-tests/src/test/scala/spray/can/client/SprayCanClientSpec.scala#L96). All it would have to do is to make sure that a custom implicit sslEngineProvider is properly used. To make things easy you could specify one like this:\n\n```\nobject CustomProviderUsedException extends spray.util.SingletonException\nimplicit val customEngineProvider: PipelineContext \u21d2 Option[SSLEngine] = throw CustomProviderUsedException\n```\n\nand then simply check in your test, whether the custom exception is thrown.\n. Ahh, I see. Yes, the exception was not a good idea and you volatile solution is absolutely fine.\nThanks a lot, Michael!\nThis looks ready for merge, the only thing missing would be you signing the CLA: http://spray.io/project-info/contributing/#contributor-license-agreement-cla\n\nWould that be ok with you?\n(We don't have a purely electronic CLA singing process in place yet, we will soon though...)\n. And one more small thing: could you change the commit message to?:\n\n```\n= can: fix custom ClientSSLEngineProvider not used by host-level client-side API\n```\n\nThanks again!\n. Great! Thanks a lot, Michael, for this important fix!\n. Apart from the one question this LGTM.\n@jrudolph Do you have any comments?\n. > I'm curious as to why the fallback loading is necessary at all though -- shouldn't the actor system's config settings load all the spray reference.conf files?\n\nYes, the ActorSystem's config _does_ contain all the default settings. However, we provide another overload, which allows you pass in a plain `Config`, which you might have constructed from a String and which only contains one setting. Then we want all the other settings to be the default ones. And for that we need the fallback.\nYour change is therefore a bit too aggressive currently.\n. Michael,\nI think you first solution alternative is indeed the best. _spray_ doesn't really work outside of an ActorSystem anyway, so requiring one to be present is not really a problem IMHO. Let's see what @jrudolph thinks about this...\n\nOne thing: Could you please prefix your commit messages with the module name?\nE.g. `= sbt: make spray.json optional package import` or `! can, routing, servlet, util: load default config settings from ActorSystem rather than explicitly`.\n. Gentlemen,\nthis discussion appears to be fruitful but, unfortunately, is going way over my head.\nUnless @jrudolph has anything to add I completely trust your combined experience and judgement as to how to best  OSGi-enable spray without unnecessarily complicating the API, the project setup or the maintenance process.\n. > @sirthias Do you want me to rebase due to conflicts with 51f325c?\n\nYes, Michael, this would be good. Thanks!\n. Excellent!\nThanks a lot again, Michael!\n. Ok, great!\nThanks for these important directions!\nI'll probably merge later todays and try to get it right... :)\n. Ok, Michael, I just merged master into the [release/1.2](https://github.com/spray/spray/tree/release/1.2) branch.\nI think I got your \"we need to remove the override import for akka.io package\" hint right, but what exactly do you mean by \"replace dependency on akka-actor with akka-osgi\"?\n. So this change:\nhttps://github.com/spray/spray/commit/989b8e49cb20f48831eb922c1c9000a6998bde4d\nis all that's required.\nThanks again for you support, Michael!\n. Ok, thanks for checking.\nI just pushed https://github.com/spray/spray/commit/53ee4ca7b9b196f80cfad77de3b746489cac1c51, which hopefully settles this ticket (for now :)...\n. Also: raw queries should not be decoded during parsing, see [this thread](https://groups.google.com/forum/?fromgroups#!topic/spray-user/Xeh1VS8Tp14)\n. No. But I am working on a fix right now. It's almost done, this ticket will be closed today.\n. yes\n. Thanks, Luciano! We'll take a look at your code as soon as we can.\n. Luciano,\ncoming to back to this we have just discussed that we'd like to keep the size of the code that we have to maintain in the next month as small as possible. Seeing that servlet 3.1 support in containers is still very limited and we can't really test this new connector we'd like to keep this PR open until the time for servlet 3.1 really has come.\n\nOne question: Are you already running on Glassfish 4 and needed this connector? Or was it purely the (good!) intention of helping us with the upgrade that would have to come eventually?\n. Ok, thanks.\nI think many servlet containers are already \"fast enough\". Jetty 9 certainly is.\nAlso, there a few servlet based frameworks/platforms in the top section of the latest techempower benchmark.\nSo, performance is probably already not bad. The disadvantage of having another adapter layer in the hot path is something that a new Servlet API version unfortunately won't be able to fix, so I guess we'll never see _spray-servlet_ be as fast as _spray-can_.\n. We are in the process of moving tickets over from spray into Akka HTTP, where all future development will happen (apart from bug fixes in spray).\nThe development and user uptake of spray over the course of the last years has demonstrated that servlet support is less and less important for the community. This is the reason that Akka HTTP will not support any Servlet API for the time being.\nI am therefore closing this PR.\n\nThank you anyway for submitting this patch!\n. A `fastSplit` would create an intermediate list structure, which I can avoid with the current solution. Still, if I could simply `reduceLeft` the resulting list I probably would have gone for it, but since we either have to add another step mapping the resulting strings to PathMatchers or drop down to a multi-case fold I thought that the low-level solution was better overall.\n\n> And wouldn't this be an example for the + commit prefix?\n\nShoot. Yes, indeed. I'll fix this right away, thanks!\n. Nikolas,\nyour solution would result in an additional `Slash` matcher prefix, which would prevent compatibility with the `path` and `pathPrefix` directives, which already prepend a `Slash` matcher themselves...\n. Derek, could you rebase on top of the current master?\n. I see. Your branch overlaps with a \"dirty\" history rewrite we did a couple of days ago. When we established our new [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages) we retroactively changed all commits messages since the 1.1-M8 tag to conform with the new rules. Since you split off your branch earlier and merged with master several times you now have our commits twice (not your fault, since we were the ones to break the rules and change public history).\n\nNevertheless, we'd like to keep all commits since the 1.1-M8 tag \"clean\" in that they adhere to the new policy.\nSo maybe a single squashed commit containing all your changes on top of the current master would be the easiest. If you like we can also do this for you. Let me know.\n. Derek, sorry for taking a while to get back to you on this.\nThe easiest way to get this done is probably to just cherry pick your commits over onto the current master (so, no merge and no rebase). If you want you can also squash (part of) your commits before in order to clean up the history and reduce the number of commits that you need to cherry-pick.\n. We are happy to address your question if you direct it to the [Mailing List](https://groups.google.com/forum/#!forum/spray-user). The ticket system here is not meant for discussions or questions.\n. Thanks, Nikolas, for the report and your observation regarding the `Empty` naming.\nI agree with all your points, so they have been included in the patch.\nThanks again!\n. Travis still reports this small compiler error:\n\n```\n[error] /home/travis/build/spray/spray/spray-http/src/test/scala/spray/http/HttpHeaderSpec.scala:104: not found: value windows-1252\n[error]       example(`Content-Type`(ContentType(MediaType.custom(\"text\", \"xml\", parameters = Map(\"version\" -> \"3\")), `windows-1252`)))_ ^\n[error]                                                                                                               ^\n[error] one error found\n```\n. As we discussed before making type class types non-invariant is not a good idea. So the only way I saw to make the desired inference work for PathMatchers is the given fix. Just out of curiosity, do you see any other alternative?\n(Not that I think we need it, just thinking options here...)\n\nI'll merge this patch later today...\n. A yes, thanks for spotting the wrong ticket number. Fixed.\n. This is a great start, Jeff, thanks a lot!\nI think after incorporation of the few remarks I just added we are good to merge...\n. Another thing: we'd need you to send us a signed CLA before we can merge your PR: http://spray.io/project-info/contributing/#contributor-license-agreement-cla\n. Thanks for the CLA, Jeff!\nTwo more things before we can merge:\n1. Can you rename the `MethodDirectivesExampleSpec.scala` to `MethodDirectivesExamplesSpec.scala` in order to fit the class name? (Also make sure to update the links to it!).\n2. Can you squash all your commits into a single one with this commit message (see also our new [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages))?:\n   \n     = docs: add method directives docs\n. @jeffywu Please let us know how you'd like to move on with this PR!\n. @jeffywu Would you agree to us \"hijacking\" your contribution? We'll do the remaining bits ourselves if you are ok with that...\n. Thanks, Marek, this is going to be a nice addition!\nCan you also change the commit message to `= http: add model for CORS headers` in accordance with out new [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages)?\n. LGTM, Marek, thanks!\nOne thing: I made a mistake in my recommendation for the commit message. Since this patch adds stuff to the public API the commit message should be `+ http: add model for CORS headers`. Sorry about that!\n\nAfter review from @jrudolph this should be ready for merge.\n. Ok, I updated the HttpHeaderParserSpec according to the header model additions.\nMarek, can you add this commit to your PR?:\nhttps://github.com/spray/spray/commit/c94badf39fcd5da3e31e137e4491c40b84c26418\n\nIt is correct that this spec currently needs to be updated after every change to the header model.\nHowever, for the time being I think this is not too bad. Additionally it makes us explicitly aware of the effect of the header model changes onto the header parser in _spray-can_ (especially the parse trie), so it even has some real benefits.\n. Mark,\ncan you add the `require` for the delta-seconds and remove your last commit (as it didn't help)?\nI think we can merge then and chase the remaining problem ourselves (since your patch doesn't appear to be the culprit).\nThanks again!\n. Excellent!\nThanks a lot again, Marek, for your contribution and patience during the review!\n. Thanks, Mike!\n. Thanks, Mark, for taking a stab at #188.\nBy looking at your solution I get the impression that we now have a lot of duplication between the `AuthenticationRequiredRejection` and the `AuthenticationFailedRejection` that we should be able to DRY up quite a bit.\n\nHow about we merge both directives into this one?:\n\n```\n/**\n * Rejection created by the 'authenticate' directive.\n * Signals that the request was rejected because the user could not be authenticated, either\n * because the `Authorization` header was not supplied or the supplied credentials are invalid.\n */\ncase class AuthenticationFailedRejection(authHeaderPresent: Boolean, scheme: String, realm: String,\n                                         params: Map[String, String] = Map.empty) extends Rejection\n```\n. Thanks, Mark, LGTM!\nAfter @jrudolph has reviewed your patch as well I think we are good to merge...\n. Apart from the minor comments this looks great!\nThanks, Mark!\n. Excellent!\nThanks again, Mark, for this nice fix!\n. Thanks, Derek, great!\nTwo things:\n1. We need a signed CLA from you: http://spray.io/project-info/contributing/#contributor-license-agreement-cla\n2. For some more potential massaging of the abstractions we have two options: we can either merge now and play around with things ourselves or we can do it together in this PR, which require more involvement from you. Would you be ok with the first option?\n. Great, thanks, Derek!\nThe CLA is the same as Typesafe's, which is itself almost identical to the Apache 2 CLA.\n. No updates yet. We are quite busy with the akka migration at this time.\n. I am very sorry that this ticket has not made it into spray.\nWe simply did not have enough capacity to fully take on this potentially big feature (with its outside dependencies) and then maintain and port it into Akka HTTP ourselves.\nNow, spray is in maintenance mode and we are reluctant to change the outside API, let alone adding completely new features.\n\nStill, as it always has been, some kind of built-in support for metrics/monitoring is of course and without a doubt a valuable thing to many users, so we'll move the corresponding spray ticket over to Akka and reference this PR, so that we can build onto @derekwyatt's work over there.\n\nOnce again, I am very sorry that this contribution hasn't made it in.\nAfter all, it addresses ticket https://github.com/spray/spray/issues/9, the last single-digit ticket in spray's history. Over in Akka HTTP-land there is now a full and paid team on everything, so slips like this shouldn't happen anymore.\n\nThanks again to everyone who has contributed to this PR!\n. I understand the issue and can see how padding can help. The `StatsSupport` appears to be pretty vulnerable to false sharing as we are allocating a whole range of AtomicLongs right after each other, which will likely cause them to end up in a single or maybe two cache lines together.\nYour patch will fix this as long as cache line size is <= 64 bytes. Even if it's higher it should prevent all 7 AtomicLongs from living in the same line.\n(While looking around I found an interesting bit from Nginx which appears to have quite a bit of logic for figuring out cache line size for modern CPUs: http://forum.nginx.org/read.php?29,226088)\n\nThe only downside to this patch I can see is the increased memory consumption, which however is not a problem since we are allocating only a single `StatsHolder` per server. So, stepping back I'm inclined to accept the patch. WDYT, Johannes?\n. Mathias,\nJohannes and I discussed this, the `LongAdder` is definitely what we'd like to use eventually. However, as you already said, we are still on Java 6, so it's out of reach for the time being.\nSo, until we officially move to Java 7 let's use your `PaddedAtomicLong` as a temporary improvement.\n\nCould you please:\n1. move your `PaddedAtomicLong` implementation to _spray-utils_\n2. add a comment `//TODO: remove after upgrade to Java 7 + JSR166e LongAdder`\n3. fix the commit messages to conform with our new [commit msg policy](http://spray.io/project-info/contributing/#git-commit-messages)\n4. send us a signed [CLA](http://spray.io/project-info/contributing/#contributor-license-agreement-cla)\n\nThanks again for contributing!\n. Thanks for reporting, Mathias!\n. It would allow you mix content that you application provides in-memory with content coming from files.\nThis might be useful, for example, for producing multipart message entities.\n. We already have support for (un)marshalling multipart messages.\nCheck out this thread for more background on this ticket: https://groups.google.com/forum/#!searchin/spray-user/wall/spray-user/oS0GNXHSMJU/uIFDFkq42moJ\n. Yes, that would be the easiest solution.\nJust want to make sure this makes sense in all cases, or whether we need to find a smarter solution in some cases...\n. Not directly relevant anymore for Akka HTTP (as we use the Akka testing infrastructure now).\n. Actually, Johannes and I discussed this improvement once more and we came up with an even simpler and yet overall better solution which I've just outlined in this post to the mailing list:\nhttps://groups.google.com/d/msg/spray-user/8sbfL-QB5es/gxuIvwtN3WEJ\n\nWDYT?\n\nWould you be willing to update this PR accordingly?\n\nI'm sorry for being the cause for the rework, you simply were too fast with this PR! :)\n. Great, thanks again, Michael, for your continued energy to help improve _spray_.\nIt's very much appreciated!\n. Thanks, Michael.\nI'm wondering if there is any drawback to _always_ adding the special header.\nThe overhead for allocating the extra header is minimal, but it might complicate things a little bit (like the tests for example). Another scenario could be proxying, where someone would like to forward to request to another server (either via \"raw\" _spray-can_ or _spray-client_). She would then have to filter the request headers first.\nAlso, always adding the header somewhat breaks behavioral symmetry (with regard to request headers) to _spray-can_.\n\nShould we hide the header addition behind an extra setting?\n. Looks good!\nThe only thing missing is adding the new `servlet-request-access` setting to the _spray-servlet_ `reference.conf`, along with a comment.\nThen we should be good for merging.\n. Excellent! Thanks a lot Michael, once again!\n. Olger, thanks!\nYour PR is currently based on an obsolete version of the master branch (due to us having rewritten history two weeks in order to change all commits since M8 to comply with our new [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages).\n\nAs a first step (and assuming that your `origin` remote points to `git://github.com/spray/spray.git`) could you please do the following in the master branch of your fork?:\n- `git fetch origin`\n- `git reset --hard origin/master`\n- `git cherry-pick 25e53a8`\n- `git push -f` into your fork\n. Sorry, but we did not have enough capacity to include this PR in the current release cycle.\nWe'll take it on when our bandwidth allows...\n. @olger Currently not, thanks! I think the easiest way forward would be to simply leave this PR where it is now. We'll get back to it when the time is right...\n. I am very sorry that this ticket has not made it into spray.\nWe simply did not have enough capacity to merge, maintain and port it into Akka HTTP ourselves. \nNow, spray is in maintenance mode and we are reluctant to change the outside API, let alone adding completely new features.\nStill built-in support for OAuth is no doubt a valuable thing to have, so we'll move [the corresponding spray ticket](https://github.com/spray/spray/issues/1007) over to Akka and reference this PR, so that we can build onto @olger's work over there. \n\nOnce again, I am very sorry that this contribution hasn't made it in in time.\nOver in Akka-land there is now a full and paid team on everything, so slips like this shouldn't happen anymore.\nThanks again to everyone who has contributed to this PR!\n. Actually I'd be inclined to move `spray.client.pipelining.unmarshal` from the package object into the `spray.httpx.unmarshalling.Unmarshaller` object and leave the name as is. This should prevent the collision in most import scenarios and, while it is a breaking change, shouldn't affect as many people as a renaming of `spray.client.pipelining.unmarshal` probably would.\n\nIf you'd like to submit a PR, we'd be happy to have it, thanks! \n. Great, looks good!\nAfter the small changes we should be good to merge.\n. Great, thanks!\nNow there is only one more small nag left: the commits don't quite fit the policy that we are trying to establish/uphold. How about something like this?:\n- `+ util: add PaddedAtomicLong`\n- `= can: upgrade StatsSupport.StatsHolder to new PaddedAtomicLong`\n- `= sbt: upgrade dependencies`\n\nSorry for being a PITA on this but we'd like to rely on the commit messages for building migration guides, which is why they are important.\n. Excellent!\nThanks again, Mathias!\n. Or, as an alternative we could use a magnet:\n\n```\nobject Unmarshaller {\n\n  def unmarshal[T](implicit m: Magnet[T]): T = m.unmarshal\n\n  trait Magnet[T]\n  object Magnet {\n    ...\n```\n\nAnd you are right, we should also move the `unmarshalSafe` method into the `Unmarshaller` object!\n. Actually my idea regarding the magnet was crap, so please just ignore it.\n\n> I would suggest renaming it to `unmarshaller[T]` to make it more clear what it actually does. Then we can add\n> `def unmarshal[T: Unmarshaller](entity: HttpEntity): Deserialized[T]` to have parity with `unmarshalUnsafe` again. > What do you think?\n\nYes, this sounds exactly right! Thanks, Age!\n. Thanks for this very nice ticket closure, Age!\n. Apart from the two comment this looks great, thanks Age!\nCould you also rewrite the commit messages to conform to our new [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages)?\n. > I'll have to refresh my Git foo for rewriting those commit messages but I'll do my best ;)\n\nThe easiest might be a `git rebase -i HEAD~2` followed by a `git push -f`.\n. Excellent! Thanks again, Age!\n. This is a great post, Matthias, thank you very much!\nApart from the project as a whole I especially like the tons of links you've included!\nExcellent stuff!\n\nLet us know when you think the post is ready for merging!\n. And the post is online.\nThanks again, Matthias!\n. Excellent, Andr\u00e9, this looks like a great patch.\nGive us some more time for the review, but judging from the first impression this will definitely have to go in!\nThanks already!\n. Andr\u00e9,\nduring the review I discovered a few things that I didn't like about the way the MediaRanges/MediaTypes are currently modeled (which has nothing to do with your patch). Adding `qValue` support presented a good opportunity to get these things improved. In the process I also fixed a few remaining issues in your patch, I hope you don't mind.\n\nWould you be able to cherry-pick [this commit](https://github.com/spray/spray/commit/fad2ff24986afd65c982dbad58cd7964a7cdc0b2) on top of your branch so it does get included into this PR?\n\nI also fixed all the tests that were still failing with your patch. The only thing that is not yet done are some additional tests verifying the correctness of the new q-value based content negotiation logic.\nIdeally we'd have some low-level unit tests in _spray-http_ (testing mainly the `acceptableContentType` logic on the `HttpMessage`) and some higher-level ones in _spray-httpx_ and/or _spray-routing_ (testing marshaling against incoming requests). Would you be able to add some?\n\nOtherwise this is a really great contribution, thank you very much for getting the ball rolling on this!\n. Two more things:\n1. Could you add some more meat to the commit message of your commit?\n   Since it introduces breaking changes it'd be good if you list what they are and what motivates them.\n2. We need you to sign [our CLA](http://spray.io/project-info/contributing/#contributor-license-agreement-cla) before we can merge...\n. To be honest, we haven't really been watching out for Windows-compatibility too much.\n(And we've been receiving very little feedback as to this matter, which makes us think that somehow the share of people building _spray_ on Windows is somewhat small...)\n\nIf you see something that you think needs fixing a PR for it would be much appreciated!\n. Could you change your last commit message to `= http: small cleanup`?\n. Let's get this fix merged ASAP and save the EOL stuff for another PR, if you don't mind.\nThis patch touches a number of important things and thus might cause conflict pretty quickly...\nSo, whenever you are ready I'd like to push the green button here...\n. Let's keep the polish patch separate, so all you need to do is squash your two most-recent commits.\nIf you don't have time for the tests now then let's save them for a later PR as well, ok?\n. Ok, great.\nNow the only thing missing is the CLA...\n. Excellent! Thanks again!\n. Nice patch!\nI especially like the new and improved tests!\nThanks once again, Age!\n. > I made the isContentEncodingHeader val lazy so it doesn't get pre-allocated unless it is needed.\n\nActually, I'd propose to keep it as a simple `val` for performance reasons. The one allocation is probably not worth the (small) performance penalty introduced by the volatile read (and the resulting memory barrier) required for the `lazy val`. Since the decoders are potentially used from many different threads at the same time this can measurable.\n\nIf you'd like to limit allocations we could move the `val` into a `Decoder` companion object, so it doesn't get instantiated once per Decoder but only once. However, the companion object would introduce a new class and another allocation as well. And since we currently only have two different decoders this would therefore not save anything...\n. > This does indicate that it might have been better to implement the removeHeader(s) stuff directly on HttpMessage (or HttpRequest?) instead of only on RequestBuilding. What do you think?\n\nYes, we could certainly move `addHeader`, `mapHeaders` and `removeHeader` functionality right into the `HttpMessage`. If you want you could submit another PR with these changes first and update (rebase) this PR after the other one has been merged...\n. And another one from Age in.\nYou are quickly rising to the top spot on _spray's_ contributor list! :)\n. Excellent, Age, please keep them coming! :)\n. We currently cannot reproduce the NPE ourselves. However, I just added a commit that should help us in at least locating what exactly is triggering it.\n. Great, Ruslan, thanks for this fix!\n. The `StringRendering` exists so we can keep everything DRY and reuse the rendering logic for producing proper `toString`s for our model objects.\nAnd for consistency we want the `toString` representation to exactly match what is also rendered as ByteStrings to the network, so pure ASCII encoding as well.\n\nWe could use the `ASCII` charset for this but I wasn't sure whether it really performs the same \"toByte\" conversion from the string characters as we do with the `ByteStringRendering`, so I used the deprecated string constructor, which does. (Briefly looking at [the ASCII charset implementation](http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/6-b14/sun/nio/cs/US_ASCII.java) gives me the impression that I was right.)\n. Excellent!\nThanks once more, Age!\n. Cool! This will be another ticket nicely solved, thanks, Andr\u00e9!\nLet us know when you are happy with this patch, then I'll merge...\n. > Additionally, if we rename it to detach we should still support the old detachTo directive so that we don't break the API but instead deprecate its usage and direct users to the new detach directive.\n\nYes, that'd be possible.\nHowever, we are already introducing breaking changes all over the place, so I'm not too worried about removing `detachTo` right away. The important thing is to format the commit message according to the new policy and explain what changed and (ideally) what is required for a migration.\nWe will compile a migration guide from all commits with breaking changes, so an update shouldn't be too hard for all users...\n. Andr\u00e9, I think one squashed commit is good enough, thanks!\n. When you've changed the last small bits and @jrudolph has signed off on this as well I think this is ready for merge.\nThanks a lot again, Andr\u00e9!\n. Yeah, there appears to be some compiler problem preventing it from seeing the implicit conversion in the parent class. In such cases introducing another level usually works, here it does as well:\n\n```\nclass DetachMagnet()(implicit val ec: ExecutionContext)\nobject DetachMagnet {\n  implicit def fromUnit(u: Unit)(implicit dm2: DetachMagnet2) = new DetachMagnet()(dm2.ec)\n}\n\nclass DetachMagnet2(val ec: ExecutionContext)\nobject DetachMagnet2 extends DetachMagnet2LowerPriorityImplicits {\n  implicit def fromImplicitExecutionContext(implicit ec: ExecutionContext) = new DetachMagnet2(ec)\n}\nprivate[directives] abstract class DetachMagnet2LowerPriorityImplicits {\n  implicit def fromImplicitRefFactory(implicit factory: ActorRefFactory) = new DetachMagnet2(factory.dispatcher)\n}\n```\n. Also, could you rebase on top of the current master?\nCurrently this patch doesn't merge cleanly anymore...\n. Another idea: let's allow for explicitly specifying an ExecutionContext as well, so we can say:\n\n```\ndetach(mySpecialExecutor) { ...\n```\n\nwith\n\n```\nobject DetachMagnet {\n  implicit def fromUnit(u: Unit)(implicit dm2: DetachMagnet2) = new DetachMagnet()(dm2.ec)\n  implicit def fromExecutionContext(ec: ExecutionContext) = new DetachMagnet()(ec)\n}\n```\n. Thanks, Andr\u00e9!\nGreat work!\n. Keep 'em coming! :)\n. Nice work, Andr\u00e9, thanks a lot!\nI think the validation is good enough, we don't have to fully match the JS identifier definition.\n\nOne additional thing: Could you rewrite the commit message to conform with our new [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages)?\n. Excellent, Andr\u00e9, thanks!\nIt's great to have so many quality contributions flowing in...\n. Can you append a \", fixes #308\" to the second commit message?\n. Another thing: I think a squashed commit is better.\nWe generally try to keep the master branch compiling between _every_ commit.\nThis way it's easier to use stuff like `git bisect`.\n\nThanks!\n. @jrudolph: any objections to a quick merge?\n. Closing as \"invalid\".\n. In order to ensure proper `keepOpenOnPeerClosed` behavior this patch requires https://github.com/spray/spray/issues/401\n. Great contribution, Andrew, thank you very much!\nApart from the small remarks this looks already very much ready.\n\nOne additional thing: we'd need you to sign the [CLA](http://spray.io/project-info/contributing/#contributor-license-agreement-cla) before we can merge.\n. Andrew,\nwe have just discussed the `copy` problem a bit more. You are raising a very good point, the `copy` semantics are hard to get right with the additional and \"silent\" `raw` member. Maybe our initial proposal of adding this member to the `Uri` model wasn't as great as hoped.\n\nWhat do you think about this alternative solution?\nInstead of uglifying our up-to-now quite clean URI model with access to the raw URI we add a new config setting `spray.can.server.raw-request-uri-header` in analogy to the already existing `remote-address-header` setting.\nPer default this setting is `off`, but when you enable it _spray-can_ will add a custom `Raw-Request-URI` header to the request.\n\nThis has several benefits:\n1. The URI model stays clean.\n2. The user has full flexibility with regard to when to access, change or remove this additional header.\n3. No performance penalty for the majority of users that don't need access to the raw URI.\n4. Symmetry with other design choices (e.g. the access to the client remote address).\n\nWDYT?\n. Andrew,\nyou are right in that we should try to keep _spray-can_ and _spray-servlet_ aligned as much as possible.\nTherefore we should add a `spray.servlet.raw-request-uri-header` setting as well (just as there already is a `spray.servlet.remote-address-header` setting).\n\nLooking at the _spray-servlet_ `ModelConverter` again I realized that _spray-servlet_ currently doesn't mirror _spray-can_ with regard to the request URIs it generated. I have justed added a respective ticket:\nhttps://github.com/spray/spray/issues/415\n\nAFAICS we can't really avoid running something like the URI rebuilding snippet you show if `spray.servlet.remote-address-header` is enabled. If not I think we should be good with what we currently have, the only thing missing is the call to `withEffectiveUri` like here: https://github.com/spray/spray/blob/master/spray-can/src/main/scala/spray/can/server/RequestParsing.scala#L46\n\nAnd I think we also have to add a `default-host-header` setting to _spray-servlet_:\nhttps://github.com/spray/spray/blob/master/spray-can/src/main/resources/reference.conf#L133\n. Yeah, sorry for leading down the wrong path initially...\nHowever, I think we can still do the simplification of the `Uri.Impl` stuff.\n\nThanks a lot again for your contribution and the great discussions. Very fruitful!\n. Yes, I just deprioritized it.\n. Comment from the ML (https://groups.google.com/forum/#!topic/spray-user/40ni-idIqcc):\n\n> That's great! I think removing the provider from `Http.Connect` and\n> `Http.HostConnectorSetup` messages makes sense. One note about the FQCN\n> though -- what about optionally setting the provider via a new message to\n> Http? Or some other technique that allows client code to access values that\n> are available at runtime but not in the actor system config? In my case, I\n> need to retrieve the SSLContext from a shared service that's not accessible\n> statically. I can get a reference to the shared service at runtime, before\n> doing any client SSL commands.\n. Duplicate of https://github.com/spray/spray/issues/721\n. Age, thanks for taking this on!\n\nLooking at your proposal I'm not so sure it really is a good idea to combine the response encoding and the request decoding into joint directives. I can image that there are quite a few use cases where you only ever need one side of the two and the other one would only add unnecessary overhead.\n\nHow about something like this instead?:\n\n```\ndef compressResponse: Directive0 = compressResponse(Gzip, Deflate)\n\ndef compressResponse(first: Encoder, more: Encoder*): Directive0 =\n  if (more.isEmpty) encodeResponse(first)\n  else more.foldLeft(encodeResponse(first)) { (r, encoder) => r | encodeResponse(encoder) }\n\ndef compressResponseIfPossible: Directive0 = compressResponseIfPossible(Gzip, Deflate)\n\ndef compressResponseIfPossible(first: Encoder, more: Encoder*): Directive0 =\n  compressResponse(first, more: _*) | encodeResponse(NoEncoding)\n\ndef compressResponseIfRequested: Directive0 = compressResponseIfRequested(Gzip, Deflate)\n\ndef compressResponseIfRequested(first: Encoder, more: Encoder*): Directive0 =\n  encodeResponse(NoEncoding) | compressResponse(first, more: _*)\n\ndef decompressRequest: Directive0 = decompressRequest(Gzip, Deflate)\n\ndef decompressRequest(first: Encoder, more: Encoder*): Directive0 =\n  if (more.isEmpty) decodeRequest(first)\n  else more.foldLeft(decodeRequest(first)) { (r, encoder) => r | decodeRequest(encoder) }\n\ndef optionallyDecompressRequest: Directive0 = optionallyDecompressRequest(Gzip, Deflate)\n\ndef optionallyDecompressRequest(first: Encoder, more: Encoder*): Directive0 =\n  decodeRequest(NoEncoding) | decompressRequest(first, more: _*)\n```\n. Looking through the sources I found another small problem that we can fix with this patch:\nThe `decodeRequest` directive currently only cancels `UnsupportedRequestEncodingRejection` rejections.\nCan you replace the respective line with this one?:\n\n```\ncancelAllRejections(ofTypes(classOf[UnsupportedRequestEncodingRejection], classOf[CorruptRequestEncodingRejection])))\n```\n. I think I manually created the hex dump on the command line.\nIIRC I wanted to make sure that the decoder is able to digest input that i has not created itself.\nIs the current solution non-adequate?\nMaybe we should add a few comments explaining the reasoning...\n. Ok, for some reason I can't reproduce the generation of this hexdump myself right now.\nWhen I run this:\n\n```\nmathias@cox ~ > echo Hello | gzip | hexdump\n0000000 1f 8b 08 00 b5 c0 fb 51 00 03 f3 48 cd c9 c9 e7\n0000010 02 00 16 35 96 31 06 00 00 00\n000001a\nmathias@cox ~ >\n```\n\nI get a non-matching output, no matter what compression level I give to `gzip`.\n\nI think a round-trip test is good, but it probably makes sense to also have a test with \"foreign\" compressed content, e.g. the one produced by `gzip` on the command line.\n. > For the exact same command I get different outputs practically every time I run it. I don't know enough details about Gzip but does the CRC contain some kind of timestamp value?\n\nHmm... I get this:\n\n```\nmathias@cox ~ > echo Hello | gzip | hexdump\n0000000 1f 8b 08 00 b5 c0 fb 51 00 03 f3 48 cd c9 c9 e7\n0000010 02 00 16 35 96 31 06 00 00 00\n000001a\nmathias@cox ~ > echo Hello | gzip | hexdump\n0000000 1f 8b 08 00 0c c5 fb 51 00 03 f3 48 cd c9 c9 e7\n0000010 02 00 16 35 96 31 06 00 00 00\n000001a\nmathias@cox ~ > echo Hello | gzip | hexdump\n0000000 1f 8b 08 00 0c c5 fb 51 00 03 f3 48 cd c9 c9 e7\n0000010 02 00 16 35 96 31 06 00 00 00\n000001a\n```\n. So we have the reason why it's hard to reproduce the exact same hex dump.\nLet's just go for something and add a comment on where the dump is from, ok?\n. Great, Age, thanks!\nJust thinking: if we test the round-trip in a way where we first compress and then decompress (rather than the other way around), we should be fine, right?\n. Sorry, for being silent for so long on this.\nI agree with Johannes in that we can probably simply remove the `decompressRequestIfRequested` directive from this patch.\nOtherwise this is a very addition, I especially like the ton of test you have added!\nAnd even though there are definitely cases where the application _requires_ the client to accept compressed responses (we have one of those ourselves) you are probably right in not going overboard with the predefined directives here.\n\nIt's good enough if someone can just say `compressResponseWith(Gzip)` in this case.\n. Since we are targeting the APIs of the world, rather than general browser-facing web applications, I would say that the requirements with regard to compression very much _do_ belong to an API specification.\n\nI know several REST APIs that _require_ their clients to send their requests with `Accept-Encoding: gzip` and will refuse to deliver any uncompressed content. Even with this patch the only way for me to express that in spray would be `compressResponseWith(GZip)`.\n\nOne step less restrictive would be to use the `compressResponse` directive, which does compress all responses except in cases where the client _does not_ accept compressed content. I'd consider this a nice middle-ground \"default\" behavior which is why I like the short `compressResponse` name for it.\n\nThe most relaxed directive with regard to compression is `compressResponseIfRequested`, which _only_ compresses if the client explicitly requests compression. In most cases I'd say this is _not_ what you want, which why I'd say the current longer name is ok.\n\nSo, I like generally the current proposal. However, we need to make sure to properly document theses directives and include a section explaining the differences and how the user can achieve exactly the behavior he/she wants.\n. I am not in favor of a combined `enableCompression` directive. Compressions on requests and compression on responses are two completely different things, which should be explicitly configured independently from each other.\n\nAs we have seen in our discussion above there is already quite a lot of potential for confusion only with regard to one side (requests or responses). Combining things together here will make it even harder to properly understand how spray handles the various possible cases.\n\nSo, I'd say we keep the things separate and document properly.\n. Excellent! Thanks, Age!\n. Great!!\nThank you very much again for all your work and perseverance with this PR... :)\n. We usually wait for a few things to accumulate before doing a merge into the release branches.\nThe last merge was on Aug 22th, so not too long ago.\nI guess the next will come this week.\n. Thanks, Ruud, for reporting!\nWe'll get this fixed...\n. Thanks, Ruud, we'll get this fixed as well.\n. Closed by https://github.com/spray/spray/commit/eddb32e5dca39af7a00dd537afd8e2e877a76246\n. Age,\nthanks for this contribution.\nI think adding a `scheme` directive is a good idea.\n\nHowever, like Johannes I am somewhat worried about namespace pollution with a very general directive name like `http`. I think the `get` and `put` directives already are a problem that we might need to fix at some point.\n\nHow about we simply leave it at the `scheme(String)` directive?\nHaving `scheme(\"http\")` in my routing structure might even be more readable with regard to intent that a sole `http`.\n\nAlso we could extend and simplify the thing like this:\n\n```\ndef scheme: Directive1[String] = extract(_.request.uri.scheme)\n\ndef scheme(schm: String): Directive0 =\n  scheme.require(_ == schm, Some(SchemeRejection(schm))) &\n    cancelAllRejections(ofType[SchemeRejection])\n```\n\nWDYT?\n. Yeah, we can't really have a zero-arg directive with the same name as one that does take parameters.\nSo my naming proposal from above was crap.\n\nHow about we follow the lead of the [HostDirectives](https://github.com/spray/spray/blob/master/spray-routing/src/main/scala/spray/routing/directives/HostDirectives.scala#L31) which face the same problem of extractors and filters?\n\nSo our zero-arg `scheme` extractor would become `schemenName`.\n. Nice! Thanks again also from my side, Age!\n. Thanks, Mark, much appreciated!\n. We have looked at the logback implementation of SLF4J and, unfortunately, the logger management is done in a very naive way. Logback _never_ releases a logger once it is created, which means an application has to itself ensure to never create more than a bounded number of loggers. In our view this is a severe limitation of logback and greatly reduces its general utility.\n\nThis also means that the `org.slf4j.LoggerFactory.getLogger(String)` overload is not really usable (unless you can ensure the set of unique strings passed to this method over the entire lifetime of your application is bounded).\n\nTo work around this problem in logback (and possibly other SLF4J implementations) we cannot use actor paths as logger names. The only viable alternative is falling back to the actor's class name for the logger name, which unfortunately not only reduces configuration flexibility (because you cannot configure logging levels for branches of your actor tree but only for branches of your package structure) but also forces us to move the actor path into the log message itself, thereby (somewhat) decreasing logging performance and wasting space on the log message line.\n\nIt really is a pity that even \"modern\" logging frameworks like logback suffer from such grave and profound architecture flaws... :-1:\n. > perhaps the actor path should be included as part of the MDC\n\nMDCs don't help as they are thread-bound and therefore quite useful in asynchronous architectures like akka's.\n\n> Actor paths are a newer Akka concept so I'm not surprised they don't play nice with Java logging frameworks whereas classnames do.\n\nYes, but I would assume that a logging framework doesn't place any requirements on the semantics of Strings used in its API. SLF4J offers a `org.slf4j.LoggerFactory.getLogger(String)` overload and logback implicitly assumes that the application will use this overload only with a bounded set of strings over the complete application lifecycle. This requirement is not documented anywhere. Why shouldn't I be able to use the string-based lookup for actor paths?\n. Sorry, Taylor, my comment from before was indeed quite confusing. I meant that the basic principle of the MDC is quite _useless_ (not useful) in async architectures. \n\nFrom the [logback manual](http://logback.qos.ch/manual/mdc.html):\n\n> Most real-world distributed systems need to deal with multiple clients simultaneously. In a typical multithreaded implementation of such a system, different threads will handle different clients.\n\nWell, that would be the typical system of 10 years ago. Nowadays this assertion not longer holds, and it certainly doesn't for akka.\n\n> A possible but slightly discouraged approach to differentiate the logging output of one client from another consists of instantiating a new and separate logger for each client. This technique promotes the proliferation of loggers and may increase their management overhead.\n\n\"Slightly discouraged\" ??\nAs we have seen this approach is simply not going to work at all! Otherwise logback will simply eat all available memory and the application will crash eventually.\n\n> The MDC manages contextual information on a per thread basis.\n\nThat is the crux.\n\nIf we controlled the actual call to SLF4J from spray would could put the actor path into the MDC before each logging call and remove it directly afterwards. This would incur some overhead of course but would be feasible. In fact it is what akka itself is doing: https://github.com/akka/akka/blob/c10eadbd3388250425efff0b790f59af6dbdc63d/akka-slf4j/src/main/scala/akka/event/slf4j/Slf4jLogger.scala#L86-L92\n\nHowever, since we don't control the actual call to SLF4J but simply create log events that go out onto akka's eventbus the MDC won't help us, unfortunately.\n. > But woudn't this mean that the feature @taylorleese requested is already implemented by akka?\n> ... [the logsource] should later be available in the MDC as `akkaSource`.\n\nYes! Right. I missed that.\nSo simply setting the logSource string correctly does make it available to the logger and addressable from a message pattern configuration.\n\nThen this indeed appears to be the best solution in my eyes. Thanks for pointing this out!\n. The reason why we have the `log-actor-paths-with-dots` feature so far was that if the actor path was in dotted form you could use logback's:\n- hierarchical logger configuration to address a whole branch of loggers at once\n- abbreviation feature to nicely shorten the logger name to a given number of chars\n\nHowever, since we can't use the actor path as the logger name anymore we loose both of these features. (Unfortunately, logback doesn't support abbreviation on fields generally, only on the logger name and class name field...). So, there is no point in still having the `log-actor-paths-with-dots` feature.\nThe other setting `log-actor-system-name` is not crucial either, so, for me, the whole point of having our own wrapper around akka's `LoggingAdapter` collapses.\n\nAs Taylor rightfully points out: people can simply use akka's `ActorLogging` trait, there is nothing we can really do to improve upon the logging experience you get from that.\n\nSo, I would vote for completely removing these things from _spray-util_:\n- `LoggingContext`\n- `SprayActorLogging`\n- `UtilSettings`\n\nand tell people do simply migrate over to `akka.actor.ActorLogging`.\n\n@jrudolph WDYT?\n. Taylor,\nwe currently don't go through a deprecation step with our API, we'll simply provide a migration guide, which should contain the necessary instructions for replacing `SprayActorLogging` with `ActorLogging`.\n. My original idea was to start publishing a \"1.1-SNAPSHOT\" version of the documentation once we have added enough new docs to make it worthwhile. As Johannes already said, also publishing the more or less equivalent \"1.0-SNAPSHOT\" and \"1.2-SNAPSHOT\" variant is probably not worth the effort...\n. > On the servlet side things are more problematic, to the point where I'd be tempted to leave the feature unimplemented there instead of misleading a user with something that normally works but mysteriously fails under some circumstances.\n\nYes, I agree. Since we cannot do this properly on the _spray-servlet_ side let's just not implement it there at all.\nHowever, I still like the cleanups you have done in the _spray-servlet_ code. Maybe you can leave those in?\n. This is a very nice and clean addition!\nThanks a lot, Andrew!\n\nI really like how you take the opportunity to clean up our codebase if you come across improvement potential!\n. Thanks again, Andrew!\n. Thanks, Cl\u00e9ment, for this patch.\nMeanwhile the discussion on ticket #421 has continued and we have decided to simply rip out all of spray's \"special logic\" for logging, since it doesn't serve any purpose anymore.\n\nWould you agree that this PR is superceeded by https://github.com/spray/spray/pull/452 ?\n. Not in scope any more for Akka HTTP, as we now live in the Akka codebase and adhere to the formatting standards there.\n. Cristian,\nI understand the problem you are trying to solve and you are correct in that the `compact-json-printing` setting that existed in earlier spray versions has now gone away.\nThe reason is that, as @jrudolph already pointed out, there is no good way to provide settings to implicitly created (un)marshallers, except for other implicits. That is why the _spray-httpx_ module currently has no settings infrastructure of its own.\n\nIf you look at this line:\nhttps://github.com/spray/spray/blob/master/spray-httpx/src/main/scala/spray/httpx/SprayJsonSupport.scala#L38\nyou'll see that you can already provide the JSON marshaller constructured by the `SprayJsonSupport` trait with the information of what `JsonPrinter` to use: simply make one available implicitly.\nThis upholds maximum flexibility as you can decide _per call site_ what printer to use.\nTherefore I don't think your proposed patch really is required.\n. See also https://github.com/spray/spray/issues/971\n. Thanks, Age!\n. Thanks, for finding this problem and providing a solution.\nThere are two things that need changing as well:\n- [Here](https://github.com/spray/spray/blob/master/spray-util/src/main/scala/spray/util/package.scala#L51) we should change the name of the method to `requirePositive` and fix the comment.\n- [These two lines](https://github.com/spray/spray/blob/master/spray-servlet/src/test/scala/spray/servlet/ModelConverterSpec.scala#L36-37) need to be updated.\n\nAdditionally we'd need to you to sign [our CLA](http://spray.io/project-info/contributing/#contributor-license-agreement-cla), but then this will be good to merge.\nThanks again!\n. And one more thing: this change is a _breaking_ change, since it changes the semantics of the API.\nFor example, if I created a `spray.can.server.ServerSettings` instance manually before and was using the `Duration.Undefined` for disabling the `request-timeout` I now have to change my code.\n\nSo the commit message should start with a `!` character.\n. Ivan,\nakka uses `context.setReceiveTimeout(Duration.Undefined)` internally for clearing a receive timeout, so I think for consistency with the akka codebase we should stick to this as well.\n\nI'll merge this nice contribution right away, thanks again!\n. Fixed since the 1.0/1.1/1.2 final release\n. The solution to #365 (which is already almost done) will provide even better performance, because spray-can itself will have to perform zero copying of the response entity. (With the current architecture (response entity wrapping a byte array) it still has to copy once.)\n. Thanks, Ian, for this proposal!\nI think it's a valid request for improvement that we should include.\n\nApart from rewriting your commit with the suggested changes could you please:\n- change the commit message to `! can: introduce dedicated exceptions for connection failure and request timeout for host-level API`\n- send us a signed copy of [our CLA](http://spray.io/project-info/contributing/#contributor-license-agreement-cla)\n. > I did a force push to rewrite the commit message and it lost all the previous comments inline with the code. I wasn't sure how to rewrite the commit message without this happening.\n\nYeah, don't worry about that. Normally github provided access to the \"outdated commits\" incl. comments, but maybe this is currently turned off for some reason.\n. Great, thanks, Ian!\nI'll wait for @jrudolph to sign off on this and then we'll merge...\n. Thanks, Lee, for this patch.\nWe just discussed this issue and would like to follow your lead and, with regard to the _spray-testkit_ module, make an exception to our rule to only depend on Akka with `provided` scope.\n\nHowever, in order to be able to merge your patch, we'd need you to\n- sign [our CLA](http://spray.io/project-info/contributing/#contributor-license-agreement-cla)\n- and change the commit message to `= sbt: change spray-testkit dependency on akka-testkit from 'provided' to 'compile'` in accordance with our [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages)\n\nAlternatively (to make things easier for you) we could also apply the patch ourselves. Let us know what you'd prefer.\nThanks again for bringing this up!\n. This is not a spray problem, but results from akka's different default handling of dead letters.\nAlso check this ML thread: https://groups.google.com/d/msg/spray-user/iakr7aRS4AI/3smE1Tg20koJ\n. Thanks, Andr\u00e9, for this patch!\nCould you do two things?:\n- Move the definition of the two new header parser rules to the `SimpleHeaders` trait? (All \"simple\" headers that only require a single parser rule live in that file.)\n- Reduce the tests for each new header type to a single test case (As you already said, these new headers exactly mirror the `Authorization` and `WWW-Authenticate` headers. I therefore would like to keep unecessary bloat in the already quite big header spec as small as possible.)\n\nThanks again!\n. > ProxyAuthenticateHeader extends WwwAuthenticateHeader and ProxyAuthorizationHeader extends AuthorizationHeader.\n\nThat would simplify the implementation but would have to ugly effect that `request.header[AuthorizationHeader]` might end up returning a `ProxyAuthorizationHeader`. We could introduce an abstract common super type though.\n. Nice!\nThanks a lot again, Andr\u00e9!\n. Ok, added cache synching and SprayActorLogging deprecation.\n. I saw that we are recreating a `UtilSettings` instance everytime a `LoggingContext` was created and that a small cache would likely provide a perf benefit here.\nEven though the `UtilSettings` are now gone, we have the same situation, e.g. with the `ClientConnectionSettings`, for example here: https://github.com/spray/spray/blob/master/spray-can/src/main/scala/spray/can/Http.scala#L67\n. Just published a fresh nightly with today's date.\n. Yes, thanks for reporting!\n. Thanks, Andrew!\n. Very nice!\n. [RFC 2616 Section 14.43](http://tools.ietf.org/html/rfc2616#section-14.43) defines the syntax for the `User-Agent` header as:\n\n```\nUser-Agent     = \"User-Agent\" \":\" 1*( product | comment )\n```\n\nwhereby\n\n```\nproduct = token [\"/\" product-version]\nproduct-version = token\ntoken = 1*<any CHAR except CTLs or separators>\nseparators = \"(\" | \")\" | \"<\" | \">\" | \"@\"\n                  | \",\" | \";\" | \":\" | \"\\\" | <\">\n                  | \"/\" | \"[\" | \"]\" | \"?\" | \"=\"\n                  | \"{\" | \"}\" | SP | HT\nCTL = <any US-ASCII control character (octets 0 - 31) and DEL (127)>\ncomment = \"(\" *( ctext | quoted-pair | comment ) \")\"\nctext = <any TEXT excluding \"(\" and \")\">\nTEXT = <any OCTET except CTLs, but including LWS>\nquoted-pair = \"\\\" CHAR\nCHAR = <any US-ASCII character (octets 0 - 127)>\n```\n\nSo `@` characters are only allows in comments.\nClients producing `User-Agent` headers that do not conform to this syntax are not following the HTTP spec.\nWe only depart from the spec in rare and well-founded cases.\n. Suggestion for solving this properly:\n\nWe introduce a special empty string value\n\n```\nobject Query {\n  val EmptyValue = new String(Array.empty[Char])\n}\n```\n\nand then parse/render like this:\n- `foo=bar`: key = \"foo\" and value = \"bar\"\n- `foo=`: key = \"foo\" and value = \"\"\n- `foo`: key = \"foo\" and value = Query.EmptyValue\n- `=foo`: key = \"\" and value = \"foo\"\n- `=`: key = \"\" and value = \"\"\n- \"\": key = \"\" and value = Query.EmptyValue\n. Closing for now. We can resurrect if necessary.\n. IMHO we shouldn't try to be compatible with every buggy HTTP implementation there is.\nFor the particular case that Derek showed I think we can add an exception, which doesn't really hurt us:\nIf we see a second `Content-Type` header and this second one is identical to the first one then we simply issue a warning and drop it but continue parsing.\nI think we should _never_ accept a message with two different `Content-Type` headers.\n. Stanislav,\nyou can already add a custom port to `DefaultHostInfo`. What else would you need?\n. also see [this thread on the akka-dev ML](https://groups.google.com/forum/#!topic/akka-dev/NXDklScAVbo)\n. I don't think this ticket will still be closed in the scope of the spray project.\nakka-http is based on reactive streams, which will solve the backpressure problem correctly once and for all. The current spray infrastructure on the contrary doesn't really allow for an easy \"proper\" fix, unfortunately.\n. akka-http-core will be ready quite shortly.\nIf you need to the full high-level server-side DSL then this is still some time out.\nCurrent estimate for availability would be sometime in fall this year.\n. We will have something that is stable enough to put weight on, yes.\nAnd since akka-stream is a core dependency of akka-http my estimate does include akka-stream, yes.\n. @danarmak Check out the talk about _akka-http_ from last week's ScalaDays for more info:\nhttp://spray.io/scaladays2014/#/\n. Fixed in Akka HTTP with the new streamified chunking model.\n. Closed by https://github.com/spray/spray/pull/480\n. This builds on https://github.com/spray/spray/issues/635.\n. > ... since it will count expired keys too ...\n\nYes. I wouldn't consider it a problem though if we clearly indicate that the `size` method returns an _upper bound_ on the number of cached elements. If you add the respective clarifications to the scaladoc (also explaining why its only an upper bound) then I think we are good to merge.\n. Perfect!\nAnd one more \"thank you\" going up to the Netherlands...\n. And another one closed.\nThanks, Age! You are quickly rising to the top of our committer list... :)\n. Ok, we have just discussed this.\nCould you change the `MultipartFormData` model to this?:\n\n```\ncase class MultipartFormData(parts: Seq[BodyPart]) extends HttpForm {\n  type FieldType = BodyPart\n  def get(partName: String): Option[BodyPart] = ...\n  def getOrElse(partName: String, default: => BodyPart): BodyPart = ...\n}\n```\n\nAlso, I think we can improve the `BodyPart` model to this:\n\n```\ncase class BodyPart(entity: HttpEntity, headers: Seq[HttpHeader]) {\n  def getNameOrEmpty: String = ...\n  def getName: Option[String] = ...\n}    \nobject BodyPart {\n  def apply(entity: HttpEntity, headers: HttpHeader*): BodyPart =\n    apply(entity, headers: _*)\n}\n```\n\nWDYT?\n. Dirk, we'd also need you to sign our [CLA](http://spray.io/project-info/contributing/#contributor-license-agreement-cla) before we can merge.\nThanks for contributing!\n. What exactly would you like to enforce?\n. @dlouwers Yes, sounds good! This will indeed be an improvement.\n. With the current state of this patch the `SslTlsSupport` pipeline stage unconditionally creates and sends an `SSLSessionEstablished` instance down the event pipeline, even if the respective setting is actually configured to `off`. I'd vote for configuring the `SslTlsSupport` stage with the setting along these lines:\n\n```\nobject SslTlsSupport {\n  def apply(publishSslSessionInfo: Boolean) = new OptionalPipelineStage[SslTlsContext] {\n    ...\n  }\n}\n```\n\nNot only will this save allocations and messages in case the header is disabled, it will also prevent higher pipeline stages from seeing messages they don't necessarily expect.\n. This is another great contribution from you, Michael!\nThanks once more!\n. I agree with Michael in that the changes are technically breaking changes to the public API, even though most users probably wouldn't be impacted.\n. Note that we won't do this before shapeless 2.0 final is out.\n. Currently it looks like we won't do the upgrade to spray.\nAkka-http will contain shapeless 2.0 compatible HLists though.\n. Ok, we just discussed that we will publish shapeless 2.0.0 compatible builds of spray 1.3.1 for Scala 2.10 as well as 2.11 in the next days.\nThat should make things a bit easier for you guys.\n\nI'll report back on this ticket (and close) when this has happened.\n. Done.\nSee https://groups.google.com/d/msg/spray-user/rxfjsg9OsUA/nytYALgIOn8J\n. Yes, I guess grouping the timeouts would be the easiest fix.\nIn a later stage we could also fold all booleans in a bit mask and provide a custom constructor that still supports all \"spelled out\" settings.\n. This is a very nice addition!\nThank you very much again, Andr\u00e9!\nIf @jrudolph agrees I think this should be merged ASAP.\n. Great, thanks Andr\u00e9!\nThis is a very nice contribution!!\nThe only thing missing for me now is the feedback from @jrudolph and the squash...\n. Andr\u00e9,\nI like the default headers addition.\nAlso, I think this patch is fine with regard to the `CloseAll` command support.\nSo, apart form the small nagging this looks ready for merge!\nThanks again!\n. Ok, Johannes and I just discussed the `DemandIdleShutdown` protocol.\nThere are a few changes that need to be put it, however, it's probably easiest if we do this ourselves after this PR has been merged.\nSo, Andr\u00e9, let me know when you are ready and I'll press the green button.\n. Excellent!\nThanks you _very_ much again, Andr\u00e9!\n(Also for fighting through this lengthy review process... :) )\n. Andr\u00e9,\nactually we need to somewhat increase our focus if we want to keep our internal goals as to the release timeline.\nSo, I think it'd be better if we postpone TLS support until after the 1.0/1.1/1.2 finals are out.\n. > BTW: Is this list up to date?\n\nYes, it's the todo list that is currently very much in the center of our attention.\nThe goal is to burn it down until the end of the month.\n. Nice work, Ferdinand, thanks for the contribution!\nApart from the small comments I think the only thing missing is the respective patch on the _spray-can_ client rendering side...\n. Ferdinand,\nas Johannes pointed out my comment was targeted at request _rendering_ only.\nFor parsing you should never produce `ContentTypes.NoContentType` references, since we simply use the default `application/octet-stream` media type if no `Content-Type` header is present.\n. Thanks, Ferdinand, nice fix!\n. Nice fix, Balaji, thanks a lot!\nIf you change the commit message to\n\n```\n= http: fix Location header parsing to accept relative URIs, closes #484 \n```\n\nthen github will automatically close the ticket when we merge this PR.\n. Thanks again, Balaji!\n. Michael,\nwe've just discussed this and would actually prefer the not reuse the \"real\" tests for the documentation.\nThe real tests are often times deeper and structured differently from what we'd like the documentation to show.\nAlso, the focus of the documentation lies more on illustrating usage rather than proving correctness.\n\nSo, could you create a new example spec underneath `/docs` for the marshalling directives?\n. No worries at all, Age.\nHow to best structure the tests and the example and so on is something that is slowly evolving over time and we ourselves have not been entirely clear on. So it's actually a good thing to have impulses come in from the outside that make us question our own approaches and force us to make decisions.\n. Great!\nThanks, Mike!\n. @mhamrah Actually, I just realized that we don't have a signed [CLA](http://spray.io/project-info/contributing/#contributor-license-agreement-cla) from you so far. Would you be able to provide me with one?\n. @guersam Can't you simply \"patch\" the missing content-type / charset header in your use case?\n. I see.\nThinking about it again I think it makes sense to change\nhttps://github.com/spray/spray/blob/master/spray-httpx/src/main/scala/spray/httpx/unmarshalling/MultipartUnmarshallers.scala#L47\nto default to UTF-8 instead of US-ASCII.\n\nIf @jrudolph agrees I'll push a small patch.\n\nThanks for bringing this up!\n. Yes, we could.\nHowever, I'd assume the default charset to be used in the absence of a part's `Content-Type` header is not something that people will really want to configure. The spec kind of dictates the default to be US-ASCII, but seeing the age of the spec I'd assume that they'd pick UTF-8 if it was written today.\nUTF-8 being US-ASCII compatible we don't really break anything.\n\nHow about we do the small fix and now and postpone the bigger change (with the configurable default charset) until someone needs it?\n. Ok, done: https://github.com/spray/spray/commit/538b2c59744f41e2e85853baa6fccb13fc0c7e92\n. Rather than opening up `SimpleLruCache` and/or `ExpiringLruCache` for user extension (and thereby exposing implementation details as public API), how about we simply add an eviction hook feature to the `Cache` trait itself?\n. Yes, that's not bad.\nIf you save the default `onEvict` instance in a public field (e.g. `Cache.EmptyOnEvict`) you can check whether the user actually set a custom `onEvict` handler and only create and attach an `EvictionListener` if really required.\n. Nice work, Douglas, thanks!\nApart from the comments, could you rewrite your commit messages to conform with our [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages) and [sign our CLA](http://spray.io/project-info/contributing/#contributor-license-agreement-cla)?\n. Ok, we'd still need your CLA and a squashing of the commits incl. a rewrite of the commit message according to our policy... Thanks!\n. Yes, thanks, Mathias!\n. Thx!\n. Great! Thanks for this important fix!\n. Ok, we just discussed the idea of better modeling deadlines.\nCurrent plan: extend this PR with a custom `spray.util.Deadline` along these lines:\n\n```\nclass Deadline(val nanos: Long) extends AnyVal\n```\n\nUnfortunately the `scala.concurrent.Deadline` suffers from three deficiencies that make it unattractive:\n1. It's not a value class.\n2. It unnecessarily wraps a `FiniteDuration` rather than a raw `Long` (which is also semantically wrong).\n3. It doesn't offer a \"no deadline\" facility.\n. Apart from the small thing in `SimpleStash` this looks good to me.\n. Thanks, Stig!\nI'd like to merge this fix but can do so only after you have signed our [CLA](http://spray.io/project-info/contributing/#contributor-license-agreement-cla). Could you do that?\n. Thanks, Stig!\n. See also this thread: https://groups.google.com/forum/#!topic/spray-user/0reUlIzU5FI\n. Cool!\n. Thanks, Chip!\n. Addressing @jrudolph's point about making the API a bit more explicitly with regard to port normalization etc.:\nI'd vote for adding this additional method to the `Uri` (above the `copy` method):\n\n```\n/**\n * The effective port of this Uri given the currently set authority and scheme values.\n * If the authority has an explicitly set port (i.e. a non-zero port value) then this port\n * is the effective port. Otherwise the default port for the current scheme is returned.\n */\ndef effectivePort: Int = if (authority.port != 0) authority.port else defaultPorts(scheme)\n```\n\nThen we can also remove this line:\nhttps://github.com/spray/spray/blob/f4a92f0c0d33ff2405339cdc9da70afd83ae4794/spray-can/src/main/scala/spray/can/HttpManager.scala#L46\n. Excellent, thanks, Ivan!\n. Thanks, Ian!\nApart from the small fix this looks ready for merge.\n. Thanks, Ian!\n. This problem appears to be fixed as a side effect of this commit 7e5c494, as least I can't reproduce the original posters problem.\n@jrudolph Can we simply close this then?\n. Should we maybe add a directive for extracting the `RequestContext` instead?\n(Which would basically be nothing more than an alias for `extract(_)` ...)\n. Great!\nThanks, Ivan!\n. Njal,\nthanks for this report and the nice analysis.\nAs you suspect a `Content-Type` header containing a `*` is not valid HTTP and as such _spray-can_ is doing the right thing by replying with `400 Bad Request`. However, the response entity is definitely not intended and needs to be fixed.\n. I'm not sure that this would be a good idea.\nA client sending a `Content-Type: application/*` header is not speaking HTTP. It's not a syntactic problem that we could be lenient with, it's a grave semantic issue that, IMHO, clearly deserves a `400 Bad Request` response.\nHow would you expect a server (or your application) to interpret a `Content-Type: application/*` header?\n. I see.\nTheoretically spray _could_ just ignore the bad `Content-Type` header, parse it as a `RawHeader` and treat the request as if it didn't contain a `Content-Type` header at all. We'll discuss the best approach.\nThanks for making your use case so clear!\n. From http://tools.ietf.org/html/draft-ietf-httpbis-p2-semantics-22#section-4.3.6:\n\n```\nA client sending a CONNECT request MUST send the authority form of\nrequest-target (Section 5.3 of [Part1]); i.e., the request-target\nconsists of only the host name and port number of the tunnel\ndestination, separated by a colon.  For example,\n\n  CONNECT server.example.com:80 HTTP/1.1\n  Host: server.example.com:80\n```\n\nTheoretically we could therefore add logic verifying that a CONNECT request doesn't come with an URI containing a path, query, etc.\nHowever, I'd vote for moving this check into a higher layer so as to not slow down spray-can more than necessary.\n. `HttpIp` is gone by now, replaced by the new `RemoteAddress.IP` model.\n. Could you rebase this on top of the current master (as it doesn't merge cleanly anymore).\nAnd we are still missing your CLA...\n\n(Btw.: No need to close the PR when you want to update it, simply `push -f` into your branch to update the PR)\n. Closing for now.\n\n@drapp Please reopen when you are ready...\n. Just came across these two lines:\nhttps://github.com/spray/spray/blob/master/spray-http/src/main/scala/spray/http/HttpMessage.scala#L272\nhttps://github.com/spray/spray/blob/master/spray-http/src/main/scala/spray/http/HttpMessage.scala#L292\nwhich also need to be fixed in the context of this ticket!\n. Aaron,\nthanks for this PR!\nHowever, the fix to #543 was somewhat more involved than originally expected, which is why we have attacked this ourselves: https://github.com/spray/spray/pull/556 should supercede this PR.\nWould you be ok with simply closing this one?\n. Maybe someone on Windows can send us a PR for this?\n. Thanks, Daniel, for finding and fixing this problem!\nCould you please change the commit message to this:\n\n```\n= routing: fix directory listings under Windows, closes #549\n```\n. Great, thanks, Daniel!\n. Thanks, I'll have to postpone this to next week...\n. Ian,\ncould you rebase on top of the current master?\nI'd like to get this patch in ASAP, so we can start cutting RC1.\n. Very nicely solved, Ian!\nThanks a lot for this great contribution!\n. Yes, agreed, spray-can should be able to deal with custom status codes.\nThanks for bringing this up!\n. Adding a header is already supported with the `addHeader(s)` members of the `RequestBuilding` trait/object in spray-httpx. Which ones exactly would you like to add?\n. We agree with your that the client-side DSL is not yet where it needs to be.\nImprovements in that area will come.\nHowever, they'll probably be more profound that simple addition of more predefined pipeline elements.\n\n> is spray already use scalaz ? is this something that can arise ?\n\nOf course you can use scalaz with spray if you want. However, we will not add a dependency on scalaz ourselves.\n. On the server side we are good.\nOn the client the higher-level spray-can API levels (host- and request-level API) this is not yet fully available.\n. Fixed in Akka HTTP by having moved to Reactive Streams for back-pressure.\n. Our interpretation of the spec is that the URI is opaque with regard to addressing a resource.\nThis means that `/foo` and `/foo?key=value` are _two different_ resources!\nTherefore we issue a `404` if a required parameter is not found.\n. No problem. Thanks for bringing this up anyway, Mike!\n. > Which kinds of subclasses of StatusCode should be created for codes outside the 100-599 range? HttpSuccess or HttpFailure? Or a new one which would make pattern matching more awkward.\n\nI'd vote for creating a separate subclass that is neither an `HttpSuccess` nor an `HttpFailure`. I should be able to specify at registration what values I want `isSuccess` and `isFailure` to return. `allowsEntity` should always be `true` for custom `StatusCode`s.\n\nSince we define `isSuccess` and `isFailure` as methods on the `StatusCode` base class, maybe we should remove `HttpStatus` and `HttpFailure` from the public API altogether?\n\n> Should int2StatusCode be allowed for creating custom status codes? (I chose 'no', because in many cases it is more likely a programmer error)\n\nYes, I'd agree with your decision.\n. Thanks, Greg, for reporting this.\nWe'll look into it.\n. Haoyi,\nok, we can merge this patch to make it easier for your to keep your websocket implementation up to date.\nCan you send us a signed [CLA](http://spray.io/project-info/contributing/#contributor-license-agreement-cla) and reformat the commit message according to [our policy](http://spray.io/project-info/contributing/#git-commit-messages)?\n. Haoyi, thanks for the CLA.\nHowever, your patch doesn't change the public API at all (since all the new method introductions are purely internal), so the commit message should be something like:\n\n```\n= can: improve HttpManager extensibility by moving child actor creations into dedicated methods\n```\n. Nice!\n. Nice!\nIsn't this a `+` change rather than a `!` change though?\n. Ok.\n. I added a separate ticket for the missing tests: https://github.com/spray/spray/issues/603\n. Fixed in Akka HTTP with the new TCP IO API.\n. Ok, feedback incorporated.\n. Yes, good idea. Will do.\n. Stig,\nthanks for this contribution.\nWe won't have any time this week to review and merge this patch though, so it won't make it into the RC1 release which we'll publish this week.\nHowever, maybe we can incorporate it into a later RC if one should be required.\n. Stig, thanks again for this contribution.\nWe have reserved some capacity this week to push for a minor update release.\nOf course we'd like to incorporate the latest and greatest in contributions! :)\nSo, since this patch here looks quite valuable, if you have time to look through our comments we'd like to get it merged.\n. To make things easy for you we can apply the missing bits of the parser definition ourselves if you supply us with a number of (potentially failing) test cases that the implementation needs to pass if correct.\n. No worries, Stig!\nIf you squash all your commits (except the merge from master) into a single commit with this commit message:\n\n```\n+ http: Add model for `Link` header\n```\n\nthen we can take it from there!\nThanks for this very nice addition!\n. Daniel,\nthanks for this contribution.\nWe won't have any time this week to review and merge this patch though, so it won't make it into the RC1 release which we'll publish this week.\nHowever, maybe we can incorporate it into a later RC if one should be required.\n. This is a very nice PR, Daniel, thanks a lot!\n\nWe have reserved some capacity this week to push for a minor update release.\nOf course we'd like to incorporate the latest and greatest in contributions! :)\nSo, if you have the time to look through my comments I think this patch would be a great addition!\n. Cool, thanks!\n. Daniel,\nI might have a few more improvements suggestions, would you want to apply these as well or would you rather we merge now and take it from here?\n\nIn any case, thanks a lot again for this very nice contribution!\n. Ok, I have added three commits on top of yours (thanks for letting me push directly into your fork!).\nThey apply some refactorings that will make it easier for us to migrate this new code into the coming akka-http (which has slightly different code conventions here and there). I have also added a few more tests, renamings, etc. The main structure of your solution however is exactly the same as it very well fits the \"spray way\".\n\nIt be great if you and @jrudolph would have another quick look over the PR as it stands right now so that we can be reasonably sure that it doesn't break things in a bad way.\n\nI have to say again that this contribution is one of the best that we have received so far! It shows that you have spent considerable time in the codebase to make sure that your patch fits in.\nVery nice work! We definitely owe you a beer! :)\n. Ok, thanks, Johannes.\n@danielwegener What do you think?\n\nI just realized that we probably should cross-check this implementation against the latest HTTPbis spec [here](http://tools.ietf.org/html/draft-ietf-httpbis-p5-range-26) and make sure that we are not implementing obsolete logic.\n. Excellent!\nGreat that you were already on HTTPbis anyway.\n:+1:\n\n(Btw.: Will you be in Berlin in June for #scaladays?)\n. Excellent! Thanks, Daniel, for verifying against the latest HTTPbis draft!\nI've added a comment to show where we (rightfully) depart from the spec by leaving out the `other-ranges-specifier` from the parser rule.\n\n> (I really hope i can come to Berlin, but I am already visiting the #reactconf next month - so since I dont have my own talks, I think I have to take a vacation for #scaladays ;) )\n\nWell, your employer really should send you there and pay your ticket! :)\nAnyway, we'll happily buy you a beer!\n. Absolutely! If you are there the beer is yours! :)\nLooking forward to it!\n. Thanks, Michael!\n. Nice, thanks, Ivan!\n. I added a ticket for the optional bounds: https://github.com/spray/spray/issues/660\n. Jeff, the problem is that we cannot relax/adapt the parser to all buggy HTTP implementations out there.\n`iso-8859-*` is not a valid charset and spray cannot decode the message content if the charset is unknown.\n\nWe've been talking about a feature that makes it easy to \"inject\" a custom actor layer between spray-can and akka-io, which would allow you to patch the incoming request bytes according to your needs. I have just created a ticket for that: https://github.com/spray/spray/issues/648\n. Yes, that's what @jrudolph's fix is doing.\n. Ok, IP parsing updated to non-resolving `InetAddress` creation.\n. Nice solution, much better than before!\n. I think that is as far we can push this fix.\n. This is solved in Akka HTTP by having the HTTP stack be completely decoupleable from the underlying TCP stack. Injecting custom logic in between in not a problem anymore.\n. Serving the same content under two different URIs (the \"with-trailing-slash\" URIs is _not_ the same as its \"without-trailing-slash\" sibling) is not really recommended. See for example: http://googlewebmastercentral.blogspot.de/2010/04/to-slash-or-not-to-slash.html\n\nThe best solution is usually to clearly decide for one or the other and then be consistent about it. If you want to allow both variants you should add a permanent redirect from the less favored variant to the \"officially selected\".\nIn order to gently push users to actually make this conscious decision we'd like the unambiguous directive to be the short one and make the less recommended one slightly less appealing.\n. Yes, with the current DSL you can easily build whatever behavior you require.\nPreviously it was harder to do \"the right thing\" (e.g. redirect non-trailing-slash siblings), which is why we made this change.\n. > Am I missing something or did this just revert the change that broke `path(\"\")` in 1.2-RC1 (from 1.2-M8)?\n\nNo, it doesn't revert to M8 behavior. The reason is that in M8 trailing slashes were always matched implicitly and it was hard to prevent that. After this patch there is no more ambiguity in any path matching directive except for `pathEndOrSingleSlash`, which matches two things.\n\n> It does seem that `pathEnd` is the more \"correct\" solution that is suggested?\n\n`pathEnd` matches exactly what it says and nothing else.\nIf you are upgrading this construct from M8\n\n```\npathPrefix(\"foo\") {\n  path(\"\") {\n    ...\n  } ~ ...\n}\n```\n\nyou now need to decide whether you want\n\n```\npathPrefix(\"foo\") {\n  pathEnd {\n    ...\n  } ~ ...\n}\n```\n\nor\n\n```\npathPrefix(\"foo\") {\n  pathEndOrSingleSlash {\n    ...\n  } ~ ...\n}\n```\n\nand we want to force you to make this conscious decision.\n. Yes, I think we should actually implement support for the `Raw-Request-URI` header on the client side.\nOn the server-side this header allows access to the raw request URI as it was parsed from the raw request bytes.\nOn the client-side we could use this header to allow for overriding the request URI with a user-defined String.\nSo, if a request contains a `Raw-Request-URI` header we render its value rather than the `uri` member of the `HttpRequest` instance.\n. Actually, spray-servlet currently _never_ produces a chunked request when converting from the `HttpServletRequest`. A prerequisite for implementing the `RegisterChunkHandler` protocol is therefore the implementation of incoming auto-chunking (i.e. the `incoming-auto-chunking-threshold-size`) as we already have it in spray-can.\nHowever, at this stage in our release process I don't see this as a necessary addition.\nHow about we therfore reassign this ticket to the `1.3` milestone?\n. Ok, closing for now.\nPlease reopen if you think the issue is still valid.\n. @jiaweihli If you are not using the `ActorLogging` trait, how are you logging?\nDo you think the problems you saw are somehow related to spray?\nIf so, could you distill a stand-alone test case and open a fresh ticket?\n. Also see this related thread on the ML:\nhttps://groups.google.com/forum/#!searchin/spray-user/Excessive/spray-user/qX44Zr8Ea-w/I71j_IGONn8J\n. Mark, thanks for taking this on.\n\nYour solution of simply requiring Windows users to not checkout the sources using the line separator of their platform would work, but I'm not sure if this is the preferred mode for a Windows users.\nI'd think that most Windows users have git `core.autocrlf` configured to `true` rather than `false`, which would cause their working copy to have CRLF line endings.\n\nWe _could_ require Windows users to configure `core.autocrlf` to `false`. However, this would then require _all_ editors that a Window user might use to respect the line ending convention of the repository, which I doubt it feasible.\n\nSo, rather than requiring people on Windows to work with unix-style working copies I'd say that we should fix all tests that do not yet properly deal with windows style line endings, no?\n. > I think think that the group that has core.autocrlf set to false on Windows is quite large, since it is asked during the installation of Git whether you want this. So both true and false are common. So just assuming it is in Windows style causes problems.\n\nOk, if it's true that there really are a lot of people on Windows that do work with unix-style line endings we cannot rely on `System.getProperty(\"line.separator\")` at all.\nOne more question: How do these people instruct their editors to not produce Windows-style line endings?\n. Thanks for reporting!\n. Ok, thanks for reporting!\n. FTR: there is already a nightly out with the fix Johannes referred to: http://nightlies.spray.io/io/spray/spray-can/1.1-20131101/\n. Yes, thanks for reporting!\n. Great, thanks Mark!\n. Not relevant for Akka HTTP any more.\n. Thanks for the feedback, Michael!\nWhen you say \"beginner\", do you mean beginner with Scala or beginner with Akka or beginner with spray?\n\n[This page](http://spray.io/documentation/1.1-SNAPSHOT/spray-can/http-client/request-level/) in our docs appears to show the most basic use of spray's client-side, but you might disagree...\nWhat's missing?\n\nIf you don't care about the difference between the \"connection-level API\" and the \"host-level API\" then you using _spray_ might not actually be the best choice, since you only have very basic client-side HTTP needs. Maybe using [AsyncHttpClient](https://github.com/AsyncHttpClient/async-http-client) or even one of the many synchronous HTTP client solutions on the JVM might be more appropriate?\n. > How do I POST something?\n\nOk, extending the example with something that shows how to marshall something into the entity of a POST request is certainly good. Thanks!\n\n> What is the ? operator?\n\nThis `?` operator has nothing to do with spray, it's Akka's \"ask\" operator and therefore not in scope for our docs.\n\n> How do I access response status, headers and body?\n\nThe example shows you how to get hold of an `HttpResponse` instance. We assume that people would expect the status, headers, body, etc. to be members of this instance (which they are).\nHowever, we should add a sentence mentioning this, thanks!\n. Thanks, @crypticmind!\nI've just created a ticket for tracking this feature: https://github.com/spray/spray/issues/709\n. @michaelklishin While this ticket is awaiting completion: Have you tried [Dispatch](http://dispatch.databinder.net/Dispatch.html)? It might be better suited to your needs with regard to documentation.\nAlso: we have a ML that you can use if you feel that the spray docs are not fit for your particular use case.\n. Nice!\n. Updated to new client-side request timeout logic as discussed.\n. Actually, enabling a direct `ByteString => HttpRequest` conversion as user-accessible API is harder than simply removing access restrictions from a few classes.\nThe current spray-can architecture would require adding a proper API for this.\nHowever, due to capacity limitations this currently out of scope.\n\nI am therefore closing as \"Won't fix\".\n. Just FTR: The request parser in akka-http should be a bit easier to use.\nSo, for new projects I'd recommend to look there.\n. Ok, significantly updated.\nPlease re-review...\n. Right.\nThanks, Andr\u00e9!\n. I just merged this fix into all three release branches, so tomorrow we should have a new nightly available containing this fix.\n. No worries, Michael!\nGlad the issue is resolved...\n. Nice fix!\n. And another thing I just spotted:\nWhile you are at it: Could you add a commit removing the `bytesLeft` method and replacing all calls with a simple `buffer.hasRemaining`?\nI have no idea why I introduced `bytesLeft` in the first place... (probably wasn't aware of `hasRemaining`'s existence).\n. Great!\n. Actually I believe in 1.2 (i.e. Akka 2.2.x) this is not a problem since akka-io in 2.2.x uses a special supervisor strategy that should prevent these log messages:\nhttps://github.com/akka/akka/blob/ce9fd56db43176562aca3eb0930aa44b94349929/akka-actor/src/main/scala/akka/io/SelectionHandler.scala#L95\n\nIn earlier Akka version this new supervisor strategy doesn't exist yet which is why we have to handle `Terminated` events \"manually\" in spray 1.0 and 1.1.\n. So, I guess we don't actually need the akka ticket #3728, but it'd probably be good to confirm that spray 1.2-RC3 doesn't show the respective error log entries.\n. Nice one...\n. Great!\n. @matanster I really like the idea that you voiced over email, namely adding hyperlinks to the respective directive doc pages to all code samples! This will be great! Thanks!\n. Asko has a good suggestion on the ML for this (https://groups.google.com/d/msg/spray-user/vBjKfxF7LhY/-MahLQECimgJ), let's also apply it to the scope of this ticket!\n. Ok, I changed the title of this ticket to clarify the feature request.\nEssence: the default value for the `spray.servlet.root-path` setting should become `AUTO`, which would mean that spray-servlet will call `ServletContext::getContextPath` in `contextInitialized` to set the effective root-path automatically.\n. Building upon Johannes' point I'd say that spray should definitely _not_ accept backslash characters in cookies since the latest cookie spec explicitly forbids them. The recommended approach would be to simply base64 encode your value.\n. Yes, Tim, you are right in that spray can be more user-friendly in that regard.\nI just created https://github.com/spray/spray/issues/737 for it.\nThanks!\n. Off the top of my head wire-logging might be best implemented as an additional stage of the spray-can client and server pipelines...\n. Given the more flexible stream setup infrastructure in Akka one could argue that a \"wire logging\" feature could be implemented outside of Akka HTTP, as a plain and simple \"log what is going through the stream\" stage.\nHowever, similar to [the wirelogging feature provided by the Apache HTTP client](http://hc.apache.org/httpclient-3.x/logging.html) there is value in having a logging stage that has a concept of HTTP.\nFor example it should be possible to restrict logging to only the message headers, i.e. suppress the message entity.\n. Related: https://github.com/spray/spray/issues/756\n. Yes, we should the following parameters to the implicit param list of `startServer`:\n- `eh: ExceptionHandler`\n- `rh: RejectionHandler`\n- `rs: RoutingSettings`\n. Ok, thanks, Andr\u00e9!\nI don't really have a problem with the fix in this patch for the time being.\nBut I agree with @jrudolph that a \"proper\" solution would be preferable...\nHow about we add a ticket \"Clarify error message localization\" and merge this PR?\n. Thanks, LGTM.\n. Nice, Andr\u00e9!\nLGTM\n. Andr\u00e9, can you rebase on top of the current master, so this merges cleanly?\n. Great! Thanks, Andr\u00e9!\n. Thanks, Andr\u00e9, for pushing this forward.\nThe problem with relying on `System.getProperty(\"line.separator\")` in any way is that we cannot be sure that what the user has checked out into his/her working copy really corresponds to the the platforms line separator or not.\nSo the third point in your three point bullet list above is the crux.\n\nHowever, we should be able to work around that by changing all tests to accept **both** types of line separators.\nIf we normalize all multi-line strings with something like `replace(\"\\r\\n\", \"\\n\").replace(\"\\n\", \"\\r\\n\")` we can be sure that we'll always see windows style line seps.\nThen we can remove all references to `spray.util.EOL` completely.\nWDYT?\n. Thanks, Andr\u00e9!\n\n> stripMargin is only used for declaring multipart content (parsed by mimepull), json samples and config samples.\n> Should I leave them as they are?\n\nYeah, since these types of things should be EOL agnostic themselves I think we should be fine as is.\n\nIf @jrudolph agrees I'm ready to merge this patch then...\n. I agree with Johannes that we cannot simply and blindly all cookies to another host!\nSo I think this patch simply exchanges one small problem against a big one.\nTherefore closing for now...\n. Hm... I'm probably somewhat slower today, but I'm not seeing what this PR does, i.e. buys us.\nCan you explain once more what the problem is that you are solving with this?\n. Ah, ok, I get it now.\nYes, the model is not perfect currently in this regard.\nLet us think a bit about how to solve it best.\n(We also have to watch out for binary compatibility with all spray patches since the final...)\n. @jrudolph Yes, sounds good!\n. Apart from the small nitpick this looks like a nice patch!\nThanks, Flavio!\n. It currently shouldn't happen, however it's \"good style\" to always write _robust_ code in the sense that it should also work if the environment around it changes (within reasonable bounds), e.g. in the case of refactoring.\nIn this case here `input.length < offset` might become a valid state at some point, which might cause a harder to find error if we keep the `==` comparison as is.\n\nAnd secondly: The current comparison immediately raises my alarm bells when reading the code and I have to actively think about whether `input.length < offset` is a valid state or not. Changing the comparison to `input.length <= offset` takes care of this (i.e. looks right), expresses the local condition better and is generally more robust.\n. Ok, thanks Flavio for the analysis and submitting this patch!\n. Thanks a lot for this patch, Mark! (And sorry for only getting to it now).\nWe have reserved some capacity this week to push for a minor update release.\nOf course we'd like to incorporate the latest and greatest in contributions, which included this patch!\n\nCould you accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) so we can get this merged ASAP?\n. Would you like us to incorporate the comments from yesterdays's review or do you want to do them?\n. Great! Thanks!\n. Great! Thanks, Mark!\n. Apart from the small comment I think this is a very nice addition, Pablo! Thanks!\nIf @jrudolph agrees then I'd like to get this merged very soon.\n(We have reserved some capacity this week to push for a minor update release.\nOf course we'd like to incorporate the latest and greatest in contributions! :)\n\n@fernandezpablo85 Could you squash all your commits into one bearing this message:\n\n```\n+ can: add support for multi-host `Connect`\n```\n\n(in accordance with out [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages))\n. Yes, @2beaucoup is right (once more).\nApart from proving correctness some basic tests would also help us in making sure that your addition remains functional even as we shift stuff around in the future....\n. Once easy test would be to bind to three ports on localhost of which only the third actually accepts connections. Then you could `Connect` to all three and verify whether you get connection attempts on all three, with only the last one actually succeeding.\n. This PR has been sitting around for far too long, given the value that it provides.\nHowever, as @jrudolph already said before, we won't be merging it into spray anymore.\nRather, we'll use it as a basis for clearing the equivalent of https://github.com/spray/spray/issues/746 on the Akka side.\n\nThanks again, @fernandezpablo85, your contribution is still very much appreciated.\nIf you like you are also very welcome to resubmit this patch against the [`release-2.3-dev` branch of the Akka codebase](https://github.com/akka/akka/tree/release-2.3-dev). Otherwise we will simply turn to your code when closing the ticket in Akka.\n. Great! Thanks @fernandezpablo85 !\n. No problem. We will probably come back to your solution here when we tackle https://github.com/akka/akka/issues/16827.\nThanks again!\n. This will happen anyway in the documentation effort for Akka HTTP.\n. Thanks, Andr\u00e9!\n. After reading up on the discussion in #739 again I think we need two different model classes for cookies: one for what is contained in the `Set-Cookie` header (what we already have) and one for what the `Cookie` header contains. Maybe the latter can be a super-type, maybe not.\n. Ok, apart from the small efficiency improvement I think this patch is ready for merging!\nThanks, Benjamin!\n. Nice! Thanks, Benjamin!\nThis is a great first OSS contribution! :)\n. Ok, thanks for this patch!\nCan you do the two following things for us to be able to merge your patch?\n1. Sign the [Typesafe CLA](http://www.typesafe.com/contribute/cla)\n2. Change the commit message to `= examples: fix incorrect dtd reference`\n. Thanks, Dominic!\n(The `=` is required by our [commit msg policy](http://spray.io/project-info/contributing/#git-commit-messages))\n. Thanks!\nBefore we can accept your pull request could you:\n1. Accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla)\n2. Change the commit message to `= docs: fix typo, closes #749` (in accordance with our [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages))\n. I understand your frustration.\nHowever, since spray is already a somewhat mature project we do have to adhere to certain processes with regard to accepting any kind of patch. Also note that there is no need to open a another PR just for making a change.\nSimply pushing into the existing one would do.\n. Thanks!\n. Sorry for taking so long in responding to your PR!\n\nWe have reserved some capacity this week to push for a minor spray release and would like to incorporate as many good contributions are possible, with this being definitely in scope.\n\nCould you squash all your commits into a single one?\n. Thanks for your input, @hamiltont, I think you definitely have a point.\nWe'll likely address it soon with our current work in akka-http.\n. Sorry for taking so long to get back to this PR!\nWe've been awfully busy with the migration to Akka HTTP, which is now nearing completion.\nTherefore spray is now in maintenance mode and we are reluctant to change the API more than absolutely necessary by crucial bug fixes.\n\nSince we are all in agreement that some kind of prepackaged proxy solution is likely to be valuable to a great number of users I'd propose to resubmit this PR against the [`release-2.3-dev branch of the Akka repo](https://github.com/akka/akka/tree/release-2.3-dev). We can promise that any PR issued against the Akka codebase will be dealt with a lot faster than we've been able to here in the spray repository.\n. I'm keeping this ticket alive and mark it for migration to Akka since @ivantopo provided a very nice discussion of the trade-offs involved in client-side request timeout handling.\nIn Akka HTTP we will have to come back to the question of how to best implement client-side request timeouts and this ticket will certainly of value.\n. Another interesting PR that we should look at when tackling this: https://github.com/spray/spray/pull/957\n. The `RoutingSpec` contains a mere 4 lines of code, so it should be easy to rebuild it for one's specific needs. Do you really think it would add that much value?\n. Closing for now.\n. Thanks for reporting.\nYes, spray should access an illegal `Origin` header as a `RawHeader`.\nI don't think we should relax the Origin parser, but we definitely need to prevent the `IllegalUriException` from crashing the parser completely.\n. Guys, we have reserved some capacity this week to push for a minor update release.\nOf course we'd like to incorporate the latest and greatest in contributions! :)\nSo, since this patch here looks valuable on its own, I'd like to get it merged.\nThe certainly also interesting additions that Andr\u00e9 proposed can be added in a potential second step.\n\nAge, would you consider this patch \"ready for merge\" as it stands now?\n. Ok, Andr\u00e9, would you say that your solution supercedes this one or would it be independent of what Age proposes here?\n\n> I'm still curious as to whether you like the \"built into the file/resource directives\" approach, which is slightly more efficient, or Andr\u00e9's more general approach.\n\nSince the `FileAndResourceDirectives` are already somewhat \"meaty\" in the sense that they do quite a lot I'd currently say that your \"built-in\" solution is good enough. Since the header is properly modelled people can still built their own logic around the `If-Modified-Since` header if they want/need to.\n. Ok, then we'll wait for your PR...\nThanks again!\n. Thanks, Alex!\n. Reassigned to milestone `akka-http` due to the breaking nature of the fix for this.\n. Will not be fixed in spray anymore.\nHowever, lazy route evaluation is what the routing DSL port to akka-http does.\n. This will not be merged short-term due to the breaking nature of this fix.\nWe'll keep it open though, because it'll come with akka-http.\n. Closing as this will not be merged into spray anymore.\n(However, the change _is_ part of the port to akka-http!)\n. Please [this ML post](https://groups.google.com/d/msg/spray-user/hMZndG4IYRs/5DRpAdGdHloJ) on why we have `StandardRoute`.\n. spray _does_ depend on Akka and will not function without it.\nWhat exactly is it you are trying to do?\n. Since we'll move into akka soon completely I don't think we'll reopen this issue.\nHaving an HTTP model that doesn't depend on akka is not a priority for us.\nWhat's your use case again?\n. @gmalouf We are not aware of many users actually making use of this stand-alone nature of spray-http. In fact, @kgurkan and you are the only ones we know of.\nSince upholding full separation between modules comes at a certain cost we do have to trade off between cost and benefit here. Sorry.\n. Yes, thanks for the heads-up!\n. Kender, thanks for reporting!\nWe'll look into this ASAP.\n. Tulio, great, thanks for this fix!\n\nTwo things we'd need before we can merge this:\n1. Can you accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla)?\n2. Please rephrase the commit message to:\n\n```\n= servlet: add termination awaiting to `Initializer::contextDestroyed`\n```\n\n(in accordance with our [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages))\n. Thanks for accepting the CLA!\nCould you also update the commit message?\n. There is no spray release yet which has been compiled against Scala 2.11.\nWe simply haven't officially upgraded yet.\nIt will come though... :)\n. Cherry-picked into master\n. Thanks, Dmitry, for finding and fixing!\n. This is a great PR, thanks Andr\u00e9!\nThis stuff is quite hard to get right and you took us almost all the way already! Very nice!\n\nApart from the inline comments I'd also agree with Age on somehow adding `If-Modified-Since` / `If-None-Match` support to the `FileAndResourceDirectives`, since this is something that people can benefit from without the need to change _anything_ about their existing routing structure.\n. Very good discussions!\n\n> Another question is if we should move this stuff to a new *CacheDirectives trait altogether which we can mix into the Directives trait by default.\n\nYes, absolutely.\nThe `CachingDirectives` are separate because they require a manually-added dependency onto `spray-caching`. We should definitely move these new things into a new trait which is mixed into `Directives`.\nHow about `CacheConditionDirectives` for the name?\n\n> I'm not yet convinced to put ETag support into FileAndResourceDirectives by default (yet).\n\nOk, I guess what we are missing is some setting of requirements, i.e. what do we really want to achieve. I'd say we need the following:\n1. Reduce server-load and bandwidth by supporting cache-conditional HTTP mechanisms (`If-Modified-Since`, `If-None-Match`) for static content.\n   \n   By building this into the `FileAndResourceDirectives` people would be benefitting from this without having to change any code, which is great.\n   Since in the `FileAndResourceDirectives` we are in complete control over how to construct ETags we can choose a mechanism that relies on file size and modification times and thus avoid the need to actually open and read the file.\n2. Give the user some tools to achieve the same benefits for his/her own non-static resources.\n   \n   This is somewhat harder since we don't generally know the parameters that cause a response to change. The dumb (and easy) way is to simply let the user code run and render the resource before using something like MD5 to compute an ETag for it. The problem is that this approach does not reduce server load, it could actually be increasing it! We would save bandwidth but the tradeoff is not necessarily in favor of actually applying ETag support with this approach.\n   Therefore it'd be better if our solution allowed the user to supply us with logic that computes an ETag purely from request parameters, i.e. a function `HttpRequest => EntityTag`. Assuming that this function is reasonably fast we can achieve the bandwidth and server-load reduction even for non-static routes.\n\n> > The spec says (RFC2616 14.26) that, if the If-None-Match tags match, the server \"MUST NOT perform the requested method\". However, we are unconditionally calling our inner route here, which will always perform the requested method. This means that we'll potentially execute PUTs (etc.) when we are not allowed to!\n> \n> Yeah that's true. How would you attempt to fix that?\n> \n> I experimented with transforming the request to HEAD and looking and the response > headers. That would only work if head is explicitly implemented (without transparent-head-requests).\n\nAs discussed above we should require the user to supply us with an ETag calculation function. The \"dumb\" approach of always running the inner route is not really attractive IMHO.\n\n> I think the directives should be tied together because if `withIfNoneMatchSupport` matches, a lower level withIfModifiedSinceSupport wouldn't see the request and validate its `If-Modified-Since`.\n\nYes, I see the connection between the two.\nHow about we solve it by making `withIfModifiedSinceSupport` only kick in if the request does not also contain an `If-None-Match` header?\n\nSo far spray's convention of building more complex directives from simpler ones has really paid off. It makes the implementations easier to understand and test and also increases flexibility for the user. Ideally I'd like to uphold this principle also here.\n\nSo, it seems to me that we need these 4 new directives:\n- `withIfModifiedSinceSupport(f: HttpRequest => DateTime)`\n- `withIfUnmodifiedSinceSupport(f: HttpRequest => DateTime)`\n- `withIfNoneMatchSupport`(f: HttpRequest => EntityTag)\n- `withIfMatchSupport`(f: HttpRequest => EntityTag)\n\nThe `FileAndResourceDirectives` would internally use `withIfModifiedSinceSupport` and `withIfMatchSupport` where they can.\n\nDoes that sound reasonable?\n. Yes, the `withIfNoneMatchSupport` would still have to check for the presence of an `If-Modified-Since` header. However, it would do so with purely its own functionality in mind.\nIt's more or less the \"do-one-thing-only\" principle, `withIfNoneMatchSupport` should have the least-possible scope to correctly do its thing.\n. Ok, I looked through the RFC again and compiled this table to show the relationship between the `If-Modified-Since` and `If-None-Match` headers (using [this](http://www.tablesgenerator.com/text_tables#)):\n\n```\n+---------------------------+--------------------------------------------------------------+\n| Response for a request to | Request Header: `If-None-Match: tag list`                    |\n+ a resource with `ETag: X` +--------------------------------------------------------------+\n| and `Last-Modified: LM`   | contains X       | does not contain X | header not present   |\n+---------------------------+------------------+--------------------+----------------------+\n|            | LM <= TS     | 304 (GET/HEAD) / |                    | 304 (GET/HEAD) /     |\n| Request    |              | 412 (other)      |                    | Inner route response |\n| Header:    |              |                  |                    | (other methods)      |\n+ `If-       +--------------+------------------+                    +----------------------+\n| Modified-  | LM > TS      |                   Inner route response                       |\n+ Since:     +--------------+------------------+                                           +\n| TS`        | header       | 304 (GET/HEAD) / |                                           |\n|            | not present  | 412 (other)      |                                           |\n+------------+--------------+------------------+-------------------------------------------+\n```\n\n(Additionally the server should attach `ETag: X` and `Last-Modified: LM` headers to all responses.)\n\nLooking at this table I agree with you in that the logic is too entangled between the two headers to efficiently be pulled apart into two dedicated directives. So we need to put it into one single directive after all (sorry for taking so long to realize this!)\n\nHow about this proposal:\n1. We add the model classes for these new headers:\n   `If-None-Match`, `If-Match`, `If-Modified-Since` and `If-Unmodified-Since` (as you already did)\n2. We reduce the scope of this PR to only implement the logic from the table above as one directive (plus extension of the `FileAndResourcesDirectives`), i.e. we remove all logic regarding the less important `If-Match` and `If-Unmodified-Since` from the scope of this PR. This should help in keeping us focussed and getting this done in time for the release.\n3. We add a `CacheConditionDirectives` trait which for the time being only holds one one new directive called `conditional` (and is the place where other directives for `If-Match` and `If-Unmodified-Since` can go in the future.)\n   \n   The `conditional` directive has this signature:\n   \n   ```\n   def conditional(eTag: EntityTag, lastModified: DateTime): Directive0\n   ```\n   \n   It doesn't take a function but requires the user to directly specify the `eTag` and the `lastModified` time stamp for the resource. This makes it a directive that \"automatically\" must be used quite far down in the route structure, where the exact resource has already been established and the `eTag` and `lastModified` values are available. It also requires that a user specifies both values (as supplying only one does not allow us to correctly implement the RFC).\n4. We extend the `FileAndResourcesDirectives` to make use of the new `conditional` directive internally.\n\nWDYT?\n. > What would you suggest as the default implementation for file/resource ETags?\n\nI'd say we take the file modification time (8 bytes) and file size (another 8 bytes) and Base64 encode (`org.parboiled.common.Base64`) these 16 bytes. This results in a 22 char String which should be good enough as an ETag value.\n. Very interesting, Andr\u00e9, thanks!\nI really should have checked the latest HTTPbis spec earlier. Your assumption is right, we do follow HTTPbis wherever we can. In that regard the table I compiled earlier is obsolete and the logic we need to implement is indeed simpler, since there is less entanglement between `If-Modified-Since` and `If-None-Match`. It seems we can implement `withIfModifiedSinceSupport` and `withIfNoneMatchSupport` after all, right?\n. Another thing: I have added the `conditional` support to the `FileAndResourceDirectives` here: https://gist.github.com/sirthias/9392921\n\nYou could simply apply that patch to your fork.\n\nBy the way: We are truly impressed by the amount of commitment you put behind this contribution as well as the quality of your code! Hats off! We definitely owe you (another :) beer! Maybe in Juni in Berlin? :)\n. We simply follow HTTPbis where it is in disagreement with RFC2616.\nIn many cases HTTPbis simply encodes what is considered \"best practice\" with all the experience that has been accumulated after RFC2616 was released.\n. The is a very good find, Age!\nI definitely need to take the time to read the HTTPbis document completely.\nLooking at these precedence rules it seems to me that a unified implementation like the one that Andr\u00e9 already implemented is the only way to go.\nWe can't sensibly put the burden of properly assembling a number of different directives in the right order onto the shoulders of the user.\nSo, my current take is: leave it at a single `conditional` directive, which closely follows the precedence logic in section 6. Would you agree?\n. @2beaucoup Stand-by for a PR for your PR adding the `If-Range` bits that are still missing (but now doable since we have merged #612)\n. Just about to figure that out... :)\n. Yes please, I'll send a PR shortly.\n. Ok, PR sent.\n. Great!\nNow we only need to wait for @jrudolph's ok and then we can tick off this beast as solved...\nThanks once more for pushing this PR through the stony path to success!\n. I'll add docs after the merge, no problem.\n. Andr\u00e9, could you prefix you second-to-last commit message with `= routing:`?\nAfter that I'd really like to finally push that green `Merge` button here... :)\n. Great, thanks again, Andr\u00e9!\n. Nice!\n. Apart from the question whether `HTTP/1.0` is still being handled correctly this LGTM.\n. Njal,\nspray follows RFC3986 with regard to URI parsing and rendering, which states in [its section 2.3](http://tools.ietf.org/html/rfc3986#section-2.3):\n\n> For consistency, percent-encoded octets in the ranges of ALPHA\n>    (%41-%5A and %61-%7A), DIGIT (%30-%39), hyphen (%2D), period (%2E),\n>    underscore (%5F), or tilde (%7E) should not be created by URI\n>    producers and, when found in a URI, should be decoded to their\n>    corresponding unreserved characters by URI normalizers.\n\nSo, what you are asking for it essentially a departure from the spec, which is something we would like to avoid.\n. Note that the fix to ticket #654 might present a nice way out of a potential compatibility problem with a misbehaving client or server.\n. I really don't know why you don't like this particularly homogeneous piece of code...\nThere is definitely some prettiness to the amount of repetition here!\n(Please refrain from `git blaming` anyone here!)\n. Thanks.\nCould you append `, fixes #804` to the commit message?\n. Thanks, Benjamin!\n. Thanks, Benjamin!\n. > What about [optional]HeaderValuesByType for extracting several values at once?\n\nWhich headers for you have in mind for this directive?\nOff the top of my head there are not that many headers that usually appears multiple times in a request.\n. Ah, I see.\nIdeally we'd be able to use an `HList` for specifying the header types, e.g. like `optionalHeaderValuesByTypes[Range :: Cookie ::`User-Agent`:: HNil]`, which would then result in a `Directive[Option[Range] :: Option[Cookie] :: Option[`User-Agent`] :: HNil]`.\nDoable, but would require another magnet layer and some shapeless magic. Maybe we leave it as an exercise to the user... :)\n. Closing for now as we think #811 is invalid.\n. Thanks, Christian!\nI think your request is a valid one, but we'd need your patch to not degrade the runtime performance characteristics of the existing code. So, if you could simply extend the existing parsing solution rather than replace it with a regex I think your patch can go in.\n. Great, thanks!\n. Btw: We'd also need you to accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) before we can merge your patch!\n. Thanks again, Christian!\n. Thanks!\nFor our code mgmt. processes it'd still be better if you targeted this PR against the master branch.\nThe releases branches can then receive it from there...\n. Superseded by https://github.com/akka/akka/issues/15495\n. Just FTR: If this ticket results in some kind of change this change will not be one that can be released in a minor spray release. So, it looks like we have some more time to discuss things here since any resolution will only be released with akka-http.\n. Fixed by the new \"binding\" API in akka-http.\n. Unfortunately this solution won't work.\nWe can not reliably have more than one ACK pending when talking to akka-io.\nThe akka-io layer does _not_ itself maintain a really buffer, it can only ever deal with one single pending Write. If you send a 2nd write before the first one has been acked you'll provoke a `CommandFailed` error.\n. > For HTTP, we can almost be sure that one Tcp.Received will be followed by a Tcp.Write (request-response pair).\n\nNo, you can't!\nThe IO layer will deliver a `Tcp.Received` for every chunk of bytes that is read from a connections send buffer in one go. A single `HttpRequest` might be arriving in one chunk or 100 partial chunks. You don't know and it's completely indeterministic.\n\n> Per @jrudolph 's concern, we'll try to add a stage under SSL stage, the new added stage will try to control the data flow sending to SSL, i.e. buffer the Tcp.Received, send one to SSL, waiting for ACK, then send next.\n\nI'm not sure what you actually want to achieve.\nWhat's you goal?\n. Note that the `FutureDirectives` already give you a way to do pretty much what you are looking for.\nE.g., you can say something like this:\n\n```\ndef futureAuthorize(check: => Future[Boolean]): Directive0 =\n  onSuccess(check).flatMap(authorize(_))\n```\n\nIn that sense one could even argue that we should simplify the `authenticate` directive to _not_ take a future and rely on the `FutureDirectives` for the future handling.\n. The problem is indeed that the server does not send a `Content-Length` header which, according to the spec, carries the semantic that the closing of the connection signals the end of the response.\nIf your server doesn't close the connection it effectively stalls any progress, the client _cannot_ be the one closing the connection in that case, because there might still be content coming.\n\nThe HTTPbis spec section that @agemooij rightfully quoted specifies exactly what the problem is (how the server is misbehaving). Therefore I don't see anything that we can do from the spray side to get this problem fixed for you.\n. We already have a branch for 2.11 which runs nicely:\nhttps://github.com/spray/spray/tree/unstable/1.3-scala-2.11\n\nSo, we could release a stripped down build of spray 1.3.1 for Scala 2.11 very soon.\nHowever, it would have to be \"stripped down\" for as long as our aux dependencies (lift-json, json4s, play-json) are not all available (so the respective (un)marshalling support for them in spray-httpx would have to be removed).\n. Ok. The current plan is to publish\n\n```\n\"io.spray\" % \"spray-xxx_2.11\" % \"1.3.1\"\n```\n\nnext week after the easter holidays, i.e. Tuesday or Wednesday.\n. Update: We are still waiting for akka to become fully released against 2.11.\nOnce this is out we'll be publishing a nightly build for Scala 2.11 which will effectively be spray 1.3.1 cross-published (with the exception of JSON support for missing aux dependencies).\n. My take is that it won't take long.\n. Ok, a Scala 2.11-compatible build is now available:\nhttps://groups.google.com/d/msg/spray-user/OMHBs_rGPG4/39eQ5i1F1LQJ\n. Very nice! Let us know how things go!\n. Sorry, @cowboy129, but we currently don't have any capacity for a code review.\nWe'll have to come back to this later.\n. I agree with @jrudolph.\nBtw: just to help me understand: what was the change from 1.3.0 to 1.3.1 that break things here?\n. Thanks @topping for your perspective.\nWe'll discuss this PR tomorrow.\n\nOne question: If we merged this PR and published a (date-stamped) nightly build, would that solve the problem for you guys in the short-term?\nLike @jrudolph explained we'd like to avoid having to go through a full release cycle just for this patch.\n. Ok, I just published `1.3.2-20140428` to http://repo.spray.io.\nLet me know whether you need this patch also in a Scala 2.11 compatible build.\n. @vatel Ok, just published `1.2.2-20140526` to `http://repo.spray.io`.\n. Ok, I have just published build `1.3.2-20140909` with cross-paths for 2.10 and 2.11 to repo.spray.io.\nLet me know if you need anything else.\n. Grzegorz, we'll eventually upgrade to shapeless 2.0 ourselves and sort out the problems, but, unfortunately, don't have to capacity to do it right now...\n. Yes, thanks once more, Andr\u00e9!\n. As we don't use mimepull anymore in akka-http we should verify that filenames in multipart/form-data uploads, which are encoded with UTF-8 on the client-side, do indeed produce the original filename when inspected on the server-side.\nAdding a respective test might be all that is required.\n. The simplest solution might be to simply scan the string produced by the Json4sMarshaller for the first non-whitespace character. If it is not `[` or `{` we should throw an exception.\n. Yes, Dominic, this is a bug. Thanks for finding and fixing!\nThe bug was always a bug but did not manifest itself in Akka version 2.0 (when we wrote this initially) as back then `promise.future` would return the `Promise` object _itself_.\nIn some Akka upgrade this was changed turning this bug into an actual problem.\n\nWhat we are missing now is a test to finally nail this thing shut.\n. No, you are still right.\nWe cannot do `store.putIfAbsent(key, promise.future)` in one place and then `store.remove(key, promise)` in another. Even if this currently works it still a bug in our code, since it relies on implementation details that we do not control.\nSo, your patch is a valid one.\n. Thanks again, Dominic!\n. Thanks for voicing your concern with the `HttpHeader` extractor.\nYou are right, the extractor will only match if you specify the header name in all lower-case.\nThe reason is that the Scala extractor design makes it _impossible_ to construct an extractor, which matches strings case insensitively!\n\nThis is the implementation:\n\n```\nobject HttpHeader {\n  def unapply(header: HttpHeader): Option[(String, String)] = Some((header.lowercaseName, header.value))\n}\n```\n\nAs you can see the logic has no way to access the string that you supply to the pattern match. All it can do is _provide_ a string that scala's pattern matching logic will then compare with the one you have specified. This comparison is based on `equals` and is therefore always case sensitive.\nThis limitation is therefore one imposed by the language and not a deficiency of spray.\n\nStill, you are right in that better documentation of this limitation is required.\n. Thanks for this patch!\nBefore we can merge it we'd need you to:\n1. also accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) (thanks for your sending us the spray one). Note that there is no need to send Typesafe anything for this.\n2. Change the commit message to\n   \n   ```\n   = httpx: unwrap `MalformedContent` error in `ResponseTransformation`\n   ```\n   \n   (as per our [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages))\n. No problem. Our travis build is unfortunately not as stable as we'd need it to...\n. Thanks, Stanislav!\nHowever, even though this is just a cosmetic fix, we'd still need you to accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) before we can merge this PR.\nAlso, could you change the commit message to\n\n```\n= docs: fix typo in handleExceptions.rst\n```\n\nto be in line with our [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages)?\n. Great! Thanks, Stanislav!\n. Thanks, Stanislav!\nCould you add this commit to https://github.com/spray/spray/pull/847?\n. And one more. Thanks again, Stanislav!\n. We have zero experience whatsoever with OSGi and therefore would need very detailed instructions regarding a fix of this problem, ideally a PR.\nFor example: what exactly do you mean by \"the import definition of the sun.misc package in spray-util-1.3.1.jar\"?\n. Thanks, Richard.\nHere we'd need the CLA as well as a rewritten commit message as well.\nThanks!\n. Yes, thanks Richard, not resetting the `_byteCount` is indeed a bug.\n\nBefore we can merge your patch we'd need you to\n1. Accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla)\n2. and change the commit msg to `= http: fix HttpData.Builder not clearing internal counter in 'clear'` in accordance with our [commit msg policy](http://spray.io/project-info/contributing/#git-commit-messages)\n\nThanks again!\n. Thanks again, Rich!\n. Yeah, thanks! And no worries about the commit msg.\n. Thanks, Chad, for putting in this PR!\nI think this is a valid addition.\nWe should be able to keep the main code path a (small) bit more efficient though if we move your codepoint logic into the last `case` branch. AFAICS the other case branches (c < 127) will never suffer from this problem.\n\nAlso, we'd need you to\n1. Accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla)\n2. and change the commit msg to `= http: fix URI encoding of non-BMP chars` in accordance with our [commit msg policy](http://spray.io/project-info/contributing/#git-commit-messages)\n\nbefore we can merge your patch.\n\nThanks again!\n. Thanks for accepting the CLA!\n\nIt seems we don't have to do the `isHighSurrogate` call for all code paths.\nFor the first three `case` branches we already know that `charSize` is `1`.\nHow about this instead?:\n\n```\nval charSize = string.charAt(ix) match {\n  case c if is(c, keep)     \u21d2 r ~~ c.toChar; 1\n  case ' ' if replaceSpaces \u21d2 r ~~ '+'; 1\n  case c if c <= 127        \u21d2 appendEncoded(c.toByte); 1\n  case c \u21d2\n    def append(s: String) = s.getBytes(charset).foreach(appendEncoded)\n    if (Character.isHighSurrogate(c)) { append(new String(Array(string.codePointAt(ix)), 0, 1)); 2 }\n    else { append(c.toString); 1 }\n}\nrec(ix + charSize)\n```\n. Encoding URLs is a hot code path in spray, so accepting a little bit of ugliness in exchange for many billions of clock cycles saved across all users of spray is ok... :)\n. Not that I can see right now.\nOnly our limited capacity for work on the next spray minor release... :)\n. Thanks, Chad!\nI think this patch as well is a valid one.\nIt does slightly decrease performance in certain cases, but since this only affects Uris that actually contain a percent sign this is quite acceptable.\n\nHere we also need you accepting the CLA and rewrite the commit msg to:\n`= http: fix Uri decoder not properly decoding pct-encoding for certain charsets`\n. Fixed by 9c70a86e8089edbd77daddd84f94b12407761f50\n. Revisiting this ticket we weren't sure anymore whether there is a nice solution that let's us improve over what we already have in akka-http within the boundaries of the current DSL design, which comes with limited inspectability.\nOnce the DSL has evolved to its next step (with better support for programmatic inspection) things will likely be easier here.\n. Thanks, Daniel!\nWe'll look into this as soon as our capacity allows.\n. It's going to be fixed with the coming akka-http.\nThe work-around is to use a patched version of spray that you build yourself.\n. Yes, absolutely.\nThis code is among the oldest we have in the codebase.\nI am surprised that this hasn't caused any issues earlier.\n. Don't worry about the Travis error, unfortunately Travis is somewhat unstable for spray.\nThanks for the patch, Haoyi!\n. Ok, thanks for reporting.\nI think this is a valid request.\n. We are following the latest HTTPbis document for the header value grammars.\nIn this case this section is the relevant one: http://tools.ietf.org/html/draft-ietf-httpbis-p6-cache-26#section-5.2\n\nWhich client/server is generating the `http://tools.ietf.org/html/draft-ietf-httpbis-p6-cache-26#section-5.2` cache directive in your case?\n. >  IIUC it has the same effective grammar here, which allows Cache-control: no-cache=field-name.\n\nAh, yes, I see it now. The grammar for `no-cache` differs depending on whether the message is a request or a response. Sorry for not seeing that earlier.\n\nHowever, it seems spray can already parse field-names after `no-cache` directives:\nhttps://github.com/spray/spray/blob/master/spray-http/src/main/scala/spray/http/parser/CacheControlHeader.scala#L44\n\nCould you provide us with a failing test case?\n. Thanks again, Brian.\nClosed by 02cd11efa78e07dfa2cbed6d2626ab28390ea143.\n. Solved in akka-http by having everything in a route structure be evaluated lazily.\n. Great, thanks!\ncherrypicked into master.\n. Ok, I see.\nSo all that is required is adding back `\"shapeless.*` to the spray-routing OSGi imports?\n. Great, thanks for verifying!\n. According to the spec governing the Cookie header cookie-pairs must be separated by `;`, not by commas: https://tools.ietf.org/html/rfc6265#section-4.1.1\nTherefore Jersey client needs to be fixed here, not spray.\n. Ah, yes, right. Missed the discussion there.\nJust though about this once more and since commas are illegal anyway in a cookie value I think there is no risk in accepting them as separators as well.\nI'll reopen this PR.\n. Could you accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) (only a few clicks) and change the commit message to\n\n```\n  = http: accept comma as cookie separator, closes #869\n```\n\n?\n. Great, thanks William!\n. If you have trouble with certain aspect of spray please ask on the ML before opening a ticket!\n. This is solved in akka-http by having everything in the route structure being evaluated lazily.\n. Nice!\n. Thanks, Greg!\nWe'll look at this patch as soon as our capacity permits.\n. Apart from the small comments this look good, thanks Greg!\nCould you accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) (only a few clicks) so we can finally get this merged?\n. Thanks, Greg!\nWe've missed the deadline for the 1.3.2 release with this, and will probably have to include it in akka-http directly.\n. The difference is that Greg's `optionalAuthenticate` directive recovers from `AuthenticationFailedRejections` produced by the authenticator by extracting `None`.\nYou can achieve the same effect by simply wrapping the underlying authenticator, yes.\nThe `optionalAuthenticate` directive simply pulls this wrapping up to the directive level.\n. Thanks again, Greg!\nAnd sorry for taking so long to finally merge your patch!\n. Great, thanks!\n. spray has traditionally followed a \"no cross version suffix\" artefact naming policy because our central dependency Akka has done the same (until Akka 2.0.x) and was always tied to a specific Scala version anyway, i.e. there were no real cross builds across several Scala versions.\nAkka 2.3 is now the first Akka version to actually be cross-built across two Scala versions.\n\nSo, this is the time when our naming policy broke down and we had to add a cross-version suffix for the Scala 2.11 build.\nHowever, we are not going to retroactively add cross version suffixes to our builds and we are not planning on releasing another spray version (apart from patches).\nSo, even though in hindsight we probably should have enabled cross-version suffixes right from the start it is now to late in spray life cycle to make this change in the artefact naming policy.\n. Thanks for this PR!\nWe'll get to it during our current migration work of spray to akka-http.\n. Ok, we just discussed this and think you change is indeed a good one.\n(As you merely change the default charset to UTF-8).\n\nCould you accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) (only a few clicks) so we can merge this?\n. Thanks!\n. Thanks, Alex.\nCould you accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla), so we can merge your patch?\n. Great! Thanks!\n. Closing as \"Wont fix\". See #685 for more details.\n. Since timeout (re)configuration between the layers will always come with its own set of problems we'd now propose to name this new directive `withShorterRequestTimeout` and have it implement an _additional_ timeout, on top of what is configured underneath.\nThis way we don't need any communication between the layers and the user would have to set a long timeout underneath which gets (gradually) refined in the routing layer.\nWhile this requires a little bit more thought and work on the part of the user it appears to be the overall cleaner solution.\n. A @jrudolph said let's discuss all issues on a case by case basis.\nPlease ask on the mailing list or open a ticket is you have a concern for anything in particular.\n. Very cool!\n. spray's cookie parser follows the spec here:\nhttps://tools.ietf.org/html/rfc6265#section-4.1.1\nwhich disallows \\u0001 in cookie octets.\nHowever, the current version of spray should not \"throw the whole response away\" but rather log a warning an deliver the offending `Cookie` header as a `RawHeader`.\nWhich spray version are you using?\n. Thanks, Tim, this LGTM.\nCould you accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) so we can get this merged?\n. Great, thanks!\n. A nice fix for this ticket would be to simply add a `port` member to the `RemoteAddress.IP` class.\nHowever, since this would be a breaking change to the spray API I'm assigning this ticket to the Akka HTTP.\n. Thanks for this patch, Richard!\nCould you accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) (only a few clicks) so we can merge it?\n. Thanks, Rich, for this patch!\n(And sorry it took so long before merging.)\nThe change will go online under http://spray.io/documentation/1.1-SNAPSHOT/spray-httpx/de-compression/ and will be moving into the main docs once we publish the next release.\n. Since the `jsonp` directive is not available anymore in akka-http I'd now say that this ticket can be closed by adding a warning to the directive's doc page.\n. Closed by e0550720f056bcb1c7836031fef8a51b36e5a6da\n. Thanks, Noel!\n. Closed by 359ea095183c451acfd51975ef3aeb75df4d7be9\n. Closing as not reproducible for us at this point.\nIn order to tackle this problem we'll need a way to trigger the problem with a higher frequency than once in 50GB.\nAlso, it'd be interesting if the problem persists with the current akka-http implementation.\n. I'd like to move this ticket to akka-http where a performance tuning section is probably more needed at this time.\n. @peterparks We generally prefer to obey official specifications (ideally RFCs) wherever we can. If you can point us to anything underpinning your request it stands a much higher chance of becoming accepted.\n. Thanks, Peter, for this PR.\nWe'll look at it during our current migration work from spray to akka(-http).\n. @peterparks We have now come to a point where we'd like to merge your patch (which I think does provide a good solution!) and then forward-port it to akka-http.\nThanks for this patch, Chris, very nice!\n\nWould you be able to rebase on top of the current master branch and accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) (only a few clicks)?\nThanks again and cheers!\n. Thanks, Peter!\n. This would definitely be a nice addition to have.\nHowever, because the spray project is now in maintenance mode, where we are reluctant to add new features, I'd propose to resubmit this PR against the [`release-2.3-dev` branch of the Akka repo](https://github.com/akka/akka/tree/release-2.3-dev).\nIt can then become part of Akka HTTP and act as (another) incentive for the migration from spray to Akka HTTP.\n\nThanks again for submitting this PR!\n. While I agree with Johannes in that this would be a nice feature I don't consider it important enough to warrant any action from our side at this point.\nOne can easily achieve the desired effect within the DSL of one's preferred test framework.\n. While I agree that having a dedicated `produceTry` directive would be nice the work-around of simply using\n\n``` scala\nproduce(instanceOf[Try[T]])\n```\n\nwhen requiring an error channel back to the API layer should be good enough.\n. I have just copied the (AFAICS) last missing artifact from repo.spray.io. to sonatype / maven central (spray-json_2.11 1.2.6).\nIf there is anything that you need from repo.spray.io which is not yet mirrored to maven central please let us know.\n. Thanks for the outstanding ticket description, Adam, I completely agree with your assessment.\nThis is a valid request and should be addressed.\n. Thanks for this PR!\nWe'll take a look at it during our current migration work from spray to akka-http.\n. Thanks again for this PR, @ajknoll and @caspark!\n(And sorry for taking so long to get back to it!)\n\nI think this is a nice fix, which we'll definitely want to merge and port to Akka HTTP.\nHowever, we'd need you, @ajknoll, to first accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla). This shouldn't take longer than 30 seconds.\nThanks again!\n. Thanks!\n. Thanks for this patch!\nCould you\n- accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) (only a few clicks)\n- change the commit message to\n  \n    = can: fix error message in response parser\n  ?\n. Thanks, Daniel!\n. Thanks!\nCould you\n- accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) (only a few clicks)\n- change the commit message to\n  \n    = docs: fix typo\n\n?\n. Great, thanks!\n. Yes, a 1.x.2 release is already planned.\nHowever, in addition to simply recompiling for spray-json 1.3.0 we'd like to also get the most pressing tickets closed and need a bit of time for that.\nHopefully we'll get it done sometime next week.\n. Recompiling (and actually re-releasing/re-publishing) is all that's required here.\n. Thanks for this patch, Chris, very nice!\nCould you\n- accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) (only a few clicks)\n- squash your commits into one\n- change the commit message to\n  \n    = can: fix SOE in spray.can.rendering.toTcpWriteCommand, closes #877\n\n?\n. @ChrisA89 I'd like to get this merged soon. I anything holding you back from accepting the Typesafe CLA?\n. Closed by f9309bb37de733ea0c99d2b2d8aca1e4076e9acf\n. Sorry, Chris, due to deadline constraints we decided to move ahead with this fix ourselves.\nThanks anyway for providing the necessary push!\n. While prioritizing spray maintenance tickets we decided that this ticket is not important enough to make the current cut. Since the problem is already fixed in akka-http we are closing this ticket now.\nPlease reopen if this thing is really an issue for your application.\n. Thanks, Christian, this looks like a valid proposal.\nWe'll take a deeper look once we port the respective code bits to akka.\nThanks again!\n. Thanks, Evan, for this patch.\nI don't see any risk in adding access to the cache keys.\nMaybe @jrudolph has a comment here?\n\nOtherwise, in order for us to be able to merge this, could you\n- accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) (only a few clicks)\n- change the commit message to\n  \n  ```\n  + caching: add access to cache key set\n  ```\n\n?\n. Thanks, Evan!\n. Mathias, even though this is an unfortunate problem with 1.x.2 we are currently busy on other fronts.\nHow serious is the issue on your side?\n. Hunor, thank you for your contribution!\nWe are currently a bit unsure of the spray-caching module's future, so we will probably not be able to merge breaking changes to it at this point.\n\nspray-caching was meant as a small and light-weight in-memory caching solution, not so much as a full-blown, feature-rich caching layer. So I'm somewhat unsure whether pluggable stores add a lot of value in this context. Maybe you can shine some more light onto the motivation for this patch?\n. Thanks, Guillaume, interesting!\nCurrently we are unsure whether we should still merge this into the spray codebase or directly port it over to akka-http.\nIn any case, could you accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) (just a few clicks) so we can accept your contribution(s)?\n. Guillaume, we are just coming back to this PR now.\nWhile we still believe that an extension of the header model along the lines of what you propose here is definitely valuable we don't want to change the existing spray APIs more than absolutely necessary (e.g. for bug fixing reasons).\nSo, if you'd still be willing to add the missing parsing rules as well as some tests we'd like to ask you to resubmit against [`release-2.3-dev branch of the Akka repo](https://github.com/akka/akka/tree/release-2.3-dev).\n\nThanks again for submitting this PR!\n. Alex, thanks for this PR!\nIt sure looks interesting!\nWe are currently reluctant to add new feature to spray as we are deep in the process of porting everything to akka-http, which requires some stability on the port base.\nWe'll come back to this PR when we are done with the initial port (which should be quite soon).\n. Alex, we just came back to this PR and do like it!\nWe think it's a nice addition to the DSL that would certainly be valuable to quite a number of users.\nHowever, since spray is now in maintenance mode and we have already ported the respective parts of the spray routing DSL, we don't think that merging this PR here in the spray repository would actually be the best thing to do.\nWould you be willing to resubmit this PR [against the `release-2.3-dev` branch of the akka repo](https://github.com/akka/akka/tree/release-2.3-dev)? This feature could become another incentive for people to upgrade to Akka HTTP!\n. Closing here.\n. Thanks, Farid, for this PR.\nWithout having looked at it closer, have you seen the discussion around cookie jars in https://github.com/spray/spray/pull/311 ?\n. Thanks again, Farid, for this patch!\nWe've decided that the complexities surrounding the topic of this ticket are too great to simply offload them onto a contributors shoulders. So we've created https://github.com/spray/spray/issues/772 to discuss and collect everything around it.\nI'm therefore closing this PR now, but we will surely come back to your implementation and steal a few ideas, if that'd be ok for you.\n. While the problem is fixed in spray at least the added test appears to be missing from Akka HTTP.\n. I think the solution should be to simply render the user-supplied headers in the same sequence as they are provided. This should solve this problem and circumvent the question to which headers the exception in the spec actually applies.\nThe good thing is that we can still render the \"important\" headers that spray produces itself in any order, before or after the user-supplied headers.\n. Yes, right, thanks for clarifying!\n\nI think we should make sure to fix actual bugs, the bonus work doesn't appear to be worth the effort at this point. So:\n\n> prioritize rendering order of response headers managed by spray (Server, Content-Length, Date, Connection, Transfer-Encoding others?) and preserve order of remaining headers\n\nSometimes the code becomes easier if we simply render managed headers when we can, which might be before or after the user-supplied headers, which is ok.\nWe should simply make sure that we don't render the user headers in reverse, which might already be the case. If so, no change necessary here.\n\n> preserve order of headers during parsing \n\nYes, maybe this is the only thing we really need to change.\n\n> bonus: combine duplicated, collapsible headers during client side parsing and prevent sending of duplicated, uncollapsible headers as spec'd above.\n\nThat would be nice to have but not worth the implementation cost, AFAICS.\n\n> WDYT? Should I give them a try?\n\nIf you'd like to take a stab at fixing the header parsing sequence issue, we'd be more than happy to merge your PR. Thanks in advance!\n. Apart form the small perf tweak this looks great! Thanks, Andr\u00e9!\n. Yes, thank you.\nWould you like to issue a PR with a fix to the docs?\n. We are just discussing this:\nActually the current behavior does make sense.\nWhen you reply to a request with a chunked response, i.e. a stream, you use the ACKs as trigger for sending the next chunk. However, if the request was a HEAD request you shouldn't actually dump you whole stream. Rather your response stream processing should be aborted, because the response cannot take an entity at all. This is why we \"ack\" with a fake `Closed` event, which is supposed to trigger the cancellation of your response stream.\n\nNow, I agree that this is not a pretty solution and having a proper abstraction for response streams (like the new reactive streams in akka-http) will make things a lot cleaner here.\n\nOne _could_ argue that we should only do this ACK-transformation when `transparent-head-requests` is enabled.\n. I see.\nIt seems to me that we should restrict the \"magic\" described above only to the case that `transparent-head-requests` are enabled. If it's disabled, as in your case, we should simply drop all response chunks coming in from the application. And I guess we should still ACK them.\n. The test we add for this also needs to be ported to akka-http.\n. Ok, thanks for the hint. We'll look into this as well in the context of this ticket.\n. This fix might need porting to akka-http.\n. Great! Thanks, Andr\u00e9!\nThis LGTM.\n. Thanks again, Andr\u00e9, for another nice contribution!\n(And sorry for taking so long to merge!)\n. Thanks, Alex, for this patch!\nCould someone with OSGi experience review this? (Unfortunately we ourselves have no clue whatsoever of all the factors coming into play here...)\nMaybe @topping has some insights?\n. Ok, thanks @alexlomov and @briantopping for your help on this!\n(And sorry for taking so long to merge.)\nI take it this PR now does what it's supposed to do. So we'll merge and get it released ASAP.\n. Thanks, J\u00fcrgen!\n. There have been some concerns about the mimepull license being incompatible with the Apache 2 license that spray is released under. However, mimepull is dual-licensed, under GPL v2 _and CDDL v1.1_.\nSince there doesn't appear to be an issue with linking to a CDDL library from an Apache 2 library we don't consider this ticket to be important enough anymore for spray.\nNote that akka-http doesn't rely on mimepull anymore but uses its own streamified mime parser.\n. Interesting, thanks Noel!\nCould you give us some more motivation as to why you'd need something like an `IgnoredSegment` path matcher?\n. Ok, interesting.\nSomehow this sounds like a rather special setup that is of not so great general utility.\nAs such I'd be inclined to not add this feature as it would likely make spray only bigger, not better.\n. `HttpMessage::asPartStream` converts the message into one with `Transfer-Encoding: chunked`.\nChunked messages must not have a `Content-Length` header. As such this conversion does indeed (necessarily) loose the information about how long the total content is.\nYou can however, if you so choose, add a custom header manually, which somehow retains the information about how long the total chunk stream is.\n. Thanks for this PR!\n(And sorry for taking so long to look at it.)\nWe'd like to merge it but need you to accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) first. Could you do that? Only takes about 30 seconds...\n. Thanks again for the improvement suggestion!\n. Thanks for this patch!\n. Thanks for the PR!\n@2beaucoup No, this tests hasn't been ported yet.\n. Nice! Thanks, Gilles!\nWe'll give it a spin.\n. The fix for this might need porting to akka-http. We'll have to verify when fixing.\n. This problem is now fixed in spray.\nWe keep the ticket open for now as a reminder for the required port to Akka HTTP.\n. Reopening because the fix didn't make it into the `release/1.3` branch yet.\nI didn't have enough time today to properly forward-port it, so this issue is only fixed in 1.1.3 and 1.2.3 but not 1.3.3.\n. Thanks, Mike, for this PR! (And sorry for taking so long to look at it!)\nThis looks like a good fix!\n. I think having a response transformer that will simply try to apply all known decoders before giving up is a useful feature for the new high-level client API of akka-http.\nSince we'd like to keep the spray API as stable as possible in the maintenance mode we are currently in I'm assigning this ticket to Akka HTTP.\n. This is a nice example of something that is fixed by the improvements to the DSL in akka-http.\nThere is nothing we can do in spray though (that upholds binary compatibility), so I'm closing this ticket here.\n. Our preferred solution to the \"can-marshal-everything\" problem that these reflection-based serialization approaches present would be to make the fully generic marshaller only available with (another) dedicated import.\nIf you don't want to import the fully generic marshaller you should be able to \"switch it on\" for a specific type with a single line, e.g. `implicit def fooFormat = json4sMarshalling[Foo]`.\n. Thanks, Daniel!\n. Brian, thanks a lot for digging into this and reporting!\n(And sorry for taking so long to respond!)\nThis looks like a valid problem with an easy fix.\nThanks again!\n. Thanks, Brian, for this PR!\n(And sorry for taking so long to look at it.)\nUnfortunately, as the discussion in https://github.com/spray/spray/issues/993 revealed, this fix is not yet good enough.\n\nWe just discussed the solution that we'd like to apply to this problem:\n1. Try to call `InetSocketAddress::getHostString` per reflection. If this succeeds all is well.\n2. If the reflection call fails (probably because the user is still on JDK 6) we fall back to the current behavior.\n\nThis should allow us to make use of the better API in JDK7+ while not giving up on JDK6 altogether.\n\nWould you like to take a stab at this solution? \n. Just `git push -f` into your branch to update this PR.\nThat'll be fine.\nThanks!\n. Thanks for this work, Brian! Great analysis to demonstrate the effectiveness of the fix!\n. If you push -f into your branch and replace the existing commit the notes will still be available.\nThey will just be folded. So, no worries.\n. Thanks, Brian, this looks good!\nI'll wait until @jrudolph has seen this as well and then push the `merge` button.\n. Ok, Brian, I am ready to merge now but realized that you still need to accept the [Typesage CLA](http://www.typesafe.com/contribute/cla/). It's a purely online process that should only take about 30 secs or so.\nWould you be able to do this?\nIdeally we'd like to publish a new release today and it'd be great, if we can get this fix in.\nThanks again!\n. Excellent.\nThanks, Brian!\n. Have you tried using a `RawHeader`?\n. Spray has a proper model for the `OAuth2BearerToken` and the character chain `Bearer` is not actually part of the token you specify. We adhere to the spec and capitalize the `B`.\nIt appears as if your server has a faulty implementation here.\n. We don't rewrite any header. We simply _write_ it correctly.\nThe `Authorization` header has a proper model for the `HttpCredentials` that is on a higher level of abstraction than a String/String header design. \n. `Could not write response part MessageChunk(Bytes(),)` means that you are trying to write a chunk before the previous chunk has been properly dispatched to the network.\nThis points to a problem in your handling write ACKs. Only when you have received the ACK for the previous chunk are you allowed to send the next one.\n. Ok, glad you got things sorted.\n. Thanks for this PR (and your excellent SO answer!)\nHowever, I'm not sure adding a very rigid `favicon` directive like the one you propose here really is inline with spray's general \"not-a-framework\" spirit.\nWe want to give users the tools to build exactly the functionality they need, not provide some pre-configured frame that an application is somehow dropped into.\n\nSo, if one want to provide a favicon this is really easy to do. I can choose which file I want to serve, from the class path or the file system, which media-type, etc.\nYour directive already makes a lot of decisions for me, which we generally do not want to do.\n\nSo, I'm inclined to leave things as is, i.e. _not_ provide a predefined favicon directive, because adding favicon support is already really easy and another directive would probably cause more problems than it solves.\n\nHowever, what is probably a good idea is to add a section to the documentation, explaining the different options one has for adding favicon support oneselves.\n. I don't consider this a bug.\nAfter all the server forcibly closing the connection should result in error messages on the client.\nNot using keep-alive connections for non-idempotent requests, just for reducing the likelihood of errors in case the server shuts down, doesn't appear to be a good trade-off in my view.\n. Assigning to akka-http. We won't add this feature to spray anymore.\n. Moving to Akka HTTP.\nI don't think this issue affect enough users to still warrant a somewhat involved fix the requires changes to the outside API in spray.\n. We already received a good PR for this in spray, which unfortunately never made it into the codebase:\nhttps://github.com/spray/spray/pull/372\n\nHowever, a lot of the work there should be highly relevant to closing this ticket for Akka HTTP.\n. Done. Upgrade to _spray_ 1.3.3 and use the _spray-routing-shapeless2_ module instead of _spray-routing_.\n. [RFC 2616 Section 3.7.1](http://tools.ietf.org/html/rfc2616#section-3.7.1) defined the rules for determining the charset for text content as follows:\n\n> The \"charset\" parameter is used with some media types to define the\n> character set (section 3.4) of the data. When no explicit charset\n> parameter is provided by the sender, media subtypes of the \"text\"\n> type are defined to have a default charset value of \"ISO-8859-1\" when\n> received via HTTP. Data in character sets other than \"ISO-8859-1\" or\n> its subsets MUST be labeled with an appropriate charset value.\n\nThe latest HTTP spec has changed this special status of `ISO-8869-1`. [RFC 7231 Appendix B. Changes from RFC 2616](http://tools.ietf.org/html/rfc7231#appendix-B) says:\n\n> The default charset of ISO-8859-1 for text media types has been\n> removed; the default is now whatever the media type definition says.\n\nUnfortunately, for many media types the definition doesn't really \"say\" anything.\nLooking at the state of the specs and the current spray implementation I'd be inclined to simply change the `ISO-8869-1` default in spray to `UTF-8` as it is almost always the right choice.\nLooking at the code this change doesn't appear to be hard to do.\n\n@jrudolph What do you think about changing the default from `ISO-8869-1` default in spray to `UTF-8`?\n. Ok, true. I can always map the request Content-Type and attach a custom `charset=...` parameter if I really need to. So I agree with you in not touching this rather basic logic in spay anymore.\nClosing.\n. Closing as the fix was reported \"effective\" by the customer.\n. Thanks for this, @mpilquist!\nGood to see that so few changes are required for the update.\n. Thanks, Michael, for this PR!\nIt really did help with making the upgrade to shapeless 2.1 a breeze.\nIf you upgrade to _spray_ 1.3.3 and use the _spray-routing-shapeless2_ module instead of _spray-routing_ things should fly nicely with shapeless 2.1.\n. Looks like this ticket can be closed, right?\n. Ok, cool.\n. No one from our side is currently working on this.\nSo it'd be great you picked this up...\nThanks in advance!\n. We had a discussion in https://github.com/akka/akka/pull/16954 about this question (click on `Show outdated diff`). Essentially it comes down to this sentence from [this section of RFC3986](http://tools.ietf.org/html/rfc3986#section-6.2.3):\n\n> Normalization should not remove delimiters when their associated\n>    component is empty unless licensed to do so by the scheme specification.\n\nIf the case you highlighted the URI has a scheme (`g`), no authority and a \"rootless\" path (`path-rootless`). The patch in https://github.com/akka/akka/pull/16954 always renders the `//` delimiter for empty authorities, if the scheme is not empty or `mailto`. As your example shows, this is not quite good enough. In your example we are not allowed to render the `//` delimiter because this would turn what is a rootless path into a host! So we need to apply a small fix to https://github.com/akka/akka/pull/16954, which prevents the rendering of the `//` delimiter if the path is rootless.\n\nSo, to recap, we need to render the `//` between a non-empty scheme and an empty authority exactly when these conditions are met:\n1. the scheme is not `mailto` (potentially more exception schemes will be added here later)\n2. the path is empty or starts with a slash (not a segment)\n\nSo, thanks for bringing this up as it shows a flaw in https://github.com/akka/akka/pull/16954 that we need to fix. After getting this right in spray, would you also want to put it an Akka PR with the correction (and maybe some more tests that we are apparently missing in Akka)?\n. If you type a search term you'll see a flyout with the respective pages.\nThere is no real \"search results\" page.\n. That's a good idea! Thanks!\n. @matsluni Sweet! This is great.\nThere is only one small problem I just noticed: With the current implementation, when I move the mouse cursor over the results list (or press up/down arrow), I get a colored background bar behind the selected list item. With your solution I somehow don't. Can this be fixed?\n. Cool! Thanks a lot!\n. Nice. Works great for me.\nDo you want to put in a proper PR?\nI could also just cherry-pick your commit... whatever you prefer.\nThanks a lot in any case!\n. Yes. Not being able to represent empty responses with a Content-Type is a limitation of spray's `HttpEntity` model that is fixed in akka-http. In Akka HTTP you can create empty responses with a Content-Type.\nHowever, in spray we will not change this anymore as the codebase is now in stable maintenance mode where we do not want to introduce breaking changes if at all possible.\n. Thanks Julian.\nWe'll merge as soon as we can.\n. Yes, thanks!\nCould you accept the [Typesage CLA](http://www.typesafe.com/contribute/cla) (only takes 30 seconds) and change the commit message to\n`= docs: fix copy-paste error` in accordance with our [git commit message policy](http://spray.io/project-info/contributing/#git-commit-messages)?\n. Excellent. Thanks, Joe!\n. What spray version are you reporting this against?\n. Ok, thanks.\nThe problem is interesting. It lies in the `toRelative` URI transformation we do when forwarding the request to the correct `HttpHostConnector`. Paths starting with a double-slash are not legal without an authority part, so we cannot actually strip off the authority, i.e. convert into a relative URI.\nThis means that, for these kinds of URIs, we need to add a `Host` and _keep_ the URI in absolute form.\n\nI just confirmed that `curl` actually does things wrong here. If you do a `curl -I http://spray.io//foo` it will send this request:\n\n```\nHEAD //foo HTTP/1.1\nUser-Agent: curl/7.30.0\nHost: spray.io\nAccept: */*\n```\n\nwhich is incorrect, because `//foo` is actually an absolute request without scheme to the root of host `foo`.\n. I do need to correct my comment from above here.\nCurl does in fact do things right and we parse request targets incorrectly.\nSee https://github.com/akka/akka/issues/17057 for more info.\n. @wjur Don't worry about the failing test. Some of our tests are a bit shaky.\nI'll look into your changes as soon as my time permits. Thanks a lot again!!\n. This looks great, thank you @wjur!\nMaybe one more small tweak and we can merge.\n. Ok, thanks. This LGTM.\n@jrudolph ?\n. Ok, great.\nNow we'd need a PR in Akka, against branch `release-2.3-dev` containing the fix to the fix.\nYou said you also would be able to do this, @wjur ?\n. Great, thanks!\nYou'll have to accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla/) thought, before we can merge. Sorry for the trouble, it only takes 30 secs!\n. And done. Thanks again!\n. The reason is that our main development master branch is still on Akka 2.1.\nThis won't change anymore in spray.\n. How do you unmarshal your JSON?\nThe `SprayJsonSupport` already appears to be doing what you want:\nhttps://github.com/spray/spray/blob/master/spray-httpx/src/main/scala/spray/httpx/SprayJsonSupport.scala#L36\n. Yes, that's right. If you use any of the existing JSON support facilities you should have to write less code and things will be decoded properly.\n. Great. I'll give it a spin on \"my\" JDKs and report back.\n. The `spray-httpx` tests are all green for me on Apple JDK 6 and Oracle JDKs 7 and 8 under OS/X and the fix looks good for me. I don't think switching to an explicit xerces dependency is necessary at this point. Do we have any indication that a relevant JDK _doesn't_ support the XMLConstants in question?\n. Thanks, James!\nI think there is value in keeping the approaches in Play, spray and Akka HTTP as closely aligned as possible. For one because the one in Play is already well tested and somewhat matured in the field and secondly because we then save the mental overhead of managing several competing solutions.\n. Yes, indeed. Thanks for digging these up!\n. Fixed.\n. Yup.\n. Please direct your questions to the [spray mailing list](https://groups.google.com/forum/#!forum/spray-user) as we use the github ticketing system only for issue tracking.\n. Nice. LGTM!\n. I just took a quick look at your test project (thanks for that!).\nThe problem only appears under Scala 2.10.5 and with `spray-routing-shapeless2`.\nUnder 2.11 or with the plain `spray-routing` module is doesn't appear.\n(Also, your test project is missing the explicit dependency onto akka, but that is unrelated.)\n\nI just added one more test exercising the exact same API:\nhttps://github.com/spray/spray/commit/340614c9af7facf6a8b95bf4cc7733e0fca9299e\nand that compiles fine. So something else appears to be going on.\nThanks for the report!\n. > Alright, so that sounds like it might be a compiler regression from 2.10.4 to 2.10.5?\n\nI haven't checked, whether the problem doesn't also appear under Scala 2.10.4.\n\n> Is there anything I could help look at?\n\nIf you could try to further isolate the problem and find out why our test compiles fine while your test project doesn't then that would certainly help us.\n. Ok, great.\nThanks for digging this up and reporting back!\n. Thanks for this pointer to Caffeine!\nWe'll keep this ticket open as a reminder to look into upgrade options when the Java 8 time has come.\n. Any logic acting on potentially malicious input should apply hard bounds to everything.\nEspecially if it, like spray, doesn't run in a true streaming mode. (By default, spray fully buffers message entities.)\n\n> And why 8M as a default ?\n\nWhat default value would you suggest instead?\n. The relevant spec for the `WWW-Authenticate` header is http://tools.ietf.org/html/rfc7235#section-4.1.\nRFC 7235 defines `auth-param` as such:\n\n```\nauth-param = token BWS \"=\" BWS ( token / quoted-string )\nauth-scheme = token\nchallenge = auth-scheme [ 1*SP ( token68 / [ ( \",\" / auth-param ) *(OWS \",\" [ OWS auth-param ] ) ] ) ]\n```\n\nSo the param value can be either a `token` or a `quoted-string`.\n\nWhat client do you have an issue with?\n. Ok, thanks for the detailed analysis and report, Csongor!\nLooks like a valid improvement request.\n\nWould you like to submit a PR?\n. Thanks for the PR. Looks good!\nRegarding the CLA, it looks like you already accepted it online with your GH user account, which is all we need. Do you need the corporate CLA nevertheless?\n. Excellent. Thanks, Csongor!\n(Fixes #1041)\n. IIUC the current implementation and the documentation is correct.\n\nThe example supports both chunked as well as unchunked requests and handles both via the \"chunked\" path. In that regard you are correct, _if_ the request is unchunked then chunking is emulated to be able to reuse the code path for chunked requests.\n\nConcerning the difference between `request-chunk-aggregation-limit` and `incoming-auto-chunking-threshold-size`: these two are two different things.\nThe former makes chunked requests appear as unchunked requests while the latter does the opposite.\n\nIf you've been testing with curl: Have you set an explicit `--header \"Transfer-Encoding: chunked\"` CLI option? Otherwise curl won't produce a chunked request.\n. > if request-chunk-aggregation-limit = 0 then the handler actor sees the MessageChunks, meaning the request is chunked.\n\nWhether the request is chunked or not is decided by the _client_. By \"chunked request\" we mean a request that was sent with `Transfer-Encoding: chunked` (and no `Content-Length` header).\nspray supports \"virtual chunking\" (via `incoming-auto-chunking-threshold-size`) as well as \"virtual unchunking\" (via `request-chunk-aggregation-limit`).\n\nMaybe the misunderstanding stems from a different interpretation of what \"chunked\" means?\n. Indeed. That is of course of problem.\nUnconditionally copying over the headers for the new request, however, will also not be good.\nI guess we'll need some smarter logic to decide, which headers can be \"redirected\" and which can't.\n. The way I understand it is that 2.2.2 fixes a bin compat issue that 2.2.1 had with 2.2.0.\nHowever, since spray is still on 2.1.0 we still need to upgrade to 2.2.0 and publish a new release, right?\nOr is 2.2.2 also binary compatible with 2.1.0?\n. A spray release takes up at least half a day of work for one of us, so we usually delay things until a release really is necessary. So, it'd be interesting to understand whether the latest spray release is indeed compatible with spray 2.2.2 or not.\n. Cool. That's great!\nI'll keep the ticket open for some time longer just in case.\n. How are you streaming the response?\nAre you using ACKed writes?\n. I agree that the behavior of `pathSuffix` is not the most intuitive.\nHowever, it behaves as \"specified\".\nThe scaladoc specifically says:\n\n```\nNote that, for efficiency reasons,\nthe given PathMatcher must match the desired suffix in reversed-segment order, ...\n```\n. Ah, I see.\nYeah, that's really not that great.\nMaybe we need to indeed bite the bullet and implement a \"proper\", even if slow, suffix match.\n. Nice, thank you for this patch!\n\nBefore being able to merge it we'd need to accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla), which can easily be done online.\n. Thanks, Bernard!\nThis should indeed go in!\n. There are quite a few threads on the ML regarding this topic, but you are right.\nA more prominent addition to the website/documentation is probably called for.\n. Cool. Thank you, Alexander!\n. Nice! Thanks a lot, Johannes.\nThis definitely make the release much easier...\n. @ktoso I'll happily push an update to the website once we have an official wording for the deprecation notice.. LGTM!. Excellent.\nMerged into develop branch.\nThanks for the contribution!\n. Maarten,\nthanks for the report.\nI see the problem and will get the fix out in the next days.\nCheers,\nMathias\n. Actually this behavior is intended. IMHO in the majority of cases path matching should \"just work\". Even when the user appended a slash the uri should still match.\nHowever, you can easily achieve the distinction you are looking for by reversing the order of the two sub routes.\nIf you try to match the \"a/\" route first it will only match if there really is a trailing slash...\n\nCheers,\nMathias\n. This was not a pedantic question at all!\nRather a perfectly valid and valuable one.\nSo please keep them coming... :)\n(You can of course also use the mailing list)\n. Thanks for the comment.\nYes, \"metrics\" is a great project.\nAs is https://github.com/twitter/ostrich ...\nUnfortunately both libs somewhat do \"too much\" and are not \"actor enabled\".\nSo currently we are planning to roll our own solution but turn to the existing ones for structure and ideas...\n. Alright, would you be able to provide a few examples on how you use Metrics in your actor code? (preferably on the mailing list)\n. Current thinking: provide a thin adapter layer to [Graphite](http://graphite.wikidot.com/), probably implemented as a singleton actor representing the \"gateway\" to Graphite.\n. Sound cool.\nI guess a small example project somewhere would be excellent!\n. Metrics really has come a long way and is nicely modular by now.\nNo point in trying to roll our own monitoring solution anymore.\nWe should provide `MetricsSupport` trait and a few simple directives to get Metrics support rolling...\n. Excellent, David!\nWe are incredibly busy with all the things we have planned for the 1.0 release, and unfortunately we had to leave this feature off the list. So if you'd like to pick it up, we'd likely be able to support it.\nOtherwise we'll have a go at this later...\n. I'd say we first try to get the basic metrics abstractions (gauges, counters, histograms, etc.) mirrored by directives and then build more complex things from the basics.\n. Well, I might want to log custom things to metrics from my directives, so something generic underneath is probably the best foundation.\n. Curtis, indeed, as far as I know there is noone working on this issue at the moment. If you have something worthwhile including we'd be more than happy to take a look.\n. There is somewhat longer-standing PR: https://github.com/spray/spray/pull/359\nthat\"s we hope to get to soon...\n. I just closed @derekwyatt's PR https://github.com/spray/spray/pull/359, which will not make it into spray anymore. Still, we should definitely use it as a basis for any future work on this feature in Akka HTTP.\n. We'll not do this since it would make things like\n\n```\nget {\n    someActor ! _\n}\n```\n\nimpossible (require wrapping)...\n. Fixed in develop and akka1.1 branches\n. Closed by https://github.com/spray/spray/commit/ca2a39713b4a9c8a134d6e208b138d10cf235688\n. Paul,\nthanks for the report and the praise (also the ones via twitter!).\nAre you seeing this initial delay with your own application or one of the examples?\nThe markdown-server example really does have a great initial delay due to the pegdown markdown-parser being created at the bytecode-level...\nApart from this we haven't experienced an especially slow startup so far.\nIs there something special about your use case?\nCheers,\nMathias \n. Thanks, Paul, for these details.\nWe'll look into this and whip something up for spray 0.6.0, which is not far at all after todays events...\n\nCheers,\nMathias\n\n---\n\nmathias@spray.cc\nhttp://www.spray.cc\n\nOn 12.05.2011, at 22:11, psnively wrote:\n\n> You're more than welcome!\n> \n> I don't believe there's anything particularly unusual about our use case so far, and once the initial request has been processed, successive operations are nice and zippy. On my 2.13 GHz, 4G RAM MacBook Air running either Tomcat 7.0.12 or Jetty 8.0.0.M2, I see the first operation go from ~1015ms to successive operations taking ~35ms. You can see the reason why very easily: please just launch your server JVM with the \"-verbose:class\" option and examine the logs. Upon handling the first request, you'll see an enormous number of classes loaded. Again, many are from the cc.spray package, but a much, much larger number are from parboiled. Anything you can do to preload these packages will be a big win for clients of that first request. :-)\n> \n> Thanks!\n> Paul\n> \n> ## \n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/spray/spray/issues/14#comment_1148968\n. of course, an corresponding Unmarshaller should be added as well\n. No considered that important any more.\nCurrently the \"preferred\" way of deployment is as one JAR using spray-can as container and the sbt-assembly plugin as packager.\n. closed by latest commits in develop\n. Use ClientConfig.requestTimeoutInMs\n. Streaming large responses is the first priority.\nBeing able to receive streamed requests asynchronously is a second priority.\n. Response streaming available in develop branch and current 0.9.0-SNAPSHOT build\n. I would strongly recommend using the 0.7.0-SNAPSHOT from the develop branch.\nCurrent artifacts for this version are available from scala-tools snapshots.\n\nBtw: All older SBT versions are still available (incl. 0.7.5.RC1 and 0.7.7).\nJust follow the installation instruction [here](http://code.google.com/p/simple-build-tool/wiki/Setup)\n. Excellent, thanks Steffen!\nI'll look into your branch as soon as I find the time...\n. Thanks for the report, Neil!\n. Yes, just confirmed that spray 0.7.0 cannot deal with quoted cookie-values due to a bug in the HTTP parser.\nThanks for the report!\n. I'll upgrade to Akka 1.2 as soon as the final is out...\n. Now on http://spray.io\n. The problem is not the '/' character but the trailing semicolon, which is not allowed according to the cookie spec (http://tools.ietf.org/html/rfc6265#section-4.1.1). So the server is indeed sending a bad header...\n. No applicable on the spray level as the underlying container/server suppresses sending of the response body for HEAD requests\n. Julien,\nthanks for your contribution.\nI'm looking forward to your multipart implementation.\nSomehow my initial feeling is that including the \"boundary\" parameter directly into the ContentType class is not the best solution. The boundary seems like an implementation detail of multipart messages that, IMHO, shouldn't be surfaced this high up in the API.\nWithout having though about it too much it seems to me that the natural solution would be to upgrade the HttpContent type to a union type with the implementations:\n- NoContent\n- SinglePartContent\n- MultiPartContent\n\nThe latter would have to be container for a List[BodyPart] as well as the boundary....\nThe BodyPart would encapsulate a List[HttpHeader] as well as an HttpContent (similar to what the HttpMessage already does). Whether or not \"NoContent\" makes sense depends also on whether body parts without content (e.g. only headers) are allowed by the spec.\nYou could also check out how the BlueEyes team has modelled this...\n\nThanks and cheers,\nMathias\n. One of our design principles is to start with thinking about how a new feature would be used rather than coming at it from an implementation perspective.\nI think the following way of consuming multipart content would be good:\n\n```\ncontent(asMultipart[Employee, Image]) { (employee, image) =>\n    ...\n}\n```\n\nSimilarly, marshalling several values into a multipart messages could be done like this:\n\n```\ncontext.complete(multipart(employee, image))\n```\n\nSo, after having pondered this a bit more, I agree that extending the HttpContent might not be best solution. A special multipart marshaller/unmarshaller pair would probably suffice, along with the \"asMultipart\" and \"multipart\" wrapper methods for unmarshalling/marshalling.\n\nCheers,\nMathias\n\n---\n\nmathias@spray.cc\nhttp://www.spray.cc\n\nOn 01.09.2011, at 19:30, shopolate wrote:\n\n> Please ignore my precedent pull request, I did not realized I was on the master branch.\n> \n> Thank you for your commentaries. I would say I am not sure subclassing HttpContent would simplify the framework. I like the way we can marshall things, especially with multipart, where we can reuse marshalling on individual parts.\n> \n> As for the \"boundary\" parameter, I agree it is quite ugly. BUT, it make sense to parse the headers just once. And if we want to parse the Content-Type just once, ContentType seems the right place to put parameter data, hence my modification. I added the same kind of processing for Content-Disposition header. I think parsing headers are an important feature of such a framework. I guess this is why you added the convenient data structures for parsing cookie headers for example. Why some headers would get the \"royal treatment\" by being fully parsed, and their data fully exposed to the end user? But I don't have a clear picture of what you are envisioning for the future of spray, so my view could be too simplistic.\n> \n> Anyway, I wanted to thank you for spray, which is one of the easiest, most powerfull framework I have ever tried. I love it and try to spread the word :)\n> \n> ## \n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/spray/spray/pull/45\n. Julien,\n\nI just realized now that your pull request as well as your points so far are addressing the \"form-data\" sub type of \"multipart\". Even though you prominently put this in the title I was somehow thinking we are talking about general \"multipart/mixed\" support (RFC 1341 sec. 7.2). Sorry for that!\nSo, please ignore the comments I made so far (as \"form-data\" of course carries names for the parts whereas the general \"multipart/mixed\" type does not).\n\nI agree with you in that form content, no matter whether `application/x-www-form-urlencoded` or `multipart/form-data`, is similar to query parameters in that it combines key/value pairs with a custom type that a user wants to unmarshal to. The difference is that query parameter values as well as the values of `application/x-www-form-urlencoded` form fields have a known encoding (url-encoded), whereas the values of `multipart/form-data` form fields can be present in any content-type. \nCurrently the approach for dealing with query parameter values is quite different from the one for message entities (bodies). The former uses implementations of the SimpleParser trait whereas the latter uses the Unmarshallers.\nTheoretically the both could be merged, but so far this seemed somewhat unnatural to me.\n\nNow, when we are thinking about something like a `formField`/`formFields` directive that extracts form field values in analogy to the `parameter`/`parameters` directive (which I think is a good approach!), merging SimpleParsers into Unmarshallers seems more attractive. Otherwise we would have to have two different directives for `application/x-www-form-urlencoded` and `multipart/form-data`, since these two would require differing type classes for deserialization.\n\nSo, in the moment, I think I'd favor this approach for deserialization:\n- somehow merge SimpleParsers into Unmarshallers\n- change the `parameters` directives to use Unmarshallers\n- introduce `formFields` directives that work with `application/x-www-form-urlencoded` as well as `multipart/form-data` form content in analogy to the `parameters` directives\n- remove the old (existing) Marshaller/Unmarshaller pair for FormContent\n\nFor marshalling, I'd probably go for something like this:\n\ncontext.complete(\n  MultiPartFormContent(\n    part(\"employee\", employee),\n    part(\"image\", image)\n  )\n)\n\nIt's basically what you suggest with two changes:\n1. An explicit MultiPartFormContent type, which would have its own Marshaller and does not require another `RequestContext.complete` overload. This marshaller also would give us the natural place for verifying acceptability of `multipart/form-data`.\n2. No way to explicitly set a media type for the part. The Marshaller for an `Employee` figures this out itself using the requests accept headers and its own capabilities.\n\nGetting all this implemented would certainly be a great step forward for spray and would significantly improve the existing (somewhat wimpy) support for `application/x-www-form-urlencoded` form content.\nThanks for pushing me to think about all this\u2026 :)\n\nCheers,\nMathias\n\n---\n\nmathias@spray.cc\nhttp://www.spray.cc\n\nOn 03.09.2011, at 16:20, shopolate wrote:\n\n> I forgot to add that relying on part order might not fit every use cases. Ideally, we would be able to map the part name with the type. This also could be achieved by a directive similar to the parameters one. The only difference is we could match a part either by its name or by its index.\n> \n> If you are interested, I could try to do it.\n> \n> As for marshalling, the problem here is you might want to specify names and content types to marshal to. It seems to me that:\n> \n> context.complete(multipart(employee, image))\n> \n> might be restrictive, especially since the default content type of a body part is text/plain. For example, something like:\n> \n> context.complete(\n>    bodyPart(employee, `application/json`, \"employee'), \n>    bodyPart(someStream, `image/png`, \"image\", fileName = \"icon.png\"), \n>    bodyPart(someStringValue, name = \"someFormKey\")) \n> \n> would allow for more control.\n> \n> cheers,\n> Julien\n> \n> ## \n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/spray/spray/pull/45#issuecomment-1986802\n. The type of `image` in my example is some custom type that a Marshaller must be available for.\nThe MultiPartFormContent Marshaller would simply delegate marshalling of the part content to the individual Marshallers.\n\n> As for the refactoring of SImpleParsers you are talking about, I have a hard time figuring it out. Unmarshallers are unmarshalling HttpContent, SImpleParsers are unmarshalling strings. It seems to me quite a stretch to merge them together, since the semantics looks quite different.\n\nI'll have to take a look at this in some more depth. You are right, currently they are quite different, but in principle they have the same responsibility. Will see what I can do...\n\n---\n\nmathias@spray.cc\nhttp://www.spray.cc\n\nOn 05.09.2011, at 16:32, shopolate wrote:\n\n> Everything makes sense. \n> \n> One last little dark area: what is the type of the image var in your unmarshalling example? It can not be InputStream or Array[Byte], otherwise, you could not deduce if the content type is application/octet-stream, image/png or image/jpeg. It could not be HttpContent neither, otherwise it would contradict your statement about the framework deducing the most fit content type from accept header itself.\n> \n> As for the refactoring of SImpleParsers you are talking about, I have a hard time figuring it out. Unmarshallers are unmarshalling HttpContent, SImpleParsers are unmarshalling strings. It seems to me quite a stretch to merge them together, since the semantics looks quite different.\n> \n> Cheers,\n> \n> Julien\n> \n> ## \n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/spray/spray/pull/45#issuecomment-2001021\n. Thanks Julien,\nI'll look into your pull request as soon as I find the time...\n\nCheers,\nMathias\n. Julien,\nform support is now fully available in the develop branch.\nThanks a lot again for this very high-quality pull request!\nEven though I haven't directly merged it in I used different parts in various places.\n(The reason that it wasn't directly merged is, that I discovered a whole series of things in the codebase I wanted to improve architecturally, and the implementation of form support was a good occasion to take this on...)\n\nThanks again and cheers,\nMathias\n. closed by [this commit](https://github.com/spray/spray/commit/780a24e0abc52043cb7b287c270777b8aa883a49)\n. Not relevant any more for 1.0-M3+\n. This was already done in the develop branch of spray-template.\nNow also in master...\n. For the time being the [DebuggingDirectives](https://github.com/spray/spray/blob/master/spray-server/src/main/scala/cc/spray/directives/DebuggingDirectives.scala) will do...\n. Could be provide some more details?\nWhere did this issue appear again?\nWhich branch are you looking at?\n. Alright, thanks for the clarification...\n. Just use the header\n\n```\n`Authorization`(BasicHttpCredentials(username = \"bob\", password = \"123xyz\"))\n```\n\nPlease use the [mailing list](http://groups.google.com/group/spray-user) for any follow-up and all other questions regarding spray that you might have..\n\nCheers and happy spraying,\nMathias\n\n---\n\nmathias@spray.cc\nhttp://www.spray.cc\n\nOn 04.11.2011, at 19:35, jamosa wrote:\n\n> Hello!\n> \n> I'm trying to do a test in my project that uses a Rest Interface with authentication. To do the test, I define some interfaces and then I try to connect and it fails because I don't introduce user and password in httpRequest() . \n> My code is the next: \n> \n> val mediaType =`text/javascript`\n> \n> val expected =\n>        \"\"\"\n>        {\n>          \"message\":\"It contains the collection of identifiers of the corpus in the system\"\n>        }\n>        \"\"\"\n> \n> ```\n>  testService(HttpRequest(GET, \"/corpus\", headers = List(`Accept`(mediaType),`Authorization`(httpCredentials)))) {\n>    mapService\n>  }.response.content.as[String] must be(Right(pretty(render(parse(expected)))))\n> ```\n> \n> So, I know that I have to specify httpCredentials but I don't know how.\n> If anyone can explain me how can I do it, I would be so grateful.\n> \n> Thank you!\n> \n> ---\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/spray/spray/issues/55\n. If I understand you right, onejigtwo, you want to use different sets of (un)marshallers with different URI patterns.\nThis is already perfectly possible:\n\n```\npath(\"v1\") {\n  import Version1Marshallers._\n  ...\n} ~\npath(\"v2\") {\n  import Version2Marshallers._\n  ..\n}\n```\n\nHTH,\nMathias\n. You can use the 0.8.0-RC3 on scala-tools snapshots or wait for the final, which will be released today.\n. Konstantin, thanks for the report.\nThis has been fixed with the documentation update for the 0.8.0 release.\n(Btw. the documentation is a wiki, you can simply apply a fix yourself...)\n. Thanks Gregg, for providing the starting point for this!\nI have added some more content, so we should be all set for now...\n. In 1.0-M3 chunked requests can be handled via a custom `receive` in the HttpService actor.\n. Thanks, Chris, I'll try to merge this ASAP ...\n. It appears you are referring to the spray-can README.\nI have added an issue there: https://github.com/spray/spray-can/issues/13\nStill, the artifact coordinates as well as info on what dependencies are marked _provided_ and need to be pulled in \"manually\" should be put into the documentation (in addition to the references to the template and examples projects)\n. Now on http://spray.io\n. actually a [bug in spray-can](https://github.com/spray/spray-can/issues/20)\n. actually a [bug in spray-can](https://github.com/spray/spray-can/issues/21)\n. Is that at all possible on the JVM? (w/o having to re-implement DNS ourselves)\n. FTR:\nnetty has already invested quite a bit of time/work into a proper solution,\ncheck out these two PRs:\n- https://github.com/netty/netty/pull/1354 which was later superceded by\n- https://github.com/netty/netty/pull/1622\n\n@normanmaurer merged the latter one into [this branch](https://github.com/netty/netty/tree/dns) just three days ago.\n\nI haven't looked at the code at all so far, but seeing the amount of discussion and review that the PRs have received I trust there is something interesting there.\n\nRelated tickets:\n- https://www.assembla.com/spaces/akka/tickets/2591 (akka)\n- https://github.com/playframework/playframework/issues/1997 (play)\n. Thanks, just updated the link.\nWill be online with the next site deployment...\nThanks!\n. This has already been fixed in the current development version (\"develop\" branch and current SNAPSHOT builds)...\n. I don't think so. ETags and `if-modified-since` are two different things that _spray_ would ideally support both. Maybe you wouldn't _use_ both at the same time because they have somewhat similar use-cases, but _spray_ should still give you the choice (and even work with both at the same time).\n. Ok, added another issue to track quality values: https://github.com/spray/spray/issues/167\n. In addition to the case described in the ML thread linked above there is a very simple test case provoking the problematic behavior: If a connection managed by spray-client times out spray-client currently doesn't properly recover and schedule the next request to another or a new connection. (See [this thread](http://groups.google.com/group/spray-user/browse_thread/thread/ac98b8ab8adec810)).\nThis issue needs to be fixed for 0.9.0.\n. Now on http://spray.io\n. Fixed in master (1.0-M3).\nNo more RootService, therefore no more \"two HttpService\" setup.\n. I'm not sure this will still required for 1.0.\nNote than you can already combine different \"handling strategies\" by concatenating routes \"blocks\" via the \u2018~\u2018 operator, which essentially achieve the same thing.\n. You could do something like this:\n\n```\ntrait ServiceA extends Directives {\n  val routeA = // ...\n}\n\ntrait ServiceB extends Directives {\n  val routeB = // ...                                                                                                             \n}\n\nclass ComplexService extends ServiceA with ServiceB {\n  val route = routeA ~ routeB\n}\n```\n\nIn this way you could build an arbitrary number of sub routing components that you plug together in any way you please.\nWouldn't that work?\n. We will heavily rework the basic RootService/HttpService architecture for 1.0.\nIn the process the rejection-to-response mapping logic will moved directly into the route structure, as will exception handling.\nThis should nicely take care of the problem you are seeing...\n\n---\n\nmathias@spray.cc\nhttp://www.spray.cc\n\nOn 12.04.2012, at 13:04, peltchu wrote:\n\n> Sorry to get back at this again.\n> \n> Nonetheless, after a night's sleep I feel there's one significant drawback with the route approach when compared to the service-level approach. \n> \n> Mappings of rejections to responses happens at the service level and this allows easy configuration of error responses from filters. The route approach seems to me problematic when you want eg. _AuthorizationFailedRejection_ to result in completely different response in route A than in route B. \n> \n> For custom filters it's of course easier but I think when utilizing the default filters (eg. _authorize_) provided by spray they would all have to be intercepted to provide custom rejections/responses. It seems that this would become an another rejection handling layer in addition to the one provided by spray and thus it feels like working against the grain. On the other hand the feature in question would allow this very easily.\n> \n> Any ideas/suggestions on this?\n> \n> ---\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/spray/spray/issues/83#issuecomment-5088713\n. Closing as not relevant anymore in master (1.0-M3).\nThe RootService is gone, the simplified architecture increases ease of use as well as flexibility.\n. Now on http://spray.io\n. Erdem,\nthanks a lot for this super-clean pull request!\n. Fixed in master (1.0-M3).\nThe implicit architecture has been much improved, the DefaultMarshallers trait is gone.\nHowever, in order to have the Marshallers be usable on the client-side as well as the server-side, i.e. for rendering to HttpRequests as well as HttpResponses they cannot directly produce redirects or rejections.\n. Hmm... I'm not sure that'd be something a lot of applications would use....\n. Thanks, Lanchlan, for this excellent bug report!\nTurned out that this was actually due to a problem in spray-can, which just got fixed with the [0.9.3 release](http://implicit.ly/spray-can-093).\n\nThanks again and cheers,\nMathias\n. The query parameters are available from the RequestContext:\n\n```\nval queryParams: Map[String, String] = ctx.request.queryParams\n```\n\nPlease direct all further questions to the [mailing list](http://groups.google.com/group/spray-user) as this space here is reserved for issues.\n. Fixed in master.\nAs of 1.0-M3 exception handling has been much improved via the new `ExceptionHandler` infrastructure.\n. Fixed in master (with the new test DSL).\n. Not in the current 1.0 milestones.\nIn spray-can 0.9 there was an extra field for the remote address in the RequestContext, but we'd like to not having to add an extra field just for this feature, which many applications don't really need.\n. Expect this issue to be closed soon, so the work-around with having nginx in front of spray-can will not be required for long. \n. Antony,\nunfortunately we are not able to develop, maintain and support a proper Java side API for spray, officially it's Scala only.\nIf you have specific recommendations as to what we might change or add in order to make life for usage from Java possible/easier we'd be happy to consider any suggestions, but we won't take on this task ourselves... sorry!\n. Thanks David for clarifying this and sharing your experiences!\n. Fixed in master.\n. Solved by new MergeStrategy in sbt-assembly\n. Thanks for the report!\nNote that this error only appears if the file you are serving is larger than the configured `file-chunking-threshold-size`.\n. Thanks for the report.\nShould be back now...\n. We currently do not publish api-docs.\nHowever, we will bring them back online with the next major cycle on the spray.io site...\n. Are you using SBT for building?\nNote that the master branch already holds the sources for spray 1.0-M1, which depends on Akka 2.0.\nIf you'd like to use spray with Akka 1.x you'll need to get release 0.9.0 from the respective tag.\n. We'll update the documentation soon.\nThe URL in your error message works for me, so it's likely a problem on your side...\n. No, not yet.\n. One note: This will only work for non-HTTPS connections!\nIn order to support proxied HTTPS we need a much more involved logic, which we currently do not want to get started with.\n. Fixed in master.\nAs of the 1.0-M3 the test DSL is completely redesigned and much improved.\n. Alex,\ngreat!\nThanks for this fix.\nHowever, could you apply your fix to the `develop` branch rather than the `master` and re-issue the pull request?\n(We follow the _git-flow_ branching model and do most of our work on the `develop` branch)...\n. Yeah, that might well be.\nWe haven't really been paying much attention to Windows compatibility.\nIf you'd like to provide a patch for fixing these things as well we'll happily merge it in.\nCheers,\nMathias\n. Thanks Alex!\nI had to cherry pick your EOL-fix commit, since you added the IPv6 Host header fix also to this pull request, which prevented it from merging cleanly...\n. Thanks, Age, for bringing this up.\nI've added a patch that makes unmarshalling a bit more lenient by allowing empty key=value pairs as in your example. They are now simply ignored. However, the fix you suggested is a bit too relaxed for me, since it silently discards content that someone might not immediately recognize as being invalid.\nIn these cases I am usually an advocate of \"fail early\" with a clear error message.\n. Hi Samira,\nthe reason that the IoWorker is not an actor is that it contains blocking code and needs to carefully manage exactly one dedicated thread, which is not directly possible with Akka 2.0 actors. Also see the Akka documentation stating that\n\n> Actors should not block (i.e. passively wait while occupying a Thread) on some external entity,\n> which might be a lock, a network socket, etc. The blocking operations should be done in some\n> special-cased thread which sends messages to the actors which shall act on them.\n\nAlso: I'd like to invite you to ask any questions on the [mailing list](http://groups.google.com/group/spray-user), so that others can benefit from the answers as well. This area here is purely reserved for issue management.\n. Yes, of course it'd be possible to block the current thread in an actor.\nHowever, Akka wasn't _designed_ for this use case. Actors in Akka 2.0 are scheduled via dispatchers that are entirely configured via the (external) configuration, so as to enable the easy fiddling with dispatching options.\n\nIn the case of the IoWorker we have very specific needs with regard to scheduling (one fixed thread) and do not require any of the usual actor features. Also, the current implementation, being extremely light-weight, performs better than a similar one implemented via an actor would.\n\nBy the way: the IO implementation in Akka 2.0 also uses a dedicated thread for managing a NIO selector.\n. Samira,\nas I suggested before, I am happy to answer any questions on the mailing list, not in this issue tracking area.\nCheers,\nMathias\n. Excellent, Mike!\nThank you for spotting and fixing this...\n\n(And congratulations to nailing you first pull request, I can't see anything wrong with it!)\n. Once thing: I cherry-picked your commit over to the `develop` branch, since thats the branch we are doing all of our developing work in (we follow the git-flow branching model).\n. Fixed in master.\nThe new spray-servlet module moves the responsibility for creation of the ActorSystem to the user, therefore she is free in naming the ActorSystem according to her needs.\n. Cherry-picked commit 1eca501da44087b0d6cef654139f79895d9f6162 into develop branch\n. #wontfix\n. Cherry-picked commit cbbb84d3038b39fcba71c4be98bc629e9d8e3f88 into develop branch\n. Jan,\nthanks for your pull request!\nDue to the much improved general architecture in 1.0-M3 (the \"master\" branch) I think you should be able to achieve what you are trying to do with your patch in a much simpler way.\nIf you have time to take a look I'd be interested in your opinion (we can also and gladly discuss things via Skype or email).\nCheers,\nMathias\n. spray already supports 3.0 compatible servlet containers.\n. Fixed in master.\nYou can now influence the error message rendered for such a parsing error with a custom ExceptionHandler.\n. Alex,\ngreat, thanks for finding and fixing the problem.\nI can't merge your pull request directly, since development has come a long way since 0.9.0.\nAre you hoping for a 0.9.1 release containing the fix or will you be able to upgrade to 1.0 anyway at some point?\n\nCheers,\nMathias\n. Thanks, Daniel,\nI'll take a look at your code as soon as I am ready to integrate it.\n. Daniel,\nthis is an interesting addition that I'd like to get into the \"master\" branch.\nHowever, since the new HttpConduit has slightly changed from the one you based your pull request on, you'd have to re-issue a pull request against the \"master\" branch.\nAlso, I'd like to have the \"cookie-jar\" functionality be part of the standard HttpConduit, not a sub-class. We should probably add a boolean setting \"keep-cookies\" to the ConduitSettings (default: on).\n\nThanks again for your contribution and cheers,\nMathias\n. Daniel,\nI think cookie-persistence across several connections of one HttpConduit would be a great feature to have.\nSo, If you'd like to contribute it, we'd sure find a way to bring it into the codebase...\n. Closing for the time being.\n. Please use [repo.spray.cc](http://repo.spray.cc).\nBtw: There is an [excellent mailing list](http://groups.google.com/group/spray-user) to which such questions are better targeted...\n. Ok, thanks for that comment.\nWe'll add a link to the README.\n. Thanks, Ismael, we'll merge ASAP....\n. Bj\u00f6rn,\nthanks for your pull request.\nUnfortunately it doesn't merge cleanly anymore, which is why I have applied your changes manually to the master branch. Could you try building the master once more in your environment and, potentially, resend a new pull request, if there are still tests that are red?\nCheers,\nMathias\n. Thanks, Bj\u00f6rn, for the confirmation.\nIncreasing the stack size is a good idea in any case with Scala....\n\nCheers,\nMathias\n\nOn 04.09.2012, at 20:38, tekener wrote:\n\n> Hi Mathias,\n> \n> every test is green now. By the way, i had some java.lang.StackOverflowError running \"sbt test\". The solution was to increase stack size to 2M.\n> \n> SBT_OPTS=\"-XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256m -Xmx1512M -Xss2M\"\n> \n> Greeting, Bj\u00f6rn\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. Thanks, Paul, for the report.\nIf the `content` directive matches it should \"cancel\" all rejections of the types that it produces itself, therefore avoiding the problem you were seeing in your use case. However, there was a small bug in the cancellation logic, that I just fixed.\n. feature/1.0-M3 is a private development branch.\nIt is not supposed to contain anything that it directly usable for you.\n. Right, thank you!\n. @pmlt This feature will not be part of the 1.0/1.1/1.2 final.\nIt will become available with the move of play onto the new akka-http sometime next year.\n. Fixed with latest rework of spray-can level parsing infrastructure.\n. Thanks for the report, I was able to reproduce the problem.\nWe'll look into it...\n. Thanks a lot for taking the time to report this issue!\n. Yes, I can reproduce the problem on my machine, but I'm not sure it's spray related.\nI'm seeing similar \"connection reset by peer\" errors with (the already patched version of) `ab` when not using persistent connections.\nHowever, as you suspected the chunking config is far from optimal in this modified example setup.\nWhen I increase the [chunk-size](https://github.com/spray/spray/blob/master/examples/spray-routing/simple-on-spray-can/src/main/resources/application.conf#L15) from 5K to 50K the test runs through smoothly, without dropped connections.\n. Alexandre,\nunfortunately we are bogged down with higher-prio work right now and therefore probably won't get to this in the next days. Sorry...\n. We'll soon upgrade to the new `SslTlsSupport` that has now been moved over to Akka 2.2 IO.\nThis will therefore have to be implemented in Akka.\n. Yes, looking at this again I also thing optional path elements will only add confusion and obfuscate what will actually be matched. Closing as \"not a good idea (any more)\"...\n. Great, thanks Age!\nI'll take a look at the conduit stopping as well...\n. Fixed in 1.0-M3\n. Age,\nwith 1.0-M3 out of the door I finally have time to look at your pull request.\nSeveral things:\n1. You are right, the scaladoc comment of the `headerValue` directive is wrong and needs to be fixed.\n2. I like your idea of providing an overload for the `headerValue` directive that simply takes the header name and extracts the corresponding header value. For this overload a `MissingHeaderRejection` makes sense.\n3. Your `HeaderUnmarshaller` has exactly the signature of the current `headerValue` directive. So, you are in fact trying to extract a custom value from a header. Since we deemed this the most common use case we shaped the `headerValue` directive into its current form. From looking at your code I see an improvement opportunity though: we can easily provide a `MalformedHeaderRejection`.\n\nSo, what I'd suggest we do is this:\n1. Add two predefined Rejection types:\n   \n   ```\n   MissingHeaderRejection(headerName: String)\n   MalformedHeaderRejection(headerName: String, error: Exception)\n   ```\n2. Add a `headerValue(headerName: String): Directive[String :: HNil]` directive, which rejects with a `MissingHeaderRejection` if the header is not present.\n3. Wrap the function execution of the other `headerValue` overload with `try/catch` logic producing a `MalformedHeaderRejection` upon any exceptions caught.\n\nThis leaves only one case to be taken care of: the `MissingHeaderRejection` in the non-String `headerValue` overload. Luckily there is an easy work-around that should solve the problem. With something like this:\n\n```\n`headerValue { case `Remote-Address`(ip) => ip } | reject(MissingHeaderRejection(\"Remote-Address\")\n```\n\nyou can easily produce a proper `MissingHeaderRejection`, if you really need it.\n\nWould all this work in your use case?\n\nCheers,\nMathias\n. > Then for point 3, could you perhaps show a use case example of what it would look like to use the non-String headerValue overload to create an instance of some custom class using the extracted header value? I'm having a hard time visualizing it.\n\nThe `cookie` directive you quoted is a good example. The [clientIP](https://github.com/spray/spray/blob/master/spray-routing/src/main/scala/spray/routing/directives/MiscDirectives.scala#L58) directive as well.\n\n> it would still be nice to have some kind of as[X] unmarshalling support to convert header (and cookie) values to some custom type, but that might not be needed by many people and it is easy to add yourself when needed.\n\nWell, I would generally encourage people to _not_ use the name-based overload, but rather the properly typed one for extracting value. It has the advantage of being type-safe in that the compiler will catch you, if you change the name or implementation of the header, whereas a regex-based approach is inherently unsafe.\n. Just to close this:\nThe ideas discussed in this issue are now implemented.\n\nCommits:\n- https://github.com/spray/spray/commit/d00fde58a6e54ee87fd37f206fb83c03ca496443 (master branch)\n- https://github.com/spray/spray/commit/dcfc944eec51eaceec39d2c870eb0aba8695c107 (release-1.0.x branch)\n. No, more something like this:\n\n```\npath(\"foo\") {\n  get {\n    proxyTo(\"http://oldapi.example.com\")\n  }\n}\n```\n. Probably not a good idea, as feedback will be too scattered.\n. Also add support for a \"confirmed\" close, i.e. delay the dispatching of a `Closed` event until the peer has ack'ed the FIN.\n. To quote Age:\n\n> I created a little alias for adding the proper gzip request-response behaviour to my own app:\n> \n> def withGzipSupport = (decodeRequest(NoEncoding) | decodeRequest(Gzip)) &\n>                      (encodeResponse(NoEncoding) | encodeResponse(Gzip))\n> \n> You could argue that this should be the default for all apps that don't explicitly specify an encoding because they would be more resilient to different clients. Any reason why this is not the case?\n. Thanks, Paul!\n. Thanks, Paul, for explaining your reasoning in detail. You are right, we'd like to have `routingSettings` be properly overridable in order to give users another way of supplying their own settings, in addition to simply putting them into the `application.conf` that is loaded by the default classloader. In that regard having the member be a `val` was a bug.\n\n> Now for a question: do you have an ETA for when an M5 might appear, and more generally, for when we might expect 1.0 and 1.1?\n\nWe'll set up nightly builds today, so that you should have access to a fixed build very shortly.\nWith regard to the ETA of 1.0/1.1 final: the feature set is pretty much complete (apart from smaller things like a few additional directives that we are going to add). Currently we are planning for a first RC in the next weeks, definitely before the end of the month.\n. now in master (as part of the akka.io backport from Akka 2.2)\n. According to the most recent WOFF spec recommendation the media-type should be `application/font-woff`.\nSee: http://www.w3.org/TR/WOFF/#appendix-b\n. Great, Jacobus!\nI think the community will certainly applaud the making public of all additional examples, so S4 stack example is going to be much appreciated, no doubt.\n\nI'm not sure we'll have to resources to maintain your example ourselves, but this shouldn't keep you from publishing it!\n. Great!\nThanks for the pull request!\n. Jan, let me know if this fix doesn't work for you...\n. cannot reproduce\n. Why?\nWhat was the idea here?\nUnclear due to crappy ticket label/description.\n. also see [this thread](https://groups.google.com/d/topic/spray-user/xmZQV5OT8Vg/discussion)\n. Sam,\nnote that that the `spray.can.client.ssl-encryption` setting is gone without replacement since M6.\nOne single HttpClient instance can now handle encrypted as well as unencrypted connections.\nThis should simplify a few things for you...\n\nCheers,\nMathias\n. sendReceive depends on an HttpConduit, which must be explicitly created.\nCan you show a snippet of what you'd like your code to look like from the user perspective?\n. > HttpConduits can be safely shared, right? As in, sharing them won't bottleneck how many requests we can make with the http client?\n\nCorrect. Sharing HttpConduits is no problem. However, since they are bound to a specific host and port in many cases sharing them will not be as useful as sharing HttpClients, which can handle thousands of connections to many different hosts.\n\n> From a user perspective, the conduit thing is just boilerplate \u2013 if you could add a sendReceive method onto the HTTPClient that would be perfect, or allow the HttpConduit to take in o aHttpClient\n\nNo, the HttpConduit is not just boilerplate. It manages a pool of connections to a specific host and thus constitutes another (different) layer of abstraction.\n. We are currently in the process of significantly refactoring the architecture of the client-side actor organization in all layers (spray-io, spray-can and spray-client). The current setup is built too much with symmetry to the server-side in mind and thereby presents a somewhat unnatural and slightly quirky solution.\n\n@Sam: Let's revisit your ideas once the new architecture is out, it might well be that your issue will have been solved in the process.\n. Basic message parsing has been updated.\nHowever, I'll leave the ticket open until we have revamped the HTTP header parser as well (probably with the advent of parboiled2).\n. This has happened with the port to Akka HTTP and will not be happening anymore (apart from bug fixes) in spray.\n. Please check out [this thread](https://groups.google.com/d/topic/spray-user/bFZ7YhWAB3I/discussion) on the mailing list for more information about this...\n. Thanks for the report.\nThis issue is already fixed in the `master` and `release-1.0.x` branches.\n. Ok, we'll probably add a dedicated rejection for rate limiting once it is implemented...\n. Well, on the client-side it's your application that triggers the establishment of new connections. So it should be easy to limit that purely in your application layer. Or would you like spray-can to deny opening a new connection if your application explicitly demands to do so?\n. Since it's not hard to queue new client-side connection requests in your application layer we are going to focus on the server-side with regard to this feature.\n. Now in master (via backport of Akka 2.2 akka.io functionality): config setting: `akka.io.tcp.max-channels`\n. Fixed with https://github.com/spray/spray/pull/351\n. Ok, thanks.\nChanging the `clientIP` directive would work but would also change the semantics.\nCurrently we have a strict ordering of preference with regard to the different source headers. Changing to `optionalHeaderValuePF` would take the first of _any_ of the three headers and therefore not be independent of header ordering.\n. Is this still a problem with 1.0-M8.1? We'll need to look into it...\n. Actually, the first fix is not good enough... we need something better.\nI'm on it...\n. `DateTime` is a model for \"HTTP times\" as defined by [RFC 2616 section 3.3.1](http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.3.1) and as such neither carries time zone information nor milliseconds. If you are searching for a DateTime implementation supporting milliseconds the one from `spray.http` is not the one you are looking for.\n. Great!\nThanks for these fixes!\n. Great. Thanks!\n. fixed in master (via new custom URI parser)\n. What kind of JDK are you compiling against?\n. I am on the exact same environment under OS/X, so I'm assuming you either have some ivy cache problem or some other \"litter\" from before.\nTry again after a \"git clean -xdf\" and possibly cleaning out everything spray related from your ivy2 cache. \n. Can you temporarily fix the collision by commenting out the offending import and insert a\n\n```\nprintln(Date.getClass.getName)\n```\n\n?\n. Another question:\nWhy do you have to build spray yourself?\nCan't you use either a milestone or nightly build?\n. Since this feature is not that hard to implement I don't think it is too far out.\n(If you send us a pull-request it might be available sooner... :) )\n. Thinking about it again I'd recommend to not start working on this issue as your first contribution to spray.\nWe are currently in the process of revamping this part of spray-can, so we might end up stepping on each others toes. We'll take on this issue for M8 soon ourselves...\nStill, thanks for offering! Much appreciated!\n. Yes, we'll get to this very soon.\n. Chris, this feature is important for several big users, so we have marked it with high priority.\nIf you consider your use case a \"big user\" as well, please shoot me a mail. We might be able to close this issue today or tomorrow if required...\n. Thanks, Mathias, for reporting!\n. When exactly is this happening?\nSimply setting `remote-address-header = on` doesn't trigger the problem for me with either the `simple-http-server` or the `on-spray-can` example.\n. Fixed in master\n. Thanks!\n. Thanks, Chap!\n. Jason,\nthis is the commit you are looking for:\nhttps://github.com/spray/spray/commit/ed23fc5dfffb8527c18303ea023fe5d56b8412dd\n\n(Please use the mailing list for questions like this!)\n. Thanks!\n. Available in master: https://github.com/spray/spray/blob/db2e32054c8c9b896f2c1864af143bc4960b78ef/spray-io/src/main/scala/spray/io/Pipelines.scala#L42\n. I agree. JSONP is incompatible with the proposed prevention of JSON hijacking.\nHowever, the question remains whether we should have the JSON marshallers produce the prefix for their outputs.\nHere I would argue that apparently modern browsers don't expose this JSON hijacking vulnerability anymore and the issue is therefore not worth it anymore.\n\n@analytically: would you therefore be ok with simply closing this ticket?\n. HTTP message rendering has be completely overhauled, this bug should not appear anymore.\n. Thanks!\n. Already fixed by commit ae61a01c03eb16932d52e5fb07d6c2fdee3aa73d\n. I think it'd make sense to fold this into [this PR/issue](https://github.com/spray/spray/pull/311) and devise a single, joined first cookie jar implementation, fulfilling all key requirements. WDYT?\n. Ok, sorry for taking so long in coming back to this.\nLooking at this patch again, couldn't we rely on the existing Cookie model for session cookies as well.\nIn what way do sessions cookies differ from \"ordinary\" cookies?\n. Magnus, how would you like to move on with this PR?\n. Ok, Magnus, thanks.\n\n@enetsee, how would you like to move on with this?\n. Closing for now.\n@enetsee, if you don't agree with folding this into #311 please let us know.\n. Fixed in master.\n. fixed in master\n. Fixed with latest rework of HTTP model in master branch.\n. Stefan, sorry for taking so long for reacting to your pull-request.\nI think it falls a bit short with regard to issue #216.\nThe HTTP spec requires that header values (and value parameters) be quoted if they contain reserved characters.\nTo simply never quote the cookie values opens up the risk of creating illegal HTTP messages.\n. [RFC 6265](http://tools.ietf.org/html/rfc6265#section-4.1.1) defines the cookie value as such:\n\n```\ncookie-value      = *cookie-octet / ( DQUOTE *cookie-octet DQUOTE )\ncookie-octet      = %x21 / %x23-2B / %x2D-3A / %x3C-5B / %x5D-7E\n                   ; US-ASCII characters excluding CTLs,\n                   ; whitespace DQUOTE, comma, semicolon,\n                   ; and backslash\n```\n\nSo, you are right in that the quoting doesn't actually expand the character class allowed in the cookie value, so we might as well not render it.\nWe are currently in the process of revamping spray's HTTP model and will change cookie rendering to omit the quotes altogether. Additionally, we'll enforce the requirement of cookie values not containing illegal characters, since there appears to be no standard quoting mechanism.\n\nUp until this is implemented I'll leave this ticket open.\nThanks for reporting!\n. Closed with latest rework of HTTP model in master branch.\n. fixed in master\n\nFrom now on all requests leaving the spray-can layer are already fully parsed.\n. Yep. These methods are no more...\n. fixed in master via new URI parser\n. Fixed in master\n. Fixed in current master\n. Sorry for taking so long in responding.\nI don't quite understand the problem you are having with the current implementation of the `optionalCookie` directive.\nCan you elaborate?\n. Ok, I understand now.\nThe `optionalCookie` directive leaking `MissingCookieRejection` is indeed a bug.\nI just pushed [this commit](https://github.com/spray/spray/commit/7e5dd8d798e0a38577a360794d010e6964db0629) with a fix. Thanks for finding and reporting!\n. Provided via `spray.can.server.remote-address-header` setting. Closing.\n. You \"got error message from google with BAD header\"?\nWhat error did you get exactly and from whom?\nCan you show the request that is going out and the response that is coming back?\nWhat exactly do you want to ignore?\n. Timothy,\nI am sorry, but we do not have the capacity to analyze and debug your code.\nIf you would like to report an issue for spray (which we'd be thankful for) we need to ask for a clear description of what exactly it is that you are experiencing, in what type of environments and how it differs from what you would expect.\nWe need to quite significantly prioritize our work on spray, vague bug reports therefore have only very slim chances of actually triggering any action from our side...\n. The warning can be disabled with this config setting:\nhttps://github.com/spray/spray/blob/master/spray-can/src/main/resources/reference.conf#L237\n. Sorry for taking so long in responding to your PR.\nIn the meantime we have support for `become` in PipelineStages ourselves (https://github.com/spray/spray/blob/db2e32054c8c9b896f2c1864af143bc4960b78ef/spray-io/src/main/scala/spray/io/Pipelines.scala#L42) and will probably be moving onto the new pipeline infrastructure coming with the new akka-io soon.\n\nSo this PR will not get merged.\nStill, thanks for contributing!\n. Ok, sorry for the delay in responding.\nWe would like to test the travis integration, so would you be able to update the travis config to Scala 2.10.1, squash your commits and rebase onto the current master?\n. We don't need `-Xss6M` for spray, `-Xss3M` is sufficient. Since the tests create quite a few threads this may already help. Can you try `-Xmx2536M` as well?\n. Interesting. The reason must be related to build process configuration.\nI just checked our Jenkins, we use these simple flags for our nightly builds (which run solidly under Debian Squeeze):\n\n```\n-Dfile.encoding=UTF8 -Xmx2048m -XX:MaxPermSize=512m -Xss2m clean test:compile test\n```\n\nOn my own box (OS/X 10.8.3) I run sbt with these flags:\n\n```\n-Dfile.encoding=UTF8 -XX:MaxPermSize=512m -Xms512m -Xmx3g -Xss3m -XX:+CMSClassUnloadingEnabled -XX:+UseConcMarkSweepGC\n```\n\nNo issues either.\nCan you find out where exactly the process hangs? (e.g. with `jps` / `jstack`)\n. Gilles, do you have any news on this?\n. Thanks a lot, Gilles!\n. Gilles, we just applied a fix (https://github.com/spray/spray/commit/20c4dfd5b54bee617b19a8ae0c3860d3b77fb68b) which seems to fix the seg-fault problem. We'll see how things go...\n. If you are not using PerConnectionHandlers (but a `SingletonHandler`) this is not an issue.\n. You can use the SingletonHandler with the `detachTo` directive or use some other way to offload route execution from the main handler actor. So, don't worry, I don't think PerConnectionHandlers really are required for this.\n. In M8 `PerConnectionHandler` handlers don't exist anymore. With the latest API it is completely up to the user to decide when and how application-level service actors are created and registered, per-connection service actors are one of several possible alternatives.\n. Fixed in M8.\n. Yeah, I think that's good enough.\n. Moved to https://github.com/spray/spray/issues/266\n. Sorry for the delay in looking at this.\nCould you rebase on top of the current master?\n. With the move to akka we'll \"automatically\" move to sonatype. Currently we don't plan to expedite this for the current spray release cycle.\n. Ok, this ticket has just been prioritized.\nWe'll have the spray 1.0.0 / 1.1.0 / 1.2.0 final artifacts on sonatype / maven central soon.\n. Ok, we have just pushed the 1.0.0, 1.1.0 and 1.2.0 artifacts from repo.spray.io to sonatype releases.\nIt'll take some time to sync up with maven central but in a few hours they should have arrived.\n. hmm... it's here:\nhttp://oss.sonatype.org/content/repositories/releases/io/spray/\n\nbut not (yet) here:\nhttp://central.maven.org/maven2/io/\n\nI'll wait another day before investigating the reasons for the missing maven central sync...\n. Yeah, just realized that after the first artifact promotion we need to explicitly request maven central sync (which I just did). Hang in there...\nSorry for the delay!\n. Ok, just got word that maven central sync has now been activated and will complete in at most 2 hours... yay!\n. Probably not a good idea.\nThe \"deeper\" model we have now somehow feels more structured.\n. Thanks, Johan, we'll look at your code as soon as our time permits...\n. Sorry, Johan, for taking so long to get to this.\nIt seems to me that this patch has now been superceded by Derek's (https://github.com/spray/spray/pull/359).\nWould you agree?\n. Ok, Johan, thanks for responding. I'll close this PR then.\n. Sorry for the delay in looking at this.\nWould you be able to squash your commits and rebase onto the current master?\n\nAlso: I'd prefer it if we disallowed the \"empty\" case like this:\n\n```\ndef setCookie(first: HttpCookie, more: HttpCookie*): Directive0 = ...\n```\n\nThis way we can simply extend the existing directive and don't even have to add a new one.\n. Note that you don't have to create a new pull request. Simply `push -f` into your fork and the existing pull request will be updated.\n\nClosing here and continuing with #295\n. already covered by `RespondWithDirectives`\n. Marek,\nthank you for submitting this PR and sorry for not reacting sooner. We've been quite swamped with high-priority stuff these last weeks.\nHowever, I'd like to merge your PR now. Would you be able to update it to the latest version of the master?\nThanks again!\n. Excellent. Keep us posted!\n. Can you simply squash your commits, rebase them into the curent master and `push -f` the resulting commit to your fork? Otherwise the merge is going to create a mess on our side...\n. Thanks, Marek!\n. Unclear.\nWhat is this supposed to mean exactly?\nPlease be more specific the next time you add a ticket!\n. Excellent observation, Nikolas.\nWe'll get that fixed ASAP.\nThanks for the report!\n. Thanks, Heiko!\n. `/api?query=\u0430\u0441\u043f\u0435\u0440\u0438\u043d` is not a valid URI according to [RFC 3986](/api?query=\u0430\u0441\u043f\u0435\u0440\u0438\u043d), which is why you are seeing the exception. Cyrillic characters are not allowed in URIs, they need to be percent encoded.\nIf you are trying to construct a URI its unencoded components you should use the respective `Uri.apply` overload.\n. Yes, we cannot move to Java7 yet.\nWe have to stick to Akka and Scala which are still Java6 compatible.\n. Sorry for the delay!\nCould you update this PR to the latest master?\nI have just merged in support for json4s and improved the tests. It should now be even simpler for you to integrate the support for play-json.\n. Luis,\ncould you sent us a signed [CLA](http://spray.io/project-info/contributing/#contributor-license-agreement-cla) and rewrite the commit message to\n\n```\n+ httpx: add support for play-json (un)marshalling\n```\n\nWe'd like to get this merged ASAP so we can start cutting RC1.\n. Great, thanks!\n. Thanks, Luis!\n. Yes. It'll be in RC1 which we'll release early next week.\n. Thanks, Heiko!\n. Fixed with latest rework of spray-can HTTP parsing layer\n. Will be solved via [this](https://github.com/jrudolph/spray-aspects).\n. Make sure to also cover the problem discussed here: https://github.com/spray/spray/issues/239\n. In order to do this properly the message protocol must be extended.\nAFAICS the best way would be to have the HostConnector accept incoming `ChunkedRequestStart` messages, send them out to a connection and spawn a per-request sub-actor (the `RequestChunkSender`), which handles the rest of the chunked request.\nThis `RequestChunkSender` would then reply back to the user (i.e. the sender of the `ChunkedRequestStart` message) in order to make its ActorRef known so that the user can then send `MessageChunks` to it, which will be dispatched across the connection.\n. Also add option for accepting `;` as separator in query.\n. As this is not a terribly hard thing to add it go in quite quickly.\nMaybe you'd like to take stab at a pull request ... :)\n. Endre, just out of curiosity, what do you use spray for? :)\n. Interesting! Let us know if you find anything you'd like to see improved (except for closing this ticket of course... :)\n. superceded by #365\n. I think you solution falls a bit short.\nAlso it'd be great if you could add some tests...\n. can you please squash your commits into one single one, so it's easier to review (no new PR necessary).\n\nAlso, I'd propose changing the approach and making `ASCII_PRINTABLE_CHARS` a compound mask that is built up from disjunct \"groups\". This way expressions like `ASCII_PRINTABLE_CHARS & ~(AMP | EQUAL)` will work again and be easier to read.\n\nAnd please rename `ASCII_PRINTABLE_CHARS` to `VCHAR` (I realized that I wasn't consistent before either, character groups should be names with singular name (so VCHAR rather than VCHARS), so maybe you can rename the preexisting groups `QUERY_FRAGMENT_CHARS` and `PATH_SEGMENT_CHARS` to their singular counterparts in the process).\n\nThanks for contributing! Much appreciated!\n. Why would you have to add 43 new groups?\nWouldn't we need just one more group that adds all visible charts we haven't yet marked and then \"or\" the bits of all the masks making up a VCHAR? (maybe there is something here I'm not seeing yet).\n. This looks quite nice already, thanks!!\nApart from the comments I think the only thing missing is a `strict: Boolean` parameter to the `UriParser` constructor and the respective `Uri.apply` overloads, so people can decide whether they want relaxed or strict URI parsing.\n. (And I think we are fine with non-strict mode being the default...)\n. This looks great now! Thanks a lot!\nDon't worry about the rest, I can easily finish up the integration of the `strict` flag myself...\nJust let me know, when you are ready.\nThen I'll merge...\n. Thanks, Njal, will merge ASAP...\n. Yes, it does. Sorry for the delay with merging...\nI was awfully busy this week.\nWe'll definitely get this in next week.\n. What about a config setting which effectively disables query parsing altogether?\nSo instead of a parsed query structure you simply get one single raw query string, which you can then subsequently parse yourself. spray would simply treat all characters after the `?` and before a potential `#` as belonging to the query string...\nWouldn't that solve the issue and still keep our architecture clean and lean without adding to much application-specific special-case-handling?\n. Ok, we'll include a new config setting `spray.can.parsing.uri-parsing-mode` with the following defined settings:\n- `strict` for what we have now\n- `relaxed` for what this PR provides\n- `relaxed-with-raw-query` for additionally disabling parsing of the internal query structure\n\nHopefully this is flexible enough for most applications.\n. In master now...\n. As of [this commit](https://github.com/spray/spray/commit/9bd96f345d9e6605decd4fcaa9c4187e0da528ae) the spray master branch (incl. the respective nightly builds) are already on Scala 2.10.1.\n. The `DateTime` defines an actual point in time, i.e. in calendar \"real-time\".\n`System.nanoTime` has no relation to the calendar and therefore cannot be used to determine an absolute point in time. It can only be used for measuring relative time periods.\nTherefore I don't see the `DateTime` type in scope of this ticket.\n. Currently this issue is not high priority for us, so I'd like to hold off with actually implementing it until the respective parts of the codebase have stabilized further. Especially the spray-can HttpServer frontend will undergo at least one more major refactoring in the next weeks...\n. Yes, the idea is to give you a way to still write low-level routing code, but make it more explicit.\nGenerally people should try work with the directives, which, in most cases, should give you all the flexibility you need.\n. It is fixed in the port to akka-http.\nhttps://github.com/akka/akka/blob/release-2.3-dev/akka-http/src/main/scala/akka/http/server/package.scala#L11\n. Wouldn't something like\n\n```\nparameters('view.as[View] ?) { view =>\n}\n```\n\nbe more readable?\n. All you need to do is to make your `fromString` on the `View` companion object implicit, and things should work.\n`View` doesn't have to be a case class for this to work.\n. [issue description replaced with this comment]\n. > I would need to implement 2 entry points doing the same thing except that one is auto-chunking mode and the other regular upload.\n\nWhat would you do in \"regular mode\" when a client sends a chunked request?\nAuto-chunking simplifies the application side logic because you only have to deal with one kind of request: chunked ones. So, I'm not sure that enabling auto-chunking per path would really buy you anything...\n. > will incoming-auto-chunking-threshold-size be supported by spray-servlet?\n\nI suspect not.\nThe only way for us to get to the request entity is via the `httpServletRequest.getInputStream`, which is a blocking abstraction. So I think it's hard for us to send a \"virtual chunk\" up to the application whenever a bunch of new bytes have arrived on the connection because we simply cannot know if and when this is the case.\nAll we see on the inputStream is EOF, which signals that we have read the complete request.\n. Yes.\nIn that regard my previous comment only applies to \"large\" requests, with \"large\" being defined by your application config. You could configure `incoming-auto-chunking-threshold-size` to `0` however to make it apply to all non-empty requests.\n. > I was thinking that if the Content-Length was over the auto-chunking threshold ...\n\nYes, but what about the cases where the incoming request is already a chunked one and there is no `Content-Length` header?\n. > Correct, that's what I have already done but I would also like the option to handle a different path's POST request in regular mode and that's no longer possible ...\n\nNote that the client might decide to send even \"small\" requests with `Transfer-Encoding: chunked`, so you really cannot make the distinction in the way you'd like to.\nCan you give us some more insight into why you'd like to handle small uploads differently from large ones?\n. > I would imagine that if the client enables Transfer-Encoding: chunked mode that it knows that that particular endpoint is capable of handling chunks, otherwise it would be a regular POST request.\n\nUnfortunately this is not the case. HTTP/1.1 _requires_ all servers to be able to handle chunked requests and all clients to accept chunked responses. Otherwise they cannot call themselves HTTP/1.1 compliant.\nSo, as a server you will have to be able to deal with incoming chunked requests no matter what.\n\nWe do realize that the current support for non-aggregated chunked requests is suboptimal in spray-routing. We are working on getting this improved. Ideally you should be able to use spray-routing on non-aggregated chunked requests pretty much as you would otherwise.\n. Agreed.\n. @acruise server-side chunkless streaming is already available for some time\n. Yes, use [this setting](https://github.com/spray/spray/blob/4d711d8e148a710c87f47cf73150cc9f15dcd98d/spray-can/src/main/resources/reference.conf#L91) to enable.\n. Mike, yes, that's good.\nNow, could you squash the three commits into one?\nThen we are ready to go...\n. Thanks!, Mike!\n. Thanks for reporting!\n. Thanks for finding and reporting, Ian!\n. Great pull-request with an excellent test-coverage...\nThank you very much, Adam!\n. After you have applied the minor tweaks I'd be happy to merge!\n. Thanks again, Adam!\n. Yes, this is the originating thread:\nhttps://groups.google.com/forum/#!searchin/spray-user/identity$20encoding/spray-user/0MjoHuFm8Jk/8eZ51MDjokMJ\n. Are you using keep-alive connections?\nWhat does \"benchmarking with ab (Apache Benchmark) and Siege fails\" mean?\nWhat error message are you getting?\nWhat exact command line arguments are you using?\n\nFrom your report it's really hard to reproduce the problem on our side.\nIf you are not sure that what you are seeing really is a bug please turn to the mailing list before filing a ticket here...\n. what happens when you add the `-k` switch to your `ab` command line?\n. You can also try https://github.com/lighttpd/weighttp or https://github.com/wg/wrk for load generation.\nThe problems concerning non-keep-alive connections with `ab` are known. I don't know whether siege suffers from similar symptoms.\n\nClosing until it becomes clearer that the issue really is spray-related.\n. This issue should be fixed in the last nightly build. Can you verify?\n. Great, thanks, we are looking forward to a test case.\nBy the way: don't worry about the server-side, we can easily supply one ourselves.\n\nClosing this issue now, since the \"illegal chunk termination\" appears to be unrelated.\n. Thanks, Eugeny, for this proposal.\nWe just pushed a similar but slightly extended solution into the master branch...\n. Thanks, Eric, for finding and reporting!\n. No, but it will be in tomorrows one.\n. I have an idea of how to greatly increase the power of Marshallers and Unmarshallers, which appears to be not that hard to implement and which should also allow us to close this ticket:\n\nCurrently Marshallers marshal only to `HttpEntity` instances and Unmarshallers unmarshal only from `HttpEntity` instances. Suppose we introduced this additional abstraction:\n\n```\ncase class HttpMessageEssentials(entity: HttpEntity, headers: List[HttpHeader] = Nil)\n```\n\nand these additional kinds of (Un)marshallers:\n- `FromMessageEssentialsUnmarshaller[T]`\n- `ToMessageEssentialsMarshaller[T]`\n- `FromRequestUnmarshaller[T]`\n- `FromResponseUnmarshaller[T]`\n- `ToRequestMarshaller[T]`\n- `ToResponseMarshaller[T]`\n\nThese type classes would form a logical chain in which more specific by default delegate to less specific ones.\nFor example, _spray-routing_ would require a `ToResponseMarshaller[T]` for completion with an object of type `T`. If there is one available all is good. If not the compiler would look for a `ToMessageEssentialsMarshaller[T]`. If none is directly available it would construct one from an implicitly available `ToEntityMarshaller[T]` (which would be identical to the existing `Marshaller[T]`. This way the user can pick the right level for writing custom Marshallers and Unmarshallers. The Entity and Essentials levels would work for the client- and server-side, whereas the most specific level (to and from requests/responses) only on one side.\n\nThis new system would nicely extend the existing infrastructure and existing code would continue to work. It would also allow for setting headers and even status codes from marshallers.\n\n@jrudolph and everyone else: WDYT? Does that sound reasonable?\n. > +1. Do you think this will make 1.1/1.2 final?\n\nYes. This is important enough and will be implemented very soon.\n. Yes, by implementing either a `FromMessageEssentialsUnmarshaller` or a `FromRequestUnmarshaller`/ `FromResponseUnmarshaller` your unmarshalling logic will have access to all message headers.\n. No worries, Age!\n. Addition:\nWhen reworking the marshalling infrastructure we should also think about how to best model composed marshallers, which delegate to sub-marshallers, e.g. because `Foo` can be rendered in three different representations (like `application/json`, `text/xml` or `text/plain`).\n\nSee also this thread: https://groups.google.com/forum/#!topic/spray-user/xtawBl2Ytx4\n. Thanks, Vincent!\nNote that you don't have to create a new pull-request when you'd like to make changes to an existing one.\nJust `push -f` into your fork and the pull-request will be updated.\n. Thanks, Andr\u00e9!\n. Please check out the mailing list for a discussion of this topic.\n(https://groups.google.com/forum/?fromgroups#!searchin/spray-user/swagger)\n. Great, thanks for finding and reporting this problem!\nWe'll get it fixed ASAP.\n. Yaroslav, thanks for this proposal.\nI think it's a good idea to allow the setting of the request timeout also for spray-servlet.\nHowever, since it's already supported for spray-can (https://github.com/spray/spray/blob/master/spray-can/src/main/scala/spray/can/Http.scala#L89) we should not introduce a separate command but rather move the `SetRequestTimeout` class from spray-can into spray-http and also use it from spray-servlet.\n\nAdditionally, while we are at it we might want to do the same for the `SetTimeoutTimeout` command.\n\nWould you like to take a stab at this as well?\n. Yaroslav,\nwe should not add any dependencies between modules for this. Adding dependencies is a major change that requires careful thought, such a small extension like the one we are doing here does not warrant a dependency addition.\n\nHow about we solve it like this:\n1. Move `SetRequestTimeout` and `SetTimeoutTimeout` to spray-http, drop the `extend Command`\n2. Introduce a `private[server] case class SetTimeoutCommand(underlying: AnyRef) extends Command` type in the `spray.can.server` package\n3. In the `HttpServerConnection` override the `running` method to wrap incoming `SetRequestTimeout` or `SetTimeoutTimeout` messages in a `SetTimeoutCommand` (look at the `HttpClientConnection` for an example, on the client-side something similar is already done)\n4. Make `spray-servlet` react to `SetRequestTimeout` and `SetTimeoutTimeout`\n\nI think this should work and keep dependencies clean.\nWDYT? \n. Could you squash your commits into a single one and `push -f` into your fork?\nThis would make the review a bit easier (and clean up the history).\nThanks!\n. Sorry for not having been clear enough before.\nI think all we need is a generic `case class CommandWrapper(cmd: Any) extends Command` type somewhere. It's probably best to put it right into the `spray.io` package (package.scala).\nThen we don't need to duplicate all the command types from spray-http in spray-can again and can simply match like this:\n\n```\ncase CommandWrapper(SetRequestTimeout(duration)) => ...\n```\n. Because we need to send e.g. a `SetRequestTimeout` instance through the command pipeline of the HttpServer or HttpClient. In order to do that we simply wrap it in a `CommandWrapper` instance (whose only member actually should be an `AnyRef` instance).\n. We'd like to uphold a certain degree of type-safety. Having `Command` and `Event` pipelines separated allows the compiler to detect basic \"wrong-message in wrong-pipeline\" situations.\n. Just match like this:\n\n```\ncase CommandWrapper(SetRequestTimeout(duration)) => ...\n```\n. The latter please, it allows us to control what events the pipeline will see. Also, it makes sure that misguided actor messages remain unhandled (and appear, for example, as an `UnhandledMessage` log entry).\n. Apart from the few small comments this looks ready to be merged.\nThanks, Yaroslav!\n. Thanks, Yaroslav!\n. And once more: Thanks, Mike! Great contribution from your side...\n. Nice catch! Thanks for finding and fixing.\nWould you be able to add a test for the this bug as well? (so as to make sure we don't see it again)\n. Gary, I just merged Mikes fix for issue #298, which I think also fixes this issues.\nCan you try with todays nightly build? (just triggered it: 1.1-20130523)\nClosing for now...\n. Thanks, Mike!\n. Gary, can you provide a simply failing test case that you'd like to see working?\nI think it's easier to properly understand the issue by looking at some example code...\n. The cache implementation is intentionally restricted to a maximum capacity of 255 headers (in total!), so it makes sense to not have it be flooded with headers on only one type.\nAdditionally the header cache is currently used on a _per-connection_ basis, for which it is unlikely to see more one single User-Agent header value.\nWe are still thinking about how to best enable sharing of header cache entries across connections, but so far this is not implemented. I'll therefore close this issue (and open issue #306 instead).\n. Since the new parboiled2-based header parser in Akka HTTP is now a lot faster than the one in spray I'd consider this optimization much less important. And since there is probably a lot more optimization potential in other parts of the stack I am closing this ticket for now.\n. In order to implement #365 we'll have to introduce Akka as a dependency for _spray-http_.\nIn a first step we'll probably just do that but leave http & httpx separate.\n. Ok, understood the issues. We'll get it fixed.\nThanks for reporting, Mathias!\n. Can you describe you current use case in some more detail? What exactly do you need?\n. I see, just wanted to make sure we fully understand your motivation.\nI think this is a valid request and should be implemented. Thanks for putting in this ticket!\n. Submitted a PR fixing this. I'll probably be reviewed (by @jrudolph) and merged today, so tomorrows nightly should allow you to access your version parameter.\n. We recently had some changes in related files, can you rebase onto the current master and `push -f` into your fork?\n. Could you clean up your commit history a bit? (If you want, you can also squash everything into a single commit).\nThis makes reviewing a bit easier.\n\n(Regarding the travis failures, let's ignore them for now...)\n. Since the CookieJar is essentially just a map we could have it extend `collection.immutable.Map` so we get the full functionality of the `Map` interface. Also, I'm not sure the map should be `Map[Uri, List[HttpCookie]]`.\nWouldn't `Map[Uri.Host, Set[HttpCookie]]` be more appropriate? (Also see: http://stackoverflow.com/questions/1612177/are-http-cookies-port-specific)\n. Johannes raised some very good points here, which lead me to think that an overly simple implementation might actually do more harm than good. It also shows that there is definitely value in solving this problem of client-side cookie management correctly for our users since it's not easy to come up with something proper oneself.\n\nSo, a \"cookie-jar\" implementation would be great to have but we probably shouldn't settle for anything too simplistic. Maybe the first step should be to agree on a first (minimal) set of requirements/features before thinking about how to best implement them?\n. We are currently in the process of moving all open spray tickets over to the Akka repository, as all future development (apart from bug fixes) will happen there.\nEven though this PR has never made it into spray we'd still like to keep it as a reference to turn to when attacking ticket https://github.com/spray/spray/issues/772.\n\nThanks to everyone, who has contributed to this ticket! Your work is not lost!\n. If we really need a dependency onto Akka 2.2 you should issue your pull-request against the `release-1.2.x` branch, which is already built against Akka 2.2. Upgrading Akka across incompatible versions (like from 2.1 to 2.2) is not something that can be done \"on the side\".\n. Brian, note that the `release-1.2.x` branch has been renamed to `release/1.2` in the meantime...\n. Brian,\nwould you say that this PR has now been superceded by @mpilquist's (https://github.com/spray/spray/pull/329) and can therefore be closed?\n. @topping: Brian, how would you like to move on with this PR?\n. @topping Would you agree that this PR can be closed for now?\n. Stepping back I'm not sure the issue you are trying to fix here is actually an issue.\nThe `DateTime` abstraction we have in `spray-http` intentionally has a precision of 1 sec, not 1 ms.\nSo `DateTime(clicks).clicks` will differ from `clicks` in 999 out of 1000 cases.\nWhy is this a problem in your use case?\n. Martijn,\nI'm sorry if my comments have offended you, I do apologize should that be the case.\n\n> DateTime(clicks) seems like it does support millisecond precision. If it doesn't, it would make more sense to accept seconds rather than milliseconds, and have the user do any rounding or truncating they may want to do themselves.\n\nI understand your point here. Having `DateTime.apply` take seconds instead of millis would be somewhat cleaner on the one hand but would also make the API more difficult to use and be error-prone in that people might just supply `System.currentTimeMillis` because that is the \"standard\" way of getting the current system time. So we have to trade off internal cleanliness against API usability.\n\nMy take now would be to improve the scaladocs of the `DateTime` type to make very clear what it is that it models: a Date + Time abstraction with second precision and without timezone support.\n\n> I figured by the way that those two heap allocations would not be a big deal.\n\nIn many cases they wouldn't be. However, in addition to the immutability we wrote this `DateTime` implementation mainly because we wanted something fast and lightweight. `DateTime` instances are created at least once for every HTTP request or response _spray_ instantiates and as such are somewhat in the \"hot path\". In these areas of the code base we tend to fight for every allocation, otherwise it'd be hard for _spray_ to reach/maintain its position of being among the very fastest of JVM-based HTTP stacks.\n\nWe happily accept pull-requests that make _spray_ better (and not just bigger or different). So, if you show what use cases would benefit from having `DateTime` provide milli-second precision (rather than the second-precision that is all that's required for HTTP) it'd be easier for us to follow you here.\n. > ... it was being outperformed a factor 300 by the original code ...\n\nInteresting. I wouldn't have expected _such_ a large difference. Do you still have your benchmarking code around somewhere?\n. Closing for now. We currently don't feel millisecond precision on `spray.http.DateTime` is required.\n. Agreed. We'll change the decoders to change/remove the `Content-Encoding` header.\nThanks for reporting!\n. I like the idea for a `removeHeaders` and `mapHeaders` transformer for the request side of the pipeline.\nThanks for the suggestion!\n. (discussion continued in PR https://github.com/spray/spray/issues/378)\n. It's already available with todays nightly builds.\n. There will probably be no M9, the next release is going to be RC1, but it will not be available before July.\nYou can simply upgrade to 1.1-20130614 and stick with it until RC1 if you want.\n. Stanislav,\nwhat version does your comment target?\nAre you implying that the fix to this issue is non-effective?\n(Just trying to understand you better...)\n. Just verified that the patch is indeed effective in fixing the bug for me.\nCould you distill a test case demonstrating the problem that you still appear to be having?\n. I see your screenshot but I cannot reproduce the issue. For example, if I change the `/ping` route of the `on-jetty` example to this:\n\n```\npath(\"ping\") { ctx =>\n  ctx.complete(ctx.request.headers.map(h => h.getClass.getName + \": \" + h) mkString \"\\n\")\n}\n```\n\nI get this output:\n\n```\nspray.http.HttpHeaders$Cookie: Cookie: __utma=96992031.823935018.1371557886.1371561377.1371565322.3; __utmc=96992031; __utmz=96992031.1371557886.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none)\nspray.http.HttpHeaders$Host: Host: 127.0.0.1:8080\nspray.http.HttpHeaders$Accept: Accept: text/html, application/xhtml+xml, application/xml, */*\nspray.http.HttpHeaders$Accept$minusLanguage: Accept-Language: en-US, en\nspray.http.HttpHeaders$RawHeader: Referer: http://127.0.0.1:8080/\nspray.http.HttpHeaders$User$minusAgent: User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.110 Safari/537.36\nspray.http.HttpHeaders$Connection: Connection: keep-alive\nspray.http.HttpHeaders$Accept$minusEncoding: Accept-Encoding: gzip, deflate, sdch\n```\n\nAs you can see all headers have been parsed into their respective model classes.\nCould you somehow provide us with a failing test-case?\nOr a failing example that we can run ourselves?\n. Stanislav,\ncan you check out this branch of the spray-template project:\nhttps://github.com/spray/spray-template/tree/wip/318\n\nWhen you start sbt, run `container:start` and browse to `http://localhost:8080` you should see that the request headers are correctly parsed into their high-level model classes.\n. Stanislav,\nyou should generally have all spray JARs on the same version. Mixing versions is not supported.\n. As I said, if you mix versions you are on your own. Why can't you simply upgrade to the 1.2-20130620 nightly?\n. The problem is that according to [the spec](http://tools.ietf.org/html/rfc6265#section-4.1.1) the `Expires` attribute of a cookie must be capitalized, so your cookie is actually invalid.\nHowever, we have relaxed the parser with [this commit](https://github.com/spray/spray/commit/1b84d752c6f4509c8d8ea16242100f5cce0c610b), so if you upgrade to todays nightly this issue should be fixed.\n. similar to what Apache provides with its [`MaxKeepAliveRequests`](http://httpd.apache.org/docs/2.4/mod/core.html#maxkeepaliverequests) setting.\n. Duplicate of #310 \n. Ian,\nhow are you accessing the JSON content?\nIf you use any of the predefined JSON Marshallers they should correctly decode the entity with UTF-8.\n(See implementation [here](https://github.com/spray/spray/blob/master/spray-httpx/src/main/scala/spray/httpx/SprayJsonSupport.scala#L34) and the test [here](https://github.com/spray/spray/blob/master/spray-httpx/src/test/scala/spray/httpx/JsonSupportSpec.scala#L44)\n. Kevin,\nit looks like for some reason the target actor of your \"ask\" has already been stopped by the time you perform the ask. As to why, I don't know.\n\nAnd: Please use the mailing list for questions like these!\n. Just triggered a \"special\" nightly build right off this PR (`1.1-20130618`). We'll do the actual merge into master and polishing (e.g. a few failing tests) tomorrow...\n. Thanks, Michael, for finding, reporting and fixing this bug.\nCould you as well add a test that fails without your fix?\n. Another thing: It's great that you are already following our new commit message policy. Could you add a blank char after the initial `=`?\n. > The normalizedSettings.sslEngineProvider has the default ClientSSLEngineProvider and hence, overrides the implicit ClientSSLEngineProvider in the HttpHostConnector constructor. \n\nHmm... the `normalizedSettings.sslEngineProvider` should also be the correct one. Let's find out why it isn't.\nAnd, from the looks of it, we can actually remove the implicit parameter on the `HttpHostConnector` constructor. The connector already receives the sslEngineProvider via the `HostConnectorSetup` parameter. The explicit parameter is therefore redundant.\n\n> I'd be happy to add a test. Could you point me to the best place to add one?\n\nIt think it'd be best to add a case [here](https://github.com/spray/spray/blob/master/spray-can-tests/src/test/scala/spray/can/client/SprayCanClientSpec.scala#L96). All it would have to do is to make sure that a custom implicit sslEngineProvider is properly used. To make things easy you could specify one like this:\n\n```\nobject CustomProviderUsedException extends spray.util.SingletonException\nimplicit val customEngineProvider: PipelineContext \u21d2 Option[SSLEngine] = throw CustomProviderUsedException\n```\n\nand then simply check in your test, whether the custom exception is thrown.\n. Ahh, I see. Yes, the exception was not a good idea and you volatile solution is absolutely fine.\nThanks a lot, Michael!\nThis looks ready for merge, the only thing missing would be you signing the CLA: http://spray.io/project-info/contributing/#contributor-license-agreement-cla\n\nWould that be ok with you?\n(We don't have a purely electronic CLA singing process in place yet, we will soon though...)\n. And one more small thing: could you change the commit message to?:\n\n```\n= can: fix custom ClientSSLEngineProvider not used by host-level client-side API\n```\n\nThanks again!\n. Great! Thanks a lot, Michael, for this important fix!\n. Apart from the one question this LGTM.\n@jrudolph Do you have any comments?\n. > I'm curious as to why the fallback loading is necessary at all though -- shouldn't the actor system's config settings load all the spray reference.conf files?\n\nYes, the ActorSystem's config _does_ contain all the default settings. However, we provide another overload, which allows you pass in a plain `Config`, which you might have constructed from a String and which only contains one setting. Then we want all the other settings to be the default ones. And for that we need the fallback.\nYour change is therefore a bit too aggressive currently.\n. Michael,\nI think you first solution alternative is indeed the best. _spray_ doesn't really work outside of an ActorSystem anyway, so requiring one to be present is not really a problem IMHO. Let's see what @jrudolph thinks about this...\n\nOne thing: Could you please prefix your commit messages with the module name?\nE.g. `= sbt: make spray.json optional package import` or `! can, routing, servlet, util: load default config settings from ActorSystem rather than explicitly`.\n. Gentlemen,\nthis discussion appears to be fruitful but, unfortunately, is going way over my head.\nUnless @jrudolph has anything to add I completely trust your combined experience and judgement as to how to best  OSGi-enable spray without unnecessarily complicating the API, the project setup or the maintenance process.\n. > @sirthias Do you want me to rebase due to conflicts with 51f325c?\n\nYes, Michael, this would be good. Thanks!\n. Excellent!\nThanks a lot again, Michael!\n. Ok, great!\nThanks for these important directions!\nI'll probably merge later todays and try to get it right... :)\n. Ok, Michael, I just merged master into the [release/1.2](https://github.com/spray/spray/tree/release/1.2) branch.\nI think I got your \"we need to remove the override import for akka.io package\" hint right, but what exactly do you mean by \"replace dependency on akka-actor with akka-osgi\"?\n. So this change:\nhttps://github.com/spray/spray/commit/989b8e49cb20f48831eb922c1c9000a6998bde4d\nis all that's required.\nThanks again for you support, Michael!\n. Ok, thanks for checking.\nI just pushed https://github.com/spray/spray/commit/53ee4ca7b9b196f80cfad77de3b746489cac1c51, which hopefully settles this ticket (for now :)...\n. Also: raw queries should not be decoded during parsing, see [this thread](https://groups.google.com/forum/?fromgroups#!topic/spray-user/Xeh1VS8Tp14)\n. No. But I am working on a fix right now. It's almost done, this ticket will be closed today.\n. yes\n. Thanks, Luciano! We'll take a look at your code as soon as we can.\n. Luciano,\ncoming to back to this we have just discussed that we'd like to keep the size of the code that we have to maintain in the next month as small as possible. Seeing that servlet 3.1 support in containers is still very limited and we can't really test this new connector we'd like to keep this PR open until the time for servlet 3.1 really has come.\n\nOne question: Are you already running on Glassfish 4 and needed this connector? Or was it purely the (good!) intention of helping us with the upgrade that would have to come eventually?\n. Ok, thanks.\nI think many servlet containers are already \"fast enough\". Jetty 9 certainly is.\nAlso, there a few servlet based frameworks/platforms in the top section of the latest techempower benchmark.\nSo, performance is probably already not bad. The disadvantage of having another adapter layer in the hot path is something that a new Servlet API version unfortunately won't be able to fix, so I guess we'll never see _spray-servlet_ be as fast as _spray-can_.\n. We are in the process of moving tickets over from spray into Akka HTTP, where all future development will happen (apart from bug fixes in spray).\nThe development and user uptake of spray over the course of the last years has demonstrated that servlet support is less and less important for the community. This is the reason that Akka HTTP will not support any Servlet API for the time being.\nI am therefore closing this PR.\n\nThank you anyway for submitting this patch!\n. A `fastSplit` would create an intermediate list structure, which I can avoid with the current solution. Still, if I could simply `reduceLeft` the resulting list I probably would have gone for it, but since we either have to add another step mapping the resulting strings to PathMatchers or drop down to a multi-case fold I thought that the low-level solution was better overall.\n\n> And wouldn't this be an example for the + commit prefix?\n\nShoot. Yes, indeed. I'll fix this right away, thanks!\n. Nikolas,\nyour solution would result in an additional `Slash` matcher prefix, which would prevent compatibility with the `path` and `pathPrefix` directives, which already prepend a `Slash` matcher themselves...\n. Derek, could you rebase on top of the current master?\n. I see. Your branch overlaps with a \"dirty\" history rewrite we did a couple of days ago. When we established our new [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages) we retroactively changed all commits messages since the 1.1-M8 tag to conform with the new rules. Since you split off your branch earlier and merged with master several times you now have our commits twice (not your fault, since we were the ones to break the rules and change public history).\n\nNevertheless, we'd like to keep all commits since the 1.1-M8 tag \"clean\" in that they adhere to the new policy.\nSo maybe a single squashed commit containing all your changes on top of the current master would be the easiest. If you like we can also do this for you. Let me know.\n. Derek, sorry for taking a while to get back to you on this.\nThe easiest way to get this done is probably to just cherry pick your commits over onto the current master (so, no merge and no rebase). If you want you can also squash (part of) your commits before in order to clean up the history and reduce the number of commits that you need to cherry-pick.\n. We are happy to address your question if you direct it to the [Mailing List](https://groups.google.com/forum/#!forum/spray-user). The ticket system here is not meant for discussions or questions.\n. Thanks, Nikolas, for the report and your observation regarding the `Empty` naming.\nI agree with all your points, so they have been included in the patch.\nThanks again!\n. Travis still reports this small compiler error:\n\n```\n[error] /home/travis/build/spray/spray/spray-http/src/test/scala/spray/http/HttpHeaderSpec.scala:104: not found: value windows-1252\n[error]       example(`Content-Type`(ContentType(MediaType.custom(\"text\", \"xml\", parameters = Map(\"version\" -> \"3\")), `windows-1252`)))_ ^\n[error]                                                                                                               ^\n[error] one error found\n```\n. As we discussed before making type class types non-invariant is not a good idea. So the only way I saw to make the desired inference work for PathMatchers is the given fix. Just out of curiosity, do you see any other alternative?\n(Not that I think we need it, just thinking options here...)\n\nI'll merge this patch later today...\n. A yes, thanks for spotting the wrong ticket number. Fixed.\n. This is a great start, Jeff, thanks a lot!\nI think after incorporation of the few remarks I just added we are good to merge...\n. Another thing: we'd need you to send us a signed CLA before we can merge your PR: http://spray.io/project-info/contributing/#contributor-license-agreement-cla\n. Thanks for the CLA, Jeff!\nTwo more things before we can merge:\n1. Can you rename the `MethodDirectivesExampleSpec.scala` to `MethodDirectivesExamplesSpec.scala` in order to fit the class name? (Also make sure to update the links to it!).\n2. Can you squash all your commits into a single one with this commit message (see also our new [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages))?:\n   \n     = docs: add method directives docs\n. @jeffywu Please let us know how you'd like to move on with this PR!\n. @jeffywu Would you agree to us \"hijacking\" your contribution? We'll do the remaining bits ourselves if you are ok with that...\n. Thanks, Marek, this is going to be a nice addition!\nCan you also change the commit message to `= http: add model for CORS headers` in accordance with out new [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages)?\n. LGTM, Marek, thanks!\nOne thing: I made a mistake in my recommendation for the commit message. Since this patch adds stuff to the public API the commit message should be `+ http: add model for CORS headers`. Sorry about that!\n\nAfter review from @jrudolph this should be ready for merge.\n. Ok, I updated the HttpHeaderParserSpec according to the header model additions.\nMarek, can you add this commit to your PR?:\nhttps://github.com/spray/spray/commit/c94badf39fcd5da3e31e137e4491c40b84c26418\n\nIt is correct that this spec currently needs to be updated after every change to the header model.\nHowever, for the time being I think this is not too bad. Additionally it makes us explicitly aware of the effect of the header model changes onto the header parser in _spray-can_ (especially the parse trie), so it even has some real benefits.\n. Mark,\ncan you add the `require` for the delta-seconds and remove your last commit (as it didn't help)?\nI think we can merge then and chase the remaining problem ourselves (since your patch doesn't appear to be the culprit).\nThanks again!\n. Excellent!\nThanks a lot again, Marek, for your contribution and patience during the review!\n. Thanks, Mike!\n. Thanks, Mark, for taking a stab at #188.\nBy looking at your solution I get the impression that we now have a lot of duplication between the `AuthenticationRequiredRejection` and the `AuthenticationFailedRejection` that we should be able to DRY up quite a bit.\n\nHow about we merge both directives into this one?:\n\n```\n/**\n * Rejection created by the 'authenticate' directive.\n * Signals that the request was rejected because the user could not be authenticated, either\n * because the `Authorization` header was not supplied or the supplied credentials are invalid.\n */\ncase class AuthenticationFailedRejection(authHeaderPresent: Boolean, scheme: String, realm: String,\n                                         params: Map[String, String] = Map.empty) extends Rejection\n```\n. Thanks, Mark, LGTM!\nAfter @jrudolph has reviewed your patch as well I think we are good to merge...\n. Apart from the minor comments this looks great!\nThanks, Mark!\n. Excellent!\nThanks again, Mark, for this nice fix!\n. Thanks, Derek, great!\nTwo things:\n1. We need a signed CLA from you: http://spray.io/project-info/contributing/#contributor-license-agreement-cla\n2. For some more potential massaging of the abstractions we have two options: we can either merge now and play around with things ourselves or we can do it together in this PR, which require more involvement from you. Would you be ok with the first option?\n. Great, thanks, Derek!\nThe CLA is the same as Typesafe's, which is itself almost identical to the Apache 2 CLA.\n. No updates yet. We are quite busy with the akka migration at this time.\n. I am very sorry that this ticket has not made it into spray.\nWe simply did not have enough capacity to fully take on this potentially big feature (with its outside dependencies) and then maintain and port it into Akka HTTP ourselves.\nNow, spray is in maintenance mode and we are reluctant to change the outside API, let alone adding completely new features.\n\nStill, as it always has been, some kind of built-in support for metrics/monitoring is of course and without a doubt a valuable thing to many users, so we'll move the corresponding spray ticket over to Akka and reference this PR, so that we can build onto @derekwyatt's work over there.\n\nOnce again, I am very sorry that this contribution hasn't made it in.\nAfter all, it addresses ticket https://github.com/spray/spray/issues/9, the last single-digit ticket in spray's history. Over in Akka HTTP-land there is now a full and paid team on everything, so slips like this shouldn't happen anymore.\n\nThanks again to everyone who has contributed to this PR!\n. I understand the issue and can see how padding can help. The `StatsSupport` appears to be pretty vulnerable to false sharing as we are allocating a whole range of AtomicLongs right after each other, which will likely cause them to end up in a single or maybe two cache lines together.\nYour patch will fix this as long as cache line size is <= 64 bytes. Even if it's higher it should prevent all 7 AtomicLongs from living in the same line.\n(While looking around I found an interesting bit from Nginx which appears to have quite a bit of logic for figuring out cache line size for modern CPUs: http://forum.nginx.org/read.php?29,226088)\n\nThe only downside to this patch I can see is the increased memory consumption, which however is not a problem since we are allocating only a single `StatsHolder` per server. So, stepping back I'm inclined to accept the patch. WDYT, Johannes?\n. Mathias,\nJohannes and I discussed this, the `LongAdder` is definitely what we'd like to use eventually. However, as you already said, we are still on Java 6, so it's out of reach for the time being.\nSo, until we officially move to Java 7 let's use your `PaddedAtomicLong` as a temporary improvement.\n\nCould you please:\n1. move your `PaddedAtomicLong` implementation to _spray-utils_\n2. add a comment `//TODO: remove after upgrade to Java 7 + JSR166e LongAdder`\n3. fix the commit messages to conform with our new [commit msg policy](http://spray.io/project-info/contributing/#git-commit-messages)\n4. send us a signed [CLA](http://spray.io/project-info/contributing/#contributor-license-agreement-cla)\n\nThanks again for contributing!\n. Thanks for reporting, Mathias!\n. It would allow you mix content that you application provides in-memory with content coming from files.\nThis might be useful, for example, for producing multipart message entities.\n. We already have support for (un)marshalling multipart messages.\nCheck out this thread for more background on this ticket: https://groups.google.com/forum/#!searchin/spray-user/wall/spray-user/oS0GNXHSMJU/uIFDFkq42moJ\n. Yes, that would be the easiest solution.\nJust want to make sure this makes sense in all cases, or whether we need to find a smarter solution in some cases...\n. Not directly relevant anymore for Akka HTTP (as we use the Akka testing infrastructure now).\n. Actually, Johannes and I discussed this improvement once more and we came up with an even simpler and yet overall better solution which I've just outlined in this post to the mailing list:\nhttps://groups.google.com/d/msg/spray-user/8sbfL-QB5es/gxuIvwtN3WEJ\n\nWDYT?\n\nWould you be willing to update this PR accordingly?\n\nI'm sorry for being the cause for the rework, you simply were too fast with this PR! :)\n. Great, thanks again, Michael, for your continued energy to help improve _spray_.\nIt's very much appreciated!\n. Thanks, Michael.\nI'm wondering if there is any drawback to _always_ adding the special header.\nThe overhead for allocating the extra header is minimal, but it might complicate things a little bit (like the tests for example). Another scenario could be proxying, where someone would like to forward to request to another server (either via \"raw\" _spray-can_ or _spray-client_). She would then have to filter the request headers first.\nAlso, always adding the header somewhat breaks behavioral symmetry (with regard to request headers) to _spray-can_.\n\nShould we hide the header addition behind an extra setting?\n. Looks good!\nThe only thing missing is adding the new `servlet-request-access` setting to the _spray-servlet_ `reference.conf`, along with a comment.\nThen we should be good for merging.\n. Excellent! Thanks a lot Michael, once again!\n. Olger, thanks!\nYour PR is currently based on an obsolete version of the master branch (due to us having rewritten history two weeks in order to change all commits since M8 to comply with our new [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages).\n\nAs a first step (and assuming that your `origin` remote points to `git://github.com/spray/spray.git`) could you please do the following in the master branch of your fork?:\n- `git fetch origin`\n- `git reset --hard origin/master`\n- `git cherry-pick 25e53a8`\n- `git push -f` into your fork\n. Sorry, but we did not have enough capacity to include this PR in the current release cycle.\nWe'll take it on when our bandwidth allows...\n. @olger Currently not, thanks! I think the easiest way forward would be to simply leave this PR where it is now. We'll get back to it when the time is right...\n. I am very sorry that this ticket has not made it into spray.\nWe simply did not have enough capacity to merge, maintain and port it into Akka HTTP ourselves. \nNow, spray is in maintenance mode and we are reluctant to change the outside API, let alone adding completely new features.\nStill built-in support for OAuth is no doubt a valuable thing to have, so we'll move [the corresponding spray ticket](https://github.com/spray/spray/issues/1007) over to Akka and reference this PR, so that we can build onto @olger's work over there. \n\nOnce again, I am very sorry that this contribution hasn't made it in in time.\nOver in Akka-land there is now a full and paid team on everything, so slips like this shouldn't happen anymore.\nThanks again to everyone who has contributed to this PR!\n. Actually I'd be inclined to move `spray.client.pipelining.unmarshal` from the package object into the `spray.httpx.unmarshalling.Unmarshaller` object and leave the name as is. This should prevent the collision in most import scenarios and, while it is a breaking change, shouldn't affect as many people as a renaming of `spray.client.pipelining.unmarshal` probably would.\n\nIf you'd like to submit a PR, we'd be happy to have it, thanks! \n. Great, looks good!\nAfter the small changes we should be good to merge.\n. Great, thanks!\nNow there is only one more small nag left: the commits don't quite fit the policy that we are trying to establish/uphold. How about something like this?:\n- `+ util: add PaddedAtomicLong`\n- `= can: upgrade StatsSupport.StatsHolder to new PaddedAtomicLong`\n- `= sbt: upgrade dependencies`\n\nSorry for being a PITA on this but we'd like to rely on the commit messages for building migration guides, which is why they are important.\n. Excellent!\nThanks again, Mathias!\n. Or, as an alternative we could use a magnet:\n\n```\nobject Unmarshaller {\n\n  def unmarshal[T](implicit m: Magnet[T]): T = m.unmarshal\n\n  trait Magnet[T]\n  object Magnet {\n    ...\n```\n\nAnd you are right, we should also move the `unmarshalSafe` method into the `Unmarshaller` object!\n. Actually my idea regarding the magnet was crap, so please just ignore it.\n\n> I would suggest renaming it to `unmarshaller[T]` to make it more clear what it actually does. Then we can add\n> `def unmarshal[T: Unmarshaller](entity: HttpEntity): Deserialized[T]` to have parity with `unmarshalUnsafe` again. > What do you think?\n\nYes, this sounds exactly right! Thanks, Age!\n. Thanks for this very nice ticket closure, Age!\n. Apart from the two comment this looks great, thanks Age!\nCould you also rewrite the commit messages to conform to our new [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages)?\n. > I'll have to refresh my Git foo for rewriting those commit messages but I'll do my best ;)\n\nThe easiest might be a `git rebase -i HEAD~2` followed by a `git push -f`.\n. Excellent! Thanks again, Age!\n. This is a great post, Matthias, thank you very much!\nApart from the project as a whole I especially like the tons of links you've included!\nExcellent stuff!\n\nLet us know when you think the post is ready for merging!\n. And the post is online.\nThanks again, Matthias!\n. Excellent, Andr\u00e9, this looks like a great patch.\nGive us some more time for the review, but judging from the first impression this will definitely have to go in!\nThanks already!\n. Andr\u00e9,\nduring the review I discovered a few things that I didn't like about the way the MediaRanges/MediaTypes are currently modeled (which has nothing to do with your patch). Adding `qValue` support presented a good opportunity to get these things improved. In the process I also fixed a few remaining issues in your patch, I hope you don't mind.\n\nWould you be able to cherry-pick [this commit](https://github.com/spray/spray/commit/fad2ff24986afd65c982dbad58cd7964a7cdc0b2) on top of your branch so it does get included into this PR?\n\nI also fixed all the tests that were still failing with your patch. The only thing that is not yet done are some additional tests verifying the correctness of the new q-value based content negotiation logic.\nIdeally we'd have some low-level unit tests in _spray-http_ (testing mainly the `acceptableContentType` logic on the `HttpMessage`) and some higher-level ones in _spray-httpx_ and/or _spray-routing_ (testing marshaling against incoming requests). Would you be able to add some?\n\nOtherwise this is a really great contribution, thank you very much for getting the ball rolling on this!\n. Two more things:\n1. Could you add some more meat to the commit message of your commit?\n   Since it introduces breaking changes it'd be good if you list what they are and what motivates them.\n2. We need you to sign [our CLA](http://spray.io/project-info/contributing/#contributor-license-agreement-cla) before we can merge...\n. To be honest, we haven't really been watching out for Windows-compatibility too much.\n(And we've been receiving very little feedback as to this matter, which makes us think that somehow the share of people building _spray_ on Windows is somewhat small...)\n\nIf you see something that you think needs fixing a PR for it would be much appreciated!\n. Could you change your last commit message to `= http: small cleanup`?\n. Let's get this fix merged ASAP and save the EOL stuff for another PR, if you don't mind.\nThis patch touches a number of important things and thus might cause conflict pretty quickly...\nSo, whenever you are ready I'd like to push the green button here...\n. Let's keep the polish patch separate, so all you need to do is squash your two most-recent commits.\nIf you don't have time for the tests now then let's save them for a later PR as well, ok?\n. Ok, great.\nNow the only thing missing is the CLA...\n. Excellent! Thanks again!\n. Nice patch!\nI especially like the new and improved tests!\nThanks once again, Age!\n. > I made the isContentEncodingHeader val lazy so it doesn't get pre-allocated unless it is needed.\n\nActually, I'd propose to keep it as a simple `val` for performance reasons. The one allocation is probably not worth the (small) performance penalty introduced by the volatile read (and the resulting memory barrier) required for the `lazy val`. Since the decoders are potentially used from many different threads at the same time this can measurable.\n\nIf you'd like to limit allocations we could move the `val` into a `Decoder` companion object, so it doesn't get instantiated once per Decoder but only once. However, the companion object would introduce a new class and another allocation as well. And since we currently only have two different decoders this would therefore not save anything...\n. > This does indicate that it might have been better to implement the removeHeader(s) stuff directly on HttpMessage (or HttpRequest?) instead of only on RequestBuilding. What do you think?\n\nYes, we could certainly move `addHeader`, `mapHeaders` and `removeHeader` functionality right into the `HttpMessage`. If you want you could submit another PR with these changes first and update (rebase) this PR after the other one has been merged...\n. And another one from Age in.\nYou are quickly rising to the top spot on _spray's_ contributor list! :)\n. Excellent, Age, please keep them coming! :)\n. We currently cannot reproduce the NPE ourselves. However, I just added a commit that should help us in at least locating what exactly is triggering it.\n. Great, Ruslan, thanks for this fix!\n. The `StringRendering` exists so we can keep everything DRY and reuse the rendering logic for producing proper `toString`s for our model objects.\nAnd for consistency we want the `toString` representation to exactly match what is also rendered as ByteStrings to the network, so pure ASCII encoding as well.\n\nWe could use the `ASCII` charset for this but I wasn't sure whether it really performs the same \"toByte\" conversion from the string characters as we do with the `ByteStringRendering`, so I used the deprecated string constructor, which does. (Briefly looking at [the ASCII charset implementation](http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/6-b14/sun/nio/cs/US_ASCII.java) gives me the impression that I was right.)\n. Excellent!\nThanks once more, Age!\n. Cool! This will be another ticket nicely solved, thanks, Andr\u00e9!\nLet us know when you are happy with this patch, then I'll merge...\n. > Additionally, if we rename it to detach we should still support the old detachTo directive so that we don't break the API but instead deprecate its usage and direct users to the new detach directive.\n\nYes, that'd be possible.\nHowever, we are already introducing breaking changes all over the place, so I'm not too worried about removing `detachTo` right away. The important thing is to format the commit message according to the new policy and explain what changed and (ideally) what is required for a migration.\nWe will compile a migration guide from all commits with breaking changes, so an update shouldn't be too hard for all users...\n. Andr\u00e9, I think one squashed commit is good enough, thanks!\n. When you've changed the last small bits and @jrudolph has signed off on this as well I think this is ready for merge.\nThanks a lot again, Andr\u00e9!\n. Yeah, there appears to be some compiler problem preventing it from seeing the implicit conversion in the parent class. In such cases introducing another level usually works, here it does as well:\n\n```\nclass DetachMagnet()(implicit val ec: ExecutionContext)\nobject DetachMagnet {\n  implicit def fromUnit(u: Unit)(implicit dm2: DetachMagnet2) = new DetachMagnet()(dm2.ec)\n}\n\nclass DetachMagnet2(val ec: ExecutionContext)\nobject DetachMagnet2 extends DetachMagnet2LowerPriorityImplicits {\n  implicit def fromImplicitExecutionContext(implicit ec: ExecutionContext) = new DetachMagnet2(ec)\n}\nprivate[directives] abstract class DetachMagnet2LowerPriorityImplicits {\n  implicit def fromImplicitRefFactory(implicit factory: ActorRefFactory) = new DetachMagnet2(factory.dispatcher)\n}\n```\n. Also, could you rebase on top of the current master?\nCurrently this patch doesn't merge cleanly anymore...\n. Another idea: let's allow for explicitly specifying an ExecutionContext as well, so we can say:\n\n```\ndetach(mySpecialExecutor) { ...\n```\n\nwith\n\n```\nobject DetachMagnet {\n  implicit def fromUnit(u: Unit)(implicit dm2: DetachMagnet2) = new DetachMagnet()(dm2.ec)\n  implicit def fromExecutionContext(ec: ExecutionContext) = new DetachMagnet()(ec)\n}\n```\n. Thanks, Andr\u00e9!\nGreat work!\n. Keep 'em coming! :)\n. Nice work, Andr\u00e9, thanks a lot!\nI think the validation is good enough, we don't have to fully match the JS identifier definition.\n\nOne additional thing: Could you rewrite the commit message to conform with our new [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages)?\n. Excellent, Andr\u00e9, thanks!\nIt's great to have so many quality contributions flowing in...\n. Can you append a \", fixes #308\" to the second commit message?\n. Another thing: I think a squashed commit is better.\nWe generally try to keep the master branch compiling between _every_ commit.\nThis way it's easier to use stuff like `git bisect`.\n\nThanks!\n. @jrudolph: any objections to a quick merge?\n. Closing as \"invalid\".\n. In order to ensure proper `keepOpenOnPeerClosed` behavior this patch requires https://github.com/spray/spray/issues/401\n. Great contribution, Andrew, thank you very much!\nApart from the small remarks this looks already very much ready.\n\nOne additional thing: we'd need you to sign the [CLA](http://spray.io/project-info/contributing/#contributor-license-agreement-cla) before we can merge.\n. Andrew,\nwe have just discussed the `copy` problem a bit more. You are raising a very good point, the `copy` semantics are hard to get right with the additional and \"silent\" `raw` member. Maybe our initial proposal of adding this member to the `Uri` model wasn't as great as hoped.\n\nWhat do you think about this alternative solution?\nInstead of uglifying our up-to-now quite clean URI model with access to the raw URI we add a new config setting `spray.can.server.raw-request-uri-header` in analogy to the already existing `remote-address-header` setting.\nPer default this setting is `off`, but when you enable it _spray-can_ will add a custom `Raw-Request-URI` header to the request.\n\nThis has several benefits:\n1. The URI model stays clean.\n2. The user has full flexibility with regard to when to access, change or remove this additional header.\n3. No performance penalty for the majority of users that don't need access to the raw URI.\n4. Symmetry with other design choices (e.g. the access to the client remote address).\n\nWDYT?\n. Andrew,\nyou are right in that we should try to keep _spray-can_ and _spray-servlet_ aligned as much as possible.\nTherefore we should add a `spray.servlet.raw-request-uri-header` setting as well (just as there already is a `spray.servlet.remote-address-header` setting).\n\nLooking at the _spray-servlet_ `ModelConverter` again I realized that _spray-servlet_ currently doesn't mirror _spray-can_ with regard to the request URIs it generated. I have justed added a respective ticket:\nhttps://github.com/spray/spray/issues/415\n\nAFAICS we can't really avoid running something like the URI rebuilding snippet you show if `spray.servlet.remote-address-header` is enabled. If not I think we should be good with what we currently have, the only thing missing is the call to `withEffectiveUri` like here: https://github.com/spray/spray/blob/master/spray-can/src/main/scala/spray/can/server/RequestParsing.scala#L46\n\nAnd I think we also have to add a `default-host-header` setting to _spray-servlet_:\nhttps://github.com/spray/spray/blob/master/spray-can/src/main/resources/reference.conf#L133\n. Yeah, sorry for leading down the wrong path initially...\nHowever, I think we can still do the simplification of the `Uri.Impl` stuff.\n\nThanks a lot again for your contribution and the great discussions. Very fruitful!\n. Yes, I just deprioritized it.\n. Comment from the ML (https://groups.google.com/forum/#!topic/spray-user/40ni-idIqcc):\n\n> That's great! I think removing the provider from `Http.Connect` and\n> `Http.HostConnectorSetup` messages makes sense. One note about the FQCN\n> though -- what about optionally setting the provider via a new message to\n> Http? Or some other technique that allows client code to access values that\n> are available at runtime but not in the actor system config? In my case, I\n> need to retrieve the SSLContext from a shared service that's not accessible\n> statically. I can get a reference to the shared service at runtime, before\n> doing any client SSL commands.\n. Duplicate of https://github.com/spray/spray/issues/721\n. Age, thanks for taking this on!\n\nLooking at your proposal I'm not so sure it really is a good idea to combine the response encoding and the request decoding into joint directives. I can image that there are quite a few use cases where you only ever need one side of the two and the other one would only add unnecessary overhead.\n\nHow about something like this instead?:\n\n```\ndef compressResponse: Directive0 = compressResponse(Gzip, Deflate)\n\ndef compressResponse(first: Encoder, more: Encoder*): Directive0 =\n  if (more.isEmpty) encodeResponse(first)\n  else more.foldLeft(encodeResponse(first)) { (r, encoder) => r | encodeResponse(encoder) }\n\ndef compressResponseIfPossible: Directive0 = compressResponseIfPossible(Gzip, Deflate)\n\ndef compressResponseIfPossible(first: Encoder, more: Encoder*): Directive0 =\n  compressResponse(first, more: _*) | encodeResponse(NoEncoding)\n\ndef compressResponseIfRequested: Directive0 = compressResponseIfRequested(Gzip, Deflate)\n\ndef compressResponseIfRequested(first: Encoder, more: Encoder*): Directive0 =\n  encodeResponse(NoEncoding) | compressResponse(first, more: _*)\n\ndef decompressRequest: Directive0 = decompressRequest(Gzip, Deflate)\n\ndef decompressRequest(first: Encoder, more: Encoder*): Directive0 =\n  if (more.isEmpty) decodeRequest(first)\n  else more.foldLeft(decodeRequest(first)) { (r, encoder) => r | decodeRequest(encoder) }\n\ndef optionallyDecompressRequest: Directive0 = optionallyDecompressRequest(Gzip, Deflate)\n\ndef optionallyDecompressRequest(first: Encoder, more: Encoder*): Directive0 =\n  decodeRequest(NoEncoding) | decompressRequest(first, more: _*)\n```\n. Looking through the sources I found another small problem that we can fix with this patch:\nThe `decodeRequest` directive currently only cancels `UnsupportedRequestEncodingRejection` rejections.\nCan you replace the respective line with this one?:\n\n```\ncancelAllRejections(ofTypes(classOf[UnsupportedRequestEncodingRejection], classOf[CorruptRequestEncodingRejection])))\n```\n. I think I manually created the hex dump on the command line.\nIIRC I wanted to make sure that the decoder is able to digest input that i has not created itself.\nIs the current solution non-adequate?\nMaybe we should add a few comments explaining the reasoning...\n. Ok, for some reason I can't reproduce the generation of this hexdump myself right now.\nWhen I run this:\n\n```\nmathias@cox ~ > echo Hello | gzip | hexdump\n0000000 1f 8b 08 00 b5 c0 fb 51 00 03 f3 48 cd c9 c9 e7\n0000010 02 00 16 35 96 31 06 00 00 00\n000001a\nmathias@cox ~ >\n```\n\nI get a non-matching output, no matter what compression level I give to `gzip`.\n\nI think a round-trip test is good, but it probably makes sense to also have a test with \"foreign\" compressed content, e.g. the one produced by `gzip` on the command line.\n. > For the exact same command I get different outputs practically every time I run it. I don't know enough details about Gzip but does the CRC contain some kind of timestamp value?\n\nHmm... I get this:\n\n```\nmathias@cox ~ > echo Hello | gzip | hexdump\n0000000 1f 8b 08 00 b5 c0 fb 51 00 03 f3 48 cd c9 c9 e7\n0000010 02 00 16 35 96 31 06 00 00 00\n000001a\nmathias@cox ~ > echo Hello | gzip | hexdump\n0000000 1f 8b 08 00 0c c5 fb 51 00 03 f3 48 cd c9 c9 e7\n0000010 02 00 16 35 96 31 06 00 00 00\n000001a\nmathias@cox ~ > echo Hello | gzip | hexdump\n0000000 1f 8b 08 00 0c c5 fb 51 00 03 f3 48 cd c9 c9 e7\n0000010 02 00 16 35 96 31 06 00 00 00\n000001a\n```\n. So we have the reason why it's hard to reproduce the exact same hex dump.\nLet's just go for something and add a comment on where the dump is from, ok?\n. Great, Age, thanks!\nJust thinking: if we test the round-trip in a way where we first compress and then decompress (rather than the other way around), we should be fine, right?\n. Sorry, for being silent for so long on this.\nI agree with Johannes in that we can probably simply remove the `decompressRequestIfRequested` directive from this patch.\nOtherwise this is a very addition, I especially like the ton of test you have added!\nAnd even though there are definitely cases where the application _requires_ the client to accept compressed responses (we have one of those ourselves) you are probably right in not going overboard with the predefined directives here.\n\nIt's good enough if someone can just say `compressResponseWith(Gzip)` in this case.\n. Since we are targeting the APIs of the world, rather than general browser-facing web applications, I would say that the requirements with regard to compression very much _do_ belong to an API specification.\n\nI know several REST APIs that _require_ their clients to send their requests with `Accept-Encoding: gzip` and will refuse to deliver any uncompressed content. Even with this patch the only way for me to express that in spray would be `compressResponseWith(GZip)`.\n\nOne step less restrictive would be to use the `compressResponse` directive, which does compress all responses except in cases where the client _does not_ accept compressed content. I'd consider this a nice middle-ground \"default\" behavior which is why I like the short `compressResponse` name for it.\n\nThe most relaxed directive with regard to compression is `compressResponseIfRequested`, which _only_ compresses if the client explicitly requests compression. In most cases I'd say this is _not_ what you want, which why I'd say the current longer name is ok.\n\nSo, I like generally the current proposal. However, we need to make sure to properly document theses directives and include a section explaining the differences and how the user can achieve exactly the behavior he/she wants.\n. I am not in favor of a combined `enableCompression` directive. Compressions on requests and compression on responses are two completely different things, which should be explicitly configured independently from each other.\n\nAs we have seen in our discussion above there is already quite a lot of potential for confusion only with regard to one side (requests or responses). Combining things together here will make it even harder to properly understand how spray handles the various possible cases.\n\nSo, I'd say we keep the things separate and document properly.\n. Excellent! Thanks, Age!\n. Great!!\nThank you very much again for all your work and perseverance with this PR... :)\n. We usually wait for a few things to accumulate before doing a merge into the release branches.\nThe last merge was on Aug 22th, so not too long ago.\nI guess the next will come this week.\n. Thanks, Ruud, for reporting!\nWe'll get this fixed...\n. Thanks, Ruud, we'll get this fixed as well.\n. Closed by https://github.com/spray/spray/commit/eddb32e5dca39af7a00dd537afd8e2e877a76246\n. Age,\nthanks for this contribution.\nI think adding a `scheme` directive is a good idea.\n\nHowever, like Johannes I am somewhat worried about namespace pollution with a very general directive name like `http`. I think the `get` and `put` directives already are a problem that we might need to fix at some point.\n\nHow about we simply leave it at the `scheme(String)` directive?\nHaving `scheme(\"http\")` in my routing structure might even be more readable with regard to intent that a sole `http`.\n\nAlso we could extend and simplify the thing like this:\n\n```\ndef scheme: Directive1[String] = extract(_.request.uri.scheme)\n\ndef scheme(schm: String): Directive0 =\n  scheme.require(_ == schm, Some(SchemeRejection(schm))) &\n    cancelAllRejections(ofType[SchemeRejection])\n```\n\nWDYT?\n. Yeah, we can't really have a zero-arg directive with the same name as one that does take parameters.\nSo my naming proposal from above was crap.\n\nHow about we follow the lead of the [HostDirectives](https://github.com/spray/spray/blob/master/spray-routing/src/main/scala/spray/routing/directives/HostDirectives.scala#L31) which face the same problem of extractors and filters?\n\nSo our zero-arg `scheme` extractor would become `schemenName`.\n. Nice! Thanks again also from my side, Age!\n. Thanks, Mark, much appreciated!\n. We have looked at the logback implementation of SLF4J and, unfortunately, the logger management is done in a very naive way. Logback _never_ releases a logger once it is created, which means an application has to itself ensure to never create more than a bounded number of loggers. In our view this is a severe limitation of logback and greatly reduces its general utility.\n\nThis also means that the `org.slf4j.LoggerFactory.getLogger(String)` overload is not really usable (unless you can ensure the set of unique strings passed to this method over the entire lifetime of your application is bounded).\n\nTo work around this problem in logback (and possibly other SLF4J implementations) we cannot use actor paths as logger names. The only viable alternative is falling back to the actor's class name for the logger name, which unfortunately not only reduces configuration flexibility (because you cannot configure logging levels for branches of your actor tree but only for branches of your package structure) but also forces us to move the actor path into the log message itself, thereby (somewhat) decreasing logging performance and wasting space on the log message line.\n\nIt really is a pity that even \"modern\" logging frameworks like logback suffer from such grave and profound architecture flaws... :-1:\n. > perhaps the actor path should be included as part of the MDC\n\nMDCs don't help as they are thread-bound and therefore quite useful in asynchronous architectures like akka's.\n\n> Actor paths are a newer Akka concept so I'm not surprised they don't play nice with Java logging frameworks whereas classnames do.\n\nYes, but I would assume that a logging framework doesn't place any requirements on the semantics of Strings used in its API. SLF4J offers a `org.slf4j.LoggerFactory.getLogger(String)` overload and logback implicitly assumes that the application will use this overload only with a bounded set of strings over the complete application lifecycle. This requirement is not documented anywhere. Why shouldn't I be able to use the string-based lookup for actor paths?\n. Sorry, Taylor, my comment from before was indeed quite confusing. I meant that the basic principle of the MDC is quite _useless_ (not useful) in async architectures. \n\nFrom the [logback manual](http://logback.qos.ch/manual/mdc.html):\n\n> Most real-world distributed systems need to deal with multiple clients simultaneously. In a typical multithreaded implementation of such a system, different threads will handle different clients.\n\nWell, that would be the typical system of 10 years ago. Nowadays this assertion not longer holds, and it certainly doesn't for akka.\n\n> A possible but slightly discouraged approach to differentiate the logging output of one client from another consists of instantiating a new and separate logger for each client. This technique promotes the proliferation of loggers and may increase their management overhead.\n\n\"Slightly discouraged\" ??\nAs we have seen this approach is simply not going to work at all! Otherwise logback will simply eat all available memory and the application will crash eventually.\n\n> The MDC manages contextual information on a per thread basis.\n\nThat is the crux.\n\nIf we controlled the actual call to SLF4J from spray would could put the actor path into the MDC before each logging call and remove it directly afterwards. This would incur some overhead of course but would be feasible. In fact it is what akka itself is doing: https://github.com/akka/akka/blob/c10eadbd3388250425efff0b790f59af6dbdc63d/akka-slf4j/src/main/scala/akka/event/slf4j/Slf4jLogger.scala#L86-L92\n\nHowever, since we don't control the actual call to SLF4J but simply create log events that go out onto akka's eventbus the MDC won't help us, unfortunately.\n. > But woudn't this mean that the feature @taylorleese requested is already implemented by akka?\n> ... [the logsource] should later be available in the MDC as `akkaSource`.\n\nYes! Right. I missed that.\nSo simply setting the logSource string correctly does make it available to the logger and addressable from a message pattern configuration.\n\nThen this indeed appears to be the best solution in my eyes. Thanks for pointing this out!\n. The reason why we have the `log-actor-paths-with-dots` feature so far was that if the actor path was in dotted form you could use logback's:\n- hierarchical logger configuration to address a whole branch of loggers at once\n- abbreviation feature to nicely shorten the logger name to a given number of chars\n\nHowever, since we can't use the actor path as the logger name anymore we loose both of these features. (Unfortunately, logback doesn't support abbreviation on fields generally, only on the logger name and class name field...). So, there is no point in still having the `log-actor-paths-with-dots` feature.\nThe other setting `log-actor-system-name` is not crucial either, so, for me, the whole point of having our own wrapper around akka's `LoggingAdapter` collapses.\n\nAs Taylor rightfully points out: people can simply use akka's `ActorLogging` trait, there is nothing we can really do to improve upon the logging experience you get from that.\n\nSo, I would vote for completely removing these things from _spray-util_:\n- `LoggingContext`\n- `SprayActorLogging`\n- `UtilSettings`\n\nand tell people do simply migrate over to `akka.actor.ActorLogging`.\n\n@jrudolph WDYT?\n. Taylor,\nwe currently don't go through a deprecation step with our API, we'll simply provide a migration guide, which should contain the necessary instructions for replacing `SprayActorLogging` with `ActorLogging`.\n. My original idea was to start publishing a \"1.1-SNAPSHOT\" version of the documentation once we have added enough new docs to make it worthwhile. As Johannes already said, also publishing the more or less equivalent \"1.0-SNAPSHOT\" and \"1.2-SNAPSHOT\" variant is probably not worth the effort...\n. > On the servlet side things are more problematic, to the point where I'd be tempted to leave the feature unimplemented there instead of misleading a user with something that normally works but mysteriously fails under some circumstances.\n\nYes, I agree. Since we cannot do this properly on the _spray-servlet_ side let's just not implement it there at all.\nHowever, I still like the cleanups you have done in the _spray-servlet_ code. Maybe you can leave those in?\n. This is a very nice and clean addition!\nThanks a lot, Andrew!\n\nI really like how you take the opportunity to clean up our codebase if you come across improvement potential!\n. Thanks again, Andrew!\n. Thanks, Cl\u00e9ment, for this patch.\nMeanwhile the discussion on ticket #421 has continued and we have decided to simply rip out all of spray's \"special logic\" for logging, since it doesn't serve any purpose anymore.\n\nWould you agree that this PR is superceeded by https://github.com/spray/spray/pull/452 ?\n. Not in scope any more for Akka HTTP, as we now live in the Akka codebase and adhere to the formatting standards there.\n. Cristian,\nI understand the problem you are trying to solve and you are correct in that the `compact-json-printing` setting that existed in earlier spray versions has now gone away.\nThe reason is that, as @jrudolph already pointed out, there is no good way to provide settings to implicitly created (un)marshallers, except for other implicits. That is why the _spray-httpx_ module currently has no settings infrastructure of its own.\n\nIf you look at this line:\nhttps://github.com/spray/spray/blob/master/spray-httpx/src/main/scala/spray/httpx/SprayJsonSupport.scala#L38\nyou'll see that you can already provide the JSON marshaller constructured by the `SprayJsonSupport` trait with the information of what `JsonPrinter` to use: simply make one available implicitly.\nThis upholds maximum flexibility as you can decide _per call site_ what printer to use.\nTherefore I don't think your proposed patch really is required.\n. See also https://github.com/spray/spray/issues/971\n. Thanks, Age!\n. Thanks, for finding this problem and providing a solution.\nThere are two things that need changing as well:\n- [Here](https://github.com/spray/spray/blob/master/spray-util/src/main/scala/spray/util/package.scala#L51) we should change the name of the method to `requirePositive` and fix the comment.\n- [These two lines](https://github.com/spray/spray/blob/master/spray-servlet/src/test/scala/spray/servlet/ModelConverterSpec.scala#L36-37) need to be updated.\n\nAdditionally we'd need to you to sign [our CLA](http://spray.io/project-info/contributing/#contributor-license-agreement-cla), but then this will be good to merge.\nThanks again!\n. And one more thing: this change is a _breaking_ change, since it changes the semantics of the API.\nFor example, if I created a `spray.can.server.ServerSettings` instance manually before and was using the `Duration.Undefined` for disabling the `request-timeout` I now have to change my code.\n\nSo the commit message should start with a `!` character.\n. Ivan,\nakka uses `context.setReceiveTimeout(Duration.Undefined)` internally for clearing a receive timeout, so I think for consistency with the akka codebase we should stick to this as well.\n\nI'll merge this nice contribution right away, thanks again!\n. Fixed since the 1.0/1.1/1.2 final release\n. The solution to #365 (which is already almost done) will provide even better performance, because spray-can itself will have to perform zero copying of the response entity. (With the current architecture (response entity wrapping a byte array) it still has to copy once.)\n. Thanks, Ian, for this proposal!\nI think it's a valid request for improvement that we should include.\n\nApart from rewriting your commit with the suggested changes could you please:\n- change the commit message to `! can: introduce dedicated exceptions for connection failure and request timeout for host-level API`\n- send us a signed copy of [our CLA](http://spray.io/project-info/contributing/#contributor-license-agreement-cla)\n. > I did a force push to rewrite the commit message and it lost all the previous comments inline with the code. I wasn't sure how to rewrite the commit message without this happening.\n\nYeah, don't worry about that. Normally github provided access to the \"outdated commits\" incl. comments, but maybe this is currently turned off for some reason.\n. Great, thanks, Ian!\nI'll wait for @jrudolph to sign off on this and then we'll merge...\n. Thanks, Lee, for this patch.\nWe just discussed this issue and would like to follow your lead and, with regard to the _spray-testkit_ module, make an exception to our rule to only depend on Akka with `provided` scope.\n\nHowever, in order to be able to merge your patch, we'd need you to\n- sign [our CLA](http://spray.io/project-info/contributing/#contributor-license-agreement-cla)\n- and change the commit message to `= sbt: change spray-testkit dependency on akka-testkit from 'provided' to 'compile'` in accordance with our [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages)\n\nAlternatively (to make things easier for you) we could also apply the patch ourselves. Let us know what you'd prefer.\nThanks again for bringing this up!\n. This is not a spray problem, but results from akka's different default handling of dead letters.\nAlso check this ML thread: https://groups.google.com/d/msg/spray-user/iakr7aRS4AI/3smE1Tg20koJ\n. Thanks, Andr\u00e9, for this patch!\nCould you do two things?:\n- Move the definition of the two new header parser rules to the `SimpleHeaders` trait? (All \"simple\" headers that only require a single parser rule live in that file.)\n- Reduce the tests for each new header type to a single test case (As you already said, these new headers exactly mirror the `Authorization` and `WWW-Authenticate` headers. I therefore would like to keep unecessary bloat in the already quite big header spec as small as possible.)\n\nThanks again!\n. > ProxyAuthenticateHeader extends WwwAuthenticateHeader and ProxyAuthorizationHeader extends AuthorizationHeader.\n\nThat would simplify the implementation but would have to ugly effect that `request.header[AuthorizationHeader]` might end up returning a `ProxyAuthorizationHeader`. We could introduce an abstract common super type though.\n. Nice!\nThanks a lot again, Andr\u00e9!\n. Ok, added cache synching and SprayActorLogging deprecation.\n. I saw that we are recreating a `UtilSettings` instance everytime a `LoggingContext` was created and that a small cache would likely provide a perf benefit here.\nEven though the `UtilSettings` are now gone, we have the same situation, e.g. with the `ClientConnectionSettings`, for example here: https://github.com/spray/spray/blob/master/spray-can/src/main/scala/spray/can/Http.scala#L67\n. Just published a fresh nightly with today's date.\n. Yes, thanks for reporting!\n. Thanks, Andrew!\n. Very nice!\n. [RFC 2616 Section 14.43](http://tools.ietf.org/html/rfc2616#section-14.43) defines the syntax for the `User-Agent` header as:\n\n```\nUser-Agent     = \"User-Agent\" \":\" 1*( product | comment )\n```\n\nwhereby\n\n```\nproduct = token [\"/\" product-version]\nproduct-version = token\ntoken = 1*<any CHAR except CTLs or separators>\nseparators = \"(\" | \")\" | \"<\" | \">\" | \"@\"\n                  | \",\" | \";\" | \":\" | \"\\\" | <\">\n                  | \"/\" | \"[\" | \"]\" | \"?\" | \"=\"\n                  | \"{\" | \"}\" | SP | HT\nCTL = <any US-ASCII control character (octets 0 - 31) and DEL (127)>\ncomment = \"(\" *( ctext | quoted-pair | comment ) \")\"\nctext = <any TEXT excluding \"(\" and \")\">\nTEXT = <any OCTET except CTLs, but including LWS>\nquoted-pair = \"\\\" CHAR\nCHAR = <any US-ASCII character (octets 0 - 127)>\n```\n\nSo `@` characters are only allows in comments.\nClients producing `User-Agent` headers that do not conform to this syntax are not following the HTTP spec.\nWe only depart from the spec in rare and well-founded cases.\n. Suggestion for solving this properly:\n\nWe introduce a special empty string value\n\n```\nobject Query {\n  val EmptyValue = new String(Array.empty[Char])\n}\n```\n\nand then parse/render like this:\n- `foo=bar`: key = \"foo\" and value = \"bar\"\n- `foo=`: key = \"foo\" and value = \"\"\n- `foo`: key = \"foo\" and value = Query.EmptyValue\n- `=foo`: key = \"\" and value = \"foo\"\n- `=`: key = \"\" and value = \"\"\n- \"\": key = \"\" and value = Query.EmptyValue\n. Closing for now. We can resurrect if necessary.\n. IMHO we shouldn't try to be compatible with every buggy HTTP implementation there is.\nFor the particular case that Derek showed I think we can add an exception, which doesn't really hurt us:\nIf we see a second `Content-Type` header and this second one is identical to the first one then we simply issue a warning and drop it but continue parsing.\nI think we should _never_ accept a message with two different `Content-Type` headers.\n. Stanislav,\nyou can already add a custom port to `DefaultHostInfo`. What else would you need?\n. also see [this thread on the akka-dev ML](https://groups.google.com/forum/#!topic/akka-dev/NXDklScAVbo)\n. I don't think this ticket will still be closed in the scope of the spray project.\nakka-http is based on reactive streams, which will solve the backpressure problem correctly once and for all. The current spray infrastructure on the contrary doesn't really allow for an easy \"proper\" fix, unfortunately.\n. akka-http-core will be ready quite shortly.\nIf you need to the full high-level server-side DSL then this is still some time out.\nCurrent estimate for availability would be sometime in fall this year.\n. We will have something that is stable enough to put weight on, yes.\nAnd since akka-stream is a core dependency of akka-http my estimate does include akka-stream, yes.\n. @danarmak Check out the talk about _akka-http_ from last week's ScalaDays for more info:\nhttp://spray.io/scaladays2014/#/\n. Fixed in Akka HTTP with the new streamified chunking model.\n. Closed by https://github.com/spray/spray/pull/480\n. This builds on https://github.com/spray/spray/issues/635.\n. > ... since it will count expired keys too ...\n\nYes. I wouldn't consider it a problem though if we clearly indicate that the `size` method returns an _upper bound_ on the number of cached elements. If you add the respective clarifications to the scaladoc (also explaining why its only an upper bound) then I think we are good to merge.\n. Perfect!\nAnd one more \"thank you\" going up to the Netherlands...\n. And another one closed.\nThanks, Age! You are quickly rising to the top of our committer list... :)\n. Ok, we have just discussed this.\nCould you change the `MultipartFormData` model to this?:\n\n```\ncase class MultipartFormData(parts: Seq[BodyPart]) extends HttpForm {\n  type FieldType = BodyPart\n  def get(partName: String): Option[BodyPart] = ...\n  def getOrElse(partName: String, default: => BodyPart): BodyPart = ...\n}\n```\n\nAlso, I think we can improve the `BodyPart` model to this:\n\n```\ncase class BodyPart(entity: HttpEntity, headers: Seq[HttpHeader]) {\n  def getNameOrEmpty: String = ...\n  def getName: Option[String] = ...\n}    \nobject BodyPart {\n  def apply(entity: HttpEntity, headers: HttpHeader*): BodyPart =\n    apply(entity, headers: _*)\n}\n```\n\nWDYT?\n. Dirk, we'd also need you to sign our [CLA](http://spray.io/project-info/contributing/#contributor-license-agreement-cla) before we can merge.\nThanks for contributing!\n. What exactly would you like to enforce?\n. @dlouwers Yes, sounds good! This will indeed be an improvement.\n. With the current state of this patch the `SslTlsSupport` pipeline stage unconditionally creates and sends an `SSLSessionEstablished` instance down the event pipeline, even if the respective setting is actually configured to `off`. I'd vote for configuring the `SslTlsSupport` stage with the setting along these lines:\n\n```\nobject SslTlsSupport {\n  def apply(publishSslSessionInfo: Boolean) = new OptionalPipelineStage[SslTlsContext] {\n    ...\n  }\n}\n```\n\nNot only will this save allocations and messages in case the header is disabled, it will also prevent higher pipeline stages from seeing messages they don't necessarily expect.\n. This is another great contribution from you, Michael!\nThanks once more!\n. I agree with Michael in that the changes are technically breaking changes to the public API, even though most users probably wouldn't be impacted.\n. Note that we won't do this before shapeless 2.0 final is out.\n. Currently it looks like we won't do the upgrade to spray.\nAkka-http will contain shapeless 2.0 compatible HLists though.\n. Ok, we just discussed that we will publish shapeless 2.0.0 compatible builds of spray 1.3.1 for Scala 2.10 as well as 2.11 in the next days.\nThat should make things a bit easier for you guys.\n\nI'll report back on this ticket (and close) when this has happened.\n. Done.\nSee https://groups.google.com/d/msg/spray-user/rxfjsg9OsUA/nytYALgIOn8J\n. Yes, I guess grouping the timeouts would be the easiest fix.\nIn a later stage we could also fold all booleans in a bit mask and provide a custom constructor that still supports all \"spelled out\" settings.\n. This is a very nice addition!\nThank you very much again, Andr\u00e9!\nIf @jrudolph agrees I think this should be merged ASAP.\n. Great, thanks Andr\u00e9!\nThis is a very nice contribution!!\nThe only thing missing for me now is the feedback from @jrudolph and the squash...\n. Andr\u00e9,\nI like the default headers addition.\nAlso, I think this patch is fine with regard to the `CloseAll` command support.\nSo, apart form the small nagging this looks ready for merge!\nThanks again!\n. Ok, Johannes and I just discussed the `DemandIdleShutdown` protocol.\nThere are a few changes that need to be put it, however, it's probably easiest if we do this ourselves after this PR has been merged.\nSo, Andr\u00e9, let me know when you are ready and I'll press the green button.\n. Excellent!\nThanks you _very_ much again, Andr\u00e9!\n(Also for fighting through this lengthy review process... :) )\n. Andr\u00e9,\nactually we need to somewhat increase our focus if we want to keep our internal goals as to the release timeline.\nSo, I think it'd be better if we postpone TLS support until after the 1.0/1.1/1.2 finals are out.\n. > BTW: Is this list up to date?\n\nYes, it's the todo list that is currently very much in the center of our attention.\nThe goal is to burn it down until the end of the month.\n. Nice work, Ferdinand, thanks for the contribution!\nApart from the small comments I think the only thing missing is the respective patch on the _spray-can_ client rendering side...\n. Ferdinand,\nas Johannes pointed out my comment was targeted at request _rendering_ only.\nFor parsing you should never produce `ContentTypes.NoContentType` references, since we simply use the default `application/octet-stream` media type if no `Content-Type` header is present.\n. Thanks, Ferdinand, nice fix!\n. Nice fix, Balaji, thanks a lot!\nIf you change the commit message to\n\n```\n= http: fix Location header parsing to accept relative URIs, closes #484 \n```\n\nthen github will automatically close the ticket when we merge this PR.\n. Thanks again, Balaji!\n. Michael,\nwe've just discussed this and would actually prefer the not reuse the \"real\" tests for the documentation.\nThe real tests are often times deeper and structured differently from what we'd like the documentation to show.\nAlso, the focus of the documentation lies more on illustrating usage rather than proving correctness.\n\nSo, could you create a new example spec underneath `/docs` for the marshalling directives?\n. No worries at all, Age.\nHow to best structure the tests and the example and so on is something that is slowly evolving over time and we ourselves have not been entirely clear on. So it's actually a good thing to have impulses come in from the outside that make us question our own approaches and force us to make decisions.\n. Great!\nThanks, Mike!\n. @mhamrah Actually, I just realized that we don't have a signed [CLA](http://spray.io/project-info/contributing/#contributor-license-agreement-cla) from you so far. Would you be able to provide me with one?\n. @guersam Can't you simply \"patch\" the missing content-type / charset header in your use case?\n. I see.\nThinking about it again I think it makes sense to change\nhttps://github.com/spray/spray/blob/master/spray-httpx/src/main/scala/spray/httpx/unmarshalling/MultipartUnmarshallers.scala#L47\nto default to UTF-8 instead of US-ASCII.\n\nIf @jrudolph agrees I'll push a small patch.\n\nThanks for bringing this up!\n. Yes, we could.\nHowever, I'd assume the default charset to be used in the absence of a part's `Content-Type` header is not something that people will really want to configure. The spec kind of dictates the default to be US-ASCII, but seeing the age of the spec I'd assume that they'd pick UTF-8 if it was written today.\nUTF-8 being US-ASCII compatible we don't really break anything.\n\nHow about we do the small fix and now and postpone the bigger change (with the configurable default charset) until someone needs it?\n. Ok, done: https://github.com/spray/spray/commit/538b2c59744f41e2e85853baa6fccb13fc0c7e92\n. Rather than opening up `SimpleLruCache` and/or `ExpiringLruCache` for user extension (and thereby exposing implementation details as public API), how about we simply add an eviction hook feature to the `Cache` trait itself?\n. Yes, that's not bad.\nIf you save the default `onEvict` instance in a public field (e.g. `Cache.EmptyOnEvict`) you can check whether the user actually set a custom `onEvict` handler and only create and attach an `EvictionListener` if really required.\n. Nice work, Douglas, thanks!\nApart from the comments, could you rewrite your commit messages to conform with our [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages) and [sign our CLA](http://spray.io/project-info/contributing/#contributor-license-agreement-cla)?\n. Ok, we'd still need your CLA and a squashing of the commits incl. a rewrite of the commit message according to our policy... Thanks!\n. Yes, thanks, Mathias!\n. Thx!\n. Great! Thanks for this important fix!\n. Ok, we just discussed the idea of better modeling deadlines.\nCurrent plan: extend this PR with a custom `spray.util.Deadline` along these lines:\n\n```\nclass Deadline(val nanos: Long) extends AnyVal\n```\n\nUnfortunately the `scala.concurrent.Deadline` suffers from three deficiencies that make it unattractive:\n1. It's not a value class.\n2. It unnecessarily wraps a `FiniteDuration` rather than a raw `Long` (which is also semantically wrong).\n3. It doesn't offer a \"no deadline\" facility.\n. Apart from the small thing in `SimpleStash` this looks good to me.\n. Thanks, Stig!\nI'd like to merge this fix but can do so only after you have signed our [CLA](http://spray.io/project-info/contributing/#contributor-license-agreement-cla). Could you do that?\n. Thanks, Stig!\n. See also this thread: https://groups.google.com/forum/#!topic/spray-user/0reUlIzU5FI\n. Cool!\n. Thanks, Chip!\n. Addressing @jrudolph's point about making the API a bit more explicitly with regard to port normalization etc.:\nI'd vote for adding this additional method to the `Uri` (above the `copy` method):\n\n```\n/**\n * The effective port of this Uri given the currently set authority and scheme values.\n * If the authority has an explicitly set port (i.e. a non-zero port value) then this port\n * is the effective port. Otherwise the default port for the current scheme is returned.\n */\ndef effectivePort: Int = if (authority.port != 0) authority.port else defaultPorts(scheme)\n```\n\nThen we can also remove this line:\nhttps://github.com/spray/spray/blob/f4a92f0c0d33ff2405339cdc9da70afd83ae4794/spray-can/src/main/scala/spray/can/HttpManager.scala#L46\n. Excellent, thanks, Ivan!\n. Thanks, Ian!\nApart from the small fix this looks ready for merge.\n. Thanks, Ian!\n. This problem appears to be fixed as a side effect of this commit 7e5c494, as least I can't reproduce the original posters problem.\n@jrudolph Can we simply close this then?\n. Should we maybe add a directive for extracting the `RequestContext` instead?\n(Which would basically be nothing more than an alias for `extract(_)` ...)\n. Great!\nThanks, Ivan!\n. Njal,\nthanks for this report and the nice analysis.\nAs you suspect a `Content-Type` header containing a `*` is not valid HTTP and as such _spray-can_ is doing the right thing by replying with `400 Bad Request`. However, the response entity is definitely not intended and needs to be fixed.\n. I'm not sure that this would be a good idea.\nA client sending a `Content-Type: application/*` header is not speaking HTTP. It's not a syntactic problem that we could be lenient with, it's a grave semantic issue that, IMHO, clearly deserves a `400 Bad Request` response.\nHow would you expect a server (or your application) to interpret a `Content-Type: application/*` header?\n. I see.\nTheoretically spray _could_ just ignore the bad `Content-Type` header, parse it as a `RawHeader` and treat the request as if it didn't contain a `Content-Type` header at all. We'll discuss the best approach.\nThanks for making your use case so clear!\n. From http://tools.ietf.org/html/draft-ietf-httpbis-p2-semantics-22#section-4.3.6:\n\n```\nA client sending a CONNECT request MUST send the authority form of\nrequest-target (Section 5.3 of [Part1]); i.e., the request-target\nconsists of only the host name and port number of the tunnel\ndestination, separated by a colon.  For example,\n\n  CONNECT server.example.com:80 HTTP/1.1\n  Host: server.example.com:80\n```\n\nTheoretically we could therefore add logic verifying that a CONNECT request doesn't come with an URI containing a path, query, etc.\nHowever, I'd vote for moving this check into a higher layer so as to not slow down spray-can more than necessary.\n. `HttpIp` is gone by now, replaced by the new `RemoteAddress.IP` model.\n. Could you rebase this on top of the current master (as it doesn't merge cleanly anymore).\nAnd we are still missing your CLA...\n\n(Btw.: No need to close the PR when you want to update it, simply `push -f` into your branch to update the PR)\n. Closing for now.\n\n@drapp Please reopen when you are ready...\n. Just came across these two lines:\nhttps://github.com/spray/spray/blob/master/spray-http/src/main/scala/spray/http/HttpMessage.scala#L272\nhttps://github.com/spray/spray/blob/master/spray-http/src/main/scala/spray/http/HttpMessage.scala#L292\nwhich also need to be fixed in the context of this ticket!\n. Aaron,\nthanks for this PR!\nHowever, the fix to #543 was somewhat more involved than originally expected, which is why we have attacked this ourselves: https://github.com/spray/spray/pull/556 should supercede this PR.\nWould you be ok with simply closing this one?\n. Maybe someone on Windows can send us a PR for this?\n. Thanks, Daniel, for finding and fixing this problem!\nCould you please change the commit message to this:\n\n```\n= routing: fix directory listings under Windows, closes #549\n```\n. Great, thanks, Daniel!\n. Thanks, I'll have to postpone this to next week...\n. Ian,\ncould you rebase on top of the current master?\nI'd like to get this patch in ASAP, so we can start cutting RC1.\n. Very nicely solved, Ian!\nThanks a lot for this great contribution!\n. Yes, agreed, spray-can should be able to deal with custom status codes.\nThanks for bringing this up!\n. Adding a header is already supported with the `addHeader(s)` members of the `RequestBuilding` trait/object in spray-httpx. Which ones exactly would you like to add?\n. We agree with your that the client-side DSL is not yet where it needs to be.\nImprovements in that area will come.\nHowever, they'll probably be more profound that simple addition of more predefined pipeline elements.\n\n> is spray already use scalaz ? is this something that can arise ?\n\nOf course you can use scalaz with spray if you want. However, we will not add a dependency on scalaz ourselves.\n. On the server side we are good.\nOn the client the higher-level spray-can API levels (host- and request-level API) this is not yet fully available.\n. Fixed in Akka HTTP by having moved to Reactive Streams for back-pressure.\n. Our interpretation of the spec is that the URI is opaque with regard to addressing a resource.\nThis means that `/foo` and `/foo?key=value` are _two different_ resources!\nTherefore we issue a `404` if a required parameter is not found.\n. No problem. Thanks for bringing this up anyway, Mike!\n. > Which kinds of subclasses of StatusCode should be created for codes outside the 100-599 range? HttpSuccess or HttpFailure? Or a new one which would make pattern matching more awkward.\n\nI'd vote for creating a separate subclass that is neither an `HttpSuccess` nor an `HttpFailure`. I should be able to specify at registration what values I want `isSuccess` and `isFailure` to return. `allowsEntity` should always be `true` for custom `StatusCode`s.\n\nSince we define `isSuccess` and `isFailure` as methods on the `StatusCode` base class, maybe we should remove `HttpStatus` and `HttpFailure` from the public API altogether?\n\n> Should int2StatusCode be allowed for creating custom status codes? (I chose 'no', because in many cases it is more likely a programmer error)\n\nYes, I'd agree with your decision.\n. Thanks, Greg, for reporting this.\nWe'll look into it.\n. Haoyi,\nok, we can merge this patch to make it easier for your to keep your websocket implementation up to date.\nCan you send us a signed [CLA](http://spray.io/project-info/contributing/#contributor-license-agreement-cla) and reformat the commit message according to [our policy](http://spray.io/project-info/contributing/#git-commit-messages)?\n. Haoyi, thanks for the CLA.\nHowever, your patch doesn't change the public API at all (since all the new method introductions are purely internal), so the commit message should be something like:\n\n```\n= can: improve HttpManager extensibility by moving child actor creations into dedicated methods\n```\n. Nice!\n. Nice!\nIsn't this a `+` change rather than a `!` change though?\n. Ok.\n. I added a separate ticket for the missing tests: https://github.com/spray/spray/issues/603\n. Fixed in Akka HTTP with the new TCP IO API.\n. Ok, feedback incorporated.\n. Yes, good idea. Will do.\n. Stig,\nthanks for this contribution.\nWe won't have any time this week to review and merge this patch though, so it won't make it into the RC1 release which we'll publish this week.\nHowever, maybe we can incorporate it into a later RC if one should be required.\n. Stig, thanks again for this contribution.\nWe have reserved some capacity this week to push for a minor update release.\nOf course we'd like to incorporate the latest and greatest in contributions! :)\nSo, since this patch here looks quite valuable, if you have time to look through our comments we'd like to get it merged.\n. To make things easy for you we can apply the missing bits of the parser definition ourselves if you supply us with a number of (potentially failing) test cases that the implementation needs to pass if correct.\n. No worries, Stig!\nIf you squash all your commits (except the merge from master) into a single commit with this commit message:\n\n```\n+ http: Add model for `Link` header\n```\n\nthen we can take it from there!\nThanks for this very nice addition!\n. Daniel,\nthanks for this contribution.\nWe won't have any time this week to review and merge this patch though, so it won't make it into the RC1 release which we'll publish this week.\nHowever, maybe we can incorporate it into a later RC if one should be required.\n. This is a very nice PR, Daniel, thanks a lot!\n\nWe have reserved some capacity this week to push for a minor update release.\nOf course we'd like to incorporate the latest and greatest in contributions! :)\nSo, if you have the time to look through my comments I think this patch would be a great addition!\n. Cool, thanks!\n. Daniel,\nI might have a few more improvements suggestions, would you want to apply these as well or would you rather we merge now and take it from here?\n\nIn any case, thanks a lot again for this very nice contribution!\n. Ok, I have added three commits on top of yours (thanks for letting me push directly into your fork!).\nThey apply some refactorings that will make it easier for us to migrate this new code into the coming akka-http (which has slightly different code conventions here and there). I have also added a few more tests, renamings, etc. The main structure of your solution however is exactly the same as it very well fits the \"spray way\".\n\nIt be great if you and @jrudolph would have another quick look over the PR as it stands right now so that we can be reasonably sure that it doesn't break things in a bad way.\n\nI have to say again that this contribution is one of the best that we have received so far! It shows that you have spent considerable time in the codebase to make sure that your patch fits in.\nVery nice work! We definitely owe you a beer! :)\n. Ok, thanks, Johannes.\n@danielwegener What do you think?\n\nI just realized that we probably should cross-check this implementation against the latest HTTPbis spec [here](http://tools.ietf.org/html/draft-ietf-httpbis-p5-range-26) and make sure that we are not implementing obsolete logic.\n. Excellent!\nGreat that you were already on HTTPbis anyway.\n:+1:\n\n(Btw.: Will you be in Berlin in June for #scaladays?)\n. Excellent! Thanks, Daniel, for verifying against the latest HTTPbis draft!\nI've added a comment to show where we (rightfully) depart from the spec by leaving out the `other-ranges-specifier` from the parser rule.\n\n> (I really hope i can come to Berlin, but I am already visiting the #reactconf next month - so since I dont have my own talks, I think I have to take a vacation for #scaladays ;) )\n\nWell, your employer really should send you there and pay your ticket! :)\nAnyway, we'll happily buy you a beer!\n. Absolutely! If you are there the beer is yours! :)\nLooking forward to it!\n. Thanks, Michael!\n. Nice, thanks, Ivan!\n. I added a ticket for the optional bounds: https://github.com/spray/spray/issues/660\n. Jeff, the problem is that we cannot relax/adapt the parser to all buggy HTTP implementations out there.\n`iso-8859-*` is not a valid charset and spray cannot decode the message content if the charset is unknown.\n\nWe've been talking about a feature that makes it easy to \"inject\" a custom actor layer between spray-can and akka-io, which would allow you to patch the incoming request bytes according to your needs. I have just created a ticket for that: https://github.com/spray/spray/issues/648\n. Yes, that's what @jrudolph's fix is doing.\n. Ok, IP parsing updated to non-resolving `InetAddress` creation.\n. Nice solution, much better than before!\n. I think that is as far we can push this fix.\n. This is solved in Akka HTTP by having the HTTP stack be completely decoupleable from the underlying TCP stack. Injecting custom logic in between in not a problem anymore.\n. Serving the same content under two different URIs (the \"with-trailing-slash\" URIs is _not_ the same as its \"without-trailing-slash\" sibling) is not really recommended. See for example: http://googlewebmastercentral.blogspot.de/2010/04/to-slash-or-not-to-slash.html\n\nThe best solution is usually to clearly decide for one or the other and then be consistent about it. If you want to allow both variants you should add a permanent redirect from the less favored variant to the \"officially selected\".\nIn order to gently push users to actually make this conscious decision we'd like the unambiguous directive to be the short one and make the less recommended one slightly less appealing.\n. Yes, with the current DSL you can easily build whatever behavior you require.\nPreviously it was harder to do \"the right thing\" (e.g. redirect non-trailing-slash siblings), which is why we made this change.\n. > Am I missing something or did this just revert the change that broke `path(\"\")` in 1.2-RC1 (from 1.2-M8)?\n\nNo, it doesn't revert to M8 behavior. The reason is that in M8 trailing slashes were always matched implicitly and it was hard to prevent that. After this patch there is no more ambiguity in any path matching directive except for `pathEndOrSingleSlash`, which matches two things.\n\n> It does seem that `pathEnd` is the more \"correct\" solution that is suggested?\n\n`pathEnd` matches exactly what it says and nothing else.\nIf you are upgrading this construct from M8\n\n```\npathPrefix(\"foo\") {\n  path(\"\") {\n    ...\n  } ~ ...\n}\n```\n\nyou now need to decide whether you want\n\n```\npathPrefix(\"foo\") {\n  pathEnd {\n    ...\n  } ~ ...\n}\n```\n\nor\n\n```\npathPrefix(\"foo\") {\n  pathEndOrSingleSlash {\n    ...\n  } ~ ...\n}\n```\n\nand we want to force you to make this conscious decision.\n. Yes, I think we should actually implement support for the `Raw-Request-URI` header on the client side.\nOn the server-side this header allows access to the raw request URI as it was parsed from the raw request bytes.\nOn the client-side we could use this header to allow for overriding the request URI with a user-defined String.\nSo, if a request contains a `Raw-Request-URI` header we render its value rather than the `uri` member of the `HttpRequest` instance.\n. Actually, spray-servlet currently _never_ produces a chunked request when converting from the `HttpServletRequest`. A prerequisite for implementing the `RegisterChunkHandler` protocol is therefore the implementation of incoming auto-chunking (i.e. the `incoming-auto-chunking-threshold-size`) as we already have it in spray-can.\nHowever, at this stage in our release process I don't see this as a necessary addition.\nHow about we therfore reassign this ticket to the `1.3` milestone?\n. Ok, closing for now.\nPlease reopen if you think the issue is still valid.\n. @jiaweihli If you are not using the `ActorLogging` trait, how are you logging?\nDo you think the problems you saw are somehow related to spray?\nIf so, could you distill a stand-alone test case and open a fresh ticket?\n. Also see this related thread on the ML:\nhttps://groups.google.com/forum/#!searchin/spray-user/Excessive/spray-user/qX44Zr8Ea-w/I71j_IGONn8J\n. Mark, thanks for taking this on.\n\nYour solution of simply requiring Windows users to not checkout the sources using the line separator of their platform would work, but I'm not sure if this is the preferred mode for a Windows users.\nI'd think that most Windows users have git `core.autocrlf` configured to `true` rather than `false`, which would cause their working copy to have CRLF line endings.\n\nWe _could_ require Windows users to configure `core.autocrlf` to `false`. However, this would then require _all_ editors that a Window user might use to respect the line ending convention of the repository, which I doubt it feasible.\n\nSo, rather than requiring people on Windows to work with unix-style working copies I'd say that we should fix all tests that do not yet properly deal with windows style line endings, no?\n. > I think think that the group that has core.autocrlf set to false on Windows is quite large, since it is asked during the installation of Git whether you want this. So both true and false are common. So just assuming it is in Windows style causes problems.\n\nOk, if it's true that there really are a lot of people on Windows that do work with unix-style line endings we cannot rely on `System.getProperty(\"line.separator\")` at all.\nOne more question: How do these people instruct their editors to not produce Windows-style line endings?\n. Thanks for reporting!\n. Ok, thanks for reporting!\n. FTR: there is already a nightly out with the fix Johannes referred to: http://nightlies.spray.io/io/spray/spray-can/1.1-20131101/\n. Yes, thanks for reporting!\n. Great, thanks Mark!\n. Not relevant for Akka HTTP any more.\n. Thanks for the feedback, Michael!\nWhen you say \"beginner\", do you mean beginner with Scala or beginner with Akka or beginner with spray?\n\n[This page](http://spray.io/documentation/1.1-SNAPSHOT/spray-can/http-client/request-level/) in our docs appears to show the most basic use of spray's client-side, but you might disagree...\nWhat's missing?\n\nIf you don't care about the difference between the \"connection-level API\" and the \"host-level API\" then you using _spray_ might not actually be the best choice, since you only have very basic client-side HTTP needs. Maybe using [AsyncHttpClient](https://github.com/AsyncHttpClient/async-http-client) or even one of the many synchronous HTTP client solutions on the JVM might be more appropriate?\n. > How do I POST something?\n\nOk, extending the example with something that shows how to marshall something into the entity of a POST request is certainly good. Thanks!\n\n> What is the ? operator?\n\nThis `?` operator has nothing to do with spray, it's Akka's \"ask\" operator and therefore not in scope for our docs.\n\n> How do I access response status, headers and body?\n\nThe example shows you how to get hold of an `HttpResponse` instance. We assume that people would expect the status, headers, body, etc. to be members of this instance (which they are).\nHowever, we should add a sentence mentioning this, thanks!\n. Thanks, @crypticmind!\nI've just created a ticket for tracking this feature: https://github.com/spray/spray/issues/709\n. @michaelklishin While this ticket is awaiting completion: Have you tried [Dispatch](http://dispatch.databinder.net/Dispatch.html)? It might be better suited to your needs with regard to documentation.\nAlso: we have a ML that you can use if you feel that the spray docs are not fit for your particular use case.\n. Nice!\n. Updated to new client-side request timeout logic as discussed.\n. Actually, enabling a direct `ByteString => HttpRequest` conversion as user-accessible API is harder than simply removing access restrictions from a few classes.\nThe current spray-can architecture would require adding a proper API for this.\nHowever, due to capacity limitations this currently out of scope.\n\nI am therefore closing as \"Won't fix\".\n. Just FTR: The request parser in akka-http should be a bit easier to use.\nSo, for new projects I'd recommend to look there.\n. Ok, significantly updated.\nPlease re-review...\n. Right.\nThanks, Andr\u00e9!\n. I just merged this fix into all three release branches, so tomorrow we should have a new nightly available containing this fix.\n. No worries, Michael!\nGlad the issue is resolved...\n. Nice fix!\n. And another thing I just spotted:\nWhile you are at it: Could you add a commit removing the `bytesLeft` method and replacing all calls with a simple `buffer.hasRemaining`?\nI have no idea why I introduced `bytesLeft` in the first place... (probably wasn't aware of `hasRemaining`'s existence).\n. Great!\n. Actually I believe in 1.2 (i.e. Akka 2.2.x) this is not a problem since akka-io in 2.2.x uses a special supervisor strategy that should prevent these log messages:\nhttps://github.com/akka/akka/blob/ce9fd56db43176562aca3eb0930aa44b94349929/akka-actor/src/main/scala/akka/io/SelectionHandler.scala#L95\n\nIn earlier Akka version this new supervisor strategy doesn't exist yet which is why we have to handle `Terminated` events \"manually\" in spray 1.0 and 1.1.\n. So, I guess we don't actually need the akka ticket #3728, but it'd probably be good to confirm that spray 1.2-RC3 doesn't show the respective error log entries.\n. Nice one...\n. Great!\n. @matanster I really like the idea that you voiced over email, namely adding hyperlinks to the respective directive doc pages to all code samples! This will be great! Thanks!\n. Asko has a good suggestion on the ML for this (https://groups.google.com/d/msg/spray-user/vBjKfxF7LhY/-MahLQECimgJ), let's also apply it to the scope of this ticket!\n. Ok, I changed the title of this ticket to clarify the feature request.\nEssence: the default value for the `spray.servlet.root-path` setting should become `AUTO`, which would mean that spray-servlet will call `ServletContext::getContextPath` in `contextInitialized` to set the effective root-path automatically.\n. Building upon Johannes' point I'd say that spray should definitely _not_ accept backslash characters in cookies since the latest cookie spec explicitly forbids them. The recommended approach would be to simply base64 encode your value.\n. Yes, Tim, you are right in that spray can be more user-friendly in that regard.\nI just created https://github.com/spray/spray/issues/737 for it.\nThanks!\n. Off the top of my head wire-logging might be best implemented as an additional stage of the spray-can client and server pipelines...\n. Given the more flexible stream setup infrastructure in Akka one could argue that a \"wire logging\" feature could be implemented outside of Akka HTTP, as a plain and simple \"log what is going through the stream\" stage.\nHowever, similar to [the wirelogging feature provided by the Apache HTTP client](http://hc.apache.org/httpclient-3.x/logging.html) there is value in having a logging stage that has a concept of HTTP.\nFor example it should be possible to restrict logging to only the message headers, i.e. suppress the message entity.\n. Related: https://github.com/spray/spray/issues/756\n. Yes, we should the following parameters to the implicit param list of `startServer`:\n- `eh: ExceptionHandler`\n- `rh: RejectionHandler`\n- `rs: RoutingSettings`\n. Ok, thanks, Andr\u00e9!\nI don't really have a problem with the fix in this patch for the time being.\nBut I agree with @jrudolph that a \"proper\" solution would be preferable...\nHow about we add a ticket \"Clarify error message localization\" and merge this PR?\n. Thanks, LGTM.\n. Nice, Andr\u00e9!\nLGTM\n. Andr\u00e9, can you rebase on top of the current master, so this merges cleanly?\n. Great! Thanks, Andr\u00e9!\n. Thanks, Andr\u00e9, for pushing this forward.\nThe problem with relying on `System.getProperty(\"line.separator\")` in any way is that we cannot be sure that what the user has checked out into his/her working copy really corresponds to the the platforms line separator or not.\nSo the third point in your three point bullet list above is the crux.\n\nHowever, we should be able to work around that by changing all tests to accept **both** types of line separators.\nIf we normalize all multi-line strings with something like `replace(\"\\r\\n\", \"\\n\").replace(\"\\n\", \"\\r\\n\")` we can be sure that we'll always see windows style line seps.\nThen we can remove all references to `spray.util.EOL` completely.\nWDYT?\n. Thanks, Andr\u00e9!\n\n> stripMargin is only used for declaring multipart content (parsed by mimepull), json samples and config samples.\n> Should I leave them as they are?\n\nYeah, since these types of things should be EOL agnostic themselves I think we should be fine as is.\n\nIf @jrudolph agrees I'm ready to merge this patch then...\n. I agree with Johannes that we cannot simply and blindly all cookies to another host!\nSo I think this patch simply exchanges one small problem against a big one.\nTherefore closing for now...\n. Hm... I'm probably somewhat slower today, but I'm not seeing what this PR does, i.e. buys us.\nCan you explain once more what the problem is that you are solving with this?\n. Ah, ok, I get it now.\nYes, the model is not perfect currently in this regard.\nLet us think a bit about how to solve it best.\n(We also have to watch out for binary compatibility with all spray patches since the final...)\n. @jrudolph Yes, sounds good!\n. Apart from the small nitpick this looks like a nice patch!\nThanks, Flavio!\n. It currently shouldn't happen, however it's \"good style\" to always write _robust_ code in the sense that it should also work if the environment around it changes (within reasonable bounds), e.g. in the case of refactoring.\nIn this case here `input.length < offset` might become a valid state at some point, which might cause a harder to find error if we keep the `==` comparison as is.\n\nAnd secondly: The current comparison immediately raises my alarm bells when reading the code and I have to actively think about whether `input.length < offset` is a valid state or not. Changing the comparison to `input.length <= offset` takes care of this (i.e. looks right), expresses the local condition better and is generally more robust.\n. Ok, thanks Flavio for the analysis and submitting this patch!\n. Thanks a lot for this patch, Mark! (And sorry for only getting to it now).\nWe have reserved some capacity this week to push for a minor update release.\nOf course we'd like to incorporate the latest and greatest in contributions, which included this patch!\n\nCould you accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) so we can get this merged ASAP?\n. Would you like us to incorporate the comments from yesterdays's review or do you want to do them?\n. Great! Thanks!\n. Great! Thanks, Mark!\n. Apart from the small comment I think this is a very nice addition, Pablo! Thanks!\nIf @jrudolph agrees then I'd like to get this merged very soon.\n(We have reserved some capacity this week to push for a minor update release.\nOf course we'd like to incorporate the latest and greatest in contributions! :)\n\n@fernandezpablo85 Could you squash all your commits into one bearing this message:\n\n```\n+ can: add support for multi-host `Connect`\n```\n\n(in accordance with out [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages))\n. Yes, @2beaucoup is right (once more).\nApart from proving correctness some basic tests would also help us in making sure that your addition remains functional even as we shift stuff around in the future....\n. Once easy test would be to bind to three ports on localhost of which only the third actually accepts connections. Then you could `Connect` to all three and verify whether you get connection attempts on all three, with only the last one actually succeeding.\n. This PR has been sitting around for far too long, given the value that it provides.\nHowever, as @jrudolph already said before, we won't be merging it into spray anymore.\nRather, we'll use it as a basis for clearing the equivalent of https://github.com/spray/spray/issues/746 on the Akka side.\n\nThanks again, @fernandezpablo85, your contribution is still very much appreciated.\nIf you like you are also very welcome to resubmit this patch against the [`release-2.3-dev` branch of the Akka codebase](https://github.com/akka/akka/tree/release-2.3-dev). Otherwise we will simply turn to your code when closing the ticket in Akka.\n. Great! Thanks @fernandezpablo85 !\n. No problem. We will probably come back to your solution here when we tackle https://github.com/akka/akka/issues/16827.\nThanks again!\n. This will happen anyway in the documentation effort for Akka HTTP.\n. Thanks, Andr\u00e9!\n. After reading up on the discussion in #739 again I think we need two different model classes for cookies: one for what is contained in the `Set-Cookie` header (what we already have) and one for what the `Cookie` header contains. Maybe the latter can be a super-type, maybe not.\n. Ok, apart from the small efficiency improvement I think this patch is ready for merging!\nThanks, Benjamin!\n. Nice! Thanks, Benjamin!\nThis is a great first OSS contribution! :)\n. Ok, thanks for this patch!\nCan you do the two following things for us to be able to merge your patch?\n1. Sign the [Typesafe CLA](http://www.typesafe.com/contribute/cla)\n2. Change the commit message to `= examples: fix incorrect dtd reference`\n. Thanks, Dominic!\n(The `=` is required by our [commit msg policy](http://spray.io/project-info/contributing/#git-commit-messages))\n. Thanks!\nBefore we can accept your pull request could you:\n1. Accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla)\n2. Change the commit message to `= docs: fix typo, closes #749` (in accordance with our [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages))\n. I understand your frustration.\nHowever, since spray is already a somewhat mature project we do have to adhere to certain processes with regard to accepting any kind of patch. Also note that there is no need to open a another PR just for making a change.\nSimply pushing into the existing one would do.\n. Thanks!\n. Sorry for taking so long in responding to your PR!\n\nWe have reserved some capacity this week to push for a minor spray release and would like to incorporate as many good contributions are possible, with this being definitely in scope.\n\nCould you squash all your commits into a single one?\n. Thanks for your input, @hamiltont, I think you definitely have a point.\nWe'll likely address it soon with our current work in akka-http.\n. Sorry for taking so long to get back to this PR!\nWe've been awfully busy with the migration to Akka HTTP, which is now nearing completion.\nTherefore spray is now in maintenance mode and we are reluctant to change the API more than absolutely necessary by crucial bug fixes.\n\nSince we are all in agreement that some kind of prepackaged proxy solution is likely to be valuable to a great number of users I'd propose to resubmit this PR against the [`release-2.3-dev branch of the Akka repo](https://github.com/akka/akka/tree/release-2.3-dev). We can promise that any PR issued against the Akka codebase will be dealt with a lot faster than we've been able to here in the spray repository.\n. I'm keeping this ticket alive and mark it for migration to Akka since @ivantopo provided a very nice discussion of the trade-offs involved in client-side request timeout handling.\nIn Akka HTTP we will have to come back to the question of how to best implement client-side request timeouts and this ticket will certainly of value.\n. Another interesting PR that we should look at when tackling this: https://github.com/spray/spray/pull/957\n. The `RoutingSpec` contains a mere 4 lines of code, so it should be easy to rebuild it for one's specific needs. Do you really think it would add that much value?\n. Closing for now.\n. Thanks for reporting.\nYes, spray should access an illegal `Origin` header as a `RawHeader`.\nI don't think we should relax the Origin parser, but we definitely need to prevent the `IllegalUriException` from crashing the parser completely.\n. Guys, we have reserved some capacity this week to push for a minor update release.\nOf course we'd like to incorporate the latest and greatest in contributions! :)\nSo, since this patch here looks valuable on its own, I'd like to get it merged.\nThe certainly also interesting additions that Andr\u00e9 proposed can be added in a potential second step.\n\nAge, would you consider this patch \"ready for merge\" as it stands now?\n. Ok, Andr\u00e9, would you say that your solution supercedes this one or would it be independent of what Age proposes here?\n\n> I'm still curious as to whether you like the \"built into the file/resource directives\" approach, which is slightly more efficient, or Andr\u00e9's more general approach.\n\nSince the `FileAndResourceDirectives` are already somewhat \"meaty\" in the sense that they do quite a lot I'd currently say that your \"built-in\" solution is good enough. Since the header is properly modelled people can still built their own logic around the `If-Modified-Since` header if they want/need to.\n. Ok, then we'll wait for your PR...\nThanks again!\n. Thanks, Alex!\n. Reassigned to milestone `akka-http` due to the breaking nature of the fix for this.\n. Will not be fixed in spray anymore.\nHowever, lazy route evaluation is what the routing DSL port to akka-http does.\n. This will not be merged short-term due to the breaking nature of this fix.\nWe'll keep it open though, because it'll come with akka-http.\n. Closing as this will not be merged into spray anymore.\n(However, the change _is_ part of the port to akka-http!)\n. Please [this ML post](https://groups.google.com/d/msg/spray-user/hMZndG4IYRs/5DRpAdGdHloJ) on why we have `StandardRoute`.\n. spray _does_ depend on Akka and will not function without it.\nWhat exactly is it you are trying to do?\n. Since we'll move into akka soon completely I don't think we'll reopen this issue.\nHaving an HTTP model that doesn't depend on akka is not a priority for us.\nWhat's your use case again?\n. @gmalouf We are not aware of many users actually making use of this stand-alone nature of spray-http. In fact, @kgurkan and you are the only ones we know of.\nSince upholding full separation between modules comes at a certain cost we do have to trade off between cost and benefit here. Sorry.\n. Yes, thanks for the heads-up!\n. Kender, thanks for reporting!\nWe'll look into this ASAP.\n. Tulio, great, thanks for this fix!\n\nTwo things we'd need before we can merge this:\n1. Can you accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla)?\n2. Please rephrase the commit message to:\n\n```\n= servlet: add termination awaiting to `Initializer::contextDestroyed`\n```\n\n(in accordance with our [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages))\n. Thanks for accepting the CLA!\nCould you also update the commit message?\n. There is no spray release yet which has been compiled against Scala 2.11.\nWe simply haven't officially upgraded yet.\nIt will come though... :)\n. Cherry-picked into master\n. Thanks, Dmitry, for finding and fixing!\n. This is a great PR, thanks Andr\u00e9!\nThis stuff is quite hard to get right and you took us almost all the way already! Very nice!\n\nApart from the inline comments I'd also agree with Age on somehow adding `If-Modified-Since` / `If-None-Match` support to the `FileAndResourceDirectives`, since this is something that people can benefit from without the need to change _anything_ about their existing routing structure.\n. Very good discussions!\n\n> Another question is if we should move this stuff to a new *CacheDirectives trait altogether which we can mix into the Directives trait by default.\n\nYes, absolutely.\nThe `CachingDirectives` are separate because they require a manually-added dependency onto `spray-caching`. We should definitely move these new things into a new trait which is mixed into `Directives`.\nHow about `CacheConditionDirectives` for the name?\n\n> I'm not yet convinced to put ETag support into FileAndResourceDirectives by default (yet).\n\nOk, I guess what we are missing is some setting of requirements, i.e. what do we really want to achieve. I'd say we need the following:\n1. Reduce server-load and bandwidth by supporting cache-conditional HTTP mechanisms (`If-Modified-Since`, `If-None-Match`) for static content.\n   \n   By building this into the `FileAndResourceDirectives` people would be benefitting from this without having to change any code, which is great.\n   Since in the `FileAndResourceDirectives` we are in complete control over how to construct ETags we can choose a mechanism that relies on file size and modification times and thus avoid the need to actually open and read the file.\n2. Give the user some tools to achieve the same benefits for his/her own non-static resources.\n   \n   This is somewhat harder since we don't generally know the parameters that cause a response to change. The dumb (and easy) way is to simply let the user code run and render the resource before using something like MD5 to compute an ETag for it. The problem is that this approach does not reduce server load, it could actually be increasing it! We would save bandwidth but the tradeoff is not necessarily in favor of actually applying ETag support with this approach.\n   Therefore it'd be better if our solution allowed the user to supply us with logic that computes an ETag purely from request parameters, i.e. a function `HttpRequest => EntityTag`. Assuming that this function is reasonably fast we can achieve the bandwidth and server-load reduction even for non-static routes.\n\n> > The spec says (RFC2616 14.26) that, if the If-None-Match tags match, the server \"MUST NOT perform the requested method\". However, we are unconditionally calling our inner route here, which will always perform the requested method. This means that we'll potentially execute PUTs (etc.) when we are not allowed to!\n> \n> Yeah that's true. How would you attempt to fix that?\n> \n> I experimented with transforming the request to HEAD and looking and the response > headers. That would only work if head is explicitly implemented (without transparent-head-requests).\n\nAs discussed above we should require the user to supply us with an ETag calculation function. The \"dumb\" approach of always running the inner route is not really attractive IMHO.\n\n> I think the directives should be tied together because if `withIfNoneMatchSupport` matches, a lower level withIfModifiedSinceSupport wouldn't see the request and validate its `If-Modified-Since`.\n\nYes, I see the connection between the two.\nHow about we solve it by making `withIfModifiedSinceSupport` only kick in if the request does not also contain an `If-None-Match` header?\n\nSo far spray's convention of building more complex directives from simpler ones has really paid off. It makes the implementations easier to understand and test and also increases flexibility for the user. Ideally I'd like to uphold this principle also here.\n\nSo, it seems to me that we need these 4 new directives:\n- `withIfModifiedSinceSupport(f: HttpRequest => DateTime)`\n- `withIfUnmodifiedSinceSupport(f: HttpRequest => DateTime)`\n- `withIfNoneMatchSupport`(f: HttpRequest => EntityTag)\n- `withIfMatchSupport`(f: HttpRequest => EntityTag)\n\nThe `FileAndResourceDirectives` would internally use `withIfModifiedSinceSupport` and `withIfMatchSupport` where they can.\n\nDoes that sound reasonable?\n. Yes, the `withIfNoneMatchSupport` would still have to check for the presence of an `If-Modified-Since` header. However, it would do so with purely its own functionality in mind.\nIt's more or less the \"do-one-thing-only\" principle, `withIfNoneMatchSupport` should have the least-possible scope to correctly do its thing.\n. Ok, I looked through the RFC again and compiled this table to show the relationship between the `If-Modified-Since` and `If-None-Match` headers (using [this](http://www.tablesgenerator.com/text_tables#)):\n\n```\n+---------------------------+--------------------------------------------------------------+\n| Response for a request to | Request Header: `If-None-Match: tag list`                    |\n+ a resource with `ETag: X` +--------------------------------------------------------------+\n| and `Last-Modified: LM`   | contains X       | does not contain X | header not present   |\n+---------------------------+------------------+--------------------+----------------------+\n|            | LM <= TS     | 304 (GET/HEAD) / |                    | 304 (GET/HEAD) /     |\n| Request    |              | 412 (other)      |                    | Inner route response |\n| Header:    |              |                  |                    | (other methods)      |\n+ `If-       +--------------+------------------+                    +----------------------+\n| Modified-  | LM > TS      |                   Inner route response                       |\n+ Since:     +--------------+------------------+                                           +\n| TS`        | header       | 304 (GET/HEAD) / |                                           |\n|            | not present  | 412 (other)      |                                           |\n+------------+--------------+------------------+-------------------------------------------+\n```\n\n(Additionally the server should attach `ETag: X` and `Last-Modified: LM` headers to all responses.)\n\nLooking at this table I agree with you in that the logic is too entangled between the two headers to efficiently be pulled apart into two dedicated directives. So we need to put it into one single directive after all (sorry for taking so long to realize this!)\n\nHow about this proposal:\n1. We add the model classes for these new headers:\n   `If-None-Match`, `If-Match`, `If-Modified-Since` and `If-Unmodified-Since` (as you already did)\n2. We reduce the scope of this PR to only implement the logic from the table above as one directive (plus extension of the `FileAndResourcesDirectives`), i.e. we remove all logic regarding the less important `If-Match` and `If-Unmodified-Since` from the scope of this PR. This should help in keeping us focussed and getting this done in time for the release.\n3. We add a `CacheConditionDirectives` trait which for the time being only holds one one new directive called `conditional` (and is the place where other directives for `If-Match` and `If-Unmodified-Since` can go in the future.)\n   \n   The `conditional` directive has this signature:\n   \n   ```\n   def conditional(eTag: EntityTag, lastModified: DateTime): Directive0\n   ```\n   \n   It doesn't take a function but requires the user to directly specify the `eTag` and the `lastModified` time stamp for the resource. This makes it a directive that \"automatically\" must be used quite far down in the route structure, where the exact resource has already been established and the `eTag` and `lastModified` values are available. It also requires that a user specifies both values (as supplying only one does not allow us to correctly implement the RFC).\n4. We extend the `FileAndResourcesDirectives` to make use of the new `conditional` directive internally.\n\nWDYT?\n. > What would you suggest as the default implementation for file/resource ETags?\n\nI'd say we take the file modification time (8 bytes) and file size (another 8 bytes) and Base64 encode (`org.parboiled.common.Base64`) these 16 bytes. This results in a 22 char String which should be good enough as an ETag value.\n. Very interesting, Andr\u00e9, thanks!\nI really should have checked the latest HTTPbis spec earlier. Your assumption is right, we do follow HTTPbis wherever we can. In that regard the table I compiled earlier is obsolete and the logic we need to implement is indeed simpler, since there is less entanglement between `If-Modified-Since` and `If-None-Match`. It seems we can implement `withIfModifiedSinceSupport` and `withIfNoneMatchSupport` after all, right?\n. Another thing: I have added the `conditional` support to the `FileAndResourceDirectives` here: https://gist.github.com/sirthias/9392921\n\nYou could simply apply that patch to your fork.\n\nBy the way: We are truly impressed by the amount of commitment you put behind this contribution as well as the quality of your code! Hats off! We definitely owe you (another :) beer! Maybe in Juni in Berlin? :)\n. We simply follow HTTPbis where it is in disagreement with RFC2616.\nIn many cases HTTPbis simply encodes what is considered \"best practice\" with all the experience that has been accumulated after RFC2616 was released.\n. The is a very good find, Age!\nI definitely need to take the time to read the HTTPbis document completely.\nLooking at these precedence rules it seems to me that a unified implementation like the one that Andr\u00e9 already implemented is the only way to go.\nWe can't sensibly put the burden of properly assembling a number of different directives in the right order onto the shoulders of the user.\nSo, my current take is: leave it at a single `conditional` directive, which closely follows the precedence logic in section 6. Would you agree?\n. @2beaucoup Stand-by for a PR for your PR adding the `If-Range` bits that are still missing (but now doable since we have merged #612)\n. Just about to figure that out... :)\n. Yes please, I'll send a PR shortly.\n. Ok, PR sent.\n. Great!\nNow we only need to wait for @jrudolph's ok and then we can tick off this beast as solved...\nThanks once more for pushing this PR through the stony path to success!\n. I'll add docs after the merge, no problem.\n. Andr\u00e9, could you prefix you second-to-last commit message with `= routing:`?\nAfter that I'd really like to finally push that green `Merge` button here... :)\n. Great, thanks again, Andr\u00e9!\n. Nice!\n. Apart from the question whether `HTTP/1.0` is still being handled correctly this LGTM.\n. Njal,\nspray follows RFC3986 with regard to URI parsing and rendering, which states in [its section 2.3](http://tools.ietf.org/html/rfc3986#section-2.3):\n\n> For consistency, percent-encoded octets in the ranges of ALPHA\n>    (%41-%5A and %61-%7A), DIGIT (%30-%39), hyphen (%2D), period (%2E),\n>    underscore (%5F), or tilde (%7E) should not be created by URI\n>    producers and, when found in a URI, should be decoded to their\n>    corresponding unreserved characters by URI normalizers.\n\nSo, what you are asking for it essentially a departure from the spec, which is something we would like to avoid.\n. Note that the fix to ticket #654 might present a nice way out of a potential compatibility problem with a misbehaving client or server.\n. I really don't know why you don't like this particularly homogeneous piece of code...\nThere is definitely some prettiness to the amount of repetition here!\n(Please refrain from `git blaming` anyone here!)\n. Thanks.\nCould you append `, fixes #804` to the commit message?\n. Thanks, Benjamin!\n. Thanks, Benjamin!\n. > What about [optional]HeaderValuesByType for extracting several values at once?\n\nWhich headers for you have in mind for this directive?\nOff the top of my head there are not that many headers that usually appears multiple times in a request.\n. Ah, I see.\nIdeally we'd be able to use an `HList` for specifying the header types, e.g. like `optionalHeaderValuesByTypes[Range :: Cookie ::`User-Agent`:: HNil]`, which would then result in a `Directive[Option[Range] :: Option[Cookie] :: Option[`User-Agent`] :: HNil]`.\nDoable, but would require another magnet layer and some shapeless magic. Maybe we leave it as an exercise to the user... :)\n. Closing for now as we think #811 is invalid.\n. Thanks, Christian!\nI think your request is a valid one, but we'd need your patch to not degrade the runtime performance characteristics of the existing code. So, if you could simply extend the existing parsing solution rather than replace it with a regex I think your patch can go in.\n. Great, thanks!\n. Btw: We'd also need you to accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) before we can merge your patch!\n. Thanks again, Christian!\n. Thanks!\nFor our code mgmt. processes it'd still be better if you targeted this PR against the master branch.\nThe releases branches can then receive it from there...\n. Superseded by https://github.com/akka/akka/issues/15495\n. Just FTR: If this ticket results in some kind of change this change will not be one that can be released in a minor spray release. So, it looks like we have some more time to discuss things here since any resolution will only be released with akka-http.\n. Fixed by the new \"binding\" API in akka-http.\n. Unfortunately this solution won't work.\nWe can not reliably have more than one ACK pending when talking to akka-io.\nThe akka-io layer does _not_ itself maintain a really buffer, it can only ever deal with one single pending Write. If you send a 2nd write before the first one has been acked you'll provoke a `CommandFailed` error.\n. > For HTTP, we can almost be sure that one Tcp.Received will be followed by a Tcp.Write (request-response pair).\n\nNo, you can't!\nThe IO layer will deliver a `Tcp.Received` for every chunk of bytes that is read from a connections send buffer in one go. A single `HttpRequest` might be arriving in one chunk or 100 partial chunks. You don't know and it's completely indeterministic.\n\n> Per @jrudolph 's concern, we'll try to add a stage under SSL stage, the new added stage will try to control the data flow sending to SSL, i.e. buffer the Tcp.Received, send one to SSL, waiting for ACK, then send next.\n\nI'm not sure what you actually want to achieve.\nWhat's you goal?\n. Note that the `FutureDirectives` already give you a way to do pretty much what you are looking for.\nE.g., you can say something like this:\n\n```\ndef futureAuthorize(check: => Future[Boolean]): Directive0 =\n  onSuccess(check).flatMap(authorize(_))\n```\n\nIn that sense one could even argue that we should simplify the `authenticate` directive to _not_ take a future and rely on the `FutureDirectives` for the future handling.\n. The problem is indeed that the server does not send a `Content-Length` header which, according to the spec, carries the semantic that the closing of the connection signals the end of the response.\nIf your server doesn't close the connection it effectively stalls any progress, the client _cannot_ be the one closing the connection in that case, because there might still be content coming.\n\nThe HTTPbis spec section that @agemooij rightfully quoted specifies exactly what the problem is (how the server is misbehaving). Therefore I don't see anything that we can do from the spray side to get this problem fixed for you.\n. We already have a branch for 2.11 which runs nicely:\nhttps://github.com/spray/spray/tree/unstable/1.3-scala-2.11\n\nSo, we could release a stripped down build of spray 1.3.1 for Scala 2.11 very soon.\nHowever, it would have to be \"stripped down\" for as long as our aux dependencies (lift-json, json4s, play-json) are not all available (so the respective (un)marshalling support for them in spray-httpx would have to be removed).\n. Ok. The current plan is to publish\n\n```\n\"io.spray\" % \"spray-xxx_2.11\" % \"1.3.1\"\n```\n\nnext week after the easter holidays, i.e. Tuesday or Wednesday.\n. Update: We are still waiting for akka to become fully released against 2.11.\nOnce this is out we'll be publishing a nightly build for Scala 2.11 which will effectively be spray 1.3.1 cross-published (with the exception of JSON support for missing aux dependencies).\n. My take is that it won't take long.\n. Ok, a Scala 2.11-compatible build is now available:\nhttps://groups.google.com/d/msg/spray-user/OMHBs_rGPG4/39eQ5i1F1LQJ\n. Very nice! Let us know how things go!\n. Sorry, @cowboy129, but we currently don't have any capacity for a code review.\nWe'll have to come back to this later.\n. I agree with @jrudolph.\nBtw: just to help me understand: what was the change from 1.3.0 to 1.3.1 that break things here?\n. Thanks @topping for your perspective.\nWe'll discuss this PR tomorrow.\n\nOne question: If we merged this PR and published a (date-stamped) nightly build, would that solve the problem for you guys in the short-term?\nLike @jrudolph explained we'd like to avoid having to go through a full release cycle just for this patch.\n. Ok, I just published `1.3.2-20140428` to http://repo.spray.io.\nLet me know whether you need this patch also in a Scala 2.11 compatible build.\n. @vatel Ok, just published `1.2.2-20140526` to `http://repo.spray.io`.\n. Ok, I have just published build `1.3.2-20140909` with cross-paths for 2.10 and 2.11 to repo.spray.io.\nLet me know if you need anything else.\n. Grzegorz, we'll eventually upgrade to shapeless 2.0 ourselves and sort out the problems, but, unfortunately, don't have to capacity to do it right now...\n. Yes, thanks once more, Andr\u00e9!\n. As we don't use mimepull anymore in akka-http we should verify that filenames in multipart/form-data uploads, which are encoded with UTF-8 on the client-side, do indeed produce the original filename when inspected on the server-side.\nAdding a respective test might be all that is required.\n. The simplest solution might be to simply scan the string produced by the Json4sMarshaller for the first non-whitespace character. If it is not `[` or `{` we should throw an exception.\n. Yes, Dominic, this is a bug. Thanks for finding and fixing!\nThe bug was always a bug but did not manifest itself in Akka version 2.0 (when we wrote this initially) as back then `promise.future` would return the `Promise` object _itself_.\nIn some Akka upgrade this was changed turning this bug into an actual problem.\n\nWhat we are missing now is a test to finally nail this thing shut.\n. No, you are still right.\nWe cannot do `store.putIfAbsent(key, promise.future)` in one place and then `store.remove(key, promise)` in another. Even if this currently works it still a bug in our code, since it relies on implementation details that we do not control.\nSo, your patch is a valid one.\n. Thanks again, Dominic!\n. Thanks for voicing your concern with the `HttpHeader` extractor.\nYou are right, the extractor will only match if you specify the header name in all lower-case.\nThe reason is that the Scala extractor design makes it _impossible_ to construct an extractor, which matches strings case insensitively!\n\nThis is the implementation:\n\n```\nobject HttpHeader {\n  def unapply(header: HttpHeader): Option[(String, String)] = Some((header.lowercaseName, header.value))\n}\n```\n\nAs you can see the logic has no way to access the string that you supply to the pattern match. All it can do is _provide_ a string that scala's pattern matching logic will then compare with the one you have specified. This comparison is based on `equals` and is therefore always case sensitive.\nThis limitation is therefore one imposed by the language and not a deficiency of spray.\n\nStill, you are right in that better documentation of this limitation is required.\n. Thanks for this patch!\nBefore we can merge it we'd need you to:\n1. also accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) (thanks for your sending us the spray one). Note that there is no need to send Typesafe anything for this.\n2. Change the commit message to\n   \n   ```\n   = httpx: unwrap `MalformedContent` error in `ResponseTransformation`\n   ```\n   \n   (as per our [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages))\n. No problem. Our travis build is unfortunately not as stable as we'd need it to...\n. Thanks, Stanislav!\nHowever, even though this is just a cosmetic fix, we'd still need you to accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) before we can merge this PR.\nAlso, could you change the commit message to\n\n```\n= docs: fix typo in handleExceptions.rst\n```\n\nto be in line with our [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages)?\n. Great! Thanks, Stanislav!\n. Thanks, Stanislav!\nCould you add this commit to https://github.com/spray/spray/pull/847?\n. And one more. Thanks again, Stanislav!\n. We have zero experience whatsoever with OSGi and therefore would need very detailed instructions regarding a fix of this problem, ideally a PR.\nFor example: what exactly do you mean by \"the import definition of the sun.misc package in spray-util-1.3.1.jar\"?\n. Thanks, Richard.\nHere we'd need the CLA as well as a rewritten commit message as well.\nThanks!\n. Yes, thanks Richard, not resetting the `_byteCount` is indeed a bug.\n\nBefore we can merge your patch we'd need you to\n1. Accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla)\n2. and change the commit msg to `= http: fix HttpData.Builder not clearing internal counter in 'clear'` in accordance with our [commit msg policy](http://spray.io/project-info/contributing/#git-commit-messages)\n\nThanks again!\n. Thanks again, Rich!\n. Yeah, thanks! And no worries about the commit msg.\n. Thanks, Chad, for putting in this PR!\nI think this is a valid addition.\nWe should be able to keep the main code path a (small) bit more efficient though if we move your codepoint logic into the last `case` branch. AFAICS the other case branches (c < 127) will never suffer from this problem.\n\nAlso, we'd need you to\n1. Accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla)\n2. and change the commit msg to `= http: fix URI encoding of non-BMP chars` in accordance with our [commit msg policy](http://spray.io/project-info/contributing/#git-commit-messages)\n\nbefore we can merge your patch.\n\nThanks again!\n. Thanks for accepting the CLA!\n\nIt seems we don't have to do the `isHighSurrogate` call for all code paths.\nFor the first three `case` branches we already know that `charSize` is `1`.\nHow about this instead?:\n\n```\nval charSize = string.charAt(ix) match {\n  case c if is(c, keep)     \u21d2 r ~~ c.toChar; 1\n  case ' ' if replaceSpaces \u21d2 r ~~ '+'; 1\n  case c if c <= 127        \u21d2 appendEncoded(c.toByte); 1\n  case c \u21d2\n    def append(s: String) = s.getBytes(charset).foreach(appendEncoded)\n    if (Character.isHighSurrogate(c)) { append(new String(Array(string.codePointAt(ix)), 0, 1)); 2 }\n    else { append(c.toString); 1 }\n}\nrec(ix + charSize)\n```\n. Encoding URLs is a hot code path in spray, so accepting a little bit of ugliness in exchange for many billions of clock cycles saved across all users of spray is ok... :)\n. Not that I can see right now.\nOnly our limited capacity for work on the next spray minor release... :)\n. Thanks, Chad!\nI think this patch as well is a valid one.\nIt does slightly decrease performance in certain cases, but since this only affects Uris that actually contain a percent sign this is quite acceptable.\n\nHere we also need you accepting the CLA and rewrite the commit msg to:\n`= http: fix Uri decoder not properly decoding pct-encoding for certain charsets`\n. Fixed by 9c70a86e8089edbd77daddd84f94b12407761f50\n. Revisiting this ticket we weren't sure anymore whether there is a nice solution that let's us improve over what we already have in akka-http within the boundaries of the current DSL design, which comes with limited inspectability.\nOnce the DSL has evolved to its next step (with better support for programmatic inspection) things will likely be easier here.\n. Thanks, Daniel!\nWe'll look into this as soon as our capacity allows.\n. It's going to be fixed with the coming akka-http.\nThe work-around is to use a patched version of spray that you build yourself.\n. Yes, absolutely.\nThis code is among the oldest we have in the codebase.\nI am surprised that this hasn't caused any issues earlier.\n. Don't worry about the Travis error, unfortunately Travis is somewhat unstable for spray.\nThanks for the patch, Haoyi!\n. Ok, thanks for reporting.\nI think this is a valid request.\n. We are following the latest HTTPbis document for the header value grammars.\nIn this case this section is the relevant one: http://tools.ietf.org/html/draft-ietf-httpbis-p6-cache-26#section-5.2\n\nWhich client/server is generating the `http://tools.ietf.org/html/draft-ietf-httpbis-p6-cache-26#section-5.2` cache directive in your case?\n. >  IIUC it has the same effective grammar here, which allows Cache-control: no-cache=field-name.\n\nAh, yes, I see it now. The grammar for `no-cache` differs depending on whether the message is a request or a response. Sorry for not seeing that earlier.\n\nHowever, it seems spray can already parse field-names after `no-cache` directives:\nhttps://github.com/spray/spray/blob/master/spray-http/src/main/scala/spray/http/parser/CacheControlHeader.scala#L44\n\nCould you provide us with a failing test case?\n. Thanks again, Brian.\nClosed by 02cd11efa78e07dfa2cbed6d2626ab28390ea143.\n. Solved in akka-http by having everything in a route structure be evaluated lazily.\n. Great, thanks!\ncherrypicked into master.\n. Ok, I see.\nSo all that is required is adding back `\"shapeless.*` to the spray-routing OSGi imports?\n. Great, thanks for verifying!\n. According to the spec governing the Cookie header cookie-pairs must be separated by `;`, not by commas: https://tools.ietf.org/html/rfc6265#section-4.1.1\nTherefore Jersey client needs to be fixed here, not spray.\n. Ah, yes, right. Missed the discussion there.\nJust though about this once more and since commas are illegal anyway in a cookie value I think there is no risk in accepting them as separators as well.\nI'll reopen this PR.\n. Could you accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) (only a few clicks) and change the commit message to\n\n```\n  = http: accept comma as cookie separator, closes #869\n```\n\n?\n. Great, thanks William!\n. If you have trouble with certain aspect of spray please ask on the ML before opening a ticket!\n. This is solved in akka-http by having everything in the route structure being evaluated lazily.\n. Nice!\n. Thanks, Greg!\nWe'll look at this patch as soon as our capacity permits.\n. Apart from the small comments this look good, thanks Greg!\nCould you accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) (only a few clicks) so we can finally get this merged?\n. Thanks, Greg!\nWe've missed the deadline for the 1.3.2 release with this, and will probably have to include it in akka-http directly.\n. The difference is that Greg's `optionalAuthenticate` directive recovers from `AuthenticationFailedRejections` produced by the authenticator by extracting `None`.\nYou can achieve the same effect by simply wrapping the underlying authenticator, yes.\nThe `optionalAuthenticate` directive simply pulls this wrapping up to the directive level.\n. Thanks again, Greg!\nAnd sorry for taking so long to finally merge your patch!\n. Great, thanks!\n. spray has traditionally followed a \"no cross version suffix\" artefact naming policy because our central dependency Akka has done the same (until Akka 2.0.x) and was always tied to a specific Scala version anyway, i.e. there were no real cross builds across several Scala versions.\nAkka 2.3 is now the first Akka version to actually be cross-built across two Scala versions.\n\nSo, this is the time when our naming policy broke down and we had to add a cross-version suffix for the Scala 2.11 build.\nHowever, we are not going to retroactively add cross version suffixes to our builds and we are not planning on releasing another spray version (apart from patches).\nSo, even though in hindsight we probably should have enabled cross-version suffixes right from the start it is now to late in spray life cycle to make this change in the artefact naming policy.\n. Thanks for this PR!\nWe'll get to it during our current migration work of spray to akka-http.\n. Ok, we just discussed this and think you change is indeed a good one.\n(As you merely change the default charset to UTF-8).\n\nCould you accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) (only a few clicks) so we can merge this?\n. Thanks!\n. Thanks, Alex.\nCould you accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla), so we can merge your patch?\n. Great! Thanks!\n. Closing as \"Wont fix\". See #685 for more details.\n. Since timeout (re)configuration between the layers will always come with its own set of problems we'd now propose to name this new directive `withShorterRequestTimeout` and have it implement an _additional_ timeout, on top of what is configured underneath.\nThis way we don't need any communication between the layers and the user would have to set a long timeout underneath which gets (gradually) refined in the routing layer.\nWhile this requires a little bit more thought and work on the part of the user it appears to be the overall cleaner solution.\n. A @jrudolph said let's discuss all issues on a case by case basis.\nPlease ask on the mailing list or open a ticket is you have a concern for anything in particular.\n. Very cool!\n. spray's cookie parser follows the spec here:\nhttps://tools.ietf.org/html/rfc6265#section-4.1.1\nwhich disallows \\u0001 in cookie octets.\nHowever, the current version of spray should not \"throw the whole response away\" but rather log a warning an deliver the offending `Cookie` header as a `RawHeader`.\nWhich spray version are you using?\n. Thanks, Tim, this LGTM.\nCould you accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) so we can get this merged?\n. Great, thanks!\n. A nice fix for this ticket would be to simply add a `port` member to the `RemoteAddress.IP` class.\nHowever, since this would be a breaking change to the spray API I'm assigning this ticket to the Akka HTTP.\n. Thanks for this patch, Richard!\nCould you accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) (only a few clicks) so we can merge it?\n. Thanks, Rich, for this patch!\n(And sorry it took so long before merging.)\nThe change will go online under http://spray.io/documentation/1.1-SNAPSHOT/spray-httpx/de-compression/ and will be moving into the main docs once we publish the next release.\n. Since the `jsonp` directive is not available anymore in akka-http I'd now say that this ticket can be closed by adding a warning to the directive's doc page.\n. Closed by e0550720f056bcb1c7836031fef8a51b36e5a6da\n. Thanks, Noel!\n. Closed by 359ea095183c451acfd51975ef3aeb75df4d7be9\n. Closing as not reproducible for us at this point.\nIn order to tackle this problem we'll need a way to trigger the problem with a higher frequency than once in 50GB.\nAlso, it'd be interesting if the problem persists with the current akka-http implementation.\n. I'd like to move this ticket to akka-http where a performance tuning section is probably more needed at this time.\n. @peterparks We generally prefer to obey official specifications (ideally RFCs) wherever we can. If you can point us to anything underpinning your request it stands a much higher chance of becoming accepted.\n. Thanks, Peter, for this PR.\nWe'll look at it during our current migration work from spray to akka(-http).\n. @peterparks We have now come to a point where we'd like to merge your patch (which I think does provide a good solution!) and then forward-port it to akka-http.\nThanks for this patch, Chris, very nice!\n\nWould you be able to rebase on top of the current master branch and accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) (only a few clicks)?\nThanks again and cheers!\n. Thanks, Peter!\n. This would definitely be a nice addition to have.\nHowever, because the spray project is now in maintenance mode, where we are reluctant to add new features, I'd propose to resubmit this PR against the [`release-2.3-dev` branch of the Akka repo](https://github.com/akka/akka/tree/release-2.3-dev).\nIt can then become part of Akka HTTP and act as (another) incentive for the migration from spray to Akka HTTP.\n\nThanks again for submitting this PR!\n. While I agree with Johannes in that this would be a nice feature I don't consider it important enough to warrant any action from our side at this point.\nOne can easily achieve the desired effect within the DSL of one's preferred test framework.\n. While I agree that having a dedicated `produceTry` directive would be nice the work-around of simply using\n\n``` scala\nproduce(instanceOf[Try[T]])\n```\n\nwhen requiring an error channel back to the API layer should be good enough.\n. I have just copied the (AFAICS) last missing artifact from repo.spray.io. to sonatype / maven central (spray-json_2.11 1.2.6).\nIf there is anything that you need from repo.spray.io which is not yet mirrored to maven central please let us know.\n. Thanks for the outstanding ticket description, Adam, I completely agree with your assessment.\nThis is a valid request and should be addressed.\n. Thanks for this PR!\nWe'll take a look at it during our current migration work from spray to akka-http.\n. Thanks again for this PR, @ajknoll and @caspark!\n(And sorry for taking so long to get back to it!)\n\nI think this is a nice fix, which we'll definitely want to merge and port to Akka HTTP.\nHowever, we'd need you, @ajknoll, to first accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla). This shouldn't take longer than 30 seconds.\nThanks again!\n. Thanks!\n. Thanks for this patch!\nCould you\n- accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) (only a few clicks)\n- change the commit message to\n  \n    = can: fix error message in response parser\n  ?\n. Thanks, Daniel!\n. Thanks!\nCould you\n- accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) (only a few clicks)\n- change the commit message to\n  \n    = docs: fix typo\n\n?\n. Great, thanks!\n. Yes, a 1.x.2 release is already planned.\nHowever, in addition to simply recompiling for spray-json 1.3.0 we'd like to also get the most pressing tickets closed and need a bit of time for that.\nHopefully we'll get it done sometime next week.\n. Recompiling (and actually re-releasing/re-publishing) is all that's required here.\n. Thanks for this patch, Chris, very nice!\nCould you\n- accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) (only a few clicks)\n- squash your commits into one\n- change the commit message to\n  \n    = can: fix SOE in spray.can.rendering.toTcpWriteCommand, closes #877\n\n?\n. @ChrisA89 I'd like to get this merged soon. I anything holding you back from accepting the Typesafe CLA?\n. Closed by f9309bb37de733ea0c99d2b2d8aca1e4076e9acf\n. Sorry, Chris, due to deadline constraints we decided to move ahead with this fix ourselves.\nThanks anyway for providing the necessary push!\n. While prioritizing spray maintenance tickets we decided that this ticket is not important enough to make the current cut. Since the problem is already fixed in akka-http we are closing this ticket now.\nPlease reopen if this thing is really an issue for your application.\n. Thanks, Christian, this looks like a valid proposal.\nWe'll take a deeper look once we port the respective code bits to akka.\nThanks again!\n. Thanks, Evan, for this patch.\nI don't see any risk in adding access to the cache keys.\nMaybe @jrudolph has a comment here?\n\nOtherwise, in order for us to be able to merge this, could you\n- accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) (only a few clicks)\n- change the commit message to\n  \n  ```\n  + caching: add access to cache key set\n  ```\n\n?\n. Thanks, Evan!\n. Mathias, even though this is an unfortunate problem with 1.x.2 we are currently busy on other fronts.\nHow serious is the issue on your side?\n. Hunor, thank you for your contribution!\nWe are currently a bit unsure of the spray-caching module's future, so we will probably not be able to merge breaking changes to it at this point.\n\nspray-caching was meant as a small and light-weight in-memory caching solution, not so much as a full-blown, feature-rich caching layer. So I'm somewhat unsure whether pluggable stores add a lot of value in this context. Maybe you can shine some more light onto the motivation for this patch?\n. Thanks, Guillaume, interesting!\nCurrently we are unsure whether we should still merge this into the spray codebase or directly port it over to akka-http.\nIn any case, could you accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) (just a few clicks) so we can accept your contribution(s)?\n. Guillaume, we are just coming back to this PR now.\nWhile we still believe that an extension of the header model along the lines of what you propose here is definitely valuable we don't want to change the existing spray APIs more than absolutely necessary (e.g. for bug fixing reasons).\nSo, if you'd still be willing to add the missing parsing rules as well as some tests we'd like to ask you to resubmit against [`release-2.3-dev branch of the Akka repo](https://github.com/akka/akka/tree/release-2.3-dev).\n\nThanks again for submitting this PR!\n. Alex, thanks for this PR!\nIt sure looks interesting!\nWe are currently reluctant to add new feature to spray as we are deep in the process of porting everything to akka-http, which requires some stability on the port base.\nWe'll come back to this PR when we are done with the initial port (which should be quite soon).\n. Alex, we just came back to this PR and do like it!\nWe think it's a nice addition to the DSL that would certainly be valuable to quite a number of users.\nHowever, since spray is now in maintenance mode and we have already ported the respective parts of the spray routing DSL, we don't think that merging this PR here in the spray repository would actually be the best thing to do.\nWould you be willing to resubmit this PR [against the `release-2.3-dev` branch of the akka repo](https://github.com/akka/akka/tree/release-2.3-dev)? This feature could become another incentive for people to upgrade to Akka HTTP!\n. Closing here.\n. Thanks, Farid, for this PR.\nWithout having looked at it closer, have you seen the discussion around cookie jars in https://github.com/spray/spray/pull/311 ?\n. Thanks again, Farid, for this patch!\nWe've decided that the complexities surrounding the topic of this ticket are too great to simply offload them onto a contributors shoulders. So we've created https://github.com/spray/spray/issues/772 to discuss and collect everything around it.\nI'm therefore closing this PR now, but we will surely come back to your implementation and steal a few ideas, if that'd be ok for you.\n. While the problem is fixed in spray at least the added test appears to be missing from Akka HTTP.\n. I think the solution should be to simply render the user-supplied headers in the same sequence as they are provided. This should solve this problem and circumvent the question to which headers the exception in the spec actually applies.\nThe good thing is that we can still render the \"important\" headers that spray produces itself in any order, before or after the user-supplied headers.\n. Yes, right, thanks for clarifying!\n\nI think we should make sure to fix actual bugs, the bonus work doesn't appear to be worth the effort at this point. So:\n\n> prioritize rendering order of response headers managed by spray (Server, Content-Length, Date, Connection, Transfer-Encoding others?) and preserve order of remaining headers\n\nSometimes the code becomes easier if we simply render managed headers when we can, which might be before or after the user-supplied headers, which is ok.\nWe should simply make sure that we don't render the user headers in reverse, which might already be the case. If so, no change necessary here.\n\n> preserve order of headers during parsing \n\nYes, maybe this is the only thing we really need to change.\n\n> bonus: combine duplicated, collapsible headers during client side parsing and prevent sending of duplicated, uncollapsible headers as spec'd above.\n\nThat would be nice to have but not worth the implementation cost, AFAICS.\n\n> WDYT? Should I give them a try?\n\nIf you'd like to take a stab at fixing the header parsing sequence issue, we'd be more than happy to merge your PR. Thanks in advance!\n. Apart form the small perf tweak this looks great! Thanks, Andr\u00e9!\n. Yes, thank you.\nWould you like to issue a PR with a fix to the docs?\n. We are just discussing this:\nActually the current behavior does make sense.\nWhen you reply to a request with a chunked response, i.e. a stream, you use the ACKs as trigger for sending the next chunk. However, if the request was a HEAD request you shouldn't actually dump you whole stream. Rather your response stream processing should be aborted, because the response cannot take an entity at all. This is why we \"ack\" with a fake `Closed` event, which is supposed to trigger the cancellation of your response stream.\n\nNow, I agree that this is not a pretty solution and having a proper abstraction for response streams (like the new reactive streams in akka-http) will make things a lot cleaner here.\n\nOne _could_ argue that we should only do this ACK-transformation when `transparent-head-requests` is enabled.\n. I see.\nIt seems to me that we should restrict the \"magic\" described above only to the case that `transparent-head-requests` are enabled. If it's disabled, as in your case, we should simply drop all response chunks coming in from the application. And I guess we should still ACK them.\n. The test we add for this also needs to be ported to akka-http.\n. Ok, thanks for the hint. We'll look into this as well in the context of this ticket.\n. This fix might need porting to akka-http.\n. Great! Thanks, Andr\u00e9!\nThis LGTM.\n. Thanks again, Andr\u00e9, for another nice contribution!\n(And sorry for taking so long to merge!)\n. Thanks, Alex, for this patch!\nCould someone with OSGi experience review this? (Unfortunately we ourselves have no clue whatsoever of all the factors coming into play here...)\nMaybe @topping has some insights?\n. Ok, thanks @alexlomov and @briantopping for your help on this!\n(And sorry for taking so long to merge.)\nI take it this PR now does what it's supposed to do. So we'll merge and get it released ASAP.\n. Thanks, J\u00fcrgen!\n. There have been some concerns about the mimepull license being incompatible with the Apache 2 license that spray is released under. However, mimepull is dual-licensed, under GPL v2 _and CDDL v1.1_.\nSince there doesn't appear to be an issue with linking to a CDDL library from an Apache 2 library we don't consider this ticket to be important enough anymore for spray.\nNote that akka-http doesn't rely on mimepull anymore but uses its own streamified mime parser.\n. Interesting, thanks Noel!\nCould you give us some more motivation as to why you'd need something like an `IgnoredSegment` path matcher?\n. Ok, interesting.\nSomehow this sounds like a rather special setup that is of not so great general utility.\nAs such I'd be inclined to not add this feature as it would likely make spray only bigger, not better.\n. `HttpMessage::asPartStream` converts the message into one with `Transfer-Encoding: chunked`.\nChunked messages must not have a `Content-Length` header. As such this conversion does indeed (necessarily) loose the information about how long the total content is.\nYou can however, if you so choose, add a custom header manually, which somehow retains the information about how long the total chunk stream is.\n. Thanks for this PR!\n(And sorry for taking so long to look at it.)\nWe'd like to merge it but need you to accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla) first. Could you do that? Only takes about 30 seconds...\n. Thanks again for the improvement suggestion!\n. Thanks for this patch!\n. Thanks for the PR!\n@2beaucoup No, this tests hasn't been ported yet.\n. Nice! Thanks, Gilles!\nWe'll give it a spin.\n. The fix for this might need porting to akka-http. We'll have to verify when fixing.\n. This problem is now fixed in spray.\nWe keep the ticket open for now as a reminder for the required port to Akka HTTP.\n. Reopening because the fix didn't make it into the `release/1.3` branch yet.\nI didn't have enough time today to properly forward-port it, so this issue is only fixed in 1.1.3 and 1.2.3 but not 1.3.3.\n. Thanks, Mike, for this PR! (And sorry for taking so long to look at it!)\nThis looks like a good fix!\n. I think having a response transformer that will simply try to apply all known decoders before giving up is a useful feature for the new high-level client API of akka-http.\nSince we'd like to keep the spray API as stable as possible in the maintenance mode we are currently in I'm assigning this ticket to Akka HTTP.\n. This is a nice example of something that is fixed by the improvements to the DSL in akka-http.\nThere is nothing we can do in spray though (that upholds binary compatibility), so I'm closing this ticket here.\n. Our preferred solution to the \"can-marshal-everything\" problem that these reflection-based serialization approaches present would be to make the fully generic marshaller only available with (another) dedicated import.\nIf you don't want to import the fully generic marshaller you should be able to \"switch it on\" for a specific type with a single line, e.g. `implicit def fooFormat = json4sMarshalling[Foo]`.\n. Thanks, Daniel!\n. Brian, thanks a lot for digging into this and reporting!\n(And sorry for taking so long to respond!)\nThis looks like a valid problem with an easy fix.\nThanks again!\n. Thanks, Brian, for this PR!\n(And sorry for taking so long to look at it.)\nUnfortunately, as the discussion in https://github.com/spray/spray/issues/993 revealed, this fix is not yet good enough.\n\nWe just discussed the solution that we'd like to apply to this problem:\n1. Try to call `InetSocketAddress::getHostString` per reflection. If this succeeds all is well.\n2. If the reflection call fails (probably because the user is still on JDK 6) we fall back to the current behavior.\n\nThis should allow us to make use of the better API in JDK7+ while not giving up on JDK6 altogether.\n\nWould you like to take a stab at this solution? \n. Just `git push -f` into your branch to update this PR.\nThat'll be fine.\nThanks!\n. Thanks for this work, Brian! Great analysis to demonstrate the effectiveness of the fix!\n. If you push -f into your branch and replace the existing commit the notes will still be available.\nThey will just be folded. So, no worries.\n. Thanks, Brian, this looks good!\nI'll wait until @jrudolph has seen this as well and then push the `merge` button.\n. Ok, Brian, I am ready to merge now but realized that you still need to accept the [Typesage CLA](http://www.typesafe.com/contribute/cla/). It's a purely online process that should only take about 30 secs or so.\nWould you be able to do this?\nIdeally we'd like to publish a new release today and it'd be great, if we can get this fix in.\nThanks again!\n. Excellent.\nThanks, Brian!\n. Have you tried using a `RawHeader`?\n. Spray has a proper model for the `OAuth2BearerToken` and the character chain `Bearer` is not actually part of the token you specify. We adhere to the spec and capitalize the `B`.\nIt appears as if your server has a faulty implementation here.\n. We don't rewrite any header. We simply _write_ it correctly.\nThe `Authorization` header has a proper model for the `HttpCredentials` that is on a higher level of abstraction than a String/String header design. \n. `Could not write response part MessageChunk(Bytes(),)` means that you are trying to write a chunk before the previous chunk has been properly dispatched to the network.\nThis points to a problem in your handling write ACKs. Only when you have received the ACK for the previous chunk are you allowed to send the next one.\n. Ok, glad you got things sorted.\n. Thanks for this PR (and your excellent SO answer!)\nHowever, I'm not sure adding a very rigid `favicon` directive like the one you propose here really is inline with spray's general \"not-a-framework\" spirit.\nWe want to give users the tools to build exactly the functionality they need, not provide some pre-configured frame that an application is somehow dropped into.\n\nSo, if one want to provide a favicon this is really easy to do. I can choose which file I want to serve, from the class path or the file system, which media-type, etc.\nYour directive already makes a lot of decisions for me, which we generally do not want to do.\n\nSo, I'm inclined to leave things as is, i.e. _not_ provide a predefined favicon directive, because adding favicon support is already really easy and another directive would probably cause more problems than it solves.\n\nHowever, what is probably a good idea is to add a section to the documentation, explaining the different options one has for adding favicon support oneselves.\n. I don't consider this a bug.\nAfter all the server forcibly closing the connection should result in error messages on the client.\nNot using keep-alive connections for non-idempotent requests, just for reducing the likelihood of errors in case the server shuts down, doesn't appear to be a good trade-off in my view.\n. Assigning to akka-http. We won't add this feature to spray anymore.\n. Moving to Akka HTTP.\nI don't think this issue affect enough users to still warrant a somewhat involved fix the requires changes to the outside API in spray.\n. We already received a good PR for this in spray, which unfortunately never made it into the codebase:\nhttps://github.com/spray/spray/pull/372\n\nHowever, a lot of the work there should be highly relevant to closing this ticket for Akka HTTP.\n. Done. Upgrade to _spray_ 1.3.3 and use the _spray-routing-shapeless2_ module instead of _spray-routing_.\n. [RFC 2616 Section 3.7.1](http://tools.ietf.org/html/rfc2616#section-3.7.1) defined the rules for determining the charset for text content as follows:\n\n> The \"charset\" parameter is used with some media types to define the\n> character set (section 3.4) of the data. When no explicit charset\n> parameter is provided by the sender, media subtypes of the \"text\"\n> type are defined to have a default charset value of \"ISO-8859-1\" when\n> received via HTTP. Data in character sets other than \"ISO-8859-1\" or\n> its subsets MUST be labeled with an appropriate charset value.\n\nThe latest HTTP spec has changed this special status of `ISO-8869-1`. [RFC 7231 Appendix B. Changes from RFC 2616](http://tools.ietf.org/html/rfc7231#appendix-B) says:\n\n> The default charset of ISO-8859-1 for text media types has been\n> removed; the default is now whatever the media type definition says.\n\nUnfortunately, for many media types the definition doesn't really \"say\" anything.\nLooking at the state of the specs and the current spray implementation I'd be inclined to simply change the `ISO-8869-1` default in spray to `UTF-8` as it is almost always the right choice.\nLooking at the code this change doesn't appear to be hard to do.\n\n@jrudolph What do you think about changing the default from `ISO-8869-1` default in spray to `UTF-8`?\n. Ok, true. I can always map the request Content-Type and attach a custom `charset=...` parameter if I really need to. So I agree with you in not touching this rather basic logic in spay anymore.\nClosing.\n. Closing as the fix was reported \"effective\" by the customer.\n. Thanks for this, @mpilquist!\nGood to see that so few changes are required for the update.\n. Thanks, Michael, for this PR!\nIt really did help with making the upgrade to shapeless 2.1 a breeze.\nIf you upgrade to _spray_ 1.3.3 and use the _spray-routing-shapeless2_ module instead of _spray-routing_ things should fly nicely with shapeless 2.1.\n. Looks like this ticket can be closed, right?\n. Ok, cool.\n. No one from our side is currently working on this.\nSo it'd be great you picked this up...\nThanks in advance!\n. We had a discussion in https://github.com/akka/akka/pull/16954 about this question (click on `Show outdated diff`). Essentially it comes down to this sentence from [this section of RFC3986](http://tools.ietf.org/html/rfc3986#section-6.2.3):\n\n> Normalization should not remove delimiters when their associated\n>    component is empty unless licensed to do so by the scheme specification.\n\nIf the case you highlighted the URI has a scheme (`g`), no authority and a \"rootless\" path (`path-rootless`). The patch in https://github.com/akka/akka/pull/16954 always renders the `//` delimiter for empty authorities, if the scheme is not empty or `mailto`. As your example shows, this is not quite good enough. In your example we are not allowed to render the `//` delimiter because this would turn what is a rootless path into a host! So we need to apply a small fix to https://github.com/akka/akka/pull/16954, which prevents the rendering of the `//` delimiter if the path is rootless.\n\nSo, to recap, we need to render the `//` between a non-empty scheme and an empty authority exactly when these conditions are met:\n1. the scheme is not `mailto` (potentially more exception schemes will be added here later)\n2. the path is empty or starts with a slash (not a segment)\n\nSo, thanks for bringing this up as it shows a flaw in https://github.com/akka/akka/pull/16954 that we need to fix. After getting this right in spray, would you also want to put it an Akka PR with the correction (and maybe some more tests that we are apparently missing in Akka)?\n. If you type a search term you'll see a flyout with the respective pages.\nThere is no real \"search results\" page.\n. That's a good idea! Thanks!\n. @matsluni Sweet! This is great.\nThere is only one small problem I just noticed: With the current implementation, when I move the mouse cursor over the results list (or press up/down arrow), I get a colored background bar behind the selected list item. With your solution I somehow don't. Can this be fixed?\n. Cool! Thanks a lot!\n. Nice. Works great for me.\nDo you want to put in a proper PR?\nI could also just cherry-pick your commit... whatever you prefer.\nThanks a lot in any case!\n. Yes. Not being able to represent empty responses with a Content-Type is a limitation of spray's `HttpEntity` model that is fixed in akka-http. In Akka HTTP you can create empty responses with a Content-Type.\nHowever, in spray we will not change this anymore as the codebase is now in stable maintenance mode where we do not want to introduce breaking changes if at all possible.\n. Thanks Julian.\nWe'll merge as soon as we can.\n. Yes, thanks!\nCould you accept the [Typesage CLA](http://www.typesafe.com/contribute/cla) (only takes 30 seconds) and change the commit message to\n`= docs: fix copy-paste error` in accordance with our [git commit message policy](http://spray.io/project-info/contributing/#git-commit-messages)?\n. Excellent. Thanks, Joe!\n. What spray version are you reporting this against?\n. Ok, thanks.\nThe problem is interesting. It lies in the `toRelative` URI transformation we do when forwarding the request to the correct `HttpHostConnector`. Paths starting with a double-slash are not legal without an authority part, so we cannot actually strip off the authority, i.e. convert into a relative URI.\nThis means that, for these kinds of URIs, we need to add a `Host` and _keep_ the URI in absolute form.\n\nI just confirmed that `curl` actually does things wrong here. If you do a `curl -I http://spray.io//foo` it will send this request:\n\n```\nHEAD //foo HTTP/1.1\nUser-Agent: curl/7.30.0\nHost: spray.io\nAccept: */*\n```\n\nwhich is incorrect, because `//foo` is actually an absolute request without scheme to the root of host `foo`.\n. I do need to correct my comment from above here.\nCurl does in fact do things right and we parse request targets incorrectly.\nSee https://github.com/akka/akka/issues/17057 for more info.\n. @wjur Don't worry about the failing test. Some of our tests are a bit shaky.\nI'll look into your changes as soon as my time permits. Thanks a lot again!!\n. This looks great, thank you @wjur!\nMaybe one more small tweak and we can merge.\n. Ok, thanks. This LGTM.\n@jrudolph ?\n. Ok, great.\nNow we'd need a PR in Akka, against branch `release-2.3-dev` containing the fix to the fix.\nYou said you also would be able to do this, @wjur ?\n. Great, thanks!\nYou'll have to accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla/) thought, before we can merge. Sorry for the trouble, it only takes 30 secs!\n. And done. Thanks again!\n. The reason is that our main development master branch is still on Akka 2.1.\nThis won't change anymore in spray.\n. How do you unmarshal your JSON?\nThe `SprayJsonSupport` already appears to be doing what you want:\nhttps://github.com/spray/spray/blob/master/spray-httpx/src/main/scala/spray/httpx/SprayJsonSupport.scala#L36\n. Yes, that's right. If you use any of the existing JSON support facilities you should have to write less code and things will be decoded properly.\n. Great. I'll give it a spin on \"my\" JDKs and report back.\n. The `spray-httpx` tests are all green for me on Apple JDK 6 and Oracle JDKs 7 and 8 under OS/X and the fix looks good for me. I don't think switching to an explicit xerces dependency is necessary at this point. Do we have any indication that a relevant JDK _doesn't_ support the XMLConstants in question?\n. Thanks, James!\nI think there is value in keeping the approaches in Play, spray and Akka HTTP as closely aligned as possible. For one because the one in Play is already well tested and somewhat matured in the field and secondly because we then save the mental overhead of managing several competing solutions.\n. Yes, indeed. Thanks for digging these up!\n. Fixed.\n. Yup.\n. Please direct your questions to the [spray mailing list](https://groups.google.com/forum/#!forum/spray-user) as we use the github ticketing system only for issue tracking.\n. Nice. LGTM!\n. I just took a quick look at your test project (thanks for that!).\nThe problem only appears under Scala 2.10.5 and with `spray-routing-shapeless2`.\nUnder 2.11 or with the plain `spray-routing` module is doesn't appear.\n(Also, your test project is missing the explicit dependency onto akka, but that is unrelated.)\n\nI just added one more test exercising the exact same API:\nhttps://github.com/spray/spray/commit/340614c9af7facf6a8b95bf4cc7733e0fca9299e\nand that compiles fine. So something else appears to be going on.\nThanks for the report!\n. > Alright, so that sounds like it might be a compiler regression from 2.10.4 to 2.10.5?\n\nI haven't checked, whether the problem doesn't also appear under Scala 2.10.4.\n\n> Is there anything I could help look at?\n\nIf you could try to further isolate the problem and find out why our test compiles fine while your test project doesn't then that would certainly help us.\n. Ok, great.\nThanks for digging this up and reporting back!\n. Thanks for this pointer to Caffeine!\nWe'll keep this ticket open as a reminder to look into upgrade options when the Java 8 time has come.\n. Any logic acting on potentially malicious input should apply hard bounds to everything.\nEspecially if it, like spray, doesn't run in a true streaming mode. (By default, spray fully buffers message entities.)\n\n> And why 8M as a default ?\n\nWhat default value would you suggest instead?\n. The relevant spec for the `WWW-Authenticate` header is http://tools.ietf.org/html/rfc7235#section-4.1.\nRFC 7235 defines `auth-param` as such:\n\n```\nauth-param = token BWS \"=\" BWS ( token / quoted-string )\nauth-scheme = token\nchallenge = auth-scheme [ 1*SP ( token68 / [ ( \",\" / auth-param ) *(OWS \",\" [ OWS auth-param ] ) ] ) ]\n```\n\nSo the param value can be either a `token` or a `quoted-string`.\n\nWhat client do you have an issue with?\n. Ok, thanks for the detailed analysis and report, Csongor!\nLooks like a valid improvement request.\n\nWould you like to submit a PR?\n. Thanks for the PR. Looks good!\nRegarding the CLA, it looks like you already accepted it online with your GH user account, which is all we need. Do you need the corporate CLA nevertheless?\n. Excellent. Thanks, Csongor!\n(Fixes #1041)\n. IIUC the current implementation and the documentation is correct.\n\nThe example supports both chunked as well as unchunked requests and handles both via the \"chunked\" path. In that regard you are correct, _if_ the request is unchunked then chunking is emulated to be able to reuse the code path for chunked requests.\n\nConcerning the difference between `request-chunk-aggregation-limit` and `incoming-auto-chunking-threshold-size`: these two are two different things.\nThe former makes chunked requests appear as unchunked requests while the latter does the opposite.\n\nIf you've been testing with curl: Have you set an explicit `--header \"Transfer-Encoding: chunked\"` CLI option? Otherwise curl won't produce a chunked request.\n. > if request-chunk-aggregation-limit = 0 then the handler actor sees the MessageChunks, meaning the request is chunked.\n\nWhether the request is chunked or not is decided by the _client_. By \"chunked request\" we mean a request that was sent with `Transfer-Encoding: chunked` (and no `Content-Length` header).\nspray supports \"virtual chunking\" (via `incoming-auto-chunking-threshold-size`) as well as \"virtual unchunking\" (via `request-chunk-aggregation-limit`).\n\nMaybe the misunderstanding stems from a different interpretation of what \"chunked\" means?\n. Indeed. That is of course of problem.\nUnconditionally copying over the headers for the new request, however, will also not be good.\nI guess we'll need some smarter logic to decide, which headers can be \"redirected\" and which can't.\n. The way I understand it is that 2.2.2 fixes a bin compat issue that 2.2.1 had with 2.2.0.\nHowever, since spray is still on 2.1.0 we still need to upgrade to 2.2.0 and publish a new release, right?\nOr is 2.2.2 also binary compatible with 2.1.0?\n. A spray release takes up at least half a day of work for one of us, so we usually delay things until a release really is necessary. So, it'd be interesting to understand whether the latest spray release is indeed compatible with spray 2.2.2 or not.\n. Cool. That's great!\nI'll keep the ticket open for some time longer just in case.\n. How are you streaming the response?\nAre you using ACKed writes?\n. I agree that the behavior of `pathSuffix` is not the most intuitive.\nHowever, it behaves as \"specified\".\nThe scaladoc specifically says:\n\n```\nNote that, for efficiency reasons,\nthe given PathMatcher must match the desired suffix in reversed-segment order, ...\n```\n. Ah, I see.\nYeah, that's really not that great.\nMaybe we need to indeed bite the bullet and implement a \"proper\", even if slow, suffix match.\n. Nice, thank you for this patch!\n\nBefore being able to merge it we'd need to accept the [Typesafe CLA](http://www.typesafe.com/contribute/cla), which can easily be done online.\n. Thanks, Bernard!\nThis should indeed go in!\n. There are quite a few threads on the ML regarding this topic, but you are right.\nA more prominent addition to the website/documentation is probably called for.\n. Cool. Thank you, Alexander!\n. Nice! Thanks a lot, Johannes.\nThis definitely make the release much easier...\n. @ktoso I'll happily push an update to the website once we have an official wording for the deprecation notice.. LGTM!. ",
    "mb0": "That is a good compromise. I should have thought of reordering before \nasking pedantic questions...\n\nThank you very much for this nice project.\n. That is a good compromise. I should have thought of reordering before \nasking pedantic questions...\n\nThank you very much for this nice project.\n. ",
    "waywardmonkeys": "You should consider using this:  https://github.com/codahale/metrics ... it is great.\n. We're using Metrics extensively in Actor-based code ... would be really nice to just have this in place. There's really not a big need to replace with something new (and then we'd have to cook up new integration code for Graphite and Nagios and ...)\n\nI'd be happy to assist with a patch to get some of this rolling.\n. I might have time to show the metrics stuff soon ... what would you want to see?\n\nMetrics talks to Graphite and Ganglia (and JMX).  If you want real stats, you'll end up needing a lot of what Metrics provides.\n. Oh, I should really update the example akka conf files as well.\n. You should consider using this:  https://github.com/codahale/metrics ... it is great.\n. We're using Metrics extensively in Actor-based code ... would be really nice to just have this in place. There's really not a big need to replace with something new (and then we'd have to cook up new integration code for Graphite and Nagios and ...)\n\nI'd be happy to assist with a patch to get some of this rolling.\n. I might have time to show the metrics stuff soon ... what would you want to see?\n\nMetrics talks to Graphite and Ganglia (and JMX).  If you want real stats, you'll end up needing a lot of what Metrics provides.\n. Oh, I should really update the example akka conf files as well.\n. ",
    "dpratt": "I've been playing a lot with Metrics lately - I'd be happy to take a first pass at this. Off the top of my head, I can think of a few directives that would be handy\n- A directive that acts as a timer so that you can get aggregate performance data on your app\n- A directive that acts as a simple counter, so you can get counts\n\nWe might want to think about instrumenting the internals of spray with some handy metrics as well - I don't think keeping track of simple rejections would be useful (since I suspect that 90% of them would be red herrings), but certainly logging things like timeouts, completely rejected requests and the like would be interesting to have available.\n. One interesting thought on this - there might be cases in which we might not want to bypass the unmarshalling framework entirely for a non-200 result.\n\nI'm thinking of cases where services return JSON of a particular format for 200 requests, but then an 'error' format for non 200 requests. It would be interesting to perhaps have a way to perhaps unmarshal that JSON and perhaps either attach it to the thrown exception.\n. I think it's important to make a point here - there's no reason that you can't use Java in a spray project. Like Mathias mentions above, spray itself is only a scala API, but you only have to use spray to build your routing logic. There is nothing about the framework that requires you to write your processing/business logic in Scala, and in fact, I've done a few projects (mainly with people not familiar with Scala) that do just this. You will have a small amount of scala in your codebase, but not terribly much, and to be honest, Scala written in an imperative style is essentially close enough to Java that an experienced Java coder can immediately read and understand it, and have the ability to make changes even after only a day or so of exposure. Note - I'm not talking about using Scala's functional parts, just basically writing Java code with scala syntax.\n\nIf you use maven on your project, you can even use the maven-scala-plugin to handle compilation and it's a drop in for your existing workflow. Basically, you'd add two scala files to your project -\n- A Boot file that gets loaded by the Spray servlet connector that initializes akka and spray\n- A class that builds your route. In this route, you'd have a simple, nice easy to read layout of all your routing logic, and you delegate to Java code at the leaves of the routing tree to process and produce your output.\n\nYou build all of this using WAR packaging with maven, and it works like any other webapp you've ever built.\n\nThat's it. Really, it's all you need - take a look at the simple spray example with jetty in the examples folder here at GitHub. You can even use that as a boilerplate. I'm willing to bet that you'll likely never touch Boot.scala, ever, and you'll only ever edit YourService.scala when you want to add a new route.\n. It's no big deal if this doesn't get pulled into 1.1 - we have it in our codebase, but it'd be nice if it was, since I'm itching to remove it's implementation from our custom stuff.\n. Already fixed.\n. +1 from me as well - this is the hardest thing I have to explain to new Scala people when they start working with Spray. It's not entirely obvious initially how routes are constructed, and it removes one more mental roadblock.\n\nI also like that you can do the following - \n\n``` scala\nRoute { ctx =>\n  //do whatever you want with the context\n}\n```\n\nAt this point it looks like usage of Route.apply() essentially stops the ability to compose Routes, and I don't necessarily think that's a bad thing. The assumption is that you at that point have very custom code or stuff that doesn't fit well into a route structure. Perhaps you want to package up completion closure by yourself (despite the fact that Spray can do it for you) or you have existing Java/Scala code that can take a request context and construct and send a response outside of the route structure itself.\n. I've been playing a lot with Metrics lately - I'd be happy to take a first pass at this. Off the top of my head, I can think of a few directives that would be handy\n- A directive that acts as a timer so that you can get aggregate performance data on your app\n- A directive that acts as a simple counter, so you can get counts\n\nWe might want to think about instrumenting the internals of spray with some handy metrics as well - I don't think keeping track of simple rejections would be useful (since I suspect that 90% of them would be red herrings), but certainly logging things like timeouts, completely rejected requests and the like would be interesting to have available.\n. One interesting thought on this - there might be cases in which we might not want to bypass the unmarshalling framework entirely for a non-200 result.\n\nI'm thinking of cases where services return JSON of a particular format for 200 requests, but then an 'error' format for non 200 requests. It would be interesting to perhaps have a way to perhaps unmarshal that JSON and perhaps either attach it to the thrown exception.\n. I think it's important to make a point here - there's no reason that you can't use Java in a spray project. Like Mathias mentions above, spray itself is only a scala API, but you only have to use spray to build your routing logic. There is nothing about the framework that requires you to write your processing/business logic in Scala, and in fact, I've done a few projects (mainly with people not familiar with Scala) that do just this. You will have a small amount of scala in your codebase, but not terribly much, and to be honest, Scala written in an imperative style is essentially close enough to Java that an experienced Java coder can immediately read and understand it, and have the ability to make changes even after only a day or so of exposure. Note - I'm not talking about using Scala's functional parts, just basically writing Java code with scala syntax.\n\nIf you use maven on your project, you can even use the maven-scala-plugin to handle compilation and it's a drop in for your existing workflow. Basically, you'd add two scala files to your project -\n- A Boot file that gets loaded by the Spray servlet connector that initializes akka and spray\n- A class that builds your route. In this route, you'd have a simple, nice easy to read layout of all your routing logic, and you delegate to Java code at the leaves of the routing tree to process and produce your output.\n\nYou build all of this using WAR packaging with maven, and it works like any other webapp you've ever built.\n\nThat's it. Really, it's all you need - take a look at the simple spray example with jetty in the examples folder here at GitHub. You can even use that as a boilerplate. I'm willing to bet that you'll likely never touch Boot.scala, ever, and you'll only ever edit YourService.scala when you want to add a new route.\n. It's no big deal if this doesn't get pulled into 1.1 - we have it in our codebase, but it'd be nice if it was, since I'm itching to remove it's implementation from our custom stuff.\n. Already fixed.\n. +1 from me as well - this is the hardest thing I have to explain to new Scala people when they start working with Spray. It's not entirely obvious initially how routes are constructed, and it removes one more mental roadblock.\n\nI also like that you can do the following - \n\n``` scala\nRoute { ctx =>\n  //do whatever you want with the context\n}\n```\n\nAt this point it looks like usage of Route.apply() essentially stops the ability to compose Routes, and I don't necessarily think that's a bad thing. The assumption is that you at that point have very custom code or stuff that doesn't fit well into a route structure. Perhaps you want to package up completion closure by yourself (despite the fact that Spray can do it for you) or you have existing Java/Scala code that can take a request context and construct and send a response outside of the route structure itself.\n. ",
    "velvia": "I'm in the middle of instrumenting our app, so I'll have a stab at this in the next few days.  How about these directives?\n\n``` scala\ncount(couterName: String) {  }        // Adds a simple counter for every request that comes in\ncountAll(metricPrefix: String) {}    // Adds request counter, meter for request handler (frequency + duration), and counter for responses by type (200, 3xx, 4xx, 5xx)\n```\n\nMeasuring the duration of Futures and things like that will be pretty tricky though.\n. Sure, but the directives operate at a level of request and response, right? So we can have primitives but they must be at that level, eg\n\nRequestCounter\nResponseCounter\nRequestMeter\nInnerRouteDurationHistogram\nLastErrorGauge\n\nEtc\n\nOn Dec 20, 2012, at 2:09 AM, Mathias notifications@github.com wrote:\n\n> I'd say we first try to get the basic metrics abstractions (gauges, counters, histograms, etc.) mirrored by directives and then build more complex things from the basics.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. Go ahead, don't think anybody has started yet. \n\n-Evan\nCarry your candle, run to the darkness\nSeek out the helpless, deceived and poor\nHold out your candle for all to see it\nTake your candle, and go light your world\n\nOn Dec 31, 2012, at 10:44 AM, Curtis Carter notifications@github.com wrote:\n\n> If someone already has the ball rolling on this get a link out. I'd like to work on it.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. @jrudolph any comments?\n. @sirthias Thanks.  CLA accepted, and commit message changed.\nI may add an ordered keys access API as well (the first commit is unordered but O(1) keys, this one would be ordered by access time).\n. I'm in the middle of instrumenting our app, so I'll have a stab at this in the next few days.  How about these directives?\n\n``` scala\ncount(couterName: String) {  }        // Adds a simple counter for every request that comes in\ncountAll(metricPrefix: String) {}    // Adds request counter, meter for request handler (frequency + duration), and counter for responses by type (200, 3xx, 4xx, 5xx)\n```\n\nMeasuring the duration of Futures and things like that will be pretty tricky though.\n. Sure, but the directives operate at a level of request and response, right? So we can have primitives but they must be at that level, eg\n\nRequestCounter\nResponseCounter\nRequestMeter\nInnerRouteDurationHistogram\nLastErrorGauge\n\nEtc\n\nOn Dec 20, 2012, at 2:09 AM, Mathias notifications@github.com wrote:\n\n> I'd say we first try to get the basic metrics abstractions (gauges, counters, histograms, etc.) mirrored by directives and then build more complex things from the basics.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. Go ahead, don't think anybody has started yet. \n\n-Evan\nCarry your candle, run to the darkness\nSeek out the helpless, deceived and poor\nHold out your candle for all to see it\nTake your candle, and go light your world\n\nOn Dec 31, 2012, at 10:44 AM, Curtis Carter notifications@github.com wrote:\n\n> If someone already has the ball rolling on this get a link out. I'd like to work on it.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. @jrudolph any comments?\n. @sirthias Thanks.  CLA accepted, and commit message changed.\nI may add an ordered keys access API as well (the first commit is unordered but O(1) keys, this one would be ordered by access time).\n. ",
    "ccarter": "If someone already has the ball rolling on this get a link out.  I'd like to work on it.\n. If someone already has the ball rolling on this get a link out.  I'd like to work on it.\n. ",
    "erikvanoosten": "Hi people, have you looked at https://github.com/erikvanoosten/metrics-scala ?\n. Hi people, have you looked at https://github.com/erikvanoosten/metrics-scala ?\n. ",
    "duncancrawford": "Hi, I was wondering if this is on your roadmap and if so any idea when we might see the implementation?\n. Hi, I was wondering if this is on your roadmap and if so any idea when we might see the implementation?\n. ",
    "jrudolph": "Superseded by akka/akka#16858\n. Superseded by akka/akka#16847\n. Won't be fixed in spray. Superseded by akka/akka#16846\n. Thanks @2beaucoup. Implemented by #797.\n. This is currently out of scope for spray and akka-http. If this is needed it will need to be provided on a higher-level.\n. The most basic way of making use of an HTTP proxy would be:\n- don't send the request over a connection to the original server but send it on a connection to the proxy\n- rewrite the request URL to be absolute\n\nI think this is already enough for some scenarios the question is more where to put that logic exactly in the existing infrastructure.\n. The main architectural challenge I see is that we will have different kind of \"transports\" in the future:\n- what is now the HostConnector which is now a connection pool of persistent HTTP connections to one (DNS) host, where each connection can handle just one request concurrently\n- a proxy which usually lies on a level above the HostConnector because in theory you need only one connection pool to the proxy to connect to a series of host\n- pipelined HTTP or SPDY which allow to have just one connection per DNS host but multiple requests in parallel\n\nThat said, if you find a non-intrusive way to make some proxy support happen now (e.g. just by rewriting the host to connect to and the request URL) without the need for any big architectural changes, I think we would include it now even if it is not as efficient as it could be with a more advanced future design.\n. And btw. thanks for your offers for help, @agemooij and @2beaucoup, this is much appreciated!\n. > One more question: Why is HttpManager making all URIs relative (code)?\n\nI think it's because this information is not needed any more once you have a HostConnector who already knows the target host/port.\n\nRegarding your approach: I think the least intrusive way for getting proxy support now is not fussing with the HostConnectorSetup at all but just patching the `HttpHostConnectionSlot` to \n1. rewrite the connection target to the proxy server when doing a `Connect`\n2. rewrite the request URI directly before sending a request to be absolute by using the configured host/port of the slot\n\nAs said before, this is not efficient because proxy connections are not managed globally but a better design will be much more intrusive.\n\nAlso, this won't work for SSL connections which need a different pipeline underneath which would have to be implemented first.\n. @2beaucoup, you are not forgotten! We are slowly getting up to speed from vacations but we hope to get to wading through the stack of PRs starting from next week.\n. I discussed this with Mathias and we came to the conclusion that we should be able to support proxies even more completely than sketched before with relatively little changes.\n\nThe idea is this:\n- the lowest level connection-level API does not handle automatic proxy support, but can be used to connect to a proxy and communicate with it\n- same for the `HttpHostConnector` implementation which should make no assumptions about whether the target is a proxy or not\n- the question is then how to obtain a HttpHostConnector based on the proxy configuration\n  - if proxy **is not** configured, a HostConnectorSetup is answered as currently; by pointing the setup\u2019s host and port to a proxy, you get a HttpHostConnector which sends requests to the proxy\n  - if a proxy **is** configured, the HttpManager will internally create a HttpHostConnector for the proxy but return a ProxiedHostConnector whose sole purpose is to rewrite requests accordingly (relative to absolute URI) and relay them to the internal HttpHostConnector for the proxy\n- we need an additional setting in HostConnectorSetup that specifies that you want to explicitly ignore proxy settings\n- proxy settings can be given in application.conf and default to `-Dhttp.proxyHost/Port` etc\n- to support headers like proxy-authentication automatically it would be useful if HostConnectorSetup would support a field to add custom headers to each request.\n\nExample how to obtain a host connector based on your proxy preference:\n- use global setting\n  \n  ``` scala\n  IO(Http) ? HostConnectorSetup(\"example.org\", 8000)\n  ```\n- don't use proxy\n  \n  ``` scala\n  IO(Http) ? HostConnectorSetup(\"example.org\", 8000, connection = Direct)\n  ```\n- override proxy\n  \n  ``` scala\n  IO(Http) ? HostConnectorSetup(\"proxy.example.org\", 8000, connection = Direct)\n  ```\n\nThe default value for `connection` would be `AutoProxied` which would enable the behavior as sketched above. WDYT?\n. > @jrudolph: Could you re-check your last two examples? They seem identical.\n\nYes, they are by purpose. With `connection = Direct` you always connect directly to the given endpoint, ignoring any proxy settings. This way you can use it both to create a host connector for a direct connection to the target host as well as to create a host connector to connect to a proxy (which you can then send requests to other targets).\n. > What would happen if I send a relative requests without a Hostheader to this connector? One advantage of having proxy info in the HttpHostConnector would be the ability to fail early in this case.\n\nI agree, a HttpHostConnector should maybe know if it can handle relative urls or not to fail fast. And related, we should specify which component is responsible for setting Host-headers / relativize URLs.\n. Kevin, was this pull request intended? What is it about?\n. Ah, of course. I was mislead because you (or github) set spray:master as the target branch for the pull request.\n. I think the first point of compiling without sbt is a separate concern. (spray is a normal maven dependency so you should be able to use any java build tool and reference spray and akka, we don't support this officially but in cases of questions like this try asking on the mailing list.)\n\nThe upcoming (and IIRC the current) version of spray should be able to support Servlet 3.0 compliant servlet containers in general. We aim to provide support for those containers by adhering to the standards and using container features to run a spray application on top of this. It seems that Jboss 6 supports servlet 3.0, so it already might be possible to use spray with JBoss 6. We don't use JBoss so if you need more information how to set this up exactly your best bet is to ask for this on the mailing list where someone may have tried a similar setup.\n. This is currently out of scope for spray or akka-http but may be easily provided as a contrib module.\n. Thanks for the analysis, @nartamonov. I agree with you and spray websocket support will enable other higher-level solutions like the ones you are describing. We will first focus on websocket support itself because that's something that has to live in spray(-can) itself. More higher-level abstractions on top may then be provided by spray itself or can also be built by third-party extensions.\n\nWould you create another issue for the high-level solution?\n. We've considered this request and found that we need more evidence that anyone would find this useful.\n\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n\nJust joking... :D The time may have finally come that we are actually going to implement this as part of akka-http. Here's the new ticket: akka/akka#16848 :fireworks:\n. @matanster, yes, I was trolling. I hope you didn't take offence.\n. Closing for spray. See the  akka/akka#16848 for the most recent developments.\n. Closing this for now as we won't implement this for the spray site and for akka-http it is a concern of Akka's documentation infrastructure.\n. Superseded by akka/akka#16845\n. I guess it's about transparently relaying a request to another HTTP server which will then answer it.\n. Superseded by akka/akka#16844.\n. This hasn't been requested for a long time and is also easy to provide in a custom fashion both for spray and akka-http so it won't be fixed.\n. We are considering this. However, it's not just migrating to \"use json4s\" because json4s is already much more than just an AST (which we would use).\n. Closing. spray-json issues will be discussed in the spray-json repo.\n. spray won't have it's own IO layer in the future and instead use the new akka IO which already has support for UDP. See http://doc.akka.io/docs/akka/2.2-M1/scala/io.html\n. Closing... while this would have been very useful it probably won't happen for spray any more. Things like this need to be considered as part of a general documentation effort for akka-http.\n. This would be a neat feature but would play a bit against the general view of spray to represent HTTP things explicitly in the routing definition. This also hasn't been requested for a long time, so closing for now...\n. Currently not planned.\n. Workaround for the example:\n\n``` scala\nobject Main extends App with SimpleRoutingApp {\n  def route =\n    get {\n      path(\"\") {\n        redirect(\"/hello\")\n      } ~\n        path(\"hello\") {\n          respondWithMediaType(`text/html`) { // XML is marshalled to `text/xml` by default, so we simply override here\n            complete {\n              <html>\n                <h1>Say hello to <em>spray</em> on <em>spray-can</em>!</h1>\n                <p>(<a href=\"/stop?method=post\">stop server</a>)</p>\n              </html>\n            }\n          }\n        }\n    } ~\n      (post | parameter('method ! \"post\")) {\n        path(\"stop\") {\n          complete {\n            system.scheduler.scheduleOnce(Duration(1, \"sec\")) {\n              system.shutdown()\n            }\n            \"Shutting down in 1 second...\"\n          }\n        }\n      }\n\n  startServer(interface = \"localhost\", port = 8080)(route)\n}\n```\n. For the record the error in question is\n\n```\n [error] (simple-routing-app/compile:compile) java.lang.Error: symbol value $buf$1 does not exist in spray.examples.Main$delayedInit$body.apply\n```\n. Not reproducible any more. Also, since the documentation examples are now all compiled for all of 1.0/1.1/1.2 we should notice in future.\n. You have to add \n\n```\nscalacOptions += \"-Ydependent-method-types\"\n```\n\nto your build definition when using Scala 2.9. See http://spray.io/documentation/spray-routing/installation/\n\nI've repurposed the ticket to remind us of improving the documentation on the example page.\n. This has happened for akka-http and is currently not planned for a maintenance release of spray. Closing for now.\n. FYI, I think #281 is the meta-issue for this one. And no, unfortunately no news yet.\n. Here's an additional idea to take into account by @fedesilva:\n\n> trigger request streaming for any request that has multipart form data, not only for size thresholds.\n. Thinking more about how this would be implemented, I think there's still a conceptual piece missing:\n\nCurrently, regardless of whether chunking is used on the connection or not, spray-can won't sent message chunks to the user-level handler but instead will automatically aggregate chunks into complete requests, i.e. exactly the opposite functionality we are trying to build here. You can switch it off (`spray.can.server.request-chunk-aggregation-limit = 0`) but this is not what you would usually would want to do for your complete server.\n\nOne possible way could be to switch off request-chunk-aggregation per default in spray-can and instead improve spray-routing to support chunking. A related question is how you would distinguish multiple concurrently incoming request chunks in a HttpService when one request spans over multiple messages from spray-can.\n\nDoes anyone have pointers about information in which way chunking / streaming is supported in other webservers?\n. [netty's handling](https://github.com/netty/netty/blob/master/codec-http/src/main/java/io/netty/handler/codec/http/HttpObjectDecoder.java#L58) seems to be similar to the one implemented for spray-can in the above commit.\n. Maybe it's something similar to what triggered the change here:\n\nhttps://github.com/spray/spray/commit/ee33b340e6850e38de371a046a1ea908ab1fe7cd#L1L440\n\nUnfortunately, I can't remember how we triggered the condition back then. It seems it should only happen for unconnected connections but I can't see how the connection could be unconnected at the place in question here.\n. Superseded by akka/akka#16855.\n. Have you solved the problem? We were seeing the same effect in the spray tests as well and I, incidentally, tried to find the problem just this morning. I'm currently suspecting that there's a race condition in Akka or the ForkJoinPool where you can get some ForkJoinPool threads not to close properly if a particular kind of work is scheduled after `system.shutdown()` was called.\n. Is this really related to jsonp? As I understand it, the problem is that if a malicious site can access a json resource with a script tag as if it was javascript code (you wouldn't do that normally) and then redefines the Array constructor, it could capture all of the json data.\n\nJsonp on the other hand _uses_ a script tag to load json content, adding `while(1)` automatically would break it.\n\nAm I missing something?\n\nAlso I'm not so sure how severe the problem still is. It seems the easiest mitigation is to always return private json data only over SSL.\n\nHere are some links but it's not totally coherent:\n\nhttp://www.sitepen.com/blog/2008/09/25/security-in-ajax/\nhttp://haacked.com/archive/2009/06/25/json-hijacking.aspx\nhttp://security.stackexchange.com/questions/7001/how-should-web-app-developers-defend-against-json-hijacking\nhttp://www.sitepen.com/blog/2008/09/25/security-in-ajax/\n. Also:\n\nhttp://stackoverflow.com/questions/16289894/is-json-hijacking-still-an-issue-in-modern-browsers\n. Superseded by akka/akka#16843\n. I think the solution in this case is just to extend from `akka.testkit.TestKitBase` instead of `akka.testkit.TestKit`. Can you try that @rbraley?\n. I'm closing this for now. If the problem persist even when using `TestKitBase` please reopen.\n. See https://groups.google.com/d/topic/scala-language/7uu0ETacJQs/discussion\n. Yes, thanks @2beaucoup. Implemented by #797.\n. Duplicate of spray/spray-json#48\n. If you set `spray.can.server.remote-address-header = on` in the configuration, spray-can will set the `Remote-Address` header from the real endpoint address. Would that be enough?\n\nSee http://spray.io/documentation/spray-can/configuration/\n\nI created #230 to track the `X-Forwarded-For` problem separately, thanks for reporting.\n. For example, googling for [`spray example`](https://www.google.com/search?q=spray+example) still points to the wiki.\n\nMaybe we could just switch off the wiki on the site but push the wiki repository somewhere so that the documentation for 0.9 isn't lost but won't be shown by google as prominently as it is now.\n. As a first step I added the deprecation note to every single page of the old wiki.\n. Gilles, I just fix one issue where the tests hang for me as well in the SslTlsSupportSpec which seemed to be one issue of the ones you were seeing.\n. The other failing tests could be due to simple timing problems when the build machine is slower than the one we are usually using.\n\nakka testkit has a [mechanism to adjust for slow testing systems](http://doc.akka.io/docs/akka/snapshot/scala/testing.html#Accounting_for_Slow_Test_Systems). Can you try using `-Dakka.test.timefactor=2` or an even higher factor when running sbt?\n. Btw, generally hanging tests and timing issue are frequent issues here. The problem is that spray/akka system usually inhibit quite some true non-determinism and it is hard to predict all the things that can go wrong on other machines. Actually, this is another reason we are glad about more third-party testing to catch more of the test (and sometimes even production) issues of this kind.\n. I just tested the timefactor locally and can reproduce the issue in HttpServiceSpec by _decreasing_ the factor to `0.1`. Consequently, by increasing it you should be able to get rid of the issues.\n. We already suspected that higher factors could have an impact as well, as there are timing dependencies which are not regulated through the `timefactor`. The more of them we discover the better.\n. I don't think hanging issues really generalize to reason about them in a broader way. The fix in b5427bc is about not to rely on any way the OS network stack might split up network data into incoming data chunks. In the failing test a 6 byte data message was delivered in 2 chunks of size 5 + 1 bytes which wasn't expected and so the test server actor crashed. The client in the test was written in blocking style and then hung because the server never answered any more.\n\nThe generalization to this problem could be \"never use blocking calls without a timeout\".\n. This is the command-line our internal jenkins uses for the nightly build:\n\n```\njava -Xmx2048m -XX:MaxPermSize=512m -Xss2m -Dnightly.build=yes -Dsbt.log.noformat=true -jar /opt/jenkins/sbt-launch-0.12.2.jar clean test:compile test publish\n```\n\nI haven't heard that this failed with memory problems so I guess there has to be another aspect to this.\n. And you are right about d658a60, we added this change considering the timing problems you've seen in travis runs.\n\nAnother thing to consider would be to limit parallelism when running tests e.g. by adding this to the sbt command line\n\n```\nsbt \"set concurrentRestrictions in Global += Tags.limit(Tags.Test, 1)\" test\n```\n\nThis should definitely remove one source of non-determinism and CPU pressure.\n. First build just finished successfully, :+1: . Thanks, Gilles.\n. This is a very low-level feature that won't be fixed in spray any more and is fixed with akka-streams in a different way. Closing...\n. We won't do this for the spray documentation any more. \n\nIn akka-http, routing isn't bound to an actor any more and therefore not a potential bottleneck across connections. So, even if the actual processing is executed in the routing thread it will only block pipelined requests on the same connection. As this is not a common use case, we may not need extra documentation for that. _If_ someone enables pipelining it should be clear that off-loading e.g. in the form of Futures is required to make use of parallelism.\n\nFor now closing as \"Won't Fix\".\n. This doesn't happen anymore (since illegal headers are now still provided via `RawHeader`). The error message is then `The resource requires authentication, which was not supplied with the request` with status code 401. Is this good enough?\n. This will get fixed as part of #486 \n. Merged as part of #536 \n. Superseded by akka/akka#16842\n. Will probably not happen for spray anymore. akka-http already now has streaming multipart support, improving support through directives may be useful.\n. Superseded by akka/akka#16841\n. Superseded by akka/akka#16840\n. The problem is that not even every OS has proper async file support. E.g. in Linux the only support for asynchronous file operations is to open files in DIRECT mode, i.e. completely circumventing the page cache which in many cases isn't what you want to do. \n\nI have come to see blocking file IO as an only small nuisance. It just doesn't make so much of a difference if you schedule 100 or 1000 IO operations at the same time because on usual hardware disk IO is so slow anyways. So, probably it suffices to create thread-pools of about a single digit number per harddisk and then use some manual queuing of disk IO jobs to get most work done efficiently.\n\nThat said: in spray we already use the zero-copy file transfer support from akka.io (using `WriteFile` which internally uses `FileChannel.transferTo`) to deliver files efficiently to the network. There's a config setting which allows setting a custom dispatcher for these copying tasks, so wrt to spray this should already handle most cases quite well.\n. If this can be supported somehow, it will come naturally by migrating akka-http's FileAndResourceDirectives to an asynchronous streaming variant. Even now, through the use of akka-streams file-access is potentially more asynchronous than in spray. Closing...\n. This would be very useful but I think it is likely it will be implemented in the scope of a more general Akka Logging/Tracing facility as e.g. (not yet) discussed in akka/akka#13817. Closing here...\n. We won't fix this in spray. In akka-http it will be naturally supported through the new streaming HttpEntity model. Closing...\n. I think this is already implemented on the one hand and on the other hand already contradicted by #401.\n. Not an issue any more: spray lives on spray pipelining, akka-http lives on akka-streams.\n. There are currently 16 occurences of `currentTimeMillis`:\n\n```\nspray-caching/src/main/scala/spray/caching/LruCache.scala:    val now = System.currentTimeMillis\nspray-caching/src/main/scala/spray/caching/LruCache.scala:  @volatile var created = System.currentTimeMillis\nspray-caching/src/main/scala/spray/caching/LruCache.scala:  @volatile var lastAccessed = System.currentTimeMillis\nspray-caching/src/main/scala/spray/caching/LruCache.scala:    lastAccessed = System.currentTimeMillis\nspray-can/src/main/scala/spray/can/client/ClientFrontend.scala:import System.{ currentTimeMillis \u21d2 now }\nspray-can/src/main/scala/spray/can/rendering/ResponseRenderingComponent.scala:    val now = System.currentTimeMillis\nspray-can/src/main/scala/spray/can/server/OpenRequest.scala:        timestamp = System.currentTimeMillis\nspray-can/src/main/scala/spray/can/server/ServerFrontend.scala:              } else openNewRequest(request, closeAfterResponseCompletion, System.currentTimeMillis)\nspray-can/src/main/scala/spray/can/server/ServerFrontend.scala:                firstOpenRequest checkForTimeout System.currentTimeMillis\nspray-can/src/main/scala/spray/can/server/StatsSupport.scala:    val startTimestamp = System.currentTimeMillis\nspray-can/src/main/scala/spray/can/server/StatsSupport.scala:      uptime = (System.currentTimeMillis - startTimestamp) millis span,\nspray-io/src/main/scala/spray/io/ConnectionTimeouts.scala:import System.{ currentTimeMillis \u21d2 now }\nspray-io/src/main/scala/spray/io/ConnectionTimeouts.scala:            if (timeout.isFinite && (lastActivity + timeout.toMillis < System.currentTimeMillis)) {\n```\n\nFrom those I think these are valid uses of `currentTimeMillis`:\n\n```\nspray-http/src/main/scala/spray/http/DateTime.scala:  def now: DateTime = apply(System.currentTimeMillis)\nspray-routing-tests/src/test/scala/spray/routing/FileAndResourceDirectivesSpec.scala:            case `Last-Modified`(dt) \u21d2 DateTime(2011, 7, 1) < dt && dt.clicks < Sys\nspray-routing/src/main/scala/spray/routing/directives/FileAndResourceDirectives.scala:    respondWithHeader(`Last-Modified`(DateTime(math.min(timestamp, System.currentT\n```\n\nThe other ones could be replaced by `nanoTime` or `scala.concurrent.duration.Deadline`.\n. There are several possible solution wrt forgetting `~`. The simplest one is: use parentheses instead of braces for subroutes. Several users have pointed out that they adopted a style for their routes where braces are only used if not otherwise possible. We'll probably adopt this style as well as it will catch those `~` composition errors in most cases.\n\nAnother possibility would be to use macros to detect situations when values of type `Route` aren't used. But this is definitively advanced territory...\n. How would you do that? There is no way to rule this out in the type-system as long as there is no effect-system. So, just using parens is probably the way to go.\n. As @sirthias said: this is fixed in akka-http. We currently won't backport these breaking changes to the spray routing DSL.\n. We should check this in akka-http but don't need to fix it in spray any more.\n. Superseded by akka/akka#16839\n. Going forward, I'd say `outgoing-auto-chunking-threshold-size` is the least important setting because this is functionality that is nice-to-have but can be implemented on the higher levels as well. \n\n`chunkless-streaming` on the client-side wouldn't be necessary for connecting against HTTP/1.1 servers as those servers are actually required to understand chunked requests, however, for compatibility reasons it would maybe still make sense to provide it as a fallback.\n\nSo in essence, I'd consider client-side `chunkless-streaming` for 1.0/1.1/1.2 and maybe postpone `outgoing-auto-chunking-threshold-size` to the next version.\n\nWDYT?\n. When #592 is merged, the only thing missing is `outgoing-auto-chunking-threshold-size` which we deemed as less important. So, I'm moving the remainder of the ticket to the next milestone.\n. @RichardBradley this is a question that came up before. You are quite right with your observations. See this [ML message](https://groups.google.com/d/msg/spray-user/YN2ocRzwhY0/0mJxOaYchi8J) and https://github.com/jrudolph/spray/commit/e707a2d7b99cd2355a76d6c2a80de040a59a59c3 for a hint at a solution with the current version of spray.\n\ntl;dr: You can use SuspendReading/ResumeReading to stop the inflow of data and it somewhat works but isn't optimal. Upcoming akka-http is going to have a much better solution (but won't be immediately available).\n. We won't implement the outgoing autochunking in spray any more. In akka-http it is naturally supported through the new streaming HttpEntity model.\n. In general, some server could rely on the URL parameters being in some order which could be destroyed by the format you are advocating.\n\nOur thinking here is: there is no real standard how URI parameters should be interpreted by a server. There are certain standards like what php is doing with the parameters etc. However, the only thing that counts is that HTML forms generate URL parameters of a certain shape. We think the most generic form which conforms to this shape (and which still leaves room for assigning a custom semantic to it) is the one suggested. \n. In which way is this related to servlets? \n\nIn the end we can always provide sensible additional options for common use cases. The question is how to model the parameters on the lowest level without losing information some possible interpretations may rely on.\n. The section from the spec above:\n\n>   Idempotent methods (Section 4.2.2 of [Part2]) are significant to\n>    pipelining because they can be automatically retried after a\n>    connection failure.  A user agent SHOULD NOT pipeline requests after\n>    a non-idempotent method until the final response status code for that\n>    method has been received, unless the user agent has a means to detect\n>    and recover from partial failure conditions involving the pipelined\n>    sequence.\n\nFrom the section about idempotent methods (http://tools.ietf.org/html/draft-ietf-httpbis-p2-semantics-22#section-4.2.2):\n\n> Request methods are considered \"idempotent\" if the intended effect of\n>     multiple identical requests is the same as for a single request.  Of\n>     the request methods defined by this specification, the PUT, DELETE,\n>     and safe request methods are idempotent.\n\nSafe methods are (http://tools.ietf.org/html/draft-ietf-httpbis-p2-semantics-22#section-4.2.1):\n\n>    Of the request methods defined by this specification, the GET, HEAD,\n>    OPTIONS, and TRACE methods are defined to be safe.\n\nSo from the common methods the complete list of idempotent methods is this:\n- `GET`\n- `HEAD`\n- `OPTIONS`\n- `TRACE`\n- `PUT`\n- `DELETE`\n. Easy to fix once we added the directives in #149\n. Cool, thanks.\n. Superseded by akka/akka#16838\n. Thanks for reporting, Gary. Do you think it is related to #298 ?\n. This won't change in spray anymore and is solved differently in akka-http(-core).\n. See also https://groups.google.com/d/msg/spray-user/8X8xBO79GDU/XtHZaxr2m8AJ\n. Here's the [section from the spec](http://tools.ietf.org/html/draft-ietf-httpbis-p2-semantics-22#section-3.1.1.1).\n. I think there are various issues around cookies that should be taken into consideration before releasing any client-side implementation, not least because of the possible security issues arising from a naive implementation.\n\nAt least these things should be taken into account:\n- paths\n- RFC 6265 has a [section](http://tools.ietf.org/html/rfc6265#section-5) about user agent requirements and how to match cookies that we should follow\n- cookie expiration \n- secure cookies must only be sent over https connections\n- cookies are logically not bound to a connection, so IMO the implementation should be at a higher level, i.e. not implemented as a pipeline stage but probably on the HttpClientSettingsGroup level. Even for a first cut the implementation on one local connection would bring much confusion because the higher levels may redistribute subsequent requests to different connections.\n. Ah, this is important information and it seems like this is indeed a bug in the rendering of the header.\n. Your stack size is likely too low. What's the `-Xss` setting you run sbt with? I'm using `-Xss2M`.\n\nThe [travis builds are green](https://travis-ci.org/spray/spray/builds/7872349).\n. Superseded by akka/akka#16837\n. Oh yes, I forgot about the tests. I started them, suspended my computer, and went home :) I think the failures are just observations of the new behavior.\n. lgtm\n. To add a bit more information to this ticket, the problem is that MultipartMarshallers automatically add a `Content-Disposition` header to each part. This is done even if a user-defined `Content-Disposition` header exists with other values.\n\nThe solution would be to look for a user-defined `Content-Disposition` header and use that one or if a user-defined one is missing use the current logic of generating the header.\n\nWDYT?\n. > Even though this is not allowed under the RFC a lot of API's count on this behavior building an \"array\" including the one I'm coding for now.\n\nAre there any publicly available API descriptions for this behavior? Any frameworks that support that style? I think it would be good to have some supporting evidence why we'd want to deviate from the RFC.\n. > That would require us start using a Seq[(String, BodyPart)]\n> instead of a Map in order to allow duplicates that would be renamed.\n\nProbably, that would be the consequence.\n\n> After that we would group the ones with the same name and rename them.\n\nWhat do you mean by \"rename them\"? When unmarshalling? How would the renaming work? Actually, since the behavior isn't standardized we should try to support in the model what's there in the wild but don't make any assumptions about how you would use it.\n. Two things:\n- The `akka.io` package is only present in spray 1.0/1.1 (where akka doesn't have that package). In spray 1.2, it lives in akka and spray doesn't have this package. Currently you seem to export and import the `akka.io` package for the spray-io module, right? Doesn't that make any problems? I think we should either \n  - add a switch into the build to use the right set of imports/exports depending on the target version (1.0/1.1/1.2)\n  - or have just the version working for spray 1.0/1.1 in this PR and then fix it when merging this to the release/1.2 branch\n- How can we test that this is working?\n. Thanks for the clarifications. It's worth a lot having these instructions here for later reference if we have to change module structure later on.\n\nManual testing is ok, I just tried it. How did you decide which packages to mark as optional in `spray-httpx`? Shouldn't spray-json be optional there as well? (That's where `pax-run` failed because I didn't yet tried the spray-json OSGI PR).\n. And, yes, maybe wait for @topping to clarify before building anything else if he has some kind of tests to check if OSGI support is working.\n. > If there are any errors due to misconfigured OSGi settings, they are printed to the console.\n\nDoes that check that classes in the bundles only reference packages as declared by their metadata? Or in other words, is there a way of checking that the declared dependency structure won't fail with runtime linking issues? (Maybe these are stupid questions but I never really got into OSGI, so please bare with me).\n. > In an OSGi environment, getClass.getClassLoader is returning the classloader of the spray-can JAR, which is distinct from the classloader of the spray-util JAR.\n\nI think this code is wrong in any case. Usually, we should use the ClassLoader from the ActorSystem for doing that. Can you try replacing the `getClass.getClassLoader` call by `actorSystem(system).dynamicAccess.classLoader` (you have to move the `withFallback` clause to the other apply method).\n. > One other question - would you prefer additional commits at this point and then a single rebase after all comments are addressed?\n\nPut them in new commits and when we are finished we can still decide how we want the final history to look like.\n. > I think you first solution alternative is indeed the best. spray doesn't really work outside of an ActorSystem anyway, so requiring one to be present is not really a problem IMHO.\n\nI agree.\n. > One option is to implement an extender bundle that checks for the existence of reference.conf files in bundles as they are resolved. Upon finding a bundle with a reference.conf, the extender bundle could load the config and store it in memory. Then, the extender could provide a merged Config as an OSGi service. This pattern doesn't play nice if you have an OSGi app that has multiple versions of the same JARs installed though. The apps I work on don't have this requirement but some apps do.\n\nCouldn't you do this so that you trigger this resolution from the top-level application (which instantiates the ActorSystem) and then go just through those jars that are in the transitive dependencies of the top-level module? This should automatically exclude multiple versions of the same module. Would that be possible?\n.  :+1: Excellent, thanks @mpilquist and @topping for driving this further.\n\nThe code seems to be a good start and I agree moving to typesafe config is the way to go\n. :+1: \n. lgtm\n. Derek, I created two branches to assist you: \n\nhttps://github.com/jrudolph/spray/tree/wip/merged-new-and-old contains all your changes (and just yours) as one squashed commit on top of a temporary branch which simplified the merge.\n\nhttps://github.com/jrudolph/spray/tree/wip/pull/336-repair: I then took the current master and cherry picked this one commit containing all your changes. If you want you can take this commit as a base for your new pull request / your update of this pull request.\n. No worries, your problems are actually our fault because we violated the eternal \"don't rebase public branches\" rule (knowing it could break stuff).\n\nIt could work like this:\n\n```\n# allows you to access my branches\ngit remote add jrudolph https://github.com/jrudolph/spray.git\ngit fetch jrudolph\n\n# create a new local branch off mine\ngit branch add-metrics jrudolph/wip/pull/336-repair\n\n# go to the new branch\ngit checkout add-metrics\n\n# remove my commit but keep the changes in your working copy\ngit reset HEAD~1\n```\n\nWhat you now got is a branch that is at the current spray master but with just your changes in the working copy. So, it will look like you've started on the master and just made your changes. Now, just go ahead and create your commits with those changes (see the commit message policy link, Mathias gave above, for more info about what we expect from PRs). Then push the branch to your repository and create the PR.\n. Right, that's something I ran into as well when trying to run spray on the Raspberry PI. Thanks for reporting.\n. Could you post the rest of the stack trace as well? It seems the interesting parts seem to be in the `Caused by` section?\n. Scrap that, it's easy enough to reproduce.\n. > How I hotfixed it: danielwegener@c495623\n\nYes, this will then later fail when a entity is tried to be encoded or decoded. I attempted another fix which won't fail at initialization time and which will make spray respond with proper error messages when a charset is missing on the platform.\n. Sorry, stupid me. This time I tried to run all the tests.\n. I rebased this PR with following changes\n- Fixed the commit messages\n- Extract common superclass for directive specs\n- Import cleanups\n. Superseded by akka/akka#16836\n. Interesting fix. I also checked that the right least upper bound type is inferred for other examples as well. These can get pretty wild depending on which types are involved, e.g. for `Segment | JavaUUID` the inferred type is \n\n```\n Comparable[_ >: String with java.util.UUID <: Comparable[_ >: String with java.util.UUID <: java.io.Serializable] with java.io.Serializable] with java.io.Serializable\n```\n\nBut I guess, that's something which 1) usually is no problem 2) we can't do anything about.\n\nSo, :+1: from me.\n. No, no idea how you could do that otherwise. Btw. the ticket number you referenced is the wrong one. It should be #344.\n. Thanks, done.\n. Jeff, thanks for the contribution. We've not yet had time to properly review it but we will do asap.\n\nOne things I noticed: the travis build fails with test errors from the spec you added. Can you look into that?\n\nPlease also see the [notes about contributing](http://spray.io/project-info/contributing/).\n. @jeffywu: I squashed your contributions and fixed some smallish stuff and created a new PR, #519. If you don't object we would close this one and go on with the new one. In any case, thank you very much for this contribution.\n. Here's a stack dump  when the problem occurs:\n\n```\nest-akka.actor.default-dispatcher-9@2133, prio=5, in group 'main', status: 'RUNNING'\n      at spray.can.parsing.HttpResponsePartParser.apply(HttpResponsePartParser.scala:40)\n      at spray.can.parsing.HttpResponsePartParser.apply(HttpResponsePartParser.scala:26)\n      at spray.can.client.ResponseParsing$$anon$1$$anon$2.spray$can$client$ResponseParsing$$anon$$anon$$parse(ResponseParsing.scala:44)\n      at spray.can.client.ResponseParsing$$anon$1$$anon$2$$anonfun$3.apply(ResponseParsing.scala:83)\n      at spray.can.client.ResponseParsing$$anon$1$$anon$2$$anonfun$3.apply(ResponseParsing.scala:79)\n      at spray.io.RawPipelineStage$$anon$3$$anonfun$2.apply(Pipelines.scala:99)\n      at spray.io.RawPipelineStage$$anon$3$$anonfun$2.apply(Pipelines.scala:99)\n      at spray.io.ConnectionTimeouts$$anon$1$$anon$2$$anonfun$2.apply(ConnectionTimeouts.scala:56)\n      at spray.io.ConnectionTimeouts$$anon$1$$anon$2$$anonfun$2.apply(ConnectionTimeouts.scala:44)\n      at spray.io.RawPipelineStage$$anon$3$$anonfun$2.apply(Pipelines.scala:99)\n      at spray.io.RawPipelineStage$$anon$3$$anonfun$2.apply(Pipelines.scala:99)\n      at spray.io.SslTlsSupport$$anon$4$$anonfun$2.apply(SslTlsSupport.scala:77)\n      at spray.io.SslTlsSupport$$anon$4$$anonfun$2.apply(SslTlsSupport.scala:62)\n      at spray.io.RawPipelineStage$$anon$3$$anonfun$2.apply(Pipelines.scala:99)\n      at spray.io.RawPipelineStage$$anon$3$$anonfun$2.apply(Pipelines.scala:99)\n      at spray.io.TickGenerator$$anon$1$$anon$2$$anonfun$1.apply(TickGenerator.scala:40)\n      at spray.io.TickGenerator$$anon$1$$anon$2$$anonfun$1.apply(TickGenerator.scala:38)\n      at spray.io.ConnectionHandler$$anonfun$running$1.applyOrElse(ConnectionHandler.scala:63)\n      at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:165)\n```\n\nThe problem seems to be that ResponseParsing pushes an `ByteString.empty` through the event pipeline when it receives a PeerClose (`ResponseParsing.scala:83`). The empty ByteString is not handled by the default case of `HttpResponsePartParser.apply` (`HttpResponsePartParser.scala:40`). @sirthias can you look into that?\n. There seem to be lots of test failures, can you look at them?\n. Ah, sorry, I missed that comment, if it's just stack size, could you add a commit that increases it in `/.jvmopts`?\n. The tests in `spray.can.parsing.HttpHeaderParserSpec` also fail for me with increased stack size.\n. Hmm, the other one in `FileAndResourceDirectiveSpec` seems to be unstable but unrelated to stack size and probably unrelated to your changes, you probably can ignore that one.\n. About the one in `HttpHeaderParserSpec`, @sirthias can you look at them? It seems that the tests make very concrete assumptions about the complete set of headers to parse (which now changed). Either, we accept that this set won't change often and just update the expected values now and then or we would need a more relaxed testing, there.\n. The interesting thing is that it is only now that FileAndResourceDirectivesSpec start failing. I guess it's because of tests now running concurrently when they before ran concurrently in another way.\n\nCan you try adding this commit to the PR: https://github.com/jrudolph/spray/commit/1dd87930f023c1d3b21af03dde215bed5814bf34?\n. I agree with @sirthias that it would be best if you remove the latter two commits and we fix the tests afterwards. I think after b15d365 the failure changed into something which looks more like a dependency issue than a timeout issue, not sure if this is good news :)\n. I updated my commit with a fix if you want to give it a try: https://github.com/jrudolph/spray/commit/3500cfd5b2fd1fbd50cbd281d203375600d8f8ea\n\nOtherwise, just remove the commits and we'll do the fix later.\n. Ok, thanks for checking this, I think let's wrap up and cleanup this PR and we'll then decide how to fix the test:\n- reset this branch to bdf20ad (@sirthias fix for the test)\n- add another commit (or rewrite your original one) to add the `require` for `deltaSeconds`\n\nThen, we'll merge the PR and care for the failing test.\n\nThanks again for your contribution and your patience!\n. :+1:\n. :+1:\n. I also don't know of a better name than `Neutral` and since it's not needed often, I guess it's fine. So, lgtm.\n. Thanks for this suggestion, Mathias. I haven't thought long enough about the general approach (using padding to avoid false sharing) to reason sufficiently enough about when to use it. I currently have those objections:\n- I don't think the padding approach works all the time (the start of the object isn't necessarily aligned on cache line bounds). I rather trust an approach if it would be on the JVM level which makes sure it works as intended (like `@Contended`).\n- I don't trust a measurement of a 3% performance gain. I found a performance change of that size usually very hard to measure with some confidence. \n. :+1:\n. lgtm\n. Otherwise, lgtm. I like the config cleanup.\n. Superseded by akka/akka#16835\n. :+1:\n. :+1:\n. lgtm\n. I just restarted the one failed job. This was indeed very strange, it seemed to get a compile error on the first compile which then just disappeared when running the tests.\n. lgtm, I agree with what @MarkvanderTol says about how to fix the commit message\n. Nice! Thanks, Matthias.\n. lgtm\n. Thanks for the fix!\n\nCould you change your commit message to conform to the new guidelines as outlined here:\n\nhttp://spray.io/project-info/contributing/\n\nWe would also need a CLA from you (if you haven't already signed one).\n. It seems something went wrong. As @MarkvanderTol said, you can usually fix errors like this by using `git rebase -i <commit-id of your first commit>~1`.\n\nEither you retry your fix or reset your branch to [this one](https://github.com/jrudolph/spray/tree/wip/fix-pull-386) where I fixed the commit messages.\n. I think we then have two issues here:\n- Your server produces a response to slow and the request is already completed with the timeout. AFAICS not much spray can do about it.\n- When the response is finally finished the connection may already be closed and you get the exception as documented here. That's because of the unsafe use of the connection actor's ActorContext in the `ResponseReceiverRef` (which will be removed sooner or later, see #187). That's something we should fix for now so that you don't get those nasty log messages.\n\nDo you agree?\n. > The best way to fix this is probably to iterate through the byte array with a @tailrec, manually \"cast\" bytes to Char and append them to the sb.\n\nI agree, that would be the best way to get rid of the warning.\n. I think the idea is that Rendering is optimized to produce `ByteStrings` / `Array[Byte]` because that's what has to be sent on the network.\n\nAFAIU StringRendering is only used with elements that produce only ASCII characters (and when they wouldn't, we still wouldn't know which charset to use). But I agree that the rendering infrastructure seems to be a bit overloaded to do different things at different places but that's probably the DRYest way to do it.\n. lgtm\n. Thanks for the reminder. I think the problem is, that we don't currently publish documentation for nightly builds. Maybe, that's the feature that is needed to update docs more frequently.\n. The fix looks good but I don't understand the original issue. If a malicious site has control over the callback parameter that's already as bad as it can get. If we want to get rid of the security issues we have to get rid of jsonp altogether. I understand that it's supposed to be somehow connected with an IE6-8 bug which may reinterpret the result as javascript when wrapped with `<script>` tags but how does this matter when jsonp is javascript in the first place?\n. @eelbaz, this issue should be fixed since several months. If you have another issue, please report it on the mailing list.\n. lgtm\n. :+1:\n. One thing to keep in mind is that there's no support for half-closed connections in SSL (but SSL on the other side requires half-closed connections from its transport layer). This means \n1. `keepOpenOnPeerClosed` is not supported on top of SSL (once you receive `PeerClosed` the connection is closed)\n2. `keepOpenOnPeerClosed` should always be enabled on the transport layer beneath SSL so that one can wait for the other side's SSL level `close_notify` message without barfing `RST` to the peer because this socket is already gone\n. Should be fixed by #406.\n. This is no essential 1.0/1.1/1.2 change for me. @sirthias ?\n. From [rfc1952](https://tools.ietf.org/html/rfc1952#page-5):\n\n```\n     MTIME (Modification TIME)\n        This gives the most recent modification time of the original\n        file being compressed.  The time is in Unix format, i.e.,\n        seconds since 00:00:00 GMT, Jan.  1, 1970.  (Note that this\n        may cause problems for MS-DOS and other systems that use\n        local rather than Universal time.)  If the compressed data\n        did not come from a file, MTIME is set to the time at which\n        compression started.  MTIME = 0 means no time stamp is\n        available.\n```\n\nUsing `gzip -n` on linux sets MTIME to zero.\n. And kudos, @agemooij, for spotting the difference, I ran it about 10 times and didn't see the output change.\n. @agemooij looks generally good to me and contains lots of useful tests.\n\nI don't understand the subtle nuances between `decompressRequest` and `decompressRequestIfRequested`. When would you use the first and when the other one? (For compressing, we can choose what to do, for decompressing we have to decompress what we get from the other side or reject the request, but in that regard both directives seem to do the same, no?)\n. When this is finished we should update the example at http://spray.io/documentation/1.2-M8/spray-routing/ to use the new directives (if the example isn't thought to show off composition features).\n. Similarly to the decompression side, having two directives on the compression side, `compressResponse` and `compressResponseIfRequested`, means you have to decide which to use.\n\nif you look at the [original Http/1.1 RFC 2616](https://tools.ietf.org/html/rfc2616#section-14.3) it says this about missing `Accept-Encoding`:\n\n> If no Accept-Encoding field is present in a request, the server MAY assume that the client will accept any content coding. In this case, if \"identity\" is one of the available content-codings, then the server SHOULD use the \"identity\" content-coding, unless it has additional information that a different content-coding is meaningful to the client.\n\nwhich I would interpret as \"you should normally use compressRequestIfRequested\" because this would choose `identity` encoding.\n\nHowever, in the latest [HTTPbis draft](http://tools.ietf.org/html/draft-ietf-httpbis-p2-semantics-23#section-5.3.4), this paragraph seems to be gone and it only says this:\n\n> A request without an Accept-Encoding header field implies that the  user agent has no preferences regarding content-codings.  Although    this allows the server to use any content-coding in a response, it    does not imply that the user agent will be able to correctly process  all encodings.\n\nSo, the conservative choice would be to normally use `compressRequestIfRequested`.\n. > Are you suggesting [...]\n\nAt least I'd like to be able to give a sane advice which directive to choose in the usual case. And I guess it would be this (from your current tests):\n\n```\nval decompressCompressIfRequested = (decompressRequest & compressResponseIfRequested)\n```\n\nSo, yes maybe it would make sense to remove `compressResponse` as well and also add the above as the standard compression/decompression directive, maybe named `decompressCompress` or just `enableCompression` or even something simpler. WDYT, @sirthias?\n\nThanks again, the code LGTM and I've got the feeling the discussion here was long but hopefully anticipates decisions and questions for our users and simplify spray's use.\n. Cool, I'm not strongly against one or the other. My main concern is that we have decided for ourselves and for the documentation what to suggest and that's what the discussion has achieved. I also agree that `(decompressRequest & compressResponse)` is clean and explicit enough to serve as a catch-all solution if you don't want to worry about things but as @sirthias said in other cases you want to explicitly define which requests/responses to de/compress in which case you'd often would specifically move the directive deeper down the route tree.\n. Very nice. Thank you. lgtm\n. Btw. docs commits don't need the `+` marker as they don't add anything to the API.\n. See this investigation: https://groups.google.com/d/msg/spray-user/_Hchc3b8Dv4/Op1XZJ0zlqkJ\n. Looking into it...\n. The PipelineLimiter didn't work at all for normal requests. I fixed that in the above PR. \n\nHowever, I'm generally sceptical about how much we are able to deflect incoming requests when the load is already high because of the way SuspendReading is delivered asynchronously to the IO connection during which the connection may already have read MBs of more data. I've got the impression that this is a general problem of event based systems handling external events and I will start a discussion on the akka-user mailing list discussing further steps.\n. I think this may already be fixed but we should verify it by testing / reviewing the code.\n. As the URI is built from `HttpServletRequest.getRequestURL()` and [its javadoc](http://docs.oracle.com/javaee/6/api/javax/servlet/http/HttpServletRequest.html#getRequestURL) states that it contains the complete url including server name and port, etc., this should already work as intended.\n. lgtm, I restarted the failed travis job part.\n. Thanks, Age. \n\nHave you examined how the method might interact with the `spray.http` package name? Usually you won't have `spray._` imported so I guess it's usually not a problem but can we think of other ones?\n\nAnd I wonder if it really warrants its own directive category but if in doubt maybe yes.\n\nRegarding documentation, the `headerValueByName` directive is already documented so you could go there for inspiration.\n. That's a good observation. With this in mind I wonder if it really is desirable to have those two as overloads. Do we have any naming scheme to distinguish pure filters and extraction directives? If not I think we should think about one. Not using overloads but different names will also lead to better error messages.\n. > figuring out the correct includecode snippet incantations\n\nIs there anything wrong with it that we could improve?\n. > Is there anything wrong with it that we could improve?\n\nI.e. apart from a bug that I just found where methods with type parameters aren't found.\n\nBtw. the build is failing with something that looks like a legitimate compile error:\n\n```\n[error] /home/travis/build/spray/spray/spray-routing/src/main/scala/spray/routing/directives/SchemeDirectives.scala:36: type mismatch;\n[error]  found   : Some[spray.routing.SchemeRejection]\n[error]  required: spray.routing.Rejection\n[error]     schemeName.require(_ == schm, Some(SchemeRejection(schm))) &\n[error]                                       ^\n[error] one error found\n```\n. > and eventually found that using `scheme(` did the trick\n\nAh, ok similar error than mine, which I fixed some time ago (https://github.com/spray/spray/commit/9dded0e6d01f79a3e54acbd43dc56f654419a03d) but which maybe was missing in your branch. With the fix it should also work without the `(`.\n. This last little thing and then it LGTM. Thanks again, Age, for getting through all of this with us just for what seemed like a simple 2 line addition :)\n. This quick fix lgtm as the complete logic here will have to go once we finally get rid of UnregisteredActorRefs\n. Thanks, @agemooij, good comments, I added a link to the mailing list thread for OS/X information. Since I've got no OS/X system I can't really offer any help on the exact procedure and it seemed the mailing list also didn't offer any definite instructions.\n. I also added another commit, that changes the build to try to locate `sphinx-build` in well known locations if `SPHINX_PATH` doesn't point to an existing file.\n. Here's the older issue and thanks for adding more reasons why it is important:\n\nhttps://github.com/spray/spray/issues/102\n. Actually, it had \"proxies\" in the name and not \"proxy server\". I just changed it 10 minutes ago to make it easier to find.\n. > This would incur some overhead of course but would be feasible. In fact it is what akka itself is doing\n\nBut woudn't this mean that the feature @taylorleese requested is already implemented by akka? If I look here: https://github.com/akka/akka/blob/c10eadbd3388250425efff0b790f59af6dbdc63d/akka-actor/src/main/scala/akka/event/Logging.scala#L202 it seems like there's a way of configuring how a LogSource is converted into a string (usually the path) and that string value should later be available in the MDC as `akkaSource`. So what is then missing?\n. What we would lose in contrast to the current approach is the possibility of hierarchical configuration. But I guess that's something that indeed can't be improved with the existing logging frameworks.\n. Yes, I guess that's the best idea. Any thoughts from @clementgarnier who originally raised the issue?\n. Otherwise, there's no easy way to access the most recent documentation (e.g. for new features) as already observed in #391.\n. That would complicate the whole procedure much and wouldn't add much because 1.0, 1.1 and 1.2 should be mostly feature equivalent.\n. This proposal would add another version in the documentation tab e.g. `1.1-20130801` in addition to everything that's there now. So, documentation would match the current source code of the version specified (as long as the documentation in the source tree is current which usually is an invariant we try to maintain through pull request reviews). Each night, we would just publish documentation for the latest nightly.\n\nA downside would be that we wouldn't be providing documentation for older nightlies (and links to those would get stale), but instead just for the latest 1.1 nightly.\n\nMaybe @sirthias has a better idea.\n. Thanks, good catches. LGTM\n. This still had some bad bugs I fixed with this new version.\n. I added code to the `simple-http-server` example which can handle file-uploads by streaming the chunks to disk and then uses mime-pull to extract the parts. This won't be the final version but I figured it would be useful to have a working example for one of the major use-cases directly for experimenting.\n. I don't think mixing with `parameters` makes sense.\n\nMy suggestion, instead, is to be explicit and support directives that map to HTTP features:\n- have separate `formFieldsUrlEncoded` and `formFieldsMultipart`\n- maybe have a joined `formFields` directive which works similar to the current one but without the [default value of `null`](https://github.com/spray/spray/blob/464ffe971953ceac2f24cd0ff2f4882a68d83106/spray-routing/src/main/scala/spray/routing/directives/FormFieldDirectives.scala#L95) for the `FEOU` which defers the error that it can't read multipart data to runtime.\n- add more documentation about how to implement joined deserializers/FEOU that can handle both kinds of formFields together.\n\nWDYT?\n. Maybe we should start with a less intrusive change: even with `multipart/formdata` common form data is transported as `text/plain` as explained here: http://www.w3.org/TR/html401/interact/forms.html#h-17.13.4.2. For unmarshalling `multipart/formdata` forms we should \n- check if we have an FEOU for the target type which would read the body part\n- otherwise, check if the content-type of the body part is `text/plain` and then just use the `Deserializer[String, T]` for deserialization\n- only then fail with the error message that we cannot decode `multipart/formdata`\n\nAlso, in the other direction, i.e. for `application/x-www-form-urlencoded`:\n- check if we have an FSOD available and use that\n- otherwise, check if we have an FEOU available and feed it with an HttpEntity(text/plain, textData)\n- and only then fail\n\nWith those changes we should probably cover 90% of the cases occurring in practice. For the remaining 10% (custom content-types for BodyParts, allowing more than more content-types for BodyParts with different unmarshallers), I find it pretty hard to wrap my head around what exactly I need to do to make it work. For those cases we need step-by-step instructions how to get from a route failing at compile-time or runtime to a program that compiles and accepts data as intended.\n. I haven't looked at the code itself yet. \n\nI wonder what the restriction you are talking about means for the original purpose of this functionality as discussed in #402. As I understand it, without the complete raw URI string as used in the request you still won't be able to support the HMAC protocol for absolute URIs even with spray-can. This restriction could be ok if you can instruct your clients never to use absolute URIs for the requests. But, e.g. what happens if someone uses an HTTP proxy which just forwards the absolute URIs it gets? I tried to find something but it seems that it is not clear how HMAC based authentication should work together with proxies, so maybe this is also not too important.\n. > This doesn't affect HMAC at all: it has a normalization pass and only the raw (undecoded) path and query string is needed. The host and port and normalized separately. For details, refer to section 3.2.1 of the draft specification.\n\nFrom the section:\n\n> The HTTP request-URI as defined by [RFC2616] section 5.1.2.\n\nand this could also be an absolute URI AFAIU.\n\n> Proxies are a different matter. According to my reading of the HTTP/1.1 spec they're not allowed to rewrite the URI target. I suspect some probably do though. (This is an additional use-case for this header, by the way: implementing a proxy.)\n\nAlso, see the referenced section 5.1.2\n\n> The absoluteURI form is REQUIRED when the request is being made to a proxy.\n\nThat's what my comment was about: if a proxy is involved it is likely that the request URI is absolute and since I don't see how HMAC say anything about just using path + query string this could break down if we just have access to path and query.\n\nThat said, I don't think we necessarily disagree about what to do about it. I actually like it the way you have done it now. Let's wait for @sirthias opinion before doing anything else.\n. Thanks Martin. You are right with your assumptions. Custom HttpMethod support is currently missing. We should probably do both: add more standard ones and add support for custom ones.\n. Closing for now. @clementgarnier, if there's something missing, please reopen.\n. Thanks, @davidillsley, for reporting and @guersam for answering. I'll close this one for now. If you think there's still something missing, please feel free to reopen.\n. Thanks for noticing and sorry for the misleading documentation.\n\nIt is mentioned here: http://spray.io/project-info/current-versions/ but I agree it would be better if it would be placed somewhere more prominently.\n. We discussed internally how we could support proxies automatically in a cleaner fashion (see #102), sorry for misleading you a bit. What's definitely worthwhile is the change adding the new headers. In a first step, could you create a new PR containing just the code for the new headers? I don't think we need the changes in `RequestBuilding` because in the best case you would be able to configure proxy credentials externally and they would be automatically picked up.\n. I think we can close this for now.\n. Cristian, thanks for finding this bug and fixing it. Two things: can you rewrite your PR commit to adhere to our commit message guidelines and sign the CLA. See http://spray.io/project-info/contributing/\n. Oh, I'm just seeing you referred to the old wiki for the documentation. The documentation in the wiki is severely outdated! See http://spray.io for the new documentation. That said I can see some value in re-introducing this feature for the current version.\n. This is solved in akka-http with the new streaming multipart support.\n. This is done in akka-http. For spray-routing we currently distribute two artifacts, one against shapeless and one against shapeless2.\n. Thanks, Julien, for the comprehensive report. We'll look into it.\n. To be clear, this situation arises when the server accepts the connection but never responds, right?\n. > From what I could see, it is more like the server does not respond to the connection attempt.\n\nYes, you're right that's how I could reproduce the issue. So, the problem is, that the `ClientFrontend` doesn't handle a request-timeout properly if the timeout is due while the connection isn't yet established.\n. And I guess the invalid assumption in ClientFrontend is that closing a connection is always possible even if not yet connected but `TcpOutgoingConnection` doesn't react on closing messages in `connecting` state. I created a ticket in akka to track this:\n\nhttps://www.assembla.com/spaces/akka/tickets/3554-io--clarify-how-to-close-connection-that-is-not-yet-established#/activity/ticket:\n\nWe could fix this on our side just by shutting down the HttpClientConnection when we get into this situation.\n. @sajanalexander, you mean before it is fixed properly? Probably, you have to send it either a `PoisonPill` or `context.stop()` it.\n. I just figured there's no way to shutdown the akka IO level TCP connection actor because the http side haven't got an ActorRef for it (usually the sender of the Tcp.Connected msg). So, the right way to handle this (as already discussed) is to shut-down the HttpClientConnection actor itself.\n. Also, all of the above explanations make no sense whatsoever if you consider that ClientFrontend and all of the pipelining stuff shouldn't even run before the connection is established! This just got a lot weirder.\n\nThis is what happens:\n\n```\n09/11 15:11:23 DEBUG[on-spray-can-akka.actor.default-dispatcher-2] s.c.c.HttpHostConnectionSlot - Attempting new connection to 1.1.1.1:6666\n09/11 15:11:24 DEBUG[on-spray-can-akka.actor.default-dispatcher-4] s.c.c.HttpClientConnection - Attempting connection to /1.1.1.1:6666\n09/11 15:11:24 DEBUG[on-spray-can-akka.actor.default-dispatcher-2] a.i.SelectionHandler - Executing [WorkerForCommand(Connect(/1.1.1.1:6666,None,List(),None),Actor[akka://on-spray-can/user/IO-HTTP/group-0/0],<function1>)]\n09/11 15:11:24 DEBUG[on-spray-can-akka.actor.default-dispatcher-3] a.i.TcpOutgoingConnection - Attempting connection to [/1.1.1.1:6666]\n09/11 15:11:24 DEBUG[on-spray-can-akka.actor.default-dispatcher-3] a.i.TcpOutgoingConnection - Connection established to [/1.1.1.1:6666]\n(true,false)\n09/11 15:11:24 DEBUG[on-spray-can-akka.actor.default-dispatcher-4] s.c.c.HttpClientConnection - Connected to /1.1.1.1:6666\n09/11 15:11:24 DEBUG[on-spray-can-akka.actor.default-dispatcher-6] a.i.TcpOutgoingConnection - [Actor[akka://on-spray-can/user/IO-HTTP/group-0/0]] registered as connection handler\n09/11 15:11:24 DEBUG[on-spray-can-akka.actor.default-dispatcher-6] s.c.c.HttpHostConnectionSlot - Connection to 1.1.1.1:6666 established, dispatching 1 pending requests\n09/11 15:11:24 DEBUG[on-spray-can-akka.actor.default-dispatcher-6] a.i.TcpOutgoingConnection - Read [0] bytes.\n09/11 15:11:24 DEBUG[on-spray-can-akka.actor.default-dispatcher-4] s.c.c.HttpHostConnectionSlot - Dispatching GET request to / across connection Actor[akka://on-spray-can/user/IO-HTTP/group-0/0]\n09/11 15:11:24 DEBUG[on-spray-can-akka.actor.default-dispatcher-3] a.i.TcpOutgoingConnection - Wrote [0] bytes to channel\n09/11 15:11:29 WARN [on-spray-can-akka.actor.default-dispatcher-2] s.c.c.HttpClientConnection - Request timed out after 5 seconds, closing connection\n09/11 15:11:29 DEBUG[on-spray-can-akka.actor.default-dispatcher-3] s.c.c.HttpHostConnectionSlot - GET request to / timed out, closing connection\n09/11 15:11:29 DEBUG[on-spray-can-akka.actor.default-dispatcher-2] a.i.TcpOutgoingConnection - Got Close command but write is still pending.\n09/11 15:11:29 WARN [on-spray-can-akka.actor.default-dispatcher-2] s.c.c.HttpClientConnection - Request timed out after 5 seconds, closing connection\n09/11 15:11:29 WARN [on-spray-can-akka.actor.default-dispatcher-3] s.c.c.HttpClientConnection - Request timed out after 5 seconds, closing connection\n```\n\nwhile this is happening on the wire:\n\n```\n15:15:06.793738 IP 192.168.7.122.35792 > 1.1.1.1.6666: Flags [S], seq 1749535247, win 14600, options [mss 1460,sackOK,TS val 4078190 ecr 0,nop,wscale 7], length 0\n15:15:07.791911 IP 192.168.7.122.35792 > 1.1.1.1.6666: Flags [S], seq 1749535247, win 14600, options [mss 1460,sackOK,TS val 4078440 ecr 0,nop,wscale 7], length 0\n15:15:09.795906 IP 192.168.7.122.35792 > 1.1.1.1.6666: Flags [S], seq 1749535247, win 14600, options [mss 1460,sackOK,TS val 4078941 ecr 0,nop,wscale 7], length 0\n15:15:13.807898 IP 192.168.7.122.35792 > 1.1.1.1.6666: Flags [S], seq 1749535247, win 14600, options [mss 1460,sackOK,TS val 4079944 ecr 0,nop,wscale 7], length 0\n```\n\nAs you can there are only SYN packages without any responses.\n\nSo, in fact, the process is like this: \n- JDK nio instantly reports the channel as connectable\n- channel.finishConnect succeeds\n- akka IO reports the connection as Connected\n- HttpClientConnection reports the connection as Connected\n- HttpClientConnection tries to send the request\n- the requests ends up in Akka IO's one-entry write queue but cannot be written (because the connection isn't actually established)\n- HttpClientConnection eventually tries to close the connection because of the request timeout\n- Akka IO tcp connection doesn't regularly close the connection as long as the write is pending (which it will be forever because the \"connected\" connection is basically idle\n. This sent me on a trip: see https://www.assembla.com/spaces/akka/tickets/3602-io--tcp-connection-establishment-always-succeeds-even-if-endpoint-never-answers for a description of the underlying problem.\n. I'd also like to try out a more aggressive one where I revert the previous changes for `finishConnect`. I'll send you a note when it's done. Thanks for helping out with testing.\n. Here's the probable underlying issue:\n\nhttp://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6371630\n. Thanks, Age. LGTM\n. Interesting, I wonder if there's code which relies on the old behavior.\n. We already did some tests on this:\n- our rendering is still not optimal, we use `akka.util.ByteStringBuilder` which copies a lot more around than necessary (see branch `wip/better-bytestringrendering` for an improvement)\n- as long as HttpEntities are based on `Array[Byte]` instead of `ByteString` we can't avoid copying things around (because of array mutability), we are already in the process of improving the HttpEntity API so that it uses `ByteString` instead of `Array[Byte]` which will allow us to avoid any copies of the payload apart from the final copy to the network, also this will enable spray to stream files without any user-space copies\n\nThat said, in many cases raw byte throughput won't be the limiting factor because you already need a 10Gb network to saturate a spray server with this benchmark (1000 RPS \\* 1Mb/req). Still saving work (and cache lines) is always a good thing.\n. I agree it makes sense for those error conditions we control. LGTM \n. lgtm\n. Thanks, lgtm\n. Looks very clean now. Thanks! :+1:\n. I actually liked the proposal to leave SprayActorLogging deprecated for a while. I mean, currently we don't make any compatibility promises but leaving it around deprecated with a comment would people help to get around this simple source change and wouldn't cost us much, right?\n. lgtm\n\nBtw. did you have use case in mind for adding the cache?\n. We'll change configuration, that this case fails fast when you try to set an invalid `user-agent-header` setting. Apart from that, what Mathias' said: we won't allow to specify invalid headers usually. If you really need to you can probably create an instance of the `User-Agent` header with an invalid `ProductVersion` instance but we won't change the API to simplify that case.\n. Reading the comment at https://github.com/spray/spray/blob/master/spray-can/src/main/scala/spray/can/client/HttpHostConnectionSlot.scala#L62 would confirm this suspicion. If it can happen in `unconnected` it could probably happen in `connecting` as well, right? Can we make this more robust?\n. Let's close this as long as we don't know that it is really needed.\n. The reason is that we want to allow all possible representations in our model. By using a special empty string we hope that you don't even notice any change unless you care.\n. Now we have some reports of server failing to understand queries rendered without '='. See https://groups.google.com/d/msg/spray-user/vU5MNsK6YZA/k5sqH5X8oPcJ\n. One possibility would be to claim that we haven't seen any `Content-Type` header when we see two and just pack those two (or more) into the `headers` field and deliver the entity with the default ContentType. This way a client can handle a broken peer in any way possible later on. WDYT?\n. @martin-g haha, good catches. Thanks for looking through it.\n. Fixed in https://github.com/spray/spray/commit/80f205ffa925c43a1878c85e70a916a9b52f64f6\n. > Can it be used in spray?\n\nIt probably could. The upcoming akka-http module will support backpressure handling through Akka's reactive streams implementation.\n\n> BTW: why does spray source have akka io source included?\n\nIt's included for Akka versions (2.0.x / 2.1.x) which come without Akka IO support. Starting with spray 1.2 (Akka 1.2.x) spray uses Akka IO proper.\n. This is fixed in akka-http by using reactive streams.\n. Also filed on the akka side:\n\nhttps://www.assembla.com/spaces/akka/tickets/3571\n. This is build from a new [release/1.2-M8](https://github.com/jrudolph/spray/tree/release/1.2-M8) branch that contains exactly two commits over the tag 1.2-M8: \n1. a bulk commit containing all published documentation changes for 1.2-M8 (last sync at 13/06/2013 with 07c046a18322147869aefbb5ea48a84def36d01d whose sources however were merged into release/1.2 only on 18/06/2013 with ff77fbcb4d19d5db5631a0f620ea059e5b84d1b1)\n2. this change\n\nI think this is a cleaner approach than building documentation updates for 1.2-M8 from the release/1.2 branch which already contains lots of documentation changes not applying to 1.2-M8 any more. WDYT?\n. Btw. IIRC that's also the way documentation updates in sbt are organized.\n. Also, see this: http://davenport.sourceforge.net/ntlm.html\n\nThe spray team won't be able to actively work on this as we will focus on improving the core of spray. We will support initiatives to make this available as an extension by providing the needed hooks if missing but it is even unlikely that we would integrate it into the core because of possible security implications we just cannot oversee.\n. This isn't planned but could be provided as a contrib module.\n. lgtm, getting rid of `Array[Byte]` feels really good.\n. There seems to be one legitimately failing test:\n\n```\n[error] x reject the request if it has a `Content-Encoding: deflate` header but the request is compressed with Gzip\n[error]    List(UnsupportedRequestEncodingRejection(gzip), CorruptRequestEncodingRejection(incorrect header check), UnsupportedRequestEncodingRejection(identity))\n[error]     is not equal to \n[error]    List(UnsupportedRequestEncodingRejection(gzip), CorruptRequestEncodingRejection(unknown compression method), UnsupportedRequestEncodingRejection(identity)) (EncodingDirectivesSpec.scala:395)\n[error] Expected: ...tion([u]n[kn]o[wn ]c[ompr]e[ssion] [method]), Un...\n[error] Actual:   ...tion([i]n[c]o[rre]c[t h]e[ader] [check]), Un...\n```\n. Great! :+1:\n. The problem this ticket is trying to solve is that incoming file data may flood the Java heap. The suggestion here would be to buffer incoming data to the disk until it can be handled. \n\nIn akka-http, backpressure seems to provide a better solution: out-of-the-box only as much data will arrive in the Java heap as the server is currently able to handle. Buffering is rarely needed because either the data should be handled in a streaming fashion directly or just streamed to a file. This will happen naturally through reactive streams combinators.\n\nSo, since we don't fix this in spray any more, this doesn't seem to be an issue any more.\n. Thanks, lgtm.\n. Thanks @BalajiSi. Yes, looks good. If this is your first contribution we'd need a CLA. Also make sure your commit message adhere's to our rules. See here:\n\nhttp://spray.io/project-info/contributing/\n\nI'd see this as a bugfix commit, so it would need to be prefixed with `=`.\n. I tried it and it fails as you say but not because the field is missing (it is actually there, but the diff in the test doesn't show it) but because fields are reordered unless you use a `ListMap` which preserves ordering. In which setting did you encounter this problem?\n. I.e. maybe you can first check if the field is really missing in a wireshark dump of your actual usage?\n. This is the commit making your test succeed: https://github.com/jrudolph/spray/commit/34ed6b093133d341e2b2da7c815929ddf7a33830\n. And FTR (as I also struggled to find out), the cause for the test diff missing essential parts in this case is `HttpBody.toString` abbreviating bodies longer than 500 chars.\n. The code itself LGTM. I'm not 100 percent sure if we really want to support it that way because of the model being not in \"normal form\" any more (see [this comment](https://github.com/spray/spray/issues/327#issuecomment-23710140)). @sirthias?\n. The few comment aside :+1:\n\nTwo things: \n1) Can you update this PR to the current master?\n2) Can you change commit line of the second commit to this:\n`! http, httpx: make multipart form-data more flexible but have it adhere to the RFC more strictly`\n\nThanks, again.\n. We'd like to finish this PR this week. I think I'll make the changes for you @dlouwers because it contains a merge conflict you didn't cause and create another PR for you to give your ok to.\n. I went ahead and completed this PR in #536, please review @dlouwers if that's ok for you.\n. Thanks @mpilquist, this is already a very good starting point.\n. The current proposal would basically duplicate the same logic for client and server side. If we would extract the logic into a pipeline-stage we could just reuse it. The event pipeline of the new stage would listen for `SSLSessionEstablished`, `HttpMessageStartEvent`, and `HttpMessage` events and then add the header to the message. Additionally, the setting would control if the pipeline stage would be enabled at all, so that users that don't care wouldn't have any code running for it.\n\nWe would need to add a new method in `HttpMessageStart` to add the header generically. WDYT?\n. > I don't follow why it would need to capture the HttpMessage event though.\n\nYeah, sorry, I meant `Http.MessageEvent`.\n. > Something like withHeaders(headers: List[HttpHeader])\n\nI think we need something more like `HttpMessage.mapHeaders`. The current meaning for `withHeaders` is to overwrite the complete list of headers.\n. I think we should keep `spray.can` free from implementation stuff. Maybe let's put the stage into `spray.can.parsing` and just access `HttpMessageStartEvent` across package boundaries.\n. This looks very clean now. I previously wanted SSLSessionInfo to carry more semantics but it is now so simple that we maybe should leave it this way and make no more assumptions about what the SSLEngine will deliver for the parameters. :+1:\n\nIf @sirthias agrees, can you squash the changes into two commits: one with just the SslTlsSupport changes (simplifying a possible akka-io backport) and one with just the spray-can changes.\n. Very nice. LGTM. Why would you say that both of those commits are `!` commits? Are there any source incompatible changes?\n. Yep, thanks for the explanations @mpilquist \n. > A general question: Should I squash right away after every iteration? I thought the updates and comments would be more comprehensible if I wait with the squash right until merging.\n\nIMO iterative commits are fine usually. If changes can be split into several standalone commits they always should be. In those cases, there has to be a time in the end where the final commit split should be reviewed.\n. I'd still like to see proxy authentication added in the same way but maybe we can add this in another step.\n. I'd like to discuss the lifetime of `ProxiedHostConnector` in relation to its underlying `HttpHostConnector`. I think what is missing is the `DemandIdleShutdown` logic on the `ProxiedHostConnector` level to avoid race-conditions and lost requests due to the underlying `HttpHostConnector` during shutdown.\n. > Wouldn't this also be useful for normal authentication? \n\nI don't think it's comparable. The proxy settings are settings of the transport mechanism while http authentication often is a feature of single requests.\n\n> We could simply add default headers to the HostConnectorSetup.\n\nThat could be one solution.\n. > I think this patch is fine with regard to the CloseAll command support.\n\nWhat, if the ProxiedHostConnector doesn't know about the DemandIdleShutdown message, how can it stop sending messages to the real HostConnector?\n\nIIUC the contract between HttpManager and HttpHostConnector is not to send any more requests to the connector once the PoisonPill is sent (because they will end up as DeadLetters). As the ProxiedHostConnector is still in the list of connectors even the HttpManager will still send requests to the ProxiedHostConnector (and through this to the HttpHostConnector). Is there a simple solution for this case?\n. For easier review here's the comparison of the changes over plain akka master:\n\nhttps://github.com/jrudolph/spray/pull/1/files\n\n(Changes are expected because our master is on akka 2.1)\n. Ok, that didn't work as it just shows the changes of your commit and not the diff.\n\nHere's the diff:\nhttps://github.com/jrudolph/spray/compare/wip/stupid-akka-backport...spray:ticket/472?expand=1\n. Ok, scrap that, it seems you can't compare two branches in github\n. LGTM\n. Yes, that's a known spurious failure. I restarted the job.\n. I think it's about _request_ rendering on the client side.\n. See https://groups.google.com/forum/#!searchin/spray-user/http$20Host$20connector$20idle-timeout/spray-user/_N9Q1dlroVg/jQGeIDnGDmMJ\n\nand\n\nhttps://groups.google.com/forum/#!searchin/spray-user/http$20Host$20connector$20idle-timeout/spray-user/iakr7aRS4AI/VavuSHuRenAJ\n\nEffectively, it means that you can never be sure the request-level API will deliver a result until you set the idle-timeout to infinite. However, this makes no sense, because especially for auto-created HttpHostConnectors you want to be sure they don't hang around indefinitely.\n. It seems this is already handled correctly with the `DemandIdleShutdown` message.\n. :+1:\n. @mhamrah, thanks a lot. A basic thing: can you add a few line-breaks into text blocks? It won't change the rendering but makes it easier to review in the github web-interface and will later make diffs more readble.\n. @sirthias what do you think about directly including tests from `spray-routing-tests` instead of adding extra-tests to `docs/test` (I think @agemooij recently set the precedent). Problems I see is:\n- tests are not necessarily written for use in documentation, i.e. they don't necessarily show the code but focus on the semantics\n- in this case we show some testing boilerplate instead of how to use the directive, at least more than a documentation reader might need to know\n- when developing and changing tests you always have to keep in mind that you not only change tests but also documentation\n\nI'd slightly prefer to add extra tests for the documentation but would also understand to safe some work by reusing the tests from the regular test suite.\n. Btw. I think it might be faster if we take this this PR as it is (or with the few improvements I commented on). We will in any case have a final round of editing when we make the next big documentation release (probably for 1.0) where we can polish things a bit more. Having these pieces of almost finished documentation is already so much better than having nothing at all. So thanks again.\n. LGTM, just needs squashing and then it's ready to merge.\n. LGTM, thanks Mike.\n. LGTM\n. The behavior is actually defined in many cases that if a Content-Type is missing one should assume ISO-8859-1. So, for accepting data we are not free to choose the charset but have to follow what the spec says.\n. > to default to UTF-8 instead of US-ASCII.\n\nCouldn't we parameterize the unmarshaller and the private method with the charset and then provide only a default one implicitly (I think either one of US-ASCII and UTF-8 would be fine)? That way you could at least easily create your own unmarshaller with a custom default.\n. It's a two-line change and why not add it right now when we are at it, especially, since the spec is so vague that we cannot sufficiently know what to expect in the wild.\n. and/or otherwise maybe just assume the maximum date and emit a warning.\n. See #488. Assume we would now update to shapeless 2.0.0-M1 and release 1.0 with a shapeless 2.0.0-M1 dependency. It would mean that we would bind all of our users to this non-production release for the whole life-time of 1.0. For that reason we won't do an update to an unstable version of shapeless for 1.0/1.1/1.2. For the time after 1.0 we will work together with @milessabin to make the compatibility story as smooth as possible.\n. I'd say let's close it for now and resurrect it / take ideas from it once 1.2 is out. In any case, thanks @andrebeat, for having a first stab at this. It will certainly be helpful when we get to it.\n. I added a new Timestamp class similar to what was proposed above. The reason for choosing Timestamp instead of Deadline was that in many cases we don't know the timeout yet when we would have to create the deadline so it seemed to make more sense to call the type Timestamp and have methods `isPast` and `isFuture` instead of `isOverdue` and `hasTimeLeft`. `Timestamp(Long.MaxValue)` has the special meaning of infinitively in the future.\n. This needs fixes for the tests.\n. I fixed all the issues from the comments.\n. Thanks @martin-g, fixed.\n. Fixed, and also made SimpleStash private[spray]\n. Thanks, lgtm.\n. Once #517 is merged I will update this PR to implement `HttpMessage.asChunkedParts` in terms of the new `HttpData.toChunkStream`.\n. Updated to implement `HttpMessage.asPartStream` in terms of `HttpData.toChunkStream`\n. Arguably `sliceData` is the more expected slice variant so maybe it would make sense to swap names and call `sliceData` simply `slice` and rename current `slice` to `sliceBytes`. WDYT?\n. I added another commit that should addresses the comments. `sliceBytes` is not currently used (= not performance relevant) so I decided that it can be implemented in terms of `slice` which only adds the cost of one temporary wrapper HttpData.\n. This overrides #349.\n. Thanks, @ivantopo. What about \"withPath\"? Seems to be missing in #514 but would probably make sense as well.\n. Cool, thanks. LGTM :+1:\n. @2beaucoup agreed, didn't notice that one.\n. > the first test contains `Host(\"192.0.2.16\") === IPv4Host(\"192.0.2.16\")`\n\nI didn't look at it but I would suspect that this is actually testing the normalization logic of `Host.apply` so it would also be `<actual> === <expected>`.\n. It looks as if the timeout logic isn't updating the `state` variable properly before sending out a response.\n. Also, the header models seem to be missing currently.\n. Thanks, Chip, these are interesting points. Being curious, do you know which part of the client side generates the `X-Playback-Session-Id` header? Is that a thing that safari/iDevices do?\n. FYI I moved this ticket to after 1.0/1.1/1.2 as this is strictly new functionality, we try to focus on getting the remaining bugs fixed so that the 1.0 release gets rolling. I think we should try to remove all roadblocks before 1.0 that would prevent a custom solution and AFAIK it currently is possible to implement it right now by writing a directive that responds properly to requests with `RawHeader(\"Range\", ...)` set.\n. Cool, that's already a good starting point. Where does `ByteRange` come from?\n. A directive that would work for any kind of inner route would probably look similar to this (only for non-chunked responses):\n\n``` scala\n  def rangeSupport: Directive0 =\n    optionalHeaderValueByName(\"Range\").flatMap[HNil] {\n      case Some(ByteRange(start_spec, end_spec)) =>\n        val start = ...\n        val length = ...\n        mapHttpResponseEntity { entity =>\n          entity.flatMap { nonEmpty =>\n            HttpEntity(nonEmpty.contentType, nonEmpty.data.slice(start, length))\n          }\n        }\n      case None => pass\n      case _ => Route.toDirective(complete(StatusCodes.BadRequest, \"Invalid range request\"))\n    }                                                                             \n```\n. This seems to be supported now for quite some time through RangeDirectives.\n. Maybe, as well. I see the RequestContext more as an internal thing. There were many messages on the ML where someone needed access to the complete URI that an extractor for such a basic element seems to make much sense. One of the downsides is that people often start to rewrite basic logic like PathMatchers if they find the URI-extracting directive first...\n. > As you suspect a Content-Type header containing a \\* is not valid HTTP\n\nActually, similar to the other discussion we had, this is debatable because again the \"subtype\" of a media type can be an arbitrary token which would include the single asterisk. Consequently, that's also what the parser accept. However, in contrast, we have a safety measure in `MediaType.custom` which prevents media types from being created with single asterisks as the subtype.\n\nIn the PR I changed `MediaType.custom` to accept the single asterisk if it's being called from the parser.\n. There are some merge conflicts because of the privatization.\n. Nice :+1:\n. :+1:\n. Here's a test case that observes the behavior:\n\n``` diff\ndiff --git a/spray-routing-tests/src/test/scala/spray/routing/ChunkingDirectivesSpec.scala b/spray-routing-tests/src/test/scala/spray/routing/ChunkingDirectivesSpec.scala\nindex 4c631de..f5f008f 100644\n--- a/spray-routing-tests/src/test/scala/spray/routing/ChunkingDirectivesSpec.scala\n+++ b/spray-routing-tests/src/test/scala/spray/routing/ChunkingDirectivesSpec.scala\n@@ -16,6 +16,9 @@\n\n package spray.routing\n\n+import spray.http.HttpHeaders.RawHeader\n+import spray.http.StatusCodes\n+\n class ChunkingDirectivesSpec extends RoutingSpec {\n\n   \"The `autoChunk` directive\" should {\n@@ -27,5 +30,22 @@ class ChunkingDirectivesSpec extends RoutingSpec {\n         new String(bytes) === text\n       }\n     }\n+    \"must reproduce status code and headers of the original response\" in {\n+      val text = \"This is a somewhat lengthy text that is being chunked by the autochunk directive!\"\n+      val responseHeader = RawHeader(\"X-Custom\", \"Test\")\n+      val route = autoChunk(8) {\n+        respondWithHeader(responseHeader) {\n+          complete((StatusCodes.PartialContent, text))\n+        }\n+      }\n+\n+      Get() ~> route ~> check {\n+        chunks must haveSize(10)\n+        val bytes = chunks.foldLeft(body.data.toByteArray)(_ ++ _.data.toByteArray)\n+        new String(bytes) === text\n+        status === StatusCodes.PartialContent\n+        header(\"x-custom\") === Some(responseHeader)\n+      }\n+    }\n   }\n```\n. I've got a change which removes HttpIp but as it turned out that only complicated the usage. Therefore, I conclude that there is some usage in HttpIp so I suggest to postpone any further action.\n\nThat said, I'd say there's some need in a decent model (i.e something else as InetAddress) for IPs/host addresses that would make a useful addition at a later time. So I'm leaving this open as a reminder that this may be useful in the future.\n. Created an akka ticket as well: https://www.assembla.com/spaces/akka/tickets/3635-ssltlssupport-should-adhere-to-the-io-message-protocol-to-upper-layers-and-support-back-pressure\n. :+1:, nice change\n. LGTM aside from the single documentation typo.\n. Thanks for the report. It seems, the generation code would need extra support for windows path separators.\n. I updated the stage to check for pending writes by sending a probe to the connection when the timeout triggers. Only if the probe acknowledges that there are no writes pending the connection is closed down after another timeout period.\n. I updated the PR and fixed the issues.\n. Will be fixed in akka-http: akka/akka#16834.\n. LGTM, apart from the particular case of parameter naming.\n. lgtm\n. lgtm\n. Definitely more robust, :+1:\n\nAlso, it's a bit stupid that `java.net.InetSocketAddress` can both contain unresolved and resolved addresses...\n. Thanks, yes that's a stupid bug introduced with 76345ba. Thanks for the report.\n. Isn't that behavior as spec'd or can you show us otherwise.\n. We currently don't register custom status codes but just generate them from scratch once we observe them.\n\n> I'd vote for creating a separate subclass that is neither an HttpSuccess nor an HttpFailure. I should be able to specify at registration what values I want isSuccess and isFailure to return. \n\nSo, we wouldn't try to reuse status code subclasses for the range 100 to 599 or would we allow overrides of the standard isSuccess/isFailure behavior then?\n\n> allowsEntity should always be true for custom StatusCodes.\n\nHmm, without registering I guess we can't do better. But that also means that it isn't possible to read responses without an entity that are missing a `Content-Length` header. In fact, I already observed it in the tests. To make this work as well we'd have to require registering custom status codes before they can be parsed (which would be fine for me).\n\n> Since we define isSuccess and isFailure as methods on the StatusCode base class, maybe we should remove HttpSuccess and HttpFailure from the public API altogether?\n\nI wouldn't mind but I don't think it would solve the problem  as we would still have to supply `isSuccess` and `isFailure` when we observe a status code without it being registered.\n\nSo, I tend to requiring registration to make it work properly. I'll rework the PR.\n. Updated to fix the discussed issues.\n. LGTM\n. fixed\n. As you suggested the problem was indeed constrained to a particular setup of sizes, in this case related to an internal buffer being of size 1024 and the total compressed data including gzip headers and trailers being just above that size. So, thanks again for reporting so we got to know of this edge case.\n. Thanks for the report. Which version of spray is it? Is it a valid URI? I.e. is this about the exception being thrown or not catched properly?\n. If it is about spray not being able to parse the invalid URI it seems like this is a duplicate of #574.\n. Ah, I understand, sorry for not reading properly.\n. You are right, the fix was easy. We'll try to get it into tonight's build.\n. Thanks @ivantopo. :+1:\n. Updated with new `reference.conf` descriptions.\n. Updated to address the comments.\n. Depends on the upcoming PathMatcher change. Will update then.\n. updated to be based #605 (so merge this first)\n. I guess for the common usages, yes. Strictly speaking, since we are moving the await call somebody could already depend on it.\n. I added https://github.com/spray/spray/issues/599 to collect ideas about what to improve in the future.\n. fixed the merge conflict\n. This was fixed in akka-http.\n. AFAICS, this vast change LGTM. Hopefully, the SSL dragon is now slayed for good. Good job! \n. Client-side retry behavior will be addressed again for akka-http. See #16852.\n. Nice, the tests are now also much more succinct. :+1:\n. In ServerFrontend, we have some code executed only if the backpressure stage is active (search for 'autoBackPressure'). We should think about what to do about it.\n. Maybe you can add the commit you based this on into the commit message. Otherwise, LGTM.\n. Nice, lgtm.\n. How are relative links to be handled? \n\n[Section 5.1](http://tools.ietf.org/html/rfc5988#section-5.1) says:\n\n> If the URI-Reference is relative, parsers MUST resolve it as per [RFC3986], Section 5.  Note that any base IRI from the message's content is not applied.\n\nAnd in section 5.2:\n\n> By default, the context of a link conveyed in the Link header field is the IRI of the requested resource.\n> \n> When present, the anchor parameter overrides this with another URI, such as a fragment of this resource, or a third resource (i.e., when the anchor value is an absolute URI).\n. We discussed this and concluded that doing URI resolution needs to be done on another layer than on the header/parsing level.\n. We'll take this PR over from here and resolve the remaining issues.\n. The match always only matches on the first element of the rejection list while the flatMap in L66 goes over all.\n. Very cool. LGTM.\n. Cool, of course, I didn't even look into RequestContext to see which complete overloads were there additionally. This simplifies everything quite a bit more. :+1:\n. This is superseded by https://github.com/akka/akka/issues/16597. Closing here...\n. How do we prevent stuck half-closed connections? E.g. we send confirmed-close but the other side has gone away in this moment dropping all packets.\n. We'll handle stuck connections with this new ticket directly on the IO level: https://github.com/spray/spray/issues/619\n. Adapted to fix issues.\n. This won't be fixed in spray and currently has no counterpart in akka-http. Closing for now...\n. See https://www.assembla.com/spaces/akka/tickets/3680-io--race-condition-which-overrides-suspendreading\n. Superseded by https://github.com/akka/akka/issues/16552.\n. Already fixed since a while.\n. I remember to have this checked in a branch. Proper back-pressure support/API will come with akka-http.\n. Thanks, lgtm.\n. We have no Windows machines to test so we'd need contributions in this regard.\n. Very nice, in the end it was a pretty small change. :+1:\n. As commented in https://github.com/spray/spray/issues/479#issuecomment-73724381 a better solution is handling backpressure as done in akka-streams.\n. Alternatively, we could introduce optional bounds for the number of repetitions to accept to the `repeated` method.\n. Thanks for the report.\n\nI think you are right about the exception but fixing that won't stop another exception from being thrown out of the parser. If the charset requested isn't available there's no way to support a request. Which is the problematic charset?\n. Yes, I also think there's not much more we can do. Ignoring a charset that we don't know (and falling back on the default charset) is no option as this would mean that we could assume the wrong interpretation of data just because on the machine is a charset missing.\n. Thanks for the report. We'll adapt the model for `X-Forwarded-For` and `Remote-Address` to be sure not to trigger any name lookup.\n. lgtm\n. :+1:\n. Finished updating this. I explicitly added the remaining possible state transitions to make sure we didn't miss something.\n. Shouldn't this be a `!` commit because of the `path` change?\n\nlgtm, otherwise.\n. lgtm\n. Makes sense. :+1:\n. No, the fix is not even merged.\n. There is already a nightly out with the fix included: http://nightlies.spray.io/io/spray/spray-can/1.1-20131101/\n. As spray-can and spray-servlet should be as compatible as possible I'm considering this a bug.\n. Closing this a won't fix known-issue for now.\n. We won't change the spray-can protocol any more and this doesn't correspond to anything in akka-http. Closing...\n. [From the ML](https://groups.google.com/d/msg/spray-user/bEEEXVtoShU/zZl7KXy2EegJ):\n\n> On Thu, Oct 31, 2013 at 3:03 PM, Jim Fulton wrote:\n> This is better. It at least forces people to think harder.  It's confusing\n> though IMO.\n> Here's an alternative, FWIW:\n> \n> ```\n>    ``def apply(key: Any)(expr: => V): Future[V]`` wraps an \"expensive\"\n>   expression with caching support.  If the cache already has a future for the\n>   key, the existing future is returned. If the cache doesn't have an entry for\n>   the key, ``expr`` is evaluated synchronously and the result wrapped\n>   in a future which is added to the cache and returned.\n> ```\n> \n> But really, I think it would be much better to change the semantics of the first case to run the given function in a future. I'd be surprised if anyone objected to this on practical grounds.\n. Can we get this merged and uploaded to the site?\n. Specifying a maximum number is already implemented in akka-http. For also specififying a minimum number, I created akka/akka#16833 which supersedes this ticket.\n. Thanks for the report. However, we'll need more information to be able to\ninvestigate it properly. Can you do the following:\n- try with 1.1-RC2\n- produce a stand-alone project we can use to reproduce the issue\n- upload a memory dump of your project (maybe not with 4G heap size,\n  100-500M should be enough)\n. Once the leak occurred it probably won't be too hard to find out what it is if you take a memory dump (you can do this live with `jmap`) and analyse it (e.g. Eclipse MAT). If you put it somewhere I can look into it. That said, from what you report it doesn't look like a spray issue.\n. > So, rather than requiring people on Windows to work with unix-style working copies I'd say that we should fix all tests that do not yet properly deal with windows style line endings, no?\n\nI agree with that. What are the occurrences where line endings make problems? In many cases it should be triple-quoted strings, right?\n. AFAIU this is fixed with the new marshaller API in akka-http.\n. lgtm\n. \"Unexpected slot state: Idle\" was filed as #652 and was already fixed after RC2\n. :+1:\n. lgtm\n. Superseded by akka/akka#16859.\n. lgtm\n. The other two commits :+1: (apart from the remaining ClientFrontend timeout issue)\n. lgtm\n. :+1:\n. apart from the smallish things :+1:\n. It's related to the `spray.can.server.max-encryption-chunk-size` setting. My guess is that `SslTlsSupport` sends acks too eagerly when only the first part (as governed by `max-encryption-chunk-size`) of a `Write` was sent successfully.\n. @mpilquist were you able to test this fix with a recent nightly?\n. Ok, will do.\n. updated\n. Also make sure the fix is backported to akka.\n. Akka ticket: https://www.assembla.com/spaces/akka/tickets/3728\n. Yeah, of course, I keep forgetting that.\n. Exactly. Thanks, @wongelz. Closing...\n. @matanster, thanks for the suggestion. I'm already trying it another way: we include code snippets using a [custom sphinx extension](https://github.com/spray/spray/blob/master/docs/_sphinx/exts/includecode.py) we copied from akka. We'll try to add the logic there.\n. @matanster the way I tried by getting sphinx to create the links was a dead end. I now implemented it exactly like you proposed in javascript directly on the browser with a directive mapping generated during the build. See #703 and its upcoming PR.\n. Superseded by akka/akka#16860.\n. You can use the `spray.servlet.root-path` setting to make the prefix disappear before it gets to your routes.\n\nWrt what to expect: that depends on the viewpoint. If you view spray as just another framework that works inside an application container you may wonder what's going on. We tend to see spray as a set of libraries for developing REST services. As such we see spray-servlet as a possibility to run spray in a servlet container more than as a possibility to write your servlet with spray.\n\nThat said, we could probably show that fact more prominently in the documentation and maybe add some helper functions which would automatically set `root-path` to the context path as reported by the `ServletContext`. As this isn't the focus of our current development we have to rely on PRs for this feature.\n. Thanks for the report. The problem is that the backslash character is not allowed in cookie values by the spec ([RFC 6265](http://www.rfc-editor.org/rfc/rfc6265.txt)). There's a pretty [good answer in SO](http://stackoverflow.com/questions/6108207/definite-guide-to-valid-cookie-values) about that topic. \n\nThere may be ways to be more lenient about it (and other software can be configured that way), however, as long as any custom behavior is not properly specified you always have the risk of choosing the wrong way of parsing things when you encounter a parsing ambiguity caused by the under-specification of such a feature.\n. Nice, I like how little had to be changed to get there. :+1:\n. Superseded by akka/akka#16832\n. Superseded by akka/akka#16831\n. Superseded by akka/akka#16830.\n. Added CachingDirectives documentation.\n. Added documentation for chunking + cookie directives\n. I extracted the routing changes into a separate commit to make reviewing easier, see #714. This PR is then based upon those changes.\n. lgtm, I'll merge and rebase my PR on top of it.\n. LGTM\n. This still needs to be done for `encodeResponse`.\n. This was just not yet deployed. Closing...\n. We currently won't fix this in spray. This is \"fixed\" in akka-http by not relying on actors anymore and not giving so strict guarantees about what is executed where.\n. We will try to address this in akka-streams/http. Superseded by akka/akka#16829.\n. :+1:\n. lgtm\n. Added search box. Needs to get styled properly.\n. Interesting, maybe it also needs to be properly specified if and what parts of error responses are expected to be localized.\n. LGTM. Thanks, Andr\u00e9.\n. lgtm, otherwise.\n. We see this currently as a one-off bug with the XML API. While consistent error message localization is nice we will be revisiting issues as they crop up.\n. Thanks @2beaucoup for looking into it. \n\nFTR: IMO the problem is several somewhat unrelated (but platform-correlated) things play together in a way that everything seems to work as long as your doing everything on one machine but breaks down easily when compilation and runtime are separated. So, any solution will still be brittle as long as it still contains some implicit assumptions.\n\nThese things are:\n- the user configuring git to fix/change line endings of checked-out files for compiling\n- the value of EOL being set correctly at runtime\n- the supposed meaning of literal newline characters in multiline strings\n\nFor me the consequences are:\n- you need to be explicit about what literal newline characters are supposed to mean in each single multiline string\n- you need to keep in mind that code is not necessarily compiled on the same computer as it is run, and that strings that were compiled on one machine, processed while the program is running on another machine, may even be interpreted on a third machine\n\nI think your solution works for the tests because you made the assumption that tests are always run on the same machine as they were compiled (and I'd agree with this assumption). Otherwise, the runtime value of EOL isn't necessarily the same as the one multiline strings were originally compiled with. So, I'm fine with your solutions for the tests. \n\nFor the `FileAndResourceDirectives` I think the right solution would be to make sure that the value of the multiline string always has the same meaning regardless of where it was compiled and where it will run later. In lack of better solutions maybe just use something like `stripMargin.replaceAll(\"\\r\\n|\\n\", \"\\n\")`.\n. Maybe we can just add a new `stripMarginWithNewline(newline: String)` to `spray.util.pimps.PimpedString` which does `string.stripMargin.replace(\"\\r\\n\", \"\\n\").replace(\"\\n\", newline)` and then use `.stripMarginWithNewline(\"\\n\")` or `.stripMarginWithNewline(\"\\r\\n\")` for all multiline strings in the tests.\n. :+1:, thanks @2beaucoup \n. Superseded by https://github.com/akka/akka/issues/15833\n. Following [the spec](http://tools.ietf.org/html/draft-ietf-httpbis-p1-messaging-25#section-3.2.6) it **is** illegal. The only thing we could do is trying to be more lenient for informal headers like this for convenience.\n. Yep, from looking into the source code it was changed by now (see `src/init.c` and look for `uagent`). It seems you can overwrite the user-agent using `-A` or `--user-agent`, have you tried that?\n. I see two issues with this proposal:\n- we need either a new entry-point for the HttpParser that can return several values or special handling for cookie values somewhere else\n- being lenient may lead to ambiguities like in #702 where we need to employ a non-standard reading of backquoted doublequotes to be able to finish parsing\n. Superseded by akka/akka#16828\n. This change seems to expose cookies (and other headers) which are not meant for the new host to the new host. Cookie always comes with security implications (see also previous https://github.com/spray/spray/pull/311). \n\nThe first thing we would need to push this further is some investigation how things should work by spec and how browsers and other notable clients are implementing redirection exactly. Once we are sufficiently sure to understand what the current consensus is how things should work and what the security implications are we can start implementing them.\n\nFor the time being I'd suggest to disable automatic redirection and implementing your own redirection logic as ugly as this workaround may be, unfortunately.\n. Maybe we should take this PR as is as the binary compatible fix to the (undisputed) broken rendering in the current version and then create another ticket to improve the model for the next version. WDYT?\n. I added #753 to remind us of improving the model in the future.\n. Thanks, @2beaucoup for the fix.\n. See [section 5.2.3 of RFC3986](http://tools.ietf.org/html/rfc3986#section-5.2.3)\n. LGTM. Flavio, can you change the commit message to \"= can: avoid parsing attempt after complete input has been consumed\" and sign the [Typesafe CLA](http://typesafe.com/contribute/cla).\n. Hopefully fixed by cd1cfb8. Once the 1.2.1 RC is out, can someone please check, that it is indeed fixed? Thanks.\n. HttpDialog is legacy API that we won't fix for now.\n. This won't be changed in spray but is superseded by akka/akka#16827.\n. Thanks, @ivantopo for the friendly reminder. We're still alive and kicking :) A 1.x.1 release is planned but I cannot make promises yet when we will actually get to it. In any case it's good to know that you have tested this PR and find it useful.\n. We decided that we'd like to keep the maintenance release as binary compatible as possible so this PR won't make the cut. \nThere's nothing wrong with the PR itself so we'll probably merge it sooner or later into the cutting edge branch for collecting things that we need to consider during the move to akka-http. Hopefully that's ok for you, @fernandezpablo85.\n. Makes sense. Thanks, @2beaucoup. :+1:\n. Thanks!\n. Superseded by akka/akka#16826\n. Thanks, @bthuillier. I don't think we want to change `host` and `scheme` directives the way you've done here. Currently, most directives either filter (in the meaning of `hrequire`) or extract value and for most people this makes the most sense. Doing both at the same time may seem elegant from an implementation point of view but makes the usage a bit more awkward for no good reason. So, I think we'll go with your first commit (+ added tests). Could you sign the CLA, as well? Thanks! http://typesafe.com/contribute/cla\n. We won't change the behavior in spray any more but I created akka/akka#16825 to review the situation with akka-streams/akka-http.\n. Superseded by akka/akka#16824\n. Discussing this a bit more we found that one problem of this directive is that it may be too easy to mess up security: what to do with cookies, authentication data, etc.? Should spray really just relay all of it to another URI without any pre-processing?\n\nThe directive is simple enough so if plain relaying is what you want, you can add it to your app in just a few lines of code. So, we are currently a bit reluctant to risk adding a potential security problem to the spray codebase without completely understanding the consequences.\n. The new ticket to target is akka/akka#16844.\n. Thanks, Mathias. LGTM. Can you add the same change to the spray-routing/on-jetty example?\n. Thanks Mathias, I took the freedom to squash both of your commits into one and merge that one. See 3a4c0b20681652afa85a5ed4fdbe33a9b9b9ab95\n. Closed by 3a4c0b20681652afa85a5ed4fdbe33a9b9b9ab95.\n. Thanks @guersam. Sorry, I overlooked your PR and made the same change anew. Your change regarding `sender` is indeed the right way to go, so we'll just cherry-pick that one in addition to ours that updated the Akka dependency.\n. This is already referenced in the overarching akka-http host-level client-side API ticket akka/akka#15681. Closing here.\n. See https://groups.google.com/d/msg/spray-user/YN2ocRzwhY0/KJOegaDIep8J\n. The integration is now tracked for akka-http in akka/akka#16856.\n. @okapies this seems to work as Mathias said: an illegal `Origin` header will be parsed as a `RawHeader`. Can you give us more information how this \"crashes spray's pipeline\"?\n. Closing for now. Please reopen, if this is still an issue and you can provide more information about your use case.\n. Thanks for the clarification. You can turn off those warnings by configuring `spray.can.server.parsing.illegal-header-warnings = off`.\n. Thanks, Age. Not sure when we'll be able to actually look into it but it seems like a very useful addition. Btw. currently the tests are failing with this mysterious message:\n\n```\n[info] x If-Modified-Since\n[error]    'Right(If-Modified-Since: Wed, 13 Jul 2011 08:12:31 GMT): scala.util.Right'\n[error]     is not equal to \n[error]    'Right(If-Modified-Since: Wed, 13 Jul 2011 08:12:31 GMT): scala.util.Right' (HttpHeaderSpec.scala:364)\n```\n. Maybe the milliseconds missing in the string representation differ?\n. Closing as superseded by #797\n. > ``` scala\n> \"If-Modified-Since\" in {\n>   \"If-Modified-Since: Wed, 13 Jul 2011 08:12:31 GMT\" =!= `If-Modified-Since`(DateTime(2011, 7, 13, 8, 12, 31))\n> }\n> \n> \"Last-Modified\" in {\n>   \"Last-Modified: Wed, 13 Jul 2011 08:12:31 GMT\" =!= `Last-Modified`(DateTime(2011, 7, 13, 8, 12, 31))\n> }\n> ```\n> \n> So I'm not sure what could have gone wrong there... any ideas?\n\nWhat was wrong here in the end?\n. Right, that's a likely cause.\n. Then this is a documentation problem which contains the line\n\n> spray-http is stand-alone in that it has no dependency on any other spray module and not even Akka. You can therefore use it directly and independently for any other type of HTTP work you might be doing.\n\nThis particular error is created by the Uri depending on the `spray.util` package object which triggers the load of `akka.actor.ActorRefFactory`.\n. Have you checked that it works, @TulioDomingos ? Otherwise, it looks simple enough to me. :+1: \n. Excellent. Thanks again.\n. Thanks @kulikov, good catch. Could you add a test checking the behavior? We'd also need you to sign the [Typesafe CLA](typesafe.com/contribute/cla). And could you please also change the commit message to `= routing: handle exception in onComplete directive` according to [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages). Thanks!\n. Also, we would need this patch to be based on the master branch. Can you rebase?\n. :+1: very nice result and outstanding cooperation. Thanks @agemooij, @2beaucoup and @sirthias.\n\nMissing points: \n- tests as hinted by @sithias last commit message\n- maybe try/finally in FileAndResourceDirective\n- documentation\n. Yay!\n. Reproducible with the 20MB data instead of 500k.\n. Updated.\n. :+1:\n. IIUC this feature doesn't work together with separate compilation. So, I don't think it is something we can fix. If you have other info, please reopen.\n. The question is if you can still use it together with other software not built with the same flag.\n. Here's a branch that shows what can happen if you use the flag without using it in all projects:\n\nhttps://github.com/jrudolph/spray/compare/w;max-classfile-name-breakage?expand=1\n\nRun `test:run` in project `spray-routing-tests`.\n. Btw. it might be that adding the flag doesn't break something _right now_ but that's just coincidental because the flag is more likely to apply to classes with long names which in many cases are anonymous, implementation-private classes that won't be referenced from third parties. However, as the example shows, it could happen to public identifiers as well which could lead to silent breakage.\n. Superseded by akka/akka#16823\n. I guess the problem is that doing the update on the master branch is harder to do. The question is if the migration effort to update to sbt 0.13 required is currently worthwhile the outcome: the spray codebase is currently mostly in maintenance mode with the active development currently being switched over to the akka codebase. Any change to that \"running system\" always comes with the risk of disrupting our well-understood and working deployment process.\nSo, closing for now...\n. I created #822 to further improve the situation once this has been improved in Akka.\n. Would it still be possible to distinguish parameter validation problems and entity content validation problems? In the case you show it seems clear enough why you'd the behavior to be consistent in other cases it may be less clear cut.\n. Ah, I understand. I agree that none is more special than the other. Maybe it's more that `ValidationRejection` is _too_ generic. In the best case, the rejection should contain as much information as possible. And that should be both a) which kind of validation failed b) which data of the HTTP request was flawed. I'm fine with everything that retains both parts of failure information.\n. > Care to elaborate on why/when/how you'd want to access these additional infos?\n\nBecause someone may want to handle validation errors differently if they occur in the entity body than when they happen during parameter validation. I'm not sure but I think it could be useful.\n. Superseded by akka/akka#15792. Does this cover everything, @2beaucoup ?\n. Oops, seemed I referenced the wrong issue in that commit.\n. This has no tests because I've not managed to reproduce it in a controlled environment. I think it must happen with some combination of connection closure during the handshake. We are missing a bit of test-infrastructure that would allow us to drive a standalone SslTlsSupport stage into exactly the problematic state (even harder if you don't know which it is exactly...).\n. You need to send one write to the SSL stage and then wait for its Ack to send the next. There is no way around this. Tests may run through in some cases (like on localhost) but that doesn't mean that it will work in general (it doesn't).\n. It will not work depending how the race conditions work out. The test framework isn't sophisticated enough to capture those kinds of race conditions reliably. \n\nIn general, if you need a reliable stream (both for akka.io TCP connections or the same when using the SslTlsSupport stage) you mustn't use any Writes without acks and you must never send a subsequent Write without having received the previous ack. That's only way to be sure that it will work under any kind of backpressure. If you need buffering support you need to build it on top of that.\n. I think, as @sirthias said above, it is already possible and very simple to build a directive that works on a `Future[Boolean]` so I'm closing this for now.\n. Thanks, @hierynomus for reporting.\n\n> However spray-client will not correctly receive the response and obtain the status code.\n\nWhat will it do instead? And what do you think should be the expected behavior?\n. We will add this in akka-http. Superseded by akka/akka#16822.\n. Thanks, @gkossakowski. One problem I see with merging this right away is that there's nothing _we_ can do with these changes. We cannot build for 2.11 even with these as long as our dependencies have not yet been upgraded. \n\nEven when they come available we may want to setup a new branch for 2.11 support because most dependencies won't be cross-published in exactly that version that we need and we may need other code changes as well. \n\nAnother concern is that we don't want to have the build definitions drift to much between our release branches and the master branch because that would increase the burden of releasing even further.\n\nSo, I'm a bit reluctant to merge this into our stable release branch. What we should do is starting a new branch for 2.11 support starting off from `release/1.3` which has a clear experimental marker and which you can use for the community build.\n. I've created the `unstable/1.3-scala-2.11` from the `release/1.3` branch. \n\nCan you\n- update the PR to target this new branch\n- rewrite the commit message according to [our policy](http://spray.io/project-info/contributing/#git-commit-messages) to be prefixed with `= build:`\n- also add the changes that are created by updating the scalariform plugin (which also seem to include a regression wrt to pattern match formatting)\n\nThanks!\n. LGTM.\n. Yep, thanks, I guess that would make sense.\n. Thanks, @topping for explaining the use case. \n\nEven if the change is a small one it is still a hack. It cannot be anything else as long as there's no \"official\" support for pluggable protocols to use with upgrade which must be handled internally in spray-can. \n\nThat said, we may still include a patch like this to reenable spray-websockets. I think what we would need is\n- some evidence that this cannot be fixed inside spray-websockets (e.g. by including patched renderer code into the spray-websockets codebase for the time being) with reasonable effort (the effort required to do a spray release is not negligible and likely not a short-term solution)\n- some more tests that this works (or doesn't break anything) also for other combinations of HTTP protocol, client and server provided Connection values, and multiple Connection values\n- comments in the code why we need the line in question\n\nWDYT @sirthis?\n. Thanks @dcaoyuan, I didn't even realized _which_ one of the spray-websocket projects this is about :) \n\nI wonder, why do you need those changes specifically? It seems you could implement `hasUpgrade` wherever you need it, right?\n\nThe point is that we don't want to (or better: may get to) make a release *) in the next months if we can avoid it, so it would be great if we there'd be a workaround for now that doesn't need any spray changes. \n\nApart from that we can, of course, discuss any PRs for the next version solving this \"properly\" so that you can get rid of any needed workarounds sooner or later.\n\n*) In spray \"a release\" still means making all of those 5 releases: 1.0.x, 1.1.x, 1.2.x, 1.3.x, 1.3.x for Scala 2.11, which always need some documentation updates etc. and all-in-all is always quite a big undertaking.\n. It's here: http://repo.spray.io/io/spray/spray-io_2.11/1.3.2-20140909/\n. Compilation seems to fail when the PR is run using Scala 2.10.3. Is this expected? Is there any reason those compilation errors should go away with Scala 2.11.x?\n\n```\n[error] /home/travis/build/spray/spray/spray-routing-tests/src/test/scala/spray/routing/FormFieldDirectivesSpec.scala:48: ambiguous implicit values:\n[error]  both method forTuple in object FieldDefMagnet2 of type [T <: Product, L <: shapeless.HList, Out0](implicit hla: shapeless.Generic.Aux[T,L], implicit fdma: spray.routing.directives.FieldDefMagnet2.FieldDefMagnetAux[L,Out0])spray.routing.directives.FieldDefMagnet2[T]{type Out = Out0}\n[error]  and method forHList in object FieldDefMagnet2 of type [L <: shapeless.HList](implicit f: shapeless.ops.hlist.LeftFolder[L,spray.routing.Directive0,spray.routing.directives.FieldDefMagnet2.MapReduce.type])spray.routing.directives.FieldDefMagnet2[L]{type Out = f.Out}\n[error]  match expected type spray.routing.directives.FieldDefMagnet2[shapeless.::[Symbol,shapeless.::[spray.routing.directives.NameReceptacle[Int],shapeless.::[spray.routing.directives.NameReceptacle[Option[String]],shapeless.::[spray.routing.directives.NameDefaultReceptacle[Boolean],shapeless.HNil]]]]]\n[error]         formFields('firstName :: (\"age\".as[Int]) :: ('sex?) :: (\"VIP\" ? false) :: HNil) { (firstName, age, sex, vip) \u21d2\n[error]                               ^\n[error] /home/travis/build/spray/spray/spray-routing-tests/src/test/scala/spray/routing/FormFieldDirectivesSpec.scala:148: diverging implicit expansion for type spray.routing.directives.FieldDefMagnet2[spray.routing.directives.RequiredValueDeserializerReceptacle[Int]]\n[error] starting with method forTuple in object FieldDefMagnet2\n[error]         formFields('oldAge.as(Deserializer.HexInt) ! 78) { completeOk }\n[error]                                                    ^\n[...] many more, see the Travis build\n```\n. Nice :+1:. Thanks @2beaucoup \n. Superseded by akka/akka#16821\n. I cannot say how relevant this is. If there's a strong demand to support the proxy protocol please reopen this ticket for akka-http.\n. Cool, glad you figured it out.\n. Superseded by akka/akka#16820\n. Btw. as a workaround it should be possible to make a local copy of the relevant parts of `FormDataUnmarshaller` with the problem fixed and bring that implicit into scope where needed.\n. > I need to be able to parse and forward any legitimate HTTP traffic.\n\nWouldn't this work in any case because a \"broken\" header will still be passed through as a `RawHeader`?\n. > being a proxy, I want to apply the Cache-Control semantics, so I need it to be parsed.\n\nYes, that makes sense, of course :)\n. I checked it, this one fails:\n\n``` scala\n      \"Cache-Control: private, no-cache, no-cache=Set-Cookie, proxy-revalidate\" =!=\n        `Cache-Control`(`private`(), `no-cache`, `no-cache`(\"Set-Cookie\"), `proxy-revalidate`)\n```\n\nI guess it's because we require quoting while the grammar from httpbis allows a plain `token` as well:\n\n```\n     Cache-Control   = 1#cache-directive\n\n     cache-directive = token [ \"=\" ( token / quoted-string ) ]\n```\n. Also, there's another bug in our parser which parses\n\n```\n\"no-cache\" [ \"=\" 1#quotedString ]\n```\n\ninstead of\n\n```\n\"no-cache\" [ \"=\" <\"> 1#field-name <\"> ]\n```\n\ni.e. it would allow multiple quoted field-names instead of allowing multiple field names **inside of** quotes which introduces an ambiguity in the parser.\n. Yep, done.\n. Thanks, that's a good point. Does it fail in an OSGI container with the dependencies declared as required if the actual jars are missing? (Just estimating the severity of this problem.) \n\n@mpilquist is there any reason we didn't add those before?\n. I had a try at fixing those issues and managed to produce a version that compiles and tests run fine. This is no promise that we'll actually publish a shapeless2 version of spray-routing but you can at least try it yourself. See https://github.com/spray/spray/tree/release/1.3-shapeless2\n. It works for me. Can you post code that observes this fact? The `- 1` should strip off the trailing `$` character of the singleton object's class name (which is an scala compiler implementation artifact).\n\nSee:\n\n``` scala\nscala> HttpHeaders.`Content-Length`.getClass.getName\nres0: String = spray.http.HttpHeaders$Content$minusLength$\n\nscala> HttpHeaders.`Content-Length`.name\nres1: String = Content-Length\n```\n. Closing... couldn't be reproduced.\n. Thanks for reporting. It seems RFC 2965 was obsoleted by [RFC 6265](http://tools.ietf.org/html/rfc6265) which doesn't allow `\",\"` any more. It may still make sense to look at the grammar of the older version and be a bit more lenient about what to accept if it is possible to do so unambiguously.\n. I would consider this now a bug as it has introduced an alternative interpretation of Cookie headers that is not consistent with what browsers implement.\n. I think we need a specification reference to warrant this change. The only thing I found on a quick glance was this line from [rfc822](http://tools.ietf.org/html/rfc822#section-4.1):\n\n```\n     message     =  fields *( CRLF *text )       ; Everything after\n                                                 ;  first null line\n                                                 ;  is message body\n```\n\nwhich would mean that no extra CRLF is necessary if the `text` is empty.\n\nAlso the current spec ([rfc2046](https://tools.ietf.org/html/rfc2046#section-5.1.1)) for multipart messages has this in its grammar:\n\n```\nbody-part := MIME-part-headers [CRLF *OCTET]\n```\n\nWe should probably make that `CRLF` optional for parts with an empty entity.\n. That said, we should make sure that all round-trips using valid grammars work properly. So, thanks for the test cases. That's helpful.\n. The grammar you posted seems to be on one level up: it's the thing that delimits several body-parts into one multipart message.\n. Will be fixed if we get to fix https://github.com/spray/spray/issues/971\n. Thanks, @RichardBradley, for the report. I agree this could be improved.\n. This could probably now quite easily be fixed by using the new `DeadLetterSuppression` trait introduced with Akka 2.3.8 for the messages in question. See https://github.com/akka/akka/issues/15163.\n. Thanks @RichardBradley, good point. I agree we should properly validate user input and report problems using the right channels where possible. Another point we should improve is to disallow URLs like the one you gave by only allowing http/https URIs for the normal constructor and require an alternative constructor for other/custom schemes.\n. Yes, you are right, only (1) was fixed. (2) is somewhat harder to achieve because what fails here is the `HostConnector` which is a standalone entity that cannot be easily associated with a single request. I think the policy should be to properly guard against user errors and report them back and rely on other mechanisms (timeouts, supervised restarting, logging) to handle other issues.\n\nEspecially, in cases like this, an temporary `HostConnector` outage isn't really much different from any kind of temporary network outage which means that user code always needs to deal with these kinds of situations anyways.\n. I'd say we do the quick and simple option (3).\n. I think this PR needs a bit more explanation about what it is supposed to achieve :) We currently build the Scala 2.10 and the Scala 2.11 version from different branches. Are you suggesting to build both versions from this single branch?\n. We know about this problem and also the more general problem of hard to avoid `DeadLetter`s when a  network of inter-depending actors is terminating. We have not yet found the general solution to that problem.\n\nThis particular problem could probably be solved by `unwatching` the handler as soon as the connection knows that it will be able to cleanup itself without any further input from the handler.\n\nThe related problem of avoiding `DeadLetter` reports is also discussed here:\n\nhttps://github.com/akka/akka/issues/15163\n. We don't have a solution to this problem but it is a valid problem so I keep it open for spray.\n. Which parts of spray would you like to use with scala-js? spray isn't likely changing enough to be running on scala-js. And I guess it will be even harder to get akka-http working on scala-js. So, I'm closing this as won't fix for now.\n. Thanks @ryryguy. Interesting idea. \n\nThinking about it, the problem is that the scoping will not work properly: The request timeout will always be applied even if the inner route didn't match. That's not what users would expect.\n. Superseded by akka/akka#16819\n. Superseded by akka/akka#15799.\n. Hi Jisoo,\n\nwe mostly worked from the httpbis draft since quite a while. That means\nthat we may have incorporated already lots of the now finalized changes. We\nwill not update spray to incorporate _all_ of the changes that are still\nmissing just for the sake of it. If something concrete needs to be changed\nwe will decide on a case-by-case fashion as usual.\n\nJohannes\n\nOn Thu, Jun 26, 2014 at 11:23 AM, Jisoo Park notifications@github.com\nwrote:\n\n> I've wrote a document aggregating the changes after the death of RFC2616:\n> https://gist.github.com/guersam/2a3d61d6429f62a43ee6\n> \n> There's no doubt akka-http will supprt new RFCs, but is there migration\n> plan for Spray as well?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/spray/spray/issues/895.\n\n## \n\nJohannes\n\n---\n\nJohannes Rudolph\nhttp://virtual-void.net\n. Closing... @dckc if you have more information or anything we can do about it please reopen.\n. Fixed by 6a78ade\n. Superseded by akka/akka#16818\n. Thanks @dvorobiov for the report. Can you post the wrongly generated JSON value as well?\n. Where did you post it? It seems it got lost somewhere :)\n. I think the problems happens on another level. Can you show the code where you actually build the request?\n. I think this is a case of the greedy lift json marshaller allowing code that shouldn't have compiled in the first place.\n. Thanks, @dlouwers, good point. It seems the example is wrong.\n. Btw. you don't need `respondWithMediaType` as the `FileAndResourceDirectives` will usually figure out the correct type from the file name extension.\n. We should probably deprecate the jsonp directive and add a warning about the security implications of using it.\n. Thanks, @sksamuel. \n\nMaybe it was just a typo, \"1968\" vs. \"1986\"? Do you have any specification source for this change?\n\nBtw. would you please also change the commit message itself to `= http: Added missing alias for US-ASCII`. We'd also need you to sign the [latest version of the Typesafe CLA](http://www.typesafe.com/contribute/cla).\n. Thanks, I see, that's what I also just found. Could you add a comment line above with the IANA link so we don't have to search it the next time.\n. Here's the test a bit adapted from the original post:\n\n``` scala\npackage spray.httpx.encoding\n\nimport org.specs2.mutable.Specification\n\nimport scala.util.Random\n\nclass CompressionTest extends Specification {\n\n  val random = new scala.util.Random\n\n  // Generate a random string of length n from the given alphabet\n  def randomString(alphabet: String)(n: Int): String =\n    Stream.continually(random.nextInt(alphabet.size)).map(alphabet).take(n).mkString\n\n  // Generate a random alphabnumeric string of length n\n  def randomAlphanumericString(n: Int) =\n    randomString(\"abcdefghijklmnopqrstuvwxyz0123456789\")(n)\n\n  \"Compression\" should {\n\n    val compressor = new DeflateCompressor()\n    val decompressor = new DeflateDecompressor()\n\n    \"Work\" in {\n\n      var totalBytesCompressed = 0\n      var totalCompressedBytes = 0\n      (1 to 1000000).foreach { i \u21d2\n        val inputString = randomAlphanumericString(64)\n        val input = inputString.getBytes(\"UTF-8\")\n        val output = compressor.compress(input).flush()\n        val result = decompressor.decompress(output)\n        val outputString = new String(result, \"UTF-8\")\n        totalBytesCompressed += input.size\n        totalCompressedBytes += output.size\n\n        if (inputString != outputString) {\n          println(s\"      INPUT $inputString\")\n          println(s\"     OUTPUT $outputString\")\n          println(s\" INPUT SIZE ${inputString.size}\")\n          println(s\"OUTPUT SIZE ${outputString.size}\")\n          println(s\"Output is prefix ${inputString.startsWith(outputString)}\")\n          println(s\"At $i total input bytes $totalBytesCompressed total compressed $totalCompressedBytes\")\n          throw new Exception(\"That hurts\")\n        }\n\n        inputString === outputString\n      }\n\n      true === true\n    }\n  }\n}\n```\n. I tested a bit more and I couldn't find a case where the current scheme would produce a corrupt stream. What does happen is that our current `flush` isn't always able to put the SYNC at exactly the position indicated so that it looks as if the stream was truncated by one (or a few) bytes. However, those bytes aren't lost and will be decompressed later on. So, this never produces a broken stream for me when the complete stream was properly compressed and `Compressor.finish` was called at the end.\n\nIf you have observed corrupted data on a higher level can you post some higher-level test code that demonstrates it?\n. Thanks, @maspwr. Could you please sign the [Typesafe CLA](http://www.typesafe.com/contribute/cla)?\n. Thanks, @maspwr !\n. Thanks! :+1:\n. Thanks, @maspwr !\n. Superseded by akka/akka#16817.\n. I agree it could be useful. One problem is that the RouteResult is basically is a future on an eventual result (either normal response, chunked response, or rejected) so you would have to wait for both results to be available before being able to compare them.\n\nAs this would need binary incompatible changes I don't think we will introduce this for spray anymore. A workaround that works now would be to implement a matcher for your test framework that does the comparison.\n. It's because it constantly trips people up who try to C&P the method which contains the line and then use it several times. As this line will globally close all connections on the ActorSystem all kinds of strange things happen. It should rarely be necessary to use it at all.\n. Thanks, @benmccann, that's a good point. \n\nCurrently, most artifacts are already also on maven central so the short-term workaround would be to rely on maven central wherever possible and remove documentation pointing to repo.spray.io.\n\nWe also need to think about whether for how long we really want to support our own repo at all or if we want to publish to oss.sonatype / maven central directly which should have the more professional infrastructure to host artifacts.\n. Created issue in akka-http as akka/akka#16803\n. I'd like to know if the concern I voiced in #737 about \n\n> being lenient may lead to ambiguities like in #702 where we need to employ a non-standard reading of backquoted doublequotes to be able to finish parsing\n\nis tested and addressed in this PR.\n\nI think we also need more test coverage, discussion, and documentation what exactly to expect in the erroneous or ambiguous cases like the one reported in #869 and #702.\n. @caspark thanks for giving your opinions and your contribution. Adding no _further_ ambiguity is certainly good. :)\n\nI agree that we need to make a decision about how to proceed. I think we need to collect all cases that we have seen up to now and need to make sure we find a solution we can keep and improve on for a while.\n. Thanks, @josephw. This seems reasonable. Could you please:\n1. point us to the W3C recommendation and also add the link to the commit\n2. sign the [Typesafe CLA](www.typesafe.com/contribute/cla)\n3. Squash your changes into one commit (they both deal with the same issue, right?)\n4. Adapt the commit line according to [our policy](http://spray.io/project-info/contributing/#git-commit-messages) and add the link to the W3C recommendation.\n\nThanks!\n. Try this:\n\n``` scala\nget{\n      pathEndOrSingleSlash { // instead of path(\"/\")\n        complete(\"done\")\n    } ~ post{\n      path(\"auth\"){ // no slash here\n        redirect(\"/\", StatusCodes.Found) // TemporaryRedirect / 307 won't redirect to GET but to POST on the browser\n      }\n    }\n```\n\nThe important thing to know is that the `path` directives will automatically match a leading slash.\n\nPlease see the documentation about the [path directive](http://spray.io/documentation/1.2.1/spray-routing/path-directives/path/#path) for more information and use stackoverflow for these kind of questions (or the mailing list for the more difficult problems). Thanks!\n. The LdapAuthenticator is basically unmaintained. If someone cares for it we would probably have to resurrect it as a contrib module. For now, it's more likely that we'll get rid of it than to make changes to it. Closing...\n. @maspwr thanks for volunteering. In case it wasn't clear: I wasn't implying that Ldap support isn't useful or that it is broken. It's just that with most activity now gradually moving over to akka-http we are in the process of clarifying the boundaries of what parts of the existing spray codebase may be most worthwhile to be maintained in the future.\n\nFor us, the easiest solution would be if the Ldap support would live and be maintained in its own repository as a standalone module depending on spray. Would that be an option for you?\n. Yes, sounds good.\n. The reason it is not explained is that - in theory - this has almost nothing to do with spray and Java SSL configuration knowledge is needed to do this properly. That said it would still make sense to provide some pointers in the documentation about how to do this.\n. Let's start with the easy one:\n\n> What is the spray behavior and what triggers it?\n\nSpray won't be able to decode partial gzip headers, so if you happen to receive just the first few bytes (less than 10, that seems to be a common size of a gzip header) of a gzip encoded entity, spray will bail out.\n\n> What does \"failing in caves\" fatally mean?\n\nNever thought about running a spray server inside a cave :skull: and linking it to the outside world using something like the [Cave-Link](http://www.cavelink.com/cl3x/index.php/en/) ? With an expected transmission rate of 104 baud, the aforementioned situation seems much more likely then in less restricted environments... :ghost:\n\n\"Fatally\" is an exaggeration I guess, because only the current connection is affected.\n. Relevant part of the spec: http://tools.ietf.org/html/rfc7232#section-2.4\n. It says:\n\n> In other words, the preferred behavior for an origin server is to send both a strong entity-tag and a Last-Modified value in successful responses to a retrieval request.\n\nSo, indeed, even if not recommended it should be possible to specify only one those parameters. We will improve the situation in akka-http.\n\nThanks for raising this issue.\n. Superseded by akka/akka#16816\n. Superseded by akka/akka#16815\n. I think this is invalid. In spray, an `AuthenticationFailedRejection` comes with very specific semantics to trigger a particular rejection handling behavior. Introducing more causes here would mean, that any rejection handler would need to support custom causes. Instead, I would propose that if you need to carry over additional data to a custom rejection handler you can also provide a custom rejection that does exactly this.\n. Fixed by 1539e49.\n. @analytically as a workaround, can you try if adding the testkit dependency as \n\n``` scala\n\"io.spray\" %% \"spray-testkit\" % \"1.3.2\" % \"test\" exclude(\"org.specs2\", \"specs2_2.11\")\n```\n\nwould work?\n. What happens exactly?\n. Thanks, that's good to know and relieves us of the pressure to issue an immediate fix release because of this issue.\n. This could be a trait / val initialization problem. Try changing `authenticator` and `routes` into `lazy val`.\n. Referenced this issue in a comment of akka/akka#15681\n. No, it isn't implemented and I doubt it will given that this proposal is from 1996 and was never implemented or spec'd properly in any of the URI RFCs. You'll need to create your own directive.\n\nClosing for now...\n. @wedens Thanks for the report. This is a bug. \n\nUntil it is fixed can you try something similar to\n\n``` scala\nval ShortNumber = IntNumber.flatMap(i => if (i <= Short.MaxNumber) Some(i.toShort) else None)\n```\n. It's `Short.MaxValue`, of course:\n\n``` scala\nval ShortNumber = IntNumber.flatMap(i \u21d2 if (i <= Short.MaxValue) Some(i.toShort) else None)\n```\n. Recreated as akka/akka#16814.\n. Which versions of spray / akka are you using exactly?\n. I may be missing someting but I don't see any component of spray depending on quasiquotes. Can you either provide some kind of reproduction or maybe try [sbt-dependency-graph](https://github.com/jrudolph/sbt-dependency-graph) or just look into `target/resolution-cache/reports/*-compile.xml` to find out what actually depends on quasiquotes?\n. Done.\n. Have you an `import MyProtocol._` somewhere? Then it should just be\n\n``` scala\npath(\"\") {\n  get {\n    complete {\n            // returns List[Business] \n            Recommender.recommend(1000)\n           // this returns the json response thoug\n          //               List(new Business(\"Jewel Thief\"),new Business(\"Abby's Confectionary\"),new Business(\"PF Changs\"))\n    }\n  }\n}\n```\n\nBtw: these kind of questions are better handled on the [mailing list](http://groups.google.com/group/spray-user) or on http://stackoverflow.com. Closing...\n. This is a somewhat \"well-known\" but unfortunate incompatibility we had to introduce. Try the latest spray-json version 1.3.1\n. There doesn't seem to be a new line in this code?!? Can you provide an example of the output you are seeing?\n. The new line is coming from the error reporting of the HttpParser. I agree, it could make sense to limit the kind of parser reporting to log, also to avoid DOS attacks by maliciously crafted input that would create lots of (or expensive) parser error reporting.\n. We should change the default logging behavior to just log a summary and make the full log output configurable for debugging purposes.\n. Thanks @ruurtjan. Would you [sign the Typesafe CLA](http://www.typesafe.com/contribute/cla) and update the commit line to our [commit message convention](http://spray.io/project-info/contributing/#git-commit-messages) which would be \n\n```\n= docs: fix wikipedia link\n```\n. Move to akka by adding a note to fix this to akka/akka#15934.\n. See http://spray.io/documentation/1.2.2/spray-http/#content-type-header\n\nYou need to have an `HttpEntity.NonEmpty` to get access to the content type. Using `request.entity.toOption` that's actually what you get. So this should work:\n\n``` scala\nrequest.entity.toOption match {\n  case Some(entity) =>\n    val contentType = entity.contentType\n  case None => null\n}\n```\n\nAn alternative would be:\n\n``` scala\nrequest.entity match {\n  case Some(HttpEntity.NonEmpty(contentType, data) =>\n    // use contentType\n  case None => null\n}\n```\n. We need more information that shows that spray is at error here. If this is still an issue and you can show this only happens with spray and not with json4s alone, please reopen.\n. Thanks, @ches. \n\nIndeed, it seems that most links are broken. They are generated by `highlight-directives.js` on the browser which makes use of an sbt generated `directives-map.js` which contains a mapping of directives to the \"group\", i.e. the file, containing the directives. In this file, groups seem to be empty for most directives:\n\n``` js\nwindow.DirectivesMap = [\n{\n\"group\": \"\",\n\"entries\": \"mapInnerRoute mapRequestContext mapRequest routeRouteResponse mapRouteResponse mapRouteResponsePF mapRejections mapHttpResponsePart mapHttpResponse mapHttpResponseEntity mapHttpResponseHeaders noop pass provide hprovide extract hextract\"},\n{\n\"group\": \"\",\n\"entries\": \"cache cachingProhibited alwaysCache\"},\n{\n\"group\": \"\",\n\"entries\": \"autoChunk autoChunkFileBytes\"},\n//...\n{\n\"group\": \"and-resource\",\n\"entries\": \"getFromFile autoChunked conditionalFor respondWithLastModifiedHeader getFromResource getFromDirectory listDirectoryContents getFromBrowseableDirectory getFromBrowseableDirectories getFromResourceDirectory\"},\n{\n\"group\": \"condition\",\n\"entries\": \"conditional\"},\n{\n\"group\": \"field\",\n\"entries\": \"filter formField formFields\"},\n{\n\"group\": \"param\",\n\"entries\": \"fdmaWrapper anyParam anyParams\"},\n{\n\"group\": \"with\",\n\"entries\": \"respondWithStatus respondWithHeader respondWithSingletonHeader respondWithHeaders respondWithSingletonHeaders respondWithMediaType\"},\n];\n```\n. It works for me when run locally, but somehow the file was broken on the uploaded site.\n. Moved to akka/akka#16813.\n. Not sure if we can improve the situation in spray alone. This is basically a Scala compiler issue. We could try to be smart and provide fake implicit values implemented by macros that try to emit warnings when something like that happens but it may be that this will create more issues than it solved.\n\nI'm closing this for now since there is currently nothing we can do about it in spray.\n. Yes, one problem with LiftJson / Json4s formats is that they are able to marshal anything and try to do so greedily. This means that you should never import the Json4sSupport implicits globally but instead very fine grained only inside the blocks where they are actually used.\n. Moved to #16812\n. I.e. akka/akka#16812.\n. Actually, passing the hostname isn't unnecessary but required to support the SSL server name indication extension (starting with Java 7). So, the fix should be to check if the inetaddress is an ip address or a host name. If it's an ip address, pass that to the SSLEngine. If it's a host name pass that one.\n. So, the fix would be to use `InetSocketAddress.getHostString` instead of `InetSocketAddress.getHostName`.\n. Ah, I missed that. I guess Java 7 is not an option right now so I guess we need a new suggestion.\n. LGTM\n. I'd say this is an expected race condition: spray-can has a pool of persistent connections on the client side. The first test creates a persistent connection that is idle after the first request completed. By shutting down the server, you forcibly shut down those idle connections which isn't happening immediately but needs to travel over the wire (i.e. the TCP FIN/RST packet). Before the connection closing is detected on the client side you run a new POST request onto this terminating connection which then fails. Because a POST request isn't idempotent there is no way to retry it automatically.\n\nIf it's just for the test, use `IO(Http) ! CloseAll` to shutdown all client connections before running another test (make sure you don't run tests in parallel in that case as it will close down connections globally for that ActorSystem).\n\nFor the real world case, this is still a common enough race condition (server closing the connection vs client sending a POST request). We could add a flag not to use a persistent connection for non-idempotent requests. This still wouldn't fix it but would maybe make it less likely.\n. @hmpatel Is the question if the connection idle timeout interferes with new or ongoing client connections? Or is it about the host connector idle timeout?\n\nHow exactly do you use spray?\n. AFAICS in http://tools.ietf.org/html/rfc3986#section-3.4 comma is not required to be encoded in URI queries (and btw also won't be encoded by browsers). If you have any other information please reopen.\n. Moved to akka/akka#16811\n. This is a duplicate of https://github.com/spray/spray/issues/947\n. We are using kind of a hack to allow proper flushing after each chunk of input data which isn't supported in Java 6 otherwise which involves reconfiguring the deflation level during compression.\n\nThe proper way would be using the [new overload of `deflate`](https://docs.oracle.com/javase/7/docs/api/java/util/zip/Deflater.html#deflate%28byte[],%20int,%20int,%20int%29) added in Java 7 and for Android starting with API level 19 (Android 4.4).\n\nI think we need some kind of configurable \"flushing strategy\" that would allow either to a) disable flushing (e.g. for Android pre API level 19), b) the current hack way for Java 6, or c) using the new deflate overload if available (if not otherwise possible per reflection).\n. Moved to akka/akka#16810.\n. @olger yes, we are now migrating this ticket to akka-http.\n\nIt is now superseded by akka/akka#16857\n. Could you please post a code example that illustrates the problem you need to be fixed?\n. It's hard to say if this would fix more issues than it would break currently.\n\nIn any case, I think the original ticket isn't valid, because the aforementioned line\n\n``` scala\nif (string.isEmpty) Empty else new Bytes(createByteStringUnsafe(string getBytes charset.nioCharset))\n```\n\nisn't related to the parser at all.\n\nSo, there should be always the workaround to change the content-type of incoming requests to include a charset if none was supplied.\n\nI'm a bit reluctant to change the overall behavior (at least as long as there's no simple way to configure it back to the old one).\n. `HttpEntity.asString` delegates to `data.asString` so this shouldn't be the problem. Maybe the problem is in your test? What's the result of `write`? If you have an implicit marshaller in scope, you don't need `write` but can just use `Put(\"/\", GoesIn(...))` directly.\n. That's not what I meant. The problem might be that `Put(path, HttpEntity(...))` is probably already using the Json4sSupport marshaller to marshal the entity as JSON data instead of marshalling the json data as an entity.\n. Btw. you don't need \n\n``` scala\nrespondWithMediaType(`application/json`)\n```\n\nthis will be automatically provided by the marshaller.\n\nTry putting `pathSingleSlash` instead of `path(\"\")` into the route.\n. Btw. we already track the problem that the json4s marshaller is overly greedy as #990 (it will only be fixed for akka-http).\n. Yep, this is currently a user-trap which we will make harder to fall into with #990.\n. Yep, LGTM. Thanks, @wjur.\n. @jroper I tried to port your code directly but that didn't work because xerces is only included under `com.sun.org.apache.xerces.internal` by default. It seems that in play you actually have an explicit dependency on xerces as introduced here: https://github.com/playframework/playframework/commit/dc94b943ee5dc1c95ec7338e52b2e1e4e80cd607\n. There are also\n- `ContentRange.Unsatisfiable`\n- `LanguageRanges.*`\n- `HttpEncodingRange.*`\n- `Uri.Empty`\n\nwhich would maybe also qualify as `case object`s.\n. Cool, LGTM.\n. :+1:\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. Slashes don't need to be encoded in the query part of a URI. See http://tools.ietf.org/html/rfc3986#section-3.4:\n\n> The characters slash (\"/\") and question mark (\"?\") may represent data\n>    within the query component.   Beware that some older, erroneous\n>    implementations may not handle such data correctly when it is used as\n>    the base URI for relative references (Section 5.1), apparently\n>    because they fail to distinguish query data from path data when\n>    looking for hierarchical separators.  However, as query components\n>    are often used to carry identifying information in the form of\n>    \"key=value\" pairs and one frequently used value is a reference to\n>    another URI, it is sometimes better for usability to avoid percent-\n>    encoding those characters.\n. @damienlevin it's in the logs:\n\n```\n[WARN] [04/10/2015 16:19:07.964] [test-akka.actor.default-dispatcher-2] [akka://test/user/IO-HTTP/group-0/4] Received illegal response: Response Content-Length 9313144 exceeds the configured limit of 8388608\n```\n\nYou need to change the limit in your application.conf. See the `spray.can.client.parsing.max-content-length` setting: https://github.com/spray/spray/blob/master/spray-can/src/main/resources/reference.conf#L327\n. No worries :)\n. Nice, thanks. Port to akka-http in https://github.com/akka/akka/pull/17230\n. What did you try so far and what was the result?\n. @pomadchin that seems like a somewhat custom setup. I hope you understand that we cannot debug any custom setups. If you want us to investigate please provide a self-contained project that demonstrates the issue and shows that this is a generic problem and not one of your setup. Feel free to reopen in this case.\n. Thanks for the report @christopheclc. I agree, we should catch errors here as well.\n. @betandr thanks for the contribution. I think adding support for parsing and rendering the directives from RFC 5861 would be a nice addition. There aren't any spray releases planned currently, though, as active development has shifted over to akka-http.\n\nA few things:\n- I think it would make sense to add support for `stale-if-error` as well to make parsing things from RFC 5861 complete\n- while you are at it maybe you want to add a little abstraction for \n\n``` scala\n   case class XXX(deltaSeconds: Long) extends ResponseDirective with ValueRenderable {\n    def render[R <: Rendering](r: R): r.type = r ~~ productPrefix ~~ '=' ~~ deltaSeconds\n  }\n```\n\nwhich is then common to several directives\n- add a test for the parser / renderer roundtrip at https://github.com/spray/spray/blob/master/spray-http/src/test/scala/spray/http/HttpHeaderSpec.scala#L157\n- you need to sign the Typesafe CLA at http://www.typesafe.com/contribute/cla\n- maybe add a ticket to akka/akka to port this over to akka-http so we don't forget about it\n\nThanks again!\n. Maybe also directly add a comment with the link to the RFC in the parser and/or at the place of the case class definitions.\n. Maybe just a simple superclass that already implements render so that you can use it like this:\n\n``` scala\ncase class `stale-if-error`(deltaSeconds: Long) extends ResponseDirectiveWithDeltaSeconds\n```\n. No worries about the commits. You can still squash everything in the end.\n\nDo the tests run through if you build the project locally?\n\nIt seems the tests fail because you removed `productPrefix` :) It's a method of `Product` which just returns the name of the `case class`. You removed it so it won't be rendered any more that's what the tests complain about.\n\nTo make it available you need to extend ResponseDirectiveWithDeltaSeconds also with Product. This should work:\n\n``` scala\ntrait ResponseDirectiveWithDeltaSeconds extends ResponseDirective with ValueRenderable with Product {\n...\n```\n. (And also add the `productPrefix` back to the `render` method in case that wasn't clear)\n. To answer where the implementation of `Product` comes from in the end: it's automatically implemented by case classes, i.e the compiler will generate its implementation.\n. Yay! LGTM.\n\nCould you finally [squash all your commits into one](http://stackoverflow.com/questions/5189560/squash-my-last-x-commits-together-using-git) and change the first commit message line slightly to start with \"+ http:\" as our policy is to prefix the line with the name of the module which changed? I think we are then ready to merge.\n. Btw. after squashing you can `git push -f` into this existing branch to have the PR updated (your old commits are lost in the process).\n. No problem, the `SimpleLruCacheSpec` one is a test that seems to fail non-deterministically, so it's not related. I restarted the failed one, so we will hopefully see the green light very soon...\n\nIn any case, thanks for keeping up.\n. Yep :)\n. I previously wondered whether I should suggest this bug fix but I didn't understand how it would apply here because it is only relevant to file downloads that are delivered via `WriteFile` and `HttpData.FileBytes` which your code doesn't use (and the log messages seem to agree).\n\nSo, I'm glad your code works now but I'm not so sure about the cause :)\n. @v-gerasimov no one is currently working on this. At this point the prerequisite before anyone would look further into this ticket would be a standalone, self-contained reproducer.\n. I know and I understand the implementation but it isn't really that useful if it leaves half of a segment from the middle of the original path in the unmatched path afterwards.\n. @kutchar it's not completely clear to me what you want to achieve. Is this about `multipart/formdata` uploads or just regular HTTP request entities (like posting `application/json` data)?\n\nI haven't looked in there for a while but I do not see a reason why putting file-based HttpData into entities would not work for requests as well (at least for regular entities, `multipart/formdata` may be harder).\n\nWhat did you try so far?\n\nIn any case, spray will probably not receive any new functionality any more, especially not wrt streaming uploads/downloads or `multipart/formdata` support. akka-http has first-class support for all of that so it should be the tool of choice if streaming data is a main application.\n. Yes, we don't have the capacity and expertise to maintain OSGI support. We will accept PRs to fix it as long as they don't break anything else (which unfortunately has happened in the past with OSGI support).\n\nThanks for providing the fixes. I'm leaving this open for now for reference.\n. For me it works exactly as specified in the docs you quote:\n- if the connection is closed during the request, i.e. before a response has been dispatched the client receives a notification\n- after the client has received the response (even if it has not finished sending the request) it does not receive any notification that the connection was closed afterwards\n\nThe implications of the second point may be somewhat unexpected, that the original creator of the connection (the actor that sent the `Connect` message) does not receive any notifications after the connection has been established. But as you say, one can work around this by watching the Http connection actor.\n\nHere's the version I tested:\n\nhttps://gist.github.com/jrudolph/ba6545a5d1b4ec7cac38\n. Yes, right, I forgot to explain that change. \n\nI put this alternative line in to illustrate the above bullet points. Depending on which line you choose you get one of the above bullet points. If you use `Connection: close` you get the second bullet point in which case everything is as you observed which may be unexpected but is working as specified.\n\nSo, if you don't control the server and its exact connection closure semantics, I would watch the connection actor and then look out both for `Terminated(connectionActor)` and `ConnectionClosed` messages.\n. @RichardBradley Yes, that's a good description of the situation.\n\nIIRC one reasoning not to send too many notifications is that it is likely that they will end up in DeadLetters if the \"owning\" actor changes over the life-time of the connection. This is all a bit fuzzy but as long as there are working solutions it is unlikely we will change the existing behavior in spray any more, especially for the more tricky situations of streaming / chunked requests which will have better solutions in akka-http.\n. @ppiotrow yes, exactly. I'm closing this. If this is still an issue, please reopen.\n. @OElesin, this doesn't seem to be a bug, so I'm closing this for now. If this is still an issue, please continue the discussion on the mailing list: https://groups.google.com/group/spray-user\n\nAnd, thanks @matsluni for the help.\n. Thanks, @RichardBradley for adding the comment. There's some description of the process of building the documentation at http://spray.io/project-info/contributing/#contributing-documentation (though not for windows).\n. This is likely to be a configuration issue where you run with an older Akka version than you are thinking you are. Try running `show externalDependencyClasspath` in sbt to find out which libraries are on your classpath.\n\nThe change that probably broke the compatibility in your case was [this](https://github.com/akka/akka/commit/33ebf24c0fec622869a5306312c3bac3ac889f68) and it predates Akka 2.3.0. \n\nIn any case, this is not a spray bug, so it's likely to be akka or spark specific. If you cannot find out I suggest asking on the akka or spark mailing lists.\n\nClosing for now...\n. Thanks for the report @discordianfish. Could you post a small reproduction of the issue or at least a tcpdump showing how this happens? That would help a lot.\n. `SprayJsonSupport` only allows marshalling from and to HttpEntity. Putting JSON into parameters is somewhat unusual (but probably possible). \n\nI'm closing this here, as this is not a bug. If you still need help, please discuss it at the mailing list at http://groups.google.com/group/spray-user. Thanks!\n. @wjur is correct about how it works, but it should still be possible to achieve what you want.\n\nWhat spray does is \"sealing\" the route passed to `runRoute`. That means it applies a default RejectionHandler (and a default ExceptionHandler), so that rejections and exceptions never escape the actor. This default RejectionHandler will handle all rejections that you haven't handled so far and, thus, convert them to status codes.\n\nIf you want to see the status codes you must make sure that you seal the route already _inside_ your logging directive.\n\nBasically you need to replicate what you see inside here:\n\nhttps://github.com/spray/spray/blob/master/spray-routing/src/main/scala/spray/routing/HttpService.scala#L34\n\nI haven't tried but it could be enough to wrap your route with sealRoute\n\n``` scala\nlogRoute {\n  sealRoute {\n    yourMainRoute\n  }\n}\n```\n. Or, you can make `sealRoute` a directive like this:\n\n``` scala\ndef seal: Directive0 = new Directive0 {\n  def happly(f: (HNil) \u21d2 Route): Route = sealRoute(f(HNil))\n}\n```\n\nand then include its functionality directly into your directive with\n\n``` scala\ndef logRoute: Directive0 = mapRequestContext { ctx => \n //...\n} & seal\n```\n. Thanks, @wjur. Exactly, it's an artifact of how the website is deployed. For historical reasons the main branch is still the 1.1 version one which has version 1.1-SNAPSHOT. When the website is built, then from this branch.\n. Thanks, @rgaskill for the report. Several bugs have been found in spray's Gzip implementation (e.g. also #937) that have been fixed in the successor of spray, akka-http. This one is one of them (see https://github.com/akka/akka/blob/master/akka-http/src/main/scala/akka/http/scaladsl/coding/Gzip.scala#L101).\n\nOnly the most important bug fixes would qualify for being fixed in an eventual fix release of spray (though none is planned, currently).\n\nLet's keep this open for reference and for people to vote on.\n. Great you found a solution for now :)\n. Also pushed branches https://github.com/spray/spray/tree/w/fix-dir-traversal-on-windows-1.2 and https://github.com/spray/spray/tree/w/fix-dir-traversal-on-windows-1.1 in case we want to issue fix releases for those as well.\n. See https://github.com/akka/akka-http/pull/695. So, the naming convention is this:\n\nspray-routing -> use with shapeless 1.x\nspray-routing-shapeless2 -> use with shapless 2.x\nspray-routing-shapeless23 -> use with shapless >= 2.3.x\n\nUnfortunately, shapeless versions 2.x and 2.3.x don't seem to be binary compatible so, with upgrading to spray 1.3.4 you also need to update to a more recent shapeless version. Sorry for not mentioning this in the release notes.\n. Superseded by akka/akka#16858\n. Superseded by akka/akka#16847\n. Won't be fixed in spray. Superseded by akka/akka#16846\n. Thanks @2beaucoup. Implemented by #797.\n. This is currently out of scope for spray and akka-http. If this is needed it will need to be provided on a higher-level.\n. The most basic way of making use of an HTTP proxy would be:\n- don't send the request over a connection to the original server but send it on a connection to the proxy\n- rewrite the request URL to be absolute\n\nI think this is already enough for some scenarios the question is more where to put that logic exactly in the existing infrastructure.\n. The main architectural challenge I see is that we will have different kind of \"transports\" in the future:\n- what is now the HostConnector which is now a connection pool of persistent HTTP connections to one (DNS) host, where each connection can handle just one request concurrently\n- a proxy which usually lies on a level above the HostConnector because in theory you need only one connection pool to the proxy to connect to a series of host\n- pipelined HTTP or SPDY which allow to have just one connection per DNS host but multiple requests in parallel\n\nThat said, if you find a non-intrusive way to make some proxy support happen now (e.g. just by rewriting the host to connect to and the request URL) without the need for any big architectural changes, I think we would include it now even if it is not as efficient as it could be with a more advanced future design.\n. And btw. thanks for your offers for help, @agemooij and @2beaucoup, this is much appreciated!\n. > One more question: Why is HttpManager making all URIs relative (code)?\n\nI think it's because this information is not needed any more once you have a HostConnector who already knows the target host/port.\n\nRegarding your approach: I think the least intrusive way for getting proxy support now is not fussing with the HostConnectorSetup at all but just patching the `HttpHostConnectionSlot` to \n1. rewrite the connection target to the proxy server when doing a `Connect`\n2. rewrite the request URI directly before sending a request to be absolute by using the configured host/port of the slot\n\nAs said before, this is not efficient because proxy connections are not managed globally but a better design will be much more intrusive.\n\nAlso, this won't work for SSL connections which need a different pipeline underneath which would have to be implemented first.\n. @2beaucoup, you are not forgotten! We are slowly getting up to speed from vacations but we hope to get to wading through the stack of PRs starting from next week.\n. I discussed this with Mathias and we came to the conclusion that we should be able to support proxies even more completely than sketched before with relatively little changes.\n\nThe idea is this:\n- the lowest level connection-level API does not handle automatic proxy support, but can be used to connect to a proxy and communicate with it\n- same for the `HttpHostConnector` implementation which should make no assumptions about whether the target is a proxy or not\n- the question is then how to obtain a HttpHostConnector based on the proxy configuration\n  - if proxy **is not** configured, a HostConnectorSetup is answered as currently; by pointing the setup\u2019s host and port to a proxy, you get a HttpHostConnector which sends requests to the proxy\n  - if a proxy **is** configured, the HttpManager will internally create a HttpHostConnector for the proxy but return a ProxiedHostConnector whose sole purpose is to rewrite requests accordingly (relative to absolute URI) and relay them to the internal HttpHostConnector for the proxy\n- we need an additional setting in HostConnectorSetup that specifies that you want to explicitly ignore proxy settings\n- proxy settings can be given in application.conf and default to `-Dhttp.proxyHost/Port` etc\n- to support headers like proxy-authentication automatically it would be useful if HostConnectorSetup would support a field to add custom headers to each request.\n\nExample how to obtain a host connector based on your proxy preference:\n- use global setting\n  \n  ``` scala\n  IO(Http) ? HostConnectorSetup(\"example.org\", 8000)\n  ```\n- don't use proxy\n  \n  ``` scala\n  IO(Http) ? HostConnectorSetup(\"example.org\", 8000, connection = Direct)\n  ```\n- override proxy\n  \n  ``` scala\n  IO(Http) ? HostConnectorSetup(\"proxy.example.org\", 8000, connection = Direct)\n  ```\n\nThe default value for `connection` would be `AutoProxied` which would enable the behavior as sketched above. WDYT?\n. > @jrudolph: Could you re-check your last two examples? They seem identical.\n\nYes, they are by purpose. With `connection = Direct` you always connect directly to the given endpoint, ignoring any proxy settings. This way you can use it both to create a host connector for a direct connection to the target host as well as to create a host connector to connect to a proxy (which you can then send requests to other targets).\n. > What would happen if I send a relative requests without a Hostheader to this connector? One advantage of having proxy info in the HttpHostConnector would be the ability to fail early in this case.\n\nI agree, a HttpHostConnector should maybe know if it can handle relative urls or not to fail fast. And related, we should specify which component is responsible for setting Host-headers / relativize URLs.\n. Kevin, was this pull request intended? What is it about?\n. Ah, of course. I was mislead because you (or github) set spray:master as the target branch for the pull request.\n. I think the first point of compiling without sbt is a separate concern. (spray is a normal maven dependency so you should be able to use any java build tool and reference spray and akka, we don't support this officially but in cases of questions like this try asking on the mailing list.)\n\nThe upcoming (and IIRC the current) version of spray should be able to support Servlet 3.0 compliant servlet containers in general. We aim to provide support for those containers by adhering to the standards and using container features to run a spray application on top of this. It seems that Jboss 6 supports servlet 3.0, so it already might be possible to use spray with JBoss 6. We don't use JBoss so if you need more information how to set this up exactly your best bet is to ask for this on the mailing list where someone may have tried a similar setup.\n. This is currently out of scope for spray or akka-http but may be easily provided as a contrib module.\n. Thanks for the analysis, @nartamonov. I agree with you and spray websocket support will enable other higher-level solutions like the ones you are describing. We will first focus on websocket support itself because that's something that has to live in spray(-can) itself. More higher-level abstractions on top may then be provided by spray itself or can also be built by third-party extensions.\n\nWould you create another issue for the high-level solution?\n. We've considered this request and found that we need more evidence that anyone would find this useful.\n\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n\nJust joking... :D The time may have finally come that we are actually going to implement this as part of akka-http. Here's the new ticket: akka/akka#16848 :fireworks:\n. @matanster, yes, I was trolling. I hope you didn't take offence.\n. Closing for spray. See the  akka/akka#16848 for the most recent developments.\n. Closing this for now as we won't implement this for the spray site and for akka-http it is a concern of Akka's documentation infrastructure.\n. Superseded by akka/akka#16845\n. I guess it's about transparently relaying a request to another HTTP server which will then answer it.\n. Superseded by akka/akka#16844.\n. This hasn't been requested for a long time and is also easy to provide in a custom fashion both for spray and akka-http so it won't be fixed.\n. We are considering this. However, it's not just migrating to \"use json4s\" because json4s is already much more than just an AST (which we would use).\n. Closing. spray-json issues will be discussed in the spray-json repo.\n. spray won't have it's own IO layer in the future and instead use the new akka IO which already has support for UDP. See http://doc.akka.io/docs/akka/2.2-M1/scala/io.html\n. Closing... while this would have been very useful it probably won't happen for spray any more. Things like this need to be considered as part of a general documentation effort for akka-http.\n. This would be a neat feature but would play a bit against the general view of spray to represent HTTP things explicitly in the routing definition. This also hasn't been requested for a long time, so closing for now...\n. Currently not planned.\n. Workaround for the example:\n\n``` scala\nobject Main extends App with SimpleRoutingApp {\n  def route =\n    get {\n      path(\"\") {\n        redirect(\"/hello\")\n      } ~\n        path(\"hello\") {\n          respondWithMediaType(`text/html`) { // XML is marshalled to `text/xml` by default, so we simply override here\n            complete {\n              <html>\n                <h1>Say hello to <em>spray</em> on <em>spray-can</em>!</h1>\n                <p>(<a href=\"/stop?method=post\">stop server</a>)</p>\n              </html>\n            }\n          }\n        }\n    } ~\n      (post | parameter('method ! \"post\")) {\n        path(\"stop\") {\n          complete {\n            system.scheduler.scheduleOnce(Duration(1, \"sec\")) {\n              system.shutdown()\n            }\n            \"Shutting down in 1 second...\"\n          }\n        }\n      }\n\n  startServer(interface = \"localhost\", port = 8080)(route)\n}\n```\n. For the record the error in question is\n\n```\n [error] (simple-routing-app/compile:compile) java.lang.Error: symbol value $buf$1 does not exist in spray.examples.Main$delayedInit$body.apply\n```\n. Not reproducible any more. Also, since the documentation examples are now all compiled for all of 1.0/1.1/1.2 we should notice in future.\n. You have to add \n\n```\nscalacOptions += \"-Ydependent-method-types\"\n```\n\nto your build definition when using Scala 2.9. See http://spray.io/documentation/spray-routing/installation/\n\nI've repurposed the ticket to remind us of improving the documentation on the example page.\n. This has happened for akka-http and is currently not planned for a maintenance release of spray. Closing for now.\n. FYI, I think #281 is the meta-issue for this one. And no, unfortunately no news yet.\n. Here's an additional idea to take into account by @fedesilva:\n\n> trigger request streaming for any request that has multipart form data, not only for size thresholds.\n. Thinking more about how this would be implemented, I think there's still a conceptual piece missing:\n\nCurrently, regardless of whether chunking is used on the connection or not, spray-can won't sent message chunks to the user-level handler but instead will automatically aggregate chunks into complete requests, i.e. exactly the opposite functionality we are trying to build here. You can switch it off (`spray.can.server.request-chunk-aggregation-limit = 0`) but this is not what you would usually would want to do for your complete server.\n\nOne possible way could be to switch off request-chunk-aggregation per default in spray-can and instead improve spray-routing to support chunking. A related question is how you would distinguish multiple concurrently incoming request chunks in a HttpService when one request spans over multiple messages from spray-can.\n\nDoes anyone have pointers about information in which way chunking / streaming is supported in other webservers?\n. [netty's handling](https://github.com/netty/netty/blob/master/codec-http/src/main/java/io/netty/handler/codec/http/HttpObjectDecoder.java#L58) seems to be similar to the one implemented for spray-can in the above commit.\n. Maybe it's something similar to what triggered the change here:\n\nhttps://github.com/spray/spray/commit/ee33b340e6850e38de371a046a1ea908ab1fe7cd#L1L440\n\nUnfortunately, I can't remember how we triggered the condition back then. It seems it should only happen for unconnected connections but I can't see how the connection could be unconnected at the place in question here.\n. Superseded by akka/akka#16855.\n. Have you solved the problem? We were seeing the same effect in the spray tests as well and I, incidentally, tried to find the problem just this morning. I'm currently suspecting that there's a race condition in Akka or the ForkJoinPool where you can get some ForkJoinPool threads not to close properly if a particular kind of work is scheduled after `system.shutdown()` was called.\n. Is this really related to jsonp? As I understand it, the problem is that if a malicious site can access a json resource with a script tag as if it was javascript code (you wouldn't do that normally) and then redefines the Array constructor, it could capture all of the json data.\n\nJsonp on the other hand _uses_ a script tag to load json content, adding `while(1)` automatically would break it.\n\nAm I missing something?\n\nAlso I'm not so sure how severe the problem still is. It seems the easiest mitigation is to always return private json data only over SSL.\n\nHere are some links but it's not totally coherent:\n\nhttp://www.sitepen.com/blog/2008/09/25/security-in-ajax/\nhttp://haacked.com/archive/2009/06/25/json-hijacking.aspx\nhttp://security.stackexchange.com/questions/7001/how-should-web-app-developers-defend-against-json-hijacking\nhttp://www.sitepen.com/blog/2008/09/25/security-in-ajax/\n. Also:\n\nhttp://stackoverflow.com/questions/16289894/is-json-hijacking-still-an-issue-in-modern-browsers\n. Superseded by akka/akka#16843\n. I think the solution in this case is just to extend from `akka.testkit.TestKitBase` instead of `akka.testkit.TestKit`. Can you try that @rbraley?\n. I'm closing this for now. If the problem persist even when using `TestKitBase` please reopen.\n. See https://groups.google.com/d/topic/scala-language/7uu0ETacJQs/discussion\n. Yes, thanks @2beaucoup. Implemented by #797.\n. Duplicate of spray/spray-json#48\n. If you set `spray.can.server.remote-address-header = on` in the configuration, spray-can will set the `Remote-Address` header from the real endpoint address. Would that be enough?\n\nSee http://spray.io/documentation/spray-can/configuration/\n\nI created #230 to track the `X-Forwarded-For` problem separately, thanks for reporting.\n. For example, googling for [`spray example`](https://www.google.com/search?q=spray+example) still points to the wiki.\n\nMaybe we could just switch off the wiki on the site but push the wiki repository somewhere so that the documentation for 0.9 isn't lost but won't be shown by google as prominently as it is now.\n. As a first step I added the deprecation note to every single page of the old wiki.\n. Gilles, I just fix one issue where the tests hang for me as well in the SslTlsSupportSpec which seemed to be one issue of the ones you were seeing.\n. The other failing tests could be due to simple timing problems when the build machine is slower than the one we are usually using.\n\nakka testkit has a [mechanism to adjust for slow testing systems](http://doc.akka.io/docs/akka/snapshot/scala/testing.html#Accounting_for_Slow_Test_Systems). Can you try using `-Dakka.test.timefactor=2` or an even higher factor when running sbt?\n. Btw, generally hanging tests and timing issue are frequent issues here. The problem is that spray/akka system usually inhibit quite some true non-determinism and it is hard to predict all the things that can go wrong on other machines. Actually, this is another reason we are glad about more third-party testing to catch more of the test (and sometimes even production) issues of this kind.\n. I just tested the timefactor locally and can reproduce the issue in HttpServiceSpec by _decreasing_ the factor to `0.1`. Consequently, by increasing it you should be able to get rid of the issues.\n. We already suspected that higher factors could have an impact as well, as there are timing dependencies which are not regulated through the `timefactor`. The more of them we discover the better.\n. I don't think hanging issues really generalize to reason about them in a broader way. The fix in b5427bc is about not to rely on any way the OS network stack might split up network data into incoming data chunks. In the failing test a 6 byte data message was delivered in 2 chunks of size 5 + 1 bytes which wasn't expected and so the test server actor crashed. The client in the test was written in blocking style and then hung because the server never answered any more.\n\nThe generalization to this problem could be \"never use blocking calls without a timeout\".\n. This is the command-line our internal jenkins uses for the nightly build:\n\n```\njava -Xmx2048m -XX:MaxPermSize=512m -Xss2m -Dnightly.build=yes -Dsbt.log.noformat=true -jar /opt/jenkins/sbt-launch-0.12.2.jar clean test:compile test publish\n```\n\nI haven't heard that this failed with memory problems so I guess there has to be another aspect to this.\n. And you are right about d658a60, we added this change considering the timing problems you've seen in travis runs.\n\nAnother thing to consider would be to limit parallelism when running tests e.g. by adding this to the sbt command line\n\n```\nsbt \"set concurrentRestrictions in Global += Tags.limit(Tags.Test, 1)\" test\n```\n\nThis should definitely remove one source of non-determinism and CPU pressure.\n. First build just finished successfully, :+1: . Thanks, Gilles.\n. This is a very low-level feature that won't be fixed in spray any more and is fixed with akka-streams in a different way. Closing...\n. We won't do this for the spray documentation any more. \n\nIn akka-http, routing isn't bound to an actor any more and therefore not a potential bottleneck across connections. So, even if the actual processing is executed in the routing thread it will only block pipelined requests on the same connection. As this is not a common use case, we may not need extra documentation for that. _If_ someone enables pipelining it should be clear that off-loading e.g. in the form of Futures is required to make use of parallelism.\n\nFor now closing as \"Won't Fix\".\n. This doesn't happen anymore (since illegal headers are now still provided via `RawHeader`). The error message is then `The resource requires authentication, which was not supplied with the request` with status code 401. Is this good enough?\n. This will get fixed as part of #486 \n. Merged as part of #536 \n. Superseded by akka/akka#16842\n. Will probably not happen for spray anymore. akka-http already now has streaming multipart support, improving support through directives may be useful.\n. Superseded by akka/akka#16841\n. Superseded by akka/akka#16840\n. The problem is that not even every OS has proper async file support. E.g. in Linux the only support for asynchronous file operations is to open files in DIRECT mode, i.e. completely circumventing the page cache which in many cases isn't what you want to do. \n\nI have come to see blocking file IO as an only small nuisance. It just doesn't make so much of a difference if you schedule 100 or 1000 IO operations at the same time because on usual hardware disk IO is so slow anyways. So, probably it suffices to create thread-pools of about a single digit number per harddisk and then use some manual queuing of disk IO jobs to get most work done efficiently.\n\nThat said: in spray we already use the zero-copy file transfer support from akka.io (using `WriteFile` which internally uses `FileChannel.transferTo`) to deliver files efficiently to the network. There's a config setting which allows setting a custom dispatcher for these copying tasks, so wrt to spray this should already handle most cases quite well.\n. If this can be supported somehow, it will come naturally by migrating akka-http's FileAndResourceDirectives to an asynchronous streaming variant. Even now, through the use of akka-streams file-access is potentially more asynchronous than in spray. Closing...\n. This would be very useful but I think it is likely it will be implemented in the scope of a more general Akka Logging/Tracing facility as e.g. (not yet) discussed in akka/akka#13817. Closing here...\n. We won't fix this in spray. In akka-http it will be naturally supported through the new streaming HttpEntity model. Closing...\n. I think this is already implemented on the one hand and on the other hand already contradicted by #401.\n. Not an issue any more: spray lives on spray pipelining, akka-http lives on akka-streams.\n. There are currently 16 occurences of `currentTimeMillis`:\n\n```\nspray-caching/src/main/scala/spray/caching/LruCache.scala:    val now = System.currentTimeMillis\nspray-caching/src/main/scala/spray/caching/LruCache.scala:  @volatile var created = System.currentTimeMillis\nspray-caching/src/main/scala/spray/caching/LruCache.scala:  @volatile var lastAccessed = System.currentTimeMillis\nspray-caching/src/main/scala/spray/caching/LruCache.scala:    lastAccessed = System.currentTimeMillis\nspray-can/src/main/scala/spray/can/client/ClientFrontend.scala:import System.{ currentTimeMillis \u21d2 now }\nspray-can/src/main/scala/spray/can/rendering/ResponseRenderingComponent.scala:    val now = System.currentTimeMillis\nspray-can/src/main/scala/spray/can/server/OpenRequest.scala:        timestamp = System.currentTimeMillis\nspray-can/src/main/scala/spray/can/server/ServerFrontend.scala:              } else openNewRequest(request, closeAfterResponseCompletion, System.currentTimeMillis)\nspray-can/src/main/scala/spray/can/server/ServerFrontend.scala:                firstOpenRequest checkForTimeout System.currentTimeMillis\nspray-can/src/main/scala/spray/can/server/StatsSupport.scala:    val startTimestamp = System.currentTimeMillis\nspray-can/src/main/scala/spray/can/server/StatsSupport.scala:      uptime = (System.currentTimeMillis - startTimestamp) millis span,\nspray-io/src/main/scala/spray/io/ConnectionTimeouts.scala:import System.{ currentTimeMillis \u21d2 now }\nspray-io/src/main/scala/spray/io/ConnectionTimeouts.scala:            if (timeout.isFinite && (lastActivity + timeout.toMillis < System.currentTimeMillis)) {\n```\n\nFrom those I think these are valid uses of `currentTimeMillis`:\n\n```\nspray-http/src/main/scala/spray/http/DateTime.scala:  def now: DateTime = apply(System.currentTimeMillis)\nspray-routing-tests/src/test/scala/spray/routing/FileAndResourceDirectivesSpec.scala:            case `Last-Modified`(dt) \u21d2 DateTime(2011, 7, 1) < dt && dt.clicks < Sys\nspray-routing/src/main/scala/spray/routing/directives/FileAndResourceDirectives.scala:    respondWithHeader(`Last-Modified`(DateTime(math.min(timestamp, System.currentT\n```\n\nThe other ones could be replaced by `nanoTime` or `scala.concurrent.duration.Deadline`.\n. There are several possible solution wrt forgetting `~`. The simplest one is: use parentheses instead of braces for subroutes. Several users have pointed out that they adopted a style for their routes where braces are only used if not otherwise possible. We'll probably adopt this style as well as it will catch those `~` composition errors in most cases.\n\nAnother possibility would be to use macros to detect situations when values of type `Route` aren't used. But this is definitively advanced territory...\n. How would you do that? There is no way to rule this out in the type-system as long as there is no effect-system. So, just using parens is probably the way to go.\n. As @sirthias said: this is fixed in akka-http. We currently won't backport these breaking changes to the spray routing DSL.\n. We should check this in akka-http but don't need to fix it in spray any more.\n. Superseded by akka/akka#16839\n. Going forward, I'd say `outgoing-auto-chunking-threshold-size` is the least important setting because this is functionality that is nice-to-have but can be implemented on the higher levels as well. \n\n`chunkless-streaming` on the client-side wouldn't be necessary for connecting against HTTP/1.1 servers as those servers are actually required to understand chunked requests, however, for compatibility reasons it would maybe still make sense to provide it as a fallback.\n\nSo in essence, I'd consider client-side `chunkless-streaming` for 1.0/1.1/1.2 and maybe postpone `outgoing-auto-chunking-threshold-size` to the next version.\n\nWDYT?\n. When #592 is merged, the only thing missing is `outgoing-auto-chunking-threshold-size` which we deemed as less important. So, I'm moving the remainder of the ticket to the next milestone.\n. @RichardBradley this is a question that came up before. You are quite right with your observations. See this [ML message](https://groups.google.com/d/msg/spray-user/YN2ocRzwhY0/0mJxOaYchi8J) and https://github.com/jrudolph/spray/commit/e707a2d7b99cd2355a76d6c2a80de040a59a59c3 for a hint at a solution with the current version of spray.\n\ntl;dr: You can use SuspendReading/ResumeReading to stop the inflow of data and it somewhat works but isn't optimal. Upcoming akka-http is going to have a much better solution (but won't be immediately available).\n. We won't implement the outgoing autochunking in spray any more. In akka-http it is naturally supported through the new streaming HttpEntity model.\n. In general, some server could rely on the URL parameters being in some order which could be destroyed by the format you are advocating.\n\nOur thinking here is: there is no real standard how URI parameters should be interpreted by a server. There are certain standards like what php is doing with the parameters etc. However, the only thing that counts is that HTML forms generate URL parameters of a certain shape. We think the most generic form which conforms to this shape (and which still leaves room for assigning a custom semantic to it) is the one suggested. \n. In which way is this related to servlets? \n\nIn the end we can always provide sensible additional options for common use cases. The question is how to model the parameters on the lowest level without losing information some possible interpretations may rely on.\n. The section from the spec above:\n\n>   Idempotent methods (Section 4.2.2 of [Part2]) are significant to\n>    pipelining because they can be automatically retried after a\n>    connection failure.  A user agent SHOULD NOT pipeline requests after\n>    a non-idempotent method until the final response status code for that\n>    method has been received, unless the user agent has a means to detect\n>    and recover from partial failure conditions involving the pipelined\n>    sequence.\n\nFrom the section about idempotent methods (http://tools.ietf.org/html/draft-ietf-httpbis-p2-semantics-22#section-4.2.2):\n\n> Request methods are considered \"idempotent\" if the intended effect of\n>     multiple identical requests is the same as for a single request.  Of\n>     the request methods defined by this specification, the PUT, DELETE,\n>     and safe request methods are idempotent.\n\nSafe methods are (http://tools.ietf.org/html/draft-ietf-httpbis-p2-semantics-22#section-4.2.1):\n\n>    Of the request methods defined by this specification, the GET, HEAD,\n>    OPTIONS, and TRACE methods are defined to be safe.\n\nSo from the common methods the complete list of idempotent methods is this:\n- `GET`\n- `HEAD`\n- `OPTIONS`\n- `TRACE`\n- `PUT`\n- `DELETE`\n. Easy to fix once we added the directives in #149\n. Cool, thanks.\n. Superseded by akka/akka#16838\n. Thanks for reporting, Gary. Do you think it is related to #298 ?\n. This won't change in spray anymore and is solved differently in akka-http(-core).\n. See also https://groups.google.com/d/msg/spray-user/8X8xBO79GDU/XtHZaxr2m8AJ\n. Here's the [section from the spec](http://tools.ietf.org/html/draft-ietf-httpbis-p2-semantics-22#section-3.1.1.1).\n. I think there are various issues around cookies that should be taken into consideration before releasing any client-side implementation, not least because of the possible security issues arising from a naive implementation.\n\nAt least these things should be taken into account:\n- paths\n- RFC 6265 has a [section](http://tools.ietf.org/html/rfc6265#section-5) about user agent requirements and how to match cookies that we should follow\n- cookie expiration \n- secure cookies must only be sent over https connections\n- cookies are logically not bound to a connection, so IMO the implementation should be at a higher level, i.e. not implemented as a pipeline stage but probably on the HttpClientSettingsGroup level. Even for a first cut the implementation on one local connection would bring much confusion because the higher levels may redistribute subsequent requests to different connections.\n. Ah, this is important information and it seems like this is indeed a bug in the rendering of the header.\n. Your stack size is likely too low. What's the `-Xss` setting you run sbt with? I'm using `-Xss2M`.\n\nThe [travis builds are green](https://travis-ci.org/spray/spray/builds/7872349).\n. Superseded by akka/akka#16837\n. Oh yes, I forgot about the tests. I started them, suspended my computer, and went home :) I think the failures are just observations of the new behavior.\n. lgtm\n. To add a bit more information to this ticket, the problem is that MultipartMarshallers automatically add a `Content-Disposition` header to each part. This is done even if a user-defined `Content-Disposition` header exists with other values.\n\nThe solution would be to look for a user-defined `Content-Disposition` header and use that one or if a user-defined one is missing use the current logic of generating the header.\n\nWDYT?\n. > Even though this is not allowed under the RFC a lot of API's count on this behavior building an \"array\" including the one I'm coding for now.\n\nAre there any publicly available API descriptions for this behavior? Any frameworks that support that style? I think it would be good to have some supporting evidence why we'd want to deviate from the RFC.\n. > That would require us start using a Seq[(String, BodyPart)]\n> instead of a Map in order to allow duplicates that would be renamed.\n\nProbably, that would be the consequence.\n\n> After that we would group the ones with the same name and rename them.\n\nWhat do you mean by \"rename them\"? When unmarshalling? How would the renaming work? Actually, since the behavior isn't standardized we should try to support in the model what's there in the wild but don't make any assumptions about how you would use it.\n. Two things:\n- The `akka.io` package is only present in spray 1.0/1.1 (where akka doesn't have that package). In spray 1.2, it lives in akka and spray doesn't have this package. Currently you seem to export and import the `akka.io` package for the spray-io module, right? Doesn't that make any problems? I think we should either \n  - add a switch into the build to use the right set of imports/exports depending on the target version (1.0/1.1/1.2)\n  - or have just the version working for spray 1.0/1.1 in this PR and then fix it when merging this to the release/1.2 branch\n- How can we test that this is working?\n. Thanks for the clarifications. It's worth a lot having these instructions here for later reference if we have to change module structure later on.\n\nManual testing is ok, I just tried it. How did you decide which packages to mark as optional in `spray-httpx`? Shouldn't spray-json be optional there as well? (That's where `pax-run` failed because I didn't yet tried the spray-json OSGI PR).\n. And, yes, maybe wait for @topping to clarify before building anything else if he has some kind of tests to check if OSGI support is working.\n. > If there are any errors due to misconfigured OSGi settings, they are printed to the console.\n\nDoes that check that classes in the bundles only reference packages as declared by their metadata? Or in other words, is there a way of checking that the declared dependency structure won't fail with runtime linking issues? (Maybe these are stupid questions but I never really got into OSGI, so please bare with me).\n. > In an OSGi environment, getClass.getClassLoader is returning the classloader of the spray-can JAR, which is distinct from the classloader of the spray-util JAR.\n\nI think this code is wrong in any case. Usually, we should use the ClassLoader from the ActorSystem for doing that. Can you try replacing the `getClass.getClassLoader` call by `actorSystem(system).dynamicAccess.classLoader` (you have to move the `withFallback` clause to the other apply method).\n. > One other question - would you prefer additional commits at this point and then a single rebase after all comments are addressed?\n\nPut them in new commits and when we are finished we can still decide how we want the final history to look like.\n. > I think you first solution alternative is indeed the best. spray doesn't really work outside of an ActorSystem anyway, so requiring one to be present is not really a problem IMHO.\n\nI agree.\n. > One option is to implement an extender bundle that checks for the existence of reference.conf files in bundles as they are resolved. Upon finding a bundle with a reference.conf, the extender bundle could load the config and store it in memory. Then, the extender could provide a merged Config as an OSGi service. This pattern doesn't play nice if you have an OSGi app that has multiple versions of the same JARs installed though. The apps I work on don't have this requirement but some apps do.\n\nCouldn't you do this so that you trigger this resolution from the top-level application (which instantiates the ActorSystem) and then go just through those jars that are in the transitive dependencies of the top-level module? This should automatically exclude multiple versions of the same module. Would that be possible?\n.  :+1: Excellent, thanks @mpilquist and @topping for driving this further.\n\nThe code seems to be a good start and I agree moving to typesafe config is the way to go\n. :+1: \n. lgtm\n. Derek, I created two branches to assist you: \n\nhttps://github.com/jrudolph/spray/tree/wip/merged-new-and-old contains all your changes (and just yours) as one squashed commit on top of a temporary branch which simplified the merge.\n\nhttps://github.com/jrudolph/spray/tree/wip/pull/336-repair: I then took the current master and cherry picked this one commit containing all your changes. If you want you can take this commit as a base for your new pull request / your update of this pull request.\n. No worries, your problems are actually our fault because we violated the eternal \"don't rebase public branches\" rule (knowing it could break stuff).\n\nIt could work like this:\n\n```\n# allows you to access my branches\ngit remote add jrudolph https://github.com/jrudolph/spray.git\ngit fetch jrudolph\n\n# create a new local branch off mine\ngit branch add-metrics jrudolph/wip/pull/336-repair\n\n# go to the new branch\ngit checkout add-metrics\n\n# remove my commit but keep the changes in your working copy\ngit reset HEAD~1\n```\n\nWhat you now got is a branch that is at the current spray master but with just your changes in the working copy. So, it will look like you've started on the master and just made your changes. Now, just go ahead and create your commits with those changes (see the commit message policy link, Mathias gave above, for more info about what we expect from PRs). Then push the branch to your repository and create the PR.\n. Right, that's something I ran into as well when trying to run spray on the Raspberry PI. Thanks for reporting.\n. Could you post the rest of the stack trace as well? It seems the interesting parts seem to be in the `Caused by` section?\n. Scrap that, it's easy enough to reproduce.\n. > How I hotfixed it: danielwegener@c495623\n\nYes, this will then later fail when a entity is tried to be encoded or decoded. I attempted another fix which won't fail at initialization time and which will make spray respond with proper error messages when a charset is missing on the platform.\n. Sorry, stupid me. This time I tried to run all the tests.\n. I rebased this PR with following changes\n- Fixed the commit messages\n- Extract common superclass for directive specs\n- Import cleanups\n. Superseded by akka/akka#16836\n. Interesting fix. I also checked that the right least upper bound type is inferred for other examples as well. These can get pretty wild depending on which types are involved, e.g. for `Segment | JavaUUID` the inferred type is \n\n```\n Comparable[_ >: String with java.util.UUID <: Comparable[_ >: String with java.util.UUID <: java.io.Serializable] with java.io.Serializable] with java.io.Serializable\n```\n\nBut I guess, that's something which 1) usually is no problem 2) we can't do anything about.\n\nSo, :+1: from me.\n. No, no idea how you could do that otherwise. Btw. the ticket number you referenced is the wrong one. It should be #344.\n. Thanks, done.\n. Jeff, thanks for the contribution. We've not yet had time to properly review it but we will do asap.\n\nOne things I noticed: the travis build fails with test errors from the spec you added. Can you look into that?\n\nPlease also see the [notes about contributing](http://spray.io/project-info/contributing/).\n. @jeffywu: I squashed your contributions and fixed some smallish stuff and created a new PR, #519. If you don't object we would close this one and go on with the new one. In any case, thank you very much for this contribution.\n. Here's a stack dump  when the problem occurs:\n\n```\nest-akka.actor.default-dispatcher-9@2133, prio=5, in group 'main', status: 'RUNNING'\n      at spray.can.parsing.HttpResponsePartParser.apply(HttpResponsePartParser.scala:40)\n      at spray.can.parsing.HttpResponsePartParser.apply(HttpResponsePartParser.scala:26)\n      at spray.can.client.ResponseParsing$$anon$1$$anon$2.spray$can$client$ResponseParsing$$anon$$anon$$parse(ResponseParsing.scala:44)\n      at spray.can.client.ResponseParsing$$anon$1$$anon$2$$anonfun$3.apply(ResponseParsing.scala:83)\n      at spray.can.client.ResponseParsing$$anon$1$$anon$2$$anonfun$3.apply(ResponseParsing.scala:79)\n      at spray.io.RawPipelineStage$$anon$3$$anonfun$2.apply(Pipelines.scala:99)\n      at spray.io.RawPipelineStage$$anon$3$$anonfun$2.apply(Pipelines.scala:99)\n      at spray.io.ConnectionTimeouts$$anon$1$$anon$2$$anonfun$2.apply(ConnectionTimeouts.scala:56)\n      at spray.io.ConnectionTimeouts$$anon$1$$anon$2$$anonfun$2.apply(ConnectionTimeouts.scala:44)\n      at spray.io.RawPipelineStage$$anon$3$$anonfun$2.apply(Pipelines.scala:99)\n      at spray.io.RawPipelineStage$$anon$3$$anonfun$2.apply(Pipelines.scala:99)\n      at spray.io.SslTlsSupport$$anon$4$$anonfun$2.apply(SslTlsSupport.scala:77)\n      at spray.io.SslTlsSupport$$anon$4$$anonfun$2.apply(SslTlsSupport.scala:62)\n      at spray.io.RawPipelineStage$$anon$3$$anonfun$2.apply(Pipelines.scala:99)\n      at spray.io.RawPipelineStage$$anon$3$$anonfun$2.apply(Pipelines.scala:99)\n      at spray.io.TickGenerator$$anon$1$$anon$2$$anonfun$1.apply(TickGenerator.scala:40)\n      at spray.io.TickGenerator$$anon$1$$anon$2$$anonfun$1.apply(TickGenerator.scala:38)\n      at spray.io.ConnectionHandler$$anonfun$running$1.applyOrElse(ConnectionHandler.scala:63)\n      at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:165)\n```\n\nThe problem seems to be that ResponseParsing pushes an `ByteString.empty` through the event pipeline when it receives a PeerClose (`ResponseParsing.scala:83`). The empty ByteString is not handled by the default case of `HttpResponsePartParser.apply` (`HttpResponsePartParser.scala:40`). @sirthias can you look into that?\n. There seem to be lots of test failures, can you look at them?\n. Ah, sorry, I missed that comment, if it's just stack size, could you add a commit that increases it in `/.jvmopts`?\n. The tests in `spray.can.parsing.HttpHeaderParserSpec` also fail for me with increased stack size.\n. Hmm, the other one in `FileAndResourceDirectiveSpec` seems to be unstable but unrelated to stack size and probably unrelated to your changes, you probably can ignore that one.\n. About the one in `HttpHeaderParserSpec`, @sirthias can you look at them? It seems that the tests make very concrete assumptions about the complete set of headers to parse (which now changed). Either, we accept that this set won't change often and just update the expected values now and then or we would need a more relaxed testing, there.\n. The interesting thing is that it is only now that FileAndResourceDirectivesSpec start failing. I guess it's because of tests now running concurrently when they before ran concurrently in another way.\n\nCan you try adding this commit to the PR: https://github.com/jrudolph/spray/commit/1dd87930f023c1d3b21af03dde215bed5814bf34?\n. I agree with @sirthias that it would be best if you remove the latter two commits and we fix the tests afterwards. I think after b15d365 the failure changed into something which looks more like a dependency issue than a timeout issue, not sure if this is good news :)\n. I updated my commit with a fix if you want to give it a try: https://github.com/jrudolph/spray/commit/3500cfd5b2fd1fbd50cbd281d203375600d8f8ea\n\nOtherwise, just remove the commits and we'll do the fix later.\n. Ok, thanks for checking this, I think let's wrap up and cleanup this PR and we'll then decide how to fix the test:\n- reset this branch to bdf20ad (@sirthias fix for the test)\n- add another commit (or rewrite your original one) to add the `require` for `deltaSeconds`\n\nThen, we'll merge the PR and care for the failing test.\n\nThanks again for your contribution and your patience!\n. :+1:\n. :+1:\n. I also don't know of a better name than `Neutral` and since it's not needed often, I guess it's fine. So, lgtm.\n. Thanks for this suggestion, Mathias. I haven't thought long enough about the general approach (using padding to avoid false sharing) to reason sufficiently enough about when to use it. I currently have those objections:\n- I don't think the padding approach works all the time (the start of the object isn't necessarily aligned on cache line bounds). I rather trust an approach if it would be on the JVM level which makes sure it works as intended (like `@Contended`).\n- I don't trust a measurement of a 3% performance gain. I found a performance change of that size usually very hard to measure with some confidence. \n. :+1:\n. lgtm\n. Otherwise, lgtm. I like the config cleanup.\n. Superseded by akka/akka#16835\n. :+1:\n. :+1:\n. lgtm\n. I just restarted the one failed job. This was indeed very strange, it seemed to get a compile error on the first compile which then just disappeared when running the tests.\n. lgtm, I agree with what @MarkvanderTol says about how to fix the commit message\n. Nice! Thanks, Matthias.\n. lgtm\n. Thanks for the fix!\n\nCould you change your commit message to conform to the new guidelines as outlined here:\n\nhttp://spray.io/project-info/contributing/\n\nWe would also need a CLA from you (if you haven't already signed one).\n. It seems something went wrong. As @MarkvanderTol said, you can usually fix errors like this by using `git rebase -i <commit-id of your first commit>~1`.\n\nEither you retry your fix or reset your branch to [this one](https://github.com/jrudolph/spray/tree/wip/fix-pull-386) where I fixed the commit messages.\n. I think we then have two issues here:\n- Your server produces a response to slow and the request is already completed with the timeout. AFAICS not much spray can do about it.\n- When the response is finally finished the connection may already be closed and you get the exception as documented here. That's because of the unsafe use of the connection actor's ActorContext in the `ResponseReceiverRef` (which will be removed sooner or later, see #187). That's something we should fix for now so that you don't get those nasty log messages.\n\nDo you agree?\n. > The best way to fix this is probably to iterate through the byte array with a @tailrec, manually \"cast\" bytes to Char and append them to the sb.\n\nI agree, that would be the best way to get rid of the warning.\n. I think the idea is that Rendering is optimized to produce `ByteStrings` / `Array[Byte]` because that's what has to be sent on the network.\n\nAFAIU StringRendering is only used with elements that produce only ASCII characters (and when they wouldn't, we still wouldn't know which charset to use). But I agree that the rendering infrastructure seems to be a bit overloaded to do different things at different places but that's probably the DRYest way to do it.\n. lgtm\n. Thanks for the reminder. I think the problem is, that we don't currently publish documentation for nightly builds. Maybe, that's the feature that is needed to update docs more frequently.\n. The fix looks good but I don't understand the original issue. If a malicious site has control over the callback parameter that's already as bad as it can get. If we want to get rid of the security issues we have to get rid of jsonp altogether. I understand that it's supposed to be somehow connected with an IE6-8 bug which may reinterpret the result as javascript when wrapped with `<script>` tags but how does this matter when jsonp is javascript in the first place?\n. @eelbaz, this issue should be fixed since several months. If you have another issue, please report it on the mailing list.\n. lgtm\n. :+1:\n. One thing to keep in mind is that there's no support for half-closed connections in SSL (but SSL on the other side requires half-closed connections from its transport layer). This means \n1. `keepOpenOnPeerClosed` is not supported on top of SSL (once you receive `PeerClosed` the connection is closed)\n2. `keepOpenOnPeerClosed` should always be enabled on the transport layer beneath SSL so that one can wait for the other side's SSL level `close_notify` message without barfing `RST` to the peer because this socket is already gone\n. Should be fixed by #406.\n. This is no essential 1.0/1.1/1.2 change for me. @sirthias ?\n. From [rfc1952](https://tools.ietf.org/html/rfc1952#page-5):\n\n```\n     MTIME (Modification TIME)\n        This gives the most recent modification time of the original\n        file being compressed.  The time is in Unix format, i.e.,\n        seconds since 00:00:00 GMT, Jan.  1, 1970.  (Note that this\n        may cause problems for MS-DOS and other systems that use\n        local rather than Universal time.)  If the compressed data\n        did not come from a file, MTIME is set to the time at which\n        compression started.  MTIME = 0 means no time stamp is\n        available.\n```\n\nUsing `gzip -n` on linux sets MTIME to zero.\n. And kudos, @agemooij, for spotting the difference, I ran it about 10 times and didn't see the output change.\n. @agemooij looks generally good to me and contains lots of useful tests.\n\nI don't understand the subtle nuances between `decompressRequest` and `decompressRequestIfRequested`. When would you use the first and when the other one? (For compressing, we can choose what to do, for decompressing we have to decompress what we get from the other side or reject the request, but in that regard both directives seem to do the same, no?)\n. When this is finished we should update the example at http://spray.io/documentation/1.2-M8/spray-routing/ to use the new directives (if the example isn't thought to show off composition features).\n. Similarly to the decompression side, having two directives on the compression side, `compressResponse` and `compressResponseIfRequested`, means you have to decide which to use.\n\nif you look at the [original Http/1.1 RFC 2616](https://tools.ietf.org/html/rfc2616#section-14.3) it says this about missing `Accept-Encoding`:\n\n> If no Accept-Encoding field is present in a request, the server MAY assume that the client will accept any content coding. In this case, if \"identity\" is one of the available content-codings, then the server SHOULD use the \"identity\" content-coding, unless it has additional information that a different content-coding is meaningful to the client.\n\nwhich I would interpret as \"you should normally use compressRequestIfRequested\" because this would choose `identity` encoding.\n\nHowever, in the latest [HTTPbis draft](http://tools.ietf.org/html/draft-ietf-httpbis-p2-semantics-23#section-5.3.4), this paragraph seems to be gone and it only says this:\n\n> A request without an Accept-Encoding header field implies that the  user agent has no preferences regarding content-codings.  Although    this allows the server to use any content-coding in a response, it    does not imply that the user agent will be able to correctly process  all encodings.\n\nSo, the conservative choice would be to normally use `compressRequestIfRequested`.\n. > Are you suggesting [...]\n\nAt least I'd like to be able to give a sane advice which directive to choose in the usual case. And I guess it would be this (from your current tests):\n\n```\nval decompressCompressIfRequested = (decompressRequest & compressResponseIfRequested)\n```\n\nSo, yes maybe it would make sense to remove `compressResponse` as well and also add the above as the standard compression/decompression directive, maybe named `decompressCompress` or just `enableCompression` or even something simpler. WDYT, @sirthias?\n\nThanks again, the code LGTM and I've got the feeling the discussion here was long but hopefully anticipates decisions and questions for our users and simplify spray's use.\n. Cool, I'm not strongly against one or the other. My main concern is that we have decided for ourselves and for the documentation what to suggest and that's what the discussion has achieved. I also agree that `(decompressRequest & compressResponse)` is clean and explicit enough to serve as a catch-all solution if you don't want to worry about things but as @sirthias said in other cases you want to explicitly define which requests/responses to de/compress in which case you'd often would specifically move the directive deeper down the route tree.\n. Very nice. Thank you. lgtm\n. Btw. docs commits don't need the `+` marker as they don't add anything to the API.\n. See this investigation: https://groups.google.com/d/msg/spray-user/_Hchc3b8Dv4/Op1XZJ0zlqkJ\n. Looking into it...\n. The PipelineLimiter didn't work at all for normal requests. I fixed that in the above PR. \n\nHowever, I'm generally sceptical about how much we are able to deflect incoming requests when the load is already high because of the way SuspendReading is delivered asynchronously to the IO connection during which the connection may already have read MBs of more data. I've got the impression that this is a general problem of event based systems handling external events and I will start a discussion on the akka-user mailing list discussing further steps.\n. I think this may already be fixed but we should verify it by testing / reviewing the code.\n. As the URI is built from `HttpServletRequest.getRequestURL()` and [its javadoc](http://docs.oracle.com/javaee/6/api/javax/servlet/http/HttpServletRequest.html#getRequestURL) states that it contains the complete url including server name and port, etc., this should already work as intended.\n. lgtm, I restarted the failed travis job part.\n. Thanks, Age. \n\nHave you examined how the method might interact with the `spray.http` package name? Usually you won't have `spray._` imported so I guess it's usually not a problem but can we think of other ones?\n\nAnd I wonder if it really warrants its own directive category but if in doubt maybe yes.\n\nRegarding documentation, the `headerValueByName` directive is already documented so you could go there for inspiration.\n. That's a good observation. With this in mind I wonder if it really is desirable to have those two as overloads. Do we have any naming scheme to distinguish pure filters and extraction directives? If not I think we should think about one. Not using overloads but different names will also lead to better error messages.\n. > figuring out the correct includecode snippet incantations\n\nIs there anything wrong with it that we could improve?\n. > Is there anything wrong with it that we could improve?\n\nI.e. apart from a bug that I just found where methods with type parameters aren't found.\n\nBtw. the build is failing with something that looks like a legitimate compile error:\n\n```\n[error] /home/travis/build/spray/spray/spray-routing/src/main/scala/spray/routing/directives/SchemeDirectives.scala:36: type mismatch;\n[error]  found   : Some[spray.routing.SchemeRejection]\n[error]  required: spray.routing.Rejection\n[error]     schemeName.require(_ == schm, Some(SchemeRejection(schm))) &\n[error]                                       ^\n[error] one error found\n```\n. > and eventually found that using `scheme(` did the trick\n\nAh, ok similar error than mine, which I fixed some time ago (https://github.com/spray/spray/commit/9dded0e6d01f79a3e54acbd43dc56f654419a03d) but which maybe was missing in your branch. With the fix it should also work without the `(`.\n. This last little thing and then it LGTM. Thanks again, Age, for getting through all of this with us just for what seemed like a simple 2 line addition :)\n. This quick fix lgtm as the complete logic here will have to go once we finally get rid of UnregisteredActorRefs\n. Thanks, @agemooij, good comments, I added a link to the mailing list thread for OS/X information. Since I've got no OS/X system I can't really offer any help on the exact procedure and it seemed the mailing list also didn't offer any definite instructions.\n. I also added another commit, that changes the build to try to locate `sphinx-build` in well known locations if `SPHINX_PATH` doesn't point to an existing file.\n. Here's the older issue and thanks for adding more reasons why it is important:\n\nhttps://github.com/spray/spray/issues/102\n. Actually, it had \"proxies\" in the name and not \"proxy server\". I just changed it 10 minutes ago to make it easier to find.\n. > This would incur some overhead of course but would be feasible. In fact it is what akka itself is doing\n\nBut woudn't this mean that the feature @taylorleese requested is already implemented by akka? If I look here: https://github.com/akka/akka/blob/c10eadbd3388250425efff0b790f59af6dbdc63d/akka-actor/src/main/scala/akka/event/Logging.scala#L202 it seems like there's a way of configuring how a LogSource is converted into a string (usually the path) and that string value should later be available in the MDC as `akkaSource`. So what is then missing?\n. What we would lose in contrast to the current approach is the possibility of hierarchical configuration. But I guess that's something that indeed can't be improved with the existing logging frameworks.\n. Yes, I guess that's the best idea. Any thoughts from @clementgarnier who originally raised the issue?\n. Otherwise, there's no easy way to access the most recent documentation (e.g. for new features) as already observed in #391.\n. That would complicate the whole procedure much and wouldn't add much because 1.0, 1.1 and 1.2 should be mostly feature equivalent.\n. This proposal would add another version in the documentation tab e.g. `1.1-20130801` in addition to everything that's there now. So, documentation would match the current source code of the version specified (as long as the documentation in the source tree is current which usually is an invariant we try to maintain through pull request reviews). Each night, we would just publish documentation for the latest nightly.\n\nA downside would be that we wouldn't be providing documentation for older nightlies (and links to those would get stale), but instead just for the latest 1.1 nightly.\n\nMaybe @sirthias has a better idea.\n. Thanks, good catches. LGTM\n. This still had some bad bugs I fixed with this new version.\n. I added code to the `simple-http-server` example which can handle file-uploads by streaming the chunks to disk and then uses mime-pull to extract the parts. This won't be the final version but I figured it would be useful to have a working example for one of the major use-cases directly for experimenting.\n. I don't think mixing with `parameters` makes sense.\n\nMy suggestion, instead, is to be explicit and support directives that map to HTTP features:\n- have separate `formFieldsUrlEncoded` and `formFieldsMultipart`\n- maybe have a joined `formFields` directive which works similar to the current one but without the [default value of `null`](https://github.com/spray/spray/blob/464ffe971953ceac2f24cd0ff2f4882a68d83106/spray-routing/src/main/scala/spray/routing/directives/FormFieldDirectives.scala#L95) for the `FEOU` which defers the error that it can't read multipart data to runtime.\n- add more documentation about how to implement joined deserializers/FEOU that can handle both kinds of formFields together.\n\nWDYT?\n. Maybe we should start with a less intrusive change: even with `multipart/formdata` common form data is transported as `text/plain` as explained here: http://www.w3.org/TR/html401/interact/forms.html#h-17.13.4.2. For unmarshalling `multipart/formdata` forms we should \n- check if we have an FEOU for the target type which would read the body part\n- otherwise, check if the content-type of the body part is `text/plain` and then just use the `Deserializer[String, T]` for deserialization\n- only then fail with the error message that we cannot decode `multipart/formdata`\n\nAlso, in the other direction, i.e. for `application/x-www-form-urlencoded`:\n- check if we have an FSOD available and use that\n- otherwise, check if we have an FEOU available and feed it with an HttpEntity(text/plain, textData)\n- and only then fail\n\nWith those changes we should probably cover 90% of the cases occurring in practice. For the remaining 10% (custom content-types for BodyParts, allowing more than more content-types for BodyParts with different unmarshallers), I find it pretty hard to wrap my head around what exactly I need to do to make it work. For those cases we need step-by-step instructions how to get from a route failing at compile-time or runtime to a program that compiles and accepts data as intended.\n. I haven't looked at the code itself yet. \n\nI wonder what the restriction you are talking about means for the original purpose of this functionality as discussed in #402. As I understand it, without the complete raw URI string as used in the request you still won't be able to support the HMAC protocol for absolute URIs even with spray-can. This restriction could be ok if you can instruct your clients never to use absolute URIs for the requests. But, e.g. what happens if someone uses an HTTP proxy which just forwards the absolute URIs it gets? I tried to find something but it seems that it is not clear how HMAC based authentication should work together with proxies, so maybe this is also not too important.\n. > This doesn't affect HMAC at all: it has a normalization pass and only the raw (undecoded) path and query string is needed. The host and port and normalized separately. For details, refer to section 3.2.1 of the draft specification.\n\nFrom the section:\n\n> The HTTP request-URI as defined by [RFC2616] section 5.1.2.\n\nand this could also be an absolute URI AFAIU.\n\n> Proxies are a different matter. According to my reading of the HTTP/1.1 spec they're not allowed to rewrite the URI target. I suspect some probably do though. (This is an additional use-case for this header, by the way: implementing a proxy.)\n\nAlso, see the referenced section 5.1.2\n\n> The absoluteURI form is REQUIRED when the request is being made to a proxy.\n\nThat's what my comment was about: if a proxy is involved it is likely that the request URI is absolute and since I don't see how HMAC say anything about just using path + query string this could break down if we just have access to path and query.\n\nThat said, I don't think we necessarily disagree about what to do about it. I actually like it the way you have done it now. Let's wait for @sirthias opinion before doing anything else.\n. Thanks Martin. You are right with your assumptions. Custom HttpMethod support is currently missing. We should probably do both: add more standard ones and add support for custom ones.\n. Closing for now. @clementgarnier, if there's something missing, please reopen.\n. Thanks, @davidillsley, for reporting and @guersam for answering. I'll close this one for now. If you think there's still something missing, please feel free to reopen.\n. Thanks for noticing and sorry for the misleading documentation.\n\nIt is mentioned here: http://spray.io/project-info/current-versions/ but I agree it would be better if it would be placed somewhere more prominently.\n. We discussed internally how we could support proxies automatically in a cleaner fashion (see #102), sorry for misleading you a bit. What's definitely worthwhile is the change adding the new headers. In a first step, could you create a new PR containing just the code for the new headers? I don't think we need the changes in `RequestBuilding` because in the best case you would be able to configure proxy credentials externally and they would be automatically picked up.\n. I think we can close this for now.\n. Cristian, thanks for finding this bug and fixing it. Two things: can you rewrite your PR commit to adhere to our commit message guidelines and sign the CLA. See http://spray.io/project-info/contributing/\n. Oh, I'm just seeing you referred to the old wiki for the documentation. The documentation in the wiki is severely outdated! See http://spray.io for the new documentation. That said I can see some value in re-introducing this feature for the current version.\n. This is solved in akka-http with the new streaming multipart support.\n. This is done in akka-http. For spray-routing we currently distribute two artifacts, one against shapeless and one against shapeless2.\n. Thanks, Julien, for the comprehensive report. We'll look into it.\n. To be clear, this situation arises when the server accepts the connection but never responds, right?\n. > From what I could see, it is more like the server does not respond to the connection attempt.\n\nYes, you're right that's how I could reproduce the issue. So, the problem is, that the `ClientFrontend` doesn't handle a request-timeout properly if the timeout is due while the connection isn't yet established.\n. And I guess the invalid assumption in ClientFrontend is that closing a connection is always possible even if not yet connected but `TcpOutgoingConnection` doesn't react on closing messages in `connecting` state. I created a ticket in akka to track this:\n\nhttps://www.assembla.com/spaces/akka/tickets/3554-io--clarify-how-to-close-connection-that-is-not-yet-established#/activity/ticket:\n\nWe could fix this on our side just by shutting down the HttpClientConnection when we get into this situation.\n. @sajanalexander, you mean before it is fixed properly? Probably, you have to send it either a `PoisonPill` or `context.stop()` it.\n. I just figured there's no way to shutdown the akka IO level TCP connection actor because the http side haven't got an ActorRef for it (usually the sender of the Tcp.Connected msg). So, the right way to handle this (as already discussed) is to shut-down the HttpClientConnection actor itself.\n. Also, all of the above explanations make no sense whatsoever if you consider that ClientFrontend and all of the pipelining stuff shouldn't even run before the connection is established! This just got a lot weirder.\n\nThis is what happens:\n\n```\n09/11 15:11:23 DEBUG[on-spray-can-akka.actor.default-dispatcher-2] s.c.c.HttpHostConnectionSlot - Attempting new connection to 1.1.1.1:6666\n09/11 15:11:24 DEBUG[on-spray-can-akka.actor.default-dispatcher-4] s.c.c.HttpClientConnection - Attempting connection to /1.1.1.1:6666\n09/11 15:11:24 DEBUG[on-spray-can-akka.actor.default-dispatcher-2] a.i.SelectionHandler - Executing [WorkerForCommand(Connect(/1.1.1.1:6666,None,List(),None),Actor[akka://on-spray-can/user/IO-HTTP/group-0/0],<function1>)]\n09/11 15:11:24 DEBUG[on-spray-can-akka.actor.default-dispatcher-3] a.i.TcpOutgoingConnection - Attempting connection to [/1.1.1.1:6666]\n09/11 15:11:24 DEBUG[on-spray-can-akka.actor.default-dispatcher-3] a.i.TcpOutgoingConnection - Connection established to [/1.1.1.1:6666]\n(true,false)\n09/11 15:11:24 DEBUG[on-spray-can-akka.actor.default-dispatcher-4] s.c.c.HttpClientConnection - Connected to /1.1.1.1:6666\n09/11 15:11:24 DEBUG[on-spray-can-akka.actor.default-dispatcher-6] a.i.TcpOutgoingConnection - [Actor[akka://on-spray-can/user/IO-HTTP/group-0/0]] registered as connection handler\n09/11 15:11:24 DEBUG[on-spray-can-akka.actor.default-dispatcher-6] s.c.c.HttpHostConnectionSlot - Connection to 1.1.1.1:6666 established, dispatching 1 pending requests\n09/11 15:11:24 DEBUG[on-spray-can-akka.actor.default-dispatcher-6] a.i.TcpOutgoingConnection - Read [0] bytes.\n09/11 15:11:24 DEBUG[on-spray-can-akka.actor.default-dispatcher-4] s.c.c.HttpHostConnectionSlot - Dispatching GET request to / across connection Actor[akka://on-spray-can/user/IO-HTTP/group-0/0]\n09/11 15:11:24 DEBUG[on-spray-can-akka.actor.default-dispatcher-3] a.i.TcpOutgoingConnection - Wrote [0] bytes to channel\n09/11 15:11:29 WARN [on-spray-can-akka.actor.default-dispatcher-2] s.c.c.HttpClientConnection - Request timed out after 5 seconds, closing connection\n09/11 15:11:29 DEBUG[on-spray-can-akka.actor.default-dispatcher-3] s.c.c.HttpHostConnectionSlot - GET request to / timed out, closing connection\n09/11 15:11:29 DEBUG[on-spray-can-akka.actor.default-dispatcher-2] a.i.TcpOutgoingConnection - Got Close command but write is still pending.\n09/11 15:11:29 WARN [on-spray-can-akka.actor.default-dispatcher-2] s.c.c.HttpClientConnection - Request timed out after 5 seconds, closing connection\n09/11 15:11:29 WARN [on-spray-can-akka.actor.default-dispatcher-3] s.c.c.HttpClientConnection - Request timed out after 5 seconds, closing connection\n```\n\nwhile this is happening on the wire:\n\n```\n15:15:06.793738 IP 192.168.7.122.35792 > 1.1.1.1.6666: Flags [S], seq 1749535247, win 14600, options [mss 1460,sackOK,TS val 4078190 ecr 0,nop,wscale 7], length 0\n15:15:07.791911 IP 192.168.7.122.35792 > 1.1.1.1.6666: Flags [S], seq 1749535247, win 14600, options [mss 1460,sackOK,TS val 4078440 ecr 0,nop,wscale 7], length 0\n15:15:09.795906 IP 192.168.7.122.35792 > 1.1.1.1.6666: Flags [S], seq 1749535247, win 14600, options [mss 1460,sackOK,TS val 4078941 ecr 0,nop,wscale 7], length 0\n15:15:13.807898 IP 192.168.7.122.35792 > 1.1.1.1.6666: Flags [S], seq 1749535247, win 14600, options [mss 1460,sackOK,TS val 4079944 ecr 0,nop,wscale 7], length 0\n```\n\nAs you can there are only SYN packages without any responses.\n\nSo, in fact, the process is like this: \n- JDK nio instantly reports the channel as connectable\n- channel.finishConnect succeeds\n- akka IO reports the connection as Connected\n- HttpClientConnection reports the connection as Connected\n- HttpClientConnection tries to send the request\n- the requests ends up in Akka IO's one-entry write queue but cannot be written (because the connection isn't actually established)\n- HttpClientConnection eventually tries to close the connection because of the request timeout\n- Akka IO tcp connection doesn't regularly close the connection as long as the write is pending (which it will be forever because the \"connected\" connection is basically idle\n. This sent me on a trip: see https://www.assembla.com/spaces/akka/tickets/3602-io--tcp-connection-establishment-always-succeeds-even-if-endpoint-never-answers for a description of the underlying problem.\n. I'd also like to try out a more aggressive one where I revert the previous changes for `finishConnect`. I'll send you a note when it's done. Thanks for helping out with testing.\n. Here's the probable underlying issue:\n\nhttp://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6371630\n. Thanks, Age. LGTM\n. Interesting, I wonder if there's code which relies on the old behavior.\n. We already did some tests on this:\n- our rendering is still not optimal, we use `akka.util.ByteStringBuilder` which copies a lot more around than necessary (see branch `wip/better-bytestringrendering` for an improvement)\n- as long as HttpEntities are based on `Array[Byte]` instead of `ByteString` we can't avoid copying things around (because of array mutability), we are already in the process of improving the HttpEntity API so that it uses `ByteString` instead of `Array[Byte]` which will allow us to avoid any copies of the payload apart from the final copy to the network, also this will enable spray to stream files without any user-space copies\n\nThat said, in many cases raw byte throughput won't be the limiting factor because you already need a 10Gb network to saturate a spray server with this benchmark (1000 RPS \\* 1Mb/req). Still saving work (and cache lines) is always a good thing.\n. I agree it makes sense for those error conditions we control. LGTM \n. lgtm\n. Thanks, lgtm\n. Looks very clean now. Thanks! :+1:\n. I actually liked the proposal to leave SprayActorLogging deprecated for a while. I mean, currently we don't make any compatibility promises but leaving it around deprecated with a comment would people help to get around this simple source change and wouldn't cost us much, right?\n. lgtm\n\nBtw. did you have use case in mind for adding the cache?\n. We'll change configuration, that this case fails fast when you try to set an invalid `user-agent-header` setting. Apart from that, what Mathias' said: we won't allow to specify invalid headers usually. If you really need to you can probably create an instance of the `User-Agent` header with an invalid `ProductVersion` instance but we won't change the API to simplify that case.\n. Reading the comment at https://github.com/spray/spray/blob/master/spray-can/src/main/scala/spray/can/client/HttpHostConnectionSlot.scala#L62 would confirm this suspicion. If it can happen in `unconnected` it could probably happen in `connecting` as well, right? Can we make this more robust?\n. Let's close this as long as we don't know that it is really needed.\n. The reason is that we want to allow all possible representations in our model. By using a special empty string we hope that you don't even notice any change unless you care.\n. Now we have some reports of server failing to understand queries rendered without '='. See https://groups.google.com/d/msg/spray-user/vU5MNsK6YZA/k5sqH5X8oPcJ\n. One possibility would be to claim that we haven't seen any `Content-Type` header when we see two and just pack those two (or more) into the `headers` field and deliver the entity with the default ContentType. This way a client can handle a broken peer in any way possible later on. WDYT?\n. @martin-g haha, good catches. Thanks for looking through it.\n. Fixed in https://github.com/spray/spray/commit/80f205ffa925c43a1878c85e70a916a9b52f64f6\n. > Can it be used in spray?\n\nIt probably could. The upcoming akka-http module will support backpressure handling through Akka's reactive streams implementation.\n\n> BTW: why does spray source have akka io source included?\n\nIt's included for Akka versions (2.0.x / 2.1.x) which come without Akka IO support. Starting with spray 1.2 (Akka 1.2.x) spray uses Akka IO proper.\n. This is fixed in akka-http by using reactive streams.\n. Also filed on the akka side:\n\nhttps://www.assembla.com/spaces/akka/tickets/3571\n. This is build from a new [release/1.2-M8](https://github.com/jrudolph/spray/tree/release/1.2-M8) branch that contains exactly two commits over the tag 1.2-M8: \n1. a bulk commit containing all published documentation changes for 1.2-M8 (last sync at 13/06/2013 with 07c046a18322147869aefbb5ea48a84def36d01d whose sources however were merged into release/1.2 only on 18/06/2013 with ff77fbcb4d19d5db5631a0f620ea059e5b84d1b1)\n2. this change\n\nI think this is a cleaner approach than building documentation updates for 1.2-M8 from the release/1.2 branch which already contains lots of documentation changes not applying to 1.2-M8 any more. WDYT?\n. Btw. IIRC that's also the way documentation updates in sbt are organized.\n. Also, see this: http://davenport.sourceforge.net/ntlm.html\n\nThe spray team won't be able to actively work on this as we will focus on improving the core of spray. We will support initiatives to make this available as an extension by providing the needed hooks if missing but it is even unlikely that we would integrate it into the core because of possible security implications we just cannot oversee.\n. This isn't planned but could be provided as a contrib module.\n. lgtm, getting rid of `Array[Byte]` feels really good.\n. There seems to be one legitimately failing test:\n\n```\n[error] x reject the request if it has a `Content-Encoding: deflate` header but the request is compressed with Gzip\n[error]    List(UnsupportedRequestEncodingRejection(gzip), CorruptRequestEncodingRejection(incorrect header check), UnsupportedRequestEncodingRejection(identity))\n[error]     is not equal to \n[error]    List(UnsupportedRequestEncodingRejection(gzip), CorruptRequestEncodingRejection(unknown compression method), UnsupportedRequestEncodingRejection(identity)) (EncodingDirectivesSpec.scala:395)\n[error] Expected: ...tion([u]n[kn]o[wn ]c[ompr]e[ssion] [method]), Un...\n[error] Actual:   ...tion([i]n[c]o[rre]c[t h]e[ader] [check]), Un...\n```\n. Great! :+1:\n. The problem this ticket is trying to solve is that incoming file data may flood the Java heap. The suggestion here would be to buffer incoming data to the disk until it can be handled. \n\nIn akka-http, backpressure seems to provide a better solution: out-of-the-box only as much data will arrive in the Java heap as the server is currently able to handle. Buffering is rarely needed because either the data should be handled in a streaming fashion directly or just streamed to a file. This will happen naturally through reactive streams combinators.\n\nSo, since we don't fix this in spray any more, this doesn't seem to be an issue any more.\n. Thanks, lgtm.\n. Thanks @BalajiSi. Yes, looks good. If this is your first contribution we'd need a CLA. Also make sure your commit message adhere's to our rules. See here:\n\nhttp://spray.io/project-info/contributing/\n\nI'd see this as a bugfix commit, so it would need to be prefixed with `=`.\n. I tried it and it fails as you say but not because the field is missing (it is actually there, but the diff in the test doesn't show it) but because fields are reordered unless you use a `ListMap` which preserves ordering. In which setting did you encounter this problem?\n. I.e. maybe you can first check if the field is really missing in a wireshark dump of your actual usage?\n. This is the commit making your test succeed: https://github.com/jrudolph/spray/commit/34ed6b093133d341e2b2da7c815929ddf7a33830\n. And FTR (as I also struggled to find out), the cause for the test diff missing essential parts in this case is `HttpBody.toString` abbreviating bodies longer than 500 chars.\n. The code itself LGTM. I'm not 100 percent sure if we really want to support it that way because of the model being not in \"normal form\" any more (see [this comment](https://github.com/spray/spray/issues/327#issuecomment-23710140)). @sirthias?\n. The few comment aside :+1:\n\nTwo things: \n1) Can you update this PR to the current master?\n2) Can you change commit line of the second commit to this:\n`! http, httpx: make multipart form-data more flexible but have it adhere to the RFC more strictly`\n\nThanks, again.\n. We'd like to finish this PR this week. I think I'll make the changes for you @dlouwers because it contains a merge conflict you didn't cause and create another PR for you to give your ok to.\n. I went ahead and completed this PR in #536, please review @dlouwers if that's ok for you.\n. Thanks @mpilquist, this is already a very good starting point.\n. The current proposal would basically duplicate the same logic for client and server side. If we would extract the logic into a pipeline-stage we could just reuse it. The event pipeline of the new stage would listen for `SSLSessionEstablished`, `HttpMessageStartEvent`, and `HttpMessage` events and then add the header to the message. Additionally, the setting would control if the pipeline stage would be enabled at all, so that users that don't care wouldn't have any code running for it.\n\nWe would need to add a new method in `HttpMessageStart` to add the header generically. WDYT?\n. > I don't follow why it would need to capture the HttpMessage event though.\n\nYeah, sorry, I meant `Http.MessageEvent`.\n. > Something like withHeaders(headers: List[HttpHeader])\n\nI think we need something more like `HttpMessage.mapHeaders`. The current meaning for `withHeaders` is to overwrite the complete list of headers.\n. I think we should keep `spray.can` free from implementation stuff. Maybe let's put the stage into `spray.can.parsing` and just access `HttpMessageStartEvent` across package boundaries.\n. This looks very clean now. I previously wanted SSLSessionInfo to carry more semantics but it is now so simple that we maybe should leave it this way and make no more assumptions about what the SSLEngine will deliver for the parameters. :+1:\n\nIf @sirthias agrees, can you squash the changes into two commits: one with just the SslTlsSupport changes (simplifying a possible akka-io backport) and one with just the spray-can changes.\n. Very nice. LGTM. Why would you say that both of those commits are `!` commits? Are there any source incompatible changes?\n. Yep, thanks for the explanations @mpilquist \n. > A general question: Should I squash right away after every iteration? I thought the updates and comments would be more comprehensible if I wait with the squash right until merging.\n\nIMO iterative commits are fine usually. If changes can be split into several standalone commits they always should be. In those cases, there has to be a time in the end where the final commit split should be reviewed.\n. I'd still like to see proxy authentication added in the same way but maybe we can add this in another step.\n. I'd like to discuss the lifetime of `ProxiedHostConnector` in relation to its underlying `HttpHostConnector`. I think what is missing is the `DemandIdleShutdown` logic on the `ProxiedHostConnector` level to avoid race-conditions and lost requests due to the underlying `HttpHostConnector` during shutdown.\n. > Wouldn't this also be useful for normal authentication? \n\nI don't think it's comparable. The proxy settings are settings of the transport mechanism while http authentication often is a feature of single requests.\n\n> We could simply add default headers to the HostConnectorSetup.\n\nThat could be one solution.\n. > I think this patch is fine with regard to the CloseAll command support.\n\nWhat, if the ProxiedHostConnector doesn't know about the DemandIdleShutdown message, how can it stop sending messages to the real HostConnector?\n\nIIUC the contract between HttpManager and HttpHostConnector is not to send any more requests to the connector once the PoisonPill is sent (because they will end up as DeadLetters). As the ProxiedHostConnector is still in the list of connectors even the HttpManager will still send requests to the ProxiedHostConnector (and through this to the HttpHostConnector). Is there a simple solution for this case?\n. For easier review here's the comparison of the changes over plain akka master:\n\nhttps://github.com/jrudolph/spray/pull/1/files\n\n(Changes are expected because our master is on akka 2.1)\n. Ok, that didn't work as it just shows the changes of your commit and not the diff.\n\nHere's the diff:\nhttps://github.com/jrudolph/spray/compare/wip/stupid-akka-backport...spray:ticket/472?expand=1\n. Ok, scrap that, it seems you can't compare two branches in github\n. LGTM\n. Yes, that's a known spurious failure. I restarted the job.\n. I think it's about _request_ rendering on the client side.\n. See https://groups.google.com/forum/#!searchin/spray-user/http$20Host$20connector$20idle-timeout/spray-user/_N9Q1dlroVg/jQGeIDnGDmMJ\n\nand\n\nhttps://groups.google.com/forum/#!searchin/spray-user/http$20Host$20connector$20idle-timeout/spray-user/iakr7aRS4AI/VavuSHuRenAJ\n\nEffectively, it means that you can never be sure the request-level API will deliver a result until you set the idle-timeout to infinite. However, this makes no sense, because especially for auto-created HttpHostConnectors you want to be sure they don't hang around indefinitely.\n. It seems this is already handled correctly with the `DemandIdleShutdown` message.\n. :+1:\n. @mhamrah, thanks a lot. A basic thing: can you add a few line-breaks into text blocks? It won't change the rendering but makes it easier to review in the github web-interface and will later make diffs more readble.\n. @sirthias what do you think about directly including tests from `spray-routing-tests` instead of adding extra-tests to `docs/test` (I think @agemooij recently set the precedent). Problems I see is:\n- tests are not necessarily written for use in documentation, i.e. they don't necessarily show the code but focus on the semantics\n- in this case we show some testing boilerplate instead of how to use the directive, at least more than a documentation reader might need to know\n- when developing and changing tests you always have to keep in mind that you not only change tests but also documentation\n\nI'd slightly prefer to add extra tests for the documentation but would also understand to safe some work by reusing the tests from the regular test suite.\n. Btw. I think it might be faster if we take this this PR as it is (or with the few improvements I commented on). We will in any case have a final round of editing when we make the next big documentation release (probably for 1.0) where we can polish things a bit more. Having these pieces of almost finished documentation is already so much better than having nothing at all. So thanks again.\n. LGTM, just needs squashing and then it's ready to merge.\n. LGTM, thanks Mike.\n. LGTM\n. The behavior is actually defined in many cases that if a Content-Type is missing one should assume ISO-8859-1. So, for accepting data we are not free to choose the charset but have to follow what the spec says.\n. > to default to UTF-8 instead of US-ASCII.\n\nCouldn't we parameterize the unmarshaller and the private method with the charset and then provide only a default one implicitly (I think either one of US-ASCII and UTF-8 would be fine)? That way you could at least easily create your own unmarshaller with a custom default.\n. It's a two-line change and why not add it right now when we are at it, especially, since the spec is so vague that we cannot sufficiently know what to expect in the wild.\n. and/or otherwise maybe just assume the maximum date and emit a warning.\n. See #488. Assume we would now update to shapeless 2.0.0-M1 and release 1.0 with a shapeless 2.0.0-M1 dependency. It would mean that we would bind all of our users to this non-production release for the whole life-time of 1.0. For that reason we won't do an update to an unstable version of shapeless for 1.0/1.1/1.2. For the time after 1.0 we will work together with @milessabin to make the compatibility story as smooth as possible.\n. I'd say let's close it for now and resurrect it / take ideas from it once 1.2 is out. In any case, thanks @andrebeat, for having a first stab at this. It will certainly be helpful when we get to it.\n. I added a new Timestamp class similar to what was proposed above. The reason for choosing Timestamp instead of Deadline was that in many cases we don't know the timeout yet when we would have to create the deadline so it seemed to make more sense to call the type Timestamp and have methods `isPast` and `isFuture` instead of `isOverdue` and `hasTimeLeft`. `Timestamp(Long.MaxValue)` has the special meaning of infinitively in the future.\n. This needs fixes for the tests.\n. I fixed all the issues from the comments.\n. Thanks @martin-g, fixed.\n. Fixed, and also made SimpleStash private[spray]\n. Thanks, lgtm.\n. Once #517 is merged I will update this PR to implement `HttpMessage.asChunkedParts` in terms of the new `HttpData.toChunkStream`.\n. Updated to implement `HttpMessage.asPartStream` in terms of `HttpData.toChunkStream`\n. Arguably `sliceData` is the more expected slice variant so maybe it would make sense to swap names and call `sliceData` simply `slice` and rename current `slice` to `sliceBytes`. WDYT?\n. I added another commit that should addresses the comments. `sliceBytes` is not currently used (= not performance relevant) so I decided that it can be implemented in terms of `slice` which only adds the cost of one temporary wrapper HttpData.\n. This overrides #349.\n. Thanks, @ivantopo. What about \"withPath\"? Seems to be missing in #514 but would probably make sense as well.\n. Cool, thanks. LGTM :+1:\n. @2beaucoup agreed, didn't notice that one.\n. > the first test contains `Host(\"192.0.2.16\") === IPv4Host(\"192.0.2.16\")`\n\nI didn't look at it but I would suspect that this is actually testing the normalization logic of `Host.apply` so it would also be `<actual> === <expected>`.\n. It looks as if the timeout logic isn't updating the `state` variable properly before sending out a response.\n. Also, the header models seem to be missing currently.\n. Thanks, Chip, these are interesting points. Being curious, do you know which part of the client side generates the `X-Playback-Session-Id` header? Is that a thing that safari/iDevices do?\n. FYI I moved this ticket to after 1.0/1.1/1.2 as this is strictly new functionality, we try to focus on getting the remaining bugs fixed so that the 1.0 release gets rolling. I think we should try to remove all roadblocks before 1.0 that would prevent a custom solution and AFAIK it currently is possible to implement it right now by writing a directive that responds properly to requests with `RawHeader(\"Range\", ...)` set.\n. Cool, that's already a good starting point. Where does `ByteRange` come from?\n. A directive that would work for any kind of inner route would probably look similar to this (only for non-chunked responses):\n\n``` scala\n  def rangeSupport: Directive0 =\n    optionalHeaderValueByName(\"Range\").flatMap[HNil] {\n      case Some(ByteRange(start_spec, end_spec)) =>\n        val start = ...\n        val length = ...\n        mapHttpResponseEntity { entity =>\n          entity.flatMap { nonEmpty =>\n            HttpEntity(nonEmpty.contentType, nonEmpty.data.slice(start, length))\n          }\n        }\n      case None => pass\n      case _ => Route.toDirective(complete(StatusCodes.BadRequest, \"Invalid range request\"))\n    }                                                                             \n```\n. This seems to be supported now for quite some time through RangeDirectives.\n. Maybe, as well. I see the RequestContext more as an internal thing. There were many messages on the ML where someone needed access to the complete URI that an extractor for such a basic element seems to make much sense. One of the downsides is that people often start to rewrite basic logic like PathMatchers if they find the URI-extracting directive first...\n. > As you suspect a Content-Type header containing a \\* is not valid HTTP\n\nActually, similar to the other discussion we had, this is debatable because again the \"subtype\" of a media type can be an arbitrary token which would include the single asterisk. Consequently, that's also what the parser accept. However, in contrast, we have a safety measure in `MediaType.custom` which prevents media types from being created with single asterisks as the subtype.\n\nIn the PR I changed `MediaType.custom` to accept the single asterisk if it's being called from the parser.\n. There are some merge conflicts because of the privatization.\n. Nice :+1:\n. :+1:\n. Here's a test case that observes the behavior:\n\n``` diff\ndiff --git a/spray-routing-tests/src/test/scala/spray/routing/ChunkingDirectivesSpec.scala b/spray-routing-tests/src/test/scala/spray/routing/ChunkingDirectivesSpec.scala\nindex 4c631de..f5f008f 100644\n--- a/spray-routing-tests/src/test/scala/spray/routing/ChunkingDirectivesSpec.scala\n+++ b/spray-routing-tests/src/test/scala/spray/routing/ChunkingDirectivesSpec.scala\n@@ -16,6 +16,9 @@\n\n package spray.routing\n\n+import spray.http.HttpHeaders.RawHeader\n+import spray.http.StatusCodes\n+\n class ChunkingDirectivesSpec extends RoutingSpec {\n\n   \"The `autoChunk` directive\" should {\n@@ -27,5 +30,22 @@ class ChunkingDirectivesSpec extends RoutingSpec {\n         new String(bytes) === text\n       }\n     }\n+    \"must reproduce status code and headers of the original response\" in {\n+      val text = \"This is a somewhat lengthy text that is being chunked by the autochunk directive!\"\n+      val responseHeader = RawHeader(\"X-Custom\", \"Test\")\n+      val route = autoChunk(8) {\n+        respondWithHeader(responseHeader) {\n+          complete((StatusCodes.PartialContent, text))\n+        }\n+      }\n+\n+      Get() ~> route ~> check {\n+        chunks must haveSize(10)\n+        val bytes = chunks.foldLeft(body.data.toByteArray)(_ ++ _.data.toByteArray)\n+        new String(bytes) === text\n+        status === StatusCodes.PartialContent\n+        header(\"x-custom\") === Some(responseHeader)\n+      }\n+    }\n   }\n```\n. I've got a change which removes HttpIp but as it turned out that only complicated the usage. Therefore, I conclude that there is some usage in HttpIp so I suggest to postpone any further action.\n\nThat said, I'd say there's some need in a decent model (i.e something else as InetAddress) for IPs/host addresses that would make a useful addition at a later time. So I'm leaving this open as a reminder that this may be useful in the future.\n. Created an akka ticket as well: https://www.assembla.com/spaces/akka/tickets/3635-ssltlssupport-should-adhere-to-the-io-message-protocol-to-upper-layers-and-support-back-pressure\n. :+1:, nice change\n. LGTM aside from the single documentation typo.\n. Thanks for the report. It seems, the generation code would need extra support for windows path separators.\n. I updated the stage to check for pending writes by sending a probe to the connection when the timeout triggers. Only if the probe acknowledges that there are no writes pending the connection is closed down after another timeout period.\n. I updated the PR and fixed the issues.\n. Will be fixed in akka-http: akka/akka#16834.\n. LGTM, apart from the particular case of parameter naming.\n. lgtm\n. lgtm\n. Definitely more robust, :+1:\n\nAlso, it's a bit stupid that `java.net.InetSocketAddress` can both contain unresolved and resolved addresses...\n. Thanks, yes that's a stupid bug introduced with 76345ba. Thanks for the report.\n. Isn't that behavior as spec'd or can you show us otherwise.\n. We currently don't register custom status codes but just generate them from scratch once we observe them.\n\n> I'd vote for creating a separate subclass that is neither an HttpSuccess nor an HttpFailure. I should be able to specify at registration what values I want isSuccess and isFailure to return. \n\nSo, we wouldn't try to reuse status code subclasses for the range 100 to 599 or would we allow overrides of the standard isSuccess/isFailure behavior then?\n\n> allowsEntity should always be true for custom StatusCodes.\n\nHmm, without registering I guess we can't do better. But that also means that it isn't possible to read responses without an entity that are missing a `Content-Length` header. In fact, I already observed it in the tests. To make this work as well we'd have to require registering custom status codes before they can be parsed (which would be fine for me).\n\n> Since we define isSuccess and isFailure as methods on the StatusCode base class, maybe we should remove HttpSuccess and HttpFailure from the public API altogether?\n\nI wouldn't mind but I don't think it would solve the problem  as we would still have to supply `isSuccess` and `isFailure` when we observe a status code without it being registered.\n\nSo, I tend to requiring registration to make it work properly. I'll rework the PR.\n. Updated to fix the discussed issues.\n. LGTM\n. fixed\n. As you suggested the problem was indeed constrained to a particular setup of sizes, in this case related to an internal buffer being of size 1024 and the total compressed data including gzip headers and trailers being just above that size. So, thanks again for reporting so we got to know of this edge case.\n. Thanks for the report. Which version of spray is it? Is it a valid URI? I.e. is this about the exception being thrown or not catched properly?\n. If it is about spray not being able to parse the invalid URI it seems like this is a duplicate of #574.\n. Ah, I understand, sorry for not reading properly.\n. You are right, the fix was easy. We'll try to get it into tonight's build.\n. Thanks @ivantopo. :+1:\n. Updated with new `reference.conf` descriptions.\n. Updated to address the comments.\n. Depends on the upcoming PathMatcher change. Will update then.\n. updated to be based #605 (so merge this first)\n. I guess for the common usages, yes. Strictly speaking, since we are moving the await call somebody could already depend on it.\n. I added https://github.com/spray/spray/issues/599 to collect ideas about what to improve in the future.\n. fixed the merge conflict\n. This was fixed in akka-http.\n. AFAICS, this vast change LGTM. Hopefully, the SSL dragon is now slayed for good. Good job! \n. Client-side retry behavior will be addressed again for akka-http. See #16852.\n. Nice, the tests are now also much more succinct. :+1:\n. In ServerFrontend, we have some code executed only if the backpressure stage is active (search for 'autoBackPressure'). We should think about what to do about it.\n. Maybe you can add the commit you based this on into the commit message. Otherwise, LGTM.\n. Nice, lgtm.\n. How are relative links to be handled? \n\n[Section 5.1](http://tools.ietf.org/html/rfc5988#section-5.1) says:\n\n> If the URI-Reference is relative, parsers MUST resolve it as per [RFC3986], Section 5.  Note that any base IRI from the message's content is not applied.\n\nAnd in section 5.2:\n\n> By default, the context of a link conveyed in the Link header field is the IRI of the requested resource.\n> \n> When present, the anchor parameter overrides this with another URI, such as a fragment of this resource, or a third resource (i.e., when the anchor value is an absolute URI).\n. We discussed this and concluded that doing URI resolution needs to be done on another layer than on the header/parsing level.\n. We'll take this PR over from here and resolve the remaining issues.\n. The match always only matches on the first element of the rejection list while the flatMap in L66 goes over all.\n. Very cool. LGTM.\n. Cool, of course, I didn't even look into RequestContext to see which complete overloads were there additionally. This simplifies everything quite a bit more. :+1:\n. This is superseded by https://github.com/akka/akka/issues/16597. Closing here...\n. How do we prevent stuck half-closed connections? E.g. we send confirmed-close but the other side has gone away in this moment dropping all packets.\n. We'll handle stuck connections with this new ticket directly on the IO level: https://github.com/spray/spray/issues/619\n. Adapted to fix issues.\n. This won't be fixed in spray and currently has no counterpart in akka-http. Closing for now...\n. See https://www.assembla.com/spaces/akka/tickets/3680-io--race-condition-which-overrides-suspendreading\n. Superseded by https://github.com/akka/akka/issues/16552.\n. Already fixed since a while.\n. I remember to have this checked in a branch. Proper back-pressure support/API will come with akka-http.\n. Thanks, lgtm.\n. We have no Windows machines to test so we'd need contributions in this regard.\n. Very nice, in the end it was a pretty small change. :+1:\n. As commented in https://github.com/spray/spray/issues/479#issuecomment-73724381 a better solution is handling backpressure as done in akka-streams.\n. Alternatively, we could introduce optional bounds for the number of repetitions to accept to the `repeated` method.\n. Thanks for the report.\n\nI think you are right about the exception but fixing that won't stop another exception from being thrown out of the parser. If the charset requested isn't available there's no way to support a request. Which is the problematic charset?\n. Yes, I also think there's not much more we can do. Ignoring a charset that we don't know (and falling back on the default charset) is no option as this would mean that we could assume the wrong interpretation of data just because on the machine is a charset missing.\n. Thanks for the report. We'll adapt the model for `X-Forwarded-For` and `Remote-Address` to be sure not to trigger any name lookup.\n. lgtm\n. :+1:\n. Finished updating this. I explicitly added the remaining possible state transitions to make sure we didn't miss something.\n. Shouldn't this be a `!` commit because of the `path` change?\n\nlgtm, otherwise.\n. lgtm\n. Makes sense. :+1:\n. No, the fix is not even merged.\n. There is already a nightly out with the fix included: http://nightlies.spray.io/io/spray/spray-can/1.1-20131101/\n. As spray-can and spray-servlet should be as compatible as possible I'm considering this a bug.\n. Closing this a won't fix known-issue for now.\n. We won't change the spray-can protocol any more and this doesn't correspond to anything in akka-http. Closing...\n. [From the ML](https://groups.google.com/d/msg/spray-user/bEEEXVtoShU/zZl7KXy2EegJ):\n\n> On Thu, Oct 31, 2013 at 3:03 PM, Jim Fulton wrote:\n> This is better. It at least forces people to think harder.  It's confusing\n> though IMO.\n> Here's an alternative, FWIW:\n> \n> ```\n>    ``def apply(key: Any)(expr: => V): Future[V]`` wraps an \"expensive\"\n>   expression with caching support.  If the cache already has a future for the\n>   key, the existing future is returned. If the cache doesn't have an entry for\n>   the key, ``expr`` is evaluated synchronously and the result wrapped\n>   in a future which is added to the cache and returned.\n> ```\n> \n> But really, I think it would be much better to change the semantics of the first case to run the given function in a future. I'd be surprised if anyone objected to this on practical grounds.\n. Can we get this merged and uploaded to the site?\n. Specifying a maximum number is already implemented in akka-http. For also specififying a minimum number, I created akka/akka#16833 which supersedes this ticket.\n. Thanks for the report. However, we'll need more information to be able to\ninvestigate it properly. Can you do the following:\n- try with 1.1-RC2\n- produce a stand-alone project we can use to reproduce the issue\n- upload a memory dump of your project (maybe not with 4G heap size,\n  100-500M should be enough)\n. Once the leak occurred it probably won't be too hard to find out what it is if you take a memory dump (you can do this live with `jmap`) and analyse it (e.g. Eclipse MAT). If you put it somewhere I can look into it. That said, from what you report it doesn't look like a spray issue.\n. > So, rather than requiring people on Windows to work with unix-style working copies I'd say that we should fix all tests that do not yet properly deal with windows style line endings, no?\n\nI agree with that. What are the occurrences where line endings make problems? In many cases it should be triple-quoted strings, right?\n. AFAIU this is fixed with the new marshaller API in akka-http.\n. lgtm\n. \"Unexpected slot state: Idle\" was filed as #652 and was already fixed after RC2\n. :+1:\n. lgtm\n. Superseded by akka/akka#16859.\n. lgtm\n. The other two commits :+1: (apart from the remaining ClientFrontend timeout issue)\n. lgtm\n. :+1:\n. apart from the smallish things :+1:\n. It's related to the `spray.can.server.max-encryption-chunk-size` setting. My guess is that `SslTlsSupport` sends acks too eagerly when only the first part (as governed by `max-encryption-chunk-size`) of a `Write` was sent successfully.\n. @mpilquist were you able to test this fix with a recent nightly?\n. Ok, will do.\n. updated\n. Also make sure the fix is backported to akka.\n. Akka ticket: https://www.assembla.com/spaces/akka/tickets/3728\n. Yeah, of course, I keep forgetting that.\n. Exactly. Thanks, @wongelz. Closing...\n. @matanster, thanks for the suggestion. I'm already trying it another way: we include code snippets using a [custom sphinx extension](https://github.com/spray/spray/blob/master/docs/_sphinx/exts/includecode.py) we copied from akka. We'll try to add the logic there.\n. @matanster the way I tried by getting sphinx to create the links was a dead end. I now implemented it exactly like you proposed in javascript directly on the browser with a directive mapping generated during the build. See #703 and its upcoming PR.\n. Superseded by akka/akka#16860.\n. You can use the `spray.servlet.root-path` setting to make the prefix disappear before it gets to your routes.\n\nWrt what to expect: that depends on the viewpoint. If you view spray as just another framework that works inside an application container you may wonder what's going on. We tend to see spray as a set of libraries for developing REST services. As such we see spray-servlet as a possibility to run spray in a servlet container more than as a possibility to write your servlet with spray.\n\nThat said, we could probably show that fact more prominently in the documentation and maybe add some helper functions which would automatically set `root-path` to the context path as reported by the `ServletContext`. As this isn't the focus of our current development we have to rely on PRs for this feature.\n. Thanks for the report. The problem is that the backslash character is not allowed in cookie values by the spec ([RFC 6265](http://www.rfc-editor.org/rfc/rfc6265.txt)). There's a pretty [good answer in SO](http://stackoverflow.com/questions/6108207/definite-guide-to-valid-cookie-values) about that topic. \n\nThere may be ways to be more lenient about it (and other software can be configured that way), however, as long as any custom behavior is not properly specified you always have the risk of choosing the wrong way of parsing things when you encounter a parsing ambiguity caused by the under-specification of such a feature.\n. Nice, I like how little had to be changed to get there. :+1:\n. Superseded by akka/akka#16832\n. Superseded by akka/akka#16831\n. Superseded by akka/akka#16830.\n. Added CachingDirectives documentation.\n. Added documentation for chunking + cookie directives\n. I extracted the routing changes into a separate commit to make reviewing easier, see #714. This PR is then based upon those changes.\n. lgtm, I'll merge and rebase my PR on top of it.\n. LGTM\n. This still needs to be done for `encodeResponse`.\n. This was just not yet deployed. Closing...\n. We currently won't fix this in spray. This is \"fixed\" in akka-http by not relying on actors anymore and not giving so strict guarantees about what is executed where.\n. We will try to address this in akka-streams/http. Superseded by akka/akka#16829.\n. :+1:\n. lgtm\n. Added search box. Needs to get styled properly.\n. Interesting, maybe it also needs to be properly specified if and what parts of error responses are expected to be localized.\n. LGTM. Thanks, Andr\u00e9.\n. lgtm, otherwise.\n. We see this currently as a one-off bug with the XML API. While consistent error message localization is nice we will be revisiting issues as they crop up.\n. Thanks @2beaucoup for looking into it. \n\nFTR: IMO the problem is several somewhat unrelated (but platform-correlated) things play together in a way that everything seems to work as long as your doing everything on one machine but breaks down easily when compilation and runtime are separated. So, any solution will still be brittle as long as it still contains some implicit assumptions.\n\nThese things are:\n- the user configuring git to fix/change line endings of checked-out files for compiling\n- the value of EOL being set correctly at runtime\n- the supposed meaning of literal newline characters in multiline strings\n\nFor me the consequences are:\n- you need to be explicit about what literal newline characters are supposed to mean in each single multiline string\n- you need to keep in mind that code is not necessarily compiled on the same computer as it is run, and that strings that were compiled on one machine, processed while the program is running on another machine, may even be interpreted on a third machine\n\nI think your solution works for the tests because you made the assumption that tests are always run on the same machine as they were compiled (and I'd agree with this assumption). Otherwise, the runtime value of EOL isn't necessarily the same as the one multiline strings were originally compiled with. So, I'm fine with your solutions for the tests. \n\nFor the `FileAndResourceDirectives` I think the right solution would be to make sure that the value of the multiline string always has the same meaning regardless of where it was compiled and where it will run later. In lack of better solutions maybe just use something like `stripMargin.replaceAll(\"\\r\\n|\\n\", \"\\n\")`.\n. Maybe we can just add a new `stripMarginWithNewline(newline: String)` to `spray.util.pimps.PimpedString` which does `string.stripMargin.replace(\"\\r\\n\", \"\\n\").replace(\"\\n\", newline)` and then use `.stripMarginWithNewline(\"\\n\")` or `.stripMarginWithNewline(\"\\r\\n\")` for all multiline strings in the tests.\n. :+1:, thanks @2beaucoup \n. Superseded by https://github.com/akka/akka/issues/15833\n. Following [the spec](http://tools.ietf.org/html/draft-ietf-httpbis-p1-messaging-25#section-3.2.6) it **is** illegal. The only thing we could do is trying to be more lenient for informal headers like this for convenience.\n. Yep, from looking into the source code it was changed by now (see `src/init.c` and look for `uagent`). It seems you can overwrite the user-agent using `-A` or `--user-agent`, have you tried that?\n. I see two issues with this proposal:\n- we need either a new entry-point for the HttpParser that can return several values or special handling for cookie values somewhere else\n- being lenient may lead to ambiguities like in #702 where we need to employ a non-standard reading of backquoted doublequotes to be able to finish parsing\n. Superseded by akka/akka#16828\n. This change seems to expose cookies (and other headers) which are not meant for the new host to the new host. Cookie always comes with security implications (see also previous https://github.com/spray/spray/pull/311). \n\nThe first thing we would need to push this further is some investigation how things should work by spec and how browsers and other notable clients are implementing redirection exactly. Once we are sufficiently sure to understand what the current consensus is how things should work and what the security implications are we can start implementing them.\n\nFor the time being I'd suggest to disable automatic redirection and implementing your own redirection logic as ugly as this workaround may be, unfortunately.\n. Maybe we should take this PR as is as the binary compatible fix to the (undisputed) broken rendering in the current version and then create another ticket to improve the model for the next version. WDYT?\n. I added #753 to remind us of improving the model in the future.\n. Thanks, @2beaucoup for the fix.\n. See [section 5.2.3 of RFC3986](http://tools.ietf.org/html/rfc3986#section-5.2.3)\n. LGTM. Flavio, can you change the commit message to \"= can: avoid parsing attempt after complete input has been consumed\" and sign the [Typesafe CLA](http://typesafe.com/contribute/cla).\n. Hopefully fixed by cd1cfb8. Once the 1.2.1 RC is out, can someone please check, that it is indeed fixed? Thanks.\n. HttpDialog is legacy API that we won't fix for now.\n. This won't be changed in spray but is superseded by akka/akka#16827.\n. Thanks, @ivantopo for the friendly reminder. We're still alive and kicking :) A 1.x.1 release is planned but I cannot make promises yet when we will actually get to it. In any case it's good to know that you have tested this PR and find it useful.\n. We decided that we'd like to keep the maintenance release as binary compatible as possible so this PR won't make the cut. \nThere's nothing wrong with the PR itself so we'll probably merge it sooner or later into the cutting edge branch for collecting things that we need to consider during the move to akka-http. Hopefully that's ok for you, @fernandezpablo85.\n. Makes sense. Thanks, @2beaucoup. :+1:\n. Thanks!\n. Superseded by akka/akka#16826\n. Thanks, @bthuillier. I don't think we want to change `host` and `scheme` directives the way you've done here. Currently, most directives either filter (in the meaning of `hrequire`) or extract value and for most people this makes the most sense. Doing both at the same time may seem elegant from an implementation point of view but makes the usage a bit more awkward for no good reason. So, I think we'll go with your first commit (+ added tests). Could you sign the CLA, as well? Thanks! http://typesafe.com/contribute/cla\n. We won't change the behavior in spray any more but I created akka/akka#16825 to review the situation with akka-streams/akka-http.\n. Superseded by akka/akka#16824\n. Discussing this a bit more we found that one problem of this directive is that it may be too easy to mess up security: what to do with cookies, authentication data, etc.? Should spray really just relay all of it to another URI without any pre-processing?\n\nThe directive is simple enough so if plain relaying is what you want, you can add it to your app in just a few lines of code. So, we are currently a bit reluctant to risk adding a potential security problem to the spray codebase without completely understanding the consequences.\n. The new ticket to target is akka/akka#16844.\n. Thanks, Mathias. LGTM. Can you add the same change to the spray-routing/on-jetty example?\n. Thanks Mathias, I took the freedom to squash both of your commits into one and merge that one. See 3a4c0b20681652afa85a5ed4fdbe33a9b9b9ab95\n. Closed by 3a4c0b20681652afa85a5ed4fdbe33a9b9b9ab95.\n. Thanks @guersam. Sorry, I overlooked your PR and made the same change anew. Your change regarding `sender` is indeed the right way to go, so we'll just cherry-pick that one in addition to ours that updated the Akka dependency.\n. This is already referenced in the overarching akka-http host-level client-side API ticket akka/akka#15681. Closing here.\n. See https://groups.google.com/d/msg/spray-user/YN2ocRzwhY0/KJOegaDIep8J\n. The integration is now tracked for akka-http in akka/akka#16856.\n. @okapies this seems to work as Mathias said: an illegal `Origin` header will be parsed as a `RawHeader`. Can you give us more information how this \"crashes spray's pipeline\"?\n. Closing for now. Please reopen, if this is still an issue and you can provide more information about your use case.\n. Thanks for the clarification. You can turn off those warnings by configuring `spray.can.server.parsing.illegal-header-warnings = off`.\n. Thanks, Age. Not sure when we'll be able to actually look into it but it seems like a very useful addition. Btw. currently the tests are failing with this mysterious message:\n\n```\n[info] x If-Modified-Since\n[error]    'Right(If-Modified-Since: Wed, 13 Jul 2011 08:12:31 GMT): scala.util.Right'\n[error]     is not equal to \n[error]    'Right(If-Modified-Since: Wed, 13 Jul 2011 08:12:31 GMT): scala.util.Right' (HttpHeaderSpec.scala:364)\n```\n. Maybe the milliseconds missing in the string representation differ?\n. Closing as superseded by #797\n. > ``` scala\n> \"If-Modified-Since\" in {\n>   \"If-Modified-Since: Wed, 13 Jul 2011 08:12:31 GMT\" =!= `If-Modified-Since`(DateTime(2011, 7, 13, 8, 12, 31))\n> }\n> \n> \"Last-Modified\" in {\n>   \"Last-Modified: Wed, 13 Jul 2011 08:12:31 GMT\" =!= `Last-Modified`(DateTime(2011, 7, 13, 8, 12, 31))\n> }\n> ```\n> \n> So I'm not sure what could have gone wrong there... any ideas?\n\nWhat was wrong here in the end?\n. Right, that's a likely cause.\n. Then this is a documentation problem which contains the line\n\n> spray-http is stand-alone in that it has no dependency on any other spray module and not even Akka. You can therefore use it directly and independently for any other type of HTTP work you might be doing.\n\nThis particular error is created by the Uri depending on the `spray.util` package object which triggers the load of `akka.actor.ActorRefFactory`.\n. Have you checked that it works, @TulioDomingos ? Otherwise, it looks simple enough to me. :+1: \n. Excellent. Thanks again.\n. Thanks @kulikov, good catch. Could you add a test checking the behavior? We'd also need you to sign the [Typesafe CLA](typesafe.com/contribute/cla). And could you please also change the commit message to `= routing: handle exception in onComplete directive` according to [commit message policy](http://spray.io/project-info/contributing/#git-commit-messages). Thanks!\n. Also, we would need this patch to be based on the master branch. Can you rebase?\n. :+1: very nice result and outstanding cooperation. Thanks @agemooij, @2beaucoup and @sirthias.\n\nMissing points: \n- tests as hinted by @sithias last commit message\n- maybe try/finally in FileAndResourceDirective\n- documentation\n. Yay!\n. Reproducible with the 20MB data instead of 500k.\n. Updated.\n. :+1:\n. IIUC this feature doesn't work together with separate compilation. So, I don't think it is something we can fix. If you have other info, please reopen.\n. The question is if you can still use it together with other software not built with the same flag.\n. Here's a branch that shows what can happen if you use the flag without using it in all projects:\n\nhttps://github.com/jrudolph/spray/compare/w;max-classfile-name-breakage?expand=1\n\nRun `test:run` in project `spray-routing-tests`.\n. Btw. it might be that adding the flag doesn't break something _right now_ but that's just coincidental because the flag is more likely to apply to classes with long names which in many cases are anonymous, implementation-private classes that won't be referenced from third parties. However, as the example shows, it could happen to public identifiers as well which could lead to silent breakage.\n. Superseded by akka/akka#16823\n. I guess the problem is that doing the update on the master branch is harder to do. The question is if the migration effort to update to sbt 0.13 required is currently worthwhile the outcome: the spray codebase is currently mostly in maintenance mode with the active development currently being switched over to the akka codebase. Any change to that \"running system\" always comes with the risk of disrupting our well-understood and working deployment process.\nSo, closing for now...\n. I created #822 to further improve the situation once this has been improved in Akka.\n. Would it still be possible to distinguish parameter validation problems and entity content validation problems? In the case you show it seems clear enough why you'd the behavior to be consistent in other cases it may be less clear cut.\n. Ah, I understand. I agree that none is more special than the other. Maybe it's more that `ValidationRejection` is _too_ generic. In the best case, the rejection should contain as much information as possible. And that should be both a) which kind of validation failed b) which data of the HTTP request was flawed. I'm fine with everything that retains both parts of failure information.\n. > Care to elaborate on why/when/how you'd want to access these additional infos?\n\nBecause someone may want to handle validation errors differently if they occur in the entity body than when they happen during parameter validation. I'm not sure but I think it could be useful.\n. Superseded by akka/akka#15792. Does this cover everything, @2beaucoup ?\n. Oops, seemed I referenced the wrong issue in that commit.\n. This has no tests because I've not managed to reproduce it in a controlled environment. I think it must happen with some combination of connection closure during the handshake. We are missing a bit of test-infrastructure that would allow us to drive a standalone SslTlsSupport stage into exactly the problematic state (even harder if you don't know which it is exactly...).\n. You need to send one write to the SSL stage and then wait for its Ack to send the next. There is no way around this. Tests may run through in some cases (like on localhost) but that doesn't mean that it will work in general (it doesn't).\n. It will not work depending how the race conditions work out. The test framework isn't sophisticated enough to capture those kinds of race conditions reliably. \n\nIn general, if you need a reliable stream (both for akka.io TCP connections or the same when using the SslTlsSupport stage) you mustn't use any Writes without acks and you must never send a subsequent Write without having received the previous ack. That's only way to be sure that it will work under any kind of backpressure. If you need buffering support you need to build it on top of that.\n. I think, as @sirthias said above, it is already possible and very simple to build a directive that works on a `Future[Boolean]` so I'm closing this for now.\n. Thanks, @hierynomus for reporting.\n\n> However spray-client will not correctly receive the response and obtain the status code.\n\nWhat will it do instead? And what do you think should be the expected behavior?\n. We will add this in akka-http. Superseded by akka/akka#16822.\n. Thanks, @gkossakowski. One problem I see with merging this right away is that there's nothing _we_ can do with these changes. We cannot build for 2.11 even with these as long as our dependencies have not yet been upgraded. \n\nEven when they come available we may want to setup a new branch for 2.11 support because most dependencies won't be cross-published in exactly that version that we need and we may need other code changes as well. \n\nAnother concern is that we don't want to have the build definitions drift to much between our release branches and the master branch because that would increase the burden of releasing even further.\n\nSo, I'm a bit reluctant to merge this into our stable release branch. What we should do is starting a new branch for 2.11 support starting off from `release/1.3` which has a clear experimental marker and which you can use for the community build.\n. I've created the `unstable/1.3-scala-2.11` from the `release/1.3` branch. \n\nCan you\n- update the PR to target this new branch\n- rewrite the commit message according to [our policy](http://spray.io/project-info/contributing/#git-commit-messages) to be prefixed with `= build:`\n- also add the changes that are created by updating the scalariform plugin (which also seem to include a regression wrt to pattern match formatting)\n\nThanks!\n. LGTM.\n. Yep, thanks, I guess that would make sense.\n. Thanks, @topping for explaining the use case. \n\nEven if the change is a small one it is still a hack. It cannot be anything else as long as there's no \"official\" support for pluggable protocols to use with upgrade which must be handled internally in spray-can. \n\nThat said, we may still include a patch like this to reenable spray-websockets. I think what we would need is\n- some evidence that this cannot be fixed inside spray-websockets (e.g. by including patched renderer code into the spray-websockets codebase for the time being) with reasonable effort (the effort required to do a spray release is not negligible and likely not a short-term solution)\n- some more tests that this works (or doesn't break anything) also for other combinations of HTTP protocol, client and server provided Connection values, and multiple Connection values\n- comments in the code why we need the line in question\n\nWDYT @sirthis?\n. Thanks @dcaoyuan, I didn't even realized _which_ one of the spray-websocket projects this is about :) \n\nI wonder, why do you need those changes specifically? It seems you could implement `hasUpgrade` wherever you need it, right?\n\nThe point is that we don't want to (or better: may get to) make a release *) in the next months if we can avoid it, so it would be great if we there'd be a workaround for now that doesn't need any spray changes. \n\nApart from that we can, of course, discuss any PRs for the next version solving this \"properly\" so that you can get rid of any needed workarounds sooner or later.\n\n*) In spray \"a release\" still means making all of those 5 releases: 1.0.x, 1.1.x, 1.2.x, 1.3.x, 1.3.x for Scala 2.11, which always need some documentation updates etc. and all-in-all is always quite a big undertaking.\n. It's here: http://repo.spray.io/io/spray/spray-io_2.11/1.3.2-20140909/\n. Compilation seems to fail when the PR is run using Scala 2.10.3. Is this expected? Is there any reason those compilation errors should go away with Scala 2.11.x?\n\n```\n[error] /home/travis/build/spray/spray/spray-routing-tests/src/test/scala/spray/routing/FormFieldDirectivesSpec.scala:48: ambiguous implicit values:\n[error]  both method forTuple in object FieldDefMagnet2 of type [T <: Product, L <: shapeless.HList, Out0](implicit hla: shapeless.Generic.Aux[T,L], implicit fdma: spray.routing.directives.FieldDefMagnet2.FieldDefMagnetAux[L,Out0])spray.routing.directives.FieldDefMagnet2[T]{type Out = Out0}\n[error]  and method forHList in object FieldDefMagnet2 of type [L <: shapeless.HList](implicit f: shapeless.ops.hlist.LeftFolder[L,spray.routing.Directive0,spray.routing.directives.FieldDefMagnet2.MapReduce.type])spray.routing.directives.FieldDefMagnet2[L]{type Out = f.Out}\n[error]  match expected type spray.routing.directives.FieldDefMagnet2[shapeless.::[Symbol,shapeless.::[spray.routing.directives.NameReceptacle[Int],shapeless.::[spray.routing.directives.NameReceptacle[Option[String]],shapeless.::[spray.routing.directives.NameDefaultReceptacle[Boolean],shapeless.HNil]]]]]\n[error]         formFields('firstName :: (\"age\".as[Int]) :: ('sex?) :: (\"VIP\" ? false) :: HNil) { (firstName, age, sex, vip) \u21d2\n[error]                               ^\n[error] /home/travis/build/spray/spray/spray-routing-tests/src/test/scala/spray/routing/FormFieldDirectivesSpec.scala:148: diverging implicit expansion for type spray.routing.directives.FieldDefMagnet2[spray.routing.directives.RequiredValueDeserializerReceptacle[Int]]\n[error] starting with method forTuple in object FieldDefMagnet2\n[error]         formFields('oldAge.as(Deserializer.HexInt) ! 78) { completeOk }\n[error]                                                    ^\n[...] many more, see the Travis build\n```\n. Nice :+1:. Thanks @2beaucoup \n. Superseded by akka/akka#16821\n. I cannot say how relevant this is. If there's a strong demand to support the proxy protocol please reopen this ticket for akka-http.\n. Cool, glad you figured it out.\n. Superseded by akka/akka#16820\n. Btw. as a workaround it should be possible to make a local copy of the relevant parts of `FormDataUnmarshaller` with the problem fixed and bring that implicit into scope where needed.\n. > I need to be able to parse and forward any legitimate HTTP traffic.\n\nWouldn't this work in any case because a \"broken\" header will still be passed through as a `RawHeader`?\n. > being a proxy, I want to apply the Cache-Control semantics, so I need it to be parsed.\n\nYes, that makes sense, of course :)\n. I checked it, this one fails:\n\n``` scala\n      \"Cache-Control: private, no-cache, no-cache=Set-Cookie, proxy-revalidate\" =!=\n        `Cache-Control`(`private`(), `no-cache`, `no-cache`(\"Set-Cookie\"), `proxy-revalidate`)\n```\n\nI guess it's because we require quoting while the grammar from httpbis allows a plain `token` as well:\n\n```\n     Cache-Control   = 1#cache-directive\n\n     cache-directive = token [ \"=\" ( token / quoted-string ) ]\n```\n. Also, there's another bug in our parser which parses\n\n```\n\"no-cache\" [ \"=\" 1#quotedString ]\n```\n\ninstead of\n\n```\n\"no-cache\" [ \"=\" <\"> 1#field-name <\"> ]\n```\n\ni.e. it would allow multiple quoted field-names instead of allowing multiple field names **inside of** quotes which introduces an ambiguity in the parser.\n. Yep, done.\n. Thanks, that's a good point. Does it fail in an OSGI container with the dependencies declared as required if the actual jars are missing? (Just estimating the severity of this problem.) \n\n@mpilquist is there any reason we didn't add those before?\n. I had a try at fixing those issues and managed to produce a version that compiles and tests run fine. This is no promise that we'll actually publish a shapeless2 version of spray-routing but you can at least try it yourself. See https://github.com/spray/spray/tree/release/1.3-shapeless2\n. It works for me. Can you post code that observes this fact? The `- 1` should strip off the trailing `$` character of the singleton object's class name (which is an scala compiler implementation artifact).\n\nSee:\n\n``` scala\nscala> HttpHeaders.`Content-Length`.getClass.getName\nres0: String = spray.http.HttpHeaders$Content$minusLength$\n\nscala> HttpHeaders.`Content-Length`.name\nres1: String = Content-Length\n```\n. Closing... couldn't be reproduced.\n. Thanks for reporting. It seems RFC 2965 was obsoleted by [RFC 6265](http://tools.ietf.org/html/rfc6265) which doesn't allow `\",\"` any more. It may still make sense to look at the grammar of the older version and be a bit more lenient about what to accept if it is possible to do so unambiguously.\n. I would consider this now a bug as it has introduced an alternative interpretation of Cookie headers that is not consistent with what browsers implement.\n. I think we need a specification reference to warrant this change. The only thing I found on a quick glance was this line from [rfc822](http://tools.ietf.org/html/rfc822#section-4.1):\n\n```\n     message     =  fields *( CRLF *text )       ; Everything after\n                                                 ;  first null line\n                                                 ;  is message body\n```\n\nwhich would mean that no extra CRLF is necessary if the `text` is empty.\n\nAlso the current spec ([rfc2046](https://tools.ietf.org/html/rfc2046#section-5.1.1)) for multipart messages has this in its grammar:\n\n```\nbody-part := MIME-part-headers [CRLF *OCTET]\n```\n\nWe should probably make that `CRLF` optional for parts with an empty entity.\n. That said, we should make sure that all round-trips using valid grammars work properly. So, thanks for the test cases. That's helpful.\n. The grammar you posted seems to be on one level up: it's the thing that delimits several body-parts into one multipart message.\n. Will be fixed if we get to fix https://github.com/spray/spray/issues/971\n. Thanks, @RichardBradley, for the report. I agree this could be improved.\n. This could probably now quite easily be fixed by using the new `DeadLetterSuppression` trait introduced with Akka 2.3.8 for the messages in question. See https://github.com/akka/akka/issues/15163.\n. Thanks @RichardBradley, good point. I agree we should properly validate user input and report problems using the right channels where possible. Another point we should improve is to disallow URLs like the one you gave by only allowing http/https URIs for the normal constructor and require an alternative constructor for other/custom schemes.\n. Yes, you are right, only (1) was fixed. (2) is somewhat harder to achieve because what fails here is the `HostConnector` which is a standalone entity that cannot be easily associated with a single request. I think the policy should be to properly guard against user errors and report them back and rely on other mechanisms (timeouts, supervised restarting, logging) to handle other issues.\n\nEspecially, in cases like this, an temporary `HostConnector` outage isn't really much different from any kind of temporary network outage which means that user code always needs to deal with these kinds of situations anyways.\n. I'd say we do the quick and simple option (3).\n. I think this PR needs a bit more explanation about what it is supposed to achieve :) We currently build the Scala 2.10 and the Scala 2.11 version from different branches. Are you suggesting to build both versions from this single branch?\n. We know about this problem and also the more general problem of hard to avoid `DeadLetter`s when a  network of inter-depending actors is terminating. We have not yet found the general solution to that problem.\n\nThis particular problem could probably be solved by `unwatching` the handler as soon as the connection knows that it will be able to cleanup itself without any further input from the handler.\n\nThe related problem of avoiding `DeadLetter` reports is also discussed here:\n\nhttps://github.com/akka/akka/issues/15163\n. We don't have a solution to this problem but it is a valid problem so I keep it open for spray.\n. Which parts of spray would you like to use with scala-js? spray isn't likely changing enough to be running on scala-js. And I guess it will be even harder to get akka-http working on scala-js. So, I'm closing this as won't fix for now.\n. Thanks @ryryguy. Interesting idea. \n\nThinking about it, the problem is that the scoping will not work properly: The request timeout will always be applied even if the inner route didn't match. That's not what users would expect.\n. Superseded by akka/akka#16819\n. Superseded by akka/akka#15799.\n. Hi Jisoo,\n\nwe mostly worked from the httpbis draft since quite a while. That means\nthat we may have incorporated already lots of the now finalized changes. We\nwill not update spray to incorporate _all_ of the changes that are still\nmissing just for the sake of it. If something concrete needs to be changed\nwe will decide on a case-by-case fashion as usual.\n\nJohannes\n\nOn Thu, Jun 26, 2014 at 11:23 AM, Jisoo Park notifications@github.com\nwrote:\n\n> I've wrote a document aggregating the changes after the death of RFC2616:\n> https://gist.github.com/guersam/2a3d61d6429f62a43ee6\n> \n> There's no doubt akka-http will supprt new RFCs, but is there migration\n> plan for Spray as well?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/spray/spray/issues/895.\n\n## \n\nJohannes\n\n---\n\nJohannes Rudolph\nhttp://virtual-void.net\n. Closing... @dckc if you have more information or anything we can do about it please reopen.\n. Fixed by 6a78ade\n. Superseded by akka/akka#16818\n. Thanks @dvorobiov for the report. Can you post the wrongly generated JSON value as well?\n. Where did you post it? It seems it got lost somewhere :)\n. I think the problems happens on another level. Can you show the code where you actually build the request?\n. I think this is a case of the greedy lift json marshaller allowing code that shouldn't have compiled in the first place.\n. Thanks, @dlouwers, good point. It seems the example is wrong.\n. Btw. you don't need `respondWithMediaType` as the `FileAndResourceDirectives` will usually figure out the correct type from the file name extension.\n. We should probably deprecate the jsonp directive and add a warning about the security implications of using it.\n. Thanks, @sksamuel. \n\nMaybe it was just a typo, \"1968\" vs. \"1986\"? Do you have any specification source for this change?\n\nBtw. would you please also change the commit message itself to `= http: Added missing alias for US-ASCII`. We'd also need you to sign the [latest version of the Typesafe CLA](http://www.typesafe.com/contribute/cla).\n. Thanks, I see, that's what I also just found. Could you add a comment line above with the IANA link so we don't have to search it the next time.\n. Here's the test a bit adapted from the original post:\n\n``` scala\npackage spray.httpx.encoding\n\nimport org.specs2.mutable.Specification\n\nimport scala.util.Random\n\nclass CompressionTest extends Specification {\n\n  val random = new scala.util.Random\n\n  // Generate a random string of length n from the given alphabet\n  def randomString(alphabet: String)(n: Int): String =\n    Stream.continually(random.nextInt(alphabet.size)).map(alphabet).take(n).mkString\n\n  // Generate a random alphabnumeric string of length n\n  def randomAlphanumericString(n: Int) =\n    randomString(\"abcdefghijklmnopqrstuvwxyz0123456789\")(n)\n\n  \"Compression\" should {\n\n    val compressor = new DeflateCompressor()\n    val decompressor = new DeflateDecompressor()\n\n    \"Work\" in {\n\n      var totalBytesCompressed = 0\n      var totalCompressedBytes = 0\n      (1 to 1000000).foreach { i \u21d2\n        val inputString = randomAlphanumericString(64)\n        val input = inputString.getBytes(\"UTF-8\")\n        val output = compressor.compress(input).flush()\n        val result = decompressor.decompress(output)\n        val outputString = new String(result, \"UTF-8\")\n        totalBytesCompressed += input.size\n        totalCompressedBytes += output.size\n\n        if (inputString != outputString) {\n          println(s\"      INPUT $inputString\")\n          println(s\"     OUTPUT $outputString\")\n          println(s\" INPUT SIZE ${inputString.size}\")\n          println(s\"OUTPUT SIZE ${outputString.size}\")\n          println(s\"Output is prefix ${inputString.startsWith(outputString)}\")\n          println(s\"At $i total input bytes $totalBytesCompressed total compressed $totalCompressedBytes\")\n          throw new Exception(\"That hurts\")\n        }\n\n        inputString === outputString\n      }\n\n      true === true\n    }\n  }\n}\n```\n. I tested a bit more and I couldn't find a case where the current scheme would produce a corrupt stream. What does happen is that our current `flush` isn't always able to put the SYNC at exactly the position indicated so that it looks as if the stream was truncated by one (or a few) bytes. However, those bytes aren't lost and will be decompressed later on. So, this never produces a broken stream for me when the complete stream was properly compressed and `Compressor.finish` was called at the end.\n\nIf you have observed corrupted data on a higher level can you post some higher-level test code that demonstrates it?\n. Thanks, @maspwr. Could you please sign the [Typesafe CLA](http://www.typesafe.com/contribute/cla)?\n. Thanks, @maspwr !\n. Thanks! :+1:\n. Thanks, @maspwr !\n. Superseded by akka/akka#16817.\n. I agree it could be useful. One problem is that the RouteResult is basically is a future on an eventual result (either normal response, chunked response, or rejected) so you would have to wait for both results to be available before being able to compare them.\n\nAs this would need binary incompatible changes I don't think we will introduce this for spray anymore. A workaround that works now would be to implement a matcher for your test framework that does the comparison.\n. It's because it constantly trips people up who try to C&P the method which contains the line and then use it several times. As this line will globally close all connections on the ActorSystem all kinds of strange things happen. It should rarely be necessary to use it at all.\n. Thanks, @benmccann, that's a good point. \n\nCurrently, most artifacts are already also on maven central so the short-term workaround would be to rely on maven central wherever possible and remove documentation pointing to repo.spray.io.\n\nWe also need to think about whether for how long we really want to support our own repo at all or if we want to publish to oss.sonatype / maven central directly which should have the more professional infrastructure to host artifacts.\n. Created issue in akka-http as akka/akka#16803\n. I'd like to know if the concern I voiced in #737 about \n\n> being lenient may lead to ambiguities like in #702 where we need to employ a non-standard reading of backquoted doublequotes to be able to finish parsing\n\nis tested and addressed in this PR.\n\nI think we also need more test coverage, discussion, and documentation what exactly to expect in the erroneous or ambiguous cases like the one reported in #869 and #702.\n. @caspark thanks for giving your opinions and your contribution. Adding no _further_ ambiguity is certainly good. :)\n\nI agree that we need to make a decision about how to proceed. I think we need to collect all cases that we have seen up to now and need to make sure we find a solution we can keep and improve on for a while.\n. Thanks, @josephw. This seems reasonable. Could you please:\n1. point us to the W3C recommendation and also add the link to the commit\n2. sign the [Typesafe CLA](www.typesafe.com/contribute/cla)\n3. Squash your changes into one commit (they both deal with the same issue, right?)\n4. Adapt the commit line according to [our policy](http://spray.io/project-info/contributing/#git-commit-messages) and add the link to the W3C recommendation.\n\nThanks!\n. Try this:\n\n``` scala\nget{\n      pathEndOrSingleSlash { // instead of path(\"/\")\n        complete(\"done\")\n    } ~ post{\n      path(\"auth\"){ // no slash here\n        redirect(\"/\", StatusCodes.Found) // TemporaryRedirect / 307 won't redirect to GET but to POST on the browser\n      }\n    }\n```\n\nThe important thing to know is that the `path` directives will automatically match a leading slash.\n\nPlease see the documentation about the [path directive](http://spray.io/documentation/1.2.1/spray-routing/path-directives/path/#path) for more information and use stackoverflow for these kind of questions (or the mailing list for the more difficult problems). Thanks!\n. The LdapAuthenticator is basically unmaintained. If someone cares for it we would probably have to resurrect it as a contrib module. For now, it's more likely that we'll get rid of it than to make changes to it. Closing...\n. @maspwr thanks for volunteering. In case it wasn't clear: I wasn't implying that Ldap support isn't useful or that it is broken. It's just that with most activity now gradually moving over to akka-http we are in the process of clarifying the boundaries of what parts of the existing spray codebase may be most worthwhile to be maintained in the future.\n\nFor us, the easiest solution would be if the Ldap support would live and be maintained in its own repository as a standalone module depending on spray. Would that be an option for you?\n. Yes, sounds good.\n. The reason it is not explained is that - in theory - this has almost nothing to do with spray and Java SSL configuration knowledge is needed to do this properly. That said it would still make sense to provide some pointers in the documentation about how to do this.\n. Let's start with the easy one:\n\n> What is the spray behavior and what triggers it?\n\nSpray won't be able to decode partial gzip headers, so if you happen to receive just the first few bytes (less than 10, that seems to be a common size of a gzip header) of a gzip encoded entity, spray will bail out.\n\n> What does \"failing in caves\" fatally mean?\n\nNever thought about running a spray server inside a cave :skull: and linking it to the outside world using something like the [Cave-Link](http://www.cavelink.com/cl3x/index.php/en/) ? With an expected transmission rate of 104 baud, the aforementioned situation seems much more likely then in less restricted environments... :ghost:\n\n\"Fatally\" is an exaggeration I guess, because only the current connection is affected.\n. Relevant part of the spec: http://tools.ietf.org/html/rfc7232#section-2.4\n. It says:\n\n> In other words, the preferred behavior for an origin server is to send both a strong entity-tag and a Last-Modified value in successful responses to a retrieval request.\n\nSo, indeed, even if not recommended it should be possible to specify only one those parameters. We will improve the situation in akka-http.\n\nThanks for raising this issue.\n. Superseded by akka/akka#16816\n. Superseded by akka/akka#16815\n. I think this is invalid. In spray, an `AuthenticationFailedRejection` comes with very specific semantics to trigger a particular rejection handling behavior. Introducing more causes here would mean, that any rejection handler would need to support custom causes. Instead, I would propose that if you need to carry over additional data to a custom rejection handler you can also provide a custom rejection that does exactly this.\n. Fixed by 1539e49.\n. @analytically as a workaround, can you try if adding the testkit dependency as \n\n``` scala\n\"io.spray\" %% \"spray-testkit\" % \"1.3.2\" % \"test\" exclude(\"org.specs2\", \"specs2_2.11\")\n```\n\nwould work?\n. What happens exactly?\n. Thanks, that's good to know and relieves us of the pressure to issue an immediate fix release because of this issue.\n. This could be a trait / val initialization problem. Try changing `authenticator` and `routes` into `lazy val`.\n. Referenced this issue in a comment of akka/akka#15681\n. No, it isn't implemented and I doubt it will given that this proposal is from 1996 and was never implemented or spec'd properly in any of the URI RFCs. You'll need to create your own directive.\n\nClosing for now...\n. @wedens Thanks for the report. This is a bug. \n\nUntil it is fixed can you try something similar to\n\n``` scala\nval ShortNumber = IntNumber.flatMap(i => if (i <= Short.MaxNumber) Some(i.toShort) else None)\n```\n. It's `Short.MaxValue`, of course:\n\n``` scala\nval ShortNumber = IntNumber.flatMap(i \u21d2 if (i <= Short.MaxValue) Some(i.toShort) else None)\n```\n. Recreated as akka/akka#16814.\n. Which versions of spray / akka are you using exactly?\n. I may be missing someting but I don't see any component of spray depending on quasiquotes. Can you either provide some kind of reproduction or maybe try [sbt-dependency-graph](https://github.com/jrudolph/sbt-dependency-graph) or just look into `target/resolution-cache/reports/*-compile.xml` to find out what actually depends on quasiquotes?\n. Done.\n. Have you an `import MyProtocol._` somewhere? Then it should just be\n\n``` scala\npath(\"\") {\n  get {\n    complete {\n            // returns List[Business] \n            Recommender.recommend(1000)\n           // this returns the json response thoug\n          //               List(new Business(\"Jewel Thief\"),new Business(\"Abby's Confectionary\"),new Business(\"PF Changs\"))\n    }\n  }\n}\n```\n\nBtw: these kind of questions are better handled on the [mailing list](http://groups.google.com/group/spray-user) or on http://stackoverflow.com. Closing...\n. This is a somewhat \"well-known\" but unfortunate incompatibility we had to introduce. Try the latest spray-json version 1.3.1\n. There doesn't seem to be a new line in this code?!? Can you provide an example of the output you are seeing?\n. The new line is coming from the error reporting of the HttpParser. I agree, it could make sense to limit the kind of parser reporting to log, also to avoid DOS attacks by maliciously crafted input that would create lots of (or expensive) parser error reporting.\n. We should change the default logging behavior to just log a summary and make the full log output configurable for debugging purposes.\n. Thanks @ruurtjan. Would you [sign the Typesafe CLA](http://www.typesafe.com/contribute/cla) and update the commit line to our [commit message convention](http://spray.io/project-info/contributing/#git-commit-messages) which would be \n\n```\n= docs: fix wikipedia link\n```\n. Move to akka by adding a note to fix this to akka/akka#15934.\n. See http://spray.io/documentation/1.2.2/spray-http/#content-type-header\n\nYou need to have an `HttpEntity.NonEmpty` to get access to the content type. Using `request.entity.toOption` that's actually what you get. So this should work:\n\n``` scala\nrequest.entity.toOption match {\n  case Some(entity) =>\n    val contentType = entity.contentType\n  case None => null\n}\n```\n\nAn alternative would be:\n\n``` scala\nrequest.entity match {\n  case Some(HttpEntity.NonEmpty(contentType, data) =>\n    // use contentType\n  case None => null\n}\n```\n. We need more information that shows that spray is at error here. If this is still an issue and you can show this only happens with spray and not with json4s alone, please reopen.\n. Thanks, @ches. \n\nIndeed, it seems that most links are broken. They are generated by `highlight-directives.js` on the browser which makes use of an sbt generated `directives-map.js` which contains a mapping of directives to the \"group\", i.e. the file, containing the directives. In this file, groups seem to be empty for most directives:\n\n``` js\nwindow.DirectivesMap = [\n{\n\"group\": \"\",\n\"entries\": \"mapInnerRoute mapRequestContext mapRequest routeRouteResponse mapRouteResponse mapRouteResponsePF mapRejections mapHttpResponsePart mapHttpResponse mapHttpResponseEntity mapHttpResponseHeaders noop pass provide hprovide extract hextract\"},\n{\n\"group\": \"\",\n\"entries\": \"cache cachingProhibited alwaysCache\"},\n{\n\"group\": \"\",\n\"entries\": \"autoChunk autoChunkFileBytes\"},\n//...\n{\n\"group\": \"and-resource\",\n\"entries\": \"getFromFile autoChunked conditionalFor respondWithLastModifiedHeader getFromResource getFromDirectory listDirectoryContents getFromBrowseableDirectory getFromBrowseableDirectories getFromResourceDirectory\"},\n{\n\"group\": \"condition\",\n\"entries\": \"conditional\"},\n{\n\"group\": \"field\",\n\"entries\": \"filter formField formFields\"},\n{\n\"group\": \"param\",\n\"entries\": \"fdmaWrapper anyParam anyParams\"},\n{\n\"group\": \"with\",\n\"entries\": \"respondWithStatus respondWithHeader respondWithSingletonHeader respondWithHeaders respondWithSingletonHeaders respondWithMediaType\"},\n];\n```\n. It works for me when run locally, but somehow the file was broken on the uploaded site.\n. Moved to akka/akka#16813.\n. Not sure if we can improve the situation in spray alone. This is basically a Scala compiler issue. We could try to be smart and provide fake implicit values implemented by macros that try to emit warnings when something like that happens but it may be that this will create more issues than it solved.\n\nI'm closing this for now since there is currently nothing we can do about it in spray.\n. Yes, one problem with LiftJson / Json4s formats is that they are able to marshal anything and try to do so greedily. This means that you should never import the Json4sSupport implicits globally but instead very fine grained only inside the blocks where they are actually used.\n. Moved to #16812\n. I.e. akka/akka#16812.\n. Actually, passing the hostname isn't unnecessary but required to support the SSL server name indication extension (starting with Java 7). So, the fix should be to check if the inetaddress is an ip address or a host name. If it's an ip address, pass that to the SSLEngine. If it's a host name pass that one.\n. So, the fix would be to use `InetSocketAddress.getHostString` instead of `InetSocketAddress.getHostName`.\n. Ah, I missed that. I guess Java 7 is not an option right now so I guess we need a new suggestion.\n. LGTM\n. I'd say this is an expected race condition: spray-can has a pool of persistent connections on the client side. The first test creates a persistent connection that is idle after the first request completed. By shutting down the server, you forcibly shut down those idle connections which isn't happening immediately but needs to travel over the wire (i.e. the TCP FIN/RST packet). Before the connection closing is detected on the client side you run a new POST request onto this terminating connection which then fails. Because a POST request isn't idempotent there is no way to retry it automatically.\n\nIf it's just for the test, use `IO(Http) ! CloseAll` to shutdown all client connections before running another test (make sure you don't run tests in parallel in that case as it will close down connections globally for that ActorSystem).\n\nFor the real world case, this is still a common enough race condition (server closing the connection vs client sending a POST request). We could add a flag not to use a persistent connection for non-idempotent requests. This still wouldn't fix it but would maybe make it less likely.\n. @hmpatel Is the question if the connection idle timeout interferes with new or ongoing client connections? Or is it about the host connector idle timeout?\n\nHow exactly do you use spray?\n. AFAICS in http://tools.ietf.org/html/rfc3986#section-3.4 comma is not required to be encoded in URI queries (and btw also won't be encoded by browsers). If you have any other information please reopen.\n. Moved to akka/akka#16811\n. This is a duplicate of https://github.com/spray/spray/issues/947\n. We are using kind of a hack to allow proper flushing after each chunk of input data which isn't supported in Java 6 otherwise which involves reconfiguring the deflation level during compression.\n\nThe proper way would be using the [new overload of `deflate`](https://docs.oracle.com/javase/7/docs/api/java/util/zip/Deflater.html#deflate%28byte[],%20int,%20int,%20int%29) added in Java 7 and for Android starting with API level 19 (Android 4.4).\n\nI think we need some kind of configurable \"flushing strategy\" that would allow either to a) disable flushing (e.g. for Android pre API level 19), b) the current hack way for Java 6, or c) using the new deflate overload if available (if not otherwise possible per reflection).\n. Moved to akka/akka#16810.\n. @olger yes, we are now migrating this ticket to akka-http.\n\nIt is now superseded by akka/akka#16857\n. Could you please post a code example that illustrates the problem you need to be fixed?\n. It's hard to say if this would fix more issues than it would break currently.\n\nIn any case, I think the original ticket isn't valid, because the aforementioned line\n\n``` scala\nif (string.isEmpty) Empty else new Bytes(createByteStringUnsafe(string getBytes charset.nioCharset))\n```\n\nisn't related to the parser at all.\n\nSo, there should be always the workaround to change the content-type of incoming requests to include a charset if none was supplied.\n\nI'm a bit reluctant to change the overall behavior (at least as long as there's no simple way to configure it back to the old one).\n. `HttpEntity.asString` delegates to `data.asString` so this shouldn't be the problem. Maybe the problem is in your test? What's the result of `write`? If you have an implicit marshaller in scope, you don't need `write` but can just use `Put(\"/\", GoesIn(...))` directly.\n. That's not what I meant. The problem might be that `Put(path, HttpEntity(...))` is probably already using the Json4sSupport marshaller to marshal the entity as JSON data instead of marshalling the json data as an entity.\n. Btw. you don't need \n\n``` scala\nrespondWithMediaType(`application/json`)\n```\n\nthis will be automatically provided by the marshaller.\n\nTry putting `pathSingleSlash` instead of `path(\"\")` into the route.\n. Btw. we already track the problem that the json4s marshaller is overly greedy as #990 (it will only be fixed for akka-http).\n. Yep, this is currently a user-trap which we will make harder to fall into with #990.\n. Yep, LGTM. Thanks, @wjur.\n. @jroper I tried to port your code directly but that didn't work because xerces is only included under `com.sun.org.apache.xerces.internal` by default. It seems that in play you actually have an explicit dependency on xerces as introduced here: https://github.com/playframework/playframework/commit/dc94b943ee5dc1c95ec7338e52b2e1e4e80cd607\n. There are also\n- `ContentRange.Unsatisfiable`\n- `LanguageRanges.*`\n- `HttpEncodingRange.*`\n- `Uri.Empty`\n\nwhich would maybe also qualify as `case object`s.\n. Cool, LGTM.\n. :+1:\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. Slashes don't need to be encoded in the query part of a URI. See http://tools.ietf.org/html/rfc3986#section-3.4:\n\n> The characters slash (\"/\") and question mark (\"?\") may represent data\n>    within the query component.   Beware that some older, erroneous\n>    implementations may not handle such data correctly when it is used as\n>    the base URI for relative references (Section 5.1), apparently\n>    because they fail to distinguish query data from path data when\n>    looking for hierarchical separators.  However, as query components\n>    are often used to carry identifying information in the form of\n>    \"key=value\" pairs and one frequently used value is a reference to\n>    another URI, it is sometimes better for usability to avoid percent-\n>    encoding those characters.\n. @damienlevin it's in the logs:\n\n```\n[WARN] [04/10/2015 16:19:07.964] [test-akka.actor.default-dispatcher-2] [akka://test/user/IO-HTTP/group-0/4] Received illegal response: Response Content-Length 9313144 exceeds the configured limit of 8388608\n```\n\nYou need to change the limit in your application.conf. See the `spray.can.client.parsing.max-content-length` setting: https://github.com/spray/spray/blob/master/spray-can/src/main/resources/reference.conf#L327\n. No worries :)\n. Nice, thanks. Port to akka-http in https://github.com/akka/akka/pull/17230\n. What did you try so far and what was the result?\n. @pomadchin that seems like a somewhat custom setup. I hope you understand that we cannot debug any custom setups. If you want us to investigate please provide a self-contained project that demonstrates the issue and shows that this is a generic problem and not one of your setup. Feel free to reopen in this case.\n. Thanks for the report @christopheclc. I agree, we should catch errors here as well.\n. @betandr thanks for the contribution. I think adding support for parsing and rendering the directives from RFC 5861 would be a nice addition. There aren't any spray releases planned currently, though, as active development has shifted over to akka-http.\n\nA few things:\n- I think it would make sense to add support for `stale-if-error` as well to make parsing things from RFC 5861 complete\n- while you are at it maybe you want to add a little abstraction for \n\n``` scala\n   case class XXX(deltaSeconds: Long) extends ResponseDirective with ValueRenderable {\n    def render[R <: Rendering](r: R): r.type = r ~~ productPrefix ~~ '=' ~~ deltaSeconds\n  }\n```\n\nwhich is then common to several directives\n- add a test for the parser / renderer roundtrip at https://github.com/spray/spray/blob/master/spray-http/src/test/scala/spray/http/HttpHeaderSpec.scala#L157\n- you need to sign the Typesafe CLA at http://www.typesafe.com/contribute/cla\n- maybe add a ticket to akka/akka to port this over to akka-http so we don't forget about it\n\nThanks again!\n. Maybe also directly add a comment with the link to the RFC in the parser and/or at the place of the case class definitions.\n. Maybe just a simple superclass that already implements render so that you can use it like this:\n\n``` scala\ncase class `stale-if-error`(deltaSeconds: Long) extends ResponseDirectiveWithDeltaSeconds\n```\n. No worries about the commits. You can still squash everything in the end.\n\nDo the tests run through if you build the project locally?\n\nIt seems the tests fail because you removed `productPrefix` :) It's a method of `Product` which just returns the name of the `case class`. You removed it so it won't be rendered any more that's what the tests complain about.\n\nTo make it available you need to extend ResponseDirectiveWithDeltaSeconds also with Product. This should work:\n\n``` scala\ntrait ResponseDirectiveWithDeltaSeconds extends ResponseDirective with ValueRenderable with Product {\n...\n```\n. (And also add the `productPrefix` back to the `render` method in case that wasn't clear)\n. To answer where the implementation of `Product` comes from in the end: it's automatically implemented by case classes, i.e the compiler will generate its implementation.\n. Yay! LGTM.\n\nCould you finally [squash all your commits into one](http://stackoverflow.com/questions/5189560/squash-my-last-x-commits-together-using-git) and change the first commit message line slightly to start with \"+ http:\" as our policy is to prefix the line with the name of the module which changed? I think we are then ready to merge.\n. Btw. after squashing you can `git push -f` into this existing branch to have the PR updated (your old commits are lost in the process).\n. No problem, the `SimpleLruCacheSpec` one is a test that seems to fail non-deterministically, so it's not related. I restarted the failed one, so we will hopefully see the green light very soon...\n\nIn any case, thanks for keeping up.\n. Yep :)\n. I previously wondered whether I should suggest this bug fix but I didn't understand how it would apply here because it is only relevant to file downloads that are delivered via `WriteFile` and `HttpData.FileBytes` which your code doesn't use (and the log messages seem to agree).\n\nSo, I'm glad your code works now but I'm not so sure about the cause :)\n. @v-gerasimov no one is currently working on this. At this point the prerequisite before anyone would look further into this ticket would be a standalone, self-contained reproducer.\n. I know and I understand the implementation but it isn't really that useful if it leaves half of a segment from the middle of the original path in the unmatched path afterwards.\n. @kutchar it's not completely clear to me what you want to achieve. Is this about `multipart/formdata` uploads or just regular HTTP request entities (like posting `application/json` data)?\n\nI haven't looked in there for a while but I do not see a reason why putting file-based HttpData into entities would not work for requests as well (at least for regular entities, `multipart/formdata` may be harder).\n\nWhat did you try so far?\n\nIn any case, spray will probably not receive any new functionality any more, especially not wrt streaming uploads/downloads or `multipart/formdata` support. akka-http has first-class support for all of that so it should be the tool of choice if streaming data is a main application.\n. Yes, we don't have the capacity and expertise to maintain OSGI support. We will accept PRs to fix it as long as they don't break anything else (which unfortunately has happened in the past with OSGI support).\n\nThanks for providing the fixes. I'm leaving this open for now for reference.\n. For me it works exactly as specified in the docs you quote:\n- if the connection is closed during the request, i.e. before a response has been dispatched the client receives a notification\n- after the client has received the response (even if it has not finished sending the request) it does not receive any notification that the connection was closed afterwards\n\nThe implications of the second point may be somewhat unexpected, that the original creator of the connection (the actor that sent the `Connect` message) does not receive any notifications after the connection has been established. But as you say, one can work around this by watching the Http connection actor.\n\nHere's the version I tested:\n\nhttps://gist.github.com/jrudolph/ba6545a5d1b4ec7cac38\n. Yes, right, I forgot to explain that change. \n\nI put this alternative line in to illustrate the above bullet points. Depending on which line you choose you get one of the above bullet points. If you use `Connection: close` you get the second bullet point in which case everything is as you observed which may be unexpected but is working as specified.\n\nSo, if you don't control the server and its exact connection closure semantics, I would watch the connection actor and then look out both for `Terminated(connectionActor)` and `ConnectionClosed` messages.\n. @RichardBradley Yes, that's a good description of the situation.\n\nIIRC one reasoning not to send too many notifications is that it is likely that they will end up in DeadLetters if the \"owning\" actor changes over the life-time of the connection. This is all a bit fuzzy but as long as there are working solutions it is unlikely we will change the existing behavior in spray any more, especially for the more tricky situations of streaming / chunked requests which will have better solutions in akka-http.\n. @ppiotrow yes, exactly. I'm closing this. If this is still an issue, please reopen.\n. @OElesin, this doesn't seem to be a bug, so I'm closing this for now. If this is still an issue, please continue the discussion on the mailing list: https://groups.google.com/group/spray-user\n\nAnd, thanks @matsluni for the help.\n. Thanks, @RichardBradley for adding the comment. There's some description of the process of building the documentation at http://spray.io/project-info/contributing/#contributing-documentation (though not for windows).\n. This is likely to be a configuration issue where you run with an older Akka version than you are thinking you are. Try running `show externalDependencyClasspath` in sbt to find out which libraries are on your classpath.\n\nThe change that probably broke the compatibility in your case was [this](https://github.com/akka/akka/commit/33ebf24c0fec622869a5306312c3bac3ac889f68) and it predates Akka 2.3.0. \n\nIn any case, this is not a spray bug, so it's likely to be akka or spark specific. If you cannot find out I suggest asking on the akka or spark mailing lists.\n\nClosing for now...\n. Thanks for the report @discordianfish. Could you post a small reproduction of the issue or at least a tcpdump showing how this happens? That would help a lot.\n. `SprayJsonSupport` only allows marshalling from and to HttpEntity. Putting JSON into parameters is somewhat unusual (but probably possible). \n\nI'm closing this here, as this is not a bug. If you still need help, please discuss it at the mailing list at http://groups.google.com/group/spray-user. Thanks!\n. @wjur is correct about how it works, but it should still be possible to achieve what you want.\n\nWhat spray does is \"sealing\" the route passed to `runRoute`. That means it applies a default RejectionHandler (and a default ExceptionHandler), so that rejections and exceptions never escape the actor. This default RejectionHandler will handle all rejections that you haven't handled so far and, thus, convert them to status codes.\n\nIf you want to see the status codes you must make sure that you seal the route already _inside_ your logging directive.\n\nBasically you need to replicate what you see inside here:\n\nhttps://github.com/spray/spray/blob/master/spray-routing/src/main/scala/spray/routing/HttpService.scala#L34\n\nI haven't tried but it could be enough to wrap your route with sealRoute\n\n``` scala\nlogRoute {\n  sealRoute {\n    yourMainRoute\n  }\n}\n```\n. Or, you can make `sealRoute` a directive like this:\n\n``` scala\ndef seal: Directive0 = new Directive0 {\n  def happly(f: (HNil) \u21d2 Route): Route = sealRoute(f(HNil))\n}\n```\n\nand then include its functionality directly into your directive with\n\n``` scala\ndef logRoute: Directive0 = mapRequestContext { ctx => \n //...\n} & seal\n```\n. Thanks, @wjur. Exactly, it's an artifact of how the website is deployed. For historical reasons the main branch is still the 1.1 version one which has version 1.1-SNAPSHOT. When the website is built, then from this branch.\n. Thanks, @rgaskill for the report. Several bugs have been found in spray's Gzip implementation (e.g. also #937) that have been fixed in the successor of spray, akka-http. This one is one of them (see https://github.com/akka/akka/blob/master/akka-http/src/main/scala/akka/http/scaladsl/coding/Gzip.scala#L101).\n\nOnly the most important bug fixes would qualify for being fixed in an eventual fix release of spray (though none is planned, currently).\n\nLet's keep this open for reference and for people to vote on.\n. Great you found a solution for now :)\n. Also pushed branches https://github.com/spray/spray/tree/w/fix-dir-traversal-on-windows-1.2 and https://github.com/spray/spray/tree/w/fix-dir-traversal-on-windows-1.1 in case we want to issue fix releases for those as well.\n. See https://github.com/akka/akka-http/pull/695. So, the naming convention is this:\n\nspray-routing -> use with shapeless 1.x\nspray-routing-shapeless2 -> use with shapless 2.x\nspray-routing-shapeless23 -> use with shapless >= 2.3.x\n\nUnfortunately, shapeless versions 2.x and 2.3.x don't seem to be binary compatible so, with upgrading to spray 1.3.4 you also need to update to a more recent shapeless version. Sorry for not mentioning this in the release notes.\n. ",
    "analytically": "+1\n. +1\n. Using 1.0-M6, Chrome 24.0.1312 logs this in the console:\n\nResource interpreted as Font but transferred with MIME type application/font-woff.\n\nhttp://code.google.com/p/chromium/issues/detail?id=70283\nSo for woff, the mime type should be application/x-font-woff?\n. Found TooManyRequests, so the issue example is invalidated. Feel free to close this one.\n. And the ClientIP directive should use optionalHeaderValuePF instead of headerValuePF.\n. Also, when the parser fails, respond with 400 Bad Request? Now when the parser fails (e.g. adding encoded UTF-16 - \"Malformed escape pair\") it returns 500 Internal Server Error. \n\n```\njava.net.URISyntaxException: Malformed escape pair at index 19: /v1/sentiment?text=%uF0B7%20We%20lower%20our%20feedback.._7\n    at java.net.URI$Parser.fail(URI.java:2829) ~[na:1.7.0_07]\n    at java.net.URI$Parser.scanEscape(URI.java:2959) ~[na:1.7.0_07]\n    at java.net.URI$Parser.scan(URI.java:2982) ~[na:1.7.0_07]\n    at java.net.URI$Parser.checkChars(URI.java:3000) ~[na:1.7.0_07]\n    at java.net.URI$Parser.parseHierarchical(URI.java:3092) ~[na:1.7.0_07]\n    at java.net.URI$Parser.parse(URI.java:3044) ~[na:1.7.0_07]\n    at java.net.URI.<init>(URI.java:595) ~[na:1.7.0_07]\n    at cc.spray.http.HttpRequest.URI(HttpRequest.scala:39) ~[affectr-api-1.0.0.jar:na]\n    at cc.spray.http.HttpRequest.path(HttpRequest.scala:43) ~[affectr-api-1.0.0.jar:na]\n```\n. This is really annoying us, and ideas?\n. Strangely enough it's only triggered on our server boxes, not on any developer machine. I will try with the `on-spray-can` example.\n. +1\n. Sure understood.\n. This is pretty neat! Thanks Adam!\n. I agree with your second point. Although I do get some consistency in the results, at times I don't at all. \n\nOn the other hand, the Disruptor project swears by it. See [blog post](http://mechanitis.blogspot.it/2011/07/dissecting-disruptor-why-its-so-fast_22.html) and https://code.google.com/p/disruptor/source/browse/trunk/code/src/main/com/lmax/disruptor/Sequence.java\n. Good point, although the jsr166e backport requires Java 7, while Spray is Java 6 IIRC. http://g.oswego.edu/dl/concurrency-interest/\n. PS http://minddotout.wordpress.com/2013/05/11/java-8-concurrency-longadder/ - so might be worth it for Spray 1.2?\n. All done.\n. I'm receiving this exception as well:\n\n```\njava.lang.NullPointerException: null\n    at spray.can.server.ResponseReceiverRef.unhandledMessage(ResponseReceiverRef.scala:81) ~[spray-can-1.2-20130801.jar:na]\n    at spray.can.server.ResponseReceiverRef.dispatch(ResponseReceiverRef.scala:70) ~[spray-can-1.2-20130801.jar:na]\n    at spray.can.server.ResponseReceiverRef.handle(ResponseReceiverRef.scala:49) ~[spray-can-1.2-20130801.jar:na]\n    at akka.spray.UnregisteredActorRefBase.$bang(UnregisteredActorRefBase.scala:72) ~[spray-util-1.2-20130801.jar:na]\n    at akka.actor.ActorRef.tell(ActorRef.scala:125) ~[akka-actor_2.10-2.2.0.jar:2.2.0]\n    at spray.routing.RequestContext$$anon$2.handle(RequestContext.scala:73) ~[spray-routing-1.2-20130801.jar:na]\n    at akka.spray.UnregisteredActorRefBase.$bang(UnregisteredActorRefBase.scala:72) ~[spray-util-1.2-20130801.jar:na]\n    at spray.routing.RequestContext.complete(RequestContext.scala:250) ~[spray-routing-1.2-20130801.jar:na]\n    at spray.routing.RequestContext.complete(RequestContext.scala:223) ~[spray-routing-1.2-20130801.jar:na]\n    at spray.routing.ExceptionHandler$$anonfun$default$1$$anonfun$applyOrElse$3.apply(ExceptionHandler.scala:49) ~[spray-routing-1.2-20130801.jar:na]\n    at spray.routing.ExceptionHandler$$anonfun$default$1$$anonfun$applyOrElse$3.apply(ExceptionHandler.scala:47) ~[spray-routing-1.2-20130801.jar:na]\n    at spray.routing.directives.ExecutionDirectives$$anonfun$handleExceptions$1$$anonfun$apply$4$$anonfun$spray$routing$directives$ExecutionDirectives$class$$anonfun$$anonfun$$handleError$1$1.apply(ExecutionDirectives.scala:34) ~[spray-routing-1.2-20130801.jar:na]\n    at spray.routing.directives.ExecutionDirectives$$anonfun$handleExceptions$1$$anonfun$apply$4$$anonfun$spray$routing$directives$ExecutionDirectives$class$$anonfun$$anonfun$$handleError$1$1.apply(ExecutionDirectives.scala:34) ~[spray-routing-1.2-20130801.jar:na]\n    at scala.PartialFunction$AndThen.apply(PartialFunction.scala:181) ~[scala-library.jar:0.12.0]\n    at spray.routing.directives.ExecutionDirectives$$anonfun$handleExceptions$1$$anonfun$apply$4$$anonfun$apply$1.applyOrElse(ExecutionDirectives.scala:37) ~[spray-routing-1.2-20130801.jar:na]\n    at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33) ~[scala-library.jar:0.12.0]\n    at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33) ~[scala-library.jar:0.12.0]\n```\n. Perhaps also improve on the error message? eg. \"The supplied JSONP callback identifier is invalid\"\n. Also please add application/json-patch+json, thanks!\n\nSee http://tools.ietf.org/html/rfc6902\n. Hmm it seems like Siege 3.0.4 fixes the issue:\n\n```\nSIEGE-3.0.4 Although RFC 2616 specifies that Location directives must contain an absolute URI, most Web clients normalize relative URIs. This release adds conventional compliance with relative URI handling. Ironically, it also includes a new default user-agent so that field is in compliance with RFC 2616.\n```\n\nI'm on Ubuntu 13.10, and the default Siege in the repositories is 3.0.1.\n. +1\n. :+1: seeing this as well, is a new release planned?\n. It block us from upgrading to 1.3.2. Is there a known workaround? Excluding quasiquotes didn't seem to work...\n. No, doesn't seem to work. I'll try some more later.\n. Yep, sorry, it did work for me as well.\n. +1\n. +1\n. +1\n. Using 1.0-M6, Chrome 24.0.1312 logs this in the console:\n\nResource interpreted as Font but transferred with MIME type application/font-woff.\n\nhttp://code.google.com/p/chromium/issues/detail?id=70283\nSo for woff, the mime type should be application/x-font-woff?\n. Found TooManyRequests, so the issue example is invalidated. Feel free to close this one.\n. And the ClientIP directive should use optionalHeaderValuePF instead of headerValuePF.\n. Also, when the parser fails, respond with 400 Bad Request? Now when the parser fails (e.g. adding encoded UTF-16 - \"Malformed escape pair\") it returns 500 Internal Server Error. \n\n```\njava.net.URISyntaxException: Malformed escape pair at index 19: /v1/sentiment?text=%uF0B7%20We%20lower%20our%20feedback.._7\n    at java.net.URI$Parser.fail(URI.java:2829) ~[na:1.7.0_07]\n    at java.net.URI$Parser.scanEscape(URI.java:2959) ~[na:1.7.0_07]\n    at java.net.URI$Parser.scan(URI.java:2982) ~[na:1.7.0_07]\n    at java.net.URI$Parser.checkChars(URI.java:3000) ~[na:1.7.0_07]\n    at java.net.URI$Parser.parseHierarchical(URI.java:3092) ~[na:1.7.0_07]\n    at java.net.URI$Parser.parse(URI.java:3044) ~[na:1.7.0_07]\n    at java.net.URI.<init>(URI.java:595) ~[na:1.7.0_07]\n    at cc.spray.http.HttpRequest.URI(HttpRequest.scala:39) ~[affectr-api-1.0.0.jar:na]\n    at cc.spray.http.HttpRequest.path(HttpRequest.scala:43) ~[affectr-api-1.0.0.jar:na]\n```\n. This is really annoying us, and ideas?\n. Strangely enough it's only triggered on our server boxes, not on any developer machine. I will try with the `on-spray-can` example.\n. +1\n. Sure understood.\n. This is pretty neat! Thanks Adam!\n. I agree with your second point. Although I do get some consistency in the results, at times I don't at all. \n\nOn the other hand, the Disruptor project swears by it. See [blog post](http://mechanitis.blogspot.it/2011/07/dissecting-disruptor-why-its-so-fast_22.html) and https://code.google.com/p/disruptor/source/browse/trunk/code/src/main/com/lmax/disruptor/Sequence.java\n. Good point, although the jsr166e backport requires Java 7, while Spray is Java 6 IIRC. http://g.oswego.edu/dl/concurrency-interest/\n. PS http://minddotout.wordpress.com/2013/05/11/java-8-concurrency-longadder/ - so might be worth it for Spray 1.2?\n. All done.\n. I'm receiving this exception as well:\n\n```\njava.lang.NullPointerException: null\n    at spray.can.server.ResponseReceiverRef.unhandledMessage(ResponseReceiverRef.scala:81) ~[spray-can-1.2-20130801.jar:na]\n    at spray.can.server.ResponseReceiverRef.dispatch(ResponseReceiverRef.scala:70) ~[spray-can-1.2-20130801.jar:na]\n    at spray.can.server.ResponseReceiverRef.handle(ResponseReceiverRef.scala:49) ~[spray-can-1.2-20130801.jar:na]\n    at akka.spray.UnregisteredActorRefBase.$bang(UnregisteredActorRefBase.scala:72) ~[spray-util-1.2-20130801.jar:na]\n    at akka.actor.ActorRef.tell(ActorRef.scala:125) ~[akka-actor_2.10-2.2.0.jar:2.2.0]\n    at spray.routing.RequestContext$$anon$2.handle(RequestContext.scala:73) ~[spray-routing-1.2-20130801.jar:na]\n    at akka.spray.UnregisteredActorRefBase.$bang(UnregisteredActorRefBase.scala:72) ~[spray-util-1.2-20130801.jar:na]\n    at spray.routing.RequestContext.complete(RequestContext.scala:250) ~[spray-routing-1.2-20130801.jar:na]\n    at spray.routing.RequestContext.complete(RequestContext.scala:223) ~[spray-routing-1.2-20130801.jar:na]\n    at spray.routing.ExceptionHandler$$anonfun$default$1$$anonfun$applyOrElse$3.apply(ExceptionHandler.scala:49) ~[spray-routing-1.2-20130801.jar:na]\n    at spray.routing.ExceptionHandler$$anonfun$default$1$$anonfun$applyOrElse$3.apply(ExceptionHandler.scala:47) ~[spray-routing-1.2-20130801.jar:na]\n    at spray.routing.directives.ExecutionDirectives$$anonfun$handleExceptions$1$$anonfun$apply$4$$anonfun$spray$routing$directives$ExecutionDirectives$class$$anonfun$$anonfun$$handleError$1$1.apply(ExecutionDirectives.scala:34) ~[spray-routing-1.2-20130801.jar:na]\n    at spray.routing.directives.ExecutionDirectives$$anonfun$handleExceptions$1$$anonfun$apply$4$$anonfun$spray$routing$directives$ExecutionDirectives$class$$anonfun$$anonfun$$handleError$1$1.apply(ExecutionDirectives.scala:34) ~[spray-routing-1.2-20130801.jar:na]\n    at scala.PartialFunction$AndThen.apply(PartialFunction.scala:181) ~[scala-library.jar:0.12.0]\n    at spray.routing.directives.ExecutionDirectives$$anonfun$handleExceptions$1$$anonfun$apply$4$$anonfun$apply$1.applyOrElse(ExecutionDirectives.scala:37) ~[spray-routing-1.2-20130801.jar:na]\n    at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33) ~[scala-library.jar:0.12.0]\n    at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33) ~[scala-library.jar:0.12.0]\n```\n. Perhaps also improve on the error message? eg. \"The supplied JSONP callback identifier is invalid\"\n. Also please add application/json-patch+json, thanks!\n\nSee http://tools.ietf.org/html/rfc6902\n. Hmm it seems like Siege 3.0.4 fixes the issue:\n\n```\nSIEGE-3.0.4 Although RFC 2616 specifies that Location directives must contain an absolute URI, most Web clients normalize relative URIs. This release adds conventional compliance with relative URI handling. Ironically, it also includes a new default user-agent so that field is in compliance with RFC 2616.\n```\n\nI'm on Ubuntu 13.10, and the default Siege in the repositories is 3.0.1.\n. +1\n. :+1: seeing this as well, is a new release planned?\n. It block us from upgrading to 1.3.2. Is there a known workaround? Excluding quasiquotes didn't seem to work...\n. No, doesn't seem to work. I'll try some more later.\n. Yep, sorry, it did work for me as well.\n. +1\n. ",
    "ghost": "You're more than welcome!\n\nI don't believe there's anything particularly unusual about our use case so far, and once the initial request has been processed, successive operations are nice and zippy. On my 2.13 GHz, 4G RAM MacBook Air running either Tomcat 7.0.12 or Jetty 8.0.0.M2, I see the first operation go from ~1015ms to successive operations taking ~35ms. You can see the reason why very easily: please just launch your server JVM with the \"-verbose:class\" option and examine the logs. Upon handling the first request, you'll see an enormous number of classes loaded. Again, many are from the cc.spray package, but a much, much larger number are from parboiled. Anything you can do to preload these packages will be a big win for clients of that first request. :-)\n\nThanks!\nPaul\n. Hi Mathias!\n\nThank you!\n\nEven though you already merged, some background: while it's clear from the existence of routingSettings and the apply() method on RoutingSettings taking a Config that you intended to allow the overriding of the routingSettings, using a val there meant that, even if I did override implicit val routingSettings, the base implementation would be called. That, of course, constructs a default RoutingSettings, which in turn calls ConfigFactory.load(), which in turn uses the default classloader. Eventually ConfigUtils.prepareSubConfig() is called, which calls checkValid() on ConfigFactory.defaultReference withFallback sprayConfigAdditions. This is OK if the default classloader can find your configuration, but fails if it can't. Since I'm using spray in an OSGi bundle, this was failing. In any event, it struck me as unlikely that you intended for the default RouteSettings constructor to be called even if a client overrides routingSettings, so a def rather than a val seemed like the appropriate change.\n\nNow for a question: do you have an ETA for when an M5 might appear, and more generally, for when we might expect 1.0 and 1.1?\n\nMany thanks and best regards,\nPaul\n\nOn Nov 2, 2012, at 8:30 AM, Mathias wrote:\n\n> Thanks, Paul!\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. Is there any progress on this? It seems pretty important for my use case as well.\n. Thanks, Mathias. I know you have to devote your resources where they make the most sense. Do we have an ETA for akka-http? Right now we're using https://github.com/wandoulabs/spray-websocket and I suppose we could fork it and put it on top of akka-http if we had to... or encourage the upstream developer to do so. :-)\n. You're more than welcome!\n\nI don't believe there's anything particularly unusual about our use case so far, and once the initial request has been processed, successive operations are nice and zippy. On my 2.13 GHz, 4G RAM MacBook Air running either Tomcat 7.0.12 or Jetty 8.0.0.M2, I see the first operation go from ~1015ms to successive operations taking ~35ms. You can see the reason why very easily: please just launch your server JVM with the \"-verbose:class\" option and examine the logs. Upon handling the first request, you'll see an enormous number of classes loaded. Again, many are from the cc.spray package, but a much, much larger number are from parboiled. Anything you can do to preload these packages will be a big win for clients of that first request. :-)\n\nThanks!\nPaul\n. Hi Mathias!\n\nThank you!\n\nEven though you already merged, some background: while it's clear from the existence of routingSettings and the apply() method on RoutingSettings taking a Config that you intended to allow the overriding of the routingSettings, using a val there meant that, even if I did override implicit val routingSettings, the base implementation would be called. That, of course, constructs a default RoutingSettings, which in turn calls ConfigFactory.load(), which in turn uses the default classloader. Eventually ConfigUtils.prepareSubConfig() is called, which calls checkValid() on ConfigFactory.defaultReference withFallback sprayConfigAdditions. This is OK if the default classloader can find your configuration, but fails if it can't. Since I'm using spray in an OSGi bundle, this was failing. In any event, it struck me as unlikely that you intended for the default RouteSettings constructor to be called even if a client overrides routingSettings, so a def rather than a val seemed like the appropriate change.\n\nNow for a question: do you have an ETA for when an M5 might appear, and more generally, for when we might expect 1.0 and 1.1?\n\nMany thanks and best regards,\nPaul\n\nOn Nov 2, 2012, at 8:30 AM, Mathias wrote:\n\n> Thanks, Paul!\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. Is there any progress on this? It seems pretty important for my use case as well.\n. Thanks, Mathias. I know you have to devote your resources where they make the most sense. Do we have an ETA for akka-http? Right now we're using https://github.com/wandoulabs/spray-websocket and I suppose we could fork it and put it on top of akka-http if we had to... or encourage the upstream developer to do so. :-)\n. ",
    "rollinsruss": "+1 this will be very handy for multiple reasons, especially for multi-environment support (which in my case is dev, test, beta, stage and production)\n. +1 this will be very handy for multiple reasons, especially for multi-environment support (which in my case is dev, test, beta, stage and production)\n. ",
    "stefri": "Following up our conversation on the mailing list. I had a first shot on upgrading the spray build to sbt 0.10. You'll those changes on branch \"feature/sbt-0.10\" on my spray fork. See https://github.com/stefri/spray/tree/feature/sbt-0.10.\n\nI based my changes on the current \"develop\" branch. So far it is possible to compile all artifacts. I didn't accomplish the renaming of \"javadoc\" into \"scaladoc\" artifacts. I have to check back tomorrow how this can be accomplished in 0.10. There are one ore two more TODOs marked in the build file (SprayBuild.scala) maybe you can have a look and give me a hint what those configurations were used for.\n\nI removed the idea-plugin settings since for 0.10 the ide plugins are best configured in the personal global plugins folder.\n. Published a merged feature/sbt-0.10 <--> feature/akka1.2 as feature/sbt-0.10_akka-1.2. It was mainly a straight forward merge, just upgraded akka 1.2 from RC3 to RC6. The compilation problems regarding the servlet-api, which I faced before are gone with sbt 0.10.\n. Following up our conversation on the mailing list. I had a first shot on upgrading the spray build to sbt 0.10. You'll those changes on branch \"feature/sbt-0.10\" on my spray fork. See https://github.com/stefri/spray/tree/feature/sbt-0.10.\n\nI based my changes on the current \"develop\" branch. So far it is possible to compile all artifacts. I didn't accomplish the renaming of \"javadoc\" into \"scaladoc\" artifacts. I have to check back tomorrow how this can be accomplished in 0.10. There are one ore two more TODOs marked in the build file (SprayBuild.scala) maybe you can have a look and give me a hint what those configurations were used for.\n\nI removed the idea-plugin settings since for 0.10 the ide plugins are best configured in the personal global plugins folder.\n. Published a merged feature/sbt-0.10 <--> feature/akka1.2 as feature/sbt-0.10_akka-1.2. It was mainly a straight forward merge, just upgraded akka 1.2 from RC3 to RC6. The compilation problems regarding the servlet-api, which I faced before are gone with sbt 0.10.\n. ",
    "svdberg": "Ah thanks, I was already thinking in that direction. Thanks for the fast feedback. I'll pass it on to the server's admins.\n. Ah thanks, I was already thinking in that direction. Thanks for the fast feedback. I'll pass it on to the server's admins.\n. ",
    "shopolate": "Oh, I forgot to mention I had to add a compile dependency: org.jvnet.mimepull (http://mimepull.java.net/). This is the parser of multipart data that is used in Jersey. This is a small, standalone dependency, I hope this will be fine for you.\n. I like:\n\ncontent(asMultipart[Employee, Image]) { (employee, image) =>\n        ...\n}\n\nExcept with complex formulars where you could have a lots of parts, most of them form data.\nI think I would prefer an extractor similar to the parameters directive, that would allow me to select which body parts I am interested in. This kind of extractor would be usefull for regular form content as well.\n. I forgot to add that relying on part order might not fit every use cases. Ideally, we would be able to map the part name with the type. This also could be achieved by a directive similar to the parameters one. The only difference is we could match a part either by its name or by its index.\n\nIf you are interested, I could try to do it.\n\nAs for marshalling, the problem here is you might want to specify names and content types to marshal to. It seems to me that:\n\ncontext.complete(multipart(employee, image))\n\nmight be restrictive, especially since the default content type of a body part is text/plain. For example, something like:\n\ncontext.complete(\n    bodyPart(employee, `application/json`, \"employee'), \n    bodyPart(someStream, `image/png`, \"image\", fileName = \"icon.png\"), \n    bodyPart(someStringValue, name = \"someFormKey\")) \n\nwould allow for more control.\n\ncheers,\nJulien\n. Everything makes sense. \n\nOne last little dark area: what is the type of the image var in your unmarshalling example? It can not be InputStream or Array[Byte], otherwise, you could not deduce if the content type is application/octet-stream, image/png or image/jpeg. It could not be HttpContent neither, otherwise it would contradict your statement about the framework deducing the most fit content type from accept header itself.\n\nAs for the refactoring of SImpleParsers you are talking about, I have a hard time figuring it out. Unmarshallers are unmarshalling HttpContent, SImpleParsers are unmarshalling strings. It seems to me quite a stretch to merge them together, since the semantics looks quite different. It looks to me that these are acting on different levels, and that both would be needed to extract content from multipart/form-data and application/x-www-form-urlencoded in a 'formFields' directive.\n\nCheers,\n\nJulien\n. formField / formFields directives on the way...\n. Ok, the support for the 'formField' / 'formFields' directive has been added to this pull request. The directive supports both 'application/x-www-form-urlencoded' and 'multipart/form-data'.\n\nIt have been designed following your recommandations, except it is using the Marshaller/Unmarshaller pair for FormContent, and thus I have not deleted it, and see little need for removing it actually.\n\nIf this pull request pleases you, I could also add:\n- a 'cookie' / 'cookies' directive on the same principle. \n- a 'multipart/form-data' marshaller.\n. Oh, I forgot to mention I had to add a compile dependency: org.jvnet.mimepull (http://mimepull.java.net/). This is the parser of multipart data that is used in Jersey. This is a small, standalone dependency, I hope this will be fine for you.\n. I like:\n\ncontent(asMultipart[Employee, Image]) { (employee, image) =>\n        ...\n}\n\nExcept with complex formulars where you could have a lots of parts, most of them form data.\nI think I would prefer an extractor similar to the parameters directive, that would allow me to select which body parts I am interested in. This kind of extractor would be usefull for regular form content as well.\n. I forgot to add that relying on part order might not fit every use cases. Ideally, we would be able to map the part name with the type. This also could be achieved by a directive similar to the parameters one. The only difference is we could match a part either by its name or by its index.\n\nIf you are interested, I could try to do it.\n\nAs for marshalling, the problem here is you might want to specify names and content types to marshal to. It seems to me that:\n\ncontext.complete(multipart(employee, image))\n\nmight be restrictive, especially since the default content type of a body part is text/plain. For example, something like:\n\ncontext.complete(\n    bodyPart(employee, `application/json`, \"employee'), \n    bodyPart(someStream, `image/png`, \"image\", fileName = \"icon.png\"), \n    bodyPart(someStringValue, name = \"someFormKey\")) \n\nwould allow for more control.\n\ncheers,\nJulien\n. Everything makes sense. \n\nOne last little dark area: what is the type of the image var in your unmarshalling example? It can not be InputStream or Array[Byte], otherwise, you could not deduce if the content type is application/octet-stream, image/png or image/jpeg. It could not be HttpContent neither, otherwise it would contradict your statement about the framework deducing the most fit content type from accept header itself.\n\nAs for the refactoring of SImpleParsers you are talking about, I have a hard time figuring it out. Unmarshallers are unmarshalling HttpContent, SImpleParsers are unmarshalling strings. It seems to me quite a stretch to merge them together, since the semantics looks quite different. It looks to me that these are acting on different levels, and that both would be needed to extract content from multipart/form-data and application/x-www-form-urlencoded in a 'formFields' directive.\n\nCheers,\n\nJulien\n. formField / formFields directives on the way...\n. Ok, the support for the 'formField' / 'formFields' directive has been added to this pull request. The directive supports both 'application/x-www-form-urlencoded' and 'multipart/form-data'.\n\nIt have been designed following your recommandations, except it is using the Marshaller/Unmarshaller pair for FormContent, and thus I have not deleted it, and see little need for removing it actually.\n\nIf this pull request pleases you, I could also add:\n- a 'cookie' / 'cookies' directive on the same principle. \n- a 'multipart/form-data' marshaller.\n. ",
    "PanAeon": "Hi it seems that  this issue has appeared again.\n. Sorry i was looking at master, and there in cc.spray.typeconversion.SprayJsonSupport still uses JsonReader instead of RootJsonReader. It's fixed in develop branch.\n. Hi it seems that  this issue has appeared again.\n. Sorry i was looking at master, and there in cc.spray.typeconversion.SprayJsonSupport still uses JsonReader instead of RootJsonReader. It's fixed in develop branch.\n. ",
    "jmolina91": "Thank you!\n. Thank you!\n. ",
    "alexanderdean": "The [recommended way](http://blog.steveklabnik.com/2011/07/03/nobody-understands-rest-or-http.html) of handling versioning for RESTful APIs is through resource-level versioning of the `Accept:` mimetype, like this:\n\n```\nGET /accounts/1 HTTP/1.1\nAccept: application/vnd.steveklabnik-v2+json\n```\n\nBut I don't think that Spray supports this form of versioning either (because the pattern matching is based on pre-defined content types like `application/json`) - but please correct me if I'm wrong Mathias...\n. Can you share an example curl to the server so we can confirm it's 2100 characters long?\n. The [recommended way](http://blog.steveklabnik.com/2011/07/03/nobody-understands-rest-or-http.html) of handling versioning for RESTful APIs is through resource-level versioning of the `Accept:` mimetype, like this:\n\n```\nGET /accounts/1 HTTP/1.1\nAccept: application/vnd.steveklabnik-v2+json\n```\n\nBut I don't think that Spray supports this form of versioning either (because the pattern matching is based on pre-defined content types like `application/json`) - but please correct me if I'm wrong Mathias...\n. Can you share an example curl to the server so we can confirm it's 2100 characters long?\n. ",
    "JanxSpirit": "I can write something up that will at least make people aware of these directives. Do you want a pull request on master? Cheers!\n\nGregg\n. nm - wiki, duh\n. Better than nothing?\nhttps://github.com/spray/spray/wiki/Form-Filters\n. I can write something up that will at least make people aware of these directives. Do you want a pull request on master? Cheers!\n\nGregg\n. nm - wiki, duh\n. Better than nothing?\nhttps://github.com/spray/spray/wiki/Form-Filters\n. ",
    "matanster": "This newer example may be relevant now, if helpful.\nhttps://github.com/matanster/spray-heroku\n. Can already use https://github.com/TooTallNate/Java-WebSocket (see [here](https://github.com/cuali/SprayEasterEggs/blob/master/src/main/scala/reactive/socket/ReactiveServer.scala) for inspiration). Shouldn't websocket support actually make more sense first as part of Akka, then bubble up to spray? Anyway Play [already implements websocket](https://www.playframework.com/documentation/2.2.x/ScalaWebSockets), it might make sense to converge the two around the same core implementation... \n. Not sure I follow. Play provides WebSocket. I thought Play was going to\nreplace Netty with spray across the board.... some of my assumptions are\nsurely wrong....\n\nOn Tue, Feb 10, 2015 at 7:04 PM, Johannes Rudolph notifications@github.com\nwrote:\n\n> We've considered this request and found that we need more evidence that\n> anyone would find this useful.\n> \n>  Just joking... :D The time may have finally come that we are actually\n> going to implement this as part of akka-http. Here's the new ticket:\n> akka/akka#16848 https://github.com/akka/akka/issues/16848 [image:\n> :fireworks:]\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/spray/spray/issues/134#issuecomment-73738490.\n. Sorry for triggering this and then begging to defer, but I think the samples spread in the current documentation _are in fact like_ a guided tour. I think what's missing is only **a crisper way of becoming aware of and navigating the great selection of pre-built directives** - which are a part that really makes a developer realize the efficiency of using spray in my view, or just too important to leave somewhat hidden as a hidden gem (although having an index of them is also good).\n. I looked into it and I think the spray documentation is rendered by [sphinx-doc](http://sphinx-doc.org/) which in turn uses [pygments](http://pygments.org/) for the code snippets highlighting. I'm not sure pygments can enable hyperlinks (nor hover boxes) so I think an elegant way to accomplish this suggestion would be adding another post-processing step that will locate all code snippets on each page already rendered by sphinx's existing rendering - which is easy as locating any `<div class=highlight-scala>` in the output (you can see that in chrome dev tools), and then check each word or parse unit for a match to any of a collection of directive names - then attach hover behavior with the right content inside it, for every match. Or hyperlinks.. \n\nThis entails including a file that _maps directive names_ to the content that should show for them or be hyperlinked from them. Probably the same documentation content that already exists per directive now already. \n\nAnd it may / will require manual work in avoiding any ~ symbol that is not meant as the spray ~ function, from becoming linked to the spray ~ function, if that function is included and not only directives. Telling this new code to ignore parse units like ~ will require some creative work... such as embedding and stripping off special marks in the code examples so that both pygments and this new code each can do their thing - unless there's some wacky Scalac flag that can result in telling which class/function every parse unit compiles to, for automatically determining whether a parse-unit is compiled to spray or not... \n. @jrudolph I guess you didn't get my private message, this sounds cool, let me know in case it runs into any problems or usability challenge..\n. This newer example may be relevant now, if helpful.\nhttps://github.com/matanster/spray-heroku\n. Can already use https://github.com/TooTallNate/Java-WebSocket (see [here](https://github.com/cuali/SprayEasterEggs/blob/master/src/main/scala/reactive/socket/ReactiveServer.scala) for inspiration). Shouldn't websocket support actually make more sense first as part of Akka, then bubble up to spray? Anyway Play [already implements websocket](https://www.playframework.com/documentation/2.2.x/ScalaWebSockets), it might make sense to converge the two around the same core implementation... \n. Not sure I follow. Play provides WebSocket. I thought Play was going to\nreplace Netty with spray across the board.... some of my assumptions are\nsurely wrong....\n\nOn Tue, Feb 10, 2015 at 7:04 PM, Johannes Rudolph notifications@github.com\nwrote:\n\n> We've considered this request and found that we need more evidence that\n> anyone would find this useful.\n> \n>  Just joking... :D The time may have finally come that we are actually\n> going to implement this as part of akka-http. Here's the new ticket:\n> akka/akka#16848 https://github.com/akka/akka/issues/16848 [image:\n> :fireworks:]\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/spray/spray/issues/134#issuecomment-73738490.\n. Sorry for triggering this and then begging to defer, but I think the samples spread in the current documentation _are in fact like_ a guided tour. I think what's missing is only **a crisper way of becoming aware of and navigating the great selection of pre-built directives** - which are a part that really makes a developer realize the efficiency of using spray in my view, or just too important to leave somewhat hidden as a hidden gem (although having an index of them is also good).\n. I looked into it and I think the spray documentation is rendered by [sphinx-doc](http://sphinx-doc.org/) which in turn uses [pygments](http://pygments.org/) for the code snippets highlighting. I'm not sure pygments can enable hyperlinks (nor hover boxes) so I think an elegant way to accomplish this suggestion would be adding another post-processing step that will locate all code snippets on each page already rendered by sphinx's existing rendering - which is easy as locating any `<div class=highlight-scala>` in the output (you can see that in chrome dev tools), and then check each word or parse unit for a match to any of a collection of directive names - then attach hover behavior with the right content inside it, for every match. Or hyperlinks.. \n\nThis entails including a file that _maps directive names_ to the content that should show for them or be hyperlinked from them. Probably the same documentation content that already exists per directive now already. \n\nAnd it may / will require manual work in avoiding any ~ symbol that is not meant as the spray ~ function, from becoming linked to the spray ~ function, if that function is included and not only directives. Telling this new code to ignore parse units like ~ will require some creative work... such as embedding and stripping off special marks in the code examples so that both pygments and this new code each can do their thing - unless there's some wacky Scalac flag that can result in telling which class/function every parse unit compiles to, for automatically determining whether a parse-unit is compiled to spray or not... \n. @jrudolph I guess you didn't get my private message, this sounds cool, let me know in case it runs into any problems or usability challenge..\n. ",
    "andresilva": "Doesn't this issue conflict with #219?\n. Hey Mathias,\nI just created a new pull request with the changes you suggested (https://github.com/spray/spray/pull/295).\n. Thanks, I'll certainly do that next time ;-)\n. Thanks for the tips. I just pushed the changes you suggested.\n. I think it makes sense to rename it to `detach`, especially if we can forego the explicit `ExecutionContext` as you suggested. Additionally, if we rename it to `detach` we should still support the old `detachTo` directive so that we don't break the API but instead deprecate its usage and direct users to the new `detach` directive.\n\nI was going to ask you exactly what magnets you'd want, thanks for clarifying beforehand ;-)\n\nI'll implement your suggestions when I get home.\n. Updated, let me know what you think. Tell me if you want all these changes into a single commit or how should I break them, so that I can redo the commit messages.\n\nCheers!\n. Done. Added a short description / migration guide. Let me know if you think it could be improved.\n. > Also, in order to avoid ambiguity with the fromUnitWithExecutionContext branch we need to move it into a DetachMagnetLowerPrioBranches superclass.\n\nI can't get the following to work: \n\n``` scala\nclass DetachMagnet(implicit val ec: ExecutionContext)\n\nobject DetachMagnet extends LowerPriorityDetachMagnet {\n  implicit def fromUnitWithExecutionContext(u: Unit)(implicit ec: ExecutionContext) = new DetachMagnet\n}\n\nprivate[routing] abstract class LowerPriorityDetachMagnet {\n  implicit def fromUnitWithRefFactory(u: Unit)(implicit refFactory: ActorRefFactory) =\n    new DetachMagnet()(refFactory.dispatcher)\n}\n```\n\nI guess the idea of moving the implicit to a superclass was according to the following rule:\n\n> When comparing two different applicable alternatives of an overloaded method or of an implicit, each method gets one point for having more speci\ufb01c arguments, and another point for being de\ufb01ned in a proper subclass. An alternative \u201cwins\u201d over another if it gets a greater number of points in these two comparisons. This means in particular that if alternatives have identical argument types, the one which is de\ufb01ned in a subclass wins.\n\nFor some reason, only the implicit defined in the `DetachMagnet` object is caught.\n\nAm I doing something stupid?\n\nedit: I've created a gist to highlight the problem: https://gist.github.com/andrebeat/6084682\n. Done. \n\nI tried a similar approach but couldn't get it work. I guess its magnets all the way down!\n. Thanks. I'm already on the lookout for the next issue ;-)\nCheers.\n. I agree with all the issues you've raised. To be honest, I looked very briefly through the codebase for a way to encode parameters, but I guess I gave up too easily ;-)\n\nI'll implement your suggestions tonight and I'll rewrite the commit messages.\n. Done.\n\nI've kept the test and implementation into two separate commits, if you prefer I can squash them.\n\nBtw, I rebased and then forced the push. I'm not sure this is the best way to share my changes since it deletes all the previous source comments which were useful for the discussion. Do you have any suggestions?\n. Done. Cheers!\n. Moved the `CharPredicate` to the companion object.\n. Yes, the error message can be more explicit. I'm in favor of making the error message as consistent as possible with the existing error messages. I guess I'll have to look at other error messages ;-)\n. @jrudolph Regarding the IE6-8 bug I guess it would allow you to create 'phising' URLs, e.g. `http://your-social-network.com/home.html?callback='<html><head><script>alert(\"stealing your cookies\")</script>...'`. Supposedly, IE will ignore the `Content-Type` and infer `text/html` from the file extension (I haven't reproduced this bug with IE). \n\nThere's also another attack vector if the callback parameter is not static, and is somehow derived from user-input. You could inject javascript by exploiting this mechanism while still not controlling the content of the site. Still, I think this scenario is very improbable.\n\nI agree with you that this is probably not a very important issue since the exploit scenarios are very flimsy and JSONP is insecure in itself. \n. The documentation in the site is tracking specific milestone versions (e.g. 1.2-M8, 1.1-M8, etc.). Maybe we should add nightly versions for each supported release, i.e. 1.0-nightly, 1.1-nightly and 1.2-nightly.\n. I'm not familiar with the `site` task, but given your changes would this produce a different nightly / master version? Or will it update the documentation for each existing version with the documentation from the nightly releases?\n\nIf the latter is true we'll end up with documentation that doesn't reflect the distributed JARs. For example, if you're using the 1.2-M8 version you can only use `detachTo` and not `detach`. Only if you're using a nightly version would you be able to use this directive (for whatever \"minor version\" since as you said they're mostly identical).\n. I agree with your proposal then! There's the downside you mentioned, but keeping track of older nightlies would probably require a different approach. Still, your proposal already improves on the current state.\n. That sounds like a good reasonable plan to me! Do you want me to keep this PR open so we can look into it in the future or should we close it now?\n. I think this can be closed.\n. Doesn't this issue conflict with #219?\n. Hey Mathias,\nI just created a new pull request with the changes you suggested (https://github.com/spray/spray/pull/295).\n. Thanks, I'll certainly do that next time ;-)\n. Thanks for the tips. I just pushed the changes you suggested.\n. I think it makes sense to rename it to `detach`, especially if we can forego the explicit `ExecutionContext` as you suggested. Additionally, if we rename it to `detach` we should still support the old `detachTo` directive so that we don't break the API but instead deprecate its usage and direct users to the new `detach` directive.\n\nI was going to ask you exactly what magnets you'd want, thanks for clarifying beforehand ;-)\n\nI'll implement your suggestions when I get home.\n. Updated, let me know what you think. Tell me if you want all these changes into a single commit or how should I break them, so that I can redo the commit messages.\n\nCheers!\n. Done. Added a short description / migration guide. Let me know if you think it could be improved.\n. > Also, in order to avoid ambiguity with the fromUnitWithExecutionContext branch we need to move it into a DetachMagnetLowerPrioBranches superclass.\n\nI can't get the following to work: \n\n``` scala\nclass DetachMagnet(implicit val ec: ExecutionContext)\n\nobject DetachMagnet extends LowerPriorityDetachMagnet {\n  implicit def fromUnitWithExecutionContext(u: Unit)(implicit ec: ExecutionContext) = new DetachMagnet\n}\n\nprivate[routing] abstract class LowerPriorityDetachMagnet {\n  implicit def fromUnitWithRefFactory(u: Unit)(implicit refFactory: ActorRefFactory) =\n    new DetachMagnet()(refFactory.dispatcher)\n}\n```\n\nI guess the idea of moving the implicit to a superclass was according to the following rule:\n\n> When comparing two different applicable alternatives of an overloaded method or of an implicit, each method gets one point for having more speci\ufb01c arguments, and another point for being de\ufb01ned in a proper subclass. An alternative \u201cwins\u201d over another if it gets a greater number of points in these two comparisons. This means in particular that if alternatives have identical argument types, the one which is de\ufb01ned in a subclass wins.\n\nFor some reason, only the implicit defined in the `DetachMagnet` object is caught.\n\nAm I doing something stupid?\n\nedit: I've created a gist to highlight the problem: https://gist.github.com/andrebeat/6084682\n. Done. \n\nI tried a similar approach but couldn't get it work. I guess its magnets all the way down!\n. Thanks. I'm already on the lookout for the next issue ;-)\nCheers.\n. I agree with all the issues you've raised. To be honest, I looked very briefly through the codebase for a way to encode parameters, but I guess I gave up too easily ;-)\n\nI'll implement your suggestions tonight and I'll rewrite the commit messages.\n. Done.\n\nI've kept the test and implementation into two separate commits, if you prefer I can squash them.\n\nBtw, I rebased and then forced the push. I'm not sure this is the best way to share my changes since it deletes all the previous source comments which were useful for the discussion. Do you have any suggestions?\n. Done. Cheers!\n. Moved the `CharPredicate` to the companion object.\n. Yes, the error message can be more explicit. I'm in favor of making the error message as consistent as possible with the existing error messages. I guess I'll have to look at other error messages ;-)\n. @jrudolph Regarding the IE6-8 bug I guess it would allow you to create 'phising' URLs, e.g. `http://your-social-network.com/home.html?callback='<html><head><script>alert(\"stealing your cookies\")</script>...'`. Supposedly, IE will ignore the `Content-Type` and infer `text/html` from the file extension (I haven't reproduced this bug with IE). \n\nThere's also another attack vector if the callback parameter is not static, and is somehow derived from user-input. You could inject javascript by exploiting this mechanism while still not controlling the content of the site. Still, I think this scenario is very improbable.\n\nI agree with you that this is probably not a very important issue since the exploit scenarios are very flimsy and JSONP is insecure in itself. \n. The documentation in the site is tracking specific milestone versions (e.g. 1.2-M8, 1.1-M8, etc.). Maybe we should add nightly versions for each supported release, i.e. 1.0-nightly, 1.1-nightly and 1.2-nightly.\n. I'm not familiar with the `site` task, but given your changes would this produce a different nightly / master version? Or will it update the documentation for each existing version with the documentation from the nightly releases?\n\nIf the latter is true we'll end up with documentation that doesn't reflect the distributed JARs. For example, if you're using the 1.2-M8 version you can only use `detachTo` and not `detach`. Only if you're using a nightly version would you be able to use this directive (for whatever \"minor version\" since as you said they're mostly identical).\n. I agree with your proposal then! There's the downside you mentioned, but keeping track of older nightlies would probably require a different approach. Still, your proposal already improves on the current state.\n. That sounds like a good reasonable plan to me! Do you want me to keep this PR open so we can look into it in the future or should we close it now?\n. I think this can be closed.\n. ",
    "2beaucoup": "This can be closed.\n. From a quick look it seems that we could extend `spray.can.client.ClientConnectionSettings` with\n- proxyHost\n- proxyPort\n- nonProxyHosts\n\nand fall back to the respective `-Dhttp(s).*` VM-params.\n\nYou can take a look at the [implementation](http://www.docjar.com/html/api/org/apache/commons/httpclient/HttpConnection.java.html#681) in `commons.httpclient`.\n\nIt gets tricky when authentication comes into play. Then `RequestBuilding.addProxyCredentials` should be added ([spec](http://tools.ietf.org/html/rfc2616#section-14.33)) and `spray.http.StatusCodes.ProxyAuthenticationRequired` has to be handled.\n\nI could take a shot when I find some time and Age doesn't beat me to it. WDYT?\n. I did some experiments and added proxy info to `HostConnectorSettings` and then patched `HostConnectorSetup.normalized` to replace the host and port. That seemed to do the trick because the connectors in `HttpManager` are keyed by their normalized `HostConnectorSetup`s.\n\nThis approach doesn't work too well with the request level API since I have to always pass the `HostConnectorSetup` with the request to make it work. I would have to put some proxy logic into `HttpHostConnector`/`HttpManager` to make this work more elegantly.\n\nWould something like this work?\n. One more question: Why is `HttpManager` making all URIs relative ([code](https://github.com/spray/spray/blob/master/spray-can/src/main/scala/spray/can/HttpManager.scala#L48))?\n. I added basic proxy support in https://github.com/spray/spray/pull/432.\n\nAnybody care to review/test? Thx!\n. @jrudolph Take your time! \n\nAs this was an often requested feature maybe some of the other commenters would like to take a look in the meantime...\n. :+1: This approach is more along the lines of what I originally had in mind.\n\n@jrudolph: Could you re-check your last two examples? They seem identical.\n\n@agemooij Yes, that's the plan. ;)\n\nI think I'll take a look at this and come back around next week.\n. Got it. I got confused by the term \"override\".\n\nWhat would happen if I send a relative requests without a `Host`header to this connector? One advantage of having proxy info in the `HttpHostConnector` would be the ability to fail early in this case.\n\nThe current impl would insert the proxy host/port when the host header is missing and leave the uri as is.\n\nWhat about `ProxyConnectorSetup(\"proxy.example.org\", 8000)` which would return a `ProxiedHostConnector`?\n. I took another shot at adding this feature. See #490.\n\n> One note: This will only work for non-HTTPS connections!\n> In order to support proxied HTTPS we need a much more involved logic, which we currently do not want to get started with.\n\nThat logic would be:\nA. connect to the proxy\nB. send\n\n```\nCONNECT www.example.com:443 HTTP/1.1\nHost: www.example.com:443\n\n```\n\nC. wait for\n\n```\n200 OK\n\n```\n\nD. establish SSL/TLS connection\n. After more thinking about SSL support I came up with the following plan:\n- add `HttpMethods.CONNECT` (could be Part of #428)\n- allow URIs to be empty in `HttpRequest` (add `allowEmptyUri: Boolean = false` to `HttpMethod`?)\n-  if encrypted, add an intermediate state `connecting` to `ProxiedHostConnector` that sends `CONNECT` and waits for `OK`\n- make sure that `HttpManager.hostConnectorFor()` returns a new connector for encrypted connections when the proxy is the same but the target host is different\n\nI think to have _full_ proxy support would be even cooler and avoid confusions.\n\nWDYT?\n. See https://github.com/akka/akka/issues/16153 for HTTPS support.\n. This can be closed.\n. :trollface:\n. Looks good Mathias. I have two questions: Why didn't you use `PimpedString`'s `fastSplit`? And wouldn't this be an example for the `+` commit prefix?\n. Perfect. Thx again for your explanations.\n. Your commit is answering a whole lot of questions I had in my pipeline.\n\nI've cherry-picked and will look at adding more tests.\n\nThx, Mathias.\n. Will do.\n\nI'm currently at a Win7 box and get a lot of EOL related [test failures](https://gist.github.com/2beaucoup/6071283). Is this known? I tried several combinations of `\\n\\r` for EOL and for `stripMargin`/`replace` but no succes.\n. I thought of going forward in small steps and squash later. Would you like to keep your polish patch separate?\n\nFor now I'm trying to fix the EOL related tests on Windows. Will be back at my Mac in a Week...\n. Merge as is without the tests?\n\nAnd again: Would you like to keep your polish patch separate or should I sqash the whole thing?\n. No prob. Done.\n. How could I forget...\n\nDone.\n. :+1: \n. Sure. I'll come up with a seperate PR.\n\nRegarding the proxy credentials in `ReuqestBuidling`: This way we could support interactive authentication scenarios. A static proxy credential config is nice but maybe not preferred in all situations. It also looked consistent with `addCredentials`.\n\nEither way since it's only a wrapper around `addHeader` I could just remove it.\n. As suggested by @jrudolph credentials support was removed from `RequestBuilding`.\n. `ProxyAuthenticateHeader` extends `WwwAuthenticateHeader` and `ProxyAuthorizationHeader` extends `AuthorizationHeader`. I'd have to move these to `SimpleHeaders` as well or mix them in.\n\nWDYT?\n. redundant tests removed\n. @sirthias: I did some refactorings. WDYT?\n. Cool. You're welcome. :sunglasses:\n. Thx for the review Mathias.\n\nI added a commit that should fix all the problems you mentioned. I will squash after your final thumbs up.\n. I also updated the PR description wich I will add to the commit msg after squashing.\n. I did the squash.\n\nA general question: Should I squash right away after every iteration? I thought the updates and comments would be more comprehensible if I wait with the squash right until merging.\n\nThx.\n. Wouldn't this also be useful for normal authentication? We could simply add default headers to the `HostConnectorSetup`.\n. > I'd like to discuss the lifetime of ProxiedHostConnector in relation to its underlying HttpHostConnector. I think what is missing is the DemandIdleShutdown logic on the ProxiedHostConnector level to avoid race-conditions and lost requests due to the underlying HttpHostConnector during shutdown.\n\nCurrently the underlying `HttpHostConnector` is just watched and the `ProxiedHostConnector` is stopped when it sees a `Terminated` msg from the connector. All other messages are just forwarded.\n\nIf `DemandIdleShutdown` needs special handling I'd love some advise for how this should be done.\n\n> > We could simply add default headers to the `HostConnectorSetup`.\n> \n> That could be one solution.\n\nShould I give that a try? The `ProxiedHostConnector` would then \"consume\" `Proxy-Authenticate` headers from the setup.\n. Another question would be if the credentials should be read from config or if applications should be responsible for supplying these. [Googling for http.proxyPassword](https://www.google.com/search?q=%22http.proxyPassword%22) suggests that this is common practice but I'd rather prefer to keep sysprops and `ProxySettings` free from credentials.\n. I added the default header stuff that allows to submit proxy authorization for all requests.\n\nWDYT?\n. here we go...\n. I didn't expect otherwise from you guys as these are rather invasive changes.\n\nAt least one less argument for _not_ using spray. :)\n\nWould be cool if you could give me some directions on how to move forward with TLS support. I already sketched out a solution in #102.\n\nCheers and thanks!\n. No worries Mathias. I'll give you a break for now.\n\nMaybe I'll come up with a proposal for the CONNECT stuff in the meantime. Let's see...\n\nBTW: Is [this list](https://github.com/spray/spray/issues?direction=desc&milestone=3&page=1&sort=created&state=open) up to date?\n. Oops, I meant it the other way around: `<actual> === <expected>` which would be the standard. Most of the tests in this file (as well as others) are written in this style. When a test fails the failure msg would be mixed up:\n\n```\nexpected: <actual>\nactual: <expected>\n```\n. The expected/actual test values are switched once again in that last commit.\n\nOtherwise nice additions!\n. Good stuff Mathias.\n\nI've one remark: `CONNECT` requests don't contain an URI which is currently [mandatory](https://github.com/spray/spray/blob/master/spray-http/src/main/scala/spray/http/HttpMessage.scala#L120) in `HttpRequest`. Should we add `allowEmptyUri: Boolean = false` to `HttpMethod`?\n. It's just the _path_ part of the URI that should be ommited.\n\nAlright then. Thx!\n. Just for the record, test output on windows currently looks like [this](https://gist.github.com/2beaucoup/7302003).\n. I'm also in favour of `System.getProperty(\"line.separator\")`. We just need to take care to replace the output of `stripMargin` consistently in all test expectations.\n\nBTW: `core.autocrlf` was set to `true` with my mingw32 installation.\n. All spray rejections and status messages currently aren't localized. We could change the [current impl](https://github.com/spray/spray/blob/master/spray-httpx/src/main/scala/spray/httpx/unmarshalling/BasicUnmarshallers.scala#L52) for xml parsing to also default to English by changing it to\n\n``` scala\nval parser = XML.parser\nparser.setProperty(\"http://apache.org/xml/properties/locale\", Locale.ROOT)\nXML.withSAXParser(parser).load(new InputStreamReader(new ByteArrayInputStream(data.toByteArray), contentType.charset.nioCharset))\n```\n\nThis way we can force them to be in English and therefore consistent within spray.\n\nUnfortunately this doesn't seem to work an JDK 6 where the output of\n\n```\njava com.sun.org.apache.xerces.internal.impl.Constants\n```\n\nis missing `http://apache.org/xml/properties/locale`.\n\nQuestion is if we should rely on implementation details (error messages) of the runtime in the test suite.\n. Fine with me. Let's discuss the overall strategy in a separate ticket: #732.\n. I added a commit that fixes the test regardless.\n\nI still think it's good practice to not rely too much on JVM impl details.\n\nPick what looks best for you.\n. rebased\n. Thanks for your thoughts Johannes.\n\nThe conclusions I draw from this:\n- Don't use multiline strings except for tests which are compiled and run almost always in combination and on the same machine.\n- When using them always explicitly specify the newline encoding.\n- Make sure that `System.getProperty(\"line.separator\")` and the newlines in your source files are consistent. For example by using `core.autocrlf = true` on Windows.\n\nI updated this PR to make sure the first two points are followed within spray by changing multiline strings in `FileAndResourceDirectives` to `\\n` and making sure all multiline strings in tests are followed by`stripMargin` and `replaceAll(EOL, ...)`\n\nBTW: No double replaces a la `replace(EOL, \"\\n\").replace(\"\\n\", \"\\r\\n\")` are required any more.\n\nThe result is a green test suite even on non-Unix machines. :)\n. I just removed the newlines from the directory index all together.\n. Convinced. `System.getProperty(\"line.separator\")` a.k.a. `spray.util.EOL` is gone now.\n\n`stripMargin` is only used for declaring multipart content (parsed by mimepull), json samples and config samples. Should I leave them as they are?\n. commit message fixed accordingly\n. Sure Mathias.\n\nWhen a server sends `SetCookie` headers they get rendered as:\n\n```\nSet-Cookie: name1=value1; path=/; expires=Tue, 20-Dec-2033 09:11:27 GMT; secure; HttpOnly\nSet-Cookie: name2=value2; path=/; expires=Tue, 20-Dec-2033 09:11:27 GMT; secure; HttpOnly\n```\n\nWhen a client sends the `Cookie`s back they should be rendered as:\n\n```\nCookie: name1=value1; name2=value2\n```\n\nWhat spray currently renders is:\n\n```\nCookie: name1=value1; path=/; expires=Tue, 20-Dec-2033 09:11:27 GMT; secure; HttpOnly; name2=value2; path=/; expires=Tue, 20-Dec-2033 09:11:27 GMT; secure; HttpOnly\n```\n\nSo individual `HttpCookie`s must be rendered differently in `SetCookie` and `Cookie` headers: The `Cookie` header should just contain a list of key-value-pairs instead of also including all attributes (`path`, `expires`, ...)\n\nTake a look at the [parser](https://github.com/spray/spray/blob/master/spray-http/src/main/scala/spray/http/parser/CookieHeaders.scala) where `SetCookie` parses `CookieAttrs` while `Cookie` parses `CookiePair`s.\n\nThis patch overrides the `Cookie` renderer to just render cookie-pairs.\n\nBut it could also be that I'm completely off with this. I'm just pretending to know what I'm doing here. ;)\n\nChristmas time == cookie time!\n. > Let us think a bit about how to solve it best.\n\nI'll just assume that I'm included in _we_. ;) Right now I can think of the following solutions:\n- custom renderer (this PR, binary compatible?)\n- change `Cookie.appy()` to remove all attributes from `HttpCookie`s  (binary compatible)\n- use `HttpCookie` only in `SetCookie` and introduce `CookiePair` in `Cookie` (not binary compatible, maybe with a legacy constructor for `HttpCookie`->`CookiePair` conversion from above)\n\nEither way: it's not urgent and there are easy workarounds but the API currently permits sending of invalid cookie headers. Therefore received cookies should be resend with caution.\n\nI'll just wait and see what you guys come up with.\n. Fine by me.\n. > @2beaucoup fixing the backwards compatibility thing. About the require/assert, it's not useful in the Connect since it's impossible to have an empty array, InetAddress.getAllByName either resolves to at least an array of one or throws an exception.\n\nThen I'd just remove the msg.\n\n> On a different matter, do you have an idea on what may be making travis crash on openjdk and not oracle's?\n\nLet's just wait for the next test run. I saw some glitches in that area in the past.\n. I don't think this `new HttpClientConnection(...)` will work as actors have to be instantiated via `context.actorOf(...)`.\n\nA test case would really help here...\n\nAs an alternative approach you could use `context.become()` like this:\n\n``` scala\nprivate class HttpClientConnection(connectCommander: ActorRef,\n                                   connect: Http.Connect,\n                                   pipelineStage: RawPipelineStage[SslTlsContext],\n                                   settings: ClientConnectionSettings) extends ConnectionHandler { actor \u21d2\n  import context.system\n  import connect._\n\n  connectTo(remoteAddresses.head)\n\n  context.setReceiveTimeout(settings.connectingTimeout)\n\n  // we cannot sensibly recover from crashes\n  override def supervisorStrategy = SupervisorStrategy.stoppingStrategy\n\n  def receive: Receive = connecting(remoteAddresses.tail)\n\n  private def connecting(remainingAddresses: List[InetSocketAddress]): Receive = {\n    case connected: Tcp.Connected \u21d2\n      context.setReceiveTimeout(Duration.Undefined)\n      log.debug(\"Connected to {}\", connected.remoteAddress)\n      val tcpConnection = sender\n      // if sslEncryption is enabled we may need keepOpenOnPeerClosed\n      tcpConnection ! Tcp.Register(self, keepOpenOnPeerClosed = connect.sslEncryption)\n      context.watch(tcpConnection)\n      connectCommander ! connected\n      context.become(running(tcpConnection, pipelineStage, pipelineContext(connected)))\n\n    case Tcp.CommandFailed(_: Tcp.Connect) \u21d2\n      retryRemainingOrStop(remainingAddresses)\n\n    case ReceiveTimeout \u21d2\n      if (remainingAddresses.nonEmpty)\n        log.warning(\"Configured connecting timeout of {} expired, connecting to {} instead\",\n          settings.connectingTimeout, remainingAddresses.head)\n      else\n        log.warning(\"Configured connecting timeout of {} expired, stopping\", settings.connectingTimeout)\n\n      retryRemainingOrStop(remainingAddresses)\n  }\n\n  private def retryRemainingOrStop(remainingAddresses: List[InetSocketAddress]) = remainingAddresses match {\n    case next :: remaining =>\n      connectTo(next)\n      context.become(connecting(remaining))\n    case _ =>\n      connectCommander ! Http.CommandFailed(connect)\n      context.stop(self)\n  }\n\n  private def connectTo(remoteAddress: InetSocketAddress) = {\n    log.debug(\"Attempting connection to {}\", remoteAddress)\n    IO(Tcp) ! Tcp.Connect(remoteAddress, localAddress, options)\n  }\n\n  ...\n}\n```\n\nThat would introduce the additional `connecting` state.\n\nWDYT?\n. I think some tests wouldn't hurt. ;)\n. You should be able to fix your first spray bug [here](https://github.com/spray/spray/blob/master/docs/documentation/spray-routing/key-concepts/big-picture.rst). ;)\n. Could you please push the button on this one if it LGTY.\n\nLooks like a no-brainer. (Famous last words.)\n. :+1: \n. Nice work Age!\n\nWhat about a more general directive that wraps an inner route and does the check whenever an `If-Modified-Since` is set on the request and `Last-Modified` is set on the response?\n\nThen setting `Last-Modified`-headers would be enough the enable caching for arbitrary resources.\n\nWDYT?\n. I actually gave that idea a try, in case you want to [take a look](https://github.com/2beaucoup/spray/compare/caching).\n. Thanks for taking a look Age.\n\n> AFAIK the value of If-None-Match could also be a wildcard (i.e. *). See http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.26\n\nYes. I'll add support for wildcards and lists.\n\n> Using response mapping would be slightly less performant because the original response is actually produced and then discarded. Static files would always be read into memory, even if they hadn't been changed. I'm not sure how important this is though.\n\nThat doesn't seem to be the case for files. `FileAndResourceDirectives` uses `HttpData(file)` where the file content is copied by the marshaller. Resources are read at completion though. Since 'File.lastModified' is rather cheap it gets added by default ([source](https://github.com/spray/spray/blob/master/spray-routing/src/main/scala/spray/routing/directives/FileAndResourceDirectives.scala#L67-L71)).\n\nThe downside of generic hashing would be that you would have to _always_ look at the content while a custom hashing function could use pre-generated data. How would you handle large chunked files for example? Maybe we can come up with support for generic hashing later.\n\n> We also need a directive that actually generates an ETag for a resource, be it static files or any entity. The usual solution is to take an md5 of the entity body so this should not be that hard.\n\nRight. `respondWithHeader(Etag(yourGeneratedMd5))` is all that is needed. `yourGeneratedMd5` could hash the response on the fly or could look at existing data\n\n> I would still write some more integration-style unit tests to verify that If-Modified-since and If-None-Match work correctly with static files, since that is their main purpose.\n\nI only implemented basic unit test so far while trying to avoid creating static files. First we need to agree on what functionality we want to support.\n\nWDYT?\n. > > That doesn't seem to be the case for files. FileAndResourceDirectives uses HttpData(file) where the file content is copied by the marshaller. Resources are read at completion though. Since 'File.lastModified' is rather cheap it gets added by default (source).\n> \n> The implementation of getFromResource does read the file into memory (source). I guess we could make a version of HttpData that would wrap an inputStream and only actually reads it on demand.\n\nThat could work. We just have to make sure to close it if you don't read it.\n\n> > The downside of generic hashing would be that you would have to always look at the content while a custom hashing function could use pre-generated data. How would you handle large chunked files for example? Maybe we can come up with support for generic hashing later.\n> \n> What kind of pre-generated data did you have in mind? I would say that the default strategy should look at the response data but that you could customize that to your liking.\n\nSay you have a bunch of videos on your server. Then you would generate a `*.md5` for every `*.mpg` and your custom directive would just read the hash from that file.\n\n> It seems most http servers, like Apache, simply take a combination of the last modified timestamp and the file size and MD5 that. File size is just as cheap to compute as last modified AFAIK. But I agree that you should be able to customize how this hash gets generated. I would more go for something like this:\n> \n> ETag {\n>   // produce response\n> }\n> where the ETag directive could use an implicit strategy with the default strategy producing an ETag header if the Last-Modified header is set, combining that with the length of the HttpData in the response into an MD5 hash..\n\nAs this would only work for files we could just change `FileAndResourceDirectives`' `respondWithLastModifiedHeader` to\n\n``` scala\nrespondWithCacheHeaders(timestamp: Long, size: Long)\n```\n\nand add the ETags there. I could add that but would first like to hear @sirthias' opinion on that one (+ `IntputStream`-based `HttpData`).\n. I just pushed an update that corrects the `If-None-Match` model and the caching directives and extends the test suite. I'll save further functionality for later...\n\nYou can pull this stuff into this PR and work from there if you like.\n\nCheers!\n. Having thought a bit more about transparent eager cache handling I came up with the following solution:\n\n``` scala\n  def matchCacheHeaders(request: HttpRequest,\n                        responseCacheHeaders: List[HttpHeader],\n                        weak: Boolean): Boolean = (\n    request.header[`If-None-Match`],\n    responseCacheHeaders.collectFirst { case h: ETag \u21d2 h },\n    request.header[`If-Modified-Since`],\n    responseCacheHeaders.collectFirst { case h: `Last-Modified` \u21d2 h }) match {\n      case (Some(`If-None-Match`(ifNoneMatch)), Some(ETag(etag)), _, _) \u21d2\n        // http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.26\n        (!weak || request.method.isSafe /* TODO: GET/HEAD only? */ ) &&\n        ifNoneMatch.matches(etag, weak)\n      case (_, _, Some(`If-Modified-Since`(ifModifiedSince)), Some(`Last-Modified`(lastModified))) \u21d2\n        // http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.25\n        ifModifiedSince >= lastModified\n      case _ \u21d2 false\n    }\n\n  def clientCache(cacheHeaders: PartialFunction[HttpResponse, List[HttpHeader]] = PartialFunction.empty,\n                  weak: Boolean = false): Directive0 =\n    mapInnerRoute { route \u21d2\n      ctx \u21d2 {\n        def matchesCached(response: HttpResponse) =\n          response.status.isSuccess && matchCacheHeaders(ctx.request, response.headers, weak)\n        route {\n          ctx.withHttpResponseMapped { response \u21d2\n            response.withDefaultHeaders(cacheHeaders(response)) // don't override\n          }.withRouteResponseHandling {\n            case response: HttpResponse if matchesCached(response) \u21d2\n              ctx.complete(NotModified)\n          }\n        }\n      }\n    }\n\n  def fastpathClientCache(cacheHeaders: PartialFunction[HttpRequest, List[HttpHeader]] = PartialFunction.empty,\n                          weak: Boolean = false): Directive0 =\n    mapInnerRoute { route \u21d2\n      ctx \u21d2 {\n        if (matchCacheHeaders(ctx.request, cacheHeaders(ctx.request), weak))\n          ctx.complete(NotModified)\n        else\n          route { ctx }\n      }\n    }\n```\n\n`clientCache` does either add cache Headers to the response or return `NotModified` for matched responses. This can be wrapped by `fastpathClientCache` which does add matching of requests by custom logic.\n\nThis would enable eager and lazy completion of cached client resources and reduce the effort of providing custom cache logic to a minimum. All that would be needed is to supply a function for requests (eager) and/or responses (lazy) that return the respective headers.\n\nThat way you can replace the on-the-fly loading and hashing of expensive resources with a cheap lookup function, e.g. issuing `HEAD` requests to a 3rd party service or looking up timestamps or pre-computed hashes.\n\nI'm on vacation the next week but can come up with a PR afterwards if somebody finds this useful.\n. Thanks Age!\n\nKeep in mind that you need at least the header model changes from https://github.com/2beaucoup/spray/branches/caching.\n\nCheers!\n. I'm working on a more general approach. Could submit a PR tomorrow. That would also include the `ETag/If-None-Match` model changes.\n. > Ok, Andr\u00e9, would you say that your solution supercedes this one or would it be independent of what Age proposes here?\n\nIt supercedes this PR.\n. see #797 \n. > What was wrong here in the end?\n\nMissing parser impl I guess. The test output is confusing. A string is compared to a header object with the same string representation.\n. Thanks for your reviews Age and Mathias. Great suggestions!\n\n> I'm not a big fan of the name clientCache since it doesn't really describe what happens here. I would go for something like ifNotModified or something similar.\n\nI just tried to make the difference in regards to the `cache` directive clear which works server side. `ifNotModified` suggests that this directive only matches on modified resources and otherwise rejects but maybe that's just me. If no better suggestions come up I'll change to `IfNotModified`. (Update: See below.)\n\nAnother question is if we should move this stuff to a new `*CacheDirectives` trait altogether which we can mix into the `Directives` trait by default.\n\n> I would add a few tests that specifically show this directive in combination with the file/resource directives\n\nI'm not a big fan of temp files in unit tests. What kind of functionality do you think isn't covered?\n\n> I would probably add a default implementation of ETags for files/resources. That's the one area where these features are most sorely missed, at least for the apps I'm currently working on.\n> I do realize that a proper default ETag implementation is not trivial to do though, looking at for instance the discussion in this Apache issue it seems that ETags need to be different for Gzipped vs unGzipped content so maybe that's a step too far for this pull request.\n\nI'm not yet convinced to put ETag support into `FileAndResourceDirectives` by default (yet).\n\nA custom impl could look like that BTW:\n\n``` scala\ndef myCustomEtag(fileName: String, offset: Long = 0, length: Long): String = ...\n\ndef filesWithEtag: Directive0 = mapHttpResponse {\ncase r @ HttpResponse(_, HttpEntity.NonEmpty(_, HttpData.FileBytes(fileName, offset, length)), _, _) =>\n  r.withDefaultHeaders(List(ETag(HttpEntityTag(myCustomEtag(fileName, offset, length)))))\ncase x => x\n}\n```\n\n> The spec says (RFC2616 14.26) that, if the If-None-Match tags match, the server \"MUST NOT perform the requested method\". However, we are unconditionally calling our inner route here, which will always perform the requested method. This means that we'll potentially execute PUTs (etc.) when we are not allowed to!\n\nYeah that's true. How would you attempt to fix that?\n\nI experimented with transforming the request to `HEAD` and looking and the response headers. That would only work if `head` is explicitly implemented (without `transparent-head-requests`).\n\n> That same spec paragraph also mandates that, in certain cases, \"the server MUST respond with a status of 412 (Precondition Failed)\", which we are not doing anywhere here.\n\nI somehow misread that paragraph. I will change the current impl to reflect it correctly.\n\n> It seems to me that we are combining two only loosely connected things into one single directive: If-Modified-Since support and If-None-Match support. I think we should split them apart into two separate directives. I also like Age's idea of naming the directives similar to their headers, so maybe withIfModifiedSinceSupport / withIfNoneMatchSupport. Just calling them ifModifiedSince kind of implies that its inner route is only executed in some cases, which it isn't. We'll always have to execute the inner route, maybe even several times.\n> With the basic directives in place we can still think about adding higher level directives which then combine the more basic ones in some form\n\nFrom the spec:\n\n> \"... the server MUST NOT perform the requested method, unless required to do so because the resource's modification date fails to match that supplied in an If-Modified-Since header field in the request\"\n\nI think the directives should be tied together because if `withIfNoneMatchSupport` matches, a lower level `withIfModifiedSinceSupport` wouldn't see the request and validate its `If-Modified-Since`.\n\nI suggest `withIfNotModifiedSupport` or `withCacheSupport`.\n\n> Apart from the inline comments I'd also agree with Age on somehow adding If-Modified-Since / If-None-Match support to the FileAndResourceDirectives, since this is something that people can benefit from without the need to change anything about their existing routing structure.\n\nBut they could also get confused if their routes suddenly start to return `304` which they didn't expect. I like the idea better to explicitly enable caching for (sub-) routes via a special directives instead of stuffing it into several directives by default.\n\nWDYT?\n\nThanks guys!\n. > Yes, I see the connection between the two.\n> How about we solve it by making withIfModifiedSinceSupport only kick in if the request does not also contain an If-None-Match header?\n\nBut couldn't a `withIfNoneMatchSupport` directive could have already completed before a lower level `withIfModifiedSinceSupport` directive gets the chance to look at the request (and execute its inner route if necessary)?\n. I don't get it. As far as i understand the spec correctly a server should only respond with `304` for requests with `If-Modified-Since` _and_ `If-None-Match` if _both_ are matched.\n\nLet's assume we have a route like this:\n\n``` scala\nwithIfNoneMatchSupport(etagForRequest) {\n  withIfModifiedSinceSupport(timestampForRequest) {\n    completeWithEtagAndTimestamp()\n  }\n}\n```\n\nand a request with `If-Modified-Since: ...` and `If-None-Match: ...`\ncomes in. How would I make sure that\n- `withIfNoneMatchSupport` doesn't reply with `304` when `withIfModifiedSinceSupport` would have completed normally\n- `withIfModifiedSinceSupport` doesn't reply with `304` if  `withIfNoneMatchSupport` didn't match\n\n?\n. Nice overview Mathias!\n\nI agree with most of what you said and updated this PR with a first draft (excluding `FileAndResourceDirectives` for now).\n\nOne problem remains: How would one communicate errors, e.g. the resource doesn't exist anymore?\n\nBTW: [This](https://github.com/apache/httpd/blob/trunk/modules/http/http_protocol.c#L528-L633) is the conditional impl in Apache httpd and [here](https://github.com/apache/httpd/blob/454a28fac363b20206d2c54939ac85cced8c9bfe/modules/http/http_etag.c#L112-L144) is the impl for file etags.\n. I agree with all of your comments (and will fix them).\n\nMeanwhile I've some more food for thought:\n\nHaving the `conditional` near the leafs of the route limits us in determining how a resource _would have_ been presented to the client. This prevents us from using strong ETags. We could combine the provided ETag with all presentation relevant headers or simply falll back to weak ETags. In general: The deeper in the route you are, the more could potentially happen afterwards... But I'm just thinking out loud here.\n\nAs a general question: Is HTTPbis preferred over rfc2616 for spray? I've seen some references to HTTPbis on the mailing list but for conditionals they seem to differ significantly. For example:\n\n[RFC2616 section 14.26 If-None-Match](http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.26):\n\n> If any of the entity tags match the entity tag of the entity that would have been returned in the response to a similar GET request (without the If-None-Match header) on that resource, or if \"*\" is given and any current entity exists for that resource, then the server MUST NOT perform the requested method, <b>unless required to do so because the resource's modification date fails to match that supplied in an If-Modified-Since header field</b> in the request. Instead, if the request method was GET or HEAD, the server SHOULD respond with a 304 (Not Modified) response, including the cache- related header fields (particularly ETag) of one of the entities that matched. For all other request methods, the server MUST respond with a status of 412 (Precondition Failed).\n\nNo mention of `If-None-Match` in the `If-Modified-Since` section.\n\n[HTTPbis Conditional Requests section 3.3 If-Modified-Since](http://tools.ietf.org/html/draft-ietf-httpbis-p4-conditional-26#section-3.3):\n\n> <b>A recipient MUST ignore If-Modified-Since if the request contains an If-None-Match header field;</b> the condition in If-None-Match is considered to be a more accurate replacement for the condition in If-Modified-Since and the two are only combined for the sake of interoperating with older intermediaries that might not implement If-None-Match.\n\nNo mention of `If-Modified-Since` in the `If-None-Match` section.\n\nI've stumbled upon https://raw.github.com/for-GET/http-decision-diagram/master/httpdd.png which seems to prefer the latter.\n\nThe more I study the spec(s) the more confusing it gets. :confused:\n\nWDYT?\n. Thanks guys! Regarding Berlin: I'll see what I can do....\n\nI've fixed the review comments and will update the impl in a later commit following our discussion.\n\nGreat find Age. I didn't get this far yet and this section seems to sum up the conditional flow quite nicely.\n\nSeems like there's a light at the end of the tunnel. :+1: \n. The failing tests show our new ETags already at work. ;)\n. Should `withRangeSupport` wrap `conditional` or the other way around?\n. Currently merging with master. Should I wait?\n. squashed\n. :+1: Nice weekend!\n. I added some updates that should fix the remaining issues.\n. Done. This one was supposed to get squashed but can as well stand on its own.\n\nNice collaborative effort. Thanks to everyone involved!\n. :+1: What about `[optional]HeaderValuesByType` for extracting several values at once?\n. You would have to specify multiple types. It would have to be correctly named `[optional]HeaderValuesByTypes`. Sorry.\n. Agreed. Thanks for explaining.\n. PRs are not a replacement for tickets. Please close this ticket only after the PR got merged. (This will happen automagically ff you append a `fixes #815` to your commit msg.)\n. `ValidationRejection` seems to be used in a variety of validation related cases, not just parameter validation. What's special about entity content validation? `MalformedRequestContentRejection` would still get thrown for failures not related to validation.\n. @jrudolph: Care to elaborate on why/when/how you'd want to access these additional infos?\n\nMy goal is to end [here](https://github.com/spray/spray/blob/master/spray-routing/src/main/scala/spray/routing/RejectionHandler.scala#L114) instead of [here](https://github.com/spray/spray/blob/master/spray-routing/src/main/scala/spray/routing/RejectionHandler.scala#L62) when a requirement fails (a.k.a. `IllegalArgumentException`).\n\nDo you think that say `EntityValidationRejection`, `ParameterValidationRejection` etc. are necessary?\n. I experimented with this [here](https://github.com/2beaucoup/spray/compare/case-class-validation).\n. :+1: \n. updated\n. Cool. Nice Weekend! :sunglasses:\n. All headers are stored lowercase in spray's http model.\n\n> `case HttpHeader(\"content-type\", \"application/json; charset=UTF-8\") => true`\n\nWhat's your use case here? Why don't you match like this:\n\n`case HttpHeaders.`Content-Type`(ContentTypes.`application/json`) => ...`\n\nBTW: You can as well specify the desired header class for a message:\n\n`httpMessage.header[`Content-Type`]`\n\nIf you work with the routing DSL you should take a look at the `HeaderDirectives` which do the conversions for you.\n\nHTH.\n. > To me, having to type qualified spray directives is like an overkill for such operation.\n\nUsing `HeaderDirectives` was just _one_ alternative. Have you tried the others I mentioned?\n\nBTW:\n\n``` scala\nheaderValueByName(\"Header-Name\") { headerValue =>\n  // complete(s\"header-name: $headerValue\")\n}\n```\n\ndoesn't require too much typing IMHO.\n. All [header directives](http://spray.io/documentation/1.2.1/spray-routing/header-directives/) are generic. While `headerValue` takes the header type as argument `headerValueByName` takes a string. Just fill your header name into the example I gave above:\n\n``` scala\nheaderValueByName(\"X-Forwarded-Proto\") { xForwardedProto =>\n  optionalHeaderValueByName(\"My-Custom-Header\") { => myCustomHeader\n    // use xForwardedFor and optional myCustomHeader here\n  }\n}\n```\n\nAs you can see there are no additional types involved and it works regardless of the casing in wich the headers are submitted.\n. Caching doesn't work together with chunking. Could you try with `spray.routing.file-chunking-threshold-size = 0`?\n\nYou should provide a `CacheKeyer` that takes the accepted encodings into account or use `compressResponseIfRequested` and move it outside of the `cache` directive. This way you are able to serve clients that don't support compression.\n. > But doesn't it disable chunking completely?\n\nThat's right. You currently have to choose between chunking and caching. You could use client side caching instead. The `FileAndResourceDirectives` already [supports](https://github.com/spray/spray/blob/5289cfcdc9cd689152c5fc5c86f8cf4d7e4a95ad/spray-routing/src/main/scala/spray/routing/directives/FileAndResourceDirectives.scala#L70) this.\n. > Just read this https://groups.google.com/forum/#!topic/spray-user/mL4sMr1qfwE ... No it looks like I better remove server cache completely.\n\nOK cool.\n\n> By client-side caching you mean using Last-Modified header?\n\n`Last-Modified` and `ETag` headers. Take a look at the `conditional` directive.\n. :+1: You're welcome.\n. `FormData` can be marshalled as `multipart/form-data` and `application/x-www-form-urlencoded` where only the latter uses escaping.\n\nThis is currently tested [here](https://github.com/spray/spray/blob/master/spray-httpx/src/test/scala/spray/httpx/marshalling/BasicMarshallersSpec.scala) and [here](https://github.com/spray/spray/blob/master/spray-httpx/src/test/scala/spray/httpx/marshalling/MultipartMarshallersSpec.scala).\n. If you add tests for this in `spray.http.HttpHeaderSpec` you'll find out that the parsing functionality is missing. Take a look at `spray.http.parser.SimpleHeaders`.\n. One more thing to keep in mind here is that HTTPbis allows for relative redirection URIs.\n\nThat being said I'd propose to handle the Redirection responses in `HttpHostConnector` and bubble it up to `ProxiedHostConnector` and `HttpManager` as long as the redirection URI isn't relative or doesn't match the connector's host.\n\n`ProxiedHostConnector` should therefore be changed to `tell` instead of `forward` in order to be able to look at responses.\n\nIf I understand the current logic right then even responses for connection level requests are redirected. That might be unexpected behaviour.\n\nWDYT?\n. The original problem here was the reverse order after client side _parsing_ of response headers.\n\nThere are several tasks here:\n- prioritize rendering order of response headers managed by spray (`Server`, `Content-Length`, `Date`, `Connection`, `Transfer-Encoding` others?) and preserve order of remaining headers\n- preserve order of headers during parsing \n- bonus: combine duplicated, collapsible headers during client side parsing and prevent sending of duplicated, uncollapsible headers as spec'd above.\n\nWDYT? Should I give them a try?\n. Only the order after parsing needs to be fixed then.\n\nUser supplied headers are currently rendered in the order they are supplied. Headers \"owned\" by spray come either first or last which shouldn't be a problem and keeps the impl sane.\n. Test failures seem unrelated.\n. Are these tests already ported to akka-http?\n. This can be closed.\n. From a quick look it seems that we could extend `spray.can.client.ClientConnectionSettings` with\n- proxyHost\n- proxyPort\n- nonProxyHosts\n\nand fall back to the respective `-Dhttp(s).*` VM-params.\n\nYou can take a look at the [implementation](http://www.docjar.com/html/api/org/apache/commons/httpclient/HttpConnection.java.html#681) in `commons.httpclient`.\n\nIt gets tricky when authentication comes into play. Then `RequestBuilding.addProxyCredentials` should be added ([spec](http://tools.ietf.org/html/rfc2616#section-14.33)) and `spray.http.StatusCodes.ProxyAuthenticationRequired` has to be handled.\n\nI could take a shot when I find some time and Age doesn't beat me to it. WDYT?\n. I did some experiments and added proxy info to `HostConnectorSettings` and then patched `HostConnectorSetup.normalized` to replace the host and port. That seemed to do the trick because the connectors in `HttpManager` are keyed by their normalized `HostConnectorSetup`s.\n\nThis approach doesn't work too well with the request level API since I have to always pass the `HostConnectorSetup` with the request to make it work. I would have to put some proxy logic into `HttpHostConnector`/`HttpManager` to make this work more elegantly.\n\nWould something like this work?\n. One more question: Why is `HttpManager` making all URIs relative ([code](https://github.com/spray/spray/blob/master/spray-can/src/main/scala/spray/can/HttpManager.scala#L48))?\n. I added basic proxy support in https://github.com/spray/spray/pull/432.\n\nAnybody care to review/test? Thx!\n. @jrudolph Take your time! \n\nAs this was an often requested feature maybe some of the other commenters would like to take a look in the meantime...\n. :+1: This approach is more along the lines of what I originally had in mind.\n\n@jrudolph: Could you re-check your last two examples? They seem identical.\n\n@agemooij Yes, that's the plan. ;)\n\nI think I'll take a look at this and come back around next week.\n. Got it. I got confused by the term \"override\".\n\nWhat would happen if I send a relative requests without a `Host`header to this connector? One advantage of having proxy info in the `HttpHostConnector` would be the ability to fail early in this case.\n\nThe current impl would insert the proxy host/port when the host header is missing and leave the uri as is.\n\nWhat about `ProxyConnectorSetup(\"proxy.example.org\", 8000)` which would return a `ProxiedHostConnector`?\n. I took another shot at adding this feature. See #490.\n\n> One note: This will only work for non-HTTPS connections!\n> In order to support proxied HTTPS we need a much more involved logic, which we currently do not want to get started with.\n\nThat logic would be:\nA. connect to the proxy\nB. send\n\n```\nCONNECT www.example.com:443 HTTP/1.1\nHost: www.example.com:443\n\n```\n\nC. wait for\n\n```\n200 OK\n\n```\n\nD. establish SSL/TLS connection\n. After more thinking about SSL support I came up with the following plan:\n- add `HttpMethods.CONNECT` (could be Part of #428)\n- allow URIs to be empty in `HttpRequest` (add `allowEmptyUri: Boolean = false` to `HttpMethod`?)\n-  if encrypted, add an intermediate state `connecting` to `ProxiedHostConnector` that sends `CONNECT` and waits for `OK`\n- make sure that `HttpManager.hostConnectorFor()` returns a new connector for encrypted connections when the proxy is the same but the target host is different\n\nI think to have _full_ proxy support would be even cooler and avoid confusions.\n\nWDYT?\n. See https://github.com/akka/akka/issues/16153 for HTTPS support.\n. This can be closed.\n. :trollface:\n. Looks good Mathias. I have two questions: Why didn't you use `PimpedString`'s `fastSplit`? And wouldn't this be an example for the `+` commit prefix?\n. Perfect. Thx again for your explanations.\n. Your commit is answering a whole lot of questions I had in my pipeline.\n\nI've cherry-picked and will look at adding more tests.\n\nThx, Mathias.\n. Will do.\n\nI'm currently at a Win7 box and get a lot of EOL related [test failures](https://gist.github.com/2beaucoup/6071283). Is this known? I tried several combinations of `\\n\\r` for EOL and for `stripMargin`/`replace` but no succes.\n. I thought of going forward in small steps and squash later. Would you like to keep your polish patch separate?\n\nFor now I'm trying to fix the EOL related tests on Windows. Will be back at my Mac in a Week...\n. Merge as is without the tests?\n\nAnd again: Would you like to keep your polish patch separate or should I sqash the whole thing?\n. No prob. Done.\n. How could I forget...\n\nDone.\n. :+1: \n. Sure. I'll come up with a seperate PR.\n\nRegarding the proxy credentials in `ReuqestBuidling`: This way we could support interactive authentication scenarios. A static proxy credential config is nice but maybe not preferred in all situations. It also looked consistent with `addCredentials`.\n\nEither way since it's only a wrapper around `addHeader` I could just remove it.\n. As suggested by @jrudolph credentials support was removed from `RequestBuilding`.\n. `ProxyAuthenticateHeader` extends `WwwAuthenticateHeader` and `ProxyAuthorizationHeader` extends `AuthorizationHeader`. I'd have to move these to `SimpleHeaders` as well or mix them in.\n\nWDYT?\n. redundant tests removed\n. @sirthias: I did some refactorings. WDYT?\n. Cool. You're welcome. :sunglasses:\n. Thx for the review Mathias.\n\nI added a commit that should fix all the problems you mentioned. I will squash after your final thumbs up.\n. I also updated the PR description wich I will add to the commit msg after squashing.\n. I did the squash.\n\nA general question: Should I squash right away after every iteration? I thought the updates and comments would be more comprehensible if I wait with the squash right until merging.\n\nThx.\n. Wouldn't this also be useful for normal authentication? We could simply add default headers to the `HostConnectorSetup`.\n. > I'd like to discuss the lifetime of ProxiedHostConnector in relation to its underlying HttpHostConnector. I think what is missing is the DemandIdleShutdown logic on the ProxiedHostConnector level to avoid race-conditions and lost requests due to the underlying HttpHostConnector during shutdown.\n\nCurrently the underlying `HttpHostConnector` is just watched and the `ProxiedHostConnector` is stopped when it sees a `Terminated` msg from the connector. All other messages are just forwarded.\n\nIf `DemandIdleShutdown` needs special handling I'd love some advise for how this should be done.\n\n> > We could simply add default headers to the `HostConnectorSetup`.\n> \n> That could be one solution.\n\nShould I give that a try? The `ProxiedHostConnector` would then \"consume\" `Proxy-Authenticate` headers from the setup.\n. Another question would be if the credentials should be read from config or if applications should be responsible for supplying these. [Googling for http.proxyPassword](https://www.google.com/search?q=%22http.proxyPassword%22) suggests that this is common practice but I'd rather prefer to keep sysprops and `ProxySettings` free from credentials.\n. I added the default header stuff that allows to submit proxy authorization for all requests.\n\nWDYT?\n. here we go...\n. I didn't expect otherwise from you guys as these are rather invasive changes.\n\nAt least one less argument for _not_ using spray. :)\n\nWould be cool if you could give me some directions on how to move forward with TLS support. I already sketched out a solution in #102.\n\nCheers and thanks!\n. No worries Mathias. I'll give you a break for now.\n\nMaybe I'll come up with a proposal for the CONNECT stuff in the meantime. Let's see...\n\nBTW: Is [this list](https://github.com/spray/spray/issues?direction=desc&milestone=3&page=1&sort=created&state=open) up to date?\n. Oops, I meant it the other way around: `<actual> === <expected>` which would be the standard. Most of the tests in this file (as well as others) are written in this style. When a test fails the failure msg would be mixed up:\n\n```\nexpected: <actual>\nactual: <expected>\n```\n. The expected/actual test values are switched once again in that last commit.\n\nOtherwise nice additions!\n. Good stuff Mathias.\n\nI've one remark: `CONNECT` requests don't contain an URI which is currently [mandatory](https://github.com/spray/spray/blob/master/spray-http/src/main/scala/spray/http/HttpMessage.scala#L120) in `HttpRequest`. Should we add `allowEmptyUri: Boolean = false` to `HttpMethod`?\n. It's just the _path_ part of the URI that should be ommited.\n\nAlright then. Thx!\n. Just for the record, test output on windows currently looks like [this](https://gist.github.com/2beaucoup/7302003).\n. I'm also in favour of `System.getProperty(\"line.separator\")`. We just need to take care to replace the output of `stripMargin` consistently in all test expectations.\n\nBTW: `core.autocrlf` was set to `true` with my mingw32 installation.\n. All spray rejections and status messages currently aren't localized. We could change the [current impl](https://github.com/spray/spray/blob/master/spray-httpx/src/main/scala/spray/httpx/unmarshalling/BasicUnmarshallers.scala#L52) for xml parsing to also default to English by changing it to\n\n``` scala\nval parser = XML.parser\nparser.setProperty(\"http://apache.org/xml/properties/locale\", Locale.ROOT)\nXML.withSAXParser(parser).load(new InputStreamReader(new ByteArrayInputStream(data.toByteArray), contentType.charset.nioCharset))\n```\n\nThis way we can force them to be in English and therefore consistent within spray.\n\nUnfortunately this doesn't seem to work an JDK 6 where the output of\n\n```\njava com.sun.org.apache.xerces.internal.impl.Constants\n```\n\nis missing `http://apache.org/xml/properties/locale`.\n\nQuestion is if we should rely on implementation details (error messages) of the runtime in the test suite.\n. Fine with me. Let's discuss the overall strategy in a separate ticket: #732.\n. I added a commit that fixes the test regardless.\n\nI still think it's good practice to not rely too much on JVM impl details.\n\nPick what looks best for you.\n. rebased\n. Thanks for your thoughts Johannes.\n\nThe conclusions I draw from this:\n- Don't use multiline strings except for tests which are compiled and run almost always in combination and on the same machine.\n- When using them always explicitly specify the newline encoding.\n- Make sure that `System.getProperty(\"line.separator\")` and the newlines in your source files are consistent. For example by using `core.autocrlf = true` on Windows.\n\nI updated this PR to make sure the first two points are followed within spray by changing multiline strings in `FileAndResourceDirectives` to `\\n` and making sure all multiline strings in tests are followed by`stripMargin` and `replaceAll(EOL, ...)`\n\nBTW: No double replaces a la `replace(EOL, \"\\n\").replace(\"\\n\", \"\\r\\n\")` are required any more.\n\nThe result is a green test suite even on non-Unix machines. :)\n. I just removed the newlines from the directory index all together.\n. Convinced. `System.getProperty(\"line.separator\")` a.k.a. `spray.util.EOL` is gone now.\n\n`stripMargin` is only used for declaring multipart content (parsed by mimepull), json samples and config samples. Should I leave them as they are?\n. commit message fixed accordingly\n. Sure Mathias.\n\nWhen a server sends `SetCookie` headers they get rendered as:\n\n```\nSet-Cookie: name1=value1; path=/; expires=Tue, 20-Dec-2033 09:11:27 GMT; secure; HttpOnly\nSet-Cookie: name2=value2; path=/; expires=Tue, 20-Dec-2033 09:11:27 GMT; secure; HttpOnly\n```\n\nWhen a client sends the `Cookie`s back they should be rendered as:\n\n```\nCookie: name1=value1; name2=value2\n```\n\nWhat spray currently renders is:\n\n```\nCookie: name1=value1; path=/; expires=Tue, 20-Dec-2033 09:11:27 GMT; secure; HttpOnly; name2=value2; path=/; expires=Tue, 20-Dec-2033 09:11:27 GMT; secure; HttpOnly\n```\n\nSo individual `HttpCookie`s must be rendered differently in `SetCookie` and `Cookie` headers: The `Cookie` header should just contain a list of key-value-pairs instead of also including all attributes (`path`, `expires`, ...)\n\nTake a look at the [parser](https://github.com/spray/spray/blob/master/spray-http/src/main/scala/spray/http/parser/CookieHeaders.scala) where `SetCookie` parses `CookieAttrs` while `Cookie` parses `CookiePair`s.\n\nThis patch overrides the `Cookie` renderer to just render cookie-pairs.\n\nBut it could also be that I'm completely off with this. I'm just pretending to know what I'm doing here. ;)\n\nChristmas time == cookie time!\n. > Let us think a bit about how to solve it best.\n\nI'll just assume that I'm included in _we_. ;) Right now I can think of the following solutions:\n- custom renderer (this PR, binary compatible?)\n- change `Cookie.appy()` to remove all attributes from `HttpCookie`s  (binary compatible)\n- use `HttpCookie` only in `SetCookie` and introduce `CookiePair` in `Cookie` (not binary compatible, maybe with a legacy constructor for `HttpCookie`->`CookiePair` conversion from above)\n\nEither way: it's not urgent and there are easy workarounds but the API currently permits sending of invalid cookie headers. Therefore received cookies should be resend with caution.\n\nI'll just wait and see what you guys come up with.\n. Fine by me.\n. > @2beaucoup fixing the backwards compatibility thing. About the require/assert, it's not useful in the Connect since it's impossible to have an empty array, InetAddress.getAllByName either resolves to at least an array of one or throws an exception.\n\nThen I'd just remove the msg.\n\n> On a different matter, do you have an idea on what may be making travis crash on openjdk and not oracle's?\n\nLet's just wait for the next test run. I saw some glitches in that area in the past.\n. I don't think this `new HttpClientConnection(...)` will work as actors have to be instantiated via `context.actorOf(...)`.\n\nA test case would really help here...\n\nAs an alternative approach you could use `context.become()` like this:\n\n``` scala\nprivate class HttpClientConnection(connectCommander: ActorRef,\n                                   connect: Http.Connect,\n                                   pipelineStage: RawPipelineStage[SslTlsContext],\n                                   settings: ClientConnectionSettings) extends ConnectionHandler { actor \u21d2\n  import context.system\n  import connect._\n\n  connectTo(remoteAddresses.head)\n\n  context.setReceiveTimeout(settings.connectingTimeout)\n\n  // we cannot sensibly recover from crashes\n  override def supervisorStrategy = SupervisorStrategy.stoppingStrategy\n\n  def receive: Receive = connecting(remoteAddresses.tail)\n\n  private def connecting(remainingAddresses: List[InetSocketAddress]): Receive = {\n    case connected: Tcp.Connected \u21d2\n      context.setReceiveTimeout(Duration.Undefined)\n      log.debug(\"Connected to {}\", connected.remoteAddress)\n      val tcpConnection = sender\n      // if sslEncryption is enabled we may need keepOpenOnPeerClosed\n      tcpConnection ! Tcp.Register(self, keepOpenOnPeerClosed = connect.sslEncryption)\n      context.watch(tcpConnection)\n      connectCommander ! connected\n      context.become(running(tcpConnection, pipelineStage, pipelineContext(connected)))\n\n    case Tcp.CommandFailed(_: Tcp.Connect) \u21d2\n      retryRemainingOrStop(remainingAddresses)\n\n    case ReceiveTimeout \u21d2\n      if (remainingAddresses.nonEmpty)\n        log.warning(\"Configured connecting timeout of {} expired, connecting to {} instead\",\n          settings.connectingTimeout, remainingAddresses.head)\n      else\n        log.warning(\"Configured connecting timeout of {} expired, stopping\", settings.connectingTimeout)\n\n      retryRemainingOrStop(remainingAddresses)\n  }\n\n  private def retryRemainingOrStop(remainingAddresses: List[InetSocketAddress]) = remainingAddresses match {\n    case next :: remaining =>\n      connectTo(next)\n      context.become(connecting(remaining))\n    case _ =>\n      connectCommander ! Http.CommandFailed(connect)\n      context.stop(self)\n  }\n\n  private def connectTo(remoteAddress: InetSocketAddress) = {\n    log.debug(\"Attempting connection to {}\", remoteAddress)\n    IO(Tcp) ! Tcp.Connect(remoteAddress, localAddress, options)\n  }\n\n  ...\n}\n```\n\nThat would introduce the additional `connecting` state.\n\nWDYT?\n. I think some tests wouldn't hurt. ;)\n. You should be able to fix your first spray bug [here](https://github.com/spray/spray/blob/master/docs/documentation/spray-routing/key-concepts/big-picture.rst). ;)\n. Could you please push the button on this one if it LGTY.\n\nLooks like a no-brainer. (Famous last words.)\n. :+1: \n. Nice work Age!\n\nWhat about a more general directive that wraps an inner route and does the check whenever an `If-Modified-Since` is set on the request and `Last-Modified` is set on the response?\n\nThen setting `Last-Modified`-headers would be enough the enable caching for arbitrary resources.\n\nWDYT?\n. I actually gave that idea a try, in case you want to [take a look](https://github.com/2beaucoup/spray/compare/caching).\n. Thanks for taking a look Age.\n\n> AFAIK the value of If-None-Match could also be a wildcard (i.e. *). See http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.26\n\nYes. I'll add support for wildcards and lists.\n\n> Using response mapping would be slightly less performant because the original response is actually produced and then discarded. Static files would always be read into memory, even if they hadn't been changed. I'm not sure how important this is though.\n\nThat doesn't seem to be the case for files. `FileAndResourceDirectives` uses `HttpData(file)` where the file content is copied by the marshaller. Resources are read at completion though. Since 'File.lastModified' is rather cheap it gets added by default ([source](https://github.com/spray/spray/blob/master/spray-routing/src/main/scala/spray/routing/directives/FileAndResourceDirectives.scala#L67-L71)).\n\nThe downside of generic hashing would be that you would have to _always_ look at the content while a custom hashing function could use pre-generated data. How would you handle large chunked files for example? Maybe we can come up with support for generic hashing later.\n\n> We also need a directive that actually generates an ETag for a resource, be it static files or any entity. The usual solution is to take an md5 of the entity body so this should not be that hard.\n\nRight. `respondWithHeader(Etag(yourGeneratedMd5))` is all that is needed. `yourGeneratedMd5` could hash the response on the fly or could look at existing data\n\n> I would still write some more integration-style unit tests to verify that If-Modified-since and If-None-Match work correctly with static files, since that is their main purpose.\n\nI only implemented basic unit test so far while trying to avoid creating static files. First we need to agree on what functionality we want to support.\n\nWDYT?\n. > > That doesn't seem to be the case for files. FileAndResourceDirectives uses HttpData(file) where the file content is copied by the marshaller. Resources are read at completion though. Since 'File.lastModified' is rather cheap it gets added by default (source).\n> \n> The implementation of getFromResource does read the file into memory (source). I guess we could make a version of HttpData that would wrap an inputStream and only actually reads it on demand.\n\nThat could work. We just have to make sure to close it if you don't read it.\n\n> > The downside of generic hashing would be that you would have to always look at the content while a custom hashing function could use pre-generated data. How would you handle large chunked files for example? Maybe we can come up with support for generic hashing later.\n> \n> What kind of pre-generated data did you have in mind? I would say that the default strategy should look at the response data but that you could customize that to your liking.\n\nSay you have a bunch of videos on your server. Then you would generate a `*.md5` for every `*.mpg` and your custom directive would just read the hash from that file.\n\n> It seems most http servers, like Apache, simply take a combination of the last modified timestamp and the file size and MD5 that. File size is just as cheap to compute as last modified AFAIK. But I agree that you should be able to customize how this hash gets generated. I would more go for something like this:\n> \n> ETag {\n>   // produce response\n> }\n> where the ETag directive could use an implicit strategy with the default strategy producing an ETag header if the Last-Modified header is set, combining that with the length of the HttpData in the response into an MD5 hash..\n\nAs this would only work for files we could just change `FileAndResourceDirectives`' `respondWithLastModifiedHeader` to\n\n``` scala\nrespondWithCacheHeaders(timestamp: Long, size: Long)\n```\n\nand add the ETags there. I could add that but would first like to hear @sirthias' opinion on that one (+ `IntputStream`-based `HttpData`).\n. I just pushed an update that corrects the `If-None-Match` model and the caching directives and extends the test suite. I'll save further functionality for later...\n\nYou can pull this stuff into this PR and work from there if you like.\n\nCheers!\n. Having thought a bit more about transparent eager cache handling I came up with the following solution:\n\n``` scala\n  def matchCacheHeaders(request: HttpRequest,\n                        responseCacheHeaders: List[HttpHeader],\n                        weak: Boolean): Boolean = (\n    request.header[`If-None-Match`],\n    responseCacheHeaders.collectFirst { case h: ETag \u21d2 h },\n    request.header[`If-Modified-Since`],\n    responseCacheHeaders.collectFirst { case h: `Last-Modified` \u21d2 h }) match {\n      case (Some(`If-None-Match`(ifNoneMatch)), Some(ETag(etag)), _, _) \u21d2\n        // http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.26\n        (!weak || request.method.isSafe /* TODO: GET/HEAD only? */ ) &&\n        ifNoneMatch.matches(etag, weak)\n      case (_, _, Some(`If-Modified-Since`(ifModifiedSince)), Some(`Last-Modified`(lastModified))) \u21d2\n        // http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.25\n        ifModifiedSince >= lastModified\n      case _ \u21d2 false\n    }\n\n  def clientCache(cacheHeaders: PartialFunction[HttpResponse, List[HttpHeader]] = PartialFunction.empty,\n                  weak: Boolean = false): Directive0 =\n    mapInnerRoute { route \u21d2\n      ctx \u21d2 {\n        def matchesCached(response: HttpResponse) =\n          response.status.isSuccess && matchCacheHeaders(ctx.request, response.headers, weak)\n        route {\n          ctx.withHttpResponseMapped { response \u21d2\n            response.withDefaultHeaders(cacheHeaders(response)) // don't override\n          }.withRouteResponseHandling {\n            case response: HttpResponse if matchesCached(response) \u21d2\n              ctx.complete(NotModified)\n          }\n        }\n      }\n    }\n\n  def fastpathClientCache(cacheHeaders: PartialFunction[HttpRequest, List[HttpHeader]] = PartialFunction.empty,\n                          weak: Boolean = false): Directive0 =\n    mapInnerRoute { route \u21d2\n      ctx \u21d2 {\n        if (matchCacheHeaders(ctx.request, cacheHeaders(ctx.request), weak))\n          ctx.complete(NotModified)\n        else\n          route { ctx }\n      }\n    }\n```\n\n`clientCache` does either add cache Headers to the response or return `NotModified` for matched responses. This can be wrapped by `fastpathClientCache` which does add matching of requests by custom logic.\n\nThis would enable eager and lazy completion of cached client resources and reduce the effort of providing custom cache logic to a minimum. All that would be needed is to supply a function for requests (eager) and/or responses (lazy) that return the respective headers.\n\nThat way you can replace the on-the-fly loading and hashing of expensive resources with a cheap lookup function, e.g. issuing `HEAD` requests to a 3rd party service or looking up timestamps or pre-computed hashes.\n\nI'm on vacation the next week but can come up with a PR afterwards if somebody finds this useful.\n. Thanks Age!\n\nKeep in mind that you need at least the header model changes from https://github.com/2beaucoup/spray/branches/caching.\n\nCheers!\n. I'm working on a more general approach. Could submit a PR tomorrow. That would also include the `ETag/If-None-Match` model changes.\n. > Ok, Andr\u00e9, would you say that your solution supercedes this one or would it be independent of what Age proposes here?\n\nIt supercedes this PR.\n. see #797 \n. > What was wrong here in the end?\n\nMissing parser impl I guess. The test output is confusing. A string is compared to a header object with the same string representation.\n. Thanks for your reviews Age and Mathias. Great suggestions!\n\n> I'm not a big fan of the name clientCache since it doesn't really describe what happens here. I would go for something like ifNotModified or something similar.\n\nI just tried to make the difference in regards to the `cache` directive clear which works server side. `ifNotModified` suggests that this directive only matches on modified resources and otherwise rejects but maybe that's just me. If no better suggestions come up I'll change to `IfNotModified`. (Update: See below.)\n\nAnother question is if we should move this stuff to a new `*CacheDirectives` trait altogether which we can mix into the `Directives` trait by default.\n\n> I would add a few tests that specifically show this directive in combination with the file/resource directives\n\nI'm not a big fan of temp files in unit tests. What kind of functionality do you think isn't covered?\n\n> I would probably add a default implementation of ETags for files/resources. That's the one area where these features are most sorely missed, at least for the apps I'm currently working on.\n> I do realize that a proper default ETag implementation is not trivial to do though, looking at for instance the discussion in this Apache issue it seems that ETags need to be different for Gzipped vs unGzipped content so maybe that's a step too far for this pull request.\n\nI'm not yet convinced to put ETag support into `FileAndResourceDirectives` by default (yet).\n\nA custom impl could look like that BTW:\n\n``` scala\ndef myCustomEtag(fileName: String, offset: Long = 0, length: Long): String = ...\n\ndef filesWithEtag: Directive0 = mapHttpResponse {\ncase r @ HttpResponse(_, HttpEntity.NonEmpty(_, HttpData.FileBytes(fileName, offset, length)), _, _) =>\n  r.withDefaultHeaders(List(ETag(HttpEntityTag(myCustomEtag(fileName, offset, length)))))\ncase x => x\n}\n```\n\n> The spec says (RFC2616 14.26) that, if the If-None-Match tags match, the server \"MUST NOT perform the requested method\". However, we are unconditionally calling our inner route here, which will always perform the requested method. This means that we'll potentially execute PUTs (etc.) when we are not allowed to!\n\nYeah that's true. How would you attempt to fix that?\n\nI experimented with transforming the request to `HEAD` and looking and the response headers. That would only work if `head` is explicitly implemented (without `transparent-head-requests`).\n\n> That same spec paragraph also mandates that, in certain cases, \"the server MUST respond with a status of 412 (Precondition Failed)\", which we are not doing anywhere here.\n\nI somehow misread that paragraph. I will change the current impl to reflect it correctly.\n\n> It seems to me that we are combining two only loosely connected things into one single directive: If-Modified-Since support and If-None-Match support. I think we should split them apart into two separate directives. I also like Age's idea of naming the directives similar to their headers, so maybe withIfModifiedSinceSupport / withIfNoneMatchSupport. Just calling them ifModifiedSince kind of implies that its inner route is only executed in some cases, which it isn't. We'll always have to execute the inner route, maybe even several times.\n> With the basic directives in place we can still think about adding higher level directives which then combine the more basic ones in some form\n\nFrom the spec:\n\n> \"... the server MUST NOT perform the requested method, unless required to do so because the resource's modification date fails to match that supplied in an If-Modified-Since header field in the request\"\n\nI think the directives should be tied together because if `withIfNoneMatchSupport` matches, a lower level `withIfModifiedSinceSupport` wouldn't see the request and validate its `If-Modified-Since`.\n\nI suggest `withIfNotModifiedSupport` or `withCacheSupport`.\n\n> Apart from the inline comments I'd also agree with Age on somehow adding If-Modified-Since / If-None-Match support to the FileAndResourceDirectives, since this is something that people can benefit from without the need to change anything about their existing routing structure.\n\nBut they could also get confused if their routes suddenly start to return `304` which they didn't expect. I like the idea better to explicitly enable caching for (sub-) routes via a special directives instead of stuffing it into several directives by default.\n\nWDYT?\n\nThanks guys!\n. > Yes, I see the connection between the two.\n> How about we solve it by making withIfModifiedSinceSupport only kick in if the request does not also contain an If-None-Match header?\n\nBut couldn't a `withIfNoneMatchSupport` directive could have already completed before a lower level `withIfModifiedSinceSupport` directive gets the chance to look at the request (and execute its inner route if necessary)?\n. I don't get it. As far as i understand the spec correctly a server should only respond with `304` for requests with `If-Modified-Since` _and_ `If-None-Match` if _both_ are matched.\n\nLet's assume we have a route like this:\n\n``` scala\nwithIfNoneMatchSupport(etagForRequest) {\n  withIfModifiedSinceSupport(timestampForRequest) {\n    completeWithEtagAndTimestamp()\n  }\n}\n```\n\nand a request with `If-Modified-Since: ...` and `If-None-Match: ...`\ncomes in. How would I make sure that\n- `withIfNoneMatchSupport` doesn't reply with `304` when `withIfModifiedSinceSupport` would have completed normally\n- `withIfModifiedSinceSupport` doesn't reply with `304` if  `withIfNoneMatchSupport` didn't match\n\n?\n. Nice overview Mathias!\n\nI agree with most of what you said and updated this PR with a first draft (excluding `FileAndResourceDirectives` for now).\n\nOne problem remains: How would one communicate errors, e.g. the resource doesn't exist anymore?\n\nBTW: [This](https://github.com/apache/httpd/blob/trunk/modules/http/http_protocol.c#L528-L633) is the conditional impl in Apache httpd and [here](https://github.com/apache/httpd/blob/454a28fac363b20206d2c54939ac85cced8c9bfe/modules/http/http_etag.c#L112-L144) is the impl for file etags.\n. I agree with all of your comments (and will fix them).\n\nMeanwhile I've some more food for thought:\n\nHaving the `conditional` near the leafs of the route limits us in determining how a resource _would have_ been presented to the client. This prevents us from using strong ETags. We could combine the provided ETag with all presentation relevant headers or simply falll back to weak ETags. In general: The deeper in the route you are, the more could potentially happen afterwards... But I'm just thinking out loud here.\n\nAs a general question: Is HTTPbis preferred over rfc2616 for spray? I've seen some references to HTTPbis on the mailing list but for conditionals they seem to differ significantly. For example:\n\n[RFC2616 section 14.26 If-None-Match](http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.26):\n\n> If any of the entity tags match the entity tag of the entity that would have been returned in the response to a similar GET request (without the If-None-Match header) on that resource, or if \"*\" is given and any current entity exists for that resource, then the server MUST NOT perform the requested method, <b>unless required to do so because the resource's modification date fails to match that supplied in an If-Modified-Since header field</b> in the request. Instead, if the request method was GET or HEAD, the server SHOULD respond with a 304 (Not Modified) response, including the cache- related header fields (particularly ETag) of one of the entities that matched. For all other request methods, the server MUST respond with a status of 412 (Precondition Failed).\n\nNo mention of `If-None-Match` in the `If-Modified-Since` section.\n\n[HTTPbis Conditional Requests section 3.3 If-Modified-Since](http://tools.ietf.org/html/draft-ietf-httpbis-p4-conditional-26#section-3.3):\n\n> <b>A recipient MUST ignore If-Modified-Since if the request contains an If-None-Match header field;</b> the condition in If-None-Match is considered to be a more accurate replacement for the condition in If-Modified-Since and the two are only combined for the sake of interoperating with older intermediaries that might not implement If-None-Match.\n\nNo mention of `If-Modified-Since` in the `If-None-Match` section.\n\nI've stumbled upon https://raw.github.com/for-GET/http-decision-diagram/master/httpdd.png which seems to prefer the latter.\n\nThe more I study the spec(s) the more confusing it gets. :confused:\n\nWDYT?\n. Thanks guys! Regarding Berlin: I'll see what I can do....\n\nI've fixed the review comments and will update the impl in a later commit following our discussion.\n\nGreat find Age. I didn't get this far yet and this section seems to sum up the conditional flow quite nicely.\n\nSeems like there's a light at the end of the tunnel. :+1: \n. The failing tests show our new ETags already at work. ;)\n. Should `withRangeSupport` wrap `conditional` or the other way around?\n. Currently merging with master. Should I wait?\n. squashed\n. :+1: Nice weekend!\n. I added some updates that should fix the remaining issues.\n. Done. This one was supposed to get squashed but can as well stand on its own.\n\nNice collaborative effort. Thanks to everyone involved!\n. :+1: What about `[optional]HeaderValuesByType` for extracting several values at once?\n. You would have to specify multiple types. It would have to be correctly named `[optional]HeaderValuesByTypes`. Sorry.\n. Agreed. Thanks for explaining.\n. PRs are not a replacement for tickets. Please close this ticket only after the PR got merged. (This will happen automagically ff you append a `fixes #815` to your commit msg.)\n. `ValidationRejection` seems to be used in a variety of validation related cases, not just parameter validation. What's special about entity content validation? `MalformedRequestContentRejection` would still get thrown for failures not related to validation.\n. @jrudolph: Care to elaborate on why/when/how you'd want to access these additional infos?\n\nMy goal is to end [here](https://github.com/spray/spray/blob/master/spray-routing/src/main/scala/spray/routing/RejectionHandler.scala#L114) instead of [here](https://github.com/spray/spray/blob/master/spray-routing/src/main/scala/spray/routing/RejectionHandler.scala#L62) when a requirement fails (a.k.a. `IllegalArgumentException`).\n\nDo you think that say `EntityValidationRejection`, `ParameterValidationRejection` etc. are necessary?\n. I experimented with this [here](https://github.com/2beaucoup/spray/compare/case-class-validation).\n. :+1: \n. updated\n. Cool. Nice Weekend! :sunglasses:\n. All headers are stored lowercase in spray's http model.\n\n> `case HttpHeader(\"content-type\", \"application/json; charset=UTF-8\") => true`\n\nWhat's your use case here? Why don't you match like this:\n\n`case HttpHeaders.`Content-Type`(ContentTypes.`application/json`) => ...`\n\nBTW: You can as well specify the desired header class for a message:\n\n`httpMessage.header[`Content-Type`]`\n\nIf you work with the routing DSL you should take a look at the `HeaderDirectives` which do the conversions for you.\n\nHTH.\n. > To me, having to type qualified spray directives is like an overkill for such operation.\n\nUsing `HeaderDirectives` was just _one_ alternative. Have you tried the others I mentioned?\n\nBTW:\n\n``` scala\nheaderValueByName(\"Header-Name\") { headerValue =>\n  // complete(s\"header-name: $headerValue\")\n}\n```\n\ndoesn't require too much typing IMHO.\n. All [header directives](http://spray.io/documentation/1.2.1/spray-routing/header-directives/) are generic. While `headerValue` takes the header type as argument `headerValueByName` takes a string. Just fill your header name into the example I gave above:\n\n``` scala\nheaderValueByName(\"X-Forwarded-Proto\") { xForwardedProto =>\n  optionalHeaderValueByName(\"My-Custom-Header\") { => myCustomHeader\n    // use xForwardedFor and optional myCustomHeader here\n  }\n}\n```\n\nAs you can see there are no additional types involved and it works regardless of the casing in wich the headers are submitted.\n. Caching doesn't work together with chunking. Could you try with `spray.routing.file-chunking-threshold-size = 0`?\n\nYou should provide a `CacheKeyer` that takes the accepted encodings into account or use `compressResponseIfRequested` and move it outside of the `cache` directive. This way you are able to serve clients that don't support compression.\n. > But doesn't it disable chunking completely?\n\nThat's right. You currently have to choose between chunking and caching. You could use client side caching instead. The `FileAndResourceDirectives` already [supports](https://github.com/spray/spray/blob/5289cfcdc9cd689152c5fc5c86f8cf4d7e4a95ad/spray-routing/src/main/scala/spray/routing/directives/FileAndResourceDirectives.scala#L70) this.\n. > Just read this https://groups.google.com/forum/#!topic/spray-user/mL4sMr1qfwE ... No it looks like I better remove server cache completely.\n\nOK cool.\n\n> By client-side caching you mean using Last-Modified header?\n\n`Last-Modified` and `ETag` headers. Take a look at the `conditional` directive.\n. :+1: You're welcome.\n. `FormData` can be marshalled as `multipart/form-data` and `application/x-www-form-urlencoded` where only the latter uses escaping.\n\nThis is currently tested [here](https://github.com/spray/spray/blob/master/spray-httpx/src/test/scala/spray/httpx/marshalling/BasicMarshallersSpec.scala) and [here](https://github.com/spray/spray/blob/master/spray-httpx/src/test/scala/spray/httpx/marshalling/MultipartMarshallersSpec.scala).\n. If you add tests for this in `spray.http.HttpHeaderSpec` you'll find out that the parsing functionality is missing. Take a look at `spray.http.parser.SimpleHeaders`.\n. One more thing to keep in mind here is that HTTPbis allows for relative redirection URIs.\n\nThat being said I'd propose to handle the Redirection responses in `HttpHostConnector` and bubble it up to `ProxiedHostConnector` and `HttpManager` as long as the redirection URI isn't relative or doesn't match the connector's host.\n\n`ProxiedHostConnector` should therefore be changed to `tell` instead of `forward` in order to be able to look at responses.\n\nIf I understand the current logic right then even responses for connection level requests are redirected. That might be unexpected behaviour.\n\nWDYT?\n. The original problem here was the reverse order after client side _parsing_ of response headers.\n\nThere are several tasks here:\n- prioritize rendering order of response headers managed by spray (`Server`, `Content-Length`, `Date`, `Connection`, `Transfer-Encoding` others?) and preserve order of remaining headers\n- preserve order of headers during parsing \n- bonus: combine duplicated, collapsible headers during client side parsing and prevent sending of duplicated, uncollapsible headers as spec'd above.\n\nWDYT? Should I give them a try?\n. Only the order after parsing needs to be fixed then.\n\nUser supplied headers are currently rendered in the order they are supplied. Headers \"owned\" by spray come either first or last which shouldn't be a problem and keeps the impl sane.\n. Test failures seem unrelated.\n. Are these tests already ported to akka-http?\n. ",
    "JasonGiedymin": "As well as Quality value plz, see http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.9\n(maybe this is already in?)\n. As well as Quality value plz, see http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.9\n(maybe this is already in?)\n. ",
    "quelgar": "This is also happening for me, even when I see \"Peer orderly closed connection\". I tried both 0.8.0 and 0.9.0 RC2 with the same result.\n. It's working for me, thanks for the fast response!\n. This is also happening for me, even when I see \"Peer orderly closed connection\". I tried both 0.8.0 and 0.9.0 RC2 with the same result.\n. It's working for me, thanks for the fast response!\n. ",
    "peltchu": "Hi,\nany chance of this making into spray 1.0? It'd allow really nice separation of logic in certain cases and also make it possible to configure different kinds of higher-level \"handling strategies\" by combining services in different ways.\n. Thanks for the response,\n\nI agree that in many cases it's possible to use routes instead. \n\nUnless I'm mistaken I however think that one problem is that handling dependencies is more difficult with the route fragment approach. Eg. when you want to configure two different \"handling strategies\" using the same route \"block\" but differing by utilizing different injected services in helper funcs (mixed in traits).\n. You are right.\n\nAlthough I think the feature in question would still provide benefits in certain scenarios, that solution would work and the corners can be always worked around. Thus no point adding it to (probably already very crammed) 1.0 release.\n\nThanks for the time && insights.\n. Sorry to get back at this again.\n\nNonetheless, after a night's sleep I feel there's one significant drawback with the route approach when compared to the service-level approach. \n\nMappings of rejections to responses happens at the service level and this allows easy configuration of error responses from filters. The route approach seems to me problematic when you want eg. _AuthorizationFailedRejection_ to result in completely different response in route A than in route B. \n\nFor custom filters it's of course easier but I think when utilizing the default filters (eg. _authorize_) provided by spray they would all have to be intercepted to provide custom rejections/responses. It seems that this would become an another rejection handling layer in addition to the one provided by spray and thus it feels like working against the grain. On the other hand the feature in question would allow this very easily.\n\n Any ideas/suggestions on this?\n. This is excellent news!\n\nIt seems to me that it will truly unleash the power that spray already has under the hood with the rejections-based approach...\n\nThanks again for all the great work!\n. Hi,\nany chance of this making into spray 1.0? It'd allow really nice separation of logic in certain cases and also make it possible to configure different kinds of higher-level \"handling strategies\" by combining services in different ways.\n. Thanks for the response,\n\nI agree that in many cases it's possible to use routes instead. \n\nUnless I'm mistaken I however think that one problem is that handling dependencies is more difficult with the route fragment approach. Eg. when you want to configure two different \"handling strategies\" using the same route \"block\" but differing by utilizing different injected services in helper funcs (mixed in traits).\n. You are right.\n\nAlthough I think the feature in question would still provide benefits in certain scenarios, that solution would work and the corners can be always worked around. Thus no point adding it to (probably already very crammed) 1.0 release.\n\nThanks for the time && insights.\n. Sorry to get back at this again.\n\nNonetheless, after a night's sleep I feel there's one significant drawback with the route approach when compared to the service-level approach. \n\nMappings of rejections to responses happens at the service level and this allows easy configuration of error responses from filters. The route approach seems to me problematic when you want eg. _AuthorizationFailedRejection_ to result in completely different response in route A than in route B. \n\nFor custom filters it's of course easier but I think when utilizing the default filters (eg. _authorize_) provided by spray they would all have to be intercepted to provide custom rejections/responses. It seems that this would become an another rejection handling layer in addition to the one provided by spray and thus it feels like working against the grain. On the other hand the feature in question would allow this very easily.\n\n Any ideas/suggestions on this?\n. This is excellent news!\n\nIt seems to me that it will truly unleash the power that spray already has under the hood with the rejections-based approach...\n\nThanks again for all the great work!\n. ",
    "agaoglu": "You're welcome.\nGlad to be of help even it's a small one such as this! Thanks for making it possible, and spray of course ;)\n. You're welcome.\nGlad to be of help even it's a small one such as this! Thanks for making it possible, and spray of course ;)\n. ",
    "gvanecek": "Sorry, but thank you.\nGeorge\n\nOn Mar 14, 2012, at 2:25 AM, Mathias wrote:\n\n> The query parameters are available from the RequestContext:\n> \n>    val queryParams: Map[String, String] = ctx.request.queryParams\n> \n> Please direct all further questions to the [mailing list](http://groups.google.com/group/spray-user) as this space here is reserved for issues.\n> \n> ---\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/spray/spray/issues/92#issuecomment-4494700\n. Sorry, but thank you.\nGeorge\n\nOn Mar 14, 2012, at 2:25 AM, Mathias wrote:\n\n> The query parameters are available from the RequestContext:\n> \n>    val queryParams: Map[String, String] = ctx.request.queryParams\n> \n> Please direct all further questions to the [mailing list](http://groups.google.com/group/spray-user) as this space here is reserved for issues.\n> \n> ---\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/spray/spray/issues/92#issuecomment-4494700\n. ",
    "dat-vikash": "this no longer seems to work for spray 1.3.1, instead you can get query params this way:\n\n```\n val queryParams: Map[String, String] = requestContext.request.message.uri.query.toMap\n```\n. this no longer seems to work for spray 1.3.1, instead you can get query params this way:\n\n```\n val queryParams: Map[String, String] = requestContext.request.message.uri.query.toMap\n```\n. ",
    "jxstanford": "+1\n. Full proxy support would include -Dhttp.proxyHost, -Dhttp.proxyPort, -Dhttps.proxyHost, -Dhttps.proxyPort and -Dhttp.nonProxyHosts\n. +1\n. Full proxy support would include -Dhttp.proxyHost, -Dhttp.proxyPort, -Dhttps.proxyHost, -Dhttps.proxyPort and -Dhttp.nonProxyHosts\n. ",
    "bearnard": "bump\n\nIs there no other way to retrieve the remoteaddr from a spray-can HttpRequest?\n. Hi,\n\nThe application I'm building makes use of geoip, but I  guess it isn't a train smash...\n\nI could always use nginx in front to set the X-Remote-Addr header.\n\nBearnard.\n. bump\n\nIs there no other way to retrieve the remoteaddr from a spray-can HttpRequest?\n. Hi,\n\nThe application I'm building makes use of geoip, but I  guess it isn't a train smash...\n\nI could always use nginx in front to set the X-Remote-Addr header.\n\nBearnard.\n. ",
    "astubbs": "Cool, thanks! No worries. Am using Play-mini instead atm, pitty, would have liked to used spray instead. Play still needs to get split up into modules, so it's a mammoth import atm.\n. Cool, thanks! No worries. Am using Play-mini instead atm, pitty, would have liked to used spray instead. Play still needs to get split up into modules, so it's a mammoth import atm.\n. ",
    "klaeufer": "Merci vielmol!\n. Merci vielmol!\n. ",
    "pvorb": "The site seems to be down again. I also couldn\u2019t find any other location or reference to it on http://spray.io/ so I'd like to ask, where it is located now.\n. Thank you. And thanks for the link to the mailing list. I just missed it in the [repo\u2019s readme](https://github.com/spray/spray/blob/master/README.markdown).\n. Thank you for the quick reply! A hint in the docs would be nice.\n. The site seems to be down again. I also couldn\u2019t find any other location or reference to it on http://spray.io/ so I'd like to ask, where it is located now.\n. Thank you. And thanks for the link to the mailing list. I just missed it in the [repo\u2019s readme](https://github.com/spray/spray/blob/master/README.markdown).\n. Thank you for the quick reply! A hint in the docs would be nice.\n. ",
    "DavidBiesack": "I'm using Scala IDE in Eclipse. \nCan someone update https://github.com/spray/spray/wiki/Requirements with the correct requriements for spray 1.0-M1 ?\nI'll see if I can get it going with SBT... right now I'm getting\n\n[error] SERVER ERROR: Service Unavailable url=http://repo.typesafe.com/typesafe/releases/com/typesafe/akka/akka-actor/2.0/akka-actor-2.0.pom\n\n...\n\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\n[warn]  ::          UNRESOLVED DEPENDENCIES         ::\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\n[warn]  :: com.typesafe.akka#akka-testkit;2.0: not found\n[warn]  :: com.typesafe.akka#akka-actor;2.0: not found\n. I'm using Scala IDE in Eclipse. \nCan someone update https://github.com/spray/spray/wiki/Requirements with the correct requriements for spray 1.0-M1 ?\nI'll see if I can get it going with SBT... right now I'm getting\n\n[error] SERVER ERROR: Service Unavailable url=http://repo.typesafe.com/typesafe/releases/com/typesafe/akka/akka-actor/2.0/akka-actor-2.0.pom\n\n...\n\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\n[warn]  ::          UNRESOLVED DEPENDENCIES         ::\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\n[warn]  :: com.typesafe.akka#akka-testkit;2.0: not found\n[warn]  :: com.typesafe.akka#akka-actor;2.0: not found\n. ",
    "cbart": "Hi!\n\nWas there any progress on this one?\n\nCheers,\nCezary\n. Hi!\n\nWas there any progress on this one?\n\nCheers,\nCezary\n. ",
    "nraychaudhuri": "This will be very useful\n. This will be very useful\n. ",
    "flazz": "+1\n. +1\n. ",
    "king-xhuang": "Hope it can be implemented soon!!!\n. Hope it can be implemented soon!!!\n. ",
    "lbalamut": "+1 for any http_proxy support.\n\nI'm not using spray only because of this missing feature.\n\nwhy don't you honor system variables *_proxy as well, \nIs there any plan to be able to configure it separately per requests?\n\nCan I help somehow?\n\nBR,\n. +1 for any http_proxy support.\n\nI'm not using spray only because of this missing feature.\n\nwhy don't you honor system variables *_proxy as well, \nIs there any plan to be able to configure it separately per requests?\n\nCan I help somehow?\n\nBR,\n. ",
    "whataboutbob": "+1\n. +1 @jrudolph has done a great job adding the support for server side request auto-chunking and removing the 2GB maxContentLength limitation. After using it for a bit, I am wondering if it might be more helpful to enable auto-chunking per path / route instead of threshold. Reason being that I would need to implement 2 entry points doing the same thing except that one is auto-chunking mode and the other regular upload. Does this make sense?\n. > What would you do in \"regular mode\" when a client sends a chunked request?\n> Auto-chunking simplifies the application side logic because you only have to deal with one kind of request: \n> chunked ones. So, I'm not sure that enabling auto-chunking per path would really buy you anything...\n\nI see your point but perhaps I misunderstood the current implementation. If I understand it correctly, assuming `incoming-auto-chunking-threshold-size = 45K`, uploading a file < 45K would trigger a regular HttpRequest (path 1), only when the file size is > 45K then auto-chunking (path 2) is triggered, can you confirm? \n. Correct, that's what I have already done but I would also like the option to handle a different path's POST request in regular mode and that's no longer possible, I think, e.g., something like this, \n\n```\ncase HttpRequest(POST, Uri.Path(\"/user/avatar/small-upload\"), _, _, _) =>\n  ...\n\ncase s@ChunkedRequestStart(HttpRequest(POST, Uri.Path(\"/user/attachments/big-upload\"), _, _, _)) =>\n  require(!chunkHandlers.contains(sender))\n  val client = sender\n  val handler = context.actorOf(Props(new FileUploadHandler(client, s)))\n  chunkHandlers += (client -> handler)\n  handler.tell(s, client)\n\ncase c: MessageChunk =>\n  if (!chunkHandlers.isEmpty)\n    chunkHandlers(sender).tell(c, sender)\n\ncase e: ChunkedMessageEnd =>\n  if (!chunkHandlers.isEmpty)  {\n    chunkHandlers(sender).tell(e, sender)\n    chunkHandlers -= sender\n  }\n```\n. > Can you give us some more insight into why you'd like to handle small uploads differently from large ones?\n\nBased on my experience with web development, it is rare to use `Transfer-Encoding: chunked` mode, in fact I have never had the need to use it until this project which allows the ability to upload very small files to huge files up to 10GB, hence my desire to handle files of all sizes for that one path using auto-chunking only. However, other POST requests will work just fine without auto-chunking and are actually simpler to implement via the higher level Spray routing. In essence, the auto-chunking need is a one-off instead of the norm in processing POST requests, at least in my experience. I would imagine that if the client enables `Transfer-Encoding: chunked` mode that it knows that that particular endpoint is capable of handling chunks, otherwise it would be a regular POST request. \n. > Unfortunately this is not the case. HTTP/1.1 requires all servers to be able to handle chunked requests and all \n> clients to accept chunked responses. Otherwise they cannot call themselves HTTP/1.1 compliant.\n\nThanks for clarifying. Given my situation, should I just wait for the upcoming non-aggregated chunked requests feature or try to make the current solution work? \n. +1\n. +1 @jrudolph has done a great job adding the support for server side request auto-chunking and removing the 2GB maxContentLength limitation. After using it for a bit, I am wondering if it might be more helpful to enable auto-chunking per path / route instead of threshold. Reason being that I would need to implement 2 entry points doing the same thing except that one is auto-chunking mode and the other regular upload. Does this make sense?\n. > What would you do in \"regular mode\" when a client sends a chunked request?\n> Auto-chunking simplifies the application side logic because you only have to deal with one kind of request: \n> chunked ones. So, I'm not sure that enabling auto-chunking per path would really buy you anything...\n\nI see your point but perhaps I misunderstood the current implementation. If I understand it correctly, assuming `incoming-auto-chunking-threshold-size = 45K`, uploading a file < 45K would trigger a regular HttpRequest (path 1), only when the file size is > 45K then auto-chunking (path 2) is triggered, can you confirm? \n. Correct, that's what I have already done but I would also like the option to handle a different path's POST request in regular mode and that's no longer possible, I think, e.g., something like this, \n\n```\ncase HttpRequest(POST, Uri.Path(\"/user/avatar/small-upload\"), _, _, _) =>\n  ...\n\ncase s@ChunkedRequestStart(HttpRequest(POST, Uri.Path(\"/user/attachments/big-upload\"), _, _, _)) =>\n  require(!chunkHandlers.contains(sender))\n  val client = sender\n  val handler = context.actorOf(Props(new FileUploadHandler(client, s)))\n  chunkHandlers += (client -> handler)\n  handler.tell(s, client)\n\ncase c: MessageChunk =>\n  if (!chunkHandlers.isEmpty)\n    chunkHandlers(sender).tell(c, sender)\n\ncase e: ChunkedMessageEnd =>\n  if (!chunkHandlers.isEmpty)  {\n    chunkHandlers(sender).tell(e, sender)\n    chunkHandlers -= sender\n  }\n```\n. > Can you give us some more insight into why you'd like to handle small uploads differently from large ones?\n\nBased on my experience with web development, it is rare to use `Transfer-Encoding: chunked` mode, in fact I have never had the need to use it until this project which allows the ability to upload very small files to huge files up to 10GB, hence my desire to handle files of all sizes for that one path using auto-chunking only. However, other POST requests will work just fine without auto-chunking and are actually simpler to implement via the higher level Spray routing. In essence, the auto-chunking need is a one-off instead of the norm in processing POST requests, at least in my experience. I would imagine that if the client enables `Transfer-Encoding: chunked` mode that it knows that that particular endpoint is capable of handling chunks, otherwise it would be a regular POST request. \n. > Unfortunately this is not the case. HTTP/1.1 requires all servers to be able to handle chunked requests and all \n> clients to accept chunked responses. Otherwise they cannot call themselves HTTP/1.1 compliant.\n\nThanks for clarifying. Given my situation, should I just wait for the upcoming non-aggregated chunked requests feature or try to make the current solution work? \n. ",
    "agemooij": "I would also expect a way to configure proxies per-host and/or per-connection. I have no experience implementing raw http clients but I might have a look. Any implementation ideas or good references for the basic mechanism to be used here?\n. I'm all for you taking a shot at it since you already seem to be ahead of me in the research department and since my todo list is usually stuff to its maximum and beyond :)\n\nSo yes please, and let me know if I can help.\n. This sounds like a very clean and simple approach to me. \n\nI assume the auto-proxy behavior would also be active when you don't configure your client code with a `HostConnectorSetup`? This means classic spray-client apps would just transparently go through a proxy configured using the Java system properties and the whole thing would work the way pretty much every user will already expect it to work anyway.\n\nI wish I had time to implement this but at the moment it's slightly too big for my available units of spare time :(\n. I agree that silently ignoring general errors is never a good thing, but for specific cases like empty name-value pairs it does prevent a lot of confusion without any significant downsides. Thanks for fixing this!\n. For my own app, I fixed it like this:\n\n```\nimport shapeless._\nimport cc.spray.routing._\nimport cc.spray.routing.directives._\nimport cc.spray.util._\nimport BasicDirectives._\n\ncase class MissingHeaderRejection(headerName: String) extends Rejection\n\ndef headerValue(headerName: String): Directive[String :: HNil] = filter {\n  _.request.headers.mapFind(header => if (header.name == headerName) Some(header.value) else None) match {\n    case Some(a) => Pass(a :: HNil)\n    case None => Reject(MissingHeaderRejection(headerName))\n  }\n}\n```\n\nAnd I use it like this:\n\n```\nheaderValue(\"Custom-Header\") { customHeader =>\n  ...\n}\n```\n\nIf this looks good to you I can implement this in Spray and update this pull request.\n\nAge\n. I'm probably the only one crazy enough to put structured content into http headers but in this case I needed to transform header values to instances of a certain class (using a regex extractor) and Reject the request if that transformation failed. So what I came up with was:\n\n```\nimport shapeless._\nimport cc.spray.http._\nimport cc.spray.routing._\nimport cc.spray.routing.directives._\nimport cc.spray.util._\nimport BasicDirectives._\n\ncase class MissingHeaderRejection(headerName: String) extends Rejection\ncase class MalformedHeaderRejection(headerName: String, rawHeaderValue: String) extends Rejection\n\ntype HeaderUnmarshaller[T] = HttpHeader => Option[T]\nval DefaultHeaderUnmarshaller = (header: HttpHeader) => Some(header.value)\n\ndef valueAs[T](implicit um: HeaderUnmarshaller[T]) = um\n\ndef header[T](headerName: String, um: HeaderUnmarshaller[T] = DefaultHeaderUnmarshaller): Directive[T :: HNil] = filter {\n  _.request.headers.mapFind(header => if (header.name == headerName) Some(header) else None) match {\n    case Some(header) => um(header) match {\n      case Some(t) => Pass(t :: HNil)\n      case None    => Reject(MalformedHeaderRejection(headerName, header.value))\n    }\n    case None => Reject(MissingHeaderRejection(headerName))\n  }\n}\n```\n\nTo be used like this:\n\n```\nimplicit def headerToThing(header: HttpHeader): Option[Thing] = {\n  val Thing = \"\"\"(\\w*):(\\w*):(\\w*)\"\"\".r\n\n  header.value match {\n    case Thing(a, b, c) => Some(Thing(a, b, c)\n    case _ => None\n  }\n}\n\n...\n\nheader(\"Custom-Header\", valueAs[Thing]) { thing =>\n  ...\n}\n```\n\nThis might be going a bit too far for inclusion in Spray though...\n\nAge\n. It's definitely a step in the right direction. \n\nI think it would be nice and consistent if it would behave the same as the cookie directive, which is defined as:\n\n```\ndef cookie(name: String): Directive[HttpCookie :: HNil] =\n  headerValue {\n    case Cookie(cookies) => cookies.find(_.name == name)\n    case _ => None\n} | reject(MissingCookieRejection(name))\n```\n\nI think that is what you meant with your point 2 and I think that would already be enough for most people.\n\nThen for point 3, could you perhaps show a use case example of what it would look like to use the non-String `headerValue` overload to create an instance of some custom class using the extracted header value? I'm having a hard time visualizing it.\n\nit would still be nice to have some kind of `as[X]` unmarshalling support to convert header (and cookie) values to some custom type, but that might not be needed by many people and it is easy to add yourself when needed. \n\nI'm not sure whether the case class extraction stuf could be reused for this, since the regular Unmarshaller code only works with HttpRequest/HttpEntity.\n\nAge\n. Is this about the spray.io site?\n. That one is also on my list of \"issues I can fix\". Maybe tonight, otherwise tomorrow. \n\nAge\n\nOn Jul 21, 2013, at 12:11, Johannes Rudolph notifications@github.com wrote:\n\n> Easy to fix once we added the directives in #149\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. +1. It would simplify a lot of my client code, which now has extra methods for producing headers that can be moved to the marshaller.\n. Related question, will you also allow unmarshallers access to the response headers? Sometimes the response contains metadata headers that I want to make part of the unmarshalled entity. \n. Dammit, should have reread the description on top :( Sorry about that.\n. I'm working on this one. \n\nAdding `mapHeaders` was no problem at all because I can just delegate to `mapHeaders` on `HttpMessage`.\n\n`removeHeader` and `removeHeaders` would take one or more header names, right? I assume it should do a `toLowerCase` and match against the `lowercaseName`?\n\nWould you implement this purely in `RequestBuilding` on top of `mapHeaders` or would you add `removeHeader` and `removeHeaders` to HtpRequest or Even to HttpMessage?\n. Would it be ok to break backwards compatibility? If so, I would suggest renaming `spray.client.pipelining.unmarshal` to `spray.client.pipelining.entityAs`\n\nIf you like the name, I can create a PR for it.\n. Agree,that's better. Working on it.\n. Discussion continued in pull request #377, which is now ready for merge (I think)\n. Are you proposing to have one magnet implementation for the no-argument case, returning the marshaller, and one implementation with a marshaller and an entity that returns the unmarshalled T? \n\nFor this case I think it would be clearer to have two separate names because the no-arg version really does something else than unmarshalling, i.e. returning the unmarshaller itself.\n\nI'll rewrite the commit messages for this one too but I gotta run so it will be tonight.\n. OK, some easy code changes and some damn hard git rebasing later and this one should be ready for merging ;)\n\nI had a hard time with the rebase when I tried squashing it all into one commit with the proper language and I ended up leaving some conflict details all the way in the bottom of the extended commit message. I have no idea how to get that bit out so I hope you don't mind.\n. The Travis build seems to have failed but I can't find what went wrong, the tests are all green (and of course they were all green locally when I pushed it). Any ideas?\n. Done. I use \"amend\" quite often but for some reason the whole rebase stuff (to merge three commits into one and rewrite the message at the same time) messed up my head ;)\n. Agreed, I'll apply the comments.\nI'll have to refresh my Git foo for rewriting those commit messages but I'll do my best ;)\n. Done\n. How about that?\n. I applied your comments.\n. Fixed. Sorry about that, a momentarily attack of laziness I guess.\n\nI made the `isContentEncodingHeader` val lazy so it doesn't get pre-allocated unless it is needed.\n\nThis does indicate that it might have been better to implement the removeHeader(s) stuff directly on HttpMessage (or HttpRequest?) instead of only on RequestBuilding. What do you think?\n. Let's get this in first since they're only slightly related and we can always update this code later when there is a better way available.\n. Well, that's why you asked for help, right? ;)\n\nStill on my list for this week are #149 and #288. It's a busy (and blisteringly hot) week but I'll do my best.\n. This is just me being curious now: if the goal is to produce a byteString from the byte[] inside an HttpResponse, why  is there a StringRendering that uses a StringBuilder to produce a String?\n\nThat being said, I'll just implement it the way you guys say it should be done, probably tonight at the local Scala hackathon.\n. Clear. I just pushed the tailrec version.\n. Hi guys, you might want to refresh the spray.io website since it still shows the old detachTo docs instead of the new ones for detach.\n. Could someone please give the Travis build a kick? It ran into a timeout in HttpDialog unrelated to the change in this branch.\n. Nice! I agree that your approach is much better so I'll implement it plus the relevant unit tests. I'll also fix and test the rejection cancellation problem.\n\nDid you see my remark about the gzip fromHex in the tests? I don't know how you produced that raw hex string but it was different than the hex produced from the Spray Gzip compressor so I went for safe and actually used the Spray compressor to produce the test values. Just checking that you didn't have some special purpose in mind with that specific hex value.\n. If you run \"Hello\" or \"Yeah!\" through the Spray Gzip compressor, you get different outputs than the ones hardcoded in the current test. The current tests don't do a roundtrip so it didn't show up. \n\nI haven't looked at a probable cause yet. There was a certain overlap in the prefix and the suffix but the middle bit was different.\n. For the exact same command  I get different outputs practically every time I run it. I don't know enough details about Gzip but does the CRC contain some kind of timestamp value?\n\nI'll look into this a bit deeper and add some \"foreign\" tests if I can manage it.\n. Yeah, so the middle bit of the first line changes in small increments, looks like seconds.\n\n```\n[UltimateLaptopIV] ~/: echo \"Hello\" | gzip -f | hexdump\n0000000 1f 8b 08 00 fe c4 fb 51 00 03 f3 48 cd c9 c9 e7\n0000010 02 00 16 35 96 31 06 00 00 00\n000001a\n[UltimateLaptopIV] ~/: echo \"Hello\" | gzip -f | hexdump\n0000000 1f 8b 08 00 ff c4 fb 51 00 03 f3 48 cd c9 c9 e7\n0000010 02 00 16 35 96 31 06 00 00 00\n000001a\n[UltimateLaptopIV] ~/: echo \"Hello\" | gzip -f | hexdump\n0000000 1f 8b 08 00 00 c5 fb 51 00 03 f3 48 cd c9 c9 e7\n0000010 02 00 16 35 96 31 06 00 00 00\n000001a\n[UltimateLaptopIV] ~/: echo \"Hello\" | gzip -f | hexdump\n0000000 1f 8b 08 00 0c c5 fb 51 00 03 f3 48 cd c9 c9 e7\n0000010 02 00 16 35 96 31 06 00 00 00\n000001a\n[UltimateLaptopIV] ~/: echo \"Hello\" | gzip -f | hexdump\n0000000 1f 8b 08 00 10 c5 fb 51 00 03 f3 48 cd c9 c9 e7\n0000010 02 00 16 35 96 31 06 00 00 00\n000001a\n```\n. So decompressing these \"foreign\" values is no problem, we just shouldn't expect them to roundtrip back to the same hex value. I expect my other roundtripping tests only succeeded because I got lucky with the timing so I'll try to find a better way to test this.\n. Yep, that should work.\n\nDo you think it makes sense to have both compressResponse and compressResponseIfPossible? Why not always fall back to NoEncoding in compressResponse so you automatically deal with \"Accept-Encoding: identity\". Are there cases when you want to reject the request if it doesn't accept compressed responses?\n\nSimilarly, I think decompressResponse should always fall back to NoEncoding. If not, then there should also be a decompressRequestIfPossible to match the response encoding API.\n\nI think optionallyDecompressRequest should be called decompressRequestIfRequested because it better describes its function and it is also more consistent with the response encoding version. \n. So I'd like to propose the following simplification:\n\n```\ndef decompressRequest: Directive0 = decompressRequest(Gzip, Deflate, NoEncoding)\ndef decompressRequestIfRequested: Directive0 = decompressRequest(NoEncoding, Gzip, Deflate)\n\ndef decompressRequest(first: Encoder, more: Encoder*): Directive0 =\n  if (more.isEmpty) decodeRequest(first)\n  else more.foldLeft(decodeRequest(first)) { (r, encoder) => r | decodeRequest(encoder) }\n\ndef compressResponse: Directive0 = compressResponse(Gzip, Deflate, NoEncoding)\ndef compressResponseIfRequested: Directive0 = compressResponse(NoEncoding, Gzip, Deflate)\n\ndef compressResponse(first: Encoder, more: Encoder*): Directive0 =\n  if (more.isEmpty) encodeResponse(first)\n  else more.foldLeft(encodeResponse(first)) { (r, encoder) => r | encodeResponse(encoder) }\n```\n. Here's my second attempt. It closely resembles your suggestion but there are a few differences:\n- As mentioned earlier, I slightly changed the naming scheme for consistency and removed `compressResponseIfPossible`\n- I ran into a tricky problem with overloading when a directive has both a no-arguments version and one with arguments. This leads to ambiguity that the compiler cannot handle and AFAIK the magnet pattern doesn't help if there is an overload with no arguments that you would like to call without braces,. I worked around it by renaming the more generic versions to `decompresRequestwith(first, more)` and `compressResponseWith(first, more)` but perhaps you guys do know how to work around this problem. \n\nI also added some roundtrip compound directive tests and fixed the rejection cancellation bug that was mentioned earlier.\n\nWhat do you think?\n. I just noticed that new directives should of course also be reflected in the Sphinx docs. I'll be updating the pull request  soon with basic documentation, i.e. at the same level as all the other directives for now :)\n. I agree, ths is something I ran into while implementing and I would have no problem with removing one of these. Which one would you choose to remove? I guess starting with NoEncoding should be the default then?\n. I agree. I'll spend some time on this this weekend, hopefully also on the documentation bits.\n. I removed the `decompressRequestIfRequested` directive and updated the spray routing example. I also rebased onto the latest master to make sure everything still applied without conflicts.\n. Are you suggesting to remove `compressResponse` and to make `compressRequestIfRequested` the default (renamed to `compressResponse`)? It would indeed be more symmetric while still allowing people to fall back to compressResponseWith(Gzip) where needed. Agreed?\n. I agree. \n\nIt's tricky to come up with a name that properly signifies that both compression **and** decompression will be enabled but the `(decompressRequest & compressResponse)` compound directive (or its nested form) carry a slightly higher barrier to entry so I suggest we add the `enableCompression` directive (after removing the above confusion).\n\nHow about then also adding `enableCompressionWith(x: Decoder with Encoder)` to make the collection complete?\n. ok, I'll leave the `compressResponseIfRequested` in place for now. Would you like the documentation to be part of this PR. I should be able to get some work on that done tonight.\n. OK, I will leave things as they are and add documentation for the directives plus a general \"what to use when\" table.\n. Sorry it took this long but it was a busy week and this stuff required quite a lot of documentation. I added even more unit tests and used those as the examples for the docs. The index also contains tables comparing the different directives with each other and a section on how to compose the decompress and compress directives.\n\nFor now, I only added documentation for the new directives since that was already a very time-consuming endeavour and I would like this PR to end at some point :)\n. Which `+` marker? :)\n. You're welcome. Any idea when you are gong to merging this into the 1.1/1.2 branches? I noticed that they haven't been updated since late July. Can I help with that?\n. Ah, sorry. My local repo was not up to date with upstream.\n. Yeah, I thought about the possible naming conflict but I think that the chance of somebody importing spray._ is pretty low (and I couldn't think of a better name).I also couldn't think of the proper existing place to put these directives, hence the new trait.\n\nI'll have a look at the docs for `headerValueByName`, thanks!\n. I agree, \"http\" and \"https\" might be a bit confusing and scheme should be clear enough.\n\nI like the extra scheme extractor and how that cleans up the implementations. I hadn't seen the `require` method on Directive1 yet, nice!.\n\nI'l probably get around to updating this PR tonight.\n. I was just working on this and I ran into a problem that I also ran into on the compression PR, namely that Scala gets very confused when there are two overloaded methods, once of which has no arguments, and you want to apply the resulting function to an argument. In this case, when you write something like this:\n\n```\n\"the scheme directive\" should {\n    \"extract the Uri scheme\" in {\n      Put(\"http://localhost/\", \"Hello\") ~> scheme { echoComplete } ~> check { entityAs[String] === \"http\" }\n    }\n  }\n```\n\nit tries to call the version of scheme that takes one String argument with `echoComplete` as the argument, resulting in:\n\n```\nSchemeDirectivesSpec.scala:23: polymorphic expression cannot be instantiated to expected type;\n[error]  found   : [T]T => (spray.routing.RequestContext => Unit)\n[error]  required: String\n[error]       Put(\"http://localhost/\", \"Hello\") ~> scheme { echoComplete } ~> check { entityAs[String] === \"http\" }\n```\n\nThe two obvious workarounds, always using `scheme()` or renaming one of the two directives, are not very elegant but AFAIK the magnet pattern doesn't work with zero-arg methods. Am I wrong?\n\nWhat do you guys think?\n. OK, will rename to schemeName. That also makes it easier to document (no naming conflict) so I'll make an initial attempt at that too.\n. That should do it. \n\nI included full directive documentation with source samples, all based on the well documented `headerValueByName` directive. Of course this took a lot longer than I had thought, especially coming up with some good examples and figuring out the correct includecode snippet incantations :)\n\nThe `schemeName` directive is modeled after the `hostName` directive. Both directives will extract an empty String if no matching value could be found but I wonder whether an `Option` would be a better fit.\n\nFYI: I noticed that the hostName directive is untested.\n. > > figuring out the correct includecode snippet incantations\n> \n> Is there anything wrong with it that we could improve?\n\nThe file contains two directives, both starting with `scheme`. When I tried to refer to the `scheme` snippet it included the signatures of both directives. I had a look at the Python, tried a bunch of things based on hunches, and eventually found that using `scheme(` did the trick. It was around 01:00 so I didn't look into the root cause of this.\n. I have no idea why the build is failing. I did two complete clean builds (clean compile test) of my branch locally and they ran without problems. Did something change on master and does Travis do some kind of merge with master?\n. Aha, Mathias changed the require directive! This PR is doomed to stay unmerged :)\n\nI'll update the branch and resubmit ASAP.\n. Updated and fixed\n. That should fix all your comments... I hope :)\n. Done\n. Ok, thanks!\n\nGithub search clearly sucks because I searched for \"client proxy server\" :(\n. Aha! :)\n. Thanks, I fixed it\n. ok, will do it tonight\n. Did it earlier :)\nThis should be clear enough.\n. I think I have some way to go before I can overtake you guys :)\n. I was actually basing my (bad) behaviour on the (old?) placeholders (using the `//#...#` syntax) I saw all over the directives api. It looked like this is how things were done but now that I think about it they have been slowly disappearing over time.\n\nIn the case of the documentation I wrote for the new encoding directives, the \"real\" test cases were so close to the ones I would have written to demo usage that my \"must never duplicate code\" alarms were tripped. Let me know if you still want me to extract them to the example code base.\n. Now I'm curious... what exactly is wrong with that line and how can it lead to a MatchError on a MissingCookieRejection, which seems to be handled just fine on line 57?\n\nhttps://github.com/spray/spray/blob/9c251c4a41adbcb3918b2ff2e37d86a62d821edc/spray-routing/src/main/scala/spray/routing/RejectionHandler.scala#L57\n. Ah, I did not notice the flatMap instead of the collect. All clear now :)\n. Hi guys. I've been away from Spray for a few months (traveling the world) but I'm back and here's a pull request to prove it :)\n\nThis is a first attempt and I think there could be some room for cleanup here and there but it gets the job done. It should help quite a bit with the perceived performance of serving static files, assuming of course that everyone is already using Cache-Control.\n\nLooking forward to your comments,\nAge\n. That is indeed a mysterious message. I'll have a look ASAP\n. Interesting indeed... the test (and the parser/renderer) is exactly equal to the one for Last-Modified:\n\n``` scala\n\"If-Modified-Since\" in {\n  \"If-Modified-Since: Wed, 13 Jul 2011 08:12:31 GMT\" =!= `If-Modified-Since`(DateTime(2011, 7, 13, 8, 12, 31))\n}\n\n\"Last-Modified\" in {\n  \"Last-Modified: Wed, 13 Jul 2011 08:12:31 GMT\" =!= `Last-Modified`(DateTime(2011, 7, 13, 8, 12, 31))\n}\n```\n\nSo I'm not sure what could have gone wrong there... any ideas?\n. I can now also reproduce this locally. One of those awesome \"Funny, it used to work before\" things :(\n\nI'll look into it later.\n. Pushed too soon. Adding a header has quite some side effects on the other sub-projects and I obviously forgot to run all the tests :( Fixed the spray-http tests and fixing spray-can now..\n. OK, all tests are green now. sorry for not testing better before submitting the request.\n\nThe `HttpHeaderParserSpec` is very much hardcoded and I do not have the deep insight to see why values have to be the exact values expected so I just adjusted all tests to fit the new reality. I'm not sure how to verify whether those new values are \"correct\" though :(\n. Great idea! Let me see if I can get that working. It might take a few days before I have enough spare time though.\n\nThis would also solve my impending problem of \"how do I add ETag support to this without making it even more complex\" ;)\n. It looks pretty good. Great that you also added ETags already! \n\nA few remarks:\n- AFAIK the value of `If-None-Match` could also be a wildcard (i.e. `*`). See http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.26\n- Using response mapping would be slightly less performant because the original response is actually produced and then discarded. Static files would always be read into memory, even if they hadn't been changed. I'm not sure how important this is though.\n- We also need a directive that actually generates an ETag for a resource, be it static files or any entity. The usual solution is to take an md5 of the entity body so this should not be that hard.\n- I would still write some more integration-style unit tests to verify that `If-Modified-since` and `If-None-Match` work correctly with static files, since that is their main purpose.\n\nHow do you want to continue?\n\nAge\n. > > Using response mapping would be slightly less performant because the original response is actually produced and then discarded. Static files would always be read into memory, even if they hadn't been changed. I'm not sure how important this is though.\n> \n> That doesn't seem to be the case for files. FileAndResourceDirectives uses HttpData(file) where the file content is copied by the marshaller. Resources are read at completion though. Since 'File.lastModified' is rather cheap it gets added by default (source).\n\nThe implementation of `getFromResource` does read the file into memory ([source](https://github.com/spray/spray/blob/master/spray-routing/src/main/scala/spray/routing/directives/FileAndResourceDirectives.scala#L119)). I guess we could make a version of HttpData that would wrap an inputStream and only actually reads it on demand.\n\n> The downside of generic hashing would be that you would have to always look at the content while a custom hashing function could use pre-generated data. How would you handle large chunked files for example? Maybe we can come up with support for generic hashing later.\n\nWhat kind of pre-generated data did you have in mind? I would say that the default strategy should look at the response data but that you could customize that to your liking.\n\n> > We also need a directive that actually generates an ETag for a resource, be it static files or any entity. The usual solution is to take an md5 of the entity body so this should not be that hard.\n> \n> Right. respondWithHeader(Etag(yourGeneratedMd5)) is all that is needed. yourGeneratedMd5 could hash the response on the fly or could look at existing data\n\nIt seems most http servers, like Apache, simply take a combination of the last modified timestamp and the file size and MD5 that. File size is just as cheap to compute as last modified AFAIK. But I agree that you should be able to customize how this hash gets generated. I would more go for something like this:\n\n```\nETag {\n  // produce response\n}\n```\n\nwhere the `ETag` directive could use an implicit strategy with the default strategy producing an `ETag` header if the `Last-Modified` header is set, combining that with the length of the `HttpData` in the response into an MD5 hash..\n. Agreed. Let's see what @sirthias and @jrudolph think.\n. Looks interesting, I'd like to experiment some more with this. I'm very busy at the moment but I'll hopefully have time soon to implement this and do some experiments in the sample routing app to see how this would work with both static files and api responses.\n\nI'll let you know how far I got before you're back. Have a great holiday!\n. Yes, I would. It does what it says in the title of this issue.\n\nI like Andr\u00e9's stuff but I haven't had time to merge anything in yet. We can move that to the existing ETag issue (#76) and take it from there.\n. I'm still curious as to whether you like the \"built into the file/resource directives\" approach, which is slightly more efficient, or Andr\u00e9's more general approach.\n. Looks good. A few remarks:\n- I'm not a big fan of the name `clientCache` since it doesn't really describe what happens here. I would go for something like `ifNotModified` or something similar.\n- I would add a few tests that specifically show this directive in combination with the file/resource directives\n- I would probably add a default implementation of ETags for files/resources. That's the one area where these features are most sorely missed, at least for the apps I'm currently working on. \n\nI do realize that a proper default ETag implementation is not trivial to do though, looking at for instance the discussion in [this Apache issue](https://issues.apache.org/bugzilla/show_bug.cgi?id=39727) it seems that ETags need to be different for Gzipped vs unGzipped content so maybe that's a step too far for this pull request.\n. +1.\n\nWhat would you suggest as the default implementation for file/resource ETags?\n. By the way, this pull request will fix issues #76 and #219.\n. Sounds good.\n. I'll buy a round as well! ;)\n\nI'm looking forward to see you guys in Berlin.\n. It's interesting that you guys are already moving towards HTTPbis. This is great but I can imagine that supporting both versions of the protocol can sometimes lead to conflicts such as this one, where older clients expect (slightly) different behaviour. Do you already have ideas/plans on how to deal with such situations?\n. I'm pretty new to HTTPbis and the location of the relevant specs. Are you referring to this section?\n\nhttp://tools.ietf.org/html/draft-ietf-httpbis-p4-conditional-26#section-6\n. Just a lucky find. My first Google hit was this nice overview page:\n\nhttps://tools.ietf.org/wg/httpbis/\n\nI had no idea that there was more to HTTPbis then Http 2.0 but these 1.1 rewrites and clarifications are indeed a great resource.\n. Awesome work Andr\u00e9! Thanks for pushing this forward!\n\n> On 07 Mar 2014, at 18:05, 2beaucoup notifications@github.com wrote:\n> \n>  Nice weekend!\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. The HTTPbis updated specs have a bit more detail here:\n\nhttp://tools.ietf.org/html/draft-ietf-httpbis-p2-semantics-26#section-6.3.6\n\nSpecifically:\n\n> In other words, a server MUST do one of the following for a 205\n>    response: a) indicate a zero-length body for the response by\n>    including a Content-Length header field with a value of 0; b)\n>    indicate a zero-length payload for the response by including a\n>    Transfer-Encoding header field with a value of chunked and a message\n>    body consisting of a single chunk of zero-length; or, c) close the\n>    connection immediately after sending the blank line terminating the\n>    header section.\n\nI'm not sure what your stub server produces that could mess with the normal spray-client response handling. Perhaps your server produces case c) ?\n\nCould you perhaps post the complete output of a curl call to such a server? Does it satisfy the above specs?\n\nDoes spray-client time out on receiving the response? Have you tested this with other servers that produce 205 responses?\n\n@jrudolph: @hierynomus is a colleague of mine, hence the third degree barrage of questions ;)\n. Any near future plans to release a 1.x.2 release?\n\nIt's not a big deal and we can easily work around it with a copy of SprayJsonSupport but I'm lazy so if you were planning to release soon anyway... ;)\n. Awesome. I would offer my services if it weren't for the fact that I'll be on holiday for the next three weeks. See you at Scala.IO ;)\n. I would also expect a way to configure proxies per-host and/or per-connection. I have no experience implementing raw http clients but I might have a look. Any implementation ideas or good references for the basic mechanism to be used here?\n. I'm all for you taking a shot at it since you already seem to be ahead of me in the research department and since my todo list is usually stuff to its maximum and beyond :)\n\nSo yes please, and let me know if I can help.\n. This sounds like a very clean and simple approach to me. \n\nI assume the auto-proxy behavior would also be active when you don't configure your client code with a `HostConnectorSetup`? This means classic spray-client apps would just transparently go through a proxy configured using the Java system properties and the whole thing would work the way pretty much every user will already expect it to work anyway.\n\nI wish I had time to implement this but at the moment it's slightly too big for my available units of spare time :(\n. I agree that silently ignoring general errors is never a good thing, but for specific cases like empty name-value pairs it does prevent a lot of confusion without any significant downsides. Thanks for fixing this!\n. For my own app, I fixed it like this:\n\n```\nimport shapeless._\nimport cc.spray.routing._\nimport cc.spray.routing.directives._\nimport cc.spray.util._\nimport BasicDirectives._\n\ncase class MissingHeaderRejection(headerName: String) extends Rejection\n\ndef headerValue(headerName: String): Directive[String :: HNil] = filter {\n  _.request.headers.mapFind(header => if (header.name == headerName) Some(header.value) else None) match {\n    case Some(a) => Pass(a :: HNil)\n    case None => Reject(MissingHeaderRejection(headerName))\n  }\n}\n```\n\nAnd I use it like this:\n\n```\nheaderValue(\"Custom-Header\") { customHeader =>\n  ...\n}\n```\n\nIf this looks good to you I can implement this in Spray and update this pull request.\n\nAge\n. I'm probably the only one crazy enough to put structured content into http headers but in this case I needed to transform header values to instances of a certain class (using a regex extractor) and Reject the request if that transformation failed. So what I came up with was:\n\n```\nimport shapeless._\nimport cc.spray.http._\nimport cc.spray.routing._\nimport cc.spray.routing.directives._\nimport cc.spray.util._\nimport BasicDirectives._\n\ncase class MissingHeaderRejection(headerName: String) extends Rejection\ncase class MalformedHeaderRejection(headerName: String, rawHeaderValue: String) extends Rejection\n\ntype HeaderUnmarshaller[T] = HttpHeader => Option[T]\nval DefaultHeaderUnmarshaller = (header: HttpHeader) => Some(header.value)\n\ndef valueAs[T](implicit um: HeaderUnmarshaller[T]) = um\n\ndef header[T](headerName: String, um: HeaderUnmarshaller[T] = DefaultHeaderUnmarshaller): Directive[T :: HNil] = filter {\n  _.request.headers.mapFind(header => if (header.name == headerName) Some(header) else None) match {\n    case Some(header) => um(header) match {\n      case Some(t) => Pass(t :: HNil)\n      case None    => Reject(MalformedHeaderRejection(headerName, header.value))\n    }\n    case None => Reject(MissingHeaderRejection(headerName))\n  }\n}\n```\n\nTo be used like this:\n\n```\nimplicit def headerToThing(header: HttpHeader): Option[Thing] = {\n  val Thing = \"\"\"(\\w*):(\\w*):(\\w*)\"\"\".r\n\n  header.value match {\n    case Thing(a, b, c) => Some(Thing(a, b, c)\n    case _ => None\n  }\n}\n\n...\n\nheader(\"Custom-Header\", valueAs[Thing]) { thing =>\n  ...\n}\n```\n\nThis might be going a bit too far for inclusion in Spray though...\n\nAge\n. It's definitely a step in the right direction. \n\nI think it would be nice and consistent if it would behave the same as the cookie directive, which is defined as:\n\n```\ndef cookie(name: String): Directive[HttpCookie :: HNil] =\n  headerValue {\n    case Cookie(cookies) => cookies.find(_.name == name)\n    case _ => None\n} | reject(MissingCookieRejection(name))\n```\n\nI think that is what you meant with your point 2 and I think that would already be enough for most people.\n\nThen for point 3, could you perhaps show a use case example of what it would look like to use the non-String `headerValue` overload to create an instance of some custom class using the extracted header value? I'm having a hard time visualizing it.\n\nit would still be nice to have some kind of `as[X]` unmarshalling support to convert header (and cookie) values to some custom type, but that might not be needed by many people and it is easy to add yourself when needed. \n\nI'm not sure whether the case class extraction stuf could be reused for this, since the regular Unmarshaller code only works with HttpRequest/HttpEntity.\n\nAge\n. Is this about the spray.io site?\n. That one is also on my list of \"issues I can fix\". Maybe tonight, otherwise tomorrow. \n\nAge\n\nOn Jul 21, 2013, at 12:11, Johannes Rudolph notifications@github.com wrote:\n\n> Easy to fix once we added the directives in #149\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. +1. It would simplify a lot of my client code, which now has extra methods for producing headers that can be moved to the marshaller.\n. Related question, will you also allow unmarshallers access to the response headers? Sometimes the response contains metadata headers that I want to make part of the unmarshalled entity. \n. Dammit, should have reread the description on top :( Sorry about that.\n. I'm working on this one. \n\nAdding `mapHeaders` was no problem at all because I can just delegate to `mapHeaders` on `HttpMessage`.\n\n`removeHeader` and `removeHeaders` would take one or more header names, right? I assume it should do a `toLowerCase` and match against the `lowercaseName`?\n\nWould you implement this purely in `RequestBuilding` on top of `mapHeaders` or would you add `removeHeader` and `removeHeaders` to HtpRequest or Even to HttpMessage?\n. Would it be ok to break backwards compatibility? If so, I would suggest renaming `spray.client.pipelining.unmarshal` to `spray.client.pipelining.entityAs`\n\nIf you like the name, I can create a PR for it.\n. Agree,that's better. Working on it.\n. Discussion continued in pull request #377, which is now ready for merge (I think)\n. Are you proposing to have one magnet implementation for the no-argument case, returning the marshaller, and one implementation with a marshaller and an entity that returns the unmarshalled T? \n\nFor this case I think it would be clearer to have two separate names because the no-arg version really does something else than unmarshalling, i.e. returning the unmarshaller itself.\n\nI'll rewrite the commit messages for this one too but I gotta run so it will be tonight.\n. OK, some easy code changes and some damn hard git rebasing later and this one should be ready for merging ;)\n\nI had a hard time with the rebase when I tried squashing it all into one commit with the proper language and I ended up leaving some conflict details all the way in the bottom of the extended commit message. I have no idea how to get that bit out so I hope you don't mind.\n. The Travis build seems to have failed but I can't find what went wrong, the tests are all green (and of course they were all green locally when I pushed it). Any ideas?\n. Done. I use \"amend\" quite often but for some reason the whole rebase stuff (to merge three commits into one and rewrite the message at the same time) messed up my head ;)\n. Agreed, I'll apply the comments.\nI'll have to refresh my Git foo for rewriting those commit messages but I'll do my best ;)\n. Done\n. How about that?\n. I applied your comments.\n. Fixed. Sorry about that, a momentarily attack of laziness I guess.\n\nI made the `isContentEncodingHeader` val lazy so it doesn't get pre-allocated unless it is needed.\n\nThis does indicate that it might have been better to implement the removeHeader(s) stuff directly on HttpMessage (or HttpRequest?) instead of only on RequestBuilding. What do you think?\n. Let's get this in first since they're only slightly related and we can always update this code later when there is a better way available.\n. Well, that's why you asked for help, right? ;)\n\nStill on my list for this week are #149 and #288. It's a busy (and blisteringly hot) week but I'll do my best.\n. This is just me being curious now: if the goal is to produce a byteString from the byte[] inside an HttpResponse, why  is there a StringRendering that uses a StringBuilder to produce a String?\n\nThat being said, I'll just implement it the way you guys say it should be done, probably tonight at the local Scala hackathon.\n. Clear. I just pushed the tailrec version.\n. Hi guys, you might want to refresh the spray.io website since it still shows the old detachTo docs instead of the new ones for detach.\n. Could someone please give the Travis build a kick? It ran into a timeout in HttpDialog unrelated to the change in this branch.\n. Nice! I agree that your approach is much better so I'll implement it plus the relevant unit tests. I'll also fix and test the rejection cancellation problem.\n\nDid you see my remark about the gzip fromHex in the tests? I don't know how you produced that raw hex string but it was different than the hex produced from the Spray Gzip compressor so I went for safe and actually used the Spray compressor to produce the test values. Just checking that you didn't have some special purpose in mind with that specific hex value.\n. If you run \"Hello\" or \"Yeah!\" through the Spray Gzip compressor, you get different outputs than the ones hardcoded in the current test. The current tests don't do a roundtrip so it didn't show up. \n\nI haven't looked at a probable cause yet. There was a certain overlap in the prefix and the suffix but the middle bit was different.\n. For the exact same command  I get different outputs practically every time I run it. I don't know enough details about Gzip but does the CRC contain some kind of timestamp value?\n\nI'll look into this a bit deeper and add some \"foreign\" tests if I can manage it.\n. Yeah, so the middle bit of the first line changes in small increments, looks like seconds.\n\n```\n[UltimateLaptopIV] ~/: echo \"Hello\" | gzip -f | hexdump\n0000000 1f 8b 08 00 fe c4 fb 51 00 03 f3 48 cd c9 c9 e7\n0000010 02 00 16 35 96 31 06 00 00 00\n000001a\n[UltimateLaptopIV] ~/: echo \"Hello\" | gzip -f | hexdump\n0000000 1f 8b 08 00 ff c4 fb 51 00 03 f3 48 cd c9 c9 e7\n0000010 02 00 16 35 96 31 06 00 00 00\n000001a\n[UltimateLaptopIV] ~/: echo \"Hello\" | gzip -f | hexdump\n0000000 1f 8b 08 00 00 c5 fb 51 00 03 f3 48 cd c9 c9 e7\n0000010 02 00 16 35 96 31 06 00 00 00\n000001a\n[UltimateLaptopIV] ~/: echo \"Hello\" | gzip -f | hexdump\n0000000 1f 8b 08 00 0c c5 fb 51 00 03 f3 48 cd c9 c9 e7\n0000010 02 00 16 35 96 31 06 00 00 00\n000001a\n[UltimateLaptopIV] ~/: echo \"Hello\" | gzip -f | hexdump\n0000000 1f 8b 08 00 10 c5 fb 51 00 03 f3 48 cd c9 c9 e7\n0000010 02 00 16 35 96 31 06 00 00 00\n000001a\n```\n. So decompressing these \"foreign\" values is no problem, we just shouldn't expect them to roundtrip back to the same hex value. I expect my other roundtripping tests only succeeded because I got lucky with the timing so I'll try to find a better way to test this.\n. Yep, that should work.\n\nDo you think it makes sense to have both compressResponse and compressResponseIfPossible? Why not always fall back to NoEncoding in compressResponse so you automatically deal with \"Accept-Encoding: identity\". Are there cases when you want to reject the request if it doesn't accept compressed responses?\n\nSimilarly, I think decompressResponse should always fall back to NoEncoding. If not, then there should also be a decompressRequestIfPossible to match the response encoding API.\n\nI think optionallyDecompressRequest should be called decompressRequestIfRequested because it better describes its function and it is also more consistent with the response encoding version. \n. So I'd like to propose the following simplification:\n\n```\ndef decompressRequest: Directive0 = decompressRequest(Gzip, Deflate, NoEncoding)\ndef decompressRequestIfRequested: Directive0 = decompressRequest(NoEncoding, Gzip, Deflate)\n\ndef decompressRequest(first: Encoder, more: Encoder*): Directive0 =\n  if (more.isEmpty) decodeRequest(first)\n  else more.foldLeft(decodeRequest(first)) { (r, encoder) => r | decodeRequest(encoder) }\n\ndef compressResponse: Directive0 = compressResponse(Gzip, Deflate, NoEncoding)\ndef compressResponseIfRequested: Directive0 = compressResponse(NoEncoding, Gzip, Deflate)\n\ndef compressResponse(first: Encoder, more: Encoder*): Directive0 =\n  if (more.isEmpty) encodeResponse(first)\n  else more.foldLeft(encodeResponse(first)) { (r, encoder) => r | encodeResponse(encoder) }\n```\n. Here's my second attempt. It closely resembles your suggestion but there are a few differences:\n- As mentioned earlier, I slightly changed the naming scheme for consistency and removed `compressResponseIfPossible`\n- I ran into a tricky problem with overloading when a directive has both a no-arguments version and one with arguments. This leads to ambiguity that the compiler cannot handle and AFAIK the magnet pattern doesn't help if there is an overload with no arguments that you would like to call without braces,. I worked around it by renaming the more generic versions to `decompresRequestwith(first, more)` and `compressResponseWith(first, more)` but perhaps you guys do know how to work around this problem. \n\nI also added some roundtrip compound directive tests and fixed the rejection cancellation bug that was mentioned earlier.\n\nWhat do you think?\n. I just noticed that new directives should of course also be reflected in the Sphinx docs. I'll be updating the pull request  soon with basic documentation, i.e. at the same level as all the other directives for now :)\n. I agree, ths is something I ran into while implementing and I would have no problem with removing one of these. Which one would you choose to remove? I guess starting with NoEncoding should be the default then?\n. I agree. I'll spend some time on this this weekend, hopefully also on the documentation bits.\n. I removed the `decompressRequestIfRequested` directive and updated the spray routing example. I also rebased onto the latest master to make sure everything still applied without conflicts.\n. Are you suggesting to remove `compressResponse` and to make `compressRequestIfRequested` the default (renamed to `compressResponse`)? It would indeed be more symmetric while still allowing people to fall back to compressResponseWith(Gzip) where needed. Agreed?\n. I agree. \n\nIt's tricky to come up with a name that properly signifies that both compression **and** decompression will be enabled but the `(decompressRequest & compressResponse)` compound directive (or its nested form) carry a slightly higher barrier to entry so I suggest we add the `enableCompression` directive (after removing the above confusion).\n\nHow about then also adding `enableCompressionWith(x: Decoder with Encoder)` to make the collection complete?\n. ok, I'll leave the `compressResponseIfRequested` in place for now. Would you like the documentation to be part of this PR. I should be able to get some work on that done tonight.\n. OK, I will leave things as they are and add documentation for the directives plus a general \"what to use when\" table.\n. Sorry it took this long but it was a busy week and this stuff required quite a lot of documentation. I added even more unit tests and used those as the examples for the docs. The index also contains tables comparing the different directives with each other and a section on how to compose the decompress and compress directives.\n\nFor now, I only added documentation for the new directives since that was already a very time-consuming endeavour and I would like this PR to end at some point :)\n. Which `+` marker? :)\n. You're welcome. Any idea when you are gong to merging this into the 1.1/1.2 branches? I noticed that they haven't been updated since late July. Can I help with that?\n. Ah, sorry. My local repo was not up to date with upstream.\n. Yeah, I thought about the possible naming conflict but I think that the chance of somebody importing spray._ is pretty low (and I couldn't think of a better name).I also couldn't think of the proper existing place to put these directives, hence the new trait.\n\nI'll have a look at the docs for `headerValueByName`, thanks!\n. I agree, \"http\" and \"https\" might be a bit confusing and scheme should be clear enough.\n\nI like the extra scheme extractor and how that cleans up the implementations. I hadn't seen the `require` method on Directive1 yet, nice!.\n\nI'l probably get around to updating this PR tonight.\n. I was just working on this and I ran into a problem that I also ran into on the compression PR, namely that Scala gets very confused when there are two overloaded methods, once of which has no arguments, and you want to apply the resulting function to an argument. In this case, when you write something like this:\n\n```\n\"the scheme directive\" should {\n    \"extract the Uri scheme\" in {\n      Put(\"http://localhost/\", \"Hello\") ~> scheme { echoComplete } ~> check { entityAs[String] === \"http\" }\n    }\n  }\n```\n\nit tries to call the version of scheme that takes one String argument with `echoComplete` as the argument, resulting in:\n\n```\nSchemeDirectivesSpec.scala:23: polymorphic expression cannot be instantiated to expected type;\n[error]  found   : [T]T => (spray.routing.RequestContext => Unit)\n[error]  required: String\n[error]       Put(\"http://localhost/\", \"Hello\") ~> scheme { echoComplete } ~> check { entityAs[String] === \"http\" }\n```\n\nThe two obvious workarounds, always using `scheme()` or renaming one of the two directives, are not very elegant but AFAIK the magnet pattern doesn't work with zero-arg methods. Am I wrong?\n\nWhat do you guys think?\n. OK, will rename to schemeName. That also makes it easier to document (no naming conflict) so I'll make an initial attempt at that too.\n. That should do it. \n\nI included full directive documentation with source samples, all based on the well documented `headerValueByName` directive. Of course this took a lot longer than I had thought, especially coming up with some good examples and figuring out the correct includecode snippet incantations :)\n\nThe `schemeName` directive is modeled after the `hostName` directive. Both directives will extract an empty String if no matching value could be found but I wonder whether an `Option` would be a better fit.\n\nFYI: I noticed that the hostName directive is untested.\n. > > figuring out the correct includecode snippet incantations\n> \n> Is there anything wrong with it that we could improve?\n\nThe file contains two directives, both starting with `scheme`. When I tried to refer to the `scheme` snippet it included the signatures of both directives. I had a look at the Python, tried a bunch of things based on hunches, and eventually found that using `scheme(` did the trick. It was around 01:00 so I didn't look into the root cause of this.\n. I have no idea why the build is failing. I did two complete clean builds (clean compile test) of my branch locally and they ran without problems. Did something change on master and does Travis do some kind of merge with master?\n. Aha, Mathias changed the require directive! This PR is doomed to stay unmerged :)\n\nI'll update the branch and resubmit ASAP.\n. Updated and fixed\n. That should fix all your comments... I hope :)\n. Done\n. Ok, thanks!\n\nGithub search clearly sucks because I searched for \"client proxy server\" :(\n. Aha! :)\n. Thanks, I fixed it\n. ok, will do it tonight\n. Did it earlier :)\nThis should be clear enough.\n. I think I have some way to go before I can overtake you guys :)\n. I was actually basing my (bad) behaviour on the (old?) placeholders (using the `//#...#` syntax) I saw all over the directives api. It looked like this is how things were done but now that I think about it they have been slowly disappearing over time.\n\nIn the case of the documentation I wrote for the new encoding directives, the \"real\" test cases were so close to the ones I would have written to demo usage that my \"must never duplicate code\" alarms were tripped. Let me know if you still want me to extract them to the example code base.\n. Now I'm curious... what exactly is wrong with that line and how can it lead to a MatchError on a MissingCookieRejection, which seems to be handled just fine on line 57?\n\nhttps://github.com/spray/spray/blob/9c251c4a41adbcb3918b2ff2e37d86a62d821edc/spray-routing/src/main/scala/spray/routing/RejectionHandler.scala#L57\n. Ah, I did not notice the flatMap instead of the collect. All clear now :)\n. Hi guys. I've been away from Spray for a few months (traveling the world) but I'm back and here's a pull request to prove it :)\n\nThis is a first attempt and I think there could be some room for cleanup here and there but it gets the job done. It should help quite a bit with the perceived performance of serving static files, assuming of course that everyone is already using Cache-Control.\n\nLooking forward to your comments,\nAge\n. That is indeed a mysterious message. I'll have a look ASAP\n. Interesting indeed... the test (and the parser/renderer) is exactly equal to the one for Last-Modified:\n\n``` scala\n\"If-Modified-Since\" in {\n  \"If-Modified-Since: Wed, 13 Jul 2011 08:12:31 GMT\" =!= `If-Modified-Since`(DateTime(2011, 7, 13, 8, 12, 31))\n}\n\n\"Last-Modified\" in {\n  \"Last-Modified: Wed, 13 Jul 2011 08:12:31 GMT\" =!= `Last-Modified`(DateTime(2011, 7, 13, 8, 12, 31))\n}\n```\n\nSo I'm not sure what could have gone wrong there... any ideas?\n. I can now also reproduce this locally. One of those awesome \"Funny, it used to work before\" things :(\n\nI'll look into it later.\n. Pushed too soon. Adding a header has quite some side effects on the other sub-projects and I obviously forgot to run all the tests :( Fixed the spray-http tests and fixing spray-can now..\n. OK, all tests are green now. sorry for not testing better before submitting the request.\n\nThe `HttpHeaderParserSpec` is very much hardcoded and I do not have the deep insight to see why values have to be the exact values expected so I just adjusted all tests to fit the new reality. I'm not sure how to verify whether those new values are \"correct\" though :(\n. Great idea! Let me see if I can get that working. It might take a few days before I have enough spare time though.\n\nThis would also solve my impending problem of \"how do I add ETag support to this without making it even more complex\" ;)\n. It looks pretty good. Great that you also added ETags already! \n\nA few remarks:\n- AFAIK the value of `If-None-Match` could also be a wildcard (i.e. `*`). See http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.26\n- Using response mapping would be slightly less performant because the original response is actually produced and then discarded. Static files would always be read into memory, even if they hadn't been changed. I'm not sure how important this is though.\n- We also need a directive that actually generates an ETag for a resource, be it static files or any entity. The usual solution is to take an md5 of the entity body so this should not be that hard.\n- I would still write some more integration-style unit tests to verify that `If-Modified-since` and `If-None-Match` work correctly with static files, since that is their main purpose.\n\nHow do you want to continue?\n\nAge\n. > > Using response mapping would be slightly less performant because the original response is actually produced and then discarded. Static files would always be read into memory, even if they hadn't been changed. I'm not sure how important this is though.\n> \n> That doesn't seem to be the case for files. FileAndResourceDirectives uses HttpData(file) where the file content is copied by the marshaller. Resources are read at completion though. Since 'File.lastModified' is rather cheap it gets added by default (source).\n\nThe implementation of `getFromResource` does read the file into memory ([source](https://github.com/spray/spray/blob/master/spray-routing/src/main/scala/spray/routing/directives/FileAndResourceDirectives.scala#L119)). I guess we could make a version of HttpData that would wrap an inputStream and only actually reads it on demand.\n\n> The downside of generic hashing would be that you would have to always look at the content while a custom hashing function could use pre-generated data. How would you handle large chunked files for example? Maybe we can come up with support for generic hashing later.\n\nWhat kind of pre-generated data did you have in mind? I would say that the default strategy should look at the response data but that you could customize that to your liking.\n\n> > We also need a directive that actually generates an ETag for a resource, be it static files or any entity. The usual solution is to take an md5 of the entity body so this should not be that hard.\n> \n> Right. respondWithHeader(Etag(yourGeneratedMd5)) is all that is needed. yourGeneratedMd5 could hash the response on the fly or could look at existing data\n\nIt seems most http servers, like Apache, simply take a combination of the last modified timestamp and the file size and MD5 that. File size is just as cheap to compute as last modified AFAIK. But I agree that you should be able to customize how this hash gets generated. I would more go for something like this:\n\n```\nETag {\n  // produce response\n}\n```\n\nwhere the `ETag` directive could use an implicit strategy with the default strategy producing an `ETag` header if the `Last-Modified` header is set, combining that with the length of the `HttpData` in the response into an MD5 hash..\n. Agreed. Let's see what @sirthias and @jrudolph think.\n. Looks interesting, I'd like to experiment some more with this. I'm very busy at the moment but I'll hopefully have time soon to implement this and do some experiments in the sample routing app to see how this would work with both static files and api responses.\n\nI'll let you know how far I got before you're back. Have a great holiday!\n. Yes, I would. It does what it says in the title of this issue.\n\nI like Andr\u00e9's stuff but I haven't had time to merge anything in yet. We can move that to the existing ETag issue (#76) and take it from there.\n. I'm still curious as to whether you like the \"built into the file/resource directives\" approach, which is slightly more efficient, or Andr\u00e9's more general approach.\n. Looks good. A few remarks:\n- I'm not a big fan of the name `clientCache` since it doesn't really describe what happens here. I would go for something like `ifNotModified` or something similar.\n- I would add a few tests that specifically show this directive in combination with the file/resource directives\n- I would probably add a default implementation of ETags for files/resources. That's the one area where these features are most sorely missed, at least for the apps I'm currently working on. \n\nI do realize that a proper default ETag implementation is not trivial to do though, looking at for instance the discussion in [this Apache issue](https://issues.apache.org/bugzilla/show_bug.cgi?id=39727) it seems that ETags need to be different for Gzipped vs unGzipped content so maybe that's a step too far for this pull request.\n. +1.\n\nWhat would you suggest as the default implementation for file/resource ETags?\n. By the way, this pull request will fix issues #76 and #219.\n. Sounds good.\n. I'll buy a round as well! ;)\n\nI'm looking forward to see you guys in Berlin.\n. It's interesting that you guys are already moving towards HTTPbis. This is great but I can imagine that supporting both versions of the protocol can sometimes lead to conflicts such as this one, where older clients expect (slightly) different behaviour. Do you already have ideas/plans on how to deal with such situations?\n. I'm pretty new to HTTPbis and the location of the relevant specs. Are you referring to this section?\n\nhttp://tools.ietf.org/html/draft-ietf-httpbis-p4-conditional-26#section-6\n. Just a lucky find. My first Google hit was this nice overview page:\n\nhttps://tools.ietf.org/wg/httpbis/\n\nI had no idea that there was more to HTTPbis then Http 2.0 but these 1.1 rewrites and clarifications are indeed a great resource.\n. Awesome work Andr\u00e9! Thanks for pushing this forward!\n\n> On 07 Mar 2014, at 18:05, 2beaucoup notifications@github.com wrote:\n> \n>  Nice weekend!\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. The HTTPbis updated specs have a bit more detail here:\n\nhttp://tools.ietf.org/html/draft-ietf-httpbis-p2-semantics-26#section-6.3.6\n\nSpecifically:\n\n> In other words, a server MUST do one of the following for a 205\n>    response: a) indicate a zero-length body for the response by\n>    including a Content-Length header field with a value of 0; b)\n>    indicate a zero-length payload for the response by including a\n>    Transfer-Encoding header field with a value of chunked and a message\n>    body consisting of a single chunk of zero-length; or, c) close the\n>    connection immediately after sending the blank line terminating the\n>    header section.\n\nI'm not sure what your stub server produces that could mess with the normal spray-client response handling. Perhaps your server produces case c) ?\n\nCould you perhaps post the complete output of a curl call to such a server? Does it satisfy the above specs?\n\nDoes spray-client time out on receiving the response? Have you tested this with other servers that produce 205 responses?\n\n@jrudolph: @hierynomus is a colleague of mine, hence the third degree barrage of questions ;)\n. Any near future plans to release a 1.x.2 release?\n\nIt's not a big deal and we can easily work around it with a copy of SprayJsonSupport but I'm lazy so if you were planning to release soon anyway... ;)\n. Awesome. I would offer my services if it weren't for the fact that I'll be on holiday for the next three weeks. See you at Scala.IO ;)\n. ",
    "mackler": "> 2beaucoup commented\n> WDYT?\n\nIf spray had full proxy support (HTTP/CONNECT), I would be using it.\n. > 2beaucoup commented\n> WDYT?\n\nIf spray had full proxy support (HTTP/CONNECT), I would be using it.\n. ",
    "alexrom": "Hi Mathias,\n\nYes, sure, I'll do that.\nIn fact I've tried but got bunch of failures running test. Most likely \nrelated to me running the test on Windows system = CRNL vs NL.\n\nOn Thursday, May 03, 2012 5:53:00 AM, Mathias wrote:\n\n> Alex,\n> great!\n> Thanks for this fix.\n> However, could you apply your fix to the `develop` branch rather than the `master` and re-issue the pull request?\n> (We follow the _git-flow_ branching model and do most of our work on the `develop` branch)...\n> \n> ---\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/spray/spray/pull/105#issuecomment-5484317\n. Hi Mathias,\n\nYes, sure, I'll do that.\nIn fact I've tried but got bunch of failures running test. Most likely \nrelated to me running the test on Windows system = CRNL vs NL.\n\nOn Thursday, May 03, 2012 5:53:00 AM, Mathias wrote:\n\n> Alex,\n> great!\n> Thanks for this fix.\n> However, could you apply your fix to the `develop` branch rather than the `master` and re-issue the pull request?\n> (We follow the _git-flow_ branching model and do most of our work on the `develop` branch)...\n> \n> ---\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/spray/spray/pull/105#issuecomment-5484317\n. ",
    "samira-t": "Thanks or response. I will post my questions in the mailing list afterward. The statement in the documentation does not mean that you cannot use actors for blocking operations. The intention is to let users know that if the actors use a blocking operation, then they will occupy a dedicated thread and that might ruin the performance. If you dedicate a thread for IO worker, then that means you can do that by actors as well.\n. Hello Mathias,\n\nI want to come back to the discussions that we had before. Since Akka has IOManagerActor which is for managing IO, why didn't you use that for your application and decided to go wit your own IO implementation?  \n\nThanks,\nSamira\n. Hi Mathias,\nI am extremely sorry. I totally forgot about that. I will post the question\nin the mailing list.\n\nSorry again,\nSamira\n\nOn Thu, Aug 30, 2012 at 2:44 AM, Mathias notifications@github.com wrote:\n\n> Samira,\n> as I suggested before, I am happy to answer any questions on the mailing\n> list, not in this issue tracking area.\n> Cheers,\n> Mathias\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/issues/110#issuecomment-8151711.\n. Thanks or response. I will post my questions in the mailing list afterward. The statement in the documentation does not mean that you cannot use actors for blocking operations. The intention is to let users know that if the actors use a blocking operation, then they will occupy a dedicated thread and that might ruin the performance. If you dedicate a thread for IO worker, then that means you can do that by actors as well.\n. Hello Mathias,\n\nI want to come back to the discussions that we had before. Since Akka has IOManagerActor which is for managing IO, why didn't you use that for your application and decided to go wit your own IO implementation?  \n\nThanks,\nSamira\n. Hi Mathias,\nI am extremely sorry. I totally forgot about that. I will post the question\nin the mailing list.\n\nSorry again,\nSamira\n\nOn Thu, Aug 30, 2012 at 2:44 AM, Mathias notifications@github.com wrote:\n\n> Samira,\n> as I suggested before, I am happy to answer any questions on the mailing\n> list, not in this issue tracking area.\n> Cheers,\n> Mathias\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/issues/110#issuecomment-8151711.\n. ",
    "kevinwright": "Of course it was intended!  It's just an upgrade to SBT 0.11.3 (plus mathias's merged changes)\n. Of course it was intended!  It's just an upgrade to SBT 0.11.3 (plus mathias's merged changes)\n. ",
    "alambert": "Hey Mathias,\n\nNo worries; we're OK with upgrading to 1.0 for this change :-) (Sorry, I would have provided a patch against the latest branch but had some trouble with my build environment.)\n\nThanks,\nAlex\n. Hey Mathias,\n\nNo worries; we're OK with upgrading to 1.0 for this change :-) (Sorry, I would have provided a patch against the latest branch but had some trouble with my build environment.)\n\nThanks,\nAlex\n. ",
    "danielschonfeld": "Sorry, it took me so long to get back to you Mathias.  Is this something you are still interested in?  should I get to work?\n. I'm not sure why one of my builds fails and the other succeeds.  I've also tried adding a Queue to the currentUri and that breaks the build completely.  Here's a log of that\nhttps://travis-ci.org/spray/spray/builds/7791772\n\nI'd appreciate some pointers as to how to get this to work properly. Thanks!!\n. Ok both will be done, but could you tell me what you had in mind in term of polishing up the CookieJar implementation.... what/how would you like it to do and be available for other uses as part of the spray.http package?\n\nI'm relatively new to sharing on projects so I'll need a lil bit of guidance.  I don't mind going through as many iterations as needed until we get it to fit just right for spray.\n. Okay so I'm a little confused.  Do you prefer to not move forward with the cookie jar implementation at this time?\n\nThe reason I chose it to be a pipeline stage is because the act of sweeping the cookies off the request and putting them back into one is a connection feature.  I believe the location what I put my jar val applies once to any instance of a client so in all actuality the jar is shared among multiple connections.  Am I wrong in that assumption?\n\nI don't feel too strongly about the location of the implementation so if you do and believe HttpClientSettingsGroup is a better suited place to start the HttpClientConnection with a List[HttpCookie] I'll be more than happy to change that.\n\nWhat were you referring to about paths?....\n. Sorry, it took me so long to get back to you Mathias.  Is this something you are still interested in?  should I get to work?\n. I'm not sure why one of my builds fails and the other succeeds.  I've also tried adding a Queue to the currentUri and that breaks the build completely.  Here's a log of that\nhttps://travis-ci.org/spray/spray/builds/7791772\n\nI'd appreciate some pointers as to how to get this to work properly. Thanks!!\n. Ok both will be done, but could you tell me what you had in mind in term of polishing up the CookieJar implementation.... what/how would you like it to do and be available for other uses as part of the spray.http package?\n\nI'm relatively new to sharing on projects so I'll need a lil bit of guidance.  I don't mind going through as many iterations as needed until we get it to fit just right for spray.\n. Okay so I'm a little confused.  Do you prefer to not move forward with the cookie jar implementation at this time?\n\nThe reason I chose it to be a pipeline stage is because the act of sweeping the cookies off the request and putting them back into one is a connection feature.  I believe the location what I put my jar val applies once to any instance of a client so in all actuality the jar is shared among multiple connections.  Am I wrong in that assumption?\n\nI don't feel too strongly about the location of the implementation so if you do and believe HttpClientSettingsGroup is a better suited place to start the HttpClientConnection with a List[HttpCookie] I'll be more than happy to change that.\n\nWhat were you referring to about paths?....\n. ",
    "ijuma": "Thanks for merging this.\n. This would be awesome.\n. How about using LongAdder from jsr166?\n. Shapeless 1.2.4 has been published for Scala 2.11[1]. Is that the only blocker to getting Spray 1.3 published for Scala 2.11?\n\n[1] https://twitter.com/milessabin/status/456884734375649280\n. That would work for us. We use our own (un)marshalling support for lift-json anyway. Slick published 2.1.0-M1 for Scala 2.11 yesterday, so Spray and lift-json are the last two Scala libraries that prevent us from moving to Scala 2.11. We'll probably have to publish lift-json to our internal Maven repository as it may take a while before official support arrives.\n. Brilliant. Thanks!\n. Thanks for the update, @sirthias. I noticed the same thing myself yesterday when I tried to update the spray branch to 2.11.0 final. Do you have any idea when Akka will publish the outstanding modules for 2.11.0?\n. Your take was right: https://github.com/akka/akka/issues/15045#issuecomment-41158008 (although it seems to be propagating still)\n. Awesome, thanks. If everything goes to plan (ie tests pass), this will be in production by tomorrow. :)\n. I'm surprised to see such fragile code in spray. Would definitely be good to replace it.\n. Thanks for merging this.\n. This would be awesome.\n. How about using LongAdder from jsr166?\n. Shapeless 1.2.4 has been published for Scala 2.11[1]. Is that the only blocker to getting Spray 1.3 published for Scala 2.11?\n\n[1] https://twitter.com/milessabin/status/456884734375649280\n. That would work for us. We use our own (un)marshalling support for lift-json anyway. Slick published 2.1.0-M1 for Scala 2.11 yesterday, so Spray and lift-json are the last two Scala libraries that prevent us from moving to Scala 2.11. We'll probably have to publish lift-json to our internal Maven repository as it may take a while before official support arrives.\n. Brilliant. Thanks!\n. Thanks for the update, @sirthias. I noticed the same thing myself yesterday when I tried to update the spray branch to 2.11.0 final. Do you have any idea when Akka will publish the outstanding modules for 2.11.0?\n. Your take was right: https://github.com/akka/akka/issues/15045#issuecomment-41158008 (although it seems to be propagating still)\n. Awesome, thanks. If everything goes to plan (ie tests pass), this will be in production by tomorrow. :)\n. I'm surprised to see such fragile code in spray. Would definitely be good to replace it.\n. ",
    "tekener": "Hi Mathias,\n\nevery test is green now. By the way, i had some java.lang.StackOverflowError  running \"sbt test\". The solution was to increase stack size to 2M. \n\nSBT_OPTS=\"-XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256m -Xmx1512M -Xss2M\"\n\nGreeting, Bj\u00f6rn\n. Hi Mathias,\n\nevery test is green now. By the way, i had some java.lang.StackOverflowError  running \"sbt test\". The solution was to increase stack size to 2M. \n\nSBT_OPTS=\"-XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256m -Xmx1512M -Xss2M\"\n\nGreeting, Bj\u00f6rn\n. ",
    "nevang": "One thought: Will the client be informed about the redirect? If the redirect is permanent all future request should use the new uri.\n. Mathias, I did try to create an authentication mechanism with custom directives. On of them was named optionalUser and it was based on optionalCookie. This produced some unexpected MissingCookieRejections. \n\nOn the following sample visiting the path `/test?test=fail` generates a MissingCookieRejection. I would expect only the ValidationRejection. As cookie is optional a rejection should not be generated.\n\n``` scala\nget {\n  optionalCookie(\"test-cookie\") { _ =>\n    (path(\"test\") & parameter('test)) { test =>\n      validate(test == \"pass\", \"wrong parameter\") {\n        complete(\"ok\")\n      }\n    }\n  }\n}\n```\n\nYou can also see the [optionalHeaderValue directive](https://github.com/spray/spray/blob/master/spray-routing/src/main/scala/spray/routing/directives/HeaderDirectives.scala#L65). It is based on headerValue but it recover from the rejection (except from the exceptions).\n. Mathias, you could you do it with a foldLeft: \n\n``` scala\ndef separateOnSlashes(string: String): PathMatcher0 =\n    string.fastSplit('/').foldLeft(Empty)(_ / _)\n```\n\nThe problem is Empty which has type PathMatcher[HNil.type]. It has to be defined as: \n\n``` scala\nval Empty: PathMatcher0 = PathMatcher.provide(HNil)\n```\n. Thanks Mathias for the clarification.\n. One thought: Will the client be informed about the redirect? If the redirect is permanent all future request should use the new uri.\n. Mathias, I did try to create an authentication mechanism with custom directives. On of them was named optionalUser and it was based on optionalCookie. This produced some unexpected MissingCookieRejections. \n\nOn the following sample visiting the path `/test?test=fail` generates a MissingCookieRejection. I would expect only the ValidationRejection. As cookie is optional a rejection should not be generated.\n\n``` scala\nget {\n  optionalCookie(\"test-cookie\") { _ =>\n    (path(\"test\") & parameter('test)) { test =>\n      validate(test == \"pass\", \"wrong parameter\") {\n        complete(\"ok\")\n      }\n    }\n  }\n}\n```\n\nYou can also see the [optionalHeaderValue directive](https://github.com/spray/spray/blob/master/spray-routing/src/main/scala/spray/routing/directives/HeaderDirectives.scala#L65). It is based on headerValue but it recover from the rejection (except from the exceptions).\n. Mathias, you could you do it with a foldLeft: \n\n``` scala\ndef separateOnSlashes(string: String): PathMatcher0 =\n    string.fastSplit('/').foldLeft(Empty)(_ / _)\n```\n\nThe problem is Empty which has type PathMatcher[HNil.type]. It has to be defined as: \n\n``` scala\nval Empty: PathMatcher0 = PathMatcher.provide(HNil)\n```\n. Thanks Mathias for the clarification.\n. ",
    "theon": "I guess this issue can be closed now https://github.com/spray/spray/pull/560 has been merged.\n. Ah thanks. We were using a nightly build from between the time LiftJsonSupport was removed and Json4sSupport added, so were using our own Json4s Unmarshaller. Now we have updated to M8 we can used the predefined ones.\n\nPlease ignore this pull request :smile: \n. +1 :+1:\n. We had done exactly what @zcox mentioned and copied this PR into an external library, but we only had it on an internal source control system. We thought it would be worth sharing on github and publishing to maven central, so we have put it [here](https://github.com/NET-A-PORTER/salad-metrics-core)\n\n(The name is because we were using it for a project called salad :smile: )\n\nFeel free to submit pull requests for clean up and we be happy to merge them in.\n. Thanks for your detailed feedback Mathias.\n\nI've updated the pull request and should have addressed all your comments. I've also emailed the CLA.\n\nI did a force push to rewrite the commit message and it lost all the previous comments inline with the code. I wasn't sure how to rewrite the commit message without this happening.\n. Updated for latest feedback.\n. Maybe just `methodParameter` is a better name.\n. Thanks for the feedback. I have made the suggested changes.\n. Good spot - I've fixed the test description now.\n. If the tests all pass on travis, then this is ready for a second peer review.\n. Hi Mathias,\n\nReady for another peer review. In addition to what we discussed, I also added handling for `308` responses. The behaviour looks like this now:\n\n| Request Method | Response Status Code | Redirection Method | Specification |\n| --- | :-: | --: | --: |\n| GET / HEAD | 301 / 302 / 303 / 307 | Original request method | [RFC 2616](http://tools.ietf.org/html/rfc2616#section-10.3) |\n| Any (except GET / HEAD) | 302 / 303 | GET | [RFC 2616](http://tools.ietf.org/html/rfc2616#section-10.3) |\n| Any | 308 | Original request method | [Draft Spec](http://tools.ietf.org/html/draft-reschke-http-status-308-07#section-3) |\n. No problem; let me know if you have any more feedback.  We have a work-around in our spray-client pipelines to follow redirects for the meantime so it's not super urgent, but like all work-arounds, it would be nice to get rid of it.\n. Done!\n. @laogao yes, `spray.can.host-connector.follow-redirects` was never made it master. Behaviour in this PR changed to what you describe before it was merged. I have edited the description of this PR to prevent further confusion.\n. Something I missed was that I am also seeing a second stack trace a few seconds later that says `Unexpected slot state: Idle`.\n\n```\n[ERROR] [11/04/2013 10:53:34.695] [netaporter-akka.actor.default-dispatcher-3] [akka://netaporter/user/IO-HTTP/group-0/0] null\njava.lang.IllegalStateException\n    at spray.io.SslTlsSupport$$anon$1$$anon$2.spray$io$SslTlsSupport$$anon$$anon$$verify(SslTlsSupport.scala:361)\n    at spray.io.SslTlsSupport$$anon$1$$anon$2$$anon$6$$anonfun$2.apply(SslTlsSupport.scala:92)\n    at spray.io.SslTlsSupport$$anon$1$$anon$2$$anon$6$$anonfun$2.apply(SslTlsSupport.scala:81)\n    at spray.io.SslTlsSupport$$anon$1$$anon$2.process(SslTlsSupport.scala:232)\n    at spray.io.DynamicPipelines$$anonfun$eventPipeline$1.apply(Pipelines.scala:48)\n    at spray.io.DynamicPipelines$$anonfun$eventPipeline$1.apply(Pipelines.scala:48)\n    at spray.io.RawPipelineStage$$anon$3$$anonfun$2.apply(Pipelines.scala:115)\n    at spray.io.RawPipelineStage$$anon$3$$anonfun$2.apply(Pipelines.scala:115)\n    at spray.io.TickGenerator$$anon$1$$anon$2$$anonfun$1.apply(TickGenerator.scala:41)\n    at spray.io.TickGenerator$$anon$1$$anon$2$$anonfun$1.apply(TickGenerator.scala:38)\n    at spray.io.ConnectionHandler$$anonfun$running$1.applyOrElse(ConnectionHandler.scala:58)\n    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:165)\n    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:425)\n    at akka.actor.ActorCell.invoke(ActorCell.scala:386)\n    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:230)\n    at akka.dispatch.Mailbox.run(Mailbox.scala:212)\n    at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:506)\n    at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:262)\n    at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)\n    at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1478)\n    at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\n\n[ERROR] [11/04/2013 10:53:34.710] [netaporter-akka.actor.default-dispatcher-10] [akka://netaporter/user/IO-HTTP/group-0/0] Monitored actor [Actor[akka://netaporter/system/IO-TCP/selectors/$a/0]] terminated (akka.actor.DeathPactException)\n[WARN] [11/04/2013 10:53:34.716] [netaporter-akka.actor.default-dispatcher-7] [akka://netaporter/user/IO-HTTP/group-0/0] event pipeline: dropped Connected(naplabs.dave.net-a-porter.com/172.29.130.33:443,/10.5.18.105:65298)\n[ERROR] [11/04/2013 10:53:37.715] [netaporter-akka.actor.default-dispatcher-10] [akka://netaporter/user/IO-HTTP/host-connector-1] Unexpected slot state: Idle\njava.lang.IllegalStateException: Unexpected slot state: Idle\n    at spray.can.client.HttpHostConnector$$anonfun$receive$1.applyOrElse(HttpHostConnector.scala:81)\n    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:425)\n    at akka.actor.ActorCell.invoke(ActorCell.scala:386)\n    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:230)\n    at akka.dispatch.Mailbox.run(Mailbox.scala:212)\n    at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:506)\n    at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:262)\n    at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)\n    at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1478)\n    at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\n\n[WARN] [11/04/2013 10:54:03.945] [qtp276382433-345] [Servlet30ConnectorServlet(akka://netaporter)] Timeout of HttpRequest(GET,http://localhost:28080/api/,List(Host: localhost:28080, Accept: */*, Accept-Language: en-us, Connection: keep-alive, User-Agent: CocoaRestClient/11 CFNetwork/520.5.1 Darwin/11.4.2 (x86_64) (MacBookPro9%2C2), Accept-Encoding: gzip, deflate),Empty,HTTP/1.1)\n[ERROR] [11/04/2013 10:54:04.438] [netaporter-akka.actor.default-dispatcher-7] [akka://netaporter/user/woas-root/rest-routing] Unable to complete request. request=4d696639-7d70-4556-92d9-a8f54233aab3\nakka.pattern.AskTimeoutException: Timed out\n    at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:312)\n    at akka.actor.DefaultScheduler$$anon$8.run(Scheduler.scala:191)\n    at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:137)\n    at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:506)\n    at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:262)\n    at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)\n    at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1478)\n    at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\n\n[WARN] [11/04/2013 10:54:04.487] [netaporter-akka.actor.default-dispatcher-7] [Servlet30ConnectorServlet(akka://netaporter)] Received a second response for a request that was already completed, dropping ...\nRequest: HttpRequest(GET,http://localhost:28080/api/,List(Host: localhost:28080, Accept: */*, Accept-Language: en-us, Connection: keep-alive, User-Agent: CocoaRestClient/11 CFNetwork/520.5.1 Darwin/11.4.2 (x86_64) (MacBookPro9%2C2), Accept-Encoding: gzip, deflate),Empty,HTTP/1.1)\nResponse: HttpResponse(500 Internal Server Error,HttpEntity(application/json; charset=UTF-8,{\"@type\":\"/api/schemas/error\"}),List(),HTTP/1.1)\n```\n. Thanks,\n\nWith the nightly I still get the first `IllegalStateException`, but I no longer `Unexpected slot state: Idle` exception. \n\n```\n[ERROR] [11/04/2013 11:26:03.212] [netaporter-akka.actor.default-dispatcher-18] [akka://netaporter/user/IO-HTTP/group-0/6] null\njava.lang.IllegalStateException\n    at spray.io.SslTlsSupport$$anon$1$$anon$2.spray$io$SslTlsSupport$$anon$$anon$$verify(SslTlsSupport.scala:361)\n    at spray.io.SslTlsSupport$$anon$1$$anon$2$$anon$6$$anonfun$2.apply(SslTlsSupport.scala:92)\n    at spray.io.SslTlsSupport$$anon$1$$anon$2$$anon$6$$anonfun$2.apply(SslTlsSupport.scala:81)\n    at spray.io.SslTlsSupport$$anon$1$$anon$2.process(SslTlsSupport.scala:232)\n    at spray.io.DynamicPipelines$$anonfun$eventPipeline$1.apply(Pipelines.scala:48)\n    at spray.io.DynamicPipelines$$anonfun$eventPipeline$1.apply(Pipelines.scala:48)\n    at spray.io.RawPipelineStage$$anon$3$$anonfun$2.apply(Pipelines.scala:115)\n    at spray.io.RawPipelineStage$$anon$3$$anonfun$2.apply(Pipelines.scala:115)\n    at spray.io.TickGenerator$$anon$1$$anon$2$$anonfun$1.apply(TickGenerator.scala:41)\n    at spray.io.TickGenerator$$anon$1$$anon$2$$anonfun$1.apply(TickGenerator.scala:38)\n    at spray.io.ConnectionHandler$$anonfun$running$1.applyOrElse(ConnectionHandler.scala:58)\n    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:165)\n    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:425)\n    at akka.actor.ActorCell.invoke(ActorCell.scala:386)\n    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:230)\n    at akka.dispatch.Mailbox.run(Mailbox.scala:212)\n    at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:506)\n    at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:262)\n    at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)\n    at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1478)\n    at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\n\n[ERROR] [11/04/2013 11:26:03.216] [netaporter-akka.actor.default-dispatcher-20] [akka://netaporter/user/IO-HTTP/group-0/6] Monitored actor [Actor[akka://netaporter/system/IO-TCP/selectors/$a/14]] terminated (akka.actor.DeathPactException)\n[WARN] [11/04/2013 11:26:03.222] [netaporter-akka.actor.default-dispatcher-20] [akka://netaporter/user/IO-HTTP/group-0/6] event pipeline: dropped Connected(naplabs.dave.net-a-porter.com/172.29.130.33:443,/10.5.18.105:49840)\n[WARN] [11/04/2013 11:26:32.785] [qtp1721461644-84] [Servlet30ConnectorServlet(akka://netaporter)] Timeout of HttpRequest(GET,http://localhost:28080/api/,List(Host: localhost:28080, Accept: */*, Accept-Language: en-us, Connection: keep-alive, User-Agent: CocoaRestClient/11 CFNetwork/520.5.1 Darwin/11.4.2 (x86_64) (MacBookPro9%2C2), Accept-Encoding: gzip, deflate),Empty,HTTP/1.1)\n[ERROR] [11/04/2013 11:26:32.867] [netaporter-akka.actor.default-dispatcher-20] [akka://netaporter/user/woas-root/rest-routing] Unable to complete request. request=348e9e95-b8d2-43b2-8627-dab538bb2e41\nakka.pattern.AskTimeoutException: Timed out\n    at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:312)\n    at akka.actor.DefaultScheduler$$anon$8.run(Scheduler.scala:191)\n    at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:137)\n    at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:506)\n    at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:262)\n    at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)\n    at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1478)\n    at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\n\n[WARN] [11/04/2013 11:26:32.869] [netaporter-akka.actor.default-dispatcher-20] [Servlet30ConnectorServlet(akka://netaporter)] Received a second response for a request that was already completed, dropping ...\nRequest: HttpRequest(GET,http://localhost:28080/api,List(Host: localhost:28080, Accept: */*, Accept-Language: en-us, Connection: keep-alive, User-Agent: CocoaRestClient/11 CFNetwork/520.5.1 Darwin/11.4.2 (x86_64) (MacBookPro9%2C2), Accept-Encoding: gzip, deflate),Empty,HTTP/1.1)\nResponse: HttpResponse(500 Internal Server Error,HttpEntity(application/json; charset=UTF-8,{\"@type\":\"/api/schemas/error\"}),List(),HTTP/1.1)\n,,,\n```\n. I guess this issue can be closed now https://github.com/spray/spray/pull/560 has been merged.\n. Ah thanks. We were using a nightly build from between the time LiftJsonSupport was removed and Json4sSupport added, so were using our own Json4s Unmarshaller. Now we have updated to M8 we can used the predefined ones.\n\nPlease ignore this pull request :smile: \n. +1 :+1:\n. We had done exactly what @zcox mentioned and copied this PR into an external library, but we only had it on an internal source control system. We thought it would be worth sharing on github and publishing to maven central, so we have put it [here](https://github.com/NET-A-PORTER/salad-metrics-core)\n\n(The name is because we were using it for a project called salad :smile: )\n\nFeel free to submit pull requests for clean up and we be happy to merge them in.\n. Thanks for your detailed feedback Mathias.\n\nI've updated the pull request and should have addressed all your comments. I've also emailed the CLA.\n\nI did a force push to rewrite the commit message and it lost all the previous comments inline with the code. I wasn't sure how to rewrite the commit message without this happening.\n. Updated for latest feedback.\n. Maybe just `methodParameter` is a better name.\n. Thanks for the feedback. I have made the suggested changes.\n. Good spot - I've fixed the test description now.\n. If the tests all pass on travis, then this is ready for a second peer review.\n. Hi Mathias,\n\nReady for another peer review. In addition to what we discussed, I also added handling for `308` responses. The behaviour looks like this now:\n\n| Request Method | Response Status Code | Redirection Method | Specification |\n| --- | :-: | --: | --: |\n| GET / HEAD | 301 / 302 / 303 / 307 | Original request method | [RFC 2616](http://tools.ietf.org/html/rfc2616#section-10.3) |\n| Any (except GET / HEAD) | 302 / 303 | GET | [RFC 2616](http://tools.ietf.org/html/rfc2616#section-10.3) |\n| Any | 308 | Original request method | [Draft Spec](http://tools.ietf.org/html/draft-reschke-http-status-308-07#section-3) |\n. No problem; let me know if you have any more feedback.  We have a work-around in our spray-client pipelines to follow redirects for the meantime so it's not super urgent, but like all work-arounds, it would be nice to get rid of it.\n. Done!\n. @laogao yes, `spray.can.host-connector.follow-redirects` was never made it master. Behaviour in this PR changed to what you describe before it was merged. I have edited the description of this PR to prevent further confusion.\n. Something I missed was that I am also seeing a second stack trace a few seconds later that says `Unexpected slot state: Idle`.\n\n```\n[ERROR] [11/04/2013 10:53:34.695] [netaporter-akka.actor.default-dispatcher-3] [akka://netaporter/user/IO-HTTP/group-0/0] null\njava.lang.IllegalStateException\n    at spray.io.SslTlsSupport$$anon$1$$anon$2.spray$io$SslTlsSupport$$anon$$anon$$verify(SslTlsSupport.scala:361)\n    at spray.io.SslTlsSupport$$anon$1$$anon$2$$anon$6$$anonfun$2.apply(SslTlsSupport.scala:92)\n    at spray.io.SslTlsSupport$$anon$1$$anon$2$$anon$6$$anonfun$2.apply(SslTlsSupport.scala:81)\n    at spray.io.SslTlsSupport$$anon$1$$anon$2.process(SslTlsSupport.scala:232)\n    at spray.io.DynamicPipelines$$anonfun$eventPipeline$1.apply(Pipelines.scala:48)\n    at spray.io.DynamicPipelines$$anonfun$eventPipeline$1.apply(Pipelines.scala:48)\n    at spray.io.RawPipelineStage$$anon$3$$anonfun$2.apply(Pipelines.scala:115)\n    at spray.io.RawPipelineStage$$anon$3$$anonfun$2.apply(Pipelines.scala:115)\n    at spray.io.TickGenerator$$anon$1$$anon$2$$anonfun$1.apply(TickGenerator.scala:41)\n    at spray.io.TickGenerator$$anon$1$$anon$2$$anonfun$1.apply(TickGenerator.scala:38)\n    at spray.io.ConnectionHandler$$anonfun$running$1.applyOrElse(ConnectionHandler.scala:58)\n    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:165)\n    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:425)\n    at akka.actor.ActorCell.invoke(ActorCell.scala:386)\n    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:230)\n    at akka.dispatch.Mailbox.run(Mailbox.scala:212)\n    at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:506)\n    at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:262)\n    at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)\n    at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1478)\n    at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\n\n[ERROR] [11/04/2013 10:53:34.710] [netaporter-akka.actor.default-dispatcher-10] [akka://netaporter/user/IO-HTTP/group-0/0] Monitored actor [Actor[akka://netaporter/system/IO-TCP/selectors/$a/0]] terminated (akka.actor.DeathPactException)\n[WARN] [11/04/2013 10:53:34.716] [netaporter-akka.actor.default-dispatcher-7] [akka://netaporter/user/IO-HTTP/group-0/0] event pipeline: dropped Connected(naplabs.dave.net-a-porter.com/172.29.130.33:443,/10.5.18.105:65298)\n[ERROR] [11/04/2013 10:53:37.715] [netaporter-akka.actor.default-dispatcher-10] [akka://netaporter/user/IO-HTTP/host-connector-1] Unexpected slot state: Idle\njava.lang.IllegalStateException: Unexpected slot state: Idle\n    at spray.can.client.HttpHostConnector$$anonfun$receive$1.applyOrElse(HttpHostConnector.scala:81)\n    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:425)\n    at akka.actor.ActorCell.invoke(ActorCell.scala:386)\n    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:230)\n    at akka.dispatch.Mailbox.run(Mailbox.scala:212)\n    at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:506)\n    at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:262)\n    at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)\n    at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1478)\n    at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\n\n[WARN] [11/04/2013 10:54:03.945] [qtp276382433-345] [Servlet30ConnectorServlet(akka://netaporter)] Timeout of HttpRequest(GET,http://localhost:28080/api/,List(Host: localhost:28080, Accept: */*, Accept-Language: en-us, Connection: keep-alive, User-Agent: CocoaRestClient/11 CFNetwork/520.5.1 Darwin/11.4.2 (x86_64) (MacBookPro9%2C2), Accept-Encoding: gzip, deflate),Empty,HTTP/1.1)\n[ERROR] [11/04/2013 10:54:04.438] [netaporter-akka.actor.default-dispatcher-7] [akka://netaporter/user/woas-root/rest-routing] Unable to complete request. request=4d696639-7d70-4556-92d9-a8f54233aab3\nakka.pattern.AskTimeoutException: Timed out\n    at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:312)\n    at akka.actor.DefaultScheduler$$anon$8.run(Scheduler.scala:191)\n    at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:137)\n    at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:506)\n    at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:262)\n    at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)\n    at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1478)\n    at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\n\n[WARN] [11/04/2013 10:54:04.487] [netaporter-akka.actor.default-dispatcher-7] [Servlet30ConnectorServlet(akka://netaporter)] Received a second response for a request that was already completed, dropping ...\nRequest: HttpRequest(GET,http://localhost:28080/api/,List(Host: localhost:28080, Accept: */*, Accept-Language: en-us, Connection: keep-alive, User-Agent: CocoaRestClient/11 CFNetwork/520.5.1 Darwin/11.4.2 (x86_64) (MacBookPro9%2C2), Accept-Encoding: gzip, deflate),Empty,HTTP/1.1)\nResponse: HttpResponse(500 Internal Server Error,HttpEntity(application/json; charset=UTF-8,{\"@type\":\"/api/schemas/error\"}),List(),HTTP/1.1)\n```\n. Thanks,\n\nWith the nightly I still get the first `IllegalStateException`, but I no longer `Unexpected slot state: Idle` exception. \n\n```\n[ERROR] [11/04/2013 11:26:03.212] [netaporter-akka.actor.default-dispatcher-18] [akka://netaporter/user/IO-HTTP/group-0/6] null\njava.lang.IllegalStateException\n    at spray.io.SslTlsSupport$$anon$1$$anon$2.spray$io$SslTlsSupport$$anon$$anon$$verify(SslTlsSupport.scala:361)\n    at spray.io.SslTlsSupport$$anon$1$$anon$2$$anon$6$$anonfun$2.apply(SslTlsSupport.scala:92)\n    at spray.io.SslTlsSupport$$anon$1$$anon$2$$anon$6$$anonfun$2.apply(SslTlsSupport.scala:81)\n    at spray.io.SslTlsSupport$$anon$1$$anon$2.process(SslTlsSupport.scala:232)\n    at spray.io.DynamicPipelines$$anonfun$eventPipeline$1.apply(Pipelines.scala:48)\n    at spray.io.DynamicPipelines$$anonfun$eventPipeline$1.apply(Pipelines.scala:48)\n    at spray.io.RawPipelineStage$$anon$3$$anonfun$2.apply(Pipelines.scala:115)\n    at spray.io.RawPipelineStage$$anon$3$$anonfun$2.apply(Pipelines.scala:115)\n    at spray.io.TickGenerator$$anon$1$$anon$2$$anonfun$1.apply(TickGenerator.scala:41)\n    at spray.io.TickGenerator$$anon$1$$anon$2$$anonfun$1.apply(TickGenerator.scala:38)\n    at spray.io.ConnectionHandler$$anonfun$running$1.applyOrElse(ConnectionHandler.scala:58)\n    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:165)\n    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:425)\n    at akka.actor.ActorCell.invoke(ActorCell.scala:386)\n    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:230)\n    at akka.dispatch.Mailbox.run(Mailbox.scala:212)\n    at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:506)\n    at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:262)\n    at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)\n    at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1478)\n    at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\n\n[ERROR] [11/04/2013 11:26:03.216] [netaporter-akka.actor.default-dispatcher-20] [akka://netaporter/user/IO-HTTP/group-0/6] Monitored actor [Actor[akka://netaporter/system/IO-TCP/selectors/$a/14]] terminated (akka.actor.DeathPactException)\n[WARN] [11/04/2013 11:26:03.222] [netaporter-akka.actor.default-dispatcher-20] [akka://netaporter/user/IO-HTTP/group-0/6] event pipeline: dropped Connected(naplabs.dave.net-a-porter.com/172.29.130.33:443,/10.5.18.105:49840)\n[WARN] [11/04/2013 11:26:32.785] [qtp1721461644-84] [Servlet30ConnectorServlet(akka://netaporter)] Timeout of HttpRequest(GET,http://localhost:28080/api/,List(Host: localhost:28080, Accept: */*, Accept-Language: en-us, Connection: keep-alive, User-Agent: CocoaRestClient/11 CFNetwork/520.5.1 Darwin/11.4.2 (x86_64) (MacBookPro9%2C2), Accept-Encoding: gzip, deflate),Empty,HTTP/1.1)\n[ERROR] [11/04/2013 11:26:32.867] [netaporter-akka.actor.default-dispatcher-20] [akka://netaporter/user/woas-root/rest-routing] Unable to complete request. request=348e9e95-b8d2-43b2-8627-dab538bb2e41\nakka.pattern.AskTimeoutException: Timed out\n    at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:312)\n    at akka.actor.DefaultScheduler$$anon$8.run(Scheduler.scala:191)\n    at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:137)\n    at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:506)\n    at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:262)\n    at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)\n    at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1478)\n    at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\n\n[WARN] [11/04/2013 11:26:32.869] [netaporter-akka.actor.default-dispatcher-20] [Servlet30ConnectorServlet(akka://netaporter)] Received a second response for a request that was already completed, dropping ...\nRequest: HttpRequest(GET,http://localhost:28080/api,List(Host: localhost:28080, Accept: */*, Accept-Language: en-us, Connection: keep-alive, User-Agent: CocoaRestClient/11 CFNetwork/520.5.1 Darwin/11.4.2 (x86_64) (MacBookPro9%2C2), Accept-Encoding: gzip, deflate),Empty,HTTP/1.1)\nResponse: HttpResponse(500 Internal Server Error,HttpEntity(application/json; charset=UTF-8,{\"@type\":\"/api/schemas/error\"}),List(),HTTP/1.1)\n,,,\n```\n. ",
    "tommcp": "+1\n. +1\n. ",
    "t3hnar": "+1\n. Hi Mathias, I'm looking into it...what's about that spray-http doesn't have dependency on srpay-io, can I add that?\n. Diving deeper:\n1. it's not so easy to remove `extend Command` from both `SetRequestTimeout` and `SetTimeoutTimeout`\n2. I don't want to add dependency to spray-http on spray-io\n\nHowever I would prefere second choice over the first one.\nWhat do you think @sirthias ?\n. I think: nice idea, lets do that way.\n. @sirthias could you review?\n. Done\n. Why do you need to have Command type instead of Any ?\n. Yes, but I mean why do you have pipeline that can work with Command type instead of AnyRef ?\nAre there anything behind it?\n. ok, clear, thx. \nI will do this trick with CommandWrapper, should I match AnyRef and wrap to CommandWrapper or match explicitly SetRequestTimeout and SetTimeoutTimeout\n. I was talking about \n`case x:AnyRef \u21d2 pipelines.commandPipeline(CommandWrapper(x))`\nor\n`case x:SetRequestTimeout \u21d2 pipelines.commandPipeline(CommandWrapper(x))`\n`case x:SetTimeoutTimeout \u21d2 pipelines.commandPipeline(CommandWrapper(x))`\nand others...\n. It's ready for review\n. Done\n. Finally :)\n. Hi @sirthias,\nDo you have an idea when this will be released?\nIt's a critical issue for me...\n. Don't think it's a good idea to use nightlies, what's about next M9 ?\n. Thanks, will do\n. +1\n. Hi Mathias, I'm looking into it...what's about that spray-http doesn't have dependency on srpay-io, can I add that?\n. Diving deeper:\n1. it's not so easy to remove `extend Command` from both `SetRequestTimeout` and `SetTimeoutTimeout`\n2. I don't want to add dependency to spray-http on spray-io\n\nHowever I would prefere second choice over the first one.\nWhat do you think @sirthias ?\n. I think: nice idea, lets do that way.\n. @sirthias could you review?\n. Done\n. Why do you need to have Command type instead of Any ?\n. Yes, but I mean why do you have pipeline that can work with Command type instead of AnyRef ?\nAre there anything behind it?\n. ok, clear, thx. \nI will do this trick with CommandWrapper, should I match AnyRef and wrap to CommandWrapper or match explicitly SetRequestTimeout and SetTimeoutTimeout\n. I was talking about \n`case x:AnyRef \u21d2 pipelines.commandPipeline(CommandWrapper(x))`\nor\n`case x:SetRequestTimeout \u21d2 pipelines.commandPipeline(CommandWrapper(x))`\n`case x:SetTimeoutTimeout \u21d2 pipelines.commandPipeline(CommandWrapper(x))`\nand others...\n. It's ready for review\n. Done\n. Finally :)\n. Hi @sirthias,\nDo you have an idea when this will be released?\nIt's a critical issue for me...\n. Don't think it's a good idea to use nightlies, what's about next M9 ?\n. Thanks, will do\n. ",
    "alexbool": "+1\n. +1\n. Thanks for the quick fix!\n. +1\n. +1\n. Thanks for the quick fix!\n. ",
    "olger": "+1\n. New commit pushed to thenewmotion/spray as requested in your comment\n. Facebook and Google sample are not the same. Details differ (look at parsing of the tokens) \n. Hi Guys,\n\nWhen you are done with The review, I'll try to fix your comments .\nOne note about the entity.as[] -> Facebook returns the response as 'text/JavaScript' (or something like that, can't check via my mobile here) that did not see the response as proper json\n. Hi Guys, what's next ?\n. Great thanks ! Any way to assist ?\n. Hi @topping,\nGreat to see you 've moved the code to 1.3\n\nTwo options to get it into spray, 1: I merge it back into my branch and outstanding pull request (I am not a git expert but that must be possible)\n2: Create a new pull request based on your fork. (that's possible for sure)\n\nI think it's up to @sirthias to indicate what he'd like to do with this. \n\nKind regards, \n\nOlger\n\nBTW: I'll have a look at your changes anyway, I am really happy that you've adapted this stuff to 1.3\n. Hi Brian, \n\nImplementation is 2.0 only, that's right. \nDo you know many things that still do 1.0 without a 2.0 option ? \n\nHaving OAuth outside of the maybe an option. I've written the code to match as close as possible (to my understanding) with the authentication code inside spray. Some of the types used are not publicly available (if I remember correctly, and that may have changed). \n\nWould be helpful to know what the plans of @sirthias are :)\n. Hi Mathias,\nIs a corresponding akka-http ticket already available ?- and - is it of use to actually get this back into akka-http ? \n. +1\n. New commit pushed to thenewmotion/spray as requested in your comment\n. Facebook and Google sample are not the same. Details differ (look at parsing of the tokens) \n. Hi Guys,\n\nWhen you are done with The review, I'll try to fix your comments .\nOne note about the entity.as[] -> Facebook returns the response as 'text/JavaScript' (or something like that, can't check via my mobile here) that did not see the response as proper json\n. Hi Guys, what's next ?\n. Great thanks ! Any way to assist ?\n. Hi @topping,\nGreat to see you 've moved the code to 1.3\n\nTwo options to get it into spray, 1: I merge it back into my branch and outstanding pull request (I am not a git expert but that must be possible)\n2: Create a new pull request based on your fork. (that's possible for sure)\n\nI think it's up to @sirthias to indicate what he'd like to do with this. \n\nKind regards, \n\nOlger\n\nBTW: I'll have a look at your changes anyway, I am really happy that you've adapted this stuff to 1.3\n. Hi Brian, \n\nImplementation is 2.0 only, that's right. \nDo you know many things that still do 1.0 without a 2.0 option ? \n\nHaving OAuth outside of the maybe an option. I've written the code to match as close as possible (to my understanding) with the authentication code inside spray. Some of the types used are not publicly available (if I remember correctly, and that may have changed). \n\nWould be helpful to know what the plans of @sirthias are :)\n. Hi Mathias,\nIs a corresponding akka-http ticket already available ?- and - is it of use to actually get this back into akka-http ? \n. ",
    "pjean": "+1\n. +1\n. ",
    "edgurgel": "+1\n. +1\n. ",
    "zerni": "+1\n. +1\n. ",
    "Bathtor": "+1\n. Yeah, I realised you are right. It started annoying me after only one day, that I depend on a custom build of spray for my main project now -.-\nI created a new project in https://github.com/Bathtor/spray-solr. Don't have a repo for releases at the moment, though.\n. +1\n. Yeah, I realised you are right. It started annoying me after only one day, that I depend on a custom build of spray for my main project now -.-\nI created a new project in https://github.com/Bathtor/spray-solr. Don't have a repo for releases at the moment, though.\n. ",
    "csenol": "+1\n. +1\n. ",
    "lihaoyi": "+1\n. I've actually written up a small extension to the spray-can http server that adds websocket functionality. It's over at https://github.com/lihaoyi/SprayWebSockets with instructions for how to try it out, and it should work with the 1.1-M7 nightly builds. \n\nThe server has been continuously serving websocket connections for the last two months on the  toy site I made to test it out (http://www.textboxplus.com/), so it seems to work. There are instructions in that project's readme as to how to use it. Installation basically involves copying to source files into your project (sorry no .jar, but it's less than 500 lines of code anyway). The plan is to contribute it back into the main repo when 1.1-M8 comes out.\n. I've updated https://github.com/lihaoyi/SprayWebSockets to be compatible with 1.2-M8, and it passes all of the Autobahn test suite as both client and server. Not tracking trunk because things are constantly changing, and I'll wait until a release is landed before updating to it and putting an artifact on maven. In the meantime you can just add the github URL to sbt and it'll immediately start working.\n. My standalone spray websocket server/client (https://github.com/lihaoyi/SprayWebSockets) has been updated to work against 1.2-RC3, if anyone wants to give it a shot and you don't want to wait.\n. [SprayWebSockets](https://github.com/lihaoyi/SprayWebSockets) has been updated to work with spray-can 1.2. Although it's not super speedy, it passes more or less the entire websocket test suite, so if you need a websocket server in a pinch it'll work\n. Updated the commit and emailed you guys the CLA\n. Sure. I had kinda assumed that \"creating more public methods\" would be\nchanging the public API, but I realize that all those classes are\nprivate[spray]. I'll update it.\n\nOn Tue, Oct 15, 2013 at 12:49 AM, Mathias notifications@github.com wrote:\n\n> Haoyi, thanks for the CLA.\n> However, your patch doesn't change the public API at all (since all the\n> new method introductions are purely internal), so the commit message should\n> be something like:\n> \n> = can: improve HttpManager extensibility by moving child actor creations into dedicated methods\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/589#issuecomment-26315136\n> .\n. Hmm, didn't think that would cause things to break. I'll get it working nicely before re-opening it\n. I don't know. Presumably the cost of computing it each time is small, matching a regex against nothing. I'm not familiar with the hotness of the various code paths in the routes, but if it was accessing it each time with reflection, it can't be that expensive =)\n. eeek travis blew up. Anyone have any idea why? The UI confuses me =(\n. +1\n. I've actually written up a small extension to the spray-can http server that adds websocket functionality. It's over at https://github.com/lihaoyi/SprayWebSockets with instructions for how to try it out, and it should work with the 1.1-M7 nightly builds. \n\nThe server has been continuously serving websocket connections for the last two months on the  toy site I made to test it out (http://www.textboxplus.com/), so it seems to work. There are instructions in that project's readme as to how to use it. Installation basically involves copying to source files into your project (sorry no .jar, but it's less than 500 lines of code anyway). The plan is to contribute it back into the main repo when 1.1-M8 comes out.\n. I've updated https://github.com/lihaoyi/SprayWebSockets to be compatible with 1.2-M8, and it passes all of the Autobahn test suite as both client and server. Not tracking trunk because things are constantly changing, and I'll wait until a release is landed before updating to it and putting an artifact on maven. In the meantime you can just add the github URL to sbt and it'll immediately start working.\n. My standalone spray websocket server/client (https://github.com/lihaoyi/SprayWebSockets) has been updated to work against 1.2-RC3, if anyone wants to give it a shot and you don't want to wait.\n. [SprayWebSockets](https://github.com/lihaoyi/SprayWebSockets) has been updated to work with spray-can 1.2. Although it's not super speedy, it passes more or less the entire websocket test suite, so if you need a websocket server in a pinch it'll work\n. Updated the commit and emailed you guys the CLA\n. Sure. I had kinda assumed that \"creating more public methods\" would be\nchanging the public API, but I realize that all those classes are\nprivate[spray]. I'll update it.\n\nOn Tue, Oct 15, 2013 at 12:49 AM, Mathias notifications@github.com wrote:\n\n> Haoyi, thanks for the CLA.\n> However, your patch doesn't change the public API at all (since all the\n> new method introductions are purely internal), so the commit message should\n> be something like:\n> \n> = can: improve HttpManager extensibility by moving child actor creations into dedicated methods\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/589#issuecomment-26315136\n> .\n. Hmm, didn't think that would cause things to break. I'll get it working nicely before re-opening it\n. I don't know. Presumably the cost of computing it each time is small, matching a regex against nothing. I'm not familiar with the hotness of the various code paths in the routes, but if it was accessing it each time with reflection, it can't be that expensive =)\n. eeek travis blew up. Anyone have any idea why? The UI confuses me =(\n. ",
    "RajivKurian": "+1\n. What would  case CompoundHttpData(head: NonEmptyHttpData, tail: HttpData) extends NonEmptyHttpData be used for?\n. Would this be based on http://tools.ietf.org/html/rfc2387 ?\n. +1\n. What would  case CompoundHttpData(head: NonEmptyHttpData, tail: HttpData) extends NonEmptyHttpData be used for?\n. Would this be based on http://tools.ietf.org/html/rfc2387 ?\n. ",
    "soaexpert": "+1\n. +1\n. ",
    "siriux": "+1\n. If you are interested, there is some work in progress here: https://github.com/siriux/spray_sse\n. +1\n. If you are interested, there is some work in progress here: https://github.com/siriux/spray_sse\n. ",
    "shogs": "+1\n. +1\n. ",
    "ramn": "+1\n. +1\n. ",
    "tlvenn": "+1\n. +1\n. ",
    "sharathp": "Eagerly waiting for this! \n\n+1\n. Eagerly waiting for this! \n\n+1\n. ",
    "hseeberger": "+1\n. The patch is my original work and that I license the work to the spray project under the project\u2019s open source license.\n. That issue is not only related to spray-can, but to all TCP (and maybe UDP) connections. Fixes needed in Akka I/O.\n. I fixed it and created [pull request 264](https://github.com/spray/spray/pull/264) for spray.io, yet this has be contributed back to Akka I/O.\n. @sirthias, thanks for your feedback. I have updated the pull request based on your comments.\n. +1\n. The patch is my original work and that I license the work to the spray project under the project\u2019s open source license.\n. That issue is not only related to spray-can, but to all TCP (and maybe UDP) connections. Fixes needed in Akka I/O.\n. I fixed it and created [pull request 264](https://github.com/spray/spray/pull/264) for spray.io, yet this has be contributed back to Akka I/O.\n. @sirthias, thanks for your feedback. I have updated the pull request based on your comments.\n. ",
    "enijns": "+1\n. +1\n. ",
    "JMacLulich": "+1 definitely looking forward to this.\n. On my mac I'm using:\n\njava version \"1.6.0_37\"\nJava(TM) SE Runtime Environment (build 1.6.0_37-b06-434-11M3909)\nJava HotSpot(TM) 64-Bit Server VM (build 20.12-b01-434, mixed mode)\n\non my Ubuntu box:\n\njava version \"1.6.0_24\"\nOpenJDK Runtime Environment (IcedTea6 1.11.3) (6b24-1.11.3-1ubuntu0.12.04.1)\nOpenJDK 64-Bit Server VM (build 20.0-b12, mixed mode)\n\nThanks.\n. Thanks, I removed my ivy2 cache to test this out quickly so it re-downloaded all the dependencies and tried git clean -xdf but I still end up with the compilation error \"overloaded method constructor Date with alternatives:\" because of the java.util.Date being hidden by an object called \"Date\". Where could that be? Did u use brew to install sbt?\n\nI'm using scala: Scala code runner version 2.9.2 -- Copyright 2002-2011, LAMP/EPFL\n. Thanks for your help; I really want to use spray.\n\nTo get it to compile, I instantiated Date like this: `new Date(1, 2, 3)`.\n\n```\nprintln(Date.getClass.getName)\n```\n\nThere is no value getClass is not a member of object Date\n\n```\nprintln(new Date(1,2,3).getClass.getName) \n```\n\nprints out \"Date\" a bunch of times.\n\nPuzzling. \n. Yeah true, thanks, I've got it compiling well enough so I can run the examples etc.. and I'll just suck it in as a dependency in my projects. Cheers, I'm also going to try and install JDK 1.7 I wonder if my JDK install is borked somehow.\n. Thanks Mathias, sorry wasn't sure where to post this question.\n. +1 definitely looking forward to this.\n. On my mac I'm using:\n\njava version \"1.6.0_37\"\nJava(TM) SE Runtime Environment (build 1.6.0_37-b06-434-11M3909)\nJava HotSpot(TM) 64-Bit Server VM (build 20.12-b01-434, mixed mode)\n\non my Ubuntu box:\n\njava version \"1.6.0_24\"\nOpenJDK Runtime Environment (IcedTea6 1.11.3) (6b24-1.11.3-1ubuntu0.12.04.1)\nOpenJDK 64-Bit Server VM (build 20.0-b12, mixed mode)\n\nThanks.\n. Thanks, I removed my ivy2 cache to test this out quickly so it re-downloaded all the dependencies and tried git clean -xdf but I still end up with the compilation error \"overloaded method constructor Date with alternatives:\" because of the java.util.Date being hidden by an object called \"Date\". Where could that be? Did u use brew to install sbt?\n\nI'm using scala: Scala code runner version 2.9.2 -- Copyright 2002-2011, LAMP/EPFL\n. Thanks for your help; I really want to use spray.\n\nTo get it to compile, I instantiated Date like this: `new Date(1, 2, 3)`.\n\n```\nprintln(Date.getClass.getName)\n```\n\nThere is no value getClass is not a member of object Date\n\n```\nprintln(new Date(1,2,3).getClass.getName) \n```\n\nprints out \"Date\" a bunch of times.\n\nPuzzling. \n. Yeah true, thanks, I've got it compiling well enough so I can run the examples etc.. and I'll just suck it in as a dependency in my projects. Cheers, I'm also going to try and install JDK 1.7 I wonder if my JDK install is borked somehow.\n. Thanks Mathias, sorry wasn't sure where to post this question.\n. ",
    "paulsabou": "+1\n. +1\n. ",
    "slothspot": "+1\n. +1\n. ",
    "shimberger": "+1\n. +1\n. ",
    "dacc": "+1\n. +1\n. ",
    "mdmarek": "+1\n. +1\n. ",
    "Grogs": "+1\n. +1\n. ",
    "cicika": "+1\n. +1\n. ",
    "pirtlj": "+1\n. +1\n. ",
    "g-eorge": "+1\n. +1\n. ",
    "kulikov": "+1\n. +1 again and again\n. done\n. +1\n. +1 again and again\n. done\n. ",
    "drewhk": "+1 ;)\n. This is something I will use immediately :)\nKeep on with the excellent work!\n. One of my hobby projects is a local http proxy that tracks and analyses visited pages and recommends links, news, crawls in the background, etc. I use spray for both the proxy and the dashboard for the application. I don't use any templates but render with js at the client side, so all I need is REST API+static content.\n. As a sidenote, currently my projects have only a very small number of small static assets, so I just load them into memory and serve them from there.\n. +1 ;)\n. This is something I will use immediately :)\nKeep on with the excellent work!\n. One of my hobby projects is a local http proxy that tracks and analyses visited pages and recommends links, news, crawls in the background, etc. I use spray for both the proxy and the dashboard for the application. I don't use any templates but render with js at the client side, so all I need is REST API+static content.\n. As a sidenote, currently my projects have only a very small number of small static assets, so I just load them into memory and serve them from there.\n. ",
    "macinux": "+1\n. +1\n. ",
    "jgrowl": "+1\n. +1\n. ",
    "avaello": "+1\n. +1\n. ",
    "greenhost87": "+1\n. +1\n. ",
    "dvorobiov": "+1\n. great work Li! thanks!\n. Updated. :)\n. ```\nimport LiftJsonSupport._ \n...\n   val exchangePipeline = sendReceive ~> unmarshal[GoogleTokenResponse]\n\n    val request = FormData(Seq(\n      (\"client_id\" -> clientId),\n        (\"client_secret\" -> clientSecret),\n          (\"code\" -> code),\n            (\"grant_type\" -> grantType),\n              (\"redirect_uri\" -> redirectUri)\n    ))\n    println(Post(exchangePath, request).entity.asString)\n\n    val response: Future[GoogleTokenResponse] =\n      exchangePipeline(Post(exchangePath, request))\n```\n. +1\n. great work Li! thanks!\n. Updated. :)\n. ```\nimport LiftJsonSupport._ \n...\n   val exchangePipeline = sendReceive ~> unmarshal[GoogleTokenResponse]\n\n    val request = FormData(Seq(\n      (\"client_id\" -> clientId),\n        (\"client_secret\" -> clientSecret),\n          (\"code\" -> code),\n            (\"grant_type\" -> grantType),\n              (\"redirect_uri\" -> redirectUri)\n    ))\n    println(Post(exchangePath, request).entity.asString)\n\n    val response: Future[GoogleTokenResponse] =\n      exchangePipeline(Post(exchangePath, request))\n```\n. ",
    "nartamonov": "+1\n. Guys, websockets support is not enough:\n1. Global browser support of websockets is nearly 70%. Not bad, but IE9 (and below) and _all_ Android versions [don't support](http://caniuse.com/#search=websocket) websockets at all. So we are forced to use long polling, xhr streaming or something else with these browsers.\n2. Websockets protocol has few serious (more or less) drawbacks that makes it hard to develope pretty complex applications such as chats. For more details see [this](http://webtide.intalio.com/2011/04/is-websocket-chat-simpler/) and [that](http://www.infoq.com/articles/Web-Sockets-Proxy-Servers).\n\nWhat we really need is abstraction layer for asynchronous communications between browser and server with fallbacks to best available transport (websockets, long polling, etc). Something lile [Bayeaux](http://cometd.org/documentation/bayeux), [SockJS](http://sockjs.org/) or [Socket.IO](http://socket.io/). That's why our competitor, [vert.x framework](http://vertx.io/), supports not only websockets, but full-blown SockJS. That's why [Atmosphere framework](https://github.com/Atmosphere) provides high-level abstraction layer _above_ details of websockets, long polling or other _low-level transport_ protocols. These abstractions in API help to develope complex asynchronous applications.\n\nP.S. By the way, SocksJS is much more preferred protocol above Socket.IO. See [detailed analysis](http://xsnippet.org/359042/) by author of Tornado (Socket.IO implementation for python).\n. +1\n. Guys, websockets support is not enough:\n1. Global browser support of websockets is nearly 70%. Not bad, but IE9 (and below) and _all_ Android versions [don't support](http://caniuse.com/#search=websocket) websockets at all. So we are forced to use long polling, xhr streaming or something else with these browsers.\n2. Websockets protocol has few serious (more or less) drawbacks that makes it hard to develope pretty complex applications such as chats. For more details see [this](http://webtide.intalio.com/2011/04/is-websocket-chat-simpler/) and [that](http://www.infoq.com/articles/Web-Sockets-Proxy-Servers).\n\nWhat we really need is abstraction layer for asynchronous communications between browser and server with fallbacks to best available transport (websockets, long polling, etc). Something lile [Bayeaux](http://cometd.org/documentation/bayeux), [SockJS](http://sockjs.org/) or [Socket.IO](http://socket.io/). That's why our competitor, [vert.x framework](http://vertx.io/), supports not only websockets, but full-blown SockJS. That's why [Atmosphere framework](https://github.com/Atmosphere) provides high-level abstraction layer _above_ details of websockets, long polling or other _low-level transport_ protocols. These abstractions in API help to develope complex asynchronous applications.\n\nP.S. By the way, SocksJS is much more preferred protocol above Socket.IO. See [detailed analysis](http://xsnippet.org/359042/) by author of Tornado (Socket.IO implementation for python).\n. ",
    "mnetship": "+1\n\nSent from my iPhone\n\nOn 20 Jun 2013, at 10:11 PM, nartamonov notifications@github.com wrote:\n\n> +1\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. Hi Guersam,\n\nI'm uploading a file using Spray client. So maybe it's the client code that needs to change. \n\nKind Regards,\nMichael\n. +1\n\nSent from my iPhone\n\nOn 20 Jun 2013, at 10:11 PM, nartamonov notifications@github.com wrote:\n\n> +1\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. Hi Guersam,\n\nI'm uploading a file using Spray client. So maybe it's the client code that needs to change. \n\nKind Regards,\nMichael\n. ",
    "romusz": "+1\n. +1\n. ",
    "mweibel": ":+1: \n. :+1: \n. ",
    "natbusa": "+1\n. +1\n. ",
    "karthikkolli": "+1\n. +1\n. ",
    "kthakore": "+1\n. +1\n. ",
    "romansergey": "+1\n. +1\n. ",
    "jestan": "+1\n. +1\n. ",
    "reinierl": "+1\n. +1\n. ",
    "agarman": "+1\n. +1\n. ",
    "chesterxgchen": "Before this support is ready in spray,  what do you think we use scalawebsocket (  https://github.com/pbuda/scalawebsocket) in the mean time ?\n\nAny suggestions?\n. Scalatra seems also have integration with Atmosphere: \n\nhttp://ymasory.github.io/websockets-and-akka-on-scalatra/#slide-126\n. Before this support is ready in spray,  what do you think we use scalawebsocket (  https://github.com/pbuda/scalawebsocket) in the mean time ?\n\nAny suggestions?\n. Scalatra seems also have integration with Atmosphere: \n\nhttp://ymasory.github.io/websockets-and-akka-on-scalatra/#slide-126\n. ",
    "martin-g": "https://github.com/pbuda/scalawebsocket looks like WebSocket client. You need a server.\nThere are several offers in the market but I don't see any of them fitting in the actor based nature of Spray. Maybe the ones that integrate with Play Framework are the closest. See https://github.com/Atmosphere/atmosphere-play and https://github.com/flowersinthesand/portal-java/tree/master/play\n. I haven't checked the latest code..\nI can use (and test) nightlies.\nThanks! \n. Check my comment in \"Files changed\" tab to see it in context.\nIn Facebook related class you use the word \"Google\" in the docs.\nSeems like a copy/paste error.\n. What is the benefit of using `new String(Array.empty[Char])` instead of `\"\"` ?\n\nI think the last two\n\n> =: key = \"\" and value = \"\"\n> \"\": key = \"\" and value = Query.EmptyValue\n\nare not really necessary.\n. @agemooij , It seems you forgot to add the link to the description of the protocol.\n. >  calling copy(..) which calls a version of Uri.apply(..)\n\nThis is where my Scala knowledge lacked. Thanks! \n. :+1: \n. > ... during Spray **lunch**\n\nBon appetit Spray! :-)\n. https://github.com/pbuda/scalawebsocket looks like WebSocket client. You need a server.\nThere are several offers in the market but I don't see any of them fitting in the actor based nature of Spray. Maybe the ones that integrate with Play Framework are the closest. See https://github.com/Atmosphere/atmosphere-play and https://github.com/flowersinthesand/portal-java/tree/master/play\n. I haven't checked the latest code..\nI can use (and test) nightlies.\nThanks! \n. Check my comment in \"Files changed\" tab to see it in context.\nIn Facebook related class you use the word \"Google\" in the docs.\nSeems like a copy/paste error.\n. What is the benefit of using `new String(Array.empty[Char])` instead of `\"\"` ?\n\nI think the last two\n\n> =: key = \"\" and value = \"\"\n> \"\": key = \"\" and value = Query.EmptyValue\n\nare not really necessary.\n. @agemooij , It seems you forgot to add the link to the description of the protocol.\n. >  calling copy(..) which calls a version of Uri.apply(..)\n\nThis is where my Scala knowledge lacked. Thanks! \n. :+1: \n. > ... during Spray **lunch**\n\nBon appetit Spray! :-)\n. ",
    "ctataryn": "+1\n. +1\n. +1\n. +1\n. ",
    "juanjovazquez": "+1\n. +1\n. ",
    "taylorleese": "+1\n. +1 This would be useful.\n. It would be nice to get the spray docs updated to reflect this here: http://spray.io/documentation/1.2-M8/spray-can/configuration/.\n. +1 I ran into this recently as well.\n. Any chance this fix can be included in a nightly build?\n. Just a note that this is a pretty major issue. It basically means you cannot use the SLF4J adapter in a production application with Spray. \n. @sirthias In re: to wasting space on the log message line perhaps the actor path should be included as part of the [MDC](http://logback.qos.ch/manual/mdc.html) so it's configurable. Actor paths are a newer Akka concept so I'm not surprised they don't play nice with Java logging frameworks whereas classnames do.\n. @sirthias Not sure I understand the comment about MDC. Couldn't you just set/clear the actor path in MDC as log events are logged or is the actor path not available at that point once it's on the event bus? You'd definitely need to clear the actor path after each log event though. Perhaps, that would require support in Akka itself? Either way, looking forward to a fix here so we can start using the SLF4J adapter in production again.\n. Sounds like a decent compromise. Thanks for pointing that out @jrudolph. I hadn't noticed the implicit `LogSource` either.\n. Does this mean you could just use https://github.com/akka/akka/blob/master/akka-actor/src/main/scala/akka/event/Logging.scala#L283-L285? It seems that's what Akka ActorLogging uses. Wouldn't this also mean that `log-actor-paths-with-dots` and `log-actor-system-name` are no longer necessary?\n. I'd suggest changing `SprayActorLogging` to extend Akka `ActorLogging` and then just mark it as deprecated in case people are using `SprayActorLogging` directly.\n. :+1: Side note, I'm curious about the cache as well.\n. @sirthias Any chance you could publish a nightly build with this?\n. @sirthias thx\n. +1\n. Just following this since I also ran into the issue described here: https://groups.google.com/forum/#!searchin/spray-user/empty$20path$20routing$20RC1/spray-user/S8PM4NUMZ8I/1CmFLr19Nu0J\n. Am I missing something or did this just revert the change that broke `path(\"\")` in 1.2-RC1 (from 1.2-M8)? It does seem that `pathEnd` is the more \"correct\" solution that is suggested?\n. @sirthias thx for the explanation\n. +1\n. +1 This would be useful.\n. It would be nice to get the spray docs updated to reflect this here: http://spray.io/documentation/1.2-M8/spray-can/configuration/.\n. +1 I ran into this recently as well.\n. Any chance this fix can be included in a nightly build?\n. Just a note that this is a pretty major issue. It basically means you cannot use the SLF4J adapter in a production application with Spray. \n. @sirthias In re: to wasting space on the log message line perhaps the actor path should be included as part of the [MDC](http://logback.qos.ch/manual/mdc.html) so it's configurable. Actor paths are a newer Akka concept so I'm not surprised they don't play nice with Java logging frameworks whereas classnames do.\n. @sirthias Not sure I understand the comment about MDC. Couldn't you just set/clear the actor path in MDC as log events are logged or is the actor path not available at that point once it's on the event bus? You'd definitely need to clear the actor path after each log event though. Perhaps, that would require support in Akka itself? Either way, looking forward to a fix here so we can start using the SLF4J adapter in production again.\n. Sounds like a decent compromise. Thanks for pointing that out @jrudolph. I hadn't noticed the implicit `LogSource` either.\n. Does this mean you could just use https://github.com/akka/akka/blob/master/akka-actor/src/main/scala/akka/event/Logging.scala#L283-L285? It seems that's what Akka ActorLogging uses. Wouldn't this also mean that `log-actor-paths-with-dots` and `log-actor-system-name` are no longer necessary?\n. I'd suggest changing `SprayActorLogging` to extend Akka `ActorLogging` and then just mark it as deprecated in case people are using `SprayActorLogging` directly.\n. :+1: Side note, I'm curious about the cache as well.\n. @sirthias Any chance you could publish a nightly build with this?\n. @sirthias thx\n. +1\n. Just following this since I also ran into the issue described here: https://groups.google.com/forum/#!searchin/spray-user/empty$20path$20routing$20RC1/spray-user/S8PM4NUMZ8I/1CmFLr19Nu0J\n. Am I missing something or did this just revert the change that broke `path(\"\")` in 1.2-RC1 (from 1.2-M8)? It does seem that `pathEnd` is the more \"correct\" solution that is suggested?\n. @sirthias thx for the explanation\n. ",
    "kardapoltsev": "+1\n. +1\n. ",
    "cheebo": "+1\n. +1\n. ",
    "maliqq": "+1\n. +1\n. ",
    "andreaferretti": "+1\n. +1\n. ",
    "pmlt": "Is this feature still on track for the final 1.2 release? If not, when can we expect it?\n. Is this feature still on track for the final 1.2 release? If not, when can we expect it?\n. ",
    "AhmedSoliman": "+100\n. +100\n. ",
    "emilianionascu": "+1\n. +1\n. ",
    "laguiz": "+1\n. +1\n. ",
    "michalj": "+1\n. +1\n. ",
    "levinotik": "+1\n. :+1: \n. Where exactly would this distinction between the `HTTP/1.0` and `HTTP/1.1` protocols need to be encoded? \n\nI see the SprayCanServerSpec has  \n\n``` scala\n\"automatically produce an error response\" in {\n      \"when the request has no absolute URI and no Host header\" in new TestSetup....\n```\n. Does that mean changing test configurations to akka.loglevel = OFF ?\n. +1\n. :+1: \n. Where exactly would this distinction between the `HTTP/1.0` and `HTTP/1.1` protocols need to be encoded? \n\nI see the SprayCanServerSpec has  \n\n``` scala\n\"automatically produce an error response\" in {\n      \"when the request has no absolute URI and no Host header\" in new TestSetup....\n```\n. Does that mean changing test configurations to akka.loglevel = OFF ?\n. ",
    "clemensberndt": "+1\n. +1\n. ",
    "martinring": "+1\n. +1\n. ",
    "canyaman": "+1\n. +1\n. ",
    "pcting": ":+1: \n. :+1: \n. ",
    "carueda": ":+1: \n. :+1: \n. ",
    "razvan-panda": ":+1:\n. :+1:\n. ",
    "pm47": "+1\n. +1\n. ",
    "rkatti": "+1 +1 \n. +1 +1 \n. ",
    "andrii-rubtsov": "+1\n. +1\n. ",
    "villiger": "+1\n. +1\n. ",
    "bchapuis": "+1\n. +1\n. ",
    "mgibowski": "+1\n. +1\n. ",
    "DarrenMowat": "+1\n. +1\n. ",
    "zdexter": "+1\n. +1\n. ",
    "troypayne": "+1\n. can someone answer the following questions about Atmosphere:\n\nWhy?\nWhat?\nHow?\n\nOn Wed, May 14, 2014 at 4:19 PM, Brian Topping notifications@github.comwrote:\n\n> Atmosphere does look very interesting. The support for different protocols\n> over the ws: connection looks promising.\n> \n> Have you considered using one of the existing spray websocket\n> implementations to create an integration with Atmosphere? It would be cool\n> to see it running!\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/issues/134#issuecomment-43132005\n> .\n. +1\n. can someone answer the following questions about Atmosphere:\n\nWhy?\nWhat?\nHow?\n\nOn Wed, May 14, 2014 at 4:19 PM, Brian Topping notifications@github.comwrote:\n\n> Atmosphere does look very interesting. The support for different protocols\n> over the ws: connection looks promising.\n> \n> Have you considered using one of the existing spray websocket\n> implementations to create an integration with Atmosphere? It would be cool\n> to see it running!\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/issues/134#issuecomment-43132005\n> .\n. ",
    "s-a-y": "+1\n. +1\n. ",
    "rejjin": "-1\n. -1\n. ",
    "boui": "+1!!\n. +1!!\n. ",
    "okapies": "+1!\n\nRecently Akka team plans to replace the pipeline architecture and IMO it is good chance to add \"upgrade protocol\" support to the Akka's pipeline.\n\n@lihaoyi 's really awesome [SprayWebSocket](https://github.com/lihaoyi/SprayWebSockets), for example, needs to re-implement whole spray's http pipeline to switch the protocol. It seems little bit verbose.\n\nhttps://groups.google.com/d/msg/akka-user/8fZ75MlVEYc/YaZGsysG8mcJ\n. I have tried to integrate spray with atmosphere, but it seemed little bit complicated because the atmosphere has its own semantics to provide full-stack HTTP server functionality. I couldn't find a sub-module _only just_ handling the protocol, instead of the whole stack.\n. I retried and turned out that this doesn't cause crash but produce the following warning in [my application](https://github.com/okapies/sircuit).\n\n```\nsircuit-server [WARN] [03/12/2014 07:53:55.970] [sircuit-akka.actor.default-dispatcher-7] [akka://sircuit/user/IO-SOCKET/listener-0/1] Illegal request header: Illegal 'Origin' header: Illegal origin, unexpected character '1' at position 0: \nsircuit-server 127.0.0.1:8080\nsircuit-server ^\n```\n. Thanks for quick reply! Hmm, I didn't expect there is such a option.\n. +1!\n\nRecently Akka team plans to replace the pipeline architecture and IMO it is good chance to add \"upgrade protocol\" support to the Akka's pipeline.\n\n@lihaoyi 's really awesome [SprayWebSocket](https://github.com/lihaoyi/SprayWebSockets), for example, needs to re-implement whole spray's http pipeline to switch the protocol. It seems little bit verbose.\n\nhttps://groups.google.com/d/msg/akka-user/8fZ75MlVEYc/YaZGsysG8mcJ\n. I have tried to integrate spray with atmosphere, but it seemed little bit complicated because the atmosphere has its own semantics to provide full-stack HTTP server functionality. I couldn't find a sub-module _only just_ handling the protocol, instead of the whole stack.\n. I retried and turned out that this doesn't cause crash but produce the following warning in [my application](https://github.com/okapies/sircuit).\n\n```\nsircuit-server [WARN] [03/12/2014 07:53:55.970] [sircuit-akka.actor.default-dispatcher-7] [akka://sircuit/user/IO-SOCKET/listener-0/1] Illegal request header: Illegal 'Origin' header: Illegal origin, unexpected character '1' at position 0: \nsircuit-server 127.0.0.1:8080\nsircuit-server ^\n```\n. Thanks for quick reply! Hmm, I didn't expect there is such a option.\n. ",
    "jtrunick": "+1 - Atmosphere support\n. +1 - Atmosphere support\n. ",
    "dborovikov": "+1!\n. +1!\n. ",
    "ScottPierce": "+1\n. +1\n. ",
    "aoiroaoino": "+1\n. +1\n. ",
    "markvandertol": "Maybe a smaller feature that will pave the way for websocket support is to just add support for `Switching Protocols` (see http://tools.ietf.org/html/draft-ietf-httpbis-p1-messaging-26#section-6.7)  in Spray client and server. Then an external library can provide websocket support, without that library having to hook deep into the internals of spray.\n. Thanks for having a look at my pull request. The only difference between these two rejections is the rejection message, so I also think that would be a good idea to merge these two rejections. I've made the changes and updated this pull request with the new commit.\n. I've made the changes discussed. The `HttpAuthenticator` generates the headers required for the rejection response. Also removed `scheme`, `realm` and `params` from `HttpAuthenticator`.\n. I've made the last changes with the last remarks fixed.\n\nEDIT: It looks that these last comments are not part of this discussion. They can be found here: https://github.com/MarkvanderTol/spray/commit/55f30abd0fb9ca7011e39b889ac6c70a110b7f25\n. @agemooij, you can change the commit message by _amending_ the commit. Some tools don't allow doing an amend when there aren't any changed files, but with the Git command line application it will work fine. The command will be `git commit --amend`. You will need a force push afterwards if want to push to the same branch the original commit is in.\n\nRebasing usually shouldn't give problems. To make rebasing easier, you first merge/pull from the branch where you want to rebase onto (don't worry about merge commits, since these will be removed later with the rebase). Rebase using `git rebase -i upstream/master`. Squash every commit, except the first. Don't reorder the commits, since that will give merge conflicts. The commit will get the date of the first commit, but you can reset the date of that commit by using `git commit --amend --reset-author`. Doing it this way always works fine for me.\n. You can merge all commits to a single commit by rebasing and squashing the commits. This done using the rebase command of git. Rebase using `git rebase -i upstream/master` (if this repo is named upstream in your remotes config). Squash every commit, except the first, by changing _pick_ to _squash_. Don't reorder the commits, since that will give merge conflicts. The commit will get the date of the first commit, but you can reset the date of that commit by using `git commit --amend --reset-author`. Doing it this way always works fine for me. You will need a force push (`git push -f master:master` in your case) afterwards if you want to push to the same branch the original commit is in.\n\nYou can change the commit message of the latest commit by _amending_ the commit. Some tools don't allow doing an amend when there aren't any changed files, but with the Git command line application it will work fine. The command will be `git commit --amend`. Here you will need a force push too if you want to push to the same branch the original commit is in.\n. I'm able to reproduce this with an HTTP client (I used Curl) that closes the connection after the timeout response has been received. When the client keeps the connection open (such as a Google Chrome), it doesn't occur.\n\n```\n07/29 12:20:51 WARN [on-spray-can-akka.actor.default-dispatcher-4] akka://on-spray-can/user/IO-HTTP/listener-0/0 - Cannot dispatch HttpResponse as response (part) for GET request to 'http://localhost:8080/timeout' since current response state is 'Completed' but should be 'Uncompleted'\n07/29 12:20:51 ERROR[on-spray-can-akka.actor.default-dispatcher-2] a.d.TaskInvocation - null\njava.lang.NullPointerException: null\n    at spray.can.server.ResponseReceiverRef.unhandledMessage(ResponseReceiverRef.scala:81) ~[classes/:na]\n    at spray.can.server.ResponseReceiverRef.dispatch(ResponseReceiverRef.scala:70) ~[classes/:na]\n    at spray.can.server.ResponseReceiverRef.handle(ResponseReceiverRef.scala:49) ~[classes/:na]\n    at akka.spray.UnregisteredActorRefBase.$bang(UnregisteredActorRefBase.scala:72) ~[classes/:na]\n\u2026\n```\n\nThe implicit conversion `spray.io.PipelineContext.pipelineContext2ActorContext` returns `null`, because `context.actorContext` becomes `null` when the connection closes. I haven't looked into why this happens.\n. This issue is only about the NPE, the warning is of course expected. I agree with the second point. \n. I think think that the group that has `core.autocrlf` set to `false` on Windows is quite large, since it is asked during the installation of Git whether you want this. So both `true` and `false` are common. So just assuming it is in Windows style causes problems.\n\nBy setting `eol=lf` in the `.gitattributes` it ignores the Git setting of the user and force Unix style, whatever the setting of the user may be. Only Windows users that currently have checked out the code and have Git configured to use Windows style line endings have to checkout the code again. They don't have update their local configuration.\n\nAnother solution would be to assume the line endings in the source files could be in either CRLF or LF, regardless of what `System.getProperty(\"line.separator\")` says.\n. > What are the occurrences where line endings make problems? In many cases it should be triple-quoted strings, right?\n\nIndeed the triple-quoted strings, but also some tests read text from resource files.\n\n> One more question: How do these people instruct their editors to not produce Windows-style line endings?\n\nIt depends on the editor. Most editors auto-detect line ending style.\n. Maybe a smaller feature that will pave the way for websocket support is to just add support for `Switching Protocols` (see http://tools.ietf.org/html/draft-ietf-httpbis-p1-messaging-26#section-6.7)  in Spray client and server. Then an external library can provide websocket support, without that library having to hook deep into the internals of spray.\n. Thanks for having a look at my pull request. The only difference between these two rejections is the rejection message, so I also think that would be a good idea to merge these two rejections. I've made the changes and updated this pull request with the new commit.\n. I've made the changes discussed. The `HttpAuthenticator` generates the headers required for the rejection response. Also removed `scheme`, `realm` and `params` from `HttpAuthenticator`.\n. I've made the last changes with the last remarks fixed.\n\nEDIT: It looks that these last comments are not part of this discussion. They can be found here: https://github.com/MarkvanderTol/spray/commit/55f30abd0fb9ca7011e39b889ac6c70a110b7f25\n. @agemooij, you can change the commit message by _amending_ the commit. Some tools don't allow doing an amend when there aren't any changed files, but with the Git command line application it will work fine. The command will be `git commit --amend`. You will need a force push afterwards if want to push to the same branch the original commit is in.\n\nRebasing usually shouldn't give problems. To make rebasing easier, you first merge/pull from the branch where you want to rebase onto (don't worry about merge commits, since these will be removed later with the rebase). Rebase using `git rebase -i upstream/master`. Squash every commit, except the first. Don't reorder the commits, since that will give merge conflicts. The commit will get the date of the first commit, but you can reset the date of that commit by using `git commit --amend --reset-author`. Doing it this way always works fine for me.\n. You can merge all commits to a single commit by rebasing and squashing the commits. This done using the rebase command of git. Rebase using `git rebase -i upstream/master` (if this repo is named upstream in your remotes config). Squash every commit, except the first, by changing _pick_ to _squash_. Don't reorder the commits, since that will give merge conflicts. The commit will get the date of the first commit, but you can reset the date of that commit by using `git commit --amend --reset-author`. Doing it this way always works fine for me. You will need a force push (`git push -f master:master` in your case) afterwards if you want to push to the same branch the original commit is in.\n\nYou can change the commit message of the latest commit by _amending_ the commit. Some tools don't allow doing an amend when there aren't any changed files, but with the Git command line application it will work fine. The command will be `git commit --amend`. Here you will need a force push too if you want to push to the same branch the original commit is in.\n. I'm able to reproduce this with an HTTP client (I used Curl) that closes the connection after the timeout response has been received. When the client keeps the connection open (such as a Google Chrome), it doesn't occur.\n\n```\n07/29 12:20:51 WARN [on-spray-can-akka.actor.default-dispatcher-4] akka://on-spray-can/user/IO-HTTP/listener-0/0 - Cannot dispatch HttpResponse as response (part) for GET request to 'http://localhost:8080/timeout' since current response state is 'Completed' but should be 'Uncompleted'\n07/29 12:20:51 ERROR[on-spray-can-akka.actor.default-dispatcher-2] a.d.TaskInvocation - null\njava.lang.NullPointerException: null\n    at spray.can.server.ResponseReceiverRef.unhandledMessage(ResponseReceiverRef.scala:81) ~[classes/:na]\n    at spray.can.server.ResponseReceiverRef.dispatch(ResponseReceiverRef.scala:70) ~[classes/:na]\n    at spray.can.server.ResponseReceiverRef.handle(ResponseReceiverRef.scala:49) ~[classes/:na]\n    at akka.spray.UnregisteredActorRefBase.$bang(UnregisteredActorRefBase.scala:72) ~[classes/:na]\n\u2026\n```\n\nThe implicit conversion `spray.io.PipelineContext.pipelineContext2ActorContext` returns `null`, because `context.actorContext` becomes `null` when the connection closes. I haven't looked into why this happens.\n. This issue is only about the NPE, the warning is of course expected. I agree with the second point. \n. I think think that the group that has `core.autocrlf` set to `false` on Windows is quite large, since it is asked during the installation of Git whether you want this. So both `true` and `false` are common. So just assuming it is in Windows style causes problems.\n\nBy setting `eol=lf` in the `.gitattributes` it ignores the Git setting of the user and force Unix style, whatever the setting of the user may be. Only Windows users that currently have checked out the code and have Git configured to use Windows style line endings have to checkout the code again. They don't have update their local configuration.\n\nAnother solution would be to assume the line endings in the source files could be in either CRLF or LF, regardless of what `System.getProperty(\"line.separator\")` says.\n. > What are the occurrences where line endings make problems? In many cases it should be triple-quoted strings, right?\n\nIndeed the triple-quoted strings, but also some tests read text from resource files.\n\n> One more question: How do these people instruct their editors to not produce Windows-style line endings?\n\nIt depends on the editor. Most editors auto-detect line ending style.\n. ",
    "crfeliz": "+1\n. +1\n. ",
    "kaesler": "+1\n. +1\n. ",
    "nefilim": "+1\n. +1\n. stuck on 1.3.0 and getting bitten by https://github.com/spray/spray/issues/785 that was fixed on 1.3.1 :( \n. @sirthias Thank you, that should tie me over until 1.3.2.\n. @sirthias thanks much, first tests are looking good so far! \n. +1\n. +1\n. stuck on 1.3.0 and getting bitten by https://github.com/spray/spray/issues/785 that was fixed on 1.3.1 :( \n. @sirthias Thank you, that should tie me over until 1.3.2.\n. @sirthias thanks much, first tests are looking good so far! \n. ",
    "briantopping": "+1, because it just takes soooo long to scroll to the end of this list and all.\n. Atmosphere does look very interesting.  The support for different protocols over the ws: connection looks promising.  \n\nHave you considered using one of the existing spray websocket implementations to create an integration with Atmosphere?  It would be cool to see it running!\n. @troypayne Not sure if that was directed to me, I've been successful integrating against spray-websockets for now.  Atmosphere seems to be very comprehensive with protocol support, and being built on Netty, seems like it would be easier to acquire new protocols that work with it.  New protocols means more capabilities.  I have no idea whether it's practical or not, which is why I was suggesting an advocate look at something like spray-websockets and wrap Atmosphere with the spray integration plumbing from it.\n. All, Mathias discussed websockets as a specific goal of the conversion of Spray to use Reactive Streams under akka-http. Suggest checking out his video from ScalaDays in Berlin!\n. Hi Matthias, any news on this?  I have users wanting to upload archives that fill dual-layer DVDs -- 7GB or so.  The archives are tgz's with a very large quantity of files in them and they need to be uploaded from a browser, so it's practically impossible for us to have them explode the archive and upload the files that way.\n. Hmm, I don't have anything additional.  I guess we're going to have to find it though ;)\n. Ok, I'm back from a bunch of office travel and a cross-continent relocation.  Will get started reformulating this against release-1.2.x branch...\n. This work is really good.  I have applied it manually to the release/1.2 branch at https://github.com/topping/spray/tree/osgi.  One question though, when I do the build there, I get a bad manifest out of spray-util with an Import-Package element that starts like the following:\n\n> Import-Package: akka.actor;version=\"${range;[==,=+);${@}}\",akka.dispatch\n>  .sysmsg;version=\"${range;[==,=+);${@}}\",akka.event;version=\"${range;[==\n>  ,=+);${@}}\",akka.pattern;version=\"${range;[==,=+);${@}}\",akka.util;vers\n>  ion=\"${range;[==,=+);${@}}\",com.typesafe.config;version=\"[1.0,2)\",scala\n>  ;version=\"[2.10,2.11)\",scala.collection;version=\"[2.10,2.11)\",scala.col\n>  lection.convert;version=\"[2.10,2.11)\",scala.collection.generic;version=\n\nDoes this make any sense?\n. Yes, this makes sense.  I had started to write about it on spray-user yesterday and decided to wait until I had something working.  They prefer the \"uberjar\" pattern whereas you had made bundles out of each.  \n\nI had forgotten why they did this when I started looking at your patch -- it's because of the config loader that needs to be able to get reference.conf contributions across foreign bundles.  I don't know how we are going to do that here without the replicated uberjar pattern that I had in my patch.\n\nIdeally, the jars would individually configure with their own reference.conf.  That's a different pattern than Akka is currently set up with though, and may need to be a 3.x type change. \n\nIn the mean time, it could be advantageous to emulate the ability to get an enumerated list of resources by having each bundle publish a service that returns it's configuration.  When the configurations were all available, some configuration lifecycle would engage and do things as normal.  But once again, we are pushing OSGi patterns on developers that may start complaining about requirements they don't understand or use.  When that happens, OSGi support tends to get removed in short order.\n\nThis is why uberjar can be politically expedient as well.\n. Thanks Michael, will re-integrate this against my PR and report back.  If everything comes up smoothly, will get a PR cut that has the spray-can demo working on Karaf.\n. One of the things I've run into is the POMs that are generated should include \"<type>bundle</type>\" for the interdependencies, otherwise they don't resolve from Nexus properly.\n. Thanks Michael, I think it was something with between Maven and Nexus resolving a jar with the same version from the wrong place, even though I had the repository group order set explicitly otherwise.  The bundle plugin is definitely set up as you point out, otherwise the dependent project couldn't generate bundles.  Changing the version numbers got around it.  \n\nWhat I've got at this point is this patch applied to the 1.2 sources and an adaptation of the spray routing on-spray--can demo service, see https://gist.github.com/topping/5971839.  This worked with the uberjar setup that I had in my other PR, but doesn't work with this new jar setup.  I've seen https://gist.github.com/mpilquist/5882185, am I to understand that I also need that code in order for this PR to work?  In working with what I have, I don't see any changes that would allow for the configurations to be loaded across bundles.  So kind of confused.\n. Gotcha.  But isn't the whole idea of this PR is to avoid a spray-osgi bundle in the first place?  I think it's a good idea to plant the seeds of individual bundles, we must also measure the cost of said bundles if the mainline spray team needs to keep additional patterns in mind.  \n\nI'm not advocating for an uberjar because I think they are disgusting hacks, but if the politics of OSGi start coming up because maintenance costs go up, we all lose.  An uberjar lets the team continue \"business as usual\" while the minority of us builds momentum around Spray OSGi.  By that time, Akka may also see the utility in adopting a configuration architecture that is less toxic to module isolation, and we all win.\n. This is a good discussion.  I'm leaning toward your thinking for what seems like the wrong reasons -- that getting the bundles started will give us a good base to work from instead of trying to get the Taj Mahal on the first try.  \n\nThat said, the \"simple things should be simple\", and I think the sample apps should have few, if any, extra lines of code in them.  What I had done in http://github.com/spray/spray/pull/312 has very few changes to the on-spray-can demo to make it work with OSGi, and all the classloader poo is buried in the libraries, as it should be.  If for no other reason, if it's a part of user apps, those apps become impossible to upgrade without a lot of user intervention. In environments with multiple developers, such code may get augmented and refactored over time, making it very difficult to upgrade in a reliable manner.  So in general, I'm against this kind of pattern in a framework for anything but short term goals (i.e. while the user base is very small).\n\nAs far as your point about multiple versions of the jars, we're going to have that and I think others will too.  Springing that on some unsuspecting user when they need that functionality is really going to hurt someone.  \n\nAs for other ideas, dynamic imports are kind of skanky.  Fragments might work if the spray-osgi bundle was the host and the current spray bundles were fragments.  That's the wrong direction long term though, and it's going to be at least as disruptive to fix that as the previous issue I mentioned.\n\nWe're really under the gun to get a working POC this week so I'm going to go with what's here and include the code from your gist.  \n. @sirthias: I think we're in heated agreement with how to move things forward.  I'd like to see a sample app as a part of this PR and wanted to merge what I had done but wasn't able to.  Other that that, avoiding uberjars is the way to go whenever possible, and if we can start with that and continue to improve, IMHO it's better than trying to agree on all this in the confines of PRs. :)\n. Ooooh, I like this idea!\n\nSo what if we were able to commit this PR as-is and work toward that?  Or should the PR be everything at once?\n. This is great news, guys!  \n. Hi Olger, I tried to merge this PR to 1.3 but am having some problems running the spray.examples.Main that you provided.  Do you have interest/time to review what I did?  It's at https://github.com/topping/spray/tree/pr372 and I added you as a collaborator there in case you see something simple that's easy to push.  Thanks for any consideration you can make on this or leads if you've already done this somewhere else!!  \n. My bad, I was using SBT improperly.  That branch in my fork is working now with Google.  Thanks for all your work on that!  Should I create another PR?\n. Thanks Olger, the hard work was yours so far, the PR should come from you!\n\nIn getting closer to a full implementation, it seems that the client and the server side code should both be implemented for oAuth 1.0a and 2.0.  It appears the code is 2.0 only, but I didn't dig enough to see if it was able to act as a client as well.  It turns out I need 1.0, so I have some work to do.\n\nIt seems to me that we should start another project for the core of oAuth, then prune down the changes that Spray needs.  This is the path that spray-websockets took and it didn't feel like there were too many compromises that needed to be taken.  It also helps the spray team as they refactor for 2.0.\n\nWDYT?\n. Hi Olger, 2.0 is considered less secure than 1.0, but much easier to implement.  The company that I am primarily using this with chose 1.0, even after 2.0 was out.  So it's important to support it.\n\nAs far as the public / private types thing, that was also a problem with spray-websockets.  One solution is to create a jar containing classes with the same package names.  This works partially, but causes a lot of problems for OSGi.  \n\nI'm going to open a ticket regarding this.  I don't like opening tickets that don't have a patch, but it's important.  \n. Hi gang, some background on this PR:  \n- Between 1.3.0 and 1.3.1, Spray stopped providing the \"Connection\" header.  This completely breaks the websocket implementations that rely on it and we can't use the 1.3.1 code.\n- There is an RFC referenced that explains exactly what is going on with the protocol.\n- The code in this PR is all additions for specific headers, not changes to existing headers or logic.  Re-adding these headers cannot possibly introduce a regression.\n- There's existing headers such as keep-alive that are handled in what appears to be the same manner.\n- Changes to the test are provided so the regression is not introduced again in the future.\n- If there's some better way to accomplish this now or in some future incarnation of the code, an additional three lines isn't going to be a major liability to work around and will provide additional de-facto requirements for the refactor. \n\nHopefully this is sufficient justification to apply the patch.  Until it is applied, spray-websockets users cannot upgrade past 1.3.0.  \n\nThanks for your consideration!!\n. @dcaoyuan @jrudolph:  Some problems with copying a whole class from a framework into a subproject just because the core framework doesn't support something:\n- Fixes occur to those classes, someone must be paying attention at all times and migrating them.\n- Sometimes injecting the changed class requires a means of injection.  Aggregator classes get duplicated, making the whole process look quite a bit more involved to the busy framework owner than it actually is.\n- A classpath can only show up in one JAR in OSGi.  Including a `spray-can/src/main/scala/spray/can/rendering/ResponseRenderingComponentPatched.scala` in spray-websocket means it must be renamed to a different package.\n- When core classes are duplicated and move packages, unresolvable class visibility issues often arise when a dependency of the duplicated class is private and is depended on by other framework classes.  This makes it actually impossible to fix a problem in a sub framework that is extremely important to some users.\n\nObservations: \n- One might argue that OSGi is causing the cascade of problems, but OSGi usually just highlights suboptimal design.  I'm not saying that is the case here, just noting my experience.\n- Over time, I've used a lot of Really Important Projects where no \"hacks\" were allowed for a time, and a few years later, the projects couldn't find anyone to contribute.  FreeBSD v. Linux is probably the writ large example of this.  Of course, the other end of the spectrum is projects that became so full of hacks that they were unreliable.  Arguably, Linux was _also_ the example of this for a period in time, but they survived because they had the user base.  It's sometimes as important to keep everyone engaged as it is to work around a few \"hacks\" until everything can be unified.\n- Contributors to projects like spray-websockets have busy lives, too.  There's a certain point that it just becomes too much effort to work around \"four lines of missing code\" and people move on.  It might be that \"the other websockets implementation\" is the better one, but with no external guidance, we're left to our own devices to figure this out.  It's time consuming, and these kinds of issues are create a lot of challenges.\n\nThanks for your renewed consideration of this patch, guys!!  We're trying to ship amazing apps with Spray and Akka!  :)\n. @sirthias +1 thank you very kindly!!\n. Should this be marked as a bug?\n. Great, thanks guys!\n. I just ran into this as well. If you happen to be using Karaf, you can specify spray-httpx as `<bundle>wrap:mvn:io.spray/spray-httpx_2.11/1.3.1$overwrite=merge&amp;Import-Package=spray.json;resolution:=optional;version=\"[1.2,2)\",net.liftweb.json;resolution:=optional,org.json4s;resolution:=optional,org.json4s.jackson;resolution:=optional,org.json4s.native;resolution:=optional,twirl.api;resolution:=optional,play.api.data.validation;resolution:=optional,play.api.libs.json;resolution:=optional,*</bundle>`. This is somewhat dangerous in that it removes import version requirements for spray-httpx, but it gets around the issue.\n\nThis is a super skanky mess because if you don't specify the optional attributes, wrap protocol removes them along with the versions. IMHO this may be a bug in the merge, it's unclear whether it makes sense for all the versions and attributes to be wiped out like that.\n. I just got a chance to try this out, not having much luck. Bnd (external library to SbtOsgi that generates the manifests) is throwing errors during each module packaging for me:\n\n`[error] bnd: Invalid syntax for version: ${@}, for cmd: range, arguments; [range, [==,=+), ${@}]`\n\nI tried to fix it, but this is a level of SBT voodoo that's beyond what I'm familiar with. Seems like there's an attempt at variable interpolation that's not getting picked up somehow (bnd is getting the \"==\" and \"=+\" and I am not aware that it handles those).\n. Hi @alexlomov, something to consider: Pull requests of the minority must play to a higher standard than those of the majority. As it applies here, most don't understand how great OSGi is. Until they do, we must double down to ensure that the generation of our OSGi attributes doesn't cause a burden to the mainstream users and developers. So indications of error displayed in red on the build are not an option, and the first thing someone might do is turn off manifest generation altogether. People don't have time for to get wrapped up in all that discussion.\n\nIs it possible to get the same result without generating the errors?\n. Of course! Once everything is cleaned up, I'd be happy to test again and recommend it's inclusion. Would be nice to get rid of the brain damaged bundle reference that I referenced in #866. \n. Hi, this looks mostly good. I've incorporated edits at https://gist.github.com/topping/bfb952f0cfe9da977606 to isolate the import to spray-httpx, further reducing spurious output.  \n\n@sirthias, @jrudolph: I did a cross-build with the contents of that gist and it's correctly modifying spray-httpx only, and only on >= 2.11. If it starts getting to more jars needing this treatment, let's revisit how it's done and keep the build from getting brittle. Thoughts?\n. +1, because it just takes soooo long to scroll to the end of this list and all.\n. Atmosphere does look very interesting.  The support for different protocols over the ws: connection looks promising.  \n\nHave you considered using one of the existing spray websocket implementations to create an integration with Atmosphere?  It would be cool to see it running!\n. @troypayne Not sure if that was directed to me, I've been successful integrating against spray-websockets for now.  Atmosphere seems to be very comprehensive with protocol support, and being built on Netty, seems like it would be easier to acquire new protocols that work with it.  New protocols means more capabilities.  I have no idea whether it's practical or not, which is why I was suggesting an advocate look at something like spray-websockets and wrap Atmosphere with the spray integration plumbing from it.\n. All, Mathias discussed websockets as a specific goal of the conversion of Spray to use Reactive Streams under akka-http. Suggest checking out his video from ScalaDays in Berlin!\n. Hi Matthias, any news on this?  I have users wanting to upload archives that fill dual-layer DVDs -- 7GB or so.  The archives are tgz's with a very large quantity of files in them and they need to be uploaded from a browser, so it's practically impossible for us to have them explode the archive and upload the files that way.\n. Hmm, I don't have anything additional.  I guess we're going to have to find it though ;)\n. Ok, I'm back from a bunch of office travel and a cross-continent relocation.  Will get started reformulating this against release-1.2.x branch...\n. This work is really good.  I have applied it manually to the release/1.2 branch at https://github.com/topping/spray/tree/osgi.  One question though, when I do the build there, I get a bad manifest out of spray-util with an Import-Package element that starts like the following:\n\n> Import-Package: akka.actor;version=\"${range;[==,=+);${@}}\",akka.dispatch\n>  .sysmsg;version=\"${range;[==,=+);${@}}\",akka.event;version=\"${range;[==\n>  ,=+);${@}}\",akka.pattern;version=\"${range;[==,=+);${@}}\",akka.util;vers\n>  ion=\"${range;[==,=+);${@}}\",com.typesafe.config;version=\"[1.0,2)\",scala\n>  ;version=\"[2.10,2.11)\",scala.collection;version=\"[2.10,2.11)\",scala.col\n>  lection.convert;version=\"[2.10,2.11)\",scala.collection.generic;version=\n\nDoes this make any sense?\n. Yes, this makes sense.  I had started to write about it on spray-user yesterday and decided to wait until I had something working.  They prefer the \"uberjar\" pattern whereas you had made bundles out of each.  \n\nI had forgotten why they did this when I started looking at your patch -- it's because of the config loader that needs to be able to get reference.conf contributions across foreign bundles.  I don't know how we are going to do that here without the replicated uberjar pattern that I had in my patch.\n\nIdeally, the jars would individually configure with their own reference.conf.  That's a different pattern than Akka is currently set up with though, and may need to be a 3.x type change. \n\nIn the mean time, it could be advantageous to emulate the ability to get an enumerated list of resources by having each bundle publish a service that returns it's configuration.  When the configurations were all available, some configuration lifecycle would engage and do things as normal.  But once again, we are pushing OSGi patterns on developers that may start complaining about requirements they don't understand or use.  When that happens, OSGi support tends to get removed in short order.\n\nThis is why uberjar can be politically expedient as well.\n. Thanks Michael, will re-integrate this against my PR and report back.  If everything comes up smoothly, will get a PR cut that has the spray-can demo working on Karaf.\n. One of the things I've run into is the POMs that are generated should include \"<type>bundle</type>\" for the interdependencies, otherwise they don't resolve from Nexus properly.\n. Thanks Michael, I think it was something with between Maven and Nexus resolving a jar with the same version from the wrong place, even though I had the repository group order set explicitly otherwise.  The bundle plugin is definitely set up as you point out, otherwise the dependent project couldn't generate bundles.  Changing the version numbers got around it.  \n\nWhat I've got at this point is this patch applied to the 1.2 sources and an adaptation of the spray routing on-spray--can demo service, see https://gist.github.com/topping/5971839.  This worked with the uberjar setup that I had in my other PR, but doesn't work with this new jar setup.  I've seen https://gist.github.com/mpilquist/5882185, am I to understand that I also need that code in order for this PR to work?  In working with what I have, I don't see any changes that would allow for the configurations to be loaded across bundles.  So kind of confused.\n. Gotcha.  But isn't the whole idea of this PR is to avoid a spray-osgi bundle in the first place?  I think it's a good idea to plant the seeds of individual bundles, we must also measure the cost of said bundles if the mainline spray team needs to keep additional patterns in mind.  \n\nI'm not advocating for an uberjar because I think they are disgusting hacks, but if the politics of OSGi start coming up because maintenance costs go up, we all lose.  An uberjar lets the team continue \"business as usual\" while the minority of us builds momentum around Spray OSGi.  By that time, Akka may also see the utility in adopting a configuration architecture that is less toxic to module isolation, and we all win.\n. This is a good discussion.  I'm leaning toward your thinking for what seems like the wrong reasons -- that getting the bundles started will give us a good base to work from instead of trying to get the Taj Mahal on the first try.  \n\nThat said, the \"simple things should be simple\", and I think the sample apps should have few, if any, extra lines of code in them.  What I had done in http://github.com/spray/spray/pull/312 has very few changes to the on-spray-can demo to make it work with OSGi, and all the classloader poo is buried in the libraries, as it should be.  If for no other reason, if it's a part of user apps, those apps become impossible to upgrade without a lot of user intervention. In environments with multiple developers, such code may get augmented and refactored over time, making it very difficult to upgrade in a reliable manner.  So in general, I'm against this kind of pattern in a framework for anything but short term goals (i.e. while the user base is very small).\n\nAs far as your point about multiple versions of the jars, we're going to have that and I think others will too.  Springing that on some unsuspecting user when they need that functionality is really going to hurt someone.  \n\nAs for other ideas, dynamic imports are kind of skanky.  Fragments might work if the spray-osgi bundle was the host and the current spray bundles were fragments.  That's the wrong direction long term though, and it's going to be at least as disruptive to fix that as the previous issue I mentioned.\n\nWe're really under the gun to get a working POC this week so I'm going to go with what's here and include the code from your gist.  \n. @sirthias: I think we're in heated agreement with how to move things forward.  I'd like to see a sample app as a part of this PR and wanted to merge what I had done but wasn't able to.  Other that that, avoiding uberjars is the way to go whenever possible, and if we can start with that and continue to improve, IMHO it's better than trying to agree on all this in the confines of PRs. :)\n. Ooooh, I like this idea!\n\nSo what if we were able to commit this PR as-is and work toward that?  Or should the PR be everything at once?\n. This is great news, guys!  \n. Hi Olger, I tried to merge this PR to 1.3 but am having some problems running the spray.examples.Main that you provided.  Do you have interest/time to review what I did?  It's at https://github.com/topping/spray/tree/pr372 and I added you as a collaborator there in case you see something simple that's easy to push.  Thanks for any consideration you can make on this or leads if you've already done this somewhere else!!  \n. My bad, I was using SBT improperly.  That branch in my fork is working now with Google.  Thanks for all your work on that!  Should I create another PR?\n. Thanks Olger, the hard work was yours so far, the PR should come from you!\n\nIn getting closer to a full implementation, it seems that the client and the server side code should both be implemented for oAuth 1.0a and 2.0.  It appears the code is 2.0 only, but I didn't dig enough to see if it was able to act as a client as well.  It turns out I need 1.0, so I have some work to do.\n\nIt seems to me that we should start another project for the core of oAuth, then prune down the changes that Spray needs.  This is the path that spray-websockets took and it didn't feel like there were too many compromises that needed to be taken.  It also helps the spray team as they refactor for 2.0.\n\nWDYT?\n. Hi Olger, 2.0 is considered less secure than 1.0, but much easier to implement.  The company that I am primarily using this with chose 1.0, even after 2.0 was out.  So it's important to support it.\n\nAs far as the public / private types thing, that was also a problem with spray-websockets.  One solution is to create a jar containing classes with the same package names.  This works partially, but causes a lot of problems for OSGi.  \n\nI'm going to open a ticket regarding this.  I don't like opening tickets that don't have a patch, but it's important.  \n. Hi gang, some background on this PR:  \n- Between 1.3.0 and 1.3.1, Spray stopped providing the \"Connection\" header.  This completely breaks the websocket implementations that rely on it and we can't use the 1.3.1 code.\n- There is an RFC referenced that explains exactly what is going on with the protocol.\n- The code in this PR is all additions for specific headers, not changes to existing headers or logic.  Re-adding these headers cannot possibly introduce a regression.\n- There's existing headers such as keep-alive that are handled in what appears to be the same manner.\n- Changes to the test are provided so the regression is not introduced again in the future.\n- If there's some better way to accomplish this now or in some future incarnation of the code, an additional three lines isn't going to be a major liability to work around and will provide additional de-facto requirements for the refactor. \n\nHopefully this is sufficient justification to apply the patch.  Until it is applied, spray-websockets users cannot upgrade past 1.3.0.  \n\nThanks for your consideration!!\n. @dcaoyuan @jrudolph:  Some problems with copying a whole class from a framework into a subproject just because the core framework doesn't support something:\n- Fixes occur to those classes, someone must be paying attention at all times and migrating them.\n- Sometimes injecting the changed class requires a means of injection.  Aggregator classes get duplicated, making the whole process look quite a bit more involved to the busy framework owner than it actually is.\n- A classpath can only show up in one JAR in OSGi.  Including a `spray-can/src/main/scala/spray/can/rendering/ResponseRenderingComponentPatched.scala` in spray-websocket means it must be renamed to a different package.\n- When core classes are duplicated and move packages, unresolvable class visibility issues often arise when a dependency of the duplicated class is private and is depended on by other framework classes.  This makes it actually impossible to fix a problem in a sub framework that is extremely important to some users.\n\nObservations: \n- One might argue that OSGi is causing the cascade of problems, but OSGi usually just highlights suboptimal design.  I'm not saying that is the case here, just noting my experience.\n- Over time, I've used a lot of Really Important Projects where no \"hacks\" were allowed for a time, and a few years later, the projects couldn't find anyone to contribute.  FreeBSD v. Linux is probably the writ large example of this.  Of course, the other end of the spectrum is projects that became so full of hacks that they were unreliable.  Arguably, Linux was _also_ the example of this for a period in time, but they survived because they had the user base.  It's sometimes as important to keep everyone engaged as it is to work around a few \"hacks\" until everything can be unified.\n- Contributors to projects like spray-websockets have busy lives, too.  There's a certain point that it just becomes too much effort to work around \"four lines of missing code\" and people move on.  It might be that \"the other websockets implementation\" is the better one, but with no external guidance, we're left to our own devices to figure this out.  It's time consuming, and these kinds of issues are create a lot of challenges.\n\nThanks for your renewed consideration of this patch, guys!!  We're trying to ship amazing apps with Spray and Akka!  :)\n. @sirthias +1 thank you very kindly!!\n. Should this be marked as a bug?\n. Great, thanks guys!\n. I just ran into this as well. If you happen to be using Karaf, you can specify spray-httpx as `<bundle>wrap:mvn:io.spray/spray-httpx_2.11/1.3.1$overwrite=merge&amp;Import-Package=spray.json;resolution:=optional;version=\"[1.2,2)\",net.liftweb.json;resolution:=optional,org.json4s;resolution:=optional,org.json4s.jackson;resolution:=optional,org.json4s.native;resolution:=optional,twirl.api;resolution:=optional,play.api.data.validation;resolution:=optional,play.api.libs.json;resolution:=optional,*</bundle>`. This is somewhat dangerous in that it removes import version requirements for spray-httpx, but it gets around the issue.\n\nThis is a super skanky mess because if you don't specify the optional attributes, wrap protocol removes them along with the versions. IMHO this may be a bug in the merge, it's unclear whether it makes sense for all the versions and attributes to be wiped out like that.\n. I just got a chance to try this out, not having much luck. Bnd (external library to SbtOsgi that generates the manifests) is throwing errors during each module packaging for me:\n\n`[error] bnd: Invalid syntax for version: ${@}, for cmd: range, arguments; [range, [==,=+), ${@}]`\n\nI tried to fix it, but this is a level of SBT voodoo that's beyond what I'm familiar with. Seems like there's an attempt at variable interpolation that's not getting picked up somehow (bnd is getting the \"==\" and \"=+\" and I am not aware that it handles those).\n. Hi @alexlomov, something to consider: Pull requests of the minority must play to a higher standard than those of the majority. As it applies here, most don't understand how great OSGi is. Until they do, we must double down to ensure that the generation of our OSGi attributes doesn't cause a burden to the mainstream users and developers. So indications of error displayed in red on the build are not an option, and the first thing someone might do is turn off manifest generation altogether. People don't have time for to get wrapped up in all that discussion.\n\nIs it possible to get the same result without generating the errors?\n. Of course! Once everything is cleaned up, I'd be happy to test again and recommend it's inclusion. Would be nice to get rid of the brain damaged bundle reference that I referenced in #866. \n. Hi, this looks mostly good. I've incorporated edits at https://gist.github.com/topping/bfb952f0cfe9da977606 to isolate the import to spray-httpx, further reducing spurious output.  \n\n@sirthias, @jrudolph: I did a cross-build with the contents of that gist and it's correctly modifying spray-httpx only, and only on >= 2.11. If it starts getting to more jars needing this treatment, let's revisit how it's done and keep the build from getting brittle. Thoughts?\n. ",
    "martin-magakian": "+1\n\nI hope it will be possible to use Atmosphere framework with Spray<br />\nhttps://github.com/Atmosphere/atmosphere\n. +1\n\nI hope it will be possible to use Atmosphere framework with Spray<br />\nhttps://github.com/Atmosphere/atmosphere\n. ",
    "milo-ft": "+1\n. +1\n. ",
    "BenDavidJamin": "+1\n. +1\n. ",
    "jonysy": "+1\n. +1\n. ",
    "trungpham": "This looks pretty good. https://github.com/wandoulabs/spray-websocket\n. This looks pretty good. https://github.com/wandoulabs/spray-websocket\n. ",
    "megothss": "+1\n. +1\n. ",
    "peter-empen": "+1\n. +1\n. ",
    "jurisk": "+1\n. +1\n. ",
    "psisoyev": "+1\n. That's why github have pull request system - to discuss changes. \n\nI wasn't sure about the change and security consequences, that's why firstly I've created discussion in the user group. It wasn't popular though :) \n\nThanks for your comments\n. +1\n. That's why github have pull request system - to discuss changes. \n\nI wasn't sure about the change and security consequences, that's why firstly I've created discussion in the user group. It wasn't popular though :) \n\nThanks for your comments\n. ",
    "skisel": "+1\n. +1\n. ",
    "andyglow": "+1\n. +1\n. ",
    "dimitrion": "+1\n. +1\n. ",
    "angiolep": "+1\n. +1\n. ",
    "reid-spencer": "@matanster - Yes, it would and that is what's in the works right now, I believe. When akka-http is released they will shortly thereafter release websocket support for it. Might take another quarter or so. Perhaps one of the akka/spray developers can comment on the status of this? \n. Having just been bitten by this route construction mistake, I'll put my $0.02 worth in on having it fixed. It has been 18 months since comment was made on it. What's the status?\n. Good to know .. looking forward to that getting removed from experimental status.\n. If you end up doing this, please make sure you use the latest pickling version. 0.8.0 had some issues.\n. +1 - I would like to see this added perhaps in the akka-http time frame?\n. @matanster - Yes, it would and that is what's in the works right now, I believe. When akka-http is released they will shortly thereafter release websocket support for it. Might take another quarter or so. Perhaps one of the akka/spray developers can comment on the status of this? \n. Having just been bitten by this route construction mistake, I'll put my $0.02 worth in on having it fixed. It has been 18 months since comment was made on it. What's the status?\n. Good to know .. looking forward to that getting removed from experimental status.\n. If you end up doing this, please make sure you use the latest pickling version. 0.8.0 had some issues.\n. +1 - I would like to see this added perhaps in the akka-http time frame?\n. ",
    "dnrusakov": "+1\n. +1\n. ",
    "nikelin": "+1\n. +1\n. ",
    "salanki": "@matanster: @jrudolph is obviously trolling you.\n. @matanster: @jrudolph is obviously trolling you.\n. ",
    "weltermann17": "Thanks for fixing this so quickly. Just pulled and tested it. Funny what a shift down by 2 lines can do.\n\nHowever, sorry to nag: Increasing by a factor of 10 (~3.8mb file) a test with siege -b -r 10 -c 100 ...\nleads to a pretty constant loss of 30%. I'm testing on an os x (10.8.2, 16g, i7 2.7, 8 core, 256g ssd, java 1.7.0.7). The server consumes > 750% cpu. This is probably something that can be fixed by configuration tuning.\n. Good to know this is is configurable. Thanks.\n. Thanks for fixing this so quickly. Just pulled and tested it. Funny what a shift down by 2 lines can do.\n\nHowever, sorry to nag: Increasing by a factor of 10 (~3.8mb file) a test with siege -b -r 10 -c 100 ...\nleads to a pretty constant loss of 30%. I'm testing on an os x (10.8.2, 16g, i7 2.7, 8 core, 256g ssd, java 1.7.0.7). The server consumes > 750% cpu. This is probably something that can be fixed by configuration tuning.\n. Good to know this is is configurable. Thanks.\n. ",
    "betehess": "Hi, we're almost ready to start our project and we're wondering if you have an estimate on when you'll have time to work on that, or if we should go with Play in the meantime.\n. Note: we started a [thread](https://groups.google.com/forum/#!topic/shapeless-dev/67xlMdn7Ms8) on shapeless-dev. Miles had the following comment:\n\n> One very quick observation: I can see a few implicit defs with\n> inferred result types. I can't say whether this is relevant to your\n> issue or not, but it's something you should never do: leaving implicit\n> result types inferred means that the semantics are dependent on the\n> textual order of definition. \n\nNot sure this is something we want to fix in this PR.\n. It might really be an English issue on my side :-)\n\nThe [documentation says](http://spray.io/documentation/1.2.3/spray-can/http-server/#chunked-requests):\n\n> If the request-chunk-aggregation-limit config setting is set to zero the connection actor also dispatches the individual request parts of chunked requests to the handler actor. In these cases a full request consists of the following messages: [...]\n\nI read it as:\n\n**if** `request-chunk-aggregation-limit = 0` **then** the handler actor sees the `MessageChunks`, meaning the request is chunked. Isn't that contrary with your explanation which mentions `incoming-auto-chunking-threshold-size = 0` for that purpose?\n\n> If you've been testing with curl: Have you set an explicit --header \"Transfer-Encoding: chunked\" CLI option? Otherwise curl won't produce a chunked request.\n\nI tried again and the behavior makes sense: because of the [settings for the app](https://github.com/spray/spray/blob/master/examples/spray-can/simple-http-server/src/main/resources/application.conf#L13), I always hit the non-chunked branch. This is consistent with your explanation above.\n. Hi, we're almost ready to start our project and we're wondering if you have an estimate on when you'll have time to work on that, or if we should go with Play in the meantime.\n. Note: we started a [thread](https://groups.google.com/forum/#!topic/shapeless-dev/67xlMdn7Ms8) on shapeless-dev. Miles had the following comment:\n\n> One very quick observation: I can see a few implicit defs with\n> inferred result types. I can't say whether this is relevant to your\n> issue or not, but it's something you should never do: leaving implicit\n> result types inferred means that the semantics are dependent on the\n> textual order of definition. \n\nNot sure this is something we want to fix in this PR.\n. It might really be an English issue on my side :-)\n\nThe [documentation says](http://spray.io/documentation/1.2.3/spray-can/http-server/#chunked-requests):\n\n> If the request-chunk-aggregation-limit config setting is set to zero the connection actor also dispatches the individual request parts of chunked requests to the handler actor. In these cases a full request consists of the following messages: [...]\n\nI read it as:\n\n**if** `request-chunk-aggregation-limit = 0` **then** the handler actor sees the `MessageChunks`, meaning the request is chunked. Isn't that contrary with your explanation which mentions `incoming-auto-chunking-threshold-size = 0` for that purpose?\n\n> If you've been testing with curl: Have you set an explicit --header \"Transfer-Encoding: chunked\" CLI option? Otherwise curl won't produce a chunked request.\n\nI tried again and the behavior makes sense: because of the [settings for the app](https://github.com/spray/spray/blob/master/examples/spray-can/simple-http-server/src/main/resources/application.conf#L13), I always hit the non-chunked branch. This is consistent with your explanation above.\n. ",
    "gzoller": "This is interesting, but I'm not sure I like the OP's idea of path( \"A\" / (\"B\") / (\"C\") ).  As I understand the path code the / is the modular element, meaning it matches / plus the next \"thing\".\n\nSo maybe optional would work with a ? as a proxy for an optional \"/\" like this:\n\npath( \"A\" ? \"B\" ? \"C\" ) { a, b, c => \n\nwhere b and c are Option[String].\n. Thanks for the quick acknowledgment!  This has been in my production system for a few months and I guess this message was just the right size to trigger the problem.  If there's a hot-fix I'll watch the repo :-)\n. Fix confirmed on nightly 1.2-20131011 build.  Thank you!\n. This is interesting, but I'm not sure I like the OP's idea of path( \"A\" / (\"B\") / (\"C\") ).  As I understand the path code the / is the modular element, meaning it matches / plus the next \"thing\".\n\nSo maybe optional would work with a ? as a proxy for an optional \"/\" like this:\n\npath( \"A\" ? \"B\" ? \"C\" ) { a, b, c => \n\nwhere b and c are Option[String].\n. Thanks for the quick acknowledgment!  This has been in my production system for a few months and I guess this message was just the right size to trigger the problem.  If there's a hot-fix I'll watch the repo :-)\n. Fix confirmed on nightly 1.2-20131011 build.  Thank you!\n. ",
    "bthuillier": "What did you mean by proxy directive?\n. https://gist.github.com/bthuillier/8351056 something like this? \n. And if I understand the end point is: `\"http://oldapi.example.com\" + unmatchedPath`\n. I beginning to work on this features in https://github.com/bthuillier/spray/tree/proxy-directives\n. I didn't find where put test for `Directive`, I need some hint here =). I send you the CLA before noon\n. I update the code to not break the directives, and `require` and `hrequire` now use `filter` and `hfilter`\n. FYI I've just sign the CLA\n. I commited the efficiency improvement, it was my first contribution to a open source project.\n. I finished a first version of the directive, with documentation and test\n. Thanks for the answer, I'll cleanup and update the code asap\n. I think is good\n. Hello @sirthias and @hamiltont I'll be glad to work on a more complete version of the proxy directives.\n. Indeed it's easy to rebuild it, I actually rebuild it for testing some custom directives, I don't know if other people use it but with a good documentation I think it worth it. \n. yes\n. Perhaps I missing something but you already can do anonymous authentication but we can do easily that with implementing your own \n\n``` scala\nUserPassAuthenticator\n```\n\nand easily handle this for authorization with: \n\n``` scala\ndef authorize(check: \u21d2 Boolean): Directive0 \n```\n. I think `authenticate` and `authorize` is two independante thing, and you want to make `authorize` dependante of `authenticate`, and for you `anonymous` should be the default, but for someone else is not the default\n. So why not used \n\n``` scala\ndef authorize(check: \u21d2 Boolean): Directive0 \n```\n\nlike this \n\n``` scala\nauthenticate(authenticateFunction) { x =>\n  authorize(authorizeFunction(x)) {\n   ....\n  }\n}\n```\n\nand if you want to add something from the RequestContext you can do something like this\n\n``` scala\nauthenticate(authenticateFunction) & extractSomethingFromRequestContext { x, y =>\n  authorize(authorizeFunction(x)) {\n   ....\n  }\n}\n```\n. I don't understand why you think that `authenticate` and `authorize` is not already separated?\n. I think you missing that HttpAuthenticator and BasicHttpAuthenticator rely on the http rfc see: \nhttp://tools.ietf.org/html/rfc1945#section-11.1 or the wikipedia page: http://en.wikipedia.org/wiki/Basic_access_authentication\n. Have you seen the unapply function of HttpHeader ? \n. I think is for simplicity that the extractor use lowerCase instead of the normal case of the Headers and less error prone\n. I really don't understand why it's a problem that header name in lowerCase is a problem for you, because I think that having a case insensitive extractor can be very difficult to do, I think they choose to use lowerCase for simplicity and readability\n. What did you mean by proxy directive?\n. https://gist.github.com/bthuillier/8351056 something like this? \n. And if I understand the end point is: `\"http://oldapi.example.com\" + unmatchedPath`\n. I beginning to work on this features in https://github.com/bthuillier/spray/tree/proxy-directives\n. I didn't find where put test for `Directive`, I need some hint here =). I send you the CLA before noon\n. I update the code to not break the directives, and `require` and `hrequire` now use `filter` and `hfilter`\n. FYI I've just sign the CLA\n. I commited the efficiency improvement, it was my first contribution to a open source project.\n. I finished a first version of the directive, with documentation and test\n. Thanks for the answer, I'll cleanup and update the code asap\n. I think is good\n. Hello @sirthias and @hamiltont I'll be glad to work on a more complete version of the proxy directives.\n. Indeed it's easy to rebuild it, I actually rebuild it for testing some custom directives, I don't know if other people use it but with a good documentation I think it worth it. \n. yes\n. Perhaps I missing something but you already can do anonymous authentication but we can do easily that with implementing your own \n\n``` scala\nUserPassAuthenticator\n```\n\nand easily handle this for authorization with: \n\n``` scala\ndef authorize(check: \u21d2 Boolean): Directive0 \n```\n. I think `authenticate` and `authorize` is two independante thing, and you want to make `authorize` dependante of `authenticate`, and for you `anonymous` should be the default, but for someone else is not the default\n. So why not used \n\n``` scala\ndef authorize(check: \u21d2 Boolean): Directive0 \n```\n\nlike this \n\n``` scala\nauthenticate(authenticateFunction) { x =>\n  authorize(authorizeFunction(x)) {\n   ....\n  }\n}\n```\n\nand if you want to add something from the RequestContext you can do something like this\n\n``` scala\nauthenticate(authenticateFunction) & extractSomethingFromRequestContext { x, y =>\n  authorize(authorizeFunction(x)) {\n   ....\n  }\n}\n```\n. I don't understand why you think that `authenticate` and `authorize` is not already separated?\n. I think you missing that HttpAuthenticator and BasicHttpAuthenticator rely on the http rfc see: \nhttp://tools.ietf.org/html/rfc1945#section-11.1 or the wikipedia page: http://en.wikipedia.org/wiki/Basic_access_authentication\n. Have you seen the unapply function of HttpHeader ? \n. I think is for simplicity that the extractor use lowerCase instead of the normal case of the Headers and less error prone\n. I really don't understand why it's a problem that header name in lowerCase is a problem for you, because I think that having a case insensitive extractor can be very difficult to do, I think they choose to use lowerCase for simplicity and readability\n. ",
    "canadianracer": "We'll be needing UDP support as well.\n. We'll be needing UDP support as well.\n. ",
    "jacobus": "If it will help, then I can try to put together a very simple CRUD example (using Slick) and upload it to my git repository. That may provide a starting point for something like this. At least it could serve as a 'getting started quick' template. I already started with something like that some time back called S4 (Spray Slick Scala Stack :-) Corny, I know ;-)\n\nIf it sounds interesting, I'll dig it up and finish it over the weekend.\n. Hi there Mathias,\n\nI published a VERY basic project - https://github.com/jacobus/s4\n\nI will try to improve it a bit, but have other work pressures for the next couple of days. Currently, it just supports a POST to create a person, and a GET to list persons on the system.\n\nStill, the skeleton is there, which may save someone else a couple of days of figuring out what goes where. \n\nKind regards,\nJacobus\n. If it will help, then I can try to put together a very simple CRUD example (using Slick) and upload it to my git repository. That may provide a starting point for something like this. At least it could serve as a 'getting started quick' template. I already started with something like that some time back called S4 (Spray Slick Scala Stack :-) Corny, I know ;-)\n\nIf it sounds interesting, I'll dig it up and finish it over the weekend.\n. Hi there Mathias,\n\nI published a VERY basic project - https://github.com/jacobus/s4\n\nI will try to improve it a bit, but have other work pressures for the next couple of days. Currently, it just supports a POST to create a person, and a GET to list persons on the system.\n\nStill, the skeleton is there, which may save someone else a couple of days of figuring out what goes where. \n\nKind regards,\nJacobus\n. ",
    "guersam": "Thanks for hard work, but IMHO you'd better to maintain it as separated project in your own repo.\n. Michael,\n\nDue to the special character \"/\", the boundary delimiter must be double-quoted like `boundary=\"rdN9YFGkt4PYl/RD6AmmIagc\"`.\n\nSee **5.1.  Syntax of the Content-Type Header Field** of http://www.ietf.org/rfc/rfc2045.txt and **5.1.1.  Common Syntax** of http://www.ietf.org/rfc/rfc2046.txt.\n. Spray has a mailing list with kind people: https://groups.google.com/forum/#!forum/spray-user\nRegarding your issue, it's reported [here](https://groups.google.com/forum/#!searchin/spray-user/cookie$20parsing$20error/spray-user/726UVXk_It0/UIfIKVnQs7AJ) and fixed in a nightly version after M8.\n. Should it be applied to unmarshallers either? IMHO it would be acceptable because there are some clients sending UTF-8 requests without encoding info (especially in multipart/form-data) and UTF-8 is basically compatible with US-ASCII.\n. I see, thanks for explaining. If so, is it possible making [this behavior](https://github.com/spray/spray/blob/master/spray-httpx/src/main/scala/spray/httpx/unmarshalling/MultipartUnmarshallers.scala#L47) overridable at least? Private `convertMimeMessage` forces me to copy whole code in `MultipartUnmarshallers` for custom `UTF8MultipartUnmarshallers`.\n. Thanks for suggestion - I'm doing this in usual cases.\n\nUnfortunately, regarding multipart data, content-type header comes from parsed mime messages by `convertMimeMessage` which is internally used by `MultipartContentUnmarshaller`.\n\nSo that It not only seems tricky to rewrite them by myself but also brings unnecessary overhead by parsing `MimeMessage` twice.\n. Thanks a lot!\n. IMHO many users including me won't want to care of the trailing slash at end of an URI, and `pathEndOrSingleSlash` looks a bit too long.\n\nHow about `pathEndWithoutSlash` for the strict behavior and `pathEnd` as decribed in #628?\n. Understood, thanks for the kind explanation with good resource!\n. Regarding the title of the second commit, I'm not sure if the prefix should be **= all:** because it doesn't affect some modules which don't depend on Akka or don't use `sender`, but I found it too verbose to make it **= can, client, routing, testkit, util, docs, examples:**.\n. superseded by #768  \n. No problem, thanks for mentioning :)\n. Current workaround I'm using:\n\n``` scala\nbodyPart.filename.map(s => new String(s.getBytes(\"ISO-8859-1\"), \"UTF-8\"))\n```\n. Thanks Johannes, nice to hear that!\n. Have you checked #876?\n. Actually I had an `optionalAuthenticate` which was implemented as the same as yours is :) Sadly (?) it's gone since I added `AnonymouseUser` to the domain itself.\n. Thanks for hard work, but IMHO you'd better to maintain it as separated project in your own repo.\n. Michael,\n\nDue to the special character \"/\", the boundary delimiter must be double-quoted like `boundary=\"rdN9YFGkt4PYl/RD6AmmIagc\"`.\n\nSee **5.1.  Syntax of the Content-Type Header Field** of http://www.ietf.org/rfc/rfc2045.txt and **5.1.1.  Common Syntax** of http://www.ietf.org/rfc/rfc2046.txt.\n. Spray has a mailing list with kind people: https://groups.google.com/forum/#!forum/spray-user\nRegarding your issue, it's reported [here](https://groups.google.com/forum/#!searchin/spray-user/cookie$20parsing$20error/spray-user/726UVXk_It0/UIfIKVnQs7AJ) and fixed in a nightly version after M8.\n. Should it be applied to unmarshallers either? IMHO it would be acceptable because there are some clients sending UTF-8 requests without encoding info (especially in multipart/form-data) and UTF-8 is basically compatible with US-ASCII.\n. I see, thanks for explaining. If so, is it possible making [this behavior](https://github.com/spray/spray/blob/master/spray-httpx/src/main/scala/spray/httpx/unmarshalling/MultipartUnmarshallers.scala#L47) overridable at least? Private `convertMimeMessage` forces me to copy whole code in `MultipartUnmarshallers` for custom `UTF8MultipartUnmarshallers`.\n. Thanks for suggestion - I'm doing this in usual cases.\n\nUnfortunately, regarding multipart data, content-type header comes from parsed mime messages by `convertMimeMessage` which is internally used by `MultipartContentUnmarshaller`.\n\nSo that It not only seems tricky to rewrite them by myself but also brings unnecessary overhead by parsing `MimeMessage` twice.\n. Thanks a lot!\n. IMHO many users including me won't want to care of the trailing slash at end of an URI, and `pathEndOrSingleSlash` looks a bit too long.\n\nHow about `pathEndWithoutSlash` for the strict behavior and `pathEnd` as decribed in #628?\n. Understood, thanks for the kind explanation with good resource!\n. Regarding the title of the second commit, I'm not sure if the prefix should be **= all:** because it doesn't affect some modules which don't depend on Akka or don't use `sender`, but I found it too verbose to make it **= can, client, routing, testkit, util, docs, examples:**.\n. superseded by #768  \n. No problem, thanks for mentioning :)\n. Current workaround I'm using:\n\n``` scala\nbodyPart.filename.map(s => new String(s.getBytes(\"ISO-8859-1\"), \"UTF-8\"))\n```\n. Thanks Johannes, nice to hear that!\n. Have you checked #876?\n. Actually I had an `optionalAuthenticate` which was implemented as the same as yours is :) Sadly (?) it's gone since I added `AnonymouseUser` to the domain itself.\n. ",
    "fommil": "For convenience \u2013\u00a0would be a lot better if openSslPipeline spawned a new HTTP Client if one doesn't already exist.\n\nMight be nicer to implement it as a trait.\n\n``` scala\nobject SendReceive {\n\n  /** Registers an `HttpClient` at `/user/https-client`,\n    * creating the singleton IOBridge if one is not already\n    * available.\n    */\n  def registerHttpsClient(system: ActorSystem) = {\n    val ioBridge = IOExtension(system).ioBridge\n    system.actorOf(\n      props = Props(\n        new HttpClient(ioBridge, ConfigFactory.parseString(\"spray.can.client.ssl-encryption = on\"))\n      ),\n      name = \"https-client\"\n    )\n  }\n\n  /** Registers an `HttpConduit` Actor, assuming `/user/https-client` exists in the system\n    * and returns the associated pipeline.\n    *\n    * @return a send/receive pipeline to the server.\n    */\n  def openSslPipeline(system: ActorSystem, server: String) = {\n    val httpsClient = system.actorFor(\"/user/https-client\")\n    val conduit = system.actorOf(\n      props = Props(\n        new HttpConduit(httpsClient, server, port = 443, sslEnabled = true)\n      ),\n      name = \"https-conduit-\" + server\n    )\n    HttpConduit.sendReceive(conduit)\n  }\n\n}\n```\n. I ended up using this single approach, which is perhaps a little wasteful because it creates a new HttpClient per HttpConduit, because the last 2-step approach doesn't work for lazy initialisation.\n\nIt turns out to be exceptionally difficult to check for existence of an ActorRef's existence in Akka. If we had a solid ActorRef to match against we could probably check the results of an actorFor call.\n\n``` scala\n\n  /** Creates a new `HttpConduit` Actor, and an associated `HttpClient`,\n    * and returns the associated pipeline.\n    *\n    * @return a send/receive pipeline to the server.\n    */\n  def openSslPipeline(system: ActorSystem, server: String) = {\n    val ioBridge = IOExtension(system).ioBridge\n    val httpsClient = system.actorOf(\n      props = Props(\n        new HttpClient(\n          ioBridge,\n          ConfigFactory.parseString(\"spray.can.client.ssl-encryption = on\")\n        )\n      ),\n      name = \"https-client-\" + server\n    )\n    val conduit = system.actorOf(\n      props = Props(\n        new HttpConduit(httpsClient, server, port = 443, sslEnabled = true)\n      ),\n      name = \"https-conduit-\" + server\n    )\n    HttpConduit.sendReceive(conduit)\n  }\n```\n. OK, one more go at this! The latest preference for testing existence of an Actor is to catch the following exception.\n\n``` scala\n\n  /** Creates a new `HttpConduit` Actor (if needed), and an `HttpClient` (if needed),\n    * and returns the associated pipeline.\n    *\n    * @return a send/receive pipeline to the server.\n    */\n  def openSslPipeline(system: ActorSystem, server: String) = {\n    val ioBridge = IOExtension(system).ioBridge\n    val clientName = \"https-client\"\n    val httpsClient = try\n      system.actorOf(\n        props = Props(\n          new HttpClient(\n            ioBridge,\n            ConfigFactory.parseString(\"spray.can.client.ssl-encryption = on\")\n          )\n        ),\n        name = clientName\n      )\n    catch {\n      case _: InvalidActorNameException => system.actorFor(clientName)\n    }\n\n    val conduitName = \"https-conduit-\" + server\n    val conduit = try system.actorOf(\n      props = Props(\n        new HttpConduit(httpsClient, server, port = 443, sslEnabled = true)\n      ),\n      name = conduitName\n    )\n    catch {\n      case _: InvalidActorNameException => system.actorFor(conduitName)\n    }\n\n    HttpConduit.sendReceive(conduit)\n  }\n```\n. Indeed, I have experienced problems with the last code \u2013\u00a0I'm back to using the code that begins \"I ended up using this single approach,\"\n. Cool :-) `DefaultHttpClient` looks like it only answers half of this RFE though \u2013\u00a0in my original issue report the `sendReceive` is the thing I really want to get hold of.\n. `HttpConduit`s can be safely shared, right? As in, sharing them won't bottleneck how many requests we can make with the http client?\n\nFrom a user perspective, the conduit thing is just boilerplate \u2013\u00a0if you could add a `sendReceive` method onto the `HTTPClient` that would be perfect, or allow the `HttpConduit` to take in o a`HttpClient`\n. how about this?\n\n``` scala\nabstract class SendReceive(server: String, port: Int = 443, sslEnabled: Boolean = true) extends ExtensionId[ExtensionActorRef] {\n\n  def createExtension(system: ExtendedActorSystem) = {\n    val client = DefaultHttpClient(system)\n    val conduitName = \"http-conduit-\" + port + \"-\" +\n      (if (sslEnabled) \"ssl\" else \"plain\") +\n      \"-\" + server\n    val conduit = system.actorOf(\n      props = Props(\n        new HttpConduit(client, server, port = port, sslEnabled = sslEnabled)\n      ),\n      name = conduitName\n    )\n    new ExtensionActorRef(conduit)\n  }\n\n  def sendReceive(system: ActorSystem) = HttpConduit.sendReceive(get(system))\n\n}\n```\n\nwhich means connections to individual servers are defined thusly\n\n``` scala\nobject Foursquare extends SendReceive(\"api.foursquare.com\")\nobject ITunes extends SendReceive(\"buy.itunes.apple.com\")\nobject ITunesSandbox extends SendReceive(\"sandbox.itunes.apple.com\")\nobject Facebook extends SendReceive(\"graph.facebook.com\")\nobject Twitter extends SendReceive(\"api.twitter.com\")\n```\n\nand obtained thusly\n\n``` scala\n  lazy val foursquarePipeline = Foursquare.sendReceive(actorRefFactory)\n  lazy val itunesPipeline = config.getBoolean(\"itunes.live\") match {\n    case true => ITunes.sendReceive(actorRefFactory)\n    case false => ITunesSandbox.sendReceive(actorRefFactory)\n  }\n  lazy val facebookPipeline = Facebook.sendReceive(actorRefFactory)\n  lazy val twitterPipeline = Twitter.sendReceive(actorRefFactory)\n```\n. @viktorklang what would you recommend?\n\nHow about\n\n``` scala\nval conduitName = \"spray/http-conduit/\" + server + \"/\" + port + \"/\" + (if (sslEnabled) \"https\" else \"http\")\n```\n. also for client connections \u2013\u00a0my concern is to avoid OOMing under high loads.\n. I would like new connection requests to be queued.\n\nBut I think if incoming connections could be limited this more or less solves the client problem for me because that's all part of the same memory footprint per request.\n. cool!\n. Shapeless are already on 2.2.1\n\nThis should be a trivial version bump.\n. FYI 2.2.1 broke binary compatibility, but 2.2.0 was ok and I understand 2.2.2 will return to binary compatibility. Therefore there may be no need to do this. \n\nSee https://github.com/milessabin/shapeless/pull/407 for more info.\n. 2.2.2 should be binary compatible. The problem seemed to be only with 2.2.1, but we can't be 100% sure that 2.2.2 is binary compatible with 2.1.x because they only recently added the binary API plugin to check.\n. if it's possible, I'd recommend releasing an update to 2.2.2 to be safe.\n. ouch. Yeah, it's working for me.\n. fyi, this will silence it (along with a bunch of other stuff that I'd rather not silence). I might have the possibility of override these methods\n\n```\n  <!-- WORKAROUND: https://github.com/spray/spray/issues/1060 -->\n  <logger name=\"spray.can.client.HttpClientConnection\" level=\"ERROR\" />\n```\n. ok, I was able to workaround by providing an `HttpClientConnection` with these methods overriden and then in the two locations that log out the warning, change them to just log the class of the message.\n\nAn alternative solution might be for akka.io `ByteString` to not implement `toString`! I'll create a ticket for that.\n. For convenience \u2013\u00a0would be a lot better if openSslPipeline spawned a new HTTP Client if one doesn't already exist.\n\nMight be nicer to implement it as a trait.\n\n``` scala\nobject SendReceive {\n\n  /** Registers an `HttpClient` at `/user/https-client`,\n    * creating the singleton IOBridge if one is not already\n    * available.\n    */\n  def registerHttpsClient(system: ActorSystem) = {\n    val ioBridge = IOExtension(system).ioBridge\n    system.actorOf(\n      props = Props(\n        new HttpClient(ioBridge, ConfigFactory.parseString(\"spray.can.client.ssl-encryption = on\"))\n      ),\n      name = \"https-client\"\n    )\n  }\n\n  /** Registers an `HttpConduit` Actor, assuming `/user/https-client` exists in the system\n    * and returns the associated pipeline.\n    *\n    * @return a send/receive pipeline to the server.\n    */\n  def openSslPipeline(system: ActorSystem, server: String) = {\n    val httpsClient = system.actorFor(\"/user/https-client\")\n    val conduit = system.actorOf(\n      props = Props(\n        new HttpConduit(httpsClient, server, port = 443, sslEnabled = true)\n      ),\n      name = \"https-conduit-\" + server\n    )\n    HttpConduit.sendReceive(conduit)\n  }\n\n}\n```\n. I ended up using this single approach, which is perhaps a little wasteful because it creates a new HttpClient per HttpConduit, because the last 2-step approach doesn't work for lazy initialisation.\n\nIt turns out to be exceptionally difficult to check for existence of an ActorRef's existence in Akka. If we had a solid ActorRef to match against we could probably check the results of an actorFor call.\n\n``` scala\n\n  /** Creates a new `HttpConduit` Actor, and an associated `HttpClient`,\n    * and returns the associated pipeline.\n    *\n    * @return a send/receive pipeline to the server.\n    */\n  def openSslPipeline(system: ActorSystem, server: String) = {\n    val ioBridge = IOExtension(system).ioBridge\n    val httpsClient = system.actorOf(\n      props = Props(\n        new HttpClient(\n          ioBridge,\n          ConfigFactory.parseString(\"spray.can.client.ssl-encryption = on\")\n        )\n      ),\n      name = \"https-client-\" + server\n    )\n    val conduit = system.actorOf(\n      props = Props(\n        new HttpConduit(httpsClient, server, port = 443, sslEnabled = true)\n      ),\n      name = \"https-conduit-\" + server\n    )\n    HttpConduit.sendReceive(conduit)\n  }\n```\n. OK, one more go at this! The latest preference for testing existence of an Actor is to catch the following exception.\n\n``` scala\n\n  /** Creates a new `HttpConduit` Actor (if needed), and an `HttpClient` (if needed),\n    * and returns the associated pipeline.\n    *\n    * @return a send/receive pipeline to the server.\n    */\n  def openSslPipeline(system: ActorSystem, server: String) = {\n    val ioBridge = IOExtension(system).ioBridge\n    val clientName = \"https-client\"\n    val httpsClient = try\n      system.actorOf(\n        props = Props(\n          new HttpClient(\n            ioBridge,\n            ConfigFactory.parseString(\"spray.can.client.ssl-encryption = on\")\n          )\n        ),\n        name = clientName\n      )\n    catch {\n      case _: InvalidActorNameException => system.actorFor(clientName)\n    }\n\n    val conduitName = \"https-conduit-\" + server\n    val conduit = try system.actorOf(\n      props = Props(\n        new HttpConduit(httpsClient, server, port = 443, sslEnabled = true)\n      ),\n      name = conduitName\n    )\n    catch {\n      case _: InvalidActorNameException => system.actorFor(conduitName)\n    }\n\n    HttpConduit.sendReceive(conduit)\n  }\n```\n. Indeed, I have experienced problems with the last code \u2013\u00a0I'm back to using the code that begins \"I ended up using this single approach,\"\n. Cool :-) `DefaultHttpClient` looks like it only answers half of this RFE though \u2013\u00a0in my original issue report the `sendReceive` is the thing I really want to get hold of.\n. `HttpConduit`s can be safely shared, right? As in, sharing them won't bottleneck how many requests we can make with the http client?\n\nFrom a user perspective, the conduit thing is just boilerplate \u2013\u00a0if you could add a `sendReceive` method onto the `HTTPClient` that would be perfect, or allow the `HttpConduit` to take in o a`HttpClient`\n. how about this?\n\n``` scala\nabstract class SendReceive(server: String, port: Int = 443, sslEnabled: Boolean = true) extends ExtensionId[ExtensionActorRef] {\n\n  def createExtension(system: ExtendedActorSystem) = {\n    val client = DefaultHttpClient(system)\n    val conduitName = \"http-conduit-\" + port + \"-\" +\n      (if (sslEnabled) \"ssl\" else \"plain\") +\n      \"-\" + server\n    val conduit = system.actorOf(\n      props = Props(\n        new HttpConduit(client, server, port = port, sslEnabled = sslEnabled)\n      ),\n      name = conduitName\n    )\n    new ExtensionActorRef(conduit)\n  }\n\n  def sendReceive(system: ActorSystem) = HttpConduit.sendReceive(get(system))\n\n}\n```\n\nwhich means connections to individual servers are defined thusly\n\n``` scala\nobject Foursquare extends SendReceive(\"api.foursquare.com\")\nobject ITunes extends SendReceive(\"buy.itunes.apple.com\")\nobject ITunesSandbox extends SendReceive(\"sandbox.itunes.apple.com\")\nobject Facebook extends SendReceive(\"graph.facebook.com\")\nobject Twitter extends SendReceive(\"api.twitter.com\")\n```\n\nand obtained thusly\n\n``` scala\n  lazy val foursquarePipeline = Foursquare.sendReceive(actorRefFactory)\n  lazy val itunesPipeline = config.getBoolean(\"itunes.live\") match {\n    case true => ITunes.sendReceive(actorRefFactory)\n    case false => ITunesSandbox.sendReceive(actorRefFactory)\n  }\n  lazy val facebookPipeline = Facebook.sendReceive(actorRefFactory)\n  lazy val twitterPipeline = Twitter.sendReceive(actorRefFactory)\n```\n. @viktorklang what would you recommend?\n\nHow about\n\n``` scala\nval conduitName = \"spray/http-conduit/\" + server + \"/\" + port + \"/\" + (if (sslEnabled) \"https\" else \"http\")\n```\n. also for client connections \u2013\u00a0my concern is to avoid OOMing under high loads.\n. I would like new connection requests to be queued.\n\nBut I think if incoming connections could be limited this more or less solves the client problem for me because that's all part of the same memory footprint per request.\n. cool!\n. Shapeless are already on 2.2.1\n\nThis should be a trivial version bump.\n. FYI 2.2.1 broke binary compatibility, but 2.2.0 was ok and I understand 2.2.2 will return to binary compatibility. Therefore there may be no need to do this. \n\nSee https://github.com/milessabin/shapeless/pull/407 for more info.\n. 2.2.2 should be binary compatible. The problem seemed to be only with 2.2.1, but we can't be 100% sure that 2.2.2 is binary compatible with 2.1.x because they only recently added the binary API plugin to check.\n. if it's possible, I'd recommend releasing an update to 2.2.2 to be safe.\n. ouch. Yeah, it's working for me.\n. fyi, this will silence it (along with a bunch of other stuff that I'd rather not silence). I might have the possibility of override these methods\n\n```\n  <!-- WORKAROUND: https://github.com/spray/spray/issues/1060 -->\n  <logger name=\"spray.can.client.HttpClientConnection\" level=\"ERROR\" />\n```\n. ok, I was able to workaround by providing an `HttpClientConnection` with these methods overriden and then in the two locations that log out the warning, change them to just log the class of the message.\n\nAn alternative solution might be for akka.io `ByteString` to not implement `toString`! I'll create a ticket for that.\n. ",
    "rkuhn": "Get-or-create is bad, since then you don\u2019t really know what you get if you didn\u2019t create it (the name might be used by someone else). The proper way to do this is using an Akka extension (like adding these to the IOExtension), which will properly scope things to be once-per-system.\n. To expand this a bit: instead of creating one \u201cextension\u201d per actor (which is not what the extension mechanism is meant to do) create one extension which will create one SendReceivePipelineSupervisor and make it so that requests to create SendReceive pipelines will be turned into messages sent to this supervisor. This will require you to make the interface asynchronous, either by being message-based (i.e. expose an ActorRef) or by returning a Future[_].\n. Although we now have tests for OSGi in whose creation I somewhat participated, I am still not on friendly terms with this technology ;-) The story of this PR reads reasonable to me and if the result works well enough then I'll consider that a success. If OTOH we need to change something in Akka then we definitely need help from the experts.\n. Get-or-create is bad, since then you don\u2019t really know what you get if you didn\u2019t create it (the name might be used by someone else). The proper way to do this is using an Akka extension (like adding these to the IOExtension), which will properly scope things to be once-per-system.\n. To expand this a bit: instead of creating one \u201cextension\u201d per actor (which is not what the extension mechanism is meant to do) create one extension which will create one SendReceivePipelineSupervisor and make it so that requests to create SendReceive pipelines will be turned into messages sent to this supervisor. This will require you to make the interface asynchronous, either by being message-based (i.e. expose an ActorRef) or by returning a Future[_].\n. Although we now have tests for OSGi in whose creation I somewhat participated, I am still not on friendly terms with this technology ;-) The story of this PR reads reasonable to me and if the result works well enough then I'll consider that a success. If OTOH we need to change something in Akka then we definitely need help from the experts.\n. ",
    "viktorklang": "Roland is right, this is exactly what Akka Extensions are for.\n\n\u221a \u2013 legendary hakker\n. Avoid top-level actors.\n. I'd recommend creating the actor _not_ at top level.\n. Roland is right, this is exactly what Akka Extensions are for.\n\n\u221a \u2013 legendary hakker\n. Avoid top-level actors.\n. I'd recommend creating the actor _not_ at top level.\n. ",
    "cbrisket": "+1 for the feature, and an _addidtional_ +1 for the name :)\n. Love this potential feature - our use-case is a header-inspection proxy and (if I understand correctly) we would be able to grab header details (from ChunkedResponseStart) prior to receiving the entity-payload.\n\nAny further thoughts about when this might be planned?\n. +1 for the feature, and an _addidtional_ +1 for the name :)\n. Love this potential feature - our use-case is a header-inspection proxy and (if I understand correctly) we would be able to grab header details (from ChunkedResponseStart) prior to receiving the entity-payload.\n\nAny further thoughts about when this might be planned?\n. ",
    "craigwblake": "Ah, thanks a bunch for the link.\n. Ah, thanks a bunch for the link.\n. ",
    "SonicWang": "I was wondering if you have an estimate on when this feature will be in? As a matter of fact, I just ran into this issue earlier today. Spray is new to me, but I had a lot experience with Netty and Node. Coming from such frameworks I took chunkless request streaming for granted. So it really surprised me to see this not working with latest Spray.\n. I was wondering if you have an estimate on when this feature will be in? As a matter of fact, I just ran into this issue earlier today. Spray is new to me, but I had a lot experience with Netty and Node. Coming from such frameworks I took chunkless request streaming for granted. So it really surprised me to see this not working with latest Spray.\n. ",
    "max8github": "Considering taking a crack at it, but i would need a bit more guidance than what stated here so far (also new to spray - currently trying to write some proof of concept code to motivate adoption).\nCould someone outline some of the ways this could be implemented within spray? Not sure will be able to actually submit a patch, but will try.\n. Considering taking a crack at it, but i would need a bit more guidance than what stated here so far (also new to spray - currently trying to write some proof of concept code to motivate adoption).\nCould someone outline some of the ways this could be implemented within spray? Not sure will be able to actually submit a patch, but will try.\n. ",
    "echojc": "Hi, is this feature still planned for M8?\n. Hi, is this feature still planned for M8?\n. ",
    "adamw": "We're also seeing this sometimes, though I'm not yet sure when. One possibility is that this happened when the network was temporarily down. (btw - haven't seen an NPE in Scala code for a long time ;) ) \n. Thanks for the review - except for a couple of echoCompletes (where I'm not sure it's possible to use it with two parameters), I fixed the code.\n. We're also seeing this sometimes, though I'm not yet sure when. One possibility is that this happened when the network was temporarily down. (btw - haven't seen an NPE in Scala code for a long time ;) ) \n. Thanks for the review - except for a couple of echoCompletes (where I'm not sure it's possible to use it with two parameters), I fixed the code.\n. ",
    "benoitheinrich": "+1\n. +1\n. ",
    "byrnedo": "+1\n. +1\n. ",
    "joeledstrom": "[Possible fix](https://github.com/spray/spray/pull/232)\n. [Possible fix](https://github.com/spray/spray/pull/232)\n. ",
    "seanparsons": "I managed to solve my issue by making my spec a class rather than an object, not entirely sure why that made a massive difference however...\n. What is involved in actually doing this?\n. I managed to solve my issue by making my spec a class rather than an object, not entirely sure why that made a massive difference however...\n. What is involved in actually doing this?\n. ",
    "nkvoll": "After poking around a little bit in the interpreter and the source, I see that .request has a `def rawPath` function that uses the `getRawPath` function from `java.net.URI`, which I guess is what I actually need to use.\n. Commenting to be notified about changes to this ticket and to add my +1 :)\n. Do you have any timeframe for when this will be implemented? I'd love to port my service to a newer nightly, but this is a blocker at the moment.\n. Were you thinking along the lines of https://github.com/spray/spray/pull/272 ?\n\nI got some errors when running the tests (`sbt spray-http/test`), but I _think_ (without looking too closely at the output) that those tests were broken even without my commit.\n. Actually, the pchar needs to be a bit more restrictive\n. I updated https://github.com/spray/spray/pull/272 with some fixes that prevent the path to match the query string etc.\n\nI'm wondering if this is the \"right\" way of doing it, as\n\n```\nmatches(ASCII_PRINTABLE_CHARS & ~(SLASH | QUESTIONMARK | NUMERIC_SIGN))\n```\n\nand similar doesn't work, as it will always match whatever is marked in `ASCII_PRINTABLE_CHARS` (as it isn't itself composed of groups). So I fell back to just generating the character range and filtering out the characters there. With the `.toSeq : _*` it gets pretty ugly at the moment, but if we change the `mark` method to accept an `IndexedSeq` instead of just a `NumericRange` we could make it prettier.\n\nNo new tests yet, as I want to hear your opinion on whether this is the right way to do it.\n\nI'm relatively new to Scala, so any and all feedback is appreciated :)\n. #272 adds support for non-strict parsing, but `;` vs `&` as a query parameter separator is not addressed by the pull request.\n\nWhat's important (at least for me, and I would reckon many others), is that support for handling `;` as a param separator must be configurable, as its already handled differently by different web servers.\n\nPreferably, it should be configurable on a per-server basis, so one could have a multiple spray servers within the same process that parses these differently. This being the reason I want you to still consider making the pipelines either configurable by client code (or at least configurable on a per-bind/per-handler basis). Even if it doesn't happen overnight, I think it might be a good future direction wich would make it easier to build upon spray for servers that need to talk to particularly weird clients.\n. I realize that spray (spray-can) is meant to be a http server primarily, and fully customizable pipelines are probably only required for writing custom protocols, which in all likelihood shouldn't be done on top of spray (but the up-coming akka.io)?\n\nSo to avoid sounding like a broken record about the pipeline-part: after this pull-request I do not, as of today, know of any requirements in my current projects that isn't handled without customizing the request/response pipeline in any way. The reason I'm advocating it is mostly because from my initial peek through the spray code base in regards to the server handling, a lot of singletons and non-overridable classes seems to be used, which I'm wondering if is the right direction with respect to extensibility.\n\nI hope this comes off as being constructive and thanks for making spray, it's been awesome to work with, and I hope I'll be able to contribute more as time goes on :)\n. Sure I can squash it. I'll do it the next time I push any changes.\n\nMaking  `ASCII_PRINTABLE_CHARS / VCHAR` a compound mask is a pretty large task. All printable chars should have their own mask (with some exceptions such as digits/lowercase letters/uppercase letters). This would result in around 43 groups, which is not representable using ints.\n\nIn this case, I think it's probably preferable to avoid having a `VCHAR` mask (only the `PARSE_FRAGMENT_CHARS` mask is using it at the moment), and just generate the larger groups like I currently do. What do you think?\n. In order to have a somewhat consistent experience with the masks and compound masks, there should really only be one way to represent the characters (otherwise, exclusion is likely to fail). A more all-encompassing solution would split all the character / or groups up into their own mask and combine from there, but I suppose that isn't needed or required here.\n\nI took the liberty of removing some duplication inside the compound masks, but left the following untouched:\n\n```\nmark(LOWER_ALPHA, 'a' to 'z')\nmark(UPPER_ALPHA, 'A' to 'Z')\nmark(DIGIT19, '1' to '9')\nmark(DIGIT04, '0' to '4')\nmark(DIGIT05, '0' to '5')\nmark(LOWER_HEX_LETTER, 'a' to 'f')\nmark(UPPER_HEX_LETTER, 'A' to 'F')\n```\n\nWhat do you think now?\n\nIn order to write a few unit tests for this, it would be great if all the current tests passed, which they don't seem to do here (spray/master fails, so it's probably not limited to my fork). Are they expected to pass? Do they pass on your end? If so, I might be running them wrong -- currently I'm running them via \"sbt spray-http/test\"\n. Regarding the unit tests, it seems like it was an error on my behalf, and it looks like it's working, so I moved around and added some asserts to test this change.\n\nAdditionally, while I was poking around, I noticed that `Query.apply(...)` was broken if called with a non-empty `Map`/`List`, since `Query.Empty` does not have `.key` or `.value`. Since it was a rather small issue and fix, I thought I should just tack it on here.\n. > This looks quite nice already, thanks!!\n> Apart from the comments I think the only thing missing is a strict: Boolean parameter to the UriParser constructor and the respective Uri.apply overloads, so people can decide whether they want relaxed or strict URI parsing.\n\nWouldn't this also entail changing all the `parseAndResolve`, `parseAbsolute`, `parseHttpRequestTarget`, `Uri.from` etc etc methods and documentation?\n. What makes me uneasy about adding the `strict: Boolean` parameter is that I'm not confident that I'll be able to locate and update all the affected code, but I've given it a shot. \n. If you see nothing wrong in merging, please go ahead :)\n. Having multiple equal signs is ambiguous and will not be allowed by this pull request.\n. I see. I'm not sure the better solution would be to add a configuration variable to allow multiple equal signs (i.e the first equal sign separates the key from the value) or adding some kind of hook where the requested URI could be pre-processed before it's parsed.\n\nThe former would be pretty clean in handling your case and the latter would enable users to manipulate and re-write quite a lot of other, similar client-quirks. @sirthias probably has some preference and thoughts on the matter.\n\nMaybe open a ticket or take it up as a discussion on the mailing list? In my opinion, this should be a case that should be possible to handle without too much difficulty in a HTTP toolkit. Dealing with quirky clients is one of the perks of working with HTTP...\n. I mentioned maybe allowing customization of the request processing pipeline (which would include the parsing step) to @sirthias earlier, but he said it would require exposing more of the internal API than they would like to maintain, at least for the time being.\n\nMaybe if you add your voice to that or a similar cause, it might get reconsidered. What got me hooked on spray (around 1.1M7) was it's great performance and that I could easily work through the peculiarities of my clients. (I also have no control over my clients, and they sometimes send me all kinds of weird stuff -- it doesn't matter to them that they're bending or breaking a spec or two, what matters to them is that I'm able to write a server that can accept and work with their data). Some of this seems lost in the current master branch (and some of it I'm sure I've just not found yet), and I'd very much like it back. :)\n. That sounds great :) +1 from me\n\nOn May 3, 2013, at 12:45 , Mathias wrote:\n\n> Ok, we'll include a new config setting spray.can.parsing.uri-parsing-mode with the following defined settings:\n> \n> strict for what we have now\n> relaxed for what this PR provides\n> relaxed-with-raw-query for additionally disabling parsing of the internal query structure\n> Hopefully this is flexible enough for most applications.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. Do you think it would be possible to receive it as an RawHeader or get the option to drop it in these cases?\n. You make a very valid point.\n\nAs I might have mentioned before, I'm writing a proxy using spray, and as such, I myself might not particularly care that a specific header is malformed, since I will not be doing anything with it myself, other than to forward it to an upstream server, which gets the final say on whether the http request is acceptable or not.\n\nIn this particular case, the Content-Type header isn't actually used, but is likely inserted by one of the many http client libraries out there that speaks its very own dialect of http, for better, or usually for worse. To make matters worse with the current handling of that and other illegal requests (such as invalid/custom http methods) I cannot even determine where the request came from, or anything about it other than \"there was one request that got that response\".\n\nIf I had received that request in my application I could have inspected the remaining valid headers and done something sensible, and at the very least figured out who the user is and contacted them about the issue.\n\nSo even if parts of the request is invalid and it has to be rejected, getting the valid parts can be of great assistance in finding the cause.\n\nIn a sense, the current handling of the User-Agent header makes sense to me: It's attempted parsed, but hey, people mess up their custom user agents all the time, and it does not follow the spec, so what I get, is not an UserAgent-header, but a RawHeader containing the \"User-Agent\". That works well because I might not care about it (or I might be religious about it, but it's still up to me).\n\nWith logging of illegal headers I could see things like this in my logs:\n\n```\nIllegal request header: Illegal 'User-Agent' header: Invalid input '/', expected CTL, OptWS, Comment, ProductVersionComment or EOI (line 1, pos 8):\nfoo/bar/baz\n```\n\nand the differences in the HttpRequest.headers is obvious. :)\n. Is this then supposed to be fixed in the latest nightly (according to the commit message)? I'm still getting the following on the current latest nightly:\n\n```\n[2013-10-31 17:18:10,921][ERROR][akka.actor.OneForOneStrategy] Unexpected slot state: Idle\njava.lang.IllegalStateException: Unexpected slot state: Idle\n    at spray.can.client.HttpHostConnector$$anonfun$receive$1.applyOrElse(HttpHostConnector.scala:81) ~[spray-can-1.2-20131031.jar:na]\n    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498) [akka-actor_2.10-2.2.3.jar:2.2.3]\n    at akka.actor.ActorCell.invoke(ActorCell.scala:456) [akka-actor_2.10-2.2.3.jar:2.2.3]\n    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237) [akka-actor_2.10-2.2.3.jar:2.2.3]\n    at akka.dispatch.Mailbox.run(Mailbox.scala:219) [akka-actor_2.10-2.2.3.jar:2.2.3]\n    at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386) [akka-actor_2.10-2.2.3.    jar:2.2.3]\n    at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.10.3.jar:na]\n    at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.10.3.jar:na]\n    at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.10.3.jar:na]\n    at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.10.3.jar:na]\n[2013-10-31 17:18:10,922][ERROR][akka.actor.OneForOneStrategy] key not found: Actor[akka://default/user/IO-HTTP/host-connector-    0/1#1484964495]\njava.util.NoSuchElementException: key not found: Actor[akka://default/user/IO-HTTP/host-connector-0/1#1484964495]\n    at scala.collection.MapLike$class.default(MapLike.scala:228) ~[scala-library-2.10.3.jar:na]\n    at scala.collection.AbstractMap.default(Map.scala:58) ~[scala-library-2.10.3.jar:na]\n    at scala.collection.MapLike$class.apply(MapLike.scala:141) ~[scala-library-2.10.3.jar:na]\n    at scala.collection.AbstractMap.apply(Map.scala:58) ~[scala-library-2.10.3.jar:na]\n    at spray.can.client.HttpHostConnector$$anonfun$receive$1.applyOrElse(HttpHostConnector.scala:76) ~[spray-can-1.2-20131031.jar:na]\n    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498) [akka-actor_2.10-2.2.3.jar:2.2.3]\n    at akka.actor.ActorCell.invoke(ActorCell.scala:456) [akka-actor_2.10-2.2.3.jar:2.2.3]\n    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237) [akka-actor_2.10-2.2.3.jar:2.2.3]\n    at akka.dispatch.Mailbox.run(Mailbox.scala:219) [akka-actor_2.10-2.2.3.jar:2.2.3]\n    at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386) [akka-actor_2.10-2.2.3.    jar:2.2.3]\n    at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.10.3.jar:na]\n    at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.10.3.jar:na]\n    at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.10.3.jar:na]\n    at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.10.3.jar:na]\n```\n. Of course. I should have noticed that :)\nOn Oct 31, 2013 6:23 PM, \"Johannes Rudolph\" notifications@github.com\nwrote:\n\n> No, the fix is not even merged.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/issues/652#issuecomment-27506692\n> .\n. I updated this issue slightly: initially I thought it was a bug that spray rendered the plus sign in path components, but it should definitely be allowed unencoded. Now the issue (hopefully) clearly states that what I'm interested in is a way of getting spray to render it as percent-encoded even if it should be allowable unencoded.\n. You're absolutely correct, and the mentioned fix solves this case. Thanks Mathias!\n. After poking around a little bit in the interpreter and the source, I see that .request has a `def rawPath` function that uses the `getRawPath` function from `java.net.URI`, which I guess is what I actually need to use.\n. Commenting to be notified about changes to this ticket and to add my +1 :)\n. Do you have any timeframe for when this will be implemented? I'd love to port my service to a newer nightly, but this is a blocker at the moment.\n. Were you thinking along the lines of https://github.com/spray/spray/pull/272 ?\n\nI got some errors when running the tests (`sbt spray-http/test`), but I _think_ (without looking too closely at the output) that those tests were broken even without my commit.\n. Actually, the pchar needs to be a bit more restrictive\n. I updated https://github.com/spray/spray/pull/272 with some fixes that prevent the path to match the query string etc.\n\nI'm wondering if this is the \"right\" way of doing it, as\n\n```\nmatches(ASCII_PRINTABLE_CHARS & ~(SLASH | QUESTIONMARK | NUMERIC_SIGN))\n```\n\nand similar doesn't work, as it will always match whatever is marked in `ASCII_PRINTABLE_CHARS` (as it isn't itself composed of groups). So I fell back to just generating the character range and filtering out the characters there. With the `.toSeq : _*` it gets pretty ugly at the moment, but if we change the `mark` method to accept an `IndexedSeq` instead of just a `NumericRange` we could make it prettier.\n\nNo new tests yet, as I want to hear your opinion on whether this is the right way to do it.\n\nI'm relatively new to Scala, so any and all feedback is appreciated :)\n. #272 adds support for non-strict parsing, but `;` vs `&` as a query parameter separator is not addressed by the pull request.\n\nWhat's important (at least for me, and I would reckon many others), is that support for handling `;` as a param separator must be configurable, as its already handled differently by different web servers.\n\nPreferably, it should be configurable on a per-server basis, so one could have a multiple spray servers within the same process that parses these differently. This being the reason I want you to still consider making the pipelines either configurable by client code (or at least configurable on a per-bind/per-handler basis). Even if it doesn't happen overnight, I think it might be a good future direction wich would make it easier to build upon spray for servers that need to talk to particularly weird clients.\n. I realize that spray (spray-can) is meant to be a http server primarily, and fully customizable pipelines are probably only required for writing custom protocols, which in all likelihood shouldn't be done on top of spray (but the up-coming akka.io)?\n\nSo to avoid sounding like a broken record about the pipeline-part: after this pull-request I do not, as of today, know of any requirements in my current projects that isn't handled without customizing the request/response pipeline in any way. The reason I'm advocating it is mostly because from my initial peek through the spray code base in regards to the server handling, a lot of singletons and non-overridable classes seems to be used, which I'm wondering if is the right direction with respect to extensibility.\n\nI hope this comes off as being constructive and thanks for making spray, it's been awesome to work with, and I hope I'll be able to contribute more as time goes on :)\n. Sure I can squash it. I'll do it the next time I push any changes.\n\nMaking  `ASCII_PRINTABLE_CHARS / VCHAR` a compound mask is a pretty large task. All printable chars should have their own mask (with some exceptions such as digits/lowercase letters/uppercase letters). This would result in around 43 groups, which is not representable using ints.\n\nIn this case, I think it's probably preferable to avoid having a `VCHAR` mask (only the `PARSE_FRAGMENT_CHARS` mask is using it at the moment), and just generate the larger groups like I currently do. What do you think?\n. In order to have a somewhat consistent experience with the masks and compound masks, there should really only be one way to represent the characters (otherwise, exclusion is likely to fail). A more all-encompassing solution would split all the character / or groups up into their own mask and combine from there, but I suppose that isn't needed or required here.\n\nI took the liberty of removing some duplication inside the compound masks, but left the following untouched:\n\n```\nmark(LOWER_ALPHA, 'a' to 'z')\nmark(UPPER_ALPHA, 'A' to 'Z')\nmark(DIGIT19, '1' to '9')\nmark(DIGIT04, '0' to '4')\nmark(DIGIT05, '0' to '5')\nmark(LOWER_HEX_LETTER, 'a' to 'f')\nmark(UPPER_HEX_LETTER, 'A' to 'F')\n```\n\nWhat do you think now?\n\nIn order to write a few unit tests for this, it would be great if all the current tests passed, which they don't seem to do here (spray/master fails, so it's probably not limited to my fork). Are they expected to pass? Do they pass on your end? If so, I might be running them wrong -- currently I'm running them via \"sbt spray-http/test\"\n. Regarding the unit tests, it seems like it was an error on my behalf, and it looks like it's working, so I moved around and added some asserts to test this change.\n\nAdditionally, while I was poking around, I noticed that `Query.apply(...)` was broken if called with a non-empty `Map`/`List`, since `Query.Empty` does not have `.key` or `.value`. Since it was a rather small issue and fix, I thought I should just tack it on here.\n. > This looks quite nice already, thanks!!\n> Apart from the comments I think the only thing missing is a strict: Boolean parameter to the UriParser constructor and the respective Uri.apply overloads, so people can decide whether they want relaxed or strict URI parsing.\n\nWouldn't this also entail changing all the `parseAndResolve`, `parseAbsolute`, `parseHttpRequestTarget`, `Uri.from` etc etc methods and documentation?\n. What makes me uneasy about adding the `strict: Boolean` parameter is that I'm not confident that I'll be able to locate and update all the affected code, but I've given it a shot. \n. If you see nothing wrong in merging, please go ahead :)\n. Having multiple equal signs is ambiguous and will not be allowed by this pull request.\n. I see. I'm not sure the better solution would be to add a configuration variable to allow multiple equal signs (i.e the first equal sign separates the key from the value) or adding some kind of hook where the requested URI could be pre-processed before it's parsed.\n\nThe former would be pretty clean in handling your case and the latter would enable users to manipulate and re-write quite a lot of other, similar client-quirks. @sirthias probably has some preference and thoughts on the matter.\n\nMaybe open a ticket or take it up as a discussion on the mailing list? In my opinion, this should be a case that should be possible to handle without too much difficulty in a HTTP toolkit. Dealing with quirky clients is one of the perks of working with HTTP...\n. I mentioned maybe allowing customization of the request processing pipeline (which would include the parsing step) to @sirthias earlier, but he said it would require exposing more of the internal API than they would like to maintain, at least for the time being.\n\nMaybe if you add your voice to that or a similar cause, it might get reconsidered. What got me hooked on spray (around 1.1M7) was it's great performance and that I could easily work through the peculiarities of my clients. (I also have no control over my clients, and they sometimes send me all kinds of weird stuff -- it doesn't matter to them that they're bending or breaking a spec or two, what matters to them is that I'm able to write a server that can accept and work with their data). Some of this seems lost in the current master branch (and some of it I'm sure I've just not found yet), and I'd very much like it back. :)\n. That sounds great :) +1 from me\n\nOn May 3, 2013, at 12:45 , Mathias wrote:\n\n> Ok, we'll include a new config setting spray.can.parsing.uri-parsing-mode with the following defined settings:\n> \n> strict for what we have now\n> relaxed for what this PR provides\n> relaxed-with-raw-query for additionally disabling parsing of the internal query structure\n> Hopefully this is flexible enough for most applications.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. Do you think it would be possible to receive it as an RawHeader or get the option to drop it in these cases?\n. You make a very valid point.\n\nAs I might have mentioned before, I'm writing a proxy using spray, and as such, I myself might not particularly care that a specific header is malformed, since I will not be doing anything with it myself, other than to forward it to an upstream server, which gets the final say on whether the http request is acceptable or not.\n\nIn this particular case, the Content-Type header isn't actually used, but is likely inserted by one of the many http client libraries out there that speaks its very own dialect of http, for better, or usually for worse. To make matters worse with the current handling of that and other illegal requests (such as invalid/custom http methods) I cannot even determine where the request came from, or anything about it other than \"there was one request that got that response\".\n\nIf I had received that request in my application I could have inspected the remaining valid headers and done something sensible, and at the very least figured out who the user is and contacted them about the issue.\n\nSo even if parts of the request is invalid and it has to be rejected, getting the valid parts can be of great assistance in finding the cause.\n\nIn a sense, the current handling of the User-Agent header makes sense to me: It's attempted parsed, but hey, people mess up their custom user agents all the time, and it does not follow the spec, so what I get, is not an UserAgent-header, but a RawHeader containing the \"User-Agent\". That works well because I might not care about it (or I might be religious about it, but it's still up to me).\n\nWith logging of illegal headers I could see things like this in my logs:\n\n```\nIllegal request header: Illegal 'User-Agent' header: Invalid input '/', expected CTL, OptWS, Comment, ProductVersionComment or EOI (line 1, pos 8):\nfoo/bar/baz\n```\n\nand the differences in the HttpRequest.headers is obvious. :)\n. Is this then supposed to be fixed in the latest nightly (according to the commit message)? I'm still getting the following on the current latest nightly:\n\n```\n[2013-10-31 17:18:10,921][ERROR][akka.actor.OneForOneStrategy] Unexpected slot state: Idle\njava.lang.IllegalStateException: Unexpected slot state: Idle\n    at spray.can.client.HttpHostConnector$$anonfun$receive$1.applyOrElse(HttpHostConnector.scala:81) ~[spray-can-1.2-20131031.jar:na]\n    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498) [akka-actor_2.10-2.2.3.jar:2.2.3]\n    at akka.actor.ActorCell.invoke(ActorCell.scala:456) [akka-actor_2.10-2.2.3.jar:2.2.3]\n    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237) [akka-actor_2.10-2.2.3.jar:2.2.3]\n    at akka.dispatch.Mailbox.run(Mailbox.scala:219) [akka-actor_2.10-2.2.3.jar:2.2.3]\n    at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386) [akka-actor_2.10-2.2.3.    jar:2.2.3]\n    at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.10.3.jar:na]\n    at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.10.3.jar:na]\n    at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.10.3.jar:na]\n    at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.10.3.jar:na]\n[2013-10-31 17:18:10,922][ERROR][akka.actor.OneForOneStrategy] key not found: Actor[akka://default/user/IO-HTTP/host-connector-    0/1#1484964495]\njava.util.NoSuchElementException: key not found: Actor[akka://default/user/IO-HTTP/host-connector-0/1#1484964495]\n    at scala.collection.MapLike$class.default(MapLike.scala:228) ~[scala-library-2.10.3.jar:na]\n    at scala.collection.AbstractMap.default(Map.scala:58) ~[scala-library-2.10.3.jar:na]\n    at scala.collection.MapLike$class.apply(MapLike.scala:141) ~[scala-library-2.10.3.jar:na]\n    at scala.collection.AbstractMap.apply(Map.scala:58) ~[scala-library-2.10.3.jar:na]\n    at spray.can.client.HttpHostConnector$$anonfun$receive$1.applyOrElse(HttpHostConnector.scala:76) ~[spray-can-1.2-20131031.jar:na]\n    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498) [akka-actor_2.10-2.2.3.jar:2.2.3]\n    at akka.actor.ActorCell.invoke(ActorCell.scala:456) [akka-actor_2.10-2.2.3.jar:2.2.3]\n    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237) [akka-actor_2.10-2.2.3.jar:2.2.3]\n    at akka.dispatch.Mailbox.run(Mailbox.scala:219) [akka-actor_2.10-2.2.3.jar:2.2.3]\n    at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386) [akka-actor_2.10-2.2.3.    jar:2.2.3]\n    at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.10.3.jar:na]\n    at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.10.3.jar:na]\n    at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.10.3.jar:na]\n    at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.10.3.jar:na]\n```\n. Of course. I should have noticed that :)\nOn Oct 31, 2013 6:23 PM, \"Johannes Rudolph\" notifications@github.com\nwrote:\n\n> No, the fix is not even merged.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/issues/652#issuecomment-27506692\n> .\n. I updated this issue slightly: initially I thought it was a bug that spray rendered the plus sign in path components, but it should definitely be allowed unencoded. Now the issue (hopefully) clearly states that what I'm interested in is a way of getting spray to render it as percent-encoded even if it should be allowable unencoded.\n. You're absolutely correct, and the mentioned fix solves this case. Thanks Mathias!\n. ",
    "Tvaroh": "Does any working SSE solution exist for the latest Spray version?\n. Does any working SSE solution exist for the latest Spray version?\n. ",
    "rbraley": "+1\n. +1\n. ",
    "mblund": "Would this enable to share a user session between a play application and a spray application?\n. I haven't really looked at the PR or compared the cookie handling in Spray and Play so I guess I don't have that much to contribute. I just saw this PR and thought it would be cool to have one clear way to share authentication between Play web application and web services implemented with Spray. \n. Would this enable to share a user session between a play application and a spray application?\n. I haven't really looked at the PR or compared the cookie handling in Spray and Play so I guess I don't have that much to contribute. I just saw this PR and thought it would be cool to have one clear way to share authentication between Play web application and web services implemented with Spray. \n. ",
    "wolfs": "The Problem is that regarding to the RFC, that these special characters are not in the quoted version, either (see CookieOctet). Therefore, it makes not much sense to conditionally quote it or not.\nIf you want to relax this rule, then it will make sense and something along the lines\n`val needsQuotes = \"\\\"\\\\\\n\\r\\t\\f\\b+ ;=\".exists(content contains _)`\nshould do the trick.\n. The Problem is that regarding to the RFC, that these special characters are not in the quoted version, either (see CookieOctet). Therefore, it makes not much sense to conditionally quote it or not.\nIf you want to relax this rule, then it will make sense and something along the lines\n`val needsQuotes = \"\\\"\\\\\\n\\r\\t\\f\\b+ ;=\".exists(content contains _)`\nshould do the trick.\n. ",
    "acruise": "Great! Hopefully the parseUri and parseHeaders(.*) methods will be removed so I'll get a fancy type error when I update? :) \n. The X-Forwarded-For parser barfs on IPv6 too; my colleague's Apache was sending X-Forwarded-For: ::1. \n. How serious is this?  I'm a few days from going live with Spray 1.0-M7. :)\n. We still have a bunch of blocking in our REST services, sadly, so from what I can tell we need PerConnectionHandler.\n. \"receive big (non-chunked) requests as chunks\" is exactly what I need right now. :)  I don't think I care about distinguishing between requests that are literally \"Transfer-Encoding: chunked\" and those that are merely large. :)\n. Just so it's clear, server-side chunkless streaming will be coming soon? :)\n\nThanks!\n. Cool, is it in M8? :)\n. Great! Hopefully the parseUri and parseHeaders(.*) methods will be removed so I'll get a fancy type error when I update? :) \n. The X-Forwarded-For parser barfs on IPv6 too; my colleague's Apache was sending X-Forwarded-For: ::1. \n. How serious is this?  I'm a few days from going live with Spray 1.0-M7. :)\n. We still have a bunch of blocking in our REST services, sadly, so from what I can tell we need PerConnectionHandler.\n. \"receive big (non-chunked) requests as chunks\" is exactly what I need right now. :)  I don't think I care about distinguishing between requests that are literally \"Transfer-Encoding: chunked\" and those that are merely large. :)\n. Just so it's clear, server-side chunkless streaming will be coming soon? :)\n\nThanks!\n. Cool, is it in M8? :)\n. ",
    "vvcephei": "The context for me is using the pipelined spray-client described here: http://spray.io/documentation/spray-client/\n\nI have worked around by adding a pipe stage that preemptively throws an error that captures the body:\n\n``` scala\ncase class HttpErrorException(status: StatusCode, message: String) extends RuntimeException\n//...\ndef guardErrors: HttpResponse => HttpResponse = {\n    response =>\n      if (response.status.isSuccess) response\n      else throw HttpErrorException(response.status, response.entity.asString)\n  }\n//...\nval pipeline = (sendReceive(conduit) ~> guardErrors ~> unmarshal[MyClass])\n```\n\nWhich really illustrates the power of the pipelined programming model.\n\nThanks for all your hard work.\n-John\n. The context for me is using the pipelined spray-client described here: http://spray.io/documentation/spray-client/\n\nI have worked around by adding a pipe stage that preemptively throws an error that captures the body:\n\n``` scala\ncase class HttpErrorException(status: StatusCode, message: String) extends RuntimeException\n//...\ndef guardErrors: HttpResponse => HttpResponse = {\n    response =>\n      if (response.status.isSuccess) response\n      else throw HttpErrorException(response.status, response.entity.asString)\n  }\n//...\nval pipeline = (sendReceive(conduit) ~> guardErrors ~> unmarshal[MyClass])\n```\n\nWhich really illustrates the power of the pipelined programming model.\n\nThanks for all your hard work.\n-John\n. ",
    "TimothyKlim": "+1\n. > You \"got error message from google with BAD header\"?\n\nyep\n\n> What error did you get exactly and from whom? Can you show the request that is going out and the response that is coming back? What exactly do you want to ignore?\n\nhttps://github.com/TimothyKlim/scala-ga\n\nYou can compile and run it to see what's happening. I want to control exception of HTTP parsing and re-fetch OAuth token for Google Analytics.\n. Mathias, I am sorry for the latest reply.\nok, this is example by curl:\n\n``` bash\ncurl -I \"https://www.googleapis.com/analytics/v3/data/ga?ids=ga%3A12312312&start-date=2013-03-04&end-date=2013-03-04&metrics=ga%3Avisits%2Cga%3AnewVisits%2Cga%3Atransactions%2Cga%3AtransactionRevenue&dimensions=ga%3Acampaign%2Cga%3Asource%2Cga%3Amedium%2Cga%3AadContent%2Cga%3Akeyword%2Cga%3Adate&filters=ga%3Acampaign%21%3D%28referral%29%3Bga%3Asource%21%3D%28referral%29%3Bga%3Amedium%21%3D%28referral%29%3Bga%3AadContent%21%3D%28referral%29%3Bga%3Akeyword%21%3D%28referral%29&max-results=10000&start-index=1&key=true\"\n```\n\nwith result:\n\n```\nHTTP/1.1 401 Unauthorized\nWWW-Authenticate: GoogleLogin realm=\"https://www.google.com/accounts/ClientLogin\", service=\"analytics\"\nContent-Type: application/json; charset=UTF-8\nDate: Sun, 03 Mar 2013 21:26:51 GMT\nExpires: Sun, 03 Mar 2013 21:26:51 GMT\nCache-Control: private, max-age=0\nX-Content-Type-Options: nosniff\nX-Frame-Options: SAMEORIGIN\nX-XSS-Protection: 1; mode=block\nServer: GSE\nTransfer-Encoding: chunked\n\n```\n\nas you can see, google returns bad header: \n\n```\nWWW-Authenticate: GoogleLogin realm=\"https://www.google.com/accounts/ClientLogin\", service=\"analytics\"\n```\n\nHow I can catch this or ignore it?\n\nThank you.\n. When I set spray.can.client.response-chunk-aggregation-limit to 0, my Future response freezing without printing any results.\n\nMy \"sbt run\" output: https://github.com/TimothyKlim/spray-can-bug/blob/master/sbt_output.log\n. +1\n. > You \"got error message from google with BAD header\"?\n\nyep\n\n> What error did you get exactly and from whom? Can you show the request that is going out and the response that is coming back? What exactly do you want to ignore?\n\nhttps://github.com/TimothyKlim/scala-ga\n\nYou can compile and run it to see what's happening. I want to control exception of HTTP parsing and re-fetch OAuth token for Google Analytics.\n. Mathias, I am sorry for the latest reply.\nok, this is example by curl:\n\n``` bash\ncurl -I \"https://www.googleapis.com/analytics/v3/data/ga?ids=ga%3A12312312&start-date=2013-03-04&end-date=2013-03-04&metrics=ga%3Avisits%2Cga%3AnewVisits%2Cga%3Atransactions%2Cga%3AtransactionRevenue&dimensions=ga%3Acampaign%2Cga%3Asource%2Cga%3Amedium%2Cga%3AadContent%2Cga%3Akeyword%2Cga%3Adate&filters=ga%3Acampaign%21%3D%28referral%29%3Bga%3Asource%21%3D%28referral%29%3Bga%3Amedium%21%3D%28referral%29%3Bga%3AadContent%21%3D%28referral%29%3Bga%3Akeyword%21%3D%28referral%29&max-results=10000&start-index=1&key=true\"\n```\n\nwith result:\n\n```\nHTTP/1.1 401 Unauthorized\nWWW-Authenticate: GoogleLogin realm=\"https://www.google.com/accounts/ClientLogin\", service=\"analytics\"\nContent-Type: application/json; charset=UTF-8\nDate: Sun, 03 Mar 2013 21:26:51 GMT\nExpires: Sun, 03 Mar 2013 21:26:51 GMT\nCache-Control: private, max-age=0\nX-Content-Type-Options: nosniff\nX-Frame-Options: SAMEORIGIN\nX-XSS-Protection: 1; mode=block\nServer: GSE\nTransfer-Encoding: chunked\n\n```\n\nas you can see, google returns bad header: \n\n```\nWWW-Authenticate: GoogleLogin realm=\"https://www.google.com/accounts/ClientLogin\", service=\"analytics\"\n```\n\nHow I can catch this or ignore it?\n\nThank you.\n. When I set spray.can.client.response-chunk-aggregation-limit to 0, my Future response freezing without printing any results.\n\nMy \"sbt run\" output: https://github.com/TimothyKlim/spray-can-bug/blob/master/sbt_output.log\n. ",
    "gildegoma": "Note: For now, I let the whole set of commits to show the investigation history, but of course a feature branch rebase is planned, when PR quality will be reached.\n. Great! Thanks Mathias for reactivating this point. \n\nI rebased onto master (f82226b) and gave it a [try on travis](https://travis-ci.org/gildegoma/spray/jobs/7199925). Unfortunately it fails with `java.lang.OutOfMemoryError` problems... The default JVM settings for sbt on Travis are following (for a 3GB-RAM 64-bit OpenVZ Linux worker):\n\n```\n-XX:ReservedCodeCacheSize=256M\n-Xms2048M\n-Xmx2048M\n-Xss6M\n-XX:+CMSClassUnloadingEnabled\n-XX:+UseConcMarkSweepGC\n-XX:MaxPermSize=512M\n```\n\nThese settings are well-proofed default values for Scala projects like:\n- https://travis-ci.org/scalaz/scalaz\n- https://travis-ci.org/novus/salat\n- https://travis-ci.org/twitter/scalding\n\nBut actually, I already faced JVM memory troubles while trying to build Akka on Travis (especially for 'remote' subproject). If more than one JVM is started, maybe memory consumption explodes... (I did not dig further by this time). Thanks to sbt-extras, we could easily override JVM arguments with a `.jvmopts` locally stored project root folder (same for sbt parameters via .sbtopts). \n\nI shortly tried to run some local builds to compare them with travis results, and I also see errors, but no OutOfMemory error (running with 8GB RAM) Sorry, I'm too short of time to make detailed report today. I can do it, if useful.\n\nDo you have already tuning idea based on your own dev/ci boxes experience? I suspect that OpenVZ container virtualization can be source of troubles, but it would be nice to first discard/solve all \"general and common\" problems, you may know already.\n. Reducing the thread stack size looks quite good:\n- No more OutOfMemory errors on the master branch, **but the [build gets stalled](https://travis-ci.org/gildegoma/spray/jobs/7242434#L2868)** for more than 10 minutes (and is finally cancelled by Travis inactivity timeout). I tried with 2 recent master commits (see Travis build 5 and 6), and get the same problem.\n- I also tried to build the tag 1.1-M7, and [it passes on travis](https://travis-ci.org/gildegoma/spray/builds/7262223) :yum: (1.1-M7 with `-Xss6M` [fails on top of Oracle JDK7](https://travis-ci.org/gildegoma/spray/builds/7264496), but passes with OpenJDK7)\n\nNext step is to better figure out why master build never terminates. For comparison, I ran the same build on a local OS X 10.7... and actually I observe exactly the same problem (build hangs exactly at the same place, the java process must be CTRL-C killed). It sounds not to be a Travis integration problem... Am I doing something wrong in both environments? What about your build environments with recent master changes? Any known problem?\n. Sorry for the delay Mathias, a bit slammed this week... I tried your flags on both environments (Local OSX 10.7 and Travis Ubuntu), **but I still obtain the same non-exit problem** (java process does not exit at the end of the build). On OSX, `jstack` does not detect anything strange (`Deadlock Detection: No deadlocks found`). I also verified that the problem is present with several Spray revisions (e.g. 7b0e88a or 0450136). \n\nIt is quite strange that you never faced similar problems... I feel that I am missing something (maybe trivial). But what could it be? Never received similar problem report from Spray contributors?\n\nIf your build stacks are solid, I am not sure if YourKit/VisualVM profiling would really help to tackle the problem. I'll first try to build some different vagrant environments to get at least a box where Spray tests run fine... I'll give you some feedbacks in the next days.\n\nBTW: like for 1.1-M7 build, you can see here that [2 test examples fail with Spray/OracleJDK7](https://travis-ci.org/gildegoma/spray/jobs/7442913#L2898), while [same examples pass with Spray/OpenJDK7](https://travis-ci.org/gildegoma/spray/jobs/7442914#L2845). \n. Thanks a lot Johannes! \n- I totally agree about the _more third-party testing_ principle, especially because of these non-deterministic aspects (hence this request ;-). I hope we get it ~stable working on travis soon!\n- Your b5427bc fix helps a lot to get rid of 'hanging' trouble: \n  - my local builds now always terminate, and actually succeed (without `akka.test.timefactor` tuning, not that slow :older_woman: machine)\n  - on travis-ci, all builds always exit as well now :yellow_heart: But there are still non-systematic errors (see [builds Nr 12 to 14](https://travis-ci.org/gildegoma/spray/builds))\n- Playing with `akka.test.timefactor` is a very good point that I totally forgot (I used it when try out to build [Akka on Travis](https://github.com/gildegoma/akka/commit/a493a7884d5899ffdcf053d47c3f7a340cb2ad92)). With higher test timefactor, there is no more error with HttpServiceSpec, but new errors now randomly occur in SprayCanClientSpec with a [factor of 3](https://travis-ci.org/gildegoma/spray/jobs/7465902#L1966) and also a [factor of 5](https://travis-ci.org/gildegoma/spray/jobs/7469379). Both errors are not exactly in the same example... \"Fortunately\" I got some [green lights](https://travis-ci.org/gildegoma/spray/builds/7465901) (encouraging :-) Since SprayCanClientSpec occur only on Travis, I guess we can find an accurate value for `akka.test.timefactor`, unless there is another good parameter to tweak? I'll go on this way and keep you informed.\n\n---\n\nFor the records, I observed following symptom when 'hanging build' occurs: The java process locks while still listening on 3 random TCP ports:\n\n```\nsome-macosx-10.7.5 spray (travis-trial-and-error)$ lsof -i | grep LISTEN | grep java\nCOMMAND     PID   USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME\njava        516 gilles  445u  IPv6 0xffffff8019588ac0      0t0  TCP *:53920 (LISTEN)\njava        516 gilles  446u  IPv6 0xffffff8018c04700      0t0  TCP localhost:53932 (LISTEN)\njava        516 gilles  457u  IPv6 0xffffff8019587f80      0t0  TCP localhost:53934 (LISTEN)\n```\n\nI also verified that forcing IPv4 instead of IPv6 did not change anything. As an exercise, I have to understand how b5427bc changed this behavior...\n. Thanks Johannes for the details about 'hanging' problem.\n\nOn my side, I have following:\n- It seems quite safer to compile and test in separated `sbt` calls (in order to avoid out of memory problem, and maybe get better test performance?)\n- with a higher `timefactor`, there is a problem in SprayCanServerSpec, because `expectNoMsg` waits for 3 **dilated** seconds (`akka.test.single-expect-default = 3s` by default, taken from [Akka docs](http://doc.akka.io/docs/akka/2.1.4/general/configuration.html)), while (I think that) during the test, the client-server connection is always closed after a **fixed period** of time around 3-5 seconds (without timefactor dilatation). So far I make a very [quick and dirty workaround that works with time factors around 2](https://github.com/gildegoma/spray/commit/a5564146f884e491f7ca367e91c8ade67216bbc0), but now I need your help to better figure out how to cleanly solve this problem. From what I understand (sorry, I feel so newbie), I see two variants:\n  - make the connection life time dependent of the `timefactor`, or\n  - don't dilate time waited in `expectNoMsg` (by overriding `.dilated` trait method. I've seen this technique somewhere already\u2026 have to find it again)\n- A `timefactor` between 1.7 and 2.5 seems - [most of the time](https://travis-ci.org/gildegoma/spray/builds/7527944) :-/ - to fit well for current travis worker performance (with factors between 0.5 and 1.5, longer timeouts frequently lead some examples to fail, but not always. see screenshot below). Once we solve all the \"higher timefactor\" issues, I guess we should be able to permanently enable travis builds... \n\n![image](https://f.cloud.github.com/assets/434654/568108/f5c37928-c6cc-11e2-9fea-8367ed10a28b.png)\n\nNow that I better realize how tricky (or complex) it can be to warranty `timefactor` flexibility, I wonder if we shouldn't also consider using Travis CI to track potential `timefactor` regressions, with a matrix like following:\n- timefactor = \"MIN\" (the lowest travis worker performance allow), could be _allowed to fail_ (because of timeout perf variance)\n- timefactor = \"STABLE\" (a proven value ~1-3, that must be stable)\n- timefactor = \"MAX\" (a quite high value, that can be used to detect dilatation errors)\n\nDoes it make sense to cover such `timefactor` range (in a further step)? Maybe overkill... \n\nApologies: Sorry if points above are incorrect or incomplete (I had unfortunately not enough time to search/learn all what I need about Spray and Akka, but I preferred to report the stand of my understanding). Thank you in advance for comprehension and help!\n. With recent commit d658a6085a08d29c67e732f387331d730dca4b18, `spray.can.server.request-timeout` is now 20 seconds (instead of 5 seconds before). Therefore it \"pragmatically solves\" the `expectNoMsg` issue in SprayCanServerSpec ( _properly support fastPath responses_ ), unless `timefactor` is greater than 20! Should this spec example be more robust, and take into account the effective value of the `request-timeout`?\n\nOtherwise... the travis builds often could pass, but are still unstable. I continue to face some random memory problems (`-Xmx2g` is not enough and `-Xmx3g` is too much for current travis workers, equipped with exactly with 3G). With `-Xmx2560M`, the [last build passed](https://travis-ci.org/gildegoma/spray/builds/7542841) (but similar settings previously failed...) I'll may try to spare on XX:MaxPermSize (or Xss?)... Maybe I'll need to use two different sets of JVM options (one for the compilation and one for the tests). Arggh... :dart: Advices are welcome...\n. Thanks @jrudolph for all the hints. (Mathias already gave jenkins settings, but I somehow miscopied them and kept going with non-necessary high values, sorry. Good that you make me see them again ;-). \n\nSoon, I'll try to configure the build with `Xss2m` + `Xmx2048`, and also to limit the parallelism... and give you some more news when available. Thanks a lot for coaching and patience :)\n. Thank to all your good advices, we're getting closer: I've juste update the PR branch with well-proven settings that :green_heart: pass. Rare \"timeout\" error still may occur, but it is always possible to manually re-trigger a new build from Travis CI GUI to remove \"false\" alarms. I thus hope it won't annoy... For sake of stability, I opted for the _test serialization_, but _test parallelization_ sounds possible as well (should be tested on a longer term).\n\nSummary:\n- `akka.test.timefactor=1.5` is a good tuning (no failure with 5 \"parallel runs\" (builds 38-42), no failure in \"serialized runs\" (builds 43-50)\n- With _test parallelization_, there are still some timing problem with higher timefactors (I did not dig into it yet):\n  - https://travis-ci.org/gildegoma/spray/jobs/7558969#L2652\n  - https://travis-ci.org/gildegoma/spray/jobs/7558623#L2653\n- With _test serialization_, timefactors of 1.0 and 1.2 have sometimes failed for a timeout:\n  - https://travis-ci.org/gildegoma/spray/jobs/7565037\n  - https://travis-ci.org/gildegoma/spray/jobs/7578757\n\nLet me know how we should go further (still pending Sphinx, timefactor robustness,...). Note that I probably won't be available before next week. Also bis bald!\n. \"Cosmetic\" Note:\n- JVM and SBT options specific to Travis environment are configured in `.travis.yml`\n- JVM options generally applicable to any Spray build environment are configured in `.jvmopts`\n\nOf course it can be changed... (e.g. configure everything from `.travis.yml` and get rid of `.jvmopts`). Like for \"e-mail notifications\" and other fine tuning aspects, I propose @sirthias and @jrudolph to adapt these points to your needs afterwards.\n. Cool! Thanks again for kindly guiding my little hands. I've learnt lots of new things from your tips.\n. I had a very quick look at travis builds, to verify how it works after two weeks. Good to see lots of :green_heart:. But beside accurate :red_circle: build errors, I also see annoying timeout build errors, and worst this one: https://travis-ci.org/spray/spray/jobs/8055545#L1138 (SIGSEGV!). Thus I want to ask you:\n- Did you sometimes retrigger failing builds from Travis UI to double check?\n- Let me know if you think travis team could do something to improve build reliability...\n. Note: For now, I let the whole set of commits to show the investigation history, but of course a feature branch rebase is planned, when PR quality will be reached.\n. Great! Thanks Mathias for reactivating this point. \n\nI rebased onto master (f82226b) and gave it a [try on travis](https://travis-ci.org/gildegoma/spray/jobs/7199925). Unfortunately it fails with `java.lang.OutOfMemoryError` problems... The default JVM settings for sbt on Travis are following (for a 3GB-RAM 64-bit OpenVZ Linux worker):\n\n```\n-XX:ReservedCodeCacheSize=256M\n-Xms2048M\n-Xmx2048M\n-Xss6M\n-XX:+CMSClassUnloadingEnabled\n-XX:+UseConcMarkSweepGC\n-XX:MaxPermSize=512M\n```\n\nThese settings are well-proofed default values for Scala projects like:\n- https://travis-ci.org/scalaz/scalaz\n- https://travis-ci.org/novus/salat\n- https://travis-ci.org/twitter/scalding\n\nBut actually, I already faced JVM memory troubles while trying to build Akka on Travis (especially for 'remote' subproject). If more than one JVM is started, maybe memory consumption explodes... (I did not dig further by this time). Thanks to sbt-extras, we could easily override JVM arguments with a `.jvmopts` locally stored project root folder (same for sbt parameters via .sbtopts). \n\nI shortly tried to run some local builds to compare them with travis results, and I also see errors, but no OutOfMemory error (running with 8GB RAM) Sorry, I'm too short of time to make detailed report today. I can do it, if useful.\n\nDo you have already tuning idea based on your own dev/ci boxes experience? I suspect that OpenVZ container virtualization can be source of troubles, but it would be nice to first discard/solve all \"general and common\" problems, you may know already.\n. Reducing the thread stack size looks quite good:\n- No more OutOfMemory errors on the master branch, **but the [build gets stalled](https://travis-ci.org/gildegoma/spray/jobs/7242434#L2868)** for more than 10 minutes (and is finally cancelled by Travis inactivity timeout). I tried with 2 recent master commits (see Travis build 5 and 6), and get the same problem.\n- I also tried to build the tag 1.1-M7, and [it passes on travis](https://travis-ci.org/gildegoma/spray/builds/7262223) :yum: (1.1-M7 with `-Xss6M` [fails on top of Oracle JDK7](https://travis-ci.org/gildegoma/spray/builds/7264496), but passes with OpenJDK7)\n\nNext step is to better figure out why master build never terminates. For comparison, I ran the same build on a local OS X 10.7... and actually I observe exactly the same problem (build hangs exactly at the same place, the java process must be CTRL-C killed). It sounds not to be a Travis integration problem... Am I doing something wrong in both environments? What about your build environments with recent master changes? Any known problem?\n. Sorry for the delay Mathias, a bit slammed this week... I tried your flags on both environments (Local OSX 10.7 and Travis Ubuntu), **but I still obtain the same non-exit problem** (java process does not exit at the end of the build). On OSX, `jstack` does not detect anything strange (`Deadlock Detection: No deadlocks found`). I also verified that the problem is present with several Spray revisions (e.g. 7b0e88a or 0450136). \n\nIt is quite strange that you never faced similar problems... I feel that I am missing something (maybe trivial). But what could it be? Never received similar problem report from Spray contributors?\n\nIf your build stacks are solid, I am not sure if YourKit/VisualVM profiling would really help to tackle the problem. I'll first try to build some different vagrant environments to get at least a box where Spray tests run fine... I'll give you some feedbacks in the next days.\n\nBTW: like for 1.1-M7 build, you can see here that [2 test examples fail with Spray/OracleJDK7](https://travis-ci.org/gildegoma/spray/jobs/7442913#L2898), while [same examples pass with Spray/OpenJDK7](https://travis-ci.org/gildegoma/spray/jobs/7442914#L2845). \n. Thanks a lot Johannes! \n- I totally agree about the _more third-party testing_ principle, especially because of these non-deterministic aspects (hence this request ;-). I hope we get it ~stable working on travis soon!\n- Your b5427bc fix helps a lot to get rid of 'hanging' trouble: \n  - my local builds now always terminate, and actually succeed (without `akka.test.timefactor` tuning, not that slow :older_woman: machine)\n  - on travis-ci, all builds always exit as well now :yellow_heart: But there are still non-systematic errors (see [builds Nr 12 to 14](https://travis-ci.org/gildegoma/spray/builds))\n- Playing with `akka.test.timefactor` is a very good point that I totally forgot (I used it when try out to build [Akka on Travis](https://github.com/gildegoma/akka/commit/a493a7884d5899ffdcf053d47c3f7a340cb2ad92)). With higher test timefactor, there is no more error with HttpServiceSpec, but new errors now randomly occur in SprayCanClientSpec with a [factor of 3](https://travis-ci.org/gildegoma/spray/jobs/7465902#L1966) and also a [factor of 5](https://travis-ci.org/gildegoma/spray/jobs/7469379). Both errors are not exactly in the same example... \"Fortunately\" I got some [green lights](https://travis-ci.org/gildegoma/spray/builds/7465901) (encouraging :-) Since SprayCanClientSpec occur only on Travis, I guess we can find an accurate value for `akka.test.timefactor`, unless there is another good parameter to tweak? I'll go on this way and keep you informed.\n\n---\n\nFor the records, I observed following symptom when 'hanging build' occurs: The java process locks while still listening on 3 random TCP ports:\n\n```\nsome-macosx-10.7.5 spray (travis-trial-and-error)$ lsof -i | grep LISTEN | grep java\nCOMMAND     PID   USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME\njava        516 gilles  445u  IPv6 0xffffff8019588ac0      0t0  TCP *:53920 (LISTEN)\njava        516 gilles  446u  IPv6 0xffffff8018c04700      0t0  TCP localhost:53932 (LISTEN)\njava        516 gilles  457u  IPv6 0xffffff8019587f80      0t0  TCP localhost:53934 (LISTEN)\n```\n\nI also verified that forcing IPv4 instead of IPv6 did not change anything. As an exercise, I have to understand how b5427bc changed this behavior...\n. Thanks Johannes for the details about 'hanging' problem.\n\nOn my side, I have following:\n- It seems quite safer to compile and test in separated `sbt` calls (in order to avoid out of memory problem, and maybe get better test performance?)\n- with a higher `timefactor`, there is a problem in SprayCanServerSpec, because `expectNoMsg` waits for 3 **dilated** seconds (`akka.test.single-expect-default = 3s` by default, taken from [Akka docs](http://doc.akka.io/docs/akka/2.1.4/general/configuration.html)), while (I think that) during the test, the client-server connection is always closed after a **fixed period** of time around 3-5 seconds (without timefactor dilatation). So far I make a very [quick and dirty workaround that works with time factors around 2](https://github.com/gildegoma/spray/commit/a5564146f884e491f7ca367e91c8ade67216bbc0), but now I need your help to better figure out how to cleanly solve this problem. From what I understand (sorry, I feel so newbie), I see two variants:\n  - make the connection life time dependent of the `timefactor`, or\n  - don't dilate time waited in `expectNoMsg` (by overriding `.dilated` trait method. I've seen this technique somewhere already\u2026 have to find it again)\n- A `timefactor` between 1.7 and 2.5 seems - [most of the time](https://travis-ci.org/gildegoma/spray/builds/7527944) :-/ - to fit well for current travis worker performance (with factors between 0.5 and 1.5, longer timeouts frequently lead some examples to fail, but not always. see screenshot below). Once we solve all the \"higher timefactor\" issues, I guess we should be able to permanently enable travis builds... \n\n![image](https://f.cloud.github.com/assets/434654/568108/f5c37928-c6cc-11e2-9fea-8367ed10a28b.png)\n\nNow that I better realize how tricky (or complex) it can be to warranty `timefactor` flexibility, I wonder if we shouldn't also consider using Travis CI to track potential `timefactor` regressions, with a matrix like following:\n- timefactor = \"MIN\" (the lowest travis worker performance allow), could be _allowed to fail_ (because of timeout perf variance)\n- timefactor = \"STABLE\" (a proven value ~1-3, that must be stable)\n- timefactor = \"MAX\" (a quite high value, that can be used to detect dilatation errors)\n\nDoes it make sense to cover such `timefactor` range (in a further step)? Maybe overkill... \n\nApologies: Sorry if points above are incorrect or incomplete (I had unfortunately not enough time to search/learn all what I need about Spray and Akka, but I preferred to report the stand of my understanding). Thank you in advance for comprehension and help!\n. With recent commit d658a6085a08d29c67e732f387331d730dca4b18, `spray.can.server.request-timeout` is now 20 seconds (instead of 5 seconds before). Therefore it \"pragmatically solves\" the `expectNoMsg` issue in SprayCanServerSpec ( _properly support fastPath responses_ ), unless `timefactor` is greater than 20! Should this spec example be more robust, and take into account the effective value of the `request-timeout`?\n\nOtherwise... the travis builds often could pass, but are still unstable. I continue to face some random memory problems (`-Xmx2g` is not enough and `-Xmx3g` is too much for current travis workers, equipped with exactly with 3G). With `-Xmx2560M`, the [last build passed](https://travis-ci.org/gildegoma/spray/builds/7542841) (but similar settings previously failed...) I'll may try to spare on XX:MaxPermSize (or Xss?)... Maybe I'll need to use two different sets of JVM options (one for the compilation and one for the tests). Arggh... :dart: Advices are welcome...\n. Thanks @jrudolph for all the hints. (Mathias already gave jenkins settings, but I somehow miscopied them and kept going with non-necessary high values, sorry. Good that you make me see them again ;-). \n\nSoon, I'll try to configure the build with `Xss2m` + `Xmx2048`, and also to limit the parallelism... and give you some more news when available. Thanks a lot for coaching and patience :)\n. Thank to all your good advices, we're getting closer: I've juste update the PR branch with well-proven settings that :green_heart: pass. Rare \"timeout\" error still may occur, but it is always possible to manually re-trigger a new build from Travis CI GUI to remove \"false\" alarms. I thus hope it won't annoy... For sake of stability, I opted for the _test serialization_, but _test parallelization_ sounds possible as well (should be tested on a longer term).\n\nSummary:\n- `akka.test.timefactor=1.5` is a good tuning (no failure with 5 \"parallel runs\" (builds 38-42), no failure in \"serialized runs\" (builds 43-50)\n- With _test parallelization_, there are still some timing problem with higher timefactors (I did not dig into it yet):\n  - https://travis-ci.org/gildegoma/spray/jobs/7558969#L2652\n  - https://travis-ci.org/gildegoma/spray/jobs/7558623#L2653\n- With _test serialization_, timefactors of 1.0 and 1.2 have sometimes failed for a timeout:\n  - https://travis-ci.org/gildegoma/spray/jobs/7565037\n  - https://travis-ci.org/gildegoma/spray/jobs/7578757\n\nLet me know how we should go further (still pending Sphinx, timefactor robustness,...). Note that I probably won't be available before next week. Also bis bald!\n. \"Cosmetic\" Note:\n- JVM and SBT options specific to Travis environment are configured in `.travis.yml`\n- JVM options generally applicable to any Spray build environment are configured in `.jvmopts`\n\nOf course it can be changed... (e.g. configure everything from `.travis.yml` and get rid of `.jvmopts`). Like for \"e-mail notifications\" and other fine tuning aspects, I propose @sirthias and @jrudolph to adapt these points to your needs afterwards.\n. Cool! Thanks again for kindly guiding my little hands. I've learnt lots of new things from your tips.\n. I had a very quick look at travis builds, to verify how it works after two weeks. Good to see lots of :green_heart:. But beside accurate :red_circle: build errors, I also see annoying timeout build errors, and worst this one: https://travis-ci.org/spray/spray/jobs/8055545#L1138 (SIGSEGV!). Thus I want to ask you:\n- Did you sometimes retrigger failing builds from Travis UI to double check?\n- Let me know if you think travis team could do something to improve build reliability...\n. ",
    "vjousse": "Ok I've rebased on top of the current master here https://github.com/spray/spray/pull/294\n. My pleasure!\n\nThanks for the tip, I'm not very familiar with Github pull requests.\n. Ok I've rebased on top of the current master here https://github.com/spray/spray/pull/294\n. My pleasure!\n\nThanks for the tip, I'm not very familiar with Github pull requests.\n. ",
    "athieriot": "Hello,\n\nI was wondering if it was still in the plan to move to Sonatype?\n\nIf so, can I do something to help?\n. Noted :)\n\nThis is rather difficult to release tools around Spray.io in this state, but I'm glad it's in the pipe.\n\n Thank you for your answer.\n. Hello,\n\nI was wondering if it was still in the plan to move to Sonatype?\n\nIf so, can I do something to help?\n. Noted :)\n\nThis is rather difficult to release tools around Spray.io in this state, but I'm glad it's in the pipe.\n\n Thank you for your answer.\n. ",
    "huntc": "Thanks @sirthias !\n. Yay! Thanks so much.\n\nIt isn't there yet, but I shall keep my eye out:\n\nhttp://mvnrepository.com/search.html?query=io.spray\n. Looks as though there's a problem... still not at Maven Central.\n. This is indeed now fully resolved. Thanks @sirthias !\n. Thanks @sirthias !\n. Yay! Thanks so much.\n\nIt isn't there yet, but I shall keep my eye out:\n\nhttp://mvnrepository.com/search.html?query=io.spray\n. Looks as though there's a problem... still not at Maven Central.\n. This is indeed now fully resolved. Thanks @sirthias !\n. ",
    "joastbg": "Hi Mattias,\n\nNo problem for me at all, the important thing is that it is supported :)\n. Hi Mattias,\n\nNo problem for me at all, the important thing is that it is supported :)\n. ",
    "marekzebrowski": "Yes, of course, I have also noticed a slight problem with my approach - one\nneed to do:\n\ntrait Json4sJacksonSupport extends MetaMarshallers\n\ninstead of just\n\ntrait Json4sJacksonSupport\n\nbecause otherwise meta marshallers will not get called. Need to work it out.\n\nIn the near future I probably try to make docs plugin - either for apiary\nor swagger - will see.\n\nBest regards,\n\nMarek\n\n2013/4/29 Mathias notifications@github.com\n\n> Marek,\n> thank you for submitting this PR and sorry for not reacting sooner. We've\n> been quite swamped with high-priority stuff these last weeks.\n> However, I'd like to merge your PR now. Would you be able to update it to\n> the latest version of the master?\n> Thanks again!\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/251#issuecomment-17159263\n> .\n\n## \n\nMarek \u017bebrowski\n. Only \"rebased json4s support\" matters\n. done\n\n2013/5/2 Mathias notifications@github.com\n\n> Can you simply squash your commits, rebase them into the curent master and push\n> -f the resulting commit to your fork? Otherwise the merge is going to\n> create a mess on our side...\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/251#issuecomment-17334661\n> .\n\n## \n\nMarek \u017bebrowski\n. Thanks for all comments - I applied them, so the pull request is much nicer now.\nOrigin was changed to contain Uri instead of String  and Access-Control-Max-Age was changed to Long\n\nBy the way - test on travis starts to fail due to to low Xss, especially spray.http.HttpHeaderSpec\n. No problem, commit message amended.\n. About failing tests, as I wrote in comment above increasing Xss in jvm makes tests pass. The most problematic is spray.http.HttpHeaderSpec, which creates a big specs example.\n. Hmm, I'll see to it in the evening.\n. Changes are made. Now the only failing test is spray.routing.FileAndResourceDirectivesSpec  that you mention is unstable.\nOracle JDK works, OpenJDK fails.\n. I tried it, but it didn't help - both builds failed.\n. I'll clean my PR as you mentioned in the eventing. \nI get the same test breakages on my local machine, but cause is always OutOfMemory Error.\nStrange\n. I managed to get test working, but I am not proud of the solution. Probably there is a better way - maybe await for actor system do be ready in TestKit\n. Reset is also done.\n. Pay attention to marshalling promises, futures etc . Json4s tries to marshall them, and that leads to unexpected behavior in default scenario, when route uses ask pattern to get data from the actor.\n. Yes, of course, I have also noticed a slight problem with my approach - one\nneed to do:\n\ntrait Json4sJacksonSupport extends MetaMarshallers\n\ninstead of just\n\ntrait Json4sJacksonSupport\n\nbecause otherwise meta marshallers will not get called. Need to work it out.\n\nIn the near future I probably try to make docs plugin - either for apiary\nor swagger - will see.\n\nBest regards,\n\nMarek\n\n2013/4/29 Mathias notifications@github.com\n\n> Marek,\n> thank you for submitting this PR and sorry for not reacting sooner. We've\n> been quite swamped with high-priority stuff these last weeks.\n> However, I'd like to merge your PR now. Would you be able to update it to\n> the latest version of the master?\n> Thanks again!\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/251#issuecomment-17159263\n> .\n\n## \n\nMarek \u017bebrowski\n. Only \"rebased json4s support\" matters\n. done\n\n2013/5/2 Mathias notifications@github.com\n\n> Can you simply squash your commits, rebase them into the curent master and push\n> -f the resulting commit to your fork? Otherwise the merge is going to\n> create a mess on our side...\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/251#issuecomment-17334661\n> .\n\n## \n\nMarek \u017bebrowski\n. Thanks for all comments - I applied them, so the pull request is much nicer now.\nOrigin was changed to contain Uri instead of String  and Access-Control-Max-Age was changed to Long\n\nBy the way - test on travis starts to fail due to to low Xss, especially spray.http.HttpHeaderSpec\n. No problem, commit message amended.\n. About failing tests, as I wrote in comment above increasing Xss in jvm makes tests pass. The most problematic is spray.http.HttpHeaderSpec, which creates a big specs example.\n. Hmm, I'll see to it in the evening.\n. Changes are made. Now the only failing test is spray.routing.FileAndResourceDirectivesSpec  that you mention is unstable.\nOracle JDK works, OpenJDK fails.\n. I tried it, but it didn't help - both builds failed.\n. I'll clean my PR as you mentioned in the eventing. \nI get the same test breakages on my local machine, but cause is always OutOfMemory Error.\nStrange\n. I managed to get test working, but I am not proud of the solution. Probably there is a better way - maybe await for actor system do be ready in TestKit\n. Reset is also done.\n. Pay attention to marshalling promises, futures etc . Json4s tries to marshall them, and that leads to unexpected behavior in default scenario, when route uses ask pattern to get data from the actor.\n. ",
    "ftokarev": "+1, this would be very handy\n. @stanpalatnik any updates on this?\n. +1, this would be very handy\n. @stanpalatnik any updates on this?\n. ",
    "stanpalatnik": "Not yet. \n. Did some debugging. While the cookies are parsed to`HttpCookie` in `HttpParser`, when looking at the request in the routes, they are still in the `RawHeader`(and the `cookies` variable is null). \n\nCorrectly parsed in Httpheader: http://d.pr/i/pIaq\n. Ah, right. I updated spray-servlet, spray-http, and spray-httpx to the latest 614 nightlies. Yes, for me the end result is still cookies in the RawHeader when they reach the routes. \n\nHere's another screenshot(using the same versions as the screenshot in the previous post), that shows the cookies in the rawheaders: http://d.pr/i/Lq6b\n. Mathias, \n\nDid you run this on Tomcat? I ran the same thing on Tomcat 7 and they are still `RawHeaders`. Will setup Jetty a bit later. \n. So it always returns the cookies in the `RawHeader` for me, and since I'm using the 614 jars, I can't think of what I'm going wrong. Here's a list of jars in my project: http://pastebin.com/FxxpUFRG. \n. Mathias, \n\nFor a sanity check I cloned the spray-template 1.2 on-jetty branch, replaced the spray-servlet with the 614 nightly, ran your code above, and the cookies were still in a `RawHeader`\n. Mathias, \n\nThat works! Thanks! Whatever was the problems seems to be solved in today's(or yesterday's) nightly.\n\nHowever I have to bug you again. When I replaced the 620 servlet jar in my main project, I got the following error: \n\njava.lang.NoSuchMethodError: spray.http.Uri$Path.startsWith(Lspray/http/Uri$Path;)Z\n    spray.servlet.ModelConverter$.rebuildUri(ModelConverter.scala:70)\n    spray.servlet.ModelConverter$.toHttpRequest(ModelConverter.scala:38)\n    spray.servlet.Servlet30ConnectorServlet.service(Servlet30ConnectorServlet.scala:62)\n    javax.servlet.http.HttpServlet.service(HttpServlet.java:728)\n\nI'm using the spray-http 614 nighty as well. \n. I was just going to edit my post. I put back the 1.2 M8 spray-http jar and it works. \n. Good point. Upgraded everything to 620 and it works. Thanks again. \n. Is it possible to add port customization as well since we already have a `DefaultHostInfo`? I'm thinking this could solve my issue here: https://groups.google.com/forum/#!topic/spray-user/264ysLLRrv8\n. +1\n. Not yet. \n. Did some debugging. While the cookies are parsed to`HttpCookie` in `HttpParser`, when looking at the request in the routes, they are still in the `RawHeader`(and the `cookies` variable is null). \n\nCorrectly parsed in Httpheader: http://d.pr/i/pIaq\n. Ah, right. I updated spray-servlet, spray-http, and spray-httpx to the latest 614 nightlies. Yes, for me the end result is still cookies in the RawHeader when they reach the routes. \n\nHere's another screenshot(using the same versions as the screenshot in the previous post), that shows the cookies in the rawheaders: http://d.pr/i/Lq6b\n. Mathias, \n\nDid you run this on Tomcat? I ran the same thing on Tomcat 7 and they are still `RawHeaders`. Will setup Jetty a bit later. \n. So it always returns the cookies in the `RawHeader` for me, and since I'm using the 614 jars, I can't think of what I'm going wrong. Here's a list of jars in my project: http://pastebin.com/FxxpUFRG. \n. Mathias, \n\nFor a sanity check I cloned the spray-template 1.2 on-jetty branch, replaced the spray-servlet with the 614 nightly, ran your code above, and the cookies were still in a `RawHeader`\n. Mathias, \n\nThat works! Thanks! Whatever was the problems seems to be solved in today's(or yesterday's) nightly.\n\nHowever I have to bug you again. When I replaced the 620 servlet jar in my main project, I got the following error: \n\njava.lang.NoSuchMethodError: spray.http.Uri$Path.startsWith(Lspray/http/Uri$Path;)Z\n    spray.servlet.ModelConverter$.rebuildUri(ModelConverter.scala:70)\n    spray.servlet.ModelConverter$.toHttpRequest(ModelConverter.scala:38)\n    spray.servlet.Servlet30ConnectorServlet.service(Servlet30ConnectorServlet.scala:62)\n    javax.servlet.http.HttpServlet.service(HttpServlet.java:728)\n\nI'm using the spray-http 614 nighty as well. \n. I was just going to edit my post. I put back the 1.2 M8 spray-http jar and it works. \n. Good point. Upgraded everything to 620 and it works. Thanks again. \n. Is it possible to add port customization as well since we already have a `DefaultHostInfo`? I'm thinking this could solve my issue here: https://groups.google.com/forum/#!topic/spray-user/264ysLLRrv8\n. +1\n. ",
    "jcranky": "Will this one happen? :)\n. Will this one happen? :)\n. ",
    "ollyw": "Is there any idea when this issue might get fixed, or suggestions for a workaround? If there is nothing imminent, I might attempt a fix myself. Any pointers welcome.\n\nBTW, thanks for all the work put into Spray - it is great software\n. Is there any idea when this issue might get fixed, or suggestions for a workaround? If there is nothing imminent, I might attempt a fix myself. Any pointers welcome.\n\nBTW, thanks for all the work put into Spray - it is great software\n. ",
    "4lex1v": "What's the status of this feature? I'd like to take a look at this, the natural way would be to do this with channels, but there are available since Java 7. Is Java 6 support required?\n. Looks like in this case Scala, without explicit type parameters, can't choose the proper `apply` method =( False alarm\n. What's the status of this feature? I'd like to take a look at this, the natural way would be to do this with channels, but there are available since Java 7. Is Java 6 support required?\n. Looks like in this case Scala, without explicit type parameters, can't choose the proper `apply` method =( False alarm\n. ",
    "danielwegener": "The cake is a lie :( According to [this mail list](http://mail.openjdk.java.net/pipermail/nio-discuss/2011-July/000577.html), there is no true async File IO in [JDK7](http://hg.openjdk.java.net/jdk7u/jdk7u/jdk/file/eeb571eea68d/src/share/classes/sun/nio/ch/SimpleAsynchronousFileChannelImpl.java#l289) or [JDK8](http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/share/classes/sun/nio/ch/SimpleAsynchronousFileChannelImpl.java#l289). It is all executed as  blocking IO on the given ExecutorService. The danger is, that akka does not even notice the potential thread consumption. It might be better do have a dedicated akka dispatcher that does the syncronous file io. Or we go [native](http://docs.jboss.org/hornetq/2.2.5.Final/user-manual/en/html/persistence.html)\n. How I hotfixed it: https://github.com/danielwegener/spray/commit/c495623497f9d6c41552f1546e7f9d9237163a11\nMight be a bit to naive :)\n. I am especially unhappy with the following things:\n- Marshallers/Umarshallers are almost perfect copies of multipart/chunked marshallers/unmarshallers. Maybe we could extract their commons.\n- IMO _RangeDirectives_ looks like imperative spaghetti code. Could you maybe give me a hint to make it more scalarific?\n- Documentation files are missing.\n- The _\"HttpHeaderParser prime an empty parser with all defined HeaderValueParsers\"_-Test is hard to fix without changing what it should ensure originally :)\n. I just fixed the tests. I am not really sure what the were supposed to ensure because the asserts seemed to have no direct meaning but \"the result is equal to an earlier run of the test\".\n\nIs there any chance this PR will find a way to master soon or is there still a 'become akka-http first' policy? :)\n. Hi Mathias!\nGreat news! Thanks for your time and feedback. I will do my homework and report back asap.\n. Green again - I guess I implemented all your suggestions. What will be the next step - squash + rebase?\n. Hi Mathias. Your decision - I'll be fine :) (still got some spare time this week if I can help out)\n. Yes, LGTM.\nI wrote the original implementation against version 24 of the HTTPbis spec so I'll check the [diff](http://www.ietf.org/rfcdiff?url1=draft-ietf-httpbis-p5-range-24&url2=draft-ietf-httpbis-p5-range-26) this evening. \n. Okay, so the only non-formal change seem to be in _3.1.  Range_ where `other-range-set = 1*CHAR` becomes `other-range-set = 1*VCHAR`. Since we do support nothing else but byte-ranges this does not really matter. ~~I noticed a subtile glitch in the definition of a 'CHAR'. draft-ietf-httpbis-p5-range-26 uses BNF-Rules from rfc5234. The CHAR definition there allows any US-ASCII-Char but 'NUL' (B1 Core Rules) while the HTTP1.1 (rfc2616) allows anything (inclusive 'NUL'). So if we want to be utra-spec-compliant ... :)~~ (nevermind, used only in `other-range-resp` anyway)\n\n(I really hope i can come to Berlin, but I am already visiting the #reactconf next month - so since I dont have my own talks, I think I have to take a vacation for #scaladays ;) )\n. @sirthias We maybe can make this beer thing up this year in Amsterdam :)\n. The cake is a lie :( According to [this mail list](http://mail.openjdk.java.net/pipermail/nio-discuss/2011-July/000577.html), there is no true async File IO in [JDK7](http://hg.openjdk.java.net/jdk7u/jdk7u/jdk/file/eeb571eea68d/src/share/classes/sun/nio/ch/SimpleAsynchronousFileChannelImpl.java#l289) or [JDK8](http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/share/classes/sun/nio/ch/SimpleAsynchronousFileChannelImpl.java#l289). It is all executed as  blocking IO on the given ExecutorService. The danger is, that akka does not even notice the potential thread consumption. It might be better do have a dedicated akka dispatcher that does the syncronous file io. Or we go [native](http://docs.jboss.org/hornetq/2.2.5.Final/user-manual/en/html/persistence.html)\n. How I hotfixed it: https://github.com/danielwegener/spray/commit/c495623497f9d6c41552f1546e7f9d9237163a11\nMight be a bit to naive :)\n. I am especially unhappy with the following things:\n- Marshallers/Umarshallers are almost perfect copies of multipart/chunked marshallers/unmarshallers. Maybe we could extract their commons.\n- IMO _RangeDirectives_ looks like imperative spaghetti code. Could you maybe give me a hint to make it more scalarific?\n- Documentation files are missing.\n- The _\"HttpHeaderParser prime an empty parser with all defined HeaderValueParsers\"_-Test is hard to fix without changing what it should ensure originally :)\n. I just fixed the tests. I am not really sure what the were supposed to ensure because the asserts seemed to have no direct meaning but \"the result is equal to an earlier run of the test\".\n\nIs there any chance this PR will find a way to master soon or is there still a 'become akka-http first' policy? :)\n. Hi Mathias!\nGreat news! Thanks for your time and feedback. I will do my homework and report back asap.\n. Green again - I guess I implemented all your suggestions. What will be the next step - squash + rebase?\n. Hi Mathias. Your decision - I'll be fine :) (still got some spare time this week if I can help out)\n. Yes, LGTM.\nI wrote the original implementation against version 24 of the HTTPbis spec so I'll check the [diff](http://www.ietf.org/rfcdiff?url1=draft-ietf-httpbis-p5-range-24&url2=draft-ietf-httpbis-p5-range-26) this evening. \n. Okay, so the only non-formal change seem to be in _3.1.  Range_ where `other-range-set = 1*CHAR` becomes `other-range-set = 1*VCHAR`. Since we do support nothing else but byte-ranges this does not really matter. ~~I noticed a subtile glitch in the definition of a 'CHAR'. draft-ietf-httpbis-p5-range-26 uses BNF-Rules from rfc5234. The CHAR definition there allows any US-ASCII-Char but 'NUL' (B1 Core Rules) while the HTTP1.1 (rfc2616) allows anything (inclusive 'NUL'). So if we want to be utra-spec-compliant ... :)~~ (nevermind, used only in `other-range-resp` anyway)\n\n(I really hope i can come to Berlin, but I am already visiting the #reactconf next month - so since I dont have my own talks, I think I have to take a vacation for #scaladays ;) )\n. @sirthias We maybe can make this beer thing up this year in Amsterdam :)\n. ",
    "lvicentesanchez": "OK. I will do it as soon as I get home today\nOn 15 May 2013 13:54, \"Mathias\" notifications@github.com wrote:\n\n> Sorry for the delay!\n> Could you update this PR to the latest master?\n> I have just merged in support for json4s and improved the tests. It should\n> now be even simpler for you to integrate the support for play-json.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/263#issuecomment-17936447\n> .\n. The first Play 2.2.0 release candidate has been released. I've modified this PR to use typesafe's repository. I have been merging all changes in master once-twice a week so it's up-to-date.\n. Play 2.2.0 has been released, dependencies have been updated.\n. I will sign it and send it to you later this evening when I arrive home; in the meantime, I will change the commit message.\n. CLA signed and sent.\n. Shapeless 2.0 has been released:\n\nhttps://github.com/milessabin/shapeless\n. Hum... ok, now I see what I was missing on my try to upgrade to shapeless 2.0, mainly I wasn't extending LowerPriorityFieldDefMagnetAux and also I had a conflict between forTuple and forHList... that's why I had some implicit resolution conflicts. \n. OK. I will do it as soon as I get home today\nOn 15 May 2013 13:54, \"Mathias\" notifications@github.com wrote:\n\n> Sorry for the delay!\n> Could you update this PR to the latest master?\n> I have just merged in support for json4s and improved the tests. It should\n> now be even simpler for you to integrate the support for play-json.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/263#issuecomment-17936447\n> .\n. The first Play 2.2.0 release candidate has been released. I've modified this PR to use typesafe's repository. I have been merging all changes in master once-twice a week so it's up-to-date.\n. Play 2.2.0 has been released, dependencies have been updated.\n. I will sign it and send it to you later this evening when I arrive home; in the meantime, I will change the commit message.\n. CLA signed and sent.\n. Shapeless 2.0 has been released:\n\nhttps://github.com/milessabin/shapeless\n. Hum... ok, now I see what I was missing on my try to upgrade to shapeless 2.0, mainly I wasn't extending LowerPriorityFieldDefMagnetAux and also I had a conflict between forTuple and forHList... that's why I had some implicit resolution conflicts. \n. ",
    "stanch": "Could we expect this to be in the nightlies soon? :)\n. Cool!\n. Looks great, thanks :)\n. Update: here is an updated version with a shortcut for `PathMatcher1`, which avoids dealing with `List[... :: HNil]`.\n\n``` scala\nimplicit class RepeatingPathMatcher[L <: HList](pm: PathMatcher[L]) {\n  def repeat = new PathMatcher1[List[L]] {\n    def apply(path: Path): Matching[List[L] :: HNil] = pm(path) match {\n      case Matched(rest, extractions) \u21d2\n        val Matched(p, l :: HNil) = this(rest)\n        Matched(p, (extractions :: l) :: HNil)\n      case Unmatched \u21d2\n        Matched(path, Nil :: HNil)\n    }\n  }\n}\nimplicit class RepeatingPathMatcher1[T](pm: PathMatcher1[T]) {\n  def repeat = new PathMatcher1[List[T]] {\n    def apply(path: Path): Matching[List[T] :: HNil] = pm(path) match {\n      case Matched(rest, extractions) \u21d2\n        val Matched(p, l :: HNil) = this(rest)\n        Matched(p, (extractions.head :: l) :: HNil)\n      case Unmatched \u21d2\n        Matched(path, Nil :: HNil)\n    }\n  }\n}\n```\n. Thanks, this is terrific!\n. On a second thought, I foresee that it will be often required to have at least one repetition, hence my `IntNumber ~ (\"|\" ~ IntNumber).repeat`. Here however we lose the utility of the `separator` argument. Would it be possible to add `nonEmpty` or `oneOrMore` flag? Or maybe have two separate methods, like in _parboiled_?\n. That would be nice! By the way, the commit message, as well as the changelog note are misleading: the actual method is called `repeat`, not `repeated`.\n. Could we expect this to be in the nightlies soon? :)\n. Cool!\n. Looks great, thanks :)\n. Update: here is an updated version with a shortcut for `PathMatcher1`, which avoids dealing with `List[... :: HNil]`.\n\n``` scala\nimplicit class RepeatingPathMatcher[L <: HList](pm: PathMatcher[L]) {\n  def repeat = new PathMatcher1[List[L]] {\n    def apply(path: Path): Matching[List[L] :: HNil] = pm(path) match {\n      case Matched(rest, extractions) \u21d2\n        val Matched(p, l :: HNil) = this(rest)\n        Matched(p, (extractions :: l) :: HNil)\n      case Unmatched \u21d2\n        Matched(path, Nil :: HNil)\n    }\n  }\n}\nimplicit class RepeatingPathMatcher1[T](pm: PathMatcher1[T]) {\n  def repeat = new PathMatcher1[List[T]] {\n    def apply(path: Path): Matching[List[T] :: HNil] = pm(path) match {\n      case Matched(rest, extractions) \u21d2\n        val Matched(p, l :: HNil) = this(rest)\n        Matched(p, (extractions.head :: l) :: HNil)\n      case Unmatched \u21d2\n        Matched(path, Nil :: HNil)\n    }\n  }\n}\n```\n. Thanks, this is terrific!\n. On a second thought, I foresee that it will be often required to have at least one repetition, hence my `IntNumber ~ (\"|\" ~ IntNumber).repeat`. Here however we lose the utility of the `separator` argument. Would it be possible to add `nonEmpty` or `oneOrMore` flag? Or maybe have two separate methods, like in _parboiled_?\n. That would be nice! By the way, the commit message, as well as the changelog note are misleading: the actual method is called `repeat`, not `repeated`.\n. ",
    "mighdoll": "+1, this will be helpful for diagnosing failures at scale\n. Glad to help and glad to practice the contribution process.\n\nI'll update the patch and do the CLA tonight (i.e. your tomorrow am).\n\nOn Tue, Aug 20, 2013 at 8:37 AM, Mathias notifications@github.com wrote:\n\n> Thanks, Lee, for this patch.\n> We just discussed this issue and would like to follow your lead and, with\n> regard to the _spray-testkit_ module, make an exception to our rule to\n> only depend on Akka with provided scope.\n> \n> However, in order to be able to merge your patch, we'd need you to\n> - sign our CLAhttp://spray.io/project-info/contributing/#contributor-license-agreement-cla\n> - and change the commit message to = sbt: change spray-testkit\n>   dependency on akka-testkit from 'provided' to 'compile' in accordance\n>   with our commit message policyhttp://spray.io/project-info/contributing/#git-commit-messages\n> \n> Alternatively (to make things easier for you) we could also apply the\n> patch ourselves. Let us know what you'd prefer.\n> Thanks again for bringing this up!\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/447#issuecomment-22954022\n> .\n. +1, this will be helpful for diagnosing failures at scale\n. Glad to help and glad to practice the contribution process.\n\nI'll update the patch and do the CLA tonight (i.e. your tomorrow am).\n\nOn Tue, Aug 20, 2013 at 8:37 AM, Mathias notifications@github.com wrote:\n\n> Thanks, Lee, for this patch.\n> We just discussed this issue and would like to follow your lead and, with\n> regard to the _spray-testkit_ module, make an exception to our rule to\n> only depend on Akka with provided scope.\n> \n> However, in order to be able to merge your patch, we'd need you to\n> - sign our CLAhttp://spray.io/project-info/contributing/#contributor-license-agreement-cla\n> - and change the commit message to = sbt: change spray-testkit\n>   dependency on akka-testkit from 'provided' to 'compile' in accordance\n>   with our commit message policyhttp://spray.io/project-info/contributing/#git-commit-messages\n> \n> Alternatively (to make things easier for you) we could also apply the\n> patch ourselves. Let us know what you'd prefer.\n> Thanks again for bringing this up!\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/447#issuecomment-22954022\n> .\n. ",
    "gmalouf": "Guys, does this address allowing for curly braces in the query string as mentioned in the earlier user group posting: \nhttps://groups.google.com/forum/#!searchin/spray-user/curly$20braces/spray-user/Uu5JnKvGW3s/xM-no0Cz-qoJ\n\nIf so, I am very much looking forward to leveraging this.\n. Thanks Mathias, I appreciate it!\n\nOn Thu, Apr 25, 2013 at 5:29 PM, Mathias notifications@github.com wrote:\n\n> Yes, it does. Sorry for the delay with merging...\n> I was awfully busy this week.\n> We'll definitely get this in next week.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/272#issuecomment-17041543\n> .\n. That would, bit for simple cases like including a $ as mentioned above it\nwould be good to still have the application parse it for you.  One\ncharacter extra seems like it should be supported in a user friendly manner.\nOn Apr 29, 2013 7:27 AM, \"Mathias\" notifications@github.com wrote:\n\n> What about a config setting which effectively disables query parsing\n> altogether?\n> So instead of a parsed query structure you simply get one single raw query\n> string, which you can then subsequently parse yourself. spray would simply\n> treat all characters after the ? and before a potential # as belonging to\n> the query string...\n> Wouldn't that solve the issue and still keep our architecture clean and\n> lean without adding to much application-specific special-case-handling?\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/272#issuecomment-17161455\n> .\n. In the Http DateTime object, should the  `clicks` field itself be changed to nanoseconds, a new field to completely replace this one be created, or leave it as milliseconds?\n. Thanks for the reminder - made me go back and re-read the javadocs.\n\nAfter searching the code base for anything using 'milliseconds', I've made most if not all of my changes where timeouts were being kept.  In general, I chose to rename the 'timestamp' variables to 'nanoTime' to make it explicitly clear which units are being used.\n\nOne thing I was reminded of while reading the javadocs is that nano time can be negative.  So far, I held off on refactoring OpenRequest.scala and any dependent classes to use nanoseconds until the cleanest way to replace the negative timestamp logic is figured out.  Do you have any strong opinions on this or would you rather me just submit a pull request with my approach for your review?\n. I can confirm that it is on the server side after testing this on my browser.  I'm not sure it is related to #298 as the reverse seems to be happening in this case.  That is, the second parameter value rather than being decoded is encoded a second time it seems.\n\nWhen you take away the &li=${CPG_ID} chunk, the CP_ID is properly decoded on the server side.\n. Sure, I will give it a shot today.  It looks like it should.\n\nOn Thu, May 23, 2013 at 4:02 AM, Mathias notifications@github.com wrote:\n\n> Gary, I just merged Mikes fix for issue #298https://github.com/spray/spray/issues/298,\n> which I think also fixes this issues.\n> Can you try with todays nightly build? (just triggered it: 1.1-20130523)\n> Closing for now...\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/issues/302#issuecomment-18328845\n> .\n. I can confirm that I no longer see this issue as of today's nightly build.  Thanks!\n. ```\npath(\"ck\") {\n          optionalCookie(OptOutCookieName) {\n            (optOut: Option[HttpCookie]) =>\n              optionalCookie(mxCookieName) {\n                (currCookie: Option[HttpCookie]) =>\n                  parameterMap {\n                    params =>\n                          redirect(generateLandingPageUrl(params), redirectionType = Found) \n                  }\n              }\n          }\n```\n\nThe generateLandingPageUrl function returns a string representing the url to redirect to including encoded ones in the query string for redirects; I have attempted tried out encoding strategies in this function to try and get spray to properly handle it.\n\nThree urls for example are being used:\n\na) http://testak.com/pixel/2167/?che=${cb}&col=${li},${cai},${pli},${cai},${cri}&l1=\nb) http://testberl.com/contact/bct?pid=287ff475-a44c8866523&_ct=pixel&adid=${cai}&action=2&REDIR=\nc) http://myurl.com\n\n1) http://testak.com/pixel/2167/?che=CB_TEST&col=LI_TEST,CAI_TEST,PLI_TEST,CAI_TEST,CRI_TEST&l1=http%3A%2F%2Ftestberl.com%2Fcontact%2Fbct%3Fpid%3D287ff475-a44c8866523%26_ct%3Dpixel%26adid%3DCAI_TEST%26action%3D2%26REDIR%3Dhttp%253A%252F%252Fmyurl.com\n\nThis function single encodes c), double encodes b) and appends the three together to create what I believe is the desired url for the client to see.  The client receiving the 302 would redirect to a) who themselves would simply decode the &l1 parameter and return a 302 to go to b) and so on.\n\n2) http://testak.com/pixel/2167/?che=CB_TEST&col=LI_TEST,CAI_TEST,PLI_TEST,CAI_TEST,CRI_TEST&l1=http://testberl.com/contact/bct?pid=287ff475-a44c-4ccf-bce3-a7d048866523&_ct=pixel&adid=CAI_TEST&action=2&REDIR=http%3A%2F%2Fmyurl.com\n\nSince I assumed spray wants to automatically encode my query string, I then decided to try not encoding what is directly in the &l1 parameter and only what is redirected to at the lowest level as I wanted that to be double-encoded.  The goal was to feed this into the HTTP response which would generate the url I showed in 1) - this did not work.\n\nLet me know if more clarification is needed.\n. Just want to comment that the latest nightly build (05232013) fixed the issue completely.  I tested both the nightly build for 05/23 as well as the latest code built locally from master - both did not have the issue.  \n. I think this issue should be looked at again.  It was really nice being able to use spray-http without depending on Akka and other I/O libraries.  Is there an easy fix for the next version to make this possible?  It's a similar concept to how spray-json does not have these dependencies.\n\nOur team had been using it from within Storm, so had no need for Akka actors there.  \n. I'm very sorry to hear that, as having an independent HTTP model was\nsomething talked about proudly in past documentation.\n\nOur use case is that we have Storm to handle streaming data (originally\nfrom Spray web servers) and do our more detailed parsing at this level.  We\nwant to use a library with minimal transitive dependencies as we have a\nlarge tree of them to manage.\n\nIf this is the state of things now, we'll consider using an alternative.\n There is a lot of value to developers in being able to produce libraries\nwith minimal transitive dependencies.\n\nOn Fri, Mar 28, 2014 at 11:52 AM, Mathias notifications@github.com wrote:\n\n> Since we'll move into akka soon completely I don't think we'll reopen this\n> issue.\n> Having an HTTP model that doesn't depend on akka is not a priority for us.\n> What's your use case again?\n> \n> ## \n> \n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/issues/784#issuecomment-38934632\n> .\n. Guys, does this address allowing for curly braces in the query string as mentioned in the earlier user group posting: \nhttps://groups.google.com/forum/#!searchin/spray-user/curly$20braces/spray-user/Uu5JnKvGW3s/xM-no0Cz-qoJ\n\nIf so, I am very much looking forward to leveraging this.\n. Thanks Mathias, I appreciate it!\n\nOn Thu, Apr 25, 2013 at 5:29 PM, Mathias notifications@github.com wrote:\n\n> Yes, it does. Sorry for the delay with merging...\n> I was awfully busy this week.\n> We'll definitely get this in next week.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/272#issuecomment-17041543\n> .\n. That would, bit for simple cases like including a $ as mentioned above it\nwould be good to still have the application parse it for you.  One\ncharacter extra seems like it should be supported in a user friendly manner.\nOn Apr 29, 2013 7:27 AM, \"Mathias\" notifications@github.com wrote:\n\n> What about a config setting which effectively disables query parsing\n> altogether?\n> So instead of a parsed query structure you simply get one single raw query\n> string, which you can then subsequently parse yourself. spray would simply\n> treat all characters after the ? and before a potential # as belonging to\n> the query string...\n> Wouldn't that solve the issue and still keep our architecture clean and\n> lean without adding to much application-specific special-case-handling?\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/272#issuecomment-17161455\n> .\n. In the Http DateTime object, should the  `clicks` field itself be changed to nanoseconds, a new field to completely replace this one be created, or leave it as milliseconds?\n. Thanks for the reminder - made me go back and re-read the javadocs.\n\nAfter searching the code base for anything using 'milliseconds', I've made most if not all of my changes where timeouts were being kept.  In general, I chose to rename the 'timestamp' variables to 'nanoTime' to make it explicitly clear which units are being used.\n\nOne thing I was reminded of while reading the javadocs is that nano time can be negative.  So far, I held off on refactoring OpenRequest.scala and any dependent classes to use nanoseconds until the cleanest way to replace the negative timestamp logic is figured out.  Do you have any strong opinions on this or would you rather me just submit a pull request with my approach for your review?\n. I can confirm that it is on the server side after testing this on my browser.  I'm not sure it is related to #298 as the reverse seems to be happening in this case.  That is, the second parameter value rather than being decoded is encoded a second time it seems.\n\nWhen you take away the &li=${CPG_ID} chunk, the CP_ID is properly decoded on the server side.\n. Sure, I will give it a shot today.  It looks like it should.\n\nOn Thu, May 23, 2013 at 4:02 AM, Mathias notifications@github.com wrote:\n\n> Gary, I just merged Mikes fix for issue #298https://github.com/spray/spray/issues/298,\n> which I think also fixes this issues.\n> Can you try with todays nightly build? (just triggered it: 1.1-20130523)\n> Closing for now...\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/issues/302#issuecomment-18328845\n> .\n. I can confirm that I no longer see this issue as of today's nightly build.  Thanks!\n. ```\npath(\"ck\") {\n          optionalCookie(OptOutCookieName) {\n            (optOut: Option[HttpCookie]) =>\n              optionalCookie(mxCookieName) {\n                (currCookie: Option[HttpCookie]) =>\n                  parameterMap {\n                    params =>\n                          redirect(generateLandingPageUrl(params), redirectionType = Found) \n                  }\n              }\n          }\n```\n\nThe generateLandingPageUrl function returns a string representing the url to redirect to including encoded ones in the query string for redirects; I have attempted tried out encoding strategies in this function to try and get spray to properly handle it.\n\nThree urls for example are being used:\n\na) http://testak.com/pixel/2167/?che=${cb}&col=${li},${cai},${pli},${cai},${cri}&l1=\nb) http://testberl.com/contact/bct?pid=287ff475-a44c8866523&_ct=pixel&adid=${cai}&action=2&REDIR=\nc) http://myurl.com\n\n1) http://testak.com/pixel/2167/?che=CB_TEST&col=LI_TEST,CAI_TEST,PLI_TEST,CAI_TEST,CRI_TEST&l1=http%3A%2F%2Ftestberl.com%2Fcontact%2Fbct%3Fpid%3D287ff475-a44c8866523%26_ct%3Dpixel%26adid%3DCAI_TEST%26action%3D2%26REDIR%3Dhttp%253A%252F%252Fmyurl.com\n\nThis function single encodes c), double encodes b) and appends the three together to create what I believe is the desired url for the client to see.  The client receiving the 302 would redirect to a) who themselves would simply decode the &l1 parameter and return a 302 to go to b) and so on.\n\n2) http://testak.com/pixel/2167/?che=CB_TEST&col=LI_TEST,CAI_TEST,PLI_TEST,CAI_TEST,CRI_TEST&l1=http://testberl.com/contact/bct?pid=287ff475-a44c-4ccf-bce3-a7d048866523&_ct=pixel&adid=CAI_TEST&action=2&REDIR=http%3A%2F%2Fmyurl.com\n\nSince I assumed spray wants to automatically encode my query string, I then decided to try not encoding what is directly in the &l1 parameter and only what is redirected to at the lowest level as I wanted that to be double-encoded.  The goal was to feed this into the HTTP response which would generate the url I showed in 1) - this did not work.\n\nLet me know if more clarification is needed.\n. Just want to comment that the latest nightly build (05232013) fixed the issue completely.  I tested both the nightly build for 05/23 as well as the latest code built locally from master - both did not have the issue.  \n. I think this issue should be looked at again.  It was really nice being able to use spray-http without depending on Akka and other I/O libraries.  Is there an easy fix for the next version to make this possible?  It's a similar concept to how spray-json does not have these dependencies.\n\nOur team had been using it from within Storm, so had no need for Akka actors there.  \n. I'm very sorry to hear that, as having an independent HTTP model was\nsomething talked about proudly in past documentation.\n\nOur use case is that we have Storm to handle streaming data (originally\nfrom Spray web servers) and do our more detailed parsing at this level.  We\nwant to use a library with minimal transitive dependencies as we have a\nlarge tree of them to manage.\n\nIf this is the state of things now, we'll consider using an alternative.\n There is a lot of value to developers in being able to produce libraries\nwith minimal transitive dependencies.\n\nOn Fri, Mar 28, 2014 at 11:52 AM, Mathias notifications@github.com wrote:\n\n> Since we'll move into akka soon completely I don't think we'll reopen this\n> issue.\n> Having an HTTP model that doesn't depend on akka is not a priority for us.\n> What's your use case again?\n> \n> ## \n> \n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/issues/784#issuecomment-38934632\n> .\n. ",
    "bergjeb": "Will this allow for the equal sign to be present in the value of a query parameter or does it only address non-separators? For example, will a query string of \"?key=a&key2=b=c&key3=d\" be allowed?\n. Thanks for the quick response nkvoll. Unfortunately its something I need to support so I'll start looking at other ways to accomplish it (I am unable to change all the clients generating the requests.)\n. Some way to hook in and define our own parser would allow some great flexibility.\n\nIn this particular case, I'm just interested in the encoded raw query, so all the default parsing is just unneeded overhead.  \n. sirthias - That config setting to bypass query parsing altogether would be perfect for my case.\n. Will this allow for the equal sign to be present in the value of a query parameter or does it only address non-separators? For example, will a query string of \"?key=a&key2=b=c&key3=d\" be allowed?\n. Thanks for the quick response nkvoll. Unfortunately its something I need to support so I'll start looking at other ways to accomplish it (I am unable to change all the clients generating the requests.)\n. Some way to hook in and define our own parser would allow some great flexibility.\n\nIn this particular case, I'm just interested in the encoded raw query, so all the default parsing is just unneeded overhead.  \n. sirthias - That config setting to bypass query parsing altogether would be perfect for my case.\n. ",
    "aloiscochard": "Very cool to see this coming! will prevent lot of mistake :-)\n\nGreat stuff\n. Well, I think the right approach is to have correct typing that avoid that `~` problem...\n\nUsing macro for this kind of things is imo not a good idea.\n. Right, misunderstood the issue, my bad.\n\nA shame there is no way to enforce usage of parens :-( \n. `View` is not a case class, it's an algebraic data type.\n\nSo how would you specify to call \"fromString\" on the companion then?\n. Oh... adding implicit in scope?\n\nsounds a bit heavyweight to me, but I could live with it :)\n\nStill there is an inference issue there, no?\n. Woow, so implictly[A => B] will look into B companion, i thought it was only working for B[T].\n\nThat's great, thank you Mathias :-)\n. Very cool to see this coming! will prevent lot of mistake :-)\n\nGreat stuff\n. Well, I think the right approach is to have correct typing that avoid that `~` problem...\n\nUsing macro for this kind of things is imo not a good idea.\n. Right, misunderstood the issue, my bad.\n\nA shame there is no way to enforce usage of parens :-( \n. `View` is not a case class, it's an algebraic data type.\n\nSo how would you specify to call \"fromString\" on the companion then?\n. Oh... adding implicit in scope?\n\nsounds a bit heavyweight to me, but I could live with it :)\n\nStill there is an inference issue there, no?\n. Woow, so implictly[A => B] will look into B companion, i thought it was only working for B[T].\n\nThat's great, thank you Mathias :-)\n. ",
    "doxxx": "The other mistake I make when composing routes is forgetting a \"~\" operator between two path elements, which happily compiles but the first half of my routes \"mysteriously\" don't work because they are essentially discarded. It's _hard_ to spot this mistake in the source code.\n. I should add that I'm using a snapshot: spray-1.2-20130612\n. The other mistake I make when composing routes is forgetting a \"~\" operator between two path elements, which happily compiles but the first half of my routes \"mysteriously\" don't work because they are essentially discarded. It's _hard_ to spot this mistake in the source code.\n. I should add that I'm using a snapshot: spray-1.2-20130612\n. ",
    "hyysguyang": "Awesome, thanks very much.\n. Awesome, thanks very much.\n. ",
    "mpilquist": "+1 for server side auto-chunking. I need to support large file upload and stream data to disk. If client doesn't send a chunked request, I want to auto-chunk to avoid heap allocating the entire uploaded file.\n. Follow up question - will incoming-auto-chunking-threshold-size be supported by spray-servlet?\n. Re: spray-servlet, if auto-chunking will never be supported, then I suppose the simplest alternative is to fall back to a manually written servlet for the resources that require auto-chunking. That's certainly possible but not particularly elegant.\n\n> The only way for us to get to the request entity is via the httpServletRequest.getInputStream, which is a blocking  abstraction. So I think it's hard for us to send a \"virtual chunk\" up to the application whenever a bunch of new bytes  have arrived on the connection because we simply cannot know if and when this is the case.\n> All we see on the inputStream is EOF, which signals that we have read the complete request.\n\nI was thinking that if the Content-Length was over the auto-chunking threshold, the connector servlet could spawn a request specific actor or future that pulled from the InputStream and generated chunks. Somewhat risky given that the blocking on the input stream could lead to starvation, but perhaps better than no support at all?\n. +1. Do you think this will make 1.1/1.2 final?\n. Hi Mathias,\n\nI removed the explicit passing of sslEngineProvider and the bug reoccurred. Turns out this is due to the import of normalizedSettings._ in HttpHostConnector. The normalizedSettings.sslEngineProvider has the default ClientSSLEngineProvider and hence, overrides the implicit ClientSSLEngineProvider in the HttpHostConnector constructor. Excluding the import of normalizedSettings.sslEngineProvider fixes.\n\nI'd be happy to add a test. Could you point me to the best place to add one?\n\nThanks,\nMichael\n. Just sent CLA via email and pushed an amended commit message.\n. Re: akka.io -- the export/import is correct for 1.0/1.1 -- it is a standard OSGi convention for a bundle to import the packages that it exports. We have to customize the version range on it though in order to override the akka.\\* version range -- that is, we don't want akka.io to be imported as [2.1,2.2) in Spray 1.1.\n\nRe: akka.io in spray 1.2 -- I assumed we'd just delete the akka.io overrides when merging to 1.2.  I'd be happy to do that.\n\nRe: testing -- manual testing is fairly easy using pax-runner (https://ops4j1.jira.com/wiki/display/paxrunner/Pax+Runner). Automated testing is somewhat more involved to configure and set up.\n\nManual testing with pax-exam can be accomplished by first creating a config file with these contents (and appropriately adjusting paths):\n\n```\nmvn:org.scala-lang/scala-library/2.10.2\nmvn:org.scala-lang/scala-reflect/2.10.2\nmvn:com.typesafe/config/1.0.0\nmvn:com.typesafe.akka/akka-actor_2.10/2.1.2\n\nmvn:org.parboiled/parboiled-core/1.1.5\nmvn:org.parboiled/parboiled-scala_2.10/1.1.5\nfile:///Users/mpilquist/.ivy2/local/io.spray/spray-json_2.10/1.2.5/bundles/spray-json_2.10.jar@update\n\nfile:///Users/mpilquist/.ivy2/local/io.spray/spray-util/1.1-SNAPSHOT/bundles/spray-util.jar@update\nfile:///Users/mpilquist/.ivy2/local/io.spray/spray-io/1.1-SNAPSHOT/bundles/spray-io.jar@update\n\nfile:///Users/mpilquist/.ivy2/local/io.spray/spray-caching/1.1-SNAPSHOT/bundles/spray-caching.jar@update\nmvn:com.googlecode.concurrentlinkedhashmap/concurrentlinkedhashmap-lru/1.3.2\n\nfile:///Users/mpilquist/.ivy2/local/io.spray/spray-http/1.1-SNAPSHOT/bundles/spray-http.jar@update\nfile:///Users/mpilquist/.ivy2/local/io.spray/spray-httpx/1.1-SNAPSHOT/bundles/spray-httpx.jar@update\nmvn:org.jvnet.mimepull/mimepull/1.9.3\n\nfile:///Users/mpilquist/.ivy2/local/io.spray/spray-can/1.1-SNAPSHOT/bundles/spray-can.jar@update\nfile:///Users/mpilquist/.ivy2/local/io.spray/spray-client/1.1-SNAPSHOT/bundles/spray-client.jar@update\nfile:///Users/mpilquist/.ivy2/local/io.spray/spray-routing/1.1-SNAPSHOT/bundles/spray-routing.jar@update\nfile:///Users/mpilquist/.ivy2/local/io.spray/spray-servlet/1.1-SNAPSHOT/bundles/spray-servlet.jar@update\nmvn:javax.servlet/javax.servlet-api/3.0.1\n```\n\nThen to run: `pax-run.sh file:spray.conf --log=debug --systemPackages=sun.misc`\n\nThat will start Apache Felix and load all of the spray jars and their dependencies. If there are any errors due to misconfigured OSGi settings, they are printed to the console. Further, typing `bundles` at the prompt will show if all bundles resolve:\n\n```\n____________________________\nWelcome to Apache Felix Gogo\n\ng! bundles\n    0|Active     |    0|org.apache.felix.framework (4.0.3)\n    1|Active     |    5|org.scala-lang.scala-library (2.10.2.v20130530-074427-VFINAL-60d462ef6e)\n    2|Active     |    5|org.scala-lang.scala-reflect (2.10.2.v20130530-074427-VFINAL-60d462ef6e)\n    3|Active     |    5|com.typesafe.config (1.0.0)\n    4|Active     |    5|com.typesafe.akka.actor (2.1.2)\n    5|Active     |    5|org.parboiled.core (1.1.5)\n    6|Active     |    5|org.parboiled.scala (1.1.5)\n    7|Active     |    5|io.spray.json (1.2.5)\n    8|Active     |    5|io.spray.util (1.1.0.SNAPSHOT)\n    9|Active     |    5|io.spray.io (1.1.0.SNAPSHOT)\n   10|Active     |    5|io.spray.caching (1.1.0.SNAPSHOT)\n   11|Active     |    5|com.googlecode.concurrentlinkedhashmap.lru (1.3.2)\n   12|Active     |    5|io.spray.http (1.1.0.SNAPSHOT)\n   13|Active     |    5|io.spray.httpx (1.1.0.SNAPSHOT)\n   14|Active     |    5|org.jvnet.mimepull (1.9.3)\n   15|Active     |    5|io.spray.can (1.1.0.SNAPSHOT)\n   16|Active     |    5|io.spray.client (1.1.0.SNAPSHOT)\n   17|Active     |    5|io.spray.routing (1.1.0.SNAPSHOT)\n   18|Active     |    5|io.spray.servlet (1.1.0.SNAPSHOT)\n   19|Active     |    5|javax.servlet-api (3.0.1)\n   20|Active     |    1|org.apache.felix.gogo.command (0.12.0)\n   21|Active     |    1|org.apache.felix.gogo.runtime (0.10.0)\n   22|Active     |    1|org.apache.felix.gogo.shell (0.10.0)\n```\n\nAutomated testing can be accomplished with pax-exam (https://ops4j1.jira.com/wiki/display/PAXEXAM3/Pax+Exam). The scala-library, akka, and ScalaTest all use pax-exam to test their OSGi headers. If interested, I can put together a pax-exam test. I didn't know how much overlap there would be with Brian's efforts.\n. > Does that check that classes in the bundles only reference packages as declared by their metadata?\n\nAnswer is somewhat complicated. The OSGi bundles will resolve successfully if all of the mandatory imported packages can be wired to providers of those packages successfully. So it is certainly possible for a bundle to resolve but then not link during runtime. Some common causes of runtime linkage errors in OSGi environments:\n # Missing import package statement\n # Optionally imported package not wired but code linked that depends on it\n # Dynamic class loading via Class.forName\n\nThe risk of the first case occurring is mitigated by using BND to generate the `Import-Package` header. BND scans the classes at build time and ensures all statically referenced classes are accounted for in the generated header.\n\nThe second case is similar to standard Java optional dependencies. E.g., if spray-json exists and is wired to spray-httpx, then the user can safely use types that have static references on spray-json types.\n\nIn the final case, the import package header must be manually augmented to deal with dynamic class loading, or dynamic class loading should use the ThreadContextClassLoader, depending on the situation.\n. > Shouldn't spray-json be optional there as well?\n\nYep, will update.\n. One issue I've run in to in using these OSGi enabled JARs is that the various settings classes that can load their config from an actor system or config object are unusable in an OSGi environment.  For example, calling `HostConnectorSettings(system)` results in an exception:\n\n```\ncom.typesafe.config.ConfigException$UnresolvedSubstitution: reference.conf: 149: Could not resolve substitution to a value: ${spray.version}\n        at com.typesafe.config.impl.ConfigReference.resolveSubstitutions(ConfigReference.java:84)\n        at com.typesafe.config.impl.ResolveSource.resolveCheckingReplacement(ResolveSource.java:110)\n        at com.typesafe.config.impl.ResolveContext.resolve(ResolveContext.java:114)\n        at com.typesafe.config.impl.ConfigConcatenation.resolveSubstitutions(ConfigConcatenation.java:178)\n        at com.typesafe.config.impl.ResolveSource.resolveCheckingReplacement(ResolveSource.java:110)\n        at com.typesafe.config.impl.ResolveContext.resolve(ResolveContext.java:114)\n        at com.typesafe.config.impl.SimpleConfigObject$1.modifyChildMayThrow(SimpleConfigObject.java:340)\n        at com.typesafe.config.impl.SimpleConfigObject.modifyMayThrow(SimpleConfigObject.java:279)\n        at com.typesafe.config.impl.SimpleConfigObject.resolveSubstitutions(SimpleConfigObject.java:320)\n        at com.typesafe.config.impl.SimpleConfigObject.resolveSubstitutions(SimpleConfigObject.java:24)\n        at com.typesafe.config.impl.ResolveSource.resolveCheckingReplacement(ResolveSource.java:110)\n        at com.typesafe.config.impl.ResolveContext.resolve(ResolveContext.java:114)\n        at com.typesafe.config.impl.SimpleConfigObject$1.modifyChildMayThrow(SimpleConfigObject.java:340)\n        at com.typesafe.config.impl.SimpleConfigObject.modifyMayThrow(SimpleConfigObject.java:279)\n        at com.typesafe.config.impl.SimpleConfigObject.resolveSubstitutions(SimpleConfigObject.java:320)\n        at com.typesafe.config.impl.SimpleConfigObject.resolveSubstitutions(SimpleConfigObject.java:24)\n        at com.typesafe.config.impl.ResolveSource.resolveCheckingReplacement(ResolveSource.java:110)\n        at com.typesafe.config.impl.ResolveContext.resolve(ResolveContext.java:114)\n        at com.typesafe.config.impl.SimpleConfigObject$1.modifyChildMayThrow(SimpleConfigObject.java:340)\n        at com.typesafe.config.impl.SimpleConfigObject.modifyMayThrow(SimpleConfigObject.java:279)\n        at com.typesafe.config.impl.SimpleConfigObject.resolveSubstitutions(SimpleConfigObject.java:320)\n        at com.typesafe.config.impl.SimpleConfigObject.resolveSubstitutions(SimpleConfigObject.java:24)\n        at com.typesafe.config.impl.ResolveSource.resolveCheckingReplacement(ResolveSource.java:110)\n        at com.typesafe.config.impl.ResolveContext.resolve(ResolveContext.java:114)\n        at com.typesafe.config.impl.SimpleConfigObject$1.modifyChildMayThrow(SimpleConfigObject.java:340)\n        at com.typesafe.config.impl.SimpleConfigObject.modifyMayThrow(SimpleConfigObject.java:279)\n        at com.typesafe.config.impl.SimpleConfigObject.resolveSubstitutions(SimpleConfigObject.java:320)\n        at com.typesafe.config.impl.SimpleConfigObject.resolveSubstitutions(SimpleConfigObject.java:24)\n        at com.typesafe.config.impl.ResolveSource.resolveCheckingReplacement(ResolveSource.java:110)\n        at com.typesafe.config.impl.ResolveContext.resolve(ResolveContext.java:114)\n        at com.typesafe.config.impl.ResolveContext.resolve(ResolveContext.java:147)\n        at com.typesafe.config.impl.ResolveContext.resolve(ResolveContext.java:157)\n        at com.typesafe.config.impl.SimpleConfig.resolve(SimpleConfig.java:60)\n        at com.typesafe.config.impl.SimpleConfig.resolve(SimpleConfig.java:55)\n        at com.typesafe.config.impl.SimpleConfig.resolve(SimpleConfig.java:33)\n        at com.typesafe.config.impl.ConfigImpl$1.call(ConfigImpl.java:372)\n        at com.typesafe.config.impl.ConfigImpl$1.call(ConfigImpl.java:365)\n        at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:58)\n        at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:86)\n        at com.typesafe.config.impl.ConfigImpl.defaultReference(ConfigImpl.java:365)\n        at com.typesafe.config.ConfigFactory.defaultReference(ConfigFactory.java:378)\n        at spray.can.client.HostConnectorSettings$.apply(HostConnectorSettings.scala:41)\n        at spray.can.client.HostConnectorSettings$.apply(HostConnectorSettings.scala:38)\n```\n\nThis is due to the call to `withFallback` in the `apply` method of HostConnectorSettings. Specifically, \n\n``` scala\nval c = config withFallback ConfigFactory.defaultReference(getClass.getClassLoader)\n```\n\nIn an OSGi environment, `getClass.getClassLoader` is returning the classloader of the spray-can JAR, which is distinct from the classloader of the spray-util JAR.\n\nThere are various ways to fix, including:\n- Avoiding the apply methods and just manually creating various config objects.\n- Adding an additional parameter to the apply methods that load configuration that disables the fallback logic: `apply(config: Config, fallback: Boolean = true)`\n\nI'm leaning towards adding the fallback parameter to each of the config classes. Further, the apply method that takes an `ActorSystem` could probably always pass `fallback = false` since Akka will have loaded all reference.conf files already. Before adding this to this PR, I wanted to get your opinion though.\n. One other question - would you prefer additional commits at this point and then a single rebase after all comments are addressed? Or just continue editing the original commit?\n. That works - I've tested in Equinox with HostConnectorSettings (and ClientConnectorSettings). I made similar changes to all of the configuration companions that were doing fallback loading. I'm curious as to why the fallback loading is necessary at all though -- shouldn't the actor system's config settings load all the spray reference.conf files?\n. Good catch -- it is caused by the fact that akka-actor 2.2.0-RC2 doesn't have OSGi metadata anymore. I just checked the JAR on Maven Central and noticed. I assume this is a mistake, b/c I know akka 2.2 actually adds more OSGi support than before. I'll ask @rkuhn.\n. As of Akka 2.2, akka-osgi embeds all of akka-actor inside it and exports those packages. So on the 1.2 branch, we should change the dependency from akka-actor to akka-osgi. Spray marks the akka-actor dependency as provided anyway, so changing to akka-osgi will have no impact on users (e.g., transitive dependencies).\n. Here's the akka commit that changed this: https://github.com/akka/akka/commit/e6b1a280cd4b9c0b7bd0f2692774a56c7e85c1b2\n\nThis does cause some annoyance with transitive dependencies. Akka user's now need to be very careful to ensure they exclude akka-actor JAR and include akka-osgi JAR or otherwise risk akka-actor ahead of akka-osgi on build time classpath, resulting in BND not being able to determine the version of the akka packages.\n. Here's how I have spray working now with Akka 2.1: https://gist.github.com/mpilquist/5882185\n\nThe basic idea is to (1) load reference.conf files from all spray bundles and the akka bundle by creating a composite classloader that delegates findResources calls to each classloader and (2) pass that composite classloader to the ActorSystem so any dynamic classloading that occurs searches all classloaders.\n\nI agree that a service registry approach is too OSGi specific. A spray-osgi uberjar is a good alternative though.\n. Gotcha. Some various ways to restore that functionality:\n- For the apply method that takes a `Config` object, add an implicit `ActorSystem`.\n- For the apply method that takes a `Config` object, use the thread context classloader if it is defined, otherwise fallback to `getClass.getClassLoader`.\n\nI think the first approach is more elegant in that the default config values come from the config of the actor system, but it is an API change. Any thoughts or other suggestions?\n. OK great, I'll make that change today.\n\n> One thing: Could you please prefix your commit messages with the module name?\n\nYep, sorry. I'll rebase and fix.\n. Rebase completed along with new config loading strategy. I added a `sprayConfig` method to the `ActorSystem` extension methods which performs the proper fallback classloading and then modified each of the settings `apply` methods to use that new method. Note the special logic needed in spray-servlet to work around a chicken and egg problem -- the actor system isn't available until the `WebBoot` has been created but the class name of the `WebBoot` instance comes from the config. To work around this, I loaded the web boot class name directly and then deferred full configuration loading until after the actor system is available.\n. Are you running in to that issue when using Maven? If so, you can easily fix it by adding this to your pom:\n\n``` xml\n<build>\n  <plugins>\n      <plugin>\n        <groupId>org.apache.felix</groupId>\n        <artifactId>maven-bundle-plugin</artifactId>\n        <version>2.4.0</version>\n        <extensions>true</extensions>\n      </plugin>\n  </plugins>\n</build>\n```\n\nAdding the plugin causes maven to be aware that a packaging type of bundle means to look for a .jar file as opposed to a .zip file. Let me know if that fixes it for you.\n. Right, with the current setup, the code assumes that whoever is creating\nthe ActorSystem passes a Config object that has all of the spray\nreference.conf files loaded. The gist you noted is a simplified version of\nwhat I'm using currently. I think an edited version of the gist could be a\ngood candidate for inclusion in a new spray-osgi JAR though. I'd remove the\nbundle context dependency (and as a result, the dependency on Eclipse\nGemini Blueprint for BundleDelegatingClassLoader).  If @sirthias and\n@jrudolph agree, I can add that module to the PR.\n\nOn Wed, Jul 10, 2013 at 9:45 PM, Brian Topping notifications@github.comwrote:\n\n> Thanks Michael, I think it was something with between Maven and Nexus\n> resolving a jar with the same version from the wrong place, even though I\n> had the repository group order set explicitly otherwise. The bundle plugin\n> is definitely set up as you point out, otherwise the dependent project\n> couldn't generate bundles. Changing the version numbers got around it.\n> \n> What I've got at this point is this patch applied to the 1.2 sources and\n> an adaptation of the spray routing on-spray--can demo service, see\n> https://gist.github.com/topping/5971839. This worked with the uberjar\n> setup that I had in my other PR, but doesn't work with this new jar setup.\n> I've seen https://gist.github.com/mpilquist/5882185, am I to understand\n> that I also need that code in order for this PR to work? In working with\n> what I have, I don't see any changes that would allow for the\n> configurations to be loaded across bundles. So kind of confused.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/329#issuecomment-20785376\n> .\n. Yeah, all good points and I share your distaste for uber-jars. There are various other fancy ways of loading reference.conf files in OSGi. \n\nOne option is to implement an extender bundle that checks for the existence of reference.conf files in bundles as they are resolved. Upon finding a bundle with a reference.conf, the extender bundle could load the config and store it in memory. Then, the extender could provide a merged Config as an OSGi service. This pattern doesn't play nice if you have an OSGi app that has multiple versions of the same JARs installed though. The apps I work on don't have this requirement but some apps do.\n\nI like the current approach -- do nothing fancy and just require the creator of the ActorSystem to load all the proper reference config -- because it is ultimately flexible. The downside is that it requires some custom code to get the right config loaded. \n\nWe could provide a spray-osgi bundle that provided a basic implementation of the custom code needed to bootstrap spray in an OSGi container if we think that's necessary. I personally wouldn't use that code though for the same reason I don't use the akka-osgi code -- I have application specific requirements that require finer grain control of how the ActorSystem is created and configured.\n\nPerhaps another option is to just embed a simplified version of the code from the gist in to your OSGi example code? That might be the best option because it shows how to load an ActorSystem with Spray config loaded without requiring a new module and without making any assumptions on production environments. What do you think?\n. That's similar to the current approach in the gist with the exception that the transitives aren't auto-determined. That's possible though by using [BundleWiring](http://www.osgi.org/javadoc/r4v43/core/org/osgi/framework/wiring/BundleWiring.html). Basic idea is to ask for the packages this bundle is wired to and generate a unique set of bundles from that package list. Then repeat that process for each of those bundles, taking care to avoid cycles. If it only had to support discovery of reference.conf files in direct dependencies, much of the complexity could be avoided.\n. Here's a gist that captures the idea: https://gist.github.com/mpilquist/6002064. It appears to work, but I haven't done full testing with it yet. Since there's nothing about this code that's related to spray, I wonder if it belongs in a general purpose OSGi extension to Typesafe Config.\n. @sirthias Do you want me to rebase due to conflicts with 51f325ce1e7eb70c6e35821cdb598b0fc4a5c947?\n. OK should be safe to merge at this point. The config refactor in 51f325c works great in OSGi.\n. Two notes about the merge to 1.2:\n- we need to remove the override import for akka.io package\n- we need to replace dependency on akka-actor with akka-osgi\n\nRegards,\nMichael\n. As of Akka 2.2, the akka-osgi JAR is a copy of akka-actor but with OSGi metadata and merged reference.conf. So we just need to literally replace akka-actor with akka-osgi in the SBT dependencies. You can see the impacts by looking at the generated Import-Package header for the spray JARs -- when using akka-osgi, they will have the version range [2.2,2.3). When using akka-actor, they will have the BND range macro instead.\n\nOn Jul 18, 2013, at 11:35 AM, Mathias notifications@github.com wrote:\n\n> Ok, Michael, I just merged master into the release/1.2 branch.\n> I think I got your \"we need to remove the override import for akka.io package\" hint right, but what exactly do you mean by \"replace dependency on akka-actor with akka-osgi\"?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. Yep, looks good. I just checked the imports and they look right. One\ncorrection though -- we need to remove \"akka.io\" from the list of exported\npackages in the sprayIO project in Build.scala:\n\n``` scala\n  lazy val sprayIO = Project(\"spray-io\", file(\"spray-io\"))\n    .dependsOn(sprayUtil)\n    .settings(sprayModuleSettings: _*)\n    .settings(osgiSettings(exports = Seq(\"spray.io\", \"akka.io\")): _*) //\n<-- just remove akka.io here\n    .settings(libraryDependencies ++= provided(akkaActor, scalaReflect))\n```\n\nI'll do some testing with Akka 2.2 and Spray 1.2 to make sure everything is\nfunctional.\n\nOn Thu, Jul 25, 2013 at 10:37 AM, Mathias notifications@github.com wrote:\n\n> So this change:\n> 989b8e4https://github.com/spray/spray/commit/989b8e49cb20f48831eb922c1c9000a6998bde4d\n> is all that's required.\n> Thanks again for you support, Michael!\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/329#issuecomment-21558187\n> .\n. I like the new approach much better (I won't show you the heinous stuff I needed to do to access the servlet request from directives). I thought about the header approach initially but I haven't been able to wrap my head around the fact that headers can have typed values. I'll update the PR now. :)\n. Yep, agreed, fixed. Sorry for the latest rebase but I wanted to show clean history in ModelConverterSpec.\n. Right thanks - forgot that when combining the commits. I'll update tonight when I'm back to a computer. Thanks!\n\nOn Jul 17, 2013, at 11:08 AM, Mathias notifications@github.com wrote:\n\n> Looks good!\n> The only thing missing is adding the new servlet-request-access setting to the spray-servlet reference.conf, along with a comment.\n> Then we should be good for merging.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. +1 It would be nice to have a way to configure the ClientSSLProvider globally as well -- in addition to per host.\n. Yeah, both `RequestParsing` and `ResponseParsing` have the same logic. I like the idea of refactoring to a pipeline stage.\n\n> The event pipeline of the new stage would listen for `SSLSessionEstablished`, `HttpMessageStartEvent`, and HttpMessage events and then add the header to the message.\n\nThe new stage could buffer the info from the last SSLSessionEstablished in a local var and then update each new `HttpMessageStartEvent` with the header. I don't follow why it would need to capture the `HttpMessage` event though.\n\n> We would need to add a new method in HttpMessageStart to add the header generically.\n\nSomething like `withHeaders(headers: List[HttpHeader])`?\n. Working through this refactoring now. Two questions that are related to each other: \n- What package should the `SSLSessionInfoSupport` pipeline stage be in? I put it in spray.can to start since its shared by client and server.\n- `HttpMessageStartEvent` is currently defined in the `RequestParsing` object, which means `SSLSessionInfoSupport` ends up with a dependency on `spray.can.server` package. Should `HttpMessageStartEvent` be moved somewhere more general? Otherwise, defining `SSLSessionInfoSupport` in `spray.can` package results in package cycle between `spray.can` and `spray.can.server`.\n. Sounds good - let me know how   1ae41dc looks.\n. All comments should be addressed now and the work is split across two commits to simplify Akka backport.\n\nThanks for the help!\n. I made 80982d4 `!` because of `SslTlsSupport` changed from extending `OptionalPipelineStage[...]` directly to providing an `OptionalPipelineStage[...]` as a result of `apply`.\n\nI made e486900 `!` because of the new parameter on `ParserSettings`.\n\nNeither of those changes would typically impact users but they are both technically source incompatible. Let me know if you want me to change either/both to `=`.\n. Based on a quick perusal of the akka-http PR from last week, it appears that a fully integrated akka-http with SSL is a ways out. I need a version of Spray with Shapeless 2 so I'm going to try to work through this upgrade. Are you interested in a PR in case other users have the same issue?\n. Scratch that, a colleague just pointed out https://github.com/spray/spray/tree/release/1.3-shapeless2. A combination 1.3-shapeless2 with 1.3_2.11 would make my day. :)\n. Easy to reproduce:\n\n```\nimport akka.actor.{ ActorSystem, IO }\nimport spray.can.Http\nimport spray.client.pipelining._\nimport spray.util._\n\nimplicit val system = ActorSystem()\nval rsp = Get(\"https://10.52.207.48:8010/\") ~> sendReceive\nrsp.await\n```\n\nResults in:\n\n```\n[ERROR] [10/24/2013 07:49:10.668] [default-akka.actor.default-dispatcher-2] [akka://default/user/IO-HTTP/group-0/0] Aborting encrypted connection to 10.52.207.48/10.52.207.48:8010 due to [SSLException:Unsupported record version Unknown-38.48]\n[INFO] [10/24/2013 07:49:10.679] [default-akka.actor.default-dispatcher-4] [akka://default/user/IO-HTTP/group-0/0] Message [akka.actor.Terminated] from Actor[akka://default/system/IO-TCP/selectors/$a/0#-992389400] to Actor[akka://default/user/IO-HTTP/group-0/0#-459547759] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.\n[WARN] [10/24/2013 07:49:10.684] [default-akka.actor.default-dispatcher-5] [akka://default/user/IO-HTTP/host-connector-0/0] Aborted in response to GET request to / with 5 retries left, retrying...\n[ERROR] [10/24/2013 07:49:10.755] [default-akka.actor.default-dispatcher-5] [LocalActorRefProvider(akka://default)] guardian failed, shutting down system\njava.lang.AssertionError: assertion failed\n        at scala.Predef$.assert(Predef.scala:165)\n        at spray.io.SslTlsSupport$$anon$1$$anon$2$$anon$11.apply(SslTlsSupport.scala:283)\n        at spray.io.SslTlsSupport$$anon$1$$anon$2$PumpAction.apply(SslTlsSupport.scala:246)\n        at spray.io.SslTlsSupport$$anon$1$$anon$2$$anon$6$$anonfun$2.apply(SslTlsSupport.scala:85)\n        at spray.io.SslTlsSupport$$anon$1$$anon$2$$anon$6$$anonfun$2.apply(SslTlsSupport.scala:81)\n        at spray.io.SslTlsSupport$$anon$1$$anon$2.process(SslTlsSupport.scala:234)\n        at spray.io.DynamicPipelines$$anonfun$eventPipeline$1.apply(Pipelines.scala:48)\n        at spray.io.DynamicPipelines$$anonfun$eventPipeline$1.apply(Pipelines.scala:48)\n        at spray.io.RawPipelineStage$$anon$3$$anonfun$2.apply(Pipelines.scala:115)\n        at spray.io.RawPipelineStage$$anon$3$$anonfun$2.apply(Pipelines.scala:115)\n        at spray.io.TickGenerator$$anon$1$$anon$2$$anonfun$1.apply(TickGenerator.scala:41)\n        at spray.io.TickGenerator$$anon$1$$anon$2$$anonfun$1.apply(TickGenerator.scala:38)\n        at spray.io.ConnectionHandler$$anonfun$running$1.applyOrElse(ConnectionHandler.scala:58)\n        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:165)\n        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n        at akka.actor.ActorCell.invoke(ActorCell.scala:456)\n        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n        at akka.dispatch.Mailbox.run(Mailbox.scala:219)\n        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n```\n. Just tested with RC4 and confirmed the fix. Sorry; I wanted to test this with the nightly last Friday but ended up getting busy with other stuff.\n. +1\n. LGTM - not sure why shapeless is imported optionally in routing though (unrelated to this PR). I'd think there's not much you can do with spray-routing without shapeless.\n. Agreed\n\nFYI - If I understand correctly, akka-http will not be using shapeless. So we may never get the benefit of using the shapeless OSGi manifest headers. Another option is to ask Miles to build a shapeless 1.2.5 that is BC with 1.2.4 but includes the manifest headers.\n. @rkrzewski The PR removed the override that sets the import to optional. BND should still add the shapeless imports. Can you check the generated MANIFEST.MF?\n. +1 for server side auto-chunking. I need to support large file upload and stream data to disk. If client doesn't send a chunked request, I want to auto-chunk to avoid heap allocating the entire uploaded file.\n. Follow up question - will incoming-auto-chunking-threshold-size be supported by spray-servlet?\n. Re: spray-servlet, if auto-chunking will never be supported, then I suppose the simplest alternative is to fall back to a manually written servlet for the resources that require auto-chunking. That's certainly possible but not particularly elegant.\n\n> The only way for us to get to the request entity is via the httpServletRequest.getInputStream, which is a blocking  abstraction. So I think it's hard for us to send a \"virtual chunk\" up to the application whenever a bunch of new bytes  have arrived on the connection because we simply cannot know if and when this is the case.\n> All we see on the inputStream is EOF, which signals that we have read the complete request.\n\nI was thinking that if the Content-Length was over the auto-chunking threshold, the connector servlet could spawn a request specific actor or future that pulled from the InputStream and generated chunks. Somewhat risky given that the blocking on the input stream could lead to starvation, but perhaps better than no support at all?\n. +1. Do you think this will make 1.1/1.2 final?\n. Hi Mathias,\n\nI removed the explicit passing of sslEngineProvider and the bug reoccurred. Turns out this is due to the import of normalizedSettings._ in HttpHostConnector. The normalizedSettings.sslEngineProvider has the default ClientSSLEngineProvider and hence, overrides the implicit ClientSSLEngineProvider in the HttpHostConnector constructor. Excluding the import of normalizedSettings.sslEngineProvider fixes.\n\nI'd be happy to add a test. Could you point me to the best place to add one?\n\nThanks,\nMichael\n. Just sent CLA via email and pushed an amended commit message.\n. Re: akka.io -- the export/import is correct for 1.0/1.1 -- it is a standard OSGi convention for a bundle to import the packages that it exports. We have to customize the version range on it though in order to override the akka.\\* version range -- that is, we don't want akka.io to be imported as [2.1,2.2) in Spray 1.1.\n\nRe: akka.io in spray 1.2 -- I assumed we'd just delete the akka.io overrides when merging to 1.2.  I'd be happy to do that.\n\nRe: testing -- manual testing is fairly easy using pax-runner (https://ops4j1.jira.com/wiki/display/paxrunner/Pax+Runner). Automated testing is somewhat more involved to configure and set up.\n\nManual testing with pax-exam can be accomplished by first creating a config file with these contents (and appropriately adjusting paths):\n\n```\nmvn:org.scala-lang/scala-library/2.10.2\nmvn:org.scala-lang/scala-reflect/2.10.2\nmvn:com.typesafe/config/1.0.0\nmvn:com.typesafe.akka/akka-actor_2.10/2.1.2\n\nmvn:org.parboiled/parboiled-core/1.1.5\nmvn:org.parboiled/parboiled-scala_2.10/1.1.5\nfile:///Users/mpilquist/.ivy2/local/io.spray/spray-json_2.10/1.2.5/bundles/spray-json_2.10.jar@update\n\nfile:///Users/mpilquist/.ivy2/local/io.spray/spray-util/1.1-SNAPSHOT/bundles/spray-util.jar@update\nfile:///Users/mpilquist/.ivy2/local/io.spray/spray-io/1.1-SNAPSHOT/bundles/spray-io.jar@update\n\nfile:///Users/mpilquist/.ivy2/local/io.spray/spray-caching/1.1-SNAPSHOT/bundles/spray-caching.jar@update\nmvn:com.googlecode.concurrentlinkedhashmap/concurrentlinkedhashmap-lru/1.3.2\n\nfile:///Users/mpilquist/.ivy2/local/io.spray/spray-http/1.1-SNAPSHOT/bundles/spray-http.jar@update\nfile:///Users/mpilquist/.ivy2/local/io.spray/spray-httpx/1.1-SNAPSHOT/bundles/spray-httpx.jar@update\nmvn:org.jvnet.mimepull/mimepull/1.9.3\n\nfile:///Users/mpilquist/.ivy2/local/io.spray/spray-can/1.1-SNAPSHOT/bundles/spray-can.jar@update\nfile:///Users/mpilquist/.ivy2/local/io.spray/spray-client/1.1-SNAPSHOT/bundles/spray-client.jar@update\nfile:///Users/mpilquist/.ivy2/local/io.spray/spray-routing/1.1-SNAPSHOT/bundles/spray-routing.jar@update\nfile:///Users/mpilquist/.ivy2/local/io.spray/spray-servlet/1.1-SNAPSHOT/bundles/spray-servlet.jar@update\nmvn:javax.servlet/javax.servlet-api/3.0.1\n```\n\nThen to run: `pax-run.sh file:spray.conf --log=debug --systemPackages=sun.misc`\n\nThat will start Apache Felix and load all of the spray jars and their dependencies. If there are any errors due to misconfigured OSGi settings, they are printed to the console. Further, typing `bundles` at the prompt will show if all bundles resolve:\n\n```\n____________________________\nWelcome to Apache Felix Gogo\n\ng! bundles\n    0|Active     |    0|org.apache.felix.framework (4.0.3)\n    1|Active     |    5|org.scala-lang.scala-library (2.10.2.v20130530-074427-VFINAL-60d462ef6e)\n    2|Active     |    5|org.scala-lang.scala-reflect (2.10.2.v20130530-074427-VFINAL-60d462ef6e)\n    3|Active     |    5|com.typesafe.config (1.0.0)\n    4|Active     |    5|com.typesafe.akka.actor (2.1.2)\n    5|Active     |    5|org.parboiled.core (1.1.5)\n    6|Active     |    5|org.parboiled.scala (1.1.5)\n    7|Active     |    5|io.spray.json (1.2.5)\n    8|Active     |    5|io.spray.util (1.1.0.SNAPSHOT)\n    9|Active     |    5|io.spray.io (1.1.0.SNAPSHOT)\n   10|Active     |    5|io.spray.caching (1.1.0.SNAPSHOT)\n   11|Active     |    5|com.googlecode.concurrentlinkedhashmap.lru (1.3.2)\n   12|Active     |    5|io.spray.http (1.1.0.SNAPSHOT)\n   13|Active     |    5|io.spray.httpx (1.1.0.SNAPSHOT)\n   14|Active     |    5|org.jvnet.mimepull (1.9.3)\n   15|Active     |    5|io.spray.can (1.1.0.SNAPSHOT)\n   16|Active     |    5|io.spray.client (1.1.0.SNAPSHOT)\n   17|Active     |    5|io.spray.routing (1.1.0.SNAPSHOT)\n   18|Active     |    5|io.spray.servlet (1.1.0.SNAPSHOT)\n   19|Active     |    5|javax.servlet-api (3.0.1)\n   20|Active     |    1|org.apache.felix.gogo.command (0.12.0)\n   21|Active     |    1|org.apache.felix.gogo.runtime (0.10.0)\n   22|Active     |    1|org.apache.felix.gogo.shell (0.10.0)\n```\n\nAutomated testing can be accomplished with pax-exam (https://ops4j1.jira.com/wiki/display/PAXEXAM3/Pax+Exam). The scala-library, akka, and ScalaTest all use pax-exam to test their OSGi headers. If interested, I can put together a pax-exam test. I didn't know how much overlap there would be with Brian's efforts.\n. > Does that check that classes in the bundles only reference packages as declared by their metadata?\n\nAnswer is somewhat complicated. The OSGi bundles will resolve successfully if all of the mandatory imported packages can be wired to providers of those packages successfully. So it is certainly possible for a bundle to resolve but then not link during runtime. Some common causes of runtime linkage errors in OSGi environments:\n # Missing import package statement\n # Optionally imported package not wired but code linked that depends on it\n # Dynamic class loading via Class.forName\n\nThe risk of the first case occurring is mitigated by using BND to generate the `Import-Package` header. BND scans the classes at build time and ensures all statically referenced classes are accounted for in the generated header.\n\nThe second case is similar to standard Java optional dependencies. E.g., if spray-json exists and is wired to spray-httpx, then the user can safely use types that have static references on spray-json types.\n\nIn the final case, the import package header must be manually augmented to deal with dynamic class loading, or dynamic class loading should use the ThreadContextClassLoader, depending on the situation.\n. > Shouldn't spray-json be optional there as well?\n\nYep, will update.\n. One issue I've run in to in using these OSGi enabled JARs is that the various settings classes that can load their config from an actor system or config object are unusable in an OSGi environment.  For example, calling `HostConnectorSettings(system)` results in an exception:\n\n```\ncom.typesafe.config.ConfigException$UnresolvedSubstitution: reference.conf: 149: Could not resolve substitution to a value: ${spray.version}\n        at com.typesafe.config.impl.ConfigReference.resolveSubstitutions(ConfigReference.java:84)\n        at com.typesafe.config.impl.ResolveSource.resolveCheckingReplacement(ResolveSource.java:110)\n        at com.typesafe.config.impl.ResolveContext.resolve(ResolveContext.java:114)\n        at com.typesafe.config.impl.ConfigConcatenation.resolveSubstitutions(ConfigConcatenation.java:178)\n        at com.typesafe.config.impl.ResolveSource.resolveCheckingReplacement(ResolveSource.java:110)\n        at com.typesafe.config.impl.ResolveContext.resolve(ResolveContext.java:114)\n        at com.typesafe.config.impl.SimpleConfigObject$1.modifyChildMayThrow(SimpleConfigObject.java:340)\n        at com.typesafe.config.impl.SimpleConfigObject.modifyMayThrow(SimpleConfigObject.java:279)\n        at com.typesafe.config.impl.SimpleConfigObject.resolveSubstitutions(SimpleConfigObject.java:320)\n        at com.typesafe.config.impl.SimpleConfigObject.resolveSubstitutions(SimpleConfigObject.java:24)\n        at com.typesafe.config.impl.ResolveSource.resolveCheckingReplacement(ResolveSource.java:110)\n        at com.typesafe.config.impl.ResolveContext.resolve(ResolveContext.java:114)\n        at com.typesafe.config.impl.SimpleConfigObject$1.modifyChildMayThrow(SimpleConfigObject.java:340)\n        at com.typesafe.config.impl.SimpleConfigObject.modifyMayThrow(SimpleConfigObject.java:279)\n        at com.typesafe.config.impl.SimpleConfigObject.resolveSubstitutions(SimpleConfigObject.java:320)\n        at com.typesafe.config.impl.SimpleConfigObject.resolveSubstitutions(SimpleConfigObject.java:24)\n        at com.typesafe.config.impl.ResolveSource.resolveCheckingReplacement(ResolveSource.java:110)\n        at com.typesafe.config.impl.ResolveContext.resolve(ResolveContext.java:114)\n        at com.typesafe.config.impl.SimpleConfigObject$1.modifyChildMayThrow(SimpleConfigObject.java:340)\n        at com.typesafe.config.impl.SimpleConfigObject.modifyMayThrow(SimpleConfigObject.java:279)\n        at com.typesafe.config.impl.SimpleConfigObject.resolveSubstitutions(SimpleConfigObject.java:320)\n        at com.typesafe.config.impl.SimpleConfigObject.resolveSubstitutions(SimpleConfigObject.java:24)\n        at com.typesafe.config.impl.ResolveSource.resolveCheckingReplacement(ResolveSource.java:110)\n        at com.typesafe.config.impl.ResolveContext.resolve(ResolveContext.java:114)\n        at com.typesafe.config.impl.SimpleConfigObject$1.modifyChildMayThrow(SimpleConfigObject.java:340)\n        at com.typesafe.config.impl.SimpleConfigObject.modifyMayThrow(SimpleConfigObject.java:279)\n        at com.typesafe.config.impl.SimpleConfigObject.resolveSubstitutions(SimpleConfigObject.java:320)\n        at com.typesafe.config.impl.SimpleConfigObject.resolveSubstitutions(SimpleConfigObject.java:24)\n        at com.typesafe.config.impl.ResolveSource.resolveCheckingReplacement(ResolveSource.java:110)\n        at com.typesafe.config.impl.ResolveContext.resolve(ResolveContext.java:114)\n        at com.typesafe.config.impl.ResolveContext.resolve(ResolveContext.java:147)\n        at com.typesafe.config.impl.ResolveContext.resolve(ResolveContext.java:157)\n        at com.typesafe.config.impl.SimpleConfig.resolve(SimpleConfig.java:60)\n        at com.typesafe.config.impl.SimpleConfig.resolve(SimpleConfig.java:55)\n        at com.typesafe.config.impl.SimpleConfig.resolve(SimpleConfig.java:33)\n        at com.typesafe.config.impl.ConfigImpl$1.call(ConfigImpl.java:372)\n        at com.typesafe.config.impl.ConfigImpl$1.call(ConfigImpl.java:365)\n        at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:58)\n        at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:86)\n        at com.typesafe.config.impl.ConfigImpl.defaultReference(ConfigImpl.java:365)\n        at com.typesafe.config.ConfigFactory.defaultReference(ConfigFactory.java:378)\n        at spray.can.client.HostConnectorSettings$.apply(HostConnectorSettings.scala:41)\n        at spray.can.client.HostConnectorSettings$.apply(HostConnectorSettings.scala:38)\n```\n\nThis is due to the call to `withFallback` in the `apply` method of HostConnectorSettings. Specifically, \n\n``` scala\nval c = config withFallback ConfigFactory.defaultReference(getClass.getClassLoader)\n```\n\nIn an OSGi environment, `getClass.getClassLoader` is returning the classloader of the spray-can JAR, which is distinct from the classloader of the spray-util JAR.\n\nThere are various ways to fix, including:\n- Avoiding the apply methods and just manually creating various config objects.\n- Adding an additional parameter to the apply methods that load configuration that disables the fallback logic: `apply(config: Config, fallback: Boolean = true)`\n\nI'm leaning towards adding the fallback parameter to each of the config classes. Further, the apply method that takes an `ActorSystem` could probably always pass `fallback = false` since Akka will have loaded all reference.conf files already. Before adding this to this PR, I wanted to get your opinion though.\n. One other question - would you prefer additional commits at this point and then a single rebase after all comments are addressed? Or just continue editing the original commit?\n. That works - I've tested in Equinox with HostConnectorSettings (and ClientConnectorSettings). I made similar changes to all of the configuration companions that were doing fallback loading. I'm curious as to why the fallback loading is necessary at all though -- shouldn't the actor system's config settings load all the spray reference.conf files?\n. Good catch -- it is caused by the fact that akka-actor 2.2.0-RC2 doesn't have OSGi metadata anymore. I just checked the JAR on Maven Central and noticed. I assume this is a mistake, b/c I know akka 2.2 actually adds more OSGi support than before. I'll ask @rkuhn.\n. As of Akka 2.2, akka-osgi embeds all of akka-actor inside it and exports those packages. So on the 1.2 branch, we should change the dependency from akka-actor to akka-osgi. Spray marks the akka-actor dependency as provided anyway, so changing to akka-osgi will have no impact on users (e.g., transitive dependencies).\n. Here's the akka commit that changed this: https://github.com/akka/akka/commit/e6b1a280cd4b9c0b7bd0f2692774a56c7e85c1b2\n\nThis does cause some annoyance with transitive dependencies. Akka user's now need to be very careful to ensure they exclude akka-actor JAR and include akka-osgi JAR or otherwise risk akka-actor ahead of akka-osgi on build time classpath, resulting in BND not being able to determine the version of the akka packages.\n. Here's how I have spray working now with Akka 2.1: https://gist.github.com/mpilquist/5882185\n\nThe basic idea is to (1) load reference.conf files from all spray bundles and the akka bundle by creating a composite classloader that delegates findResources calls to each classloader and (2) pass that composite classloader to the ActorSystem so any dynamic classloading that occurs searches all classloaders.\n\nI agree that a service registry approach is too OSGi specific. A spray-osgi uberjar is a good alternative though.\n. Gotcha. Some various ways to restore that functionality:\n- For the apply method that takes a `Config` object, add an implicit `ActorSystem`.\n- For the apply method that takes a `Config` object, use the thread context classloader if it is defined, otherwise fallback to `getClass.getClassLoader`.\n\nI think the first approach is more elegant in that the default config values come from the config of the actor system, but it is an API change. Any thoughts or other suggestions?\n. OK great, I'll make that change today.\n\n> One thing: Could you please prefix your commit messages with the module name?\n\nYep, sorry. I'll rebase and fix.\n. Rebase completed along with new config loading strategy. I added a `sprayConfig` method to the `ActorSystem` extension methods which performs the proper fallback classloading and then modified each of the settings `apply` methods to use that new method. Note the special logic needed in spray-servlet to work around a chicken and egg problem -- the actor system isn't available until the `WebBoot` has been created but the class name of the `WebBoot` instance comes from the config. To work around this, I loaded the web boot class name directly and then deferred full configuration loading until after the actor system is available.\n. Are you running in to that issue when using Maven? If so, you can easily fix it by adding this to your pom:\n\n``` xml\n<build>\n  <plugins>\n      <plugin>\n        <groupId>org.apache.felix</groupId>\n        <artifactId>maven-bundle-plugin</artifactId>\n        <version>2.4.0</version>\n        <extensions>true</extensions>\n      </plugin>\n  </plugins>\n</build>\n```\n\nAdding the plugin causes maven to be aware that a packaging type of bundle means to look for a .jar file as opposed to a .zip file. Let me know if that fixes it for you.\n. Right, with the current setup, the code assumes that whoever is creating\nthe ActorSystem passes a Config object that has all of the spray\nreference.conf files loaded. The gist you noted is a simplified version of\nwhat I'm using currently. I think an edited version of the gist could be a\ngood candidate for inclusion in a new spray-osgi JAR though. I'd remove the\nbundle context dependency (and as a result, the dependency on Eclipse\nGemini Blueprint for BundleDelegatingClassLoader).  If @sirthias and\n@jrudolph agree, I can add that module to the PR.\n\nOn Wed, Jul 10, 2013 at 9:45 PM, Brian Topping notifications@github.comwrote:\n\n> Thanks Michael, I think it was something with between Maven and Nexus\n> resolving a jar with the same version from the wrong place, even though I\n> had the repository group order set explicitly otherwise. The bundle plugin\n> is definitely set up as you point out, otherwise the dependent project\n> couldn't generate bundles. Changing the version numbers got around it.\n> \n> What I've got at this point is this patch applied to the 1.2 sources and\n> an adaptation of the spray routing on-spray--can demo service, see\n> https://gist.github.com/topping/5971839. This worked with the uberjar\n> setup that I had in my other PR, but doesn't work with this new jar setup.\n> I've seen https://gist.github.com/mpilquist/5882185, am I to understand\n> that I also need that code in order for this PR to work? In working with\n> what I have, I don't see any changes that would allow for the\n> configurations to be loaded across bundles. So kind of confused.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/329#issuecomment-20785376\n> .\n. Yeah, all good points and I share your distaste for uber-jars. There are various other fancy ways of loading reference.conf files in OSGi. \n\nOne option is to implement an extender bundle that checks for the existence of reference.conf files in bundles as they are resolved. Upon finding a bundle with a reference.conf, the extender bundle could load the config and store it in memory. Then, the extender could provide a merged Config as an OSGi service. This pattern doesn't play nice if you have an OSGi app that has multiple versions of the same JARs installed though. The apps I work on don't have this requirement but some apps do.\n\nI like the current approach -- do nothing fancy and just require the creator of the ActorSystem to load all the proper reference config -- because it is ultimately flexible. The downside is that it requires some custom code to get the right config loaded. \n\nWe could provide a spray-osgi bundle that provided a basic implementation of the custom code needed to bootstrap spray in an OSGi container if we think that's necessary. I personally wouldn't use that code though for the same reason I don't use the akka-osgi code -- I have application specific requirements that require finer grain control of how the ActorSystem is created and configured.\n\nPerhaps another option is to just embed a simplified version of the code from the gist in to your OSGi example code? That might be the best option because it shows how to load an ActorSystem with Spray config loaded without requiring a new module and without making any assumptions on production environments. What do you think?\n. That's similar to the current approach in the gist with the exception that the transitives aren't auto-determined. That's possible though by using [BundleWiring](http://www.osgi.org/javadoc/r4v43/core/org/osgi/framework/wiring/BundleWiring.html). Basic idea is to ask for the packages this bundle is wired to and generate a unique set of bundles from that package list. Then repeat that process for each of those bundles, taking care to avoid cycles. If it only had to support discovery of reference.conf files in direct dependencies, much of the complexity could be avoided.\n. Here's a gist that captures the idea: https://gist.github.com/mpilquist/6002064. It appears to work, but I haven't done full testing with it yet. Since there's nothing about this code that's related to spray, I wonder if it belongs in a general purpose OSGi extension to Typesafe Config.\n. @sirthias Do you want me to rebase due to conflicts with 51f325ce1e7eb70c6e35821cdb598b0fc4a5c947?\n. OK should be safe to merge at this point. The config refactor in 51f325c works great in OSGi.\n. Two notes about the merge to 1.2:\n- we need to remove the override import for akka.io package\n- we need to replace dependency on akka-actor with akka-osgi\n\nRegards,\nMichael\n. As of Akka 2.2, the akka-osgi JAR is a copy of akka-actor but with OSGi metadata and merged reference.conf. So we just need to literally replace akka-actor with akka-osgi in the SBT dependencies. You can see the impacts by looking at the generated Import-Package header for the spray JARs -- when using akka-osgi, they will have the version range [2.2,2.3). When using akka-actor, they will have the BND range macro instead.\n\nOn Jul 18, 2013, at 11:35 AM, Mathias notifications@github.com wrote:\n\n> Ok, Michael, I just merged master into the release/1.2 branch.\n> I think I got your \"we need to remove the override import for akka.io package\" hint right, but what exactly do you mean by \"replace dependency on akka-actor with akka-osgi\"?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. Yep, looks good. I just checked the imports and they look right. One\ncorrection though -- we need to remove \"akka.io\" from the list of exported\npackages in the sprayIO project in Build.scala:\n\n``` scala\n  lazy val sprayIO = Project(\"spray-io\", file(\"spray-io\"))\n    .dependsOn(sprayUtil)\n    .settings(sprayModuleSettings: _*)\n    .settings(osgiSettings(exports = Seq(\"spray.io\", \"akka.io\")): _*) //\n<-- just remove akka.io here\n    .settings(libraryDependencies ++= provided(akkaActor, scalaReflect))\n```\n\nI'll do some testing with Akka 2.2 and Spray 1.2 to make sure everything is\nfunctional.\n\nOn Thu, Jul 25, 2013 at 10:37 AM, Mathias notifications@github.com wrote:\n\n> So this change:\n> 989b8e4https://github.com/spray/spray/commit/989b8e49cb20f48831eb922c1c9000a6998bde4d\n> is all that's required.\n> Thanks again for you support, Michael!\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/329#issuecomment-21558187\n> .\n. I like the new approach much better (I won't show you the heinous stuff I needed to do to access the servlet request from directives). I thought about the header approach initially but I haven't been able to wrap my head around the fact that headers can have typed values. I'll update the PR now. :)\n. Yep, agreed, fixed. Sorry for the latest rebase but I wanted to show clean history in ModelConverterSpec.\n. Right thanks - forgot that when combining the commits. I'll update tonight when I'm back to a computer. Thanks!\n\nOn Jul 17, 2013, at 11:08 AM, Mathias notifications@github.com wrote:\n\n> Looks good!\n> The only thing missing is adding the new servlet-request-access setting to the spray-servlet reference.conf, along with a comment.\n> Then we should be good for merging.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. +1 It would be nice to have a way to configure the ClientSSLProvider globally as well -- in addition to per host.\n. Yeah, both `RequestParsing` and `ResponseParsing` have the same logic. I like the idea of refactoring to a pipeline stage.\n\n> The event pipeline of the new stage would listen for `SSLSessionEstablished`, `HttpMessageStartEvent`, and HttpMessage events and then add the header to the message.\n\nThe new stage could buffer the info from the last SSLSessionEstablished in a local var and then update each new `HttpMessageStartEvent` with the header. I don't follow why it would need to capture the `HttpMessage` event though.\n\n> We would need to add a new method in HttpMessageStart to add the header generically.\n\nSomething like `withHeaders(headers: List[HttpHeader])`?\n. Working through this refactoring now. Two questions that are related to each other: \n- What package should the `SSLSessionInfoSupport` pipeline stage be in? I put it in spray.can to start since its shared by client and server.\n- `HttpMessageStartEvent` is currently defined in the `RequestParsing` object, which means `SSLSessionInfoSupport` ends up with a dependency on `spray.can.server` package. Should `HttpMessageStartEvent` be moved somewhere more general? Otherwise, defining `SSLSessionInfoSupport` in `spray.can` package results in package cycle between `spray.can` and `spray.can.server`.\n. Sounds good - let me know how   1ae41dc looks.\n. All comments should be addressed now and the work is split across two commits to simplify Akka backport.\n\nThanks for the help!\n. I made 80982d4 `!` because of `SslTlsSupport` changed from extending `OptionalPipelineStage[...]` directly to providing an `OptionalPipelineStage[...]` as a result of `apply`.\n\nI made e486900 `!` because of the new parameter on `ParserSettings`.\n\nNeither of those changes would typically impact users but they are both technically source incompatible. Let me know if you want me to change either/both to `=`.\n. Based on a quick perusal of the akka-http PR from last week, it appears that a fully integrated akka-http with SSL is a ways out. I need a version of Spray with Shapeless 2 so I'm going to try to work through this upgrade. Are you interested in a PR in case other users have the same issue?\n. Scratch that, a colleague just pointed out https://github.com/spray/spray/tree/release/1.3-shapeless2. A combination 1.3-shapeless2 with 1.3_2.11 would make my day. :)\n. Easy to reproduce:\n\n```\nimport akka.actor.{ ActorSystem, IO }\nimport spray.can.Http\nimport spray.client.pipelining._\nimport spray.util._\n\nimplicit val system = ActorSystem()\nval rsp = Get(\"https://10.52.207.48:8010/\") ~> sendReceive\nrsp.await\n```\n\nResults in:\n\n```\n[ERROR] [10/24/2013 07:49:10.668] [default-akka.actor.default-dispatcher-2] [akka://default/user/IO-HTTP/group-0/0] Aborting encrypted connection to 10.52.207.48/10.52.207.48:8010 due to [SSLException:Unsupported record version Unknown-38.48]\n[INFO] [10/24/2013 07:49:10.679] [default-akka.actor.default-dispatcher-4] [akka://default/user/IO-HTTP/group-0/0] Message [akka.actor.Terminated] from Actor[akka://default/system/IO-TCP/selectors/$a/0#-992389400] to Actor[akka://default/user/IO-HTTP/group-0/0#-459547759] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.\n[WARN] [10/24/2013 07:49:10.684] [default-akka.actor.default-dispatcher-5] [akka://default/user/IO-HTTP/host-connector-0/0] Aborted in response to GET request to / with 5 retries left, retrying...\n[ERROR] [10/24/2013 07:49:10.755] [default-akka.actor.default-dispatcher-5] [LocalActorRefProvider(akka://default)] guardian failed, shutting down system\njava.lang.AssertionError: assertion failed\n        at scala.Predef$.assert(Predef.scala:165)\n        at spray.io.SslTlsSupport$$anon$1$$anon$2$$anon$11.apply(SslTlsSupport.scala:283)\n        at spray.io.SslTlsSupport$$anon$1$$anon$2$PumpAction.apply(SslTlsSupport.scala:246)\n        at spray.io.SslTlsSupport$$anon$1$$anon$2$$anon$6$$anonfun$2.apply(SslTlsSupport.scala:85)\n        at spray.io.SslTlsSupport$$anon$1$$anon$2$$anon$6$$anonfun$2.apply(SslTlsSupport.scala:81)\n        at spray.io.SslTlsSupport$$anon$1$$anon$2.process(SslTlsSupport.scala:234)\n        at spray.io.DynamicPipelines$$anonfun$eventPipeline$1.apply(Pipelines.scala:48)\n        at spray.io.DynamicPipelines$$anonfun$eventPipeline$1.apply(Pipelines.scala:48)\n        at spray.io.RawPipelineStage$$anon$3$$anonfun$2.apply(Pipelines.scala:115)\n        at spray.io.RawPipelineStage$$anon$3$$anonfun$2.apply(Pipelines.scala:115)\n        at spray.io.TickGenerator$$anon$1$$anon$2$$anonfun$1.apply(TickGenerator.scala:41)\n        at spray.io.TickGenerator$$anon$1$$anon$2$$anonfun$1.apply(TickGenerator.scala:38)\n        at spray.io.ConnectionHandler$$anonfun$running$1.applyOrElse(ConnectionHandler.scala:58)\n        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:165)\n        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n        at akka.actor.ActorCell.invoke(ActorCell.scala:456)\n        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n        at akka.dispatch.Mailbox.run(Mailbox.scala:219)\n        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n```\n. Just tested with RC4 and confirmed the fix. Sorry; I wanted to test this with the nightly last Friday but ended up getting busy with other stuff.\n. +1\n. LGTM - not sure why shapeless is imported optionally in routing though (unrelated to this PR). I'd think there's not much you can do with spray-routing without shapeless.\n. Agreed\n\nFYI - If I understand correctly, akka-http will not be using shapeless. So we may never get the benefit of using the shapeless OSGi manifest headers. Another option is to ask Miles to build a shapeless 1.2.5 that is BC with 1.2.4 but includes the manifest headers.\n. @rkrzewski The PR removed the override that sets the import to optional. BND should still add the shapeless imports. Can you check the generated MANIFEST.MF?\n. ",
    "RichardBradley": "I'm trying to use `incoming-auto-chunking-threshold-size` to stream in large requests, while using only bounded local memory, similar to [mpilquist's comment](https://github.com/spray/spray/issues/281#issuecomment-23356271).\n\nMy backing store is slow (a downstream SOAP service).\n\nI am finding that Spray is slurping in the whole request as fast as it can, and holding the whole thing in local memory as `MessageChunk`s. I've put some details on [StackOverflow](http://stackoverflow.com/questions/23077215).\n\nAm I right in thinking that this feature only actually satisfies the stated use case if you can process the chunks faster than the client can send them? If so, should we add that to the docs for this setting, or is it simply implied?\n\nIs there anything I can do to limit memory usage if I have a fast connection to the client but a slow backing store?\n. Thanks, I am arranging for the CLA to be signed by the relevant powers that be. I'll reformat the PR then.\n. ETA 2-3 months for the CLA\n. (I never got around to changing the commit message, sorry. I did finally get the CLA signed off, as you have noticed :))\n. This bug prevents round-tripping empty multipart `BodyParts` through marshall / unmarshall, as demonstrated in the following test:\n\n```\nimport org.specs2.mutable.Specification\nimport spray.http._\nimport spray.httpx.marshalling._\nimport spray.httpx.unmarshalling._\n\nclass SprayMimeSpec\n  extends Specification {\n\n  \"spray multipart MIME support\" should {\n\n    \"round-trip an empty multipart message\" in {\n\n      val parts = MultipartFormData(Map(\n        \"message\" -> BodyPart(HttpEntity.Empty)\n      ))\n\n      val entity: HttpEntity.NonEmpty = marshalUnsafe(parts).toOption.get\n\n      println(entity.asString)\n      /**\n       * Here the entity looks like:\n--VVzFwaDWx5e8ZTUARUUAsL+s\nContent-Disposition: form-data; name=message\n--VVzFwaDWx5e8ZTUARUUAsL+s--\n\n       * This is a bug; we need to transform it more like:\n--VVzFwaDWx5e8ZTUARUUAsL+s\nContent-Disposition: form-data; name=message\n\n--VVzFwaDWx5e8ZTUARUUAsL+s--\n\n       * If you attempt to unmarshal the broken version of the message, then\n       * the MIME unmarshallers get confused and treat the Content-Disposition\n       * header as part of the entity content for the BodyPart\n       */\n      val correctedStr = entity.asString.replace(\"message\\r\\n--\", \"message\\r\\n\\r\\n--\")\n      println(correctedStr)\n\n      val correctedEntity = entity.copy(data = HttpData(correctedStr).asInstanceOf[HttpData.NonEmpty])\n\n      val roundtrippedParts = correctedEntity.as[MultipartFormData].right.get\n\n      roundtrippedParts mustEqual parts\n    }\n  }\n}\n```\n. Interesting -- those grammars do look to support the above syntax. Perhaps the marshaller is correct and we just need to fix the unmarshaller.\n\nMy initial reading above was based on this bit of [RFC1341](http://www.w3.org/Protocols/rfc1341/7_2_Multipart.html):\n\n```\nmultipart-body := preamble 1*encapsulation \n               close-delimiter epilogue \n\nencapsulation := delimiter CRLF body-part \n```\n\nWhere the CRLF is not shown as optional. I think that RFC is probably superseded by your links though.\n\nI'll see if I can find any real MIME messages with empty parts from other apps. I had a look before raising this issue, but I haven't found any yet.\n. Yes, you're right. And RFC 822 says that the CRLF is optional (but also allowed) if the body is omitted, in the grammar you posted above.\n\nSo this is an unmarshaller bug.\n. I think this is a bug in the underlying `org.jvnet.mimepull` library:\n\n```\nimport collection.JavaConversions._\nimport java.io._\n\n    \"org.jvnet.mimepull should understand empty BodyParts\" in {\n      val body = \"\"\"mime preamble\nmime preamble\n--rNmigcMAf6HIo0Q2xJ9-vjkH\nContent-Disposition: form-data; name=root-fields\n--rNmigcMAf6HIo0Q2xJ9-vjkH--\"\"\"\n\n      val mm = new org.jvnet.mimepull.MIMEMessage(\n        new ByteArrayInputStream(body.getBytes), \"rNmigcMAf6HIo0Q2xJ9-vjkH\")\n\n      mm.parseAll()\n      mm.getAttachments.toList must beLike {\n        case List(att) =>\n          CharStreams.toString(new InputStreamReader(att.readOnce)) mustEqual \"\"\n      }\n    }\n```\n\nfails with `'Content-Disposition: form-data; name=root-fields' is not equal to ''`\n\nI'll raise a ticket with them, but I don't know how often they release updates. \n. https://java.net/jira/browse/MIMEPULL-12\n. @jrudolph -- there were two separate issues above: 1) malformed urls not caught and 2) an exception at the wrong part of the http stack causes a timeout instead of a Failure.\n\nAm I right in thinking that your patch addresses only (1)?\n\nDo you think it's worth looking into (2)? Maybe it's fine to leave this, as Akka 2.3 HTTP will replace all that code soon enough.\n. Ok, thanks, agreed.\n. > Could you accept the Typesafe CLA (only a few clicks) so we can merge it?\n\nI hope to do this soon, but I'm having bureaucratic issues at this end. I'll update here and my other PRs when I can.\n. Thanks. \nThe delay was mostly at my end due to issues with getting the CLA signed off here.\n. Thanks\n. During the test, the following dead letter is logged:\n\n```\n[INFO] [10/01/2015 08:20:06.677] [default-akka.actor.default-dispatcher-5] [akka://default/system/IO-TCP/selectors/$a/1] Message [akka.io.Tcp$Close$] \n  from Actor[akka://default/user/IO-HTTP/group-0/0#424065311] to Actor[akka://default/system/IO-TCP/selectors/$a/1#826331771] was not delivered.\n   [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.\n```\n\nThe two actors involved have the following classes:\n\n```\nActor[akka://default/user/IO-HTTP/group-0/0#424065311] => spray.can.client.HttpClientConnection\nActor[akka://default/system/IO-TCP/selectors/$a/1#826331771] => akka.io.TcpOutgoingConnection\n```\n\nI think that the connection actor should send a connection closed message to the client actor.\n. The relevant docs about this are: \n- http://spray.io/documentation/1.2.3/spray-can/http-client/#chunked-requests\n- http://spray.io/documentation/1.2.3/spray-can/http-client/connection-level/#connectionlevelapi\n\nIn particular, the second says:\n\n> Close notification events are dispatched to the senders of all requests that still have unfinished responses pending as well as all actors that might have already sent Http.CloseCommand messages.\n\n... which isn't happening in this case, which is a bug IMO.\n\nI can work around this for now by watching the connection actor for its death.\n. You made one key change, which means your version doesn't do what I need:\nIn the server, you replaced:\n\n```\nsender() ! HttpResponse(StatusCodes.RequestEntityTooLarge, headers = Connection(\"close\") :: Nil)\n```\n\nwith:\n\n```\nsender() ! Http.Abort\n```\n\nThis means that the client never receives the error message response; it just sees an aborted connection.\n\nSo my questions are:\n1. Why does the server using \"Http.Abort\" cause the client to notice the closed connection, whereas the server using \"Connection: close\" leaves the client in zombie state (uploading to a dead connection). Are these two different kinds of \"close\"?\n2. How can I both return a response and then close the connection? I'd like to include the max upload size in the response in my app. Responding with \"connection: close\" seems to work in Akka HTTP, see #18203\n. Hmm, ok, thanks.\n\nSo you are saying that the precise wording of the docs applies here:\n\n> Close notification events are dispatched to the senders of all requests that still have unfinished responses pending as well as all actors that might have already sent Http.CloseCommand messages.\n\ni.e. because this client does not have an unfinished response pending, it won't get notified of the close?\n\nThat seems like a tricky edge case to me. Why not either a) have the client watch the connection actor for death in all cases, rather than send close notifications at all or b) include uncompleted requests in this list of who gets notified?\n\nMaybe I'll clarify the docs. This only applies to chunked requests. How about the following addition to http://spray.io/documentation/1.2.3/spray-can/http-client/#chunked-requests ?\n\n> Very large chunked requests (large uploads) are at risk of the server responding early and closing the connection (for example with a 413 response). Clients sending a large chunked request should watch for either a `Tcp.ConnectionClosed` message or a `HttpResponse` message from Spray and stop sending the request on receipt of either one. (See RFC 7230 section 6.5.) (Alternatively, you could watch for the death of the connection actor and stop the upload in that event.)\n. @jrudolph - here's the doc note about detecting aborted uploads from chunked HTTP request clients discussed at #1073.\n\nI haven't been able to figure out how build the docs from SBT, so I'm not sure that this will render correctly. This is my best guess based on the surrounding \"rst\" markup.\n\nDo you know how to build the HTML docs? (Is it possible on Windows?)\n. I tried to build the documentation based on those instructions, using a CentOS VM.\nAfter a few false starts, I am now stuck at:\n\n```\n[vagrant@localhost spray]$ sbt\n[info] Loading project definition from /spray/project\n[info] Set current project to root (in build file:/spray/)\nroot > project site\n[info] Set current project to site (in build file:/spray/)\nsite > compile\n[success] Total time: 11 s, completed Dec 2, 2015 9:26:57 PM\nsite > re-start\n[info] Recompiling Sphinx sources...\n[info] /usr/bin/sphinx-build -b json -d /spray/site/target/scala-2.10/resource_managed/main/sphinx/doctrees -D version=1.1-SNAPSHOT -D release=1.1-SNAPSHOT /spray/docs /spray/site/target/scala-2.10/resource_managed/main/sphinx/json\n[info] Running Sphinx v0.6.6\n[error]\n[error] Exception occurred:\n[error]   File \"/spray/docs/_sphinx/exts/includecode.py\", line 108, in setup\n[error]     app.require_sphinx('1.0')\n[error] AttributeError: 'Sphinx' object has no attribute 'require_sphinx'\n[error] The full traceback has been saved in /tmp/sphinx-err-JfZXXw.log, if you want to report the issue to the developers.\n[error] Please also report this if it was a user error, so that a better error message can be provided next time.\n[error] Either send bugs to the mailing list at <http://groups.google.com/group/sphinx-dev/>,\n[error] or report them in the tracker at <http://bitbucket.org/birkenfeld/sphinx/issues/>. Thanks!\n[trace] Stack trace suppressed: run last site/*:sphinxCompile for the full output.\n[error] (site/*:sphinxCompile) Error compiling sphinx sources\n[error] Total time: 21 s, completed Dec 2, 2015 9:22:30 PM\n```\n\nWould it be possible for you to check the \"rst\" markup, if this builds for you?\nOr let me know how to fix the above build error.\nThanks,\n\nRich\n. (See also #1073.)\n. I'm trying to use `incoming-auto-chunking-threshold-size` to stream in large requests, while using only bounded local memory, similar to [mpilquist's comment](https://github.com/spray/spray/issues/281#issuecomment-23356271).\n\nMy backing store is slow (a downstream SOAP service).\n\nI am finding that Spray is slurping in the whole request as fast as it can, and holding the whole thing in local memory as `MessageChunk`s. I've put some details on [StackOverflow](http://stackoverflow.com/questions/23077215).\n\nAm I right in thinking that this feature only actually satisfies the stated use case if you can process the chunks faster than the client can send them? If so, should we add that to the docs for this setting, or is it simply implied?\n\nIs there anything I can do to limit memory usage if I have a fast connection to the client but a slow backing store?\n. Thanks, I am arranging for the CLA to be signed by the relevant powers that be. I'll reformat the PR then.\n. ETA 2-3 months for the CLA\n. (I never got around to changing the commit message, sorry. I did finally get the CLA signed off, as you have noticed :))\n. This bug prevents round-tripping empty multipart `BodyParts` through marshall / unmarshall, as demonstrated in the following test:\n\n```\nimport org.specs2.mutable.Specification\nimport spray.http._\nimport spray.httpx.marshalling._\nimport spray.httpx.unmarshalling._\n\nclass SprayMimeSpec\n  extends Specification {\n\n  \"spray multipart MIME support\" should {\n\n    \"round-trip an empty multipart message\" in {\n\n      val parts = MultipartFormData(Map(\n        \"message\" -> BodyPart(HttpEntity.Empty)\n      ))\n\n      val entity: HttpEntity.NonEmpty = marshalUnsafe(parts).toOption.get\n\n      println(entity.asString)\n      /**\n       * Here the entity looks like:\n--VVzFwaDWx5e8ZTUARUUAsL+s\nContent-Disposition: form-data; name=message\n--VVzFwaDWx5e8ZTUARUUAsL+s--\n\n       * This is a bug; we need to transform it more like:\n--VVzFwaDWx5e8ZTUARUUAsL+s\nContent-Disposition: form-data; name=message\n\n--VVzFwaDWx5e8ZTUARUUAsL+s--\n\n       * If you attempt to unmarshal the broken version of the message, then\n       * the MIME unmarshallers get confused and treat the Content-Disposition\n       * header as part of the entity content for the BodyPart\n       */\n      val correctedStr = entity.asString.replace(\"message\\r\\n--\", \"message\\r\\n\\r\\n--\")\n      println(correctedStr)\n\n      val correctedEntity = entity.copy(data = HttpData(correctedStr).asInstanceOf[HttpData.NonEmpty])\n\n      val roundtrippedParts = correctedEntity.as[MultipartFormData].right.get\n\n      roundtrippedParts mustEqual parts\n    }\n  }\n}\n```\n. Interesting -- those grammars do look to support the above syntax. Perhaps the marshaller is correct and we just need to fix the unmarshaller.\n\nMy initial reading above was based on this bit of [RFC1341](http://www.w3.org/Protocols/rfc1341/7_2_Multipart.html):\n\n```\nmultipart-body := preamble 1*encapsulation \n               close-delimiter epilogue \n\nencapsulation := delimiter CRLF body-part \n```\n\nWhere the CRLF is not shown as optional. I think that RFC is probably superseded by your links though.\n\nI'll see if I can find any real MIME messages with empty parts from other apps. I had a look before raising this issue, but I haven't found any yet.\n. Yes, you're right. And RFC 822 says that the CRLF is optional (but also allowed) if the body is omitted, in the grammar you posted above.\n\nSo this is an unmarshaller bug.\n. I think this is a bug in the underlying `org.jvnet.mimepull` library:\n\n```\nimport collection.JavaConversions._\nimport java.io._\n\n    \"org.jvnet.mimepull should understand empty BodyParts\" in {\n      val body = \"\"\"mime preamble\nmime preamble\n--rNmigcMAf6HIo0Q2xJ9-vjkH\nContent-Disposition: form-data; name=root-fields\n--rNmigcMAf6HIo0Q2xJ9-vjkH--\"\"\"\n\n      val mm = new org.jvnet.mimepull.MIMEMessage(\n        new ByteArrayInputStream(body.getBytes), \"rNmigcMAf6HIo0Q2xJ9-vjkH\")\n\n      mm.parseAll()\n      mm.getAttachments.toList must beLike {\n        case List(att) =>\n          CharStreams.toString(new InputStreamReader(att.readOnce)) mustEqual \"\"\n      }\n    }\n```\n\nfails with `'Content-Disposition: form-data; name=root-fields' is not equal to ''`\n\nI'll raise a ticket with them, but I don't know how often they release updates. \n. https://java.net/jira/browse/MIMEPULL-12\n. @jrudolph -- there were two separate issues above: 1) malformed urls not caught and 2) an exception at the wrong part of the http stack causes a timeout instead of a Failure.\n\nAm I right in thinking that your patch addresses only (1)?\n\nDo you think it's worth looking into (2)? Maybe it's fine to leave this, as Akka 2.3 HTTP will replace all that code soon enough.\n. Ok, thanks, agreed.\n. > Could you accept the Typesafe CLA (only a few clicks) so we can merge it?\n\nI hope to do this soon, but I'm having bureaucratic issues at this end. I'll update here and my other PRs when I can.\n. Thanks. \nThe delay was mostly at my end due to issues with getting the CLA signed off here.\n. Thanks\n. During the test, the following dead letter is logged:\n\n```\n[INFO] [10/01/2015 08:20:06.677] [default-akka.actor.default-dispatcher-5] [akka://default/system/IO-TCP/selectors/$a/1] Message [akka.io.Tcp$Close$] \n  from Actor[akka://default/user/IO-HTTP/group-0/0#424065311] to Actor[akka://default/system/IO-TCP/selectors/$a/1#826331771] was not delivered.\n   [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.\n```\n\nThe two actors involved have the following classes:\n\n```\nActor[akka://default/user/IO-HTTP/group-0/0#424065311] => spray.can.client.HttpClientConnection\nActor[akka://default/system/IO-TCP/selectors/$a/1#826331771] => akka.io.TcpOutgoingConnection\n```\n\nI think that the connection actor should send a connection closed message to the client actor.\n. The relevant docs about this are: \n- http://spray.io/documentation/1.2.3/spray-can/http-client/#chunked-requests\n- http://spray.io/documentation/1.2.3/spray-can/http-client/connection-level/#connectionlevelapi\n\nIn particular, the second says:\n\n> Close notification events are dispatched to the senders of all requests that still have unfinished responses pending as well as all actors that might have already sent Http.CloseCommand messages.\n\n... which isn't happening in this case, which is a bug IMO.\n\nI can work around this for now by watching the connection actor for its death.\n. You made one key change, which means your version doesn't do what I need:\nIn the server, you replaced:\n\n```\nsender() ! HttpResponse(StatusCodes.RequestEntityTooLarge, headers = Connection(\"close\") :: Nil)\n```\n\nwith:\n\n```\nsender() ! Http.Abort\n```\n\nThis means that the client never receives the error message response; it just sees an aborted connection.\n\nSo my questions are:\n1. Why does the server using \"Http.Abort\" cause the client to notice the closed connection, whereas the server using \"Connection: close\" leaves the client in zombie state (uploading to a dead connection). Are these two different kinds of \"close\"?\n2. How can I both return a response and then close the connection? I'd like to include the max upload size in the response in my app. Responding with \"connection: close\" seems to work in Akka HTTP, see #18203\n. Hmm, ok, thanks.\n\nSo you are saying that the precise wording of the docs applies here:\n\n> Close notification events are dispatched to the senders of all requests that still have unfinished responses pending as well as all actors that might have already sent Http.CloseCommand messages.\n\ni.e. because this client does not have an unfinished response pending, it won't get notified of the close?\n\nThat seems like a tricky edge case to me. Why not either a) have the client watch the connection actor for death in all cases, rather than send close notifications at all or b) include uncompleted requests in this list of who gets notified?\n\nMaybe I'll clarify the docs. This only applies to chunked requests. How about the following addition to http://spray.io/documentation/1.2.3/spray-can/http-client/#chunked-requests ?\n\n> Very large chunked requests (large uploads) are at risk of the server responding early and closing the connection (for example with a 413 response). Clients sending a large chunked request should watch for either a `Tcp.ConnectionClosed` message or a `HttpResponse` message from Spray and stop sending the request on receipt of either one. (See RFC 7230 section 6.5.) (Alternatively, you could watch for the death of the connection actor and stop the upload in that event.)\n. @jrudolph - here's the doc note about detecting aborted uploads from chunked HTTP request clients discussed at #1073.\n\nI haven't been able to figure out how build the docs from SBT, so I'm not sure that this will render correctly. This is my best guess based on the surrounding \"rst\" markup.\n\nDo you know how to build the HTML docs? (Is it possible on Windows?)\n. I tried to build the documentation based on those instructions, using a CentOS VM.\nAfter a few false starts, I am now stuck at:\n\n```\n[vagrant@localhost spray]$ sbt\n[info] Loading project definition from /spray/project\n[info] Set current project to root (in build file:/spray/)\nroot > project site\n[info] Set current project to site (in build file:/spray/)\nsite > compile\n[success] Total time: 11 s, completed Dec 2, 2015 9:26:57 PM\nsite > re-start\n[info] Recompiling Sphinx sources...\n[info] /usr/bin/sphinx-build -b json -d /spray/site/target/scala-2.10/resource_managed/main/sphinx/doctrees -D version=1.1-SNAPSHOT -D release=1.1-SNAPSHOT /spray/docs /spray/site/target/scala-2.10/resource_managed/main/sphinx/json\n[info] Running Sphinx v0.6.6\n[error]\n[error] Exception occurred:\n[error]   File \"/spray/docs/_sphinx/exts/includecode.py\", line 108, in setup\n[error]     app.require_sphinx('1.0')\n[error] AttributeError: 'Sphinx' object has no attribute 'require_sphinx'\n[error] The full traceback has been saved in /tmp/sphinx-err-JfZXXw.log, if you want to report the issue to the developers.\n[error] Please also report this if it was a user error, so that a better error message can be provided next time.\n[error] Either send bugs to the mailing list at <http://groups.google.com/group/sphinx-dev/>,\n[error] or report them in the tracker at <http://bitbucket.org/birkenfeld/sphinx/issues/>. Thanks!\n[trace] Stack trace suppressed: run last site/*:sphinxCompile for the full output.\n[error] (site/*:sphinxCompile) Error compiling sphinx sources\n[error] Total time: 21 s, completed Dec 2, 2015 9:22:30 PM\n```\n\nWould it be possible for you to check the \"rst\" markup, if this builds for you?\nOr let me know how to fix the above build error.\nThanks,\n\nRich\n. (See also #1073.)\n. ",
    "jdevelop": "Isn't it better to  have parametersMap return Map[String,RType], where RType is like\n\nabstract sealed class RType\ncase class Single(value: String) extends RType\ncase class Multiple(value: Array(String)) extends RType\n?\n. I would say that Servlet specification doesn't state parameter order at all, so for most cases ability to get array of values is much better, than doing custom parsing and aggregation.\n. This is related in the way that most widely used specification in Java world doesn't care about the order, so why Spray should do some specific thing about order of request parameters?\n\nAs far as I understand, if somebody is really interested in getting parameters in some order - he can always get access to RawUrl and parse it. But most of scenarios wouldn't care of the values order.\n. Isn't it better to  have parametersMap return Map[String,RType], where RType is like\n\nabstract sealed class RType\ncase class Single(value: String) extends RType\ncase class Multiple(value: Array(String)) extends RType\n?\n. I would say that Servlet specification doesn't state parameter order at all, so for most cases ability to get array of values is much better, than doing custom parsing and aggregation.\n. This is related in the way that most widely used specification in Java world doesn't care about the order, so why Spray should do some specific thing about order of request parameters?\n\nAs far as I understand, if somebody is really interested in getting parameters in some order - he can always get access to RawUrl and parse it. But most of scenarios wouldn't care of the values order.\n. ",
    "mikemckibben": "Updated submission to use pattern match over list. Does this look similar to what you had in mind?\n. Great! Commits are now rebased into one.\n. amended the commit message per spray contributing policy\n. Hmm, maybe I'm interpreting this wrong but to me this error condition is a malformed request, since the resource is found, but a required query parameter is missing.\n\n400 Bad Request\n\nThe request could not be understood by the server due to malformed syntax. The client SHOULD NOT repeat the request without modifications.\n\n404 Not Found\n\nThe server has not found anything matching the Request-URI. No indication is given of whether the condition is temporary or permanent. The 410 (Gone) status code SHOULD be used if the server knows, through some internally configurable mechanism, that an old resource is permanently unavailable and has no forwarding address. This status code is commonly used when the server does not wish to reveal exactly why the request has been refused, or when no other response is applicable.\n\nMost APIs I've used return 400 for this error condition, for example AWS (http://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/CommonErrors.html), or Netflix (http://developer.netflix.com/docs/HTTP_Status_Codes), windows azure (http://msdn.microsoft.com/en-us/library/windowsazure/dd179357.aspx). But if this behavior is as designed in spray, I won't argue too strongly against it. It's simple enough to provide a non-default rejection handler to override this.\n\n--Mike\n. Makes sense. Thanks for the explanation!\n. Updated submission to use pattern match over list. Does this look similar to what you had in mind?\n. Great! Commits are now rebased into one.\n. amended the commit message per spray contributing policy\n. Hmm, maybe I'm interpreting this wrong but to me this error condition is a malformed request, since the resource is found, but a required query parameter is missing.\n\n400 Bad Request\n\nThe request could not be understood by the server due to malformed syntax. The client SHOULD NOT repeat the request without modifications.\n\n404 Not Found\n\nThe server has not found anything matching the Request-URI. No indication is given of whether the condition is temporary or permanent. The 410 (Gone) status code SHOULD be used if the server knows, through some internally configurable mechanism, that an old resource is permanently unavailable and has no forwarding address. This status code is commonly used when the server does not wish to reveal exactly why the request has been refused, or when no other response is applicable.\n\nMost APIs I've used return 400 for this error condition, for example AWS (http://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/CommonErrors.html), or Netflix (http://developer.netflix.com/docs/HTTP_Status_Codes), windows azure (http://msdn.microsoft.com/en-us/library/windowsazure/dd179357.aspx). But if this behavior is as designed in spray, I won't argue too strongly against it. It's simple enough to provide a non-default rejection handler to override this.\n\n--Mike\n. Makes sense. Thanks for the explanation!\n. ",
    "alippai": "I am sorry for the short description. So a longer one:\n- platform: both Windows 8 and Debian Squeeze\n- scala version: 2.10.1\n- spray-can version: fresh from GitHub and 1.1-20130511\n- akka-actor version: 2.1.2\n- `sbt \"project server-benchmark\" run`\n\nTesting with:\n- `siege -c10 -b http://xxxx:8080/ping` from Linux\n  or\n- `ab -c4 -n10000 http://xxxx:8080/ping` both from Linux and Windows\n\nHowever I can't reproduce the bug with:\n- spray-can version: 1.2-20130501\n- akka-actor version: 2.2-M3\n\nAbout the hardware env:\n- Linux: Dual-core Xeon with HT, 4GB RAM\n- Windows: core i5-2500, 8GB RAM\n- Network: local, same switch, 100MBit ethernet\n. Keep alive fixes the problem and also eliminates the 3sec long responses.\n. I am sorry for the short description. So a longer one:\n- platform: both Windows 8 and Debian Squeeze\n- scala version: 2.10.1\n- spray-can version: fresh from GitHub and 1.1-20130511\n- akka-actor version: 2.1.2\n- `sbt \"project server-benchmark\" run`\n\nTesting with:\n- `siege -c10 -b http://xxxx:8080/ping` from Linux\n  or\n- `ab -c4 -n10000 http://xxxx:8080/ping` both from Linux and Windows\n\nHowever I can't reproduce the bug with:\n- spray-can version: 1.2-20130501\n- akka-actor version: 2.2-M3\n\nAbout the hardware env:\n- Linux: Dual-core Xeon with HT, 4GB RAM\n- Windows: core i5-2500, 8GB RAM\n- Network: local, same switch, 100MBit ethernet\n. Keep alive fixes the problem and also eliminates the 3sec long responses.\n. ",
    "ereichert": "It looks to be fixed.  Thanks.  \n\nWe're still seeing the illegal chunk termination errors but I believe that's because of dropped data.  I'm working on an example based on public data now.\n. We're in business.  Thanks for getting this done so quickly.  Was this included in last night's nightly?\n. Sorry.  Dumb question.  Got excited and didn't check the dates.\n\nThanks again.\n. It looks to be fixed.  Thanks.  \n\nWe're still seeing the illegal chunk termination errors but I believe that's because of dropped data.  I'm working on an example based on public data now.\n. We're in business.  Thanks for getting this done so quickly.  Was this included in last night's nightly?\n. Sorry.  Dumb question.  Got excited and didn't check the dates.\n\nThanks again.\n. ",
    "michael-smith": "Currently using spray 1.1-M8 with marshallers and would like to be able to set custom headers from my marshaller.  Similarly, it would be nice to have access to the headers in an unmarshaller instead of just the entity.  Is this still planned for 1.1?\n. +1 This looks exactly like what I want. \n. Currently using spray 1.1-M8 with marshallers and would like to be able to set custom headers from my marshaller.  Similarly, it would be nice to have access to the headers in an unmarshaller instead of just the entity.  Is this still planned for 1.1?\n. +1 This looks exactly like what I want. \n. ",
    "rbuckland": "Simple +1 . That sounds very reasonable to me. It would solve my issue neatly! \n. Simple +1 . That sounds very reasonable to me. It would solve my issue neatly! \n. ",
    "newlandsvalley": "+1. Me too.  I hope it will allow me to marshall Validations without having to resort to throwing exceptions for Validation failures.\n. +1. Me too.  I hope it will allow me to marshall Validations without having to resort to throwing exceptions for Validation failures.\n. ",
    "jimfulton": "Well, I actually _also_ want to specify a weigher.\n\nI can copy the SimpleLRUCache class, which I will in the short term.\n\nMaybe it would be better if it:\n- It was possible to subclass the class\n- and construction of the store was moved to a method that could be overridden.\n. Gaaa. My use case probably doesn't fit this design, at least not if I want to take file size into account, as the file size of a future file isn't known when it's added to the cache.\n. Well, I actually _also_ want to specify a weigher.\n\nI can copy the SimpleLRUCache class, which I will in the short term.\n\nMaybe it would be better if it:\n- It was possible to subclass the class\n- and construction of the store was moved to a method that could be overridden.\n. Gaaa. My use case probably doesn't fit this design, at least not if I want to take file size into account, as the file size of a future file isn't known when it's added to the cache.\n. ",
    "sclasen": "Oh hey sorry for the short ticket, thought the TODO in code would explain. The parameters look to be parsed, but the subsequently dropped by a parser rule. https://github.com/spray/spray/blob/master/spray-http/src/main/scala/spray/http/parser/AcceptHeader.scala#L30\n\nIn my particular case Heroku versions its API using parameters in the Accept header.\n\n`Accept: application/vnd.heroku+json; version=3`\n\nCurently spray just trashes anything past the `;`.  I really only need the string representation to not be truncated in this particular case, but realistically spray should model the parameters, looks like in MediaType.\n\nWould be great If I could do \n\n```\nrequest.header[`Accept`].parameter(\"version\")\n```\n\nbut If I could just get the full accept string value I would be set for now. \n. Ping.\n\nIs there any way to get at the raw headers or customize the parsing of headers in spray-can such that I could hack a work-around to this?\n. Awesome!\n\nSent from my iPhone\n\nOn Jun 19, 2013, at 3:35 AM, Mathias notifications@github.com wrote:\n\n> Submitted a PR fixing this. I'll probably be reviewed (by @jrudolph) and merged today, so tomorrows nightly should allow you to access your version parameter.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. Oh hey sorry for the short ticket, thought the TODO in code would explain. The parameters look to be parsed, but the subsequently dropped by a parser rule. https://github.com/spray/spray/blob/master/spray-http/src/main/scala/spray/http/parser/AcceptHeader.scala#L30\n\nIn my particular case Heroku versions its API using parameters in the Accept header.\n\n`Accept: application/vnd.heroku+json; version=3`\n\nCurently spray just trashes anything past the `;`.  I really only need the string representation to not be truncated in this particular case, but realistically spray should model the parameters, looks like in MediaType.\n\nWould be great If I could do \n\n```\nrequest.header[`Accept`].parameter(\"version\")\n```\n\nbut If I could just get the full accept string value I would be set for now. \n. Ping.\n\nIs there any way to get at the raw headers or customize the parsing of headers in spray-can such that I could hack a work-around to this?\n. Awesome!\n\nSent from my iPhone\n\nOn Jun 19, 2013, at 3:35 AM, Mathias notifications@github.com wrote:\n\n> Submitted a PR fixing this. I'll probably be reviewed (by @jrudolph) and merged today, so tomorrows nightly should allow you to access your version parameter.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. ",
    "martijnhoekstra": "I'm working on some things over at https://github.com/martijnhoekstra/spray/tree/cookiejar\nFor now I'm still running in problems with non-latin domains, my inability to get sbt to do what I want to to, and with uncertainty on the spec with regards to cookie paths.\n\nAbout the paths, I'm not sure if multiple values of the cookie should exist for different paths on the same host, and if they do, what cookies should be retrieved if a path matches more than one identically named cookie. According to the specs it seems that both should be sent, but I think most browsers implement that differently. How does spray feel those should be handled?\n\nI starting to think the way I integrated it is a bad idea, and should just expose the pipeline stages seperately, but the cookie implementation itself seems to mostly work\n. I've been trying this from another angle at https://github.com/martijnhoekstra/spray-cookies\n\nif you have any feedback on it I'd love to hear it.\n. Note, this tests fine on travis, but I notice now this is actually broken for machines with a default timezone other than GMT. Should not yet be merged.\n. That should be it\n. Ah, I missed that that was intentional. It seemed like it should be an identity at the time, and I actually think that that should still be the case, it not being an identity feels plain wrong to me; DateTime(clicks) seems like it does support millisecond precision. If it doesn't, it would make more sense to accept seconds rather than milliseconds, and have the user do any rounding or truncating they may want to do themselves. Whitespace changes by the way are fixed back to to standard form in 35a333d - they should never have been there in the first place.\n. I figured by the way that those two heap allocations would not be a big deal. I was wrong, they are a terribly big deal. I'll make sure to have a pull request that doesn't allocate on the heap soon, and fixes all outstanding comments. (would you like to have a single squashed commit, or a commit chain btw?). Whether or not you want millisecond precision or not is then up to you. I do understand there is limited use for millisecond precision within the library, but on the other hand, failing that identity is IMO pretty ugly.\n. Look at us doing the awkard dance of text-only based communication misunderstanding (I was offended, and realised shortly afterwards that I had no reason to be and was acting like an idiot about it, which is why I edited above comment to try to take out the original passive aggressive tone, which I clearly didn't fully succeed in. No need for apologies at all).\n\nThe original \"use case\" is not so much a use case, but more a violation of least surprise, I had failing unit tests that led back to the failure in roundtrip on millis, which was somewhat annoying to figure out.\n\nWhen about the heap allocations I say \"I was wrong, they are a terribly big deal\" I mean it: I do understand this is a very hot code path, and after benchmarking and finding out it was being outperformed a factor 300 by the original code, I was caught by surprise, and I realised that this doesn't count as a \"a bit slower\" and does count as \"a big deal\".\n\nThe computations currently at the apply method are a bit complicated, and I'm not 100% sure why I am still failing unit tests if I just ajust that - but I'll get there in the course of this week. I hope that'll result in a pull request you're happy to merge, and will actually be an improvement.\n\ntaking seconds in apply might have been a good idea at some point (also up for debate), but I fully agree that changing it in this point of the spray lifecycle is completely indefensible.\n. On Jun 11, 2013 12:32 PM, \"Mathias\" notifications@github.com wrote:\n\n> > ... it was being outperformed a factor 300 by the original code ...\n> \n> Interesting. I wouldn't have expected such a large difference. Do you\n> still have your benchmarking code around somewhere?\n\nIt was a super basic loop with java Date now used as a crude stopwatch,\ngoing through it 100000 or maybe 1000000 times, I fiddled with it a bit. I\ndon't have it around anymore, but I can probably trivially reproduce it, so\nstay tuned. both versions of the body inlined (and it used the second\ncommit with two heap allocations per loop)\n\n> \u2014\n> Reply to this email directly or view it on GitHub.\n. I figured it had something to do with my windows setup, which us relatively rare I suppose (it does seem to lead to other test errors in encoding), but this one was indeed fixed by stack space expansion for sbt. Confirmd as not a bug.\n. I'm working on some things over at https://github.com/martijnhoekstra/spray/tree/cookiejar\nFor now I'm still running in problems with non-latin domains, my inability to get sbt to do what I want to to, and with uncertainty on the spec with regards to cookie paths.\n\nAbout the paths, I'm not sure if multiple values of the cookie should exist for different paths on the same host, and if they do, what cookies should be retrieved if a path matches more than one identically named cookie. According to the specs it seems that both should be sent, but I think most browsers implement that differently. How does spray feel those should be handled?\n\nI starting to think the way I integrated it is a bad idea, and should just expose the pipeline stages seperately, but the cookie implementation itself seems to mostly work\n. I've been trying this from another angle at https://github.com/martijnhoekstra/spray-cookies\n\nif you have any feedback on it I'd love to hear it.\n. Note, this tests fine on travis, but I notice now this is actually broken for machines with a default timezone other than GMT. Should not yet be merged.\n. That should be it\n. Ah, I missed that that was intentional. It seemed like it should be an identity at the time, and I actually think that that should still be the case, it not being an identity feels plain wrong to me; DateTime(clicks) seems like it does support millisecond precision. If it doesn't, it would make more sense to accept seconds rather than milliseconds, and have the user do any rounding or truncating they may want to do themselves. Whitespace changes by the way are fixed back to to standard form in 35a333d - they should never have been there in the first place.\n. I figured by the way that those two heap allocations would not be a big deal. I was wrong, they are a terribly big deal. I'll make sure to have a pull request that doesn't allocate on the heap soon, and fixes all outstanding comments. (would you like to have a single squashed commit, or a commit chain btw?). Whether or not you want millisecond precision or not is then up to you. I do understand there is limited use for millisecond precision within the library, but on the other hand, failing that identity is IMO pretty ugly.\n. Look at us doing the awkard dance of text-only based communication misunderstanding (I was offended, and realised shortly afterwards that I had no reason to be and was acting like an idiot about it, which is why I edited above comment to try to take out the original passive aggressive tone, which I clearly didn't fully succeed in. No need for apologies at all).\n\nThe original \"use case\" is not so much a use case, but more a violation of least surprise, I had failing unit tests that led back to the failure in roundtrip on millis, which was somewhat annoying to figure out.\n\nWhen about the heap allocations I say \"I was wrong, they are a terribly big deal\" I mean it: I do understand this is a very hot code path, and after benchmarking and finding out it was being outperformed a factor 300 by the original code, I was caught by surprise, and I realised that this doesn't count as a \"a bit slower\" and does count as \"a big deal\".\n\nThe computations currently at the apply method are a bit complicated, and I'm not 100% sure why I am still failing unit tests if I just ajust that - but I'll get there in the course of this week. I hope that'll result in a pull request you're happy to merge, and will actually be an improvement.\n\ntaking seconds in apply might have been a good idea at some point (also up for debate), but I fully agree that changing it in this point of the spray lifecycle is completely indefensible.\n. On Jun 11, 2013 12:32 PM, \"Mathias\" notifications@github.com wrote:\n\n> > ... it was being outperformed a factor 300 by the original code ...\n> \n> Interesting. I wouldn't have expected such a large difference. Do you\n> still have your benchmarking code around somewhere?\n\nIt was a super basic loop with java Date now used as a crude stopwatch,\ngoing through it 100000 or maybe 1000000 times, I fiddled with it a bit. I\ndon't have it around anymore, but I can probably trivially reproduce it, so\nstay tuned. both versions of the body inlined (and it used the second\ncommit with two heap allocations per loop)\n\n> \u2014\n> Reply to this email directly or view it on GitHub.\n. I figured it had something to do with my windows setup, which us relatively rare I suppose (it does seem to lead to other test errors in encoding), but this one was indeed fixed by stack space expansion for sbt. Confirmd as not a bug.\n. ",
    "dlouwers": "This can already be done by adding a `Content-Disposition` header yourself. But where currently this header is naively added perhaps it could be merged? I'll formulate a pull request form this.\n. Yeah, sure, that would be even simpler to implement and more flexible as well. The only thing that feels weird is that it would render the map key moot. However it would open up the possibility of using multiple fields with the same name. Even though this is not allowed under the RFC a lot of API's count on this behavior building an \"array\" including the one I'm coding for now. In order to add more \"attachments\" I would need multiple fields with this name. With the current code this isn't possible but with the option of overriding the Content-Disposition this would indeed become an option. So sounds like a plan!\n. Done. Happy to hear your feedback.\n. Yeah, Rackspace Mailgun API supports this style though you can go the route\nmost client libraries go and rename parameters like \"parameter[0]\" to\n\"parameter[n]\", which is also supported by it. Their documentation is\nrather abstracted but after mailing them I got this confirmed.\nI guess we could do the same as most libraries do and that is rename fields\nfor the user. That would require us start using a Seq[(String, BodyPart)]\ninstead of a Map in order to allow duplicates that would be renamed. After\nthat we would group the ones with the same name and rename them. Your\nthoughts?\n\n2013/9/4 Johannes Rudolph notifications@github.com\n\n> Even though this is not allowed under the RFC a lot of API's count on this\n> behavior building an \"array\" including the one I'm coding for now.\n> \n> Are there any publicly available API descriptions for this behavior? Any\n> frameworks that support that style? I think it would be good to have some\n> supporting evidence why we'd want to deviate from the RFC.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/issues/327#issuecomment-23774608\n> .\n. I meant renaming them when marshaling to include be postfixed with \"[n]\"\nsince that is very commonly seen (cURL with -F, Ruby RestClient, python\nrequest etc.).\n\nAnother option is to have the user understand the RFC and handle parameter\nnaming themselves. Since I feel that RFC adherence and enforcing is in line\nwith most of Spray the following comes to mind:\n1. Disallow Content-Disposition, Content-Type and other headers that are\n   automatically handled to avoid duplicate headers. This needs to be checked\n   for both Multipart types.\n2. Change the values of MultipartFormData's Map to allow adding\n   additional parameters to the Content-Disposition header. At it's simplest\n   (BodyPart, Seq[HttpHeader]) OR like I proposed initially, merge in\n   additional headers (like filename) specified in a Content-Disposition\n   header which would maintain backwards compatibility better.\n3. Check that any parameters within headers don't contain duplicates\n\n2013/9/4 Johannes Rudolph notifications@github.com\n\n> That would require us start using a Seq[(String, BodyPart)]\n> instead of a Map in order to allow duplicates that would be renamed.\n> \n> Probably, that would be the consequence.\n> \n> After that we would group the ones with the same name and rename them.\n> \n> What do you mean by \"rename them\"? When unmarshalling? How would the\n> renaming work? Actually, since the behavior isn't standardized we should\n> try to support in the model what's there in the wild but don't make any\n> assumptions about how you would use it.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/issues/327#issuecomment-23777152\n> .\n. Please uncomment the \"text\" field to trigger the behavior. Commenting any other field makes it pass again. So looks like size is an issue.\n. Ah yes, it was just the unfortunate cut-off that got me. Thanks for helping me verify. Guess the correct fix would be to use ListMap in that test instead of Map.\nI was caught between a rock and a hard place trying to send multipart/form-data due to issue #327. I tried to solve it by using multipart/mixed and trying to switch out the header later with mapHeaders, but that wasn't allowed. So I just changed the multipart/mixed marshaller to return a multipart/form-data marshaller with the same mechanics as the multipart/mixed marshaller and that worked. \nFeels like it should be possible to have a bit more control over the request if needed. Is there a reason that logRequest only has access to part of the request that has been built? \n. Guess this issue can be closed now.\n. True, though it's arguably better than allowing duplicate headers as it\nstands now.\n\n2013/9/4 Johannes Rudolph notifications@github.com\n\n> The code itself LGTM. I'm not 100 percent sure if we really want to\n> support it that way because of the model being not in \"normal form\" any\n> more (see this commenthttps://github.com/spray/spray/issues/327#issuecomment-23710140).\n> @sirthias https://github.com/sirthias?\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/486#issuecomment-23774767\n> .\n. Looks good. Will offer more flexibility. Would you like to reinforce RFC\ncompliance? If so, shall we enforce them in the MultipartFormData\nconstructor? Same for MultipartContent?\n\nDirk\n\n2013/9/4 Mathias notifications@github.com\n\n> Ok, we have just discussed this.\n> Could you change the MultipartFormData model to this?:\n> \n> case class MultipartFormData(parts: Seq[BodyPart]) extends HttpForm {\n>   type FieldType = BodyPart\n>   def get(partName: String): Option[BodyPart] = ...\n>   def getOrElse(partName: String, default: => BodyPart): BodyPart = ...\n> }\n> \n> Also, I think we can improve the BodyPart model to this:\n> \n> case class BodyPart(entity: HttpEntity, headers: Seq[HttpHeader]) {\n>   def getNameOrEmpty: String = ...\n>   def getName: Option[String] = ...\n> }\n> object BodyPart {\n>   def apply(entity: HttpEntity, headers: HttpHeader_): BodyPart =\n>     apply(entity, headers: __)\n> }\n> \n> WDYT?\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/486#issuecomment-23792667\n> .\n. Hi Mathias,\n\nI've nearly worked your suggested model into the code. One side effect was\nthat I also had to change FormData since I had to change the type signature\nin HttpForm from Map[String, FieldType] to Seq[FieldType]. What I would\nlike to enforce is:\n- Reject BodyParts with the same name parameter in their\n  content-disposition header as well as duplicate parameters. I guess that by\n  looking at the RFC name parameters could be case sensitive.\n- Reject multiple headers of the same key disregarding case\n- Reject multiple same header parameters respecting case\n\nMore could be done but I guess this would improve things from where they\nstand now. We could create convenience factory methods that still accept\nMap[String, FieldType] to maintain backwards compatibility.\nYour thoughts?\n\n2013/9/5 Mathias notifications@github.com\n\n> What exactly would you like to enforce?\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/486#issuecomment-23877072\n> .\n. Ok, I am done now. The changes touched a bit more code than expected. I\nhave managed to make it backwards compatible using convenience factory\nmethods. It is however strictly speaking a breaking change since it adheres\nto the RFC more strictly.\n\nWhen working with the code I noticed that validation is done in the\nunmarshaller. I would like to suggest moving this logic into the\nMultiPartFormData constructor if that's ok with you.\n\nBest,\n\nDirk\n\n2013/9/10 Mathias notifications@github.com\n\n> @dlouwers https://github.com/dlouwers Yes, sounds good! This will\n> indeed be an improvement.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/486#issuecomment-24154669\n> .\n. Looks good to me! Thanks for going ahead and adding the improvements and doing the merge!\n. This can already be done by adding a `Content-Disposition` header yourself. But where currently this header is naively added perhaps it could be merged? I'll formulate a pull request form this.\n. Yeah, sure, that would be even simpler to implement and more flexible as well. The only thing that feels weird is that it would render the map key moot. However it would open up the possibility of using multiple fields with the same name. Even though this is not allowed under the RFC a lot of API's count on this behavior building an \"array\" including the one I'm coding for now. In order to add more \"attachments\" I would need multiple fields with this name. With the current code this isn't possible but with the option of overriding the Content-Disposition this would indeed become an option. So sounds like a plan!\n. Done. Happy to hear your feedback.\n. Yeah, Rackspace Mailgun API supports this style though you can go the route\nmost client libraries go and rename parameters like \"parameter[0]\" to\n\"parameter[n]\", which is also supported by it. Their documentation is\nrather abstracted but after mailing them I got this confirmed.\nI guess we could do the same as most libraries do and that is rename fields\nfor the user. That would require us start using a Seq[(String, BodyPart)]\ninstead of a Map in order to allow duplicates that would be renamed. After\nthat we would group the ones with the same name and rename them. Your\nthoughts?\n\n2013/9/4 Johannes Rudolph notifications@github.com\n\n> Even though this is not allowed under the RFC a lot of API's count on this\n> behavior building an \"array\" including the one I'm coding for now.\n> \n> Are there any publicly available API descriptions for this behavior? Any\n> frameworks that support that style? I think it would be good to have some\n> supporting evidence why we'd want to deviate from the RFC.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/issues/327#issuecomment-23774608\n> .\n. I meant renaming them when marshaling to include be postfixed with \"[n]\"\nsince that is very commonly seen (cURL with -F, Ruby RestClient, python\nrequest etc.).\n\nAnother option is to have the user understand the RFC and handle parameter\nnaming themselves. Since I feel that RFC adherence and enforcing is in line\nwith most of Spray the following comes to mind:\n1. Disallow Content-Disposition, Content-Type and other headers that are\n   automatically handled to avoid duplicate headers. This needs to be checked\n   for both Multipart types.\n2. Change the values of MultipartFormData's Map to allow adding\n   additional parameters to the Content-Disposition header. At it's simplest\n   (BodyPart, Seq[HttpHeader]) OR like I proposed initially, merge in\n   additional headers (like filename) specified in a Content-Disposition\n   header which would maintain backwards compatibility better.\n3. Check that any parameters within headers don't contain duplicates\n\n2013/9/4 Johannes Rudolph notifications@github.com\n\n> That would require us start using a Seq[(String, BodyPart)]\n> instead of a Map in order to allow duplicates that would be renamed.\n> \n> Probably, that would be the consequence.\n> \n> After that we would group the ones with the same name and rename them.\n> \n> What do you mean by \"rename them\"? When unmarshalling? How would the\n> renaming work? Actually, since the behavior isn't standardized we should\n> try to support in the model what's there in the wild but don't make any\n> assumptions about how you would use it.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/issues/327#issuecomment-23777152\n> .\n. Please uncomment the \"text\" field to trigger the behavior. Commenting any other field makes it pass again. So looks like size is an issue.\n. Ah yes, it was just the unfortunate cut-off that got me. Thanks for helping me verify. Guess the correct fix would be to use ListMap in that test instead of Map.\nI was caught between a rock and a hard place trying to send multipart/form-data due to issue #327. I tried to solve it by using multipart/mixed and trying to switch out the header later with mapHeaders, but that wasn't allowed. So I just changed the multipart/mixed marshaller to return a multipart/form-data marshaller with the same mechanics as the multipart/mixed marshaller and that worked. \nFeels like it should be possible to have a bit more control over the request if needed. Is there a reason that logRequest only has access to part of the request that has been built? \n. Guess this issue can be closed now.\n. True, though it's arguably better than allowing duplicate headers as it\nstands now.\n\n2013/9/4 Johannes Rudolph notifications@github.com\n\n> The code itself LGTM. I'm not 100 percent sure if we really want to\n> support it that way because of the model being not in \"normal form\" any\n> more (see this commenthttps://github.com/spray/spray/issues/327#issuecomment-23710140).\n> @sirthias https://github.com/sirthias?\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/486#issuecomment-23774767\n> .\n. Looks good. Will offer more flexibility. Would you like to reinforce RFC\ncompliance? If so, shall we enforce them in the MultipartFormData\nconstructor? Same for MultipartContent?\n\nDirk\n\n2013/9/4 Mathias notifications@github.com\n\n> Ok, we have just discussed this.\n> Could you change the MultipartFormData model to this?:\n> \n> case class MultipartFormData(parts: Seq[BodyPart]) extends HttpForm {\n>   type FieldType = BodyPart\n>   def get(partName: String): Option[BodyPart] = ...\n>   def getOrElse(partName: String, default: => BodyPart): BodyPart = ...\n> }\n> \n> Also, I think we can improve the BodyPart model to this:\n> \n> case class BodyPart(entity: HttpEntity, headers: Seq[HttpHeader]) {\n>   def getNameOrEmpty: String = ...\n>   def getName: Option[String] = ...\n> }\n> object BodyPart {\n>   def apply(entity: HttpEntity, headers: HttpHeader_): BodyPart =\n>     apply(entity, headers: __)\n> }\n> \n> WDYT?\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/486#issuecomment-23792667\n> .\n. Hi Mathias,\n\nI've nearly worked your suggested model into the code. One side effect was\nthat I also had to change FormData since I had to change the type signature\nin HttpForm from Map[String, FieldType] to Seq[FieldType]. What I would\nlike to enforce is:\n- Reject BodyParts with the same name parameter in their\n  content-disposition header as well as duplicate parameters. I guess that by\n  looking at the RFC name parameters could be case sensitive.\n- Reject multiple headers of the same key disregarding case\n- Reject multiple same header parameters respecting case\n\nMore could be done but I guess this would improve things from where they\nstand now. We could create convenience factory methods that still accept\nMap[String, FieldType] to maintain backwards compatibility.\nYour thoughts?\n\n2013/9/5 Mathias notifications@github.com\n\n> What exactly would you like to enforce?\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/486#issuecomment-23877072\n> .\n. Ok, I am done now. The changes touched a bit more code than expected. I\nhave managed to make it backwards compatible using convenience factory\nmethods. It is however strictly speaking a breaking change since it adheres\nto the RFC more strictly.\n\nWhen working with the code I noticed that validation is done in the\nunmarshaller. I would like to suggest moving this logic into the\nMultiPartFormData constructor if that's ok with you.\n\nBest,\n\nDirk\n\n2013/9/10 Mathias notifications@github.com\n\n> @dlouwers https://github.com/dlouwers Yes, sounds good! This will\n> indeed be an improvement.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/486#issuecomment-24154669\n> .\n. Looks good to me! Thanks for going ahead and adding the improvements and doing the merge!\n. ",
    "jsuereth": "@mpilquist +1 on that suggestion.  I think it provides quite an elegant way to beat the system and ensure you can access resource.conf files for relevant bundles.\n\n@havocp - what would you think of including such a beast right in typesafe-config for the OSGi masses?\n. @havocp You can also have osgi.jar an \"optional\" dependency.  But yeah, where should we continue discussion?  I think @mpilquist 's config loader combined with his existing OSGi changes are definitely the way forward for OSGi support of typesafe-config projects.   Is there an appropriate mailing list to have that discussion?\n. @mpilquist +1 on that suggestion.  I think it provides quite an elegant way to beat the system and ensure you can access resource.conf files for relevant bundles.\n\n@havocp - what would you think of including such a beast right in typesafe-config for the OSGi masses?\n. @havocp You can also have osgi.jar an \"optional\" dependency.  But yeah, where should we continue discussion?  I think @mpilquist 's config loader combined with his existing OSGi changes are definitely the way forward for OSGi support of typesafe-config projects.   Is there an appropriate mailing list to have that discussion?\n. ",
    "havocp": "I assume we don't want a dependency of the main typesafe config jar on an osgi jar (right?) but it could be a separate jar artifact perhaps inside the typesafe-config project. The only problem with that is that I'm currently the maintainer of the typesafe-config project and I don't know much about OSGi. Which puts me in a difficult position if anyone reports bugs, so it would be good to have some help on that.\n\nIt might be helpful (for me at least) to show how the simple lib example changes to support OSGi:\nhttps://github.com/typesafehub/config/blob/master/examples/simple-lib/src/main/scala/simplelib/SimpleLib.scala\n\nThough, maybe the change isn't to the lib but the user of the lib? Since otherwise every lib has to depend on OSGi?\nIn that case maybe we need an osgi app example alongside https://github.com/typesafehub/config/blob/master/examples/simple-app/src/main/scala/SimpleApp.scala  Maybe we could add an examples/osgi-app ? Which could use the little OSGi bridge jar.\n\nbtw if the Spray guys find this is a tangent (seems like one) feel free to move the discussion to a typesafe config issue or something.\n. Why don't we discuss it here: https://github.com/typesafehub/config/issues/79\n. I assume we don't want a dependency of the main typesafe config jar on an osgi jar (right?) but it could be a separate jar artifact perhaps inside the typesafe-config project. The only problem with that is that I'm currently the maintainer of the typesafe-config project and I don't know much about OSGi. Which puts me in a difficult position if anyone reports bugs, so it would be good to have some help on that.\n\nIt might be helpful (for me at least) to show how the simple lib example changes to support OSGi:\nhttps://github.com/typesafehub/config/blob/master/examples/simple-lib/src/main/scala/simplelib/SimpleLib.scala\n\nThough, maybe the change isn't to the lib but the user of the lib? Since otherwise every lib has to depend on OSGi?\nIn that case maybe we need an osgi app example alongside https://github.com/typesafehub/config/blob/master/examples/simple-app/src/main/scala/SimpleApp.scala  Maybe we could add an examples/osgi-app ? Which could use the little OSGi bridge jar.\n\nbtw if the Spray guys find this is a tangent (seems like one) feel free to move the discussion to a typesafe config issue or something.\n. Why don't we discuss it here: https://github.com/typesafehub/config/issues/79\n. ",
    "dzhulk": "Is there workaround?\n. Great! Will it be available as nightly build?\n. Is there workaround?\n. Great! Will it be available as nightly build?\n. ",
    "vladgurgov": "thanks for quick fix!\n. thanks for quick fix!\n. ",
    "lukiano": "Hey, I'm not using this connector, so there is no rush to merge it. I did it because (and I haven't done any benchmarks yet because I'm not quite sure what would be the best course of action) I was wondering IF (and a big if) it was possible that a servlet container that implemented 3.1 could be as fast and lightweight as Netty or Spray IO (well, Akka IO). Like saying, if you have to deploy your Spray server in a servlet, maybe the performance hit is minimal.\n. Hey, I'm not using this connector, so there is no rush to merge it. I did it because (and I haven't done any benchmarks yet because I'm not quite sure what would be the best course of action) I was wondering IF (and a big if) it was possible that a servlet container that implemented 3.1 could be as fast and lightweight as Netty or Spray IO (well, Akka IO). Like saying, if you have to deploy your Spray server in a servlet, maybe the performance hit is minimal.\n. ",
    "derekwyatt": "Done\n. I gave this a go, but I'm getting a ton of conflicts, not in my code.  My attempt was to rebase 9d48cc74546ea30334f9c37cf58c4dc1938d907f since that was my first commit, move all my bits to the end and squash them, but in trying to get there, git is reporting conflict after conflict during the rebasing operation.\n\nThis is beyond my git-fu.\n. OK. That's going to be a pain (for me) to do with how things are currently set up.  Why don't we just kill this pull request, and I'll clean things up on my side and we'll go again?\n. Johannes, thanks a lot.  I'm saddened by this though, because it means I really have to show off how ignorant I am of the whole github thing; I do basic branching and merging only.\n\nSo how would I manage this?  I currently have a master branch that's \"borked\".  You've got a very clean 336-repair branch.  How do I take the work you've done, integrate it into my forked repo, clean up my master, merge the commit over and then do the subsequent pull request?\n. Alright, I think I've got it.  Thanks for your help - I hope it's OK.\n\nhttps://github.com/spray/spray/pull/359\n. Poke\n. I'm getting our lawyer to go over 1) and for number 2) I'm not fussy.  I'm more than happy to merge it in and let you guys do the good voodoo you do.  I'm sure you guys will do the right thing, even if (or maybe, especially if) none of my original code remains :)\n\nI'll send you an email when I get 1) finished up.  Hopefully very soon.\n. You should have the CLA now.\n. Yeah, that was definitely discussed in the beginning, but at the time it was just too much of a PITA.  All I wanted was CodaHale metrics integrated, and it seemed like a reasonable first approach.  I'm sure the folks would appreciate an enhanced PR. :)\n. This was a pretty specific case and I agree that, if you're going to do anything about it, then it should just be for duplicates.  \n. Done\n. I gave this a go, but I'm getting a ton of conflicts, not in my code.  My attempt was to rebase 9d48cc74546ea30334f9c37cf58c4dc1938d907f since that was my first commit, move all my bits to the end and squash them, but in trying to get there, git is reporting conflict after conflict during the rebasing operation.\n\nThis is beyond my git-fu.\n. OK. That's going to be a pain (for me) to do with how things are currently set up.  Why don't we just kill this pull request, and I'll clean things up on my side and we'll go again?\n. Johannes, thanks a lot.  I'm saddened by this though, because it means I really have to show off how ignorant I am of the whole github thing; I do basic branching and merging only.\n\nSo how would I manage this?  I currently have a master branch that's \"borked\".  You've got a very clean 336-repair branch.  How do I take the work you've done, integrate it into my forked repo, clean up my master, merge the commit over and then do the subsequent pull request?\n. Alright, I think I've got it.  Thanks for your help - I hope it's OK.\n\nhttps://github.com/spray/spray/pull/359\n. Poke\n. I'm getting our lawyer to go over 1) and for number 2) I'm not fussy.  I'm more than happy to merge it in and let you guys do the good voodoo you do.  I'm sure you guys will do the right thing, even if (or maybe, especially if) none of my original code remains :)\n\nI'll send you an email when I get 1) finished up.  Hopefully very soon.\n. You should have the CLA now.\n. Yeah, that was definitely discussed in the beginning, but at the time it was just too much of a PITA.  All I wanted was CodaHale metrics integrated, and it seemed like a reasonable first approach.  I'm sure the folks would appreciate an enhanced PR. :)\n. This was a pretty specific case and I agree that, if you're going to do anything about it, then it should just be for duplicates.  \n. ",
    "ladlestein": "Ah, so that's why my dang test was failing.\n. Ah, so that's why my dang test was failing.\n. ",
    "jeffywu": "Think I got all the edits you wanted, I'll get that CLA signed and emailed to you guys tomorrow.\n. Think I got all the edits you wanted, I'll get that CLA signed and emailed to you guys tomorrow.\n. ",
    "wenjiezhang2013": "Which version of spray has this change? And I was trying to find the documentation for configuration, and couldn't find it, is there a document for guidance? \n. Which version of spray has this change? And I was trying to find the documentation for configuration, and couldn't find it, is there a document for guidance? \n. ",
    "stig": "Any hope of seeing this merged soon?\n. Yep, will do. \n. Just sent you the signed CLA.\n. I'll try to address it as best as I can. \n\nStig\n\nSent from my iPhone\n\n> On 3 Mar 2014, at 15:58, Mathias notifications@github.com wrote:\n> \n> Stig, thanks again for this contribution.\n> We have reserved some capacity this week to push for a minor update release.\n> Of course we'd like to incorporate the latest and greatest in contributions! :)\n> So, since this patch here looks quite valuable, if you have time to look through our comments we'd like to get it merged.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. OK, thanks for the feedback. How is it shaping up now? It's still partially implemented, as the full grammar of the link header is quite exhaustive, but I think this is a useful subset. I can try to add some more tests, but not sure I'll have time. (I'm giving a talk tomorrow!)\n. Excellent. I managed to get that done before going to work :-)\n. Fabulous. Do you want me to rebase onto current master first?\n. Any hope of seeing this merged soon?\n. Yep, will do. \n. Just sent you the signed CLA.\n. I'll try to address it as best as I can. \n\nStig\n\nSent from my iPhone\n\n> On 3 Mar 2014, at 15:58, Mathias notifications@github.com wrote:\n> \n> Stig, thanks again for this contribution.\n> We have reserved some capacity this week to push for a minor update release.\n> Of course we'd like to incorporate the latest and greatest in contributions! :)\n> So, since this patch here looks quite valuable, if you have time to look through our comments we'd like to get it merged.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. OK, thanks for the feedback. How is it shaping up now? It's still partially implemented, as the full grammar of the link header is quite exhaustive, but I think this is a useful subset. I can try to add some more tests, but not sure I'll have time. (I'm giving a talk tomorrow!)\n. Excellent. I managed to get that done before going to work :-)\n. Fabulous. Do you want me to rebase onto current master first?\n. ",
    "nap-sam-dean": "+1\n. +1\n. ",
    "arikogan": "+1\n. +1\n. ",
    "ayosec": "What about Statsd instead of Codahal Metrics?\n\nIt would be better if the _metrics engine_ can be configurable.\n. What about Statsd instead of Codahal Metrics?\n\nIt would be better if the _metrics engine_ can be configurable.\n. ",
    "zcox": "Any updates on this making it into Spray? Derek's code seems to work really well. \n. Thanks for the update Mathias. In the meantime, I need to use Metrics in Spray now on at least one project (and possibly several) and I also have a few additions to this PR that I'd like to share (counters/meters for HTTP response status codes) so would it make sense to just develop this as a standalone library outside of Spray for now, and possibly merge it in sometime in the future? This PR doesn't really require any core changes to Spray - even the new around directive can live externally.\n\nDerek et al - if this sounds good I can definitely assist.\n. Any updates on this making it into Spray? Derek's code seems to work really well. \n. Thanks for the update Mathias. In the meantime, I need to use Metrics in Spray now on at least one project (and possibly several) and I also have a few additions to this PR that I'd like to share (counters/meters for HTTP response status codes) so would it make sense to just develop this as a standalone library outside of Spray for now, and possibly merge it in sometime in the future? This PR doesn't really require any core changes to Spray - even the new around directive can live externally.\n\nDerek et al - if this sounds good I can definitely assist.\n. ",
    "vatel": "I am going to try metrics code in several days adapting it to Statsd. I think such a library (maybe separate from Spray) makes sense. Derek and Zcox - thank you for you work!\n. @theon thanks!\n. +1 agree with guersam. I do not see why it is so important to distinguish trailing slashes.\n. > > Serving the same content under two different URIs (the \"with-trailing-slash\" URIs is not the same as its \"without-trailing-slash\" sibling) is not really recommended. See for example: http://googlewebmastercentral.blogspot.de/2010/04/to-slash-or-not-to-slash.html\n\nok, this seems to be a holly-war or \"ideological\" thing ;) In such situations it is almost impossible to make happy all people. IMHO what can be done is to provide simple solutions for both sides of barricades - let people decide what is good and what is wrong for them.\n. Thanks for this patch!\n\n@sirthias may I ask you to publish 1.2.2 snapshot too (we depend on ReactiveMongo which is not ready for akka 2.3 yet).\n. thank you @sirthias !\n. I am going to try metrics code in several days adapting it to Statsd. I think such a library (maybe separate from Spray) makes sense. Derek and Zcox - thank you for you work!\n. @theon thanks!\n. +1 agree with guersam. I do not see why it is so important to distinguish trailing slashes.\n. > > Serving the same content under two different URIs (the \"with-trailing-slash\" URIs is not the same as its \"without-trailing-slash\" sibling) is not really recommended. See for example: http://googlewebmastercentral.blogspot.de/2010/04/to-slash-or-not-to-slash.html\n\nok, this seems to be a holly-war or \"ideological\" thing ;) In such situations it is almost impossible to make happy all people. IMHO what can be done is to provide simple solutions for both sides of barricades - let people decide what is good and what is wrong for them.\n. Thanks for this patch!\n\n@sirthias may I ask you to publish 1.2.2 snapshot too (we depend on ReactiveMongo which is not ready for akka 2.3 yet).\n. thank you @sirthias !\n. ",
    "agourlay": "@theon thx for sharing :+1: \n. fixed in 1.2-RC3 :+1: \n. @theon thx for sharing :+1: \n. fixed in 1.2-RC3 :+1: \n. ",
    "graph1zzlle": "Bump ! Any updates on this one ?\n. Bump ! Any updates on this one ?\n. ",
    "chrisabrams": "Any updates?\n. Any updates?\n. ",
    "kaeawc": "+1\n. +1\n. ",
    "zackangelo": "+1\n. +1\n. ",
    "tovbinm": "Hi guys, good stuff. Any plans on adding OAuth Authorization / Resource Server code to Spay as well?\n. Hi guys, good stuff. Any plans on adding OAuth Authorization / Resource Server code to Spay as well?\n. ",
    "matsluni": "Hi Johannes,\nthank you for your feedback. This helps a lot.\n\nMatthias\n. Hi,\n\nI added the feedback to the blogpost and also completely have rewritten the paragraph about the JVM on the RPI. I hope this is cleaner now and helps to understand the choices one can make.\n\nI also added some information about the RPI-Version I used for the benchmark.\n\nIf you think its ok, then its good to go!\n\nMatthias\n. Hi,\n\nI added the feedback. I hope we are now good to go. On my side its fine.\nThanks for the feedback and the help with this PR.\n\nMatthias\n. Hi @dwalend,\nhave you seen this recent ML post: https://groups.google.com/forum/#!msg/spray-user/GFXMrfeNwzs/9j9sKJqRIlAJ\n\nIn the comments of the Coderwall post (https://coderwall.com/p/0izzta/cors-directive-for-spray) there is also an example with authentication. \n\nMaybe this helps you?!\n\nMats\n. I thought I give it a try. Please have a look at my proposal. I followed the advice from @alexandrnikitin to show explicitly that nothing was found for the search term. If you need a pull request let me know. \n. @sirthias, Oh I missed this one. I will have a look at this after work. Should be possible. \n. I changed some css classes. I think this should be it. Let me know what you think.\n. Good! I will send a proper PR. :-)\n. Hi,\n\nfor me it looks like you don't handle the preflight request correctly. In the screenshot you see an OPTIONS-Request (triggered by the browser) before your intended request will be issued.\n\nSee here for more information:\nhttps://developer.mozilla.org/en-US/docs/Web/HTTP/Access_control_CORS#Preflighted_requests\n\nSee here for a possible solution:\nhttps://gist.github.com/joseraya/176821d856b43b1cfe19\n\nHTH,\nMatthias\n. Hi Johannes,\nthank you for your feedback. This helps a lot.\n\nMatthias\n. Hi,\n\nI added the feedback to the blogpost and also completely have rewritten the paragraph about the JVM on the RPI. I hope this is cleaner now and helps to understand the choices one can make.\n\nI also added some information about the RPI-Version I used for the benchmark.\n\nIf you think its ok, then its good to go!\n\nMatthias\n. Hi,\n\nI added the feedback. I hope we are now good to go. On my side its fine.\nThanks for the feedback and the help with this PR.\n\nMatthias\n. Hi @dwalend,\nhave you seen this recent ML post: https://groups.google.com/forum/#!msg/spray-user/GFXMrfeNwzs/9j9sKJqRIlAJ\n\nIn the comments of the Coderwall post (https://coderwall.com/p/0izzta/cors-directive-for-spray) there is also an example with authentication. \n\nMaybe this helps you?!\n\nMats\n. I thought I give it a try. Please have a look at my proposal. I followed the advice from @alexandrnikitin to show explicitly that nothing was found for the search term. If you need a pull request let me know. \n. @sirthias, Oh I missed this one. I will have a look at this after work. Should be possible. \n. I changed some css classes. I think this should be it. Let me know what you think.\n. Good! I will send a proper PR. :-)\n. Hi,\n\nfor me it looks like you don't handle the preflight request correctly. In the screenshot you see an OPTIONS-Request (triggered by the browser) before your intended request will be issued.\n\nSee here for more information:\nhttps://developer.mozilla.org/en-US/docs/Web/HTTP/Access_control_CORS#Preflighted_requests\n\nSee here for a possible solution:\nhttps://gist.github.com/joseraya/176821d856b43b1cfe19\n\nHTH,\nMatthias\n. ",
    "unoexperto": "Oh wow, it took me \"just\" 20 mins :) It's 4AM on my side. I hope you guys can build nightly with this fix. I'll email CLA tomorrow.\n\nThanks!\n. I'm closing this pull request in favor of https://github.com/spray/spray/pull/388\n. Could you please push new build to nightlies ? :)\n. Oh wow, it took me \"just\" 20 mins :) It's 4AM on my side. I hope you guys can build nightly with this fix. I'll email CLA tomorrow.\n\nThanks!\n. I'm closing this pull request in favor of https://github.com/spray/spray/pull/388\n. Could you please push new build to nightlies ? :)\n. ",
    "stebourbi": "In order to make the code compiling simply replace Url by a String (sorry for not being aware of this).\n\n``` scala\ndef fetchDocument(url:String) = {\n    val pipeline: HttpRequest => Future[HttpResponse] = sendReceive\n    val fResponse: Future[HttpResponse] = pipeline(Get(url))\n    fResponse onComplete {\n      case Success(response) =>  println(response.status.value)\n\n      case Failure(error) => println(error.getMessage)\n    }\n  }\n```\n. In order to make the code compiling simply replace Url by a String (sorry for not being aware of this).\n\n``` scala\ndef fetchDocument(url:String) = {\n    val pipeline: HttpRequest => Future[HttpResponse] = sendReceive\n    val fResponse: Future[HttpResponse] = pipeline(Get(url))\n    fResponse onComplete {\n      case Success(response) =>  println(response.status.value)\n\n      case Failure(error) => println(error.getMessage)\n    }\n  }\n```\n. ",
    "eelbaz": "rudolph have you found a way around this?\n. rudolph have you found a way around this?\n. ",
    "vincenzovitale": "+1\n. +1\n. ",
    "asnare": "So, I thought I'd implement the proposal to see what pops up. The rough bits at the moment are:\n- The `Uri`/`Impl` construction and `apply()` chains are a little convoluted. Something more elegant would be nice.\n- The semantics of `copy()` are a little unclear. I implemented it due to the way the original mock `HttpServletRequest` worked. It's probably wrong and should be removed.\n- I haven't looked deeply into how `raw` will behave in servlet containers. Context prefixes, in particular, may interact strangely.\n. Regarding the semantics of `copy`, I'm very nervous about maintaining the `raw` member. It will easily be out of sync with the logical parsed fields, and I am guessing that this will inevitably lead to an unpleasant surprise for someone.\n\nI see these alternatives:\n1. Current behaviour as committed so far.\n2. Don't maintain the field. The moment a copy occurs, `raw` reverts to `toString`. This is the lazy (but simple!) option.\n3. Try to be smart about when to maintain the field, and when not to. This sounds like a fair amount of complexity (and corner cases) for relatively little benefit.\n\nDoes this change your thoughts on things?\n. TLDR: I agree.\n\nI don't see any technical problems with your proposal, and it would work. The counterpoints I see are:\n- It would make it more complicated to write a directive (_cough_) that works in both spray-can and spray-servlet.\n- It's a little unfortunate that such a directive would require a specific config setting in order to work.\n\nThe main benefit of your proposal is that it solves the immediate problem, and allows any API changes to be deferred until the problem (and use-cases) are better understood.\n\nThe other problem I was having was getting `ModelConverter` to play nicely. (The reason for the switch from `getRequestURL` to `getRequestURI` was that the former has decoded everything, while the latter provides access to the raw URL.) The `rebuildUri` method has to do a lot of work to do things correctly. Something like:\n\n``` scala\n\n    val scheme = hsRequest.getScheme()\n    val authority = hsRequest.getHeader(\"host\") match {\n      case null \u21d2 {\n        val serverName = hsRequest.getServerName()\n        (scheme, hsRequest.getServerPort()) match {\n          case (\"http\", 80)   \u21d2 serverName\n          case (\"https\", 443) \u21d2 serverName\n          case (_, port)      \u21d2 serverName + ':' + port\n        }\n      }\n      case x \u21d2 x\n    }\n    val requestUri = scheme + \"://\" + authority + hsRequest.getRequestURI()\n    val rawUri = hsRequest.getQueryString match {\n      case null \u21d2 requestUri\n      case x    \u21d2 requestUri + '?' + x\n    }\n```\n\nYuck. And even this is guessing when it comes to whether the port number is present or not.\n\nGiven the original use-case, and problems with `Uri.copy`, another solution could be to introduce a `Path.raw` member instead. This would address our original use-case and easily be integrated with spray-can and spray-servlet.\nMy thoughts are that this might be worth thinking about in the future, but for now `spray.can.server.raw-request-uri-header` is sufficient (and non-controversial).\n\nBy the way, going back to the original use-case we spoke of OAuth2 HTTP-MAC. There's also related use-case if anyone wants to build a proxy: the [http specifiction](http://www.w3.org/Protocols/rfc2616/rfc2616-sec5.html#sec5.1.2) suggests proxies shouldn't rewrite the URI, although I have no idea how well-behaved proxies are in practice.\n. Matthias,\nThanks for clarifying the proposal. I think it makes sense to start this on a new pull request; the existing commits are largely superfluous. I'm unlikely to have time to do anything today, but hopefully I can have something ready for Monday morning.\n. I was running a little behind, but have submitted new pull requests:\n- #416 is the changes to the Uri internals that we wanted to keep;\n- #427 is the `Raw-Request-URI` implementation as discussed.\n\nI think this pull request can be closed.\n. This doesn't affect HMAC at all: it has a normalization pass and only the raw (undecoded) path and query string is needed. The host and port and normalized separately. For details, refer to [section 3.2.1 of the draft specification](http://tools.ietf.org/html/draft-ietf-oauth-v2-http-mac-01#section-3.2.1).\n\nProxies are a different matter. According to my reading of the HTTP/1.1 spec they're not allowed to rewrite the URI target. I suspect some probably do though. (This is an additional use-case for this header, by the way: implementing a proxy.)\n\nSo in summary:\n- it should support original HTTP-MAC use-case, including proxy traversal.\n- it should support implementing a HTTP proxy.\n\nI could make the following changes:\n- Rename `Raw-Request-URI` to `Raw-Request-PathQuery`\n- Update the spray can to strip off the scheme/host/port. ie, only the path and query string would be present.\n\nThis would be sufficient for our use-case, and also be more consistent with respect to spray can/servlet behaviour. The impact is that it would no longer be possible to implement a fully compliant HTTP proxy on spray can. (It's already not possible with the servlet API.)\n. You are absolutely correct. It's a while since I'd read the normalization spec, and you're right: it specifies the request-URI as is, in any of the allowed forms. (Note to self: the host/port must be from the `Host` header, which may be absent, and never the URL. Ho hum.)\n\nOn the spray can side we're good. On the servlet side things are more problematic, to the point where I'd be tempted to leave the feature unimplemented there instead of misleading a user with something that normally works but mysteriously fails under some circumstances. (A determined application using spray-servlet could instead get the `HttpServletRequest` and do things itself.)\n\nI'll await @sirthias before doing anything further. :)\n. Thanks for the kind feedback!\n\nI've made the changes you flagged in the commits above, and removed the servlet change entirely. In a few minutes I'll submit the servlet refactoring you wanted to keep as an independent pull request.\n. So, I thought I'd implement the proposal to see what pops up. The rough bits at the moment are:\n- The `Uri`/`Impl` construction and `apply()` chains are a little convoluted. Something more elegant would be nice.\n- The semantics of `copy()` are a little unclear. I implemented it due to the way the original mock `HttpServletRequest` worked. It's probably wrong and should be removed.\n- I haven't looked deeply into how `raw` will behave in servlet containers. Context prefixes, in particular, may interact strangely.\n. Regarding the semantics of `copy`, I'm very nervous about maintaining the `raw` member. It will easily be out of sync with the logical parsed fields, and I am guessing that this will inevitably lead to an unpleasant surprise for someone.\n\nI see these alternatives:\n1. Current behaviour as committed so far.\n2. Don't maintain the field. The moment a copy occurs, `raw` reverts to `toString`. This is the lazy (but simple!) option.\n3. Try to be smart about when to maintain the field, and when not to. This sounds like a fair amount of complexity (and corner cases) for relatively little benefit.\n\nDoes this change your thoughts on things?\n. TLDR: I agree.\n\nI don't see any technical problems with your proposal, and it would work. The counterpoints I see are:\n- It would make it more complicated to write a directive (_cough_) that works in both spray-can and spray-servlet.\n- It's a little unfortunate that such a directive would require a specific config setting in order to work.\n\nThe main benefit of your proposal is that it solves the immediate problem, and allows any API changes to be deferred until the problem (and use-cases) are better understood.\n\nThe other problem I was having was getting `ModelConverter` to play nicely. (The reason for the switch from `getRequestURL` to `getRequestURI` was that the former has decoded everything, while the latter provides access to the raw URL.) The `rebuildUri` method has to do a lot of work to do things correctly. Something like:\n\n``` scala\n\n    val scheme = hsRequest.getScheme()\n    val authority = hsRequest.getHeader(\"host\") match {\n      case null \u21d2 {\n        val serverName = hsRequest.getServerName()\n        (scheme, hsRequest.getServerPort()) match {\n          case (\"http\", 80)   \u21d2 serverName\n          case (\"https\", 443) \u21d2 serverName\n          case (_, port)      \u21d2 serverName + ':' + port\n        }\n      }\n      case x \u21d2 x\n    }\n    val requestUri = scheme + \"://\" + authority + hsRequest.getRequestURI()\n    val rawUri = hsRequest.getQueryString match {\n      case null \u21d2 requestUri\n      case x    \u21d2 requestUri + '?' + x\n    }\n```\n\nYuck. And even this is guessing when it comes to whether the port number is present or not.\n\nGiven the original use-case, and problems with `Uri.copy`, another solution could be to introduce a `Path.raw` member instead. This would address our original use-case and easily be integrated with spray-can and spray-servlet.\nMy thoughts are that this might be worth thinking about in the future, but for now `spray.can.server.raw-request-uri-header` is sufficient (and non-controversial).\n\nBy the way, going back to the original use-case we spoke of OAuth2 HTTP-MAC. There's also related use-case if anyone wants to build a proxy: the [http specifiction](http://www.w3.org/Protocols/rfc2616/rfc2616-sec5.html#sec5.1.2) suggests proxies shouldn't rewrite the URI, although I have no idea how well-behaved proxies are in practice.\n. Matthias,\nThanks for clarifying the proposal. I think it makes sense to start this on a new pull request; the existing commits are largely superfluous. I'm unlikely to have time to do anything today, but hopefully I can have something ready for Monday morning.\n. I was running a little behind, but have submitted new pull requests:\n- #416 is the changes to the Uri internals that we wanted to keep;\n- #427 is the `Raw-Request-URI` implementation as discussed.\n\nI think this pull request can be closed.\n. This doesn't affect HMAC at all: it has a normalization pass and only the raw (undecoded) path and query string is needed. The host and port and normalized separately. For details, refer to [section 3.2.1 of the draft specification](http://tools.ietf.org/html/draft-ietf-oauth-v2-http-mac-01#section-3.2.1).\n\nProxies are a different matter. According to my reading of the HTTP/1.1 spec they're not allowed to rewrite the URI target. I suspect some probably do though. (This is an additional use-case for this header, by the way: implementing a proxy.)\n\nSo in summary:\n- it should support original HTTP-MAC use-case, including proxy traversal.\n- it should support implementing a HTTP proxy.\n\nI could make the following changes:\n- Rename `Raw-Request-URI` to `Raw-Request-PathQuery`\n- Update the spray can to strip off the scheme/host/port. ie, only the path and query string would be present.\n\nThis would be sufficient for our use-case, and also be more consistent with respect to spray can/servlet behaviour. The impact is that it would no longer be possible to implement a fully compliant HTTP proxy on spray can. (It's already not possible with the servlet API.)\n. You are absolutely correct. It's a while since I'd read the normalization spec, and you're right: it specifies the request-URI as is, in any of the allowed forms. (Note to self: the host/port must be from the `Host` header, which may be absent, and never the URL. Ho hum.)\n\nOn the spray can side we're good. On the servlet side things are more problematic, to the point where I'd be tempted to leave the feature unimplemented there instead of misleading a user with something that normally works but mysteriously fails under some circumstances. (A determined application using spray-servlet could instead get the `HttpServletRequest` and do things itself.)\n\nI'll await @sirthias before doing anything further. :)\n. Thanks for the kind feedback!\n\nI've made the changes you flagged in the commits above, and removed the servlet change entirely. In a few minutes I'll submit the servlet refactoring you wanted to keep as an independent pull request.\n. ",
    "OlegYch": "still seeing leeked file descriptors here \nat java.io.FileInputStream.<init>(FileInputStream.java:139)\n    at java.io.FileInputStream.<init>(FileInputStream.java:93)\n    at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)\n    at sun.net.www.protocol.file.FileURLConnection.initializeHeaders(FileURLConnection.java:110)\n    at sun.net.www.protocol.file.FileURLConnection.getHeaderField(FileURLConnection.java:146)\n    at sun.net.www.protocol.jar.JarURLConnection.getHeaderField(JarURLConnection.java:228)\n    at java.net.URLConnection.getHeaderFieldDate(URLConnection.java:654)\n    at java.net.URLConnection.getLastModified(URLConnection.java:559)\n    at spray.routing.directives.FileAndResourceDirectives$class.getFromResource(FileAndResourceDirectives.scala:126)\nspray 1.3.3 jvm8u31 on windows, and oracle 8u72 on ubuntu\n\nbtw i have routes defined like runRoute(respondWithHeaders(){requestInstance {parameterMap {routes ~ getFromResource}}) and resource is accessed on every request instead of only if \"routes\" didn't match, so every request leaks a file handle, any reason for this?\n. still seeing leeked file descriptors here \nat java.io.FileInputStream.<init>(FileInputStream.java:139)\n    at java.io.FileInputStream.<init>(FileInputStream.java:93)\n    at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)\n    at sun.net.www.protocol.file.FileURLConnection.initializeHeaders(FileURLConnection.java:110)\n    at sun.net.www.protocol.file.FileURLConnection.getHeaderField(FileURLConnection.java:146)\n    at sun.net.www.protocol.jar.JarURLConnection.getHeaderField(JarURLConnection.java:228)\n    at java.net.URLConnection.getHeaderFieldDate(URLConnection.java:654)\n    at java.net.URLConnection.getLastModified(URLConnection.java:559)\n    at spray.routing.directives.FileAndResourceDirectives$class.getFromResource(FileAndResourceDirectives.scala:126)\nspray 1.3.3 jvm8u31 on windows, and oracle 8u72 on ubuntu\n\nbtw i have routes defined like runRoute(respondWithHeaders(){requestInstance {parameterMap {routes ~ getFromResource}}) and resource is accessed on every request instead of only if \"routes\" didn't match, so every request leaks a file handle, any reason for this?\n. ",
    "ivantopo": "Guys, I've been thinking about this issue and, even while it only manifests itself because of a buggy version of ab in OSX machines I think it is necessary to do something about it because as I see it, anyone with access to that buggy version of ab could DoS attack any spray-can instance and take it down in a few seconds.. My initial propose would be to introduce a new \"max-open-requests-per-connection\" setting with a sensitive default and, when for a given connection the number of open requests exceeds this limit, spray-can will assume it is under attack, close the connection and log a meaningful warning about it. What do you think?\n. @jrudolph should I try out your branch on android now or do you have some more magic pending there?\n. Mathias, thanks for your corrections, I already updated the PR with all the requested modifications.. I also went through the code searching for uses of Duration.Undefined and found a few context.setReceiveTimeout(Duration.Undefined) but in those contexts having Duration.Inf would yield the exact same behaviour.. It might be changed just to make the whole codebase agree on what \"infinite\" should be, let me know if you guys agree with this to introduce that change as well.\n. Thanks for the advice @jrudolph, I just included the missing \"withPath\".\n. @2beaucoup, it is already `<expected> === <actual>`, some tests there are expressed that way and some other the way around, I guess there is no harm in this case.. thanks for the comment!\n. @2beaucoup that certainly needs to be reordered!.. regarding similarity with other tests, I would like to know if I'm misunderstanding the intention in some of them, for example, the first test contains `Host(\"192.0.2.16\") === IPv4Host(\"192.0.2.16\")` and many similar assertions come after that. What I assume there is that `Host(..)` is stable and we are testing `IPv4Host(..)`, so the layout is `<expected> === <actual>` in a lot of assertions.\n\nI already reordered the case I introduced, but would like to know your opinion about reordering the rest of the cases in the file, as well as keeping it in mind for future contributions.. repetitive but simple task anyway.\n. Just updated based on all your comments. The only missing thing is this overload:\n  `def withAuthority(host: String, port: Int, userinfo: String = \"\")`\nsince the compiler only allows one overload with default arguments.. also rebased with the latest from master.\n. @sirthias Very reasonable! I just pushed a commit including the `effectivePort` method and usage as suggested.\n. Ohh noo! Silly me :s.. sorry about that, I'll keep a third eye on that for upcoming contributions! Thanks for the catch, again, @2beaucoup \n. @jrudolph I just pushed a commit based on your feedback. After thinking a little bit about it, the main reason why I was including the section about wrapping with the `dynamic` directive was because I thought these directives were dynamic and got bitten by them yesterday, thus I wanted to make sure the clarification is available to anyone trying to use them. Maybe, providing a tag with each directive stating if it is dynamic or static is a good idea.\n. Thanks!\n. Awesome guys! I'm going to test this right away :).\n. @sirthias @jrudolph how are you guys?, it would be awesome if you can take a look at this PR and give some feedback.. we have deployed one of our apps a couple weeks ago with a custom build (Spray 1.2.0 + this PR) and it is working flawlessly as far as we can tell.\n\nMaybe a 1.x.1 will arrive soon with this and other fixes? :D, hope to hear from you soon, best regards!\n. @caoilte I didn't try to merge with the current codebase, but from a quick look it seems like none of the modified files changed lately, it should merge cleanly.. It would be nice to hear back from you with the results of your tests!\n. Guys, I've been thinking about this issue and, even while it only manifests itself because of a buggy version of ab in OSX machines I think it is necessary to do something about it because as I see it, anyone with access to that buggy version of ab could DoS attack any spray-can instance and take it down in a few seconds.. My initial propose would be to introduce a new \"max-open-requests-per-connection\" setting with a sensitive default and, when for a given connection the number of open requests exceeds this limit, spray-can will assume it is under attack, close the connection and log a meaningful warning about it. What do you think?\n. @jrudolph should I try out your branch on android now or do you have some more magic pending there?\n. Mathias, thanks for your corrections, I already updated the PR with all the requested modifications.. I also went through the code searching for uses of Duration.Undefined and found a few context.setReceiveTimeout(Duration.Undefined) but in those contexts having Duration.Inf would yield the exact same behaviour.. It might be changed just to make the whole codebase agree on what \"infinite\" should be, let me know if you guys agree with this to introduce that change as well.\n. Thanks for the advice @jrudolph, I just included the missing \"withPath\".\n. @2beaucoup, it is already `<expected> === <actual>`, some tests there are expressed that way and some other the way around, I guess there is no harm in this case.. thanks for the comment!\n. @2beaucoup that certainly needs to be reordered!.. regarding similarity with other tests, I would like to know if I'm misunderstanding the intention in some of them, for example, the first test contains `Host(\"192.0.2.16\") === IPv4Host(\"192.0.2.16\")` and many similar assertions come after that. What I assume there is that `Host(..)` is stable and we are testing `IPv4Host(..)`, so the layout is `<expected> === <actual>` in a lot of assertions.\n\nI already reordered the case I introduced, but would like to know your opinion about reordering the rest of the cases in the file, as well as keeping it in mind for future contributions.. repetitive but simple task anyway.\n. Just updated based on all your comments. The only missing thing is this overload:\n  `def withAuthority(host: String, port: Int, userinfo: String = \"\")`\nsince the compiler only allows one overload with default arguments.. also rebased with the latest from master.\n. @sirthias Very reasonable! I just pushed a commit including the `effectivePort` method and usage as suggested.\n. Ohh noo! Silly me :s.. sorry about that, I'll keep a third eye on that for upcoming contributions! Thanks for the catch, again, @2beaucoup \n. @jrudolph I just pushed a commit based on your feedback. After thinking a little bit about it, the main reason why I was including the section about wrapping with the `dynamic` directive was because I thought these directives were dynamic and got bitten by them yesterday, thus I wanted to make sure the clarification is available to anyone trying to use them. Maybe, providing a tag with each directive stating if it is dynamic or static is a good idea.\n. Thanks!\n. Awesome guys! I'm going to test this right away :).\n. @sirthias @jrudolph how are you guys?, it would be awesome if you can take a look at this PR and give some feedback.. we have deployed one of our apps a couple weeks ago with a custom build (Spray 1.2.0 + this PR) and it is working flawlessly as far as we can tell.\n\nMaybe a 1.x.1 will arrive soon with this and other fixes? :D, hope to hear from you soon, best regards!\n. @caoilte I didn't try to merge with the current codebase, but from a quick look it seems like none of the modified files changed lately, it should merge cleanly.. It would be nice to hear back from you with the results of your tests!\n. ",
    "davidillsley": "FWIW, This bug would definitely stop me putting a service on the internet based on a version of Spray without a fix for this.\n. Thanks. It was http://spray.io/documentation/1.2-M8/spray-can/dependencies/ which really threw me.\n. FWIW, This bug would definitely stop me putting a service on the internet based on a version of Spray without a fix for this.\n. Thanks. It was http://spray.io/documentation/1.2-M8/spray-can/dependencies/ which really threw me.\n. ",
    "fedesilva": "+1 and another reason: debugging. \n\nUsing a desktop proxy for debugging purposes is very valuable when developing to a poorly documented rest api and when developing in general. There are several of this programs like Charles (which I use) and Fiddler which most of my windows using coworkers like.\n\nAdditionally, respecting the standard java properties and the standard linux env variables would be extra valuable.\n. +1 and another reason: debugging. \n\nUsing a desktop proxy for debugging purposes is very valuable when developing to a poorly documented rest api and when developing in general. There are several of this programs like Charles (which I use) and Fiddler which most of my windows using coworkers like.\n\nAdditionally, respecting the standard java properties and the standard linux env variables would be extra valuable.\n. ",
    "clementgarnier": "Thank you for fixing this, and sorry for having entirely missed the discussion.\n\nWhen writing the fix, my primary concern was to disrupt as few as possible, but indeed, it made `SprayActorLogging` pointless so I totally agree with your design choices.\n\nThanks again! Will upgrade to the latest nightly build right away.\n. Thank you for fixing this, and sorry for having entirely missed the discussion.\n\nWhen writing the fix, my primary concern was to disrupt as few as possible, but indeed, it made `SprayActorLogging` pointless so I totally agree with your design choices.\n\nThanks again! Will upgrade to the latest nightly build right away.\n. ",
    "kozura": "Seconded, UPnP also uses other methods like SUBSCRIBE, NOTIFY, and M-SEARCH, so have to drop down to tcp instead of using the full http stack.  Is there any reason these have to be private?\n. Seconded, UPnP also uses other methods like SUBSCRIBE, NOTIFY, and M-SEARCH, so have to drop down to tcp instead of using the full http stack.  Is there any reason these have to be private?\n. ",
    "cvrabie": "I'll rewrite the commit message, remove the unnecessary import and fix the comment. \nThere's also a thing that I wasn't unsure about (commented in the code above).\n. I can confirm this is still happening on `nightly 20131106`. Doesn't seem to have any obvious negative impact though. \n\n```\n[ERROR] akka.actor.OneForOneStrategy - null\njava.lang.IllegalStateException: null\nat spray.io.SslTlsSupport$$anon$1$$anon$2.spray$io$SslTlsSupport$$anon$$anon$$verify(SslTlsSupport.scala:363) ~[spray-io-1.2-20131106.jar:na]\nat spray.io.SslTlsSupport$$anon$1$$anon$2$$anon$6$$anonfun$2.apply(SslTlsSupport.scala:92) ~[spray-io-1.2-20131106.jar:na]\nat spray.io.SslTlsSupport$$anon$1$$anon$2$$anon$6$$anonfun$2.apply(SslTlsSupport.scala:81) ~[spray-io-1.2-20131106.jar:na]\nat spray.io.SslTlsSupport$$anon$1$$anon$2.process(SslTlsSupport.scala:234) ~[spray-io-1.2-20131106.jar:na]\n```\n. I'll rewrite the commit message, remove the unnecessary import and fix the comment. \nThere's also a thing that I wasn't unsure about (commented in the code above).\n. I can confirm this is still happening on `nightly 20131106`. Doesn't seem to have any obvious negative impact though. \n\n```\n[ERROR] akka.actor.OneForOneStrategy - null\njava.lang.IllegalStateException: null\nat spray.io.SslTlsSupport$$anon$1$$anon$2.spray$io$SslTlsSupport$$anon$$anon$$verify(SslTlsSupport.scala:363) ~[spray-io-1.2-20131106.jar:na]\nat spray.io.SslTlsSupport$$anon$1$$anon$2$$anon$6$$anonfun$2.apply(SslTlsSupport.scala:92) ~[spray-io-1.2-20131106.jar:na]\nat spray.io.SslTlsSupport$$anon$1$$anon$2$$anon$6$$anonfun$2.apply(SslTlsSupport.scala:81) ~[spray-io-1.2-20131106.jar:na]\nat spray.io.SslTlsSupport$$anon$1$$anon$2.process(SslTlsSupport.scala:234) ~[spray-io-1.2-20131106.jar:na]\n```\n. ",
    "suiryc": "Not really, even if that may lead to the same result.\nFrom what I could see, it is more like the server does not respond to the connection attempt.\n\nI was also checking what would happen trying to connect using netcat client (on Linux): it returns `Connection timed out` after a long time.\nAs opposed to the other 2 cases I mentioned:\n- unreachable host: netcat returns a `No route to host` after a few seconds\n- port closed: netcat returns `Connection refused` immediatly\n\nI understand there are timeouts for the connection attempt and for the \"ask\" pattern.\nWhat is strange are all the warnings, and the ~2 mins delay between connection attempts. From what I could understand, the warnings come from `spray.can.client.ClientFrontend` where the request is kept as head of the `openRequests` queue: since the timeout state is not saved in the `RequestRecord`, each time `checkForTimeout` is called the warning is logged and a message is sent to close the connection, until the request is removed from the queue.\nSo maybe remembering the timeout was already reached is part of the solution here, but there is still the long time it takes to remove the request from this queue and then retry.\n. Not really, even if that may lead to the same result.\nFrom what I could see, it is more like the server does not respond to the connection attempt.\n\nI was also checking what would happen trying to connect using netcat client (on Linux): it returns `Connection timed out` after a long time.\nAs opposed to the other 2 cases I mentioned:\n- unreachable host: netcat returns a `No route to host` after a few seconds\n- port closed: netcat returns `Connection refused` immediatly\n\nI understand there are timeouts for the connection attempt and for the \"ask\" pattern.\nWhat is strange are all the warnings, and the ~2 mins delay between connection attempts. From what I could understand, the warnings come from `spray.can.client.ClientFrontend` where the request is kept as head of the `openRequests` queue: since the timeout state is not saved in the `RequestRecord`, each time `checkForTimeout` is called the warning is logged and a message is sent to close the connection, until the request is removed from the queue.\nSo maybe remembering the timeout was already reached is part of the solution here, but there is still the long time it takes to remove the request from this queue and then retry.\n. ",
    "sajanalexander": "I'm also seeing this problem.  How would you recommend going about shutting down the HttpClientConnection?\n. Yes, before it is fixed properly.  I am seeing this on a live application and we've run into an OutOfMemoryError in a certain situation, so I wanted to patch this symptom before the real bug can be fixed.  \n\nI'm a bit unsure as to where to tell the HttpClientConnection to shut down.  I was thinking I could call context.stop(rec.sender) within the checkForTimeout method in ClientFrontend, but I wasn't sure if that would cause problems with connections which had timed out after actually connecting.  Or is there a way to detect this case within HttpClientConnection and stop it from within?  \n. I'm also seeing this problem.  How would you recommend going about shutting down the HttpClientConnection?\n. Yes, before it is fixed properly.  I am seeing this on a live application and we've run into an OutOfMemoryError in a certain situation, so I wanted to patch this symptom before the real bug can be fixed.  \n\nI'm a bit unsure as to where to tell the HttpClientConnection to shut down.  I was thinking I could call context.stop(rec.sender) within the checkForTimeout method in ClientFrontend, but I wasn't sure if that would cause problems with connections which had timed out after actually connecting.  Or is there a way to detect this case within HttpClientConnection and stop it from within?  \n. ",
    "dkowis": "That's cool!\n\nOf course, this performance bench isn't real-world so much. I just thought that I'd see closer to the times I was getting from Node.js/Scala. \n\nThanks for looking into this so quickly :)\n. That's cool!\n\nOf course, this performance bench isn't real-world so much. I just thought that I'd see closer to the times I was getting from Node.js/Scala. \n\nThanks for looking into this so quickly :)\n. ",
    "christoph-buente": "Does that mean, that major browsers do violate RFC rules? Some examples:\r\n\r\n* Mozilla/5.0 (Linux; Android 7.0; SAMSUNG-SM-G890A Build/NRD90M; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/58.0.3029.83 Mobile Safari/537.36 [Pinterest/Android]\r\n* Mozilla/5.0 (iPhone; CPU iPhone OS 10_3_1 like Mac OS X) AppleWebKit/603.1.30 (KHTML, like Gecko) Mobile/14E304 [FBAN/FBIOS;FBAV/95.0.0.54.70;FBBV/59901094;FBDV/iPhone8,2;FBMD/iPhone;FBSN/iOS;FBSV/10.3.1;FBSS/3;FBCR/Globe;FBID/phone;FBLC/en_US;FBOP/5;FBRV/0]\r\n* Mozilla/5.0 (Linux; Android 7.0; SM-G935T Build/NRD90M; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/58.0.3029.83 Mobile Safari/537.36 [FB_IAB/FB4A;FBAV/126.0.0.23.77;]\r\n* Mozilla/5.0 (Linux; Android 7.0; Moto G (4) Build/NPJ25.93-14) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.83 Mobile Safari/537.36\r\n* Mozilla/5.0 (Linux; Android 6.0.1; SM-G935V Build/MMB29M; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/58.0.3029.83 Mobile Safari/537.36 [FB_IAB/FB4A;FBAV/126.0.0.23.77;]\r\n\r\nUnfortunately reality is, that many vendors don't really care and send a user agent which seems to be invalid from RFC point of view. But that doesn't mean, we don't have to handle it some how.\r\n\r\nWhat are your suggestions to work with the data that comes in?. My bad, thanks :). Does that mean, that major browsers do violate RFC rules? Some examples:\r\n\r\n* Mozilla/5.0 (Linux; Android 7.0; SAMSUNG-SM-G890A Build/NRD90M; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/58.0.3029.83 Mobile Safari/537.36 [Pinterest/Android]\r\n* Mozilla/5.0 (iPhone; CPU iPhone OS 10_3_1 like Mac OS X) AppleWebKit/603.1.30 (KHTML, like Gecko) Mobile/14E304 [FBAN/FBIOS;FBAV/95.0.0.54.70;FBBV/59901094;FBDV/iPhone8,2;FBMD/iPhone;FBSN/iOS;FBSV/10.3.1;FBSS/3;FBCR/Globe;FBID/phone;FBLC/en_US;FBOP/5;FBRV/0]\r\n* Mozilla/5.0 (Linux; Android 7.0; SM-G935T Build/NRD90M; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/58.0.3029.83 Mobile Safari/537.36 [FB_IAB/FB4A;FBAV/126.0.0.23.77;]\r\n* Mozilla/5.0 (Linux; Android 7.0; Moto G (4) Build/NPJ25.93-14) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.83 Mobile Safari/537.36\r\n* Mozilla/5.0 (Linux; Android 6.0.1; SM-G935V Build/MMB29M; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/58.0.3029.83 Mobile Safari/537.36 [FB_IAB/FB4A;FBAV/126.0.0.23.77;]\r\n\r\nUnfortunately reality is, that many vendors don't really care and send a user agent which seems to be invalid from RFC point of view. But that doesn't mean, we don't have to handle it some how.\r\n\r\nWhat are your suggestions to work with the data that comes in?. My bad, thanks :). ",
    "ktoso": "Firstly, spray is end of life and not maintained. Please upgrade to Akka HTTP, the migration is pretty simple. The EOL was announced a few years ago.\r\n\r\nPlease check if the same happens on Akka http and if so we can solve things there :). Akka HTTP has now been released as stable module ( akka.io/news/2016/11/22/akka-http-10.0.0-released.html ) thus we'll be more vocal about end-of-life of spray.\r\n\r\nWe just added a notice to the spray.io site that directs people to the akka docs etc.\r\nFor long term projects it makes sense to use Akka HTTP nowadays, esp. since it's now fully stable. . Feel free to ask on https://groups.google.com/forum/#!forum/akka-user if you have any more questions!. LGTM, would be very good to merge this as soon as possible so we can release akka-http and this together\n. IMO let's just patch 1.3 unless someone requests, after annoucement that they need older versions too\n. Hi there, \nthanks for reporting!\n\nWe were looking into a similar issue (about Content-Disposition) in Akka HTTP just recently, see here: https://github.com/akka/akka-http/issues/386\nCould you check if the same problem exists in Akka HTTP now?\n\nSpray won't be receiving maintenance unless it's security updates, I highly recommend you upgrade to Akka HTTP. The Akka HTTP DSL will be released as stable (core is already stable), as close as this or next week. Thanks in advance!\n. (and if yes, open an issue about it over there?)\nThanks for your help / understanding!\n. It's on maven-central, as all artifacts. Seems artifact name changed though?\n\nhttp://search.maven.org/#search|ga|1|spray-routing-shapeless `spray-routing-shapeless23_2.11`\n. We're documenting this fact in Akka HTTP btw: https://github.com/akka/akka-http/pull/412\n. That's not really planned since Spray is nearing end-of-life, the upgrade path being to move to Akka HTTP.\n\nIf community would prepare the build to work for 2.12 we could run a release but we're not going to spend time on it currently. We didn't see customers request it so far.\n. Thanks, it's true that now is the time to announce this more strongly than before.\r\nWe'll want to be a bit more specific than that, as well as link to 10.+ etc.\r\n\r\nI'll build upon your PR tomorrow to get the message right - we also want the same on the website.. Note was added by @sirthias to the site: https://github.com/spray/spray/commit/67e1fe800956cef1d410b49ca6f88270a4b9be0d\r\nAnd I'll update the README.\r\n\r\nThanks for the reminder @adamdecaf !. Readme update: https://github.com/spray/spray/pull/1134. Thanks :)\r\nUff, that took some years to finish! \ud83d\ude09 . Please see this issue: https://github.com/spray/spray/issues/1132\r\n\r\n<img width=\"746\" alt=\"spray___rest_http_for_your_akka_scala_actors\" src=\"https://cloud.githubusercontent.com/assets/120979/25067816/b8d5d656-2289-11e7-9e4d-8104baf2691b.png\">\r\n\r\n\r\nSpray is EndOfLife (which has been foreshadowed and announced for a number of years),\r\nplease upgrade to Akka HTTP if you need 2.12 support. You'll also get a number of other awesome features etc.\r\n\r\nIf community would prepare the build to work for 2.12 we could run a release but we're not going to spend time on it currently - shipping HTTP/2 in Akka is more important currently. If you really need 2.12 in Spray you can either work on the upgrade and make a PR (we'd release), or contact lightbend for support.. Here's our migration guide from Spray: http://doc.akka.io/docs/akka-http/10.0.5/scala/http/migration-guide/migration-from-spray.html#migration-guide-from-spray. (Maybe will leave open so if someone wants to work on it they would see this). Firstly, spray is end of life and not maintained. Please upgrade to Akka HTTP, the migration is pretty simple. The EOL was announced a few years ago.\r\n\r\nPlease check if the same happens on Akka http and if so we can solve things there :). Akka HTTP has now been released as stable module ( akka.io/news/2016/11/22/akka-http-10.0.0-released.html ) thus we'll be more vocal about end-of-life of spray.\r\n\r\nWe just added a notice to the spray.io site that directs people to the akka docs etc.\r\nFor long term projects it makes sense to use Akka HTTP nowadays, esp. since it's now fully stable. . Feel free to ask on https://groups.google.com/forum/#!forum/akka-user if you have any more questions!. LGTM, would be very good to merge this as soon as possible so we can release akka-http and this together\n. IMO let's just patch 1.3 unless someone requests, after annoucement that they need older versions too\n. Hi there, \nthanks for reporting!\n\nWe were looking into a similar issue (about Content-Disposition) in Akka HTTP just recently, see here: https://github.com/akka/akka-http/issues/386\nCould you check if the same problem exists in Akka HTTP now?\n\nSpray won't be receiving maintenance unless it's security updates, I highly recommend you upgrade to Akka HTTP. The Akka HTTP DSL will be released as stable (core is already stable), as close as this or next week. Thanks in advance!\n. (and if yes, open an issue about it over there?)\nThanks for your help / understanding!\n. It's on maven-central, as all artifacts. Seems artifact name changed though?\n\nhttp://search.maven.org/#search|ga|1|spray-routing-shapeless `spray-routing-shapeless23_2.11`\n. We're documenting this fact in Akka HTTP btw: https://github.com/akka/akka-http/pull/412\n. That's not really planned since Spray is nearing end-of-life, the upgrade path being to move to Akka HTTP.\n\nIf community would prepare the build to work for 2.12 we could run a release but we're not going to spend time on it currently. We didn't see customers request it so far.\n. Thanks, it's true that now is the time to announce this more strongly than before.\r\nWe'll want to be a bit more specific than that, as well as link to 10.+ etc.\r\n\r\nI'll build upon your PR tomorrow to get the message right - we also want the same on the website.. Note was added by @sirthias to the site: https://github.com/spray/spray/commit/67e1fe800956cef1d410b49ca6f88270a4b9be0d\r\nAnd I'll update the README.\r\n\r\nThanks for the reminder @adamdecaf !. Readme update: https://github.com/spray/spray/pull/1134. Thanks :)\r\nUff, that took some years to finish! \ud83d\ude09 . Please see this issue: https://github.com/spray/spray/issues/1132\r\n\r\n<img width=\"746\" alt=\"spray___rest_http_for_your_akka_scala_actors\" src=\"https://cloud.githubusercontent.com/assets/120979/25067816/b8d5d656-2289-11e7-9e4d-8104baf2691b.png\">\r\n\r\n\r\nSpray is EndOfLife (which has been foreshadowed and announced for a number of years),\r\nplease upgrade to Akka HTTP if you need 2.12 support. You'll also get a number of other awesome features etc.\r\n\r\nIf community would prepare the build to work for 2.12 we could run a release but we're not going to spend time on it currently - shipping HTTP/2 in Akka is more important currently. If you really need 2.12 in Spray you can either work on the upgrade and make a PR (we'd release), or contact lightbend for support.. Here's our migration guide from Spray: http://doc.akka.io/docs/akka-http/10.0.5/scala/http/migration-guide/migration-from-spray.html#migration-guide-from-spray. (Maybe will leave open so if someone wants to work on it they would see this). ",
    "pstrzelczak": "This is already implemented in Akka IO's https://github.com/akka/akka/pull/1909\nCan it be used in spray?\nBTW: why does spray source have akka io source included?\n. It will take some time until akka-http becomes a part of standard akka.\nUntil this happens it would be good to expose pullMode in spray as in our tests we observe up to 50MB of data read before reading is suspended using currently available scheme. I am afraid without pullMode we will hit out-of-memory with multiple clients using our service.\nIt does not seem to be difficult to expose pullMode from akka in spray.\nI can also volunteer to try to do it if necessary :)\n. This is already implemented in Akka IO's https://github.com/akka/akka/pull/1909\nCan it be used in spray?\nBTW: why does spray source have akka io source included?\n. It will take some time until akka-http becomes a part of standard akka.\nUntil this happens it would be good to expose pullMode in spray as in our tests we observe up to 50MB of data read before reading is suspended using currently available scheme. I am afraid without pullMode we will hit out-of-memory with multiple clients using our service.\nIt does not seem to be difficult to expose pullMode from akka in spray.\nI can also volunteer to try to do it if necessary :)\n. ",
    "danarmak": "@sirthias when you say it will be ready, do you mean a production-quality release? Does that mean akka-streams will also have a production release by then? \n. @sirthias, that's great!\n\nHow will user code interact with akka-http-core? With actors that speak the akka-stream protocol? Just like in spray, with new extra messages for flow control? Something else?\n. We had to expose the spray-can parser (by writing code in the spray.can package) to build our own ByteString => HttpMessage function. This isn't drop-in ready to be used, but maybe someone who sees this ticket will find it useful: https://gist.github.com/danarmak/ed144326a3d90ecf067a\n\nIt uses the future-streams library, but is basically a simple state machine.\n. You should update reference.conf, it still says:\n\n> Enabling this mode causes all connections to be closed after a streaming\n> response has been finished since there is no other way to signal the\n> response end to the client.\n\nThis confused me for a short while.\n. I'll reference HTTPbis from now on. IIUC it has the same effective grammar here, which allows Cache-control: no-cache=field-name.\n\nI'm running a proxy server based on Spray that accesses many different backends. I'm no longer sure where this particular response came from, but I think it's a WordPress server behind Akamai CDN. I could check, but in any case the server is out of my hands. I need to be able to parse and forward any legitimate HTTP traffic.\n\nIf you think tracking down the particular server would help, I can do that.\n. But this header isn't broken, and being a proxy, I want to apply the Cache-Control semantics, so I need it to be parsed.\n. I'm experiencing this too. Apart from the logged error, is this actually harmful? Can I just ignore the error?\n. @pslavazza The error is logged because some actor died due to DeathPactException. That is, an actor watched my connection-handling actor, but didn't handle its death and so died itself. The actual error is logged by that actor's supervisor OneForOne strategy.\n\nI thought that the actor which died was the ConnectionHandler. If it remains alive as you say, then what actor is dying with DeathPactException?\n. If there's a resource leak or another issue with this beyond just a logged error, then is there a workaround? Could my per-connection actor induce or wait for the Spray actor's death and only then stop itself?\n. What if my per-connection actor waits for the ConnectionHandler to die? It _is_ going to die eventually after the connection closes, right?\n\nI don't have time to test this atm...\n. What is spray/akka's policy for RFC 2616 features deprecated or even forbidden in the new RFCs? Backward compatibility is sadly necessary for a long time on the web.\n. I don't know why the test failed, but the PR only changes a log message and exception message string (and the original string doesn't appear in any tests) so I'm pretty sure that can't be it.\n. @sirthias done.\n. What does \"failing in caves\" fatally mean? What is the spray behavior and what triggers it?\n. @jrudolph OK, thanks. You get a lot of [spiders](https://en.wikipedia.org/wiki/Category:Cave_spiders) in caves, crawling on a shared 104 baud link can't be good for them.\n. @sirthias the new title is partial: the problem is not _just_ that I don't get the original ack, but also that I get a Closed message when the connection isn't actually closed.\n. If the client sends a HEAD request, and later sends another request on the same connection, and I have an actor per connection, then when I get the Closed message I kill the actor and can't handle the second request. What should I be doing?\n. Also: I'm not dumping the whole stream (i.e. sending the real entity), I'm sending a ChunkedResponseStart with an empty entity and then a ChunkedMessageEnd. I have acks there because I have acks everywhere, it's generic code. I don't mind not using acks in certain cases (e.g. responses to HEAD requests), but as it stands I have to explicitly treat the Closed message as an ack. \n. That would be great, thanks.\n. I haven't checked, but the same problem probably applies to 304 Not Modified responses, which also have Content-Length values different from their actual rendered entities.\n. A series of chunked messages (`ChunkedXxxStart`, `MessageChunk`, ..., `ChunkedMessageEnd`) is not transferred using `Transfer-Encoding: Chunked` in chunkless-streaming mode, and a `Content-Length` header is then useful. I understand that changing the method would probably entail changing some/most of the call sites and so won't be done.\n. @sirthias when you say it will be ready, do you mean a production-quality release? Does that mean akka-streams will also have a production release by then? \n. @sirthias, that's great!\n\nHow will user code interact with akka-http-core? With actors that speak the akka-stream protocol? Just like in spray, with new extra messages for flow control? Something else?\n. We had to expose the spray-can parser (by writing code in the spray.can package) to build our own ByteString => HttpMessage function. This isn't drop-in ready to be used, but maybe someone who sees this ticket will find it useful: https://gist.github.com/danarmak/ed144326a3d90ecf067a\n\nIt uses the future-streams library, but is basically a simple state machine.\n. You should update reference.conf, it still says:\n\n> Enabling this mode causes all connections to be closed after a streaming\n> response has been finished since there is no other way to signal the\n> response end to the client.\n\nThis confused me for a short while.\n. I'll reference HTTPbis from now on. IIUC it has the same effective grammar here, which allows Cache-control: no-cache=field-name.\n\nI'm running a proxy server based on Spray that accesses many different backends. I'm no longer sure where this particular response came from, but I think it's a WordPress server behind Akamai CDN. I could check, but in any case the server is out of my hands. I need to be able to parse and forward any legitimate HTTP traffic.\n\nIf you think tracking down the particular server would help, I can do that.\n. But this header isn't broken, and being a proxy, I want to apply the Cache-Control semantics, so I need it to be parsed.\n. I'm experiencing this too. Apart from the logged error, is this actually harmful? Can I just ignore the error?\n. @pslavazza The error is logged because some actor died due to DeathPactException. That is, an actor watched my connection-handling actor, but didn't handle its death and so died itself. The actual error is logged by that actor's supervisor OneForOne strategy.\n\nI thought that the actor which died was the ConnectionHandler. If it remains alive as you say, then what actor is dying with DeathPactException?\n. If there's a resource leak or another issue with this beyond just a logged error, then is there a workaround? Could my per-connection actor induce or wait for the Spray actor's death and only then stop itself?\n. What if my per-connection actor waits for the ConnectionHandler to die? It _is_ going to die eventually after the connection closes, right?\n\nI don't have time to test this atm...\n. What is spray/akka's policy for RFC 2616 features deprecated or even forbidden in the new RFCs? Backward compatibility is sadly necessary for a long time on the web.\n. I don't know why the test failed, but the PR only changes a log message and exception message string (and the original string doesn't appear in any tests) so I'm pretty sure that can't be it.\n. @sirthias done.\n. What does \"failing in caves\" fatally mean? What is the spray behavior and what triggers it?\n. @jrudolph OK, thanks. You get a lot of [spiders](https://en.wikipedia.org/wiki/Category:Cave_spiders) in caves, crawling on a shared 104 baud link can't be good for them.\n. @sirthias the new title is partial: the problem is not _just_ that I don't get the original ack, but also that I get a Closed message when the connection isn't actually closed.\n. If the client sends a HEAD request, and later sends another request on the same connection, and I have an actor per connection, then when I get the Closed message I kill the actor and can't handle the second request. What should I be doing?\n. Also: I'm not dumping the whole stream (i.e. sending the real entity), I'm sending a ChunkedResponseStart with an empty entity and then a ChunkedMessageEnd. I have acks there because I have acks everywhere, it's generic code. I don't mind not using acks in certain cases (e.g. responses to HEAD requests), but as it stands I have to explicitly treat the Closed message as an ack. \n. That would be great, thanks.\n. I haven't checked, but the same problem probably applies to 304 Not Modified responses, which also have Content-Length values different from their actual rendered entities.\n. A series of chunked messages (`ChunkedXxxStart`, `MessageChunk`, ..., `ChunkedMessageEnd`) is not transferred using `Transfer-Encoding: Chunked` in chunkless-streaming mode, and a `Content-Length` header is then useful. I understand that changing the method would probably entail changing some/most of the call sites and so won't be done.\n. ",
    "GatesDA": "I can take a crack at it; I'll just need a bit of time to wrap my head around the protocol and Spray's codebase.\n. I can take a crack at it; I'll just need a bit of time to wrap my head around the protocol and Spray's codebase.\n. ",
    "jmorabito": "Thanks everyone!  Ridiculously quick response to this feature request.\n. Thanks everyone!  Ridiculously quick response to this feature request.\n. ",
    "balajisivaraman": "As a relative beginner to Scala and Spray, I had a look at this and came up with [this solution](https://github.com/BalajiSi/spray/commit/b8ba3836c8c147453f83d779845be8cd3af2d374). If anyone can confirm whether this looks good, I will go ahead and send a PR.\n\nEDIT: The way I understood is that the original spec only allowed absolute URIs, hence the usage of parseAbsolute() in the rules class. I changed it to take any URI, not only absolute ones.\n. @sirthias Done! :-)\n. As a relative beginner to Scala and Spray, I had a look at this and came up with [this solution](https://github.com/BalajiSi/spray/commit/b8ba3836c8c147453f83d779845be8cd3af2d374). If anyone can confirm whether this looks good, I will go ahead and send a PR.\n\nEDIT: The way I understood is that the original spec only allowed absolute URIs, hence the usage of parseAbsolute() in the rules class. I changed it to take any URI, not only absolute ones.\n. @sirthias Done! :-)\n. ",
    "dyross": "Any ETA on this?\n. Any ETA on this?\n. ",
    "tixxit": "Where can I find information on akka-http? When would an initial release be made?\n. That's great - thanks so much!\n. Where can I find information on akka-http? When would an initial release be made?\n. That's great - thanks so much!\n. ",
    "fsvehla": "I\u2019m pretty sure my branch didn\u2019t lead to that JVM core dump here https://travis-ci.org/spray/spray/builds/11032543 :)\n. @sirthias I think this pretty much matches your idea in #491 - any initial thoughts on this?\n. @sirthias I\u2019m not that familar with spray\u2019s modelling yet, how would this effect **spray-can** client rendering?\nWould we return `ContentTypes.NoContentType` for responses with missing Content Type headers?\n. @sirthias Like this?\n. @jrudolph Sounds fair. Done.\n. I\u2019m pretty sure my branch didn\u2019t lead to that JVM core dump here https://travis-ci.org/spray/spray/builds/11032543 :)\n. @sirthias I think this pretty much matches your idea in #491 - any initial thoughts on this?\n. @sirthias I\u2019m not that familar with spray\u2019s modelling yet, how would this effect **spray-can** client rendering?\nWould we return `ContentTypes.NoContentType` for responses with missing Content Type headers?\n. @sirthias Like this?\n. @jrudolph Sounds fair. Done.\n. ",
    "rvinay88": ":+1: \n. :+1: \n. ",
    "mhamrah": "Absolutely.  I will probably move on to documenting the futures directives\nand track work with #442.\n\nThank you Johannes, I'm happy to help.\n\nMike\n\nOn Mon, Sep 9, 2013 at 9:19 AM, Johannes Rudolph\nnotifications@github.comwrote:\n\n> @mhamrah https://github.com/mhamrah, thanks a lot. A basic thing: can\n> you add a few line-breaks into text blocks? It won't change the rendering\n> but makes it easier to review in the github web-interface and will later\n> make diffs more readble.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/496#issuecomment-24072895\n> .\n. @sirthias Makes sense.  I will elaborate more on usage and create example specs underneath /docs.  \n. I took another stab at this by creating a new \"MarshallingDirectivesExamplesSpec.scala\" file under /docs/documentation/spray-routing/code/docs/directives/.  I followed the naming convention of existing files.  However, I kept the signature pointing to the /spray-routing project.  I figure it didn't make much sense to copy the method signature.\n\nI simplified the examples as a starting point, and used spray-json for marshalling as that is probably a large use-case.\n\nI'm happy to elaborate the examples or make any other modifications.  One thing I thought of was an example using marshalling and a future showing the requirement of an implicit executionContext. But maybe that's better suited for the future directive documentation.\n\nAgain, happy to make any modifications.\n\nMike\n. Squashed.  Thanks Johannes, let me know if you'd like anything else.\n\nMike\n. Actually, that is the designed behavior. Spray routing can be a little tricky when you're just starting out, but [this document on the spray website](http://spray.io/documentation/1.2.0/spray-routing/advanced-topics/understanding-dsl-structure/) will help you figure out what is going on.\n. The spray mailing list is very active and has a wealth of information as\nwell. That's probably your best bet for finding information:\nhttps://groups.google.com/forum/m/#!forum/spray-user.\n\nOn Feb 1, 2014, at 8:50 AM, zhijun notifications@github.com wrote:\n\nSorry for creating this issue without comprehensive understanding of Spray.\nBut I know It's really a great framework and I don't want to turn to any\nother solution for REST handling. Thank you so much for guidance.\n\n## \n\nReply to this email directly or view it on\nGitHubhttps://github.com/spray/spray/issues/770#issuecomment-33872212\n.\n. Absolutely.  I will probably move on to documenting the futures directives\nand track work with #442.\n\nThank you Johannes, I'm happy to help.\n\nMike\n\nOn Mon, Sep 9, 2013 at 9:19 AM, Johannes Rudolph\nnotifications@github.comwrote:\n\n> @mhamrah https://github.com/mhamrah, thanks a lot. A basic thing: can\n> you add a few line-breaks into text blocks? It won't change the rendering\n> but makes it easier to review in the github web-interface and will later\n> make diffs more readble.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/496#issuecomment-24072895\n> .\n. @sirthias Makes sense.  I will elaborate more on usage and create example specs underneath /docs.  \n. I took another stab at this by creating a new \"MarshallingDirectivesExamplesSpec.scala\" file under /docs/documentation/spray-routing/code/docs/directives/.  I followed the naming convention of existing files.  However, I kept the signature pointing to the /spray-routing project.  I figure it didn't make much sense to copy the method signature.\n\nI simplified the examples as a starting point, and used spray-json for marshalling as that is probably a large use-case.\n\nI'm happy to elaborate the examples or make any other modifications.  One thing I thought of was an example using marshalling and a future showing the requirement of an implicit executionContext. But maybe that's better suited for the future directive documentation.\n\nAgain, happy to make any modifications.\n\nMike\n. Squashed.  Thanks Johannes, let me know if you'd like anything else.\n\nMike\n. Actually, that is the designed behavior. Spray routing can be a little tricky when you're just starting out, but [this document on the spray website](http://spray.io/documentation/1.2.0/spray-routing/advanced-topics/understanding-dsl-structure/) will help you figure out what is going on.\n. The spray mailing list is very active and has a wealth of information as\nwell. That's probably your best bet for finding information:\nhttps://groups.google.com/forum/m/#!forum/spray-user.\n\nOn Feb 1, 2014, at 8:50 AM, zhijun notifications@github.com wrote:\n\nSorry for creating this issue without comprehensive understanding of Spray.\nBut I know It's really a great framework and I don't want to turn to any\nother solution for REST handling. Thank you so much for guidance.\n\n## \n\nReply to this email directly or view it on\nGitHubhttps://github.com/spray/spray/issues/770#issuecomment-33872212\n.\n. ",
    "drapp": "If that's what you prefer, the version I have looks like this\n\n```\nclass SimpleLruCache[V](maxCapacity: Int, initialCapacity: Int, onEvict: Future[V] => Unit  = { _ => () }) extends Cache[V] {\n  require(maxCapacity >= 0, \"maxCapacity must not be negative\")\n  require(initialCapacity <= maxCapacity, \"initialCapacity must be <= maxCapacity\")\n\n\n  protected val store = {\n    val evictionListener = new EvictionListener[Any, Future[V]] {\n      def onEviction(key: Any, value: Future[V]) {\n        onEvict(value)\n      }\n    }\n    new ConcurrentLinkedHashMap.Builder[Any, Future[V]]\n      .initialCapacity(initialCapacity)\n      .maximumWeightedCapacity(maxCapacity)\n      .listener(evictionListener)\n      .build()\n  }\n...\n```\n\nDoes that look good to you? I'll need to add the same thing to `ExpiringLruCache` and the `LruCache.apply` method, along with some specs. Anything else?\n. I have the squashed PR here:\nhttps://github.com/spray/spray/pull/542\nIt should have everything addressed. Let me work on that CLA now\n. If that's what you prefer, the version I have looks like this\n\n```\nclass SimpleLruCache[V](maxCapacity: Int, initialCapacity: Int, onEvict: Future[V] => Unit  = { _ => () }) extends Cache[V] {\n  require(maxCapacity >= 0, \"maxCapacity must not be negative\")\n  require(initialCapacity <= maxCapacity, \"initialCapacity must be <= maxCapacity\")\n\n\n  protected val store = {\n    val evictionListener = new EvictionListener[Any, Future[V]] {\n      def onEviction(key: Any, value: Future[V]) {\n        onEvict(value)\n      }\n    }\n    new ConcurrentLinkedHashMap.Builder[Any, Future[V]]\n      .initialCapacity(initialCapacity)\n      .maximumWeightedCapacity(maxCapacity)\n      .listener(evictionListener)\n      .build()\n  }\n...\n```\n\nDoes that look good to you? I'll need to add the same thing to `ExpiringLruCache` and the `LruCache.apply` method, along with some specs. Anything else?\n. I have the squashed PR here:\nhttps://github.com/spray/spray/pull/542\nIt should have everything addressed. Let me work on that CLA now\n. ",
    "photex": "heh, please ignore the comment after the link. I eventually discovered MediaType.custom :)\n. Hi @jrudolph,\n\nAs an example use case, I followed a packet dump between mobile Safari and NGinx:\n\n-> Request from safari for the resource\n\n<- nginx responds with 200, includes \"Accept-Range: bytes\" in the header and starts streaming the bytes of the file in it's entirety.\n\n-> Another request immediately from safari for the resource but sends the If-Modified-Since header, and asks for the byte range 0-1\n\n<- nginx responds with 304, and doesn't send anything else.\n\n-> Safari makes yet another request for a byte range of the file and includes the header \"X-Playback-Session-Id: <UUID>\" and \"Connection: keep-alive\".\n\n<- nginx reponds with 206 with the requested content range\n\n-> Safari makes yet another request for the byte range 0-1, but is missing the If-Modified-Since header\n\n<- nginx responds with 206 and the bytes requested.\n\n... and so on and so on until Safari is happy with all the apparently duplicated bytes it's received (or ignored I suppose). Safari/AppleCoreMedia seems to be super chatty and appears to be trying to grab pieces of the file like the header, other metadata (or maybe a preview frame) while it's loading the contents being streamed from the first request. Trying to scrub the video generates more range requests as well.\n\nSomething that seems pretty interesting to me is the X-Playback-Session-Id and how I could make use of that from the DSL.\n\nAlso, looking at a packet dump from Chrome if you respond to a request with \"Accept-Range: bytes\" it will also request the first two bytes of a file and include the If-Modified-Since header. My first naive attempt at serving video interpreted this as two unique requests for the resource and sent the file twice instead of responding correctly.\n\nCheers!\n\nChip\n. The user agent is reported as: AppleCoreMedia/1.0.0.10B329 (iPad; U; CPU OS 6_1_3 like Mac OS X; en_us)\n\nI believe that it's specific to when an iDevice is requesting media via http, and you'd see a different user agent for other requests. I would be more than happy to collect a set of short packet captures (single requests) of different types if that would be useful.\n. No problems at all. It'll give me a chance to learn a bit more about how the directives are implemented and perhaps take a stab at the feature myself.\n\nIn the meantime this is what I have as a really basic working proof of concept with the existing set of features in the DSL (I still want to chunk the ranges back similar to how getFromFile would do it):\n\n``` scala\npath(\"oceans-clip.mp4\") {\n        val file = new File(\"oceans-clip.mp4\")\n\n        headerValueByName(\"If-Modified-Since\") { modified_since =>\n          if (date_format.parse(modified_since).getTime > file.lastModified()) {\n            complete(NotModified)\n          } else {\n            complete(OK)\n          }\n        } ~\n        headerValueByName(\"Range\") { range_slice =>\n          range_slice match {\n              case ByteRange(start_spec, end_spec) => {\n                val file_length = file.length()\n                val start = if (start_spec == \"\") 0 else start_spec.toInt\n                val end = if (end_spec == \"\") file_length-1 else end_spec.toInt\n                val range_size = end - start\n\n                val buffer = new Array[Byte](range_size.toInt+1)\n                val input = new RandomAccessFile(file, \"r\")\n\n                input.seek(start)\n                val bytes_read = input.read(buffer)\n                if (bytes_read < range_size) {\n                  log.error(s\"only read $bytes_read of $range_size bytes.\")\n                }\n                input.close()\n\n                val headers = List(\n                  HttpHeaders.RawHeader(\"Content-Range\", s\"bytes $start-$end/$file_length\")\n                )\n                respondWithHeaders(headers) {\n                  complete(PartialContent, buffer)\n                }\n              }\n\n              case _ => {\n                complete(BadRequest)\n              }\n            }\n        } ~\n        respondWithHeader(HttpHeaders.RawHeader(\"Accept-Range\", \"bytes\")) {\n          getFromFile(file, ContentType(`video/mp4`))\n        }\n      }\n    }\n```\n. `ByteRange` is just a regex which I forgot to add at the top apparently! :)\n\n``` scala\nval ByteRange = \"^bytes=(.*)-(.*)$\".r\n```\n\nThanks for the example directive. I'll start putting something together. \n\nHere is a quick attempt to chunk the range request that _sorta_ works:\n\n``` scala\n  def byteArrayStreamForRange(file:File,\n                              start:Int, end:Int,\n                              chunk_size:Int): Stream[Array[Byte]] = {\n    val input = new FileInputStream(file)\n    input.skip(start)\n\n    def readChunk(offset:Int):Stream[Array[Byte]] = {\n      val buffer_size =\n        if ((offset + chunk_size) > end) end - offset\n        else chunk_size\n      val buffer = new Array[Byte](buffer_size)\n      val bytes_read = input.read(buffer)\n      if (bytes_read > 0 && buffer.length > 0) {\n        Stream.cons(buffer, readChunk(offset+bytes_read))\n      } else {\n        // TODO: What happens when the connection is closed before we get to this?\n        input.close()\n        Stream.empty\n      }\n    }\n\n    readChunk(0)\n  }\n```\n. heh, please ignore the comment after the link. I eventually discovered MediaType.custom :)\n. Hi @jrudolph,\n\nAs an example use case, I followed a packet dump between mobile Safari and NGinx:\n\n-> Request from safari for the resource\n\n<- nginx responds with 200, includes \"Accept-Range: bytes\" in the header and starts streaming the bytes of the file in it's entirety.\n\n-> Another request immediately from safari for the resource but sends the If-Modified-Since header, and asks for the byte range 0-1\n\n<- nginx responds with 304, and doesn't send anything else.\n\n-> Safari makes yet another request for a byte range of the file and includes the header \"X-Playback-Session-Id: <UUID>\" and \"Connection: keep-alive\".\n\n<- nginx reponds with 206 with the requested content range\n\n-> Safari makes yet another request for the byte range 0-1, but is missing the If-Modified-Since header\n\n<- nginx responds with 206 and the bytes requested.\n\n... and so on and so on until Safari is happy with all the apparently duplicated bytes it's received (or ignored I suppose). Safari/AppleCoreMedia seems to be super chatty and appears to be trying to grab pieces of the file like the header, other metadata (or maybe a preview frame) while it's loading the contents being streamed from the first request. Trying to scrub the video generates more range requests as well.\n\nSomething that seems pretty interesting to me is the X-Playback-Session-Id and how I could make use of that from the DSL.\n\nAlso, looking at a packet dump from Chrome if you respond to a request with \"Accept-Range: bytes\" it will also request the first two bytes of a file and include the If-Modified-Since header. My first naive attempt at serving video interpreted this as two unique requests for the resource and sent the file twice instead of responding correctly.\n\nCheers!\n\nChip\n. The user agent is reported as: AppleCoreMedia/1.0.0.10B329 (iPad; U; CPU OS 6_1_3 like Mac OS X; en_us)\n\nI believe that it's specific to when an iDevice is requesting media via http, and you'd see a different user agent for other requests. I would be more than happy to collect a set of short packet captures (single requests) of different types if that would be useful.\n. No problems at all. It'll give me a chance to learn a bit more about how the directives are implemented and perhaps take a stab at the feature myself.\n\nIn the meantime this is what I have as a really basic working proof of concept with the existing set of features in the DSL (I still want to chunk the ranges back similar to how getFromFile would do it):\n\n``` scala\npath(\"oceans-clip.mp4\") {\n        val file = new File(\"oceans-clip.mp4\")\n\n        headerValueByName(\"If-Modified-Since\") { modified_since =>\n          if (date_format.parse(modified_since).getTime > file.lastModified()) {\n            complete(NotModified)\n          } else {\n            complete(OK)\n          }\n        } ~\n        headerValueByName(\"Range\") { range_slice =>\n          range_slice match {\n              case ByteRange(start_spec, end_spec) => {\n                val file_length = file.length()\n                val start = if (start_spec == \"\") 0 else start_spec.toInt\n                val end = if (end_spec == \"\") file_length-1 else end_spec.toInt\n                val range_size = end - start\n\n                val buffer = new Array[Byte](range_size.toInt+1)\n                val input = new RandomAccessFile(file, \"r\")\n\n                input.seek(start)\n                val bytes_read = input.read(buffer)\n                if (bytes_read < range_size) {\n                  log.error(s\"only read $bytes_read of $range_size bytes.\")\n                }\n                input.close()\n\n                val headers = List(\n                  HttpHeaders.RawHeader(\"Content-Range\", s\"bytes $start-$end/$file_length\")\n                )\n                respondWithHeaders(headers) {\n                  complete(PartialContent, buffer)\n                }\n              }\n\n              case _ => {\n                complete(BadRequest)\n              }\n            }\n        } ~\n        respondWithHeader(HttpHeaders.RawHeader(\"Accept-Range\", \"bytes\")) {\n          getFromFile(file, ContentType(`video/mp4`))\n        }\n      }\n    }\n```\n. `ByteRange` is just a regex which I forgot to add at the top apparently! :)\n\n``` scala\nval ByteRange = \"^bytes=(.*)-(.*)$\".r\n```\n\nThanks for the example directive. I'll start putting something together. \n\nHere is a quick attempt to chunk the range request that _sorta_ works:\n\n``` scala\n  def byteArrayStreamForRange(file:File,\n                              start:Int, end:Int,\n                              chunk_size:Int): Stream[Array[Byte]] = {\n    val input = new FileInputStream(file)\n    input.skip(start)\n\n    def readChunk(offset:Int):Stream[Array[Byte]] = {\n      val buffer_size =\n        if ((offset + chunk_size) > end) end - offset\n        else chunk_size\n      val buffer = new Array[Byte](buffer_size)\n      val bytes_read = input.read(buffer)\n      if (bytes_read > 0 && buffer.length > 0) {\n        Stream.cons(buffer, readChunk(offset+bytes_read))\n      } else {\n        // TODO: What happens when the connection is closed before we get to this?\n        input.close()\n        Stream.empty\n      }\n    }\n\n    readChunk(0)\n  }\n```\n. ",
    "arschles": "thanks @jrudolph. will keep an eye on this\n. sounds good. thanks for the improvement\n. thanks @jrudolph. will keep an eye on this\n. sounds good. thanks for the improvement\n. ",
    "laogao": "Is <code>spray.can.host-connector.follow-redirects</code> still used?\n. Looking at the source (release/1.3), it seems that <code>spray.can.host-connector.max-redirects</code> is used instead. Setting <code>spray.can.host-connector.follow-redirects</code> to <code>true</code> has no visible effect.  And I have to set <code>spray.can.host-connector.max-redirects</code> manually to something bigger than 0 to make it work. From what I see (as of 1.3.1) the default value for <code>spray.can.host-connector.max-redirects</code> is 0 rather than 5.\n. This link explains it all: http://spray.io/documentation/1.2.1/spray-can/http-client/#redirection-following\nIf you're like me, getting here via Google search, the above link is where you should be looking at.\n. Is <code>spray.can.host-connector.follow-redirects</code> still used?\n. Looking at the source (release/1.3), it seems that <code>spray.can.host-connector.max-redirects</code> is used instead. Setting <code>spray.can.host-connector.follow-redirects</code> to <code>true</code> has no visible effect.  And I have to set <code>spray.can.host-connector.max-redirects</code> manually to something bigger than 0 to make it work. From what I see (as of 1.3.1) the default value for <code>spray.can.host-connector.max-redirects</code> is 0 rather than 5.\n. This link explains it all: http://spray.io/documentation/1.2.1/spray-can/http-client/#redirection-following\nIf you're like me, getting here via Google search, the above link is where you should be looking at.\n. ",
    "evantill": "see discussion on https://groups.google.com/forum/#!topic/spray-user/xxleBYL2q74\n. addQueryParameter but the question is more general.\n\nthe dsl in spray-routing is very clear but when I try to use the spray-client pipeline dsl I feel less comfortable.\nRequestBuilding, ResponseTransformation, and the  spray.client.pipelining dsl could be more homogeneous\n\nfor example, SendReceive type use Future where as ResponseTransformer is not.\nAn other example, when trying a pipeline with  \n`val pipeline: HttpRequest => Future[HttpResponse] = sendReceive ~> logResponse(log)` I have this error `value ~> is not a member of scala.concurrent.Future[spray.http.HttpResponse]`\n\nwhat about using lenses in the RequestBuilding ? \nhttps://gist.github.com/evantill/6882766\n\nis spray already use scalaz ? is this something that can arise ?\n. see discussion on https://groups.google.com/forum/#!topic/spray-user/xxleBYL2q74\n. addQueryParameter but the question is more general.\n\nthe dsl in spray-routing is very clear but when I try to use the spray-client pipeline dsl I feel less comfortable.\nRequestBuilding, ResponseTransformation, and the  spray.client.pipelining dsl could be more homogeneous\n\nfor example, SendReceive type use Future where as ResponseTransformer is not.\nAn other example, when trying a pipeline with  \n`val pipeline: HttpRequest => Future[HttpResponse] = sendReceive ~> logResponse(log)` I have this error `value ~> is not a member of scala.concurrent.Future[spray.http.HttpResponse]`\n\nwhat about using lenses in the RequestBuilding ? \nhttps://gist.github.com/evantill/6882766\n\nis spray already use scalaz ? is this something that can arise ?\n. ",
    "jeffwatson10": "Sorry, This is using the 1.2-20130822 nightly build, but I did check the 1.2 branch and the issue looks to still be present in Latest.\n\nThe URL in question is in fact invalid, having a unexpected =. The issue is that the IllegalRequestException bubbles up into the Servlet, bypassing the logging, to be caught higher up in Tomcat. \n\nThe result is that the Log message is outside all of the apps configured logging\n. It looks like a fairly straightfoward fix, there is a def request of line 60, and a val request on line 62... I'm guessing the log.warning on linee 67 is supposed to be using the val request, which I believe is out of scope at this point, resulting is a second invocation of request()\n. Thank you much\n. Morning Johannes,\n\nIts actually stemming from a bunch of legacy apps, which are actually\npassing the requests as ISO-8859-1 but using the charset iso-8859-*, which\nthen throws the exception\n\nUnfortunately, I don't know of a good way to rewrite that header before the\nrequest gets to Spray, or having Spray ignore that Header and use a\ndefault. I'm certainly open to other ideas.\n\nThanks\nJeff\n\nOn Tue, Oct 29, 2013 at 5:26 AM, Johannes Rudolph\nnotifications@github.comwrote:\n\n> Thanks for the report.\n> \n> I think you are right about the exception but fixing that won't stop\n> another exception from being thrown out of the parser. If the charset\n> requested isn't available there's no way to support a request. Which is the\n> problematic charset?\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/issues/637#issuecomment-27288102\n> .\n\n## \n- Jeff **Watson  *|Software Tech Lead\n- w:\\* 770-226-2719      _e:_ jeff.watson@weather.com\n. I do understand that, iso-8859-\\* is definitely an invalid charset. My\nrequest is simply to treat it exactly like an Unsupported Charset is\ncurrently, catch the exception and return a None\n\nThanks\nJeff\n\nOn Tue, Oct 29, 2013 at 11:14 AM, Mathias notifications@github.com wrote:\n\n> Jeff, the problem is that we cannot relax/adapt the parser to all buggy\n> HTTP implementations out there.\n> iso-8859-\\* is not a valid charset and spray cannot decode the message\n> content if the charset is unknown.\n> \n> We've been talking about a feature that makes it easy to \"inject\" a custom\n> actor layer between spray-can and akka-io, which would allow you to patch\n> the incoming request bytes according to your needs. I have just created a\n> ticket for that: #648 https://github.com/spray/spray/issues/648\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/issues/637#issuecomment-27311608\n> .\n\n## \n- Jeff **Watson  *|Software Tech Lead\n- w:\\* 770-226-2719      _e:_ jeff.watson@weather.com\n. Sorry, This is using the 1.2-20130822 nightly build, but I did check the 1.2 branch and the issue looks to still be present in Latest.\n\nThe URL in question is in fact invalid, having a unexpected =. The issue is that the IllegalRequestException bubbles up into the Servlet, bypassing the logging, to be caught higher up in Tomcat. \n\nThe result is that the Log message is outside all of the apps configured logging\n. It looks like a fairly straightfoward fix, there is a def request of line 60, and a val request on line 62... I'm guessing the log.warning on linee 67 is supposed to be using the val request, which I believe is out of scope at this point, resulting is a second invocation of request()\n. Thank you much\n. Morning Johannes,\n\nIts actually stemming from a bunch of legacy apps, which are actually\npassing the requests as ISO-8859-1 but using the charset iso-8859-*, which\nthen throws the exception\n\nUnfortunately, I don't know of a good way to rewrite that header before the\nrequest gets to Spray, or having Spray ignore that Header and use a\ndefault. I'm certainly open to other ideas.\n\nThanks\nJeff\n\nOn Tue, Oct 29, 2013 at 5:26 AM, Johannes Rudolph\nnotifications@github.comwrote:\n\n> Thanks for the report.\n> \n> I think you are right about the exception but fixing that won't stop\n> another exception from being thrown out of the parser. If the charset\n> requested isn't available there's no way to support a request. Which is the\n> problematic charset?\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/issues/637#issuecomment-27288102\n> .\n\n## \n- Jeff **Watson  *|Software Tech Lead\n- w:\\* 770-226-2719      _e:_ jeff.watson@weather.com\n. I do understand that, iso-8859-\\* is definitely an invalid charset. My\nrequest is simply to treat it exactly like an Unsupported Charset is\ncurrently, catch the exception and return a None\n\nThanks\nJeff\n\nOn Tue, Oct 29, 2013 at 11:14 AM, Mathias notifications@github.com wrote:\n\n> Jeff, the problem is that we cannot relax/adapt the parser to all buggy\n> HTTP implementations out there.\n> iso-8859-\\* is not a valid charset and spray cannot decode the message\n> content if the charset is unknown.\n> \n> We've been talking about a feature that makes it easy to \"inject\" a custom\n> actor layer between spray-can and akka-io, which would allow you to patch\n> the incoming request bytes according to your needs. I have just created a\n> ticket for that: #648 https://github.com/spray/spray/issues/648\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/issues/637#issuecomment-27311608\n> .\n\n## \n- Jeff **Watson  *|Software Tech Lead\n- w:\\* 770-226-2719      _e:_ jeff.watson@weather.com\n. ",
    "devmage": "Pursuant to the linked discussion, it would be even better (!!) if an option to send raw paths were also considered.\n. Pursuant to the linked discussion, it would be even better (!!) if an option to send raw paths were also considered.\n. ",
    "jiaweihli": "Ah, I wasn't even aware RC1/2 had been released!\n\nI updated to RC2 and gave it a whirl.  It's been going for about 20 minutes, but seems to have fixed the issue - old gen is decreasing at regular intervals as it should.\n\nI'll let it sit for a few more days and report back if I notice anything unusual.\n. I saw problems again under higher load (~5k rps, spikes up to 20k).\nSeems related to #421 - though I'm not using the ActorLogging or SprayActorLogging trait.\n\nI removed the logback.xml config file and that seemed to solve the problem.\n. Not logging using log.info or log.debug - I'm logging via the Akka config by turning on debug mode and logging all receive and lifecycle events.\nI supply a custom logback.xml that writes to a file.\n\nWhen both of these are in place, I see memory leaking slowly for every request.\nWhen this logging is disabled, memory doesn't leak.\n\nMy service sees ~1k rps.  It leaks about 100-200 MB / day.\n\nI'm not sure if this is due to Akka, Spray, or Logback.  This is the first time I've seen anything like it.\n\nI'll open a fresh ticket after I set up a test case integrated with wrk or something.\n. Ah, I wasn't even aware RC1/2 had been released!\n\nI updated to RC2 and gave it a whirl.  It's been going for about 20 minutes, but seems to have fixed the issue - old gen is decreasing at regular intervals as it should.\n\nI'll let it sit for a few more days and report back if I notice anything unusual.\n. I saw problems again under higher load (~5k rps, spikes up to 20k).\nSeems related to #421 - though I'm not using the ActorLogging or SprayActorLogging trait.\n\nI removed the logback.xml config file and that seemed to solve the problem.\n. Not logging using log.info or log.debug - I'm logging via the Akka config by turning on debug mode and logging all receive and lifecycle events.\nI supply a custom logback.xml that writes to a file.\n\nWhen both of these are in place, I see memory leaking slowly for every request.\nWhen this logging is disabled, memory doesn't leak.\n\nMy service sees ~1k rps.  It leaks about 100-200 MB / day.\n\nI'm not sure if this is due to Akka, Spray, or Logback.  This is the first time I've seen anything like it.\n\nI'll open a fresh ticket after I set up a test case integrated with wrk or something.\n. ",
    "michaelklishin": "if you want to see examples of beginner-friendly documentation, see\n- [Requests](http://www.python-requests.org)\n- [Langohr](http://clojurerabbitmq.info)\n- [Django](https://docs.djangoproject.com/en/1.6/#first-steps)\n- [Ember.js](http://emberjs.com/guides/)\n\nAll of them focus on the essential parts first and gradually introduce the reader to more advanced topics.\n. Spray beginner.\n\nThe page you've linked to barely explains anything from the list in the original issue.\n- How do I POST something?\n- What is the `?` operator?\n- How do I access response status, headers and body?\n\nIf you don't explain those 3, how do you think people will be able to use your library?\n. > This ? operator has nothing to do with spray, it's Akka's \"ask\" operator and therefore not in scope for our docs.\n\nYour users may or may not know anything about Akka. At least mention where it's coming from.\n\n> The example shows you how to get hold of an HttpResponse instance. We assume that people would expect the\n> status, headers, body, etc. to be members of this instance (which they are).\n\nDon't make me think. At the very least, link to the API reference (Scaladoc or similar), and it would not hurt to simply\nprovide 3 lines in the same example.\n. @sirthias any updates on this? I find it ridiculous how hard it is to get started with Spray. Second weekend wasted reading code to produce something really really basic. Issues like this should be top priority if you want people to use your stuff.\n. if you want to see examples of beginner-friendly documentation, see\n- [Requests](http://www.python-requests.org)\n- [Langohr](http://clojurerabbitmq.info)\n- [Django](https://docs.djangoproject.com/en/1.6/#first-steps)\n- [Ember.js](http://emberjs.com/guides/)\n\nAll of them focus on the essential parts first and gradually introduce the reader to more advanced topics.\n. Spray beginner.\n\nThe page you've linked to barely explains anything from the list in the original issue.\n- How do I POST something?\n- What is the `?` operator?\n- How do I access response status, headers and body?\n\nIf you don't explain those 3, how do you think people will be able to use your library?\n. > This ? operator has nothing to do with spray, it's Akka's \"ask\" operator and therefore not in scope for our docs.\n\nYour users may or may not know anything about Akka. At least mention where it's coming from.\n\n> The example shows you how to get hold of an HttpResponse instance. We assume that people would expect the\n> status, headers, body, etc. to be members of this instance (which they are).\n\nDon't make me think. At the very least, link to the API reference (Scaladoc or similar), and it would not hurt to simply\nprovide 3 lines in the same example.\n. @sirthias any updates on this? I find it ridiculous how hard it is to get started with Spray. Second weekend wasted reading code to produce something really really basic. Issues like this should be top priority if you want people to use your stuff.\n. ",
    "SeanTAllen": "When it came time to pick something for a quick prototype, spray was on our list but given 'quick' I moved on as spray documentation was way too dense to get started quickly and learn by doing.\n. When it came time to pick something for a quick prototype, spray was on our list but given 'quick' I moved on as spray documentation was way too dense to get started quickly and learn by doing.\n. ",
    "crypticmind": "Another thing I would add, when you start writing your first clients, you definitely want to see what's actually sent over the wire. In those cases, HttpClient's [wire logging](http://hc.apache.org/httpclient-3.x/logging.html) has proven invaluable to me. All I could get in spray so far was to add logging at the pipeline level, but that will not let me see, for instance, the host header.\n. Another thing I would add, when you start writing your first clients, you definitely want to see what's actually sent over the wire. In those cases, HttpClient's [wire logging](http://hc.apache.org/httpclient-3.x/logging.html) has proven invaluable to me. All I could get in spray so far was to add logging at the pipeline level, but that will not let me see, for instance, the host header.\n. ",
    "wongelz": "You'll need to use pathEnd instead of path(\"\").\n\nSee these discussion threads: \nhttps://groups.google.com/forum/#!searchin/spray-user/pathend/spray-user/W6D6DlL2MvU/d0dfcX-BaKgJ\nhttps://groups.google.com/forum/#!searchin/spray-user/pathend/spray-user/S8PM4NUMZ8I/ZlQ9mp2Z6b4J\n. You'll need to use pathEnd instead of path(\"\").\n\nSee these discussion threads: \nhttps://groups.google.com/forum/#!searchin/spray-user/pathend/spray-user/W6D6DlL2MvU/d0dfcX-BaKgJ\nhttps://groups.google.com/forum/#!searchin/spray-user/pathend/spray-user/S8PM4NUMZ8I/ZlQ9mp2Z6b4J\n. ",
    "yuhanonescreen": "pathEnd works great! thanks. :)\n. pathEnd works great! thanks. :)\n. ",
    "kgignatyev": "Precisely, I my comment is from perspective of coming from \"other\" frameworks, thought it might be useful for you to know.\n\nAnd I like your idea Rudolpg of setting root-path automatically by servlet as reported by ServletContext, this behavior makes more sense to me than documentation \n. Makes sense. \n. Good to know, thanks.\nJust my opinion:  Base64 sucks for development because it is not readable and current support by browsers and frameworks for escaping with backslash makes so much more sense than RFC6265\n. Precisely, I my comment is from perspective of coming from \"other\" frameworks, thought it might be useful for you to know.\n\nAnd I like your idea Rudolpg of setting root-path automatically by servlet as reported by ServletContext, this behavior makes more sense to me than documentation \n. Makes sense. \n. Good to know, thanks.\nJust my opinion:  Base64 sucks for development because it is not readable and current support by browsers and frameworks for escaping with backslash makes so much more sense than RFC6265\n. ",
    "timcharper": "I think there may be an answer between \"SUPPORT BAD COOKIES\" and \"NO\". We've encountered an issue where embedded javascript plugins (on our site, that we don't control) are generating bad cookies.\n\nThis isn't such a big deal, we don't need to read those cookies on our Scala backend. HOWEVER, if you have one bad cookie, then it the entire parse fails, and no cookies are recognized.\n\nI would move that spray at least gracefully handle poorly formatted cookies and ignore them.\n. I have signed the typesafe CLA, per instructions https://www.typesafe.com/contribute/cla/verify\n. Bump\n. Yes, this patch enables both to build from the same branch.\n\nThe larger issue is that 2.10 builds and 2.11 builds are not symmetrically uploaded. Looking here: http://repo.spray.io/io/spray/\n\n```\nspray-caching/                 2014-05-26 12:26:59\nspray-caching_2.11/            2014-04-23 14:04:16\nspray-can/                     2014-05-26 12:27:12\nspray-can_2.11/                2014-04-23 14:04:57\nspray-client/                  2014-05-26 12:27:03\nspray-client_2.11/             2014-04-23 14:05:00\nspray-http/                    2014-05-26 12:27:18\nspray-http_2.11/               2014-04-23 14:04:40\nspray-httpx/                   2014-05-26 12:27:10\nspray-httpx_2.11/  \n```\n\nBecause scala 2.10 builds are not suffixed with their version, it makes my job, as one building cross-built libraries for spray, very difficult.\n\nMy small patch allows both to be built from the same branch.\n\nI would advise you remove core support for libraries that aren't going 2.11 anytime soon, and provide them in separate, scala-2.10 only jars. I can help with this if you like.\n. Looks like you guys decided to do it after all?\n\nhttp://repo.spray.io/io/spray/\n\n```\nIndex of /io/spray/\n\n../\nsbt-twirl_2.10_0.13/                  2014-03-17 16:07:53\nsbt-twirl_2.9.1_0.11.2/               2013-05-02 09:55:18\nsbt-twirl_2.9.1_0.11.3/               2013-05-02 09:55:12\nsbt-twirl_2.9.2_0.12/                 2014-03-17 16:07:50\nspray-caching/                        2014-05-26 12:26:59\nspray-caching_2.10/                   2014-06-25 13:12:51\nspray-caching_2.11/                   2014-06-25 13:02:36\nspray-can/                            2014-05-26 12:27:12\nspray-can_2.10/                       2014-06-25 13:12:58\nspray-can_2.11/                       2014-06-25 13:02:47\nspray-client/                         2014-05-26 12:27:03\nspray-client_2.10/                    2014-06-25 13:12:53\nspray-client_2.11/                    2014-06-25 13:02:41\nspray-http/                           2014-05-26 12:27:18\nspray-http_2.10/                      2014-06-25 13:13:01\nspray-http_2.11/                      2014-06-25 13:02:49\nspray-httpx/                          2014-05-26 12:27:10\nspray-httpx_2.10/                     2014-06-25 13:12:56\nspray-httpx_2.11/   \n```\n\nThis is fantastic. Thank you!\n. I think there may be an answer between \"SUPPORT BAD COOKIES\" and \"NO\". We've encountered an issue where embedded javascript plugins (on our site, that we don't control) are generating bad cookies.\n\nThis isn't such a big deal, we don't need to read those cookies on our Scala backend. HOWEVER, if you have one bad cookie, then it the entire parse fails, and no cookies are recognized.\n\nI would move that spray at least gracefully handle poorly formatted cookies and ignore them.\n. I have signed the typesafe CLA, per instructions https://www.typesafe.com/contribute/cla/verify\n. Bump\n. Yes, this patch enables both to build from the same branch.\n\nThe larger issue is that 2.10 builds and 2.11 builds are not symmetrically uploaded. Looking here: http://repo.spray.io/io/spray/\n\n```\nspray-caching/                 2014-05-26 12:26:59\nspray-caching_2.11/            2014-04-23 14:04:16\nspray-can/                     2014-05-26 12:27:12\nspray-can_2.11/                2014-04-23 14:04:57\nspray-client/                  2014-05-26 12:27:03\nspray-client_2.11/             2014-04-23 14:05:00\nspray-http/                    2014-05-26 12:27:18\nspray-http_2.11/               2014-04-23 14:04:40\nspray-httpx/                   2014-05-26 12:27:10\nspray-httpx_2.11/  \n```\n\nBecause scala 2.10 builds are not suffixed with their version, it makes my job, as one building cross-built libraries for spray, very difficult.\n\nMy small patch allows both to be built from the same branch.\n\nI would advise you remove core support for libraries that aren't going 2.11 anytime soon, and provide them in separate, scala-2.10 only jars. I can help with this if you like.\n. Looks like you guys decided to do it after all?\n\nhttp://repo.spray.io/io/spray/\n\n```\nIndex of /io/spray/\n\n../\nsbt-twirl_2.10_0.13/                  2014-03-17 16:07:53\nsbt-twirl_2.9.1_0.11.2/               2013-05-02 09:55:18\nsbt-twirl_2.9.1_0.11.3/               2013-05-02 09:55:12\nsbt-twirl_2.9.2_0.12/                 2014-03-17 16:07:50\nspray-caching/                        2014-05-26 12:26:59\nspray-caching_2.10/                   2014-06-25 13:12:51\nspray-caching_2.11/                   2014-06-25 13:02:36\nspray-can/                            2014-05-26 12:27:12\nspray-can_2.10/                       2014-06-25 13:12:58\nspray-can_2.11/                       2014-06-25 13:02:47\nspray-client/                         2014-05-26 12:27:03\nspray-client_2.10/                    2014-06-25 13:12:53\nspray-client_2.11/                    2014-06-25 13:02:41\nspray-http/                           2014-05-26 12:27:18\nspray-http_2.10/                      2014-06-25 13:13:01\nspray-http_2.11/                      2014-06-25 13:02:49\nspray-httpx/                          2014-05-26 12:27:10\nspray-httpx_2.10/                     2014-06-25 13:12:56\nspray-httpx_2.11/   \n```\n\nThis is fantastic. Thank you!\n. ",
    "13h3r": "@jrudolph but I got this every request on my desktop\n@sirthias thanks for quick fix. I will check build from master on next Monday\n. Any updates on this?\n. @jrudolph but I got this every request on my desktop\n@sirthias thanks for quick fix. I will check build from master on next Monday\n. Any updates on this?\n. ",
    "ladrl": "Hi sirthias, \nI look for some part in spray and I figured that this feature could be a starting point. What do you think? Would it make sense? If yes, where would be the best point to look for the place to implement the wire logging?\n\nCheers\n\nLukas\n. Hi sirthias, \nI look for some part in spray and I figured that this feature could be a starting point. What do you think? Would it make sense? If yes, where would be the best point to look for the place to implement the wire logging?\n\nCheers\n\nLukas\n. ",
    "yaderbh": "Any update on this? Is there documentation on how to configure client-side SSL cert? The ability to provide CA file, etc.\n. Any update on this? Is there documentation on how to configure client-side SSL cert? The ability to provide CA file, etc.\n. ",
    "ScalaWilliam": "+1\n. +1. \n. +1\n. +1. \n. ",
    "maffstephens": "This would be a nice fix to have.\n. This would be a nice fix to have.\n. ",
    "DavidPerezIngeniero": "It would be easier to use without the CommandWrapper\n. It would be easier to use without the CommandWrapper\n. ",
    "slorber": "+1\n. +1\n. ",
    "jonathan-neufeld-hs": "FYI, I was looking for some way to suppress logging warnings for these malformed headers and found it  here: 19a1ff23ccb68b97ef5bfadd7afa013bbb621581\n\nSet `spray.can.parsing.illegal-header-warnings = off`\n\nBut it appears that this setting doesn't work, Spray still logs illegal header warnings.\n. FYI, I was looking for some way to suppress logging warnings for these malformed headers and found it  here: 19a1ff23ccb68b97ef5bfadd7afa013bbb621581\n\nSet `spray.can.parsing.illegal-header-warnings = off`\n\nBut it appears that this setting doesn't work, Spray still logs illegal header warnings.\n. ",
    "argyakrivos": "How about copying the cookies on redirect if it's the same host?\n. How about copying the cookies on redirect if it's the same host?\n. ",
    "fwbrasil": "I amended the commit and signed the Typesafe CLA.\n. @sirthias \"input.length < offset\" is a valid state?\n. I don't like the idea of introduce code to handle a invalid state, since we can't tell much about how the system should behave if its condition is reached. For instance I couldn't find tests for the invalid state, so if we introduce the \"<=\", I think it is also necessary to introduce tests for it. Maybe this is something that shouldn't be on the scope of this PR, that is focused on a performance enhancement.\n. I amended the commit and signed the Typesafe CLA.\n. @sirthias \"input.length < offset\" is a valid state?\n. I don't like the idea of introduce code to handle a invalid state, since we can't tell much about how the system should behave if its condition is reached. For instance I couldn't find tests for the invalid state, so if we introduce the \"<=\", I think it is also necessary to introduce tests for it. Maybe this is something that shouldn't be on the scope of this PR, that is focused on a performance enhancement.\n. ",
    "giabao": "I get this error with spray 1.3 (only with high through puts)\nhttps://gist.github.com/giabao/9453227\n. what is \"separate compilation\"?\n\nI clone spray (branch release/1.3) into my home folder - which is encrypted (I use ubuntu 12.04) and can't compile it.\nWhen I add the above scalac options then I can run `sbt test`, `sbt publish-local` without error.\n. I get this error with spray 1.3 (only with high through puts)\nhttps://gist.github.com/giabao/9453227\n. what is \"separate compilation\"?\n\nI clone spray (branch release/1.3) into my home folder - which is encrypted (I use ubuntu 12.04) and can't compile it.\nWhen I add the above scalac options then I can run `sbt test`, `sbt publish-local` without error.\n. ",
    "caoilte": "Hi,\nI can't get this to work for me in an over the wire spray-client test. I'm seeing the log warning about the fake http header but it's still doing the extra decoding.\n. Arrrgh! No. Apologies! I wasn't decoding again properly at the other end! \n. I've also just come up against this in load testing. I understand how it makes sense in Akka for the timeout to only start once the HTTP request has been made, but from a user point of view it can very quickly cascade to a point where it is impossible for a system to recover.\n\n@ivantopo I'm going to try making a local build with the patch. Should it still work?\n. Hi,\nI can't get this to work for me in an over the wire spray-client test. I'm seeing the log warning about the fake http header but it's still doing the extra decoding.\n. Arrrgh! No. Apologies! I wasn't decoding again properly at the other end! \n. I've also just come up against this in load testing. I understand how it makes sense in Akka for the timeout to only start once the HTTP request has been made, but from a user point of view it can very quickly cascade to a point where it is impossible for a system to recover.\n\n@ivantopo I'm going to try making a local build with the patch. Should it still work?\n. ",
    "markdrago": "Grumble.  I'll take a look.  Perhaps there's something downstream that is re-en/decoding the URL.  I've put the project I was working on that required this change on hold, but I'll try to get this patch cleaned up and working regardless.\n. Mathias - I have accepted the typesafe CLA.  Thanks!\n. I'll be making those changes in the next few hours. Thanks for asking!\nOn Mar 4, 2014 5:43 AM, \"Mathias\" notifications@github.com wrote:\n\n> Would you like us to incorporate the comments from yesterdays's review or\n> do you want to do them?\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/745#issuecomment-36612093\n> .\n. I made the changes that you recommended and rebased the diff on top of master.  Please let me know if there are any additional changes to be made.  Thanks for your help getting this in!\n. Grumble.  I'll take a look.  Perhaps there's something downstream that is re-en/decoding the URL.  I've put the project I was working on that required this change on hold, but I'll try to get this patch cleaned up and working regardless.\n. Mathias - I have accepted the typesafe CLA.  Thanks!\n. I'll be making those changes in the next few hours. Thanks for asking!\nOn Mar 4, 2014 5:43 AM, \"Mathias\" notifications@github.com wrote:\n\n> Would you like us to incorporate the comments from yesterdays's review or\n> do you want to do them?\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/745#issuecomment-36612093\n> .\n. I made the changes that you recommended and rebased the diff on top of master.  Please let me know if there are any additional changes to be made.  Thanks for your help getting this in!\n. ",
    "fernandezpablo85": "Weird, \"it worked on my machine\"\u00ae\n\nSeriously though, on travis it's failing for openjdk7 but not for oracle's jdk\n. Maybe an issue with `Tcp.Connect`, which is not multi-dns? But how come it works on a different jvm? I'm a bit lost here, @jrudolph is there something obvious I'm missing?\n\nPD: The only change on the semantics I can think about is that `InetAddess.getAllByName` fails if it can't resolve the hostname, while the previous implemenations (`InetSocketAddress`) did not.\n. @2beaucoup fixing the backwards compatibility thing. About the `require/assert`, it's not useful in the `Connect` since it's impossible to have an empty array, `InetAddress.getAllByName` either resolves to at least an array of one or throws an exception.\n\nOn a different matter, do you have an idea on what may be making travis crash on openjdk and not oracle's?\n. > Then I'd just remove the msg.\n\nMy bad. It's indeed possible to construct an invalid Connect object by using the default constructor. Will move the require there to avoid that case.\n. Totally missed the fact that this was also an actor, I'm an idiot. Your alternative looks good, will merge it to the rest of the changes\n. @sirthias \n\nSure, besides the commit message and the Array -> List change is there\nanything else that needs tweaking?\n\nOn Mon, Mar 3, 2014 at 11:09 AM, Mathias notifications@github.com wrote:\n\n> Apart from the small comment I think this is a very nice addition, Pablo!\n> Thanks!\n> If @jrudolph https://github.com/jrudolph agrees then I'd like to get\n> this merged very soon.\n> (We have reserved some capacity this week to push for a minor update\n> release.\n> Of course we'd like to incorporate the latest and greatest in\n> contributions! :)\n> \n> @fernandezpablo85 https://github.com/fernandezpablo85 Could you squash\n> all your commits into one bearing this message:\n> - can: add support for multi-host `Connect`\n> \n> (in accordance with out commit message policyhttp://spray.io/project-info/contributing/#git-commit-messages\n> )\n> \n> ## \n> \n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/747#issuecomment-36512693\n> .\n. I'm all in for writing some tests for it, but I'm having some issues (maybe related to my inexperience with akka-test).\n\nWhat's the best way to stub out `InetSocketAddress` to reply with a set of addresses, having some of those result in a connection timeout?\n. (in the meantime, squashed all commits and changed commit message as requested)\n. @jrudolph sure, no problem. I was going to jump in and say that the tests are taking me some time because I've never done any akka testing, at all.\n. @sirthias  I'll resubmit the patch to the akka repository soon.\n. @sirthias I have checked the akka code and has changed substantially since this pull request, it's hard for me to move the patch there. Hopefully someone with more experience in akka internals will translate it properly.\n\nToo bad, it would have been great to collaborate with the akka team :disappointed: , but I guess it will be in another occasion. \n. Weird, \"it worked on my machine\"\u00ae\n\nSeriously though, on travis it's failing for openjdk7 but not for oracle's jdk\n. Maybe an issue with `Tcp.Connect`, which is not multi-dns? But how come it works on a different jvm? I'm a bit lost here, @jrudolph is there something obvious I'm missing?\n\nPD: The only change on the semantics I can think about is that `InetAddess.getAllByName` fails if it can't resolve the hostname, while the previous implemenations (`InetSocketAddress`) did not.\n. @2beaucoup fixing the backwards compatibility thing. About the `require/assert`, it's not useful in the `Connect` since it's impossible to have an empty array, `InetAddress.getAllByName` either resolves to at least an array of one or throws an exception.\n\nOn a different matter, do you have an idea on what may be making travis crash on openjdk and not oracle's?\n. > Then I'd just remove the msg.\n\nMy bad. It's indeed possible to construct an invalid Connect object by using the default constructor. Will move the require there to avoid that case.\n. Totally missed the fact that this was also an actor, I'm an idiot. Your alternative looks good, will merge it to the rest of the changes\n. @sirthias \n\nSure, besides the commit message and the Array -> List change is there\nanything else that needs tweaking?\n\nOn Mon, Mar 3, 2014 at 11:09 AM, Mathias notifications@github.com wrote:\n\n> Apart from the small comment I think this is a very nice addition, Pablo!\n> Thanks!\n> If @jrudolph https://github.com/jrudolph agrees then I'd like to get\n> this merged very soon.\n> (We have reserved some capacity this week to push for a minor update\n> release.\n> Of course we'd like to incorporate the latest and greatest in\n> contributions! :)\n> \n> @fernandezpablo85 https://github.com/fernandezpablo85 Could you squash\n> all your commits into one bearing this message:\n> - can: add support for multi-host `Connect`\n> \n> (in accordance with out commit message policyhttp://spray.io/project-info/contributing/#git-commit-messages\n> )\n> \n> ## \n> \n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/747#issuecomment-36512693\n> .\n. I'm all in for writing some tests for it, but I'm having some issues (maybe related to my inexperience with akka-test).\n\nWhat's the best way to stub out `InetSocketAddress` to reply with a set of addresses, having some of those result in a connection timeout?\n. (in the meantime, squashed all commits and changed commit message as requested)\n. @jrudolph sure, no problem. I was going to jump in and say that the tests are taking me some time because I've never done any akka testing, at all.\n. @sirthias  I'll resubmit the patch to the akka repository soon.\n. @sirthias I have checked the akka code and has changed substantially since this pull request, it's hard for me to move the patch there. Hopefully someone with more experience in akka internals will translate it properly.\n\nToo bad, it would have been great to collaborate with the akka team :disappointed: , but I guess it will be in another occasion. \n. ",
    "dwalend": "Submitting the change gives a 500 error\u2026 And it looks like github is down for the moment. Maybe later tonight or tomorrow.\n\nDave\n\nOn Jan 7, 2014, at 10:18 AM, 2beaucoup wrote:\n\n> You should be able to fix your first spray bug here. ;)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. Seems to have worked today.\n\nDave\n\nOn Jan 7, 2014, at 10:18 AM, 2beaucoup wrote:\n\n> You should be able to fix your first spray bug here. ;)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. I give up. I'm keeping my \"s\" to myself.\n\nOn Jan 16, 2014, at 3:20 AM, Mathias wrote:\n\n> Thanks!\n> Before we can accept your pull request could you:\n> \n>   \u2022 Accept the Typesafe CLA\n>   \u2022 Change the commit message to = docs: fix typo, closes #749 (in accordance with our commit message policy)\n> \u2014\n> Reply to this email directly or view it on GitHub.\n. Any way to get this included in the next version of Spray.io? Pickling may not be the present, but will be the future.\n\nThanks,\n\nDave\n. Sorry about the four edits. Pasting html into markdown took a few tries.\n. Thanks. See you at akka/http .\n. In https://github.com/spray/spray/blob/master/spray-httpx/src/main/scala/spray/httpx/BaseJson4sSupport.scala\n\nShould line 19 use the data field?\n\n``` scala\ntry serialization.read\\[T\\](x.data.asString(defaultCharset = HttpCharsets.`UTF-8`))\n```\n. GoesIn looks fine. (Wish it were that. It was the first thing I looked at, too.)\n\nprintln(write(GoesIn(\"Shows up as ascii codes in the println\")))\n{\"name\":\"Shows up as ascii codes in the println\"}\n\n## \n\nShould I not already have an implicit unmarshaller for JValue from Json4sSupport ?\n. Edit: Looks like you're six minutes ahead of me.\n\nI think you're on to something with your suspicion of the test, though. Json4s looks like it is turning the HttpEntity into json, then filling up the data field with the json string for the HttpEntity. How do I fix things so it won't do that (without peppering the code with json calls)?\n\nTL/DR: I separated out the Put in the test code via\n\n``` scala\n      val put = Put(\"/\", HttpEntity(`application/json`, write(GoesIn(\"Shows up as ascii codes in the println\")) ))\n\n      println(put)\n      println()\n      println(put.entity.data.asString)\n```\n\nand got\n\n```\nHttpRequest(PUT,/,List(),HttpEntity(application/json; charset=UTF-8,{\"contentType\":{\"mediaType\":{}},\"data\":{\"bytes\":[123,34,110,97,109,101,34,58,34,83,104,111,119,115,32,117,112,32,97,115,32,97,115,99,105,105,32,99,111,100,101,115,32,105,110,32,116,104,101,32,112,114,105,110,116,108,110,34,125]}}),HTTP/1.1)\n\n{\"contentType\":{\"mediaType\":{}},\"data\":{\"bytes\":[123,34,110,97,109,101,34,58,34,83,104,111,119,115,32,117,112,32,97,115,32,97,115,99,105,105,32,99,111,100,101,115,32,105,110,32,116,104,101,32,112,114,105,110,116,108,110,34,125]}}\n```\n\nIn the REPL, it looks correct (despite starting with the same imports):\n\n```\nscala> val put = Put(\"/\", HttpEntity(`application/json`, write(GoesIn(\"Shows up as ascii codes in the println\")) ))\nput: spray.http.HttpRequest = HttpRequest(PUT,/,List(),HttpEntity(application/json,{\"name\":\"Shows up as ascii codes in the println\"}),HTTP/1.1)\n```\n\n## \n\nTest code looks like this:\n\n``` scala\npackage com.example\n\nimport org.json4s.native.JsonMethods.parse\nimport org.specs2.mutable.Specification\nimport spray.testkit.Specs2RouteTest\nimport spray.http._\nimport StatusCodes._\nimport spray.http.MediaTypes.`application/json`\nimport spray.http.{ContentType, HttpEntity}\nimport org.json4s.native.Serialization.{read, write}\n\nclass MyServiceSpec extends Specification with Specs2RouteTest with MyService {\n  def actorRefFactory = system\n\n  \"MyService\" should {\n\n    \"return an OK status for PUT of a GoesIn\" in {\n      val put = Put(\"/\", HttpEntity(`application/json`, write(GoesIn(\"Shows up as ascii codes in the println\")) ))\n\n      println(put)\n      println()\n      println(put.entity.data.asString)\n\n      put ~> myRoute ~> check {\n        status == OK\n      }\n    }\n  }\n}\n```\n.  val put = Put(\"/\",  GoesIn(\"Shows up as ascii codes in the println\") )\n\ncreates a correct request in the test. Now I'm getting a \"Request was neither completed nor rejected within 1 second\"\n. Thanks for all the help, Johannes. I've got the test example working now.\n\nIt might be worth updating the entity directive example to not use its own HttpEntity in the test. There's no other bug here.\n\nAnd thanks again for leading me through all this. I wouldn't have suspected a relationship with #990 beyond my own unease with the implicits in scope.\n. I think it was just a matter of changing the entity directive from \n\nentity(as[JsonValue]) { goesIn:GoesIn =>\n\n to \n\nentity(as[GoesIn]) { goesIn:GoesIn =>\n\nIf that doesn't fix it, let me know and I'll dig some more. (It's been a busy three weeks.)\n. And for posterity: \"Request was neither completed nor rejected within 1 second\" is from the default value for the routeTestTimeout.\n\nimplicit val routeTestTimeout = RouteTestTimeout(60.second)\n\nwill give you a minute. (Figured this out while waiting to parse an html response with xerces, completely different part of the system.)\n. Thanks. I found a third one to build from. I don't think it's complete, but we only need it for dev.\n\n``` scala\n//adapted from https://gist.github.com/joseraya/176821d856b43b1cfe19\nobject gruntWatchCorsSupport extends Directive0 with RouteConcatenation {\n\n  import spray.http.HttpHeaders.{`Access-Control-Allow-Methods`, `Access-Control-Max-Age`, `Access-Control-Allow-Headers`,`Access-Control-Allow-Origin`}\n  import spray.routing.directives.RespondWithDirectives.respondWithHeaders\n  import spray.routing.directives.MethodDirectives.options\n  import spray.routing.directives.RouteDirectives.complete\n  import spray.http.HttpMethods.{OPTIONS,GET,POST}\n  import spray.http.AllOrigins\n\n  private val allowOriginHeader = `Access-Control-Allow-Origin`(AllOrigins)\n  private val optionsCorsHeaders = List(\n    `Access-Control-Allow-Headers`(\"Origin, X-Requested-With, Content-Type, Accept, Accept-Encoding, Accept-Language, Host, Referer, User-Agent, Authorization\"),\n    `Access-Control-Max-Age`(1728000)) //20 days\n\n  val gruntWatch:Boolean = ConfigFactory.load().getBoolean(\"shrine.steward.gruntWatch\")\n\n  override def happly(f: (HNil) => Route): Route = {\n    if(gruntWatch) {\n      options {\n        respondWithHeaders(`Access-Control-Allow-Methods`(OPTIONS, GET, POST) ::  allowOriginHeader :: optionsCorsHeaders){\n          complete(StatusCodes.OK)\n        }\n      } ~ f(HNil)\n    }\n    else f(HNil)\n  }\n}\n```\n. Submitting the change gives a 500 error\u2026 And it looks like github is down for the moment. Maybe later tonight or tomorrow.\n\nDave\n\nOn Jan 7, 2014, at 10:18 AM, 2beaucoup wrote:\n\n> You should be able to fix your first spray bug here. ;)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. Seems to have worked today.\n\nDave\n\nOn Jan 7, 2014, at 10:18 AM, 2beaucoup wrote:\n\n> You should be able to fix your first spray bug here. ;)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. I give up. I'm keeping my \"s\" to myself.\n\nOn Jan 16, 2014, at 3:20 AM, Mathias wrote:\n\n> Thanks!\n> Before we can accept your pull request could you:\n> \n>   \u2022 Accept the Typesafe CLA\n>   \u2022 Change the commit message to = docs: fix typo, closes #749 (in accordance with our commit message policy)\n> \u2014\n> Reply to this email directly or view it on GitHub.\n. Any way to get this included in the next version of Spray.io? Pickling may not be the present, but will be the future.\n\nThanks,\n\nDave\n. Sorry about the four edits. Pasting html into markdown took a few tries.\n. Thanks. See you at akka/http .\n. In https://github.com/spray/spray/blob/master/spray-httpx/src/main/scala/spray/httpx/BaseJson4sSupport.scala\n\nShould line 19 use the data field?\n\n``` scala\ntry serialization.read\\[T\\](x.data.asString(defaultCharset = HttpCharsets.`UTF-8`))\n```\n. GoesIn looks fine. (Wish it were that. It was the first thing I looked at, too.)\n\nprintln(write(GoesIn(\"Shows up as ascii codes in the println\")))\n{\"name\":\"Shows up as ascii codes in the println\"}\n\n## \n\nShould I not already have an implicit unmarshaller for JValue from Json4sSupport ?\n. Edit: Looks like you're six minutes ahead of me.\n\nI think you're on to something with your suspicion of the test, though. Json4s looks like it is turning the HttpEntity into json, then filling up the data field with the json string for the HttpEntity. How do I fix things so it won't do that (without peppering the code with json calls)?\n\nTL/DR: I separated out the Put in the test code via\n\n``` scala\n      val put = Put(\"/\", HttpEntity(`application/json`, write(GoesIn(\"Shows up as ascii codes in the println\")) ))\n\n      println(put)\n      println()\n      println(put.entity.data.asString)\n```\n\nand got\n\n```\nHttpRequest(PUT,/,List(),HttpEntity(application/json; charset=UTF-8,{\"contentType\":{\"mediaType\":{}},\"data\":{\"bytes\":[123,34,110,97,109,101,34,58,34,83,104,111,119,115,32,117,112,32,97,115,32,97,115,99,105,105,32,99,111,100,101,115,32,105,110,32,116,104,101,32,112,114,105,110,116,108,110,34,125]}}),HTTP/1.1)\n\n{\"contentType\":{\"mediaType\":{}},\"data\":{\"bytes\":[123,34,110,97,109,101,34,58,34,83,104,111,119,115,32,117,112,32,97,115,32,97,115,99,105,105,32,99,111,100,101,115,32,105,110,32,116,104,101,32,112,114,105,110,116,108,110,34,125]}}\n```\n\nIn the REPL, it looks correct (despite starting with the same imports):\n\n```\nscala> val put = Put(\"/\", HttpEntity(`application/json`, write(GoesIn(\"Shows up as ascii codes in the println\")) ))\nput: spray.http.HttpRequest = HttpRequest(PUT,/,List(),HttpEntity(application/json,{\"name\":\"Shows up as ascii codes in the println\"}),HTTP/1.1)\n```\n\n## \n\nTest code looks like this:\n\n``` scala\npackage com.example\n\nimport org.json4s.native.JsonMethods.parse\nimport org.specs2.mutable.Specification\nimport spray.testkit.Specs2RouteTest\nimport spray.http._\nimport StatusCodes._\nimport spray.http.MediaTypes.`application/json`\nimport spray.http.{ContentType, HttpEntity}\nimport org.json4s.native.Serialization.{read, write}\n\nclass MyServiceSpec extends Specification with Specs2RouteTest with MyService {\n  def actorRefFactory = system\n\n  \"MyService\" should {\n\n    \"return an OK status for PUT of a GoesIn\" in {\n      val put = Put(\"/\", HttpEntity(`application/json`, write(GoesIn(\"Shows up as ascii codes in the println\")) ))\n\n      println(put)\n      println()\n      println(put.entity.data.asString)\n\n      put ~> myRoute ~> check {\n        status == OK\n      }\n    }\n  }\n}\n```\n.  val put = Put(\"/\",  GoesIn(\"Shows up as ascii codes in the println\") )\n\ncreates a correct request in the test. Now I'm getting a \"Request was neither completed nor rejected within 1 second\"\n. Thanks for all the help, Johannes. I've got the test example working now.\n\nIt might be worth updating the entity directive example to not use its own HttpEntity in the test. There's no other bug here.\n\nAnd thanks again for leading me through all this. I wouldn't have suspected a relationship with #990 beyond my own unease with the implicits in scope.\n. I think it was just a matter of changing the entity directive from \n\nentity(as[JsonValue]) { goesIn:GoesIn =>\n\n to \n\nentity(as[GoesIn]) { goesIn:GoesIn =>\n\nIf that doesn't fix it, let me know and I'll dig some more. (It's been a busy three weeks.)\n. And for posterity: \"Request was neither completed nor rejected within 1 second\" is from the default value for the routeTestTimeout.\n\nimplicit val routeTestTimeout = RouteTestTimeout(60.second)\n\nwill give you a minute. (Figured this out while waiting to parse an html response with xerces, completely different part of the system.)\n. Thanks. I found a third one to build from. I don't think it's complete, but we only need it for dev.\n\n``` scala\n//adapted from https://gist.github.com/joseraya/176821d856b43b1cfe19\nobject gruntWatchCorsSupport extends Directive0 with RouteConcatenation {\n\n  import spray.http.HttpHeaders.{`Access-Control-Allow-Methods`, `Access-Control-Max-Age`, `Access-Control-Allow-Headers`,`Access-Control-Allow-Origin`}\n  import spray.routing.directives.RespondWithDirectives.respondWithHeaders\n  import spray.routing.directives.MethodDirectives.options\n  import spray.routing.directives.RouteDirectives.complete\n  import spray.http.HttpMethods.{OPTIONS,GET,POST}\n  import spray.http.AllOrigins\n\n  private val allowOriginHeader = `Access-Control-Allow-Origin`(AllOrigins)\n  private val optionsCorsHeaders = List(\n    `Access-Control-Allow-Headers`(\"Origin, X-Requested-With, Content-Type, Accept, Accept-Encoding, Accept-Language, Host, Referer, User-Agent, Authorization\"),\n    `Access-Control-Max-Age`(1728000)) //20 days\n\n  val gruntWatch:Boolean = ConfigFactory.load().getBoolean(\"shrine.steward.gruntWatch\")\n\n  override def happly(f: (HNil) => Route): Route = {\n    if(gruntWatch) {\n      options {\n        respondWithHeaders(`Access-Control-Allow-Methods`(OPTIONS, GET, POST) ::  allowOriginHeader :: optionsCorsHeaders){\n          complete(StatusCodes.OK)\n        }\n      } ~ f(HNil)\n    }\n    else f(HNil)\n  }\n}\n```\n. ",
    "ryandbair": "I'm looking into this, came up with a first draft, and am currently trying to introduce a test to validate the behavior. Does anyone know a way to close the TCP connection without terminating the TLS session first? \n. I created a patch which fixes the issue, but _may_ introduce a vector for truncation attacks. I was only able to test it against a \"bad\" server implementation as I could not figure out how to create a test case that replicated the behavior. \n\nAs for the truncation attack, RFC 2818 which you quoted, states that the HTTP layer should be the one to determine if truncation of a message happened or not. I did not verify if spray does this on its own. If it does, I believe this patch is safe and introduces no truncation attack vulnerability. \n\nhttps://gist.github.com/ryandbair/c17fc97c3bbc7a59b4a9\n. I'm looking into this, came up with a first draft, and am currently trying to introduce a test to validate the behavior. Does anyone know a way to close the TCP connection without terminating the TLS session first? \n. I created a patch which fixes the issue, but _may_ introduce a vector for truncation attacks. I was only able to test it against a \"bad\" server implementation as I could not figure out how to create a test case that replicated the behavior. \n\nAs for the truncation attack, RFC 2818 which you quoted, states that the HTTP layer should be the one to determine if truncation of a message happened or not. I did not verify if spray does this on its own. If it does, I believe this patch is safe and introduces no truncation attack vulnerability. \n\nhttps://gist.github.com/ryandbair/c17fc97c3bbc7a59b4a9\n. ",
    "tootedom": "Hi there,\n\nNo problem, I've done both of those now.\n\ncheers\n/dom\n\nOn Tue, Jan 14, 2014 at 9:22 AM, Mathias notifications@github.com wrote:\n\n> Ok, thanks for this patch!\n> Can you do the two following things for us to be able to merge your patch?\n> 1. Sign the Typesafe CLA http://www.typesafe.com/contribute/cla\n> 2. Change the commit message to = examples: fix incorrect dtd reference\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/759#issuecomment-32249406\n> .\n. Sorry... done now... didn't relise you want the '=' in the commit message.\nLet me know if that's ok or not.\n\nApologies,\n\ncheers\n/dom\n\nOn Tue, Jan 14, 2014 at 9:34 AM, Dominic Tootell\ndominic.tootell@gmail.comwrote:\n\n> Hi there,\n> \n> No problem, I've done both of those now.\n> \n> cheers\n> /dom\n> \n> On Tue, Jan 14, 2014 at 9:22 AM, Mathias notifications@github.com wrote:\n> \n> > Ok, thanks for this patch!\n> > Can you do the two following things for us to be able to merge your patch?\n> > 1. Sign the Typesafe CLA http://www.typesafe.com/contribute/cla\n> > 2. Change the commit message to = examples: fix incorrect dtd\n> >    reference\n> > \n> > \u2014\n> > Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/759#issuecomment-32249406\n> > .\n. Ah, massive apologies.\nIn that case maybe this it isn't a bug, and it's really me just assuming it's a bug (massive apologies).  As far as I can tell (looking at scala 2.10.3 source) promise.future is still returning the promise itself, and therefore `promise` and `promise.future` are the same thing.   I never looked under the hood to see the return values; I just took it on the face value, where `promise` and `promise.future` look different (apologies).\n\nIn this case up to you on whether to replace `promise` with `promise.future`.  As far as I can see looking in to the underlying scala promise code, it's not a bug.\n\ncheers\n/dom\n. In that case, \n\nI've added a test case to the commit, that checks the exception path.  The test is the same as that performed in the ExpiringLruCacheSpec; for the same path.\n\ncheers\n/dom\n. Hi there,\n\nNo problem, I've done both of those now.\n\ncheers\n/dom\n\nOn Tue, Jan 14, 2014 at 9:22 AM, Mathias notifications@github.com wrote:\n\n> Ok, thanks for this patch!\n> Can you do the two following things for us to be able to merge your patch?\n> 1. Sign the Typesafe CLA http://www.typesafe.com/contribute/cla\n> 2. Change the commit message to = examples: fix incorrect dtd reference\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/759#issuecomment-32249406\n> .\n. Sorry... done now... didn't relise you want the '=' in the commit message.\nLet me know if that's ok or not.\n\nApologies,\n\ncheers\n/dom\n\nOn Tue, Jan 14, 2014 at 9:34 AM, Dominic Tootell\ndominic.tootell@gmail.comwrote:\n\n> Hi there,\n> \n> No problem, I've done both of those now.\n> \n> cheers\n> /dom\n> \n> On Tue, Jan 14, 2014 at 9:22 AM, Mathias notifications@github.com wrote:\n> \n> > Ok, thanks for this patch!\n> > Can you do the two following things for us to be able to merge your patch?\n> > 1. Sign the Typesafe CLA http://www.typesafe.com/contribute/cla\n> > 2. Change the commit message to = examples: fix incorrect dtd\n> >    reference\n> > \n> > \u2014\n> > Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/pull/759#issuecomment-32249406\n> > .\n. Ah, massive apologies.\nIn that case maybe this it isn't a bug, and it's really me just assuming it's a bug (massive apologies).  As far as I can tell (looking at scala 2.10.3 source) promise.future is still returning the promise itself, and therefore `promise` and `promise.future` are the same thing.   I never looked under the hood to see the return values; I just took it on the face value, where `promise` and `promise.future` look different (apologies).\n\nIn this case up to you on whether to replace `promise` with `promise.future`.  As far as I can see looking in to the underlying scala promise code, it's not a bug.\n\ncheers\n/dom\n. In that case, \n\nI've added a test case to the commit, that checks the exception path.  The test is the same as that performed in the ExpiringLruCacheSpec; for the same path.\n\ncheers\n/dom\n. ",
    "hamiltont": "+1. I'm not a spray expert by any means, but here's my input on why I'd like to see this included (even as-is). \n\nI can definitely understand part of your security and maintenance concerns, but as a raw beginner to spray and scala, I can't easily add this myself in a few lines - it's an hour or two of reading to figure out a simple proxy. Nothing against your codebase - I'm absolutely loving spray - but I'm just not good enough yet to whip up the 10-15 lines for a reverse proxy quickly. \n\nAs spray is predominantly API focused at the moment, the main use case I see here is a simple reverse proxy to the web interface if the user hits the root (e.g. `localhost:8080` reverse proxies to `localhost:9090`, but `localhost:8080/my/api/path` works as expected). I've done this on my own setup with a redirect, but it's frustrating to have to constantly change the port back to 8080 when I want to switch between looking at the website vs testing the API\n\nI know this PR isn't all that's desired when saying \"proxy\", but I would vote to see it included as something like \"basicProxy\" (e.g. named to highlight that it's not a full-fledged proxy and you should read the docs). For the security concerns, this basicProxy _could_ only allow though whitelisted request/response objects, such as only contains X headers (or blacklist, whatever makes sense). As an end user, I'd like to have this functionality now as it seems available, instead of having to wait for when someone has time to create+test a complete proxy (yes, I could compile it myself. I might if this ever becomes a pressing need, but presumably if I can compile a new spray library then I can also write those 10-15 lines mentioned earlier ;-) )\n\nHope you're open to unsolicited feedback, and thanks for the great tool!!\n. Thanks @sirthias ! For what it's worth to future readers, I got a (little) bit better at spray and solved my need for a simple reverse proxy like this. Input welcome if anyone spots major problems\n\n```\n          requestUri { uri =>\n            implicit val system = context.system\n            implicit val timeout: akka.util.Timeout = this.timeout\n            val dashUri = uri.withPort(9090)\n            val auth = BasicHttpCredentials(\"user\", \"pass\")\n            val outbound = HttpRequest(GET, dashUri, List(Authorization(auth)))\n            val response = IO(UHttp).ask(outbound).mapTo[HttpResponse]\n            complete(response)\n          }\n```\n. +1. I'm not a spray expert by any means, but here's my input on why I'd like to see this included (even as-is). \n\nI can definitely understand part of your security and maintenance concerns, but as a raw beginner to spray and scala, I can't easily add this myself in a few lines - it's an hour or two of reading to figure out a simple proxy. Nothing against your codebase - I'm absolutely loving spray - but I'm just not good enough yet to whip up the 10-15 lines for a reverse proxy quickly. \n\nAs spray is predominantly API focused at the moment, the main use case I see here is a simple reverse proxy to the web interface if the user hits the root (e.g. `localhost:8080` reverse proxies to `localhost:9090`, but `localhost:8080/my/api/path` works as expected). I've done this on my own setup with a redirect, but it's frustrating to have to constantly change the port back to 8080 when I want to switch between looking at the website vs testing the API\n\nI know this PR isn't all that's desired when saying \"proxy\", but I would vote to see it included as something like \"basicProxy\" (e.g. named to highlight that it's not a full-fledged proxy and you should read the docs). For the security concerns, this basicProxy _could_ only allow though whitelisted request/response objects, such as only contains X headers (or blacklist, whatever makes sense). As an end user, I'd like to have this functionality now as it seems available, instead of having to wait for when someone has time to create+test a complete proxy (yes, I could compile it myself. I might if this ever becomes a pressing need, but presumably if I can compile a new spray library then I can also write those 10-15 lines mentioned earlier ;-) )\n\nHope you're open to unsolicited feedback, and thanks for the great tool!!\n. Thanks @sirthias ! For what it's worth to future readers, I got a (little) bit better at spray and solved my need for a simple reverse proxy like this. Input welcome if anyone spots major problems\n\n```\n          requestUri { uri =>\n            implicit val system = context.system\n            implicit val timeout: akka.util.Timeout = this.timeout\n            val dashUri = uri.withPort(9090)\n            val auth = BasicHttpCredentials(\"user\", \"pass\")\n            val outbound = HttpRequest(GET, dashUri, List(Authorization(auth)))\n            val response = IO(UHttp).ask(outbound).mapTo[HttpResponse]\n            complete(response)\n          }\n```\n. ",
    "thiloplanz": "When using this code, I am getting warnings about some HTTP response headers received from the backend being ignored.\n\n```\n[WARN]  Explicitly set response header 'Server: spray-can/1.3.3' is ignored, the spray-can HTTP layer sets this header automatically!\n[WARN]  Explicitly set response header 'Date: Thu, 18 Feb 2016 01:40:47 GMT' is ignored, the spray-can HTTP layer sets this header automatically!\n[WARN]  Explicitly set response header 'Content-Type: application/json; charset=UTF-8' is ignored, the response Content-Type is set via the response's HttpEntity!\n[WARN]  Explicitly set response header 'Content-Length: 111' is ignored, another `Content-Length` header was already rendered\n```\n\nWhat do I do about that? \n. When using this code, I am getting warnings about some HTTP response headers received from the backend being ignored.\n\n```\n[WARN]  Explicitly set response header 'Server: spray-can/1.3.3' is ignored, the spray-can HTTP layer sets this header automatically!\n[WARN]  Explicitly set response header 'Date: Thu, 18 Feb 2016 01:40:47 GMT' is ignored, the spray-can HTTP layer sets this header automatically!\n[WARN]  Explicitly set response header 'Content-Type: application/json; charset=UTF-8' is ignored, the response Content-Type is set via the response's HttpEntity!\n[WARN]  Explicitly set response header 'Content-Length: 111' is ignored, another `Content-Length` header was already rendered\n```\n\nWhat do I do about that? \n. ",
    "tsandall": "I've run into the same problem with a project I'm working on. I would really like to see this dealt with in some way. \n\nIt would be nice if Spray could at least avoid sending requests if the ask timeout has expired and the future has fired. In this case, it doesn't seem like there's a way for the caller to be notified, so sending any requests after this point is a waste. I view deadline evaluation prior to retries as an optimization, i.e., it wouldn't bother me as much if requests were made AND THEN the ask future timed out, resulting in a bit of waste.\n. I've run into the same problem with a project I'm working on. I would really like to see this dealt with in some way. \n\nIt would be nice if Spray could at least avoid sending requests if the ask timeout has expired and the future has fired. In this case, it doesn't seem like there's a way for the caller to be notified, so sending any requests after this point is a waste. I view deadline evaluation prior to retries as an optimization, i.e., it wouldn't bother me as much if requests were made AND THEN the ask future timed out, resulting in a bit of waste.\n. ",
    "zhijun": "Sorry for creating this issue without comprehensive understanding of Spray. But I know It's really a great framework and I don't want to turn to any other solution for REST handling. Thank you so much for guidance. \n. Sorry for creating this issue without comprehensive understanding of Spray. But I know It's really a great framework and I don't want to turn to any other solution for REST handling. Thank you so much for guidance. \n. ",
    "tomer-ben-david": "asking this questions in google groups first so closing on meanwhile..\n. asking this questions in google groups first so closing on meanwhile..\n. ",
    "mattroberts297": "I just bumped into this. I dubbed it 'inconsistent directive evaluation behaviour' and was about to raise a bug, but found this improvement. Here's an example of the problem and how I worked around it:\n\n``` scala\n  def route = path(\"foo\") {\n    get {\n      println(\"In foo\")                     // Run on start-up, not run on request!\n      complete {\n        BodylessHttpResponse(OK)\n      }\n    }\n  } ~ path(\"bar\") {\n    get {\n      complete {\n        println(\"In bar\")                   // Run on request, not run at start up.\n        BodylessHttpResponse(OK)\n      }\n    }\n  } ~ path(\"foo\" / Rest) { rest =>\n    get {\n      complete {\n        println(s\"In foo $rest\")            // Run on request, not run at start up.\n        BodylessHttpResponse(OK)\n      }\n    }\n  } ~ path(\"bar\" / Rest) { rest =>\n    get {\n      println(s\"In bar $rest\")              // Run on request, not run at start up.\n      complete(BodylessHttpResponse(OK))\n    }\n  }\n```\n\nExample interactions:\n\n``` bash\ncurl http://localhost:8080/foo       # Nothing printed\ncurl http://localhost:8080/bar       # In bar printed\ncurl http://localhost:8080/foo/bar   # In foo bar printed\ncurl http://localhost:8080/bar/foo   # In bar foo printed\n```\n. I just bumped into this. I dubbed it 'inconsistent directive evaluation behaviour' and was about to raise a bug, but found this improvement. Here's an example of the problem and how I worked around it:\n\n``` scala\n  def route = path(\"foo\") {\n    get {\n      println(\"In foo\")                     // Run on start-up, not run on request!\n      complete {\n        BodylessHttpResponse(OK)\n      }\n    }\n  } ~ path(\"bar\") {\n    get {\n      complete {\n        println(\"In bar\")                   // Run on request, not run at start up.\n        BodylessHttpResponse(OK)\n      }\n    }\n  } ~ path(\"foo\" / Rest) { rest =>\n    get {\n      complete {\n        println(s\"In foo $rest\")            // Run on request, not run at start up.\n        BodylessHttpResponse(OK)\n      }\n    }\n  } ~ path(\"bar\" / Rest) { rest =>\n    get {\n      println(s\"In bar $rest\")              // Run on request, not run at start up.\n      complete(BodylessHttpResponse(OK))\n    }\n  }\n```\n\nExample interactions:\n\n``` bash\ncurl http://localhost:8080/foo       # Nothing printed\ncurl http://localhost:8080/bar       # In bar printed\ncurl http://localhost:8080/foo/bar   # In foo bar printed\ncurl http://localhost:8080/bar/foo   # In bar foo printed\n```\n. ",
    "jonathancrosmer": "This page:\nhttp://spray.io/introduction/what-is-spray/  \nstill has the incorrect statement about spray-http being independent of akka.\n. This page:\nhttp://spray.io/introduction/what-is-spray/  \nstill has the incorrect statement about spray-http being independent of akka.\n. ",
    "TulioDomingos": "Done!\n. @jrudolph Yep, it works! The following errors don't show up in Tomcat anymore\n\nSEVERE: The web application [/spray-template_2.10-0.1] appears to have started a thread named [example-scheduler-1] but has failed to stop it. This is very likely to create a memory leak.\n\nSEVERE: The web application [/spray-template_2.10-0.1] appears to have started a thread named [example-akka.actor.default-dispatcher-2] but has failed to stop it. This is very likely to create a memory leak.\n. You're welcome. Cheers!\n. Done!\n. @jrudolph Yep, it works! The following errors don't show up in Tomcat anymore\n\nSEVERE: The web application [/spray-template_2.10-0.1] appears to have started a thread named [example-scheduler-1] but has failed to stop it. This is very likely to create a memory leak.\n\nSEVERE: The web application [/spray-template_2.10-0.1] appears to have started a thread named [example-akka.actor.default-dispatcher-2] but has failed to stop it. This is very likely to create a memory leak.\n. You're welcome. Cheers!\n. ",
    "jbirch": "This can be worked around with HttpResponse.\n. After updating my toy application to use Akka 2.3.0-RC4, Spray 1.3-RC4, and Scala 2.11.0-RC1, this issue was fixed. Looks like I had just the wrong mix of bleeding edge happening :)\n\nIt's important to note I had to add `\"org.scala-lang.modules\" %% \"scala-xml\" % \"1.0.0\"` to my library dependencies -- BasicMarshallers' NodeSeqMarshaller makes use of scala.xml which is no longer present in the base scala distribution and has to be pulled in with scala-xml. This means that in order to return basic String responses, we necessarily must bring in an xml library, but I suppose that's a ticket for another day.\n. This can be worked around with HttpResponse.\n. After updating my toy application to use Akka 2.3.0-RC4, Spray 1.3-RC4, and Scala 2.11.0-RC1, this issue was fixed. Looks like I had just the wrong mix of bleeding edge happening :)\n\nIt's important to note I had to add `\"org.scala-lang.modules\" %% \"scala-xml\" % \"1.0.0\"` to my library dependencies -- BasicMarshallers' NodeSeqMarshaller makes use of scala.xml which is no longer present in the base scala distribution and has to be pulled in with scala-xml. This means that in order to return basic String responses, we necessarily must bring in an xml library, but I suppose that's a ticket for another day.\n. ",
    "choffmeister": "Closed. Open pull request instead.\n. PR #816 has been merged.\n. OK. Since the milliseconds are discarded anyway, I guess this should be easy. Will update my PR tonight.\n. Updated the PR and sent the signed CLA to info (at) spray (dot) io\n. Your version is more concise, but exactly what I supposed in my initial post :) Might it even be possible, to merge synchronous and asynchronous versions for `authorize` under one common name with the magnet pattern?\n\nOn your argument for removing the asynchronous `authorize` overload: It depends on what authentication means: If authentication is only the collecting of the user credentials, then it does not need to allow asynchronous behaviour. But in my understanding, authentication also means to validate the credentials and extract an object representing the authentication state (for example the user that has been authenticated). In this context the `authenticate` directive has to allow asynchronous behaviour.\n. We could change the meaning of `authenticate` to not reject anymore (at least not, because of credentials missing), but in instead extract an `Option[T]`. So it would change so\n\n``` scala\n// old\ndef authenticate[T](magnet: AuthMagnet[T]): Directive1[T] = magnet.directive\n// new\ndef authenticate[T](magnet: AuthMagnet[T]): Directive1[Option[T]] = magnet.directive\n```\n\nwhere an extraction of `None` means an anonymous unauthenticated user. The actual rejection could then take place in the `authorize` directives.\n\nThis would make a clean seperation of concern: `authenticate` only extracts who's there and `authorize` decides, if the authenticated user (or the anonymous user) is allowed to access a given resource.\n\nAlso I think it would be good, to adopt `authorize` to to always take an `Option[T]` for the user.\n\n``` scala\n// old\ndef authorize(check: RequestContext \u21d2 Boolean): Directive0\n// new\ndef authorize[T](check: (RequestContext, Option[T]) \u21d2 Boolean): Directive0\n```\n\nOr go one step futher and make the `authenticate` directive attach the user context to the request context to avoid the need for passing the user object around.\n. Of course anonymous authentication is already possible. But I think it should be the default. Suppose we have a `UserPassAuthenticator[U]`. Then `authenticate` should:\n- extract `Option.empty[U]` if credentials are missing\n- reject if credentials are given but invalid\n- extract `Some[U](\u2026)` if credentials are given and valid\n\nAnd `authorize` should get the extracted user. This way we could clearly separate the tasks of `authenticate` and `authorize`:\n- `authenticate` extracts the authenticated user (which might be anonymous `None`) and only rejects if credentials are invalid\n- `authorize` rejects if permissions are insufficient\n. Authorization indeed depends on authentication. For example look [here](http://www.cyberciti.biz/faq/authentication-vs-authorization/) (there are many other places where one can look up the difference and connection of authentication and authorization).\n\nThe term _anonymous_ is replaceable. We can just call it _unauthenticated_ or something else. The key point is, that authorization is not the place to verify _who is there_. The _who_ is verified by authentication. What one is allowed to do is determined by authorization (which can incorporate the _who_ given by previous authentication in it\u2019s decision).\n\nIt seems kind of inconsistent to handle the case of not being authenticated in a special way. If one is not authenticated, then there is a (application specific) neutral state involved. Why not unify things and just make this state a special user, the _anonymous_ user? If a web service should be private only, then the `authorize` directive just has to block all access for the _anonymous_ user.\n\nIf `Option[T]` is the right way to represent this is another question\u2026 But I think that its semantics match the situation. One is either _unauthenticated_ or authenticated as a certain principle. \n. I am not saying that it is not possible at the moment. I am saying that the semantics of `authenticate` and `authorize` should be separated more clearly...\n. The combination of `def authenticate[T](auth: ContextAuthenticator[T])` together with [HttpAuthenticator](https://github.com/spray/spray/blob/master/spray-routing/src/main/scala/spray/routing/authentication/HttpAuthenticator.scala#L39) rejects the request if no credentials are passed in. And rejecting because of insufficient permission is task of the authorization, not the authentication.\n. Well at least I see the problem with my approach. Supposing that `authenticate` yields `None` than `authorize` cannot request new authentication from the client (else `authorize` must know what `ContextAuthenticator` was used by `authenticate`)...\n. Feels like HTTP Basic is \"login or die trying\"...\n. No, but that seems to be pretty much what I wanted. Although I think my implementation is \"better\" since it leverages the compositional nature of directives and dependes on `authenticate`. Hence changes on `authenticate` would immediately reflect in the optional authentication directive.\n. Great. Looking forward to the first akka-http release!\n. Thx for reflecting my sugggestion in akka-http. Guess this issue can be considered solved.\n. Closed. Open pull request instead.\n. PR #816 has been merged.\n. OK. Since the milliseconds are discarded anyway, I guess this should be easy. Will update my PR tonight.\n. Updated the PR and sent the signed CLA to info (at) spray (dot) io\n. Your version is more concise, but exactly what I supposed in my initial post :) Might it even be possible, to merge synchronous and asynchronous versions for `authorize` under one common name with the magnet pattern?\n\nOn your argument for removing the asynchronous `authorize` overload: It depends on what authentication means: If authentication is only the collecting of the user credentials, then it does not need to allow asynchronous behaviour. But in my understanding, authentication also means to validate the credentials and extract an object representing the authentication state (for example the user that has been authenticated). In this context the `authenticate` directive has to allow asynchronous behaviour.\n. We could change the meaning of `authenticate` to not reject anymore (at least not, because of credentials missing), but in instead extract an `Option[T]`. So it would change so\n\n``` scala\n// old\ndef authenticate[T](magnet: AuthMagnet[T]): Directive1[T] = magnet.directive\n// new\ndef authenticate[T](magnet: AuthMagnet[T]): Directive1[Option[T]] = magnet.directive\n```\n\nwhere an extraction of `None` means an anonymous unauthenticated user. The actual rejection could then take place in the `authorize` directives.\n\nThis would make a clean seperation of concern: `authenticate` only extracts who's there and `authorize` decides, if the authenticated user (or the anonymous user) is allowed to access a given resource.\n\nAlso I think it would be good, to adopt `authorize` to to always take an `Option[T]` for the user.\n\n``` scala\n// old\ndef authorize(check: RequestContext \u21d2 Boolean): Directive0\n// new\ndef authorize[T](check: (RequestContext, Option[T]) \u21d2 Boolean): Directive0\n```\n\nOr go one step futher and make the `authenticate` directive attach the user context to the request context to avoid the need for passing the user object around.\n. Of course anonymous authentication is already possible. But I think it should be the default. Suppose we have a `UserPassAuthenticator[U]`. Then `authenticate` should:\n- extract `Option.empty[U]` if credentials are missing\n- reject if credentials are given but invalid\n- extract `Some[U](\u2026)` if credentials are given and valid\n\nAnd `authorize` should get the extracted user. This way we could clearly separate the tasks of `authenticate` and `authorize`:\n- `authenticate` extracts the authenticated user (which might be anonymous `None`) and only rejects if credentials are invalid\n- `authorize` rejects if permissions are insufficient\n. Authorization indeed depends on authentication. For example look [here](http://www.cyberciti.biz/faq/authentication-vs-authorization/) (there are many other places where one can look up the difference and connection of authentication and authorization).\n\nThe term _anonymous_ is replaceable. We can just call it _unauthenticated_ or something else. The key point is, that authorization is not the place to verify _who is there_. The _who_ is verified by authentication. What one is allowed to do is determined by authorization (which can incorporate the _who_ given by previous authentication in it\u2019s decision).\n\nIt seems kind of inconsistent to handle the case of not being authenticated in a special way. If one is not authenticated, then there is a (application specific) neutral state involved. Why not unify things and just make this state a special user, the _anonymous_ user? If a web service should be private only, then the `authorize` directive just has to block all access for the _anonymous_ user.\n\nIf `Option[T]` is the right way to represent this is another question\u2026 But I think that its semantics match the situation. One is either _unauthenticated_ or authenticated as a certain principle. \n. I am not saying that it is not possible at the moment. I am saying that the semantics of `authenticate` and `authorize` should be separated more clearly...\n. The combination of `def authenticate[T](auth: ContextAuthenticator[T])` together with [HttpAuthenticator](https://github.com/spray/spray/blob/master/spray-routing/src/main/scala/spray/routing/authentication/HttpAuthenticator.scala#L39) rejects the request if no credentials are passed in. And rejecting because of insufficient permission is task of the authorization, not the authentication.\n. Well at least I see the problem with my approach. Supposing that `authenticate` yields `None` than `authorize` cannot request new authentication from the client (else `authorize` must know what `ContextAuthenticator` was used by `authenticate`)...\n. Feels like HTTP Basic is \"login or die trying\"...\n. No, but that seems to be pretty much what I wanted. Although I think my implementation is \"better\" since it leverages the compositional nature of directives and dependes on `authenticate`. Hence changes on `authenticate` would immediately reflect in the optional authentication directive.\n. Great. Looking forward to the first akka-http release!\n. Thx for reflecting my sugggestion in akka-http. Guess this issue can be considered solved.\n. ",
    "cowboy129": "We want to send multiple frames out of the pipeline (from serverConn), please check spec code.\nACK is send back from akka.io.TcpConnection actor which has a buffer.\nActually we have passed Autobahn test cases, which is a websocket testsuite, with or without SSL.\nPlease check https://github.com/wandoulabs/spray-websocket\nthanks.\n\n@dcaoyuan \n. @jrudolph could you please help review this code?\n. @sirthias Could you please help us to compile and publish 1.3.2-20140428 with Scala 2.11 ?\n. We want to send multiple frames out of the pipeline (from serverConn), please check spec code.\nACK is send back from akka.io.TcpConnection actor which has a buffer.\nActually we have passed Autobahn test cases, which is a websocket testsuite, with or without SSL.\nPlease check https://github.com/wandoulabs/spray-websocket\nthanks.\n\n@dcaoyuan \n. @jrudolph could you please help review this code?\n. @sirthias Could you please help us to compile and publish 1.3.2-20140428 with Scala 2.11 ?\n. ",
    "dcaoyuan": "For HTTP, we can almost be sure that one Tcp.Received will be followed by a Tcp.Write (request-response pair). But in other cases, for instance, WebSocket, we can not guarantee this req-resp pair, instead,  data my be sent to server's socket with multiple frames without waiting for a response.\n\nPer @jrudolph 's concern, we'll try to add a stage under SSL stage, the new added stage will try to control the data flow sending to SSL, i.e. buffer the Tcp.Received, send one to SSL, waiting for ACK, then send next.\n. @sirthias \nI'd like to pass the spec test added in this patch:\n\n```\n+  \"properly handle write multiple frames at the same time\" in new TestSetup {\n +    val server = new SpraySslServer\n +    val connAttempt = attemptSpraySslClientConnection(server.address)\n +    val serverConn = server.acceptOne()\n +    val clientConn = connAttempt.finishConnect()\n +\n +    clientConn.writeLn(\"Foo\")\n +    serverConn.expectReceivedString(\"Foo\\n\")\n +\n +    serverConn.writeLn(\"bar1\")\n +    serverConn.writeLn(\"bar2\")\n +    serverConn.writeLn(\"bar3\")\n +    clientConn.expectReceivedString(\"bar1\\n\")\n +    clientConn.expectReceivedString(\"bar2\\n\")\n +    clientConn.expectReceivedString(\"bar3\\n\")\n +\n +    clientConn.command(Tcp.Close)\n +    serverConn.events.expectMsg(Tcp.PeerClosed)\n +    TestUtils.verifyActorTermination(serverConn.handler)\n +    clientConn.events.expectMsg(Tcp.Closed)\n +    TestUtils.verifyActorTermination(clientConn.handler)\n +    server.close()\n +  }\n```\n\nAny suggestion, or the above  test  should just not work :-)\n. We tried write with ack on stage above SSL, but, it's much complicated. The PumpAction of encrypt/decrypt may form a call-chain (encrypt -> decypt) when first Write arrives (but Tcp.Received(data) are still fired from under-layer, BTW, you don't know when will it be over.), and causes the state of SSL stage unreliable. We may write a simple code to show this later.\n. @jrudolph @sirthias \n\nTo get spray-websocket working by self-patched, it would be appreciated that the spray-can applies patches that are spec related , i.e. these two files:\n\n`spray-can/src/main/scala/spray/can/rendering/RenderSupport.scala`\n`spray-http/src/main/scala/spray/http/HttpHeader.scala`\n\nThen, we can include a patched `spray-can/src/main/scala/spray/can/rendering/ResponseRenderingComponent.scala` as  `spray-can/src/main/scala/spray/can/rendering/ResponseRenderingComponentPatched.scala` in spray-websocket only, as a temporary solution.\n\nBTW, the 'upgrade' spec which spray-websocket implemented is a generic upgradable supporting, which is not sticking on websocket protocol only. Please take a look at:\n\nhttps://github.com/wandoulabs/spray-websocket/blob/master/spray-websocket/src/main/scala/spray/can/server/UpgradeSupport.scala\n\nhttps://github.com/wandoulabs/spray-websocket/blob/master/spray-websocket/src/main/scala/spray/can/server/UpgradableHttpListener.scala\n\nhttps://github.com/wandoulabs/spray-websocket/blob/master/spray-websocket/src/main/scala/spray/can/client/UpgradableHttpClientSettingsGroup.scala\n. @jrudolph Every spray based WebSocket implementation needs rendering 'upgrade' header :-)\n\nBefore spray 1.3.1, the 'upgrade' header will be rendered by `spray-can/src/main/scala/spray/can/rendering/ResponseRenderingComponent.scala` until 2a1f1bfe0790d\n\nAnyway, we' ll try to include a `spray-can/src/main/scala/spray/can/rendering/ResponseRenderingComponentPatched.scala` in spray-websocket project, and looking forward the official WebSocket implementation by spray team in the near future.\n. We're running a 6 nodes spray cluster keeping around 600k long-live connections, with lots of connecting/disconnecting frequently.\n\nWe also observed the logs and try to follow the solution. The major concern is that we're not sure if it will lead to resource leak. we're observing.\n\n@cowboy129\n. After long-run, we can confirm, there is no resource leak related to this DeathPactException.\n. For HTTP, we can almost be sure that one Tcp.Received will be followed by a Tcp.Write (request-response pair). But in other cases, for instance, WebSocket, we can not guarantee this req-resp pair, instead,  data my be sent to server's socket with multiple frames without waiting for a response.\n\nPer @jrudolph 's concern, we'll try to add a stage under SSL stage, the new added stage will try to control the data flow sending to SSL, i.e. buffer the Tcp.Received, send one to SSL, waiting for ACK, then send next.\n. @sirthias \nI'd like to pass the spec test added in this patch:\n\n```\n+  \"properly handle write multiple frames at the same time\" in new TestSetup {\n +    val server = new SpraySslServer\n +    val connAttempt = attemptSpraySslClientConnection(server.address)\n +    val serverConn = server.acceptOne()\n +    val clientConn = connAttempt.finishConnect()\n +\n +    clientConn.writeLn(\"Foo\")\n +    serverConn.expectReceivedString(\"Foo\\n\")\n +\n +    serverConn.writeLn(\"bar1\")\n +    serverConn.writeLn(\"bar2\")\n +    serverConn.writeLn(\"bar3\")\n +    clientConn.expectReceivedString(\"bar1\\n\")\n +    clientConn.expectReceivedString(\"bar2\\n\")\n +    clientConn.expectReceivedString(\"bar3\\n\")\n +\n +    clientConn.command(Tcp.Close)\n +    serverConn.events.expectMsg(Tcp.PeerClosed)\n +    TestUtils.verifyActorTermination(serverConn.handler)\n +    clientConn.events.expectMsg(Tcp.Closed)\n +    TestUtils.verifyActorTermination(clientConn.handler)\n +    server.close()\n +  }\n```\n\nAny suggestion, or the above  test  should just not work :-)\n. We tried write with ack on stage above SSL, but, it's much complicated. The PumpAction of encrypt/decrypt may form a call-chain (encrypt -> decypt) when first Write arrives (but Tcp.Received(data) are still fired from under-layer, BTW, you don't know when will it be over.), and causes the state of SSL stage unreliable. We may write a simple code to show this later.\n. @jrudolph @sirthias \n\nTo get spray-websocket working by self-patched, it would be appreciated that the spray-can applies patches that are spec related , i.e. these two files:\n\n`spray-can/src/main/scala/spray/can/rendering/RenderSupport.scala`\n`spray-http/src/main/scala/spray/http/HttpHeader.scala`\n\nThen, we can include a patched `spray-can/src/main/scala/spray/can/rendering/ResponseRenderingComponent.scala` as  `spray-can/src/main/scala/spray/can/rendering/ResponseRenderingComponentPatched.scala` in spray-websocket only, as a temporary solution.\n\nBTW, the 'upgrade' spec which spray-websocket implemented is a generic upgradable supporting, which is not sticking on websocket protocol only. Please take a look at:\n\nhttps://github.com/wandoulabs/spray-websocket/blob/master/spray-websocket/src/main/scala/spray/can/server/UpgradeSupport.scala\n\nhttps://github.com/wandoulabs/spray-websocket/blob/master/spray-websocket/src/main/scala/spray/can/server/UpgradableHttpListener.scala\n\nhttps://github.com/wandoulabs/spray-websocket/blob/master/spray-websocket/src/main/scala/spray/can/client/UpgradableHttpClientSettingsGroup.scala\n. @jrudolph Every spray based WebSocket implementation needs rendering 'upgrade' header :-)\n\nBefore spray 1.3.1, the 'upgrade' header will be rendered by `spray-can/src/main/scala/spray/can/rendering/ResponseRenderingComponent.scala` until 2a1f1bfe0790d\n\nAnyway, we' ll try to include a `spray-can/src/main/scala/spray/can/rendering/ResponseRenderingComponentPatched.scala` in spray-websocket project, and looking forward the official WebSocket implementation by spray team in the near future.\n. We're running a 6 nodes spray cluster keeping around 600k long-live connections, with lots of connecting/disconnecting frequently.\n\nWe also observed the logs and try to follow the solution. The major concern is that we're not sure if it will lead to resource leak. we're observing.\n\n@cowboy129\n. After long-run, we can confirm, there is no resource leak related to this DeathPactException.\n. ",
    "larryboymi": "I know it's been called simple, but for any future (pun) Googler's ending up here,  [here you go](https://gist.github.com/larryboymi/2838db7c476873a71d22).\n. I know it's been called simple, but for any future (pun) Googler's ending up here,  [here you go](https://gist.github.com/larryboymi/2838db7c476873a71d22).\n. ",
    "hierynomus": "@jrudolph I'd expect a Response and not a time-out, sorry for not listing that.\n\n@agemooij here's some curl output:\n\nTomcat7 server running Apache CXF (Jax-RS) for REST:\n\n```\ncurl -v -X POST -d \"<herd></herd>\" http://localhost:9090/yak-shop/load                                                                                                                                                             \n* Adding handle: conn: 0x7ffa8a004000\n* Adding handle: send: 0\n* Adding handle: recv: 0\n* Curl_addHandleToPipeline: length: 1\n* - Conn 0 (0x7ffa8a004000) send_pipe: 1, recv_pipe: 0\n* About to connect() to localhost port 9090 (#0)\n*   Trying ::1...\n* Connected to localhost (::1) port 9090 (#0)\n> POST /yak-shop/load HTTP/1.1\n> User-Agent: curl/7.30.0\n> Host: localhost:9090\n> Accept: */*\n> Content-Length: 13\n> Content-Type: application/x-www-form-urlencoded\n>\n* upload completely sent off: 13 out of 13 bytes\n< HTTP/1.1 205 Reset Content\n* Server Apache-Coyote/1.1 is not blacklisted\n< Server: Apache-Coyote/1.1\n< Date: Wed, 19 Mar 2014 16:03:10 GMT\n* no chunk, no close, no size. Assume close to signal end\n<\n^C\n```\n\nSo it might seem that the server is actually misbehaved, (ie. it doesn't close the connection) which of course is not impossible. \n. Agreed. Thanks for helping out!\n\n2014-03-21 9:33 GMT+01:00 Mathias notifications@github.com:\n\n> The problem is indeed that the server does not send a Content-Lengthheader which, according to the spec, carries the semantic that the closing\n> of the connection signals the end of the response.\n> If your server doesn't close the connection it effectively stalls any\n> progress, the client _cannot_ be the one closing the connection in that\n> case, because there might still be content coming.\n> \n> The HTTPbis spec section that @agemooij https://github.com/agemooijrightfully quoted specifies exactly what the problem is (how the server is\n> misbehaving). Therefore I don't see anything that we can do from the spray\n> side to get this problem fixed for you.\n> \n> ## \n> \n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/issues/832#issuecomment-38256889\n> .\n. @jrudolph I'd expect a Response and not a time-out, sorry for not listing that.\n\n@agemooij here's some curl output:\n\nTomcat7 server running Apache CXF (Jax-RS) for REST:\n\n```\ncurl -v -X POST -d \"<herd></herd>\" http://localhost:9090/yak-shop/load                                                                                                                                                             \n* Adding handle: conn: 0x7ffa8a004000\n* Adding handle: send: 0\n* Adding handle: recv: 0\n* Curl_addHandleToPipeline: length: 1\n* - Conn 0 (0x7ffa8a004000) send_pipe: 1, recv_pipe: 0\n* About to connect() to localhost port 9090 (#0)\n*   Trying ::1...\n* Connected to localhost (::1) port 9090 (#0)\n> POST /yak-shop/load HTTP/1.1\n> User-Agent: curl/7.30.0\n> Host: localhost:9090\n> Accept: */*\n> Content-Length: 13\n> Content-Type: application/x-www-form-urlencoded\n>\n* upload completely sent off: 13 out of 13 bytes\n< HTTP/1.1 205 Reset Content\n* Server Apache-Coyote/1.1 is not blacklisted\n< Server: Apache-Coyote/1.1\n< Date: Wed, 19 Mar 2014 16:03:10 GMT\n* no chunk, no close, no size. Assume close to signal end\n<\n^C\n```\n\nSo it might seem that the server is actually misbehaved, (ie. it doesn't close the connection) which of course is not impossible. \n. Agreed. Thanks for helping out!\n\n2014-03-21 9:33 GMT+01:00 Mathias notifications@github.com:\n\n> The problem is indeed that the server does not send a Content-Lengthheader which, according to the spec, carries the semantic that the closing\n> of the connection signals the end of the response.\n> If your server doesn't close the connection it effectively stalls any\n> progress, the client _cannot_ be the one closing the connection in that\n> case, because there might still be content coming.\n> \n> The HTTPbis spec section that @agemooij https://github.com/agemooijrightfully quoted specifies exactly what the problem is (how the server is\n> misbehaving). Therefore I don't see anything that we can do from the spray\n> side to get this problem fixed for you.\n> \n> ## \n> \n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/issues/832#issuecomment-38256889\n> .\n. ",
    "gkossakowski": "Thanks for creating a branch for me. The PR is coming shortly!\n. @jrudolph: shapeless 1.2.4 is [not](https://twitter.com/milessabin/status/446685993954201601) going to be released for Scala 2.11. Should I submit a patch (to this branch) that upgrades spray 1.3 to shapeless 2.0?\n. @jrudolph: upgrade to shapeless 2.0-M1 submitted in #837.\n. No, it's not expected. My mistake was to test my changes with `compile` command only and not with `test:compile`. I'll see if I can do something about it. I won't be digging too deep as my time dedicated to dbuild is running out now.\n. Unfortunately, I don't have time do understand the changes between shapeless 1.2.4 and 2.0 to fix this problem. We'll have to skip compiling and running tests for spray in dbuild and keep the fork for now. I wish shapeless 1.2.4 was released against Scala 2.11.\n. Understood. Once you are done with the migration, please remember to ping Scala team at Typesafe (you can contact me directly). Thanks!\n. Thanks for creating a branch for me. The PR is coming shortly!\n. @jrudolph: shapeless 1.2.4 is [not](https://twitter.com/milessabin/status/446685993954201601) going to be released for Scala 2.11. Should I submit a patch (to this branch) that upgrades spray 1.3 to shapeless 2.0?\n. @jrudolph: upgrade to shapeless 2.0-M1 submitted in #837.\n. No, it's not expected. My mistake was to test my changes with `compile` command only and not with `test:compile`. I'll see if I can do something about it. I won't be digging too deep as my time dedicated to dbuild is running out now.\n. Unfortunately, I don't have time do understand the changes between shapeless 1.2.4 and 2.0 to fix this problem. We'll have to skip compiling and running tests for spray in dbuild and keep the fork for now. I wish shapeless 1.2.4 was released against Scala 2.11.\n. Understood. Once you are done with the migration, please remember to ping Scala team at Typesafe (you can contact me directly). Thanks!\n. ",
    "colestanfield": "+1\n. +1\n. ",
    "DGolubets": "+1\nI'm stuck with 1.3.0 cos of this bug.\n. +1\nI'm stuck with 1.3.0 cos of this bug.\n. ",
    "wkozaczuk": "Could you please publish Scala 2.11 equivalent of patched 1.3.2-20140428?\n. Could you please publish Scala 2.11 equivalent of patched 1.3.2-20140428?\n. ",
    "driekken": "Hi guys,\n\nNo new useful information from me (others above mentioned the same problems we had in trying to get websockets working with spray-websocket, spray and Scala 2.11), just wanted to show that interest is high in getting this through.\n\nThank you in advance and keep up the good work!\n. @sirthias : thank you!\n. @sirthias : Quick question. \n\nAny reason I cannot find it on repo.spray.io (e.g. http://repo.spray.io/io/spray/spray-io/1.3.2-20140909/ doesn't exist?). Is there supposed to be a delay before it becomes listed?\n. Right you are, I thought it would follow the same naming patterns as 1.3.2-20140428 (e.g. http://repo.spray.io/io/spray/spray-io/1.3.2-20140428/). \n\nMany thanks for the pointer.\n. Hi guys,\n\nNo new useful information from me (others above mentioned the same problems we had in trying to get websockets working with spray-websocket, spray and Scala 2.11), just wanted to show that interest is high in getting this through.\n\nThank you in advance and keep up the good work!\n. @sirthias : thank you!\n. @sirthias : Quick question. \n\nAny reason I cannot find it on repo.spray.io (e.g. http://repo.spray.io/io/spray/spray-io/1.3.2-20140909/ doesn't exist?). Is there supposed to be a delay before it becomes listed?\n. Right you are, I thought it would follow the same naming patterns as 1.3.2-20140428 (e.g. http://repo.spray.io/io/spray/spray-io/1.3.2-20140428/). \n\nMany thanks for the pointer.\n. ",
    "savulchik": "Already fixed in fb7e3a3\n. @sirthias I have accepted Typesafe CLA and fixed commit messages.\n. @sirthias sure. I'll close this pull request.\n. Already fixed in fb7e3a3\n. @sirthias I have accepted Typesafe CLA and fixed commit messages.\n. @sirthias sure. I'll close this pull request.\n. ",
    "abdusahin": "As far as I know http headers are case insensitive , so I think extractor\nshould handle it regardless of case sensitivity.\nOn 2 Apr 2014 15:19, \"Thuillier Benjamin\" notifications@github.com wrote:\n\n> I think is for simplicity that the extractor use lowerCase instead of the\n> normal case of the Headers and less error prone\n> \n> ## \n> \n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/issues/844#issuecomment-39335408\n> .\n. Surely using Spray directives would solve the issue, however the essential issue is still there. Is that how Spray handles http headers?. To me, having to type qualified spray directives is like an overkill for such operation.   \n. noticed that, I think that will work for me.\nThanks\n. As far as I know http headers are case insensitive , so I think extractor\nshould handle it regardless of case sensitivity.\nOn 2 Apr 2014 15:19, \"Thuillier Benjamin\" notifications@github.com wrote:\n\n> I think is for simplicity that the extractor use lowerCase instead of the\n> normal case of the Headers and less error prone\n> \n> ## \n> \n> Reply to this email directly or view it on GitHubhttps://github.com/spray/spray/issues/844#issuecomment-39335408\n> .\n. Surely using Spray directives would solve the issue, however the essential issue is still there. Is that how Spray handles http headers?. To me, having to type qualified spray directives is like an overkill for such operation.   \n. noticed that, I think that will work for me.\nThanks\n. ",
    "mfulgo": "Ack.\nYou might want to update http://spray.io/project-info/contributing/#contributor-license-agreement-cla to include the step of accepting the Typesafe CLA.\n\n2beaucoup: good catch\n. Mathias,\nLet me know if you need me to update the title of the pull request as well.\nThanks,\n-Matt\n. Mathias,\n\nCan you retrigger the Travis CI build?  It looks like it was having issues (but I don't have access to restart it).\n\nThanks,\n-Matt\n. Ack.\nYou might want to update http://spray.io/project-info/contributing/#contributor-license-agreement-cla to include the step of accepting the Typesafe CLA.\n\n2beaucoup: good catch\n. Mathias,\nLet me know if you need me to update the title of the pull request as well.\nThanks,\n-Matt\n. Mathias,\n\nCan you retrigger the Travis CI build?  It looks like it was having issues (but I don't have access to restart it).\n\nThanks,\n-Matt\n. ",
    "iwaltgen": "+1\n. +1\n. ",
    "eweise": "Closed because I realized that I should just include the servlet config parameters in the resources directory so it gets picked up by the classpath since it will never change.\n. Closed because I realized that I should just include the servlet config parameters in the resources directory so it gets picked up by the classpath since it will never change.\n. ",
    "chadselph": "Failed test is unrelated to this pull request; a test of which none of my code affects failed to bind on a TCP port.\n. I've submitted the Typesafe CLA.\n\nI can move the `codePointAt` into the case statement; looking at the implementations of `charAt` and `codePointAt` implementations, it maybe saves a few if statements. I still need to determine what to add to `ix` by checking the `charCount`, but `charCount` only works on Integers (codePoints), not Chars. To avoid the `charPointAt` call, I'm just checking for the char to be a high surrogate.\n. Actually, I tried it that way first, but thought it was too ugly. If you don't think so, I'll do it like that.\n. anything else preventing this from being pulled in?\n. is there anything I can do to help get this fix into akka-http?\n. Failed test is unrelated to this pull request; a test of which none of my code affects failed to bind on a TCP port.\n. I've submitted the Typesafe CLA.\n\nI can move the `codePointAt` into the case statement; looking at the implementations of `charAt` and `codePointAt` implementations, it maybe saves a few if statements. I still need to determine what to add to `ix` by checking the `charCount`, but `charCount` only works on Integers (codePoints), not Chars. To avoid the `charPointAt` call, I'm just checking for the char to be a high surrogate.\n. Actually, I tried it that way first, but thought it was too ugly. If you don't think so, I'll do it like that.\n. anything else preventing this from being pulled in?\n. is there anything I can do to help get this fix into akka-http?\n. ",
    "javierarrieta": "I have started working on this issue and in my first implementation (see referenced commit) the order is not kept so I would have to change all the tests (server is after date now) or make a bigger change in the implementation. Any feedback appreciated\n. I have started working on this issue and in my first implementation (see referenced commit) the order is not kept so I would have to change all the tests (server is after date now) or make a bigger change in the implementation. Any feedback appreciated\n. ",
    "dwestheide": "I thought about implementing a fix myself. However, I have no idea how to make the correct `warnOnIllegalHeader` function available to the `FormDataUnmarshallers` trait, as the latter is located in the spray-httpx module.\n. Great hint, thanks!\n. I thought about implementing a fix myself. However, I have no idea how to make the correct `warnOnIllegalHeader` function available to the `FormDataUnmarshallers` trait, as the latter is located in the spray-httpx module.\n. Great hint, thanks!\n. ",
    "mordonez-me": "This is going to be fixed? what is exactly the workaround?\n. This is going to be fixed? what is exactly the workaround?\n. ",
    "stillalex": "yes, the deployment fails if the 3 jars (caching, can, io) are missing, also 'caching' needs the 'concurrentlinkedhashmap' module to be available as well.\n\nfyi, I'm documenting the needed bundles here: \nhttps://github.com/alexparvulescu/scala-osgi-trials/blob/master/tiny-spray/README.md\n. I think it's because shapeless 1.2.x doesn't export OSGi headers. It does so from 2.x on, but upgrading spray to use shapeless 2.x is a different discussion.\n. Ok, I'm not sure who is/was supposed to take care of the shapeless thing, so I did it on this branch :)\n\nNow, the shapeless import is mandatory, if someone could ask Miles to produce a 1.2.x with OSGi headers, that would be awesome.\n\nAnything else missing?\n. thanks guys!\n. yes, the deployment fails if the 3 jars (caching, can, io) are missing, also 'caching' needs the 'concurrentlinkedhashmap' module to be available as well.\n\nfyi, I'm documenting the needed bundles here: \nhttps://github.com/alexparvulescu/scala-osgi-trials/blob/master/tiny-spray/README.md\n. I think it's because shapeless 1.2.x doesn't export OSGi headers. It does so from 2.x on, but upgrading spray to use shapeless 2.x is a different discussion.\n. Ok, I'm not sure who is/was supposed to take care of the shapeless thing, so I did it on this branch :)\n\nNow, the shapeless import is mandatory, if someone could ask Miles to produce a 1.2.x with OSGi headers, that would be awesome.\n\nAnything else missing?\n. thanks guys!\n. ",
    "rkrzewski": "Making `shapeless` package import optional makes no sense. spray-routing classes fail to link in OSGi environment if Shapeless is not available. This import should be made mandatory, and the burden of providing Shapeless bundle with correct manifest falls on the user. Even if the version published on Maven central does have it, it's trivially easy to produce it locally using tools like BND / maven-bundle-plugin / Bndtools / Bundlor etc.\nOTOH, :+1: to making spray-can, spray-io and spray-caching imports optional.\n. Um, now you removed `shapeless.*` import completely. That's not right. As a result `spray.routing` bundle will fail to load even when shapeless bundle is present in the framework!\n. Oh right, it does indeed work correctly! Now I have checked the generated bundle, and `shapeless` import was generated by bnd. Sorry for making a fuss :)\n. Making `shapeless` package import optional makes no sense. spray-routing classes fail to link in OSGi environment if Shapeless is not available. This import should be made mandatory, and the burden of providing Shapeless bundle with correct manifest falls on the user. Even if the version published on Maven central does have it, it's trivially easy to produce it locally using tools like BND / maven-bundle-plugin / Bndtools / Bundlor etc.\nOTOH, :+1: to making spray-can, spray-io and spray-caching imports optional.\n. Um, now you removed `shapeless.*` import completely. That's not right. As a result `spray.routing` bundle will fail to load even when shapeless bundle is present in the framework!\n. Oh right, it does indeed work correctly! Now I have checked the generated bundle, and `shapeless` import was generated by bnd. Sorry for making a fuss :)\n. ",
    "rjsvaljean": "Thanks @jrudolph . Works perfectly\n. Thanks @jrudolph . Works perfectly\n. ",
    "twillouer": "Sure, but see discussion here https://github.com/spray/spray/issues/869\n. @sirthias yes, done !\n. Sure, but see discussion here https://github.com/spray/spray/issues/869\n. @sirthias yes, done !\n. ",
    "saleh-sedighi-ck": "Any resolution , #971 is tagged as \"Won't Fix\"\n. Any resolution , #971 is tagged as \"Won't Fix\"\n. ",
    "gregbeech": "@sirthias I've updated the docs with those improvements and accepted the CLA.\n. No worries :+1: \n. @sirthias I've updated the docs with those improvements and accepted the CLA.\n. No worries :+1: \n. ",
    "fzakaria": "I don't understand the need for the new directive.\nThe AuthMagnet is already generic on T.\nI have a working example where I've simply made T an Option[User] and get the same implementation without this directive?\n. got it.\n. Ah I see the discussion and perhaps this PR is at fault at some of those concerns raised (over simplification).\nThe implementation is largely copied from the Play! framework so I was under misguided assumptions that they may have already taken the brunt of the work in figuring those difficult questions out.\n. I don't understand the need for the new directive.\nThe AuthMagnet is already generic on T.\nI have a working example where I've simply made T an Option[User] and get the same implementation without this directive?\n. got it.\n. Ah I see the discussion and perhaps this PR is at fault at some of those concerns raised (over simplification).\nThe implementation is largely copied from the Play! framework so I was under misguided assumptions that they may have already taken the brunt of the work in figuring those difficult questions out.\n. ",
    "ChrisCanCompute": "This just cropped up in a project I'm working on, so I've added a test and fixed it.\n. Here's the pull request: https://github.com/spray/spray/pull/934\n. Yes, I'm working for a customer, so I'm trying to get their legal team's OK before I sign it. Sorry about the delay.\n. This just cropped up in a project I'm working on, so I've added a test and fixed it.\n. Here's the pull request: https://github.com/spray/spray/pull/934\n. Yes, I'm working for a customer, so I'm trying to get their legal team's OK before I sign it. Sorry about the delay.\n. ",
    "Dinduks": ":+1:\n\n`spray.http.IllegalUriException: Invalid port -1` when forgetting `http[s]://` is confusing.\n. :+1:\n\n`spray.http.IllegalUriException: Invalid port -1` when forgetting `http[s]://` is confusing.\n. ",
    "hepin1989": "yes,so let's waiting akka http\n. yes,so let's waiting akka http\n. ",
    "pslavazza": "@danarmak I think that it is not totally safe: in my understanding, every time you see such an error it is and indirect symptom of the fact that an actor which is expected to stop (i.e. the ConnectionHandler) remains instead alive - even if inactive. Let's say that there is no harm to the communications handled by your application, but the health of the whole system is worsening. But maybe @jrudolph has better judgement on this case...\n. @danarmak well, I think that you are actually right: I totally missed the point about the fact that the DeathPactException make the ConnectionHandler die. Still, given things aren't going as expected in the code, I am unsure about the fact that the overall state could be considred 100% safe - but I am not aware of any good workaround.\n. @danarmak I think that it is not totally safe: in my understanding, every time you see such an error it is and indirect symptom of the fact that an actor which is expected to stop (i.e. the ConnectionHandler) remains instead alive - even if inactive. Let's say that there is no harm to the communications handled by your application, but the health of the whole system is worsening. But maybe @jrudolph has better judgement on this case...\n. @danarmak well, I think that you are actually right: I totally missed the point about the fact that the DeathPactException make the ConnectionHandler die. Still, given things aren't going as expected in the code, I am unsure about the fact that the overall state could be considred 100% safe - but I am not aware of any good workaround.\n. ",
    "pheaver": "I was able to reliably eliminate the error messages by waiting 1ms before terminating when the connection is closed:\n\n```\n  override def onConnectionClosed(ev: Tcp.ConnectionClosed): Unit = {\n    context.system.scheduler.scheduleOnce(1 millisecond) {\n      self ! akka.actor.PoisonPill\n    }\n  }\n```\n\nNot the best solution, and it isn't a guarantee, but so far it has worked 100% for me.\n. I was able to reliably eliminate the error messages by waiting 1ms before terminating when the connection is closed:\n\n```\n  override def onConnectionClosed(ev: Tcp.ConnectionClosed): Unit = {\n    context.system.scheduler.scheduleOnce(1 millisecond) {\n      self ! akka.actor.PoisonPill\n    }\n  }\n```\n\nNot the best solution, and it isn't a guarantee, but so far it has worked 100% for me.\n. ",
    "mindade": "@pheaver suggestion didn't quite work for me (onConnectionClosed was never called), but I added something similar in receive:\n\n```\nimport context._\n...\ndef receive = {\n  ...\n  case _: Http.ConnectionClosed =>\n    val slf = self\n    context.system.scheduler.scheduleOnce(1 millisecond) {\n      slf ! akka.actor.PoisonPill\n    }\n}\n```\n\nI prefer to use spray-can anyway, and this solution doesn't need spray-router.\n. @pheaver suggestion didn't quite work for me (onConnectionClosed was never called), but I added something similar in receive:\n\n```\nimport context._\n...\ndef receive = {\n  ...\n  case _: Http.ConnectionClosed =>\n    val slf = self\n    context.system.scheduler.scheduleOnce(1 millisecond) {\n      slf ! akka.actor.PoisonPill\n    }\n}\n```\n\nI prefer to use spray-can anyway, and this solution doesn't need spray-router.\n. ",
    "rahilb": "see PR #886 \n. @sirthias done :+1: \n. see PR #886 \n. @sirthias done :+1: \n. ",
    "bblfish": "On the Scala-JS mailing list [I got the following answer in a thread where I asked the question](https://groups.google.com/d/msg/scala-js/Z41csH50WMA/s16HoVMVQhMJ)\n\n> Hopefully parboiled2 will cut the dependency on java for parboiled...\n> \n> > As far as I know parboiled2 is in pure scala and there was an attempt to port parboiled to scalajs by someone but he had problems with porting Shapeless.\n\nI did not know of [parboiled2](https://github.com/sirthias/parboiled2). So I suppose this is an issue for moving to parboiled2... I'll try finding the issue with Shapeless.\n. There is an issue on Parboiled2 to make it compile on Scala-JS https://github.com/sirthias/parboiled2/issues/77\nwhich would be a dependency of this issue.\n. On the Scala-JS mailing list [I got the following answer in a thread where I asked the question](https://groups.google.com/d/msg/scala-js/Z41csH50WMA/s16HoVMVQhMJ)\n\n> Hopefully parboiled2 will cut the dependency on java for parboiled...\n> \n> > As far as I know parboiled2 is in pure scala and there was an attempt to port parboiled to scalajs by someone but he had problems with porting Shapeless.\n\nI did not know of [parboiled2](https://github.com/sirthias/parboiled2). So I suppose this is an issue for moving to parboiled2... I'll try finding the issue with Shapeless.\n. There is an issue on Parboiled2 to make it compile on Scala-JS https://github.com/sirthias/parboiled2/issues/77\nwhich would be a dependency of this issue.\n. ",
    "voitau": "Accepted.\n. Accepted.\n. ",
    "dtaniwaki": "+1\n. +1\n. ",
    "ryryguy": "Ah... you're right, good point!\n\nI guess if there aren't any filtering directives in the inner route it should be okay, but there's no way to guarantee that, is there?  Is there some way to catch a match rejection in the inner route and restore the original timeout?  (Finding the \"original timeout\" might be problematic in itself since you can't really query the responder for it, but I'd think resetting to the value that is set in config should be all right.)\n. Ah... you're right, good point!\n\nI guess if there aren't any filtering directives in the inner route it should be okay, but there's no way to guarantee that, is there?  Is there some way to catch a match rejection in the inner route and restore the original timeout?  (Finding the \"original timeout\" might be problematic in itself since you can't really query the responder for it, but I'd think resetting to the value that is set in config should be all right.)\n. ",
    "Bohtvaroh": "For a reason, spray libraries where duplicated: I got 1.2.0 and 1.2.1 in the classpath. Fixed by:\n\n```\n  val sprayCaching = \"io.spray\" %% \"spray-caching\" % sprayVersion\n  val sprayCan = \"io.spray\" %% \"spray-can\" % sprayVersion\n  val sprayHttp = \"io.spray\" %% \"spray-http\" % sprayVersion\n  val sprayHttpx = \"io.spray\" %% \"spray-httpx\" % sprayVersion\n  val sprayIO = \"io.spray\" %% \"spray-io\" % sprayVersion\n  val sprayJson = \"io.spray\" %% \"spray-json\" % sprayJsonVersion\n  val sprayRouting = \"io.spray\" %% \"spray-routing\" % sprayVersion\n  val sprayUtil = \"io.spray\" %% \"spray-util\" % sprayVersion\n```\n. I apologize. Problem is still reproducible on 1.3.1. I have only 1.3.1 libraries on classpath.\n\nBTW, minified version http://fb.me/react-with-addons-0.10.0.min.js is served correctly. Probably, this is due to file size (600kb).\n. Yes, setting `spray.routing.file-chunking-threshold-size` to 0 fixes the problem. But doesn't it disable chunking completely? I need it to serve file downloads like this:\n\n```\nval routingSettings = implicitly[RoutingSettings]\nautoChunk(routingSettings.fileChunkingThresholdSize, routingSettings.fileChunkingChunkSize) {\n  complete {\n    cvController.export(username, language)\n  }\n}\n```\n. Can't I just disable chunking for static resources at /static and leave it enabled for /api/export/... ? I still can't get why chunking doesn't work with caching. As I understand, caching is just an optimization to prevent calling clazz.getResource every time when accessing static resource, or am I wrong? By client-side caching you mean using Last-Modified header? From this perspective I would have both client and server-side caching.\n. Just read this https://groups.google.com/forum/#!topic/spray-user/mL4sMr1qfwE ... Now it looks like I better remove server cache completely.\n. OK, thanks for your help and pointing out to `compressResponseIfRequested` directive.\n. For a reason, spray libraries where duplicated: I got 1.2.0 and 1.2.1 in the classpath. Fixed by:\n\n```\n  val sprayCaching = \"io.spray\" %% \"spray-caching\" % sprayVersion\n  val sprayCan = \"io.spray\" %% \"spray-can\" % sprayVersion\n  val sprayHttp = \"io.spray\" %% \"spray-http\" % sprayVersion\n  val sprayHttpx = \"io.spray\" %% \"spray-httpx\" % sprayVersion\n  val sprayIO = \"io.spray\" %% \"spray-io\" % sprayVersion\n  val sprayJson = \"io.spray\" %% \"spray-json\" % sprayJsonVersion\n  val sprayRouting = \"io.spray\" %% \"spray-routing\" % sprayVersion\n  val sprayUtil = \"io.spray\" %% \"spray-util\" % sprayVersion\n```\n. I apologize. Problem is still reproducible on 1.3.1. I have only 1.3.1 libraries on classpath.\n\nBTW, minified version http://fb.me/react-with-addons-0.10.0.min.js is served correctly. Probably, this is due to file size (600kb).\n. Yes, setting `spray.routing.file-chunking-threshold-size` to 0 fixes the problem. But doesn't it disable chunking completely? I need it to serve file downloads like this:\n\n```\nval routingSettings = implicitly[RoutingSettings]\nautoChunk(routingSettings.fileChunkingThresholdSize, routingSettings.fileChunkingChunkSize) {\n  complete {\n    cvController.export(username, language)\n  }\n}\n```\n. Can't I just disable chunking for static resources at /static and leave it enabled for /api/export/... ? I still can't get why chunking doesn't work with caching. As I understand, caching is just an optimization to prevent calling clazz.getResource every time when accessing static resource, or am I wrong? By client-side caching you mean using Last-Modified header? From this perspective I would have both client and server-side caching.\n. Just read this https://groups.google.com/forum/#!topic/spray-user/mL4sMr1qfwE ... Now it looks like I better remove server cache completely.\n. OK, thanks for your help and pointing out to `compressResponseIfRequested` directive.\n. ",
    "bennetimo": "No problem- done.\n. No problem- done.\n. ",
    "JyotsMore": "Download spray-can example\n. Download spray-can example\n. ",
    "sksamuel": "I thought that at first but they're both listed here http://www.iana.org/assignments/character-sets/character-sets.xhtml\n. I thought that at first but they're both listed here http://www.iana.org/assignments/character-sets/character-sets.xhtml\n. ",
    "benblack86": "When I saw corruption it was rare. I was streaming 20MB/s (when uncompressed) and would see corruption happen on average after 30 minutes.\n. When I saw corruption it was rare. I was streaming 20MB/s (when uncompressed) and would see corruption happen on average after 30 minutes.\n. ",
    "maspwr": "@jrudolph Apologies for the delay. CLA has been accepted.\n. @jrudolph Apologies for the delay. CLA has been accepted.\n. I can open a PR if this sounds reasonable. Basically it will give an option to `LdapAuthenticator` to enable StartTLS, and then an appropriate `tls.negotiate` and `ctx.reconnect()` call will be done prior to returning the `ctx` object.\n. @jrudolph I would be happy volunteering time to maintain it because we find it useful. If that means moving it into a contrib module, that is fine too. We can of course just maintain our own internal version, but it seems like it would be nice to contribute that back to the community. What do you suggest?\n. @jrudolph Would you prefer that I open a PR to remove the code and docs?\n. @jrudolph That makes sense, and we'd be open to maintaining something like that in a separate repository. We have some other spray libraries we might want to open source and maintain in a similar way. Why don't we wait until akka-http dust settles some more and revisit?\n. Here's a quick diff of the changes for ease of reference, https://github.com/maspwr/spray-template/compare/spray-json-error?expand=1.\n. OK. No problem. Are there any references to a description of the incompatibility? I'm actually curious what the underlying cause is.\n. The error goes away when updating to latest spray-json version 1.3.1 (https://github.com/maspwr/spray-template/tree/spray-json-fixed).\n. I came across https://groups.google.com/forum/#!searchin/spray-user/spray-json/spray-user/oCKjHmUMDb0/pFZ5mJ0w9u4J and https://github.com/spray/spray/issues/932 which both describe the issue. I see that the newer spray libraries were compiled against the newer spray-json, which have that new type. That clears it up for me.\n\nI'm going to close this issue now that my curiosity has been satisfied.\n. @jrudolph Apologies for the delay. CLA has been accepted.\n. @jrudolph Apologies for the delay. CLA has been accepted.\n. I can open a PR if this sounds reasonable. Basically it will give an option to `LdapAuthenticator` to enable StartTLS, and then an appropriate `tls.negotiate` and `ctx.reconnect()` call will be done prior to returning the `ctx` object.\n. @jrudolph I would be happy volunteering time to maintain it because we find it useful. If that means moving it into a contrib module, that is fine too. We can of course just maintain our own internal version, but it seems like it would be nice to contribute that back to the community. What do you suggest?\n. @jrudolph Would you prefer that I open a PR to remove the code and docs?\n. @jrudolph That makes sense, and we'd be open to maintaining something like that in a separate repository. We have some other spray libraries we might want to open source and maintain in a similar way. Why don't we wait until akka-http dust settles some more and revisit?\n. Here's a quick diff of the changes for ease of reference, https://github.com/maspwr/spray-template/compare/spray-json-error?expand=1.\n. OK. No problem. Are there any references to a description of the incompatibility? I'm actually curious what the underlying cause is.\n. The error goes away when updating to latest spray-json version 1.3.1 (https://github.com/maspwr/spray-template/tree/spray-json-fixed).\n. I came across https://groups.google.com/forum/#!searchin/spray-user/spray-json/spray-user/oCKjHmUMDb0/pFZ5mJ0w9u4J and https://github.com/spray/spray/issues/932 which both describe the issue. I see that the newer spray libraries were compiled against the newer spray-json, which have that new type. That clears it up for me.\n\nI'm going to close this issue now that my curiosity has been satisfied.\n. ",
    "vendamere": "Unfortunately the service I'm trying to make requests to doesn't support <code>multipart/form-data</code>, which isn't spray's problem. But I still think that there should be some method to encode sub-delimiters in <code>application/x-www-form-urlencoded</code> requests.\n. [W3C HTML 4.01 Specification](http://www.w3.org/TR/html401/interact/forms.html#h-17.13.4.1) 17.13.4 Form content types subsection 1 application/x-www-form-urlencoded\n\n> Control names and values are escaped. Space characters are replaced by '+', and then **reserved \n> characters are escaped** as described in [RFC1738], section 2.2: Non-alphanumeric characters are\n> replaced by '%HH', a percent sign and two hexadecimal digits representing the ASCII code of the \n> character. Line breaks are represented as \"CR LF\" pairs (i.e., '%0D%0A').\n\nI believe this says all reserved characters within the value must be escaped.\n\n[RFC 1738](http://www.ietf.org/rfc/rfc1738.txt) Section 2.2 Paragraph 8\n\n> Many URL schemes reserve certain characters for a special meaning:\n> their appearance in the scheme-specific part of the URL has a\n> designated semantics. If the character corresponding to an octet is\n> reserved in a scheme, **the octet must be encoded**.  The characters \";\",\n> \"/\", \"?\", \":\", \"@\", \"=\" and \"&\" are the characters which may be\n> reserved for special meaning within a scheme. No other characters may\n> be reserved within a scheme.\n. @sirthias Thanks, rebased and accepted.\n. Unfortunately the service I'm trying to make requests to doesn't support <code>multipart/form-data</code>, which isn't spray's problem. But I still think that there should be some method to encode sub-delimiters in <code>application/x-www-form-urlencoded</code> requests.\n. [W3C HTML 4.01 Specification](http://www.w3.org/TR/html401/interact/forms.html#h-17.13.4.1) 17.13.4 Form content types subsection 1 application/x-www-form-urlencoded\n\n> Control names and values are escaped. Space characters are replaced by '+', and then **reserved \n> characters are escaped** as described in [RFC1738], section 2.2: Non-alphanumeric characters are\n> replaced by '%HH', a percent sign and two hexadecimal digits representing the ASCII code of the \n> character. Line breaks are represented as \"CR LF\" pairs (i.e., '%0D%0A').\n\nI believe this says all reserved characters within the value must be escaped.\n\n[RFC 1738](http://www.ietf.org/rfc/rfc1738.txt) Section 2.2 Paragraph 8\n\n> Many URL schemes reserve certain characters for a special meaning:\n> their appearance in the scheme-specific part of the URL has a\n> designated semantics. If the character corresponding to an octet is\n> reserved in a scheme, **the octet must be encoded**.  The characters \";\",\n> \"/\", \"?\", \":\", \"@\", \"=\" and \"&\" are the characters which may be\n> reserved for special meaning within a scheme. No other characters may\n> be reserved within a scheme.\n. @sirthias Thanks, rebased and accepted.\n. ",
    "msilb": "I am using spray to issue a POST request containing an ISO-formatted date, e.g. 2015-04-01T00:00:00Z. The server complains about malformed body but when I manually encode colon as %3A and use simple string as entity it works fine. It would be great if you could fix it by encoding the colons as proposed in RFCs. Thanks.\n. Currently the following workaround exists:\n\n``` scala\nval uri = Uri(\"http://example.com\").withQuery(Query.asBodyData(Seq((\"param\", \"value1,value2,value3\"))))\n```\n\nBut obviously `asBodyData` implies usage in a `POST` request rather than `GET`.\n. I am using spray to issue a POST request containing an ISO-formatted date, e.g. 2015-04-01T00:00:00Z. The server complains about malformed body but when I manually encode colon as %3A and use simple string as entity it works fine. It would be great if you could fix it by encoding the colons as proposed in RFCs. Thanks.\n. Currently the following workaround exists:\n\n``` scala\nval uri = Uri(\"http://example.com\").withQuery(Query.asBodyData(Seq((\"param\", \"value1,value2,value3\"))))\n```\n\nBut obviously `asBodyData` implies usage in a `POST` request rather than `GET`.\n. ",
    "kanuku": "Hi @jrudolph Can you provide more information.\nWhat is the reason for removing this type of expressions from the examples?\nThnx\n. @sslavic \nIn this case it comes from an API specification which is used by a [client](https://docs.docker.com/reference/api/registry_api/#status-check-for-registry).\nUnfortunately it still removes the content-type:\n\n``` scala\n  val statusRoute =\n    path(\"v1\" / \"_ping\") {\n      respondWithSingletonHeaders(REGISTRY_VERSION, VARY) {\n        respondWithMediaType(`application/json`) {\n          get {\n            complete {\n              \"\"\n            }\n          }\n        }\n      }\n    }\n```\n\nFixed it by adding a space in the response, hope that the client doesn't check the response since it is just a status check supposed to return 200 and always be empty.\n. Hi @jrudolph Can you provide more information.\nWhat is the reason for removing this type of expressions from the examples?\nThnx\n. @sslavic \nIn this case it comes from an API specification which is used by a [client](https://docs.docker.com/reference/api/registry_api/#status-check-for-registry).\nUnfortunately it still removes the content-type:\n\n``` scala\n  val statusRoute =\n    path(\"v1\" / \"_ping\") {\n      respondWithSingletonHeaders(REGISTRY_VERSION, VARY) {\n        respondWithMediaType(`application/json`) {\n          get {\n            complete {\n              \"\"\n            }\n          }\n        }\n      }\n    }\n```\n\nFixed it by adding a space in the response, hope that the client doesn't check the response since it is just a status check supposed to return 200 and always be empty.\n. ",
    "benmccann": "thanks!!\n. thanks!!\n. ",
    "caspark": "PR updated to fix merge conflict from #869. Note the behaviour (documented in commit notes and as a test) resulting from having commas in cookie values.\n\nCommas in cookie values are technically illegal according to the grammar, yet RFC6265 requires user agents to handle them and send them back as is ([more notes on comma handling in cookies](https://jira.atlassian.com/browse/CWD-4141?focusedCommentId=681650&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-681650)).\n\nAs mentioned in the commit notes, Spray currently handles a bad cookie of the form `a=1,2` by throwing away all cookies; this PR will ensure all other cookies are parsed and the bad cookie is parsed as `a=1`. (I also have a preferred alternative which throws away the whole bad cookie - which is incompatible with treating commas as a cookie separator, as per #869 - so let me know if you'd prefer that instead.)\n\nAnyway, this is ready to be merged; it's still a big improvement over the current state of \"bad cookie\" handling.\n. @jrudolph re your voiced concern, I don't think this introduces any _further_ ambiguity.\n\nThere are tests added by this PR which demonstrate what happens when:\n1. A cookie has an invalid name: the cookie with invalid name is completely discarded and other cookies are parsed properly.\n2. A cookie has an invalid value (specifically, a comma in the value): everything after the comma is discarded for that cookie and other cookies are parsed properly.\n   - Discarding part of the value is unfortunate but I believe it's an unavoidable consequence of the attempt to be lenient in cookie parsing by accepting commas as cookie separators introduced in #869 .\n   - Nobody should be relying on being able to parse a cookie with a comma in its value since #869 was merged, as that change made all cookies be dropped if any of them had a comma in the value.\n   - Like I said earlier, I would prefer to revert #869 instead and then drop the entire cookie if there's a value with a comma in it, as that would be consistent with the behaviour for cookies with invalid names. But that's a call for you (and presumably @sirthias ) to make as core project members.\n\nWe're happy to add any extra tests you'd like to see (@ajknoll has already offered extra property-based tests in his initial comment), discuss with other people (if you tell us who you want this discussed with), and/or update documentation (if you give us a pointer to the file(s) to update).\n. PR updated to fix merge conflict from #869. Note the behaviour (documented in commit notes and as a test) resulting from having commas in cookie values.\n\nCommas in cookie values are technically illegal according to the grammar, yet RFC6265 requires user agents to handle them and send them back as is ([more notes on comma handling in cookies](https://jira.atlassian.com/browse/CWD-4141?focusedCommentId=681650&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-681650)).\n\nAs mentioned in the commit notes, Spray currently handles a bad cookie of the form `a=1,2` by throwing away all cookies; this PR will ensure all other cookies are parsed and the bad cookie is parsed as `a=1`. (I also have a preferred alternative which throws away the whole bad cookie - which is incompatible with treating commas as a cookie separator, as per #869 - so let me know if you'd prefer that instead.)\n\nAnyway, this is ready to be merged; it's still a big improvement over the current state of \"bad cookie\" handling.\n. @jrudolph re your voiced concern, I don't think this introduces any _further_ ambiguity.\n\nThere are tests added by this PR which demonstrate what happens when:\n1. A cookie has an invalid name: the cookie with invalid name is completely discarded and other cookies are parsed properly.\n2. A cookie has an invalid value (specifically, a comma in the value): everything after the comma is discarded for that cookie and other cookies are parsed properly.\n   - Discarding part of the value is unfortunate but I believe it's an unavoidable consequence of the attempt to be lenient in cookie parsing by accepting commas as cookie separators introduced in #869 .\n   - Nobody should be relying on being able to parse a cookie with a comma in its value since #869 was merged, as that change made all cookies be dropped if any of them had a comma in the value.\n   - Like I said earlier, I would prefer to revert #869 instead and then drop the entire cookie if there's a value with a comma in it, as that would be consistent with the behaviour for cookies with invalid names. But that's a call for you (and presumably @sirthias ) to make as core project members.\n\nWe're happy to add any extra tests you'd like to see (@ajknoll has already offered extra property-based tests in his initial comment), discuss with other people (if you tell us who you want this discussed with), and/or update documentation (if you give us a pointer to the file(s) to update).\n. ",
    "ajknoll": "@sirthias I've accepted the CLA. Glad to hear this is going ahead :) Please feel free to use the ScalaCheck tests as well if additional verification is needed, as suggested by @caspark.\n. @sirthias I've accepted the CLA. Glad to hear this is going ahead :) Please feel free to use the ScalaCheck tests as well if additional verification is needed, as suggested by @caspark.\n. ",
    "josephw": "The recommendation is [this section of the HTML 4.01 Specification](http://www.w3.org/TR/1999/REC-html401-19991224/appendix/notes.html#h-B.2.2). I've squashed the commits with a rewritten commit message including that link, and signed the Typesafe CLA.\n. The recommendation is [this section of the HTML 4.01 Specification](http://www.w3.org/TR/1999/REC-html401-19991224/appendix/notes.html#h-B.2.2). I've squashed the commits with a rewritten commit message including that link, and signed the Typesafe CLA.\n. ",
    "egisj": "Done!\n. Done!\n. ",
    "wedens": "Also it would be nice to \"auto-detect\" root url instead of hard coding it.\n. Thanks. Changing `routes` to `lazy val` helped.\n. Also it would be nice to \"auto-detect\" root url instead of hard coding it.\n. Thanks. Changing `routes` to `lazy val` helped.\n. ",
    "jhappoldt": "@jrudolph That workaround worked for me.  Thanks!\n. @jrudolph That workaround worked for me.  Thanks!\n. ",
    "kovacshuni": "Could anyone please review this?\n. Hi Mathias,\n\n`ConcurrentLinkedHashMap` keeps the data inside the JVM. From time to time the garbage collector scans over the heap. A full garbage collection event pauses all threads in the JVM, nothing happens during the pause. If this pause takes more than a few seconds, it will become noticeable in the apps as well. [EhCache](http://ehcache.org) uses the same idea but warns about the same GC problem. They have an external storing solution but that is to be paid and in-mem caches nowadays go into the gigabytes range. Well maybe not for you or the rest of the spray library but i'm sure others would be happy for a free and good Scala Futures cache library.\n\nAll i did was extracting/pulling-up an interface (trait), plugging back what was already there, but letting a chance for other implementations. I think it became more lightweight than before. It's not a breaking change, existing code will run perfectly with my patch as well, they don't need to change anything, however new users will have more power if they want to.\n\nThanks for considering anyways :)\n. Could anyone please review this?\n. Hi Mathias,\n\n`ConcurrentLinkedHashMap` keeps the data inside the JVM. From time to time the garbage collector scans over the heap. A full garbage collection event pauses all threads in the JVM, nothing happens during the pause. If this pause takes more than a few seconds, it will become noticeable in the apps as well. [EhCache](http://ehcache.org) uses the same idea but warns about the same GC problem. They have an external storing solution but that is to be paid and in-mem caches nowadays go into the gigabytes range. Well maybe not for you or the rest of the spray library but i'm sure others would be happy for a free and good Scala Futures cache library.\n\nAll i did was extracting/pulling-up an interface (trait), plugging back what was already there, but letting a chance for other implementations. I think it became more lightweight than before. It's not a breaking change, existing code will run perfectly with my patch as well, they don't need to change anything, however new users will have more power if they want to.\n\nThanks for considering anyways :)\n. ",
    "MasseGuillaume": "@sirthias ok maybe I should implement this instead, it's still a candidate http://www.w3.org/TR/CSP/\n@2beaucoup ok I will add tests\n. @sirthias ok maybe I should implement this instead, it's still a candidate http://www.w3.org/TR/CSP/\n@2beaucoup ok I will add tests\n. ",
    "fblundun": "My mistake! My requests were longer than I thought they were.\n. My mistake! My requests were longer than I thought they were.\n. ",
    "alekstorm": "Whoops, there was a failing test. Fixed.\n. Sure, I'd be happy to do that. It looks like only minimal changes were made to the relevant files in the migration from spray to akka-http, in any case.\n. Whoops, there was a failing test. Fixed.\n. Sure, I'd be happy to do that. It looks like only minimal changes were made to the relevant files in the migration from spray to akka-http, in any case.\n. ",
    "chamarts": "  val akkaV = \"2.3.6\"\n  val sprayV = \"1.3.2\"\n val sparkV = \"1.1.0\"\nscalaVersion  := \"2.10.4\"\n. I couldn't find it in dependency graph but some reason sbt is always complaining about this artifact. I did use exclude for all the artifacts mentioned in build.sbt dependencies list and was able to get rid of it. \n.   val akkaV = \"2.3.6\"\n  val sprayV = \"1.3.2\"\n val sparkV = \"1.1.0\"\nscalaVersion  := \"2.10.4\"\n. I couldn't find it in dependency graph but some reason sbt is always complaining about this artifact. I did use exclude for all the artifacts mentioned in build.sbt dependencies list and was able to get rid of it. \n. ",
    "alexlomov": "Hi,\nWhenever you try building the release/1.3 with an sbt console without my patch you'll get a lot of those bnd error, but it doesn't break the build. Neither the patch does.\n. Hi Brian,\nI should check the whole build, not only my patch. Let's see what I can do to get rid of all those annoying errors. Are you OK with that?\n. The build error on 2.10.4 and openjdk7 look weird:\n[ERROR] [11/18/2014 18:17:02.439] [spray-io-SslTlsSupportSpec-akka.actor.default-dispatcher-17] [akka://spray-io-SslTlsSupportSpec/user/server3] Aborting encrypted connection to localhost.localdomain/127.0.0.1:35564 due to [SSLHandshakeException:no cipher suites in common] -> [SSLHandshakeException:no cipher suites in common].\n. Could anybody with the write access restart the Travis build please? The build error seems to be not related to the OSGi headers maintenance.\n. Thanks! It looks like everything's OK. \nBrian @topping, could you please give it another review?\n. Hi,\nWhenever you try building the release/1.3 with an sbt console without my patch you'll get a lot of those bnd error, but it doesn't break the build. Neither the patch does.\n. Hi Brian,\nI should check the whole build, not only my patch. Let's see what I can do to get rid of all those annoying errors. Are you OK with that?\n. The build error on 2.10.4 and openjdk7 look weird:\n[ERROR] [11/18/2014 18:17:02.439] [spray-io-SslTlsSupportSpec-akka.actor.default-dispatcher-17] [akka://spray-io-SslTlsSupportSpec/user/server3] Aborting encrypted connection to localhost.localdomain/127.0.0.1:35564 due to [SSLHandshakeException:no cipher suites in common] -> [SSLHandshakeException:no cipher suites in common].\n. Could anybody with the write access restart the Travis build please? The build error seems to be not related to the OSGi headers maintenance.\n. Thanks! It looks like everything's OK. \nBrian @topping, could you please give it another review?\n. ",
    "noelmarkham": "In the API design for our application, we planned to have two path parameters, something like:\n\n```\n/api/param1/param2/something\n```\n\nWe coded it that way, but then it turned out that `param2` wasn't necessary at all. We have clients out in the wild using this path, so it's not something we can change. Ideally I'd like to ignore `param2` so that any request will match:\n\n```\n/api/param1/whatever/something\n/api/param1/another/something\n```\n\n...etc, as long as there is a segment there.\n. Fair enough, but I know colleagues have found it useful here to use our `IgnoredSegment` as a placeholder for later then enforcing a particular pattern in a URL - the reverse of my previous suggestion, I suppose.\n. In the API design for our application, we planned to have two path parameters, something like:\n\n```\n/api/param1/param2/something\n```\n\nWe coded it that way, but then it turned out that `param2` wasn't necessary at all. We have clients out in the wild using this path, so it's not something we can change. Ideally I'd like to ignore `param2` so that any request will match:\n\n```\n/api/param1/whatever/something\n/api/param1/another/something\n```\n\n...etc, as long as there is a segment there.\n. Fair enough, but I know colleagues have found it useful here to use our `IgnoredSegment` as a placeholder for later then enforcing a particular pattern in a URL - the reverse of my previous suggestion, I suppose.\n. ",
    "fntz": "> akka-actor 2.2.x (with \u2018provided\u2019 scope, i.e. you need to pull it in yourself)\n\nhttp://spray.io/documentation/1.2.2/spray-routing/dependencies/\n. > akka-actor 2.2.x (with \u2018provided\u2019 scope, i.e. you need to pull it in yourself)\n\nhttp://spray.io/documentation/1.2.2/spray-routing/dependencies/\n. ",
    "mkrogemann": "Here is an example from my app (stdout redirected to a log file):\n\n[WARN] [12/04/2014 23:21:24.475] [xyz-backend-akka.actor.default-dispatcher-9] [akka://xyz-backend/user/IO-HTTP/listener-0/1095] Illegal request header: Illegal 'Accept-Encoding' header: Invalid input '/', expected TokenChar, EncodingQuality, ListSep or EOI (line 1, pos 5):\ntext/plain\n    ^\n\nMy guess is that the '\\' yields a newline in the output but I may be wrong\n. Here is an example from my app (stdout redirected to a log file):\n\n[WARN] [12/04/2014 23:21:24.475] [xyz-backend-akka.actor.default-dispatcher-9] [akka://xyz-backend/user/IO-HTTP/listener-0/1095] Illegal request header: Illegal 'Accept-Encoding' header: Invalid input '/', expected TokenChar, EncodingQuality, ListSep or EOI (line 1, pos 5):\ntext/plain\n    ^\n\nMy guess is that the '\\' yields a newline in the output but I may be wrong\n. ",
    "0xdevalias": "_facepalm_ I can't believe I missed that.. Thanks. _wanders off to find more coffee_\n. _facepalm_ I can't believe I missed that.. Thanks. _wanders off to find more coffee_\n. ",
    "sslavic": "That \"group\": \"and-resource\" should probably be \"file-and-resource\"\nMaybe because of that link with getFromResourceDirectory label to http://spray.io/documentation/1.2.2/spray-routing/file-and-resource-directives/getFromResourceDirectory/ in longer example on http://spray.io/documentation/1.2.2/spray-routing/ is broken.\n. When there's no body in response, why should there be [content type header](http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.17) set? If it's just to please the clients which set accept header to e.g. application/json even though request can besides returning json body also return empty response, then one can just for cases where empty body is returned \n\n```\ncancelAllRejections(ofType[UnacceptedResponseContentTypeRejection]) {\n  ...\n}\n```\n. That \"group\": \"and-resource\" should probably be \"file-and-resource\"\nMaybe because of that link with getFromResourceDirectory label to http://spray.io/documentation/1.2.2/spray-routing/file-and-resource-directives/getFromResourceDirectory/ in longer example on http://spray.io/documentation/1.2.2/spray-routing/ is broken.\n. When there's no body in response, why should there be [content type header](http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.17) set? If it's just to please the clients which set accept header to e.g. application/json even though request can besides returning json body also return empty response, then one can just for cases where empty body is returned \n\n```\ncancelAllRejections(ofType[UnacceptedResponseContentTypeRejection]) {\n  ...\n}\n```\n. ",
    "davidhoyt": "I've seen this too and swapping out json4s with an alternative (spray-json) worked for me.\n. I've seen this too and swapping out json4s with an alternative (spray-json) worked for me.\n. ",
    "briandignan": "I got ahead of myself and pushed the commit before running the tests. The change is causing some to fail. I'll take a closer look tomorrow.  \n. I don't know where I went wrong when I first ran the tests, but they're all passing now. \n. Good catch! I didn't know that. Do you want me to submit an updated PR with the change? Also (you may have already seen this), but I noticed in the JavaDoc for InetSocketAddress that the getHostString method was first introduced in Java 1.7. \n\nOn another note, I'll be at Scala Days in San Fran this year. I'm looking forward to Roland Kuhn's presentation on Akka HTTP!\n. It seems like the original host string would need to be passed along with the InetSocketAddress object so that it could be used in the call to `sslContext.createSSLEngine( host, port )`. From what I recall, it would require several intermediate functions to change. Maybe there's a cleaner way to do it? I'm not sure.. \n\nDoes Akka HTTP also do reverse DNS resolution for SSL connections that are initiated with an IP Address? If so, I could look into fixing there instead. \n. Hey @sirthias . Let me know what you think about the latest commit. Thanks!\n. I'll definitely take a stab at it. You know it's funny though.. Last night I had a dream about using reflection to solve this. Weird huh?\n\nWhat's the proper way to submit an updated commit? Should I just submit a new commit to the existing PR? Thanks!\n. Hey @sirthias, @jrudolph . Let me know what you think of the update. Since the Scala reflection API is still marked as experimental, I used the Java reflection API instead. If you prefer Scala reflection though, I can change it.  \n\nIf there's anything at all that you would do differently, definitely let me know! Digging into Spray has been a great learning experience, and I'm open to any feedback that you guys have. \n\nI did some manual testing before and after implementing the changes. The factors for the tests were:\n- Java 1.7.0_67 / Java 1.6.0_65\n- Fix to #993 / No fix \n- Request by IP / Request by Name\n\nI manually tested all 8 combinations. In every test, I was requesting the root URL from www.google.com with SSL. Between each test, I flushed DNS on my Mac with \" sudo killall -HUP mDNSResponder\". \n\nWhat I saw in packet captures mostly matched what I was expecting. The behavior for Java 6 was the same with or without the fix, which was expected. In the Java 7 test where the fix was implemented and the request was sent to an IP, reverse DNS queries were no longer being sent (good). I did notice something strange about the other three Java 7 tests though.  DNS queries for \"SOA local\" were being sent after the 3-way TCP handshake but before the TLS Hello. This wasn't happening with Java 6. It could be something unique about my machine. Either way though, I wanted to make sure you guys were aware of the behavior. \n. In case you're interested, here are the screenshots from the testing:\n\nJava 6, Without fix, Request IP\n![java-6-without-fix-by-ip](https://cloud.githubusercontent.com/assets/5083022/6182026/831ff47e-b30a-11e4-959e-c3201892967b.png)\n\nJava 6, With fix, Request IP\n![java-6-with-fix-by-ip](https://cloud.githubusercontent.com/assets/5083022/6182028/9bd1ae04-b30a-11e4-8a91-f6818f9103f4.png)\n\nJava 6, Without fix, Request Name\n![java-6-without-fix-by-name](https://cloud.githubusercontent.com/assets/5083022/6182033/ad98ff84-b30a-11e4-9dd3-bf11cbe38f3f.png)\n\nJava 6, With fix, Request Name\n![java-6-with-fix-by-name](https://cloud.githubusercontent.com/assets/5083022/6182036/b9b0bcc6-b30a-11e4-9e4d-e5bec4b64944.png)\n\nJava 7, Without fix, Request IP\n![java-7-without-fix-by-ip](https://cloud.githubusercontent.com/assets/5083022/6182040/c3949172-b30a-11e4-8142-56bd10a82313.png)\n\nJava 7, With fix, Request IP\n![java-7-with-fix-by-ip](https://cloud.githubusercontent.com/assets/5083022/6182043/cba5a3ce-b30a-11e4-9b4c-66685ac414bb.png)\n\nJava 7, Without fix, Request Name\n![java-7-without-fix-by-name](https://cloud.githubusercontent.com/assets/5083022/6182047/d756d4a4-b30a-11e4-8924-f9465527d873.png)\n\nJava 7, With fix, Request Name\n![java-7-with-fix-by-name](https://cloud.githubusercontent.com/assets/5083022/6182049/df856adc-b30a-11e4-949b-99aa3a8342a3.png)\n. Hey @sirthias. I amended the commit based on your comments so that additional reflection isn't required for subsequent creations of an SSLEngine object. Let me know what you think.  \n. Sounds good @sirthias . Is there a way for me to amend the commit without erasing all of your notes? I could do another commit, but I figured that you wouldn't want multiple commit related to the same issue in the git history. \n. Open Source FTW!!\n. Done!\n\nI attended Roland's presentation on Akka HTTP last week at Scala Days. It was really impressive. I can't wait to start using it!\n. Try increasing this setting in your config file:\n\nspray.can.client {\n  response-chunk-aggregation-limit = 5m\n}\n\nI had the same problem until I did this. \n. I got ahead of myself and pushed the commit before running the tests. The change is causing some to fail. I'll take a closer look tomorrow.  \n. I don't know where I went wrong when I first ran the tests, but they're all passing now. \n. Good catch! I didn't know that. Do you want me to submit an updated PR with the change? Also (you may have already seen this), but I noticed in the JavaDoc for InetSocketAddress that the getHostString method was first introduced in Java 1.7. \n\nOn another note, I'll be at Scala Days in San Fran this year. I'm looking forward to Roland Kuhn's presentation on Akka HTTP!\n. It seems like the original host string would need to be passed along with the InetSocketAddress object so that it could be used in the call to `sslContext.createSSLEngine( host, port )`. From what I recall, it would require several intermediate functions to change. Maybe there's a cleaner way to do it? I'm not sure.. \n\nDoes Akka HTTP also do reverse DNS resolution for SSL connections that are initiated with an IP Address? If so, I could look into fixing there instead. \n. Hey @sirthias . Let me know what you think about the latest commit. Thanks!\n. I'll definitely take a stab at it. You know it's funny though.. Last night I had a dream about using reflection to solve this. Weird huh?\n\nWhat's the proper way to submit an updated commit? Should I just submit a new commit to the existing PR? Thanks!\n. Hey @sirthias, @jrudolph . Let me know what you think of the update. Since the Scala reflection API is still marked as experimental, I used the Java reflection API instead. If you prefer Scala reflection though, I can change it.  \n\nIf there's anything at all that you would do differently, definitely let me know! Digging into Spray has been a great learning experience, and I'm open to any feedback that you guys have. \n\nI did some manual testing before and after implementing the changes. The factors for the tests were:\n- Java 1.7.0_67 / Java 1.6.0_65\n- Fix to #993 / No fix \n- Request by IP / Request by Name\n\nI manually tested all 8 combinations. In every test, I was requesting the root URL from www.google.com with SSL. Between each test, I flushed DNS on my Mac with \" sudo killall -HUP mDNSResponder\". \n\nWhat I saw in packet captures mostly matched what I was expecting. The behavior for Java 6 was the same with or without the fix, which was expected. In the Java 7 test where the fix was implemented and the request was sent to an IP, reverse DNS queries were no longer being sent (good). I did notice something strange about the other three Java 7 tests though.  DNS queries for \"SOA local\" were being sent after the 3-way TCP handshake but before the TLS Hello. This wasn't happening with Java 6. It could be something unique about my machine. Either way though, I wanted to make sure you guys were aware of the behavior. \n. In case you're interested, here are the screenshots from the testing:\n\nJava 6, Without fix, Request IP\n![java-6-without-fix-by-ip](https://cloud.githubusercontent.com/assets/5083022/6182026/831ff47e-b30a-11e4-959e-c3201892967b.png)\n\nJava 6, With fix, Request IP\n![java-6-with-fix-by-ip](https://cloud.githubusercontent.com/assets/5083022/6182028/9bd1ae04-b30a-11e4-8a91-f6818f9103f4.png)\n\nJava 6, Without fix, Request Name\n![java-6-without-fix-by-name](https://cloud.githubusercontent.com/assets/5083022/6182033/ad98ff84-b30a-11e4-9dd3-bf11cbe38f3f.png)\n\nJava 6, With fix, Request Name\n![java-6-with-fix-by-name](https://cloud.githubusercontent.com/assets/5083022/6182036/b9b0bcc6-b30a-11e4-9e4d-e5bec4b64944.png)\n\nJava 7, Without fix, Request IP\n![java-7-without-fix-by-ip](https://cloud.githubusercontent.com/assets/5083022/6182040/c3949172-b30a-11e4-8142-56bd10a82313.png)\n\nJava 7, With fix, Request IP\n![java-7-with-fix-by-ip](https://cloud.githubusercontent.com/assets/5083022/6182043/cba5a3ce-b30a-11e4-9b4c-66685ac414bb.png)\n\nJava 7, Without fix, Request Name\n![java-7-without-fix-by-name](https://cloud.githubusercontent.com/assets/5083022/6182047/d756d4a4-b30a-11e4-8924-f9465527d873.png)\n\nJava 7, With fix, Request Name\n![java-7-with-fix-by-name](https://cloud.githubusercontent.com/assets/5083022/6182049/df856adc-b30a-11e4-949b-99aa3a8342a3.png)\n. Hey @sirthias. I amended the commit based on your comments so that additional reflection isn't required for subsequent creations of an SSLEngine object. Let me know what you think.  \n. Sounds good @sirthias . Is there a way for me to amend the commit without erasing all of your notes? I could do another commit, but I figured that you wouldn't want multiple commit related to the same issue in the git history. \n. Open Source FTW!!\n. Done!\n\nI attended Roland's presentation on Akka HTTP last week at Scala Days. It was really impressive. I can't wait to start using it!\n. Try increasing this setting in your config file:\n\nspray.can.client {\n  response-chunk-aggregation-limit = 5m\n}\n\nI had the same problem until I did this. \n. ",
    "jn1": "Being new to the spray api, we were just a little surprised when the api decided to arbitrarily rewrite our header for us.  I verified that I am able to get the desired behavior with RawHeader.  Thanks for the pointer! :)\n. Being new to the spray api, we were just a little surprised when the api decided to arbitrarily rewrite our header for us.  I verified that I am able to get the desired behavior with RawHeader.  Thanks for the pointer! :)\n. ",
    "manasapte": "@briandignan I made that change, I see the same problem with lower frequency now. Do you understand what is going on? Can you explain it to me?\n. well, I tried the ACK based solution and still saw the same issue. I am going to try and deploy that jar again and post my results here. Thanks @sirthias \n. @sirthias the ACK based protocol works. Thanks!\n. @briandignan I made that change, I see the same problem with lower frequency now. Do you understand what is going on? Can you explain it to me?\n. well, I tried the ACK based solution and still saw the same issue. I am going to try and deploy that jar again and post my results here. Thanks @sirthias \n. @sirthias the ACK based protocol works. Thanks!\n. ",
    "liangqing": "hi @manasapte how do you fix this? how to use or config the spray to using ACK based protocol?\r\nthanks very much. hi @manasapte how do you fix this? how to use or config the spray to using ACK based protocol?\r\nthanks very much. ",
    "dk14": "Agree. Maybe it's better to just process `GET /favicon.ico` by default if client's code doesn't handle it. Unfortunatelly, It doesn't much comply with lightweight-framework idea, as it implies some implicit behaviour.  So some receipt in the doc is the best solution.\n\nOfftopic: By the way, I assume that spray (especially spray-routing) **is** a framework as it provides IoC (in general sense) by: 1) implementing observer pattern for incoming requests, 2) using directives to describe behaviour - so you're actually calling the user's code by predicate instead of letting user check this predicate and make side effect by himself. \n\nSometimes it even affects customization. For instance, you're calculating all directives for incoming request even if higher prioity directive is already true - formally that's fine as you can use it to regroup or parallelize predicates. But user can't change this behavior, more than that - non-expirienced users could have some stupid troubles with this. \n\nAnyway, I like the idea of avoiding \"default behaviour\" thing as reference implementation is more natural way to provide frequently-used features\n. Agree. Maybe it's better to just process `GET /favicon.ico` by default if client's code doesn't handle it. Unfortunatelly, It doesn't much comply with lightweight-framework idea, as it implies some implicit behaviour.  So some receipt in the doc is the best solution.\n\nOfftopic: By the way, I assume that spray (especially spray-routing) **is** a framework as it provides IoC (in general sense) by: 1) implementing observer pattern for incoming requests, 2) using directives to describe behaviour - so you're actually calling the user's code by predicate instead of letting user check this predicate and make side effect by himself. \n\nSometimes it even affects customization. For instance, you're calculating all directives for incoming request even if higher prioity directive is already true - formally that's fine as you can use it to regroup or parallelize predicates. But user can't change this behavior, more than that - non-expirienced users could have some stupid troubles with this. \n\nAnyway, I like the idea of avoiding \"default behaviour\" thing as reference implementation is more natural way to provide frequently-used features\n. ",
    "hmpatel": "What about the case where the idle-timeout is set to some seconds. When spray tries to close such idle connections on the timeout, it would still be the same race condition situation (server closing the connection vs client sending a POST request) ? \n\nWe do see such Connection Reset very heavily into our system where we have the idleTimeout set to 60 seconds. \n. Is the question if the connection idle timeout interferes with new or ongoing client connections? Or is it about the host connector idle timeout?\nYes that's correct. We are using the host level API and have not set idle-timeout explicitly so essentially the default idle-timeout of 30 seconds is getting applied in this case.  Here is our Spray configuration :\nspray.can {\n  host-connector {\n    max-connections = 500\n    max-retries = 3\n  }\n  client {\n    request-timeout = 40 s\n    connecting-timeout = 2 s\n  }\n  manager-dispatcher = \"spray-dispatcher\"\n  settings-group-dispatcher = \"spray-dispatcher\"\n  host-connector-dispatcher = \"spray-dispatcher\"\n  listener-dispatcher = \"spray-dispatcher\"\n  connection-dispatcher = \"spray-dispatcher\"\n}\n\nWe are experiencing lot of connection reset errors in production:\nloggerName=\"s.can.client.HttpHostConnectionSlot\" ErrorClosed(Connection reset by peer) in response to POST request to /api/v1/blah/blah with no retries left, dispatching error... \n\nHow exactly do you use spray? \nWe use the Host Level API since we connect to multiple hosts and want to maintain different connection pools, timeouts etc for each host. \n\nFurthermore, from the TCP capture we noticed that the client is resetting the connecting exactly after 120 seconds in all the cases. In other words, from the TCP stream, we see there is series of packets exchanged and then there is 120 seconds of  pause with no activity and immediately followed by the RST packet later. \n. @jrudolph - On your comment - \"We could add a flag not to use a persistent connection for non-idempotent requests.\" Is this feature supported in Spray? \n. What about the case where the idle-timeout is set to some seconds. When spray tries to close such idle connections on the timeout, it would still be the same race condition situation (server closing the connection vs client sending a POST request) ? \n\nWe do see such Connection Reset very heavily into our system where we have the idleTimeout set to 60 seconds. \n. Is the question if the connection idle timeout interferes with new or ongoing client connections? Or is it about the host connector idle timeout?\nYes that's correct. We are using the host level API and have not set idle-timeout explicitly so essentially the default idle-timeout of 30 seconds is getting applied in this case.  Here is our Spray configuration :\nspray.can {\n  host-connector {\n    max-connections = 500\n    max-retries = 3\n  }\n  client {\n    request-timeout = 40 s\n    connecting-timeout = 2 s\n  }\n  manager-dispatcher = \"spray-dispatcher\"\n  settings-group-dispatcher = \"spray-dispatcher\"\n  host-connector-dispatcher = \"spray-dispatcher\"\n  listener-dispatcher = \"spray-dispatcher\"\n  connection-dispatcher = \"spray-dispatcher\"\n}\n\nWe are experiencing lot of connection reset errors in production:\nloggerName=\"s.can.client.HttpHostConnectionSlot\" ErrorClosed(Connection reset by peer) in response to POST request to /api/v1/blah/blah with no retries left, dispatching error... \n\nHow exactly do you use spray? \nWe use the Host Level API since we connect to multiple hosts and want to maintain different connection pools, timeouts etc for each host. \n\nFurthermore, from the TCP capture we noticed that the client is resetting the connecting exactly after 120 seconds in all the cases. In other words, from the TCP stream, we see there is series of packets exchanged and then there is 120 seconds of  pause with no activity and immediately followed by the RST packet later. \n. @jrudolph - On your comment - \"We could add a flag not to use a persistent connection for non-idempotent requests.\" Is this feature supported in Spray? \n. ",
    "discordianfish": "I just opened https://github.com/spray/spray/issues/1081 which might be related. I don't know how the test servers work, but if it set `Connection: close` it should be valid to close them on the server side.\n. The problem seems to be unrelated to connection closing. It's still unclear why I get this error but that's more likely a marathon issue so I opened https://github.com/mesosphere/marathon/issues/2489 for that. Will open a new issue in case it turns out spray is doing something wrong.\n. I just opened https://github.com/spray/spray/issues/1081 which might be related. I don't know how the test servers work, but if it set `Connection: close` it should be valid to close them on the server side.\n. The problem seems to be unrelated to connection closing. It's still unclear why I get this error but that's more likely a marathon issue so I opened https://github.com/mesosphere/marathon/issues/2489 for that. Will open a new issue in case it turns out spray is doing something wrong.\n. ",
    "mkneissl": "Let me know if I can help (and how). Would \"magic\" be acceptable (using the platform name / version and / or reflection to find the most correct strategy?\n. Let me know if I can help (and how). Would \"magic\" be acceptable (using the platform name / version and / or reflection to find the most correct strategy?\n. ",
    "2rs2ts": ":clap: I was just about to file an issue, and what a pleasant surprise this is!\n. :clap: I was just about to file an issue, and what a pleasant surprise this is!\n. ",
    "joey-he8x": "For example, the request is \n\n```\nUser-Agent: Mozilla/4.0\nAccept: */*\nHost: *.*.*.*\nPragma: no-cache\nContent-Length: 282\nContent-Type: text/xml\n\\<xml>\n\\<ToUserName><![CDATA[123456]]></ToUserName>\n\\<FromUserName><![CDATA[123231232]]></FromUserName>\n\\<CreateTime>1423709842</CreateTime>\n\\<MsgType><![CDATA[text]]></MsgType>\n\\<Content><![CDATA[cc ......(some unicode characters)]]></Content>\n\\<MsgId>6114787210588045455</MsgId>\n\\</xml>\n```\n\nroutes:\n\n```\nhandleWith {\n            msg:NodeSeq =>\n}\n```\n\nSince the request didn't mention charactersets but only text/xml.  I figure that in spray.http.ContentType \n\n```\n  def charset: HttpCharset = definedCharset getOrElse HttpCharsets.`ISO-8859-1`\n```\n\nand HttpParser will use that code to build HttpData\n\n```\nif (string.isEmpty) Empty else new Bytes(createByteStringUnsafe(string getBytes charset.nioCharset))\n```\n\nwhich I have no means to interfer until I get that wrong encoded data.\n. For example, the request is \n\n```\nUser-Agent: Mozilla/4.0\nAccept: */*\nHost: *.*.*.*\nPragma: no-cache\nContent-Length: 282\nContent-Type: text/xml\n\\<xml>\n\\<ToUserName><![CDATA[123456]]></ToUserName>\n\\<FromUserName><![CDATA[123231232]]></FromUserName>\n\\<CreateTime>1423709842</CreateTime>\n\\<MsgType><![CDATA[text]]></MsgType>\n\\<Content><![CDATA[cc ......(some unicode characters)]]></Content>\n\\<MsgId>6114787210588045455</MsgId>\n\\</xml>\n```\n\nroutes:\n\n```\nhandleWith {\n            msg:NodeSeq =>\n}\n```\n\nSince the request didn't mention charactersets but only text/xml.  I figure that in spray.http.ContentType \n\n```\n  def charset: HttpCharset = definedCharset getOrElse HttpCharsets.`ISO-8859-1`\n```\n\nand HttpParser will use that code to build HttpData\n\n```\nif (string.isEmpty) Empty else new Bytes(createByteStringUnsafe(string getBytes charset.nioCharset))\n```\n\nwhich I have no means to interfer until I get that wrong encoded data.\n. ",
    "donigian": "would you please elaborate on how you got rid of  \"Request was neither completed nor rejected within 1 second\" issue?\n. would you please elaborate on how you got rid of  \"Request was neither completed nor rejected within 1 second\" issue?\n. ",
    "wjur": "Could you please tell me if anyone is working on it? I would like to contribute and backport it by myself but I am not sure if the ticked is already \"taken\".\n. I've started from analysing the fixing commit in akka and one thing confused me. \nCode:\n\n``` Scala\n\"normal examples\" in {\n    resolve(\"g:h\") shouldEqual \"g:h\"\n```\n\nchanges to: \n\n``` Scala\n\"normal examples\" in {\n    resolve(\"g:h\") shouldEqual \"g://h\"\n```\n\nWhile the RFC says (in \"normal examples\")\n\n> \"g:h\"           =  \"g:h\"\n\nFor me, it seems that the commit breaks compliance with the RFC. Maybe I just don't get it. Could you explain it to me a bit? \nI backported the fix to current master and still one test fails. Now, I'm wondering whether I should change resolve(\"g:h\") === \"g:h\" to resolve(\"g:h\") === \"g://h\" in the other part of the tests, too (As this kind of tests do not exist in akka).\n. I have prepared my RP. Please take a look the comment in the code as I am not sure if we understand the flaw in the same way.\nI will appreciate your comments, too (I am willing to polish it so do not hesitate to point out my mistakes or any other doubts).\n\nYes, I would like to contribute to Akka, too. I would be proud :)\n. Ok, I see the build failed on OpenJdk. I humbly admit that I have checked if it's working only on Oracle JVM. I need to investigate this...\n. I have updated the PR according to your comment.\n. Thank you for the merge, guys.\nYes, I will prepare the fix for Akka (or at least try to prepare).\n. @sirthias I have prepared the PR: https://github.com/akka/akka/pull/17052\n. There is a catch! If you invoke curl like that then bash replaces $expand with a value of variable 'expand' defined in bash (which is not defined in my case). To confirm that this really happens please add -v parameter to curl (to see what it actually sends to the server in the query). Finally, you can try using a variable that has a value:\n\n```\n$ curl -v \"http://localhost:8080/test?limit=5&$PATH=opportunities\"\n```\n\nI tried going to the URL using a web browser and everything worked well and the output was as expected:\n\n```\nThe parameters are limit = '5', $expand = 'opportunities'\n```\n\n@haifengl, could you please confirm?\n. > Is there a way to access such information at the very end when the route has been completed?\n\nWhen a request is rejected, the rejection goes to RejectionHandler. Then the handler generates a status code and a response. (See: https://github.com/spray/spray/blob/master/spray-routing/src/main/scala/spray/routing/RejectionHandler.scala). So, there is no status code before the rejection is handled. Finally, I think the short answer is \"no\".\n\nOn the other hand, you could customize the handler to log both the status code and the message (this is one of its purpose, I guess).\nhttp://spray.io/documentation/1.1.3/spray-routing/key-concepts/rejections/#rejectionhandler\n\nAlso, you might check http://stackoverflow.com/questions/34620297/how-to-generically-wrap-a-rejection-with-akka-http. It's akka-http not spray but in Spray it would look almost the same.\n. @MasseGuillaume, these versions have the same API. The difference is that they're built against different versions of Akka and shapeless.\n\nYou can read more about the versioning scheme at http://spray.io/project-info/current-versions/\n. The stacktrace points to BasicFormats.scala:121 which is in StringJsonFormat object:\n<code>require(x ne null)</code>\nIt seems that one of the strings in the Profile instance is null and Spray can't convert it to Json.\nIf nulls are correct values for the string fields you could use Option[String] instead of String. If they're not then you have to look for the code that introduces nulls.\n. Could you please tell me if anyone is working on it? I would like to contribute and backport it by myself but I am not sure if the ticked is already \"taken\".\n. I've started from analysing the fixing commit in akka and one thing confused me. \nCode:\n\n``` Scala\n\"normal examples\" in {\n    resolve(\"g:h\") shouldEqual \"g:h\"\n```\n\nchanges to: \n\n``` Scala\n\"normal examples\" in {\n    resolve(\"g:h\") shouldEqual \"g://h\"\n```\n\nWhile the RFC says (in \"normal examples\")\n\n> \"g:h\"           =  \"g:h\"\n\nFor me, it seems that the commit breaks compliance with the RFC. Maybe I just don't get it. Could you explain it to me a bit? \nI backported the fix to current master and still one test fails. Now, I'm wondering whether I should change resolve(\"g:h\") === \"g:h\" to resolve(\"g:h\") === \"g://h\" in the other part of the tests, too (As this kind of tests do not exist in akka).\n. I have prepared my RP. Please take a look the comment in the code as I am not sure if we understand the flaw in the same way.\nI will appreciate your comments, too (I am willing to polish it so do not hesitate to point out my mistakes or any other doubts).\n\nYes, I would like to contribute to Akka, too. I would be proud :)\n. Ok, I see the build failed on OpenJdk. I humbly admit that I have checked if it's working only on Oracle JVM. I need to investigate this...\n. I have updated the PR according to your comment.\n. Thank you for the merge, guys.\nYes, I will prepare the fix for Akka (or at least try to prepare).\n. @sirthias I have prepared the PR: https://github.com/akka/akka/pull/17052\n. There is a catch! If you invoke curl like that then bash replaces $expand with a value of variable 'expand' defined in bash (which is not defined in my case). To confirm that this really happens please add -v parameter to curl (to see what it actually sends to the server in the query). Finally, you can try using a variable that has a value:\n\n```\n$ curl -v \"http://localhost:8080/test?limit=5&$PATH=opportunities\"\n```\n\nI tried going to the URL using a web browser and everything worked well and the output was as expected:\n\n```\nThe parameters are limit = '5', $expand = 'opportunities'\n```\n\n@haifengl, could you please confirm?\n. > Is there a way to access such information at the very end when the route has been completed?\n\nWhen a request is rejected, the rejection goes to RejectionHandler. Then the handler generates a status code and a response. (See: https://github.com/spray/spray/blob/master/spray-routing/src/main/scala/spray/routing/RejectionHandler.scala). So, there is no status code before the rejection is handled. Finally, I think the short answer is \"no\".\n\nOn the other hand, you could customize the handler to log both the status code and the message (this is one of its purpose, I guess).\nhttp://spray.io/documentation/1.1.3/spray-routing/key-concepts/rejections/#rejectionhandler\n\nAlso, you might check http://stackoverflow.com/questions/34620297/how-to-generically-wrap-a-rejection-with-akka-http. It's akka-http not spray but in Spray it would look almost the same.\n. @MasseGuillaume, these versions have the same API. The difference is that they're built against different versions of Akka and shapeless.\n\nYou can read more about the versioning scheme at http://spray.io/project-info/current-versions/\n. The stacktrace points to BasicFormats.scala:121 which is in StringJsonFormat object:\n<code>require(x ne null)</code>\nIt seems that one of the strings in the Profile instance is null and Spray can't convert it to Json.\nIf nulls are correct values for the string fields you could use Option[String] instead of String. If they're not then you have to look for the code that introduces nulls.\n. ",
    "alexandrnikitin": "Sorry, it isn't very intuitive. \nCould you add a flyout with something like \"The \"x\" term wasn't found.\"? Would be enough I guess.\nThanks.\n. Thanks!!\n. Sorry, it isn't very intuitive. \nCould you add a flyout with something like \"The \"x\" term wasn't found.\"? Would be enough I guess.\nThanks.\n. Thanks!!\n. ",
    "joescii": "Thank you for your patience with my non-compliance, Mathias. :)  I had flippantly made this update all within github without bothering to glance at contribution policies.  I have executed the agreement (I swear I had done that before... perhaps it has been updated) and modified the commit message accordingly.  Thanks!\n. Thank you for your patience with my non-compliance, Mathias. :)  I had flippantly made this update all within github without bothering to glance at contribution policies.  I have executed the agreement (I swear I had done that before... perhaps it has been updated) and modified the commit message accordingly.  Thanks!\n. ",
    "damienlevin": "Sorry I forgot to mention, I'm using 1.3.1.\nEdit : I'm having the same issue with 1.3.2\n. Great news, thanks Mathias !\n. My bad, I totally missed that. Thanks\n. Out of curiosity, why is this even configurable ? It's seem really strange to have such limitation. And why 8M as a default ?\n. Sorry I forgot to mention, I'm using 1.3.1.\nEdit : I'm having the same issue with 1.3.2\n. Great news, thanks Mathias !\n. My bad, I totally missed that. Thanks\n. Out of curiosity, why is this even configurable ? It's seem really strange to have such limitation. And why 8M as a default ?\n. ",
    "pascaldanek": "Hello Mathias and thanks for your quick answer,\n\nActually, I don\u2019t use spray json facilities (maybe I should?). So I simply deserialize as string and then pass the string to my jackson based deserializer.\n\nSo I guess it is more that code which is being called:\n\nhttps://github.com/spray/spray/blob/master/spray-httpx/src/main/scala/spray/httpx/unmarshalling/BasicUnmarshallers.scala#L44\n\nSo you probably answered the question, and the reason why I observe this behaviour is that the string marshaller is a general one, not tied to any specific content type\u2026\n\nThanks,\n\n\u2014pascal\n\nLe 23 mars 2015 \u00e0 14:10, Mathias notifications@github.com a \u00e9crit :\n\n> How do you unmarshal your JSON?\n> The SprayJsonSupport already appears to be doing what you want:\n> https://github.com/spray/spray/blob/master/spray-httpx/src/main/scala/spray/httpx/SprayJsonSupport.scala#L36\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. Hello Mathias and thanks for your quick answer,\n\nActually, I don\u2019t use spray json facilities (maybe I should?). So I simply deserialize as string and then pass the string to my jackson based deserializer.\n\nSo I guess it is more that code which is being called:\n\nhttps://github.com/spray/spray/blob/master/spray-httpx/src/main/scala/spray/httpx/unmarshalling/BasicUnmarshallers.scala#L44\n\nSo you probably answered the question, and the reason why I observe this behaviour is that the string marshaller is a general one, not tied to any specific content type\u2026\n\nThanks,\n\n\u2014pascal\n\nLe 23 mars 2015 \u00e0 14:10, Mathias notifications@github.com a \u00e9crit :\n\n> How do you unmarshal your JSON?\n> The SprayJsonSupport already appears to be doing what you want:\n> https://github.com/spray/spray/blob/master/spray-httpx/src/main/scala/spray/httpx/SprayJsonSupport.scala#L36\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. ",
    "jroper": "On Play we simply force xerces.  We don't add an explicit dependency - Oracle JDK and OpenJDK 1.6 and onwards both come with xerces.  No one has complained so far ;)  I'm not sure if anyone has tried to use Play on other JVMs - we do occassionally get some J9 bugs, never seen JRockit.\n\nThe `SAXParserFactory.setFeature` method is specified to throw an exception if the feature isn't supported.  So, at least if it's not supported, it'll fail loudly - that's how you can know if a JDK does or doesn't support them :)\n. I see, I didn't realise that.\n. On Play we simply force xerces.  We don't add an explicit dependency - Oracle JDK and OpenJDK 1.6 and onwards both come with xerces.  No one has complained so far ;)  I'm not sure if anyone has tried to use Play on other JVMs - we do occassionally get some J9 bugs, never seen JRockit.\n\nThe `SAXParserFactory.setFeature` method is specified to throw an exception if the feature isn't supported.  So, at least if it's not supported, it'll fail loudly - that's how you can know if a JDK does or doesn't support them :)\n. I see, I didn't realise that.\n. ",
    "adamdecaf": "@sirthias Alright, so that sounds like it might be a compiler regression from 2.10.4 to 2.10.5? Is there anything I could help look at?\n. Well, I haven't had much luck so far. Just some debugging from the implicit search from scalac. I'm not very proficient with shapeless so these are a bit tricky to nail down.\n\n```\n[info] /Users/adam/src/banno/spray-parameters-bug/src/main/scala/Boot.scala:22: directives.this.ParamDefMagnet2.forIdentityHList is not a valid implicit value for spray.routing.directives.ParamDefMagnet2[(spray.routing.directives.NameDefaultReceptacle[Boolean], spray.routing.directives.NameDefaultReceptacle[Boolean])] because:\n[info] hasMatchingSymbol reported error: could not find implicit value for parameter f: shapeless.ops.hlist.LeftFolder[(spray.routing.directives.NameDefaultReceptacle[Boolean], spray.routing.directives.NameDefaultReceptacle[Boolean]),spray.routing.Directive0,spray.routing.directives.ParamDefMagnet2.MapReduce.type]\n[info]     parameters('thing.as[Boolean] ? true, 'thing2.as[Boolean] ? false)\n[info]               ^\n[info] /Users/adam/src/banno/spray-parameters-bug/src/main/scala/Boot.scala:22: shapeless.this.Generic.materialize is not a valid implicit value for shapeless.Generic.Aux[(spray.routing.directives.NameDefaultReceptacle[Boolean], spray.routing.directives.NameDefaultReceptacle[Boolean]),L] because:\n[info] hasMatchingSymbol reported error: type mismatch;\n[info]  found   : $4$\n[info]  required: shapeless.Generic.Aux[(spray.routing.directives.NameDefaultReceptacle[Boolean], spray.routing.directives.NameDefaultReceptacle[Boolean]),Nothing]\n[info]     (which expands to)  shapeless.Generic[(spray.routing.directives.NameDefaultReceptacle[Boolean], spray.routing.directives.NameDefaultReceptacle[Boolean])]{type Repr = Nothing}\n[info]     parameters('thing.as[Boolean] ? true, 'thing2.as[Boolean] ? false)\n[info]               ^\n[info] /Users/adam/src/banno/spray-parameters-bug/src/main/scala/Boot.scala:22: directives.this.ParamDefMagnet2.forHList is not a valid implicit value for spray.routing.directives.ParamDefMagnet2[(spray.routing.directives.NameDefaultReceptacle[Boolean], spray.routing.directives.NameDefaultReceptacle[Boolean])] because:\n[info] hasMatchingSymbol reported error: could not find implicit value for parameter hla: shapeless.Generic.Aux[(spray.routing.directives.NameDefaultReceptacle[Boolean], spray.routing.directives.NameDefaultReceptacle[Boolean]),L]\n[info]     parameters('thing.as[Boolean] ? true, 'thing2.as[Boolean] ? false)\n[info]               ^\n```\n\nI've updated the example app with some more attempts to narrow this down. \n. @sirthias I figured it out, we need to add `-Yfundep-materialization` to our build's compiler flags for 2.10.5.\n\nFrom: https://github.com/milessabin/shapeless/wiki/Migration-guide:-shapeless-2.0.0-to-2.1.0#macro-paradise-plugin-required-for-scala-210x\n. Awesome, thanks! . @sirthias Alright, so that sounds like it might be a compiler regression from 2.10.4 to 2.10.5? Is there anything I could help look at?\n. Well, I haven't had much luck so far. Just some debugging from the implicit search from scalac. I'm not very proficient with shapeless so these are a bit tricky to nail down.\n\n```\n[info] /Users/adam/src/banno/spray-parameters-bug/src/main/scala/Boot.scala:22: directives.this.ParamDefMagnet2.forIdentityHList is not a valid implicit value for spray.routing.directives.ParamDefMagnet2[(spray.routing.directives.NameDefaultReceptacle[Boolean], spray.routing.directives.NameDefaultReceptacle[Boolean])] because:\n[info] hasMatchingSymbol reported error: could not find implicit value for parameter f: shapeless.ops.hlist.LeftFolder[(spray.routing.directives.NameDefaultReceptacle[Boolean], spray.routing.directives.NameDefaultReceptacle[Boolean]),spray.routing.Directive0,spray.routing.directives.ParamDefMagnet2.MapReduce.type]\n[info]     parameters('thing.as[Boolean] ? true, 'thing2.as[Boolean] ? false)\n[info]               ^\n[info] /Users/adam/src/banno/spray-parameters-bug/src/main/scala/Boot.scala:22: shapeless.this.Generic.materialize is not a valid implicit value for shapeless.Generic.Aux[(spray.routing.directives.NameDefaultReceptacle[Boolean], spray.routing.directives.NameDefaultReceptacle[Boolean]),L] because:\n[info] hasMatchingSymbol reported error: type mismatch;\n[info]  found   : $4$\n[info]  required: shapeless.Generic.Aux[(spray.routing.directives.NameDefaultReceptacle[Boolean], spray.routing.directives.NameDefaultReceptacle[Boolean]),Nothing]\n[info]     (which expands to)  shapeless.Generic[(spray.routing.directives.NameDefaultReceptacle[Boolean], spray.routing.directives.NameDefaultReceptacle[Boolean])]{type Repr = Nothing}\n[info]     parameters('thing.as[Boolean] ? true, 'thing2.as[Boolean] ? false)\n[info]               ^\n[info] /Users/adam/src/banno/spray-parameters-bug/src/main/scala/Boot.scala:22: directives.this.ParamDefMagnet2.forHList is not a valid implicit value for spray.routing.directives.ParamDefMagnet2[(spray.routing.directives.NameDefaultReceptacle[Boolean], spray.routing.directives.NameDefaultReceptacle[Boolean])] because:\n[info] hasMatchingSymbol reported error: could not find implicit value for parameter hla: shapeless.Generic.Aux[(spray.routing.directives.NameDefaultReceptacle[Boolean], spray.routing.directives.NameDefaultReceptacle[Boolean]),L]\n[info]     parameters('thing.as[Boolean] ? true, 'thing2.as[Boolean] ? false)\n[info]               ^\n```\n\nI've updated the example app with some more attempts to narrow this down. \n. @sirthias I figured it out, we need to add `-Yfundep-materialization` to our build's compiler flags for 2.10.5.\n\nFrom: https://github.com/milessabin/shapeless/wiki/Migration-guide:-shapeless-2.0.0-to-2.1.0#macro-paradise-plugin-required-for-scala-210x\n. Awesome, thanks! . ",
    "csomogyi": "Note, that I made a quick test by simulating the two different header responses with:\n\n```\nrespondWithHeader(HttpHeaders.RawHeader(\"WWW-Authenticate\", \"Basic realm=\\\"\\\"\"))\n```\n\nand\n\n```\nrespondWithHeader(HttpHeaders.RawHeader(\"WWW-Authenticate\", \"Basic realm=\"))\n```\n. I tested it in Firefox, Chrome and Safari. I had the issue in Chrome and Safari.\n\nI found the information, that the realm string must be a `quoted-string`, at http://tools.ietf.org/html/rfc1945#section-11\n\n```\n       challenge      = auth-scheme 1*SP realm *( \",\" auth-param )\n\n       realm          = \"realm\" \"=\" realm-value\n       realm-value    = quoted-string\n```\n\nand at http://tools.ietf.org/html/rfc2617#section-1.2\n\n```\n      challenge   = auth-scheme 1*SP 1#auth-param\n\n...\n\n      realm       = \"realm\" \"=\" realm-value\n      realm-value = quoted-string\n```\n\nI found test cases of the basic authentication challenge for various relatively recent browser versions on this page: http://greenbytes.de/tech/tc/httpauth/\n\nIf you look at the `simplebasictok` test case then there is an interesting remark pointing to a paragraph in the IETF draft spec of HTTP 1.1: http://greenbytes.de/tech/webdav/draft-ietf-httpbis-p7-auth-25.html#rfc.section.2.2\n\n> For historical reasons, a sender must only generate the quoted-string syntax. Recipients might have to support both token and quoted-string syntax for maximum interoperability with existing clients that have been accepting both notations for a long time.\n\nBased on these findings, I believe quotation is mandatory due to historical reasons.\n. Hi Mathias, I've started working on the PR. I'll finish it asap.\n. `SslTlsSupportSpec` in the OpenJDK build has failed. Nothing to do with the `http` module.\n\nIs it possible to rerun the test (could be a timing issue as the timeout was set to 1s, which can be short if the env was very busy during the build)? Or can the PR go further despite the failed build?\n. Mathias, I have another issue. I'd like to sign the CLA but on the project page I can find only Individual CLA and not Corporate. I'd need the latter. Could you pls send me such one for me?\n. I discussed the CLA matter with my business partners and it's ok for now if I go with the individual CLA, which I sent in the meantime. Thanks!\n. Note, that I made a quick test by simulating the two different header responses with:\n\n```\nrespondWithHeader(HttpHeaders.RawHeader(\"WWW-Authenticate\", \"Basic realm=\\\"\\\"\"))\n```\n\nand\n\n```\nrespondWithHeader(HttpHeaders.RawHeader(\"WWW-Authenticate\", \"Basic realm=\"))\n```\n. I tested it in Firefox, Chrome and Safari. I had the issue in Chrome and Safari.\n\nI found the information, that the realm string must be a `quoted-string`, at http://tools.ietf.org/html/rfc1945#section-11\n\n```\n       challenge      = auth-scheme 1*SP realm *( \",\" auth-param )\n\n       realm          = \"realm\" \"=\" realm-value\n       realm-value    = quoted-string\n```\n\nand at http://tools.ietf.org/html/rfc2617#section-1.2\n\n```\n      challenge   = auth-scheme 1*SP 1#auth-param\n\n...\n\n      realm       = \"realm\" \"=\" realm-value\n      realm-value = quoted-string\n```\n\nI found test cases of the basic authentication challenge for various relatively recent browser versions on this page: http://greenbytes.de/tech/tc/httpauth/\n\nIf you look at the `simplebasictok` test case then there is an interesting remark pointing to a paragraph in the IETF draft spec of HTTP 1.1: http://greenbytes.de/tech/webdav/draft-ietf-httpbis-p7-auth-25.html#rfc.section.2.2\n\n> For historical reasons, a sender must only generate the quoted-string syntax. Recipients might have to support both token and quoted-string syntax for maximum interoperability with existing clients that have been accepting both notations for a long time.\n\nBased on these findings, I believe quotation is mandatory due to historical reasons.\n. Hi Mathias, I've started working on the PR. I'll finish it asap.\n. `SslTlsSupportSpec` in the OpenJDK build has failed. Nothing to do with the `http` module.\n\nIs it possible to rerun the test (could be a timing issue as the timeout was set to 1s, which can be short if the env was very busy during the build)? Or can the PR go further despite the failed build?\n. Mathias, I have another issue. I'd like to sign the CLA but on the project page I can find only Individual CLA and not Corporate. I'd need the latter. Could you pls send me such one for me?\n. I discussed the CLA matter with my business partners and it's ok for now if I go with the individual CLA, which I sent in the meantime. Thanks!\n. ",
    "pomadchin": "Tried to exclude libs like in the gist above (tried different spray versions and akka versions), and still got the error `java.lang.NoSuchMethodError: akka.actor.ActorSystem.dispatcher()Lscala/concurrent/ExecutionContext;`\n\nThe full example: https://gist.github.com/pomadchin/48d57b8f5b3525c96f69\nProject tried to run on a new spark: https://github.com/geotrellis/gt-admin (it is not easy to build it cause of lots of local deps)\n. Sure, thx for quick response.\n. Tried to exclude libs like in the gist above (tried different spray versions and akka versions), and still got the error `java.lang.NoSuchMethodError: akka.actor.ActorSystem.dispatcher()Lscala/concurrent/ExecutionContext;`\n\nThe full example: https://gist.github.com/pomadchin/48d57b8f5b3525c96f69\nProject tried to run on a new spark: https://github.com/geotrellis/gt-admin (it is not easy to build it cause of lots of local deps)\n. Sure, thx for quick response.\n. ",
    "ngbinh": "upgrade to shapeless 2.2.2 and no problem with spray 1.3.3 so far.\n. upgrade to shapeless 2.2.2 and no problem with spray 1.3.3 so far.\n. ",
    "ahjohannessen": "@sirthias Any plans on cutting a new release when shapeless 2.3 comes out? It is not binary compatible with shapeless 2.2.x versions (except for 2.2.5).\n. @sirthias Any plans on cutting a new release when shapeless 2.3 comes out? It is not binary compatible with shapeless 2.2.x versions (except for 2.2.5).\n. ",
    "betandr": "@jrudolph Thanks for the reply. I've signed the CLA and confirmed with my GitHub account. I'll add the other header as you suggested, and the test/comments. Could you give me an idea of what sort of abstraction you were thinking of? Just so I'm not going off-piste too much. :)\n. @jrudolph Have refactored the case classes into a trait:\n\n```\ntrait ResponseDirectiveWithDeltaSeconds extends ResponseDirective with ValueRenderable {\n  val deltaSeconds: Long\n  def render[R <: Rendering](r: R): r.type = r ~~  '=' ~~ deltaSeconds\n}\n```\n\n...although I'm missing out `productPrefix` in the render function as I'm not sure where it's coming from and I can't seem to just reference it. What should I be doing here? :)\n. PS: Sorry about the noisy commits; just getting used to the project!\n\nAlso, the build seems to keep failing but I can't see that it's failing due to the things I've added, and it seems to pass on my local machine.\n. Aaaah, I understand now, thanks! :)\n. @jrudolph Ok, all added and tests passing. :)\n. @jrudolph Odd, I've squashed all of the commits but it's failing on `SimpleLruCacheSpec` now...\n. @jrudolph Looks like it's passing now! :tropical_drink: :)\n. @jrudolph Awesome, thanks for your help! :)\n. @jrudolph Thanks for the reply. I've signed the CLA and confirmed with my GitHub account. I'll add the other header as you suggested, and the test/comments. Could you give me an idea of what sort of abstraction you were thinking of? Just so I'm not going off-piste too much. :)\n. @jrudolph Have refactored the case classes into a trait:\n\n```\ntrait ResponseDirectiveWithDeltaSeconds extends ResponseDirective with ValueRenderable {\n  val deltaSeconds: Long\n  def render[R <: Rendering](r: R): r.type = r ~~  '=' ~~ deltaSeconds\n}\n```\n\n...although I'm missing out `productPrefix` in the render function as I'm not sure where it's coming from and I can't seem to just reference it. What should I be doing here? :)\n. PS: Sorry about the noisy commits; just getting used to the project!\n\nAlso, the build seems to keep failing but I can't see that it's failing due to the things I've added, and it seems to pass on my local machine.\n. Aaaah, I understand now, thanks! :)\n. @jrudolph Ok, all added and tests passing. :)\n. @jrudolph Odd, I've squashed all of the commits but it's failing on `SimpleLruCacheSpec` now...\n. @jrudolph Looks like it's passing now! :tropical_drink: :)\n. @jrudolph Awesome, thanks for your help! :)\n. ",
    "v-gerasimov": "Akka log (akka.io.tcp.trace-logging = on)\n\n```\n[DEBUG] [06/17/2015 10:27:25.594] [system-akka.actor.default-dispatcher-13] [akka://system/system/IO-TCP/selectors/$a/46] Wrote [65536] bytes to channel\n[DEBUG] [06/17/2015 10:27:25.595] [system-akka.actor.default-dispatcher-13] [akka://system/system/IO-TCP/selectors/$a/46] Wrote [49152] bytes to channel\n[DEBUG] [06/17/2015 10:27:34.003] [system-akka.actor.default-dispatcher-15] [akka://system/system/IO-TCP/selectors/$a/20] Dropping write because queue is full\n[DEBUG] [06/17/2015 10:28:23.212] [system-akka.actor.default-dispatcher-8] [akka://system/system/IO-TCP/selectors/$a/20] Wrote [49152] bytes to channel\n[DEBUG] [06/17/2015 10:28:23.212] [system-akka.actor.default-dispatcher-8] [akka://system/system/IO-TCP/selectors/$a/20] Dropping write because writing is suspended\n[ERROR] [06/17/2015 10:28:23.215] [system-akka.actor.default-dispatcher-8] [akka://system/user/IO-HTTP/listener-0/19] Could not write response part MessageChunk(Bytes(<65536 bytes>),), aborting connection.\n[DEBUG] [06/17/2015 10:28:23.218] [system-akka.actor.default-dispatcher-8] [akka://system/system/IO-TCP/selectors/$a/20] Got Abort command. RESETing connection.\n[DEBUG] [06/17/2015 10:28:23.219] [system-akka.actor.default-dispatcher-5] [akka://system/user/IO-HTTP/listener-0/19] Connection was Aborted, awaiting TcpConnection termination...\n[DEBUG] [06/17/2015 10:28:23.219] [system-akka.actor.default-dispatcher-5] [akka://system/user/IO-HTTP/listener-0/19] TcpConnection terminated, stopping\n[DEBUG] [06/17/2015 10:28:25.842] [system-akka.actor.default-dispatcher-8] [akka://system/system/IO-TCP/selectors/$a/46] Dropping write because queue is full\n[DEBUG] [06/17/2015 10:28:30.496] [system-akka.actor.default-dispatcher-3] [akka://system/system/IO-TCP/selectors/$a/46] Closing connection due to IO error java.io.IOException: Connection reset by peer\n```\n. I fond this issue https://github.com/akka/akka/issues/15991. I use Apache Spark library which dependency of own Akka library: \"akka-remote_2.10\" % \"2.3.4-spark\". This library doesn't fix problem with race between sending out acks and UpdatePendingWrite in TcpConnection. I exclude it and use Akka 2.3.11. Spray works fine. Thanks!\n. @jrudolph Unfortunately, it doesn't work. I again got `Could not write response part MessageChunk`.  Bad day.\n. Akka log is same:\n\n```\n[DEBUG] [06/17/2015 18:32:51.630] [system-akka.actor.default-dispatcher-39] [akka://system/system/IO-TCP/selectors/$a/116] Wrote [17379] bytes to channel\n[DEBUG] [06/17/2015 18:32:51.662] [system-akka.actor.default-dispatcher-40] [akka://system/system/IO-TCP/selectors/$a/230] Wrote [16384] bytes to channel\n[DEBUG] [06/17/2015 18:32:51.663] [system-akka.actor.default-dispatcher-36] [akka://system/system/IO-TCP/selectors/$a/230] Dropping write because writing is suspended\n[ERROR] [06/17/2015 18:32:51.667] [system-akka.actor.default-dispatcher-36] [akka://system/user/IO-HTTP/listener-0/229] Could not write response part MessageChunk(Bytes(<\n65536 bytes>),), aborting connection.\n[DEBUG] [06/17/2015 18:32:51.668] [system-akka.actor.default-dispatcher-36] [akka://system/system/IO-TCP/selectors/$a/230] Got Abort command. RESETing connection.\n[DEBUG] [06/17/2015 18:32:51.668] [system-akka.actor.default-dispatcher-40] [akka://system/user/IO-HTTP/listener-0/229] Connection was Aborted, awaiting TcpConnection ter\nmination...\n[DEBUG] [06/17/2015 18:32:51.668] [system-akka.actor.default-dispatcher-40] [akka://system/user/IO-HTTP/listener-0/229] TcpConnection terminated, stopping\n```\n\nakka.log-config-on-start tells:\n\n```\n        # Akka version, checked against the runtime version of Akka.\n        \"version\" : \"2.3.11\"\n```\n\nand\n\n```\n        # Always contains the deployed version of spray.\n        # Referenced, for example, from the `spray.can.server.server-header` setting.\n        \"version\" : \"1.3.3\"\n```\n. @sirthias If I understand you correctly, then I streaming the response by streamMarshaller which is using ACKed writes.\nhttps://github.com/spray/spray/blob/master/spray-httpx/src/main/scala/spray/httpx/marshalling/MetaMarshallers.scala\nBesides that, I use PimpedInputStream class in order to represent InputStream to Stream[Array[Byte]]. \nhttps://github.com/spray/spray/blob/master/spray-util/src/main/scala/spray/util/pimps/PimpedInputStream.scala\nI would like to specify about my InputStream, it is SequenceInputStream of over 1000 InputStreams. Could it cause this error?\n. Problem has frequently reproduced when I try to download big file (> 1GB) by several clients.\n. Hi! Is there any news about it? All my attempts to solve the problem have failed.\n. I figured out the problem, there was Nginx wrong settings. I'm sorry to have troubled you.\n. Akka log (akka.io.tcp.trace-logging = on)\n\n```\n[DEBUG] [06/17/2015 10:27:25.594] [system-akka.actor.default-dispatcher-13] [akka://system/system/IO-TCP/selectors/$a/46] Wrote [65536] bytes to channel\n[DEBUG] [06/17/2015 10:27:25.595] [system-akka.actor.default-dispatcher-13] [akka://system/system/IO-TCP/selectors/$a/46] Wrote [49152] bytes to channel\n[DEBUG] [06/17/2015 10:27:34.003] [system-akka.actor.default-dispatcher-15] [akka://system/system/IO-TCP/selectors/$a/20] Dropping write because queue is full\n[DEBUG] [06/17/2015 10:28:23.212] [system-akka.actor.default-dispatcher-8] [akka://system/system/IO-TCP/selectors/$a/20] Wrote [49152] bytes to channel\n[DEBUG] [06/17/2015 10:28:23.212] [system-akka.actor.default-dispatcher-8] [akka://system/system/IO-TCP/selectors/$a/20] Dropping write because writing is suspended\n[ERROR] [06/17/2015 10:28:23.215] [system-akka.actor.default-dispatcher-8] [akka://system/user/IO-HTTP/listener-0/19] Could not write response part MessageChunk(Bytes(<65536 bytes>),), aborting connection.\n[DEBUG] [06/17/2015 10:28:23.218] [system-akka.actor.default-dispatcher-8] [akka://system/system/IO-TCP/selectors/$a/20] Got Abort command. RESETing connection.\n[DEBUG] [06/17/2015 10:28:23.219] [system-akka.actor.default-dispatcher-5] [akka://system/user/IO-HTTP/listener-0/19] Connection was Aborted, awaiting TcpConnection termination...\n[DEBUG] [06/17/2015 10:28:23.219] [system-akka.actor.default-dispatcher-5] [akka://system/user/IO-HTTP/listener-0/19] TcpConnection terminated, stopping\n[DEBUG] [06/17/2015 10:28:25.842] [system-akka.actor.default-dispatcher-8] [akka://system/system/IO-TCP/selectors/$a/46] Dropping write because queue is full\n[DEBUG] [06/17/2015 10:28:30.496] [system-akka.actor.default-dispatcher-3] [akka://system/system/IO-TCP/selectors/$a/46] Closing connection due to IO error java.io.IOException: Connection reset by peer\n```\n. I fond this issue https://github.com/akka/akka/issues/15991. I use Apache Spark library which dependency of own Akka library: \"akka-remote_2.10\" % \"2.3.4-spark\". This library doesn't fix problem with race between sending out acks and UpdatePendingWrite in TcpConnection. I exclude it and use Akka 2.3.11. Spray works fine. Thanks!\n. @jrudolph Unfortunately, it doesn't work. I again got `Could not write response part MessageChunk`.  Bad day.\n. Akka log is same:\n\n```\n[DEBUG] [06/17/2015 18:32:51.630] [system-akka.actor.default-dispatcher-39] [akka://system/system/IO-TCP/selectors/$a/116] Wrote [17379] bytes to channel\n[DEBUG] [06/17/2015 18:32:51.662] [system-akka.actor.default-dispatcher-40] [akka://system/system/IO-TCP/selectors/$a/230] Wrote [16384] bytes to channel\n[DEBUG] [06/17/2015 18:32:51.663] [system-akka.actor.default-dispatcher-36] [akka://system/system/IO-TCP/selectors/$a/230] Dropping write because writing is suspended\n[ERROR] [06/17/2015 18:32:51.667] [system-akka.actor.default-dispatcher-36] [akka://system/user/IO-HTTP/listener-0/229] Could not write response part MessageChunk(Bytes(<\n65536 bytes>),), aborting connection.\n[DEBUG] [06/17/2015 18:32:51.668] [system-akka.actor.default-dispatcher-36] [akka://system/system/IO-TCP/selectors/$a/230] Got Abort command. RESETing connection.\n[DEBUG] [06/17/2015 18:32:51.668] [system-akka.actor.default-dispatcher-40] [akka://system/user/IO-HTTP/listener-0/229] Connection was Aborted, awaiting TcpConnection ter\nmination...\n[DEBUG] [06/17/2015 18:32:51.668] [system-akka.actor.default-dispatcher-40] [akka://system/user/IO-HTTP/listener-0/229] TcpConnection terminated, stopping\n```\n\nakka.log-config-on-start tells:\n\n```\n        # Akka version, checked against the runtime version of Akka.\n        \"version\" : \"2.3.11\"\n```\n\nand\n\n```\n        # Always contains the deployed version of spray.\n        # Referenced, for example, from the `spray.can.server.server-header` setting.\n        \"version\" : \"1.3.3\"\n```\n. @sirthias If I understand you correctly, then I streaming the response by streamMarshaller which is using ACKed writes.\nhttps://github.com/spray/spray/blob/master/spray-httpx/src/main/scala/spray/httpx/marshalling/MetaMarshallers.scala\nBesides that, I use PimpedInputStream class in order to represent InputStream to Stream[Array[Byte]]. \nhttps://github.com/spray/spray/blob/master/spray-util/src/main/scala/spray/util/pimps/PimpedInputStream.scala\nI would like to specify about my InputStream, it is SequenceInputStream of over 1000 InputStreams. Could it cause this error?\n. Problem has frequently reproduced when I try to download big file (> 1GB) by several clients.\n. Hi! Is there any news about it? All my attempts to solve the problem have failed.\n. I figured out the problem, there was Nginx wrong settings. I'm sorry to have troubled you.\n. ",
    "skumargithub": "OK, done!\n. OK, done!\n. ",
    "jonathanhood": "After further debug, we found the effected system was configured with default parameters for both the `tcp_fin_timeout` and `tcp_max_orphans` parameters. These parameters are [documented](https://github.com/torvalds/linux/blob/v3.17/Documentation/networking/ip-sysctl.txt#L259)  as part of the linux kernel. \n\nThese connections, noted above, have been hung in the `FIN_WAIT_2` state for multiple hours, much larger than the default 60 second tcp_fin_timeout. According to kernel documentation, connections in this state are only forcefully closed if the socket is orphaned, meaning no longer owned by a process.\n\nSince the sockets above are not orphans (they are associated with a valid PID) the kernel is not forcefully cleaning them up. Spray is still holding on to these sockets even though they are in this state. In this specific case, the final FIN was likely already sent and will never come again. Spray should probably not retain handles to TCP sockets in this state.\n. We've seen this issue crop up again. The new piece of information we have gathered is that this issue seems to only occur for connections to an HTTPS secured remote host. Unsecured HTTP connections seem to be unaffected.\n\nWe have an internal test which can recreate the issue (which isn't shareable, yet) and can confirm via packet capture that the final FIN is not being sent by the remote host causing the FIN_WAIT2 state for the socket.\n. After further debug, we found the effected system was configured with default parameters for both the `tcp_fin_timeout` and `tcp_max_orphans` parameters. These parameters are [documented](https://github.com/torvalds/linux/blob/v3.17/Documentation/networking/ip-sysctl.txt#L259)  as part of the linux kernel. \n\nThese connections, noted above, have been hung in the `FIN_WAIT_2` state for multiple hours, much larger than the default 60 second tcp_fin_timeout. According to kernel documentation, connections in this state are only forcefully closed if the socket is orphaned, meaning no longer owned by a process.\n\nSince the sockets above are not orphans (they are associated with a valid PID) the kernel is not forcefully cleaning them up. Spray is still holding on to these sockets even though they are in this state. In this specific case, the final FIN was likely already sent and will never come again. Spray should probably not retain handles to TCP sockets in this state.\n. We've seen this issue crop up again. The new piece of information we have gathered is that this issue seems to only occur for connections to an HTTPS secured remote host. Unsecured HTTP connections seem to be unaffected.\n\nWe have an internal test which can recreate the issue (which isn't shareable, yet) and can confirm via packet capture that the final FIN is not being sent by the remote host causing the FIN_WAIT2 state for the socket.\n. ",
    "haifengl": "You are right!! Escaping \"\\$expand\" will works. Thank you very much!\n. You are right!! Escaping \"\\$expand\" will works. Thank you very much!\n. ",
    "pshirshov": "Actually, it's possible to run Spray under OSGi environment even for now.\n\nYou should manually compile shapeless v1 and parboiled v1 with my patches:\nhttps://github.com/sirthias/parboiled/pull/92\nhttps://github.com/milessabin/shapeless/pull/437\n\nYou should use ;clean;packagedArtifacts SBT command to do that.\n\nThen you should install these artifacts into your container manually.\nThen you may install Spray and your application and everything should work.\n. Actually, it's possible to run Spray under OSGi environment even for now.\n\nYou should manually compile shapeless v1 and parboiled v1 with my patches:\nhttps://github.com/sirthias/parboiled/pull/92\nhttps://github.com/milessabin/shapeless/pull/437\n\nYou should use ;clean;packagedArtifacts SBT command to do that.\n\nThen you should install these artifacts into your container manually.\nThen you may install Spray and your application and everything should work.\n. ",
    "raboof": "Printed, signed, scanned and mailed the CLA as described at http://spray.io/project-info/contributing/\n. Travis fails for oracle but not for openjdk:\n\n```\n[info] SimpleLruCacheSpec\n[info] ! not cache exceptions\n[error]  RuntimeException: : Naa  (SimpleLruCacheSpec.scala:30)\n[error] spray.caching.SimpleLruCacheSpec$$anonfun$1$$anonfun$apply$1$$anonfun$2.apply(SimpleLruCacheSpec.scala:30)\n[error] spray.caching.SimpleLruCacheSpec$$anonfun$1$$anonfun$apply$1$$anonfun$2.apply(SimpleLruCacheSpec.scala:30)\n[error] spray.caching.Cache$Keyed$$anonfun$apply$1.apply(Cache.scala:38)\n[error] spray.caching.Cache$Keyed$$anonfun$apply$1.apply(Cache.scala:38)\n[error] spray.caching.SimpleLruCache.apply(LruCache.scala:76)\n[error] spray.caching.Cache$Keyed.apply(Cache.scala:38)\n[error] spray.caching.SimpleLruCacheSpec$$anonfun$1$$anonfun$apply$1.apply(SimpleLruCacheSpec.scala:30)\n[error] spray.caching.SimpleLruCacheSpec$$anonfun$1$$anonfun$apply$1.apply(SimpleLruCacheSpec.scala:30)\n[error] spray.caching.SimpleLruCacheSpec$$anonfun$1.apply(SimpleLruCacheSpec.scala:30)\n[error] spray.caching.SimpleLruCacheSpec$$anonfun$1.apply(SimpleLruCacheSpec.scala:28)\n```\n\nThis looks like a test instability unrelated to this PR.\n. OK never mind.\n. Printed, signed, scanned and mailed the CLA as described at http://spray.io/project-info/contributing/\n. Travis fails for oracle but not for openjdk:\n\n```\n[info] SimpleLruCacheSpec\n[info] ! not cache exceptions\n[error]  RuntimeException: : Naa  (SimpleLruCacheSpec.scala:30)\n[error] spray.caching.SimpleLruCacheSpec$$anonfun$1$$anonfun$apply$1$$anonfun$2.apply(SimpleLruCacheSpec.scala:30)\n[error] spray.caching.SimpleLruCacheSpec$$anonfun$1$$anonfun$apply$1$$anonfun$2.apply(SimpleLruCacheSpec.scala:30)\n[error] spray.caching.Cache$Keyed$$anonfun$apply$1.apply(Cache.scala:38)\n[error] spray.caching.Cache$Keyed$$anonfun$apply$1.apply(Cache.scala:38)\n[error] spray.caching.SimpleLruCache.apply(LruCache.scala:76)\n[error] spray.caching.Cache$Keyed.apply(Cache.scala:38)\n[error] spray.caching.SimpleLruCacheSpec$$anonfun$1$$anonfun$apply$1.apply(SimpleLruCacheSpec.scala:30)\n[error] spray.caching.SimpleLruCacheSpec$$anonfun$1$$anonfun$apply$1.apply(SimpleLruCacheSpec.scala:30)\n[error] spray.caching.SimpleLruCacheSpec$$anonfun$1.apply(SimpleLruCacheSpec.scala:30)\n[error] spray.caching.SimpleLruCacheSpec$$anonfun$1.apply(SimpleLruCacheSpec.scala:28)\n```\n\nThis looks like a test instability unrelated to this PR.\n. OK never mind.\n. ",
    "ppiotrow": "Hi, the correct setting is `spray.can.server.parsing.illegal-header-warnings = off`\n. Hi, the correct setting is `spray.can.server.parsing.illegal-header-warnings = off`\n. ",
    "fleipold": "Ok. Obviously this is a bigger change ;-)\n. Ok. Obviously this is a bigger change ;-)\n. ",
    "janisz": "I found [this](https://groups.google.com/forum/#!topic/spray-user/w0oeFC7eJ_s) and it looks it's not a Marathon issue\n. I found [this](https://groups.google.com/forum/#!topic/spray-user/w0oeFC7eJ_s) and it looks it's not a Marathon issue\n. ",
    "rkoval": "Closing this. I opened a stackoverflow with this concern, and it has been addressed basically as a gotcha on my behalf. \n\nReference: http://stackoverflow.com/a/33535390/1899385\n. Closing this. I opened a stackoverflow with this concern, and it has been addressed basically as a gotcha on my behalf. \n\nReference: http://stackoverflow.com/a/33535390/1899385\n. ",
    "ataraxer": "Akka HTTP also affected:\n\n``` scala\nimplicit val system = ActorSystem(\"http-streaming\")\nimplicit val flowBuilder = ActorMaterializer()\n\nval result = HttpEntity.Chunked.fromData(MediaTypes.`text/html`, Source.empty)\nval route = encodeResponse & path(\"stream\") & complete(result)\n\nHttp().bindAndHandle(route, \"localhost\", 8080)\n```\n\n```\n$ http :8080/stream\nHTTP/1.1 200 OK\nContent-Encoding: gzip\nContent-Type: text/html\nDate: Sat, 05 Dec 2015 22:15:02 GMT\nServer: akka-http/2.3.12\nTransfer-Encoding: chunked\n\n\nhttp: error: ContentDecodingError: ('Received response with content-encoding: gzip, but failed to decode it.', error('Error -3 while decompressing: incorrect header check',))\n\n```\n. Akka HTTP also affected:\n\n``` scala\nimplicit val system = ActorSystem(\"http-streaming\")\nimplicit val flowBuilder = ActorMaterializer()\n\nval result = HttpEntity.Chunked.fromData(MediaTypes.`text/html`, Source.empty)\nval route = encodeResponse & path(\"stream\") & complete(result)\n\nHttp().bindAndHandle(route, \"localhost\", 8080)\n```\n\n```\n$ http :8080/stream\nHTTP/1.1 200 OK\nContent-Encoding: gzip\nContent-Type: text/html\nDate: Sat, 05 Dec 2015 22:15:02 GMT\nServer: akka-http/2.3.12\nTransfer-Encoding: chunked\n\n\nhttp: error: ContentDecodingError: ('Received response with content-encoding: gzip, but failed to decode it.', error('Error -3 while decompressing: incorrect header check',))\n\n```\n. ",
    "shigemk2": "Sorry, I solved this problem.\nMy \"idle-timeout\" was default 60 seconds. \nI found it in reference.conf.\nSo I changed idle-timeout settings from 60 s to infinite.\n. Sorry, I solved this problem.\nMy \"idle-timeout\" was default 60 seconds. \nI found it in reference.conf.\nSo I changed idle-timeout settings from 60 s to infinite.\n. ",
    "mjuchli": "Same error but setting idle-timeout doesn't help for me. Would be thankful for any further optimization tips?!\n. Same error but setting idle-timeout doesn't help for me. Would be thankful for any further optimization tips?!\n. ",
    "sofianito": "Fixed it using: HttpRequest.withHeaders(Host(\"host_value\"))\n\n:)\n. Fixed it using: HttpRequest.withHeaders(Host(\"host_value\"))\n\n:)\n. ",
    "nataliagrybovska": "Customizing the `RejectionHandler` was the last resort as there is no way to somehow override the default one. \nSeems like the only option is to copy the default rejection handler and put logging before route completion, which is extremely inconvenient.\n\nThanks for the help,\nNatalia\n. Fantastic, it works.\nThanks for the help.\n. Customizing the `RejectionHandler` was the last resort as there is no way to somehow override the default one. \nSeems like the only option is to copy the default rejection handler and put logging before route completion, which is extremely inconvenient.\n\nThanks for the help,\nNatalia\n. Fantastic, it works.\nThanks for the help.\n. ",
    "littlejin": "Very Helpful @wjur , issue closed\n. Very Helpful @wjur , issue closed\n. ",
    "dskrvk": "Akka versions < 2.4.17 are [vulnerable](http://doc.akka.io/docs/akka/2.4/security/2017-02-10-java-serialization.html) to [CVE-2017-1000034](https://nvd.nist.gov/vuln/detail/CVE-2017-1000034).\r\n\r\nNot sure how much it could affect Spray, but still seems like a good idea to upgrade.. Akka versions < 2.4.17 are [vulnerable](http://doc.akka.io/docs/akka/2.4/security/2017-02-10-java-serialization.html) to [CVE-2017-1000034](https://nvd.nist.gov/vuln/detail/CVE-2017-1000034).\r\n\r\nNot sure how much it could affect Spray, but still seems like a good idea to upgrade.. ",
    "keynmol": "While I was taking a shower, it kept on: ![](https://www.dropbox.com/s/67vy757841hm69n/Screenshot%202016-04-21%2012.48.24.png?dl=1)\n. While I was taking a shower, it kept on: ![](https://www.dropbox.com/s/67vy757841hm69n/Screenshot%202016-04-21%2012.48.24.png?dl=1)\n. ",
    "jason-ni": "Not sure my case is the same with you. I have a Spray-can reverse proxy service and observed it has memory leaks. In my case, it's the akka memory leaks: https://github.com/akka/akka/issues/19216\nAnd in my test case, there's heavy load of the akka system. \n. Not sure my case is the same with you. I have a Spray-can reverse proxy service and observed it has memory leaks. In my case, it's the akka memory leaks: https://github.com/akka/akka/issues/19216\nAnd in my test case, there's heavy load of the akka system. \n. ",
    "icha024": "Needed this for triggering event and log on failure\n. Needed this for triggering event and log on failure\n. ",
    "axos88": "This was also discussed here: https://groups.google.com/forum/#!topic/spray-user/sB4R7164OZ0\n. This was also discussed here: https://groups.google.com/forum/#!topic/spray-user/sB4R7164OZ0\n. ",
    "nicolasdalsass": "Explained here : http://spray.io/documentation/1.1.3/spray-routing/advanced-topics/understanding-dsl-structure/\n. Explained here : http://spray.io/documentation/1.1.3/spray-routing/advanced-topics/understanding-dsl-structure/\n. ",
    "ametrocavich": "See: https://github.com/ametrocavich/spray/blob/7523ae908c159ec3ab5bd605c68ce1e4474fa251/spray-http/src/main/scala/spray/http/ContentType.scala#L72\n. See: https://github.com/ametrocavich/spray/blob/7523ae908c159ec3ab5bd605c68ce1e4474fa251/spray-http/src/main/scala/spray/http/ContentType.scala#L72\n. ",
    "rgaskill": "@jrudolph thanks!  \n\nI am working around it by re-implementing the `GzipDecompressor` in our code base.\n. @jrudolph thanks!  \n\nI am working around it by re-implementing the `GzipDecompressor` in our code base.\n. ",
    "ianjuma": "spray `1.3.3` should be fine for akka 2.3.4\n. spray `1.3.3` should be fine for akka 2.3.4\n. ",
    "benhoyt": "Actually, the relevant RFC might be https://tools.ietf.org/html/rfc2231, because that's actually about MIME / Content-Disposition headers, but the above still applies, that RFC specifies the `*=` encoding.\n. Actually, the relevant RFC might be https://tools.ietf.org/html/rfc2231, because that's actually about MIME / Content-Disposition headers, but the above still applies, that RFC specifies the `*=` encoding.\n. ",
    "onmomo": "The issue still seems to persist with akka-http 10.0.4. I'll create the ticket over there.. The issue still seems to persist with akka-http 10.0.4. I'll create the ticket over there.. ",
    "dlinov": "Thank you, somehow I missed the fact artifact name changed.\n. Thank you, somehow I missed the fact artifact name changed.\n. ",
    "allen-servedio": "@jangoolie we solved this by adding handleRejections and handleExceptions directives within the respondWithHeader directive.\n\nSo basically something like:\n\n```\nrespondWithHeader(`Access-Control-Allow-Origin`(AllOrigins)) {\n  (handleRejections(rejectionHandler) & handleExceptions(exceptionHandler)) {\n\n    // ... then your routes go within this like:\n      get {\n        path( \"allow\" ) { \n          complete(\"foo\")\n        } ~\n        path ( \"reject\" ) { \n          import spray.http.HttpHeaders.RawHeader\n          complete(\n            Unauthorized,\n            List(RawHeader(\"WWW-Authenticate\", \"Negotiate\")),\n            \"The resource requires authentication, which was not supplied with the request\"\n          )\n        } ~\n        path ( \"secured\" ) { \n          authenticate(BasicAuth(myUserPassAuthenticator _, realm = \"secure site\")) { userName =>\n            complete(\"This will never complete\")\n          }\n        }\n  }\n}\n```\n. @jangoolie we solved this by adding handleRejections and handleExceptions directives within the respondWithHeader directive.\n\nSo basically something like:\n\n```\nrespondWithHeader(`Access-Control-Allow-Origin`(AllOrigins)) {\n  (handleRejections(rejectionHandler) & handleExceptions(exceptionHandler)) {\n\n    // ... then your routes go within this like:\n      get {\n        path( \"allow\" ) { \n          complete(\"foo\")\n        } ~\n        path ( \"reject\" ) { \n          import spray.http.HttpHeaders.RawHeader\n          complete(\n            Unauthorized,\n            List(RawHeader(\"WWW-Authenticate\", \"Negotiate\")),\n            \"The resource requires authentication, which was not supplied with the request\"\n          )\n        } ~\n        path ( \"secured\" ) { \n          authenticate(BasicAuth(myUserPassAuthenticator _, realm = \"secure site\")) { userName =>\n            complete(\"This will never complete\")\n          }\n        }\n  }\n}\n```\n. ",
    "jangoolie": "Thanks for the responses.\n\nAfter reading through a lot of Spray source code I understand now why Rejections and completing a request with a 401 are handled differently, but it's not exactly obvious. It's also a little confusing that respondWithHeaders doesn't apply to rejections.\n\nAnyway I ended up implementing a CORS directive based on the discussion from this thread:\n\nhttps://gist.github.com/joseraya/176821d856b43b1cfe19\n\nBut I added this additional case:\n\n``` scala\ncase Rejected(reasons) if ( \n  reasons.collect{ case AuthenticationFailedRejection(CredentialsMissing, _) => () }.nonEmpty\n) => { \n  val challengeHeaders = reasons.collect { case AuthenticationFailedRejection(CredentialsMissing, ch) => ch }.head\n  context.complete(Unauthorized, challengeHeaders :+ allowOriginHeader :+ allowCredentialsHeaders, rejectionMessage)\n} \n```\n\nSo that the right headers will still be set on 401 responses. Seems to be working now, thanks.   \n. Thanks for the responses.\n\nAfter reading through a lot of Spray source code I understand now why Rejections and completing a request with a 401 are handled differently, but it's not exactly obvious. It's also a little confusing that respondWithHeaders doesn't apply to rejections.\n\nAnyway I ended up implementing a CORS directive based on the discussion from this thread:\n\nhttps://gist.github.com/joseraya/176821d856b43b1cfe19\n\nBut I added this additional case:\n\n``` scala\ncase Rejected(reasons) if ( \n  reasons.collect{ case AuthenticationFailedRejection(CredentialsMissing, _) => () }.nonEmpty\n) => { \n  val challengeHeaders = reasons.collect { case AuthenticationFailedRejection(CredentialsMissing, ch) => ch }.head\n  context.complete(Unauthorized, challengeHeaders :+ allowOriginHeader :+ allowCredentialsHeaders, rejectionMessage)\n} \n```\n\nSo that the right headers will still be set on 401 responses. Seems to be working now, thanks.   \n. ",
    "klpx": "Ok. Thank you!\n. Ok. Thank you!\n. "
}
{
    "creationix": "Sent out surveys to all backers asking for details.\n. Miranda is working on this.\n. Ordered and delivered.  Miranda is handling dispatching.\n. I wonder how much value there is in having interop at the level of the actual files in the `.git` folder.  I know several people showed interest in using js-git server-side to replace shelling out to git for various deployment situations.\n\nAlso I'm not sure that leaning on browserify is such a good idea if it's shims will make it hard to take full advantage of the native APIs in the browser.  I guess it will just depend on what the tradeoffs end up being.\n\nUnfortunately I won't be able to start coding on this for a couple more weeks.  I do hope that I'll be able to take advantage of a lot of @chrisdickinson's great work.\n. I'm travelling all day today (It's Thursday morning in France)  I'll be\ngetting home late tonight in Texas (Thursday night).  Tomorrow morning I\nstart full-time on js-git.  Thanks for the pointers.  Also, what should go\nin this repo?  Should this be the ceramic that consumes the many tiny\nmodules or something?\n\nOn Thu, Apr 25, 2013 at 7:39 AM, Chris Dickinson\nnotifications@github.comwrote:\n\n> okay, so the inflate implementationhttps://github.com/chrisdickinson/inflateis \"fast enough\" for now -- there are a couple of places where it could be\n> sped up, but it doesn't take an hour to clone \"mature\" repos now. I've\n> wired it up locally using levelidb for storage.\n> \n> Next steps:\n> 1. Need a module that given a commit object, a \"find\" function, and a\n>    path, can reconstitute the data for that path.\n> 2. Need to rework the git-walk-tree module so that it can check the\n>    type of an object before recursing into it (to avoid loading all of the\n>    blobs at once).\n> 3. Creating commits.\n> 4. Creating packfiles.\n> 5. Implementing git-receive-pack and git-send-pack similar to how\n>    git-fetch-pack works.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/creationix/js-git/issues/3#issuecomment-16989552\n> .\n. Since we've been working together for the last week and a half and already depend on some of @chrisdickinson's repos, I'm going to close this issue.  See new issues for specific tasks.\n. I think all the stickers have been sent at least once.  I have one letter on my desk that was returned.  I still owe Mozilla a trip to their office when they want it.  I'm going to close this issue since it's done all I can do for now.  I'm glad everyone enjoyed their stickers!.\n. Maybe this would be better served as a milestone?  The biggest obstacle I see is code that's on the real filesystem can't be accessed from the chrome app.  Though I guess a node implementation could do this as a command-line tool.\n. Converted to milestone. https://github.com/creationix/js-git/issues?milestone=2\n. Right now, it's mostly discussion in irc (#js-git on freenode) and code being pushed to various projects at https://github.com/chrisdickinson/.  I'll start pushing a demo app to this repo soon.  We've decided to have many small modules and this repo will be docs and examples for how to use them together.  I'll probably host a pre-built single-js file somewhere that can be dropped into a website when done. (if that makes sense).\n\nAlso I've posted several code snippets to gists and published them via my twitter account and the irc channel.\n. So what happened was Chris already implemented half of the low-level plumbing before I had a chance to even start working on js-git.  So I've been spending the last few days working out how to best reuse his code and not duplicate efforts.\n\nI think much of the porcelain will also be individual repos.  There is a lot of complexity mapping between how git works internally and the interface we're all familiar with.  Git is, after all, just a content-addressable key / value store.\n\nSince IRC isn't accessible to everyone and time zones can be a problem, I'm fine with more discussions being here on github.\n. No js-git will still be a new js library.  Chris is backing with code\ninstead of money to save me time and help the project get farther.  I'm\nwriting code as well.\n\nAlso, I'm not blindly pulling in all his code.  Js-git will be very much my\nstyle and coding standards.  People backed me because they believed I could\nmake this project.\n\nIt shouldn't matter if I build a single monolithic repo from scratch or\nreuse code already written and use a more modular approach.  The end goal\nis the same.\nOn May 1, 2013 7:15 AM, \"John-Philip Johansson\" notifications@github.com\nwrote:\n\n> As a KS backer I feel a bit ... confused. I was expecting one library in\n> one Github repo, kinda like jQuery. I was expecting code to migrate from\n> any other project into this project.\n> \n> Do you mean this JS-Git repo is going to be a documentation and sample\n> project for chrisdickinson's work?\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/creationix/js-git/issues/6#issuecomment-17278783\n> .\n. @mihailik there is a ton of code at https://github.com/chrisdickinson/.  He's been working on this for a long while.  All the code I have is pushed here already.  Remember I did just start working on this last Friday.  Also I'm trying to document the highlights of the irc conversations in github issues here.\n. Yep, since all the repos are merged, gc can only be done on a whole-system basis.\n. I made a quick fs wrapper around requestFileSystem. (only works in chrome).  I think most the semantics are sane.  There are still parts to work out around move and copy commands. https://github.com/creationix/js-git/blob/master/demo/app.js#L47-L155\n. The idea is that this common db interface not include hash logic in every\nimplementation.  Then a higher got layer can do it once and share the code\namong all the db implementations.  Also I may want to store refs and tags\nin the db which aren't keyed by their content hash.\nOn May 15, 2013 11:04 PM, \"Matias Garcia Isaia\" notifications@github.com\nwrote:\n\n> If all objects will be stored in the DB with their hash as the key,\n> wouldn't it be wrong to ask for the key value when storing? Not only it can\n> be calculated by the DB,it HAS TO, for preventing a corrupted key/value\n> pair to be stored.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/creationix/js-git/issues/7#issuecomment-17983588\n> .\n. So @mgarciaisaia ended up being right about the hash being inside the db interface as seen at https://github.com/creationix/js-git/blob/master/specs/git-db.md#savesource---continuablehash.  I should probably close this issue since most these things are now written out in the specs folder.\n. Added a LICENSE file to the main repo.  Most the sub repos all have a license: MIT field in their respective package.json files.\n. Done https://github.com/creationix/min-stream/blob/master/demux.js\n. Done https://github.com/creationix/min-stream/blob/master/dup.js\n. Done\n. This tool will be rewritten using `git-repo` and the new version of git-list-pack and inflate that @chrisdickinson is working on.  Once it's on a more stable foundation we can continue adding features.\n. Rewrote again, this time the main library is here in js-git.  The node platform implementation is in js-git-node-platform, and a sample node CLI tool is at js-git-node.\n. I'm closing this and punting the pack-file storage as an implementation detail of fs-db.\n. Thanks, but these files are obsolete already.  I've been working in several repositories at once.  Later when the code base gets a little more stable we can do this kind of organizing.  At the moment there is nothing depending on this repo.  It's used only for the issue tracker at the moment.\n. Good idea guys. I'll do this when I get home.\nOn Jun 19, 2013 4:48 PM, \"Cory Boyd\" notifications@github.com wrote:\n\n> maybe you can't link to a \"competitor\" Kickstarter\n> \n> There is no reason why you shouldn't be able to link to Bountysource in a\n> Kickstarter update. Do it!\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/creationix/js-git/issues/16#issuecomment-19717489\n> .\n. http://www.kickstarter.com/projects/creationix/js-git/posts/518104\n. The problem with needing a constructor is it assumes a prototype and dependence on a proper \"this\" when calling.  Also I much prefer defining a interface rather than a concrete type.  I don't want to have to subclass `Stream` to create a stream.\n. As far as always requiring that sinks be objects, I think that's unnecessary.  A stream is very often a value you pass around and makes sense to be an object because it has two very distinct channels (data and close).  But a sink it usually either a standalone helper function in some library or a part of some other object.  The only thing I want to standardize is the property name for the sink in the case where it's part of a duplex stream object.  I think `consume` is fine.  I chose `sink` to be consistent with min-streams and also because it's four letters just like `read` and `stop`. \n. Yeah, the fact that read is a continuable is just a coincidence, not an API goal.\n. @Raynos I really like the tcp accepting a transformer instead of just providing a duplex object and ignoring the return value!\n\nI'm fine with `stop` requiring the callback if you feel strongly about it.  It will make the API a little harder to use, but considering it's not a hot path for most programs, that should be fine.\n\n> We should specify the allowed states. A stream is basically zero or more values followed by an end (an end which is either natural or caused by an error)\n\nYes\n\n> We should specify that once the callback fires any call to `.read()` will not return any more values. Optionally specify that when you call `.stop()` and even before the callback fires it should not return any values to the read callback. Basically it would be useful to spec out the relationship between `stop()` and `read()`\n\nThis gets tricky in larger chains.  The source may be done sending events and indeed never send anything after calling the callback to `.stop()`, but the layers after it may still have data pending in the pipeline that eventually comes out.  The `stop` channel is very fast and often will be a direct reference to the stop function in the source.  The spec should then say:\n\nThe source will never output data events after a stop request has been received, but be aware that other layers downstream may still contain data.\n. Though, I guess since technically every layer exports a new \"source\" interface, it would need to observe the same rule for stop and clear it's data queue when stop is called.  This makes chaining stop much harder, hmmmm.\n\n``` js\nmodule.exports = function (source) {\n  // ... code including definition of dataQueue\n  return { read: read, stop: stop };\n  function stop(err, callback) {\n    dataQueue.length = 0;\n    source.stop(err, callback);\n  }\n  function read(callback) {\n    // do stuff\n  }\n}\n```\n. Even truncating `dataQueue`, there can still be pending read calls to the parent source and a flag would need to be set that tells onRead to ignore the value when it finally returns (or call onRead directly or something otherwise crazy)\n. @dominictarr oh cool.  Do you think it would be confusing to use the same names, but have slightly different semantics.  I see leveldown doesn't accept a reason (error) when ending and my data events won't have key and value, just value.  Otherwise it seems very close.\n. I think we could get away with not having `err` in stop/end/close if we slightly changed how error propagation worked.  In the new system, `end` would be used _only_ to notify upstream that we won't be consuming anymore.  We could send an appropriate error event downstream at the same time from wherever the error started.  Upstream would never know about the error, but I think that's fine.  It just needs to know it can safely clean up resources.\n. @dominictarr \n\n> regards a consume method, I think consistency trumps necessity.\n> There are places where you don't care whether something is a complete sink, or just one side of a transform. Also, if a sink is an object, then you can explain sources, explain sinks, and then explain a transform as both a source and a sink.\n> \n> I started using the word \"sink\" to designate a stream that has no readable side - this reflects the usage of the word 'sink' in graph theory http://en.wikipedia.org/wiki/Sink_(disambiguation) . having a method \"sink\" on a transform stream breaks this.\n\nHaving a transform that is an object with both readable and writable ends is interesting. (also much closer to how node works with readable and writable streams)\n\n``` js\nvar source = fs.readStream(\"input.txt\");\nvar sink = fs.writeStream(\"output.txt\");\n\nfunction transform() {\n  return {\n    next: function (callback) { ... },\n    end: function (callback) { ... },\n    consume: function (stream) { ... }\n  }\n}\n\n// Very nice chaining API though\nsink.consume(transform()).consume(source);\n\n```\n\nBut this has issues.  `next` and `end` have no meaning and nothing to pull from until `consume` is called setting up their data source.   To enable chaining, `consume` could return the thing it just consumed.\n\nThe sink will start pulling from the transform before it has a chance to connect to it's source.  This would mean all `next` functions would need an extra state to handle this early call and defer calling the callback.  The nice syntax may or may not be worth this cost. (keep in mind most transforms would already have internal queues, flags and checks.  Adding one more isn't too bad in most cases)\n\n``` js\nvar source = fs.readStream(\"input.txt\");\nvar sink = fs.writeStream(\"output.txt\");\n\nfunction transform(source) {\n  return {\n    next: function (callback) { ... },\n    end: function (callback) { ... }\n  };\n}\n\n// Very simple consuming API\n// note that I'm still expressing the sink as an object even though the transform is just a function.\nsink.consume(transform(source));\n```\n\nIf a transform was modeled as a function that accepted a source and returned a new source, it would be more straightforward.\n\nNow duplex streams (as opposed to transform filters) are modeled great as a single object with `{next, end, consume}` and app logic where symmetry is needed is easy to model.\n\n``` js\nvar jsonCodec = require('json-codec');\nvar lineCodec = require('line-codec');\n\ntcp.createServer(8080, function (socket) {\n  // socket is a \"duplex\" stream with {next, end, consume}\n  // Apply protocol de-framing and framing on the duplex stream\n  socket = lineCodec(socket);\n  // Apply JSON parsing and Serialization to the duplex stream\n  socket = jsonCodec(socket);\n  // App is a simple echo server, so echo objects back\n  socket.consume(socket);\n\n  // or written as a single expression\n  socket.consume(jsonCodec(lineCodec(socket))\n});\n```\n\nBut if we make the TCP library act like a filter itself, then socket is no longer duplex.\n\n``` js\nvar json = require('json-codec');\nvar line = require('line-codec');\n\ntcp.createServer(8080, function (socket) {\n  // Written as a sequence of actions\n  socket = line.deframe(socket);\n  socket = json.decode(socket);\n  socket = app(socket);\n  socket = json.encode(socket);\n  socket = line.frame(socket);\n  return socket;\n\n  // Or written as one expression:\n  return line.frame(json.encode(app(json.decode(line.deframe(socket)))));\n});\n\nfunction app(stream) {\n  // Just an echo server\n  return stream;\n}\n```\n. @raynos, so then looks like you vote for modeling transforms as `function (stream) -> stream` and letting external libraries make it pretty (basically what I've been doing for min-streams all along)\n. Ok, so recap with the latest API proposal:\n\n``` js\nvar stream = {\n  next: function (callback) { ... },\n  end: function (callback) { ... }\n};\n\nfunction transform(stream) {\n  // ...\n  return { next: next, end: end };\n  function next(callback) { ... }\n  function end(callback) { ... }\n}\n\nvar duplexStream = {\n  next: ...\n  end: ...\n  consume: ...\n};\n```\n\nA transform function could be a duplex transform and accept a duplex stream and return a new duplex stream.  Though it's probably better to write transforms as separate encode and decode.  A generic duplex transform that accepted encode and decode would be easy to write.\n\n``` js\nfunction duplex(decode, encode) {\n  return function (original) {\n    var transformed = decode(original);\n    transformed.consume = function (source) {\n      original.consume(encode(source));\n    };\n    return transformed;\n  };\n}\n```\n. Usage of `duplex` above would be:\n\n``` js\n// Create a duplex stream\nvar socket = tcp.connect(1337);\n\n// The protocol is framed, let's remove that layer.\nsocket = duplex(line.deframe, line.frame)(socket);\n\n// The protocol is also JSON encoded\nsocket = duplex(json.decode, json.encode)(socket);\n```\n. Hmm, I just realized this `duplex` helper is just a weaker version of the `series` helper from above from the point of view of the consumer.  It works differently inside because it's working with duplex streams and not simple streams though.\n. > sinks need more of a spec. in particular, I didn't know what how to communicate completion/errors out to client, non-stream code. I'm +1 on @Raynos's suggestion.\n\nIn min-streams, sinks return continuables for exactly this reason.  It's worked out great so far in my js-git code.  The continuable will resolve with the end/error event in the stream.\n\n> it feels much easier to get right than min-streams.\n\nThat's the goal.\n\n> is it the responsibility of the sink to call .end on its input stream to allow for cleanup? or is stream.end() more exceptional than that?\n\nNo, you only call `end` if you want to end the stream early.  Perhaps calling is `end` is confusing with the `end` in node's writable stream interface?\n\n> what do we do with extra read callbacks? the dom example has a filter stream module that collects a list of callbacks. right now I simply truncate that buffer on end, then forward the end. is this correct?\n> there's more boilerplate than using through, but impressively, not much.\n\nAfter sleeping on it, I think it's best to say that calling `.end()` doesn't guarantee the data stream will stop right away.  That adds too much boilerplate and extra code to each and every layer for little gain.  Also I don't think the source needs to insert an end event into the stream when `.end()` is called.  The \"end\" event in the stream is for natural ends.  If a filter in the middle wants to cleanup something it needs to listen for both \"end\" events and the callback to \".end(callback)\" since either could end the stream (and both may happen sometimes).\n\nAs far as truncating callbacks, just make sure to _always_ eventually call every callback.  It really messes up programs when callbacks never get called.\n. I think there needs to be very little interaction between `read` and `abort`.  If you try to read from a source that's been stopped, it will simply emit end.  So I take back what I said about pieces in the middle needing to intercept the abort call.  Everyone can just keep reading from their source till value is `undefined` and then know the stream is done.\n\n``` js\nvar stream = {\n  read: function (callback) { /* callback(err, value) */ },\n  abort: function (callback) { /* callback(err) */ }\n};\n\nfunction transform(source) {\n  return { read: read, abort: source.abort };\n  function read(callback) {\n    source(function (err, value) {\n      if (value === undefined) return callback(err);\n      callback(null, value.toUpperCase());\n    });\n  }\n}\n```\n. Also I think I want to model tcp streams as stream transforms instead of duplex streams.  Adding in duplex complicates the model a lot.\n\n``` js\n// Echo server\ntcp.createServer(8080, function (socket) {\n  return socket;\n});\n\n// Echo client\ntcp.connect(8080, function (socket) {\n  return socket;\n});\n```\n. As to @chrisdickinson's question about the cases where you really need a writable interface (where normal imperative logic is better than a state-machine transformer):\n\n``` js\ntcp.createServer(8080, function (input) {\n  var output = writableSource();\n  // output has both readable and writable interfaces:\n  // { read(callback), abort(callback), write(value, callback), end(err, callback) }\n  // or { read(callback), abort(callback), emit(err, value, callback) }\n  // The exact interface for writable doesn't matter because it's not part of the simple-stream spec.\n  return output;\n});\n```\n\nThough you usually also want your input to be pushed to you in the app case, so the push-filter interface is best here I think.\n\n``` js\ntcp.createServer(8080, pushToPull(function (emit) {\n  // Call emit(err, item) every time we want to write data outwards.\n  return function (err, item) {\n    // Called every time data is written to us\n  };\n}));\n```\n. I don't see transforms as streams.  That's just a pattern I've seen done in node where there are duplex streams.  Transforms can be modeled as duplex streams, but I don't feel it properly represents them.\n\nTo me, having only streams be objects is quite symmetrical and clean.  The _only_ basic type here is the stream.  It's the only thing in the spec that everything else has to agree on.  Things like sources, filters (transforms), and sinks are simply functions that either accept or return streams. Push-filters are just an easy way to create normal stream consuming filters and are really not part of the spec.\n- source = function that returns a stream (and accepts optional stream setup arguments)\n- filter = function that accepts a stream and returns a stream (and may have additional option arguments)\n- sink = function that accepts a stream (and returns a continuable)\n. So to further explain, let's take some known APIs from the non-streaming world.  A string is like a stream, but instead of through time, it's a stream of bytes through memory, but all seen in an instant.  If I wanted to take a string of JSON and transform it into the object that it represents, I don't use an object to do that conversion, I use a function.\n\n``` js\nvar obj = JSON.parse(json);\n```\n\nLikewise if I have a stream of raw json strings through time and want a new stream of parsed objects through time, I would do the same thing.\n\n``` js\nvar objStream = json.decode(jsonStream);\n```\n. The signature of `JSON.parse` is `(jsonString) -> object` and the signature of `json.decode` is `(stream<jsonString>) -> stream<object>`.\n. @dominictarr basically yes, and I changed the error handling slightly so that you don't send a reason when aborting the stream and you don't wait for the source to reflect the reason, but send your own error downstream directly.\n. Ok, I've updated the official spec to reflect these changes.  It's looking real clean. https://github.com/creationix/js-git/blob/master/specs/simple-stream.md\n. @dominictarr also, since sink isn't part of the stream spec, you're free to implement your transforms/filters as objects if you want.  I can still interop as long as we all use the same interface for the streams themselves.  As for me personally, I much prefer filters being functions that accept and return streams.  I aim to write all js-git related filters as functions.\n. @dominictarr \n\n> so, I can think of a few situations where the reason might be important. example: on tcp you want to know if the stream failed because there was no server, or if it dropped the connection, or it timed out.\n\nSorry if I didn't explain right, but of course you need the reason downstream.  And you'll still have it, that's what the `err` argument in read's callback is for.  I was talking about the `err` argument that would be in abort before the callback.\n\nAll of these error cases you mentioned would still be reported and come out of the continuable that the sink returns.\n. The \"reason\" that's not important is for a source to know why it's consumer is going to no-longer consume from it.  It doesn't care why, it just needs to know so it can clean up stuff.  It's downstream, the consumer, that cares why stuff is broken.\n. @Raynos \n\n> > sink = function that accepts a stream (and returns a continuable)\n> \n> It might make sense for sink to be a function that returns an object with a consume method for purposes of structural typing. That consume method then accepts a stream and returns a continuable\n\nThere is no reason you can't do that.  I just don't want to force such a verbose construct in the spec since it's not needed or even wanted most the time.\n\nStructural typing matters more for anonymous things that are passed around and used as return values and arguments all the time.  Streams definitely fall under this category.  Sinks are more like API endpoints that consume streams.  I don't think they need structural typing as much.  I know that `fs.writeStream(stream, path, options) -> continuable` is a sink because of it's API docs, it's name, and it's documented signature.\n. So usage would be:\n\n``` js\nfs.writeStream(path, options).consume(stream)(callback);\n```\n\nvs\n\n``` js\nfs.writeStream(stream, path, options)(callback);\n```\n\nWith the first one, I feel an urge to use a promise instead of a continuable so that it's `.consume(stream).then(callback)`\n. And I'll say, well if you insist on taking-no-args-and-then-returning-an-object-that-has-a-prototype-that-has-a-method-that-accepts-a-callback-and-an-errback-and-optional-progressback instead of just take-the-callback, then I guess you insist on complexity and better change the name away from \"simple streams\"\n. @Gozala I was wondering when/if you would comment on this thread.\n\nYes, I agree that the only interface that needs to be specified is the readable stream.\n\nAs far as using special tokens for END, ABORT, and Error classes, I'd rather not.  `instanceof Error` doesn't work if the error is from another context.  There is no Error.isError helper function though Object.prototype.toString.call(err) === \"[object Error]\" seems to be reliable.  I'd hate to force such a verbose type check on each and every data chunk that goes through the stream.  Having two positional arguments tells us a lot that speeds up such checks.\n\nYes back-pressure can be done with a manual side-channel and pause and resume commands, but I much prefer the implicit backpressure provided by pull style.  In my experience I'm much more likely to get it right if I'm using pull-streams than writing the back-pressure by hand using manual pause and resume.\n\nYes there are general helpers that convert between types.  I am publishing a module right now called push-to-pull that lets you write the easier push filters, but use them as back-pressure honoring pull-filters without writing your own queues.  A reduce transform could easily be written as could a filter transform.\n\nThanks for the input.\n. @mhart I'm glad you like `abort`.  You can thank @dominictarr for convincing me to add that to the official spec.  I really didn't want to.\n\nI've also considered what a stream would look like if we had access to ES6 generators.  I think the simplest construct would be a generator that yielded values.\n\n``` js\nfunction* source() {\n  yield 1\n  yield 2\n  yield 3\n}\n\n// Consume like any other generator to get [1, 2, 3]\n```\n\nBut like most I/O streams, you can't yield everything at once, so the generator could yield continuables instead of raw values.\n\nBy happy coincidence, simple-stream's read function is itself a continuable.  So turning a simple-stream into a generator based stream is as simple as:\n\n``` js\nfunction* () {\n  // Create a simple-stream\n  var stream = fs.readStream(\"myfile.txt\");\n  // and yield it forever\n  while (true) yield stream.read\n}\n```\n\nIn fact my [gen-run](https://github.com/creationix/gen-run) library does something very much like this, but as a control-flow helper library.\n\n``` js\nrun(function* () {\n  var stream = fs.readStream(\"myfile.txt\");\n  var data;\n  var items = [];\n  while (data = yield stream.read) {\n    items.push(data);\n  }\n  return items;\n});\n```\n\nI don't want to require generators for streams since it will be a long time before most js environment can assume ES6 generators.  I am, however, very aware of how they will interact and keep these things in mind.\n. @gozalla, I'm having trouble understanding you.  I do like the idea of the main data channel being _only_ data and letting everything else go through a meta channel.\n\nI'm pretty sure I want pull based for several reasons.  Besides the natural back-pressure it provides, it also provides a nice 1:1 mapping between continuations chains since each callback will be called only once for each read call.  This makes tracing and error handling much easier.  I'm currently working on improving domains in node.js and wish that everything in node had this nice 1:1 mapping.  It makes for a very simple and robust system when every stack has a direct and obvious parent stack that initiated it.\n\nI know that you can't pause some inputs easily (like user clicks or http requests), but that doesn't mean pull-streams are a bad idea.  You just buffer the events at the source waiting for someone to pull them.  Even those cases can usually be paused somewhat in extreme cases (you could disable the UI button if the stream wasn't ready to handle it or tell the TCP socket to stop accepting connections)\n\nAlready have two data channels in the form of two arguments to onRead callbacks `(err, item)`.  When `item` is anything other than `undefined`, then it's a data item.  Otherwise, it's a meta value that signifies natural end or error end.  The channel for closing an upstream source goes the other direction and so can't be encoded here.\n\nDo you have any ideas that are modifications to the current design that could simplify this?\n. Thanks!\n. Btw, others use continuables too, but yeah, there aren't too many of us. https://github.com/Raynos/continuable/blob/master/spec.md\n. Also, be advised that I recently modified the API to push-to-pull in the 0.1.x release.  I updated the docs, test, and code to reflect the change.\n. API changes https://github.com/creationix/push-to-pull/commit/f1c77934ba9f24ffaad0e22f7a58c9cfcb70fdef\n. I'm going to close this issue here.  Push-to-pull has been working without any problems for a while.  But feel free to add more unit tests and create an issue on the push-to-pull repo if you wish.\n. Closing since we don't use sub-stream anymore.  I decided it's not worth the performance cost and complexity to stream object bodies.\n. So, two of the main goals of jsgit are to be lightweight and extremely portable (running on a variety of platforms and storage and network backends).  The C libraries make a lot of assumptions about the platform they run on that make that extremely difficult.  Also many of the C libraries that would be useful to me are GPL or LGPL licensed and I want to keep js-git more open using MIT.\n\nThe main technical problem with enscripten, even if the C libraries were exactly what I needed, is that it generates a _lot_ of javacsript.  I don't want 2mb of generated code.  I want something hand-coded so that it can be a lot leaner.  Remember this is just a library and not an entire application.  It's just a part of someone else's project.\n\nAlso the emscripten toolchain is a pain to setup, especially on platforms like ChromeOS where I expect people to use js-git a lot.  I would really like it if people could hack on js-git on the same platforms that use it.  There is no emscripten (not one that runs at usable speeds at least) in browser-style environments.\n. I'm not sure, I'm using the ssh2 module from https://github.com/mscdex/ssh2\n\nIf you want to take a shot at it, options get passed into the ssh wrapper at https://github.com/creationix/js-git/blob/master/protocols/ssh.js#L31 and the node.js ssh wrapper is at https://github.com/creationix/js-git-node-platform/blob/master/ssh.js\n. @dominictarr did you ever figure this out?\n. So the reason it thinks all the files were deleted is because the `.git/index` file is missing.  `repo.checkout` is supposed to create that file after extracting the tree from the commit, but it doesn't seem to in this case.  I can reproduce locally.\n\nAlso, github lets you clone using ssh on my repo?  I thought that only worked if you have write access.  That's really cool if they added read-only support for the ssh protocol.\n. As far as ssh not working in 0.11.x I suspect that's because mcsdex/ssh2 hasn't been tested for that version of node.  It still seems to export the old streams interface from back in node 0.6.x\n. So I found two issues, both on github's end. (though both I could workaround if needed).\n1. If you override the `Agent` header to be what the git cli tool sends (`git/1.8.1.2` instead of `jsgit/0.2.3`), it will give a redirect instead of a 404\n2. It redirects.  I haven't implemented redirect following because url parsing isn't cross-platform yet.  Why don't they provide the proper url for you to clone?\n\nIf you clone using the proper url, it works fine, even **without** changing away from my custom agent header.\n\n``` sh\nTRACE=1 jsgit clone http://gist.github.com/gist/036c175d18a8a692a89d.git milestones\n```\n\nNotice the `/gist/` at the front of the path.\n. So for redirects, I just need more logic in smart-http.  But the problem with url parsing is I'm using the \"url\" module from node.js.  In http redirects it gives the new url as a string, but smart-http works in terms of already parsed urls.\n\nThere is a strict line between what's allowed to be used in js-git proper and what js-git-node-platform can use.  Anything exported by the platform can be used by js-git, but that then means all ports of the platform have to implement that interface as well.\n\nUrlparse is pure javascript.  Maybe we could just include the source from node.js in the browser ports and make it part of the platform interface?\n. Actually, since the node url parser doesn't work for all git urls anyway, (It can't handle urls like `git@github.com:user/repo.git`), I should just include the url parser in `helpers/urlparse.js`.\n. @stuartpb yes, I'm sending the proper content type.  It's sniffing on the user agent.  I'm implementing redirect support and a way to send a custom useragent.  That should make it possible to clone from gists.\n. Added hack in http code for gist. https://github.com/creationix/git-net/commit/708721e0cbdde6d6ec9355aa7e96df19571a16d2\n\nNow to implement redirect...\n. Done!\nhttps://github.com/creationix/git-net/commit/da8c27d478bcc32a851e0d22645be1f8a4580e3a\n. Waiting on bountysource to fix a bug so that I can access the names.\n. https://github.com/creationix/git-browser\nApp in Firefox Marketplace is still pending review.\n. https://github.com/creationix/git-browser\nhttps://chrome.google.com/webstore/detail/git-browser/cladogmhjppclibenkdbnjcogiaifnbd\n. This is now implemented in the newly refactored js-git code.\nhttps://github.com/creationix/js-git/blob/master/js-git.js#L593-L688\nhttps://github.com/creationix/git-net/blob/master/fetch.js#L14-L44\n. I wonder if your git version is too old for the index format I'm writing. Are you able to upgrade and try with a newer git?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  From: Florent JabySent: Saturday, August 10, 2013 5:37 AMTo: creationix/js-gitReply To: creationix/js-gitSubject: [js-git] after cloning, git can't read index (#30)I successfully cloned one of my repos via https. but running git status in the repo results in a error message like this:\n\nfatal: Unknown index entry format 28dc0000\n\nBecause I also have some git prompt addition the error messages come after each command.\n\n$ git --version\ngit version 1.7.10.4\n\nI installed js-git-node from npm. I cloned \n\n\u2014Reply to this email directly or view it on GitHub.\n. Sorry I read the version wrong. That's not very old.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  From: Florent JabySent: Saturday, August 10, 2013 3:38 PMTo: creationix/js-gitReply To: creationix/js-gitCc: Tim CaswellSubject: Re: [js-git] after cloning, git can't read index (#30)it's the git version in the apt repositories for ubuntu 12.10. so I suppose\n\nthe same version is used for 12.04 (LTE) I can try and get some newer git\n\nand tell you how that goes.\n\nLe 10 ao\u00fbt 2013 18:47, \"Tim Caswell\" notifications@github.com a \u00e9crit :\n\n> I wonder if your git version is too old for the index format I'm writing.\n> \n> Are you able to upgrade and try with a newer git? From: Florent JabySent:\n> \n> Saturday, August 10, 2013 5:37 AMTo: creationix/js-gitReply To:\n> \n> creationix/js-gitSubject: [js-git] after cloning, git can't read index\n> \n> (#30)I successfully cloned one of my repos via https. but running git\n> \n> status in the repo results in a error message like this:\n> \n> fatal: Unknown index entry format 28dc0000\n> \n> Because I also have some git prompt addition the error messages come after\n> \n> each command.\n> \n> $ git --version\n> \n> git version 1.7.10.4\n> \n> I installed js-git-node from npm. I cloned\n> \n> \u2014Reply to this email directly or view it on GitHub.\n> \n> \u2014\n> \n> Reply to this email directly or view it on GitHubhttps://github.com/creationix/js-git/issues/30#issuecomment-22443014\n> \n> .\n\n\u2014Reply to this email directly or view it on GitHub.\n. A past version of js-git had this for the clone.  I wrote a barebones index so that git status wouldn't complain about stuff breaking.  \n. @maks, here is my old code that was writing the index. https://github.com/creationix/js-git/commit/2baa4f9f821ac2a2af4e3e9c82fc3f30e08b0aea \n. Actually I just got done cleaning up and organizing everything.  This repo now houses the core logic that is command-line and browser agnostic.  It consumes various simple interfaces and implements git on top of them.\n\nThe node CLI tool is in the `js-git-node` repo with the node version of the various js-git interfaces implemented in the `js-git-node-platform` library.  That way the `js-git` can be used from node without being dependent on the CLI program.\n\nI'm still working on a browser version.\n\nI do apologize that the code has been less organized so far.  I prioritize getting work done over keeping things clean.  Especially when I don't even know what I want most the time.  It is starting to take shake and firming up though.  This is why I announced the project on the node list recently.  It's time for people to start consuming the library through the CLI tool to help iron out bugs and platform issues.  Right now I'm working on creating a small browser based app to test that side of the abstraction.\n. Good idea.\n\nSo for a bare local repo, all that's needed is the git-db interface.  Inflate and Deflate are internal implementation details for that.  Network communication is a little trickier.  For clone/fetch/ls-remote the platform needs to provide tcp, http, and/or ssh interfaces (depending on which protocols you want to support).\n\nIf you want to use the built-in fs-db library, then you do need inflate/deflate/sha1 as well as an fs instance.\n\nIf you want to have a working directory, you need to provide an fs instance as part of the git-db interface.  You don't have to use the fs-db adapter library.  You could do this with only indexdb by emulating the fs on top of the indexdb system, but using indexdb directly for the git-db implementation.\n. I don't there would be such a high-level library, and if there was, it would be simply:\n\n``` js\nrequire('js-git/lib/platform.js')(require('js-git-node-platform'));\nmodule.exports = require('js-git');\n```\n\nAssuming that js-git eventually exports a central thing.\n\nAll the high-level APIs will either be in js-git itself or in js-git addon modules, both which would be platform independent I think.\n. I documented this in the main README at the js-git repo.\n\nWe'll be adding more as we implement the other milestones.\n. Yes, but it's a ways off.  Maybe milestone #4 can be custom git servers.\n. Actually this will be part of milestone #2 and I'm working on it today.  Hopefully I get it done soon.\n. This was harder than expected (and I got sick) so it's still not done.  When I announce milestone#2 done, then you will be able to do this.\n. Thanks for writing this up.  I very much value use cases to know which direction to prioritize.  This can be `milestone #4` if that's soon enough (a few months out). Right now I'm working super hard to finish `milestone #1` since that alone will be enough for many people's use cases.  A large part of this work is creating packfiles which is currently part of `milestone #2`.\n\nOne comment on ssh.  It's probably best to reuse the existing ssh server in servers.  That's how real git works.  What I would be re-implementing is the various shell commands that get spawned.  These mini processes would then need to connect to the main git server process and talk to it on the client's behalf.\n\nNow it would be more efficient if node was also also the ssh server and only understood git commands.  But then your server wouldn't have an ssh server for remote logins anymore (unless node handled that too).\n\nAlso I would like to keep a js-git powered client as a separate task as a js-git powered server.  Your use case may require both ends to be node powered, but many others do not.\n. I have a sample js-git server I'm using to develop the server-side of js-git.  I just finished a basic version of `repo.receivePack` that allows you to push to js-git powered servers!\nhttps://github.com/creationix/js-git/blob/master/examples/serve.js\n\nIt's still missing the protocol \"report-status\" extension that will be needed for post-commit hooks though.\n. Done!\nhttps://github.com/creationix/git-net/commit/4d5f962e844d49193b87e4c9913da386a370b2b9\nhttps://github.com/creationix/git-net/commit/3037b111cdd9ea06d64cfab0d1f3ea3a59fc1f78\n. Now implemented in refactored js-git core.\n. This is now done.  The final design is slightly different from the diagram, but the idea is the same.  The main modules are:\n- https://github.com/creationix/js-git - Core logic for local git repositories\n- https://github.com/creationix/git-net - Network logic for working with remotes\n- https://github.com/creationix/git-node-platform - Platform implementation shimming native node.js APIs\n- https://github.com/creationix/git-pack-codec - Packfile encoding and decoding (only decoding at the moment)\n- https://github.com/creationix/git-fs-db - Adapter for converting fs instances to git-db instances.\n. Yes it's on the TODO list, but not scheduled yet.  Which of the 4 schedules milestones do you feel this fits in?\n\nKeep in mind that I won't have packfile _writing_ of any kind in milestone 1, but perhaps reading of existing git repos could fit in there.  I just don't want to block the other work on something most people won't need.\n. Ok, I'll tack it to the end of my milestone 1 tasks.  I'm hoping to get that milestone done this week before I travel to StrangeLoop, but it's getting pretty close.  Maybe I'll finish at the conference or while traveling.\n. Sorry for delaying this issue.  I think it will be the last issue in milestone #2 after I complete all the network stuff.  With this and the network stuff done, js-git will be feature complete enough to be the backend for things like howtonode.org.\n. Done\nhttps://github.com/creationix/git-net/commit/9d7facd79895c4e3025046ab1417999a7dd38f61\nhttps://github.com/creationix/git-net/commit/a4ad7bb3214227f2f5af24fdbc1229cbaabf85d5\n. https://github.com/creationix/git-node/commit/c2f619ef2ce0c89439b9097219945678d1b01593\n. Started https://github.com/creationix/jsgit/commit/3cf5387a7b606f42acb5e967ddc9cabb2e5af246\n. Done, now just pending docs.\n. Re-opening since the APIs changed again.\n. Updated and new versions of app submitted to both the stores and new web version published to web.\n. Implemented in:\nhttps://github.com/creationix/js-git/commit/3df0854d1c792bf2ee4a239991538ea0cbe45a10\n. Implemented in:\nhttps://github.com/creationix/js-git/commit/3df0854d1c792bf2ee4a239991538ea0cbe45a10\n. Actually this was a problem with git submodules.  FIxed in https://github.com/creationix/js-git/commit/9d44902c03beecbb05bfb83fe9451d6c0a69d13c\n. This was a problem with the unpack algorithm.  It had assumed that all deltas were unique, but this isn't always the case.  Fixed in master.\n. Fixed in https://github.com/creationix/git-node-platform/commit/444923217ed23bc8e51fb25685c6f874ead3eae0\n. Documented repo and db interfaces, those are the most public facing ones.\n. Done and sample deployed to http://git-browser.creationix.com/\n. https://github.com/creationix/git-web-platform/tree/master/build\n. Did you try with the new chrome app support in cordova?  I was unable to get it to run at all, but I really like the push to chrome adt workflow if it actually worked.\n. Yeah, when using git:// protocol there is just one connect per clone/pull command.  I hope they improve their code soon.\n. That would be great.  The main thing I need is the most minimal set of files to create a winjs app, the most minimal build tools (preferably something that could be done on a chromebook or using well-supported web-services)\n\nThen I would need adapters for the primitives.  These APIs are still changing, but for git-browser it's tcp, http, and storage.\n. A leveldb based backend would great for node as well!  If there is a browser version of the the api, then the leveldb adapter could be used there too.\n\nBut I agree with @aaronpowell that minimal dependencies are important for web stuff where bandwidth and performance are often limited.\n. @aaronpowell, to test, clone the git-browser app and replace the localStorage backend in the web variant with your indexedDB implementation.  That will test 90% of the code I think.\n\nTo run the web version, just build using the included `make` file and build the `web` target.  Then serve the `build/web` folder using a http server of your choice to test.\n. Looks great.  I'm closing this issue for now since it's at least as functional as the other backends.  Feel free to make further improvements on your own.\n\nThanks for the contribution!\n. This is almost done.  I still need to implement `have` and `deepen` commands in the negotiation phase.\n\nAlso the packer isn't optimal.  It doesn't support many of the extensions and doesn't search for ref-deltas to create.  That just means we'll more bandwidth than real git, but less CPU.\n. This is now implemented as `repo.receivePack()` in `mixins/server.js`.\n\nThe current form doesn't implement the \"report-status\" capability, but does allow for git clients to push to a js-git server.\n. You would create the child directory first since it's hash will be part of the entry in the parent directory.\n\nSo using generator style, the code would look something like:\n\n``` js\nvar hash = yield repo.saveAs(\"tree\", [\n  { mode: 0100644, name: \"one.js\", hash: yield repo.saveAs(\"blob\", one_js_body) },\n  { mode: 040000, name: \"sub\", hash: yield repo.saveAs(\"tree\", [\n    ...\n  ]) },\n  ...\n]);\n```\n. ![create_repo](https://f.cloud.github.com/assets/89353/1430624/b77d9a10-40c3-11e3-82ac-feafcf3e83eb.png)\n\nThe UI now has placeholder menu items for this.  They are activated by right-clicking on the empty area of the tree.\n. ![capture](https://f.cloud.github.com/assets/89353/1466190/6f3165b2-456d-11e3-81cc-af5b77218dd1.PNG)\n\nCreating a new local repository is now done.  Next up, implementing cloning existing repos.\n. Done!\n\n![capture](https://f.cloud.github.com/assets/89353/1466829/608db880-4577-11e3-9f4f-cb0f1ebfab58.PNG)\n. Deployed live server to http://tedit.creationix.com/ that has working websocket-git proxy.\n. ![folder-ops](https://f.cloud.github.com/assets/89353/1430638/f1be47ce-40c3-11e3-8334-178da0edc555.png)\n![file-ops](https://f.cloud.github.com/assets/89353/1430643/00d515e4-40c4-11e3-96fd-042968eaa0b2.png)\n\nUI placeholders for folder and file ops in place.\n. ![capture](https://f.cloud.github.com/assets/89353/1466082/9ebe70c4-456b-11e3-880e-61c4bd4911af.PNG)\n![capture2](https://f.cloud.github.com/assets/89353/1466051/4af69c28-456b-11e3-95e8-8b89db1093fb.PNG)\n\nImplemented everything except rename.  Almost done.\n. Now done!  Tedit has full CRUD capabilities for easy git testing.  Test/play at http://creationix.com/tedit/\n. Most the examples are pure client side.  See the https://github.com/creationix/git-browser and https://github.com/creationix/tedit demo apps for examples. \nGit-browser supports multiple environments such as web (with a local node websocket proxy), chrome apps, and firefox os apps (with other ports in progress).\n\nSince each and every platform has different APIs for basic things like SHA1, TCP, HTTP, and persistence, js-git is platform agnostic and expects you to provide it with an implementation of the abstract platform interface.\n. The thing is that browsers on their own are incapable of talking to most git servers.  Browsers only support websockets and HTTP.  Most git hosts support HTTP, but they don't implement CORS.  So unless your website is hosted _on_ github itself, you won't be able to talk to it using XHR.  The browser's domain security restrictions will deny the connection.\n\nI've asked Github to enable CORS on their http git endpoints, They told me they were reviewing the security implications and I haven't heard back from them.\n\nThe reason there isn't a single browser-ready version of js-git is because there is no clear-cut configuration that works for everyone.  Every platform and use case is unique.  There are literally hundreds of configurations that I've seen people and platforms need.\n\nNow github does have CORS on their other HTTP APIs.  You can use those, but they aren't git protocol and aren't supported by js-git.\n. The recommended configuration for now is either:\n1. If you're a webpage, you can run your own web server and host a websocket proxy to get around the cross domain issues.  A live example is at http://git-browser.creationix.com/\n2. You can submit your app to the various app stores and get access to more powerful APIs.  I have example code that runs standalone in chrome packaged apps as well as firefox os apps.  These apps don't need a server or node at all to talk to git servers.\n. What was the error?\n. Also the code is being refactored at the moment. Sorry for the temporary unstability.\n. Thanks!\n. Also, I keep forgetting to tag you on twitter. https://twitter.com/creationix/status/399926262644240384\n. Neat idea, but I'm not sure I want to automatically do this.  I like keeping the API simple and low-level.  Also, I noticed you're calling a db function that doesn't exist in the interface.\n. Yep, it's all in flux, and git-net will soon be deprecated to migrate to the new, better architecture.  This should all be stable soon as part of finishing milestone#2\n. The 0.6.x series is pretty buggy.  It will all be resolved for milestone#2.  If you want, you can create issues against milestone#2 and that way we'll make sure the things get fixed.  But most of these I'm aware of and will fix as soon as I get time.\n. Yeah, it's there.  I've been pretty sick the last few weeks.  Sorry for the delay.\n\nhttps://github.com/creationix/js-git/blob/master/mixins/server.js#L175\n. Sorry, this got paused while I do paying work.  It's one of the first\nthings to get done when I have time for js-git again.\n\nOn Wed, May 14, 2014 at 1:30 AM, Alexandru Vl\u0103du\u0163u <notifications@github.com\n\n> wrote:\n> \n> @creationix https://github.com/creationix hey, any updates on this? I\n> see the server.js file has been removed since your last comment.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/creationix/js-git/issues/74#issuecomment-43046704\n> .\n. I'm thinking this only makes sense for blobs.  Maybe we could add a new type to `loadAs` and `saveAs` called `\"stream\"`.  Then in the db interface we would add optional `getStream` and `setStream` primitives to the interface.  For backends that are unable to implement streaming, js-git will just use the non-streaming interface and buffer internally.\n\nWhat do you think?\n. Interesting.  If you have other API ideas, I'm open.  I will mention that\nwith some backends like js-github you are required to know what kind of\nobject you're loading before loading it.  I get around this in js-github by\ncaching hash <-> type information wherever I can. (If you have a hash, you\ngot it from somewhere and probably already know the type from where you got\nit).\n\nOn Fri, Dec 13, 2013 at 3:44 PM, Aria Stewart notifications@github.comwrote:\n\n> Seems reasonable; I'd love streaming for trees too but it's just for\n> consistency in that case.\n> \n> loadAs is a weird interface though -- you have to know the type ahead of\n> time, and since trees can contain both trees and blobs, that's weird.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/creationix/js-git/issues/83#issuecomment-30546145\n> .\n. Because it uses this api: http://developer.github.com/v3/git/  Which\nrequires the type in the request.\n\nOn Mon, Dec 16, 2013 at 3:10 PM, Aria Stewart notifications@github.comwrote:\n\n> Oh, interesting -- since github doesn't expose raw git in a\n> browser-accessible way, eh?\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/creationix/js-git/issues/83#issuecomment-30700923\n> .\n. Yes it's possible, but it still a lot of work from where we are today.\n\nOn Thu, Dec 12, 2013 at 6:22 PM, norman784 notifications@github.com wrote:\n\n> Checking your repos see that you have git-browserhttps://github.com/creationix/git-browserand my question here is (since yesterday chrome launch there offline /\n> desktop apps) you think there can be a way that a chrome app use this to\n> work as I want, thats basically is a project manager that runs on the\n> browser and sync your files via git, a example (but not really what I want\n> to accomplish, but can compare with it) can be dropbox, or services that\n> sync files between computers / accounts and the cloud. The only thing that\n> I'm not sure are the sync part the other things like auth and all those its\n> a pancake. The app flow will be something like this\n> - List projects\n> - Select project\n> - Clone it (git clone) and save locally in the user selected folder\n> - Edit there files\n> - Update (git push, if needed a rebase or similar, need to think how\n>   to manage this because the target users are not smart, an app for dummies\n>   xD)\n> \n> Regards\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/creationix/js-git/issues/84\n> .\n. Right now, we have clone, and low-level operations on the local git repo.  There is no working directory and no way to push changes back to a remote server.  https://tedit.creationix.com/ and http://git-browser.creationix.com/ are examples of what can be done now.\n. A chrome app will have all abilities once js-git is done.  It has pretty good network and persistence primitives so that's no a problem like is it in browser apps.\n\nThe main issue is js-git is not done.  Chrome apps can access the filesystem as of chrome 31, they have indexeddb and the chrome.storage apis.  They have direct TCP and UDP client and server access as well.\n. Thanks for the report.\n. Thanks, but the issue is with fs-db, not js-git.  I originally modeled this API after the implementation in fs-db which used nested directories.  But after working with all the other backends, I decided to make it not care about folders.  The refs are new treated like strings instead of nested directories.\n. Correct, there is no built-in index in js-git.  But manually creating a commit and a tree is quite simple.\n\nOne example is an old tool I wrote that scrapes a http based database, extracts the sqlite contents, parses the html and json and writes json files to a tree structure.  The goal was to run in on a schedule and create commits whenever the root tree changed. https://github.com/creationix/gospel-git-library\n\nFor some simpler code that shows how to create commits, the examples folder in js-git is a good place. https://github.com/creationix/js-git/blob/master/examples/create.js\n\nAlso I recently wrote a simple tool that imports trees from a filesystem and gives you back the root hash of the imported data. https://github.com/creationix/giblush/blob/master/importfs.js\n\nWith all the techniques, the basic idea is the same.  You import some data into the git database.  Eventually you'll have the hash to the root tree of your new data.  You then create a new commit who's parent is the last commit and who's tree is the tree.  After you create this commit, you move the relevant head ref to point to it. https://github.com/creationix/js-git/blob/master/examples/create.js#L51-L55\n\nI hope this helps.\n. So in case it wasn't clear.  You just need to import your data and check the root tree hash.  If the hash has changed from the last commit, then something has changed since the last import and you can create a new commit and update the head to save the changes.\n. There is no concept of detached head yet.  That only makes sense when you have an index and working directory (all part of milestone #3).  Currently what's implemented is essentially equivalent to a bare git repo (what git servers usually have).  All versions and commits are there at the same there.  There is no currently checked out version.\n. Good catch!\n. There is no config at this level.  You set the name and email when creating a commit.  In my tedit-app, I store username and email in local chrome storage (chrome app version of localStorage)\n. I do have some really basic code used by tedit-app that parses and encodes git config files.  I use it in tedit to read and write .gitmodules files for managing submodules. https://github.com/creationix/js-git/blob/master/lib/config-codec.js\n\nThe latest generation of js-git code doesn't yet have a filesystem backend that uses git compatible `.git` folders.  JS-Git is generally not compatible with command-line git on the same local system. (though there is nothing from stopping someone from creating that on top).  Since my main use case is places where real git doesn't exist, I'm compatible with the network protocols and use the same hashing algorithms so that people can share repos between js-git and real git.\n. Btw, the config codec has been improved and has docs and unit tests now.\n\nhttps://github.com/creationix/js-git/blob/master/doc/lib/config-codec.md\n. @ljharb what do you mean?\n. I see.  I really feel this is backwards though.  If you want to consume js-git using ES6 promises, you have promises and already have the polyfill.  If you want to consume js-git using node callbacks or continuables, then you don't want the bloat of a 2k minified polyfill included.\n. In other words.  I'm willing to make js-git play nice in a promise environment, but it's outside the scope of js-git to provide such an environment if it doesn't exist already.\n. @Bartvds that's essentially what I'm doing.  I'm just using `window.Promise` as the entry point to pass in the promise implementation.  I don't see this as an issue since I'm only planning to support ES6's Promise API.  Is there ever a case where a user would have `window.Promise` but want to use a _different_ Promise constructor in js-git?\n. It probably wouldn't be hard to have a mixin that went through a set of known API signatures and turned them into promise based APIs.  The harder task would be to automatically convery *any* api already mixed in.. Go for it and let me know how it works.. Fixed in 77f1a82466c23dced0d0a68ccb97213a7edf4092\n. Fixed in https://github.com/creationix/js-git/commit/7bf8da57b8e56d51ea7fd21faf986f0111bbc1b9\n. Wow, thanks.  I can't believe I had that typo.\n. It's on the roadmap, but currently incomplete.  I'm not actively working on js-git right now because I ran out of money and need to refill my bank.\n\nGetting push and pull working via HTTPS is probably easier, but I think it would be possible to implement enough ssh to clone using the TLS code in the forge project.\n. So it's technically possible to speak git over https using `XMLHttpRequest`.  The main problem is none of the git hosting companies are willing to add CORS headers to allow the cross-domain request.  Also specefic to bitbucket is this issue, but it looks like it's being resolved. https://bitbucket.org/site/master/issues/6666/detect-git-requests-by-content-type-header\n\nWhile chrome extensions can't use the raw socket API, they can bypass cross-domain restrictions.\n\nI had written this extension a while ago that allowed me to clone from a browser. https://github.com/creationix/corson\n. If you're worried about privacy, then using a websocket to ssh proxy is a serious man-in-the middle unless you're implementing all of ssh in the browser.\n\nI actually designed such a proxy for git to use (was planning on doing cross-domain https over it, but ssh would work too).  https://github.com/creationix/git-proxy\n\nMy proxy does support being a TLS link which means it can see all traffic going through it.  This works great for cloning public resources that don't need a password, but is a problem for private repos or pushing commits.  My plan eventually was to use the forge project to implement TLS in the browser.  I've already implemented HTTP in JS a few times.\n\nThe other problem with a proxy is the wasted network resources and dependence on new infrastructure.\n\nThe ideal solution would be for the git providers to simply add CORS headers, but I've been asking for years.  I'm pretty sure github will never do this, but maybe BitBucket will if you ask nicely.\n. Correct, if you're making the XHR calls from your extension you can just request cross-domain permission in the manifest.  My use case was an extension that can be enabled on a per-site basis by the user that bypasses the CORS restrictions.  To simulate what would happen if the providers had added it themselves.\n\nYou don't need the user's password to auth with HTTPS, I'm pretty sure github at least will allow user tokens which can be managed the same as ssh keys.  I'm not sure about bitbucket though.\n\nAs far as actually doing git over xhr, I never completed that with js-git since I hit the hard technical roadblocks with lack of CORS.  I had a prototype of cloning, but never did pushing.\n\nThat said, there are a couple other git in JS implementations out there that use xhr in chrome apps and extensions with the requested domain exception.  You'll see mention of them in the issue I linked at bitbucket.  I think everyone is using the library written for Taylor.\n\nYou could do an SSH to Websocket proxy where the user deploys their own proxy and embeds their private key in it.  That would work well.  Then the only problem is authenticating the browser so that not just anyone can access it.  But any authentication scheme will work, even http basic auth.  It's up to the dev deploying the proxy and implementing the website.  They would need `wss://` and not plain `ws://` on the proxy or whatever they are using to authenticate would be leaked to anyone listening.  This means deploying with an SSL/TLS cert.\n. Thanks, but I don't want node-specific code in this repo.  I created a new repo for the node version at https://github.com/creationix/git-node-fs.  It would be good to model it after the chrome html5 version I currently use in tedit at https://github.com/creationix/git-chrome-fs.\n. If you could reduce this PR to just fixes to the docs comment and create a new PR for git-node-fs, that would be great.\n. Thanks.\n. Implemented.  Basic app now works on safari on desktop.\n. Yep.  If you're in node.js, you can mount the repo using https://github.com/creationix/git-node-fs.  If you're in a chrome packaged app, you can use https://github.com/creationix/git-chrome-fs.\n. Do note that it expects the actual git database, so point to the `.git` folder.  Also note that it ignores your staging area and working files.  I recommend using `--bare` clones with this so that there are no extra files laying around to confuse you.  But on the other hand, having the working dir lets you do things that bare repos aren't allowed to do (like manually merge remote changes).\n. Yeah, it's meant to be installed from git.  I just tagged and pushed to npm as version 0.0.1, but it's dependency is also listed as a git dependency.\n\nYou can install from git using:\n\n``` sh\nnpm install git-node-fs@git://github.com/creationix/git-node-fs.git\n```\n. Actually, nevermind.  I just removed the dependency.  You can now install it from npm or from git without problem.\n. The fs-db backend can read and write disk repos.  The node version is at https://github.com/creationix/git-node-fs\n. Yes, you can read files from the browser, but you can't scan directories.  Many files (pack files for example) are unguessable filenames.  As long as your repo doesn't use pack-files (most do), it would be possible to write a custom backend using nothing than xhr requests.\n. Another option would be to add support for the metadata http://git-scm.com/docs/git-update-server-info provides.  Then a \"dumb\" client could read the git repo with nothing more than xhr requests.  You'd have to externally make sure the extra metadata is kept up to date whenever the git repo is modified.\n. I'm fine with this change assuming the two notes are fixed.  I also notice you submitted a PR to git-node-fs.  I also need the same change made to git-chrome-fs.\n\nThe API it uses appears to also support rename, so it should be fine. http://www.html5rocks.com/en/tutorials/file/filesystem/#toc-copy-rename-move\n\nhttps://github.com/creationix/git-chrome-fs/blob/master/lib/chrome-fs.js\n\nOnce `git-chrome-fs` and `git-node-fs have the fix, I'll merge the fixed change here.\n. Are you sure that's the one you meant to tag?  Doesn't that depend on the other two repos adding rename?\n. Thanks.  I didn't notice the other two. Looks great.\n. js-git is packaged as common-js.  You'll need to use some sort of build system to pre-process the files for use in the browser.  I usually either do it server-side in wheaty https://github.com/creationix/wheaty-cjs-bundler or client side. https://github.com/creationix/tedit/blob/master/build/ui/loader.js\n\nYou could probably use browersify too, but I haven't tried in a while. http://browserify.org/\n. Here is a example of using wheaty-cjs-bundler. https://github.com/creationix/conquest/blob/svg/conquest.js\n\nThis repo is served with wheaty https://github.com/creationix/wheaty.  Since the file is marked as executable in git and starts with `#!js` it will be run server-side in wheaty and serve the result to the browser.\n. I'm glad it helped.  No, I'm not going to rewrite in AMD.  I may rewrite in\nES6 modules once the tool ecosystem is more mature for them though.\n\nOn Wed, Sep 10, 2014 at 2:50 AM, Jouke Waleson notifications@github.com\nwrote:\n\n> Thanks, browserify seems to do the trick! I'll see if I can get a working\n> example and send a pull request with added documentation about build\n> instructions. Do you have any thoughts about rewriting from common-js to\n> AMD?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/creationix/js-git/issues/105#issuecomment-55082019.\n. You can do mist everything over the rest protocol, its just that its non standard buggy and really really slow for clones.  Also for any sizable repo, you'd hit the rate limit long before finishing a full clone.\n\n-----Original Message-----\nFrom: \"aaron\" notifications@github.com\nSent: \u200e9/\u200e17/\u200e2014 5:14 PM\nTo: \"creationix/js-git\" js-git@noreply.github.com\nSubject: [js-git] Github CORS requirement clarification (#106)\n\nI've been asking Github to enable CORS headers to their HTTPS git servers, but they've refused to do it. This means that a browser can never clone from github because the browser will disallow XHR requests to the domain.\nThey do, however, offer a REST interface to the raw git data.\nUsing this I wrote a mixin for js-git that uses github as the backend store.\nI am trying to understand if the lack of CORS support is limiting, if you are able to use the raw git data API. I guess this just necessitates the js-github adapter as opposed to using the normal git communication mechanisms. is it otherwise feature complete? Does this include pushing commits back to github?\n\u2014\nReply to this email directly or view it on GitHub.=\n. No.   I've never gotten a reason of why they won't do it.  I either get no\nresponse or they say they are looking into it.  I've been trying for almost\n18 months now.\n\nOn Thu, Sep 18, 2014 at 12:58 PM, aaron notifications@github.com wrote:\n\n> Ah ok, this does seem like a problem. Did the github team give any\n> specific pushback on why they would not enable CORS?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/creationix/js-git/issues/106#issuecomment-56077589.\n. Sorry I didn't see this issue for some reason.\n\nFetch and clone is mostly done, yes it needs more docs.\n\nDepending on the backend there are different ways to initialize js-git.\n\ncommits can easily be created manually using saveAs and updateRef\n\nDiff isn't implemented at all.  I haven't started this.\n\nMerge depends on diff so it's also not started\n\nBranching can be done manually by simply creating new refs.  A branch in git is literally nothing more than a new bookmark.\n\npush isn't quite done.  I've been fighting the restrictions of web pages for years and trying to get the git hosting companies to open up CORS.  I want to do push correctly without leaking people's credentials.\n. Github has always had CORS for their proprietary API, but that's not what git uses to push over https or ssh.  You can use the github api via js-github.  This is what tedit.creationix.com does when you sync a github mount.\n. @nickj12497 please see #122 for a discussion on capabilities and examples.\n. What a great idea!  Can a chrome app register an url handler and do arbitrary action based on it?  I thought the protocol handlers could only return a url.\n\nAs far as cloning, most the needed code is there already.\n. Also keep in mind that chrome apps can create http servers using\nchrome.sockets and a lot of js.\nOn Oct 31, 2014 7:34 PM, \"Kyle Graehl\" notifications@github.com wrote:\n\n> Hmm after doing a little searching I found out there may be a small\n> improvement to be made in this process. Instead of using\n> chrome.runtime.sendMessage it looks like this can bypass that step:\n> https://developer.chrome.com/apps/manifest/url_handlers\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/creationix/js-git/issues/109#issuecomment-61348976.\n. Thanks.\n. You're right.  For a readme, I should use the public paths, not internal paths.\n. Thanks for the fix.  I'm sorry the README errors cost you some time.\n. Actually, diff is not part of git core.  The internal database stores the full version of every file and folder in the history.  The way that git keeps from exploding in size is via packfiles.  js-git doesn't yet support writing packfiles, but it can read them.  Yes packfiles use some form of binary diff internally, but this data is useless for the kind of diff you want to calculate.\n\nSo using one of the filesystem backends to js-git (https://github.com/creationix/git-node-fs for example), you can mount an existing git repo and read it using js-git.  Then using this, you can walk the graph manually using the low-level `.loadAs(type, hash)` api.  Also I provide a couple of higher-level APIs via the walker mixin (https://github.com/creationix/js-git/blob/master/mixins/walkers.js).  `logWalk` lets you walk the history as a flattened linear list of commits (sortes the branches by date).  The `treeWalk` walks the trees and blobs of a commit letting you do a depth first traversal of the tree as a linear stream.\n\nI'm afraid you'll have to manually track the committer and calculate diffs on your own.  I haven't implemented this logic yet.  It might be easier to find some libgit2 bindings for node that I assume would provide the backends to commands like git blame.\n\nSo, yes, js-git can give you the raw data about commits, history, files, contents, etc, but won't calculate composite information like diff, blame, per-file history, etc.\n. I have no idea.  JS-Git is a set of modules that implement various interfaces, the core interface is the .saveAs, .loadAs, etc calls.\n\nJS-Git can use real git repos as backends using the git-fs backend.  There is a concrete implementation using node's FS apis at https://github.com/creationix/git-node-fs\n. JS-Git is super low level, It can easily read local repos, but it doesn't serve them over network protocols or give you user management APIs.\n. Ouch, thanks!\n. Thanks!\n. Sorry, I haven't looked at this in quite a while.  I do remember that clone-test was working over XHR (as long as I installed the browser extension to inject the CORS headers).  It is possible that your code has a bug though I can't see it right away.\n\nI've never seen that trick of using Buffer.concat in the data loop (I usually build a normal array and then concat once inside end), but I think your style should work fine as well.\n\nIt's also possible that something in js-git broke since I was testing clone-test.  Maybe compare the last date I pushed to clone-test with the date of the last js-git and related publishes?\n. Sounds good I think.  Like I said before, I haven't touched this code in years.\n. Thanks!\n. Let me work backwards from where I am today.  It's been a couple years since I've worked on the original js-git.  I recently ported lua-git to use duktape javascript and an example can be found at https://github.com/creationix/dukgit/blob/master/test.js#L58-L64\n\n``` js\n  db = mount(\".git\");\n  var queue = [\"HEAD\"];\n  while (queue.length) {\n    var commit = db.loadAs(\"commit\", queue.shift());\n    p(commit);\n    queue.push.apply(queue, commit.parents);\n  }\n```\n\nBut this does require duktape coroutines which will not work in any other JavaScript VM.  Once async/await lands I can port to that pretty easily.\n\nThe lua-git has a similar API at https://github.com/creationix/lua-git/.\n\nJS-Git was my first API in this regard and is _very_ modular because of the unique constraints I was trying to solve.  The README for https://github.com/creationix/git-node-fs is the simplest way to get started with js-git and node.  Use that to mount a repo.\n\n``` js\nvar repo = {};\nvar path = path.join(\"some/bare/repo.git\");\nrequire('git-node-fs/mixins/fs-db')(repo, path);\n```\n\nOnce you have a repo, loading commits and walking them is pretty easy using direct `repo.loadAs(...)` calls like I do in dukgit. https://github.com/creationix/js-git#basic-object-loading\n\nBut there are also helpers for some common tasks like iterating the history or a file tree as a linear list. \n\nhttps://github.com/creationix/js-git#using-walkers\n\nSo a complete example showing how to walk history and files would be:\n\n``` js\nvar path = require('path');\nvar run = require('gen-run');\n\n// use gen-run to block on contunuables (mini promises) using ES6 yield.\nrun(function*() {\n  // Create a repo object\n  var repo = {};\n  // Mixin the base DB operations using local git database on disk.\n  require('git-node-fs/mixins/fs-db')(repo, path.resolve(__dirname, \"../.git\"));\n  // Mixin the walker helpers.\n  require('js-git/mixins/walkers')(repo);\n\n  // Look up the hash that master currently points to.\n  var commitHash = yield repo.readRef(\"master\");\n\n  // Create a log stream\n  var logStream = yield repo.logWalk(commitHash);\n\n  // Looping through the stream is easy by repeatedly waiting on `read`.\n  var commit, object;\n  while ((commit = yield logStream.read())) {\n\n    console.log(commit);\n\n    // We can also loop through all the files of each commit version.\n    var treeStream = yield repo.treeWalk(commit.tree);\n    while ((object = yield treeStream.read())) {\n      console.log(object);\n    }\n  }\n});\n```\n. Currently js-git is basically just git core, this is simple and you can learn it very quickly.  There are 4 kinds of objects, they are commit, tag, tree, and blob.  Tags are used for annotated tags.  You'll find them used by some projects for release tags.  The tag ref will point to a tag object instead of a commit object.  The tag will contain metadata about the release and link to the commit.\n\nhttps://git-scm.com/book/en/v2/Git-Internals-Git-Objects\n\nCommits point to a tree object and zero or more parent commits (merge commits typically have two parents, but there can be more!).  They also have extra metadata like tags do.\n\nA tree contains a list of pointers to other trees, blobs, or commits (submodules).  Each entry will have a name, mode, and hash.\n\nA blob is just raw binary data.\n\nSo using `repo.loadAs` you can manually walk the tree looking for things.\n\nJS-git doesn't have support for the working directory so you can't do things like `git status`.  Also I haven't implemented diff yet so anything involving that can't be done including merges.  The network actions like push and pull aren't done either.  The main thing you can do currently with js-git is read/write the core git database directly.\n. I would like to take another stab at this, but I'm in a different situation now.  I don't need money anymore, I have a well paying salaried job.  My current limitation is time since any time I spend on js-git will take away from time with my children.  That said, there are some nights I can't sleep because of a mild sleep disorder and I will often work on stuff while they sleep.\n\nAs far as technical direction, I learned a lot from the failure, that was the js-git fundraiser.  My goals were too ambitious and the browser tech wasn't ready for what I was trying to make.\n\nToday, we have async/await coming soon and transpilers that let you use it now.  Also with service workers coming along we have much better offline support.\n\nAs far as I know, browsers still have no way to make direct TCP connections (for git://) to git servers and no git hosting company has added CORS headers to their smart-https (https://) endpoints.  I've designed a great websocket based git protocol (wss://) that can be easily deployed if you host your own git repos or could be setup as a proxy to other git servers and would love if it became used eventually by hosting services like Github.\n\nInitially I focussed on browser-based apps using only native browser I/O and direct connections to services (which is still impossible), but if we relax the requirements a little and allow proxies for browser apps, custom hosted git services, or node.js or chrome app clients, then the network constraints are no longer an issue.\n. @kmalakoff technically speaking, ssh is no harder than `git://` or `http://` since they all require a TCP connection which browsers don't have, but practically speaking, implementing ssh on top of tcp is a _lot_ more work than implementing just `git://` or even `http://`.\n\nImplementing `https://` is simply `http://` over TLS.  I've implemented HTTP in pure js and lua multiple times, that isn't hard, especially for the reduced use case of git clones.\n\nThere is a pure js implementation of tls that I've been using at https://github.com/digitalbazaar/forge, also I've been recently discovered https://tls.mbed.org/ which can probably be compiled to JS using emscripten.  My experience with emscripten, however, is that it generates massive JS blobs (hence the primary motivator for web assembly).\n\nI believe that implementing enough of ssh to do a git clone over it isn't that bad.  Also on the node side, you have the openssl bindings available and can use something like https://github.com/mscdex/ssh2.\n. Actually, thinking about it, about the only use case where you'd be interested in reading ssh keys from `$HOME/.ssh/` is on a full developer workstation running Linux or OSX.  In those cases you have access to node or are probably using electron if writing a graphical native app so still have node access.\n\nWhile it is possible to generate ssh keys in a browser or chrome app and store them in some persistent storage, I doubt it will be the default workflow for most people.  I would expect https be used more in those use cases.\n. The real git clients uses an index file to cache data for status, see the format at https://github.com/git/git/blob/master/Documentation/technical/index-format.txt\n. The issue I had before with the index file approach was that not all the filesystem backends I was targeting supported fine grained stat data. (Chrome's FS API for example)\n\nIt's hard to abstract because some backends will want to use the git index format while others will have to invent something custom to their abilities.\n. So the two-level abstraction is what I've planned to do all along.  The higher level abstraction will consume the lower-level abstraction.  In this case, there will be something for working checkouts and it only needs to know if a file has changed, the git index format on disk is an implementation detail of one of the backends.  But many platforms can offer a good FS level implementation and for those, it would be good for js-git to bridge the gap between fs and working directory for you.\n\nI already do this with git-fs which implements the js-git database interface on top of a generic file system interface.  You don't have to use git-fs if you're on some strange platform where it doesn't make sense.\n\nFor example, I had a backend that stored objects and refs in indexeddb in the browser without first emulating a filesystem.  That would be unneeded overhead and complexity.\n\nSo to answer your question, the git-fs module needs to implement the needed high-level APIs for doing status, add, checkout and other working directory commands.  We also need to design what exactly those APIs are, but not hard-code them to the filesystem.  Users of js-git will be able to choose implementing the high-level interface directly or just implementing the low-level fs API and using git-fs to bridge the interface gap.\n. Sorry for the rambling, I've been sick this weekend and don't have a lot of brain power left.  But I hope this directs you in the right direction.  Feel free to propose added APIs as needed in the high-level space.\n. Yes, I think you're right.  Sorry about getting the names wrong (it's literally been years since I worked on this stuff).\n\nMost the mixins and programs use the db interface directly, there isn't much in the way of super high-level APIs yet (no merge, diff, blame, per-file history, etc).  The tedit project implements a FS on top of the js-git db interface (https://tedit.creationix.com/ https://github.com/creationix/tedit)  The \n\nI am giving a talk in Paris in a month (http://www.dotjs.io/) and hope to  be using JS-Git for a some of it.  This means I'll have some time to use this code myself and hopefully get things a little farther along.\n. I'm not sure I want to change the official js-git to typescript, but I think you're doing something cool.  I can certainly see the benefits you derive from this.\n. You'll want to have the protocol docs handy for sure. https://github.com/git/git/blob/master/Documentation/technical/pack-protocol.txt\n\nhttps://gist.github.com/schacon/6092633\n\nPart of fetch (the main part) is parsing a pack file and streaming the objects to the js-git data store.  If I remember correctly, this needs raw loading and saving objects (used to apply delta objects).  I've written code for clone a few times in a few different places.  I even once had a node command-line tool that would clone using node + js, it's published to npm at https://www.npmjs.com/package/jsgit and you can probably find the code by installing it. (I'm pretty sure it's for an older incarnation of js-git though).\n\nThe only parts of js-git that are truly useful and re-usable are the codecs.  Parsing and writing git objects, applying binary deltas given two buffers, streaming parsing of pack-files, etc.  Feel free to change the rest to whatever works best.\n. At one point, I had a demo working that could clone over XHR (as long as you installed a browser extension to get around the cross-domain issue Github refuses to fix).\n\nhttps://github.com/creationix/clone-test\n\nhttps://github.com/creationix/corson\n. Clone and pull are the same at the protocol level (they are both fetch), but what you do locally with the refs is what makes them different.  Push is a similar protocol, but I never implemented it because I never had a secure channel from the browser to github.  Cloud hosted proxies work\\* for cloning public stuff, but nor for pushing changes because then the proxy can man-in-the-middle and get your credentials.\n\n*work - as is it technically works, but it's a pain to host a proxy just to read a public resource.\n. If working code is easier for you, so be it, but git tends to use a lot of mem-mapping that isn't possible in JavaScript.  I personally had better luck just reading the protocol [spec](https://github.com/git/git/blob/master/Documentation/technical/pack-protocol.txt) and then implementing from scratch, but then again, I'm not that good at reading other people's code yet.\n. I'm not sure what the issue is. (Sorry I'm sick and can't think clearly at the moment), but I just noticed you're using promises and await.  You might have a better time wrapping the continuable instead of using the callback interface.\n\nFor example, make a function that converts continuables to full promises.\n\n``` js\nfunction c2p(fn) {\n  return new Promise(function (reject, resolve) {\n    return fn(function (err, result) {\n      if (err) { return reject(err); }\n      return resolve(result);\n    };\n  };\n}\n```\n\nThen you can do:\n\n``` js\nvar result = await c2p(logStream.read());\n```\n. By \"binary\" do you mean \"decimal\"?  Also since these are just constants, I'm fine with using parseInt.  The performance cost will be one-time for the most part.\n. Thanks.  I guess I've never ran across a case that exercises this code.  How did you run across it?\n. Also master was missing some commits.  I think everything is merged in now and `0.7.8` is published to npm.\r\n\r\nLet me know if this helps.\r\n\r\nAlso I have some newer git code that I wrote for a recent project I used rollup on.\r\n\r\nhttps://github.com/creationix/revision/blob/master/src/libs/git-codec.ts\r\n. Good luck fellows!. And sorry I never had the time to finish this project.  Gotta pay the bills and I couldn't get enough funding to finish this.. So you just want real only of public repos that allow cloning over the smart http protocol?  What about the plain TCP git:// protocol? . Back when I was working on this, I had written this extension that added CORS to github response headers . https://github.com/creationix/corson\r\n\r\nNow to see if I can find the repo that powered the test site.. Found it.  See if this helps. https://github.com/creationix/clone-test\r\n. Yep you're right.  Thanks for digging in.  I completely forgot about this and wouldn't have been able to help.\r\n\r\nhttps://github.com/creationix/js-git/blob/master/mixins/pack-ops.js#L16-L18\r\n\r\n. I am confused. If you could check the modified version in the README, let me know if it works.. Sent out surveys to all backers asking for details.\n. Miranda is working on this.\n. Ordered and delivered.  Miranda is handling dispatching.\n. I wonder how much value there is in having interop at the level of the actual files in the `.git` folder.  I know several people showed interest in using js-git server-side to replace shelling out to git for various deployment situations.\n\nAlso I'm not sure that leaning on browserify is such a good idea if it's shims will make it hard to take full advantage of the native APIs in the browser.  I guess it will just depend on what the tradeoffs end up being.\n\nUnfortunately I won't be able to start coding on this for a couple more weeks.  I do hope that I'll be able to take advantage of a lot of @chrisdickinson's great work.\n. I'm travelling all day today (It's Thursday morning in France)  I'll be\ngetting home late tonight in Texas (Thursday night).  Tomorrow morning I\nstart full-time on js-git.  Thanks for the pointers.  Also, what should go\nin this repo?  Should this be the ceramic that consumes the many tiny\nmodules or something?\n\nOn Thu, Apr 25, 2013 at 7:39 AM, Chris Dickinson\nnotifications@github.comwrote:\n\n> okay, so the inflate implementationhttps://github.com/chrisdickinson/inflateis \"fast enough\" for now -- there are a couple of places where it could be\n> sped up, but it doesn't take an hour to clone \"mature\" repos now. I've\n> wired it up locally using levelidb for storage.\n> \n> Next steps:\n> 1. Need a module that given a commit object, a \"find\" function, and a\n>    path, can reconstitute the data for that path.\n> 2. Need to rework the git-walk-tree module so that it can check the\n>    type of an object before recursing into it (to avoid loading all of the\n>    blobs at once).\n> 3. Creating commits.\n> 4. Creating packfiles.\n> 5. Implementing git-receive-pack and git-send-pack similar to how\n>    git-fetch-pack works.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/creationix/js-git/issues/3#issuecomment-16989552\n> .\n. Since we've been working together for the last week and a half and already depend on some of @chrisdickinson's repos, I'm going to close this issue.  See new issues for specific tasks.\n. I think all the stickers have been sent at least once.  I have one letter on my desk that was returned.  I still owe Mozilla a trip to their office when they want it.  I'm going to close this issue since it's done all I can do for now.  I'm glad everyone enjoyed their stickers!.\n. Maybe this would be better served as a milestone?  The biggest obstacle I see is code that's on the real filesystem can't be accessed from the chrome app.  Though I guess a node implementation could do this as a command-line tool.\n. Converted to milestone. https://github.com/creationix/js-git/issues?milestone=2\n. Right now, it's mostly discussion in irc (#js-git on freenode) and code being pushed to various projects at https://github.com/chrisdickinson/.  I'll start pushing a demo app to this repo soon.  We've decided to have many small modules and this repo will be docs and examples for how to use them together.  I'll probably host a pre-built single-js file somewhere that can be dropped into a website when done. (if that makes sense).\n\nAlso I've posted several code snippets to gists and published them via my twitter account and the irc channel.\n. So what happened was Chris already implemented half of the low-level plumbing before I had a chance to even start working on js-git.  So I've been spending the last few days working out how to best reuse his code and not duplicate efforts.\n\nI think much of the porcelain will also be individual repos.  There is a lot of complexity mapping between how git works internally and the interface we're all familiar with.  Git is, after all, just a content-addressable key / value store.\n\nSince IRC isn't accessible to everyone and time zones can be a problem, I'm fine with more discussions being here on github.\n. No js-git will still be a new js library.  Chris is backing with code\ninstead of money to save me time and help the project get farther.  I'm\nwriting code as well.\n\nAlso, I'm not blindly pulling in all his code.  Js-git will be very much my\nstyle and coding standards.  People backed me because they believed I could\nmake this project.\n\nIt shouldn't matter if I build a single monolithic repo from scratch or\nreuse code already written and use a more modular approach.  The end goal\nis the same.\nOn May 1, 2013 7:15 AM, \"John-Philip Johansson\" notifications@github.com\nwrote:\n\n> As a KS backer I feel a bit ... confused. I was expecting one library in\n> one Github repo, kinda like jQuery. I was expecting code to migrate from\n> any other project into this project.\n> \n> Do you mean this JS-Git repo is going to be a documentation and sample\n> project for chrisdickinson's work?\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/creationix/js-git/issues/6#issuecomment-17278783\n> .\n. @mihailik there is a ton of code at https://github.com/chrisdickinson/.  He's been working on this for a long while.  All the code I have is pushed here already.  Remember I did just start working on this last Friday.  Also I'm trying to document the highlights of the irc conversations in github issues here.\n. Yep, since all the repos are merged, gc can only be done on a whole-system basis.\n. I made a quick fs wrapper around requestFileSystem. (only works in chrome).  I think most the semantics are sane.  There are still parts to work out around move and copy commands. https://github.com/creationix/js-git/blob/master/demo/app.js#L47-L155\n. The idea is that this common db interface not include hash logic in every\nimplementation.  Then a higher got layer can do it once and share the code\namong all the db implementations.  Also I may want to store refs and tags\nin the db which aren't keyed by their content hash.\nOn May 15, 2013 11:04 PM, \"Matias Garcia Isaia\" notifications@github.com\nwrote:\n\n> If all objects will be stored in the DB with their hash as the key,\n> wouldn't it be wrong to ask for the key value when storing? Not only it can\n> be calculated by the DB,it HAS TO, for preventing a corrupted key/value\n> pair to be stored.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/creationix/js-git/issues/7#issuecomment-17983588\n> .\n. So @mgarciaisaia ended up being right about the hash being inside the db interface as seen at https://github.com/creationix/js-git/blob/master/specs/git-db.md#savesource---continuablehash.  I should probably close this issue since most these things are now written out in the specs folder.\n. Added a LICENSE file to the main repo.  Most the sub repos all have a license: MIT field in their respective package.json files.\n. Done https://github.com/creationix/min-stream/blob/master/demux.js\n. Done https://github.com/creationix/min-stream/blob/master/dup.js\n. Done\n. This tool will be rewritten using `git-repo` and the new version of git-list-pack and inflate that @chrisdickinson is working on.  Once it's on a more stable foundation we can continue adding features.\n. Rewrote again, this time the main library is here in js-git.  The node platform implementation is in js-git-node-platform, and a sample node CLI tool is at js-git-node.\n. I'm closing this and punting the pack-file storage as an implementation detail of fs-db.\n. Thanks, but these files are obsolete already.  I've been working in several repositories at once.  Later when the code base gets a little more stable we can do this kind of organizing.  At the moment there is nothing depending on this repo.  It's used only for the issue tracker at the moment.\n. Good idea guys. I'll do this when I get home.\nOn Jun 19, 2013 4:48 PM, \"Cory Boyd\" notifications@github.com wrote:\n\n> maybe you can't link to a \"competitor\" Kickstarter\n> \n> There is no reason why you shouldn't be able to link to Bountysource in a\n> Kickstarter update. Do it!\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/creationix/js-git/issues/16#issuecomment-19717489\n> .\n. http://www.kickstarter.com/projects/creationix/js-git/posts/518104\n. The problem with needing a constructor is it assumes a prototype and dependence on a proper \"this\" when calling.  Also I much prefer defining a interface rather than a concrete type.  I don't want to have to subclass `Stream` to create a stream.\n. As far as always requiring that sinks be objects, I think that's unnecessary.  A stream is very often a value you pass around and makes sense to be an object because it has two very distinct channels (data and close).  But a sink it usually either a standalone helper function in some library or a part of some other object.  The only thing I want to standardize is the property name for the sink in the case where it's part of a duplex stream object.  I think `consume` is fine.  I chose `sink` to be consistent with min-streams and also because it's four letters just like `read` and `stop`. \n. Yeah, the fact that read is a continuable is just a coincidence, not an API goal.\n. @Raynos I really like the tcp accepting a transformer instead of just providing a duplex object and ignoring the return value!\n\nI'm fine with `stop` requiring the callback if you feel strongly about it.  It will make the API a little harder to use, but considering it's not a hot path for most programs, that should be fine.\n\n> We should specify the allowed states. A stream is basically zero or more values followed by an end (an end which is either natural or caused by an error)\n\nYes\n\n> We should specify that once the callback fires any call to `.read()` will not return any more values. Optionally specify that when you call `.stop()` and even before the callback fires it should not return any values to the read callback. Basically it would be useful to spec out the relationship between `stop()` and `read()`\n\nThis gets tricky in larger chains.  The source may be done sending events and indeed never send anything after calling the callback to `.stop()`, but the layers after it may still have data pending in the pipeline that eventually comes out.  The `stop` channel is very fast and often will be a direct reference to the stop function in the source.  The spec should then say:\n\nThe source will never output data events after a stop request has been received, but be aware that other layers downstream may still contain data.\n. Though, I guess since technically every layer exports a new \"source\" interface, it would need to observe the same rule for stop and clear it's data queue when stop is called.  This makes chaining stop much harder, hmmmm.\n\n``` js\nmodule.exports = function (source) {\n  // ... code including definition of dataQueue\n  return { read: read, stop: stop };\n  function stop(err, callback) {\n    dataQueue.length = 0;\n    source.stop(err, callback);\n  }\n  function read(callback) {\n    // do stuff\n  }\n}\n```\n. Even truncating `dataQueue`, there can still be pending read calls to the parent source and a flag would need to be set that tells onRead to ignore the value when it finally returns (or call onRead directly or something otherwise crazy)\n. @dominictarr oh cool.  Do you think it would be confusing to use the same names, but have slightly different semantics.  I see leveldown doesn't accept a reason (error) when ending and my data events won't have key and value, just value.  Otherwise it seems very close.\n. I think we could get away with not having `err` in stop/end/close if we slightly changed how error propagation worked.  In the new system, `end` would be used _only_ to notify upstream that we won't be consuming anymore.  We could send an appropriate error event downstream at the same time from wherever the error started.  Upstream would never know about the error, but I think that's fine.  It just needs to know it can safely clean up resources.\n. @dominictarr \n\n> regards a consume method, I think consistency trumps necessity.\n> There are places where you don't care whether something is a complete sink, or just one side of a transform. Also, if a sink is an object, then you can explain sources, explain sinks, and then explain a transform as both a source and a sink.\n> \n> I started using the word \"sink\" to designate a stream that has no readable side - this reflects the usage of the word 'sink' in graph theory http://en.wikipedia.org/wiki/Sink_(disambiguation) . having a method \"sink\" on a transform stream breaks this.\n\nHaving a transform that is an object with both readable and writable ends is interesting. (also much closer to how node works with readable and writable streams)\n\n``` js\nvar source = fs.readStream(\"input.txt\");\nvar sink = fs.writeStream(\"output.txt\");\n\nfunction transform() {\n  return {\n    next: function (callback) { ... },\n    end: function (callback) { ... },\n    consume: function (stream) { ... }\n  }\n}\n\n// Very nice chaining API though\nsink.consume(transform()).consume(source);\n\n```\n\nBut this has issues.  `next` and `end` have no meaning and nothing to pull from until `consume` is called setting up their data source.   To enable chaining, `consume` could return the thing it just consumed.\n\nThe sink will start pulling from the transform before it has a chance to connect to it's source.  This would mean all `next` functions would need an extra state to handle this early call and defer calling the callback.  The nice syntax may or may not be worth this cost. (keep in mind most transforms would already have internal queues, flags and checks.  Adding one more isn't too bad in most cases)\n\n``` js\nvar source = fs.readStream(\"input.txt\");\nvar sink = fs.writeStream(\"output.txt\");\n\nfunction transform(source) {\n  return {\n    next: function (callback) { ... },\n    end: function (callback) { ... }\n  };\n}\n\n// Very simple consuming API\n// note that I'm still expressing the sink as an object even though the transform is just a function.\nsink.consume(transform(source));\n```\n\nIf a transform was modeled as a function that accepted a source and returned a new source, it would be more straightforward.\n\nNow duplex streams (as opposed to transform filters) are modeled great as a single object with `{next, end, consume}` and app logic where symmetry is needed is easy to model.\n\n``` js\nvar jsonCodec = require('json-codec');\nvar lineCodec = require('line-codec');\n\ntcp.createServer(8080, function (socket) {\n  // socket is a \"duplex\" stream with {next, end, consume}\n  // Apply protocol de-framing and framing on the duplex stream\n  socket = lineCodec(socket);\n  // Apply JSON parsing and Serialization to the duplex stream\n  socket = jsonCodec(socket);\n  // App is a simple echo server, so echo objects back\n  socket.consume(socket);\n\n  // or written as a single expression\n  socket.consume(jsonCodec(lineCodec(socket))\n});\n```\n\nBut if we make the TCP library act like a filter itself, then socket is no longer duplex.\n\n``` js\nvar json = require('json-codec');\nvar line = require('line-codec');\n\ntcp.createServer(8080, function (socket) {\n  // Written as a sequence of actions\n  socket = line.deframe(socket);\n  socket = json.decode(socket);\n  socket = app(socket);\n  socket = json.encode(socket);\n  socket = line.frame(socket);\n  return socket;\n\n  // Or written as one expression:\n  return line.frame(json.encode(app(json.decode(line.deframe(socket)))));\n});\n\nfunction app(stream) {\n  // Just an echo server\n  return stream;\n}\n```\n. @raynos, so then looks like you vote for modeling transforms as `function (stream) -> stream` and letting external libraries make it pretty (basically what I've been doing for min-streams all along)\n. Ok, so recap with the latest API proposal:\n\n``` js\nvar stream = {\n  next: function (callback) { ... },\n  end: function (callback) { ... }\n};\n\nfunction transform(stream) {\n  // ...\n  return { next: next, end: end };\n  function next(callback) { ... }\n  function end(callback) { ... }\n}\n\nvar duplexStream = {\n  next: ...\n  end: ...\n  consume: ...\n};\n```\n\nA transform function could be a duplex transform and accept a duplex stream and return a new duplex stream.  Though it's probably better to write transforms as separate encode and decode.  A generic duplex transform that accepted encode and decode would be easy to write.\n\n``` js\nfunction duplex(decode, encode) {\n  return function (original) {\n    var transformed = decode(original);\n    transformed.consume = function (source) {\n      original.consume(encode(source));\n    };\n    return transformed;\n  };\n}\n```\n. Usage of `duplex` above would be:\n\n``` js\n// Create a duplex stream\nvar socket = tcp.connect(1337);\n\n// The protocol is framed, let's remove that layer.\nsocket = duplex(line.deframe, line.frame)(socket);\n\n// The protocol is also JSON encoded\nsocket = duplex(json.decode, json.encode)(socket);\n```\n. Hmm, I just realized this `duplex` helper is just a weaker version of the `series` helper from above from the point of view of the consumer.  It works differently inside because it's working with duplex streams and not simple streams though.\n. > sinks need more of a spec. in particular, I didn't know what how to communicate completion/errors out to client, non-stream code. I'm +1 on @Raynos's suggestion.\n\nIn min-streams, sinks return continuables for exactly this reason.  It's worked out great so far in my js-git code.  The continuable will resolve with the end/error event in the stream.\n\n> it feels much easier to get right than min-streams.\n\nThat's the goal.\n\n> is it the responsibility of the sink to call .end on its input stream to allow for cleanup? or is stream.end() more exceptional than that?\n\nNo, you only call `end` if you want to end the stream early.  Perhaps calling is `end` is confusing with the `end` in node's writable stream interface?\n\n> what do we do with extra read callbacks? the dom example has a filter stream module that collects a list of callbacks. right now I simply truncate that buffer on end, then forward the end. is this correct?\n> there's more boilerplate than using through, but impressively, not much.\n\nAfter sleeping on it, I think it's best to say that calling `.end()` doesn't guarantee the data stream will stop right away.  That adds too much boilerplate and extra code to each and every layer for little gain.  Also I don't think the source needs to insert an end event into the stream when `.end()` is called.  The \"end\" event in the stream is for natural ends.  If a filter in the middle wants to cleanup something it needs to listen for both \"end\" events and the callback to \".end(callback)\" since either could end the stream (and both may happen sometimes).\n\nAs far as truncating callbacks, just make sure to _always_ eventually call every callback.  It really messes up programs when callbacks never get called.\n. I think there needs to be very little interaction between `read` and `abort`.  If you try to read from a source that's been stopped, it will simply emit end.  So I take back what I said about pieces in the middle needing to intercept the abort call.  Everyone can just keep reading from their source till value is `undefined` and then know the stream is done.\n\n``` js\nvar stream = {\n  read: function (callback) { /* callback(err, value) */ },\n  abort: function (callback) { /* callback(err) */ }\n};\n\nfunction transform(source) {\n  return { read: read, abort: source.abort };\n  function read(callback) {\n    source(function (err, value) {\n      if (value === undefined) return callback(err);\n      callback(null, value.toUpperCase());\n    });\n  }\n}\n```\n. Also I think I want to model tcp streams as stream transforms instead of duplex streams.  Adding in duplex complicates the model a lot.\n\n``` js\n// Echo server\ntcp.createServer(8080, function (socket) {\n  return socket;\n});\n\n// Echo client\ntcp.connect(8080, function (socket) {\n  return socket;\n});\n```\n. As to @chrisdickinson's question about the cases where you really need a writable interface (where normal imperative logic is better than a state-machine transformer):\n\n``` js\ntcp.createServer(8080, function (input) {\n  var output = writableSource();\n  // output has both readable and writable interfaces:\n  // { read(callback), abort(callback), write(value, callback), end(err, callback) }\n  // or { read(callback), abort(callback), emit(err, value, callback) }\n  // The exact interface for writable doesn't matter because it's not part of the simple-stream spec.\n  return output;\n});\n```\n\nThough you usually also want your input to be pushed to you in the app case, so the push-filter interface is best here I think.\n\n``` js\ntcp.createServer(8080, pushToPull(function (emit) {\n  // Call emit(err, item) every time we want to write data outwards.\n  return function (err, item) {\n    // Called every time data is written to us\n  };\n}));\n```\n. I don't see transforms as streams.  That's just a pattern I've seen done in node where there are duplex streams.  Transforms can be modeled as duplex streams, but I don't feel it properly represents them.\n\nTo me, having only streams be objects is quite symmetrical and clean.  The _only_ basic type here is the stream.  It's the only thing in the spec that everything else has to agree on.  Things like sources, filters (transforms), and sinks are simply functions that either accept or return streams. Push-filters are just an easy way to create normal stream consuming filters and are really not part of the spec.\n- source = function that returns a stream (and accepts optional stream setup arguments)\n- filter = function that accepts a stream and returns a stream (and may have additional option arguments)\n- sink = function that accepts a stream (and returns a continuable)\n. So to further explain, let's take some known APIs from the non-streaming world.  A string is like a stream, but instead of through time, it's a stream of bytes through memory, but all seen in an instant.  If I wanted to take a string of JSON and transform it into the object that it represents, I don't use an object to do that conversion, I use a function.\n\n``` js\nvar obj = JSON.parse(json);\n```\n\nLikewise if I have a stream of raw json strings through time and want a new stream of parsed objects through time, I would do the same thing.\n\n``` js\nvar objStream = json.decode(jsonStream);\n```\n. The signature of `JSON.parse` is `(jsonString) -> object` and the signature of `json.decode` is `(stream<jsonString>) -> stream<object>`.\n. @dominictarr basically yes, and I changed the error handling slightly so that you don't send a reason when aborting the stream and you don't wait for the source to reflect the reason, but send your own error downstream directly.\n. Ok, I've updated the official spec to reflect these changes.  It's looking real clean. https://github.com/creationix/js-git/blob/master/specs/simple-stream.md\n. @dominictarr also, since sink isn't part of the stream spec, you're free to implement your transforms/filters as objects if you want.  I can still interop as long as we all use the same interface for the streams themselves.  As for me personally, I much prefer filters being functions that accept and return streams.  I aim to write all js-git related filters as functions.\n. @dominictarr \n\n> so, I can think of a few situations where the reason might be important. example: on tcp you want to know if the stream failed because there was no server, or if it dropped the connection, or it timed out.\n\nSorry if I didn't explain right, but of course you need the reason downstream.  And you'll still have it, that's what the `err` argument in read's callback is for.  I was talking about the `err` argument that would be in abort before the callback.\n\nAll of these error cases you mentioned would still be reported and come out of the continuable that the sink returns.\n. The \"reason\" that's not important is for a source to know why it's consumer is going to no-longer consume from it.  It doesn't care why, it just needs to know so it can clean up stuff.  It's downstream, the consumer, that cares why stuff is broken.\n. @Raynos \n\n> > sink = function that accepts a stream (and returns a continuable)\n> \n> It might make sense for sink to be a function that returns an object with a consume method for purposes of structural typing. That consume method then accepts a stream and returns a continuable\n\nThere is no reason you can't do that.  I just don't want to force such a verbose construct in the spec since it's not needed or even wanted most the time.\n\nStructural typing matters more for anonymous things that are passed around and used as return values and arguments all the time.  Streams definitely fall under this category.  Sinks are more like API endpoints that consume streams.  I don't think they need structural typing as much.  I know that `fs.writeStream(stream, path, options) -> continuable` is a sink because of it's API docs, it's name, and it's documented signature.\n. So usage would be:\n\n``` js\nfs.writeStream(path, options).consume(stream)(callback);\n```\n\nvs\n\n``` js\nfs.writeStream(stream, path, options)(callback);\n```\n\nWith the first one, I feel an urge to use a promise instead of a continuable so that it's `.consume(stream).then(callback)`\n. And I'll say, well if you insist on taking-no-args-and-then-returning-an-object-that-has-a-prototype-that-has-a-method-that-accepts-a-callback-and-an-errback-and-optional-progressback instead of just take-the-callback, then I guess you insist on complexity and better change the name away from \"simple streams\"\n. @Gozala I was wondering when/if you would comment on this thread.\n\nYes, I agree that the only interface that needs to be specified is the readable stream.\n\nAs far as using special tokens for END, ABORT, and Error classes, I'd rather not.  `instanceof Error` doesn't work if the error is from another context.  There is no Error.isError helper function though Object.prototype.toString.call(err) === \"[object Error]\" seems to be reliable.  I'd hate to force such a verbose type check on each and every data chunk that goes through the stream.  Having two positional arguments tells us a lot that speeds up such checks.\n\nYes back-pressure can be done with a manual side-channel and pause and resume commands, but I much prefer the implicit backpressure provided by pull style.  In my experience I'm much more likely to get it right if I'm using pull-streams than writing the back-pressure by hand using manual pause and resume.\n\nYes there are general helpers that convert between types.  I am publishing a module right now called push-to-pull that lets you write the easier push filters, but use them as back-pressure honoring pull-filters without writing your own queues.  A reduce transform could easily be written as could a filter transform.\n\nThanks for the input.\n. @mhart I'm glad you like `abort`.  You can thank @dominictarr for convincing me to add that to the official spec.  I really didn't want to.\n\nI've also considered what a stream would look like if we had access to ES6 generators.  I think the simplest construct would be a generator that yielded values.\n\n``` js\nfunction* source() {\n  yield 1\n  yield 2\n  yield 3\n}\n\n// Consume like any other generator to get [1, 2, 3]\n```\n\nBut like most I/O streams, you can't yield everything at once, so the generator could yield continuables instead of raw values.\n\nBy happy coincidence, simple-stream's read function is itself a continuable.  So turning a simple-stream into a generator based stream is as simple as:\n\n``` js\nfunction* () {\n  // Create a simple-stream\n  var stream = fs.readStream(\"myfile.txt\");\n  // and yield it forever\n  while (true) yield stream.read\n}\n```\n\nIn fact my [gen-run](https://github.com/creationix/gen-run) library does something very much like this, but as a control-flow helper library.\n\n``` js\nrun(function* () {\n  var stream = fs.readStream(\"myfile.txt\");\n  var data;\n  var items = [];\n  while (data = yield stream.read) {\n    items.push(data);\n  }\n  return items;\n});\n```\n\nI don't want to require generators for streams since it will be a long time before most js environment can assume ES6 generators.  I am, however, very aware of how they will interact and keep these things in mind.\n. @gozalla, I'm having trouble understanding you.  I do like the idea of the main data channel being _only_ data and letting everything else go through a meta channel.\n\nI'm pretty sure I want pull based for several reasons.  Besides the natural back-pressure it provides, it also provides a nice 1:1 mapping between continuations chains since each callback will be called only once for each read call.  This makes tracing and error handling much easier.  I'm currently working on improving domains in node.js and wish that everything in node had this nice 1:1 mapping.  It makes for a very simple and robust system when every stack has a direct and obvious parent stack that initiated it.\n\nI know that you can't pause some inputs easily (like user clicks or http requests), but that doesn't mean pull-streams are a bad idea.  You just buffer the events at the source waiting for someone to pull them.  Even those cases can usually be paused somewhat in extreme cases (you could disable the UI button if the stream wasn't ready to handle it or tell the TCP socket to stop accepting connections)\n\nAlready have two data channels in the form of two arguments to onRead callbacks `(err, item)`.  When `item` is anything other than `undefined`, then it's a data item.  Otherwise, it's a meta value that signifies natural end or error end.  The channel for closing an upstream source goes the other direction and so can't be encoded here.\n\nDo you have any ideas that are modifications to the current design that could simplify this?\n. Thanks!\n. Btw, others use continuables too, but yeah, there aren't too many of us. https://github.com/Raynos/continuable/blob/master/spec.md\n. Also, be advised that I recently modified the API to push-to-pull in the 0.1.x release.  I updated the docs, test, and code to reflect the change.\n. API changes https://github.com/creationix/push-to-pull/commit/f1c77934ba9f24ffaad0e22f7a58c9cfcb70fdef\n. I'm going to close this issue here.  Push-to-pull has been working without any problems for a while.  But feel free to add more unit tests and create an issue on the push-to-pull repo if you wish.\n. Closing since we don't use sub-stream anymore.  I decided it's not worth the performance cost and complexity to stream object bodies.\n. So, two of the main goals of jsgit are to be lightweight and extremely portable (running on a variety of platforms and storage and network backends).  The C libraries make a lot of assumptions about the platform they run on that make that extremely difficult.  Also many of the C libraries that would be useful to me are GPL or LGPL licensed and I want to keep js-git more open using MIT.\n\nThe main technical problem with enscripten, even if the C libraries were exactly what I needed, is that it generates a _lot_ of javacsript.  I don't want 2mb of generated code.  I want something hand-coded so that it can be a lot leaner.  Remember this is just a library and not an entire application.  It's just a part of someone else's project.\n\nAlso the emscripten toolchain is a pain to setup, especially on platforms like ChromeOS where I expect people to use js-git a lot.  I would really like it if people could hack on js-git on the same platforms that use it.  There is no emscripten (not one that runs at usable speeds at least) in browser-style environments.\n. I'm not sure, I'm using the ssh2 module from https://github.com/mscdex/ssh2\n\nIf you want to take a shot at it, options get passed into the ssh wrapper at https://github.com/creationix/js-git/blob/master/protocols/ssh.js#L31 and the node.js ssh wrapper is at https://github.com/creationix/js-git-node-platform/blob/master/ssh.js\n. @dominictarr did you ever figure this out?\n. So the reason it thinks all the files were deleted is because the `.git/index` file is missing.  `repo.checkout` is supposed to create that file after extracting the tree from the commit, but it doesn't seem to in this case.  I can reproduce locally.\n\nAlso, github lets you clone using ssh on my repo?  I thought that only worked if you have write access.  That's really cool if they added read-only support for the ssh protocol.\n. As far as ssh not working in 0.11.x I suspect that's because mcsdex/ssh2 hasn't been tested for that version of node.  It still seems to export the old streams interface from back in node 0.6.x\n. So I found two issues, both on github's end. (though both I could workaround if needed).\n1. If you override the `Agent` header to be what the git cli tool sends (`git/1.8.1.2` instead of `jsgit/0.2.3`), it will give a redirect instead of a 404\n2. It redirects.  I haven't implemented redirect following because url parsing isn't cross-platform yet.  Why don't they provide the proper url for you to clone?\n\nIf you clone using the proper url, it works fine, even **without** changing away from my custom agent header.\n\n``` sh\nTRACE=1 jsgit clone http://gist.github.com/gist/036c175d18a8a692a89d.git milestones\n```\n\nNotice the `/gist/` at the front of the path.\n. So for redirects, I just need more logic in smart-http.  But the problem with url parsing is I'm using the \"url\" module from node.js.  In http redirects it gives the new url as a string, but smart-http works in terms of already parsed urls.\n\nThere is a strict line between what's allowed to be used in js-git proper and what js-git-node-platform can use.  Anything exported by the platform can be used by js-git, but that then means all ports of the platform have to implement that interface as well.\n\nUrlparse is pure javascript.  Maybe we could just include the source from node.js in the browser ports and make it part of the platform interface?\n. Actually, since the node url parser doesn't work for all git urls anyway, (It can't handle urls like `git@github.com:user/repo.git`), I should just include the url parser in `helpers/urlparse.js`.\n. @stuartpb yes, I'm sending the proper content type.  It's sniffing on the user agent.  I'm implementing redirect support and a way to send a custom useragent.  That should make it possible to clone from gists.\n. Added hack in http code for gist. https://github.com/creationix/git-net/commit/708721e0cbdde6d6ec9355aa7e96df19571a16d2\n\nNow to implement redirect...\n. Done!\nhttps://github.com/creationix/git-net/commit/da8c27d478bcc32a851e0d22645be1f8a4580e3a\n. Waiting on bountysource to fix a bug so that I can access the names.\n. https://github.com/creationix/git-browser\nApp in Firefox Marketplace is still pending review.\n. https://github.com/creationix/git-browser\nhttps://chrome.google.com/webstore/detail/git-browser/cladogmhjppclibenkdbnjcogiaifnbd\n. This is now implemented in the newly refactored js-git code.\nhttps://github.com/creationix/js-git/blob/master/js-git.js#L593-L688\nhttps://github.com/creationix/git-net/blob/master/fetch.js#L14-L44\n. I wonder if your git version is too old for the index format I'm writing. Are you able to upgrade and try with a newer git?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  From: Florent JabySent: Saturday, August 10, 2013 5:37 AMTo: creationix/js-gitReply To: creationix/js-gitSubject: [js-git] after cloning, git can't read index (#30)I successfully cloned one of my repos via https. but running git status in the repo results in a error message like this:\n\nfatal: Unknown index entry format 28dc0000\n\nBecause I also have some git prompt addition the error messages come after each command.\n\n$ git --version\ngit version 1.7.10.4\n\nI installed js-git-node from npm. I cloned \n\n\u2014Reply to this email directly or view it on GitHub.\n. Sorry I read the version wrong. That's not very old.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  From: Florent JabySent: Saturday, August 10, 2013 3:38 PMTo: creationix/js-gitReply To: creationix/js-gitCc: Tim CaswellSubject: Re: [js-git] after cloning, git can't read index (#30)it's the git version in the apt repositories for ubuntu 12.10. so I suppose\n\nthe same version is used for 12.04 (LTE) I can try and get some newer git\n\nand tell you how that goes.\n\nLe 10 ao\u00fbt 2013 18:47, \"Tim Caswell\" notifications@github.com a \u00e9crit :\n\n> I wonder if your git version is too old for the index format I'm writing.\n> \n> Are you able to upgrade and try with a newer git? From: Florent JabySent:\n> \n> Saturday, August 10, 2013 5:37 AMTo: creationix/js-gitReply To:\n> \n> creationix/js-gitSubject: [js-git] after cloning, git can't read index\n> \n> (#30)I successfully cloned one of my repos via https. but running git\n> \n> status in the repo results in a error message like this:\n> \n> fatal: Unknown index entry format 28dc0000\n> \n> Because I also have some git prompt addition the error messages come after\n> \n> each command.\n> \n> $ git --version\n> \n> git version 1.7.10.4\n> \n> I installed js-git-node from npm. I cloned\n> \n> \u2014Reply to this email directly or view it on GitHub.\n> \n> \u2014\n> \n> Reply to this email directly or view it on GitHubhttps://github.com/creationix/js-git/issues/30#issuecomment-22443014\n> \n> .\n\n\u2014Reply to this email directly or view it on GitHub.\n. A past version of js-git had this for the clone.  I wrote a barebones index so that git status wouldn't complain about stuff breaking.  \n. @maks, here is my old code that was writing the index. https://github.com/creationix/js-git/commit/2baa4f9f821ac2a2af4e3e9c82fc3f30e08b0aea \n. Actually I just got done cleaning up and organizing everything.  This repo now houses the core logic that is command-line and browser agnostic.  It consumes various simple interfaces and implements git on top of them.\n\nThe node CLI tool is in the `js-git-node` repo with the node version of the various js-git interfaces implemented in the `js-git-node-platform` library.  That way the `js-git` can be used from node without being dependent on the CLI program.\n\nI'm still working on a browser version.\n\nI do apologize that the code has been less organized so far.  I prioritize getting work done over keeping things clean.  Especially when I don't even know what I want most the time.  It is starting to take shake and firming up though.  This is why I announced the project on the node list recently.  It's time for people to start consuming the library through the CLI tool to help iron out bugs and platform issues.  Right now I'm working on creating a small browser based app to test that side of the abstraction.\n. Good idea.\n\nSo for a bare local repo, all that's needed is the git-db interface.  Inflate and Deflate are internal implementation details for that.  Network communication is a little trickier.  For clone/fetch/ls-remote the platform needs to provide tcp, http, and/or ssh interfaces (depending on which protocols you want to support).\n\nIf you want to use the built-in fs-db library, then you do need inflate/deflate/sha1 as well as an fs instance.\n\nIf you want to have a working directory, you need to provide an fs instance as part of the git-db interface.  You don't have to use the fs-db adapter library.  You could do this with only indexdb by emulating the fs on top of the indexdb system, but using indexdb directly for the git-db implementation.\n. I don't there would be such a high-level library, and if there was, it would be simply:\n\n``` js\nrequire('js-git/lib/platform.js')(require('js-git-node-platform'));\nmodule.exports = require('js-git');\n```\n\nAssuming that js-git eventually exports a central thing.\n\nAll the high-level APIs will either be in js-git itself or in js-git addon modules, both which would be platform independent I think.\n. I documented this in the main README at the js-git repo.\n\nWe'll be adding more as we implement the other milestones.\n. Yes, but it's a ways off.  Maybe milestone #4 can be custom git servers.\n. Actually this will be part of milestone #2 and I'm working on it today.  Hopefully I get it done soon.\n. This was harder than expected (and I got sick) so it's still not done.  When I announce milestone#2 done, then you will be able to do this.\n. Thanks for writing this up.  I very much value use cases to know which direction to prioritize.  This can be `milestone #4` if that's soon enough (a few months out). Right now I'm working super hard to finish `milestone #1` since that alone will be enough for many people's use cases.  A large part of this work is creating packfiles which is currently part of `milestone #2`.\n\nOne comment on ssh.  It's probably best to reuse the existing ssh server in servers.  That's how real git works.  What I would be re-implementing is the various shell commands that get spawned.  These mini processes would then need to connect to the main git server process and talk to it on the client's behalf.\n\nNow it would be more efficient if node was also also the ssh server and only understood git commands.  But then your server wouldn't have an ssh server for remote logins anymore (unless node handled that too).\n\nAlso I would like to keep a js-git powered client as a separate task as a js-git powered server.  Your use case may require both ends to be node powered, but many others do not.\n. I have a sample js-git server I'm using to develop the server-side of js-git.  I just finished a basic version of `repo.receivePack` that allows you to push to js-git powered servers!\nhttps://github.com/creationix/js-git/blob/master/examples/serve.js\n\nIt's still missing the protocol \"report-status\" extension that will be needed for post-commit hooks though.\n. Done!\nhttps://github.com/creationix/git-net/commit/4d5f962e844d49193b87e4c9913da386a370b2b9\nhttps://github.com/creationix/git-net/commit/3037b111cdd9ea06d64cfab0d1f3ea3a59fc1f78\n. Now implemented in refactored js-git core.\n. This is now done.  The final design is slightly different from the diagram, but the idea is the same.  The main modules are:\n- https://github.com/creationix/js-git - Core logic for local git repositories\n- https://github.com/creationix/git-net - Network logic for working with remotes\n- https://github.com/creationix/git-node-platform - Platform implementation shimming native node.js APIs\n- https://github.com/creationix/git-pack-codec - Packfile encoding and decoding (only decoding at the moment)\n- https://github.com/creationix/git-fs-db - Adapter for converting fs instances to git-db instances.\n. Yes it's on the TODO list, but not scheduled yet.  Which of the 4 schedules milestones do you feel this fits in?\n\nKeep in mind that I won't have packfile _writing_ of any kind in milestone 1, but perhaps reading of existing git repos could fit in there.  I just don't want to block the other work on something most people won't need.\n. Ok, I'll tack it to the end of my milestone 1 tasks.  I'm hoping to get that milestone done this week before I travel to StrangeLoop, but it's getting pretty close.  Maybe I'll finish at the conference or while traveling.\n. Sorry for delaying this issue.  I think it will be the last issue in milestone #2 after I complete all the network stuff.  With this and the network stuff done, js-git will be feature complete enough to be the backend for things like howtonode.org.\n. Done\nhttps://github.com/creationix/git-net/commit/9d7facd79895c4e3025046ab1417999a7dd38f61\nhttps://github.com/creationix/git-net/commit/a4ad7bb3214227f2f5af24fdbc1229cbaabf85d5\n. https://github.com/creationix/git-node/commit/c2f619ef2ce0c89439b9097219945678d1b01593\n. Started https://github.com/creationix/jsgit/commit/3cf5387a7b606f42acb5e967ddc9cabb2e5af246\n. Done, now just pending docs.\n. Re-opening since the APIs changed again.\n. Updated and new versions of app submitted to both the stores and new web version published to web.\n. Implemented in:\nhttps://github.com/creationix/js-git/commit/3df0854d1c792bf2ee4a239991538ea0cbe45a10\n. Implemented in:\nhttps://github.com/creationix/js-git/commit/3df0854d1c792bf2ee4a239991538ea0cbe45a10\n. Actually this was a problem with git submodules.  FIxed in https://github.com/creationix/js-git/commit/9d44902c03beecbb05bfb83fe9451d6c0a69d13c\n. This was a problem with the unpack algorithm.  It had assumed that all deltas were unique, but this isn't always the case.  Fixed in master.\n. Fixed in https://github.com/creationix/git-node-platform/commit/444923217ed23bc8e51fb25685c6f874ead3eae0\n. Documented repo and db interfaces, those are the most public facing ones.\n. Done and sample deployed to http://git-browser.creationix.com/\n. https://github.com/creationix/git-web-platform/tree/master/build\n. Did you try with the new chrome app support in cordova?  I was unable to get it to run at all, but I really like the push to chrome adt workflow if it actually worked.\n. Yeah, when using git:// protocol there is just one connect per clone/pull command.  I hope they improve their code soon.\n. That would be great.  The main thing I need is the most minimal set of files to create a winjs app, the most minimal build tools (preferably something that could be done on a chromebook or using well-supported web-services)\n\nThen I would need adapters for the primitives.  These APIs are still changing, but for git-browser it's tcp, http, and storage.\n. A leveldb based backend would great for node as well!  If there is a browser version of the the api, then the leveldb adapter could be used there too.\n\nBut I agree with @aaronpowell that minimal dependencies are important for web stuff where bandwidth and performance are often limited.\n. @aaronpowell, to test, clone the git-browser app and replace the localStorage backend in the web variant with your indexedDB implementation.  That will test 90% of the code I think.\n\nTo run the web version, just build using the included `make` file and build the `web` target.  Then serve the `build/web` folder using a http server of your choice to test.\n. Looks great.  I'm closing this issue for now since it's at least as functional as the other backends.  Feel free to make further improvements on your own.\n\nThanks for the contribution!\n. This is almost done.  I still need to implement `have` and `deepen` commands in the negotiation phase.\n\nAlso the packer isn't optimal.  It doesn't support many of the extensions and doesn't search for ref-deltas to create.  That just means we'll more bandwidth than real git, but less CPU.\n. This is now implemented as `repo.receivePack()` in `mixins/server.js`.\n\nThe current form doesn't implement the \"report-status\" capability, but does allow for git clients to push to a js-git server.\n. You would create the child directory first since it's hash will be part of the entry in the parent directory.\n\nSo using generator style, the code would look something like:\n\n``` js\nvar hash = yield repo.saveAs(\"tree\", [\n  { mode: 0100644, name: \"one.js\", hash: yield repo.saveAs(\"blob\", one_js_body) },\n  { mode: 040000, name: \"sub\", hash: yield repo.saveAs(\"tree\", [\n    ...\n  ]) },\n  ...\n]);\n```\n. ![create_repo](https://f.cloud.github.com/assets/89353/1430624/b77d9a10-40c3-11e3-82ac-feafcf3e83eb.png)\n\nThe UI now has placeholder menu items for this.  They are activated by right-clicking on the empty area of the tree.\n. ![capture](https://f.cloud.github.com/assets/89353/1466190/6f3165b2-456d-11e3-81cc-af5b77218dd1.PNG)\n\nCreating a new local repository is now done.  Next up, implementing cloning existing repos.\n. Done!\n\n![capture](https://f.cloud.github.com/assets/89353/1466829/608db880-4577-11e3-9f4f-cb0f1ebfab58.PNG)\n. Deployed live server to http://tedit.creationix.com/ that has working websocket-git proxy.\n. ![folder-ops](https://f.cloud.github.com/assets/89353/1430638/f1be47ce-40c3-11e3-8334-178da0edc555.png)\n![file-ops](https://f.cloud.github.com/assets/89353/1430643/00d515e4-40c4-11e3-96fd-042968eaa0b2.png)\n\nUI placeholders for folder and file ops in place.\n. ![capture](https://f.cloud.github.com/assets/89353/1466082/9ebe70c4-456b-11e3-880e-61c4bd4911af.PNG)\n![capture2](https://f.cloud.github.com/assets/89353/1466051/4af69c28-456b-11e3-95e8-8b89db1093fb.PNG)\n\nImplemented everything except rename.  Almost done.\n. Now done!  Tedit has full CRUD capabilities for easy git testing.  Test/play at http://creationix.com/tedit/\n. Most the examples are pure client side.  See the https://github.com/creationix/git-browser and https://github.com/creationix/tedit demo apps for examples. \nGit-browser supports multiple environments such as web (with a local node websocket proxy), chrome apps, and firefox os apps (with other ports in progress).\n\nSince each and every platform has different APIs for basic things like SHA1, TCP, HTTP, and persistence, js-git is platform agnostic and expects you to provide it with an implementation of the abstract platform interface.\n. The thing is that browsers on their own are incapable of talking to most git servers.  Browsers only support websockets and HTTP.  Most git hosts support HTTP, but they don't implement CORS.  So unless your website is hosted _on_ github itself, you won't be able to talk to it using XHR.  The browser's domain security restrictions will deny the connection.\n\nI've asked Github to enable CORS on their http git endpoints, They told me they were reviewing the security implications and I haven't heard back from them.\n\nThe reason there isn't a single browser-ready version of js-git is because there is no clear-cut configuration that works for everyone.  Every platform and use case is unique.  There are literally hundreds of configurations that I've seen people and platforms need.\n\nNow github does have CORS on their other HTTP APIs.  You can use those, but they aren't git protocol and aren't supported by js-git.\n. The recommended configuration for now is either:\n1. If you're a webpage, you can run your own web server and host a websocket proxy to get around the cross domain issues.  A live example is at http://git-browser.creationix.com/\n2. You can submit your app to the various app stores and get access to more powerful APIs.  I have example code that runs standalone in chrome packaged apps as well as firefox os apps.  These apps don't need a server or node at all to talk to git servers.\n. What was the error?\n. Also the code is being refactored at the moment. Sorry for the temporary unstability.\n. Thanks!\n. Also, I keep forgetting to tag you on twitter. https://twitter.com/creationix/status/399926262644240384\n. Neat idea, but I'm not sure I want to automatically do this.  I like keeping the API simple and low-level.  Also, I noticed you're calling a db function that doesn't exist in the interface.\n. Yep, it's all in flux, and git-net will soon be deprecated to migrate to the new, better architecture.  This should all be stable soon as part of finishing milestone#2\n. The 0.6.x series is pretty buggy.  It will all be resolved for milestone#2.  If you want, you can create issues against milestone#2 and that way we'll make sure the things get fixed.  But most of these I'm aware of and will fix as soon as I get time.\n. Yeah, it's there.  I've been pretty sick the last few weeks.  Sorry for the delay.\n\nhttps://github.com/creationix/js-git/blob/master/mixins/server.js#L175\n. Sorry, this got paused while I do paying work.  It's one of the first\nthings to get done when I have time for js-git again.\n\nOn Wed, May 14, 2014 at 1:30 AM, Alexandru Vl\u0103du\u0163u <notifications@github.com\n\n> wrote:\n> \n> @creationix https://github.com/creationix hey, any updates on this? I\n> see the server.js file has been removed since your last comment.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/creationix/js-git/issues/74#issuecomment-43046704\n> .\n. I'm thinking this only makes sense for blobs.  Maybe we could add a new type to `loadAs` and `saveAs` called `\"stream\"`.  Then in the db interface we would add optional `getStream` and `setStream` primitives to the interface.  For backends that are unable to implement streaming, js-git will just use the non-streaming interface and buffer internally.\n\nWhat do you think?\n. Interesting.  If you have other API ideas, I'm open.  I will mention that\nwith some backends like js-github you are required to know what kind of\nobject you're loading before loading it.  I get around this in js-github by\ncaching hash <-> type information wherever I can. (If you have a hash, you\ngot it from somewhere and probably already know the type from where you got\nit).\n\nOn Fri, Dec 13, 2013 at 3:44 PM, Aria Stewart notifications@github.comwrote:\n\n> Seems reasonable; I'd love streaming for trees too but it's just for\n> consistency in that case.\n> \n> loadAs is a weird interface though -- you have to know the type ahead of\n> time, and since trees can contain both trees and blobs, that's weird.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/creationix/js-git/issues/83#issuecomment-30546145\n> .\n. Because it uses this api: http://developer.github.com/v3/git/  Which\nrequires the type in the request.\n\nOn Mon, Dec 16, 2013 at 3:10 PM, Aria Stewart notifications@github.comwrote:\n\n> Oh, interesting -- since github doesn't expose raw git in a\n> browser-accessible way, eh?\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/creationix/js-git/issues/83#issuecomment-30700923\n> .\n. Yes it's possible, but it still a lot of work from where we are today.\n\nOn Thu, Dec 12, 2013 at 6:22 PM, norman784 notifications@github.com wrote:\n\n> Checking your repos see that you have git-browserhttps://github.com/creationix/git-browserand my question here is (since yesterday chrome launch there offline /\n> desktop apps) you think there can be a way that a chrome app use this to\n> work as I want, thats basically is a project manager that runs on the\n> browser and sync your files via git, a example (but not really what I want\n> to accomplish, but can compare with it) can be dropbox, or services that\n> sync files between computers / accounts and the cloud. The only thing that\n> I'm not sure are the sync part the other things like auth and all those its\n> a pancake. The app flow will be something like this\n> - List projects\n> - Select project\n> - Clone it (git clone) and save locally in the user selected folder\n> - Edit there files\n> - Update (git push, if needed a rebase or similar, need to think how\n>   to manage this because the target users are not smart, an app for dummies\n>   xD)\n> \n> Regards\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/creationix/js-git/issues/84\n> .\n. Right now, we have clone, and low-level operations on the local git repo.  There is no working directory and no way to push changes back to a remote server.  https://tedit.creationix.com/ and http://git-browser.creationix.com/ are examples of what can be done now.\n. A chrome app will have all abilities once js-git is done.  It has pretty good network and persistence primitives so that's no a problem like is it in browser apps.\n\nThe main issue is js-git is not done.  Chrome apps can access the filesystem as of chrome 31, they have indexeddb and the chrome.storage apis.  They have direct TCP and UDP client and server access as well.\n. Thanks for the report.\n. Thanks, but the issue is with fs-db, not js-git.  I originally modeled this API after the implementation in fs-db which used nested directories.  But after working with all the other backends, I decided to make it not care about folders.  The refs are new treated like strings instead of nested directories.\n. Correct, there is no built-in index in js-git.  But manually creating a commit and a tree is quite simple.\n\nOne example is an old tool I wrote that scrapes a http based database, extracts the sqlite contents, parses the html and json and writes json files to a tree structure.  The goal was to run in on a schedule and create commits whenever the root tree changed. https://github.com/creationix/gospel-git-library\n\nFor some simpler code that shows how to create commits, the examples folder in js-git is a good place. https://github.com/creationix/js-git/blob/master/examples/create.js\n\nAlso I recently wrote a simple tool that imports trees from a filesystem and gives you back the root hash of the imported data. https://github.com/creationix/giblush/blob/master/importfs.js\n\nWith all the techniques, the basic idea is the same.  You import some data into the git database.  Eventually you'll have the hash to the root tree of your new data.  You then create a new commit who's parent is the last commit and who's tree is the tree.  After you create this commit, you move the relevant head ref to point to it. https://github.com/creationix/js-git/blob/master/examples/create.js#L51-L55\n\nI hope this helps.\n. So in case it wasn't clear.  You just need to import your data and check the root tree hash.  If the hash has changed from the last commit, then something has changed since the last import and you can create a new commit and update the head to save the changes.\n. There is no concept of detached head yet.  That only makes sense when you have an index and working directory (all part of milestone #3).  Currently what's implemented is essentially equivalent to a bare git repo (what git servers usually have).  All versions and commits are there at the same there.  There is no currently checked out version.\n. Good catch!\n. There is no config at this level.  You set the name and email when creating a commit.  In my tedit-app, I store username and email in local chrome storage (chrome app version of localStorage)\n. I do have some really basic code used by tedit-app that parses and encodes git config files.  I use it in tedit to read and write .gitmodules files for managing submodules. https://github.com/creationix/js-git/blob/master/lib/config-codec.js\n\nThe latest generation of js-git code doesn't yet have a filesystem backend that uses git compatible `.git` folders.  JS-Git is generally not compatible with command-line git on the same local system. (though there is nothing from stopping someone from creating that on top).  Since my main use case is places where real git doesn't exist, I'm compatible with the network protocols and use the same hashing algorithms so that people can share repos between js-git and real git.\n. Btw, the config codec has been improved and has docs and unit tests now.\n\nhttps://github.com/creationix/js-git/blob/master/doc/lib/config-codec.md\n. @ljharb what do you mean?\n. I see.  I really feel this is backwards though.  If you want to consume js-git using ES6 promises, you have promises and already have the polyfill.  If you want to consume js-git using node callbacks or continuables, then you don't want the bloat of a 2k minified polyfill included.\n. In other words.  I'm willing to make js-git play nice in a promise environment, but it's outside the scope of js-git to provide such an environment if it doesn't exist already.\n. @Bartvds that's essentially what I'm doing.  I'm just using `window.Promise` as the entry point to pass in the promise implementation.  I don't see this as an issue since I'm only planning to support ES6's Promise API.  Is there ever a case where a user would have `window.Promise` but want to use a _different_ Promise constructor in js-git?\n. It probably wouldn't be hard to have a mixin that went through a set of known API signatures and turned them into promise based APIs.  The harder task would be to automatically convery *any* api already mixed in.. Go for it and let me know how it works.. Fixed in 77f1a82466c23dced0d0a68ccb97213a7edf4092\n. Fixed in https://github.com/creationix/js-git/commit/7bf8da57b8e56d51ea7fd21faf986f0111bbc1b9\n. Wow, thanks.  I can't believe I had that typo.\n. It's on the roadmap, but currently incomplete.  I'm not actively working on js-git right now because I ran out of money and need to refill my bank.\n\nGetting push and pull working via HTTPS is probably easier, but I think it would be possible to implement enough ssh to clone using the TLS code in the forge project.\n. So it's technically possible to speak git over https using `XMLHttpRequest`.  The main problem is none of the git hosting companies are willing to add CORS headers to allow the cross-domain request.  Also specefic to bitbucket is this issue, but it looks like it's being resolved. https://bitbucket.org/site/master/issues/6666/detect-git-requests-by-content-type-header\n\nWhile chrome extensions can't use the raw socket API, they can bypass cross-domain restrictions.\n\nI had written this extension a while ago that allowed me to clone from a browser. https://github.com/creationix/corson\n. If you're worried about privacy, then using a websocket to ssh proxy is a serious man-in-the middle unless you're implementing all of ssh in the browser.\n\nI actually designed such a proxy for git to use (was planning on doing cross-domain https over it, but ssh would work too).  https://github.com/creationix/git-proxy\n\nMy proxy does support being a TLS link which means it can see all traffic going through it.  This works great for cloning public resources that don't need a password, but is a problem for private repos or pushing commits.  My plan eventually was to use the forge project to implement TLS in the browser.  I've already implemented HTTP in JS a few times.\n\nThe other problem with a proxy is the wasted network resources and dependence on new infrastructure.\n\nThe ideal solution would be for the git providers to simply add CORS headers, but I've been asking for years.  I'm pretty sure github will never do this, but maybe BitBucket will if you ask nicely.\n. Correct, if you're making the XHR calls from your extension you can just request cross-domain permission in the manifest.  My use case was an extension that can be enabled on a per-site basis by the user that bypasses the CORS restrictions.  To simulate what would happen if the providers had added it themselves.\n\nYou don't need the user's password to auth with HTTPS, I'm pretty sure github at least will allow user tokens which can be managed the same as ssh keys.  I'm not sure about bitbucket though.\n\nAs far as actually doing git over xhr, I never completed that with js-git since I hit the hard technical roadblocks with lack of CORS.  I had a prototype of cloning, but never did pushing.\n\nThat said, there are a couple other git in JS implementations out there that use xhr in chrome apps and extensions with the requested domain exception.  You'll see mention of them in the issue I linked at bitbucket.  I think everyone is using the library written for Taylor.\n\nYou could do an SSH to Websocket proxy where the user deploys their own proxy and embeds their private key in it.  That would work well.  Then the only problem is authenticating the browser so that not just anyone can access it.  But any authentication scheme will work, even http basic auth.  It's up to the dev deploying the proxy and implementing the website.  They would need `wss://` and not plain `ws://` on the proxy or whatever they are using to authenticate would be leaked to anyone listening.  This means deploying with an SSL/TLS cert.\n. Thanks, but I don't want node-specific code in this repo.  I created a new repo for the node version at https://github.com/creationix/git-node-fs.  It would be good to model it after the chrome html5 version I currently use in tedit at https://github.com/creationix/git-chrome-fs.\n. If you could reduce this PR to just fixes to the docs comment and create a new PR for git-node-fs, that would be great.\n. Thanks.\n. Implemented.  Basic app now works on safari on desktop.\n. Yep.  If you're in node.js, you can mount the repo using https://github.com/creationix/git-node-fs.  If you're in a chrome packaged app, you can use https://github.com/creationix/git-chrome-fs.\n. Do note that it expects the actual git database, so point to the `.git` folder.  Also note that it ignores your staging area and working files.  I recommend using `--bare` clones with this so that there are no extra files laying around to confuse you.  But on the other hand, having the working dir lets you do things that bare repos aren't allowed to do (like manually merge remote changes).\n. Yeah, it's meant to be installed from git.  I just tagged and pushed to npm as version 0.0.1, but it's dependency is also listed as a git dependency.\n\nYou can install from git using:\n\n``` sh\nnpm install git-node-fs@git://github.com/creationix/git-node-fs.git\n```\n. Actually, nevermind.  I just removed the dependency.  You can now install it from npm or from git without problem.\n. The fs-db backend can read and write disk repos.  The node version is at https://github.com/creationix/git-node-fs\n. Yes, you can read files from the browser, but you can't scan directories.  Many files (pack files for example) are unguessable filenames.  As long as your repo doesn't use pack-files (most do), it would be possible to write a custom backend using nothing than xhr requests.\n. Another option would be to add support for the metadata http://git-scm.com/docs/git-update-server-info provides.  Then a \"dumb\" client could read the git repo with nothing more than xhr requests.  You'd have to externally make sure the extra metadata is kept up to date whenever the git repo is modified.\n. I'm fine with this change assuming the two notes are fixed.  I also notice you submitted a PR to git-node-fs.  I also need the same change made to git-chrome-fs.\n\nThe API it uses appears to also support rename, so it should be fine. http://www.html5rocks.com/en/tutorials/file/filesystem/#toc-copy-rename-move\n\nhttps://github.com/creationix/git-chrome-fs/blob/master/lib/chrome-fs.js\n\nOnce `git-chrome-fs` and `git-node-fs have the fix, I'll merge the fixed change here.\n. Are you sure that's the one you meant to tag?  Doesn't that depend on the other two repos adding rename?\n. Thanks.  I didn't notice the other two. Looks great.\n. js-git is packaged as common-js.  You'll need to use some sort of build system to pre-process the files for use in the browser.  I usually either do it server-side in wheaty https://github.com/creationix/wheaty-cjs-bundler or client side. https://github.com/creationix/tedit/blob/master/build/ui/loader.js\n\nYou could probably use browersify too, but I haven't tried in a while. http://browserify.org/\n. Here is a example of using wheaty-cjs-bundler. https://github.com/creationix/conquest/blob/svg/conquest.js\n\nThis repo is served with wheaty https://github.com/creationix/wheaty.  Since the file is marked as executable in git and starts with `#!js` it will be run server-side in wheaty and serve the result to the browser.\n. I'm glad it helped.  No, I'm not going to rewrite in AMD.  I may rewrite in\nES6 modules once the tool ecosystem is more mature for them though.\n\nOn Wed, Sep 10, 2014 at 2:50 AM, Jouke Waleson notifications@github.com\nwrote:\n\n> Thanks, browserify seems to do the trick! I'll see if I can get a working\n> example and send a pull request with added documentation about build\n> instructions. Do you have any thoughts about rewriting from common-js to\n> AMD?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/creationix/js-git/issues/105#issuecomment-55082019.\n. You can do mist everything over the rest protocol, its just that its non standard buggy and really really slow for clones.  Also for any sizable repo, you'd hit the rate limit long before finishing a full clone.\n\n-----Original Message-----\nFrom: \"aaron\" notifications@github.com\nSent: \u200e9/\u200e17/\u200e2014 5:14 PM\nTo: \"creationix/js-git\" js-git@noreply.github.com\nSubject: [js-git] Github CORS requirement clarification (#106)\n\nI've been asking Github to enable CORS headers to their HTTPS git servers, but they've refused to do it. This means that a browser can never clone from github because the browser will disallow XHR requests to the domain.\nThey do, however, offer a REST interface to the raw git data.\nUsing this I wrote a mixin for js-git that uses github as the backend store.\nI am trying to understand if the lack of CORS support is limiting, if you are able to use the raw git data API. I guess this just necessitates the js-github adapter as opposed to using the normal git communication mechanisms. is it otherwise feature complete? Does this include pushing commits back to github?\n\u2014\nReply to this email directly or view it on GitHub.=\n. No.   I've never gotten a reason of why they won't do it.  I either get no\nresponse or they say they are looking into it.  I've been trying for almost\n18 months now.\n\nOn Thu, Sep 18, 2014 at 12:58 PM, aaron notifications@github.com wrote:\n\n> Ah ok, this does seem like a problem. Did the github team give any\n> specific pushback on why they would not enable CORS?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/creationix/js-git/issues/106#issuecomment-56077589.\n. Sorry I didn't see this issue for some reason.\n\nFetch and clone is mostly done, yes it needs more docs.\n\nDepending on the backend there are different ways to initialize js-git.\n\ncommits can easily be created manually using saveAs and updateRef\n\nDiff isn't implemented at all.  I haven't started this.\n\nMerge depends on diff so it's also not started\n\nBranching can be done manually by simply creating new refs.  A branch in git is literally nothing more than a new bookmark.\n\npush isn't quite done.  I've been fighting the restrictions of web pages for years and trying to get the git hosting companies to open up CORS.  I want to do push correctly without leaking people's credentials.\n. Github has always had CORS for their proprietary API, but that's not what git uses to push over https or ssh.  You can use the github api via js-github.  This is what tedit.creationix.com does when you sync a github mount.\n. @nickj12497 please see #122 for a discussion on capabilities and examples.\n. What a great idea!  Can a chrome app register an url handler and do arbitrary action based on it?  I thought the protocol handlers could only return a url.\n\nAs far as cloning, most the needed code is there already.\n. Also keep in mind that chrome apps can create http servers using\nchrome.sockets and a lot of js.\nOn Oct 31, 2014 7:34 PM, \"Kyle Graehl\" notifications@github.com wrote:\n\n> Hmm after doing a little searching I found out there may be a small\n> improvement to be made in this process. Instead of using\n> chrome.runtime.sendMessage it looks like this can bypass that step:\n> https://developer.chrome.com/apps/manifest/url_handlers\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/creationix/js-git/issues/109#issuecomment-61348976.\n. Thanks.\n. You're right.  For a readme, I should use the public paths, not internal paths.\n. Thanks for the fix.  I'm sorry the README errors cost you some time.\n. Actually, diff is not part of git core.  The internal database stores the full version of every file and folder in the history.  The way that git keeps from exploding in size is via packfiles.  js-git doesn't yet support writing packfiles, but it can read them.  Yes packfiles use some form of binary diff internally, but this data is useless for the kind of diff you want to calculate.\n\nSo using one of the filesystem backends to js-git (https://github.com/creationix/git-node-fs for example), you can mount an existing git repo and read it using js-git.  Then using this, you can walk the graph manually using the low-level `.loadAs(type, hash)` api.  Also I provide a couple of higher-level APIs via the walker mixin (https://github.com/creationix/js-git/blob/master/mixins/walkers.js).  `logWalk` lets you walk the history as a flattened linear list of commits (sortes the branches by date).  The `treeWalk` walks the trees and blobs of a commit letting you do a depth first traversal of the tree as a linear stream.\n\nI'm afraid you'll have to manually track the committer and calculate diffs on your own.  I haven't implemented this logic yet.  It might be easier to find some libgit2 bindings for node that I assume would provide the backends to commands like git blame.\n\nSo, yes, js-git can give you the raw data about commits, history, files, contents, etc, but won't calculate composite information like diff, blame, per-file history, etc.\n. I have no idea.  JS-Git is a set of modules that implement various interfaces, the core interface is the .saveAs, .loadAs, etc calls.\n\nJS-Git can use real git repos as backends using the git-fs backend.  There is a concrete implementation using node's FS apis at https://github.com/creationix/git-node-fs\n. JS-Git is super low level, It can easily read local repos, but it doesn't serve them over network protocols or give you user management APIs.\n. Ouch, thanks!\n. Thanks!\n. Sorry, I haven't looked at this in quite a while.  I do remember that clone-test was working over XHR (as long as I installed the browser extension to inject the CORS headers).  It is possible that your code has a bug though I can't see it right away.\n\nI've never seen that trick of using Buffer.concat in the data loop (I usually build a normal array and then concat once inside end), but I think your style should work fine as well.\n\nIt's also possible that something in js-git broke since I was testing clone-test.  Maybe compare the last date I pushed to clone-test with the date of the last js-git and related publishes?\n. Sounds good I think.  Like I said before, I haven't touched this code in years.\n. Thanks!\n. Let me work backwards from where I am today.  It's been a couple years since I've worked on the original js-git.  I recently ported lua-git to use duktape javascript and an example can be found at https://github.com/creationix/dukgit/blob/master/test.js#L58-L64\n\n``` js\n  db = mount(\".git\");\n  var queue = [\"HEAD\"];\n  while (queue.length) {\n    var commit = db.loadAs(\"commit\", queue.shift());\n    p(commit);\n    queue.push.apply(queue, commit.parents);\n  }\n```\n\nBut this does require duktape coroutines which will not work in any other JavaScript VM.  Once async/await lands I can port to that pretty easily.\n\nThe lua-git has a similar API at https://github.com/creationix/lua-git/.\n\nJS-Git was my first API in this regard and is _very_ modular because of the unique constraints I was trying to solve.  The README for https://github.com/creationix/git-node-fs is the simplest way to get started with js-git and node.  Use that to mount a repo.\n\n``` js\nvar repo = {};\nvar path = path.join(\"some/bare/repo.git\");\nrequire('git-node-fs/mixins/fs-db')(repo, path);\n```\n\nOnce you have a repo, loading commits and walking them is pretty easy using direct `repo.loadAs(...)` calls like I do in dukgit. https://github.com/creationix/js-git#basic-object-loading\n\nBut there are also helpers for some common tasks like iterating the history or a file tree as a linear list. \n\nhttps://github.com/creationix/js-git#using-walkers\n\nSo a complete example showing how to walk history and files would be:\n\n``` js\nvar path = require('path');\nvar run = require('gen-run');\n\n// use gen-run to block on contunuables (mini promises) using ES6 yield.\nrun(function*() {\n  // Create a repo object\n  var repo = {};\n  // Mixin the base DB operations using local git database on disk.\n  require('git-node-fs/mixins/fs-db')(repo, path.resolve(__dirname, \"../.git\"));\n  // Mixin the walker helpers.\n  require('js-git/mixins/walkers')(repo);\n\n  // Look up the hash that master currently points to.\n  var commitHash = yield repo.readRef(\"master\");\n\n  // Create a log stream\n  var logStream = yield repo.logWalk(commitHash);\n\n  // Looping through the stream is easy by repeatedly waiting on `read`.\n  var commit, object;\n  while ((commit = yield logStream.read())) {\n\n    console.log(commit);\n\n    // We can also loop through all the files of each commit version.\n    var treeStream = yield repo.treeWalk(commit.tree);\n    while ((object = yield treeStream.read())) {\n      console.log(object);\n    }\n  }\n});\n```\n. Currently js-git is basically just git core, this is simple and you can learn it very quickly.  There are 4 kinds of objects, they are commit, tag, tree, and blob.  Tags are used for annotated tags.  You'll find them used by some projects for release tags.  The tag ref will point to a tag object instead of a commit object.  The tag will contain metadata about the release and link to the commit.\n\nhttps://git-scm.com/book/en/v2/Git-Internals-Git-Objects\n\nCommits point to a tree object and zero or more parent commits (merge commits typically have two parents, but there can be more!).  They also have extra metadata like tags do.\n\nA tree contains a list of pointers to other trees, blobs, or commits (submodules).  Each entry will have a name, mode, and hash.\n\nA blob is just raw binary data.\n\nSo using `repo.loadAs` you can manually walk the tree looking for things.\n\nJS-git doesn't have support for the working directory so you can't do things like `git status`.  Also I haven't implemented diff yet so anything involving that can't be done including merges.  The network actions like push and pull aren't done either.  The main thing you can do currently with js-git is read/write the core git database directly.\n. I would like to take another stab at this, but I'm in a different situation now.  I don't need money anymore, I have a well paying salaried job.  My current limitation is time since any time I spend on js-git will take away from time with my children.  That said, there are some nights I can't sleep because of a mild sleep disorder and I will often work on stuff while they sleep.\n\nAs far as technical direction, I learned a lot from the failure, that was the js-git fundraiser.  My goals were too ambitious and the browser tech wasn't ready for what I was trying to make.\n\nToday, we have async/await coming soon and transpilers that let you use it now.  Also with service workers coming along we have much better offline support.\n\nAs far as I know, browsers still have no way to make direct TCP connections (for git://) to git servers and no git hosting company has added CORS headers to their smart-https (https://) endpoints.  I've designed a great websocket based git protocol (wss://) that can be easily deployed if you host your own git repos or could be setup as a proxy to other git servers and would love if it became used eventually by hosting services like Github.\n\nInitially I focussed on browser-based apps using only native browser I/O and direct connections to services (which is still impossible), but if we relax the requirements a little and allow proxies for browser apps, custom hosted git services, or node.js or chrome app clients, then the network constraints are no longer an issue.\n. @kmalakoff technically speaking, ssh is no harder than `git://` or `http://` since they all require a TCP connection which browsers don't have, but practically speaking, implementing ssh on top of tcp is a _lot_ more work than implementing just `git://` or even `http://`.\n\nImplementing `https://` is simply `http://` over TLS.  I've implemented HTTP in pure js and lua multiple times, that isn't hard, especially for the reduced use case of git clones.\n\nThere is a pure js implementation of tls that I've been using at https://github.com/digitalbazaar/forge, also I've been recently discovered https://tls.mbed.org/ which can probably be compiled to JS using emscripten.  My experience with emscripten, however, is that it generates massive JS blobs (hence the primary motivator for web assembly).\n\nI believe that implementing enough of ssh to do a git clone over it isn't that bad.  Also on the node side, you have the openssl bindings available and can use something like https://github.com/mscdex/ssh2.\n. Actually, thinking about it, about the only use case where you'd be interested in reading ssh keys from `$HOME/.ssh/` is on a full developer workstation running Linux or OSX.  In those cases you have access to node or are probably using electron if writing a graphical native app so still have node access.\n\nWhile it is possible to generate ssh keys in a browser or chrome app and store them in some persistent storage, I doubt it will be the default workflow for most people.  I would expect https be used more in those use cases.\n. The real git clients uses an index file to cache data for status, see the format at https://github.com/git/git/blob/master/Documentation/technical/index-format.txt\n. The issue I had before with the index file approach was that not all the filesystem backends I was targeting supported fine grained stat data. (Chrome's FS API for example)\n\nIt's hard to abstract because some backends will want to use the git index format while others will have to invent something custom to their abilities.\n. So the two-level abstraction is what I've planned to do all along.  The higher level abstraction will consume the lower-level abstraction.  In this case, there will be something for working checkouts and it only needs to know if a file has changed, the git index format on disk is an implementation detail of one of the backends.  But many platforms can offer a good FS level implementation and for those, it would be good for js-git to bridge the gap between fs and working directory for you.\n\nI already do this with git-fs which implements the js-git database interface on top of a generic file system interface.  You don't have to use git-fs if you're on some strange platform where it doesn't make sense.\n\nFor example, I had a backend that stored objects and refs in indexeddb in the browser without first emulating a filesystem.  That would be unneeded overhead and complexity.\n\nSo to answer your question, the git-fs module needs to implement the needed high-level APIs for doing status, add, checkout and other working directory commands.  We also need to design what exactly those APIs are, but not hard-code them to the filesystem.  Users of js-git will be able to choose implementing the high-level interface directly or just implementing the low-level fs API and using git-fs to bridge the interface gap.\n. Sorry for the rambling, I've been sick this weekend and don't have a lot of brain power left.  But I hope this directs you in the right direction.  Feel free to propose added APIs as needed in the high-level space.\n. Yes, I think you're right.  Sorry about getting the names wrong (it's literally been years since I worked on this stuff).\n\nMost the mixins and programs use the db interface directly, there isn't much in the way of super high-level APIs yet (no merge, diff, blame, per-file history, etc).  The tedit project implements a FS on top of the js-git db interface (https://tedit.creationix.com/ https://github.com/creationix/tedit)  The \n\nI am giving a talk in Paris in a month (http://www.dotjs.io/) and hope to  be using JS-Git for a some of it.  This means I'll have some time to use this code myself and hopefully get things a little farther along.\n. I'm not sure I want to change the official js-git to typescript, but I think you're doing something cool.  I can certainly see the benefits you derive from this.\n. You'll want to have the protocol docs handy for sure. https://github.com/git/git/blob/master/Documentation/technical/pack-protocol.txt\n\nhttps://gist.github.com/schacon/6092633\n\nPart of fetch (the main part) is parsing a pack file and streaming the objects to the js-git data store.  If I remember correctly, this needs raw loading and saving objects (used to apply delta objects).  I've written code for clone a few times in a few different places.  I even once had a node command-line tool that would clone using node + js, it's published to npm at https://www.npmjs.com/package/jsgit and you can probably find the code by installing it. (I'm pretty sure it's for an older incarnation of js-git though).\n\nThe only parts of js-git that are truly useful and re-usable are the codecs.  Parsing and writing git objects, applying binary deltas given two buffers, streaming parsing of pack-files, etc.  Feel free to change the rest to whatever works best.\n. At one point, I had a demo working that could clone over XHR (as long as you installed a browser extension to get around the cross-domain issue Github refuses to fix).\n\nhttps://github.com/creationix/clone-test\n\nhttps://github.com/creationix/corson\n. Clone and pull are the same at the protocol level (they are both fetch), but what you do locally with the refs is what makes them different.  Push is a similar protocol, but I never implemented it because I never had a secure channel from the browser to github.  Cloud hosted proxies work\\* for cloning public stuff, but nor for pushing changes because then the proxy can man-in-the-middle and get your credentials.\n\n*work - as is it technically works, but it's a pain to host a proxy just to read a public resource.\n. If working code is easier for you, so be it, but git tends to use a lot of mem-mapping that isn't possible in JavaScript.  I personally had better luck just reading the protocol [spec](https://github.com/git/git/blob/master/Documentation/technical/pack-protocol.txt) and then implementing from scratch, but then again, I'm not that good at reading other people's code yet.\n. I'm not sure what the issue is. (Sorry I'm sick and can't think clearly at the moment), but I just noticed you're using promises and await.  You might have a better time wrapping the continuable instead of using the callback interface.\n\nFor example, make a function that converts continuables to full promises.\n\n``` js\nfunction c2p(fn) {\n  return new Promise(function (reject, resolve) {\n    return fn(function (err, result) {\n      if (err) { return reject(err); }\n      return resolve(result);\n    };\n  };\n}\n```\n\nThen you can do:\n\n``` js\nvar result = await c2p(logStream.read());\n```\n. By \"binary\" do you mean \"decimal\"?  Also since these are just constants, I'm fine with using parseInt.  The performance cost will be one-time for the most part.\n. Thanks.  I guess I've never ran across a case that exercises this code.  How did you run across it?\n. Also master was missing some commits.  I think everything is merged in now and `0.7.8` is published to npm.\r\n\r\nLet me know if this helps.\r\n\r\nAlso I have some newer git code that I wrote for a recent project I used rollup on.\r\n\r\nhttps://github.com/creationix/revision/blob/master/src/libs/git-codec.ts\r\n. Good luck fellows!. And sorry I never had the time to finish this project.  Gotta pay the bills and I couldn't get enough funding to finish this.. So you just want real only of public repos that allow cloning over the smart http protocol?  What about the plain TCP git:// protocol? . Back when I was working on this, I had written this extension that added CORS to github response headers . https://github.com/creationix/corson\r\n\r\nNow to see if I can find the repo that powered the test site.. Found it.  See if this helps. https://github.com/creationix/clone-test\r\n. Yep you're right.  Thanks for digging in.  I completely forgot about this and wouldn't have been able to help.\r\n\r\nhttps://github.com/creationix/js-git/blob/master/mixins/pack-ops.js#L16-L18\r\n\r\n. I am confused. If you could check the modified version in the README, let me know if it works.. ",
    "mrscreationix": "Miranda is done with this :D\n. I do too, davidmarkclements!\n. All Open Web Warrior rewards are ready to be sent, minus the one who hasn't filled out his survey.  Will go out in the morning (May 9th) from New Boston, TX.\n. All domestic Open Web Warrior rewards were sent out.  The 8 international rewards will be mailed out this afternoon after I fill out all 8 customs forms.  I'm not sure if an $8 sheet of stickers really needs a customs form, but I'd rather fill it out when it didn't need it, than leave it unfulfilled and have it returned or lost or something.  These are some pretty cool stickers!\n. Oops!  I forgot to get to the post office before they closed today!  (Rural towns like ours don't have APCs, so if you don't get there before they close, you are out of luck.)  LOTS of rewards will go out tomorrow!\n. ALMOST DONE!!  Big handful of rewards went to the PO, and another big handful are sitting in my mailbox waiting for the post-person to come by on his/her rounds.  About 40 left!  All Open Web Warrior rewards are done, about 2/3 of the Believer rewards are done.\n. Miranda is done with this :D\n. I do too, davidmarkclements!\n. All Open Web Warrior rewards are ready to be sent, minus the one who hasn't filled out his survey.  Will go out in the morning (May 9th) from New Boston, TX.\n. All domestic Open Web Warrior rewards were sent out.  The 8 international rewards will be mailed out this afternoon after I fill out all 8 customs forms.  I'm not sure if an $8 sheet of stickers really needs a customs form, but I'd rather fill it out when it didn't need it, than leave it unfulfilled and have it returned or lost or something.  These are some pretty cool stickers!\n. Oops!  I forgot to get to the post office before they closed today!  (Rural towns like ours don't have APCs, so if you don't get there before they close, you are out of luck.)  LOTS of rewards will go out tomorrow!\n. ALMOST DONE!!  Big handful of rewards went to the PO, and another big handful are sitting in my mailbox waiting for the post-person to come by on his/her rounds.  About 40 left!  All Open Web Warrior rewards are done, about 2/3 of the Believer rewards are done.\n. ",
    "davidmarkclements": "mrscreationix hah! love it!\n. +1 for modular though, there will be more mileage from a modular approach than a monolithic approach\n. mrscreationix hah! love it!\n. +1 for modular though, there will be more mileage from a modular approach than a monolithic approach\n. ",
    "chrisdickinson": "current working theory:\n1. git in js/browser can lean on browserify so the various modules work both in-browser and out\n2. git in browser should not implement packfile / loose stores, it can use whatever \"natural\" for the browser; this also means that we don't _need_ a filesystem.\n3. all that needs to happen to get git in the browser working is to be able to generate and parse packfiles (sans index files).\n\nthis also implies:\n1. git in browser need not necessarily parse or create `.git/index` files; at least not necessarily in the same way that filesystem-backed git does.\n\nwhat does this leave out?\n- garbage collection / repacking.\n- I don't have a plan for human-readable diff tooling yet.\n- We can get away with inefficient packfiles at first for a quick demo; i.e., we don't need to be able to create packfiles containing `ofs` and `ref` deltas off the bat; the [packing heuristics](http://www.opensource.apple.com/source/Git/Git-33/src/git-htmldocs/technical/pack-heuristics.txt) are kind of poorly documented.\n\nwhat are weaknesses of the browserify approach?\n- Buffers are shimmed in browserify -- but aren't backed by fast Uint8Arrays. This kind of sucks.\n- The `zlib` shim only includes `inflateSync`, which means that we'll have to make use of the `\"browser\"` field in package.json's to use inflate/deflate. And those shims should certainly seek to use web workers where possible, to lower the chance of a \"this tab timed out\" notification while generating/parsing packfiles.\n\nwhat are weaknesses in node that make this hard?\n- To parse a packfile without an index, you have to be able to say \"take N bytes into inflate until you get a result of X size\"; currently node's zlib (and by extension, browserify's), doesn't handle this use case well.\n\nmy plan for the near future:\n- create a readable/writable stream that accepts packfile data and outputs offsets that can be used by `git-packfile` to read objects.\n- square away a `git-smart-http-remote` repo that works with `git-fetch-pack`.\n- work on a quick read-only git demo in browser\n- then do the read/write demo.\n. @maks yep, the idea is that the \"local checkout\" package will be entirely separate from the other packages -- you'll populate it using [git-walk-tree](http://npm.im/git-walk-tree).\n\nbasically, you should be able to use any \"local checkout\" package that supports being piped to from walk-tree; whether that's backed by plain JS objects, indexeddb, localstorage, requestFileSystem, etc, etc.\n\nThe other bonus point here is that by making the utility functions agnostic of the ODB being used (and the filesystem), you could actually back this with redis (or some other KV store) on the server.\n. @creationix Agreed on there not being a huge amount of value for interop at the filesystem level; at best it provides us with test data to feed into the other packages. It shouldn't be the focus (though really, at this point it's 90% done -- both pack and loose odbs are implemented and while it's currently readonly, it's really simple to create loose pack objects).\n\nre: browserify's shims: `zlib` is probably the most problematic -- though that can be shimmed for something like `zpipe` using browserify's `\"browser\": {\"./file.js\": \"shim-file.js\"}` package.json directive. Really, the primary problem with most of the zlib implementations I've found is that they don't differentiate data returned with `Z_STREAM_END` from data returned with `Z_OK`; I've been banging my head against this for the last week or so. Ideally, we'll support node (so you can run a git server entirely in JS), so this'll have to get solved sooner rather than later. Dan Lucraft's `git.js` got around this by including a slightly modified version of `raw-deflate.js` (the zlib implementation that's been floating around the internet, unlicensed, since 1998) that counts the number of compressed bytes involved in generating output.\n\nIn practice, the buffer shim isn't a huge problem -- it can be mostly solved by converting to typed arrays either on-the-fly (which is what I've been doing thus far) using `new Uint8Array(buffer.parent).subarray(buffer.offset, buffer.offset + buffer.length)` and keeping a map of buffer parents to uint8array instances around (to offset the cost of creating uint8arrays) or at entry / exit points (harder, since this implies that every stream will have to be reworked to accept only uint8arrays and have to be piped through a conversion stream. Really, browserify buys us a lot more than it costs us -- especially with the `--standalone` generation option.\n\n@kuba-kubula thanks for the link! I recently found (like, last night) that imaya's inflate implementation might not be able to inflate certain decompressed streams (namely, those with predefined dictionaries) so I might try plugging in zpipe instead.\n\nIf all goes well (and I'm optimistic -- I just got the `git-pack-unpack` stream 90% working last night; it emits offsets of objects within a packfile so we can generate an index / pull them out and put those objects in a more browser-sane object db), I should have some sort of demo by the end of the coming weekend.\n. also, in re: the zlib shim not being ideal -- it's fairly easy to swap it out for other implementations using browserify.\n\nthe harder part is node: its builtin implementation is problematic WRT actually figuring out how many compressed bytes it takes to generate a payload of a given size ):\n. ahaha, ignore me! browserify's zlib deals just fine with the data, I was just feeding it bad data. So, imaya's zlib should work.\n\n@kuba-kubula looked into zpipe. Sadly, it probably won't work for our purposes:\n- There's no license attached.\n- It only operates on strings (this is a no-no -- we need arrays or typed arrays or buffers -- since utf8 can be lossy (invalid characters might be transcoded to `0xFFFD` which loses the original data the characters represented)). \n. [this will pull down packfiles and emit unparsed objects and offsets](https://gist.github.com/chrisdickinson/5290339)\n. okay, so the [inflate implementation](https://github.com/chrisdickinson/inflate) is \"fast enough\" for now -- there are a couple of places where it could be sped up, but it doesn't take an hour to clone \"mature\" repos now. I've wired it up locally using levelidb for storage.\n\nNext steps:\n1. Need a module that given a commit object, a \"find\" function, and a path, can reconstitute the data for that path.\n2. Need to rework the `git-walk-tree` module so that it can check the type of an object before recursing into it (to avoid loading all of the blobs at once).\n3. Creating commits.\n4. Creating packfiles.\n5. Implementing `git-receive-pack` and `git-send-pack` similar to how `git-fetch-pack` works.\n. @creationix Cool. Welcome back! So, ideally, this repo would provide the following: the project site itself plus possibly a dev blog (as a gh-pages branch), the documentation (which should cover the constituent libraries and give those curious an idea of how it all hangs together), and a porcelain library which should provide similar functionality to the `git` commands after loading a repo. \n\nSuper ideally, but totally up to you: \n- It would use vanilla node concepts -- streams, event emitters, callbacks; \n- It should be concentrated on solving the 80% use case instead of being flexible. i.e., we _could_ allow plugging different odb backends here, but it might be better to just pick one that works in node and one that works in browser and ship something like that.\n. some thoughts:\n\n`git.hashObject(object)` should probably accept a `Buffer` or `TypedArray` instance. Strings can be lossy with utf8 encoding ): We could make the api something like so:\n\n``` javascript\nvar obj = git.objects.commit('author', 'message')\n\ngit.save(obj, function(err, oid) {\n\n})\n\n```\n\n`fs.rmfile` might be best as `fs.unlink` for parity with node's `fs` module.\n\n`git.updateIndex` might be better as `git.index.(get|set|remove)`.\n. @juliangruber We'll probably need a `db.keys(callback)` that returns all keys for gc'ing, but other than that there should be relatively few operations that require fine-grained control over key iteration.\n\nBasically, the less we need from `db`, the better -- that way it can easily be represented by `localStorage`, `indexeddb`, `js object / json`, `fs`, etc.\n. that actually brings up an interesting implication: gc'ing in this approach means that we need to get _all_ of the refs from _all_ of the repos (represented by uuids) and iterate keys to find the ones that aren't reachable, assuming object ids (hashes) aren't prefixed with the uuid (which would be nice for storage size reasons).\n. @creationix awesome work! that would be great as a standalone package on npm!\n. I took some time to put together [a little playground](https://github.com/chrisdickinson/simple-streams-playground) to test simple-streams out. I have yet to test out the duplex pattern.\n\nSome thoughts:\n- sinks need more of a spec. in particular, I didn't know what how to communicate completion/errors out to client, non-stream code. I'm +1 on @Raynos's suggestion.\n- it feels _much_ easier to get right than min-streams.\n- is it the responsibility of the sink to call `.end` on its input stream to allow for cleanup? or is `stream.end()` more exceptional than that?\n- what do we do with extra read callbacks? the [dom example](https://github.com/chrisdickinson/simple-streams-playground/blob/master/dom) has a [filter stream](https://github.com/chrisdickinson/simple-streams-playground/blob/master/dom/lib/filter.js#L47) module that collects a list of callbacks. right now I simply truncate that buffer on `end`, then forward the `end`. is this correct?\n- there's more boilerplate than using `through`, but impressively, not much.\n. Yeah. I was thinking that `cancel` or `abort`  would be more descriptive than `end`. +1 on a name change. \n\nOn Jul 2, 2013, at 8:33 AM, Raynos notifications@github.com wrote:\n\n> btw I would vote for { read, abort } as more intuitive then { next, end }\n> \n> Also \"just make sure to always eventually call every callback.\" answers a lot of questions about read and abort interaction.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. @tsgautier if you're looking for a simple read-only interface to git repos containing pack files in node, you might try [git-fs-repo](http://npm.im/git-fs-repo) in the meantime until this feature is in js-git. \n\n@creationix this seems like it might be a milestone 1 goal, as it'll be a pretty common use case. \n. current working theory:\n1. git in js/browser can lean on browserify so the various modules work both in-browser and out\n2. git in browser should not implement packfile / loose stores, it can use whatever \"natural\" for the browser; this also means that we don't _need_ a filesystem.\n3. all that needs to happen to get git in the browser working is to be able to generate and parse packfiles (sans index files).\n\nthis also implies:\n1. git in browser need not necessarily parse or create `.git/index` files; at least not necessarily in the same way that filesystem-backed git does.\n\nwhat does this leave out?\n- garbage collection / repacking.\n- I don't have a plan for human-readable diff tooling yet.\n- We can get away with inefficient packfiles at first for a quick demo; i.e., we don't need to be able to create packfiles containing `ofs` and `ref` deltas off the bat; the [packing heuristics](http://www.opensource.apple.com/source/Git/Git-33/src/git-htmldocs/technical/pack-heuristics.txt) are kind of poorly documented.\n\nwhat are weaknesses of the browserify approach?\n- Buffers are shimmed in browserify -- but aren't backed by fast Uint8Arrays. This kind of sucks.\n- The `zlib` shim only includes `inflateSync`, which means that we'll have to make use of the `\"browser\"` field in package.json's to use inflate/deflate. And those shims should certainly seek to use web workers where possible, to lower the chance of a \"this tab timed out\" notification while generating/parsing packfiles.\n\nwhat are weaknesses in node that make this hard?\n- To parse a packfile without an index, you have to be able to say \"take N bytes into inflate until you get a result of X size\"; currently node's zlib (and by extension, browserify's), doesn't handle this use case well.\n\nmy plan for the near future:\n- create a readable/writable stream that accepts packfile data and outputs offsets that can be used by `git-packfile` to read objects.\n- square away a `git-smart-http-remote` repo that works with `git-fetch-pack`.\n- work on a quick read-only git demo in browser\n- then do the read/write demo.\n. @maks yep, the idea is that the \"local checkout\" package will be entirely separate from the other packages -- you'll populate it using [git-walk-tree](http://npm.im/git-walk-tree).\n\nbasically, you should be able to use any \"local checkout\" package that supports being piped to from walk-tree; whether that's backed by plain JS objects, indexeddb, localstorage, requestFileSystem, etc, etc.\n\nThe other bonus point here is that by making the utility functions agnostic of the ODB being used (and the filesystem), you could actually back this with redis (or some other KV store) on the server.\n. @creationix Agreed on there not being a huge amount of value for interop at the filesystem level; at best it provides us with test data to feed into the other packages. It shouldn't be the focus (though really, at this point it's 90% done -- both pack and loose odbs are implemented and while it's currently readonly, it's really simple to create loose pack objects).\n\nre: browserify's shims: `zlib` is probably the most problematic -- though that can be shimmed for something like `zpipe` using browserify's `\"browser\": {\"./file.js\": \"shim-file.js\"}` package.json directive. Really, the primary problem with most of the zlib implementations I've found is that they don't differentiate data returned with `Z_STREAM_END` from data returned with `Z_OK`; I've been banging my head against this for the last week or so. Ideally, we'll support node (so you can run a git server entirely in JS), so this'll have to get solved sooner rather than later. Dan Lucraft's `git.js` got around this by including a slightly modified version of `raw-deflate.js` (the zlib implementation that's been floating around the internet, unlicensed, since 1998) that counts the number of compressed bytes involved in generating output.\n\nIn practice, the buffer shim isn't a huge problem -- it can be mostly solved by converting to typed arrays either on-the-fly (which is what I've been doing thus far) using `new Uint8Array(buffer.parent).subarray(buffer.offset, buffer.offset + buffer.length)` and keeping a map of buffer parents to uint8array instances around (to offset the cost of creating uint8arrays) or at entry / exit points (harder, since this implies that every stream will have to be reworked to accept only uint8arrays and have to be piped through a conversion stream. Really, browserify buys us a lot more than it costs us -- especially with the `--standalone` generation option.\n\n@kuba-kubula thanks for the link! I recently found (like, last night) that imaya's inflate implementation might not be able to inflate certain decompressed streams (namely, those with predefined dictionaries) so I might try plugging in zpipe instead.\n\nIf all goes well (and I'm optimistic -- I just got the `git-pack-unpack` stream 90% working last night; it emits offsets of objects within a packfile so we can generate an index / pull them out and put those objects in a more browser-sane object db), I should have some sort of demo by the end of the coming weekend.\n. also, in re: the zlib shim not being ideal -- it's fairly easy to swap it out for other implementations using browserify.\n\nthe harder part is node: its builtin implementation is problematic WRT actually figuring out how many compressed bytes it takes to generate a payload of a given size ):\n. ahaha, ignore me! browserify's zlib deals just fine with the data, I was just feeding it bad data. So, imaya's zlib should work.\n\n@kuba-kubula looked into zpipe. Sadly, it probably won't work for our purposes:\n- There's no license attached.\n- It only operates on strings (this is a no-no -- we need arrays or typed arrays or buffers -- since utf8 can be lossy (invalid characters might be transcoded to `0xFFFD` which loses the original data the characters represented)). \n. [this will pull down packfiles and emit unparsed objects and offsets](https://gist.github.com/chrisdickinson/5290339)\n. okay, so the [inflate implementation](https://github.com/chrisdickinson/inflate) is \"fast enough\" for now -- there are a couple of places where it could be sped up, but it doesn't take an hour to clone \"mature\" repos now. I've wired it up locally using levelidb for storage.\n\nNext steps:\n1. Need a module that given a commit object, a \"find\" function, and a path, can reconstitute the data for that path.\n2. Need to rework the `git-walk-tree` module so that it can check the type of an object before recursing into it (to avoid loading all of the blobs at once).\n3. Creating commits.\n4. Creating packfiles.\n5. Implementing `git-receive-pack` and `git-send-pack` similar to how `git-fetch-pack` works.\n. @creationix Cool. Welcome back! So, ideally, this repo would provide the following: the project site itself plus possibly a dev blog (as a gh-pages branch), the documentation (which should cover the constituent libraries and give those curious an idea of how it all hangs together), and a porcelain library which should provide similar functionality to the `git` commands after loading a repo. \n\nSuper ideally, but totally up to you: \n- It would use vanilla node concepts -- streams, event emitters, callbacks; \n- It should be concentrated on solving the 80% use case instead of being flexible. i.e., we _could_ allow plugging different odb backends here, but it might be better to just pick one that works in node and one that works in browser and ship something like that.\n. some thoughts:\n\n`git.hashObject(object)` should probably accept a `Buffer` or `TypedArray` instance. Strings can be lossy with utf8 encoding ): We could make the api something like so:\n\n``` javascript\nvar obj = git.objects.commit('author', 'message')\n\ngit.save(obj, function(err, oid) {\n\n})\n\n```\n\n`fs.rmfile` might be best as `fs.unlink` for parity with node's `fs` module.\n\n`git.updateIndex` might be better as `git.index.(get|set|remove)`.\n. @juliangruber We'll probably need a `db.keys(callback)` that returns all keys for gc'ing, but other than that there should be relatively few operations that require fine-grained control over key iteration.\n\nBasically, the less we need from `db`, the better -- that way it can easily be represented by `localStorage`, `indexeddb`, `js object / json`, `fs`, etc.\n. that actually brings up an interesting implication: gc'ing in this approach means that we need to get _all_ of the refs from _all_ of the repos (represented by uuids) and iterate keys to find the ones that aren't reachable, assuming object ids (hashes) aren't prefixed with the uuid (which would be nice for storage size reasons).\n. @creationix awesome work! that would be great as a standalone package on npm!\n. I took some time to put together [a little playground](https://github.com/chrisdickinson/simple-streams-playground) to test simple-streams out. I have yet to test out the duplex pattern.\n\nSome thoughts:\n- sinks need more of a spec. in particular, I didn't know what how to communicate completion/errors out to client, non-stream code. I'm +1 on @Raynos's suggestion.\n- it feels _much_ easier to get right than min-streams.\n- is it the responsibility of the sink to call `.end` on its input stream to allow for cleanup? or is `stream.end()` more exceptional than that?\n- what do we do with extra read callbacks? the [dom example](https://github.com/chrisdickinson/simple-streams-playground/blob/master/dom) has a [filter stream](https://github.com/chrisdickinson/simple-streams-playground/blob/master/dom/lib/filter.js#L47) module that collects a list of callbacks. right now I simply truncate that buffer on `end`, then forward the `end`. is this correct?\n- there's more boilerplate than using `through`, but impressively, not much.\n. Yeah. I was thinking that `cancel` or `abort`  would be more descriptive than `end`. +1 on a name change. \n\nOn Jul 2, 2013, at 8:33 AM, Raynos notifications@github.com wrote:\n\n> btw I would vote for { read, abort } as more intuitive then { next, end }\n> \n> Also \"just make sure to always eventually call every callback.\" answers a lot of questions about read and abort interaction.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. @tsgautier if you're looking for a simple read-only interface to git repos containing pack files in node, you might try [git-fs-repo](http://npm.im/git-fs-repo) in the meantime until this feature is in js-git. \n\n@creationix this seems like it might be a milestone 1 goal, as it'll be a pretty common use case. \n. ",
    "maks": "@chrisdickinson that's really great work.\n\nworking theory points 2 & 3 I whole heartedly agree with, there is no point using c-git filesystem dir structure for object or metadata storage in browser. But then there's the practical need to actual be able to checkout a working direcvtory which does require something resembling a typical file system.\n\nLikewise agree with not needing to use existing index file format, especially given its a binary format that's not especially well documented, but will need to have something similar to allow for staging commits, the index does serve that rather core purpose of preparing what will become the next commits tree and diff'ing current working dir against it.\n\nfor testing against smart http remotes from a browser, pushover seems a great fit, as it's simple to add CORS support by putting a reverse proxy in front of it or maybe just adding it directly to pushover itself.\n. pretty much agree with @chrisdickinson about a porcelain level library and documentation and \"reference\" implemenation. Especially for the in-browser use case, I'd imagine that people (such as me) would want to use it directly within a text editor/IDE webapp due to sandboxing, rather than as some sort of stand-alone app like you would on a desktop/mobile os.\n\nWith different ODBs, @chrisdickinson doesn't your https://github.com/chrisdickinson/git-odb already provide a nice pluggable means of having multiple backends?\n. Its very strange that you saw that error, as so far I've only come across it while working on the index (aka dircache) implementation in git-html5 and as far as I know js-git has no implementation for writing the index file in the same binary format the cgit (original git) uses.\n. oh hey @creationix I didn't realise that! Well that will be handy to refer to as I'm in the middle of getting my writing working. But in relation to this, in my tests atm as far as I can tell I'm seeing this due to a bad value in the 16-bit flags field, the one just before the pathname field in a ver2 index file.\n. @creationix yes but I was thinking about the \"platform\" package and what implementations of it need to provide. \n\nBut now that I think about I have confused the low-level with higher level requirements!\n\nSo with https://github.com/creationix/js-git-node-platform it provides node-based implementations of: sha1, inflate/deflate, fs, http, ssh, etc\n\nAnd then a (imaginary) js-git-node-lib would provide object-db, working-dir, index-db, ls-remote, fetch, push, etc on top of what the platform module provides. The rough equivalent of cgit \"plumping\" cmds.\n\nDoes that makes sense?\n. @creationix Yes you're right as usual! :-) \nI had an even harder think about it and yes i can see that a platform just needs to _expose_ the git-db interface for bare repo access.\nFor any kind of remote ops, it needs to expose at least 1 transport protocol (http, ssh, etc). \nAnd then to support working dirs it needs to expose a fs interface.\n\nBut what about the index, do we need a seperate interface for it? or is it just something that client code of js-git will just handle itself?\n. As discussed on irc, the index interface should have: add/remove/read/list functions.\n. @chrisdickinson that's really great work.\n\nworking theory points 2 & 3 I whole heartedly agree with, there is no point using c-git filesystem dir structure for object or metadata storage in browser. But then there's the practical need to actual be able to checkout a working direcvtory which does require something resembling a typical file system.\n\nLikewise agree with not needing to use existing index file format, especially given its a binary format that's not especially well documented, but will need to have something similar to allow for staging commits, the index does serve that rather core purpose of preparing what will become the next commits tree and diff'ing current working dir against it.\n\nfor testing against smart http remotes from a browser, pushover seems a great fit, as it's simple to add CORS support by putting a reverse proxy in front of it or maybe just adding it directly to pushover itself.\n. pretty much agree with @chrisdickinson about a porcelain level library and documentation and \"reference\" implemenation. Especially for the in-browser use case, I'd imagine that people (such as me) would want to use it directly within a text editor/IDE webapp due to sandboxing, rather than as some sort of stand-alone app like you would on a desktop/mobile os.\n\nWith different ODBs, @chrisdickinson doesn't your https://github.com/chrisdickinson/git-odb already provide a nice pluggable means of having multiple backends?\n. Its very strange that you saw that error, as so far I've only come across it while working on the index (aka dircache) implementation in git-html5 and as far as I know js-git has no implementation for writing the index file in the same binary format the cgit (original git) uses.\n. oh hey @creationix I didn't realise that! Well that will be handy to refer to as I'm in the middle of getting my writing working. But in relation to this, in my tests atm as far as I can tell I'm seeing this due to a bad value in the 16-bit flags field, the one just before the pathname field in a ver2 index file.\n. @creationix yes but I was thinking about the \"platform\" package and what implementations of it need to provide. \n\nBut now that I think about I have confused the low-level with higher level requirements!\n\nSo with https://github.com/creationix/js-git-node-platform it provides node-based implementations of: sha1, inflate/deflate, fs, http, ssh, etc\n\nAnd then a (imaginary) js-git-node-lib would provide object-db, working-dir, index-db, ls-remote, fetch, push, etc on top of what the platform module provides. The rough equivalent of cgit \"plumping\" cmds.\n\nDoes that makes sense?\n. @creationix Yes you're right as usual! :-) \nI had an even harder think about it and yes i can see that a platform just needs to _expose_ the git-db interface for bare repo access.\nFor any kind of remote ops, it needs to expose at least 1 transport protocol (http, ssh, etc). \nAnd then to support working dirs it needs to expose a fs interface.\n\nBut what about the index, do we need a seperate interface for it? or is it just something that client code of js-git will just handle itself?\n. As discussed on irc, the index interface should have: add/remove/read/list functions.\n. ",
    "maxogden": "fs-browserify could be useful here, it implements a few of the most common fs operations on top of indexeddb\n\nSent from my iPhone\n\nOn Mar 31, 2013, at 4:25 AM, Maksim Lin notifications@github.com wrote:\n\n> @chrisdickinson that's really great work.\n> \n> working theory points 2 & 3 I whole heartedly agree with, there is no point using c-git filesystem dir structure for object or metadata storage in browser. But then there's the practical need to actual be able to checkout a working direcvtory which does require something resembling a typical file system.\n> \n> Likewise agree with not needing to use existing index file format, especially given its a binary format that's not especially well documented, but will need to have something similar to allow for staging commits, the index does serve that rather core purpose of preparing what will become the next commits tree and diff'ing current working dir against it.\n> \n> for testing against smart http remotes from a browser, pushover seems a great fit, as it's simple to add CORS support by putting a reverse proxy in front of it or maybe just adding it directly to pushover itself.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. fs-browserify could be useful here, it implements a few of the most common fs operations on top of indexeddb\n\nSent from my iPhone\n\nOn Mar 31, 2013, at 4:25 AM, Maksim Lin notifications@github.com wrote:\n\n> @chrisdickinson that's really great work.\n> \n> working theory points 2 & 3 I whole heartedly agree with, there is no point using c-git filesystem dir structure for object or metadata storage in browser. But then there's the practical need to actual be able to checkout a working direcvtory which does require something resembling a typical file system.\n> \n> Likewise agree with not needing to use existing index file format, especially given its a binary format that's not especially well documented, but will need to have something similar to allow for staging commits, the index does serve that rather core purpose of preparing what will become the next commits tree and diff'ing current working dir against it.\n> \n> for testing against smart http remotes from a browser, pushover seems a great fit, as it's simple to add CORS support by putting a reverse proxy in front of it or maybe just adding it directly to pushover itself.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n. ",
    "kuba-kubula": "For zip and other packaging, what about emscripten / asm.js conversion of zlib? see: https://github.com/richardassar/zpipe\nor the whole list of compiled into JS libs at https://github.com/kripken/emscripten/wiki\n. Yay! :+1:\n. Got mine today, and I'm in _Prague - Czech Republic_ in Europe :)\nBig thanks.\n. For zip and other packaging, what about emscripten / asm.js conversion of zlib? see: https://github.com/richardassar/zpipe\nor the whole list of compiled into JS libs at https://github.com/kripken/emscripten/wiki\n. Yay! :+1:\n. Got mine today, and I'm in _Prague - Czech Republic_ in Europe :)\nBig thanks.\n. ",
    "juliangruber": "@chrisdickinson you are purely awesome!\n. Re db: What about iterators over keys?\n- `db.each(fn)` // iterate over everything\n- `db.each('foo/bar', fn)` // iterate over everything in `foo/bar`\n- `db.each({ start : 'a', end : 'z' }, fn)` // iterate over a range\n\nOr is adding a strict sorting of keys difficult with the underlying implementation?\n\nRe fs: Could `fs.writefile` have an append mode?\n. [mux-demux](https://github.com/dominictarr/mux-demux)?\n. [tee-1](https://github.com/godmodelabs/tee)?\n. @jez0990 I think it should rather by a storage option itself, right? So if you make it compatibly with the [abstract leveldown](https://github.com/rvagg/abstract-leveldown) interface people can use it with levelup in the browser or in node.\n. @chrisdickinson you are purely awesome!\n. Re db: What about iterators over keys?\n- `db.each(fn)` // iterate over everything\n- `db.each('foo/bar', fn)` // iterate over everything in `foo/bar`\n- `db.each({ start : 'a', end : 'z' }, fn)` // iterate over a range\n\nOr is adding a strict sorting of keys difficult with the underlying implementation?\n\nRe fs: Could `fs.writefile` have an append mode?\n. [mux-demux](https://github.com/dominictarr/mux-demux)?\n. [tee-1](https://github.com/godmodelabs/tee)?\n. @jez0990 I think it should rather by a storage option itself, right? So if you make it compatibly with the [abstract leveldown](https://github.com/rvagg/abstract-leveldown) interface people can use it with levelup in the browser or in node.\n. ",
    "ptillemans": "got mine in the post this morning. :-D\n. got mine in the post this morning. :-D\n. ",
    "joefeser": "The stickers look amazing. Great Job.\n. The stickers look amazing. Great Job.\n. ",
    "mstade": "Are these smaller separate modules considered the plumbing and if so, will this project provide porcelain? Would it perhaps make sense to move longer running API discussions and the like to issues here in GitHub instead of IRC, to increase visibility?\n. +1 for discussion on github!\n\nIt probably makes sense to make the porcelain be individual repos as well, if nothing else for sanity's sake. I'm really psyched about this project; there are several projects where I've wanted to use git from javascript. (I found it after the campaign ended so couldn't contribute, I hope to be able to contribue time and effort instead!)\n. If the modules are reasonably high level (i.e. on the level of git porcelain) it should be real easy to build a monolithic wrapper that pulls them in and provides a different style of interface.\n\nPersonally, I think the modular approach is sound. For instance, I wrote a dependency management system that utilized git, and it basically only ever cloned/switched branches and inspected tags/commits. No need to pull in more functionality than that.\n\nI also think there's value in a general purpose library, kind of in the style of [shelljs](https://github.com/arturadib/shelljs), but it should be straight forward to provide this as a sort of wrapper around what these multiple projects.\n. Cool, sounds like all questions in this thread have been answered.\n. Are these smaller separate modules considered the plumbing and if so, will this project provide porcelain? Would it perhaps make sense to move longer running API discussions and the like to issues here in GitHub instead of IRC, to increase visibility?\n. +1 for discussion on github!\n\nIt probably makes sense to make the porcelain be individual repos as well, if nothing else for sanity's sake. I'm really psyched about this project; there are several projects where I've wanted to use git from javascript. (I found it after the campaign ended so couldn't contribute, I hope to be able to contribue time and effort instead!)\n. If the modules are reasonably high level (i.e. on the level of git porcelain) it should be real easy to build a monolithic wrapper that pulls them in and provides a different style of interface.\n\nPersonally, I think the modular approach is sound. For instance, I wrote a dependency management system that utilized git, and it basically only ever cloned/switched branches and inspected tags/commits. No need to pull in more functionality than that.\n\nI also think there's value in a general purpose library, kind of in the style of [shelljs](https://github.com/arturadib/shelljs), but it should be straight forward to provide this as a sort of wrapper around what these multiple projects.\n. Cool, sounds like all questions in this thread have been answered.\n. ",
    "seriema": "As a KS backer I feel a bit ... confused. I was expecting one library in one Github repo, kinda like jQuery. I was expecting code to migrate from any other project into this project.\n\nDo you mean this JS-Git repo is going to be a documentation and sample project for chrisdickinson's work?\n. @creationix Sounds good. It just sounded like you were turning JS-Git into a repo of examples and documentation for other libraries. Glad to hear you're keeping an eye on quality. Can't wait to see more commits pushed!\n. Ah, too bad. When can we expect this repo to have a little more life?\n. As a KS backer I feel a bit ... confused. I was expecting one library in one Github repo, kinda like jQuery. I was expecting code to migrate from any other project into this project.\n\nDo you mean this JS-Git repo is going to be a documentation and sample project for chrisdickinson's work?\n. @creationix Sounds good. It just sounded like you were turning JS-Git into a repo of examples and documentation for other libraries. Glad to hear you're keeping an eye on quality. Can't wait to see more commits pushed!\n. Ah, too bad. When can we expect this repo to have a little more life?\n. ",
    "mihailik": "Separate modules or not, people are eager to see some code, please!\n. Separate modules or not, people are eager to see some code, please!\n. ",
    "mgarciaisaia": "If all objects will be stored in the DB with their hash as the key, wouldn't it be wrong to ask for the key value when storing? Not only it can be calculated by the DB,it HAS TO, for preventing a corrupted key/value pair to be stored.\n. If all objects will be stored in the DB with their hash as the key, wouldn't it be wrong to ask for the key value when storing? Not only it can be calculated by the DB,it HAS TO, for preventing a corrupted key/value pair to be stored.\n. ",
    "corytheboyd": ":thumbsup: :thumbsup: :thumbsup: \n. > maybe you can't link to a \"competitor\" Kickstarter\n\nThere is no reason why you shouldn't be able to link to Bountysource in a Kickstarter update. Do it!\n. :thumbsup: :thumbsup: :thumbsup: \n. > maybe you can't link to a \"competitor\" Kickstarter\n\nThere is no reason why you shouldn't be able to link to Bountysource in a Kickstarter update. Do it!\n. ",
    "ccverg": ":+1: \n. :+1: \n. ",
    "dominictarr": "I approve of `consume`.\n\nHmm, if you want this to appeal to a standards committee, you probably should expand the names into something more verbose, with camel case. read -> readNext, stop -> abortStream, consume -> readFrom\n\nOf course, it will need a constructor type, and `isSimpleStream` static method.\n\nI'm actually being serious. Add extra stuff so that people can argue about it, and then vote to leave off the extras.\n. oh, I just realized that this api is nearly identical to the iterator api used in https://github.com/rvagg/node-leveldown see also https://github.com/dominictarr/async-iterator\n\nwe originally adopted this because it would be easy to wrap into either streams1, or stream2 and modeled the underlying leveldb api pretty closely.\n\nwe chose `next` and `end`, but same idea otherwise.\n. regards a `consume` method, I think consistency trumps necessity.\nThere are places where you don't care whether something is a complete sink, or just one side of a transform. Also, if a sink is an object, then you can explain sources, explain sinks, and then explain a transform as both a source and a sink.\n\nI started using the word \"sink\" to designate a stream that has no readable side - this reflects the usage of the word 'sink' in graph theory http://en.wikipedia.org/wiki/Sink_(disambiguation) . having a method \"sink\" on a transform stream breaks this.\n. hmm, yes I think that leveldown could be made compatible with this proposal. the difference is very slight.\n. What about if a transform stream is still a `{consume}` but it just returns it self. sure there is a thing here where someone might read from it, but it doesn't have an input yet, but that can be abstracted, and is even useful in some cases.\n\nAt least, as the stated benefit of objects is that you get structural typing, making other streams objects, but transforms not objects is not consistent. Conceptually, they are still streams.\n. right, I see, so the only difference between this design and pull/min is that it's two separate functions, rather than one that combines them.\n. so, I can think of a few situations where the reason might be important. example: on tcp you want to know if the stream failed because there was no server, or if it dropped the connection, or it timed out.\n\nif you where writing to a file, you want to know if the error was that you didn't have permissions, or if you ran out of diskspace. hmm, although I guess you could just use the continuable to get the error type...\n. There isn't gonna be one right answer to this stream thing. not with the languages we have today. Maybe in some future scifi language, but today, the best we can do hope to fit some fairly broad but non-exhaustive set of use-cases.\n\nAnyway, it's not that hard to write custom stream stuff that there needs to be One True Stream. You can always convert from one to the other, and pick the stream best suits the way you think and the sort of programming you do.\n. ah, I didn't have time to look into it, just submitting the error I found.\n. I approve of `consume`.\n\nHmm, if you want this to appeal to a standards committee, you probably should expand the names into something more verbose, with camel case. read -> readNext, stop -> abortStream, consume -> readFrom\n\nOf course, it will need a constructor type, and `isSimpleStream` static method.\n\nI'm actually being serious. Add extra stuff so that people can argue about it, and then vote to leave off the extras.\n. oh, I just realized that this api is nearly identical to the iterator api used in https://github.com/rvagg/node-leveldown see also https://github.com/dominictarr/async-iterator\n\nwe originally adopted this because it would be easy to wrap into either streams1, or stream2 and modeled the underlying leveldb api pretty closely.\n\nwe chose `next` and `end`, but same idea otherwise.\n. regards a `consume` method, I think consistency trumps necessity.\nThere are places where you don't care whether something is a complete sink, or just one side of a transform. Also, if a sink is an object, then you can explain sources, explain sinks, and then explain a transform as both a source and a sink.\n\nI started using the word \"sink\" to designate a stream that has no readable side - this reflects the usage of the word 'sink' in graph theory http://en.wikipedia.org/wiki/Sink_(disambiguation) . having a method \"sink\" on a transform stream breaks this.\n. hmm, yes I think that leveldown could be made compatible with this proposal. the difference is very slight.\n. What about if a transform stream is still a `{consume}` but it just returns it self. sure there is a thing here where someone might read from it, but it doesn't have an input yet, but that can be abstracted, and is even useful in some cases.\n\nAt least, as the stated benefit of objects is that you get structural typing, making other streams objects, but transforms not objects is not consistent. Conceptually, they are still streams.\n. right, I see, so the only difference between this design and pull/min is that it's two separate functions, rather than one that combines them.\n. so, I can think of a few situations where the reason might be important. example: on tcp you want to know if the stream failed because there was no server, or if it dropped the connection, or it timed out.\n\nif you where writing to a file, you want to know if the error was that you didn't have permissions, or if you ran out of diskspace. hmm, although I guess you could just use the continuable to get the error type...\n. There isn't gonna be one right answer to this stream thing. not with the languages we have today. Maybe in some future scifi language, but today, the best we can do hope to fit some fairly broad but non-exhaustive set of use-cases.\n\nAnyway, it's not that hard to write custom stream stuff that there needs to be One True Stream. You can always convert from one to the other, and pick the stream best suits the way you think and the sort of programming you do.\n. ah, I didn't have time to look into it, just submitting the error I found.\n. ",
    "Raynos": "@dominictarr I think in recent times the W3C standards favor short names.\n\nThey probably will need a constructor though.\n. +1 for avoiding `this`\n. @creationix \n\n``` js\nvar writer = window.FileWriter(location1)\nvar stream = window.FileStream(location2)\nwriter.consume(stream)\n```\n\nI have a feeling that the W3C may like that sort of interface more. It also means you can do duck type checks to see whether something is a sink.\n. ## simple streams in generators\n\nThe example is `while (part = yield stream.read)` which feels weird.\n\nHaving `while (part = yield stream.read())` would be nicer but would murder the API for generators convenience.\n. ## read(callback)\n\nWe should specify the allowed states. A stream is basically zero or more values followed by an end (an end which is either natural or caused by an error)\n\n## stop(err, [callback])\n\nWe should specify that once the callback fires any call to `.read()` will not return any more values. Optionally specify that when you call `.stop()` and even before the callback fires it should not return any values to the read callback. Basically it would be useful to spec out the relationship between `stop()` and `read()`\n\nAlso I don't think the callback should be optional. \n\n``` js\nfileStream.stop(null, function (err) {\n  if (err) { /* disk close error */ } \n})\n```\n\n`stop()` may cause some kind of error when the stream tries to close itself, when that happens it should inform someone.\n\n## pullTransform(stream) -> stream\n\nI have called a `pullTransform` a `duplex` function before. and I would do duplex TCP like\n\n``` js\n// tcp.createServer := (Number, (Stream) => Stream)\ntcp.createServer(8080, function (socket) {\n  return socket\n});\n```\n\nA more interesting one would be \n\n``` js\n// tcp.createServer := (Number, (Stream) => Stream)\ntcp.createServer(8080, function (socket) {\n  return jsonSerialize(app(jsonParse(socket)))\n});\n```\n. > I'm fine with stop requiring the callback if you feel strongly about it. \n\nI don't mind too much, I think it would make things simpler.\n\n> I think we could get away with not having err in stop/end/close if we slightly changed how error propagation worked\n\nThat could work nicely, all upstream would do with it is echo it back anyway so that's probably better.\n. I am wary of \n\n``` js\nfunction transform() {\n  return {\n    next: function (callback) { ... },\n    end: function (callback) { ... },\n    consume: function (stream) { ... }\n  }\n}\n```\n\nThat is a function that takes no arguments and returns two things as a result. I am personally going to avoid this  and focus on `fn(input) -> output` instead of `fn() -> { input, output }`\n. We can write a simple `series` helper like @creationix `series` function and @dominictarr `continuable-series` module to make working with a chain of duplex function stream things easier.\n\nI am +1 for this as it allows us to model implementation details as functions and embed a helper flow control function in the app code _without_ relying on a simple-stream flow control library.\n\nIt also means the implementation of `\"json-code\"` probably has no dependency as it's a simple transform function\n\n``` js\nvar json = require('json-codec');\nvar line = require('line-codec');\n\ntcp.createServer(8080, function (socket) {\n  return series(\n    json.decode,\n    line.deframe,\n    app,\n    line.frame,\n    json.encode\n  )(socket)\n});\n\nfunction app(stream) {\n  // Just an echo server\n  return stream;\n}\n\nfunction series() {\n  var args = [].slice.call(arguments)\n  return function duplex(stream) {\n    for (var i = 0; i < args.length; i++) {\n      stream = args[i](stream)\n    }\n    return stream\n  }\n}\n```\n. yes! \n. @creationix I think the symmetrical sugar is weak enough that something like\n\n``` js\nfunction serialize(stream) {\n  return series(line.deframe, json.decode, stream, line.frame, json.encode)\n}\n```\n\nWould cover most use cases.\n. ## pushTransform(emit) -> emit\n\nI have yet to really understand the motivation for that one. I find `function (stream) -> stream` simple enough that I don't know when to use a push transform instead.\n. ## Sink with errors / finish\n\n``` js\nvar sink = {\n  consume: function (stream, callback) { }\n}\n```\n\nif a `sink` took a callback it could call it with an error if it failed to consume the source. It can also call it without an error to signal that it finished consuming the source.\n. @chrisdickinson I would assume that if a stream ends with or without an error then you should NOT call `.end()` / `.stop()` / `.abort()` on said stream.\n\nAlso you can do `filter` way simpler\n\n``` js\nfunction filter(lambda) {\n    return function duplex(stream) {\n        return { next: next, end: stream.end }\n\n        function next(callback) {\n            stream.next(function onread(err, value) {\n                if (value === undefined) {\n                    return callback(err)\n                }\n\n                var keep = lambda(value)\n                if (keep) {\n                    return callback(null, value)\n                }\n\n                stream.next(onread)\n            })\n        }\n    }\n}\n```\n. @creationix with min streams I believe calling `next()` and `end()` concurrently was invalid. Because both `next()` and `end()` were the same function.\n\nMaybe we should dis allow the consumer of a stream to call those two functions concurrently and force them to wait for a result from `next()` before they can call `end()`\n. btw I would vote for `{ read, abort }` as more intuitive then `{ next, end }`\n\nAlso \"just make sure to always eventually call every callback.\" answers a lot of questions about `read` and `abort` interaction. \n. @creationix \n\n``` js\n// Echo client\ntcp.connect(8080, function (socket) {\n  return socket;\n});\n```\n\nTHAT. THAT A MILLION TIMES. client and server just became the exact same code. This is amazing.\n. > The only basic type here is the stream. It's the only thing in the spec that everything else has to agree on.\n\nI have found the difference between standardizing on `promise` and `continuable` to be about standardize the types and not the functions. With streams we can all agree what `Readable` and `Writable` is and then we can go do our own things with transform functions, monads or duplex stream objects.\n. > sink = function that accepts a stream (and returns a continuable)\n\nIt might make sense for `sink` to be a function that returns an object with a `consume` method for purposes of structural typing. That consume method then accepts a stream and returns a continuable\n. Aw man promises. That's going to be a hard battle.\n\nW3C is going to be like \"your api seems nice but read() should return a promise\" \n. @creationix you are preaching to the choir.\n. @dominictarr I think in recent times the W3C standards favor short names.\n\nThey probably will need a constructor though.\n. +1 for avoiding `this`\n. @creationix \n\n``` js\nvar writer = window.FileWriter(location1)\nvar stream = window.FileStream(location2)\nwriter.consume(stream)\n```\n\nI have a feeling that the W3C may like that sort of interface more. It also means you can do duck type checks to see whether something is a sink.\n. ## simple streams in generators\n\nThe example is `while (part = yield stream.read)` which feels weird.\n\nHaving `while (part = yield stream.read())` would be nicer but would murder the API for generators convenience.\n. ## read(callback)\n\nWe should specify the allowed states. A stream is basically zero or more values followed by an end (an end which is either natural or caused by an error)\n\n## stop(err, [callback])\n\nWe should specify that once the callback fires any call to `.read()` will not return any more values. Optionally specify that when you call `.stop()` and even before the callback fires it should not return any values to the read callback. Basically it would be useful to spec out the relationship between `stop()` and `read()`\n\nAlso I don't think the callback should be optional. \n\n``` js\nfileStream.stop(null, function (err) {\n  if (err) { /* disk close error */ } \n})\n```\n\n`stop()` may cause some kind of error when the stream tries to close itself, when that happens it should inform someone.\n\n## pullTransform(stream) -> stream\n\nI have called a `pullTransform` a `duplex` function before. and I would do duplex TCP like\n\n``` js\n// tcp.createServer := (Number, (Stream) => Stream)\ntcp.createServer(8080, function (socket) {\n  return socket\n});\n```\n\nA more interesting one would be \n\n``` js\n// tcp.createServer := (Number, (Stream) => Stream)\ntcp.createServer(8080, function (socket) {\n  return jsonSerialize(app(jsonParse(socket)))\n});\n```\n. > I'm fine with stop requiring the callback if you feel strongly about it. \n\nI don't mind too much, I think it would make things simpler.\n\n> I think we could get away with not having err in stop/end/close if we slightly changed how error propagation worked\n\nThat could work nicely, all upstream would do with it is echo it back anyway so that's probably better.\n. I am wary of \n\n``` js\nfunction transform() {\n  return {\n    next: function (callback) { ... },\n    end: function (callback) { ... },\n    consume: function (stream) { ... }\n  }\n}\n```\n\nThat is a function that takes no arguments and returns two things as a result. I am personally going to avoid this  and focus on `fn(input) -> output` instead of `fn() -> { input, output }`\n. We can write a simple `series` helper like @creationix `series` function and @dominictarr `continuable-series` module to make working with a chain of duplex function stream things easier.\n\nI am +1 for this as it allows us to model implementation details as functions and embed a helper flow control function in the app code _without_ relying on a simple-stream flow control library.\n\nIt also means the implementation of `\"json-code\"` probably has no dependency as it's a simple transform function\n\n``` js\nvar json = require('json-codec');\nvar line = require('line-codec');\n\ntcp.createServer(8080, function (socket) {\n  return series(\n    json.decode,\n    line.deframe,\n    app,\n    line.frame,\n    json.encode\n  )(socket)\n});\n\nfunction app(stream) {\n  // Just an echo server\n  return stream;\n}\n\nfunction series() {\n  var args = [].slice.call(arguments)\n  return function duplex(stream) {\n    for (var i = 0; i < args.length; i++) {\n      stream = args[i](stream)\n    }\n    return stream\n  }\n}\n```\n. yes! \n. @creationix I think the symmetrical sugar is weak enough that something like\n\n``` js\nfunction serialize(stream) {\n  return series(line.deframe, json.decode, stream, line.frame, json.encode)\n}\n```\n\nWould cover most use cases.\n. ## pushTransform(emit) -> emit\n\nI have yet to really understand the motivation for that one. I find `function (stream) -> stream` simple enough that I don't know when to use a push transform instead.\n. ## Sink with errors / finish\n\n``` js\nvar sink = {\n  consume: function (stream, callback) { }\n}\n```\n\nif a `sink` took a callback it could call it with an error if it failed to consume the source. It can also call it without an error to signal that it finished consuming the source.\n. @chrisdickinson I would assume that if a stream ends with or without an error then you should NOT call `.end()` / `.stop()` / `.abort()` on said stream.\n\nAlso you can do `filter` way simpler\n\n``` js\nfunction filter(lambda) {\n    return function duplex(stream) {\n        return { next: next, end: stream.end }\n\n        function next(callback) {\n            stream.next(function onread(err, value) {\n                if (value === undefined) {\n                    return callback(err)\n                }\n\n                var keep = lambda(value)\n                if (keep) {\n                    return callback(null, value)\n                }\n\n                stream.next(onread)\n            })\n        }\n    }\n}\n```\n. @creationix with min streams I believe calling `next()` and `end()` concurrently was invalid. Because both `next()` and `end()` were the same function.\n\nMaybe we should dis allow the consumer of a stream to call those two functions concurrently and force them to wait for a result from `next()` before they can call `end()`\n. btw I would vote for `{ read, abort }` as more intuitive then `{ next, end }`\n\nAlso \"just make sure to always eventually call every callback.\" answers a lot of questions about `read` and `abort` interaction. \n. @creationix \n\n``` js\n// Echo client\ntcp.connect(8080, function (socket) {\n  return socket;\n});\n```\n\nTHAT. THAT A MILLION TIMES. client and server just became the exact same code. This is amazing.\n. > The only basic type here is the stream. It's the only thing in the spec that everything else has to agree on.\n\nI have found the difference between standardizing on `promise` and `continuable` to be about standardize the types and not the functions. With streams we can all agree what `Readable` and `Writable` is and then we can go do our own things with transform functions, monads or duplex stream objects.\n. > sink = function that accepts a stream (and returns a continuable)\n\nIt might make sense for `sink` to be a function that returns an object with a `consume` method for purposes of structural typing. That consume method then accepts a stream and returns a continuable\n. Aw man promises. That's going to be a hard battle.\n\nW3C is going to be like \"your api seems nice but read() should return a promise\" \n. @creationix you are preaching to the choir.\n. ",
    "Gozala": "I'm jumping to this little late (& maybe I should not do it at all). But I'll still provide my feedback based of my experience working on streams / signals / channels or whatever you wanna call them.\n1. I think in nature there is just input if you look it from the other side it will be an output. This is to say you don't need sync or duplex, you just need a data type for representing collection of eventual data chunks. Then you can write transformation functions that transforms a -> b. If you want duplex is just pair of same data types where data is pushed from left side to input end and from right side on the output end. `sync` is just a function that's just aware of data type's interface and there for can read data out of it and do whatever it needs to. That's also where `reduce`  functions is somewhat relevant since it takes accumulates state by calling reducer with previous state and a next value.\n2. It took me a while but I got to understand that data types (or shapes if you prefer so)  are a lot more composable. That to say that I dislike `.end`, `.abort` `.close`.  I think stream / signal API does not needs any of these, although it can be added as a sugar if desired. If I'd done it today I'd define stream / signal as simple as this:\n\n``` js\n var stream = {\n   spawn: function(next) {\n     next(1);\n     next(2);\n     next(END);\n   }\n}\n```\n\nWhere `END` is whatever you desire it to be special value, type or shape does not matter as long as it's specified. If streams has an error it can pass them and good thing is JS already has Error type for this.\n\n``` js\n var stream = {\n   spawn: function(next) {\n     next(1);\n     next(2);\n     next(Error(\"oops!\");\n   }\n}\n```\n1. Sync or I'd rather say consumer can have a direct coordination with an input without having to have a hold of it or having a methods like `abort` on every transformation. All it needs to is return value:\n\n``` js\nfunction take(n, input) {\n  return {\n    spawn: function(next) {\n      var left = n;\n      input.spawn(function(value) {\n        return n === 0 ? ABORT : next(value)\n      })\n    }\n  }\n}\n```\n\nOf course our input will have to recognize `ABORT` and send back `END`. If `input` does not really recognizes some of this messages we still can wrap it a normalizer to force it to comply, or to the very prevent it's brokenness to  infect\nrest of the pipeline.\n1. Of course we can't talk about streams without back-pressure, but you may have noticed that 3. actually illustrates\n   I/O coordination and back-pressure is just a different form of it. To be more specific consumer can return back data\n   indicating that backpressure should be applied, then respectful source will do that. And if you happen to deal with streams that don't really respect backpressure still not a big deal, cause you can write `buffer(input)` that will respect\n   backpressure and will buffer up input for consumer. I have explored this technique in fs-reduce library where all of the streams respect backpressure, but if you happen to face stream (like array) that does not it will be just buffered up until pressure is released.\n2. I don't think there is any winner in pull VS push type streams, and if there was a one it'll be push since in nature we have events we can't actually pause (users clicking their mice for example). But hey that's not a big deal to since\n   that's just another flavor of I/O coordination all you need to do is make `pull(stream)` which will give you pull based API (maybe that'll match min-stream proposal) and all it will have to do is:\n\n``` js\nfunction pull(stream) {\n  var buffer = [];\n  var reads = [];\n  var resume = function() { stream.spawn(accumulate); }\n\n  function drain() {\n    while (reads.length && buffer.length) reads.shift()(buffer.shift())\n    return buffer.length ? PAUSE : null;\n  }\n\n  function accumulate(value) {\n    // save value and pause stream\n    buffer.push(value);\n    return drain(buffer, reads);\n  }\n\n  // stream will give provide function to resume it.\n  function PAUSE(go) { resume = go }\n\n  return {\n    read: function(callback) {\n      reads.push(callback);\n      drain();\n    }\n  }\n}\n```\n\nThis is queue like (imperfect) API it should give an idea how different kind of ones can be easily created in form of simple functions that compose.\n\nThat sums up my opinion on how streams should work based experience of building them for at least 4 times over past few years. I hope this will be helpful and not totally boring and insane. \n. ## Regards\n\nIrakli Gozalishvili\nWeb: http://www.jeditoolkit.com/\n\nOn Tuesday, 2013-07-02 at 19:36 , Tim Caswell wrote:\n\n> @Gozala (https://github.com/Gozala) I was wondering when/if you would comment on this thread.\n> Yes, I agree that the only interface that needs to be specified is the readable stream.\n> As far as using special tokens for END, ABORT, and Error classes, I'd rather not. instanceof Error doesn't work if the error is from another context. There is no Error.isError helper function though Object.prototype.toString.call(err) === \"[object Error]\" seems to be reliable. I'd hate to force such a verbose type check on each and every data chunk that goes through the stream. Having two positional arguments tells us a lot that speeds up such checks.\n\nActually you don't have to handle them at all it's matter of just having some sort of transform operation that passes meta values between input and output. For example you could have something like this folds:\n\nhttps://github.com/Gozala/signalize/blob/master/core.js#L146-L172\n\nThen all the filter map drop etc.. can be easily implemented without concerning self with either error checking or special value handling:\nhttps://github.com/Gozala/signalize/blob/master/core.js#L192-L284\n\n> Yes back-pressure can be done with a manual side-channel and pause and resume commands, but I much prefer the implicit backpressure provided by pull style. In my experience I'm much more likely to get it right if I'm using pull-streams than writing the back-pressure by hand using manual pause and resume.\n\nMain issue with pull is it's inherently solver and you can't apply category theory to optimise transformation pipeline. And of course you can not represent streams that can't be paused or stopped like user events. That is why I prefer to decouple notion of stream from consumption semantics since pull is one of the ways there are more and from case to case you may want different ones. I have started writing some spec for push & pull signals that in best case perform as push and in worst case de-optimize to plain pull & of course any other case in between:\nhttps://gist.github.com/Gozala/5314269  \n\nIt's slightly out of date though\n\n> Yes there are general helpers that convert between types. I am publishing a module right now called push-to-pull that lets you write the easier push filters, but use them as back-pressure honoring pull-filters without writing your own queues. A reduce transform could easily be written as could a filter transform.\n> Thanks for the input.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub (https://github.com/creationix/js-git/issues/17#issuecomment-20392201).\n. I'm jumping to this little late (& maybe I should not do it at all). But I'll still provide my feedback based of my experience working on streams / signals / channels or whatever you wanna call them.\n1. I think in nature there is just input if you look it from the other side it will be an output. This is to say you don't need sync or duplex, you just need a data type for representing collection of eventual data chunks. Then you can write transformation functions that transforms a -> b. If you want duplex is just pair of same data types where data is pushed from left side to input end and from right side on the output end. `sync` is just a function that's just aware of data type's interface and there for can read data out of it and do whatever it needs to. That's also where `reduce`  functions is somewhat relevant since it takes accumulates state by calling reducer with previous state and a next value.\n2. It took me a while but I got to understand that data types (or shapes if you prefer so)  are a lot more composable. That to say that I dislike `.end`, `.abort` `.close`.  I think stream / signal API does not needs any of these, although it can be added as a sugar if desired. If I'd done it today I'd define stream / signal as simple as this:\n\n``` js\n var stream = {\n   spawn: function(next) {\n     next(1);\n     next(2);\n     next(END);\n   }\n}\n```\n\nWhere `END` is whatever you desire it to be special value, type or shape does not matter as long as it's specified. If streams has an error it can pass them and good thing is JS already has Error type for this.\n\n``` js\n var stream = {\n   spawn: function(next) {\n     next(1);\n     next(2);\n     next(Error(\"oops!\");\n   }\n}\n```\n1. Sync or I'd rather say consumer can have a direct coordination with an input without having to have a hold of it or having a methods like `abort` on every transformation. All it needs to is return value:\n\n``` js\nfunction take(n, input) {\n  return {\n    spawn: function(next) {\n      var left = n;\n      input.spawn(function(value) {\n        return n === 0 ? ABORT : next(value)\n      })\n    }\n  }\n}\n```\n\nOf course our input will have to recognize `ABORT` and send back `END`. If `input` does not really recognizes some of this messages we still can wrap it a normalizer to force it to comply, or to the very prevent it's brokenness to  infect\nrest of the pipeline.\n1. Of course we can't talk about streams without back-pressure, but you may have noticed that 3. actually illustrates\n   I/O coordination and back-pressure is just a different form of it. To be more specific consumer can return back data\n   indicating that backpressure should be applied, then respectful source will do that. And if you happen to deal with streams that don't really respect backpressure still not a big deal, cause you can write `buffer(input)` that will respect\n   backpressure and will buffer up input for consumer. I have explored this technique in fs-reduce library where all of the streams respect backpressure, but if you happen to face stream (like array) that does not it will be just buffered up until pressure is released.\n2. I don't think there is any winner in pull VS push type streams, and if there was a one it'll be push since in nature we have events we can't actually pause (users clicking their mice for example). But hey that's not a big deal to since\n   that's just another flavor of I/O coordination all you need to do is make `pull(stream)` which will give you pull based API (maybe that'll match min-stream proposal) and all it will have to do is:\n\n``` js\nfunction pull(stream) {\n  var buffer = [];\n  var reads = [];\n  var resume = function() { stream.spawn(accumulate); }\n\n  function drain() {\n    while (reads.length && buffer.length) reads.shift()(buffer.shift())\n    return buffer.length ? PAUSE : null;\n  }\n\n  function accumulate(value) {\n    // save value and pause stream\n    buffer.push(value);\n    return drain(buffer, reads);\n  }\n\n  // stream will give provide function to resume it.\n  function PAUSE(go) { resume = go }\n\n  return {\n    read: function(callback) {\n      reads.push(callback);\n      drain();\n    }\n  }\n}\n```\n\nThis is queue like (imperfect) API it should give an idea how different kind of ones can be easily created in form of simple functions that compose.\n\nThat sums up my opinion on how streams should work based experience of building them for at least 4 times over past few years. I hope this will be helpful and not totally boring and insane. \n. ## Regards\n\nIrakli Gozalishvili\nWeb: http://www.jeditoolkit.com/\n\nOn Tuesday, 2013-07-02 at 19:36 , Tim Caswell wrote:\n\n> @Gozala (https://github.com/Gozala) I was wondering when/if you would comment on this thread.\n> Yes, I agree that the only interface that needs to be specified is the readable stream.\n> As far as using special tokens for END, ABORT, and Error classes, I'd rather not. instanceof Error doesn't work if the error is from another context. There is no Error.isError helper function though Object.prototype.toString.call(err) === \"[object Error]\" seems to be reliable. I'd hate to force such a verbose type check on each and every data chunk that goes through the stream. Having two positional arguments tells us a lot that speeds up such checks.\n\nActually you don't have to handle them at all it's matter of just having some sort of transform operation that passes meta values between input and output. For example you could have something like this folds:\n\nhttps://github.com/Gozala/signalize/blob/master/core.js#L146-L172\n\nThen all the filter map drop etc.. can be easily implemented without concerning self with either error checking or special value handling:\nhttps://github.com/Gozala/signalize/blob/master/core.js#L192-L284\n\n> Yes back-pressure can be done with a manual side-channel and pause and resume commands, but I much prefer the implicit backpressure provided by pull style. In my experience I'm much more likely to get it right if I'm using pull-streams than writing the back-pressure by hand using manual pause and resume.\n\nMain issue with pull is it's inherently solver and you can't apply category theory to optimise transformation pipeline. And of course you can not represent streams that can't be paused or stopped like user events. That is why I prefer to decouple notion of stream from consumption semantics since pull is one of the ways there are more and from case to case you may want different ones. I have started writing some spec for push & pull signals that in best case perform as push and in worst case de-optimize to plain pull & of course any other case in between:\nhttps://gist.github.com/Gozala/5314269  \n\nIt's slightly out of date though\n\n> Yes there are general helpers that convert between types. I am publishing a module right now called push-to-pull that lets you write the easier push filters, but use them as back-pressure honoring pull-filters without writing your own queues. A reduce transform could easily be written as could a filter transform.\n> Thanks for the input.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub (https://github.com/creationix/js-git/issues/17#issuecomment-20392201).\n. ",
    "mhart": "I really like the `abort` method - it's something that still puzzles me about streams as they stand in v0.10.x - hence my [as-yet-unanswered question](https://groups.google.com/d/msg/nodejs/eGukJUQrOBY/URD8I7tNxRUJ) on the node.js group.\n\nThe only other question I'd pose (not sure if it has been already) is whether you should include anything about ES6 iterators (and generators) in the spec - I notice they're not mentioned at all and figure they should at least be referred to, even if it's to say that there's no goal to make them compatible, or whatever.\n. I really like the `abort` method - it's something that still puzzles me about streams as they stand in v0.10.x - hence my [as-yet-unanswered question](https://groups.google.com/d/msg/nodejs/eGukJUQrOBY/URD8I7tNxRUJ) on the node.js group.\n\nThe only other question I'd pose (not sure if it has been already) is whether you should include anything about ES6 iterators (and generators) in the spec - I notice they're not mentioned at all and figure they should at least be referred to, even if it's to say that there's no goal to make them compatible, or whatever.\n. ",
    "neilk": "Wanted to be finished by today, but I spent the time I allotted Sunday just understanding it - as far as I can tell you're the only one who uses terms like \"continuable\" in this way. I see the usefulness - it's promise-like, although not quite the same. Might have tests by end of today\n. Got it, thanks\n. Wanted to be finished by today, but I spent the time I allotted Sunday just understanding it - as far as I can tell you're the only one who uses terms like \"continuable\" in this way. I see the usefulness - it's promise-like, although not quite the same. Might have tests by end of today\n. Got it, thanks\n. ",
    "mscdex": "I recognize this is old, but what you want here is the `passphrase` option that's set in the connection config.\n. I recognize this is old, but what you want here is the `passphrase` option that's set in the connection config.\n. ",
    "ghostbar": "Just tried with `node/v0.10.15` and it's the very same thing. With `node/v0.11.5` throws `Error: Invalid HMAC` BUT cloning from HTTPS directly at github (not gists) works like a charm even in `node/v0.11.5`.\n. Yes! As long as you have a `ssh-key` registered on GitHub you can do `ssh` read-only which is really awesome :-D\n\nI think the issue with `v0.11.x` has to do with that actually. Now that I re-reproduce it the stacktrace directs to the ssh2 parser:\n\n```\nError: Invalid HMAC\n    at Parser.execute (/usr/local/lib/node_modules/js-git-node/node_modules/js-git-node-platform/node_modules/ssh2/lib/Parser.js:204:30)\n    at Socket.<anonymous> (/usr/local/lib/node_modules/js-git-node/node_modules/js-git-node-platform/node_modules/ssh2/lib/Connection.js:1332:18)\n    at Socket.EventEmitter.emit (events.js:100:17)\n    at Socket.Readable.read (_stream_readable.js:357:10)\n    at flow (_stream_readable.js:725:26)\n    at emitReadable_ (_stream_readable.js:410:3)\n    at emitReadable (_stream_readable.js:403:7)\n    at readableAddChunk (_stream_readable.js:160:9)\n    at Socket.Readable.push (_stream_readable.js:122:10)\n    at TCP.onread (net.js:512:21)\n```\n. So: What are you using for the redirections and how can I help to solve the cross-platform issue? Where should be implemented and what should I be looking for?\n. Just tried with `node/v0.10.15` and it's the very same thing. With `node/v0.11.5` throws `Error: Invalid HMAC` BUT cloning from HTTPS directly at github (not gists) works like a charm even in `node/v0.11.5`.\n. Yes! As long as you have a `ssh-key` registered on GitHub you can do `ssh` read-only which is really awesome :-D\n\nI think the issue with `v0.11.x` has to do with that actually. Now that I re-reproduce it the stacktrace directs to the ssh2 parser:\n\n```\nError: Invalid HMAC\n    at Parser.execute (/usr/local/lib/node_modules/js-git-node/node_modules/js-git-node-platform/node_modules/ssh2/lib/Parser.js:204:30)\n    at Socket.<anonymous> (/usr/local/lib/node_modules/js-git-node/node_modules/js-git-node-platform/node_modules/ssh2/lib/Connection.js:1332:18)\n    at Socket.EventEmitter.emit (events.js:100:17)\n    at Socket.Readable.read (_stream_readable.js:357:10)\n    at flow (_stream_readable.js:725:26)\n    at emitReadable_ (_stream_readable.js:410:3)\n    at emitReadable (_stream_readable.js:403:7)\n    at readableAddChunk (_stream_readable.js:160:9)\n    at Socket.Readable.push (_stream_readable.js:122:10)\n    at TCP.onread (net.js:512:21)\n```\n. So: What are you using for the redirections and how can I help to solve the cross-platform issue? Where should be implemented and what should I be looking for?\n. ",
    "camshaft": "Component has a really interesting [implementation](https://github.com/component/url) of parsing urls in the browser.\n. Component has a really interesting [implementation](https://github.com/component/url) of parsing urls in the browser.\n. ",
    "JamesMGreene": "Might be able to steal some of the logic from @visionmedia's https://npmjs.org/package/github-url-from-git\n. Might be able to steal some of the logic from @visionmedia's https://npmjs.org/package/github-url-from-git\n. ",
    "stuartpb": "wrt. the first Github issue (the one based on the user-agent), is js-git sending the appropriate Content-Type (application/x-git- something)? I know there's an extant bug ticket for Bitbucket about it (with the author of the Python Git module Dulwich arguing against UA spoofing): https://bitbucket.org/site/master/issue/6666/detect-git-requests-by-content-type-header\n. wrt. the first Github issue (the one based on the user-agent), is js-git sending the appropriate Content-Type (application/x-git- something)? I know there's an extant bug ticket for Bitbucket about it (with the author of the Python Git module Dulwich arguing against UA spoofing): https://bitbucket.org/site/master/issue/6666/detect-git-requests-by-content-type-header\n. ",
    "Floby": "it's the git version in the apt repositories for ubuntu 12.10. so I suppose\nthe same version is used for 12.04 (LTE) I can try and get some newer git\nand tell you how that goes.\nLe 10 ao\u00fbt 2013 18:47, \"Tim Caswell\" notifications@github.com a \u00e9crit :\n\n> I wonder if your git version is too old for the index format I'm writing.\n> Are you able to upgrade and try with a newer git? From: Florent JabySent:\n> Saturday, August 10, 2013 5:37 AMTo: creationix/js-gitReply To:\n> creationix/js-gitSubject: [js-git] after cloning, git can't read index\n> (#30)I successfully cloned one of my repos via https. but running git\n> status in the repo results in a error message like this:\n> \n> fatal: Unknown index entry format 28dc0000\n> \n> Because I also have some git prompt addition the error messages come after\n> each command.\n> \n> $ git --version\n> git version 1.7.10.4\n> \n> I installed js-git-node from npm. I cloned\n> \n> \u2014Reply to this email directly or view it on GitHub.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/creationix/js-git/issues/30#issuecomment-22443014\n> .\n. it's the git version in the apt repositories for ubuntu 12.10. so I suppose\nthe same version is used for 12.04 (LTE) I can try and get some newer git\nand tell you how that goes.\nLe 10 ao\u00fbt 2013 18:47, \"Tim Caswell\" notifications@github.com a \u00e9crit :\n\n> I wonder if your git version is too old for the index format I'm writing.\n> Are you able to upgrade and try with a newer git? From: Florent JabySent:\n> Saturday, August 10, 2013 5:37 AMTo: creationix/js-gitReply To:\n> creationix/js-gitSubject: [js-git] after cloning, git can't read index\n> (#30)I successfully cloned one of my repos via https. but running git\n> status in the repo results in a error message like this:\n> \n> fatal: Unknown index entry format 28dc0000\n> \n> Because I also have some git prompt addition the error messages come after\n> each command.\n> \n> $ git --version\n> git version 1.7.10.4\n> \n> I installed js-git-node from npm. I cloned\n> \n> \u2014Reply to this email directly or view it on GitHub.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/creationix/js-git/issues/30#issuecomment-22443014\n> .\n. ",
    "heapwolf": "w00t! :D\n. w00t! :D\n. ",
    "vanthome": "Ok, thanks for the hint to the milestone plan\n. Ok, thanks for the hint to the milestone plan\n. ",
    "bewest": "I'd like to help with this if you can throw me some pointers in the right direction.\n\nI've been staring at your tcp examples, I'm hoping to add http cloning/pushing to https://github.com/bewest/restify-git-json\n. I'd like to help with this if you can throw me some pointers in the right direction.\n\nI've been staring at your tcp examples, I'm hoping to add http cloning/pushing to https://github.com/bewest/restify-git-json\n. ",
    "mattapperson": "My use case right now just requires js-git on the server... I would rather the client be able to use regular GIT.\n\nSSH makes sense, I just wanted to point out the need for it.\n\nMilestone 4 works... the sooner the better, but at the end of the day I understand the need to wait till `milestone #4`\n. My use case right now just requires js-git on the server... I would rather the client be able to use regular GIT.\n\nSSH makes sense, I just wanted to point out the need for it.\n\nMilestone 4 works... the sooner the better, but at the end of the day I understand the need to wait till `milestone #4`\n. ",
    "tsgautier": "I haven't looked at the milestones.  Where can I find them?  For me it's a showstopper because I want to read files out of a git repo, and for any arbitrary git repo that pretty much means reading from pack files is a requirement.\n\nI don't need to write to them, so for my use case, it's ok to do read first and then write.\n. Ah, yes, I see, I found the milestones.  Well, for me, I would prefer to have the ability to read data from a git repository in the first milestone, but that's my use case.  Not sure specifically what use cases you are targeting.  \n\nOn a practical note, proving that you can read data before trying to write to it seems like a logical way to divide and conquer and has the side benefit of addressing my use case first ;-)\n. @chrisdickinson thanks for the tip.  I'll take a look.  @creationix sounds good.  Looking forward to it.  Say hello to Alex Miller for me if you see him at Strange Loop.\n. I haven't looked at the milestones.  Where can I find them?  For me it's a showstopper because I want to read files out of a git repo, and for any arbitrary git repo that pretty much means reading from pack files is a requirement.\n\nI don't need to write to them, so for my use case, it's ok to do read first and then write.\n. Ah, yes, I see, I found the milestones.  Well, for me, I would prefer to have the ability to read data from a git repository in the first milestone, but that's my use case.  Not sure specifically what use cases you are targeting.  \n\nOn a practical note, proving that you can read data before trying to write to it seems like a logical way to divide and conquer and has the side benefit of addressing my use case first ;-)\n. @chrisdickinson thanks for the tip.  I'll take a look.  @creationix sounds good.  Looking forward to it.  Say hello to Alex Miller for me if you see him at Strange Loop.\n. ",
    "kzahel": "I did some work with this and I found the cordova socket plugin to be pretty unusable at this point.\n. Yeah I was porting my torrent app to the chrome.socket cordova stuff. I got\nit working, after debugging some very fundamental bugs in the\nchromesocket.java implementation. But the socket code is very old school\njava (thread-per-connection) and .connect() calls block the entire js\ninterpreter thread... so the current chromesocket.java implementation is\nreally a prototype. I was considering rewriting it to be evented, but I\nhave such a hard time with java :-\\ i think it would likely work fine for\ngit-browser, i doubt it calls connect very often?\n\nOn Fri, Feb 7, 2014 at 9:39 AM, Tim Caswell notifications@github.comwrote:\n\n> Did you try with the new chrome app support in cordova? I was unable to\n> get it to run at all, but I really like the push to chrome adt workflow if\n> it actually worked.\n> \n> ## \n> \n> Reply to this email directly or view it on GitHubhttps://github.com/creationix/js-git/issues/53#issuecomment-34479485\n> .\n. I'm looking forward to progress toward this goal. I see a number of interesting projects that could be based on this ability.\n. There's a slightly less than ideal way to register protocol handlers. You need to do it on a website. There's no way yet for an app to register the protocol handler for itself (though I can see this coming in a future chrome version)\n\nhttps://github.com/JSTorrent/jstorrent.github.io/blob/master/js/index.js#L29\n\nBut when the app is installed, it could give a notification with instructions for how to install it. (like https://plus.google.com/105605890151842939367/posts/VDaYzQoFDRW)\n\nOnce navigator.registerProtocolHandler has the url, you can use chrome.runtime.sendMessage to give the URL to the app (https://github.com/JSTorrent/jstorrent.github.io/blob/master/share/share.js#L129)\n. Hmm after doing a little searching I found out there may be a small improvement to be made in this process. Instead of using chrome.runtime.sendMessage it looks like this can bypass that step:\nhttps://developer.chrome.com/apps/manifest/url_handlers\n. I did some work with this and I found the cordova socket plugin to be pretty unusable at this point.\n. Yeah I was porting my torrent app to the chrome.socket cordova stuff. I got\nit working, after debugging some very fundamental bugs in the\nchromesocket.java implementation. But the socket code is very old school\njava (thread-per-connection) and .connect() calls block the entire js\ninterpreter thread... so the current chromesocket.java implementation is\nreally a prototype. I was considering rewriting it to be evented, but I\nhave such a hard time with java :-\\ i think it would likely work fine for\ngit-browser, i doubt it calls connect very often?\n\nOn Fri, Feb 7, 2014 at 9:39 AM, Tim Caswell notifications@github.comwrote:\n\n> Did you try with the new chrome app support in cordova? I was unable to\n> get it to run at all, but I really like the push to chrome adt workflow if\n> it actually worked.\n> \n> ## \n> \n> Reply to this email directly or view it on GitHubhttps://github.com/creationix/js-git/issues/53#issuecomment-34479485\n> .\n. I'm looking forward to progress toward this goal. I see a number of interesting projects that could be based on this ability.\n. There's a slightly less than ideal way to register protocol handlers. You need to do it on a website. There's no way yet for an app to register the protocol handler for itself (though I can see this coming in a future chrome version)\n\nhttps://github.com/JSTorrent/jstorrent.github.io/blob/master/js/index.js#L29\n\nBut when the app is installed, it could give a notification with instructions for how to install it. (like https://plus.google.com/105605890151842939367/posts/VDaYzQoFDRW)\n\nOnce navigator.registerProtocolHandler has the url, you can use chrome.runtime.sendMessage to give the URL to the app (https://github.com/JSTorrent/jstorrent.github.io/blob/master/share/share.js#L129)\n. Hmm after doing a little searching I found out there may be a small improvement to be made in this process. Instead of using chrome.runtime.sendMessage it looks like this can bypass that step:\nhttps://developer.chrome.com/apps/manifest/url_handlers\n. ",
    "StanLindsey": "I'm purchasing a Surface 2 at the end of the month and it looks like git is the only failing point as a developer - I'd love to help out with this if possible. \n. I'm purchasing a Surface 2 at the end of the month and it looks like git is the only failing point as a developer - I'd love to help out with this if possible. \n. ",
    "aaronpowell": "I've made a start [here](https://github.com/aaronpowell/git-indexeddb) but I don't understand how to actually test it so I'm just coding in the event that someone can show me how to actually test the code out :stuck_out_tongue_closed_eyes: \n. Well itd give me an excuse to play with levelup, but i like the idea of minimal dependencies, hence why I've done it raw\n\n---\n\nFrom: Julian Grubermailto:notifications@github.com\nSent: \u00fd23/\u00fd10/\u00fd2013 20:03\nTo: creationix/js-gitmailto:js-git@noreply.github.com\nCc: Aaron Powellmailto:me@aaron-powell.com\nSubject: Re: [js-git] Implement db interface on top of IndexedDb (#57)\n\n@jez0990https://github.com/jez0990 I think it should rather by a storage option itself, right? So if you make it compatibly with the abstract leveldownhttps://github.com/rvagg/abstract-leveldown interface people can use it with levelup in the browser or in node.\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/creationix/js-git/issues/57#issuecomment-26889940.\n. So the version up on my github (linked previously) contains the bulk of the API implemented, I've also sent a PR for the `git-browser` repo which adds support for it.\n\nMy tests have successfully pulled down the repository and it can be inspected (although I don't know how do inspect the contents of a commit, only the whole tree).\n. I've pushed `0.1.2` to npm which has the db API implemented completely. Next step is to do some optimisations of the code and the store.\n. I've made a start [here](https://github.com/aaronpowell/git-indexeddb) but I don't understand how to actually test it so I'm just coding in the event that someone can show me how to actually test the code out :stuck_out_tongue_closed_eyes: \n. Well itd give me an excuse to play with levelup, but i like the idea of minimal dependencies, hence why I've done it raw\n\n---\n\nFrom: Julian Grubermailto:notifications@github.com\nSent: \u00fd23/\u00fd10/\u00fd2013 20:03\nTo: creationix/js-gitmailto:js-git@noreply.github.com\nCc: Aaron Powellmailto:me@aaron-powell.com\nSubject: Re: [js-git] Implement db interface on top of IndexedDb (#57)\n\n@jez0990https://github.com/jez0990 I think it should rather by a storage option itself, right? So if you make it compatibly with the abstract leveldownhttps://github.com/rvagg/abstract-leveldown interface people can use it with levelup in the browser or in node.\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/creationix/js-git/issues/57#issuecomment-26889940.\n. So the version up on my github (linked previously) contains the bulk of the API implemented, I've also sent a PR for the `git-browser` repo which adds support for it.\n\nMy tests have successfully pulled down the repository and it can be inspected (although I don't know how do inspect the contents of a commit, only the whole tree).\n. I've pushed `0.1.2` to npm which has the db API implemented completely. Next step is to do some optimisations of the code and the store.\n. ",
    "refset": "Is the levelup interface not of interest? It would give you a tonne of underlying storage options, including IndexedDb, for free.\n. Is the levelup interface not of interest? It would give you a tonne of underlying storage options, including IndexedDb, for free.\n. ",
    "rhasson": "Thanks Tim.\n. Thanks Tim.\n. ",
    "Pomax": "Maybe I'm still missing something, but that git-browser example still relies on having a node server running to talk to, instead of being a pure client lib that you point to an arbitrary git server. I was looking at https://github.com/michael/github but that's a connector specifically for github rather than an arbitrary git server connector, so I was hoping js-git would be the latter. Is there a version of js-git that you just compile to jsgit.js and then include on an abitrary webpage, with a constructor like `var api = new Git(\"servername\", \"username\", \"password\");` without needing anything other than that library to talk to git servers?\n. ahh, I see. Thank you very much for clarifying.\n. Maybe I'm still missing something, but that git-browser example still relies on having a node server running to talk to, instead of being a pure client lib that you point to an arbitrary git server. I was looking at https://github.com/michael/github but that's a connector specifically for github rather than an arbitrary git server connector, so I was hoping js-git would be the latter. Is there a version of js-git that you just compile to jsgit.js and then include on an abitrary webpage, with a constructor like `var api = new Git(\"servername\", \"username\", \"password\");` without needing anything other than that library to talk to git servers?\n. ahh, I see. Thank you very much for clarifying.\n. ",
    "potomak": "@creationix no worries!\nI'm using js-git to make a project for the nodeKO.\nRunning your examples a couple of days ago (using 0.5.4) everything was fine, trying again the same script this morning everything was broken... (\u256f\u00b0\u25a1\u00b0\uff09\u256f\ufe35 \u253b\u2501\u253b\nI just reverted to 0.5.4 right now.\n. @creationix no worries!\nI'm using js-git to make a project for the nodeKO.\nRunning your examples a couple of days ago (using 0.5.4) everything was fine, trying again the same script this morning everything was broken... (\u256f\u00b0\u25a1\u00b0\uff09\u256f\ufe35 \u253b\u2501\u253b\nI just reverted to 0.5.4 right now.\n. ",
    "mynoheart": "sorry, i have very bad english\n. In the interface \"db-fs\" i do pull request with added need method.  Initially I did it myself(created tags), but I thought it would be a crutch. GIT it does itself, it turns out not match.\n. sorry, i have very bad english\n. In the interface \"db-fs\" i do pull request with added need method.  Initially I did it myself(created tags), but I thought it would be a crutch. GIT it does itself, it turns out not match.\n. ",
    "penartur": "Thank you for your comment!\n\nI'm experiencing a lot of problems with 0.6.1 (and, previously, with 0.5.4) trying to clone simple small test repositories from github; I'm getting messages such as `unexpected end of input stream`, `invalid prefix`, `invalid packfile header` and `failed header check` all the time. Does it make sense to file the appropriate issue, or the problems are known and are going to be fixed in milestone#2?\n. Thank you for your comment!\n\nI'm experiencing a lot of problems with 0.6.1 (and, previously, with 0.5.4) trying to clone simple small test repositories from github; I'm getting messages such as `unexpected end of input stream`, `invalid prefix`, `invalid packfile header` and `failed header check` all the time. Does it make sense to file the appropriate issue, or the problems are known and are going to be fixed in milestone#2?\n. ",
    "funroll": "Is what you have so far in `master`? I searched for `receive-pack` and `pack` but didn't see anything that seemed to fit the bill.\n\nThanks much!\n. Is what you have so far in `master`? I searched for `receive-pack` and `pack` but didn't see anything that seemed to fit the bill.\n\nThanks much!\n. ",
    "alessioalex": "@creationix hey, any updates on this? I see the `server.js` file has been removed since your last comment.\n. @creationix hey, any updates on this? I see the `server.js` file has been removed since your last comment.\n. ",
    "aredridel": "Seems reasonable; I'd love streaming for trees too but it's just for consistency in that case.\n\n`loadAs` is a weird interface though -- you have to know the type ahead of time, and since trees can contain both trees and blobs, that's weird.\n. Oh, interesting -- since github doesn't expose raw git in a browser-accessible way, eh?\n. Oh, I'd forgotten that trees internal representation does include the type of the next nodes. That makes it less heinous.\n. Of course! I'd be remiss if I didn't.\n. Seems reasonable; I'd love streaming for trees too but it's just for consistency in that case.\n\n`loadAs` is a weird interface though -- you have to know the type ahead of time, and since trees can contain both trees and blobs, that's weird.\n. Oh, interesting -- since github doesn't expose raw git in a browser-accessible way, eh?\n. Oh, I'd forgotten that trees internal representation does include the type of the next nodes. That makes it less heinous.\n. Of course! I'd be remiss if I didn't.\n. ",
    "norman784": "What would be posible at this stage js-git?\n. And this limitation respond to the lack of support of the js api to local files? is any other progress or feature needed to the project accomplish this goal? Because in other cases we can use nodejs (but I want to avoid it if is possible).\n\nIf you need something help with anything just tell me and I would try to help.\n. Great, then I can work on my app relaxed with this lib, so when its available I will be able to implement it! Thanks\n. What would be posible at this stage js-git?\n. And this limitation respond to the lack of support of the js api to local files? is any other progress or feature needed to the project accomplish this goal? Because in other cases we can use nodejs (but I want to avoid it if is possible).\n\nIf you need something help with anything just tell me and I would try to help.\n. Great, then I can work on my app relaxed with this lib, so when its available I will be able to implement it! Thanks\n. ",
    "fyockm": "Thanks much for the examples and clarification. I used a combination of https://github.com/creationix/js-git/blob/master/examples/create.js (which I had already looked at) and https://github.com/creationix/giblush/blob/master/importfs.js. One problem I ran into with `importfs.js`, is that I was creating git repo in the same directory that I was traversing. Took me a little bit to figure out what was going on. The git repo was itself being included with the commit. Not sure if this will affect what you're doing, but if so, you may want to filter out '.git' directories as I did.\nThanks again.\n. Thanks much for the examples and clarification. I used a combination of https://github.com/creationix/js-git/blob/master/examples/create.js (which I had already looked at) and https://github.com/creationix/giblush/blob/master/importfs.js. One problem I ran into with `importfs.js`, is that I was creating git repo in the same directory that I was traversing. Took me a little bit to figure out what was going on. The git repo was itself being included with the commit. Not sure if this will affect what you're doing, but if so, you may want to filter out '.git' directories as I did.\nThanks again.\n. ",
    "alevicki": "Ok, thanks for the answer!\n. Ok, thanks for the answer!\n. ",
    "hemanth": " @creationix OH OK! We were trying for config in particular. \n. :+1: Kool! Will paw at it.\n. Thanks for the info :+1: \n.  @creationix OH OK! We were trying for config in particular. \n. :+1: Kool! Will paw at it.\n. Thanks for the info :+1: \n. ",
    "ljharb": "It'd still be nice imo to fall back to a shim.\n. If you rely on the existence of `window.Promise`, then only ES6 environments, or user-shimmed environments, will be able to benefit from the Promise API.\n\nIf instead you provide a Promise shim when a global implementation is unavailable, more environments (more people) will be able to benefit from the Promise API, which will then make them forwards-compatible as well :-)\n. @creationix a spec-compliant polyfill is tiny. See https://github.com/paulmillr/es6-shim/blob/master/es6-shim.js#L1065-L1410 - 350 lines is not going to be that large.\n\nI'd hope that js-git would have a build process anyways, so I can always choose which parts I want to include and which not, so a Promise shim could just be omitted if I didn't want it.\n. for the record: now, 3 years later, the best practice has indeed unequivocally become to just assume `Promise` is globally available and expect the user to shim it.. It'd still be nice imo to fall back to a shim.\n. If you rely on the existence of `window.Promise`, then only ES6 environments, or user-shimmed environments, will be able to benefit from the Promise API.\n\nIf instead you provide a Promise shim when a global implementation is unavailable, more environments (more people) will be able to benefit from the Promise API, which will then make them forwards-compatible as well :-)\n. @creationix a spec-compliant polyfill is tiny. See https://github.com/paulmillr/es6-shim/blob/master/es6-shim.js#L1065-L1410 - 350 lines is not going to be that large.\n\nI'd hope that js-git would have a build process anyways, so I can always choose which parts I want to include and which not, so a Promise shim could just be omitted if I didn't want it.\n. for the record: now, 3 years later, the best practice has indeed unequivocally become to just assume `Promise` is globally available and expect the user to shim it.. ",
    "Bartvds": "If I may pitch in: \n\nSome libraries allow users to pass their own promise constructor, or else fall back to a global defined Promise (or else bail with an error). So they don't ship any implementation at all.\n\nThis means user can pick either a light shim or a heavy implementation like bluebird, but without Promise.cast() etc.\n. @creationix Using window.Promise is similar, but not quite the same as passing it in a call: global vars have a way of becoming problematic. \n\nSimplest example: if you have an application with one module overwriting the global Promise with bluebird and another module who sets it to Q and then rely on their respective methods.\n\nIt is probably a bad idea to assume non-spec features about promises returned from a external module unless you can explicitly pass a constructor; any module might return an internal shim even with a global override set.\n. If I may pitch in: \n\nSome libraries allow users to pass their own promise constructor, or else fall back to a global defined Promise (or else bail with an error). So they don't ship any implementation at all.\n\nThis means user can pick either a light shim or a heavy implementation like bluebird, but without Promise.cast() etc.\n. @creationix Using window.Promise is similar, but not quite the same as passing it in a call: global vars have a way of becoming problematic. \n\nSimplest example: if you have an application with one module overwriting the global Promise with bluebird and another module who sets it to Q and then rely on their respective methods.\n\nIt is probably a bad idea to assume non-spec features about promises returned from a external module unless you can explicitly pass a constructor; any module might return an internal shim even with a global override set.\n. ",
    "mariusGundersen": "What is the status on this? It would be nice to use with async/await now that it is getting supported, instead of using gen-run. I haven't looked into it, but couldn't there be a mixin that promisifies the api? Then nothing would be needed to be done to the core of js-git. . @creationix, I'm not familiar with the full scope of the api possible to create with mixins, but as long as they are enumerable methods on the repo object, then it shouldn't be too hard to loop through and promisify them. There are several npm packages that do this. I can try to have a look at this. . neat, didn't know of that one. I will have a look and compare them :). I'm guessing we have slightly different needs. I'm more interested in the low level storage, while you seem to be more interested in the high-level features and using it with a file-system. The best would be if gitkit could use the lowlevel stuff from js-git, instead of having to reimplement it all by itself.. @strangesast cool, I haven't looked into those yet. I've just tried to get it to compile as a TypeScript project. I'm going to focus on a subset of it for now, since getting everything to work is a bit too much for me and the time I have available at the moment. Probably I will publish it as several npm packages that can be combined, instead of one large package.. Small progress update: I've started working on this more now, and have gotten quite far. I have most of the mixins converted already, and will start to look at the network and protocol code next. I've decided to make it as multiple packages that will be published independently instead of having one package with everything. Nothing is published yet, as I'm still working out the most difficult part: naming.\r\n\r\nThe forked repo is here: https://github.com/mariusGundersen/es-git/tree/master/packages. Ok, so I've got most of the features done, including basic fetch and push using https. This means I can now start using the project for something useful. I've created both a github and npm organization, to make it easier for others to have write-access to it, if you want to. \r\n\r\nThe repository is now here: https://github.com/es-git/es-git\r\n\r\nIt's currently quite limited in documentation, but I hope to add at least a basic readme file to each package.. So es-git is written to be a low-level git, for when you need the most basic git functionality. But it's designed to be easily extended, so you are free to build other stuff on top of it.\r\n\r\nI have put a few examples online: https://es-git-examples.mariusgundersen.net/\r\n\r\nCurrently I'm using es-git in my side-project, which is also the main driver for implementing new features: https://github.com/mariusGundersen/Ekkiog. What is the status on this? It would be nice to use with async/await now that it is getting supported, instead of using gen-run. I haven't looked into it, but couldn't there be a mixin that promisifies the api? Then nothing would be needed to be done to the core of js-git. . @creationix, I'm not familiar with the full scope of the api possible to create with mixins, but as long as they are enumerable methods on the repo object, then it shouldn't be too hard to loop through and promisify them. There are several npm packages that do this. I can try to have a look at this. . neat, didn't know of that one. I will have a look and compare them :). I'm guessing we have slightly different needs. I'm more interested in the low level storage, while you seem to be more interested in the high-level features and using it with a file-system. The best would be if gitkit could use the lowlevel stuff from js-git, instead of having to reimplement it all by itself.. @strangesast cool, I haven't looked into those yet. I've just tried to get it to compile as a TypeScript project. I'm going to focus on a subset of it for now, since getting everything to work is a bit too much for me and the time I have available at the moment. Probably I will publish it as several npm packages that can be combined, instead of one large package.. Small progress update: I've started working on this more now, and have gotten quite far. I have most of the mixins converted already, and will start to look at the network and protocol code next. I've decided to make it as multiple packages that will be published independently instead of having one package with everything. Nothing is published yet, as I'm still working out the most difficult part: naming.\r\n\r\nThe forked repo is here: https://github.com/mariusGundersen/es-git/tree/master/packages. Ok, so I've got most of the features done, including basic fetch and push using https. This means I can now start using the project for something useful. I've created both a github and npm organization, to make it easier for others to have write-access to it, if you want to. \r\n\r\nThe repository is now here: https://github.com/es-git/es-git\r\n\r\nIt's currently quite limited in documentation, but I hope to add at least a basic readme file to each package.. So es-git is written to be a low-level git, for when you need the most basic git functionality. But it's designed to be easily extended, so you are free to build other stuff on top of it.\r\n\r\nI have put a few examples online: https://es-git-examples.mariusgundersen.net/\r\n\r\nCurrently I'm using es-git in my side-project, which is also the main driver for implementing new features: https://github.com/mariusGundersen/Ekkiog. ",
    "ulion": "Hi @creationix I'm developing a google extension to use git with google apps script in google's online script editor page. so I found js-git, the only one project focused on the browser side git. the problem is, google extension does not expose socket api, not like chrome apps, and I do want the extension can save the codes to remote git servers, like bitbucket and github ones, mainly focus to the private repos. I read some tickets related to https/ssh, etc, and it seems currently there is no way to do that? or would this one be helpful? https://github.com/aluzzardi/wssh I know the js-github has some workaround, but which does not works with bitbucket, I would like to support use bitbucket for its free private repo reason. so is it possible use the wssh or something like that to do with ssh git repos like bitbucket and github ones?\n. Thank you very much. So we can use https for both github and bitbucket? I will dig into your extension project to learn how to do that.\n\nOn the other hand, HTTPS may still expose user's password to the extension, maybe not the best way. while ssh way can benefit from the add ssh key to github/bitbucket account way to protect the password. So I still want to know is it possible to do the ssh git protocol by some workaround, e.g. WebSocket->ssh proxy tech, what need to be done for it?\n. Thank you. for my chrome extension case. I checked the google dev doc, it seems as long as the target hosts were added into the permissions part of of manifest.json, google will allow extension use xhr to access those hosts. so maybe the CORS adjust in https://github.com/creationix/corson will be not required?\n\nthen if this HTTPS way works, I can connect to github or bitbucket, though I have to use user's password to get it authed. And I still do not know how to connect to https git endpoint with js-git, specially in chrome extension, is there any example code for it? it's really pieces everywhere and I do not have idea how to put them together.\n\nOn the other hand, I agree with you that the proxy has security problem. but if the proxy is hosted by the user own, e.g. by an heroku click to deploy button with source code, then they can trust the proxy, the only problem is how to auth with github/bitbucket ssh git together with the proxy. if this can be done, we can work with github/bitbucket private repos with low risk for any users, at least before the browser side TLS implemented.\n\nWhat do you think, is this worth a try?\n. so with your guide, I found git-html5, the other js git library, which may works with https, I will look into it.\n\non the other hand, I just tested with heroku websocket sample code, it does works with https/wss natively (browser only allow wss in https webpages) this is my sample deploy: https://serene-springs-79062.herokuapp.com/ and the source code from heroku: https://github.com/heroku-examples/node-ws-test\n\nat the same time, I will look into git-browser to see how to put these pieces together (any other recommendations?), js-git has too many pieces that I can hardly get the whole picture and required sample code to make it works for my purpose.\n. instead of build http/https by js code, how about use wrappered https (by browser XHR) as a layer, to do with https git? I don't know how to do this with js-git, it is possible and what need to be done?\n\nand I also verified, github does support to generate access token to use with https, while bitbucket does not, it ask password for https, and bitbucket recommended some tools on different platforms to save the password to avoid input it every time. so, for bitbucket, use ssh would be better, though https also an option if I can only support that.\n. Finally, inspired by cloud9, I wrote chrome extension + app to export google apps scripts to local folder, then use cmdline git to handle them. I think git in web is still not strong enough for real life usage.\n. Hi @creationix I'm developing a google extension to use git with google apps script in google's online script editor page. so I found js-git, the only one project focused on the browser side git. the problem is, google extension does not expose socket api, not like chrome apps, and I do want the extension can save the codes to remote git servers, like bitbucket and github ones, mainly focus to the private repos. I read some tickets related to https/ssh, etc, and it seems currently there is no way to do that? or would this one be helpful? https://github.com/aluzzardi/wssh I know the js-github has some workaround, but which does not works with bitbucket, I would like to support use bitbucket for its free private repo reason. so is it possible use the wssh or something like that to do with ssh git repos like bitbucket and github ones?\n. Thank you very much. So we can use https for both github and bitbucket? I will dig into your extension project to learn how to do that.\n\nOn the other hand, HTTPS may still expose user's password to the extension, maybe not the best way. while ssh way can benefit from the add ssh key to github/bitbucket account way to protect the password. So I still want to know is it possible to do the ssh git protocol by some workaround, e.g. WebSocket->ssh proxy tech, what need to be done for it?\n. Thank you. for my chrome extension case. I checked the google dev doc, it seems as long as the target hosts were added into the permissions part of of manifest.json, google will allow extension use xhr to access those hosts. so maybe the CORS adjust in https://github.com/creationix/corson will be not required?\n\nthen if this HTTPS way works, I can connect to github or bitbucket, though I have to use user's password to get it authed. And I still do not know how to connect to https git endpoint with js-git, specially in chrome extension, is there any example code for it? it's really pieces everywhere and I do not have idea how to put them together.\n\nOn the other hand, I agree with you that the proxy has security problem. but if the proxy is hosted by the user own, e.g. by an heroku click to deploy button with source code, then they can trust the proxy, the only problem is how to auth with github/bitbucket ssh git together with the proxy. if this can be done, we can work with github/bitbucket private repos with low risk for any users, at least before the browser side TLS implemented.\n\nWhat do you think, is this worth a try?\n. so with your guide, I found git-html5, the other js git library, which may works with https, I will look into it.\n\non the other hand, I just tested with heroku websocket sample code, it does works with https/wss natively (browser only allow wss in https webpages) this is my sample deploy: https://serene-springs-79062.herokuapp.com/ and the source code from heroku: https://github.com/heroku-examples/node-ws-test\n\nat the same time, I will look into git-browser to see how to put these pieces together (any other recommendations?), js-git has too many pieces that I can hardly get the whole picture and required sample code to make it works for my purpose.\n. instead of build http/https by js code, how about use wrappered https (by browser XHR) as a layer, to do with https git? I don't know how to do this with js-git, it is possible and what need to be done?\n\nand I also verified, github does support to generate access token to use with https, while bitbucket does not, it ask password for https, and bitbucket recommended some tools on different platforms to save the password to avoid input it every time. so, for bitbucket, use ssh would be better, though https also an option if I can only support that.\n. Finally, inspired by cloud9, I wrote chrome extension + app to export google apps scripts to local folder, then use cmdline git to handle them. I think git in web is still not strong enough for real life usage.\n. ",
    "kriskowal": "Moved to https://github.com/creationix/git-node-fs/pull/2\n. Subsumed by #98 \n. Moved to https://github.com/creationix/git-node-fs/pull/2\n. Subsumed by #98 \n. ",
    "rashad612": "@creationix,  thanks that helped :)\n. Sorry, but running `npm install git-node-fs` gives me:\n\n``` bash\nnpm ERR! 404 404 Not Found: git-node-fs\n```\n. @creationix,  thanks that helped :)\n. Sorry, but running `npm install git-node-fs` gives me:\n\n``` bash\nnpm ERR! 404 404 Not Found: git-node-fs\n```\n. ",
    "codingisacopingstrategy": "Interesting stuff. That requires running a server process though. What I wanted to use js-git for, is to read in a local git repository so I can display its tree: given that I can read local files through ajax requests, I wouldn\u2019t need anything beyond what\u2019s already in the browser.\n\nWould I need to write a special back-end for this?\n\nThanks,\n. Hey,\n\nThanks for the swift reply + the insights in git internals. It\u2019s true that it\u2019s less useful if this metadata has to be kept up to date for each commit\u2026 do you know if this metadata is transferred when a repository is pushed to a remote, i.e. a github repository?\n. The use case I imagined is for expanded README\u2019s and other situations where code and documentation can be combined in novel ways.\n. Interesting stuff. That requires running a server process though. What I wanted to use js-git for, is to read in a local git repository so I can display its tree: given that I can read local files through ajax requests, I wouldn\u2019t need anything beyond what\u2019s already in the browser.\n\nWould I need to write a special back-end for this?\n\nThanks,\n. Hey,\n\nThanks for the swift reply + the insights in git internals. It\u2019s true that it\u2019s less useful if this metadata has to be kept up to date for each commit\u2026 do you know if this metadata is transferred when a repository is pushed to a remote, i.e. a github repository?\n. The use case I imagined is for expanded README\u2019s and other situations where code and documentation can be combined in novel ways.\n. ",
    "dfries": "On Mon, Aug 11, 2014 at 06:06:12AM -0700, Tim Caswell wrote:\n\n> I'm fine with this change assuming the two notes are fixed.  I also notice you submitted a PR to git-node-fs.  I also need the same change made to git-chrome-fs.\n> \n> The API it uses appears to also support rename, so it should be fine. http://www.html5rocks.com/en/tutorials/file/filesystem/#toc-copy-rename-move\n\nAny pointers for how to setup something to test the chrome-fs changes?\n\n> https://github.com/creationix/git-chrome-fs/blob/master/lib/chrome-fs.js\n> \n> Once `git-chrome-fs` and `git-node-fs have the fix, I'll merge the fixed change here.\n\n## \n\nDavid Fries david@fries.net    PGP pub CB1EE8F0\nhttp://fries.net/~david/\n. Commit ab50767ab50c83d2af65be8e6f72baff6ce68d9f is ready to merge.\n. I did update the pull requests for the other two repositories first, I assume you would look at them in order.  Is there a different way I should go about doing the pull requests?\n. On Mon, Aug 11, 2014 at 06:06:12AM -0700, Tim Caswell wrote:\n\n> I'm fine with this change assuming the two notes are fixed.  I also notice you submitted a PR to git-node-fs.  I also need the same change made to git-chrome-fs.\n> \n> The API it uses appears to also support rename, so it should be fine. http://www.html5rocks.com/en/tutorials/file/filesystem/#toc-copy-rename-move\n\nAny pointers for how to setup something to test the chrome-fs changes?\n\n> https://github.com/creationix/git-chrome-fs/blob/master/lib/chrome-fs.js\n> \n> Once `git-chrome-fs` and `git-node-fs have the fix, I'll merge the fixed change here.\n\n## \n\nDavid Fries david@fries.net    PGP pub CB1EE8F0\nhttp://fries.net/~david/\n. Commit ab50767ab50c83d2af65be8e6f72baff6ce68d9f is ready to merge.\n. I did update the pull requests for the other two repositories first, I assume you would look at them in order.  Is there a different way I should go about doing the pull requests?\n. ",
    "jtwaleson": "Thanks, browserify seems to do the trick! I'll see if I can get a working example and send a pull request with added documentation about build instructions. Do you have any thoughts about rewriting from common-js to AMD?\n. Thanks, browserify seems to do the trick! I'll see if I can get a working example and send a pull request with added documentation about build instructions. Do you have any thoughts about rewriting from common-js to AMD?\n. ",
    "kumavis": "@jtwaleson would like to see your work, as I plan to use this with browserify\n. Ah ok, this does seem like a problem. Did the github team give any specific pushback on why they would not enable CORS?\n. I sent an email and filed an issue on Isaac's unofficial github issue tracker https://github.com/isaacs/github/issues/263\n. @jtwaleson would like to see your work, as I plan to use this with browserify\n. Ah ok, this does seem like a problem. Did the github team give any specific pushback on why they would not enable CORS?\n. I sent an email and filed an issue on Isaac's unofficial github issue tracker https://github.com/isaacs/github/issues/263\n. ",
    "rksm": "A browserified version of js-git (that uses promises instead of generators) can be found here: https://github.com/LivelyKernel/js-git-browser.\n\nThx @creationix for this great project!\n. A browserified version of js-git (that uses promises instead of generators) can be found here: https://github.com/LivelyKernel/js-git-browser.\n\nThx @creationix for this great project!\n. ",
    "mehdisadeghi": "@creationix Could you please share the status of push implementation? As I see at least Github API v3 supports CORS.\n. @creationix Could you please share the status of push implementation? As I see at least Github API v3 supports CORS.\n. ",
    "nickj12497": "Hey, @creationix. I, like @timaeudg, am working on an app to do a lot of the git commands but without use of the CLI because we want people to be able to install the app and not have to worry about installing git. One of the main things we want the app to do is clone, but I am having a hard time finding the examples or the documentation on how to properly clone using js-git, js-github, and git-node-fs. Any help would be greatly appreciated.\n. Thank you for responding so soon and thank you for your help.\n. Hey, @creationix. I, like @timaeudg, am working on an app to do a lot of the git commands but without use of the CLI because we want people to be able to install the app and not have to worry about installing git. One of the main things we want the app to do is clone, but I am having a hard time finding the examples or the documentation on how to properly clone using js-git, js-github, and git-node-fs. Any help would be greatly appreciated.\n. Thank you for responding so soon and thank you for your help.\n. ",
    "alexbirkett": "Ok thanks!\n. Ok thanks!\n. ",
    "phuicy": "Oh cool.  \n\nNodeJS Git Server offers a multi-tenant git server using NodeJS. Thus, it has the outward facing interfaces, and user management. Like a backend to a github clone.\n\nHowever, if this is easily achieved with js-git and git-node-fs, awesome.\n. That's why I would like to use it with a server.\n. Oh cool.  \n\nNodeJS Git Server offers a multi-tenant git server using NodeJS. Thus, it has the outward facing interfaces, and user management. Like a backend to a github clone.\n\nHowever, if this is easily achieved with js-git and git-node-fs, awesome.\n. That's why I would like to use it with a server.\n. ",
    "techniq": "@creationix any ideas?\n. @creationix any ideas?\n. ",
    "jauco": "I can reproduce this bug. @whyleee it seems you have fixed this bug in https://github.com/whyleee/nogit/issues/7 what was the fix?\n. It might have broken when you switched to culvert, or it might only break on certain data streams.\n\nWhat happens is that at the end the onSave method calls channel.take(onRead) when the channel is empty. (https://github.com/creationix/js-git/blob/master/mixins/pack-ops.js#L129 ) This puts the onRead callback on the readQueue in culvert (https://github.com/creationix/culvert/blob/master/channel.js#L59 ). However the readQueue will not be touched until someone adds some new data to the stream (which won't happen because all data was already pushed to the stream). \n\nIt looks like I need to add a method to culvert to close a channel right? and then have the users of culvert use that method. @creationix is that correct?\n. I can reproduce this bug. @whyleee it seems you have fixed this bug in https://github.com/whyleee/nogit/issues/7 what was the fix?\n. It might have broken when you switched to culvert, or it might only break on certain data streams.\n\nWhat happens is that at the end the onSave method calls channel.take(onRead) when the channel is empty. (https://github.com/creationix/js-git/blob/master/mixins/pack-ops.js#L129 ) This puts the onRead callback on the readQueue in culvert (https://github.com/creationix/culvert/blob/master/channel.js#L59 ). However the readQueue will not be touched until someone adds some new data to the stream (which won't happen because all data was already pushed to the stream). \n\nIt looks like I need to add a method to culvert to close a channel right? and then have the users of culvert use that method. @creationix is that correct?\n. ",
    "whyleee": "@jauco I haven't really fixed this issue, I just used regex for progress output to find the last chunk :smile: https://github.com/whyleee/nogit/commit/8dfcb01574707b85739f497fd6e256d3d63cd402#diff-f8fba8b03f3acaae3ecdcdbb453e0d7d\n. @jauco I haven't really fixed this issue, I just used regex for progress output to find the last chunk :smile: https://github.com/whyleee/nogit/commit/8dfcb01574707b85739f497fd6e256d3d63cd402#diff-f8fba8b03f3acaae3ecdcdbb453e0d7d\n. ",
    "fabrixxm": "worked after I changed line 92 in `lib/pack-codec.js`:\n\n```\n- if (!state) return;\n+ if (!state) { if(checksum.length===40) emit(); return; }\n```\n\nIt works, but could be wrong...\n. worked after I changed line 92 in `lib/pack-codec.js`:\n\n```\n- if (!state) return;\n+ if (!state) { if(checksum.length===40) emit(); return; }\n```\n\nIt works, but could be wrong...\n. ",
    "kmalakoff": "Hi Tim, \n\nThank you for the quick response! I really appreciate you providing a list to resources and the example (plus, I'm a big fan of your work and podcast appearances!).\n\nI think that my problem is that the API is that it is very low level so there a gap between what I want to do and knowing how to do it. I would really like to find examples of performing common operations like [here](https://github.com/nodegit/nodegit/tree/master/examples) but for js-git.\n\nBecause I do not understand the structure of a git repo or how to use git at a low level (I've used a small number of CLI commands and GUI tools), it makes me feel like I have three options for using js-git:\n\n1) first port a higher level API (like nodegit) to use js-git as the driver - this way I can study how they walk things and perform operations \n\n2) find someone to port the [nodegit examples](https://github.com/nodegit/nodegit/tree/master/examples) to js-git\n\n3) find a programming guide to using git - I'm not sure if there is a reference that someone can recommend, but it would basically be a cookbook to perform common operations (eg. checking status, etc) \n\nI guess I just need help connecting the dots between the low level API and what I would like to accomplish.\n. Excellent. Thank you for explaining and the reference materials. This is much clearer now.\n\nAre you going to be doing another fund raising project to add git status, diff, etc?\n. I remember hearing you mention about your fund raising experience and the lack of direct access to services like Github on one of your podcast appearances. It is great that the passing of time is helping address some technical issues (like async/await).\n\nI'm working on a project with electron so focussing on the node.js ecosystem would be a good scope for my needs. I need to basically write a git client so diff, status, remote sync, etc are on my needs list. I also played around with the browser, but because my project's main, initial use case is the user's hard drive, I am fine to deprioritize the browser for now (but want to leave the door open for it which is why js-git is so appealing!).\n\nLoosening the requirements to proxy git requests is a fine compromise although I'm not sure if it puts other limits around git over SSH, for example. Unfortunately, my knowledge is a bit shallow on network and protocol considerations so I cannot add much there. \n. @Dashed I'm just doing the minimal work for a proof of concept so really messy, half-baked code...not really in any useable / sharable form. If I figure out a good path, I'll keep you posted.\n. @Dashed last night I looked into implementing status with js-git (again still in an experimental way).\n\nIt looks like it is implemented like:\n\n1) resolve the HEAD reference to an actual reference commit hash and collect the entries of type blob\n2) traverse the filesystem in a way that ignores the .git folder and respects .gitignore to collect the hashes for each file\n3) compare the results of 1) and 2) in two ways: a) by path looking for hash changes, b) by hash looking for renamed paths.\n\nAlso, I think there could be come caching in there to reduce rehashing checks based on modification checks.\n\nThe problem I immediately run into is that js-git's mixins virtualize git but do not virtualize the file system so writing a general-purpose implementation would require API additions. It makes sense that you would want to virtualize git so you do not need to emulate a filesystem-like interface into each type of storage, but then it seems like you might want to virtualize the filesystem anyway to implement things like status. \n\n@creationix two ways to implement this come to mind: \n\n1) virtualize the filesystem and rewrite all of the mixins to the new interface - I'm assuming that you probably think this is a little crazy given the benefits of virtualizing different types of storage methods and work that has been done to date. This is the path I've personally been experimenting with by emulating Node's fs module for a memory representation, eg. virtual file system.\n\n2) add some sort of interface like walking the filesystem - it would need to respect .gitignore and I've found a module [gitignore-parser](https://www.npmjs.com/package/gitignore-parser) that helps with testing files.\n\n@creationix given that you've written a bunch of drivers for different git implementations, with your current state of knowledge and ignoring sunk costs f what you have already implemented, what do you think is the better approach...extending the storage driver mixin APIs, emulating the filesystem, or something else? (since it seems like a slippery slope in any approach)\n\nAlso, it looks like .git/HEAD stores the working directory so in the filesystem case, it isn't a barrier to the implementation although maybe the problem is that you are referring to is a general purpose solution would need to extend the storage driver mixin API to store and modify those values and eventually the refspecs.\n\nI'm very much still climbing the git learning curve so I might have got some things wrong here!\n. I can see how given the index spec status is a problem to solve in a a general purpose way. That said, index sound emulatable on platforms like you say.\n\nAs for the API question to scan the filesystem, if you could solve the index problem in a general way, how would you see the API for js-git evolving? I realize that it is a little controversial for me to ask about abstracting in a different way (eg. by file system), but I'm on the edge of deciding what to do and would like to get your opinion since you have already thought about this and can foresee things that I'll encounter the hard way!\n\nSpecifically:\n\n1) how would you abstract traversing the file tree to check the hashes for each file?\n2) how would you abstract concepts like the working directory in HEAD, refspecs in config, etc? \n\nI guess I'm thinking that by abstracting by filesystem instead of git concepts allows filesystem drivers to be developed and tested like black-boxes, and allow for one codebase to do all of the above like updating HEAD rather than the other way around. I definitely see the benefits of both, but would like to hear your take on how you would approach it in the current API as the feature set increases.\n\nThank you!\n. Excellent!\n\nIf I understand correctly, the high-level API should be like a database that then is ported to the file system rather than emulating the filesystem. \n\nI've been noodling on a querying-based idea as a way for my own git API as a higher level of abstraction to provide an interface that requires no knowledge of the git structure...when I came up with [BackboneORM](https://github.com/vidigami/backbone-orm) and [BackboneREST](https://github.com/vidigami/backbone-rest), I made a general purpose, serializable query interface similar to MongoDB (but simpler) that was ported to each driver and could even be used in the browser's query parameters. I'm sort of used to just using higher level abstractions like `sort`, `limit`, `offset`, `pick` `values`, `in` that dealing with traversing data structures directly seems like a lot of detail to be thinking about. Digression over. \n\nI found https://github.com/creationix/git-fs-db. It this looks like a database-like interface into the filesystem. I think `git-fs-db` is the one you are referring to by `git-fs` above. Also, I found https://github.com/creationix/git-chrome-local-db which seems to have a similar interface so I think this is the `backend that stored objects and refs in indexeddb in the browser without first emulating a filesystem` example.\n\nHowever, I'm missing the piece that actually uses the higher-level interface since these modules look more like drivers to be consumed somewhere else. I've looked in https://github.com/creationix/js-git/tree/master/mixins and in some of your repositories to the module that uses them.\n. @creationix thank you for the resources. I'll go through them and try to come up to speed on what is involved...famous last words!\n\nUnless I'm mistaken, it looks like most of these example are cloning. Are there examples of push or pull as well? (I'm assuming clone is the easiest case since it is with a clean slate, right?) I'm just starting from zero knowledge on pack files so any code or tests to help me figure out some of the details on manage these probably complex operations will help me get up the learning curve fast. If not, that's fine too!\n. Thank you for clarifying. \n\nWhen I have some time, I'll study something like libgit2 or the git source code to figure out what to do for pull and push - it is easiest with some working code as a starting place. Big learning curve ahead!\n. I just find that seeing the cases that need to be dealt with and how is helpful which is why I opened this issue in the first place! That said, I haven't looked at the code yet (but did develop in C/C++ for a few years) so it might end up being a blind alley...\n\nI'll definitely end up doing a bit of both! Thank you for the advice.\n\nFYI: I've actually been quickly / hackily using js-git as a driver layer for the nodegit API for this reason. I'm hoping that the process will teach me enough about git and provide enough working code to then refactor it into a higher level API. It's been a bit time-consuming, but very helpful for learning at least.\n. Yeah, decimal - updated and submitted a pull request. https://github.com/creationix/js-git/pull/127\n. Hi Tim, \n\nThank you for the quick response! I really appreciate you providing a list to resources and the example (plus, I'm a big fan of your work and podcast appearances!).\n\nI think that my problem is that the API is that it is very low level so there a gap between what I want to do and knowing how to do it. I would really like to find examples of performing common operations like [here](https://github.com/nodegit/nodegit/tree/master/examples) but for js-git.\n\nBecause I do not understand the structure of a git repo or how to use git at a low level (I've used a small number of CLI commands and GUI tools), it makes me feel like I have three options for using js-git:\n\n1) first port a higher level API (like nodegit) to use js-git as the driver - this way I can study how they walk things and perform operations \n\n2) find someone to port the [nodegit examples](https://github.com/nodegit/nodegit/tree/master/examples) to js-git\n\n3) find a programming guide to using git - I'm not sure if there is a reference that someone can recommend, but it would basically be a cookbook to perform common operations (eg. checking status, etc) \n\nI guess I just need help connecting the dots between the low level API and what I would like to accomplish.\n. Excellent. Thank you for explaining and the reference materials. This is much clearer now.\n\nAre you going to be doing another fund raising project to add git status, diff, etc?\n. I remember hearing you mention about your fund raising experience and the lack of direct access to services like Github on one of your podcast appearances. It is great that the passing of time is helping address some technical issues (like async/await).\n\nI'm working on a project with electron so focussing on the node.js ecosystem would be a good scope for my needs. I need to basically write a git client so diff, status, remote sync, etc are on my needs list. I also played around with the browser, but because my project's main, initial use case is the user's hard drive, I am fine to deprioritize the browser for now (but want to leave the door open for it which is why js-git is so appealing!).\n\nLoosening the requirements to proxy git requests is a fine compromise although I'm not sure if it puts other limits around git over SSH, for example. Unfortunately, my knowledge is a bit shallow on network and protocol considerations so I cannot add much there. \n. @Dashed I'm just doing the minimal work for a proof of concept so really messy, half-baked code...not really in any useable / sharable form. If I figure out a good path, I'll keep you posted.\n. @Dashed last night I looked into implementing status with js-git (again still in an experimental way).\n\nIt looks like it is implemented like:\n\n1) resolve the HEAD reference to an actual reference commit hash and collect the entries of type blob\n2) traverse the filesystem in a way that ignores the .git folder and respects .gitignore to collect the hashes for each file\n3) compare the results of 1) and 2) in two ways: a) by path looking for hash changes, b) by hash looking for renamed paths.\n\nAlso, I think there could be come caching in there to reduce rehashing checks based on modification checks.\n\nThe problem I immediately run into is that js-git's mixins virtualize git but do not virtualize the file system so writing a general-purpose implementation would require API additions. It makes sense that you would want to virtualize git so you do not need to emulate a filesystem-like interface into each type of storage, but then it seems like you might want to virtualize the filesystem anyway to implement things like status. \n\n@creationix two ways to implement this come to mind: \n\n1) virtualize the filesystem and rewrite all of the mixins to the new interface - I'm assuming that you probably think this is a little crazy given the benefits of virtualizing different types of storage methods and work that has been done to date. This is the path I've personally been experimenting with by emulating Node's fs module for a memory representation, eg. virtual file system.\n\n2) add some sort of interface like walking the filesystem - it would need to respect .gitignore and I've found a module [gitignore-parser](https://www.npmjs.com/package/gitignore-parser) that helps with testing files.\n\n@creationix given that you've written a bunch of drivers for different git implementations, with your current state of knowledge and ignoring sunk costs f what you have already implemented, what do you think is the better approach...extending the storage driver mixin APIs, emulating the filesystem, or something else? (since it seems like a slippery slope in any approach)\n\nAlso, it looks like .git/HEAD stores the working directory so in the filesystem case, it isn't a barrier to the implementation although maybe the problem is that you are referring to is a general purpose solution would need to extend the storage driver mixin API to store and modify those values and eventually the refspecs.\n\nI'm very much still climbing the git learning curve so I might have got some things wrong here!\n. I can see how given the index spec status is a problem to solve in a a general purpose way. That said, index sound emulatable on platforms like you say.\n\nAs for the API question to scan the filesystem, if you could solve the index problem in a general way, how would you see the API for js-git evolving? I realize that it is a little controversial for me to ask about abstracting in a different way (eg. by file system), but I'm on the edge of deciding what to do and would like to get your opinion since you have already thought about this and can foresee things that I'll encounter the hard way!\n\nSpecifically:\n\n1) how would you abstract traversing the file tree to check the hashes for each file?\n2) how would you abstract concepts like the working directory in HEAD, refspecs in config, etc? \n\nI guess I'm thinking that by abstracting by filesystem instead of git concepts allows filesystem drivers to be developed and tested like black-boxes, and allow for one codebase to do all of the above like updating HEAD rather than the other way around. I definitely see the benefits of both, but would like to hear your take on how you would approach it in the current API as the feature set increases.\n\nThank you!\n. Excellent!\n\nIf I understand correctly, the high-level API should be like a database that then is ported to the file system rather than emulating the filesystem. \n\nI've been noodling on a querying-based idea as a way for my own git API as a higher level of abstraction to provide an interface that requires no knowledge of the git structure...when I came up with [BackboneORM](https://github.com/vidigami/backbone-orm) and [BackboneREST](https://github.com/vidigami/backbone-rest), I made a general purpose, serializable query interface similar to MongoDB (but simpler) that was ported to each driver and could even be used in the browser's query parameters. I'm sort of used to just using higher level abstractions like `sort`, `limit`, `offset`, `pick` `values`, `in` that dealing with traversing data structures directly seems like a lot of detail to be thinking about. Digression over. \n\nI found https://github.com/creationix/git-fs-db. It this looks like a database-like interface into the filesystem. I think `git-fs-db` is the one you are referring to by `git-fs` above. Also, I found https://github.com/creationix/git-chrome-local-db which seems to have a similar interface so I think this is the `backend that stored objects and refs in indexeddb in the browser without first emulating a filesystem` example.\n\nHowever, I'm missing the piece that actually uses the higher-level interface since these modules look more like drivers to be consumed somewhere else. I've looked in https://github.com/creationix/js-git/tree/master/mixins and in some of your repositories to the module that uses them.\n. @creationix thank you for the resources. I'll go through them and try to come up to speed on what is involved...famous last words!\n\nUnless I'm mistaken, it looks like most of these example are cloning. Are there examples of push or pull as well? (I'm assuming clone is the easiest case since it is with a clean slate, right?) I'm just starting from zero knowledge on pack files so any code or tests to help me figure out some of the details on manage these probably complex operations will help me get up the learning curve fast. If not, that's fine too!\n. Thank you for clarifying. \n\nWhen I have some time, I'll study something like libgit2 or the git source code to figure out what to do for pull and push - it is easiest with some working code as a starting place. Big learning curve ahead!\n. I just find that seeing the cases that need to be dealt with and how is helpful which is why I opened this issue in the first place! That said, I haven't looked at the code yet (but did develop in C/C++ for a few years) so it might end up being a blind alley...\n\nI'll definitely end up doing a bit of both! Thank you for the advice.\n\nFYI: I've actually been quickly / hackily using js-git as a driver layer for the nodegit API for this reason. I'm hoping that the process will teach me enough about git and provide enough working code to then refactor it into a higher level API. It's been a bit time-consuming, but very helpful for learning at least.\n. Yeah, decimal - updated and submitted a pull request. https://github.com/creationix/js-git/pull/127\n. ",
    "dashed": "> I need to basically write a git client so diff, status, remote sync, etc are on my needs list.\n\n@kmalakoff  is there a github repo for this being developed? I'm interested in seeing how this is done without git bindings for node.js\n. > I need to basically write a git client so diff, status, remote sync, etc are on my needs list.\n\n@kmalakoff  is there a github repo for this being developed? I'm interested in seeing how this is done without git bindings for node.js\n. ",
    "marionebl": "My bad, misunderstood the API and initalized without fs adapter :dizzy_face: \n. My bad, misunderstood the API and initalized without fs adapter :dizzy_face: \n. ",
    "li-yinan": "I plan to make a update program for my nw.js client, the core tech of git make the update small and zipped, it also can calculate the diff of two version, so the client can update from any version.\n\nI found js-git is pretty good, so I add some high-level api, such as fetch, checkout, just some wrapper for js-git, I push the compiled code to github, the client fetch from github every 10 minute, and checkout code from .git folder when the client reboot.\n\nIt works well for a long time, but one day it run into infinite loop, maybe I push a file similar to an old one, and github calculate the difference, change the old one into a ref-delta.\n\nBtw, high-level api from official repository is very much appreciated.\n. I plan to make a update program for my nw.js client, the core tech of git make the update small and zipped, it also can calculate the diff of two version, so the client can update from any version.\n\nI found js-git is pretty good, so I add some high-level api, such as fetch, checkout, just some wrapper for js-git, I push the compiled code to github, the client fetch from github every 10 minute, and checkout code from .git folder when the client reboot.\n\nIt works well for a long time, but one day it run into infinite loop, maybe I push a file similar to an old one, and github calculate the difference, change the old one into a ref-delta.\n\nBtw, high-level api from official repository is very much appreciated.\n. ",
    "CMCDragonkai": "Perhaps there can be some improvements lifted from: https://github.com/SamyPesse/gitkit-js. Just having read through the issues here, and also played around with the js-git code, there's alot of functionality missing from js-git (like init not creating the refs and objects directory, and git log not working because refs directory is missing heads/master file). I'm going to be exploring gitkit and see how it performs. (It's also more recent, with most commits 1 year old).. I've also decided to start a fork from gitkit instead of js-git. Gitkit is\nalso missing some things. In pursuit of this I hard forked webpack's\nmemory-fs to create virtualfs which supports symlinks simulated stat data\nand soon hardlinks.\n\nOn 15 Jun 2017 23:17, \"Sam Zagrobelny\" <notifications@github.com> wrote:\n\n> Nice work @mariusGundersen <https://github.com/mariusgundersen>!\n> I'd like to point out that I'm working on a similar project here\n> <http:///strangesast/js-git/tree/working>\n>\n> Much of our code is very similar, but differences I'm aware of include my\n> addition of karma testing and a removal of dependencies on creationix's\n> bodec, culvert, \"run\", etc with now-available TextDecoder/Encoder and typed\n> array operations.\n>\n> I'd be willing to concert our efforts if that's something you're\n> interested in.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/creationix/js-git/issues/132#issuecomment-308726483>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAnHHW1IwLgMHI3GvqediwehUtWvX4Zvks5sES8EgaJpZM4NwLwG>\n> .\n>\n. https://github.com/MatrixAI/VirtualFS\n\nOn 16 Jun 2017 02:59, \"Roger Qiu\" <roger.qiu@polycademy.com> wrote:\n\n> I've also decided to start a fork from gitkit instead of js-git. Gitkit is\n> also missing some things. In pursuit of this I hard forked webpack's\n> memory-fs to create virtualfs which supports symlinks simulated stat data\n> and soon hardlinks.\n>\n> On 15 Jun 2017 23:17, \"Sam Zagrobelny\" <notifications@github.com> wrote:\n>\n>> Nice work @mariusGundersen <https://github.com/mariusgundersen>!\n>> I'd like to point out that I'm working on a similar project here\n>> <http:///strangesast/js-git/tree/working>\n>>\n>> Much of our code is very similar, but differences I'm aware of include my\n>> addition of karma testing and a removal of dependencies on creationix's\n>> bodec, culvert, \"run\", etc with now-available TextDecoder/Encoder and typed\n>> array operations.\n>>\n>> I'd be willing to concert our efforts if that's something you're\n>> interested in.\n>>\n>> \u2014\n>> You are receiving this because you commented.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/creationix/js-git/issues/132#issuecomment-308726483>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AAnHHW1IwLgMHI3GvqediwehUtWvX4Zvks5sES8EgaJpZM4NwLwG>\n>> .\n>>\n>\n. @wmhilton Cool.\r\n\r\nI'm actually working on js-virtualgit https://github.com/MatrixAI/js-virtualgit which is a port of libgit2 to JS using emscripten and monkeypatching its FS to point to our recently developed js-virtualfs: https://github.com/MatrixAI/js-virtualfs\r\n\r\nOur virtualfs also completely supports symlinks, hardlinks, file descriptors, streams (tested that the stream has similar if not the same asynchronous behaviour as the node fs streams), and slated to add character devices (`/dev/null`, `/dev/zero`), fs singleton, copyfile, lseek, mknode, URL and Uint8Array, emscripten library support.. this is all in a separate branch `emscripten`.\r\n\r\nWe didn't see BrowserFS before until we started on js-virtualfs. But I think we have a slightly cleaner implementation. One downside, no browser support yet because of rollup issues.\r\n\r\nI think there's room for a JS implementation of Git, and an official port of libgit2 to JS.. @wmhilton The browserfs emscripten implementation doesn't replace the rootfs of emscripten. Because emscripten doesn't have proper rootfs remounting capability. I found a way to do this though, and have to refactor the virtualfs a bit a support some extra stuff (that I mentioned above) to make this monkeypatching work.\r\n\r\nI'm also focused on a filesystem focus of git, because I need the concept of a \"working\" directory and staging changes, thus the virtualfs requirement and needing the monkeypatch the emscripten rootfs mounting. It's also because libgit2 isn't going to be the only module interacting with the fs, I intend to get the other libraries like node tar and monkeypatch their fs reference such that I can archive the in-memory fs implementation.\r\n\r\nOther than that, the actual translation of libgit2 to JS is not difficult and is already working. https://github.com/kripken/emscripten/issues/4902. Perhaps there can be some improvements lifted from: https://github.com/SamyPesse/gitkit-js. Just having read through the issues here, and also played around with the js-git code, there's alot of functionality missing from js-git (like init not creating the refs and objects directory, and git log not working because refs directory is missing heads/master file). I'm going to be exploring gitkit and see how it performs. (It's also more recent, with most commits 1 year old).. I've also decided to start a fork from gitkit instead of js-git. Gitkit is\nalso missing some things. In pursuit of this I hard forked webpack's\nmemory-fs to create virtualfs which supports symlinks simulated stat data\nand soon hardlinks.\n\nOn 15 Jun 2017 23:17, \"Sam Zagrobelny\" <notifications@github.com> wrote:\n\n> Nice work @mariusGundersen <https://github.com/mariusgundersen>!\n> I'd like to point out that I'm working on a similar project here\n> <http:///strangesast/js-git/tree/working>\n>\n> Much of our code is very similar, but differences I'm aware of include my\n> addition of karma testing and a removal of dependencies on creationix's\n> bodec, culvert, \"run\", etc with now-available TextDecoder/Encoder and typed\n> array operations.\n>\n> I'd be willing to concert our efforts if that's something you're\n> interested in.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/creationix/js-git/issues/132#issuecomment-308726483>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAnHHW1IwLgMHI3GvqediwehUtWvX4Zvks5sES8EgaJpZM4NwLwG>\n> .\n>\n. https://github.com/MatrixAI/VirtualFS\n\nOn 16 Jun 2017 02:59, \"Roger Qiu\" <roger.qiu@polycademy.com> wrote:\n\n> I've also decided to start a fork from gitkit instead of js-git. Gitkit is\n> also missing some things. In pursuit of this I hard forked webpack's\n> memory-fs to create virtualfs which supports symlinks simulated stat data\n> and soon hardlinks.\n>\n> On 15 Jun 2017 23:17, \"Sam Zagrobelny\" <notifications@github.com> wrote:\n>\n>> Nice work @mariusGundersen <https://github.com/mariusgundersen>!\n>> I'd like to point out that I'm working on a similar project here\n>> <http:///strangesast/js-git/tree/working>\n>>\n>> Much of our code is very similar, but differences I'm aware of include my\n>> addition of karma testing and a removal of dependencies on creationix's\n>> bodec, culvert, \"run\", etc with now-available TextDecoder/Encoder and typed\n>> array operations.\n>>\n>> I'd be willing to concert our efforts if that's something you're\n>> interested in.\n>>\n>> \u2014\n>> You are receiving this because you commented.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/creationix/js-git/issues/132#issuecomment-308726483>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AAnHHW1IwLgMHI3GvqediwehUtWvX4Zvks5sES8EgaJpZM4NwLwG>\n>> .\n>>\n>\n. @wmhilton Cool.\r\n\r\nI'm actually working on js-virtualgit https://github.com/MatrixAI/js-virtualgit which is a port of libgit2 to JS using emscripten and monkeypatching its FS to point to our recently developed js-virtualfs: https://github.com/MatrixAI/js-virtualfs\r\n\r\nOur virtualfs also completely supports symlinks, hardlinks, file descriptors, streams (tested that the stream has similar if not the same asynchronous behaviour as the node fs streams), and slated to add character devices (`/dev/null`, `/dev/zero`), fs singleton, copyfile, lseek, mknode, URL and Uint8Array, emscripten library support.. this is all in a separate branch `emscripten`.\r\n\r\nWe didn't see BrowserFS before until we started on js-virtualfs. But I think we have a slightly cleaner implementation. One downside, no browser support yet because of rollup issues.\r\n\r\nI think there's room for a JS implementation of Git, and an official port of libgit2 to JS.. @wmhilton The browserfs emscripten implementation doesn't replace the rootfs of emscripten. Because emscripten doesn't have proper rootfs remounting capability. I found a way to do this though, and have to refactor the virtualfs a bit a support some extra stuff (that I mentioned above) to make this monkeypatching work.\r\n\r\nI'm also focused on a filesystem focus of git, because I need the concept of a \"working\" directory and staging changes, thus the virtualfs requirement and needing the monkeypatch the emscripten rootfs mounting. It's also because libgit2 isn't going to be the only module interacting with the fs, I intend to get the other libraries like node tar and monkeypatch their fs reference such that I can archive the in-memory fs implementation.\r\n\r\nOther than that, the actual translation of libgit2 to JS is not difficult and is already working. https://github.com/kripken/emscripten/issues/4902. ",
    "strangesast": "Nice work @mariusGundersen!\r\nI'd like to point out that I'm working on a similar project [here](/strangesast/js-git/tree/working)\r\n\r\nMuch of our code is very similar, but differences I'm aware of include my addition of karma testing and a removal of creationix's bodec, culvert, \"run\", etc in favor of now-available TextDecoder/Encoder and typed array operations.\r\n\r\nI'd be willing to concert our efforts if that's something you're interested in.. Nice work @mariusGundersen!\r\nI'd like to point out that I'm working on a similar project [here](/strangesast/js-git/tree/working)\r\n\r\nMuch of our code is very similar, but differences I'm aware of include my addition of karma testing and a removal of creationix's bodec, culvert, \"run\", etc in favor of now-available TextDecoder/Encoder and typed array operations.\r\n\r\nI'd be willing to concert our efforts if that's something you're interested in.. ",
    "wmhilton": "Gosh darnit @mariusGundersen. I guess I'll need a new name for my pile of files... https://github.com/wmhilton/esgit \ud83d\ude06\r\n\r\nWhere were all you forkers a year ago when I started on this?  ^ the above repo being my 2nd or 3rd rewrite (see predecesor from [Feb](https://github.com/meshdb/wikiboard/tree/0dcae635dc9255e01c3dd19a7ba1881b0f3df047/src/meshdb/src)) Thank god this idea is taking off! It's a freaking brilliant idea. Think we can make \"git in the browser\" a standard feature of every webapp by 2020?\r\n\r\n@CMCDragonkai I am using [BrowserFS](https://github.com/jvilk/BrowserFS) to handle git in the browser. It is a very nice, feature complete implementation of the Node fs api. I believe it has support for simulated symlinks and fs.stat calls, depending on the backend. The only thing it doesn't have is fs.watch and I'm already working on an implementation of that.. > We didn't see BrowserFS before until we started on js-virtualfs. But I think we have a slightly cleaner implementation. One downside, no browser support yet because of rollup issues.\n\nStory of my life! Something should make a service that scans Github and autodetects similar projects. FYI BrowserFS has fairly good emscripten support. I don't think it has virtual character devices, but it is fairly extensible. Once you get past the TypeScript (ugh) it's not hard to work in. I recently upgraded the XHR backend to use window. fetch, and am planning to add a window.caches backend that should support much larger files than local storage or indexed db backends. There's a key/value store interface that makes it possible to implement a new fs backend on top of any key value store interface, but since window.caches supports streaming, the author may want to take advantage of that and do something more ambitious than treating it as a kv store.\n\nGood luck getting libgit2 to run in emscripten! I absolutely agree, there is room for many many implementations. I'm focusing on a file system based implementation rather than using an in memory or indexed db approach, and focusing on replicating the file system behavior of the git CLI so I can use it interchangeably to manipulate repos on my desktop.. For anyone following this thread, I'm nearing a 1.0 release for my module! I named it [isomorphic-git](https://npm.im/isomorphic-git). I've tried to distinguish it from js-git and es-git (and gitkit, and js-virtual-git) by creating a high-level API that's more like the git CLI, and focusing on interoperability with the `.git` directory structure so it works with repos on the desktop too. The commands still left to implement before 1.0 are git log and git merge, but the rest (clone, init, fetch, push, add, remove, commit, status, checkout) are all there: https://github.com/wmhilton/isomorphic-git. Gosh darnit @mariusGundersen. I guess I'll need a new name for my pile of files... https://github.com/wmhilton/esgit \ud83d\ude06\r\n\r\nWhere were all you forkers a year ago when I started on this?  ^ the above repo being my 2nd or 3rd rewrite (see predecesor from [Feb](https://github.com/meshdb/wikiboard/tree/0dcae635dc9255e01c3dd19a7ba1881b0f3df047/src/meshdb/src)) Thank god this idea is taking off! It's a freaking brilliant idea. Think we can make \"git in the browser\" a standard feature of every webapp by 2020?\r\n\r\n@CMCDragonkai I am using [BrowserFS](https://github.com/jvilk/BrowserFS) to handle git in the browser. It is a very nice, feature complete implementation of the Node fs api. I believe it has support for simulated symlinks and fs.stat calls, depending on the backend. The only thing it doesn't have is fs.watch and I'm already working on an implementation of that.. > We didn't see BrowserFS before until we started on js-virtualfs. But I think we have a slightly cleaner implementation. One downside, no browser support yet because of rollup issues.\n\nStory of my life! Something should make a service that scans Github and autodetects similar projects. FYI BrowserFS has fairly good emscripten support. I don't think it has virtual character devices, but it is fairly extensible. Once you get past the TypeScript (ugh) it's not hard to work in. I recently upgraded the XHR backend to use window. fetch, and am planning to add a window.caches backend that should support much larger files than local storage or indexed db backends. There's a key/value store interface that makes it possible to implement a new fs backend on top of any key value store interface, but since window.caches supports streaming, the author may want to take advantage of that and do something more ambitious than treating it as a kv store.\n\nGood luck getting libgit2 to run in emscripten! I absolutely agree, there is room for many many implementations. I'm focusing on a file system based implementation rather than using an in memory or indexed db approach, and focusing on replicating the file system behavior of the git CLI so I can use it interchangeably to manipulate repos on my desktop.. For anyone following this thread, I'm nearing a 1.0 release for my module! I named it [isomorphic-git](https://npm.im/isomorphic-git). I've tried to distinguish it from js-git and es-git (and gitkit, and js-virtual-git) by creating a high-level API that's more like the git CLI, and focusing on interoperability with the `.git` directory structure so it works with repos on the desktop too. The commands still left to implement before 1.0 are git log and git merge, but the rest (clone, init, fetch, push, add, remove, commit, status, checkout) are all there: https://github.com/wmhilton/isomorphic-git. ",
    "rdzakmic": "I got the same issue.. In my case just http protocol.. Thanks, i'll give it a try.. I got the same issue.. In my case just http protocol.. Thanks, i'll give it a try.. ",
    "TheKnarf": "[`applyParser`](https://github.com/creationix/js-git/blob/master/mixins/pack-ops.js#L181) seem to need a `onError` argument, but is called without one here:\r\n\r\n[`return callback(null, applyParser({ take: take }, encodePack));`](https://github.com/creationix/js-git/blob/master/mixins/pack-ops.js#L147)\r\n\r\nI also don't understand how to use the `stream.take` that I get from calling `repo.pack`;\r\n. This seems to work:\r\n\r\n```\r\nrepo.pack(hashes, {}, (err, stream) => {\r\n\tif (err) throw err;\r\n\r\n\tfunction readStream() {\r\n\t\tstream.take((err, data) => {\r\n\t\t\tif(err) throw err;\r\n\t\t\tif(data === undefined) return;\r\n\t\t\tprocess.stdout.write(data);\r\n\t\t\treadStream();\r\n\t\t});\r\n\t}\r\n\treadStream();\r\n});\r\n```\r\n\r\nPerhaps the documentation should be updated.. [`applyParser`](https://github.com/creationix/js-git/blob/master/mixins/pack-ops.js#L181) seem to need a `onError` argument, but is called without one here:\r\n\r\n[`return callback(null, applyParser({ take: take }, encodePack));`](https://github.com/creationix/js-git/blob/master/mixins/pack-ops.js#L147)\r\n\r\nI also don't understand how to use the `stream.take` that I get from calling `repo.pack`;\r\n. This seems to work:\r\n\r\n```\r\nrepo.pack(hashes, {}, (err, stream) => {\r\n\tif (err) throw err;\r\n\r\n\tfunction readStream() {\r\n\t\tstream.take((err, data) => {\r\n\t\t\tif(err) throw err;\r\n\t\t\tif(data === undefined) return;\r\n\t\t\tprocess.stdout.write(data);\r\n\t\t\treadStream();\r\n\t\t});\r\n\t}\r\n\treadStream();\r\n});\r\n```\r\n\r\nPerhaps the documentation should be updated.. "
}
{
    "dgouldin": "Taking a stab at this now.\n. @stripecodahale before I spend the time to write tests, would you be willing to review this commit and let me know if my solution is acceptable?\nhttps://github.com/dgouldin/aws-go/commit/b3c2a1b636407f7af3adafb2660fbecdc233b333\nI'm a bit unhappy with all of the copy/paste in the template file. Once we get that branch to a place where you'd be happy to accept, I'll add test coverage.\n. I'm a little unclear on your next to last point. ValidationErrors (the map type) does already implement error. So I could simply change the return type of Validate and leave it as-is. As for aggregating in that type rather than in the model's Validate, how would you suggest that be done? Maybe something like:\nerrors := ValidationErrors{}\nerrors.ValidateRequired(v, fieldName)\n// ... other validations\nif len(errors) > 0 {\n    return errors\n}\nreturn nil\n. Also, I initially tried to factor out the template into a common reusable one, but AFAIK you can only pass 1 value to a template, and I need 2: both the key ($name) and value ($m) of the range over $s.Members.\n. @bradleyankrom any progress?\n. I got bit by inconsistent capitalization when trying to write validators. I was trying to find a field called ResourceId on route53.ChangeTagsForResourceRequest because that's how it appears in the JSON schema:\nhttps://github.com/stripe/aws-go/blob/master/apis/route53/2013-04-01.api.json#L1187\n. Yep, I realized that later. Oops.\n. Almost, except now you're emitting container elements for empty slices:\n<ResourceRecords></ResourceRecords>\n. Um, looks like I lost <EvaluateTargetHealth>false</EvaluateTargetHealth> with that commit.\n. aws.Boolean(false). It was there before that last commit. Let me try using aws.False instead ...\n. Yeah, same. It's still missing with aws.False.\n. There's no way to explicitly specify that EvaluateTargetHealth has to always be included is there?\n. Sorry if I'm being dense. <EvaluateTargetHealth>false</EvaluateTargetHealth> is back but so is <ResourceRecords></ResourceRecords>. Is there something I need to do to get this working?\n. Looks like both EvaluateTargetHealth and ResourceRecords render as I would expect when using that branch. Oddly, though, now we have a mystery element <->...</-> whose contents are the HostedZoneID used in the uri path:\n<ChangeResourceRecordSetsRequest xmlns=\"https://route53.amazonaws.com/doc/2013-04-01/\">\n    <ChangeBatch>\n        <Change>\n            <Action>CREATE</Action>\n            <ResourceRecordSet>\n                <AliasTarget>\n                    <DNSName>{elb.DNSName}</DNSName>\n                    <EvaluateTargetHealth>false</EvaluateTargetHealth>\n                    <HostedZoneId>{elb.HostedZoneID}</HostedZoneId>\n                </AliasTarget>\n                <Name>{hostname}</Name>\n                <Type>A</Type>\n            </ResourceRecordSet>\n        </Change>\n    </ChangeBatch>\n    <->{hostedZoneID}</->\n</ChangeResourceRecordSetsRequest>\nThis wasn't happening in previous commits.\n. Ah, noticed one more regression: good ole <Changes> between <ChangeBatch> and <Change>. :frowning: \n. :bell: :bell: :bell: we have a winner!\n. I'll check it right now! :speedboat: \n. Ok, the response I got from Amazon was \"Unexpected complex element termination\", which is different than the previous \"Invalid XML ; Premature end of file.\", so feels like progress if not a fix.\n. I'm attempting to alias an ELB inside a VPC to a public Hosted Zone. I inspected the body, prettied it up. It's generating:\n<ChangeResourceRecordSetsRequest>\n    <ChangeBatch>\n        <Changes>\n            <Action>CREATE</Action>\n            <ResourceRecordSet>\n                <AliasTarget>\n                    <DNSName>{elbDNSName}</DNSName>\n                    <HostedZoneId>{hostedZoneID}</HostedZoneId>\n                </AliasTarget>\n                <Name>{targetDomainName}</Name>\n                <Type>A</Type>\n            </ResourceRecordSet>\n        </Changes>\n    </ChangeBatch>\n</ChangeResourceRecordSetsRequest>\n. Comparing to Amazon's example:\nhttp://docs.aws.amazon.com/Route53/latest/APIReference/CreateAliasRRSAPI.html\nlooks like the parent  tag is missing as is the namespace\n. Hmm, getting a nil pointer dereference panic. Not sure why yet ...\n. Yep, works. My panic was unrelated.\n. Derp, spoke too soon:\n\"Invalid content was found starting with element 'Change'. One of '{\"https://route53.amazonaws.com/doc/2013-04-01/\":Comment, &quot\n;https://route53.amazonaws.com/doc/2013-04-01/\":Changes}' is expected.\"\n. Right, looks like it goes straight from <ChangeBatch> to <Change> and skips <Changes>.\n. I found the same error. Naming discrepancy is between:\nhttps://github.com/stripe/aws-go/blob/3af6c09b369877162b7ee6d3d5aae44dbd6a489d/gen/route53/route53.go#L2161\nand\nhttps://github.com/stripe/aws-go/blob/3af6c09b369877162b7ee6d3d5aae44dbd6a489d/gen/route53/route53.go#L2173\n. More context. This is the line that broke it:\nhttps://github.com/stripe/aws-go/commit/04948d889a32eb32eebe49b4b80d728a77759b9b#diff-a88380de8940c76b325825cd2ad7a536R1977\nIt seems incorrect to always assume that the structure name is the same as the xml name. In the JSON schema, they're different:\nhttps://github.com/stripe/aws-go/blob/7d2cf43a60673367f706efcee32454e3f5855b37/apis/route53/2013-04-01.api.json#L2049\n<Config> is the right element name according to the documentation:\nhttp://docs.aws.amazon.com/Route53/latest/APIReference/API_ListHostedZones.html#list-hosted-zones-response-config\n. So I've taken a look, and I think it's the wrong approach to use a struct tag on XMLName for generated models. As this bug report shows, Go's XML marshaling requires the tag name of the struct to match any field with its own struct tag where that struct is used. Unfortunately, AWS APIs aren't always consistent with the tag name used to represent a given shape.\nAfter studying the JSON schema a bit, it looks like xml namespaces are defined on the operation's input, not on the shape. Theoretically the same shape could be used with different namespaces for different operations (though I doubt that ever actually happens). But Go has an order of precedence to determine the XML tag and namespace to use:\n- the tag on the XMLName field, if the data is a struct\n- the value of the XMLName field of type xml.Name\n- the tag of the struct field used to obtain the data\n- the name of the struct field used to obtain the data\n- the name of the marshaled type\nI have a commit locally which updates the code generation template to specify a value for the input root node's XMLName just before marshaling the request body. But now we have another problem. We've forked the XML marshaler because of its omitempty behavior, and the fork makes the assumption that the tag will always be defined in the XMLName struct tag. So now I'm reading about reflect in an attempt to add that behavior to the forked marshaler.\nA bit of a rabbit hole, but ... progress. Kind of.\n. I'll look at the test failures now.\n. Ugh, yeah they're different formats as well as content types:\nhttp://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html#RESTErrorResponses\nvs\nhttp://docs.aws.amazon.com/Route53/latest/APIReference/requests-rest-responses.html#requests-rest-error-responses\n. Sad times: https://github.com/boto/botocore/blob/978614e459e0fab2d17889a996f220e23939d01f/botocore/parsers.py#L651\n. Perhaps the uri should be built up using net/url's URL. It should do the escaping for you.\n. Never mind, I got the same signing error using URL. Now to figure out why ...\n. Ok, it looks as though there's a discrepancy between go's url escaping and Amazon's. Go explicitly ignores commas in uri paths:\nhttp://golang.org/src/net/url/url.go#L81\nAnd what's more, they're right to do so. From RFC 3986 Section 3.3:\nAside from dot-segments in hierarchical paths, a path segment is\nconsidered opaque by the generic syntax.  URI producing applications\noften use the reserved characters allowed in a segment to delimit\nscheme-specific or dereference-handler-specific subcomponents.  For\nexample, the semicolon (\";\") and equals (\"=\") reserved characters are\noften used to delimit parameters and parameter values applicable to\nthat segment.  The comma (\",\") reserved character is often used for\nsimilar purposes.  For example, one URI producer might use a segment\nsuch as \"name;v=1.1\" to indicate a reference to version 1.1 of\n\"name\", whereas another might use a segment such as \"name,1.1\" to\nindicate the same.  Parameter types may be defined by scheme-specific\nsemantics, but in most cases the syntax of a parameter is specific to\nthe implementation of the URI's dereferencing algorithm.\nEven without the non-ascii characters in your bucket name, if you include a comma, you get the invalid signature error message. It appears Amazon's escaping algorithm is not to spec. I'm now going to take a look at how python and boto handle this case.\n. That first line does the escaping: https://play.golang.org/p/X-5frxEgZA\n. Found this gem: https://github.com/mitchellh/goamz/blob/master/s3/s3.go#L660\n. Even worse, looks like boto has an s3 specific variant for v4 hmac auth:\nhttps://github.com/boto/boto/blob/develop/boto/auth.py#L549\n. Wow, um, yubikey fail :frowning: \n. ",
    "stripecodahale": "Some notes:\n- The model package is intended to be purely for generator code. If you want generated code to use common code, that's what the aws package is for.\n- golint is going to complain about having an else branch where the if branch always returns.\n- Instead of just a simple map, I think we'll want a multi-error type which itself implements error. I'd rather see func (b *Blah) Validate() error, which would simplify client code. Then folks could type check or interface check for more details. This would also allow us to push most of the error aggregation code into that type, vs. checking for nil and aggregating in the generated code.\n- You could fix the copypasta by moving it into a common template (i.e., a template defined in common).\nOther than that, I think this is great!\n. Yes, I meant changing the return type of Validate to be error.\nIn terms of reducing the generated code, I mean taking something like this:\ngo\nif err := model.ValidateEnum(v, \"OnFailure\", OnFailureEnum); err != nil {\n    errors[\"OnFailure\"] = append(errors[\"OnFailure\"], err)\n}\nAnd turning it into something like this:\ngo\nerrors.Add(model.ValidateEnum(v, \"OnFailure\", OnFailureEnum))\nValidateEnum would return an error implementation which had a Field method (or something), which Add would check against (after checking for nil, naturally) and do the aggregation internally.\nDoes that make sense?\n. Awesome. I think it's probably as simple as generating structs which implement error for those shapes, then mapping the APIError returned by the client to those.\n. Awesome, thanks!\n. If that enthusiastic thumbs up means you'd like to work on it, ideally it'd be another func returning a Credentials implementation. The trick, I think, will be to make sure the expiration is handled transparently.\n. I haven't looked into it yet, hence the lack of implementation. :) Sorry.\n. Not at all. I'd rather do it sooner than later, in fact.\n. This was added for IAM creds in a20ba02.\n. This should be fixed now.\n. Thanks!\n. Thanks!\n. Thanks!\n. You need to set the ContentLength field of the PutObjectRequest struct.\nThis type of error will be a lot easier to understand once client-side validation is implemented.\n. Hmm, interesting.\nI'm not sure what the issue is, then\u2014I've got almost exactly the same code and it works fine.\n. The only difference between my application and your code snippet is that I'm using a *bytes.Reader for the body and you're using an *os.File. That doesn't seem like it should make a difference, though.\n. I don't have time right now to do that, but I'll try to get around to it.\nAuthentication errors like that tend to provide full debug details\u2014could you look for those (i.e., in the response body) and see what isn't lining up?\n. Just tried it with a hard-coded bucket name, the us-east-1 region, a ~1KiB file and it worked. Can't repro, sorry.\n. Either way, glad to hear it's working!\n. Happy to track here; our convention at Stripe is to prepend [WIP] to the PR title until it's done.\nI don't think this PR should handle the combination of credentials. I think just adding an IAMCredentials func would suffice. Handling the fallback logic can wait.\nAlso, @nelhage brings up a good point. The current API is just a best guess; I'm happy to change it to meet use cases.\n. It's only used by the request signing code, in aws/sign.go.\n. I think that the Credentials refactor is part of this. Specifically, I'd like to avoid a refactor done in advance of having the use cases nailed down.\n. LGTM so far. For testing, I think it would make sense to keep the metadata server URL as a package var and swap it out for an httptest.Server URL in a test.\n. I'd recommend github.com/juju/errors.Annotatef.\n. It might need the x-amz-security-token header to be signed?\n. Try merging in master and seeing how that goes. You'll need to resolve some conflicts, sorry.\n. Ok, I pushed some changes which are going to make life short-term hard but long-term awesome.\nI split out aws.CredentialsProvider (an interface with a single method which returns aws.Credentials or an error) and turned aws.Credentials into a simple struct.\nSo now it's just a matter of implementing a CredentialsProvider to return the proper creds.\n. Sorry to steal the glory, @pwaller, but the API changes meant that the IAM creds logic become super easy to implement.\nOn the upside, IAM creds should now work!\nThanks for your work, and for helping us talk this out.\n. Closed by #15. Thanks!\n. Awesome! Thank you.\n. Given that there are literally two of these types of fields, I'm just special-casing them to be float64 members. That's a lot easier than making all of the other packages deal with wrapper types.\n. Thanks!\n. I haven't found a good mechanism for this.\nIn particular, KMS has GrantOperationReEncryptTo, but Route53 Domains has CountryCodeTO.\n. That's not really related to this issue.\nIn the generator templates, any name needs to be passed to the exportable helper func in order to be a Go identifier. That's totally undocumented, which is my bad, but ResourceId is an unambiguous case, unlike country codes.\n. Thanks!\n. Thanks for pointing this out! I circled back and filled out the implementation for both Query and EC2 clients.\n. Thanks!\n. Thanks for pointing this out!\n. This change removes the ability to parse just about any other responses:\ne.g. from autoscaling.go:\ndiff\n type DescribeAutoScalingInstancesType struct {\n-   InstanceIDs []string         `xml:\"InstanceIds>member\"`\n-   MaxRecords  aws.IntegerValue `xml:\"MaxRecords\"`\n-   NextToken   aws.StringValue  `xml:\"NextToken\"`\n+   InstanceIDs []string         `xml:\"member\"`\n+   MaxRecords  aws.IntegerValue `xml:\"\"`\n+   NextToken   aws.StringValue  `xml:\"\"`\n }\n. I looked into this, and the distinction here is that SQS's MessageList shape has a flattened attribute, which is new to me. I'll add some code to support it.\n. Sorry about that. This should fix it.\n. @fumin, can you verify that this fixes your problem?\n. Yep, totally something this library should do.\nI'd say add it as an exported method to Context.\n. All of the gen subpackages are generated. If you want to add a non-generated package, it would go next to aws and gen\u2014like the cfn package.\n. I'm honestly not seeing why this would require code generation, nor why this wouldn't be a small number of public methods on Context: SignS3URL, SignCloudFrontURL etc.\n. I still don't think that warrants doubling the API's surface area.\n. Thanks!\n. Awesome, thanks!\n. Yeah, it looks like a bug.\n. I don't have a good Route53 use case or much time to work on this, but I'd absolutely accept a PR for this. It looks, in general, like the input shape for ChangeResourceRecordSets doesn't have payload set, which means the current template skips it. Not all requests should be serialized into request entities, so I'm not sure how to disambiguate. Perhaps you can.\n. Let's track the debugging over here, @dgouldin. That last change should do it?\n. How about now?\n. Did you set that using aws.False or aws.Boolean? It should only skip that if the value is nil.\n. This looks like an inconsistency w/ encoding/xml: http://play.golang.org/p/4pOo8Knh8c\n. Yep: https://github.com/golang/go/issues/5452\nNot sure what to do w/ this.\n. Ideally, the pointer serves as that sigil, but that doesn't work with encoding/json.\nWe can either encode everything or skip all zero values.\n. d3f7b74d06c4998ff974b2e06bcb694ce041c561 should restore manual control, at least, but it won't be satisfactory until the underlying bug in encoding/json is fixed.\n. ResourceRecords has omitempty set, so it shouldn't be marshaling empty slices.\nI'm tracking the XML zero value issue over at #32, but it's mostly awkward staring at stdlib bugs.\n. I might have a fix: https://github.com/stripe/aws-go/tree/xml-zero-value-kludge\nWould you give that a try? I think it should line up with expectations.\n. Pushed a fix for that\u2014I hadn't re-implented the bit where it skips elements named -. Try again?\n. Fixed. Maybe now?\n\nCoda Hale\ncoda@stripe.com\n\nOn Dec 20, 2014, at 2:12 PM, David Gouldin notifications@github.com wrote:\nAh, noticed one more regression: good ole  between  and . \n\u2014\nReply to this email directly or view it on GitHub https://github.com/stripe/aws-go/issues/30#issuecomment-67751783.\n. Hmm. Might be the lack of namespace?\n. Try this latest fix?\n. Yaaaay. Thanks for your help.\n. Ah, yeah. It needs that wrapper element.\n. I agree. I don't have time to work on this myself, but I'd happily accept a patch.\n. A recent PR added the response status code to APIError. Do you think that's sufficient for your needs, or did you have anything else in mind? Looking at what documentation exists for JSON-based protocols, I can't find any examples of attributes beyond Type and Message.\n. I'm going to close this, given the changes to the error handling in the past month. If there's a specific piece of information which isn't represented in the errors, please open another issue.\n. Thanks!\n. Very cool! Thanks!\n. Awesome, thanks!\n. Good call. I'd happily accept a patch for this.\n. I've heard from more than a few people that their test suite has major bugs, and I don't know if those have been fixed. If you'd like to take this on, you're more than welcome to.\n. Thanks!\n. I think this is correct. If it's not absolutely correct, I think it's closer than what I had.\n\nThank you!\n. Included in #55.\n. Included in #55.\n. Included in #55.\n. Included in #55.\n. Awesome stuff! Thanks!\nIn the future, I'd prefer single-issue PRs, since they're easier to review and reference over time, but this wasn't too much trouble.\n. Thanks!\n. Awesome work! Thank you!\n. One second has worked for us in production, but I'd accept a PR to make that configurable, yeah.\n. The \"default timeout\" of net/http/Client is zero, which means requests will hang indefinitely. This is not the desired behavior, hence the timeout.\n. This isn't currently possible, but it would be a welcome change.\n. As long as the overrides are for specific regions \u2014 e.g. endpoints.Add(\"test1\", etc)\u2014it sounds good.\n. Sorry, that was vague. I'm going through email before having coffee. :)\nI mean, I'd want to provide the ability to add test regions with specific endpoints rather than redefine the behavior of existing regions. Essentially, just add a region name to the (service, endpoint) tuple.\nThis way, test code could e.g. safely depend on a test-1 region instead of modifying the behavior of an existing region.\n. Awesome, thanks.\n. Oh, sorry. I'm going to have to back this out. I didn't see that it's a change to a generated file. I'll merge this functionality in myself.\n. I added the warning in c3701e6801cfc1a6108360908404b86951f65946 and https://github.com/stripe/aws-go/commit/902d3ec7dfd82589e9cbf95983e101e93ae3e3ce, and added the overrides in e8ad61ee613925915591aa0a97946fbf70f0a3f8, so you should be good to go.\n. None of these changes are in botocore that I can see. I'm explicitly not curating my own model data with aws-go, but rather simply tracking botocore's develop branch. Where did you get the model data for this?\n. Once this lands in botocore I'll update the model data and re-generate the client.\n. Awesome, thanks!\n. How about RequestCredentials?\nAlso, it should be ID.\n. ",
    "bradleyankrom": "I'll take a look at this this week. \n. Honestly, not much as of yet. I'll have a bit more time to work on it this week, but if someone else wants to jump in and take care of it quickly, just let me know.\n. ",
    "lsegal": "Develop was merged into master today, so I am marking this as closed since it should be resolved by the new implementation. There are still improvements to be made to error codes in general. Thanks for reporting! Feel free to open a new issue or open a PR to further improve error handling functionality in the SDK.\n. Develop was merged into master today, so I am marking this as closed since it should be resolved by the new implementation. The new implementation uses a consistent capitalization scheme that should handle most of these cases. Feel free to report if there are any outlying incorrectly capitalized names. Thanks for reporting!\n. Sample code working with the API changes in the develop branch:\n``` go\nsvc := s3.New(nil)\nreq, _ := svc.GetObjectRequest(&s3.GetObjectInput{\n    Bucket: aws.String(\"BUCKET\"),\n    Key:    aws.String(\"KEY\"),\n})\nurl, err := req.Presign(300 * time.Second)\nif err != nil {\n    panic(err)\n}\nfmt.Println(url)\n``\n. @nathany the above code supports signed URLs for any operation.\n. @nathany they've diverged because of some master-specific fixes that will eventually be clobbered.\n. I just added an experimental branch (paginators`) that adds pagination methods to each operation that has pagination configuration. Sample usage might be:\n``` go\npackage main\nimport (\n    \"fmt\"\n\"github.com/awslabs/aws-sdk-go/aws\"\n\"github.com/awslabs/aws-sdk-go/aws/awsutil\"\n\"github.com/awslabs/aws-sdk-go/service/dynamodb\"\n\n)\nfunc main() {\n    c := dynamodb.New(nil)\n    pages := c.ListTablesPages(&dynamodb.ListTablesInput{\n        Limit: aws.Long(1),\n    })\n    for page := range pages {\n        fmt.Println(awsutil.StringValue(page))\n    }\n}\n```\nThe API is early and still subject to change, and feedback is welcome.\n. @nightlyone break should work \"correctly\", see this example: http://play.golang.org/p/Kl7Rg3mF-C\nAt worst, our current implementation might be sending at most one more page than needed when breaking mid-way, but that's an implementation issue that can be cleaned up by re-organizing the code just a bit (by calling page.Send() after sending to the channel and pre-warming for the first request).\n. @nightlyone good point about the garbage, I was unaware of this discrepancy. I think the biggest win for wanting iterators over a func callback is that the func callback would require the signature declare both the actual response struct type being passed in as well as a boolean (or error) \"continue\" return value flag:\ngo\ndb.ListTablesPages(func(idx int, page *dynamodb.ListTablesOutput) bool {\n    if idx >= 3 {\n      return false\n    }\n    // do something with `page`\n    return true\n})\nThe range iteration syntax let's users take advantage of type inference. There are a lot of structures to deal with in the SDK-- anywhere we can take advantage of inference (especially on output), the better the experience would be, in my opinion.\nThat said, if the signature is acceptable, we can actually potentially do some fancy things with reflection to avoid generating separate iterator methods per operation. For example, we can use the already-existing ListTablesRequest() operation which returns aws.Request. Since the output type is already attached to req.Data, we can match it and make the following work:\ngo\nreq, _ := db.ListTablesRequest(nil)\nreq.EachPage(func(idx int, page *dynamodb.ListTablesOutput) bool {\n    // ...\n})\nWe could even go as far as having an EachPage operation right on aws.DynamoDB itself, since we could probably match output types to their respective operation:\ngo\ndb.EachPage(func(idx int, page *dynamodb.ListTablesOutput) bool {\n    // we know to call ListTables because we have the ListTablesOutput type\n})\nThe last one is a bit magical and might have some restrictions but certainly the easiest to use.\nThoughts?\n. @jasdel cursor-style iteration is difficult because the return value can only be of a single \"Page\" type. This works fine when there is only a single type, but when you're dealing with hundreds of different pageable types, the API doesn't seem quite as nice. Functionally, although this might work, a generic page type would be extremely difficult to use, as each page would have to be type asserted to the known type (which would not be obvious). Basically you lose all type information, which is really not ideal. \nAt least from the callback perspective, you can maintain the type information.\n. @jasdel note also that the Next logic is already supported in the current implementation with the NextPage() code. A user could certainly access the API like that already (at least with the current impl), but again, it would be a really awkward interface to use directly.\n. @jasdel PageX types would work, but I'm concerned about the general approach of adding N new types for each new feature. After this, and waiter support, and others, we might be looking at serious type bloat to the point that it might start affecting discoverability / documentation. Ideally I'd like to avoid the ListTablesPages() et al operations altogether as well. Callbacks would be the best choice from that perspective (since a single EachPage could be used as described above). Basically there are a bunch of pros and cons to juggle here. Technically speaking there are lots of ways to accomplish pagination, many of which seem equally idiomatic-- the trick is finding the one with the best balance of maximum usability and minimal bloat.\n. API was just updated to use function callbacks. This probably won't be the last iteration on the API, but it should solve the goroutine garbage issue for anyone making use of this for now.\n. Landed in #247.\n. There is experimental support for waiters in the waiters branch. Sample waiter code:\n``` go\npackage main\nimport (\n    \"fmt\"\n\"github.com/awslabs/aws-sdk-go/aws\"\n\"github.com/awslabs/aws-sdk-go/service/ec2\"\n\n)\nfunc main() {\n    svc := ec2.New(nil)\n    fmt.Println(\"Waiting on instance...\")\n    svc.WaitUntilInstanceRunning(&ec2.DescribeInstancesInput{\n        InstanceIDs: []*string{aws.String(\"i-12345678f\")},\n    })\n    fmt.Println(\"RUNNING!\")\n}\n```\n. @cbroglie there's some internal refactoring in the works to improve extensibility like adding retry handling, once that drops in we'd certainly be up for looking at a PR. That said, this feature is definitely on our plate to get the Go SDK up to speed with the AWS SDKs for our other supported languages. I've already been doing a bit of digging on this myself.\n. There's actually some basic implementation for this in 2a6ecb44b037ee9479d43c62fb4cb463fee6d9de-- it's not yet a \"complete\" implementation, though.\n. Develop was merged into master today, so I am marking this as closed. Basic retry support is now in master, although there are definitely improvements that can be made (specifically around throttling).\n. Hey!\nSimpleDB requires signature version 2 signing which the SDK does not currently support. We're looking into that but getting the rest of the SDK up to speed is currently the priority. Sorry for the confusion about the expectation that SimpleDB should work-- I'll be updating our \"supported services\" list soon to reflect a more accurate listing of what is currently working today.\n. Thanks for the suggestion. It looks like github.com/aarzilli/sandblast depends on the old package so we can't quite switch over yet. Fortunately there is only one instance of such an import and it's in our code generator layer only, which means that it should not affect runtime usage.\nIn the meantime I'll leave this open until we find a way to either update the dependency or move off of sandblast.\n. Sweet, @aarzilli! That was fast. I'll move our code over in a bit.\n. This is fixed in develop and will likely be merged into master prior to 1.5 so I will mark this as closed.\n. You can currently override the http.Client in the service constructor itself, for example in DynamoDB:\ngo\n// New returns a new DynamoDB client.\nfunc New(creds aws.CredentialsProvider, region string, client *http.Client) *DynamoDB {\nNote that this API will be changing a bit (see the new Config structure used by service constructors in the develop branch), but that struct will also allow overriding of the HTTP client.\n. Marking this as closed since this is supported in both master and develop branches.\n. @justonia @kidoman thanks for reporting and supporting this feature. Can you elaborate a little bit more on how the SDK itself would expose this context pattern? The SDK currently does not expose goroutines and we are leaving this to be handled by consumers of the library. Since we don't deal with concurrency primitives at the SDK level, I'm trying to understand how the context pattern would fit into our design.\nIt seems to me like the context pattern could be applied by being built on top of the SDK via composition (in the same way that goroutines themselves would be). Is there something the SDK needs to do explicitly to support this pattern being built out around the core API?\n. It's worth noting that the above code provided by @jasdel is already usable today in the public API if you do want to cancel early. Adding req.Cancel() would be a convenience method only.\n. @kidoman \n\nI always thought that \"golang.org/x\" was home for packages which the team does not want to add to the stdlib but is a critical part of the golang eco-system. \n\n/x/ is explicitly unstable: https://github.com/golang/go/wiki/SubRepositories -- the widespread usage does not mean very much, since breaking changes can and have occurred (even through the short lifespan of this project when we had previously used some packages from /x/). We are attempting to have very strict backward compatibility guarantees on the first major version of our SDK, so we cannot base our entire architecture on a library that is explicitly not making those same guarantees. Instead, we are inverting are architecture such that the core codebase is stable but extensible enough to support these abstractions when built on top.\n\nI would rather use it everywhere vs looking for API specific cancellation mechanisms.\n\nThe idea here is that it should be possible to build context patterns on top of the primitives that the SDK exposes. In other words, you should be able to wrap API specific cancellation mechanisms in a nice context package without affecting core SDK logic. After you write this abstraction once, you should not have to deal with the API directly, if that's your specific goal.\nIf someone is having a problem building out these abstractions, I think that's a better conversation to have at this time. One of the goals of the SDK is to provide a solid core foundation to compose other abstractions if they are not yet provided, so if it is not possible to throw context into the mix, that's something we should look at making more feasible. \n. @seiffert \nJust a preliminary note here to avoid confusion: the SDK team can provide a better / more up-to-date answer, but I just wanted to chime in with some extra info from when I was working on this codebase (I no longer do), so here goes:\nHaving separable context logic seems sensible, and ctxaws looks very cool. A couple things to note, though. Firstly, there are a number of ways to solve this problem, and many of them do not require async. Specifically, you probably don't actually want to be canceling HTTP requests anyway (and there are synchronous ways to do that with HTTP timeouts, as ctxaws seems to use).\nOne way to do it would be to lower your MaxRetries configuration parameter. If exponential backoff really is creating a slowdown, you might want to just not retry that many times and try later. You can even completely turn it off and use your own retry logic outside of the request context. That's a little uglier, but it works. \nAlso, contrary to your above statements, you can control precisely how and when a request retries. First, ShouldRetry is only ignored when the request already has an explicit answer about whether it should retry from an earlier part of the request cycle. This mostly only happens for clear scenarios like networking errors or checksum issues; basically, things that you should probably not be overriding anyway. In other words, it should be safe enough to implement Retryer. That said, if you really wanted full control of retryability, you can do so by hooking into the ValidateResponse step and (re-)setting r.Retryable yourself. Both of these allow you to cancel \"atomically\", i.e., stopping between HTTP requests rather than sending an HTTP request and canceling while it's been sent (a somewhat unsafe move if the server already received the request, as you noted).\n\nNormally when performing requests to a backing service via HTTP, I prefer to use the x/net/context/ctxhttp package which provides helper methods to perform cancellable HTTP requests. So why not use them in the AWS SDK's \"send handler\"? This is what I did and until now I'm quite confident about this approach.\n\nThe issue here is that the \"send handler\" is not the entire request, and, in fact, only a small part of it. In the case of a low latency service like DynamoDB, the Send portion of the request is only about ~5-50ms of the request. The actual retry delay happens in further handlers, so wrapping just Send inside of a context would not make the request cancelable in your specific scenario, namely, when your CPU is waiting on exponential backoff. Send is also the least safe place to cancel at, because of the notes above. You should use regular HTTP timeouts for that. \nIf the SDK does ever end up using contexts, they would have to use it in the way you implemented it with ctxaws, wrapping the entire request handler chain inside of a context object, not just individual handlers. Basically it would have to be composed the other way around. As I mentioned before, a separable and optional context package like the one you created is actually a pretty solid idea and, at least from my perspective, how I envisioned context support being cleanly built on top of the SDK.\n. @seiffert you're right that Timeout raises the same potential issues. If you are not having network slowdown issues, then Timeout isn't what you want anyway. An HTTP request that was timed out may actually trigger a retry in the SDK (with backoff, though I'm not 100% sure if it actually retries, currently), so it doesn't fundamentally solve the problem anyhow. You can completely omit the Timeout portion of this solution and still get the most robust version of what you are looking for.\nAnd yes, implementing the Retryer or ValidateResponse step to check whether your context has exceeded would be the best way to do it. Doing this solves the problem of being able to effectively cancel a request at a safe stop point during the request's lifetime when it is wrapped in a controlling context object.\n\nIf I understand you correctly, you suggest to set sensible HTTP timeouts and rely on a custom Retryer otherwise, correct?\n\nTo reiterate, only the Retryer (or ValidateResponse) is the necessary part. Timeouts can be useful in some cases where the network is the bottleneck (long requests due to latency / packet loss, or large amounts of data being pulled out, and ideally in a read operation). You may choose to enable timeouts in those cases, but in the specific case of a low-latency DynamoDB connection, it's probably not needed (your mileage may vary).\n. This should be fixed by the above commit in develop. I am going to mark this as resolved since this is a feature request. Thanks for bringing this up!\n. Hey Justin,\nThanks for the feedback. We definitely know that there are improvements that can be made to logging capabilities in the SDK. Incremental changes will be made as we start to build out these features, but I think the above commit is a good start that solves the core problem reported in this issue. If you want to put together a PR that uses a more user-friendly interface, I'd be happy to take a look at it.\n. @justonia I still have to take a close look, but can you explain why the logging interface would be affected by the context pattern? Are you suggesting that simply using the log.Logger type as alluded to above would still be insufficient?\n. @nightlyone I think a PR to use log.Logger would be a step in the right direction while we discuss the implications of the context pattern in the SDK more generally.\n. Hey @pwaller. I tried pulling down your program and running it (with a few modifications) and wasn't able to reproduce the signing error. This is the diff I used to get it working for me is below, mostly I just updated the imported package name and changed code to use a more generic credential provider:\n``` diff\ndiff --git a/main.go b/main.go\nindex 3e1865c..8ba2294 100644\n--- a/main.go\n+++ b/main.go\n@@ -7,8 +7,8 @@ import (\n        \"log\"\n        \"os\"\n\n\"github.com/stripe/aws-go/aws\"\n\"github.com/stripe/aws-go/gen/s3\"\n\"github.com/awslabs/aws-sdk-go/aws\"\n\"github.com/awslabs/aws-sdk-go/gen/s3\"\n )\n\nfunc main() {\n@@ -25,8 +25,7 @@ func main() {\n        }\n        bucket, key := flag.Arg(0), flag.Arg(1)\n\ncreds := aws.IAMCreds()\ncreds := aws.DetectCreds(\"\", \"\", \"\")s := s3.New(creds, \"eu-west-1\", nil)\nresp, err := s.GetObject(&s3.GetObjectRequest{\n        Bucket: aws.String(bucket),\n\n```\n\n\nAre you using this through a proxy by any chance?\n. @pwaller it shouldn't be, but it might have to do with the session token. I'm using credentials provided in the shared credentials file, which doesn't use a session token. That might be the issue. I'll look into it.\n. I'm using master to test this. This is more than likely an issue with the way we are signing (or not signing, in this case) the session token. I'm investigating now and should have a quick patch shortly.\n. @pwaller indeed, looks like the signer was adding the session token after building the signature, it was a simple fix to swap order. Thanks for reporting!\nAs a sidenote, I would recommend vendoring dependencies on this library, especially in the coming weeks as we will be doing some API shuffles and refactors that may break you-- hopefully not with regressions like these though (we will be beefing up tests as we go)!\n. This looks like a duplicate of #72. If you agree I'd be inclined to close this out and follow the discussion there.\n. Closing this out for #72. Let's have this discussion there. \n. @ncw thanks for the comments. Develop is definitely still in the works w.r.t. any code quality, the unchecked assertion won't stick around and will be replaced by more standard client-side validation (see #1) on the Body type.\nThat said, it will be necessary to have a seekable stream in order to sign any large payload with S3, otherwise you simply cannot sign large payloads in chunks. Still trying to figure out a good way to do this, I imagine there will probably be some extra reflection necessary to get at the underlying type wrapped in the nopCloser. Any other suggestions would be helpful. \n. > Is it possible to send the signed hash as an http trailer? \nUnfortunately not, AWS does not currently support trailing headers.\n\nYou shouldn't need to do reflection to sort out what sort of io.Reader you have, just do type assertions to the various interfaces and give an error if you can't find one you need.\n\nThe problem is listed in your panic() error, but in my tests, wrapping even a seekable stream (os.Open()) inside of a ioutil.NopCloser will make it so the checked assertion fails with the above check. You have to actually indirect the interface to get at the underlying type, but I may be doing something wrong. I'm still looking into how we can make this work, ideally I want to be using ReadClosers too.\n\nIdeally it would fall back to not sending the hash if it was just a plain io.Reader.\n\nUnfortunately checksumming the body is part of the version 4 signature protocol itself-- in other words, you must sign the body when using sigv4, so it's not possible to fall back. That's why a seekable reader is required here. That is, if the checksum is not already computed into the X-Amz-Content-Sha-256 header by another handler-- meaning you could pass a non-seekable if you pre-computed this header yourself, but it still must be computed somehow, and your code will run into the same problem on any non-rewindable stream.\n. Example of panic() on a seekable reader wrapped in ioutil.NopCloser that demonstrates the problem with typed assertions on wrapped types:\nhttp://play.golang.org/p/BrqRDAPMPb\n. Hey Nick,\nQuickly, just so we're on the same page with the intent of this SDK,\n1. We want to support bare io.Readers eventually. rclone's use case should continue to work,\n2. We want to follow best practices in the SDK, which means always trying to checksum bodies by default. Disabling it should be allowed, but users shouldn't need to jump through hoops (if there are hoops at all) to follow best practices,\n3. Ideally there are no hoops for both kinds of users.\nThe above reflects the direction we're going, not necessarily the code that exists today. There are some technical hurdles keeping us from getting there, I've responded inline with those:\n\nIf an io.ReadSeeker is required then you should declare that as the type. If not then you can use an io.Reader and upgrade it with a type assertion or type switch to an io.ReadSeeker to get one of those where it would be helpful, falling back to the plain io.Reader.\n\nThere's an implementation issue right now. The problem is that the logic to generate the type for the Body member has no context of whether it is being used in input or output. Input requires seeking (for now, see below), and Output requires closing, so the easiest way to get them both working is to just type io.Reader and type assert. Eventually we have to do some refactoring to add context to the generator so it knows what kind of type to generate. This will eventually solve the typing issue.\nThat said, we still want input to be a bare io.Reader on input, and I do want the SDK to be able to not do checksumming iff the user asks. There are a few notes here:\n1. The SDK we inherited currently only supports V4 signing for all services. V4 is a new signing protocol that is only recently being used with S3. Most of our SDKs have a special S3 signer and only upgrade to V4 in regions that require it (noting that many new regions like cn-north-1 and eu-central-1 require V4 signing, so it's not something you can just ignore). V4 requires checksums on bodies as part of the signing mechanism, which is not necessarily the case for the previous S3 signing protocol. We have to figure out if we will implement the older protocol for classic regions or if we can find another workaround. But remember the \"new region\" rule-- V4 signing will be needed for many buckets. My guess is rclone currently does not support buckets in these regions.\n2. We always want to be checksumming content when possible. This is a best practice. V4 gives this to us for free, whereas the S3 signer does not, but allows for the Content-MD5 header. Note that all official SDKs provide some kind of checksum by default, either the X-Amz-Content-Sha256 in V4 or Content-MD5, so we would always want a seekable stream in the ideal case. There is a discussion to be had about whether the SDK should fall back silently on not checksumming, which could create a lot of user confusion from people dealing with bit-rot when they expected the SDK to be providing that layer of validation. It feels very un-Go-like to implicitly and silently fall back. Providing a non-seekable stream should be a special case that the user is explicit about, so maybe we can make a wrapper for this scenario.\nNow some specific responses:\n\nThat seems a bit of an oversight to me - ruling out streaming like the previous signing protocols.\n\nTrue streaming was never possible with S3, even without a checksum, because S3 requires a \"Content-Length\" be provided regardless of signing protocol. You can \"stream\" in a PutObject call, but you still need to know how much data you are putting to S3. Although you technically don't need to seek the stream to know this, the technical limitation is similar. \n\nIn the specific case we are talking about here, object upload, would it be possible to break the stream up into chunks which can be buffered in memory, signed then uploaded?\n\nYep, this is called a \"multipart upload\" and is already supported by the S3 API (CreateMultipartUpload, UploadPart x N, CompleteMultipartUpload). This is actually the ideal mechanism for uploading files, since you get better failure recovery (you only have to retry individual parts) and you can much better performance by uploading in parallel. Not sure if goamz does this silently for you, but this is how you should be managing uploads, ideally. Eventually the SDK will have a \"managed upload\" abstraction to help do this for you (we have such an abstraction in most of our other SDKs), but for now we're focusing on the 1:1 mappings of S3's core API.\n\nMy choices would be ... Any other ideas?\n\nMultipart uploads. This allows you to emulate streaming by sending the file in a minimum of 5mb chunks (the last part can be of any size assuming you have at least 2 parts in the total upload). It's also compatible with V4 signing (assuming you buffer each chunk into memory using some seekable buffer), which alleviates the concern for new regions like eu-central-1.\nDoes that help?\n. Develop was merged into master today. There have been changes to handling of signing that may make this issue invalid, but I'm not sure. Let me know if the original issue here is still something that needs to be looked at.\n. Closing this since the panics originally reported should be resolved and we're settling on the use of seekable readers for now.\n. I'm actually looking at this right now. There's a little more to it than using RequestURI, because the Go escaping doesn't really cover the AWS specifics of query escaping (there are a few differences). aws-go was using aws.EscapePath for this (see aws/rest.go). Also, escaping in the path needs to be done in the REST param builder, not the signer. I'll have something soon but this will be in the develop branch, not master.\n. This is fixed in develop. Backporting this to master would be fairly difficult given the difference in REST handling (where this change actually was made). I am marking this as closed since develop is where the action is happening, I would recommend using S3 off of that branch for now.\n. Closing in favor of #100. This is a limitation of the current EC2 implementation, which does not yet fully support the full protocol. Follow along in the related ticket for progress updates on EC2 support.\n. Thanks for the patch! Unfortunately changing the model will not work because the bug will be re-introduced when we update the models. The model itself is correct here, the problem is in the generator, which should be trying to parse unix timestamps in json, since that's the protocol contract. I have a fix for this coming.\n. Note that in the future development happens in the develop branch, we make few changes in master and aren't accepting PRs to master currently (except for serious issues).\n. @GeneticGenesis thanks for bringing this up! Hopefully I can try and explain why I believe the current behavior is the right one:\nA common pattern that we follow for most of our SDKs is to have a \"low-level\" layer where we map operations in a 1:1 fashion against the API. In this mapping, we try to keep semantic decisions about the API as minimal as possible and simply expose the API surface as the service (in this case DynamoDB) wants to expose it. This works because it keeps us from making interpretations about how the API should work on behalf of the service itself (we work closely with service teams, but they still drive their own roadmap). For example, consider what would happen if DynamoDB started returning diagnostic information along with Query responses. That actually already happens somewhat today, here's an empty response from the JS SDK, note the Count and ScannedCount properties:\njs\n{ Count: 0, Items: [], ScannedCount: 0 }\nThis data would be lost if we returned a single error object. Basically, if the response is a valid response from the service's eyes, it should also be a valid response from the SDK's perspective. Only if the service returns an error should the SDK return an error in this low-level layer. This isn't a question of consistency with other SDKs, but rather, consistency with the API itself. Avoiding these decisions at the low-level allows us to isolate from changes in the underlying API-- it also makes things less confusing for users, because the canonical API documentation (see DynamoDB's Query operation docs) represents the same world-view that the SDK has of the service.\nNow, we're currently in the process of building out this low-level layer, making sure that it's solid before we start building on top. If an API like you are suggesting were to exist, I think it would belong in a higher level, not the low-level API mapping. We will eventually have a higher level API for certain services-- I believe that's where this discussion should be had. That said, our focus on the low-level layer leaves room open for developers to build rich abstractions like you are describing as third-party libraries, and third-party libs might be the right solution in cases like these when there are \"Many Ways To Do It\".\nHope that helps explain things a bit!\n. As per the discussion in #81 I think it might be dangerous to silently buffer into memory when a Reader is not also Seekable. I would rather see an explicit wrapper like ioutil.NopCloser that converts a non-seekable Reader into something that responds to Seek() (and does nothing) in order to meet the protocol. That way, if you really want to pass something in that is not seekable, you would do:\ngo\nBody: aws.NonSeekable(reader)\nOtherwise it seems like it might be too easy for a user to unknowingly pass Body: reader of an arbitrary reader without realizing the memory implications.\nOther than that, I think this is a good change.\nThoughts?\n. > I feel having to know about a helper method to use a reader is slightly less usable.\nI'm not sure usability would really be affected, since it would be very similar to the already-accepted practice of wrapping readers in ioutil.NopCloser(reader) for those methods that require it (like in the HTTP client itself). Concretely, it would look something like this:\ngo\ns3.PutObject(&s3.PutObjectInput{Body: awsutil.NopSeeker(gzippedFile), ...});\nInstead of:\ngo\ns3.PutObject(&s3.PutObjectInput{Body: gzippedFile, ...});\nThat doesn't seem any less usable, just more explicit, and as far as I understand, explicit seems to be the preference.\n\nI think if someone's uploading large objects that aren't seekable (e.g. reading a file through gzip) they should be using multipart uploads anyways \n\nIdeally, yes, but realistically not all users are aware of this best practice. IMO this is why the io.ReadSeeker declaration is important, because it is documentative of the expected interface.\n\nand providing an easier path to using those transparently will go a lot of the way to assuage your concern.\n\nSure, but work on managed upload support is not yet planned. By the same token, the managed upload component, when built, can be the thing that provides said \"easier path\" to transparent usage, whereby the low-level interface can maintain its clear expectation.\n. @GeneticGenesis the work in the develop branch is exposing a \"handler architecture\" similar to our other SDKs (JS and Ruby) that allow you to easily drop the HTTP client out and replace it with a mock response. Although you could mock the http.Client out today, it might be easier to just mock the entire pipeline so you don't have to deal with wire protocols. Here's an example to do the mocking with a helper:\n``` go\n// create mocked service object\ndb := dynamodb.New(nil)\ndb.Handlers.Clear()\ndb.Handlers.Send.PushBack(func(r aws.Request) {\n    // always return a specific response to every request\n    data := r.Data.(dynamodb.ListTablesOutput)\n    data.TableNames = []string{\"hello\", \"world\"}\n})\n// later in test\nr, _ := db.ListTables(nil)\nfmt.Printf(\"%+v\\n\", r)\n```\nWill print:\n&{LastEvaluatedTableName:<nil> TableNames:[hello world]}\nThe exact API is still in flux, but mocking should be feasible all with the direct Go data structures so you maintain the same type safety in your tests as well.\n. @GeneticGenesis you can assert the operation and params by checking req.Operation and req.Params respectively. \nI wouldn't call this poor encapsulation, the extensibility is a feature specifically allowing you to perform these use cases. \n. @sclasen as mentioned above, you can write a handler to intercept the input object already-- any of the handlers can do this. The code listed above shows a concrete example for returning a mocked response, and you could do the same for input by setting req.Params (though this would have little use in a unit test).\n. Precisely. \n. @GeneticGenesis the handlers architecture should work as a superset of #79. Can you provide an example for what the interface enables that the handlers do not?\n. >  IMO it would be sensible for this library to allow people to continue to do that.\naws-sdk-go should already be allowing users to continue using a library like gomock if they want. AFAIK, that library should work just fine with the API as is. In other words, you should be able to define the interfaces from #79 in your own test code:\n``` go\npackage mypkg_test\ntype AutoScalingAPI interface {\n    AttachInstances(req *autoscaling.AttachInstancesQuery) (err error)\n    // and anything else you might be using\n}\n```\nEven gomock's own documentation lists step 1 under the assumption that the interface is not yet defined for the thing you are trying to mock-- my guess is that this is likely the case for many libraries out there-- so it seems fairly reasonable to define a custom interface for the few methods of the API that you are using.\nI also think it seems fair to keep this declaration in your tests only, since this interface would only ever be useful to test code specifically using one of these testing libraries. I'm not sure there is value to having an extra interface in the documentation for every service when it is only useful for very specific unit testing scenarios. If we supported like #79, I would vote that we should probably at least be placing those interfaces in a separate package to simplify that documentation story.\nIn that same vein, another solution would be to not actually generate the interfaces but allow those interface files to be generated on-demand using the same codebase as our generators and document supported instructions for that testing strategy when using something like gomock (using a command workflow similar to gomock). This, again, wouldn't be a necessary workflow path, it would simply be a convenience generator for doing the above en-masse in the cases where you are testing with a large body of API calls from a single service.\nDoes any of this sound reasonable?\n. Note that the handlers are attached to the service object in this case, so you could create multiple r53 objects to separate out handlers. You can also attach handlers directly to a request.Request object (using the *Request() service method form) in which case the callbacks would only execute for that specific operation:\n``` go\nr53 := route53.New(session.New(), nil)\nr53.Handlers.Clear()\nr53.Handlers.Build.PushBack(func(r *request.Request) {\n    log.Printf(\"first: %T\\n\", r.Params)\n})\nreq, _ := r53.ListResourceRecordSetsRequest(&route53.ListResourceRecordSetsInput{\n    HostedZoneId: aws.String(\"abc\"),\n})\nreq.Handlers.Build.PushBack(func(r *request.Request) {\n    log.Printf(\"second: %T\\n\", r.Params)\n})\n// Callback executes on this operation\nreq.Send()\n// But not on this one\nr53.ChangeResourceRecordSets(&route53.ChangeResourceRecordSetsInput{\n    HostedZoneId: aws.String(\"abc\"),\n    ChangeBatch:  &route53.ChangeBatch{},\n})\n``\n. @shangsunsetreq.Presign()doesn't make any remote requests, it's all local computation, so is there a strong reason to need that method mocked?. This should be fixed by #91\n. Fixed by above commit, thanks for reporting!\n. Thanks!\n. Can you explain a little about the purpose of this change? Was something not working? What use case does this enable?\n. This should be resolved by some of the internal refactors in develop. Most notably, endpoints has been moved to the internal/endpoints package, and all generators have been updated accordingly. For that reason, this commit is likely stale and seems to be resolved. I'm going to mark this as closed, but thanks for bringing up the notes about the incorrect package name. If you still think there are improvements, please open a new pull request!\n. Hey @aranhoide, in addition to the change to generated code, this patch is also built against the master branch. Currently we are doing development indevelop` and are not merging PRs to master, since a lot of that code will be changing.\nI would recommend looking at the develop branch and making changes outside of the generated files-- this either means modifying the protocol implementation or the generator itself. If you have more questions I'd be glad to answer, but I will mark this as closed for the above reasons. Thanks for opening this up, though!\n. >  I'm unable to authenticate with DynamoDB (I've actually only tried DynamoDB Local):\nAmazon DynamoDB itself works with the signing mechanism provided (we have integration tests for this case). My guess is DynamoDB Local does not (yet) support query string signing in V4, so I would recommend opening a thread on the DynamoDB Forums to point this out.\n\nMy understanding of AWS auth is limited, but it seems that the pre-signing logic should only apply to S3, and other services should revert to using the Authentication header.\n\nSignature version 4 supports signing in the query string, also known as \"GET request signing\". The following is the official documentation, which explains how to build these signatures:\nhttp://docs.aws.amazon.com/general/latest/gr/sigv4_signing.html\nNote that the example uses IAM, not S3.\n. I wouldn't recommend overriding v4.go, as that would require a fork. You can do this without forking the SDK, simply register your own signer:\n``` go\ndb := dynamodb.New(nil)\ndb.Handlers.Sign.Clear()\ndb.Handlers.Sign.PushBack(myv4signer.SignWithHeader)\n// make request\nr, e := db.ListTables(nil)\n// ...\n```\nWhere mysigner.SignWithHeader is your own v4 implementation that either constructs the signature yourself, or uses the existing v4.Sign method and simply rewrites the query string params back down to their respective headers. The latter would be much simpler to implement.\nIf you end up just rewriting the query string params, you can even drop the db.Handlers.Sign.Clear() and just piggy-back off of the existing v4 implementation (the request would have been built up by the previous handler) instead of calling that code yourself:\ngo\ndb := dynamodb.New(nil)\ndb.Handlers.Sign.PushBack(myv4signer.RewriteSignedURLToHeader)\n. I think if we were to support it within the library it would be part of v4.go (or the v4 package at least) for performance reasons. The suggestions to override / add a new handler were to get a user-land workaround going for your code, not necessarily the best implementation for the SDK itself.\nThat said, we are looking into how to best sign requests, so it may be that we have some kind of switch for header/query string signing. I'll leave the PR open for now as we plan the best way forward, but I would recommend using your plugin as a separate package in your own usage for now, rather than trying to maintain a fork.\n. Closing this since we backed out the presigned URL strategy in develop and requests should now be signed in \"header mode\" unless req.Presign() is used. This should be resolved.\n. @bracki it should be resolved. Are you sure you've updated your dependency?\n. Thanks!\n. @schmatz time support in EC2 is not yet fully implemented. This is on the list of things to do. Closing this in favor of #100.\n. @ezbercih can you provide the code you used to reproduce this?\n. @ncw your fix was very close. Patch coming up.\nNote however that ListBuckets will currently not parse buckets from the response-- there is likely an unrelated issue in the response parser.\n. Yes, I mentioned this. There is likely an unrelated issue in the response parser that will eventually be resolved as we build up support for REST-XML services. This specific panic is resolved, though. \n. Thanks for the PR @cbroglie! After some investigation we pulled back on the strategy of generating presigned URL signatures, since this causes issues with other protocols as well. req.Presign() still works, but header signing is now the default mode and both strategies are supported in the signer. It would possibly be worth refactoring some of this out into separate implementations, but the current implementation obsoletes this change, so I will close. I'll take a look at this code if we ever loop around to do a refactor.\n. @schmatz what quick patches do you have? Right now the EC2 protocol is not wired up at all in the develop branch (the only branch we are accepting PRs), so there's a bit of work to do first. If you actually got something working in develop I'd be more than happy to take a look.\nAs for timeline, we don't have any public timelines just yet.\n. @schmatz sorry for not responding to this. We're actually moving away from the ...Value types, or at least experimenting with the concept (see #117 for more context), but we have made changes in develop to use time pointer types so that timestamps may be nil.\n. This is now working in develop (pending more integration testing), marking as closed.\n. We're moving away from *Value types (for now) because they don't add much value, namely you can't actually hang methods off of pointer types, and they obscure the actual type. That said, the develop branch does have an aws.Time() helper to return *time.Time objects for you based on this PR. Thanks for opening!\n. This report will no longer be valid once the develop branch is merged into master. I am going to mark this as closed since this report describes the old generator code. The new generator code in develop should generate the right types. Thanks for reporting this.\n. Closing in favor of #106 to track the underlying XML parsing issue.\n. @cbroglie I like the marshaler code that can be used to convert regular maps down to DynamoDB attributes, and I think that would work really well to alleviate some of the pain around passing in structures. \nI'm not as big a fan on the table/key abstractions. Typically whenever we've tried to make assumption about how a service works in the past we eventually find that those assumptions break as new features get added. For example, there will possibly be more configuration to creating a table than the throughput capacity, at some point, in DynamoDB-- any changes to this would require a breaking change to the signature on CreateTable(). \nOne thing we do have in other SDKs is a concept of \"resource mappings\", which allows us to auto-generate mappings from individual CRUD operations to resource objects in a consistent way across all services. That would get us to a point where we have some kind of \"Table\" abstraction that could be .Create'd in that syntactically nice way. I'd rather go down that route than hand-coding special logic just for DynamoDB, but we haven't yet figured out what the timeline is or if we will be going down that route at all for this SDK. \nYou can look at how resources are implemented in the Ruby SDK to get an idea of what could be implemented. If you wanted to jump on this that would be the way to go, I think.\n. @cbroglie yes :)\n. Closing in favor of #106\n. This is working in develop, marking as closed.\n. Thanks for reporting this. Using environment variables definitely seems like the right strategy. Admittedly, I was under the assumption that user.Current() would just defer to this type of environment variable lookup for the same cross-compatibility reasons-- certainly a little surprised to see that's not the case.\n. This is fixed in the develop branch. I will close this issue when we merge develop into master. Thanks for reporting the issue!\n. Thanks for the suggestion! We're still looking into how to best expose regions. It might indeed make sense to have a list of constant string values for better readability for those region names.\n. The user agent is required, but that header should not be signed, so GAE should be able to append values to it without affecting the request. Note that all development is currently happening in the develop branch, where the signer has changed, so this might not work in master but it will once develop is merged back in.\n. Develop was merged into master today, so I am marking this as closed since we should now be signing these requests correctly. Thanks for reporting! Feel free to re-open if you still run into this issue.\n. I'd like to close this in favor of #100, where we are tracking work on the EC2 protocol support. The SDK currently does not fully support the EC2 protocol, we're working on a refactor to resolve this. \nI will leave this open for a bit in case someone else has an idea for a patch that you can apply locally to get you unblocked, but note that we are not accepting pull requests on master right now (development is happening in the develop branch).\n. Develop was merged into master today, so I am marking this as closed since it should be resolved by the new implementation. Thanks for reporting! Feel free to re-open if you still run into this issue.\n. This should now be working in develop.\n. Unfortunately we're not currently accepting PRs against master. If you reapply this on develop I would accept this. \n. Thanks!\n. Thanks for opening an issue. We're still looking into the best behavior for this and the API may change.\nOne important thing to note that I'm not sure how other SDKs handle, is the distinction in AWS APIs between an \"unset value\"-- this is why we've been ensuring that all types are pointers, even for primitive values. For example, many AWS operations differ between providing an empty string or 0-value number and an actual value. If we didn't allow for explicit nil values, our marshalers would not be able to tell if you wanted to reset a field or set it with 0. How do other community implementations currently handle this? Maybe we can take some solutions there.\nI personally would love to support string literal initialization, but without solving this issue it would be difficult.\nFWIW you can use a function to alleviate most of the ugliness:\ngo\nid := Instance{ InstanceId: aws.String(\"i-123456\") }\n. @cbroglie\n\nThis won't help with integer types, but you can leverage the omitempty json struct tag to omit empty strings from serialization\n\nThe issue is that \"empty string\" in Go does not imply that it should not be sent over the wire. This also applies to any value coming back from a response-- getting a \"\" from a service often means something different from getting a null or omitted value, and some services can actually return with a literal null.\n\nSince DynamoDB doesn't allow empty strings, there is no need to disambiguate. \n\nNot now, but if this ever changed this SDK would be seriously broken, which is why we would want to avoid designing around this assumption.\n\nNot sure if other services need to allow for strings to be present but empty.\n\nThere are quite a few services out there in which an empty string is meaningful and different from an omitted value.\n. In many cases we share the same structure shapes on input and output to allow for roundtripping requests, so it wouldn't always be possible to have literals on input and pointers on output, unless we separated the shapes and disallowed roundtripping, which I think would reduce usability quite a bit.\nThe alternate option would be to have separate methods that accepted separate copies of the structures with non-pointer types when users wanted the convenience, but that would explode the API surface quite considerably, so I'm not a huge fan of that option.\n. For what it's worth, a lot of the ugliness around pointers and string literals have a lot to do with Go's current state of type inference rules. Theoretically, Go could infer the string literal as a pointer type in some future version, which would allow for the initially suggested syntax without any changes to this SDK. I'm not sure if type inference improvements like that are even being considered in Go, but if so, this could be solved without us having to do anything at all. I suppose that is worth noting.\n. The develop to master merge makes this change obsolete, but thank you for taking the time to open this issue. Feel free to re-open if the underlying issue was not resolved, but note that the implementation will likely have to change since a lot of the internals have.\n. Thank you for reporting. This issue should be fixed in the develop branch (where we are currently doing development work) and will eventually be merged into the master branch. I will leave this open until we make the merge in case anyone else comes across this issue.\n. @tobstarr unfortunately we can't share ETAs but I can tell you we're hyper-focused on getting develop into a mergeable state. Stay tuned.\n. Develop was merged into master today, so I am marking this as closed. Thanks for reporting! \n. We're actually looking at moving away from StringValues based on other user feedback, but this would be an interesting use case to keep it around. Note that these types have already been removed out of develop.\nA better solution might be to provide some external util method that pretty prints response structures so you can debug more easily.\n. You can see this generated in develop already, but we are using regular string, int, float, and bool pointers. The feedback we got was that the IntValue, BoolValue, and the like distract from the actual type and obfuscate the intent that they are just regular strings, ints, bools, etc.\nWe're still not settled on the solution, so we may go back to Value types. There seems to be some pros and cons for both arguments, but right now for simplicity we're just generating regular pointers. Feel free to weigh in on what might be best from your perspective!\n. FWIW I added a awsutil.StringValue() helper in the paginators branch for a similar use case. It's possible that we might just pull this into develop/master as we go forward.\n. awsutil.StringValue() is now in master.\n. I'm closing this since awsutil.StringValue allows inspection of these objects in a pretty reliable way. We can certainly find a way to make this more convenient, but I think that should unblock the immediate problem. Thanks for bringing this up!\n. Checking out the branch is sufficient to use it, you should not need to run the Makefile or build anything, but note that the services folder is (almost) empty because of an ongoing refactor that updates protocol support for all services. Generating services right now won't necessarily work due to the refactor. Only DynamoDB is fully supported in develop right now.\n. Closing since this is answered. Note that most services are back in develop now, and you shouldn't need to build anything yourself anymore (except for a handful of yet-unsupported-services).\n. @codemac you're not missing anything, this is likely a bug in the signer. Note that the develop branch has updated signing logic that will eventually be merged in and will likely address this issue.\n. I can confirm that this is signing correctly in develop.\n. Develop was merged into master today, so I am marking this as closed. Thanks for reporting! \n. @pwaller ContentLength will be calculated automatically by the SDK when it can (there is code for this in develop), but note that it is a required field and cannot be omitted by the SDK (whether it's automatically filled or not).\nOne trick you can use to get around this restriction is to use multipart uploads, which allows you to send an object as multiple arbitrary sized parts (though you must know the size of each individual part). You can see the multipart upload overview documentation for more information on this technique. \nNote that a multipart upload has to be at minimum 5mb, so you would have to buffer that much into memory and determine if you can use this technique before actually initiating the request.\n. Endpoints are already configurable in the develop branch. Once that gets merged into master you should have more luck. There's not much else we need to do to make things compatible with Eucalyptus, AFAIK.\n. @fsouza develop was merged into master today! I am marking this as closed since this is supported.\n. Perhaps another strategy would be to have a configuration option to deserialize zero values as empty values so that strings and the like are never nil in responses. Would that work?\n. @pges can you illustrate how this would work with a code sample? I'm having trouble understanding how a user would construct a request using the above code in a way that would support nilable values (and in a way that could be applied generically across services). What would the API look like to the user? I couldn't find any end-to-end usage examples in your bitbucket repo.\n. Thank you for reporting. This issue should be fixed in the develop branch (where we are currently doing development work) and will eventually be merged into the master branch. I will leave this open until we make the merge in case anyone else comes across this issue.\n. Develop was merged into master today, so I am marking this as closed since it should be resolved by the new implementation. Thanks for reporting! Feel free to re-open if you still run into this issue.\n. Thank you for reporting. This issue should be fixed in the develop branch (where we are currently doing development work) and will eventually be merged into the master branch. I will leave this open until we make the merge in case anyone else comes across this issue.\n. Develop was merged into master today, so I am marking this as closed since it should be resolved by the new implementation. Thanks for reporting! Feel free to re-open if you still run into this issue.\n. This is right for now. The builtin JSON marshaller will eventually be replaced with custom code to more consistently follow the locationName and other traits that we have setup for other protocols. Until then, updating the json tag seems fine.\n. Thanks!\n. The AWS wire protocol (with all of its potential quirks) should not be a concern for the default JSON marshaler. In other words, the fact that clusterName is lowercased on the wire in AWS requests should not be important to a user trying to marshal a Cluster struct to JSON outside of AWS. They probably want \"ClusterName\", as it appears in the Go struct. If anything, tying those JSON tags directly to the struct makes things worse for interop, because the SDK is now enforcing that same AWS wire protocol whenever a user wants to serialize the object as a structure to any other source. There wire protocol should be an implementation detail that isn't visible outside of the SDK.\nFor a concrete example, consider the REST-JSON protocol. In this protocol, structure members that map to headers must be omitted from a serialized JSON body (because they are part of headers, not the body), and as such, the \"Go way\" to do it is to add the json:\"-\" tag to this field. This works out great, allowing us to omit this field from body serialization (after we have read/written it from/into a header). It might look like this:\ngo\ntype Email struct {\n    Sender string `json:\"-\"` // because this comes from the \"email\" header in req/responses\n    // ...\n}\nThe only problem with this is that now a user cannot take that structure and serialize it in their JSON logs or to disk, or any other source, because this important exported field that is part of the API has now been marked as \"ignored\" from all JSON serialization by the SDK:\ngo\nb, err := json.Marshal(resp)\nfmt.Println(string(b)) // Prints \"{}\" even when \"Email\" is set.\nPlayground link\nThis is problematic for any kind of data use of these structs outside the context of the SDK, so in order not to break the above use case, we're trying to stay away from tying the wire protocol to Go's marshalers.\n. @euank just a heads up here: the change in this pull request has effectively been reverted due to the discussion above (by the above commit). If you're relying on these tags being present in some custom logic, note that the logic will no longer be valid.\nWe should have a discussion about your use case so we can figure out what if anything we can do to support the customizations you need. Or maybe there are alternative routes that would allow us to avoid changes in the generator codebase. Shoot me an email and let's chat.\n. Thank you for reporting. This issue should be fixed in the develop branch (where we are currently doing development work) and will eventually be merged into the master branch. I will leave this open until we make the merge in case anyone else comes across this issue.\n. Develop was merged into master today, so I am marking this as closed. Thanks for reporting!\n. @mbrevoortgit there is actually a PR from #130 that fixes the way the ECS client is generated but the actual client code has not been regenerated yet.\n. Fixed in develop with the above commit, thanks for reporting!\n. It looks like there's (unnecessary) churn on the service.go files that we should probably resolve. At the very least, we don't need to regen service.go files.\nAlso, now that I see the diff, we should probably only generate the JSON traits on JSON protocols. No need for the churn on XML protocols.\n. I was actually going to be jumping on this, so I'll take a look.\n. The above commit re-generates. Looks like your code already only generates for JSON protocols, so that wasn't an issue, but I added some code to sort imports so there won't be as much churn in the future.\n. I'm not sure this would be the right way to handle inflections. Why not just write a small script to append custom inflections to inflections.csv and then revert those changes after the build completes (git checkout would do the trick)? That would accomplish the same task without needing custom logic embedded in the SDK.\n. > I'd also be willing to take another approach like an extra -flag that adds inflection files to merge if you're just concerned about this implementation.\nThe concern here isn't only the specific implementation. The concern is adding and maintaining code to the SDK that is never actually exercised by the SDK itself. We would never use a flag to specify custom inflections on our end, and given that the inflection code is entirely part of a private (internal) API, there's no way we can guarantee that this behavior won't be refactored away in the future, either intentionally or unintentionally. Basically, it's going to be ugly either way as long as there are private APIs involved. That said, it doesn't have to be that bad:\nin file.go:\n//go:generate make custom-api\nin your Makefile:\n```\nSDKPATH=${GOPATH}/src/github.com/awslabs/aws-sdk-go\nINFLECTFILE=internal/model/api/inflections.csv\nCUSTOMINFLECTFILE=path/to/custom-inflections.csv\ncustom-api:\n    cat ${CUSTOMINFLECTFILE} >> ${SDKPATH}/${INFLECTFILE}\n    go run ${SDKPATH}/model/cli/gen-api/main.go api/*.json\n    cd ${SDKPATH}\n    git checkout -- ${INFLECTFILE}\n```\nIn other words, the suggestion is not to make changes in your vendored version, or even maintain a fork, but simply create a task that modifies this file at build-time and then resets it. That should work for now and until we have a public API for those generators, that would be the best approach.\nAs a sidenote, you shouldn't have to go build and call the binary. Just call go run; we do it this way.\n. Thanks for reporting. The above commit fixes this in develop.\n. @awinstan service files are only regenerated when -force is used in the code generator. Or at least they should be-- it looks like that's not hooked up correctly, I'll go ahead and fix that. Basically we intend to apply many other customizations to S3 and other services by hooking into service.go-- the initial file is simply boilerplate for future customizations.\nHope that explains things.\n. Thanks!\n. Thanks!\n. The SDK will be adding ContentLength when possible (i.e., for seekable streams), but it won't always be possible, in which case ContentLength can be provided manually. Note that you currently can provide a ContentLength to PutObject, which should allow S3 to work currently.\n. The above commit should improve the logic for the SDK building Content-Length on seekable reader objects. Non-seekable streams will still require a Content-Length to be provided, but that will actually cause an error to be thrown prior to sending, so the Transfer-Encoding behavior should not occur in develop (and in master, once the branch is merged).\n. Related to #139\n. @pwaller that's an old commit. The code above should work on tip of the branch except for the int64 ContentLength issue-- ContentLength is currently modeled as an int32, which is a known issue. Note that you can omit the ContentLength parameter, since the SDK will fill it in automatically.\n. Related to #138\n. Thanks for the PR! Unfortunately we're not accepting pull requests against master at this time since it will eventually go away when we merge in the develop branch. We've already refactored the endpoints logic to use an updated file format, so this wouldn't apply cleanly to the new codebase.\n. Can you explain what you mean by double-overriding? You can provide a custom endpoint directly in develop by providing an Endpoint parameter to the service.New() call.\n. I just tried this in develop and it seems to sign properly. In other words, this will be resolved once develop is merged into master.\n. Develop was merged into master today, so I am marking this as closed. Thanks for reporting!\n. The NonSeekable work has not yet been started. This is something we're going to start looking at once internals are refactored. We will also be able to look at other improvements for S3 that will make code more performant in these cases.\nIt's likely you will still need to pass an io.ReadSeeker at the low-level API call, but there will be a wrapper for this, likely something more akin to a ReadSeekCloser. This is because we will probably not be implementing chunked upload immediately if we can avoid it. Chunked uploading adds a significant amount of complexity that in practice is not often necessary (see below for why not).\n\nSince I have the Content-Length, it seems like I should be able to decide between PutObject and MultipartUpload based on size \n\nYes. You should actually be making this decision at the 5MB mark, since this is the minimum allowed size for a multipart upload. This is a feature we typically add on top of the core API to our SDKs known as the S3 transfer manager, or upload manager. We plan on adding this to the Go SDK as well. Once this functionality is added, the SDK will be able to accept non-seekable streams for this operation and decide based on the buffered input length whether a multi-part upload is possible (i.e., if the payload is >5MB). \nMultipart uploads are typically always preferred over simple PutObjects for a few reasons:\n1. Each individual part can be retried and recovered independently, so a bad connection 75% way through your upload won't cause a retry of all 75% of the file, just of the current part.\n2. More importantly, each individual part can be sent in parallel (using a pool) which allows you to utilize full throughput and drastically improves upload performance in our tests across SDKs.\nTo go back to the chunked upload point: once you're using multipart uploads, you don't need to worry about chunked signing, since you're doing the chunking as part of the UploadPart() operation locally. Once you have an individual part to upload, you likely have that data in memory and can seek on it as desired.\nBasically, the plan going forward will be to implement a managed uploader. That should alleviate most of the issues around signing, seekable streams, and other issues while also making the operation much more robust and performant.\n. Also, if you want to help out and do end up writing something similar to what was described above, we'd be happy to look at it if you sent it back to us as a PR. You can look at some of the other SDKs for reference implementations. The Java and JS implementations are linked to above, but we have others, and I can track them down if you're interested.\n. > 5 MB buffers is much more than I would like. (5 MB * 200 concurrent uploads, where uploads can take a few minutes = 1 GB)\nBuffer sizes would be configurable as they are in other SDKs.\n\nI don't understand how this upload manager will make non-seekable streams possible? Unless the intention is to buffer 5 MB parts to do the V4 signing calculation? \n\nExactly. Note that buffering in this solution is not done for signing, but for the general problem of not knowing exactly how much data needs to be sent in the multipart upload (the \"content length\" problem), or if you're even able to send a multipart in the first place-- and of course, retries. You need to pick a buffer size and buffer into memory/disk either way if you want any of these benefits. The fact that you now have a complete buffered object just makes signing all that much easier. \nChunked signing could reduce memory overhead somewhat by streaming as you send the multiple parts, but you still need to do at least 5mb of buffering to determine if you are sending a single part or not, and you would lose the ability to retry, which is an extremely commonly requested feature, especially as your files get larger. If memory use is still a concern, an option to buffer to disk would still be a much more manageable implementation than chunked signing, and that could be supported by the SDK. With prevalence of SSD these days, the cost would not be terribly significant (your bottleneck would still be the network).\n\nTo receive the benefits of parallel uploads and recovery, I would need to write the request.Body to disk and then read it back again to get a ReadSeeker.\n\nYou could buffer into memory as well.\n. Develop was merged into master today and with it comes a few changes. One of the changes was an aws.ReadSeekCloser(io.Reader) operation which is similar to NopCloser() except it passes any operations through to the underlying type (Read, Seek, and Close) if the underlying type responds. This should allow non-seekable types to be passed in, but note that they won't sign correctly. There probably needs to be more investigation into how that should be handled (if we should even explicitly disallow this), but based on the conversation above, it seems as though all payload operations will require seeking going forward at the client level. If that decision is fair, I think this issue can be closed.\n. @nathany you can check out the s3/s3manager package that landed above for streaming upload support with bare io.Readers.\n. Overrides are no longer a thing in develop, and won't be in master once we merge. You can (will be able to) specify Endpoints directly by passing them to the config object of the service.New() call.\nThis is similar to #140 \n. As a sidenote, and unrelated to the actual report, you can avoid using httptest.Server altogether by relying on recommendations from #88, either the ones about using handlers or the suggestions regarding mocking libraries. I would recommend this, as this allows you to avoid dealing with the wire protocol when crafting mock responses for the SDK to manage. The wire protocol of the AWS APIs should not be a concern of your application when using the SDK, so mocking at a layer above the wire allows you to avoid these concerns.\n. Develop was merged into master today so I am marking this as closed. Endpoints can be specified manually with the Endpoint configuration option to a client's New() function. Thanks for reporting this.\n. There's likely an issue in the signing code that is incorrectly attempting to generate the canonicalized query string. I'll take a look at this, thanks for reporting!\n. Thanks for the patch, looks like that resolves it! Thanks also for following up and reporting with Go, that documentation is indeed why we got it wrong here.\nThis can be closed but I'll reference #46 to improve test coverage and handle this case in signer tests.\n. @xstevens thanks for reporting this. The API will eventually support body payloads. Note that develop is still under development and S3 is not yet fully supported in the branch.\n. The above commit should fix this issue and allow resp.Body to be pulled out of a GetObject() call:\n``` go\nfunc main() {\n    c := s3.New(nil)\n    resp, err := c.GetObject(&s3.GetObjectInput{\n        Bucket: aws.String(\"bucket\"),\n        Key:    aws.String(\"key\"),\n    })\n    if err != nil {\n        panic(err)\n    }\ndefer resp.Body.Close()\nb, _ := ioutil.ReadAll(resp.Body)\n// ...\n\n}\n```\n. @rcway the develop branch was merged into master today which slightly changes the SDK API. This worked for me with the latest codebase:\n``` go\npackage main\nimport (\n    \"fmt\"\n\"github.com/awslabs/aws-sdk-go/aws\"\n\"github.com/awslabs/aws-sdk-go/service/sqs\"\n\n)\nfunc main() {\n    svc := sqs.New(&aws.Config{Region: \"us-east-1\"})\nqueues, err := svc.ListQueues(nil)\nif err != nil {\n    panic(err)\n}\n\nurl := queues.QueueURLs[0]\nresp, err := svc.SendMessage(&sqs.SendMessageInput{\n    QueueURL:    url,\n    MessageBody: aws.String(\"body\"),\n})\nif err != nil {\n    panic(err)\n}\n\nfmt.Println(\"MD5 of body:\", *resp.MD5OfMessageBody)\n\n}\n```\nLet me know if this can be closed / if answers your question.\n. The sample provided is the correct way to send this request. \n. @ando-masaki generally you don't need to provide a ContentLength manually as the SDK will generate this for you. This is only needed if you want to override the default behavior or if the SDK cannot determine the length itself. I am going to mark this as closed since you got it working, but we will be making improvements to the PutObject parameters in the near future. Thanks for reporting!\n. Thanks!\n. Good catch, thanks!\n. Thanks for reporting. I'm looking into this now.\n. The default body was likely a side-effect of some of the internal refactors / unit testing being done. Doesn't look like any tests rely on this behavior anymore, so it can be removed. I'll add a presign test to the S3 integrations to make sure this doesn't regress.\n. Thanks for reporting. Turns out we can keep setting an empty body (as that makes things easier for parsing) but presigned URLs should continue to use UNSIGNED-PAYLOAD. Simply using a nil body was actually incorrect for presigned GETs.\n. @kshinn our plan is to always return the API response as provided by the service at the low-level clients. This provides a more consistent interface that reflects all AWS documentation (including our own, which is generated from service documentation) and won't break if the underlying API makes subtle adjustment to their own return values. I would recommend opening a thread on the Amazon S3 forums to request feature support from the API itself.\nAs a sidenote, a lot of this complexity will go away (or at least hidden) once pagination support (#58) is added, since the SDK will be able to correctly manage this logic for you.\n. Thanks, I am too! :)\n. @pdalinis we've not yet thought about this, but I wouldn't be opposed to looking at a PR. One thing I would note is that this logic does not rely on any core functionality of the SDK itself (it's just a very simple raw HTTP request as you pointed to), so the only benefit of putting it in the core SDK package would be from a visibility standpoint. That said, we do have metadata service abstractions in other SDKs, so there is an argument to made to pull it in.\nWe're still looking at how we are going to handle plugins and third-party additions like this one. I'll mark this as a feature request., but for now I would recommend making use of that function as a workaround. \n. @jsdir you should collaborate with @pdalinis who may already be working on this or may even have code available to use. As mentioned I'm not opposed to adding this.\n. Thanks for reporting this. This actually came up in discussion just this morning. ContentMD5 is a required parameter for the DeleteObjects (and other) requests. The SDK should be filling this automatically, but we currently do not have these customizations implemented yet.\n. @bmatsuo \n\nFor the time being I altered restxml.Build()\n\nYou don't need to alter restxml.Build, you can add in your own handler to handle this:\n``` go\ns := s3.New(nil)\ns.Handlers.Build.PushBack(func(r *aws.Request) {\n  // add content MD5 to req.Params if r.Operation.Name == \"DeleteObjects\"\n})\ns.DeleteObjects(...)\n```\nThis is how we plan on implementing customizations in the SDK.\n\nFor example, it seems like the API spec documents (under directory apis/) are the right spot to put these requirements. Is there already enough information there to determine when the header needs to be added? It doesn't seem so to me.\n\nThe models in apis/ don't model any conditional logic for when to perform checksums, this is why we need customizations to our builders for scenarios like checksums, validation, etc.\n. @pdalinis the issue here is that we are generating the global endpoint but signing with the us-west-2 region. I'll take a look at this, thanks for reporting!\n. Thanks for reporting @pdalinis. The above commit should resolve incorrect credential scope errors when signing requests for services with global endpoints.\n. Having nilable config values would make configuration much more complicated than it should be. Realistically you should not be rewriting aws.DefaultConfig, though I can see how this could be a problem if you took an existing config object and ran Merge on it. \nI'd prefer to avoid pointer values for primitive types in configuration if there's a way that could be done.\n. @sclasen ExportableName is used to generate all shape and operation names, so if there is a problem there it is likely affecting other generated names and we should resolve that there. Note that ExportableName needs to be used in the load passes of the API, not during template generation, in order to generate the proper errors when inflections fail.\n. @sclasen only for those very specific examples, and it doesn't work in all cases. In order to guarantee consistency across all exported names going forward we need to explicitly whitelist all words (see inflections.csv) we will have to rely on ExportableName.\n. @sclasen basically the ExportableName function is made to be a more consistent replacement system. If it's not working correctly we need to fix that rather than abandoning it for enums only. The diff showing incorrectly inflected terms like \"VPN\" lead me to believe that it's not being setup properly, because these terms are being correctly inflected for all other operation / shape names.\n. @sclasen given that it works on other shapes, it's probably not setup correctly for enums. I mentioned above how to use the load passes to correctly setup enums while loading instead of while generating template output-- it doesn't look like this is being done. \n. Looks good, but we probably need to go through those inflections and make sure they are correct. I see a few that need some changes.\n. @sclasen you can explicitly call go run ./internal/model/cli/gen-api/main.go apis/rds/rds-YYYY-MM-DD.normal.json to call for that specific version.\n. @sclasen no, old schemas are supported and may be generated in the future under different packages.\n. >  is there a mechanism for using only the latest generator other than explicitly enumerating the go:generate calls in service/generate.go ?\n@sclasen this is not yet supported. As mentioned in the comments, the SDK should be generating with the latest API version, but there might be another (unrelated) generator bug here.\n. @sclasen looking through the generated inflections, I'm still seeing a lot of missing or incorrect entries. Ideally all country codes, for example, should be capitalized. We'll still need some time to look through the code in this PR and make sure it's generating the right things. \nI think there's also still a general question about how these enums will be exposed and what kind of possible bloat will they be providing. Some of this discussion may change how the generation is done. Right now, the generated constants have no association to the structure types they are being used inside of, which makes for a poor documentation story. One thing that could alleviate this would be to create special string alias types for these constants, which would give better visibility to where these constants are used. Another thing I noticed is that the type prefix on these constants occasionally makes for very long names, which could potentially be resolved by relying on the type alias for communicating part of that name. Finally, I'm concerned about some constants bloating the package, specifically the country codes in Route53 Domains, which seems a little superfluous to me. Having a list of 100+ constants in documentation might be distracting and not really provide that much value.\nAgain, I'm not opposed to constants, but I'd like to see if we can address some of these points before merging.\nIn the meantime, while we look at the inflections and think about how to improve visibility in documentation, you should be able to generate these constant names in a separate package in your own codebase (i.e. copy paste the string consts you use) and import that for now. That should unblock you until this gets merged in while still allowing you to switch over the rest of your code.\n. @dpiddy @sclasen we're still looking at this. I think the best way to move forward is to use the generated enums in an external (vendored) package so you are not blocked by this pull request. Specifically, given the simplicity of the enums themselves, you don't even need to rely on the internal logic (i.e. #201) to parse the JSON models and pull out enum structures into Go code.\n. For example, here's a playground link for parsing enums out of an abridged version of the autoscaling/2011-01-01.normal.json file: http://play.golang.org/p/JLHAHwOusH\nIt would be fairly simple to convert this into a generalized executable to write enums for various service packages.\n. The service should be functioning correctly. Let us know if you run into any issues!\n. Marking as closed since Elastic Transcoder is currently supported.\n. @richardbowden the SDK should support goroutines. We won't be providing many goroutine-specific primitives in the SDK, but it should be compatible with them, i.e., go run svc.DeleteBucket(...) should be a supported use case. Let us know if this is not working for you.\n. I'm looking into this. It looks like it's related to an incorrectly applied flattened trait (or lack thereof). Modifying the json file won't be a valid solution, as those models are auto-generated and would get wiped out in an update, but the info provided here is extremely useful, thanks!\n. Thanks for reporting. Looks like the query protocol was not correctly respecting the flattened trait on lists. This should now be corrected.\n. Added in 2bad910f78c1042c6c3cbe837e61ccc6738ecd40\n. You should typically not need to provide a content length on any stream, but I have a fix coming for this.\n. Thanks for reporting, this should work now!\n. @iyee MaxNumberOfMessages is a max value of items to return, it does not guarantee that that many messages will be returned. From the docs:\n\n\"The maximum number of messages to return. Amazon SQS never returns more messages than this value but may return fewer. \"\n\nAlso note that ReceiveMessage does not actually remove a message from a queue. You must explicitly delete the message yourself after successfully processing it.\n. @magegu can you provide what m.barcode is? My guess is that the signing error is related to the SDK incorrectly escaping special characters in the key.\n. @magegu You can set the Config.LogLevel to 1 to see debug output which should provide more information about why the signature is failing:\ngo\nm.s3 = s3.New(&aws.Config{LogLevel: 1})\nFeel free to censor anything sensitive out of your debug log.\n. This is likely due to the fact that the SDK is base64 encoding on your behalf. In other words, you do not (read: should not) need to encode the key.\nedit: Removing the base64 encoding worked for me.\n. The documentation is currently inaccurate. We're still working on improving the generated documentation  in the SDK, some things may not reflect reality. Generally, blob shapes should never require explicit base64 encoding or decoding.\n. Marking this as closed since the SDK behavior here is correct and we have tasks for doc improvements planned. Glad you got it working!\n. Thanks! You can use initRequest instead of initService to add the handlers only to specific operations like so: https://github.com/awslabs/aws-sdk-go/blob/master/service/sts/customizations.go#L6-L10\nIt's also a more efficient check (against the *Operation instead of by Name).\nAs a sidenote, here's the full list of operations that require ContentMD5: https://github.com/aws/aws-sdk-js/blob/master/lib/services/s3.js#L153-L157\n. >  it doesn't seem safe to handle a list.List directly\nDo you have any references on this assertion? Looking through the tests for container/list (https://golang.org/src/container/list/list_test.go), I see non-pointer lists being directly exercised-- see the last 3 tests specifically.\nIt's very possible we're doing the copy incorrectly, but I'm not sure we need to make them pointers.\nAs a sidenote, I recall running into that exact same error, but I forget how I solved it.\n. I'm not seeing anything wrong with the copy semantics, specifically the list doesn't seem broken by the copy implementation (the one type assertion removed):\nhttp://play.golang.org/p/MNVBTEGVY5\n. I see this now, @bmatsuo. I wonder if this is a bug in the Go List implementation or if this is expected behavior. Given that I don't see any literature explicitly warning of static use (in fact, the documentation implies that it explicitly supports this usage pattern: \"The zero value for List is an empty list ready to use.\"), plus the l.Len() is returning an incorrect size, I'm leaning towards \"bug\".\nThat said, it sounds like we will have to work around this bug (or not-bug) somehow. I'd prefer not to have to pay the abstraction penalty of initializing the list at runtime (that would leak into the HandlerList requiring a New which would leak into the Service/Requests constructing these explicitly too). The alternative is that we perform a bounded iteration that checks against Len() instead of just e != nil. Given that the lists are only iterated over inside of Run(), that should work for now.\nI'm going to go ahead and make this change but feel free to provide any thoughts on this. At the very least, that should unblock this PR for now and we can look at putting in place a more permanent solution if needed.\n. Actually, doing some extra testing, it looks like the  element is not the only problem-- items are being pushed in reverse order after the copy. I'll take a look at switching over to list pointers or possibly just changing the backing store completely over to slices.\n. @bmatsuo rebasing off of the above commit in master should resolve the list issue-- I've replace list usage with slices under the covers, which should provide more consistent behavior and also simplifies the interface.\nLet me know if things work with this change and I can merge this in.\n. Awesome work, @bmatsuo, thanks! :100: \n. @iyee thanks for reporting. I can reproduce this. Looks like we had a lack of testing on this specific scenario. I'll have a fix shortly.\n. The above commit should fix this, thanks again for reporting!\n. @geraldstanje yes, it is possible to upload applications and launch them with the Elastic Beanstalk API. You can read through the CreateApplication and CreateApplicationVersion API calls to see how to do this. Elastic Beanstalk will take your source bundle from S3, so you would use the S3 client in the SDK to upload the bundle to your bucket and then provide the URL to the Elastic Beanstalk API call. \nHope that helps!\n. @geraldstanje the predefined configuration field is a parameter in the API of CreateConfigurationTemplate (SolutionStackName). I don't know of any Go specific examples, but you should be able to take examples provided in other languages and repurpose them to Go. I would recommend looking at the developer guides or the forums for those examples.\n. Marking this as closed. Feel free to reopen if you have other questions, though I recommend visiting the Elastic Beanstalk developer guide and forums if you have questions specific to that service.\n. @cbroglie they are intentionally omitted. See here for an explanation why we don't rely on the native marshaling tags in our data structures to represent the AWS wire protocol.\n. FWIW our marshaler already behaves using omitempty by default, so it's not necessary to specify.\n. This looks related to #153.\n. This looks related to the way that XML response is structured. I'll try to take a look at this sometime this week. Thanks for reporting, @sjansen. \n. It looks like BinaryValue is being built when it should be ignored. Thanks for reporting this, I'll take a look.\n. Currently the best way to report issues is via GitHub. We're looking into potentially using IRC, but we've not set it up yet. I'll be sure to let you know if that changes!\n. The above commit should fix this. Note that you need to use \"String\" as the message attribute type, not \"string\" (lowercase). This parameter is case-sensitive in the service.\n. Can you provide debugging output to see if the DeleteMessageBatch operation was successful?\nIn general, this would not be a \"concurrency\" problem, as these requests are independent of one another. It's possible that there is an eventual consistency issue in the service, but this would be out of the SDK's control. The other possibility is that the SDK is simply not successfully issuing the request and then incorrectly reporting its result. A debug log would tell the full story.\n. > ReceiveMessage get same msgs multiple times.\n@iyee as mentioned in #174, ReceiveMessage will always continue to receive a message until it's been deleted from the queue, and marked as such by a successful 200 response from a Delete operation request (simply sending a request does not imply that it was necessarily deleted yet). My guess is you have a race condition between your producer and consumer goroutines, and this is something you will have to managed in your codebase. Or messages aren't being deleted... see below.\n\nI've looked into the logs myself, seems the delete action is fine, but SQS continues to produce more same msgs to consume.\n\nIf the SDK is receiving 200 OK responses then there is not much else the SDK can be doing here-- the service is responsible for doing the correct handling of messages once the request has been received. In other words, we cannot control how the service behaves, only how we construct the requests. If the requests are constructed and received correctly, the issue is not with the SDK but with the operational behavior of the service or application.\nThat said, I'm seeing quite a few 400 responses in your second set of logs. In fact, it seems as though all of your batch delete requests are failing, which would explain why you're still receiving the same messages. \nThe error message seems to look something like:\n\nAWS.SimpleQueueService.BatchEntryIdsNotDistinct: Id 96e07041-1681-464f-9bc7-1e4fddb57590 repeated.\n\nWhich implies to me that you are not constructing the correct request. You must use unique IDs when constructing the batch of items to delete. You cannot duplicate any of those IDs in the list or the request will be rejected.\nAre you properly handling the err response value from the delete operation? If so, can you confirm that the SDK is returning the above error? If it is not doing this, that would be a bug.\n. @iyee based on the following sample log data:\nI0407 18:47:43.917988   33239 worker.go:24] worker: Received 4 messages\nI0407 18:47:43.918022   33239 worker.go:39] worker: Spawned worker goroutine\nI0407 18:47:43.918028   33239 handler.go:21] msg_id:059e10d7-633a-421b-a927-0d660d6a9567, msg:Send with \u2665 through SQS5\nI0407 18:47:43.918072   33239 worker.go:39] worker: Spawned worker goroutine\nI0407 18:47:43.918091   33239 handler.go:21] msg_id:059e10d7-633a-421b-a927-0d660d6a9567, msg:Send with \u2665 through SQS5\nI0407 18:47:43.918154   33239 worker.go:39] worker: Spawned worker goroutine\nI0407 18:47:43.918163   33239 handler.go:21] msg_id:059e10d7-633a-421b-a927-0d660d6a9567, msg:Send with \u2665 through SQS5\nI0407 18:47:43.918199   33239 worker.go:39] worker: Spawned worker goroutine\nI0407 18:47:43.918204   33239 handler.go:21] msg_id:059e10d7-633a-421b-a927-0d660d6a9567, msg:Send with \u2665 through SQS5\nIt seems like you are processing the same msg multiple times instead of all 4, and then deleting only that single message of the 4.\nCan you provide the code from handler.go that actually loops over the messages to process them? I've verified that the SDK is correctly unmarshalling all the received messages from the ReceiveMessage request, and it looks like it's pulling all of them out, and in my tests they are unique messages.\n. \"aws-go\" is not the current user-agent being sent by the SDK in master. I'm not sure which version of the SDK Terraform is using-- I would recommend opening an issue against that repository instead.\n. Marking this as closed so as not to leave 2 dupe tickets open.\n. The SDK seems to be behaving correctly here. In order to get MessageAttributes back you must ask for them in the MessageAttributeNames parameter of the ReceiveMessageInput request structure. This worked for me:\ngo\nresp, err := s.ReceiveMessage(&sqs.ReceiveMessageInput{\n    QueueURL:              url,\n    MessageAttributeNames: []*string{aws.String(\"All\")},\n})\n. There is currently no support for CloudFront signed URLs in the SDK. I will mark this as a feature request.\n. @zshenker there aren't currently any plans for this but I've marked it as a feature request. Thanks for suggesting!\n. @zshenker what @hallas said. CloudFront URL signing actually doesn't need to rely on any part of the existing SDK, so you could write this as a completely independent component and use it in your own project for now.\nHere's an implementation in Node.js: https://github.com/jasonsims/aws-cloudfront-sign\n. The above commit should add in appropriate locks, though I wasn't able to actually reproduce any issues with your code without the mutexes given the way the assignment being strictly used for memoization. At the very least, this should squelch warnings from go run -race.\nThanks for reporting!\n. The above change only wraps a mutex around request object initialization, which is a fairly non-intensive set of memory allocations and property assignments. The mutex shouldn't be wrapping any I/O.\n. I ran some benchmarks on this and am not seeing any noticeable difference:\ngo\nfunc BenchmarkLocker(b *testing.B) {\n    s := ec2.New(&aws.Config{\n        Region:      \"mock-region\",\n        Credentials: aws.DetectCreds(\"AKID\", \"SECRET\", \"\"),\n    })\n    b.RunParallel(func(pb *testing.PB) {\n        for pb.Next() {\n            req, _ := s.DescribeInstancesRequest(nil)\n            req.Build()\n        }\n    })\n}\nOutput:\n~/aws/go/src/github.com/awslabs/aws-sdk-go ((2b2c925...))$ go test -bench=. lock_test.go\ntesting: warning: no tests to run\nPASS\nBenchmarkLocker   200000         12409 ns/op\nok      command-line-arguments  2.825s\n~/aws/go/src/github.com/awslabs/aws-sdk-go ((2b2c925...))$ git checkout master\nPrevious HEAD position was 2b2c925... Remove println\nSwitched to branch 'master'\nYour branch is up-to-date with 'origin/master'.\n~/aws/go/src/github.com/awslabs/aws-sdk-go (master)$ go test -bench=. lock_test.go\ntesting: warning: no tests to run\nPASS\nBenchmarkLocker   200000         12433 ns/op\nok      command-line-arguments  2.730s\nCommit 2b2c925 was the SHA prior to this patch. Similar results if I replace RunParallel with a WaitGroup and explicit goroutine calls.\n. @magegu thanks for reporting. This is likely not an issue with thread safety, but rather an issue with throttling in S3.\nTypically, if you send an extremely large volume of requests to any AWS service, you will eventually be throttled. Due to the way S3 functions (accepting large PUT or GET requests), S3 handles throttled requests slightly differently, by disconnecting you some time after the request has begun, usually with EOF or connection reset by peer errors. The EOF happens because, due to the way HTTP works (*); S3 has no opportunity to stop you from beginning to send the data payload once the connection is made, it can only break the connection in this way. Note that these errors could be caused by your own network connection being saturated and dropping packets anywhere before S3, but my guess is this is likely an issue with throttling.\nRegardless, the solution in both cases is the same-- an error like the above implies that you are sending too many requests for your connection (or S3) to handle, and you need to adjust your application to send fewer requests or queue fewer goroutines in parallel. You can either do this as a recovery step from the TCP errors, or you can start with a lower concurrency level so as to not run into these errors in the first place-- this is up to you.\n(*) HTTP does have a feature called \"Expect 100 continue\" which allows user-agents to wait for a server response before beginning to send a large payload, and S3 does support this, but this behavior is not built into the SDK yet. Once we support this functionality, the TCP errors would turn into actual throttling errors from S3's end and be more apparent in your logs. You would still be throttled, though, and would still need to start sending fewer requests to resolve this.\nHope that helps to explain!\n. Marking this as closed since this is likely an issue with throttling and not the SDK. Feel free to reopen if you believe there is something else going on here.\n. @magegu I tried for a while to reproduce this but was unable to narrow down a root cause. If you're able to reproduce this reliably it would help a bunch if you could also provide wire logs (using HTTP not HTTPS) of the failing requests so we could find out what headers S3 isn't liking.\n. @jen20 we have a couple of these open (#182) that we're still deciding between. We haven't made any decisions about which will be accepted, so if you do want to take a stab at it, feel free. If you can cross off all the stuff on our list (see #182 for details) with proper tests, that would be the easiest patch to pull in.\n. Thanks!\n. I can't seem to reproduce this on my machine. Can you pass &aws.Config{LogLevel:1} to cloudformation.New() and provide the debug output?\n. Are you using a proxy, by any chance?\nI ask because based on those logs it looks like the timestamp AWS is expecting in the signature is not the same timestamp provided by the X-Amz-Date header, which could be caused by a proxy or other program inserting a \"Date\" header into the request before it hits the service. If this is happening, you would have to configure your proxy not to send a Date header and override the timestamp.\n. Glad you got it working!\n. @julienvey passing an empty list should work now ([]*string{}). This was a bug in the serializer for the query protocols. Thanks for reporting!\n. @richarddbarnett first, huge props for diving into the code and getting a PR up! :+1: \nI'll have to take a closer look at the code before merging but good catch on the problem and thanks again for the fix. I'll come back if anything needs fixing up.\n. > Now I also get an error object and after doing aws.Error(err).Error() on it, an empty string is the result.\naws.Error(err) will only return an error object if it is an APIError, otherwise it will return nil. That does not mean there is no error object, though. If aws.Error() returns nil but err is not nil, you should print the result of err itself to see what non-AWS error was generated.\nCan you run your code with &aws.Config{LogLevel: 1} in your dynamodb.New() call so we can see the debug output of the request? That would help figure out what happened.\n. @sacheendra DynamoDB does support UTF-8. Our serializer was not correctly handling non-printable characters. I have a fix coming that should correct this, but I will leave this open to also track correct parsing of errors.\n. We're going to close this since there are no plans to expose the API externally in the short-term, especially given all the API flux that is going on right now. We can certainly bring this discussion up again later on when we are closer to stabilizing the SDK in a 1.0. At that point, we might be able to commit to a stable API API!\n. > but requiring a user to have the environment variable just changes where that region is dictated and makes tests less consistent between developers.\n@euank the tests should not be any less consistent across regions. If they are, seeing those failures would help us identify the poorly written tests.\n. @justinsb we don't yet directly support anonymous credentials, but you can work around this with the following code snippet:\ngo\nsvc := s3.New(nil)\nreq, resp := svc.GetObjectRequest(params) // assume params is GetObjectInput\nreq.Handlers.Sign.Clear() // make this request unsigned\nerr := req.Send()\nif err != nil {\n    panic(err)\n}\nfmt.Println(awsutil.StringValue(resp))\nHope that helps.\n. It's possible this is related to #196 and #204 where we aren't correctly seeking back to the beginning of a payload when retrying a request. If either of those patches work for you, that would narrow it down.\n. @mateusz if you can provide detailed logging by setting the LogLevel config option to 1, that would help.\nsvc := kinesis.New(&aws.Config{LogLevel: 1})\n. That looks like an error coming from Go's socket library code. It could indeed be a connection issue with the service. Are you using a proxy, by any chance?\n. > Do you know how to disable keep-alives on these aws sdk requests? Maybe force connection closure after every request just to see if that would fix it?\nYou can pass in your own http.Client as Config.HTTPClient and disable it that way.\nJust to clarify, are you using a proxy?\n. @mateusz have you looked into disabling KeepAlive? Did this help at all?\n. Thanks for the feedback @itsjamie. Can you give us a little more info on what alternative you would expect from the SDK?\n. > Not knowing what the types are used for internally, it just seemed awkward to me. I took a quick look and saw that Part had the necessary exposed fields for CompleteMultipartUpload, so I wasn't sure why a different type was used.\nThe types are different because they expose different properties. CompletedPart only allows for ETag and PartNumber, whereas the Part structure represents the parts list from a ListParts operation that contains more metadata about the parts. Although they share certain members, they are not the same structures. Converting would be a reasonable thing to do here.\nThat said, you can avoid the extra call to ListParts by doing your book-keeping internally, which would make your implementation more efficient (saving an extra API call); this would also avoid having to convert the slice.\n. > Seems like the best thing I will be able to do is make a convenience function that converts Part to CompletedPart if I find myself needing to do it a lot.\nThis would definitely be the most pragmatic approach.\n\nMy suggestion would be, since CompletedPart is entirely a subset of Part with a different field name for the unexposed metadata (prefixed with complete), I'd ask you to reconsider changing the API before freezing it to take Parts instead. \n\nMy only concern with this approach would be that the documentation story would likely lead to confusion if we were to merge these two types together. I could see users tripping up on thinking they need to provide Size information about parts when sending a CompleteMultipartUpload request-- the documentation in this scenario would certainly imply it was possible. Although these structures may seem similar, they are not quite the same.\n\nIt seems wasteful to me to force an external user to loop potentially ten thousand times essentially allocating a bunch of tiny structs'o'pointers that will need to be cleaned up later..\n\nFor an edge-case of 10,000 parts, the conversion of Parts -> CompletedParts would actually be the least of your concern-- given that ListParts can only list 1,000 items at a time, you would also be dealing with the overhead of 10 HTTP requests with extremely large payloads going over the wire and coming back that have to all be serialized and de-serialized. In other words, relying on ListParts in such a case already introduces a significant amount of overhead (both performance and memory) that could be avoided by doing the book-keeping locally and ensuring that they are stored as CompletedParts from the start. This is what you want to optimize for if you can. Incidentally, this is how the AWS SDKs for JavaScript, Ruby, and others implement, and I am actually working on a managed uploader abstraction in this SDK as we speak that does the same.\n. @jeffw-wherethebitsroam the SDK for Go uses signature version 4 signing for S3 everywhere. This signing version includes a SHA-256 checksum of all payloads to ensure data integrity. Because of this, we've removed ContentMD5 from the SDK because it is no longer necessary to provide extra integrity checking-- this step is already automatically enforced by default for all signed operations.\n. Thanks for the feedback. Marking this as closed since the behavior is working but we will be doing some extra work on improving documentation down the road that should improve the experience here.\n. @c4milo can you provide the code that you are using with the SDK that is causing this issue? I will say that if the service is accepting your request with a 200 successful response, it's unlikely that the SDK is the cause of any issues here-- this looks like a configuration issue with an EC2 instance itself. You can also verify that the SDK is sending the correct request by enabling logging (LogLevel: 1 in the service constructor).\n. @c4milo it looks like you are using the SDK correctly, but you may not be using the API correctly (i.e. sending the right values to EC2). I'm not entirely sure what your intended goal is, but this doesn't look like a problem with the SDK unless you can show wire logs of incorrect data being sent to the service.\n. I also like the idea of the triState pattern. Looks like we could use that more broadly in our config options too!\n. Thanks for bringing this up @cbroglie. You're right that this is a bug-- and is not intentional-- we actually caught this in a review the other day and are looking at it!\nWe're currently doing some refactoring on Credentials and retry logic (see #211) that need to land first, though.\n. Can you explain what you are trying to do? If you're looking to download a file, the operation you want is GetObject(), not CopyObject(). CopyObject is meant to copy objects inside of your bucket from one key to another, it does not pull contents down across the wire.\n. Glad you got it working!\n. Thanks for reporting. The above commit should resolve this issue.\n. We're actually in the process of doing a major refactor of the credentials module, so much of this codebase will be obsolete fairly soon. Stay tuned!\n. @luck02 to answer your question more directly:\n\nIs there any further documentation on how this is intended to work? Why is there a bunk IAM credential endpoint in there and what is the expected method of setting a functional endpoint?\n\nThis credential provider is actually there for a specific feature called IAM Roles for EC2 Instances which is a specific feature of EC2 to get credentials from IAM roles on your instance. The URL is not \"bunk\", it's actually a real address that is accessible from EC2 instances. If you are not on an EC2 instance, this operation will timeout, and that's likely the error you are seeing.\nYou will probably see more explanatory documentation as we improve this module (via the refactor and in the near future).\n. @sixbytestring \nI've run this a few times on EC2 instances and have had no issues. That example looks like correct usage-- but note that you should not need to specify credentials, the SDK should be able to detect the IAM role credentials automatically. Only if you need to set an explicit Timeout should you ever need to pass in credentials explicitly-- I assume that's what you're trying to do here.\n. @sixbytestring note that Docker might need to map the metadata host somehow? I've never tested this under Docker, so that might be related. Perhaps it's unable to route that endpoint.\n. @ohookins \n\nSince there is no body of the 404 response to the head request, awserr.Message is empty, but so is awserr.Code.\n\nThis is a bug likely related to #197. Code should not be nil in these situations, neither should Message. You should be relying on Code in these cases. You should not be checking StatusCode, it's only there for debugging support, and in fact, may be removed.\nHope that helps to explain a little bit!\n. @ohookins #197 is a issue tracking our work on ensuring all service operations return an error in the case of an error response. Currently the SDK does not reliably return errors for failed requests.\n. Thanks for the patch @oremj!\n. @chpapa thanks for opening this issue. This is expected behavior in the SDK and you will see this same error in the Ruby SDK, JS SDK, CLI, and others. You must ensure that your paths are properly URL encoded in the format that you want. Note that even the console UI does not allow spaces (a space delimits a new invalidation), and if you wanted to do this there you would also have to provide an encoded URL.\n. @chpapa the SDK maps operations directly to the API in a 1:1 fashion so as to be as efficient and consistent with the service operation as possible. As such, we don't focus on making semantic manipulations of your data. If you believe that CloudFront should handle spaces in a specific way, that's something that the API itself should be handling, in other words, it should escape the space on its end. If you think that this feature is useful, I would suggest opening a thread on the Amazon CloudFront forums to request this functionality.\nThat said, the other reason we would not want to escape on your behalf is that the paths passed into CloudFront are considered literal strings of paths, not URI components, and they are not interpreted in any way. In other words, hitting CloudFront with the path http://../foo would generate a different cache entry from http://../foo? (note the question-mark) and would have to be invalidated as separate entries, even though semantically they are equivalent URLs. If our SDK were to interpret your path, there is a chance we may interpret incorrectly. In your case, a space can be encoded as \"%20\" or as \"+\", and both of these paths (a+b vs a%20b) are considered different URLs in CloudFront and must be invalidated separately.\n\nSeems there are nothing compliance to the encode CF invalidation need in the standard library.\n\nYou can use something like url.ParseRequestURI or url.QueryEscape (playground link), but note that each interpretation may generate a different URL, and the one that you select to invalidate depends on how your application exposes these URLs. It's possible you may want to invalidate both formats.\nFYI I found this documentation from CloudFront, which echoes the idea that encoding the URL is a consumer responsibility (doc link):\n\nIf the path includes non-ASCII characters or unsafe characters as defined in RFC 1783 (http://www.ietf.org/rfc/rfc1738.txt), URL-encode those characters. Do not URL-encode any other characters in the path, or CloudFront will not invalidate the old version of the updated object.\n. @stephen-mw thanks for opening this discussion.\n\nThis is expected behavior in the SDK. We can do a better job of documenting this in the future, but you must have either a region or endpoint configured at minimum to use the SDK, even for operations that may enumerate other regions. The SDK will never default to a region on your behalf. If us-east-1 is a default that works for you, it's something you can configure in your application logic.\nHope that helps!\n. Thanks for opening this issue. For the time being, you can grab the Content-Range header from the HTTPResponse property of the request object:\ngo\nreq, _ := svc.GetObjectRequest(...)\nerr := req.Send()\nif err != nil {\n    panic(err)\n}\nrange := req.HTTPResponse.Header.Get(\"Content-Range\")\n. I'd personally prefer to see the deadlocking resolved without resorting to a separate error channel, as managing multiple channels introduces quite a bit of extra complexity. I'm fairly certain that something as simple as switching to a RWMutex should be able to resolve the deadlock.\n. @cespare if you could split the deadlock issue out into its own PR I can take the memory leak patch now. It looks like the deadlock part will need a little more investigation.\n. Thanks!\n. > My plan right now is to fork s3manager and make it retry each chunk a bunch of times,\nThe SDK already retries each UploadPart as a separate operation, in other words, using standard retry logic. There should be no reason to fork the SDK to implement this. If the SDK is not retrying parts, it's due to an error that is not currently marked as retryable, which is something we can correct depending on the error. For example, the SDK currently doesn't retry TCP level errors, but this is a known limitation, not expected behavior. We would accept pull requests to improve retry logic, but note that we are currently under a refactor of error handling, so things will likely change.\nYou can also implement your own retry logic by adding Retry event handlers (svc.Handlers.Retry).\n\nAny suggestions for making very large file uploads resilient to the flaky nature of S3?\n\nSeeing EOF / timeout errors usually implies that you are getting throttled by S3. If you want to improve resiliency, the best idea is to simply slow down. You can improve retry behavior, but you're still going to run into errors so long as you're getting throttled.\n. @cespare updated the title of this issue to reflect the retryability portion of this issue.\n. > It would be helpful if the error exposed to the programmer would indicate throttling, then. (I don't know whether this is an API or SDK limitation). It seems like my workaround is the best solution if throttling can manifest as TCP connections getting dropped.\nSee #190 (comment) for more information on why we can't quite provide that feedback yet. tl;dr we would need support for Expect: 100-continue in the HTTP layer, which is not quite there yet in Go.\n\nOr to put it another way: wouldn't you expect that one could upload a 50GB file to S3 using s3manager, using the default settings? (I was unable to do so in multiple tests.)\n\nI would. I haven't tried a file of quite that size, but I will. That said, the point is simply that if this were not possible, it would not be an issue with the s3manager component, but rather an issue with the underlying retry logic lower in the SDK. In other words, we should not need to change code in the s3manager to make this work.\n. awsutil.StringValue is a convenience method for pretty printing values. If you want the actual string value, you just need to dereference the pointer type. Hope that helps!\n. Marking this as closed since the API is returning the correct string in this case.\n. @anacrolix you don't want to be modifying HTTPRequest here, you should be using the HeadObject operation instead.\n. Marking this as closed since GetObject should only perform a GET.\n. @cespare I did quite a bit of stress testing on the current implementation and was unable to come up with a scenario that caused a deadlock. Reading through the code, I can't see a case that would cause a deadlock either. Can you explain if you have actually reproduced this issue, and if so, provide code that reproduces the issue?\n. > The network blips and all the readChunk workers get an error and break out of their loop on line 362\nI was able to generate a contrived scenario that caused the producer to fill up the buffered channel and block prior to the next set of consumer reads, thereby reproducing the issue. Basically the only scenario that could cause this type of issue would be a blackout type networking failure that killed all connections immediately. This is further exasperated by the fact that we're not yet retrying TCP errors (see #227)-- once we do that, this would be even less likely, though we should fix it.\nBased on that scenario, it seems like the trivial fix would be to simply not have the workers exit due to a send() failure, and instead continue consuming until the channel has closed. The producer can signal to the consumers to exit by simply closing the channel. This seems like what we should be doing regardless, since there should only be one condition to exit a worker. I can have a fix out for this shortly.\n. @cbroglie you're only looking at Put/Get operations. Consider the AttributeValueUpdate struct: https://github.com/awslabs/aws-sdk-go/blob/master/service/dynamodb/api.go#L877\n. @cespare I would be open to this implementation. This came up in discussion today actually. If we can cast to a io.Seeker we should be able to use a seeking strategy when assigning parts to be sent.\n. > After generating 10k parts, should s3manager immediately abort and report the failure to the user rather than trying to complete an invalid upload?\nA better idea would be to require the user to pass in an \"estimated size\" for arbitrary streams so we can scale the part size appropriately. The Ruby SDK does this, for example.\nFailing the upload at 10k parts would be unhelpful, as the user would have no way to complete the upload and they would get stuck with having to restart from scratch. If we can fail early, it would save quite a bit of time.\n\nIn the new proposed code path for Seekers, should we do the PartSize adjustment I did in my code? (A different option would be immediately reporting some \"part size too small\" error after discovering the size of the file.)\n\nYes. We do this in Ruby and JS. This is an outstanding known issue in the s3manager implementation (mostly because we don't yet support Seekable streams).\n. > Well, failing and aborting at that point would be more helpful than allowing the user to upload another 50k chunks\nYou are correct here-- we should check at the start in addition to failing at 10k.\n\nAnd this still might be a case we want to handle if the user doesn't provide an estimated size\n\nThe idea would be that this would be a required field if the size is not computable, so it would not be possible to omit this. \n. @anacrolix thanks for the feedback. Let me see if I can answer some of your questions and get some more information from you:\n\nHaving the separate objects for these makes things very difficult, in part because Go doesn't have a generic way to set those fields, 2 separate functions are required in the caller to assign all the fields.\n\nI am guessing that you are referring to constructing the s3.HeadObjectInput vs s3.GetObjectInput structures passed into each operation (correct me if I'm wrong). Can you provide an example that shows where this is difficult? It should be simple enough to construct either structure depending on what your use case is. Specifically, your application logic should usually know well in advance whether you are performing a HEAD or GET. If you can describe your use case and possibly provide code we can get a better sense of why this might be overly complex in your case.\n\nI'm currently working around this by setting GetObjectInput.HTTPRequest.Method, which fortunately works.\n\nYou should not workaround this behavior in this way, since the current behavior is expected. Messing with the underlying HTTP object is not a primary interface and is exposed indirectly for this reason. The proper usage should be to rely on HeadObject, as we have no plans to remove this operation, and it is supported in all of our new SDKs (Ruby, JS, PHP, Python), including the CLI.\n\nexposing Method to a more generic RetrieveObjectInput or dropping HeadObjectInput and retaining only GetObjectInput.\n\nThe SDK works by abstracting away the underlying HTTP layer-- exposing details of the HTTP request in a more direct way is not something we are planning on doing. \nAnother primary feature of the SDK design is that we expose each public API operation in every service in a 1:1 mapping. In other words, for every publicly documented API operation in S3's documentation, our SDKs expose an equivalent operation for that specific method. At this low-level interface, we never map multiple Go functions to a single API operation, nor do we ever split a single API operation into multiple Go functions. This is done to provide consistency in usage and documentation across SDKs, but also to provide proper visibility of these operations from the context of a given service. For example, if you come across an operation like \"HEAD Object\" in S3's API documentation, you can be sure that there is an equivalent singular method that surfaces this API in our SDKs. This behavior is by design and unlikely to change, even though in this case the operations may seem very similar. In reality, they do very different things which have very different associated costs.\n. @anacrolix thanks for the follow up. We've recently updated our awsutil.Copy() helper to handle copying values across different structure types when they have similar members, which should be a pretty viable workaround to your scenario:\n``` go\nfunc myHandler(w http.ResponseWriter, r *http.Request) {\n    hinput := &HeadObjectInput{\n        Bucket: aws.String(\"bucket\"),\n        Key:    aws.String(\"key\"),\n        // other options\n    }\nif r.Method == \"GET\" {\n    ginput := &GetObjectInput{}\n    awsutil.Copy(ginput, hinput) // do copy from HeadObjectInput\n    s3svc.GetObject(ginput)\n    // ...\n} else if r.Method == \"HEAD\" {\n    s3svc.HeadObject(hinput)\n    // ...\n}\n\n}\n```\nBasically, you should be able to use either HeadObjectInput or GetObjectInput as a single structure and simply copy values to the other structure as needed.\nGiven that this use case is very specific to re-implementing a REST-style web service on top of our SDK, I'm not sure this warrants making the proposed changes to the SDK. It might seem easy to overlap these structures in theory, but there are some stiff technical challenges in re-working our generated code to handle this mechanism, and it introduces quite a bit of extra complexity. Hope the workaround helps, though!\n. Thanks for reporting! This is likely related to #197 where invalid status codes are occasionally not handled correctly. We are currently refactoring error handling that will eventually improve this behavior.\n. > Whatever conveniences the custom type provides internally it's a big headache for consumers.\nNote that the conveniences here are very much external. The err.Code() property is extremely important when handling errors that come from AWS requests, so having to forego the type cast to get at this information improves usability quite a bit.\n. @squirkle yea we had looked at this. The problem is partly multiplicity but also partly the fact that the Message may change with parameter information (and other information, like the AWS RequestID, currently not yet backported into the global Error interface), so the error object is rarely the same error object by reference equality. The only constant portion is the Code.\n. @LuqmanSahaf if you're getting a successful response from the service it is not likely the SDK, but it could be related to the parameters you're passing. Without seeing the code it's hard to say what is going on.\n. Have you waited until the instances are fully started and verified by sshing into the machine? Note that some of these mappings may be eventually consistent as the instance is being initialized.\nI think the right venue for this question would be the Amazon EC2 forums as this does not seem to be an SDK specific issue, and the CLI is showing the same behavior.\n. Marking this as closed since a 200 response indicates that the SDK is behaving correctly and the CLI is behaving the same way.\n. Thanks for opening this issue. This support is still on our TODO list. We can keep this open to track progress.\n. The above commit resolves an issue where the SDK wouldn't correctly escape embedded (and escaped) JSON. Let us know if you're still having serialization issues with SWF.\n. You probably want to rebase off of master for this.\n. It's worth noting that applying this patch makes the marshalling process roughly 20% slower:\n``` sh\nbefore\n$ go test -bench . ./internal/protocol/json/jsonutil/\nPASS\nBenchmarkBuildJSON     50000         32727 ns/op\nBenchmarkStdlibJSON   100000         12064 ns/op\nok      github.com/awslabs/aws-sdk-go/internal/protocol/json/jsonutil   3.516s\nafter\n$ go test -bench . ./internal/protocol/json/jsonutil/\nPASS\nBenchmarkBuildJSON     50000         39859 ns/op\nBenchmarkStdlibJSON   100000         12254 ns/op\nok      github.com/awslabs/aws-sdk-go/internal/protocol/json/jsonutil   3.930s\n```\nGiven that the specification for string is fairly simple (http://www.json.org/), I'm not sure it warrants the overhead.\n. Yea that is a good idea. I'll take a look tomorrow at beefing up the test suite to make sure we're explicitly handling cases like yours (and beyond).\n. Glad you got it working!\n. The goal is to use reflection to do this so we don't have the maintenance burden of copying all those attributes over. That's why we haven't tackled this one yet.\n. @alexaandru yes, only for these structures, not across the entire SDK. We know that the members in UploadInput match 1:1 to the UploadPartInput and PutObjectInput structures respectively, so we can set them all using member reflection rather than hardcoding each individual member.\n. AFAIK @jasdel had a work in progress solution that he wasn't too far away on.\n. I can't seem to reproduce this off of master:\nResourceNotFoundException: The resource you requested does not exist.\nAre you sure you're running off of the latest commit?\n. In that case I'm fairly confident that this was fixed in f68fa41d8779a8daa1e4b8059605afb0fe3b344f.\n. Closing since this is very likely resolved in master. Let us know if you're still having the issue, David.\n. @squirkle how are you providing configuration to your process? Are you using IAM roles?\n. This is related to #114. We have plans to remove the pointer from maps based on feedback in that thread. That should solve this issue. I agree that it's ugly!\n/cc @jasdel \n. Thanks for reporting!\n. Thanks for reporting this, API support should now be up-to-date.\n. > But, you need to pass a region name in as well, and there's no default for this,\n@foresmac we default to AWS_REGION, actually. Our getting started guide has more information on this in the configuring section:\nhttps://github.com/aws/aws-sdk-go/wiki/Getting-Started-Configuration#setting-the-region\nIt's very common to pass nil to your service constructor, which will initialize using aws.DefaultConfig.\n. @foresmac if your goal is to perform a basic transcode job, you might want to take a look at some of our other sample projects using Elastic Transcoder. The JS implementation might be a good start. It's not in Go, but should provide good information on what values should be set and to what-- members should map 1:1 to our Go structures. Also, our API docs should also be answering questions like \"should this be auto or Auto\" / \"is this the default\". Those should be in the docs themselves. Please let us know if that's not the case.\n(Note that all strings are pointers and must be passed in as aws.String(...). This should be documented in our Getting Started Guide as well as our examples for each operation)\n. > As I noted on the confusion on whether a Go string literal or aws.String was required, the Getting Started guide shows examples of aws.Config using string literals as a counter example. So, maybe it's ok there but not in params for services?\naws.Config does not take string pointers, but all service operations do. Using a string literal simply would not compile, since the type is a *string. We have examples for every service operation in our API documentation that should show how to pass parameters in. You don't need to use aws.String(), but at the very least, you cannot pass a string literal, since they are pointers. If you already have string variables holding these values you could use &myvar instead. @jasdel is working on updating our guides to show examples in our \"Making Requests\" section, which should clarify.\n\nI'm really not trying to be obtuse, but you can see how when I need to look at four different places ... it gets hard to be sure if I understand things correctly.\n\nEventually we will be able to consolidate some of our documentation into a single spot (godoc doesn't really like rich \"guide style\" documentation), but perhaps I can explain what each doc is used for, since you shouldn't need all of them at the same time.\n- Getting Started Guide - this document should be used to get a general idea of how to configure and make basic requests with the SDK. If it's your first run with the SDK, you probably want to be following this in addition to the API documentation, since this doc focuses solely on the syntax and behavior of the library itself, and not on the behavior of our AWS services.\n- SDK godocs (API docs) - this should be your primary stop for reading about the service you're trying to use. Everything in Elastic Transcoder's API reference is in our own API documentation, including what values can be passed in to which structures. For example, JobInput mentions using \"auto\" in a number of places, and these are the same places that you would find them in the regular API docs. Our example operations show all required fields with a // Required comment, so if you do not see that then the field is optional and has a server-side default. We could do a better job of pointing this out in the textual documentation (the non-example portions), but for now you can identify required params with the required:\"true\" tag on the member.\n- Service Developer Guide (Elastic Transcoder for example) - this document extends far beyond the scope of the SDK. If you're getting started with a service and want to understand the concepts, you should probably give this doc a read since it will give you a basis for vocabulary and other terminology. You should not need this document for coding, though in some cases, services may supply helpful samples that you might want to look out for.\nBasically, the API docs should cover 80% of what you need after you've ramped up on SDK basics and service vocabulary. Note that all of the documentation text in our API docs for services come from the service documentation itself-- in other words, you will find the exact same information in Elastic Transcoder's \"API Reference\": http://docs.aws.amazon.com/elastictranscoder/latest/developerguide/create-job.html#create-job-request-input\nIf you're having trouble comprehending our API documentation for any reason, I would suggest visiting the API reference documentation linked above and supplying feedback, since all of our content comes directly from those pages.\nHope that helps.\n. > the use of custom base types and structs with only a single field seems overkill.\n@stevenh the big problem is that although these structures may have a single field now, it's very possible they won't in the future. It's also very possible that even required parameters become conditionally optional with the introduction of new fields, so you can't avoid the possibility of a nil on any field. In any case where we add a singular argument method replacement, it would have to be a separate method name so as to not break compatibility in an update, and even that isn't even a great experience, since those methods would effectively become deprecated (on a case-by-case basis) when this happened.\n. > I agree with the point here; if the SDK abstracted some of the complicated configuration set up, it would alleviate a lot of the need for explanations requested in #262, for instance.\n@foresmac I'm not sure solving the main points of this issue would necessarily resolve yours in #262. Specifically, the request here is only for structures with a single input, which is not the case for Elastic Transcoder. We could setup abstracted helper methods for various \"common cases\" with these services, which I think is what you're getting at, but that falls into the same compatibility burden as pointed out above. When the service changes (and it eventually always will) to introduce new features, those new features will not be reflected in these helper methods, and if we were to rely on simplistic methods with positional arguments, supporting any new service feature for those targeted common cases would require a breaking change in the SDK. I think the right way forward for #262 is better documentation and examples, but that's a discussion for that issue.\n@stevenh @foresmac \nAs a sidenote, a lot of the complexity around using these structures mainly comes from the lack of syntactic support in the language for dealing with nested structures. A lot of things could transparently become much simpler in the SDK if Go were to improve its inference on structural and literal types. With Go 1.5's introduction of map key inference it seems more and more likely that Go is heading in that direction. A few very simple syntactic changes in the language could remove quite a bit of the complexity in the API for free.\n. @foresmac if you are willing to open source your helpers we'd be happy to look at creating a section in the wiki to advertise these kinds of helper libraries with the \"backward compatibility enthusiasts beware\" caveat.\n. > In the compatibility situation you describe, when you introduce new fields, aren't you breaking compatibility anyway?\nNo, adding new fields to a structure does not break compatibility. Additional fields would never be required. We actually pay quite a lot of attention to ensure that we never introduce new required fields. The scenario I described actually shows the opposite case, where the introduction of a new member would come alongside removing the required trait on another member.\n\nBut this whole \"declare a request object before making every request\" thing this library makes us do \n\nI'm not quite sure what you are referring to here. You're not required to declare a request object. The two code examples you listed shows that this is not the case.\n\nI just called a method named GetHealthCheckStatus, what else could I possibly expect as output than a list of statuses? If there's an insistence on this single-return-type model, maybe:\n\nWe actually looked at doing this but backed off on the change. In some services this makes some sense, but it does not always make that much sense. In your example, an API update in Route53 could introduce another \"type\" of \"Observations\" list that would collide with this singular field making the \"Observations\" name ambiguous. HealthCheckObservations might be one of the easier examples to list, but there are a few more cases (in AutoScaling, for example) where removing the seemingly redundant prefix actually makes the structure name fairly ambiguous.\n. > @lsegal actually the two examples I gave show it is the case. I just omitted the declaration of the req variable. In fact all/most (haven't actually read every page) of the documentation uses this methodology in examples: http://godoc.org/github.com/aws/aws-sdk-go/service/route53#pkg-examples (though you're using the variable name params instead of req).\nI think we might be having an issue with terminology then. Those are not considered \"request objects\", they are simply input parameters to the request, and should not be prefixed as \"req\". There is an actual request object (see http://godoc.org/github.com/aws/aws-sdk-go/service/route53#Route53.GetHealthCheckCountRequest which actually returns a request object that can be sent later) and this object is a first-class object in the SDK.\nRegardless, you are still not required to \"declare input before making the request\". The examples are generated this way for clarity only. Nothing stops you from passing parameters inline:\ngo\nresp, err := route53Svc.GetHealthCheckStatus(&route53.GetHealCheckStatusInput{...})\nAnd, of course, if you have no parameters to pass, you can also just pass nil. Most operations have some required parameters, of course, so it's hard to avoid having to pass in arguments to an operation.\n. Thanks! However, most of those files touched are auto-generated, so we would need to fix the generators that create these files in order to accept this patch.\nRunning make generate and make generate-protocol-test should show you what's been overwritten.\n. > Go generate is never run automatically by go build, go get, go test, and so on. It must be run explicitly.\nUnfortunately because of this rule in go generate.\n. Are you looking specifically to print output structures to disk using JSON?\n. @fatih I generally agree with this. We could add omitempty (and I'll keep this open to track progress), but I'm concerned that we will get similar requests like this in the future for any other supported protocol (like XML). Basically, I'm not sure this is a sustainable solution. \nI really think that the encoder should ultimately be responsible for how the data is marshalled, not the struct declaration itself. For example, xml.Decoder has a few toggles to control how data is marshalled into types for the general case. I believe similar toggled could probably be added to Go's json.Encoder type (like an \"OmitEmpty\" flag), and I would recommend opening an issue with go to request this. This would allow the consumer to control the data formatting on their own without requiring every library to opt-in on every single data structure.\n. > If someone is already using an io.Reader, they would have to read the entire of buffer into memory in order to make an io.ReadSeeker.\nThis is not exactly true. That said, you're probably looking for the s3manager.Uploader abstraction. You can read more about it in our Getting Started Guide. We will be doing a better job of advertising these features as we bootstrap our documentation.\n. > If you look into the source code the library uses a ReadSeeker to calculate to content MD5 \n@marcosnils actually just a minor pedantic correction, but Content-MD5 is not calculated for PutObject. That said, we do calculate checksums in the payload signature logic: https://github.com/aws/aws-sdk-go/blob/master/internal/signer/v4/v4.go#L311 -- so, basically the same thing. We also need a seekable body so that we can rewind it in case of retries, so there are multiple reasons why a seekable reader is required, and you correctly touched on one of them.\n. @DavidHuie the Uploader performs multi-part uploads.\n. @DavidHuie if you're calling UploadPart directly then you must provide a seekable stream. This is required for SHA-256 checksumming in the signature layer as well as for retry support which is integral to the SDK's feature-set. There are a couple of ways to go about this-- if you are concerned about memory footprint of each part, you may consider caching each part uploaded from the client on disk. If your application server is backed by some amount of SSD filesystem access, the I/O cost should be fairly low.\n. You might also want to check #142 which has some more information about this specific issue.\n. No problem!\n. Passing LogLevel: 1, LogHTTPBody: true to your configuration should provide information on what the SDK was unable to parse.\nMy guess is that DynamoDB Local is generating an error and returning an invalid JSON response (probably plaintext indicating the failure).\n. Also note that you should not set Endpoint directly on service, use configuration instead:\ngo\nsvc := dynamodb.New(&aws.Config{Region: \"us-east-1\", Endpoint: \"http://localhost:8080\"})\n. @diptanu I would recommend opening a feature request on the Amazon DynamoDB forums if you would like to see support for anonymous access in Local.\n. @diptanu yes you can reuse the same client. The credentials will refresh as necessary. \n. @jyehbrightcove I just merged a pull request #285 that resolves this, thanks for reporting!\n. @zshenker you can set the S3ForcePathStyle configuration option to have the SDK use path style addressing instead of virtual hosted bucket style addressing: http://godoc.org/github.com/aws/aws-sdk-go/aws#Config\n. @zshenker it's definitely recommended to use the default behavior of bucket-style addressing. Using bucket-style addressing takes advantage of nameserver caching for your bucket, i.e., mybucket.s3.-REGION.amazonaws.com will resolve directly to the S3 endpoint that hosts your data without having to be routed through the frontend.\n. I am going to mark this as closed since there is a way to disable this default behavior, and using bucket-style addressing is the default behavior in many of our other SDKs and tools.\n. @drombosky the decision to use a list of pointers mostly stems around trying to maintain consistency in the API usage. Having Foo: aws.String(\"x\"), Bars: []Bar{\"not-a-ptr\"} could potentially be confusing to differentiate. It also means that any values in the list are passed by value when the rest of the input is not, which may have behavioral effects on plugins that operate on data mid-way through the request.\n@conslo there is a good discussion of this issue in #114, and I would recommend reading through that to understand the context of this issue. Note that using []Type would not solve the problem of requiring the helper methods, since scalars must be nilable or at the very least support a tri-state style \"unset\" value. You can read more about this in the related issue.\n. > If it's a value I know I actually have to put something there, if it's a pointer, I know the API will accept (possibly only under qualifying circumstances) the value being omitted.\nUnfortunately, designing around this assumption would force us to break the API any time a service changed the required-ness of a parameter, which happens fairly often. For example, services often add new parameters that cause a previously required parameter to now be conditionally required, based on other inputs. Alternatively, services occasionally just make parameters optional by introducing defaults computed on the server side. The general assumption is that any parameter can be omitted, even if it is marked as required today.\n. > could you give an example of a service that could return sparse arrays; specifically where the existence of said null members is important information for the user?\n@conslo the Metadata field in the TrustedAdvisorCheckDescription type in AWS Support API retrieves tabular-style column values in the response, some of which can be null. Omitting these values affects the output, and representing these values as empty strings may not be valid (in this specific case it might be, in a future case it might not). \n. > I feel like a broken record, but I didn't suggest substituting with empty strings, I suggested omitting them. \nI understand you did not suggest this, I was simply laying out the problem with the only possible alternative. Omitting the values entirely is not an option since the columns have significance. This was actually explicitly fixed in the JS SDK recently due to a bug report specifically about omitting nulls. Omitting values shifts the columns and yields incorrect results. \nThe issue here isn't about the specific API, but the fact that AWS APIs can contain sparse arrays (there is a precedent and there can be other instances that either already exist or APIs that may be created in the future). We therefore cannot make the assumption that arrays are non-null.\n\nMore importantly, did you view the discussion on CL 9376 / have you tested this library against this sort of data output on Go 1.5 yet?\n\nI believe @jasdel has already tested against Go 1.5, but he can confirm that.\n. Thanks for fixing this, @Pursuit92!\n. > I could do a HEAD request first to get this, but it's an additional round trip, and there's no telling if what I download is the same resource that I got the HEAD of.\nThis seems like a reasonable workaround to me. If you're worried about the object being concurrently written to while you are downloading it then the initial HEAD request will be the least of your concerns, as the downloader itself is likely making multiple concurrent GET requests-- and the object could theoretically change in between those requests as well.\nBut I agree that having progress support and totalBytes would be useful. The only problem is that they are not known at the start of the download, so if we exposed this it would likely be via function callback.\n. Looks like you probably have a space at the end of your AWS_ACCESS_KEY_ID environment variable.\n. Actually the reason you are seeing that error is because the header is being signed but you are not passing the header through to your curl operation. Certain headers (like SSE) must be passed in to your curl operation as headers. In this case you must pass the same data through to the x-amz-server-side-encryption header when you use curl:\ncurl -H \"x-amz-server-side-encryption: AES256\" ...\n. @shamiq order doesn't matter on headers, but you are correct that all location:\"header\" members should be passed through in your curl command.\n. @conslo I'm not sure the attribution of \"We don't want to have to change the SDK if the API changes\" is an accurate one. We're not opposed to changing the SDK when updates come. The distinction here is that we do not want to introduce breaking changes into a stable library. It is pretty standard practice for stable libraries not to break their downstream consumers with minor updates. In fact, it would probably extremely un-Go-like to follow the proposed path of breaking the SDK at each update, since even Golang understands the strong importance of backward compatibility. \nThat said, if you can offer suggestions that does not incur significant backwards incompatible changes, we are all for looking at them.\n. Perhaps it might be worth looking into having an optional builder pattern structure that users could use when the heavy struct initialization becomes burdensome. For example, we could theoretically allow for:\ngo\nparams := &dynamodb.PutItemInput{}.WithTableName(\"foo\").WithItem(\n    &dynamodb.AttributeValue{}.WithS(\"key\", \"value\").WithS(\"key2\", \"value2\"))\nOr for the simple S3 case:\ngo\nparams := &s3.PutObjectInput{}.WithBucket(\"bucket\").WithKey(\"key\").WithBody(reader)\nReading output values would still use the same pointer structure, but could be improved with some of @mitchellh's suggestions.\n. This is the breaking change that @jasdel was referring to:\n\nThen, new feature comes along, I want to use an identity struct concept that's available to me now, I have two types of options:\nresp, err := foo.Bar(id, \"\").SetIdentity(&ident).Send()\n// Assuming the presence of an identity overrides an empty name, which seems sane to me, or:\nresp, err := foo.Bar(id, \"\").SetIdentity(&ident).SetName(nil).Send()\n\nIf Bar's previously required Name member becomes conditionally optional, the above operation would likely fail on the AWS side because the SDK cannot determine that \"\" is a zero-value that should not be serialized. From the perspective of the SDK, passing the empty string to the Name parameter of an AWS service call is a valid thing that the SDK should support. In order to know that in this case, Name cannot be empty, would mean having to special case a whole host of parameters so as to not serialize their zero-values-- and this too would cause breaking changes, since services can relax a requirement to begin allowing empty strings for certain parameters (and this has happened).\n\nThe default methods are just that, defaults, sane defaults, like disabling SSLv2 on a webserver program by default instead of requiring anyone who uses it to be versed in every aspect of the technology before they can do anything ...\n\nWe might be having to different conversations here. Defaults are great, you won't find any argument from our side about having defaults. The reality is, though, that defaults should come from the API, not an SDK. Defaults, in fact, are provided by AWS APIs, and this relates back to the required vs. optional argument. In other words-- the way that our SDKs surface \"default values\" is by making a parameter optional. You will find this behavior fairly consistent across our other SDKs.\nIf you're asking for defaults, what you are effectively asking for is optional arguments from the perspective of the SDK and API. Most of our AWS APIs already do a good job of (a) making secondary parameters optional, and, (b) providing these sane defaults when the parameters are optional. If you're running into scenarios where you are providing too many parameters for a given service / operation and believe that the service or operation should be providing defaults, please let us know about those or open a forum / support issue with that specific service requesting those parameters to provide defaults in the API. If you can provide concrete examples of an operation that has this problem we can discuss that, too.\n. Thanks!\n. The behavior of having certain output tokens nil is valid. The behavior of SetValueAt*Path for a nil value should be to clear the value at the given path. Namely:\ngo\nSetValueAtAnyPath(obj, \"Foo.Bar\", nil)\nShould be equivalent to:\ngo\nobj.Foo.Bar = nil\nThe actual pagination logic is correct-- if one of those output tokens is indeed nil, it should be copied over to the next input structure (as nil).\n. @dmac the contract is actually to set the pointer or the value depending on the context. The *Value*AtPath API is intended to look at structures more abstractly than their pointer/value types. @jasdel is correct here-- we can fix this by allowing SetValueAt*Path to set paths to nil.\nNote that we would not support setting &str to a path, nor would we ever return a pointer value from a path. You can consider nil a special case in this API, if that makes more sense.\n. While I'm not yet sure if this should be considered a bug in our implementations, it's worth noting that this behavior is consistent across many of our SDKs. Specifically, passing an empty list will cause the same failure in both the Ruby and JavaScript SDKs. I still have to verify if we might be serializing empty lists incorrectly, but note that if you provide an empty list to the SDK, it will serialize that empty list across the wire to EC2 (which is probably the behavior you want). At that point, it's up to EC2 to decide whether it can accept an empty list or not. There are two possibilities here:\n1. We're not correctly serializing the empty list and EC2 is having trouble parsing this input (possible but unlikely given that this behavior is consistent across SDKs), or,\n2. EC2 is reading in an empty list and is not allowing this as a valid input. In this case, there is not much our SDK can do to work around this. If this is the case, the right way to fix this would be to have EC2 allow empty block device mapping lists passed in to this operation.\nI'll do some more checking to see which it is, but hopefully that provides some background. Thanks for reporting this!\n. I cannot reproduce this behavior. The ETag shows up for me:\ngo\n{\n  AcceptRanges: \"bytes\",\n  Body: buffer(0xc208277280),\n  ContentLength: 0,\n  ContentType: \"application/octet-stream\",\n  ETag: \"\\\"d41d8cd98f00b204e9800998ecf8427e\\\"\",\n  LastModified: 2015-05-08 21:41:45 +0000 UTC,\n  Metadata: {\n  }\n}\nWe also use http.Header.Get() when reading the header value, so the casing of the locationName should not affect reading the value.\nCan you show code that reproduces this? Are you sure you are using the SDK off of the latest commit in the repository?\n. @diptanu if you're just looking for an estimate, you can JSON marshal your Item object for a relatively accurate estimate, or look at the HTTP request/response content lengths. Note that this represents the JSON data sent across the wire, but not the actual storage size, which may differ by some amount. If you're looking for an accurate-to-the-byte value, you might want to open a thread on the Amazon DynamoDB forums to ask there.\n. @bkeroackdsc there are a few discussions about style going on in other issues, I would recommend looking through open issues and jumping in with specifics there, that said,\n\nFor example, the usage of aws.String(foo) instead of the more idiomatic (and simpler/faster) &foo. \n\nNothing stops you from using &foo when possible, and in fact, if you are actually have a variable foo, you should use &foo. The SDK does not make any recommendations in any of our documentation to use aws.String() on existing string variable values. The only place our documentation uses aws.String() and the like are for string (and number) literals, and this is due to a limitation in Go, namely, you cannot take the address of a literal without some variable assignment. For example, you cannot do:\ngo\ninput := &s3.CreateBucketInput{Bucket: &\"bucket-name\"}\nAnd it would be awkward to have to do:\ngo\nbucket := \"bucket-name\"\ninput := &s3.CreateBucketInput{Bucket: &bucket}\nThe aws.String() helper allows this to be written in one line:\ngo\ninput := &s3.CreateBucketInput{Bucket: aws.String(\"bucket-name\")}\nHopefully that explains the logic there.\n\nThere seems to be a similar aesthetic with the Credentials subpackage -- why can't I for example pass the access id/secret key directly to s3.New()?\n\nThis was an intentional change to discourage the use of hardcoded credentials passed to constructors. I would suggest reading through the Getting Started Guide, specifically Configuring Credentials. It is not recommended to hardcode credentials and there is usually a better way to configure credentials in the SDK-- in fact, the SDK does pretty good automatic detection of credentials that, in most cases, should make it so you don't need to pass credentials into your constructor at all. This is the recommended way to initialize S3, for instance:\ngo\nsvc := s3.New(nil) // no credentials needed\n. @bkeroackdsc you can find the discussion about pointers in a few of our existing discussion issues as mentioned above; #114, for example. Instead of rehashing the explanation, I would suggest reading there. The short explanation is that most parameters (if not all) can be omitted, and it is not easily (or feasible) to differentiate Go's zero values from omitted values. You can see the linked issue for more details.\n. @porjo \n\nIs that duplication really necessary?\n\nThe methods exposed mimic the operations of the underlying surface. The low-level SDK operations are designed to be a 1:1 mapping of the API operations in the exposed services. The benefit of this design is that you can always trace the parameters to the underlying API documentation. From a documentation perspective, you'll note that the docstrings are unique and occasionally provide fairly important details about contextual details in creating and updating stacks. This information would not be easily captured if these two structures were shared.\n. @catsby don't specify the empty string if your intent is to omit the parameter. You are not required to pass in optional parameters (our documentation denotes which parameters are required), and you should omit passing values for those if you don't want to set them. Unlike goamz, the AWS SDK will serialize empty string values across the wire. My guess is EC2 does not allow an empty string for the NoDevice parameter.\n. It actually looks like the value is incorrectly modeled as a string, but all the documentation I'm seeing for NoDevice lists it as a boolean. You may want to try passing \"true\" or \"false\" as values (\"true\" in your case, I suppose) to see if that works. If so, note that the fix here would likely be a breaking change to turn this string into a bool for an upcoming release.\n. I've pushed two related fixes to master which should resolve this issue. I fixed the copy semantics on nil slices and maps, and I made a similar fix on our path value operations which were causing the subsequent pagination to run forever because the nil slices in the output were being treated as valid tokens.\nThis operation should now correctly paginate. Feel free to re-open if you're still running into this issue. Thanks for reporting!\n. Request IDs haven't yet been implemented for our other protocols. This is still on our TODO list. You can access the HTTP headers directly (also on the request object) to get them for now. I'll mark this as a feature request.\n. @sclasen they have different header names and also some protocols place the request ID data in the body, not headers.\n. @sclasen it should indeed work out of the box. I'm not at my dev machine right now but I'll be sure to investigate when I have a chance. You can look at the RetryCount property on the aws.Request struct (if you call with the *Request methods) to see if it's ever non-zero. That should give you some more insight. Turning on logging would, as well.\n. @sclasen ah good catch, I didn't think to mention this. This is a known issue (#157) that should be resolved by #276 pretty soon.\n. I can't think of any reason why this shouldn't get added. For reference, JS uses these:\nhttps://github.com/aws/aws-sdk-js/blob/master/lib/service.js#L343-L347\n. Thanks for reporting, @sclasen!\n. It's similar in Go:\ngo\ndb := dynamodb.New(&aws.Config{Endpoint: \"http://localhost:8080\"})\n. @bertabus are your other objects in the bucket by any chance marked as public-read or have some other ACL that would make them accessible without a presigned URL? That's what I'm thinking is happening. Presigned URLs should be signed for a specific key, since the signature contains the path portion of the request.\n. @kciredor I'm not sure this would be an issue with the AWS SDK for Go since it seems like you were successfully making these calls and the service successfully initiated the creation, which implies the SDK successfully serialized the requests. The SDK performs no other operation besides the exact operation you called. \nAs for why the requests are being marked as DELETED immediately after creation, my best guess would be there is some type of data validation happening on the service side causing Machine Learning to remove the data source, but I could be wrong. Also, I'm not sure of all the details of your app so I could be wrong again, but if the 2nd CreateDataSourceFromS3 call relies on the presence of the first, you should note that these calls may be \"eventually consistent\"; in other words, it's likely that the first call has not always completed in time for the 2nd to begin-- that would certainly explain the \"randomness\" portion of the report. I would suggest opening a thread on the Amazon Machine Learning forums to ask about your issue, giving the input you're providing to the calls. It's likely they would be able to provide more precise feedback as to what may be happening on the service side. \nAs a sidenote, and I'm not sure this is relevant but I thought it might be worth pointing out: it looks like you're setting dataset.ObsDataSourceID and dataset.EvlDataSourceID values based on the returned IDs from the successful calls, but you claim to be running this over multiple datasets. Reading through the code, it looks like it will only set the last ID, since you're only storing a single ID from each of these calls. Again, I don't know the details of your app, but that may not be what you want.\n. The issue here is that TTL is conditionally required depending on what other parameters are provided in the operation, see the Route53 API Reference documentation for more information. Specifically, TTL is not required for alias resource record sets, which is why it is not marked as required. It seems like we should potentially sync some of that extra API documentation down to our SDKs, though.\n. @wari GetObjectRequest() returns the aws.Request object that can be Presign()d. Hope that helps!\n. @drombosky just to confirm, is your goal to send arbitrary binary data, or are you simply attempting to send UTF-8 encoded payloads? Note that SQS only allows Unicode formatted payloads (specifically it seems only UTF-8), and does not allow arbitrary binary data. \nIf you do want to send binary data (non-Unicode data), I would recommend base64 encoding your payload.\n. @SergeyTsalkov is the report specifically regarding support for DreamObjects? I would imagine that if DreamObjects is \"S3-compatible\" as you mention, then it should support whatever is supported by S3, including absoluteURI syntax (otherwise it may not be entirely S3-compatible). Specifically, the SDK is not doing anything that another tool, or even a very well intentioned proxy, might be doing, and according to the HTTP/1.1 spec, web servers (including a server like DreamObjects) \"MUST\" support the absoluteURI form that the SDK is providing in order to be compliant-- even though you could argue that we \"SHOULD\" not be crafting the request in this way.\nThe problem is that, unfortunately, there is no other way to control encoding logic for URIs other than using the Opaque field, and since the URL value is part of our public API (req.HTTPRequest.URL), we want to ensure that it is a self-contained API; in other words, calling req.HTTPRequest.URL.String() returns the fully qualified endpoint without having to manually build it yourself by concatenating values from the URL and the HTTPRequest.Host, which can be confusing and complicated to do. Request pre-signing specifically needs this functionality, for example.\nIf the issue is really specifically regarding SDK compatibility with DreamObjects (since the SDK itself works fine with S3), I would recommend opening an issue with DreamObjects and requesting that they investigate support for the absoluteURI syntax in their web server logic-- according to the HTTP/1.1 spec, this is something they should (\"MUST\", technically) be supporting, and something they should be supporting \"in all requests in future versions of HTTP\", so it is explicitly not a legacy thing.\n. AccountId is auto filled by the SDK when not provided. You can also provide \"-\" instead of an explicit account, if you do provide one. Since it looks like you got it working, I am going to mark this as closed. Glad you got it!\n. 500 status codes should all automatically be retried. Have you checked the RetryCount on the request object? If so, can you provide that value? Note that you may have to use the *Request() form to get the req object to check this property.\n. 3 times is indeed the default. \n. @mattes aws.String() returns a pointer because nil is a valid input to these parameters. A lengthier discussion can be found in #114 and others.\n. @mattes using empty interface types would completely remove all type checking / inference in the language and tooling, and I would imagine this would cause much more confusion in the library and -1's from the community at large where type information is considered extremely useful.\n. @stevvooe \n\nLooking at Config, as an example, not one field cannot be correctly detected via zero-valued types.\n\nActually, Config was changed specifically because zero-values didn't cut it. See #157 for details. Specifically, a MaxRetries of 0 is not the same as a default max retry setting. There are other settings in there with the same problem. Users ran into real world blockers when Config worked the way you suggest. The SDK had to change because zero-values don't work there.\n\nCould you give some concrete examples in the API where an empty string, zero-value number or other zero-valued type needs to be distinguished?\n\nAny API where freeform text can be set, or pretty much any API operation that takes a number. Numbers are much easier to give concrete examples for. \"0\" is often a valid input for numeric parameters in AWS APIs. Since you asked, a concrete example would be DynamoDB's AttributeValue struct, where the N field is an integer of any value. Going down your suggested path would mean that our SDK would be unable to determine if a user set the N field in the case of setting it to 0 (and would therefore be unable to serialize the payload).\nI would suggest stepping away from playing the \"example-counter-example\" game, though. The general problem to this argument is that the path forward needs to be guided by the API specification, not specific incidences of APIs. AWS has over 50 active services with probably hundreds (if not more) API calls, and thousands (if not tens of thousands) of parameters. If any of these values can theoretically have an empty string or zero value, this argument immediately falls apart. Furthermore, if any service in the future can theoretically rely on this spec, the argument falls apart too. The next AWS service may rely on using empty strings and 0's to differentiate from unset fields, and the SDK has to be able to support such a service going forward. Even an existing service might decide to begin differentiating these two values, which would be an even more insidious failure point. There are already some of these services that exist today. It's actually not even realistically possible to track all of the incidents where a zero-value is valid input to the service, because it's entirely contextually dependent to the service and may change at any time if the service makes any changes to their API. The SDK has to observe the wire protocol specification, just like you would want your HTTP libraries to correctly implement those specs and not make assumptions.\n. > if APIs changes, then shouldn't the users need update their code anyway? \nActually, no, users do not need to update their code to work with API changes in the AWS ecosystem, since almost all API updates are themselves backwards compatible. For example, the same code you wrote against the S3 API 4 years ago will likely still work today as is, even though there are plenty more switches and toggles to play with in newer SDK updates. SDKs should reflect backwards compatibility in the wire protocol-- the problem here is that with many of the proposed syntax changes, a lot of otherwise-backwards-compatible changes in the wire protocol would end up being backwards-incompatible in the SDK. In short, if it's backwards compatible in the AWS API, it should be equally backwards compatible in the SDK. This is the expectation I would have as a developer using the AWS ecosystem.\n. > Well, in that case, the solution would be to use idiomatic API and let the users use a reproducible build solution like Go 1.5 Vendoring Experiment, Godep, or one of the many other.\nVendoring doesn't solve this problem. I want to be able to stay up to date with the Go SDK version, because I want the security patches, bug fixes, performance improvements, and, occasionally, feature updates, but I don't want to have to change my code every time I sync from upstream. This is the reason why backwards compatibility is important, not just to one Go library, but to almost every development platform out there. Compatibility isn't just about making old code work on old projects at a snapshot in time, it's about keeping old code working as projects continue to evolve.\n. > Well, if you want new features then you're probably already updating your code\n\"Updating code\" is more complex than a simple binary statement. If I have 10,000 lines of code in my existing system that rely on SDK features, and I am \"updating my code\" to add 20 lines of code that rely on new SDK features, I don't want to be liable for potentially having to upgrade and rewrite 10,020 lines of code. My goal was to simply work on 20 lines of code, not ten thousand. In other words, just because your system is evolving, it doesn't mean you have the developer cycles to constantly evolve all of it in one piece.\n\nToo much emphasize [sic] on back-compatibility ends up with something a la PHP.\n\nPHP is a successful product, I don't see a problem here. That said, if you don't like PHP, you could just as easily replace that statement with:\n- Too much emphasis on back-compatibility ends up with something a la the Linux Kernel.\n- Too much emphasis on back-compatibility ends up with something a la the Java programming language.\n- Too much emphasis on back-compatibility ends up with something a la the C++ programming language.\n- Too much emphasis on back-compatibility ends up with something a la the Go programming language.\nAll of these products have the same BC emphasis and yet are huge successes, largely in part to their commitment to backwards compatibility, and all of these are great projects that prove compatibility can work well, even if you think PHP did it wrong (the last one being highly relevant).\n. As a sidenote: the [debatable] difference with PHP was that it was not architected to handle change well, with its global namespace and lack of overloading, etc. It seems like the entire goal surrounding this issue in the Go SDK right now is to architect it in such a way that it does handle backwards-compatible changes well.\nIn other words: sure, if you try to tack on backwards compatibility to a poorly architected system, you get a huge mess. That's why the developers of the Go SDK are trying to architect this right-- and I don't believe any of the proposed changes help to get it there (if anything they make things worse).\n. :+1:\n. :+1:\n. :+1:\n. :+1:\n. It looks like Io is coming up twice, we probably want to make sure this list is uniq'd before adding the full thing.\n. Not passing a function to this method would be a programmer error, not some kind of runtime error. It would never make sense to handle an error like that, since there would be no sensible recovery. Basically this should be a compile-time error but Go cannot quite represent the type here. That said, EachPage() is a low-level call that is typically not used unless you know what you're doing. The *Pages() methods have the correct type checking in them.\n. The unfortunate reality is that simply using reflect already opens us up to arbitrary panics (Go is by definition ignoring its own above stance here), few of which we are guarding from because they are all equally edge-cased programmer errors, but there are cases where, for instance, fn.Call() could fail for various reasons, etc. -- all of those are panics.\nJust to reinforce a few things though: the likelihood of an actual panic in production code is really low. You would have to do bad things to create such a state such that your code could not have ever worked. These are mostly just edge case guards to generate \"nicer\" panics than the ones reflect will emit. The goal isn't to try to defensively guard against these scenarios.\n. Spoke about this with @jasdel and I agree that we can make this change. I'll take a look at this now.\n. ",
    "pwaller": ":+1: !\n. @stripecodahale could you point me at your favourite documentation which describes the exchange with the AWS API and broadly how you might like it implemented? I'd contemplate taking a shot then, if I can find enough time.\n. For reference, this is what boto does.\nI'll take a look at doing something this evening. If you don't hear from me in the near future, I didn't make it.\n. The API seems not ideal because the process of retrieving AccessKeyID etc may return an error (e.g, the metadata endpoint may be temporarily unavailable.\nIs it a dealbreaker to change the Credentials interface to return errors?\n. There is also the question of how to know when to fall back to using the instance role. Do we just try in the case that no other authentication is provided, and see what happens?\n. I don't yet have much experience with the API. Who calls AccessKeyID? And how long between their call of it and the use of it?\n. :+1: on the credentials being a struct.\n. So what should I attempt to change in this PR, then? It seems like the Credentials thing is its own refactor. If it's not a big job, I could make a separate PR for that first.\n. Okay, so I'll attempt it now.\n. I can't think of a good name for the new Credentials struct. I'm going to go with CredentialsValue for now, which I hate, but I don't think renaming the existing Credentials interface is warranted.\n. Okay, I've made the change. It is currently untested. Will come back to it when I have a moment. Comments welcomed!\n. I've made those improvements. Now I just need to try it out and write a test, I think. Other comments still welcomed.\n. One thing I'd like to know your convention for: what to do about all of the error conditions. It always feels ugly to just propagate the error without attaching any contextual information. Do you have any thoughts on that without generating too much clutter in the code?\nI saw you're using juju's errors library.\n. @stripecodahale, everywhere, or in specific places?\nCurrently I'm wrestling with this when I try to use the credentials to GetObject an s3 object. Any ideas?\naws.APIError{Type:\"\", Code:\"AccessDenied\", Message:\"There were headers present in the request which were not signed\", RequestID:\"\", HostID:\"\", Specifics:map[string]string{\"BucketName\":\"\"}}\n. I tried for a while, but couldn't figure out why it's giving me that error. Here is some code you can try:\nhttps://gist.github.com/pwaller/10231b176626a3b86948\nRun with s4cat mybucket myobject. It should work given a bucket policy of this form on mybucket, but I cannot get it to work. It does work with aws s3 cp via awscli.\n{\n    \"Version\": \"2008-10-17\",\n    \"Id\": \"PolcyId\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowRoleS3Access\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::<account number>:role/<machine role>\"\n            },\n            \"Action\": \"s3:*\",\n            \"Resource\": [\n                \"arn:aws:s3:::mybucket/*\",\n            ]\n        }\n    ]\n}\nI'm heading to bed for now :sleeping: - input appreciated!\n. Thank you for the quick turn around. :heart:\nAllowed us to make this which requires no dependencies to run, and can fetch things from S3 buckets using the IAM role:\nhttps://github.com/scraperwiki/s4cat\n. Is it possible to consider having the API generator spit out a parallel set of calls which instead of executing the API request, return a signed URL that represents it?\n. @stripecodahale, I was just under the impression that you could delegate just about any operation. If that's the case, then perhaps it would be useful to be able to reuse the API generation to be able to generate URLs for those operations.\n. Nice, thanks!\n. Tidying up my personal issues list, so closing this. Please create a new issue if you're still interested in tracking it.\nLooks like this is very possible on the new development branch anyway, which will soon be live.\n. First I need to understand, why is a timeout different from the default go http client timeout appropriate or necessary?\nWe made this call early after machine boot and its failure prevented the machine from coming up. Maybe it is a bit slower near boot? In any case I would rather give it a decent chance at trying before failing it.\nOn 14 January 2015 19:28:15 GMT+00:00, Coda Hale notifications@github.com wrote:\n\nOne second has worked for us in production, but I'd accept a PR to make\nthat configurable, yeah.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/stripe/aws-go/issues/62#issuecomment-69975540\n\n\nSent from my Android device with K-9 Mail. Please excuse my brevity.\n. Hm. I thought there was a defined time. Would a minute be appropriate? Large enough to be pretty certain it should respond in that time, small enough for it to be on a human attention timescale?\nOn 14 January 2015 22:53:23 GMT+00:00, Coda Hale notifications@github.com wrote:\n\nThe \"default timeout\" of net/http/Client is zero, which means\nrequests will hang indefinitely. This is not the desired behavior,\nhence the timeout.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/stripe/aws-go/issues/62#issuecomment-70009091\n\n\nSent from my Android device with K-9 Mail. Please excuse my brevity.\n. Thanks for the quick response!\nAh, it must be the difference between IAMCreds and DetectCreds, then?\n. (I will investigate :eyes:)\n. I seem to still be hitting it. I'm relying on using the instance credentials for authentication. It's a bit of a mystery that it would work for you and not me.\n. I'm compiling against b8aa0b2 (the current master), by the way. Are you against develop?\n. Yep, I did vendor them. Thanks for fixing it :dart: \n. I think what actually happened is that ContentLength is compulsory. Is this bug then that aws-sdk-go doesn't check that the header is provided?\nIs there a way of doing an upload when the size isn't known in advance?\n. Thanks for the rapid response! Fantastic :smiley: \n. (I'm glad I put the commit SHA in, I was under the belief it was a fresh clone, whoops).\n. I just hit it again in another context, and again I have no idea what caused the error :cry: \n. I've just now hit it in a third context! It's the same one in each case, and I know it is just this error because when I modify the sdk code it shows me something sensible.\n. For some reason the provider still doesn't work. Now I use this:\ngo\ns.Config.HTTPClient.Transport = &http.Transport{Dial: viaBastionDialer}\n// Copy HTTPClient configuration for EC2RoleProvider\ns.Config.Credentials = defaults.CredChain(s.Config, defaults.Handlers())\nWith debugging enabled:\n```\n2016/01/14 15:28:24 DEBUG: Request ec2metadata/GetMetadata Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nGET /latest/meta-data/iam/security-credentials HTTP/1.1\nHost: 169.254.169.254\nUser-Agent: aws-sdk-go/1.0.8 (go1.6beta2; linux; amd64)\nAccept-Encoding: gzip\n\n2016/01/14 15:28:27 DEBUG: Response ec2metadata/GetMetadata Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/0.0 0 status code 0\n\n``\n. Ah, I think I might be hitting the fact [that I'm still using thehttp.DefaultClient`, and it gets overridden especially for the metadata service](https://github.com/aws/aws-sdk-go/blob/6811074135399057ee6a6c457b2f23fbd6a067d6/aws/ec2metadata/service.go#L46).\n. I got it working in the end with this:\ngo\nuseClient := *http.DefaultClient\nuseClient.Transport = &http.Transport{Dial: viaBastionDialer}\ns.Config.HTTPClient = &useClient\ns.Config.Credentials = defaults.CredChain(s.Config, defaults.Handlers())\nAre there likely to be any other HTTPClients that I've missed?\n. Thanks for the fix there. I guess there are still two issues remaining for anyone hitting this problem to beware:\n1. Just replacing s.HTTPClient after session.New() does not affect the EC2RoleProvider.\n2. If you replace http.DefaultTransport.(*Transport).Dial, it will not be respected by EC2RoleProvider.\nWorkarounds to these issues:\n1. After setting s.HTTPClient, remake s.Credentials to its default with something like s.Config.Credentials = defaults.CredChain(s.Config, defaults.Handlers()).\n2. If you want to modify http.DefaultTransport and have it affect EC2RoleProvider, you must instead modify the transport the s.HTTPClient.\nThese are hard to fix in the Go AWS SDK because:\n1. Would require refactoring EC2RoleProvider so that it instead held a pointer to the config, rather than taking a copy of the configuration at the moment of construction. \n2. There is no way that I'm aware of to test to see if http.DefaultTransport's dialer has been modified from the default, since it's a) not possible to compare function pointers b) not possible to obtain a reference to the dialer before any libraries might modify it anyway (even in init).\nTo conclude: Could we fix (1) in aws-sdk-go, or would that be too much breakage? (2) I guess just requires that you don't expect modifications to DefaultTransport to have effect, unless a smart gopher can figure out a way to achieve that, or such a mechanism is put in the standard library.\n. Aha. So with #511 now the transport is not touched. So (2) no longer applies from the above post. That's a great start.\nI see two remaining problems:\n1. The first problem from the above post still applies (you have to re-initialise .Credentials).\n2. If http.DefaultClient is modified, those choices won't be respected, because a new http.Client is constructed from blank rather than copying the http.DefaultClient.\n. @jasdel, only the application itself (and not libraries) should modify the default timeout, and I would expect that any well behaved application would do so before initiating any requests to EC2.\nOne other consideration: I have seen this timeout fail on occasion on EC2 instances. This is more annoying than if the request had waited longer and eventually succeeded. I would have rather it waited a bit longer and completed, or even that it would have retried a few times.\nMaybe one option is to relax the timeout once we know the metadata endpoint is accessible? That way you get both properties: reliability if the machine/metadata endpoint is slow, and the ability to give up on Role based credentials if the metadata endpoint is inaccessible.\n. Thanks @jasdel, seems reasonable to me. Did you decide against relaxing the timeout after the first successful attempt?\n. By the way, the implementation of readCache mirrors what botocore does.\n. I'm happy to continue to improve this PR, or to close it and have more discussion in #842 first.\n. :+1:. This makes total sense to me, even more so after having used this in anger a bit.\n. To preempt an obvious response: I am aware I can achieve this effect by manipulating the environment myself. What I would like is a standard way of securely using MFA and roles implemented by the library so that it's not necessary to do this. Software implemented using aws-sdk-go should be able to work with MFA and assume-role \"for-free\", or easily opt-in by way of configuration applied to .aws/config. What I'm after here is not to just solve the problem for myself, I want to solve it for colleagues and users of the software I make using aws-sdk-go.\n. Mostly sounds reasonable. I think what needs to happen is first this needs to be made to work:\n[default]\naws_access_key_id=FOO\naws_secret_access_key=BAR\nmfa_serial = GAHP01234567\nmfa_cache_lifetime = 12h\nThe thing is, the STS credential provider you linked to demands a role to assume. But I think the workflow that is really desired to make the above work is to call get-session-token, and cache the result of this.\nThis session token can then later be used to assume roles that require MFA. This is useful because an assumed role's credentials can only last up to 1 hour, whereas a session token can last considerably longer. So I would want to see this cache the get-session-token credentials and then, if a role is specified to be assumed, assume a role as often as is needed.\n. I have put together https://github.com/pwaller/aws-creds which is a program you can run which does the credential caching I described in the post above, if you want to see what that feels like.\n. Hi @jasdel, what is the best way to progress this from here? Is it something you are looking to implement before the end of the year, or is that unlikely to happen?\n. @xibz: indeed the OrigErr is empty. The code posted is a complete code to reproduce, so it should be easy to modify. In addition, even if OrigErr were populated, the mention of an \"upload\" error is misleading.. A definite improvement, thanks!. @jasdel @xibz: Hm, we were actually using the fact it was an infinite loop (with a long sleep in the pager function) in order to poll CloudWatchLogs. We actually exit the loop on a different condition entirely. This change caused a regression as a consequence, since the stream came to an end and we weren't expecting it to. Is there an idiomatic way to re-enable it while calling GetLogEventsPages, or must we use the pagination API?\nI also note that this change wasn't mentioned in the release notes, so it took us a while to attribute the breakage to the AWS update.. By the way, it seems the CLI has dropped support for AWS_REGION now, which lead to some confusion since I was expecting both to work. After a quick search I actually can't find anything mentioning this (changelogs, git commits) in either boto3 or the awscli repository. I just observe that AWS_REGION isn't mentioned in the aws-cli documentation and doesn't work anymore.\nFWIW I would like to see these tools behave as consistently as possible, though I understand the pressure to remain backwards compatible. Perhaps pressure should be applied to awscli to re-support AWS_REGION?. ",
    "squirkle": "I am still getting this error with SQS.\n. > Because of this, I'll make an update which switches the return types back to error, but keep the underlying interface awserr.Error which can be cast to in order to retrieve extra information.\n@jasdel--that sounds like the perfect solution, thanks!\n. @jasdel It seems to make more sense to me to default to what is idiomatic in Go and provide extra functionality to those who want it by letting them cast to aws.Error (principal of least surprise, and all that?).\n. It may also be worth suggesting that a common idiomatic pattern in Go is to use \"named\" error types for decisioning around error handling.\n``` Go\nmyDb := someDBLib.Init()\nerr := myDb.Query(queryStr)\nif err != nil {\n  if err == someDBLib.ErrorNotFound {\n    // handle not found error\n  }\n  if err == someDBLib.ErrorMalformedQuery {\n    // handle malformed query\n  }\n}\n```\nMaybe the multiplicity of error types you are dealing with would prevent using this kind of pattern but I thought it might be worth mentioning.\nThanks!\n. @lsegal Makes perfect sense.  Was pretty sure you would have already thought of this but just wanted to check :)\n. IAM, correct.  The process gets the credentials and runs successfully for a while before failing with the expired token error.\n. @lsegal am I correct in assuming that expired credentials should be automatically refreshed by the aws-sdk or is this the responsibility of the calling program?\n. @jasdel Awesome!  Thanks for the quick response!\n. @jasdel Unfortunately this issue is still occurring.\n. @jasdel It is the same error but I believe I may not have deployed the fix properly.  I'm going to re-close.  If I see the issue again I will let you know.  Thanks!\n. ",
    "jasdel": "Hi @squirkle thanks for reporting this. Would you mind opening a new issue with information about the issue you are experiencing? \n. What about a purely synchronous pagination API?  That way the caller has full control, and its obvious what context the page result will be in when provided. I'll use an example similar to bufio.Scanner's Next(). This could perform action to get the next page when it is called, and returned when the data is available synchronously. Behind the scenes the function might be async, but the interface is synchronous.  In this case there is no worry about exposed or leaked channels, or context of callbacks. Operations will only be performed as the caller chooses. Using a callback also raises questions about what goroutine context the callback will be executed in.\nInterestingly the reason Walk uses a callback is because it uses the return of WalkFunc's to determine how to proceed. In filepath.Walk's case WalkFunc is really a yield pattern, not an async operation. In the case of Walk calling WalkFunc I think its actually synchronous within the same goroutine's context as its self, but that behavior is undefined by the docs.\nI'd like to suggest an API such as:\nGo\n    type Page interface { // Generic page interface, or service/api specific struct\n    }\n    type Pager interface{\n        Err() error\n        Next() bool\n        Page() Page\n    }\n    func (db *Dynomodb) Paginate() Pager {...}\nThe caller would then do something like\nGo\n    p := db.Paginate()\n    for p.Next() {\n        page, := p.Page()\n         // ...\n     }\n     if err := p.Err(); err != nil {...}\nOr some other variation on this where a method can be called and it blocks until data is available returning it or an error synchronously. This pattern fully supports stopping partway, and fits in nicely with Go's common patterns of synchronous apis. This complicates the internals of Paginate, but I think the gains are worth it.\nWhat do you think?\n. @lsegal good point, thanks for the feedback. I didn't realize so many of the API's could be paginated. In that case I totally agree a generic Page type would be problematic. Would API specific PageX's types make sense that NextPage() /Pages() could provide? This would increase the generated code a bit since logic for each paging may be duplicated between API's because of the type specific PageX. Though maybe NextPage()/Pages() could defer to a private nextPage()/Pages which operates on the interface{} method like in this branch. This would provide a public typesafe interface, without as much duplicated logic.\nTrying to think of a way to use Pages() and NextPage without interface{} thats compile time typesafe.\n. Hi @georgyo I updated the waiters branch. Syncing it with the latest version of the SDK. We still are working on finishing up the Waiter support. Specifically we need to improve the JSON path searching waiters use to know what fields of a service response signify the status the waiter is waiting on.\n. Waiters have now been added to the SDK. Check them out in the latest release v0.10.3.  Let us know if you have any questions or feedback.\n. Hi @luck02 I just pushed the credentials refactor and I think it will provide the functionality you are looking for.\nFrom the documentation of the EC2RoleProvider you can now pass a custom http.Client to the credentials provider. The client will be used when requests retrieve IAM roles on your EC2 instance.\nGo\nsvc := kinensis.New(&aws.Config{\n    Region: \"us-west-2\",\n    Credentials: credentials.NewEC2RoleCredentials(\n        // Pass in a custom timeout to be used when requesting\n        // IAM EC2 Role credentials.\n        &http.Client{\n            Timeout: 10 * time.Second,\n        },\n        // Use default EC2 Role metadata endpoint\n        \"\",\n        // Do not use early expiry of credentials.\n        0,\n    ),\n})\nBut you shouldn't need to use the EC2RoleProvider if your application isn't running on an EC2 instance.\n. Hi @nbari The SDK does not currently support SimpleDB since the SDK only supports SigV4 request signing. We have the outstanding PR @316 which covers adding SigV2 signer, and simpledb support, but it is still be reviewed.\n. Thank you very much for the examples and information. Context is a very interesting pattern, which deserves more investigation. We'd like to hold off updating the SDK wholesale to use Context for the moment, and watch to see where the Go community goes with this pattern.\nOne initial roadblock to integrating Context would be the need to refactor the API operations to be asynchronous instead of synchronous. The time and effort to refactor the SDK to be async is not a problem. Our primary concern arises that the refactor would impose an async/concurrency model on all developers using the SDK. With the SDK's current pattern of sync API operations developers are free to use any concurrency pattern they would like. This style seems to be very idiomatic in Go where function calls are only async if absolutely needed.\nThe SDK's current design allows nearly any concurrency model to be used on top of the API requests. This includes Context. Though I see having Context built in would cut down on boilerplate code wrapping the SDK. Also request specific cancellation handling isn't really supported fully at the moment. You could implement with a custom Send Handler, and a non-trivial amount of code to support cancellation to support per request cancellation.\nAlso on a side note you can use the API's Request functions to separate building a request from sending it.\ngo\noutput, req := s3.ListBucketsRequest(&s3.ListBucketsInput{})\n// output will be populated when Send() returns.\nerr := req.Send()\n...\nThe key/value data payload Context provides is interesting. Though we would lose some of our compile time type validation if the SDK were to use the values passed in. Alternatively having concrete types for the values would reduce the need or runtime type casting or checking.\n. Closing since the SDK should not require and specific concurrency model. Using wrapper functions around the SDK's Request operations the Context library can be used.\n. Thanks for the feedback. Context certainly is a powerful tool. With that said we would like to avoid requiring users of the SDK to use a specific upstream dependencies within their applications. Especially since Context is still marked as an experimental package, and is subject to change. Once Context is moved into the stdlib I think we should reevaluate that decision.\nWould adding a Cancel method to aws.Request address the use case you are looking for? With this pattern you would still be able to proactively cancel outstanding requests.\n``` go\nreq, result := sqsSvc.ReceiveMessageRequest(...)\nerr := req.Send()\n// In some other  goroutine\nerr := req.Cancel()\n// Request.Cancel() would preform something like the following\nif t, ok : req.HTTPClient.Transport.(http.Transport); ok {\n   t.CancelRequest(req.HTTPRequest)\n}\n``\n. Thank you very much for the discussion and feedback if the SDK should be using the Context library and pattern. Since we would like to ensure the SDK does not impose a specific concurrency pattern we would   like to hold off integrating Context into the SDK. Since the SDK doesn't impose its own pattern you are still free to build your own concurrency pattern on top of it, including Context if that makes sense for your application. Again, thank you for your feedback.\n. Hi @ryanfowler Thanks for posting this suggestion.  If you'd like to create a PR it would be great to have a simple example like this added to the SDK's [example](https://github.com/aws/aws-sdk-go/tree/master/example) folder. Probably under the folder,example/aws/request`.\nIf you don't mind I'd also like to add your example to the SDKs wiki.\n. Thanks, I've added a section to the SDK's wiki on using context with SDK requests.\n. Thanks for letting us know @seiffert I'm working to get the context example added to the dev guide.\n. @seiffert pagination is a little bit more complicated. At first glance I can see the option of treating all pages as a single request, but this easily breaks down for timeout contexts. What should really be done here is a way to provide a new Context for the next page's request to use. But its not immediately obvious how to do this because the request.Request value is not exposed for Pagination functions. It might be possible to address this with the SDK's request handlers, but I think that would require separate service client instances per Pagination operation the user is looking to perform, which isn't a great option. \nI think this deserves a feature request issue if you'd like to create one. I don't think this issue can be solved just with a doc update, but will need some code change along with it.\n. Hi @manugarri were you able to have any success using COPY instead of bulk INSERT for Redshift?\n. Closing this issue for now, please reopen or create a new issue if you are still having problems with this use case.\n. I've just added generating interfaces for each service. With the interface you'll be able to generate mocks using a tool like mockery. The new interfaces can be found in the iface package under each service. If you are wanting to use mocks with the service interfaces you'll need to make sure to use the service interfaces instead of concrete service structs. Here's an example:\n``` go\npackage main\nimport (\n    \"github.com/awslabs/aws-sdk-go/service/s3\"\n    \"github.com/awslabs/aws-sdk-go/service/s3/s3iface\"\n\"log\"\n\n)\nfunc main() {\n    var svc s3iface.S3API = s3.New(nil)\n// With Interface.\nbuckets, err := listBuckets(svc)\nif err != nil {\n    log.Fatalln(\"Failed to list buckets\", err)\n}\n\nlog.Println(\"Buckets:\", buckets)\n\n}\nfunc listBuckets(svc s3iface.S3API) ([]string, error) {\n    resp, err := svc.ListBuckets(&s3.ListBucketsInput{})\n    if err != nil {\n        return nil, err\n    }\nbuckets := make([]string, 0, len(resp.Buckets))\nfor _, b := range resp.Buckets {\n    buckets = append(buckets, *b.Name)\n}\n\nreturn buckets, nil\n\n}\n```\nIn this example I manually ran mockery to generate the \"mocks\" package for the s3 interface. The mocked packages weren't included in this commit, because people might have different ways they would like to mock out the service apis.\n``` go\npackage main\nimport (\n    \"github.com/awslabs/aws-sdk-go/aws\"\n    \"github.com/awslabs/aws-sdk-go/service/s3\"\n    \"github.com/awslabs/aws-sdk-go/service/s3/s3iface/mocks\"\n    \"github.com/stretchr/testify/assert\"\n    \"github.com/stretchr/testify/mock\"\n    \"testing\"\n)\nfunc TestListBucketsWithInf(t testing.T) {\n    svc := new(mocks.S3API)\n    svc.On(\"ListBuckets\", mock.AnythingOfType(\"s3.ListBucketsInput\")).Return(&s3.ListBucketsOutput{\n        Buckets: []*s3.Bucket{\n            &s3.Bucket{Name: aws.String(\"First Bucket\")},\n            &s3.Bucket{Name: aws.String(\"Second Bucket\")},\n        },\n    }, nil)\nb, err := listBuckets(svc)\nassert.Nil(t, err, \"Expected no error\")\n\nassert.Len(t, b, 2, \"Expect two buckets\")\nassert.Equal(t, \"First Bucket\", b[0], \"Expected first bucket\")\nassert.Equal(t, \"Second Bucket\", b[1], \"Expected Second Bucket\")\n\n}\n```\n. Thanks for contacting us @jmassara this task is still in our backlog. We're reviewing how best to implement this in the SDK. Some of the AWS SDKs already support this, and we expect the AWS SDK for Go will also be adding support for querying features, and services once implemented.\n. Getting this feature out is a priority of ours. I don't have a rough ETA when the work will be completed, but we are actively working on this feature.\n. > Why is this a pointer? Maps can be nil without making them a pointer. I can think of very few use cases for a pointer to a map in Go and this isn't one of them. Also, why are maps made into pointers but slices (like AttributesToGet) not?\nd993b41 was just pushed which addresses the map usage in the SDK.  The map have been replaced with just normal maps and should improve the usage of maps throughout the SDK.\n. @dhawal55 Slices should already be by value not pointer. If there are *[] instances that we missed please let us know so we can correct them also.\nAlso I'd like to note that this change did not impact the usage of pointer element types within maps and slices.  The element types are still pointers.\n. Thank you very much for the in depth discussion on this issue. Because of it, we've added conversion utility funcs for []T <-> []_T, and map[string]string <-> map[string]_string. These are available in the aws package. These conversion funcs should help deal with the pain points of working with []_T fields in the SDK. E.g aws.StringSlice will convert []string to []_string. aws.StringValueSlice performs the reverse.\nOn the issue of replacing []*T throughout the SDK with []T, lets continue that discussion in #284. \n. Closing as pointer and value conversion utilities are now available.\n. Thanks for this discussion, we've just accepted the PR from @cbrogli which adds converter helper methods to the service/dynamodb/dynamodbattribute package.  #231 \nClosing this issue now that converting to/from concrete types to attributes has been added. Please let us know if you encounter any issues, questions, or feedback.\n. @radeksimko Thanks for the heads up on the related issues. Adding modeling of policies to the SDK would be a helpful feature and some of the SDKs provide similar support. I think this would be good to add to the SDK.\nI think initially we need to determine and define the definition of the various policy formats. There is a generic base container format for a policy but the internals can vary wildly by service, and some are not necessary compatibile with others. Syncing with how the other SDKs handle policy type definitions will also be helpful.\n. Hi @radeksimko we are still having continuing discussions about how we'd like to represent the types across our SDKs in a consistent way. Potentially using generation, and models to define the types.\n. Thanks for the feedback. This will help us determine which features we implement in the future. Currently Java and to a lesser extend .Net are the only SDKs which support policy generation or typing. Ruby v1 had this feature but it was dropped in v2, due to lack of maintainability.  With that said this feature would be helpful and alleviate the confusion and pain points surrounding creating and specifying AIM policies.\nWe currently do not have a standardized method of annotating, or tracking the nuances of AIM policy customizations that needed for various usages. This information is mostly captured in documentation, but not directly available in an way accessible to code. \nSomething that would help us determine how to implement this feature would be to get a better idea how users would use the utility, and what they would expect it to provide.\n- Since policies very pretty widely from service to service, is there a specific subset that would be more helpful that others to have?\n- How would you use the utility? Would you expect it to be a type that you set fields on, or more of a builder pattern?\n. Thanks for the update @pmoust. We still have this task in our backlog, and having these discussions will help us build the policy models when that work is started.\n. Thanks for the extra information @radeksimko this will help us prioritize the SDK's backlog. I agree it makes sense to prioritize the IAM policies over other policies. In addition thanks for linking to the terraform policy model. I think the model you linked is a good place to start for the SDK to start with. Further down the road it would be great if the SDK were able to generate definitions/validators for various service policies, but that information isn't easily available.\nOut of curiosity, why is the do does Terraform need to normalize the JSON strings? is this to ensure the string is encoded correct?\n. @c4milo sorry to hear that. In #915 there is a discussion on how the s3manager.Uploader can simplify this process for you, and not require the fill file's contents to be in memory.\n. Glad that solution worked for you @c4milo! let us know if you have any additional questions, feedback, or issues.\nAgreed, for the download streaming case the vanilla GetObject is the best tool for streaming download to another source.\nOne thing that might be helpful in your use case is for your service to vend temporary pre-signed PutObject URLs to the clients. and the client uploads the content your S3 bucket it self. In addition for download you could use the similar pattern but redirect the client to a pre-signed S3 download URL.  If your use case doesn't require modification of the content of the stream and have control of how the clients perform the upload this pattern would help reduce the load on your service.\n. @dthuering I think the reason you're seeing an issue is because the ContentType value is being set. This value is required to be a header in the presigned URL. You should be able to get this to work by using PresignRequest instead of Presign . PresignRequest will return a http.Header of the headers signed in the request, that need to be sent when the request is made by a downstream client.\n```go\ninput := &s3.PutObjectInput{\n    Bucket: aws.String(s.Bucket),\n    Key: aws.String(key),\n    ContentType: aws.String(contentType),\n}\nreq, _ := s.s3.PutObjectRequest(input)\nusrStr, headers, err := req.PresignRequest(30 *time.Minute)\n```\nPlease open an new issue and we can investigate the issue you're experiencing further.. In PR #182 I provided an update about retrieving EC2 Metadata to the SDK.\nQuick update for this, I've added an ec2MetadataPreview branch. This implementation takes advantage of the SDK's request retry logic, and is consistent with the existing service clients.\nTake a look at the aws/ec2metadata package.\nI'm currently working through a series of circular dependancies blocking integrating this fully into the SDK. Specifically credentials/ec2rolecreds.EC2RoleProvider should be updated to use the EC2 Metadata service but circular dependencies are preventing this.\n. PR #182, has been superseded by #330 which includes a minor package structure refactor also.\nAny feedback, or additional metadata path helper that would be helpful please let us know.\n. This issue was fixed with PR #330 please let us know if you run into any issues with the metadata parser, or any other feedback.\n. HI @pdalinis in #276 i provide two possible methods for addressing the Config.Merg issues.  using pointers for all primitive or create custom structs for each primitive. Any feedback you would have would be very helpful.  Thanks.\n. Would this be the cause of requests to s3 failing with a 301 Location header missing error for s3 GetBucket requests?  Though ListBuckets responds correctly with the list of buckets. Interestingly requests to GetBucketLocation's response object GetBucketLocationOutput is its zero value.\n. @grahamc , @pdalinis, Sorry for the delay getting back to you on this. We are discussing how best to implement this functionality. We think a custom service client would be created to make requests to the EC2 metadata service. Following this pattern we'd be able to ensure the metadata client would reuse the request, config, and error handling logic the existing services utilize.\nI think I basically need to make a few minor changes to make GetMetaData, GetRegion, and IsInAws as operations to a metadata service client (no need for Input/Output structs, primitives are good), and moving it into a nested package under aws.\n. Quick update for this, I've added an ec2MetadataPreview branch. This implementation takes advantage of the SDK's request retry logic, and is consistent with the existing service clients.\nTake a look at the aws/ec2metadata package.\nI'm currently working through a series of circular dependancies blocking integrating this fully into the SDK. Specifically credentials/ec2rolecreds.EC2RoleProvider should be updated to use the EC2 Metadata service but circular dependencies are preventing this.\n. Hi all I just created a PR #330 which takes the ideas provided here, and updates them to use the SDK's service and request utilities. Providing access to existing retry, and logging functionality.  Apologizes for the size of the PR. A minor package refactor was needed in order to prevent circular dependencies.\nWith this pattern adding additional metadata paths as go funcs should be very quick.\nClosing this PR in favor of #330. Lets take the discussion there if anyone has any feedback.\n. A CloudFront URL Signer was just submitted in #336. Here is a simple example how to sign your own URL using the CloudFront Canned Policy to expire in 1 hour.\ngo\n// Sign URL to be valid for 1 hour from now.\nsigner := sign.NewURLSigner(keyID, privKey)\nsignedURL, err := signer.Sign(rawURL, time.Now().Add(1*time.Hour))\nif err != nil {\n    log.Fatalf(\"Failed to sign url, err: %s\\n\", err.Error())\n}\n. @apremalal i'm not aware of any way to use EC2 IAM roles for signing. CloudFront's signing is all certificate based and doesn't have a way to use AWS Credentials to my knowledge. . Hi @magegu have you tried using only a single goroutine to see if you're able to reproduce the problem that way?  You can also use the \"-race\" go build flag to add race detection to the built executable. This will add panic like logging if goroutines try to access or write data used by another goroutine.\nbash\ngo build -race\nAlso Have you tried setting the Config.LogLevel = 1 ?  This add wire debugging to your output logs, and might shine some light on what is failing.\n. While I wasn't able to reproduce the error you had I was able to see a few race conditions in the SDK.\nI built a simple batch uploader using your sample and I did encounter a few race conditions in the SDK which may be related to the issue you're seeing.\nYou can see the batch uploader here: https://gist.github.com/jasdel/4fd7acda16a7e15d65a2. Is this similar to what you are doing?\nI'll open this back up since we are seeing race conditions in the SDK that need more investigation.\n. Dump of race condition information.\ngo run -race ./s3RaceExample.go -bucket  -files 10 -routines 32 -size 1024\nWARNING: DATA RACE\nRead by goroutine 7:\n  github.com/awslabs/aws-sdk-go/internal/protocol/rest.escapePath()\n      /Users/jasdel/workspace/golang/src/github.com/awslabs/aws-sdk-go/internal/protocol/rest/build.go:172 +0x5b\n  github.com/awslabs/aws-sdk-go/internal/protocol/rest.buildURI()\n      /Users/jasdel/workspace/golang/src/github.com/awslabs/aws-sdk-go/internal/protocol/rest/build.go:121 +0x196\n  github.com/awslabs/aws-sdk-go/internal/protocol/rest.buildLocationElements()\n      /Users/jasdel/workspace/golang/src/github.com/awslabs/aws-sdk-go/internal/protocol/rest/build.go:57 +0x7e7\n  github.com/awslabs/aws-sdk-go/internal/protocol/rest.Build()\n      /Users/jasdel/workspace/golang/src/github.com/awslabs/aws-sdk-go/internal/protocol/rest/build.go:24 +0xeb\n  github.com/awslabs/aws-sdk-go/internal/protocol/restxml.Build()\n      /Users/jasdel/workspace/golang/src/github.com/awslabs/aws-sdk-go/internal/protocol/restxml/restxml.go:17 +0x43\n  github.com/awslabs/aws-sdk-go/aws.(_HandlerList).Run()\n      /Users/jasdel/workspace/golang/src/github.com/awslabs/aws-sdk-go/aws/handlers.go:80 +0x9b\n  github.com/awslabs/aws-sdk-go/aws.(_Request).Build()\n      /Users/jasdel/workspace/golang/src/github.com/awslabs/aws-sdk-go/aws/request.go:107 +0x13f\n  github.com/awslabs/aws-sdk-go/aws.(_Request).Sign()\n      /Users/jasdel/workspace/golang/src/github.com/awslabs/aws-sdk-go/aws/request.go:115 +0x47\n  github.com/awslabs/aws-sdk-go/aws.(_Request).Send()\n      /Users/jasdel/workspace/golang/src/github.com/awslabs/aws-sdk-go/aws/request.go:125 +0x47\n  github.com/awslabs/aws-sdk-go/service/s3.(*S3).PutObject()\n      /Users/jasdel/workspace/golang/src/github.com/awslabs/aws-sdk-go/service/s3/api.go:1622 +0x69\n  main.putFile()\n      /Users/jasdel/workspace/golang/src/aws-sdk-go-examples/s3RaceExample/main.go:90 +0x238\n  main.func\u00b7002()\n      /Users/jasdel/workspace/golang/src/aws-sdk-go-examples/s3RaceExample/main.go:72 +0x178\nRest of race hits https://gist.github.com/jasdel/b0858b5c06cbd88f6a6b\n. Thanks @magegu, I'm able to reproduce the NotImplemented error message you're seeing with 12k files, 32 goroutines, and no waits between upload.  From you're sample code it's worth noting that the sleeps only kick in if their is an error. By the time S3 begins throttling the upload you've already saturated it with connections from the other goroutines.\nI agree with @lsegal adding additional waits between uploads, and/or reducing the number of concurrent uploads should prevent these errors.  I re-ran my test with only 10 upload goroutines, and a 100ms within each routines's loop after the upload. I uploading +60k files without error. I suggest playing around with the wait between goroutine's upload and number of concurrent uploads.  I updated the gist with sample code.\nThere are still warnings of race conditions, but I don't think they have any impact on the issue you are seeing.\n. @magegu are you still running into an issue with the SDK usage across goroutines?\n. Closing this as we've been up able to reproduce the problem.  Please let us know if you're still encountering problems.\n. Both this PR and #182 have not been updated recently.\nWe would like to expose the metadata as a service. Maybe, service/ec2metadata. This would allow us to use a custom endpoint configuration, and API consistent usage with our existing service API landscape.\n. Closing this PR in favor of the discussion on #182. A ec2MetadataPreview branch was added which implements this functionality and is consistent with the existing service clients.\nTake a look at the aws/ec2metadata package.\n. Taking a look at both #204 and #196 to address the issue of retry failing.\n. Hi @richarddbarnett Thank you very much for the PR. Closing this since the changes were pull into #211\n. Hi @sacheendra our updated error refactoring just landed, and will address the issue with this bug.  I've verified that errors will now be exposed as expected.\nOne important bit of information to note in order to be more consistent with Go's expectation of errors, the aws.Error() function was removed, and  the SDK uses the more Go idiomatic method of casting the error to an awserr. The most basic error interface is awserr.Error. It provides the same information as the old aws.APIError structure, but using accessor methods instead of struct fields. Take a look at the awserr package docs for more information.\nExample:\n``` go\nresp, err := s3Svc.ListBuckets(&s3.ListBucketsInput{})\nif err != nil {\n    if awsErr, ok := err.(awserr.Error); ok {\n        // Generic AWS Error with Code, Message, and original error (if any)\n        fmt.Println(awsErr.Code(), awsErr.Message(), awsErr.OrigErr())\n        if reqErr, ok := err.(awserr.RequestFailure); ok {\n            // A service error occurred\n            fmt.Println(reqErr.StatusCode(), reqErr.RequestID())\n        }\n    } else {\n        fmt.Println(err.Error())\n    }\n}\n```\n. Closing this since the error should be exposed correctly now.  Please contact us if you run into any issues.\n. Hi @pwaller thanks for the PR. Did you hit the same error that failed for you previous but in the other context, or a different error due to an error message not being logged?\n. Thanks @pwaller PR looks good, I'm going to go ahead and pull it it.  Please let us know if you encounter other situations like this.\n. Thanks for reporting this. It looks like the godoc.org docs are actually missing a few other services packages as well. I manullay hit the Lambda godocs godocs page, and they are now showing up correctly for the Lamdba service.\nIt looks like some of the service API packages are not being sourced automatically by godocs unless they are explicitly hit by a user. I'll dig into this and see how we can make sure all docs are visible.\n. I just updated the Lambda service API. @pas256 please let us know if you encounter any issue with the updated API. It now matches the AWS online documentation of supported actions. http://docs.aws.amazon.com/lambda/latest/dg/API_Operations.html\n. Thanks for the PR @abustany I am actually in the process of refactoring and cleaning up the Credentials logic/structure which will include caching for the providers.  This refactor should be out soon and will address issues just like the one your PR addresses.\n. Interesting idea @sclasen. At the moment like @leelynne mentions the packages in the \"internal\" path are not intended for general consumption and their surface may change unexpectedly. Initially implemented in Go 1.4 but will be enforced in Go 1.5 any package path containing an \"internal\" element will only be allowed to be imported by a package in the same tree as the internal folder.\nI'd like to hear more about your use case where adding generated code on top of the APIs would be helpful.  I did recently add a little bit of documentation to the function calls, but the process of generating Go code isn't really well documented at the moment. And I don't think we can really consider those internal packages stable yet.\n. Thanks for the PR @euank !  Tagging for review, but looks good to me so far. \nI think e7b9655 to generate io.ReadSeeker for streaming type is still needed #204 improved string support but didn't address the incorrect generated code.\nI think we would like to hold off on 6914ebf hardcoding region for integration test. This would only allow the integration tests to run against a single region, and we'd like to not bind the integration tests to any particular region. What do you think about causing make generate/generate-test to fail If AWS_REGION is not set?\n. Pulling this in, thanks @euank for fixing this up!\n. Fixed in 1a0d8fa04\n. Thanks for the PR @josharian! What do you think about the idea of using a new BeforeRetry handler instead of ValidateResponse handler for resetting the reader?\n. In #196 the r.Retryable = ... and r.RetryDelay logic was moved to a RetryHandler. It might make sense for r.ResetReaderBody to be there instead a BeforeRetry handler.\n. Taking a look at both #204 and #196 to address the issue of retry failing.\n. Thanks @josharian, the script will definitely help.\n. Hi @justinsb I just pushed a change that will make using anonymous credentials support much easier.\nTake a look at the updated docs aws/credentials for details, but the following is all you need to do.\ngo\nsvc := s3.New(&aws.Config{Credentials: credentials.AnonymousCredentials})\n// Access public S3 buckets.\n. Taking a look at the code it looks like Kinesis uses the jsonrpc Unmarshal for reading the response body from the service serializing it into the output structure, but does not close the response body reader. Though I don't think this is the cause of the problem you are seeing.  Locally I added the Body.Close to the jsonrpc used by Kensis, but was still able to reproduce the issue you are seeing with your sample code after the second request.\nChanging the delay between send to 3s and 9s I no longer received any failures.  We need to do more investigation, but I think you might be correct in that this is a situation where the connection is being terminated by the server or client at the perfectly wrong time. \n. I verified this is a Go net/http Client or Transport issue.  Gist: readTimeoutExample.go reproduces the issue outside of the SDK. Where Client will not open a new connection if Client doesn't know the server closed its pooled connection prior to Transport.RoundTrip() writing the request to the pooled connection.\nI'll submit this issues upstream to the Go repo. In the meantime I suggest disabling keep-alive, or alternatively add retry logic for requests which fail with the EOF error.\nGo\n// minimalist Client options to disable keep-alive.\nclient := &http.Client{\n    Transport: &http.Transport{\n        DisableKeepAlives:   true,\n    }\n}\n. Cool, looks like the issue is already reported golang/go#8946 and tagged for Go 1.5Maybe.  Though the current proposed change is limited to HEAD/GET methods at the moment because they are idempotent.\n. You're correct @dpiddy that issue did allow retries to work correctly. Going to go ahead and close this issue.  Thanks for the heads up.\n. Hi @Colin-Murphy no problem at all :) glad digging into the code helped with the issue you are seeing.  Credentials are in a refactor now which should make using and setting credentials much easier and more obvious.\n. bash\ngit fetch upstream pull/211/head:PR_211_fixRetries\ngit checkout PR_211_fixRetries\nwhere upstream is the remote: github.com/awslabs/aws-sdk-go, and PR_211_fixRetries is the local branch name to checkout this PR to.\n. The biggest argument I could see against using a pointer versus a type which knows if its wrapping value was set, is that with a pointer you must do nil checks before using the type's value. \nI don't think we plan on using triState itself from flag, but the idea I think is very useful in our config state. The idea we are thinking that a TriStateBool Get() will return either the zero value, or value set.  If the user of TriStateBool cares if the value was actually set IsSet() can be used.\nIt is my understanding that a pointer receiver means the object reference will be used and pointer to it will be provided to the function.  Whereas not using a pointer receiver a copy of the receiver is made prior to being given to the function. This is why modifying a structure's property with a value receiver will not update the original structure's property the way you'd expect. e.g: http://play.golang.org/p/mNQxoUjGkM \n. Thanks for the PR @mondok I merged this in after making a small tweak to add a new line between the line and environment variables.\n. Hi @michaeljs1990 initially i think the retryDownload is failing because \ngo\nRange:  aws.String(\"bytes=\" + strconv.FormatInt(*obj.ContentLength, 10) + \"-\"),\nproduces a range request header value like:\nbytes= 6665384021-\nThis in incorrect when retryDownload is called for the first time due to downloadFileFromS3 encountering an error.  The value that should be used in the number of bytes written to your file.  So that the retryDownload starts off where the connection error occurred.\nAlso I'd suggest updating retryDownload to take the f file reader, and length of previous copy instead of s3.GetObjectOutput pointer.  This will ensure that the correct bytes will be requested.  This also prevents the need for the bufio and flushes if io.Copy is used instead.\n. Awesome thanks for the update, glad you got it working!\nA managed downloader is something that would be very helpful to have and would go nicely with the managed uploader that was recently just committed.\n. Hi @luck02 just pushed an update which refactors the credentials.\nThe aws/credentials EC2RoleProvider should help in understanding how the IAM Roles for EC2 instances can be accessed with the SDK.\n. We recently pushed the refactored error handling within the SDK.  The SDK's error handling should be more consistent now. Internally this was fairly large, but externally the impact should be minor.\nIn order to be more consistent with Go's expectation of errors, the aws.Error() function was removed, and  the SDK uses the more Go idiomatic method of casting the error to an awserr. The most basic error interface is awserr.Error.  It provides the same information as the old aws.APIError structure, but using accessor methods instead of struct fields. Take a look at the awserr package docs for more information.\nExample:\n``` go\nresp, err := s3Svc.ListBuckets(&s3.ListBucketsInput{})\nif err != nil {\n    if awsErr, ok := err.(awserr.Error); ok {\n        // Generic AWS Error with Code, Message, and original error (if any)\n        fmt.Println(awsErr.Code(), awsErr.Message(), awsErr.OrigErr())\n        if reqErr, ok := err.(awserr.RequestFailure); ok {\n            // A service error occurred\n            fmt.Println(reqErr.StatusCode(), reqErr.RequestID())\n        }\n    } else {\n        fmt.Println(err.Error())\n    }\n}\n``\n. Also one additional note that might be helpful. If you are printing the error viaerr.Error()` the output should be more useful now and include both code and message, along with any additional information about underlying original errors such as request or io errors.\nAs for constants this is something we'e are looking into how best to expose. \nClosing this issue and we can use #155 #158 to continue our discussion on constants.\n. Thanks a lot for the PR @oremj looks good. Before accepting I'd like to verify that this is the expected behavior with all services and not an S3 special case.\n. LGTM\n. Hi @justincampbell thanks for the PR. For now we would like to hold off including the Zone ID to Region mapping directly in the SDK. We currently do not have a good method of generating this mapping to ensure the list doesn't become outdated, which is inline with the other AWS SDKs.\n. Thanks for taking the time to create this PR @cbroglie ! This would be a very helpful set of utility functions to convert user types to and from AttributeValue collections. Since these functions seem to be more about conversion of types not marshaling to/from the wire I think it would make sense for us to rename the Marshal and Unmarshal to ConvertTo and ConvertFrom respectively.  We could then put the conversion functions into a nested package under service/dynamodb.\nAn additional note AttributeValues returned from dynamodb can also be scalars and slices in addition to maps.  Because of this, it would make sense to create three versions of the ConvertTo/ConvertFrom methods for Map, List, Scalar.\nFor example the following set of methods could provide a simple interface for developers to convert to and from attribute maps returned by the SDK.\n``` Go\n// Convert To AttributeValue collection from user type.\nfunc ConvertToMap(v interface{}) (item map[string]AttributeValue, err error) {...}\nfunc ConvertToList(v interface{}) (item []AttributeValue, err error) {...}\nfunc ConvertTo(v interface{}) (item *AttributeValue, err error) {...}\n// Convert from AttributeValue collection to user types.\nfunc ConvertFromMap(item map[string]AttributeValue, v interface{}) (err error) {...}\nfunc ConvertFromList(item []AttributeValue, v interface{}) (err error) {...}\nfunc ConvertFrom(item *AttributeValue, v interface{}) (err error) {...}\n```\n. Thanks for putting this update in @cbroglie! I'll take a look at the changes. \n\nservice/dynamodb/converter.go:313: cannot use &mp (type _map[string]_AttributeValue) as type map[string]_AttributeValue in assignment\nservice/dynamodb/converter.go:386: invalid indirect of a.M (type map[string]_AttributeValue)\n\nI think this might be caused just because the address of a map is being taken and set to a.M instead of just the map value.\n. Looks good @cbroglie I made a few tweeks to the errors handling to use our awserr.Error type, but other than that looks good thanks!  If there is a need we can adjust the json decoder/encoder later.\n. Thanks again for getting this started. Closing now that this changes are pushed.\n. No problem, we are always glad to receive PRs.\n. Hi @anacrolix going to close this issue. Please let us know if you're having any issues with Copy, or anything else.\n. Hi @cmdrkeene thanks for reporting this. You are correct, this is a case where the of the definitions just need to be updated.  I'll get started on pulling the updates in.\n. Hi @cmdrkeene I just pushed the updated dynamodb updates for KeyConditionExpression. You should be able to use the new functionality now.\n. Marking as closed, let us know if you have any issues.\n. Hi @abourget  Just wanted to let you know the error refactoring had landed, and making a PutObject operation with an incorrect region no longer yields and empty output with no error.\nI verified locally that the following error is returned from the operation logged via err.Error():\ntext\nAuthorizationHeaderMalformed: The authorization header is malformed; the region 'us-east-1' is wrong; expecting 'us-west-2'\n    status code: 400, request id: []\nSee the awserr package documentation for more information on usage of the awserr.Error.\n. Closing since this issue should now be fixed. Please let us know if you encounter any issues.\n. HI @justincampbell thanks for reporting this. Looks this issue is related to the response coming back as the policy's JSON format, but the SDK's S3 unmarshaller is expecting XML. Need to dig a bit deeper but looks like we may just need a custom unmarshaller for the GetBucketPolicy operation.\n. @justincampbell You should now be able to retrieve the bucket policies using the SDK. Let us know if you run into any other issues.\n. Hi @suzuken Thanks for creating a PR. We are actually going to go with the pattern of adding a stub go file to the aws-sdk-go's root to prevent the error.\n. I just pushed a change 15323f6f which will address this issue with a stub.go file in the root of the project.  Thanks again for bringing this issue up!.\n. Hi @squirkle thanks for the feedback.  I don't think awserr.Error type shouldn't have any issue assigning to an error type value since awserr.Error is an interface which can satisfy the generic error interface. But I'd love to see an example if that is the case. Though, It would introduce an issues if other function calls returned an error in the same scope and the value was set to the same err an SDK awserr.Error was set to.  That would cause a conflict because err is of type aws.Error interface, not generic error.\nto use @b6g's example\ngo\noutput, err := s3manage.Upload(svc, input, opts)\n_, err = io.Copy(dst, src) // * we cannot do this *\nBecause of this, I'll make an update which switches the return types back to error, but keep the underlying interface awserr.Error which can be cast to in order to retrieve extra information.\n. With Code being so important for determining how to handle errors which occur does that convenience outweigh the annoyances of needing to use var err error when it conflicts with other returned errors within the same scope?\nI can definitely see using a different variable name for err being a potential cause for hard to track down bugs when err was set prior to the SDK call, and err is mistakenly returned instead of the otherErr variable the sdk returned.\n. Just saw your comment in the commit @b6g makes sense, thanks!\n. Just pushed the update which will address this issue. Let us know you run into any issues with the update, or any more feedback\n. the change I just pushed should allow you to use all of the s3manager UploadInput parameters.  Please let us know if you have any issues.\n. Hi @rlcomte could you post your wirelogs of this request by modifying your dynamodb service initialization like the following:\ngo\nsvc := dynamodb.New(&aws.Config{LogLevel: 1})\n. If the error is ValidationException: The provided key element does not match the schema This implies that there is information missing in the GetItemInput.\nWas the groups table created with Key as the Hash Key, and does the groups table also have a Range Key?  If the table has a range key in addition to a hash key both must be specified.\n. Hi @pungoyal the AWS Console allows querying tables with a primary key (aka hash key), and optionally an additional sort key value.  Under the hood the AWS console will build DynamoDB requests internally to query for record with that key.. @pungoyal Sorry I think I miss understood your question. Are you looking for how does the AWS Console query for items in a DynamoDB table using a hash key? Query is how this would be done. Sorry for confusion, when I said internal i meant internal to the AWS Console's webpage logic. not an internal API.. Hi @pmoust Thanks for posting this. I think the reason you're seeing the failure is because err is nil.  I looked back over my example code and I made a typo. The error example should of been wrapped in a nil check before using the error.\nI think if you just add the err nil check to your error handling and the code should no longer panic.\ngo\nif err != nil {\n    if awsErr, ok := err.(awserr.Error); ok {\n        // Generic AWS Error with Code, Message, and original error (if any)\n        fmt.Println(awsErr.Code(), awsErr.Message(), awsErr.OrigErr())\n        if reqErr, ok := err.(awserr.RequestFailure); ok {\n            // A service error occurred\n            fmt.Println(reqErr.Code(), reqErr.Message(), reqErr.StatusCode(), reqErr.RequestID())\n        }\n    } else {\n        // This case should never be hit, The SDK should alwsy return an\n        // error which satisfies the awserr.Error interface.\n        fmt.Println(err.Error())\n    }\n}\n. I went back and updated the previous issue comments where the same code was mentioned.  I'm unable to update the commit message regrettably. The godoc for awserr has been updated as well.\n. Great, I've also updated the generated sample code with the correct error checking. 29717a7\n. May be able to use aws/awsutil: Copy. It copies one structure fields to another's where they match by type and name.\n. I just pushed an update which merges @alexaandru's change with a slightly refactored awsutil.Copy to allow coping between different struct types.\n. @alexaandru thanks again for getting this started and submitting the PR, going to close this as the issue should now be resolved.\n. Correct @squirkl, the request should be retried if the service returned a response with \"ExpiredTokenException\" error code. Is this exception code you are receiving?\nLooking at the expired creds request retry test case, it looks like there might be a bug around the AfterRetry request handler. The test case was doing a bit more mocking that it should, and may of been hiding the bug.\n. After removing the extra code from the expired creds request test it doesn't look like the issue you're seeing is related to the AfterRetry request handler after all.\nAre you seeing the same request being retried multiple times with expired credentials, without refreshing them or is the request not attempting to retry at all, and just returning an error?\n. I think I've identified the problem, and in am in the process of verifying the fix.\nThe SDK only considered a single error code when determining if credentials should be refreshed, but there are two other codes that were not being checked.\n. Hi @squirkle A fix for this bug has been pushed and you should no longer be encountering any more expired token errors.  Please let us know if you run into any further problem.\n. Thanks for the feedback @squirkle are you seeing the same error message or a different one?\n. Great thanks for the update!\n. LGTM\n. Hi @purohit thanks for reporting this. From the stack trace it looks like the service c pointer is nil.  Were you able to reproduce this with any other service or operations?\nHow is your code initializing the EC2 service struct? New() is the best method to create and initialize a service before it is used. Calling a method on a nil *struct actually won't panic until something on that struct is accessed. Example: http://play.golang.org/p/8BnCwUiECt\nIs it possible when an error is received the code uses an incorrect or uninitialized service instance?\n. @purohit you can also check if you're using a var svc *sqs.SQS anywhere which may not get set to a value. This also will have the same result if a struct field was never initialized.\n. hi @purohit I'm not sure how you're seeing this bug. There is no condition in which ec2.New would not return a valid pointer.  Is there any chance you are trying to use client within a goroutine, and access it from outside that goroutine?\n. a single service client can be used with multiple goroutines.  The important condition is that the client be created prior to any of the goroutines use it.  I would suggest adding -race to your go build command and running your application.  It may be possible that there is a race condition between your goroutines and where client is created.\nhttps://golang.org/doc/articles/race_detector.html\n. Hi @sanathp would you mind opening a new issue with the panic you're seeing. In addition could you also please include the version of the SDK you're using along with the version of Go. Also if you are able to reproduce the error please include a script to help us debug the issue you're seeing.\n. thanks for the update. Let us know if you run into any issues with the SDK or have any questions that we can help with.\n. I just pushed a changed which converts the *map to just map. This should improve the usability of maps throughout the SDK.\n. Thanks for posting this @sclasen. An additional place this pain point is returning awserr.Error compatible error when using custom credentials providers.\nIf we limit to the Code/Message/OrigErr fields then the New(...) constructor could be exposed via aws/awserr package. Though the awserr.RequestFailure interface would still present a pain point for the same reasons as awserr.Error. A awserr.NewRequestFailure(...) Would also make sense.\n. Thanks for suggesting this feature @sclasen. Now that the change has been pushed I'm going to go ahead and close this issue.  Please let open an issue, if you have any further feedback, or issues.\n. Thanks for reporting this @mennis Looking into why this broke.\n. Thanks @mennis this issue is now fixed, and the build should no longer be failing. Please let us know if you encounter any further issues.\n. Thanks for the suggestion @mhat, tagging this as a feature request.\n. #697 is the work in progress implementation of this feature request.\n. This feature has been implemented in release v1.4.0 please let us know if you have any additional feedback, or run into any issues. Thanks!\n. Thanks for the request @3XX0. At the moment there is not a Mechanical Turk Go SDK.\n. Since Mechanical Turk is not an AWS service we don't plan to support it within the AWS SDK for Go. Though I suggest contacting Mechanical Turk to request a Go SDK for that service.  \nhttps://requester.mturk.com/contactus\n. Thanks for the feedback @foresmac. Improving documentation is one of our goals. Are there any specific types of transcoding jobs/pipelines or other operations examples would be the most helpful?\nIn the meantime using the transcoder console to create test jobs would be helpful since the SDK operation input parameters match the console pretty closely in this case.\n. Sorry, ElasticTranscoder does not have a method to extract the pipeline/job config from the console.\nIf you'd like to give feedback requesting this feature you can do so on the ElasticTranscoder Console page. In the bottom left corner is a black and white \"Feedback\" button.\n. Good catch, thank you for submitting the PR @mrekucci. Looks good, going to merge this in. Great to have the extra unit tests.\n. Good find!\n. Thank you very much for your feedback. The design of pointers, style, and calling method is complex and  very important to the long term maintainability of the SDK. We have a few issues which are duplicating each other and I'd like to collapse the discussion down to a single issue. Lets use #363 for further discussion.\nDuplicates: #363\nRelated: #284, #294, #306\n. Thanks for taking the time to create a PR @mateusz.  STS credentials provider is something we were definitely wanting to add. I'm not sure about having the Provider in the sts package.  Ideally it would nice for credentials providers to be together, but like you mentioned the circular dependence prevents STS Provider being in the credentials package. Because of this, putting credentials providers like this as a subpackage under aws/credentials would make sense. How about: aws/credentials/stscreds.\nIn PR #270 I added a shared struct Expiry which can be used by providers to implement the IsExpired for credentials.Provider interface. Take a look at how this modified EC2RoleProvider for its usage.\n. Thanks @mateusz !  you're correct the test failure is an unrelated timing issue. \n. Hi @mateusz  thanks for the PR, i just merged it in with a few doc tweaks, 35fecd3\n. @clbanning Interesting proposal, but I'd be hesitant to suggest using unsafe, because if the order of the parameters of the awsImage are changed in anyway or new parameters are added it will produced unexpected results by overlaying myImage.\nhttp://play.golang.org/p/X-nWQhnXod // Adding an ID field produces an unexpected result.\n. It would be a good feature for the SDK to allow (un)marshalling JSON blobs into the SDK API types. I think the SDK would need to expose a custom JSON marshaller the SDK could use to unmarshal a JSON io.Reader or []byte to a SDK shape\n. Thanks for reporting and submitting a PR for this issue @phinze. I can definitely see how a non url.Error request error would cause this.\n. After reviewing the change and error condition more.  I think it makes sense to use a similar pattern to how url.Error is handled by setting the HTTPRequest object in the case of an error.  HTTPRequest was actually being set to a valid object in most error cases, but in the case you're experiencing it looks like req is nil.\nI can add this locally to your PR, and I'll investigate why travis if failing. Thanks!\n. Thanks for the PR @attilaolah , looks good pulling it in.\n. HI @radeksimko thanks for reporting this.  the EC2 VPC Flog log support was  added in v0.6.2 release. https://github.com/aws/aws-sdk-go/tree/v0.6.2\n. going to go ahead and close this issue since the service is updated.  Thanks let us know if you have any problem with it.\n. HI @diptanu going to close this issue. Please let us know if you have any questions or feedback that we can help with.\n. Hi @avitex What method of watching for S3 uploads were you thinking?  Were you looking to monitor uploads via the s3manager.Uploader or just S3's PutObject?\n. @avitex an io.Reader will only allow you to monitor when a part is prepared for upload to S3, not the upload progress it self. I think this feature request would require the s3manager.Upload to expose some functionality to track the when individual parts have completed their upload, or some kind of overall progress counter.\n. Hi @minervadata, thanks for your support of this issue. This issue is still in our backlog, and would like to implement this feature, but have not started work on it yet. We're also always more than glad to review PRs if anyone is looking to add this feature to the s3 manager. In addition we can discuss the design of the feature here too, to help the implementation.\n. Hi @krishnakhandagale thanks for voting for support for this feature. This work is in our backlog and hand not be started yet. We're glad to review PRs that look at adding progress this issue though.  It would be good to have a design chat prior to a PR to hash out how progress could be exposed.. Documented the functionality via the package doc along with examples.\n. Thanks for pointing that out @pkazmierczak. I updated the wiki to use a  bytes.Reader instead of a bytes.Buffer. Please let us know if you find anything else that can be improved!\n. We've investigated if we can switch the []*T to []T, but found instances where a service could return sparse arrays in operation responses. Because of this we're unable to remove pointers generically from slices and maps since the service response may have sparse arrays and map elements values which are null.\n. Thank you very much for your feedback. The design of pointers, style, and calling method is complex and  very important to the long term maintainability of the SDK. We have a few issues which are duplicating each other and I'd like to collapse the discussion down to a single issue. Lets use #363 for further discussion.\nDuplicates: #363\nRelated: #265, #294, #306\n. Hi @xunchangguo this feature had not been added yet. It is still an outstanding item in our backlog. We're more than glad to discuss and review PR for feature requests for items such as this too :)\n. Thanks for opening this issue @mischief, it looks like the SDK is not extracting the error code correctly from the response.\nJust incase it wasn't already known, If you're still encountering this error the request is being made being made to the wrong region.\n. @mischief would you mind adding a little bit more information about how you were able to get this exception?  I see how our Error Unmarshaller will fail with that response from EC2, but I'm having a little bit of difficulty reproducing it.  How does your code set the Config's Region, and is it setting an Endpoint also?\n. This bug looks to be caused by the S3 bucket you're using is in a different region than the region you're trying to import to. Is your code providing the UserBucket(bucket/key) or URL to ImportSnapshot? If you're using the UserBucket bucket/key pair the operation may work if you provide a URL instead. Since the URL can specify the region of the bucket you're trying to import from.\n. Thanks for the update, I agree this is a bug that the error unmarshaller should be smarter to handle multiple error response formats. \n. @mischief if you don't mind was the wrong region being set on the service config or the s3 URL?\n. @mischief Also would you mind posting the full request and response output including the endpoints the request is being made to?  Please remove any unique private information prior to posting. This will help us address the issue for all SDKs.\n. HI @mischief I'm going to close this issue since we haven't been able to reproduce it or find a configuration condition which would cause the issue.  Please let us know if you're able to reproduce this, any any steps or configuration needed to do so.\n. Hi @r03 thanks for bringing up this idea, but I think adding this would add confusion when using the SDK. Right now since the s3manager and s3.PutObject both operate on io.Readers, not os.File directly it would not be able to use mime.TypeByExtention, without special case interface casting, and Uploader's functionality wouldn't be consistent if the file if wrapped in some other io.Reader such as gzip or encryption.\n. Hi @rchakra1 each service's Handlers contain a Sign() handler which can be replaced. A straightforward way to replace the signer with a custom one would be:\ngo\nsvc := s3.New(...)\nsvc.Handlers.Sign.Clear()\nsvc.Handlers.Sign.PushBack(aws.BuildContentLength)\nsvc.Handlers.Sign.PushBack(custom_signer)\nWhere custom_signer is a function pointer with the type of func (r *aws.Request) , and would be the signer logic you need in order to communicate with the custom endpoint.\n. @rchakra1, sorry the AWS SDK for Go does not have a util sign with v2 method. The SDK only includes util for v4 signing. With that said, the process to sign a request using v2 is well documented in Signature Version 2 Signing Process\n. Glad to help. Going to go ahead and close this issue. Please let us know if you have any questions, or anything we can help with. \n. Hi @shamiq  closing this issue  since it looks like your issue has been resolved.  If not please reopen, or create a new issue if you're having any other issues with request pre signing\n. Thank you for the feedback, I updated the OP to clarify our commitment to maintaining API major version stability after release. Apologizes for not making that as clear as it could be.\n@conslo I took a look at your gist, it seems similar to the idea proposed by @pges where a builder like pattern would be used to construct the request parameters. This is an interesting idea. I think we would loose our compile-time validation and sanity checking provided by the compiler though. Compile-time checking is one of the strongest benefits we get from request structure initialization. Please let us know any additional thoughts or ideas you have though. We are always interested in better ways to implement the SDK that are maintainable, and won't break API stability when the services add, or make minor changes to their API fields.\n@mitchellh On your point about crash surface area being an issue since pointers are commonly used, would helper functions which return the primitive's value, or zero value if the pointer is nil be helpful? I have been playing around with it in the ConfigMergeViaPointer branch. aws.StringPtrValue() would return the value pointed to by *string or the zero value.\nIn addition to this would adding String() and GoString() to the service Input/Output structs simplify the usage of log.Printf(\"%#v\") even more? With this you would not need to to call aws.StringValue() directly. [addStringers](https://github.com/jasdel/aws-sdk-go/blob/addStringers/service/s3/api.go#L1579) branch of my fork experiments with this idea.\n. To clarify the builder pattern we would be considering would not provide common job helper funcs due to the size and breadth of the sdk. Common helper functions would not be maintainable. Even if we provided helper functions for required parameters per operation there are many cases where the common usage would be to also provide optional parameters. For examples3.PutObjectis commonly used withACL` settings in the same API call. Or even worse if a required parameter changes to optional the SDK would need to make a breaking change, and this is something we want to avoid.\nTo expand on the examples provided by @lsegal using Elastic Transcoder to create a Pipeline using the builder pattern would look something like the following. Disappointingly go fmt will remove extra indentions added to WithThumbnailConfig(&elastictranscoder.PipelineOutputConfig{}'s nested builder methods. Though i think the formating is a bug in go fmt due to nested builders not being as common.\n``` go\nclient := elastictranscoder.New(nil)\nparams := &elastictranscoder.CreatePipelineInput{}.\n    WithInputBucket(\"InputBucket\").\n    WithOutputBucket(\"OutputBucket\").\n    WithName(\"PipelineName\").\n    WithRole(\"RoleToUse\").\n    WithThumbnailConfig(&elastictranscoder.PipelineOutputConfig{}.\n    WithBucket(\"ThumbnailBucket\")).\n    WithNotification(elastictranscoder.Notifications{}.\n    WithCompleted(\"SnsCompletedTopic\").\n    Witherror(\"SNSErrorTopic\"))\nresult, err := client.CreatePipeline(params)\n// ...\n```\nThe same example using the struct initialization method:\n``` go\nclient := elastictranscoder.New(nil)\nparams := &elastictranscoder.CreatePipelineInput{\n    InputBucket:  aws.String(\"InputBucket\"),\n    OutputBucket: aws.String(\"OutputBucket\"),\n    Name:         aws.String(\"PipelineName\"),\n    Role:         aws.String(\"RoleToUse\"),\n    ThumbnailConfig: &elastictranscoder.PipelineOutputConfig{\n        Bucket: aws.String(\"ThumbnailBucket\"),\n    },\n    Notification: &elastictranscoder.Notifications{\n        Completed: aws.String(\"SNSCompletedTopic\"),\n        Error:     aws.String(\"SNSErrorTopic\"),\n    },\n}\nresult, err := client.CreatePipeline(params)\n// ...\n```\n. Improving our documentation and examples would go a long way in addressing pain points new beginner customers experience. Through this we think we can greatly improve the experience using the SDK for both new and experienced users. The SDK it self targets long term usability and maintainability, because after a few weeks or months of using the SDK a user will no longer be a beginner and will want or need more flexibly and customizability in how they use the SDK.\nWith services with a static unchanging API using the idea of function helpers with positional arguments would make perfect sense. This is not the case with AWS services. The services all iterating, adding new features, and extending the capability of existing features. While AWS services are dedicated to backward compatibility, a SDK focusing on specific current use cases would be introducing breaking changes often.\nOne of the biggest downside about function helpers like this is that as the API grows these function helpers start to develop holes as parameters become optional, or alternative new fields become the preferred way of using the API.\nAs an example using a fictitious service Foo. Foo starts out with an API operation Bar. Bar requires a JobID and can be given an optional Name field also. The overwhelming common use case is to provide name also even though it's optional. The generic form of foo.BarRequest(p *foo.BarInput)(*BarOutput, err)) would still exist, but would not be commonly used.\ngo\nfoo.Bar(id JobID, name string) (*BarOutput, err)\nThis works great until Foo's Bar operation is updated to instead of taking a Name string an Identiy struct could be provided. Which creates a conflict with the current SDK. In order to not create a breaking change additional method needs to be added.\ngo\nfoo.Bar(id JobID, name string) (*BarOutput, err)\nfoo.BarWithIdentity(id JobID, ident Identity) (*BarOutput, err)\nNow the capabilities of Identity have been expanded to include information about the job so explicitly passing JobID is no longer required. This puts the users in a situation where operation functions start having holes in them because nil would be provided for jobID. To workaround users needed to use nil parameters in the function additional forms of the function would be created just for Identity.\ngo\nfoo.BarWithOnlydentity(ident Identity) (*BarOutput, err)\nWith these changes users are faced the conundrum of do they update the application to the new form of bar.BarWithOnlyIdentity or do they simplify and switch to the basic method of foo.BarRequest(p *foo.BarInput)(*BarOutput, err)) to reduce churn.\nAs the service APIs evolve and grow the surface area of the SDK would grow exponentially quickly become a mess of helper functions for slightly different use cases. Some of which, may no longer be preferred, or even wise to use.\n. Thank you very much for your feedback. The design of pointers, style, and calling method is complex and  very important to the long term maintainability of the SDK. We have a few issues which are duplicating each other and I'd like to collapse the discussion down to a single issue. Lets use #363 for further discussion.\nDuplicates: #363\nRelated: #265, #284, #306\n. Hi @r03 are you able to reproduce this with any other endpoint, or simple Go program?  This sounds like a local connection issues, since the connection failure is not consistent. I doubt gb is causing any issue, but it may be helpful to try the program using the standard Go build environment.\nIt may be helpful to take a look at the wirelog produced by the SDK. A simple request to experiment with this is:\ngo\nsvc := s3.New(&aws.Config{Region:  \"eu-west-1\", LogLevel: 1})\nresult, err := svc.ListBuckets(&s3.ListBucketsInput{})\n. Hi @lukaf thanks for finding this bug.  I just corrected the doc example. Please let us know if there are any other issues you find, or any questions we can anwser.\nThe example for NewChainCredentials should not of referenced creds.Retrieve() that is a Provider method, and NewChainCredentials returns a Credentials object.\n. Hi @mutuelinvestor are you use the the SDK to create pre-signed URLS? If so the Request.Presign() method will return the pre-signed URL. If you're still encountering problem you might be running into #94\nIf you are looking to pre-sign the URL yourself the documents linked are a good resource to start with.\n. Hi @mutuelinvestor going to close this issue, Please reopen or get back to us if you're still having issues with your requests.\n. At first glance I think there might be an issue with the iterator *string being given to GetKinesisRecordsAndUpdateIterator Could the iterator be set by another goroutine or set in a loop?\nI know this can sometimes cause unexpected results and might be the cause of the expired iterators\n``` go\nfor i := 0; i < 3; i++ {\n    go func() {\n        fmt.Println(\"Got\", i)\n    }()\n}\n// outputs\n// Got 3\n// Got 3\n// Got 3\n```\nhttp://play.golang.org/p/eqW58QXz5x\nIn a situation like this a new variable needs to be used within the loop.\ngo\nfor i := 0; i < 3; i++ {\n    myIdx := i\n    go func() {\n        fmt.Println(\"Got\", myIdx)\n    }()\n}\nhttp://play.golang.org/p/E9e7g6NxMy\nI'm not sure about the connection reset by peer issue. Is this running on a local system or EC2 instance? I'm ask because GetKinesisRecordsAndUpdateIterator took over 5 minutes to return. Though the other connections are completed very quickly \n. Hi @fantyz have you been able to reproduce this issue with a simple single goroutine test app?  Also I'd be curious your SDK client is retrying the failed connection multiple times with exponential backoff before exhausting all retry changes. This might shed some light in what is going on. I would suggest enable SDK logging. Using the aws.Config's LogLevel value above 0 will enable request logging.\ngo\nsvc := kinesis.New(&aws.Config{LogLevel: 1})\n//... use the service.\n. Hi @fantyz With our latest changes to aws.Config the best way to see the request and response wire data.\ngo\nsvc := kinesis.New(aws.NewConfig().WithLogLevel(aws.LogDebug))\n// ... use the service.\n. Hi @fantyz did you get a chance to gather log outputs when you were encountering this issue?  \nI'm going to close this issue, please reopen if you are still encountering this problem.\n. Hi thanks for the updated information @fantyz, and the sample and output logs. The section that is marked as <skipping body> in the log is cut out by you correct? It would be very interesting to see the output response headers at status code. From your description it sounds like the SDK knows that an error occurs but it doesn't know how to handle this error.\nI'll try to reproduce this on my end. Are you using Kinesis Streams or Firehose? In addition about how many records do you have in your stream, and about how large are the records?\n. Thanks for the update and additional info @fantyz. Updating this issue to a bug, because the SDK should at least provide more information about the error causing the retry. I'll work to see if I can reproduce this on my end using a similar setup to what you described.\nCopying from our gitter discussion:\njson\n{\"__type\":\"ProvisionedThroughputExceededException\",\"message\":\"Rate exceeded for shard shardId-000000000003 in stream brim-prod-analytics under account xxx.\"}\nProvisionedThroughputExceededException is enumerated as a throttled exception, which overrides the fact that the server's response is a 400 status code. By default the SDK only retries 5xx status codes. Unless the error code overrides the request to be retried.\nWhen your code receives a ThroughputExceededException error is the HTTP status code a 400 also?  If so the SDK needs to add ThroughputExceededException to our list of throttle exceptions. This will tell the SDK it is allowed to retry that request.\nThe delay you're seeing with GetRecords may be related to a Provisioned Throughput set too load for your use case.\n. In the case of the eventual successful request that was delayed, was retry logging enabled? If not I'm thinking that the request were just being throttled and retried multiple times. The alternative would be that Kinesis is holding the connection open. and delaying a response. But this delay would be limited by the Go default HTTP timeouts of about a minute. \nIf you can provide me with a request ID I can forward this on to the Kinesis service team. In addition it might be helpful to ask on the Kinesis  AWS Forums, since others may of experienced issues similar to this.\n. The base aws.LogDebug will log each request/response made including retries. The biggest difference between LogDebug and LogDebugWithRequestErrors is the error will be logged along with if the request will be retried. I agree it sounds like the requests aren't being retried in this case. \nThe debug responses you're seeing do not include the HTTP body, and LogDebugWithHTTPBody is not set as the log level, correct? The first thing that comes to mind is that the SDK is waiting for the response to download from the Kinesis endpoint.  The GetRecords call won't be able to return until the full response body is downloaded and parsed. Are you able to see the Content-Length of the responses? From the logs listed above it also looks like the connection is being closed/broken preventing download of the full response in some cases.\nThe request id should be visible in the logged response's HTTP Headers. With the request ID I can work with the Kinesis team to hopefully investigate why the request is taking so long to download.\nIn addition how many shards is your stream set for?\n. I'm looking into reproduce the delay your seeing but not yet able to. I created a scripts to put data to Kinesis, and retrieve it.\nIn my tests the retrieves are taking about 3s on average. I haven't seen the delay yet, but will let it run for a while.\n. Hi @fantyz I let my test run overnight, but no luck reproducing the issue. The longest delay I found was 22s. \n. Thanks for the update @fantyz I've forward this information on the the Kinesis team to get their insight.\nOne thing that we could try is to add a custom io.Rearder into the pipeline of unmarshing these responses. The idea here is that we can log the bytes read to hopefully identify if the hang up is in receiving the bytes across the wire, or in the SDK's unmarshaler.\nI've updated the kinesisStream.go gist example including this extra debug. It adds detailed logging to the GetRecords request and adds a DebugReadCloser wrapping the HTTP responses's Body that will log each block of bytes that are read from the HTTP body stream.\nThis will produce output like the following. All non-read times are relative to the time the GetRecords time was started at. \nStarting GetRecords build/sign request, took 156.776\u00b5s\nFinished GetRecords Send, took 840.307398ms\nStarted GetRecords Unmarshal, took 840.367146ms\n2016/07/08 15:59:55 DEBUG: read 512 bytes, took 56.036969ms\n2016/07/08 15:59:55 DEBUG: read 1024 bytes, took 1.007\u00b5s\n2016/07/08 15:59:55 DEBUG: read 2048 bytes, took 455ns\n2016/07/08 15:59:55 DEBUG: read 512 bytes, took 1.106\u00b5s\n2016/07/08 15:59:55 DEBUG: read 3584 bytes, took 3.127\u00b5s\n2016/07/08 15:59:55 DEBUG: read 512 bytes, took 492ns\n2016/07/08 15:59:55 DEBUG: read 7680 bytes, took 1.318\u00b5s\n2016/07/08 15:59:55 DEBUG: read 512 bytes, took 917ns\n2016/07/08 15:59:55 DEBUG: read 708 bytes, took 11.502\u00b5s\n2016/07/08 15:59:55 DEBUG: read 15164 bytes, took 121.372523ms\n...\n2016/07/08 16:00:08 DEBUG: read 16384 bytes, took 2.966015ms\n2016/07/08 16:00:08 DEBUG: read 1024 bytes, took 12.278\u00b5s\n2016/07/08 16:00:08 DEBUG: read 16384 bytes, took 2.600777ms\n2016/07/08 16:00:08 DEBUG: read 1024 bytes, took 96.543\u00b5s\n2016/07/08 16:00:08 DEBUG: read 16384 bytes, took 12.09666ms\n2016/07/08 16:00:08 DEBUG: read 1024 bytes, took 40.412\u00b5s\n2016/07/08 16:00:08 DEBUG: read 16384 bytes, took 2.655483ms\n2016/07/08 16:00:08 DEBUG: read 1024 bytes, took 17.227\u00b5s\n2016/07/08 16:00:08 DEBUG: read 16384 bytes, took 3.108551ms\n2016/07/08 16:00:08 DEBUG: read 1024 bytes, took 24.52\u00b5s\n2016/07/08 16:00:08 DEBUG: read 5853 bytes, took 861.306\u00b5s\nFinished GetRecords body read, took 14.634752912s\nFinished GetRecords Unmarshal, took 14.634783236s\nFinished GetRecords request, 14 records from shard 0, took 14.634786265s\nIts a lot of extra information in the log, but it might help identify where things are getting hung up at.\n. I used the following to find long running requests:\nsh\nAWS_REGION=<region> go run kinesisStream.go <stream> > /tmp/kinesisStream.out\nsh\ngrep \"records from shard\" /tmp/kinesisStream.out | awk '{print $10}' | go run durSort.go\ndurSort.go\n14.634786265s\n13.839473696s\n13.650340904s\n12.790674943s\n12.221052297s\n12.187944525s\n10.668927175s\n10.666806916s\n9.716013485s\n9.563715184s\n. Thanks for the update @fantyz .The 16k read taking 10 min, and other multiple min delays reading from the network are surprising. I've included the request ID in my reach out to the Kinesis team.  This should help investigating this issue further on their end.\n. Good idea, I think this is the best workaround for the issue. I will get back to you when Kinesis has any additional information. I don't think there is very much we can do from the SDK side beyond the work you're doing to add retries in .\n. Hi @fantyz I heard back from the service team. For the request ID you provided the service finished processing and responding to the the request quickly. This suggests the latency is being introduced client side or on the route between the server and the client.\nIn addition Kinesis agreed using timeouts and retry logic for the rare cases that networking issues cause latency.\n. Hi we've merged in PR ##1166 which adds retrying of connection reset errors discovered during unmarshaling response body, and read timeouts for Kinesis API Get operations. The read timeouts should prevent your application hanging for significant amounts of time.. Great find @dmac when you encountered this issue did you happen to notice what the length of tokens slice was, and i?\nThis seems to be made possible when r.Operation.InputToken slice is longer than the r.Operation.OutputToken slice.\n. The bug definitely is with SetValueAtAnyPath not correctly handling nil value to set.  I've added a test locally for this to help investigation.\n. Hi @dmac I pushed a fix to the SetValueAtPath utility to allow setting pointer fields to nil. Let us know if you have any issues, or feedback.\n. @jszwedko I've done some digging into this and the SDKs are intentionally not omitting the field to maintain wire compatible with the service protocols.\nIn this case the best way to deal with this is to pass in nil for list/map which are empty to prevent this error.  I've forwarded on your issue onto EC2. I also suggest posting this request on the EC2 forums since others may have this issue. Helping to get the change accepted.\n. No problem, Let us know if you have any additional questions, or feedback. \n. @jszwedko I've verified with the EC2 team. The empty list is intended to not be valid, but they are looking at improving the error messaging that is returned to clear up the requirement since the current error message isn't descriptive.\n. Hi @diptanu closing this issue, please reopen or contact us if you're still having any issues, or we can answer any questions you might have.\n. Thank you very much for your feedback. The design of pointers, style, and calling method is complex and  very important to the long term maintainability of the SDK. We have a few issues which are duplicating each other and I'd like to collapse the discussion down to a single issue. Lets use #363 for further discussion.\nDuplicates: #363\nRelated: #265, #284, #294\n. Hi @catsby Are you still encountering this issue?\n. Hi @catsby I did some research on this issue and the workaround you implemented in the correct workaround for the API. The NoDevice parameter is not compatibly with any other parameter other than DeviceName like you used.\nI'll pass this information upstream to the EC2 team to improve the documentation of this parameter and BlockDeviceMapping's other parameters.\nI've also updated our wiki to mention this until the EC2 docs are updated.\nThanks again for letting us know about this issue. Since the change you have is the proper way to make the request i'm going to close this issue.  Please let us know if you have any feedback, questions, or other issues using the SDK.\n. Thanks for the PR @euank. I don't see any issue with this PR. Though it does highlight an issue that these interfaces probably should have all the methods of the service operations, or users of the SDK will be prevented from using xRequest operation forms and use the interface types as well.\nIf we can find a way to cleanly update the interface and concrete struct methods generations to be done through the same code path instead of through two separate paths like is being done now we could ensure the two are up to date.\n. Hi @euank I just pushed an update which adds Pages, and Request forms of the API operations. This should allow all operation calls to be mocked out. Either with a tool such as mockify or using the example included in the commit message.\n. Hi @purohit thanks for reporting this! These fields definitely could use some improvement. We're taking a look into how we can correct the surface of these operations.\n. Thanks for requesting this feature @purohit. We've provided this feedback  the service team for, but we are unable to make changes to the SDK's public API as it would be a breaking change.. Hi @sclasen thanks for your feedback. I've updated the SDK so services, like ELB which use the query protocol will populate the r.RequestID in both success and failure cases.\n. Replaced with #325\n. Replaced with #325\n. I hanks for the update! I'll get a chance to finish review the PR tonight. Overall it looks good.\nI agree for now keeping the v2 signer in the internal/signer package is best. In the future a nice improvement would be to refactor some of the internal packages out of internal to be exposed, signers would be refactored out also so that they could be used standalone and swapping signers.\n. Hi @jjeffery thanks for taking the time, and working with us to get this PR built and submitted. It looks pretty solid, and the way the code generation is done works great. Overall it looks good. I only had a few minor comments mentioned above.\nI'll also be able to add smoke integration test for SimpleDB once the PR is merged in.\n. @jjeffery Thanks a lot for your hard work getting writing the v2 signer so that we can adds the SimpleDB service to the Go SDK. \n. This change was merged in with release v0.7.0\n. @Aprimit I think connecting to a DynamoDB table that is owned by a different account should be the same as connecting to a table owned by your account. The important part is that the table owned by the other account needs to add an IAM policy for your account to have access. \nCheck out this doc Authentication and Access Control for Amazon DynamoDB It may help setting up the access control for DynamoDB.. @vincent6767 some credentials are required, but they do not need to be real credentials. Your credentials are used by DynamoDB local to maintain separate databases for different users. Your credentials won't be checked, but those configuration parameters do serve a purpose.. Replaced with #325\n. Good idea @sclasen I think this will go nicely with our logger changes coming in PR #315.\n. @sclasen I just pushed an update which implements two new debug log sub levels to log request retries, counts, and errors.  \nThe additional log sub levels are additive and can be used together.\ngo\nsvc := s3.New(aws.NewClient().WithLogLevel(aws.LogDebugWithRequestRetries | aws.LogDebugWithRequestErrors)\nBoth of these new log levels imply aws.LogDebug will be set also.\n. This change was commited with release v0.7.0.\n. Hi @parkr thank you for your feedback, and apologizes for any complications the update caused.\nWith the SDK still in Developer Preview we strongly recommend vendoring the SDK library. Doing so will be the best method to protect your code base from complications when changes are made to the Developer Preview SDK until you're able to update in a controlled way. Godep or similar tools provide very good dependency vendoring capabilities.\n. Hi @rina-sleeping which service are you using when you encountered this issue?  I'm guessing at S3, is that correct? Would you mind providing a example of the operation call you are looking to do so?\n. Great thanks, I've pushed a change which will allow using the SDK to set these style of key names in S3\n. Thank you very much for the PR @cliffcrosland, \n. Hi @catsby I just pushed the Aurora support in our Release v0.7.1\n. Thanks for the heads up is this only seen with using a dependence mangment tool? Which tool are you using?\nGo 1.5 compatibility checks are what I'm looking at now so any additional information would be great.\n. Thanks for the detailed investigation, this is very helpful! I'd really like to find a way to reproduce this locally before opening an issue with Go. I wasn't able to find any references in Go's Github issues of it reported yet.  \nI'm going to try to build the project using Go 1.5 running the travis build steps locally.\n. Looks like the issue has been fixed in tip. Thanks for the PR. Pulling it in now so we can keep track of tip changes as well.\n. Thanks for reporting the issue @dstokes. I think for this type of error we'll need to handle it genericly since it may be possible for it occur on other services also.  I expect once it is fixed, the error returned will have a Code of ServiceUnavailableException\n. Hi @jitcompile I'm having a little difficulty reproducing the issue you're encountering.  The following example based on your above code is able to report metrics correctly to CloudWatch.  Are you able to reproduce your issue using this example? Leaving the Value field unset (nil) should be the correct action.\n``` go\npackage main\nimport (\n    \"github.com/aws/aws-sdk-go/aws\"\n    \"github.com/aws/aws-sdk-go/service/cloudwatch\"\n    \"log\"\n    \"time\"\n)\ntype Stat struct {\n    Name, Unit                         string\n    Timestamp                          time.Time\n    Minimum, Maximum, SampleCount, Sum float64\n}\nvar stats = []Stat{\n    {\"Name\", \"Seconds\", time.Now().Add(-1 * time.Second), 0, 5, 10, 100},\n    {\"Name\", \"Seconds\", time.Now().Add(-2 * time.Second), 1, 5, 10, 100},\n    {\"Name\", \"Seconds\", time.Now().Add(-3 * time.Second), 2, 5, 10, 100},\n    {\"Name\", \"Seconds\", time.Now().Add(-3 * time.Second), 3, 5, 10, 100},\n    {\"Name\", \"Seconds\", time.Now().Add(-4 * time.Second), 4, 6, 10, 100},\n    {\"Name\", \"Seconds\", time.Now().Add(-5 * time.Second), 5, 7, 10, 100},\n    {\"Name\", \"Seconds\", time.Now().Add(-6 * time.Second), 6, 8, 10, 100},\n    {\"Name\", \"Seconds\", time.Now().Add(-7 * time.Second), 6, 9, 10, 100},\n}\nvar runIDDimension = cloudwatch.Dimension{\n    Name:  aws.String(\"DimName\"),\n    Value: aws.String(\"i-dcba\"),\n}\nfunc main() {\n    client := cloudwatch.New(nil)\n    input := cloudwatch.PutMetricDataInput{}\n    input.Namespace = aws.String(\"test\")\ninstanceNameDimension := cloudwatch.Dimension{\n    Name:  aws.String(\"InstanceName\"),\n    Value: aws.String(\"i-abcd\"),\n}\n\nfor _, stat := range stats {\n    datum := cloudwatch.MetricDatum{\n        MetricName: aws.String(stat.Name),\n        Timestamp:  aws.Time(stat.Timestamp),\n        Unit:       aws.String(stat.Unit),\n    }\n    datum.Dimensions = append(datum.Dimensions, &instanceNameDimension)\n    datum.StatisticValues = &cloudwatch.StatisticSet{\n        Maximum:     aws.Float64(stat.Maximum),\n        Minimum:     aws.Float64(stat.Minimum),\n        SampleCount: aws.Float64(stat.SampleCount),\n        Sum:         aws.Float64(stat.Sum),\n    }\n    input.MetricData = append(input.MetricData, &datum)\n}\n_, err := client.PutMetricData(&input)\nif err != nil {\n    log.Fatalln(err)\n}\n\n}\n```\nI suggest updating your config to print out wire logs of requests.  With that we'll be able to see what the SDK is sending. You can use the following options to enable logging.\ngo\nclient := cloudwatch.New(aws.NewConfig().WithLogLevel(aws.LogDebugWithHTTPBody))\n...\n. @ncw Friday I committed a change #326 which shoulders enabled it. Is your code based synced to tip? If you're already on please let us know.\n. Ah thanks I see the problem now.  The path was being cleaned previously to remove trailing slashes, and  duplicate slashes. This is what prevented using the SDK on folder style object names directly in #326.\nThe fix for #326 was to remove the URL cleaning. This looks to have an unintended consequence where endpoints with trailing slashes, or keys with leading slashes were no longer being squashed.\nI think we can fix this by being a little bit more smart about how paths are joined. \n. Hi @ncw I just pushed a change which corrects this functionality. Your use case should be working correctly now.\n. Thanks for the suggestion @zollie. Adding this functionality is in our backlog since the SDK does not support the HAL protocol. \n. Thanks for the votes, and information on halgo.  I haven't see that library before. When we add support for API Gateway it would be fore the full service API. So you can expect full feature functionality once support is added. When API Gateway is supported in the SDK you can expect it to follow existing supported services operation request/response patterns. \n. HI @zollie @radeksimko @hopkinsth I just pushed an update which adds support for the API Gateway API.  You should now be able to start using API Gateway's API calls using the Go SDK with release v0.9.17.\n. Hi @drombosky are you looking to write binary data to SQS or unicode codepoints?\nIt looks like you're running into an condition where SQS is unable to process the message body and replaces stors the 0x80-0xFF ASCII character with \ufffd \\xEF\\xBF\\xBD. If you look in your SQS queue using the web console you'll see three bytes per message using the test program you provided with all the same value. This character is what the checksum error returned correlates to.\nTo use 128 as an example 0x80 ASCII is not a valid unicode codepoint. The unicode codepoint for 0x80 ASCII is U+0080 or hex encoded as C2 80. 0x7F is the maximum ASCII value which is valid unicode. Here is a good reference of unicode codepoints and hex for the 0x00 to 0xFF range.\n. @drombosky Are you still encountering this issue?  I'm going to close this issue for now, but please reopen if encoding the message content as UTF-8 doesn't resolve the issue.\n. It looks like the tool you're using to go get the packages is doing both github.com/awslabs/aws-sdk-go and  github.com/aws/aws-sdk-go  The awslabs is no longer used as the SDK has moved into GA and is now within the aws github orginization.\nDoes your application still reference the awslabs version of the repository?\n. I would suggest using a tool such as godeps or similar to vender your dependencies updating their import paths until those dependencies are able to be updated to reference the correct SDK project path.\nWith the release of the SDK's Dev Preview the SDK moved from awslabs to aws organization preparing for general release. The awslabs/aws-sdk-go was a temporary location for the SDK until it was released.\n. Glad this process will work for you @swenson let us know if you run into any other complications, questions, or feedback.\n. Hi @masonoise it looks like the issue is caused by profile being included in the section definition.  If you change it to mytest the shared credentials should work.\n. @masonoise I just updated our wiki documentation to clarify the different between the shared credentials profile file and AWS CLI config file. \n. Hi @davidsonff initially the header metadata key names should be x-amz-key and x-amz-iv. The x-amz-meta- prefix will automatically be added by the SDK when using S3 operations. \nUsing this example I was able to write an object into S3 with the x-amz-key and x-amz-iv with dummy data.\n``` go\npackage main\nimport (\n    \"bytes\"\n    \"github.com/aws/aws-sdk-go/aws\"\n    \"github.com/aws/aws-sdk-go/service/s3\"\n    \"github.com/aws/aws-sdk-go/service/s3/s3manager\"\n    \"log\"\n)\nfunc main() {\n    svc := s3.New(nil)\nuploader := s3manager.NewUploader(&s3manager.UploadOptions{\n    S3: svc,\n})\n\nresult, err := uploader.Upload(&s3manager.UploadInput{\n    Bucket: aws.String(\"bucketName\"),\n    Key:    aws.String(\"keyName\"),\n    Body:   bytes.NewReader(make([]byte, 5*1024*1024)),\n    Metadata: map[string]*string{\n        \"x-amz-key\": aws.String(\"encKeyStr\"),\n        \"x-amz-iv\":  aws.String(\"initVect\"),\n    },\n})\nif err != nil {\n    log.Fatalln(err)\n}\n\nlog.Println(result)\n\n}\n```\n. In order to help debug your issue I suggest enabling debug logging with HTTP body. This will print the detailed response body before it is deserialized by the SDK.\ngo\nsvc := s3.New(aws.NewConfig().WithLogLevel(aws.LogDebugWithHTTPBody))\n. If you haven't already found it, take a look at this help doc it provides a few useful links at the bottom with examples how the Java AWS SDK handles client side encryption.\n. Thanks for reporting this issue@sjwhyte Which service are you using? You can safely ignore the SDKShapeTraits field in all Input and Output structures since it is only there for serialization by the SDK's protocol serializers.\n. From the docs for EC2's CreateTags an error will be returned by the service if creating the tag fails. The return field of the operation is not implemented by the SDK because this field can never be false. Either the the tag was created successfully and an empty result will be returned with no error, or an error will be returned by the SDK.\nAre you seeing cases where error is not being returned, and the tag is not being created?\n. Hi @SergeyTsalkov Thanks for reporting this issue. This is occurring due to the SDK usage of net/url's URL.Opaque field. This field is needed by the SDK to prevent the Go net/url library from mutating the URI by escaping and unescaping. URL.String() defines how Opaque is used when constructing a URL.\nFrom what I found absolute URI for all methods except CONNECT is valid. According to the HTTP1.1 spec\n\nTo allow for transition to absoluteURIs in all requests in future versions of HTTP, all HTTP/1.1 servers MUST accept the absoluteURI form in requests, even though HTTP/1.1 clients will only generate them in requests to proxies.\n\nThe Go net/url seems to be taking this as being allowed to use absoluteURI when URL.Opaque is set. The SDK could be updated to use the Request.Host and only set the URL's path in URL.Opaque since URL.Host would be ignored when making requests. But this breaks the ability to call URL.String() to receive a useful URL string. Since URL.Host is ignored.\n. @SergeyTsalkov I'm going to close this issue since using the absolute URL within the HTTP request is apart of the HTTP spec, and valid. The Go net/http.URL auto (un)escapes values in the URL which are incompatible with the AWS services. The SDK needs to use the Opaque field of the URL to ensure the auto unescaping logic is not used, and the request are made how services need them.\n. Hi @pilt Thanks for reporting this bug.  I just pushed a fix that will correct the EC2Metadata service client endpoint initialization. You should no longer need to specify the EC2Metadata client and associated Endpoint to EC2RoleProvider for this functionality to work.\n. HI @chadgrant thanks for getting in contact with us. Would you mind opening a new issue with this so we can investigate it better? I'm trying to reproduce the issue reported in coreos/coreos-kubernetes#94 with the SDK but am not having much luck. Using the following Go script I'm able to successfully retrieve credentials from a EC2 host's metadata service.\n``` go\npackage main\nimport (\n    \"fmt\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\n)\nfunc main() {\n    sess := session.New(aws.NewConfig().WithLogLevel(aws.LogDebugWithHTTPBody))\nc, err := sess.Config.Credentials.Get()\nif err != nil {\n    fmt.Println(\"failed to get credentials\", err)\n    return\n}\n\nfmt.Println(\"retrieved credentials\", c)\n\n}\n```\nWhen run on a properly configured EC2 Instance with an IAM role you should see output similar to:\n```\n[ec2-user@ip-172-31-61-27 ~]$ ./repo-i345 \n2016/01/16 07:34:06 DEBUG: Request ec2metadata/GetMetadata Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nGET /latest/meta-data/iam/security-credentials HTTP/1.1\nHost: 169.254.169.254\nUser-Agent: aws-sdk-go/1.0.9 (go1.5.3; linux; amd64)\nAccept-Encoding: gzip\n\n2016/01/16 07:34:06 DEBUG: Response ec2metadata/GetMetadata Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.0 200 OK\nConnection: close\nContent-Length: 29\nAccept-Ranges: bytes\nConnection: close\nContent-Type: text/plain\nDate: Sat, 16 Jan 2016 07:34:06 GMT\nEtag: \"2413478838\"\nLast-Modified: Sat, 16 Jan 2016 07:28:37 GMT\nServer: EC2ws\nmy-role-name\n2016/01/16 07:34:06 DEBUG: Request ec2metadata/GetMetadata Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nGET /latest/meta-data/iam/security-credentials/aws-elasticbeanstalk-ec2-role HTTP/1.1\nHost: 169.254.169.254\nUser-Agent: aws-sdk-go/1.0.9 (go1.5.3; linux; amd64)\nAccept-Encoding: gzip\n\n2016/01/16 07:34:06 DEBUG: Response ec2metadata/GetMetadata Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.0 200 OK\nConnection: close\nContent-Length: 918\nAccept-Ranges: bytes\nConnection: close\nContent-Type: text/plain\nDate: Sat, 16 Jan 2016 07:34:06 GMT\nEtag: \"2803750666\"\nLast-Modified: Sat, 16 Jan 2016 07:28:37 GMT\nServer: EC2ws\n{\n  \"Code\" : \"Success\",\n  \"LastUpdated\" : \"2016-01-16T07:29:29Z\",\n  \"Type\" : \"AWS-HMAC\",\n  \"AccessKeyId\" : \"\",\n  \"SecretAccessKey\" : \"\",\n  \"Token\" : \"\",\n  \"Expiration\" : \"2016-01-16T14:02:13Z\"\n}\n\nretrieved credentials {  }\n```\n. @chadgrant What version of the SDK are you using? If you're on the latest version of the SDK then I think you just need to make a minor modification to your code.  in v0.10.0 the SDK was updated to require session and config information to be passed into service clients. If they are not provided the client will be unable to operate and panics.\ngo\nWithCredentials(ec2rolecreds.NewCredentials(sesssion.New())\nIf you already have a session.Session defined you can use that instead of creating a new instance.\n. Hi @ajaybc thanks for posting this issue. Taking a look into what is going on here.\n. @ajaybc is your application using the latest version of the AWS Go SDK? Yesterday I fixed a bug which prevented ec2 roles begin retrieved, and this morning added a fix aws/aws-sdk-go@e9380f7 which corrects the way ec2rolecreds detects assume role errors.\nTo help narrow down the problem I suggest updating your application's service client config to only use the ec2rolecreds.EC2RoleProvider. This will narrow down what is going on, and return a EC2 Role specific error message.\n``` go\nconfig := aws.NewConfig()\nconfig.Region = regionName\n// Set to use only EC2 AIM Role credentials\nconfig.Credentials = ec2rolecreds.NewCredentials(nil, 5*time.Minute)\ncog := cognitoidentity.New(config)\nparams := &cognitoidentity.GetOpenIDTokenForDeveloperIdentityInput{\n...\n}\nresp, err := cog.GetOpenIDTokenForDeveloperIdentity(params)\n```\n. Thanks for the update, the EC2 Role Provider has now been fixed to correctly return the credentials on a successful response from iam/security-credentials. \n@aibou, that was exactly what was left out of the commit I made yesterday. This has been corrected to check for Success when determining success.\n@ajaybc If you sync with the latest tip you will be able to use EC2 Roles again. \n. Great thanks for your feedback, and filing this issue.  I'll go ahead and close this issue now since it is resolved. If you run into any other issues, please open and issue or contact us via gitter.\n. I pulled in this update and updated the docs. The EC2 request docs are now updated. Thanks for taking the time to report this issue.\nhttp://docs.aws.amazon.com/sdk-for-go/api/service/ecs/ECS.html#ListContainerInstancesPages-instance_method \n. Thanks @matematik7 for finding and submitting a PR for this bug.  I pulled this in 47f3c2a and added unit test in f107bc3.\n. Hi apologies for the late response response, I'm working on pulling this in now. Combining it with #355\n. I pulled in this update and updated the docs. The EC2 request docs are now updated. Thanks for taking the time to report this issue.\nhttp://docs.aws.amazon.com/sdk-for-go/api/service/ecs/ECS.html#ListContainerInstancesPages-instance_method \n. Thanks for reporting this @asemt the Getting Started guide was missed when updating aws.DefaultConfig to the defaults package. to become defaults.DefaultConfig. I've updated the wiki docs with the correct usage.\nThe usage should be:\ndefaults.DefaultConfig.Region = aws.String(\"us-west-2\")\nAnd the defaults import path added.\n\"github.com/aws/aws-sdk-go/aws/defaults\"\n. Going to close this issue, since the wiki docs have been updated.  Thanks for taking the time to report this issue.\n. Thanks for reporting this @strife25 I'm working on fixing this. In the mean time the godoc.org docs will have details on how to use ec2metadata.Client \n. I pulled in this update and updated the docs. The ec2metadata.Client docs are now updated. Thanks for taking the time to report this issue.\nhttp://docs.aws.amazon.com/sdk-for-go/api/aws/ec2metadata/Client.html\n. Hi @ncw thanks for submitting this bug.  The SDK should not be retrying request when they error with this request. The 301 error returned is due to the request being made to the wrong region. The Go SDK does require that the request are made to the correct region.\nThe redirect provided by S3 is not intended to actually be followed in this case. That is why the Location header is not set. If your application has access to the bucket you could do S3.GetBucketLocaltion() to get the bucket's region.  Or if not not performing a HEAD request on the bucket to receive the Bucket's location in the x-amz-bucket-region header.\nsh\ncurl -I \"https://bucketname.s3.amazonaws.com\"\nOr in Go, This functionality isn't currently in the SDK but can be implemented simply with the below.\ngo\nresp, err := http.Head(\"https://bucketname.s3.amazonaws.com\")\nif err != nil {\n    fmt.Println(\"ERROR\", err)\n    return\n}\nfmt.Println(\"Region:\", resp.Header.Get(\"X-Amz-Bucket-Region\"))\n. I pushed 9411a1e which fixes the multiple retries when the request should of not retried at all. Thanks for reporting this issue, if you have an more issues or feedback please let us know.\n. Great let us know if you run into any issues with the SDK, new features/tools we can add, or things we can do better.\n. Hi @strife25 thanks for catching the typo, pulling in PR.\n. Looks good, thanks for catching this and summiting the PR.\n. Hi @nlamirault thanks for contacting us. Reading over your code I'm not seeing anything which is glaring as causing the problem.  \nThe only difference between our code I see is the AccountId isn't defined in your example. Is it safe to assume that was just deleted for the example purpose?\ngo\nsvc := glacier.New(nil)\nresult, err := svc.DescribeVault(&glacier.DescribeVaultInput{\n    AccountId: &accountId,\n    VaultName: &vaultName,\n})\nfmt.Println(result, err)\nWhat output do you see when logging the result of DescribeVault?  Additionally you could try instructing the SDK to log debug information about the requests with the following aws.Config additional logging settings.\nconfig := aws.Config{\n    LogLevel: aws.LogLevel(aws.LogDebug),\n    // other config settings ...\n}\n. Thank for catching this, and submitting the PR @vially. \n. @sclasen Are these failures new, or have they been occurring for some time?\nAn additional way to check the number of retries with aws.Config's LogLevel, aws.LogDebugWithRequestRetries. This will log the individual requests, their retry attempts, and the error the specific request encountered.\nI was able to verify locally that kinesis.GetRecord failing with a 500 status code does get retried, but only three times.\n. Correct, not populating the RequestID is a bug, #311.  Until this is fixed you can workaround this issue by using aws.LogDebug or any of the aws.LogDebugX variants to log the RequestID. The RequestID will be X-Amzn-Requestid header in the response.\nThis can also be retrieved programmatically via req.RequestID  Once #311 is fixed, both request error and req.RequestID will be populated.\n. @sclasen correct #311 will cover all cases of RequestID not being populated. Both for success and error responses.\n. Hi @mattes, The aws.String is a helper so literal strings can be used when setting API field pointer values without needing to define a local variable first, because &\"xxx\" is not valid syntax. If the value is already defined as a local variable &xxxStr can also be used.\nThe AWS API operations include many optional fields which should not be included in the marshaled request if not set. In order to represent the ternary state for these fields pointers are used. This is done because it is not possible to distinguish between a never set value from a value set to the type's Zero value. The SDK uses pointers for all primitive and struct type fields. In many cases the type's Zero value of a field has meaning, and is different from unset, and a non zero value. All API fields in Input and Output structs are pointers, because even though a field is required today, it may not be required in the future. A breaking change would be required to allow not setting the once required field.\n. Hi @omeid take a look at #294 and #306. Between the two the cover the topic pretty well.\nSince the service APIs are iterating quickly, fields which were previously required may become optional. This recently occurred with EC2 making changing parameter from required to optional. If we had methods for these operations it would be impossible for you not to provide a value for the now optional parameter without making a breaking change to the SDK. Once the SDK is released and ready for production we want to ensure it is stable.\n. Thanks for the feedback. We definitely want to provide the best idiomatic experience within the context of the AWS service APIs. How the SDK exposes its fields is something we really want to make sure is a good experience prior to releasing the SDK.\n. @endophage The sqs.SendMessageInput.MessageBody actually must be a valid unicode strings, any non-unicode binary data should be unicode escaped. If the API used a []byte instead of string it may imply that binary data is valid for the message body, which isn't the case. (#339) With that said, if you're looking to include binary or numbers within a message not escaped sqs.SendMessageInput.MessageAttributes using sqs.MessageAttributeValue is the best option to specify the additional binary or number data.\n. @stevvooe Since many fields within the the API operations are optional a concept is needed to identify if the field was set to a non-zero value, not set at all, or set to the zero value for the type. When using by value there is no way to distinguish the difference between set to zero value, and not set at all. Since zero value for primitive types are valid API operation values, the common omitempty for zero value pattern is not valid. In order to do so, extra information is needed.\nTo provide that extra information there are four common ways this could be done. \n- Use pointers instead of values. This was the original proposed solution Go applications began to use to provide dynamic not-set marshalling for concrete structure fields. Non-string primitives such as bool and int64 highlight this issue more than string within the context of the SDK since using zero value for omitempty would prevent setting a field to 0 or false. But like you mentioned the usage of pointers will increase number of allocations, even more so for operations with many fields.\n- Use map[string]interface{}. This would remove the need for pointers, but also remove the highly valued compile time type checking. In addition, it would remove any type ahead assistance various IDEs may provide. In addition field names would need to be exposed as consts in order to prevent typo's in field names.\n- Use function params for required fields. e.g. svc.GetObject(\"bucketName\", \"keyName\") If the service APIs were static this would be a great pattern for the SDK to follow. But since the service API operations are always evolving fields which were once required can become optional. Required fields becoming optional or optionally required is not uncommon. This pattern would require the SDK to make breaking changes to allow users to not to set previously required fields. This is a perfect pattern to be written as a level above the SDK, since users would be able to control when they upgrade, and choose to update their helper functions as they need. While also being able to update the SDK for new features without breaking their existing code.\n- Wrap primitives struct. As an alternative to pointers we looked into using structs to wrap primitives. The struct would embed the needed tri-state information. With this pattern we could remove the need for pointers of primitives, but it would still require a aws.String(\"xxx\") like method to be able to set the value for a field inline. At the same time it would remove the ability to set a field with the variable directly, such as Bucket: &bucketName which is an option today. These wrappers also have the added benefit of removing the possibility of dereferencing a nil pointer for unset response output structs.StringType below is an abbreviated example of this tri-state wrapper.\ngo\ntype StringType struct {\n    s string\n    set bool\n}\nfunc String(s string) StringType { return StringType{s: s, set: true} }\nOf these options primitive pointers and wrapping structs are the two which provide the most flexibility and  stability for their cost and ease of use. Any feedback on these methods you have are more than welcome. I have experimented a few times (#276, #294) with using the primitive wrappers, but have not pushed it forward because it removes the ability to use &fieldValue and requires a helper func aws.String(\"xxx\"). The helper func is required because initializing the struct by itself such as aws.StringType{S: s, Set: true} would require setting the Set field to be provided also, and would be a big risk for users thinking they set a value when they really didn't. \n. @stevvooe Check out this post on Go, REST APIs, and Pointers which provides a great explanation why marshalling Go structs with optional fields that can intentionally not be set, because their zero value has meaning needs knowledge if the field was ever set. In addition, google/go-github#19 the issue mentioned by the post, discusses this topic and explores a few other alternatives as well.\nLike @lsegal mentioned aws.Config's case is a little special. For this struct pointers are used because a default config is merged with a user's provide config when instantiating a service client object. In order to merge two structs correctly knowledge of set or unset is needed. Especially since for nearly all of the fields the type's zero value has meaning. For example, Config.DisableSSL if it were set to true in the default config it could never be unset when creating a new service client with a config which had the field set to false, because false is bool's zero value, unless knowledge of set or unset was available. Without this knowledge the user would need to manually call svc.Config.DisableSSL = false after instantiating the service client. This is what pointers fields provide for aws.Config.\n. @endophage the comparison between encoding/json and SQS MessageBody are interesting. What is your usecase for the MessageBody? Is it safe to assume it is being used for marshaled JSON/XML data?\nIs it the allocation caused by casting a []byte to a string you're looking to avoid?\n. Thanks for the info @endophage . To keep this thread from becoming too muddled with too many topics what do you think about creating a new issue to track this topic specifically. If you'd like to open up a new issue about this we can further discuss this use case, and pain points using these string fields cause. Also if there are any other services or operations' fields where you are also experiencing this pain points, please include those as well so we have a wider base to launch off from.\n. Thanks for the feedback. Providing a SDK which is easy to use and understand is very important to us. I'm a bit late to this discussion, but I'd like to understand the issues you're experiencing with the SDK. Since the client APIs are generated from models defining the service API we want the SDK to be friendly and feel handwritten as much as possible. In designing the SDK we considered several options for how we could represent the service APIs in a Go SDK in a easy to use way, while also ensuring the SDK will not introduce breaking changes when service updates occur. The choice to use structure initialization over function parameters was to ensure the SDK is stable as service APIs are updated.\nFor the SDK's general availability release we would like to provide the initial functionality that is needed for developers to use AWS with Go. This will give the SDK a good base on which other tools and features can be built on top of.\nFor API calls like S3's PutObject it's straightforward to create helper methods since Bucket, Key, and Body are the only required fields. But, this becomes more complex when we look at API calls which contain optionally required parameters such as EC2's AuthorizeSecurityGroupIngress. In these cases there are multiple ways the API could be called because the parameters are mutually exclusive.\nI'm very excited to see where the Go community lands with vendoring. I think it will go a long way to reduce developer's package management headaches. Even now there are several options, but no standard. The experimental 1.5 vendoring will be a good step forward. With golang/go#12302, versioning of packages might be supported in the future.\nI've consolidated a few related issues into this one so the discussion can be in a central location.\n. To wrap this issue up I'd like to thank everyone for their feedback, and the discussion in this thread along with the other related threads. The SDK at its core should stay close to the service API definitions. As the service APIs are consistently adding new features and improvements. With the core SDK built we have a place to build on top of. Especially any customizations and stylistic invocations that are helpful.\n. Hi @yasker Thanks for reporting this issue you're experiencing. The delay is unexpected and the SDK should be experiencing similar polling experience as the CLI. \nIf you are able to grab wire log for the requests it would be very helpful in understanding the problem, and might shed some light on solving it.\nYou can do this by adding aws.LogDebugWithRequestRetries which will report each time a request is retried.  You can add this to your configuration such as:\ngo\nsvc := ec2.New(aws.NewConfig().WithLogLevel(aws.LogDebugWithRequestRetries))\n...\n. Hi @yasker I'm taking a look into why this is happening. In my example I'm seeing the Progress getting stuck at 2%, and another time at 99% with code similar to yours.\n. ``` go\npackage main\nimport (\n    \"fmt\"\n    \"os\"\n    \"time\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/service/ec2\"\n\n)\nfunc main() {\n    svc := ec2.New(aws.NewConfig().\n        WithLogLevel(aws.LogDebugWithHTTPBody))\nsnapshot, err := svc.CreateSnapshot(&ec2.CreateSnapshotInput{\n    VolumeId: aws.String(os.Args[1]),\n})\nif err != nil {\n    fmt.Println(\"Failed to create snapshot:\", err)\n    return\n}\n\nif err := waitForSnapshotComplete(svc, snapshot); err != nil {\n    fmt.Println(\"Failed to wait for snapshot:\", err)\n    return\n}\n\nfmt.Println(\"Snapshot created\")\n\n}\nfunc waitForSnapshotComplete(svc ec2.EC2, snapshot ec2.Snapshot) error {\n    if *snapshot.State == ec2.SnapshotStateCompleted {\n        return nil\n    }\nparams := &ec2.DescribeSnapshotsInput{\n    Filters: []*ec2.Filter{\n        {\n            Name: aws.String(\"snapshot-id\"),\n            Values: []*string{\n                snapshot.SnapshotId,\n            },\n        },\n    },\n    OwnerIds:            []*string{snapshot.OwnerId},\n    RestorableByUserIds: []*string{snapshot.OwnerId},\n    SnapshotIds:         []*string{snapshot.SnapshotId},\n}\n\nfor *snapshot.State == ec2.SnapshotStatePending {\n    fmt.Printf(\"DEBUG Snapshot %v process %v\", *snapshot.SnapshotId, *snapshot.Progress)\n    time.Sleep(time.Second)\n\n    res, err := svc.DescribeSnapshots(params)\n    if err != nil {\n        return err\n    }\n    s := findSnapshotByID(*snapshot.SnapshotId, res.Snapshots)\n    if s == nil {\n        fmt.Println(\"Lost reference to snapshot status\", snapshot.SnapshotId)\n    }\n    snapshot = s\n    fmt.Print(\"Snapshot Status:\\n\", snapshot)\n}\nreturn nil\n\n}\nfunc findSnapshotByID(id string, snapshots []ec2.Snapshot) ec2.Snapshot {\n    for _, s := range snapshots {\n        if id == *s.SnapshotId {\n            return s\n        }\n    }\n    return nil\n}\n```\ngo run makeSnapshot.go \"volume-id\"\n. I think this is a caching issue with the service that you might be running into.  In my example if I adjust the sleep.Time(time.Second * 20) I haven't seen the problem yet. Even a delay of 5s between DescribeSnapshots calls seams to resolve this issue.\nThis makes sense, because the CLI will use waiters to determine when to retry based on a service defined wait time. The Go SDK hasn't integrated these waiters yet. The SDK branch waiters implements an early version of the waiters, but this is something we want to complete prior to releasing the SDK.\n. The waiters for EC2's DescribeSnapshot would use 15 seconds delay between DescribeSnapshot requests up to 40 max attempts.\n. Interesting, are you creating multiple snapshots at once? For curiosity, would you want to try and disable HTTP KeepAlive in your HTTP Client?  You can do this by adding the following to your EC2 service Config.\ngo\nsvc := ec2.New(aws.NewConfig().WithHTTPClient(&http.Client{\n    Transport: &http.Transport{\n        DisableKeepAlives:   true,\n    }\n})\n// ...\nI'm curious if because Go's default HTTP client reusing the TCP connection is causing you to still get cached status hits.\nAlso an alternative suggestion would be to increase the initial delay before checking the status of the snapshot, before starting the polling.\n. Great, glad its working for you now. Let us know if you run into any more issues, or any feedback.\n. @abhiofdoon The Ruby SDK would be the best place to ask this, and linking to this issue with aws/aws-sdk-go#364 Are you seeing the issue every time you create a snapshot?\n. Good find, thanks for submitting the PR to fix this @keithpitt \n. Thanks for reporting this @satyenr. It looks like the the CLI support for INI format is a bit more open and less restrictive than the INI parser the SDK is using. We are open to suggestions of other INI parsers. The INI format is fairly simple so writing our own wouldn't be overly complex either and would remove the only 3rd party runtime dependency from the SDK\n. Thanks posting this issue. We've switched the INI library to github.com/go-ini/ini. This library supports the  colon assignment token. You should now be able to use AWS credentials files with colons in their assignments. Let us know if you have any questions, or feedback.\n. Thanks for reporting this @n-boy I think this is related to the SDK's restjson marshaller not correctly marshalling payload structs contents as the body, and instead marshals entire input struct.\n. @n-boy I just pushed a change which fixes the marshaling of the request body so the correct contents are written. Let us know if you have any other issues, or feedback.\n. Hi @nariyu thank you very much for taking the time to create this PR.  According to the AttributeValue docs the String Set (SS) must include unique values. This change would allow non unique strings to be included in a SS which may cause errors when the AttributeValues are sent to the service.\nAnother issue I noticed with this proposed change is that structs and maps with []string will be converted in two different ways like the following example code.\n``` go\npackage main\nimport (\n    \"fmt\"\n    \"github.com/aws/aws-sdk-go/service/dynamodb/dynamodbattribute\"\n)\nfunc main() {\n    fmt.Println(\"convert struct:\")\n    fmt.Println(\n        dynamodbattribute.ConvertTo(struct {\n            Strings []string\n        }{\n            Strings: []string{\"a\", \"b\", \"c\"},\n        }))\nfmt.Println(\"convert map:\")\nfmt.Println(\n    dynamodbattribute.ConvertTo(map[string]interface{}{\n        \"Strings\": []string{\"a\", \"b\", \"c\"},\n    }))\n\n}\n```\nconvert map:\ngo\n{\n  M: {\n    Strings: {\n      L: [{\n          S: \"a\"\n        },{\n          S: \"b\"\n        },{\n          S: \"c\"\n        }]\n    }\n  }\n}\nconvert struct:\ngo\n{\n  M: {\n    Strings: {\n      SS: [\"a\",\"b\",\"c\"]\n    }\n  }\n}\n. Hi @murrekatt, thanks for the feedback. The Request.Presign method used to generate presigned URLs is valid for all HTTP methods used by AWS services.\nIn our wiki we have an example how you create a S3 PutObject presigned URL that you can then hand off for another to upload their object to your S3 bucket. With this URL they would just need to create the HTTP request with the POST method, supply the content, and send it.\n. HI @murrekatt I'm going to go ahead and close this issue. Please reopen if there is anything we can do to help clarify how to create a presigned URL, or additional documentation we could add.\n. Discussion original started in #363 for extra context\n. @endophage thanks again for this request. These fields being *string instead of []byte is a representation of the underling wire service protocols. Since the API service clients are generated from service API models their representation will be very similar to the the model definitions. This is also why pointer types are so commonly implemented in the SDK instead of value for future optional parameter support. In this case the message body parameter is a string within the service's model.\nThe SDK does have the ability to create customizations, overriding what is specified in the model. In a few specific cases this functionality is used to modify the ingested models specifically based on the SDK's capabilities. This functionality is used sparingly to prevent creating situations where maintainability is hampered, or preventing backwards incompatible breaking changes. There are numerous other types this style of customization could be applied to also. Each one brings the SDK further away from the service API model's definition, and increases the maintenance burden. Increasing the risk inconsistencies in the API which cannot be corrected without a breaking change.\nBecause of this we would like to hold off making customizations to the service API model like this. Similar types such as Policies and the more extreme CloudFormation Templates would be better and more completely served by concrete types or builders to address their pain points.\n. Thanks again for the feature request. I'm going to close this issue for now since we don't plan on adding this customization to the SDK. Please let us know if you have any addition questions, feedback, or ideas how the SDK can be improved.\n. Thanks for creating this PR @romesh-mccullough. Looks good, going to pull this in.\n. @mattes when creating the Elastic IP about how much time between when the create call is made and the IP is available are you seeing? Is it on the order of seconds, or minutes?\nWhen performing the second allocate Elastic IP does it return the same IP as the first allocation call, or different?\n. @mattes are you still running into the issue where the IP may not be immediately accessible? I think there is a small propagation delay between when an IP is allocated and when it is available. For your code, having retry logic probably will be the best way to to handle this small availability delay.\nI'm going to close this issue, but I encourage you to create a post on the EC2 AWS forums if you are still encountering this issue.\n. Hi @yasker To be safe should wait and ensure the volume is properly mounted to the instance checking attachments state for the EC2 InstanceID the volume was attached to is VolumeAttachmentStateAttached.  The same is true for detaching instances but with volume state is VolumeStateAvailable, and attachement state is VolumeAttachmentStateDetached.\nAlso it's important to note that when detaching a volume that the volume is unmounted in the instance before detaching it.\n. I would take the conservative approach when detaching and check for volume state is available, and attachment list is empty, or all attachment states for this volumeID are detached. Realistically a volume should be attached to only a single instance, but verifying the attachments list will be more robust.\n``` go\nres, err := svc.DescribeVolumes(...)\nfor _, vol := range res.Volumes {\n    id := aws.StringValue(vol.VolumeId)\n    if aws.StringValue(vol.State) == ec2.VolumeStateAvailable && !hasAttachements(id, vol.Attachments) {\n        fmt.Printf(\"volume %s no longer attached\\n\", id)\n    }\n}\nfunc hasAttachements(id string, attachements []*ec2.VolumeAttachment) bool {\n    for _, a := range attachements {\n         if aws.StringValue(a.VolumeId) == id && aws.StringValue(a.State) != ec2.VolumeAttachmentStateDetached {\n              return true\n         }\n    }\n    return false\n}\n``\n. Thanks for finding that @andrewgaul the wiki has been updated.\n. Great find @bpot ! Thanks for submitting this PR. Allocations of the protocol marshallers is something we definitely want to improve. There are a series of benchmarks  ininternal/test/perf/protocol`, but only for marshallers. None for unmarshalling have been added yet.\nI noticed this issue is present in several of the other marshallers, so similar fixes might be able to be applied to them also. I created #377 to track improving the protocol marshallers in general.\n. #404 added a nice improvement to the performance of marshalling JSONRPC, JSONUTIL, and RESTJSON protocols. With a performance improvement of about 60%.  Simple structures didn't see as much impact, but more complex input structs with nested types saw larger improvements.\nmake bench-protocol snip:\n```\nBenchmarkJSONRPCBuild_Simple_dynamodbPutItem:\n0.9.17             50000         32969 ns/op        9757 B/op        211 allocs/op\n8236f00           100000         20773 ns/op        4194 B/op        105 allocs/op\nBenchmarkJSONUtilBuild_Simple_dynamodbPutItem:\n0.9.17             50000         32709 ns/op        9693 B/op        209 allocs/op\n8236f00           100000         20495 ns/op        4130 B/op        103 allocs/op\nBenchmarkRESTJSONBuild_Complex_elastictranscoderCreateJobInput\n0.9.17              3000        439981 ns/op       89382 B/op       2554 allocs/op\n8236f00             5000        280141 ns/op       41221 B/op       1131 allocs/op\nBenchmarkRESTJSONBuild_Simple_elastictranscoderListJobsByPipeline-8\n0.9.17            100000         16040 ns/op        5104 B/op         98 allocs/op\n8236f00           100000         14590 ns/op        4304 B/op         80 allocs/op\n``\n. Hi @bpot Thanks for digging deeper into this pattern using_` is an interesting alternative. Ill review the PR at #453 and update you there with any feedback.\n. Updated in #722. We're currently investigating how the marshalers can be improved. Hopefully this will lead to replacing the memory duplication and reflection type walking with code generation.. Thanks for the feedback @jonaskint. We started the work with this task via #1554 targeting RESTXML and RESTJSON marshaling first. We discovered the SDK's protocol tests were not comprehensive and allowed a bug to leak to master. We reverted the change due to an error in the code generation and marshaling of the payloads. This PR represents the pending work on  the RESTJSON and RESTXML marshalers.  We've not yet started work on improving the unmarshalers.\nFor the s3 manager issue #1554 won't fix that. I think s3manager needs to be refactored significantly to improve its memory usage and footprint. This work is still in our backlog.. Merging #722 and this issue together to unify the discussion and work on this issue.  Both can be solved by improved SDK (un)marshaling support.. @teastburn  have you tried to preallocated the buffer used by the WriteAtBuffer?  You can do this by passing in a preallocated []byte slice into NewWriteAtBuffer.  Without a preallocated buffer each time the underlying buffer of WriteAtBuffer needs to grow it would need to allocate a new byte array and copy in the old data. The old buffer will eventually be garbage collected, but depending on the order of parts downloaded of the object this could cause many allocations. . Thanks for the update @teastburn. Is your application using io.Copy from the resp.Body into another buffer, or is your application reading directly from the resp.Body reader?. Using the buff reader like your are is a good way to get the ByteReader interface. I suggest instead of the close method on S3Getter that the returned reader has the Close method on it. This would allow close handling to be performed by the downstream consumer of the stream, instead of making S3Getter stateful. If S3Getter.Get is accidentally called before the S3Getter.Close method is called, the previous object's body may never be closed.\nSomething like the following might work better for closing the body.\n```go\ntype ByteReadCloser interface {\n    ByteReader\n    io.Closer\n}\ntype S3FileGetter struct {\n     Client *s3.S3 // Add client shared across all object gets.\n}\nfunc (s3g *S3Getter) Get(s3Bucket, s3Path string) (ByteReadCloser, error) {\n    // ...\n    resp, err := s3g.Client.GetObject(&s3.GetObjectInput{\n        Bucket: aws.String(s3Bucket),\n        Key:    aws.String(s3Path),\n    })\n    // ...\n    return NewBufferedReadCloser(resp.Body), nil\n}\ntype BufferedReadCloser struct {\n    *bufio.Reader\n    io.Closer\n}\nfunc NewBufferedReadCloser(reader io.ReadCloser) *BufferedReadCloser {\n    return &BufferedReadCloser{\n         Reader: bufio.NewReader(reader),\n         Closer: reader,\n    }\n}\nfunc (b *ByteReadCloser) Close() error {\n     return b.Closer.Close()\n}\n```\nThen be used similar to the following. Also, try to only initialize the S3 client once per region/account to ensure that duplicate resources aren't created within the applications.\n```go\nobjGetter := S3Getter{Client: s3.New(session.Must(session.NewSession(s3Cfg)))}\n// Get each object, and process it\nreader, err := objGetter.Get(myBucket, myKey)\n// handle error\ndefer reader.Close()\n// process reader content.\n```. Hi @yaso195 I'm trying to reproduce the bug you're seeing but not having much luck.  Would you mind adding a debug parameter to your configuration which will log the HTTP body of the request and response for this service call.\nIn the text you paste you can delete the AccessKey from the request URL and UserId/Arn fields from the response if it is successful. We won't need those.\ngo\n// I'm assuming iamcli is a *iam.AIM instance\niamcli := iam.New(aws.NewConfig().WithLogLevel(aws.LogDebugWithHTTPBody))\nThe test i used was:\n``` go\npackage main\nimport (\n    \"fmt\"\n    \"github.com/aws/aws-sdk-go/aws\"\n    \"github.com/aws/aws-sdk-go/service/iam\"\n)\nfunc main() {\n    svc := iam.New(aws.NewConfig().WithLogLevel(aws.LogDebugWithHTTPBody))\nres, err := svc.CreateUser(&iam.CreateUserInput{\n    UserName: aws.String(\"temp_user\"),\n    Path:     aws.String(\"/\"),\n})\n\nfmt.Println(res, err)\n\n}\n``\n. Hi @onlyjob like @jeskew mentioned the comment refers to the content under the comment which is a standard practice attributing third party content. End SiteCatalyst code version: H.25.2. ` on line 30 denotes where the attributed content terminates.\nThe linked third party script is not included with the SDK, and is a runtime dependency of the generated API documentation available at http://docs.aws.amazon.com/sdk-for-go/api/\nPlease contact us directly, and reference this github issue, at aws-security@amazon.com with any additional information about your concerns of privacy breach and unsafeness of the third party library.\n. Hi @onlyjob, thanks for the feedback. I'm going to close this issue since the third party library is attributed, and not used within the SDK. But only within the SDK's API documentation which is also external to the SDK. \n. @onlyjob, I'm not sure I understand the issue that would prevent including the SDK in a Debian package. The third party js file is not included within the SDK's repository. It is only referenced in a html template file, which is not used by the SDK it self and only used as a template to generate API documentation.\n. Hi @jitcompile thanks for reporting this issue. When you encountered this issue, was the region specified as us-east-1? In addition was the Config.Endpoint value set for this operation request? I'm trying to reproduce the issue you're seeing with the below code, but it's coming back with the correct response.\nIf you could add your configuration, and even better HTTP body output like in the following sample it would be very helpful.\n``` go\npackage main\nimport (\n    \"fmt\"\n    \"github.com/aws/aws-sdk-go/aws\"\n    \"github.com/aws/aws-sdk-go/service/s3\"\n)\nfunc main() {\n    svc := s3.New(aws.NewConfig().\n        WithLogLevel(aws.LogDebugWithHTTPBody).\n        WithRegion(\"us-east-1\"))\nres, err := svc.GetBucketLocation(&s3.GetBucketLocationInput{\n    Bucket: aws.String(\"bucket-thats-in-ap-southeast-1\"),\n})\n\nfmt.Println(res, err)\n\n}\n```\n. Thanks for the extra information. I think I figured this problem out. The GetBucketLocation should be using the S3 path style where the bucket name is apart of the URI path instead of host.\nA workaround you can use until this is fixed is in your Config enable S3ForcePathStyle. This will ensure the S3 service client makes the request correctly for cross region bucket information.\ngo\nsvc := s3.New(aws.NewConfig().\n        WithS3ForcePathStyle(true)\nor\ngo\nsvc := s3.New(&aws.Config{S3ForcePathStyle: aws.Bool(true)})\n. Hi @jitcompile This bug was fixed in the SDK's latest release. If you sync with it you'll be able to use GetBucketLocation without setting the S3ForcePathStyle config option.\n. Thanks for reporting this @radeksimko. The new field is now available in the latest version (v0.9.7) of the SDK.  Let us know if you run into any problems with it, or have additional feedback.\n. Hi @dragonfly90 Thanks for pointing this out.  params := &ec2.RunInstance{ should of been params := &ec2.RunInstanceInput{. Input was missing from the struct name. The wiki had a typo, and has been corrected.\nLike @marcosnils mentioned the api docs are a great additional place to find examples, and they will cover all service API operations. Especially the required fields.\nIf you run into any other issues, or have suggestions please let us known.\n. Hi @santhoshdaivajna it looks like the SDK is making the request correctly. I verified your issue with the sample code you provided (Thanks!). This looks to be an issue with EC2 not returning prcing data for VPC instance types. I also verified similar issue with the Amazon EC2 Console Pricing History which does not show VPC pricing.\nI suggest posting this issue on the AWS EC2 forums for a faster response. I'll also forward this issue along to the EC2 service team so they can take a look at it. I'm going to go ahead and close this issue since it's not a bug with the SDK itself. \n. Hi @mtibben thanks for posting this issue. Correct the Go SDK only supports the credentials file not the AWS CLI's config file. I'll mark this as a feature request were we could add support for the CLI specific configuration options.\n. Other than the CLI config, the only environment vars the Go SDK doesn't support which the CLI is AWS_DEFAULT_REGION and AWS_DEFAULT_PROFILE in the SDKs these are AWS_REGION, AWS_PROFILE. \nIt should be noted though that adding support for these two additional environment configuration values would introduce breaking changes into the SDK.\n. Thanks for the suggestion @phoolish This is a nice feature of the CLI and would add nice functionality. I think you're able to do this now with the Go SDK but it requires a bit of manual process. Having this built in would be helpful.  We're always looking for new features and PR, if you'd like to take a look at how this could be added to the SDK we'd be more than glad to review it. If you do, please create an additional issue so we can track that feature separately.\n. @smile-on Thanks for looking into this. We haven't added because we need to determine, if the SDKs were to support the CLI's config file, dhow far to go with the integration. Generally the region and credentials are helpful because it is an obvious configuration value which applies to all the SDK's and CLI.  But the majority of the CLI's config file is specific to the CLI and its features. \nIn addition any change we make to how the SDK sources its configuration will need to backwards compatible. This most likely will mean loading a configuration from an additional source will need to be an opt-in process. Either through a flag in code or or environmental variable so that it does not automatically enable unexpectedly.\n. Hi i'm starting to look at implementing this feature. Support for this feature would need to be opt so not to unexpectedly break user application's. Users should be able to opt into this feature vai an env var, and via in code configuration.\n. Thanks for the link @mtibben I've created a PR #761 which is the initial step to update the SDK to load configuration from the shared config file (~/.aws/config)  in addition to the shared credentials file (~/.aws/credentials). I'm working on the Assume Role work now, and will update the PR once that work is ready. \n. Hi all I've created PR #761 which adds support for reading the shared config file to the SDK, in addition to assume role from a profile in the SDK's default credential chain.\nI'm looking to gather any feedback about the change.\n. Thanks for the update, and link @mtibben! I've merged in the change that implements the shared config with assume role support.\nLet us know if you have any issues or additional feedback.\n. Thanks for reporting this @gregory-m you're correct that the current s3Manager's s3 client cannot be mocked out. I took a look and I don't see a reason we couldn't update the s3 Upload and Download managers to use the interface instead.\n. @gregory-m I just pushed a change which replaces the Uploader and Downloader's option's S3 client to the interface instead of concrete type. Let us know if you run into any issue, or have other feedback.\n. Thanks for creating taking the time to create this PR @vito . It looks good I'll update the comment and pull it in.\n. Thanks @vito I pushed this PR, closing this copy.\n. Hi @aweick, it looks like the issue you were having is resolved. Like @reedobrien mentioned S3 does not actually store objects in a file hierarchy, but by a key. Tools such as the AWS S3 Console will display content as a file hierarchy by interpreting the \\ as directory separators to make it easier to view buckets with a lot prefixed of content.\nIf you're looking to maintain this functionality I agree using path.Join() is a good way to join these components. path.Join is OS/Platform augnostic using \\ as path component seperator.\n. Hi @endophage I think this is an issue with the S3 documentation could be more clear. Marker field refers to where S3 will start listing from, but is non-inclusive. This means S3 will use the Marker to start searching from and return objects which come after the Marker lexicographically. So Marker doesn't function as a traditional pagination Token parameter.\nAs an example if we have objects with the following keys in our bucket.\nresource/001/item-01\nresource/001/item-02\nresource/001/item-01\nA Marker of resource/001/item-01 will return a list of objects starting with resource/001/item-02.\nAlso it's worth noting that the Marker value does not actually need to be an existing object key. This means that you could use a lexicographical prefix to specify where in a bucket to start listing objects from. If we continue on the previous example. A Marker of resource/002/ will return a list of objects starting with resource/002/item-01 and continue lexicographically from there.\nI'll forward this issue on to the S3 so the documentation can be improved.\n. Hi @endophage I've forwarded this issue along to the S3 team, and will go ahead and close this issue since the issue is with the documentation provided by S3. I would suggest creating an issue on the S3 AWS forums for visibility, since there may be others which encountered similar issues. Let us know if there is any other questions we can answer, or any feedback you may have.\n. Hi @luck02 Correct CreateStreamt response is intentionally empty. Your code should poll for the stream's status to go from CREATING to ACTIVE.\nAre you seeing issues with the stream's not becoming active?\n. Thanks for the feedback @luck02. There are a few API operations which use empty structs as request input, or output response. The SDK keeps these structs around so that if optional fields are added in the future to these API operations the SDK would not need to make a breaking change to support them.\n. @luck02 I'm going to close this issue since the empty responses are by design, so the SDK can support service APIs returning responses in the future without making breaking changes to the SDK.  Please reopen if there is anything we can clarify or, more documentation that could be added.\n. Hi @rosenhouse we would be more than glad to help getting some concrete models created for these templates.  The SDK doesn't have any models which we can generate template code from. The closest I could find is the documentation in Template Anatomy. \nI think for the top level concrete type for the Template makes sense. Metadata, and Properties probably can be just map[string]interface{} since they are fairly freeform. I think the others can be map[string]<type>\n`` go\ntype Template struct {\n    AWSTemplateFormatVersion stringjson:\",omitempty\"Description              stringjson:\",omitempty\"`\nMetadata map[string]interface{} `json:\",omitempty\"`\n\nMappings   map[string]Mapping   `json:\",omitempty\"`\nParameters map[string]Parameter `json:\",omitempty\"`\nConditions map[string]Condition `json:\",omitempty\"`\nResources  map[string]Resource  `json:\",omitempty\"`\nOutputs    map[string]Output    `json:\",omitempty\"`\n\n}\ntype Output struct {\n    Description string json:\",omitempty\"\n    Value       string\n}\ntype Mapping map[string]map[string]string\ntype Parameter struct {\n    // TODO fields\n}\ntype Resource struct {\n    // TODO fields\n}\ntype Condition map[string]interface{} // Not sure what this structure should be yet.\n```\nI think all of the Fn:x elements we probably should model as types so we can attach custom marshallers. Similar to how the SDK does it for CloudFront URL policy signing. Though this creates a complication that we'd need to figure out with parts like Fn::FindInMap since they can be used in place of string values. I think it makes sense for these be helpers which provide a String() method. \n. HI @rosenhouse I'd like to move the cloud formation templates out of the sdk temporarily to a repo under awslabs while we're in the process of developing the template code. This will ensure breaking changes will not hinder the development of the template package. Once we have the template code being generated and stable we can merge it back into the SDK as service/cloudformation/template like it is currently. How would this impact your development/release flow?\n. @rosenhouse I've moved the CloudFormation template code to github.com/awslabs/aws-cfn-go-template We can collaberate here, where we can add the generated code to.  Once it's stable we can merge it back into the Go SDK.\n. Hi all,  CloudFormation has started a new project creating Go utilities for working with cloud formation templates. Check them out at https://github.com/awslabs/goformation. Since CloudFormation is in the works to create a library for this feature I'll close this issue.. Thanks for creating this issue @vtapaskar I'll add this to our backlog. \nTo implement this I think we need update our API generation code (internal/model) to extract the error codes from the API model json docs. More investigation is needed, but it may make sense to collect shapes which have an error.code property, and write the error shape name and error code string as const for each service api.\nAuto Scaling group error example:\njson\n    \"LimitExceededFault\":{\n      \"type\":\"structure\",\n      \"members\":{\n        \"message\":{\"shape\":\"XmlStringMaxLen255\"}\n      },\n      \"error\":{\n        \"code\":\"LimitExceeded\",\n        \"httpStatusCode\":400,\n        \"senderFault\":true\n      },\n      \"exception\":true\n    }\nThis would generate something like the following in service/autoscaling/api.go\ngo\nconst (\n     // Errors\n     ErrCodeLimitExceededFault = \"LimitExceeded\"\n     // ...\n)\n. Thanks for the suggestion @vtapaskar . I think some of the service generic error codes could be moved to a top level like you mentioned auth, and in addition some of the service unavailable error codes.\nFor httpStatusCode are you looking for the association of status code to error code, or just the status code to be included in the error? Currently, if the error is an awserr.RequestFailure it will include the http status code. In addition to this if you use the request form of the operation. e.g s3Client.PutObjectRequest the request.Request object returned can also be used to get the http status code for all requests, not just errors.\n. Hi @vtapaskar and @polds I've added generated error codes for all of the errors service teams model in their APIs.  This doesn't include all errors, especially for S3 which does not currently model most of the error codes the service can return.  I'll reach out to the service team about modeling the service's error codes.. Closing this issue as the SDK generates the error codes that are modeled. I've reached out to the S3 service to add error codes to their API models. Closing this for now since the feature is implemented and when services add error codes to their models they will be automatically picked up by the SDK.. Hi @radeksimko, I pushed release 0.9.11 which includes support for the new Amazon Elasticsearch Service. This should get you up and going with the new service. Let us know if you have any feedback, or issues.\n. @radeksimko the service names are generated based the model defining each service API and its metadata. In this case the service's public name is \"Amazon Elasticsearch Service\" the SDK's API generation logic will take this string strip off the \"Amazon\", remove spaces, and lowercase the name.\n. Going to go ahead and close this issue since the service is now released. Let us know if you have any feedback, issues, or questions.\n. Looks good, thanks for creating this PR. Would you mind adding a comment to the right of those fields for the service they are associated with?\n. Hi @treystout I would expect the service API to fail immediately with no timeout. A credentials chain is a list of credential providers which will attempt to retrieve the credentials from their associated source. The default Config uses a credentials chain of the following provider. Environment variables, shared credentials file (~/aws/credentials), and finally EC2 instance role.\nHow are the user's credentials being given to your application, or how does the application expect to use them? Are you setting the Config.Credentials manually?\n. Thanks for the update. Where should your customers provide their AWS credentials? By default the sdk will look in Environment variables, shared credentials file, and finally ec2 instance roles. Is this code expected to run on ec2 using instance roles?\n. I see now thanks for the update. It looks like the delay is occurring when the SDK attempts to get the EC2 instance role. The SDK makes a request to the EC2 metadata service (169.254.169.254) and this times out after 30 seconds. This request is retried 3 more times causing the 2 minute delay your seeing.\nI think we can reduce this to 10-20 seconds by using a dial timeout on the default http client used by the ec2metdata. Some timeout is still needed because when running on an EC2 instance the EC2 metadata service may not be up and running for a few seconds after the instance is started. \n. Hi @treystout @caitlin615 I updated the ec2metadata client to use a shorter connect timeout. So when the SDK searches for EC2 instance roles when not running in EC2 the credential chain will fail much sooner than it was previously.  In my test it failed within 5-15 seconds.\n. Thanks @paulmach for the heads up on this issue. In hindsight it makes sense that rand.Source needs to be concurrency safe when used across goroutines. This should be fairly easy to fix. For now I think copying the lockSource from math/rand since it's exactly what we want. In the long run I've opened a discussion with the golang-dev group on google groups about exposing the lockedSource.\nIn the end since jitter's randomality isn't really critical, using the built in global rand functions will work just as well. For this use case request retry delay just needs to be varied by some small amount.\n. @paulmach , thanks for creating this issue. It is updated now and the SDK no longer be causing issues if you're using the --race build flag with your programs. Let us know if you encounter any other race conditions, or have other feedback.\n. Thanks for starting this @andrewgaul. You're correct S3 uses an alternative signing method. I'm thinking this might make sense as a drop in request handler that could be set specifically by the user of the service client. Since the signing wouldn't be standard, and and alternative endpoints are being also set. This would also help to isolate the S3 specific code into a module separate from the service similar to how the existing signers and all their configuration is internal within the request handler.\n. Hi @tumiao Like @jjeffery mentioned the v2 signer included with the SDK is not compatible with S3.  This is because S3 does not actually support v2 signatures. S3 only supports S3 V4 signatures, and a older S3 V2 signature. These S3 specific signers are different than the generic AWS V2 and V4 signature versions.  S3 V4 signature version is the preferred signer to use for S3, as it is supported in all regions and V2 is not.\n@tumiao What situation are you seeing where you need to use the S3 V2 signature version?\n. Thank you all for providing feedback for this feature. We do not plan to provide support for the signature version 2 for S3 to the SDK, because S3 will stop accepting requests signed using SigV2 in all regions on June 24, 2019 per the AWS Forums announcement. \nWith that said the SDK does support injecting customer signers into a service's Sign request handler list using the request.HandlerList.Swap functionality. This will replace the request handler of the name provided with your custom handler.\n```go\nsvc := s3.New(sess)\nsvc.Handlers.Sign.Swap(v4.SignRequestHandler.Name, func(req *request.Request) {\n   // TODO custom signing logic.\n})\n// Make API operation calls with svc client.\n```\nRelated: #2212 & aws/aws-sdk-go-v2#228. Hi @rosenhouse thanks a lot for taking the time and putting this preliminary PR. I'm looking for predefined models of the template shapes to see if there is something we can use to generate code from.  In the mean time I'll review the PR and get back to you with feedback.\nUsing helper functions instead of structs for Fn::x should work just as well too. \n. Thanks a lot for your hard work getting this PR together @rosenhouse. It looks good and I think we can pull it in.\nI agree being able to generate the code would be the best plan, especially for the resources and property resources. The .NET SDK uses a JSON scheme for their Visual Studio's CloudFormation Template generator wizard. I'm investigating if it is possible for the Go SDK to reuse the scheme for our resource and resource-property structs.\n. Looks great, thanks, and I merged the change in. This will be a great place to get started building templates programmatically. I'll update #393 when more information is available about auto generating the resources and resource-properties.\n. The AWS SDKs currently don't support API Gateway, because it uses HAL. A format not yet supported by the SDKs. Lets use the original feature request, #338, to track this feature, and we'll post updates there.\n. Hi @wmh thanks for reporting this issue. I'm taking a look at what is going on here. I've been able to reproduce this locally. I'll let you know when I have some information.\n. @wmh I did some digging into this issue. It looks like this effect is expected. The reason the CacheControl is not being applied is because without the s3.CopyObjectInput.MetadataDirective value also set to s3.MetadataDirectiveReplace the metadata directives are ignored. The documentation isn't very clear here, and I'll forward this onto the S3 team.  Its also worth noting that MetadataDirectiveReplace will replace the metadata, and not copy the original object's metadata. The only current way to copy and merge in new metadata is to gather all of the original object's metadata yourself and build the CopyObjectInput with all values you want to exist on the new object.\n. @wmh If you wanted to post an topic on the S3 forums about this issue it might help speed up the process of adding copy with merge feature.  In addition there may be other users experiencing the same issue.\n. Hi @wmh Going to go ahead and close this issue. Please let us know if there is anything else we can help with. \n. Thanks for the gettings this PR together @bpot! Its great to see significant improvements in the marshallers performance. I'll take a look at the code and get back to you with my feedback.\nRelated to #404\n. Great change @bpot this reduced the time, allocations, and total bytes allocated per RESTJSON, JSONRPC, and JSONUtil by about 60%.  I pulled this change in and is available at tip of master.\n. @n-boy thanks for contacting us. The SDK doesn't use bufio reader's directly, so I'm not sure yet how the SDK could be updated to use larger buffer sizes.  Is the file you're trying to upload being read across a network filesystem, or from a slow medium (CD/USB/ect)? If so the following might help.\nI'm curious if wrapping the file in your example with a bufio.NewReaderSize would help with the issue you're seeing.\ngo\norigFileReader, _ := os.Open(filePath)\ndefer origFileReader.Close()\nfileReader := bufio.NewReaderSize(origFileReader, 128 * 1024)\n// ...\nSince os.File is not buffered, any IO delay in reading from the file will be present in the network request performance. Adding a buffering wrapper around os.File might help your performance without needing to modify the Go stdlib.\n. Updating this a feature request to investigate and improve the performance of upload requests using Windows. This sounds like it might be a issue with the stdlib bytes buffer size and windows vs linux.\n. Closing this issue since it is related to Go stdlib's usage of bytes.Buffer. #377 and #722 should address the memory issue for all platforms.. Hi @brunoksato thanks for contacting us.  At the moment the Go SDK does not provide the helper the JS SDK has.  I've marked this as a feature request, as it would be good for the SDK to provide a helper to retrieve Cognito Identity Credentials.\nTo do this now your self in Go you could use the following API calls. This is the same thing the JS SDK does but wrapped in a credentials provider.\nIf credentials are to be accessed with just an \"IdentityPoolId\"\n``` go\nsvc := cognitoidentity.New(nil)\nidRes, err := svc.GetId(&cognitoidentity.GetIdInput{\n    /input params/\n})\ncredRes, err := svc.GetCredentialsForIdentity(&cognitoidentity.GetCredentialsForIdentity{\n    /input params/\n})\n// credRes.Credentials  contains the AWS credentials\n```\nIf credentials are to be accessed with both an \"RoleArn\" and \"IdentityPoolId\"\n``` go\nsvc := cognitoidentity.New(nil)\ntokenRes, err := svc.GetOpenIdToken(cognitoidentity.GetOpenIdTokenInput{\n    /input params/\n})\nstsSvc := sts.New(nil)\nroleRes, err := stsSvc.AssumeRoleWithWebIdentity(&sts.AssumeRoleWithWebIdentityInput{\n    /input params/\n})\n// roleRes.Credentials contains the AWS credentials\n```\n. The documentation for AssumeRoleWithIdentity can found on the SDK's API documentation.\nYou should be able to use GetOpenIdTokenForDeveloperIdentity to regenerate the token and provide that token to STS's AssumeRoleWithWebIdentity. The API doc link above will help you with which params are needed in your case.\n. Thanks for contacting us @bctsui. Yes you'll need to close the GetObjectOutput.Body io.ReadCloser when your done with it. This body is basically the same as a http.Response.Body and needs to be closed after ever response.  Thanks about pointing out the documentation could be better. Tagging this so we can add documentations around this.\n. Hi @bctsui I've updated our wiki documentation to include the need to close the body on operation responses with a Body or io.ReadCloser fields.\nhttps://github.com/aws/aws-sdk-go/wiki/making-requests#handling-operation-response-body\n. Hi @lixingwang thanks for contacting us.  The Go SDK does not currently provide the Optimistic Locking feature. I'll make this as a feature request and we can add it to the features we would like to implement.  We are also always looking for pull requests if you're looking to add, or get started, with this feature.\n. @psankar The Java and .Net api docs are the two best sources for how optimistic locking can be implemented.\nIf you have an specific questions we'll be glad to help investigate the best way this logic can be implemented in the SDK.\n. An alternative to annotations could be struct tags. This would allow us to use user defined structs with reflection, but it would make it difficult, if not impossible, to use maps. Alternatively we'd could consider a builder pattern. But you're right we should think through this design so it is easy to use but also not restricting.\nWe'd like to keep the discussion going here or Gitter so that its easier for everyone to track the progress and changers going on since the two services are fairly well integrated. For code segments we can link to Gists of sample prototypes. Wed' definitely glad to see the work you did with your test program.  The best way to include sample programs is a PR to the project's /example/ folder\n. Released this change in tag v0.10.0\n. Hi @kahing thanks for creating the PR. In the error log you provided were you using the aws.LogDebugWithRequestErrors LogLevel?  From the error message it looked like the request was expected to retry. Did you see multiple entries of the following?\n\n2015/10/27 22:03:03 DEBUG: Validate Response s3/UploadPart failed, will retry, ...\n\nWould you be able to include the error code and message that you received as a response for the UploadPart APi call?\n. From what i can find this error occurs when the service does not read the full Content-Length specified in your request. Specifically this can occur when the Content-Length is greater than the number of bytes sent in the request.\nSometimes this could occur the SDK retrying a request which failed, but for some reason did not rewind the request body, or a data corruption occurred. Are you able to reproduce this bug consistently?  I don't see any reason not to take in this change, but i want to make sure there is not another issue going on in the SDK that will be hidden with the retries.\nRelated to:\n- aws/aws-sdk-js#281\n- https://forums.aws.amazon.com/message.jspa?messageID=215367\n. So it sounds like in your case the full 5mb is not being uploaded. Is this correct? Something happens causing only part of the bites to be sent. Are you connecting through a proxy? Also is your app running on a server like EC2 instance or locally on a dev box?\n. Thanks for the info! Pulling in the PR.\nI would not of expected that delay in an EC2 instance.  Out of curiosity is your data coming from an os.File or some other buffer. Is it possible the provider of the buffer is not providing data as fast as expected, or gets stuck?  Have you tried to to send just something like bytes.NewBuffer(make([]bytes, 5 * 1024 * 1024)) instead to see if the issue still occur?\n. Merged in this PR, ae0b628\n. Thanks for submitting this PR @ablewhiskey looks good and merged in.\n. With the addition of request options on API this functionality is accessible by using request options.. Thanks for getting in contact us @salmanbukhari. This looks to be a bug in the documentation. The documentation should say that these fields cannot be specified at the same time. In addition EnableDnsHostnames can only be enabled if EnableDnsSupport was previously set.\nI'll forward this issue to the EC2 team so they can correct their documentation. It may also help if you post your issue on the EC2 AWS forums. Other people may of also run into this issue, and it will help resolving this issue sooner.\n. Correct, the ModifyVpcAttribute API requires that only one be provided at a time. Improving the documentation to clearly state this will hopefully clear this up.\n. Thanks for catching this type-o @jesusjjf. Pulling it in. \n. Hi @philpennock thanks for getting in contact with us. I've been able to reproduce the issue you reported. It looks to occur anytime a Object is downloaded with the S3 Download manager which is less than 5MB. Is this consistent with what you are seeing? \nI think you're correct that the best change is if ContentRange is nil grab ContentLength instead.\n. @philpennock I just pushed 2c52a85d which fixes this issue. Thanks for reporting this issue!\n. Hi @zeedunk, thanks for contacting us. Is it possible the delimiter parameter is empty? If the delimiter is  not set at all or an empty string no NextMarkers are returned. If a delimiter is provided NextMarkers will be included in the response. \n``` go\npackage main\nimport (\n    \"fmt\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\n)\nfunc main() {\n    svc := s3.New(session.New(aws.NewConfig().WithLogLevel(aws.LogDebugWithHTTPBody)))\n    err := svc.ListObjectsPages(&s3.ListObjectsInput{\n        Bucket:    aws.String(\"bucket-name\"),\n        Delimiter: aws.String(\"/\"),\n        MaxKeys:   aws.Int64(10),\n    }, func(p *s3.ListObjectsOutput, lastPage bool) bool {\n        fmt.Println(\"items\", len(p.Contents), aws.StringValue(p.NextMarker))\n        return true\n    })\n    if err != nil {\n        fmt.Println(\"error\", err)\n        return\n    }\n}\n``\n. Hi @zeedunk Are you still having issues with working with S3 pagination?\n. Hi @zeedunk I'm going to close this issue. Please let us know if you're having issues with paginating requests, or any other feedback.\n. Hi @stack72, Are you getting an error response? If so what is the error you're getting.\n. @stack72 the issue you are seeing is [intentional](http://docs.aws.amazon.com/Route53/latest/APIReference/API_CreateHealthCheck.html#create-health-check-request-measure-latency). Once theMeasureLatencyis set on a Route53 health check it cannot be changed. Since theMeasureLatencycannot be changed during an update, the only option is to delete the health check, and recreate it with MeasureLatency, set to the value you want.\n. Glad to help, let us know if have any other questions, feedback, or issues.\n. Hi @AbhishekSaha In [release v0.10.0](https://github.com/aws/aws-sdk-go/releases/tag/v0.10.0) we made a breaking change which updates the SDK to use asession.Sessionwhen creating service clients. Sessions add the benefit of a central configuration, replacing the potentially racydefaults.DefaultConfig, and simplify settingrequest.Handler` which are shared across multiple service clients.\nIf you'd like to update your code to use the latest version of the SDK the the simplest way is to add a session.Session to your code. Session can be found in the github.com/aws/aws-sdk-go/aws/session package. A session is safe to share between multiple clients concurrently. Though it is not safe to modify the session concurrently. Clients receive a copy of the session and will not be impacted by any changes made to a session after the client is created.\nThe following will examples provide suggestions on how you could update your current code to use the new Sessions.\ngo\n// This code will be used as an example of SDK v0.9.17 code,\n// and how it can be updated.\nsvc := dynamodb.New(&aws.Config{Region: aws.String(\"us-east-1\")})\nAdd session to a service client\ngo\nsvc := dynamodb.New(session.New(), &aws.Config{Region: aws.String(\"us-east-1\")})\nShare a session between service clients\n`` go\n// All clients created with this session will also be configured for theus-east-1` region.\nsess := session.New(&aws.Config{Region: aws.String(\"us-east-1\")})\nddbSvc := dynamodb.New(sess)\nsqsSvc := sqs.New(sess)\n```\nShare session, and have client specific configuration\n`` go\n// All clients created with this session will also be configured for theus-east-1` region.\nsess := session.New(&aws.Config{Region: aws.String(\"us-east-1\")})\nddbSvc := dynamodb.New(sess)\n// Add specific configuration to the SQS service client instance.\nsqsSvc := sqs.New(sess, &aws.Config{LogLevel: aws.LogLevel(aws.LogDebug)})\n```\nShare session with shared request handlers\n``` go\n// Create a session, and add additional handlers for all service\n// clients created with the session to use.\nsess := session.New()\nsess.Handlers.Build.pushBack(func(r *request.Handlers) {\n    // Log every request made and its payload\n    logger.Println(\"Request: %s/%s, Payload: %s\",\n        r.ClientInfo.ServiceName, r.Operation, r.Params)\n})\n// Service clients created with the session will log all requests.\nddbSvc := dynamodb.New(sess)\nsqsSvc := sqs.New(sess)\n```\nIf you'd like to stay with the v0.9.17 version of the SDK you can use the following console command to update which version your local workspace contains.\nsh\ncd $GOPATH/src/github.com/aws/aws-sdk-go\ngit fetch\ngit checkout -b v0.9.17 v0.9.17\n. Hi @AbhishekSaha let us know if this didn't resolve the issue you are experiencing. Closing this issue for now. If you're able I'd suggest using Go 1.4 and then you shouldn't be experiencing any issues with the SDK.\n. HI @tfutada from the stack trace you linked it looks like the problem you're running into is because the SDK is because there might be multiple versions of the SDK being imported. From the stack trace, the type wanted is github.com/aws/aws-sdk-go/aws/client\".ConfigProvider.\nSpecifically the type used is:\ngo\nClientConfig(string, ...*\"github.com/tfutada/vendor/github.com/aws/aws-sdk-go/aws\".Config)\nBut it should be \ngo\nClientConfig(string, ...*\"github.com/aws/aws-sdk-go/aws\".Config)\nNotice the type's import path of the aws.Config is different.\n. Hi @AbhishekSaha This is an odd error to be seeing. Is the error you see when compiling the SDK with your code? What version of Go are you running on your linux machine? You can check with go version.\nThis suggest there might be a conflict in your Go package cache. It might be necessary to run the following command to refresh Go's package cache for the SDK.\nsh\ncd $GOPATH/src/github.com/aws/aws-sdk-go/\ngo clean ./...\ngo install ./...\n. Ah I see that might be the problem the SDK requires a minimum of Go 1.4. If you are able to get a system with a more uptodate SDK I'd suggest that first. Alternatively I think you can work around this by selecting an older version of the SDK. It Looks like the net.Dialer usage by ec2metadata was added in v0.9.15.  If you set your SDK to v0.9.14 I think you should be able to work around the Go version limitation.\nsh\ncd $GOPATH/src/github.com/aws/aws-sdk-go/\ngit checkout -b v0.9.14 v0.9.14\ngo install github.com/aws/aws-sdk-go/...\n. Correct, for 0.9.14 its like the following where each service client is given a config when created. Alternatively you could also pass in nil to the New function to use only the default configuration. \ngo\nsvc := dynamodb.New(&aws.Config{Region: aws.String(\"us-west-1\")})\n. I'm not aware of an issue with aws.DefaultRetries off hand. Where are you see the error that it is not undefined at?\n. i think you might be having a conflict of versions of the SDK. In the file $GOPATH/src/github.com/aws/aws-sdk-go/aws/config.go DefaultRetries should be defined near the top of the file as:\ngo\n// The default number of retries for a service. The value of -1 indicates that\n// the service specific retry default will be used.\nconst DefaultRetries = -1\nDo you see this?\n. I haven't been able to reproduce this issue on v0.9.14 locally. One way that might work to fix this is to remove the $GOPATH/src/github.com/aws/aws-sdk-go directory, cd into $GOPATH/src/github.com/aws and git clone https://github.com/aws/aws-sdk-go. Then checkout the specific version of the SDK git checkout -b v0.9.14 v0.9.14.\n. Hi @AbhishekSaha I'm going to go ahead and close this issue since #421 it looks like you were able to get a previous version of the SDK.\n. Hi Thanks @kirupani for suggesting this feature. I've marked this as a feature request.  We are also are glad to review Pull Requests if you're interested in contributing. \n. The AWS SDK for CPP implemented clock skew in aws/aws-sdk-cpp#285. This SDK should consider that design.. Hi @upccup thanks for contacting us. Is it the TaskAgent.Params field you are looking to unmarshal? Would you be able to include the struct you're wanting to unmarshal the JSON to?\nThe SDK uses custom tags and marshallers so the SDK can unmarshal additional metadata provided by the AWS services. Because of this you may not be able to marshal or unmarshal SDK operation structs.\nI created this example on Go Playground to highlight this issue.\n. Hi @upccup, are you looking for the SDK to provide the functionality to register your types with during the unmarshal of response from service API calls? At the moment the SDK does not have a way to register handler types for content messages received as API call responses. The best workaround for this is to use a switch based on different message types that can be handled.\n. Great, let us know you have any other questions or feedback.\n. Hi @rjeczalik thanks for getting in touch with us. I agree being able to set the retryers more generally would be a good enhancement. I'd like to find a way to ensure at compile time RequestRetryer satisfies the client.Retryer interface without circular dependencies. Because of this, adding a typed RequestRetryer to Config is going to be problematic. Adding the retryer to the session.Session would alleviate this problem, but creates the situation where SDK begins to store its configuration in multiple types without a clear definition between the two.\nConceptually I think this issue is similar to #414. Instead of generically adding request handlers, you would add handlers which modify service clients generically when they are created with a session.\n. The initial idea around client.ConfigProvider as an interface is so that additional functionality could be added by conditionally casting to additional interfaces. session.Session would always be updated to support satisfying these additional interfaces. Specifically this could be used for more robust retry logic such as per operation retry logic and deadline timeouts.\nSince the SDK isn't GA yet we still have room to make some design changes as needed to improve the SDK. With that in mind, are there any changes you'd make to the SDK to better support concepts like passing the retryer safely into service clients?\n. Hi @rjeczalik we would be glad to get a PR on this feature. I agree, adding the RequestRetryer to Config probably is the best path here. Then with the request.WithRetryer typed helper func would be good addition. We probably would also want to add a Config.Logger.Log() in client.Client when the Config.RequestRetryer does not satisfy the request.Retryer interface.\nFor now I'd like to leave handlers on session since working with them on Config would require casting, and for #414.\n. Numbers should be stored in the StringValue or StringListValues fields as a string representation of the number. This field is a string so very large/tiny numbers are support. The service will parse the number as a decimal. As a string the service is able to support multiple number formats; integers, float, double, ect. The SQSMessageAttribute DataTypes documentation has some more information on using Numbers.\n. Hi @ando-masaki thanks for contacting us. Which version of the SDK are you using and. From the code above it looks like a custom credential chain is being setup is this correct?\nIf you're using the latest version of the SDK I think the issue is that in v0.10.0 a change was made which added Sessions to the SDK. Along with this change the ec2rolecreds.EC2RoleProvider requires a ec2metatdata.EC2Metadata client. The below example should be fix this issue.\n``` go\nsess := session.New() // Or your existing session.\ncreds := credentials.NewChainCredentials([]credentials.Provider{\n    &credentials.EnvProvider{},\n    &credentials.SharedCredentialsProvider{Filename: \"\", Profile: \"\"},\n    &ec2rolecreds.EC2RoleProvider{\n        Client:       ec2metadata.New(sess),\n        ExpiryWindow: 5 * time.Minute,\n    },\n    &stscreds.AssumeRoleProvider{RoleARN: roleArn},\n})\nsess.Config.Credentials = creds\n``\n. @Rking788 Thanks for catching that. Mycredentials.NewChainCredentialsshould of set the returned credentials value tocreds` like.\n``` go\nsess := session.New() // Or your existing session.\ncreds := credentials.NewChainCredentials([]credentials.Provider{\n    &credentials.EnvProvider{},\n    &credentials.SharedCredentialsProvider{Filename: \"\", Profile: \"\"},\n    &ec2rolecreds.EC2RoleProvider{\n        Client:       ec2metadata.New(sess),\n        ExpiryWindow: 5 * time.Minute,\n    },\n    &stscreds.AssumeRoleProvider{RoleARN: roleArn},\n})\nsess.Config.Credentials = creds\n```\nUpdated the previous example\n. Great, glad that cleared up the issue. I'm going to close this issue as it looks to be resolved. Please let us know if you run into any issues using the credential chain, or if we can improve the documentation around them.\n. Thanks for updating the Properties. Looks good, pulling in.\n. Thanks for adding the additional fields to the Resource type. I think it would be helpful to have more concrete types, or enums for these fields especially UpdatePolicy, and CreatePolicy. \nFor this iteration I think we can keep the interface{}. Once we have code generation in place, we can consider replacing these fields with more concrete types.\n. Thanks @bpot this was definitely an oversight. Pulling in your PR.\n. Hi @bpot thanks for making this improvement reducing the allocations when marshaling to JSON. The code change looks good.\n. Thanks again for the PR @bpot, We're definitely whittling down the allocations of the marshalers. Using a global cache seems a little heavy handed for addressing the lookup of these types.  I'm curious if we could instead use interfaces to extract this information instead of performing lookup via reflect.\nSomething like moving the payload:\"field\" tag from SDKShapeTraits and move the value into a method on the type. It would be great to remove the metadata<T> and SDKShapeTraits fields, but that is a secondary.\nI pushed marshalPayloadInf branch that we can take a look at. Do you see similar performance differences?\nThe follow benchmark compares caching vs using a payload interface.\n```\nbenchmark                                                                      old ns/op     new ns/op     delta\nBenchmarkEC2QueryBuild_Complex_ec2AuthorizeSecurityGroupEgress-8               48102         46714         -2.89%\nBenchmarkEC2QueryBuild_Simple_ec2AttachNetworkInterface-8                      13399         13266         -0.99%\nBenchmarkBuildJSON-8                                                           10548         9839          -6.72%\nBenchmarkStdlibJSON-8                                                          8374          7651          -8.63%\nBenchmarkJSONRPCBuild_Simple_dynamodbPutItem-8                                 13871         13398         -3.41%\nBenchmarkJSONUtilBuild_Simple_dynamodbPutItem-8                                13705         13062         -4.69%\nBenchmarkEncodingJSONMarshal_Simple_dynamodbPutItem-8                          6103          6064          -0.64%\nBenchmarkRESTJSONBuild_Complex_elastictranscoderCreateJobInput-8               217430        214609        -1.30%\nBenchmarkRESTBuild_Complex_elastictranscoderCreateJobInput-8                   8455          7145          -15.49%\nBenchmarkEncodingJSONMarshal_Complex_elastictranscoderCreateJobInput-8         37409         37262         -0.39%\nBenchmarkRESTJSONBuild_Simple_elastictranscoderListJobsByPipeline-8            13425         10628         -20.83%\nBenchmarkRESTBuild_Simple_elastictranscoderListJobsByPipeline-8                10420         8807          -15.48%\nBenchmarkEncodingJSONMarshal_Simple_elastictranscoderListJobsByPipeline-8      1052          955           -9.22%\nBenchmarkRESTXMLBuild_Complex_cloudfrontCreateDistribution-8                   370145        377077        +1.87%\nBenchmarkRESTXMLBuild_Simple_cloudfrontDeleteStreamingDistribution-8           17148         14620         -14.74%\nBenchmarkEncodingXMLMarshal_Simple_cloudfrontDeleteStreamingDistribution-8     3682          3563          -3.23%\nbenchmark                                                                      old allocs     new allocs     delta\nBenchmarkEC2QueryBuild_Complex_ec2AuthorizeSecurityGroupEgress-8               237            237            +0.00%\nBenchmarkEC2QueryBuild_Simple_ec2AttachNetworkInterface-8                      67             67             +0.00%\nBenchmarkBuildJSON-8                                                           49             49             +0.00%\nBenchmarkStdlibJSON-8                                                          13             13             +0.00%\nBenchmarkJSONRPCBuild_Simple_dynamodbPutItem-8                                 52             52             +0.00%\nBenchmarkJSONUtilBuild_Simple_dynamodbPutItem-8                                50             50             +0.00%\nBenchmarkEncodingJSONMarshal_Simple_dynamodbPutItem-8                          8              8              +0.00%\nBenchmarkRESTJSONBuild_Complex_elastictranscoderCreateJobInput-8               798            781            -2.13%\nBenchmarkRESTBuild_Complex_elastictranscoderCreateJobInput-8                   59             51             -13.56%\nBenchmarkEncodingJSONMarshal_Complex_elastictranscoderCreateJobInput-8         7              7              +0.00%\nBenchmarkRESTJSONBuild_Simple_elastictranscoderListJobsByPipeline-8            71             54             -23.94%\nBenchmarkRESTBuild_Simple_elastictranscoderListJobsByPipeline-8                57             49             -14.04%\nBenchmarkEncodingJSONMarshal_Simple_elastictranscoderListJobsByPipeline-8      2              2              +0.00%\nBenchmarkRESTXMLBuild_Complex_cloudfrontCreateDistribution-8                   1832           1815           -0.93%\nBenchmarkRESTXMLBuild_Simple_cloudfrontDeleteStreamingDistribution-8           88             71             -19.32%\nBenchmarkEncodingXMLMarshal_Simple_cloudfrontDeleteStreamingDistribution-8     9              9              +0.00%\nbenchmark                                                                      old bytes     new bytes     delta\nBenchmarkEC2QueryBuild_Complex_ec2AuthorizeSecurityGroupEgress-8               10921         10922         +0.01%\nBenchmarkEC2QueryBuild_Simple_ec2AttachNetworkInterface-8                      3696          3696          +0.00%\nBenchmarkBuildJSON-8                                                           1200          1200          +0.00%\nBenchmarkStdlibJSON-8                                                          1200          1200          +0.00%\nBenchmarkJSONRPCBuild_Simple_dynamodbPutItem-8                                 2577          2577          +0.00%\nBenchmarkJSONUtilBuild_Simple_dynamodbPutItem-8                                2513          2513          +0.00%\nBenchmarkEncodingJSONMarshal_Simple_dynamodbPutItem-8                          808           808           +0.00%\nBenchmarkRESTJSONBuild_Complex_elastictranscoderCreateJobInput-8               25293         24524         -3.04%\nBenchmarkRESTBuild_Complex_elastictranscoderCreateJobInput-8                   2800          2464          -12.00%\nBenchmarkEncodingJSONMarshal_Complex_elastictranscoderCreateJobInput-8         4906          4906          +0.00%\nBenchmarkRESTJSONBuild_Simple_elastictranscoderListJobsByPipeline-8            3936          3104          -21.14%\nBenchmarkRESTBuild_Simple_elastictranscoderListJobsByPipeline-8                3328          2928          -12.02%\nBenchmarkEncodingJSONMarshal_Simple_elastictranscoderListJobsByPipeline-8      120           120           +0.00%\nBenchmarkRESTXMLBuild_Complex_cloudfrontCreateDistribution-8                   103376        102536        -0.81%\nBenchmarkRESTXMLBuild_Simple_cloudfrontDeleteStreamingDistribution-8           9104          8272          -9.14%\nBenchmarkEncodingXMLMarshal_Simple_cloudfrontDeleteStreamingDistribution-8     4656          4656          +0.00%\n```\n. Hi @bpot closing this PR in favor of reviewing the interface style. We can reopen it later if we want to pick up caching again. Lets discuss the interface style on #377.\n. Thanks for reporting this issue @bpot I'm taking a look at this. Would you mind including the test case you're running?\n. Great thanks for the update. I can reproduce the bug now.\n. Hi @bpot your idea was correct. The empty map was being wrapped within a slice when nil should of been returned by ValuesAtPath.  Thanks for submitting this issue!  Please let us know if you have any other issues.\n. Hi @abustany thanks for taking the time to create this PR. Adding error handling for these classes of errors will be a good addition to the SDK. \nTo handle the composite errors I'd suggest creating a error sub type similar to awserr.RequestFailure and s3manager.MultiUploadFailure. For this case I'd add a new error interface type to awserr which still has a code and message, but also contains a list of additional error code and messages. Normal err.Error() would print out the error code and message like normal along with the batched errors. But if a user wants to get details on the specific batched errors they could cast awserr.Error to something like awserr.BatchedErrors.\nI'll look through the SDK to see if batched errors can occur with other service API operations, but in the interim I think it makes sense for the base XML protocol parser to handle batched errors if it can be done generically.\n. Hi @abustany good idea to scan the models. I verified this as well InvalidChangeBatch is the only error type the SDK receives which is a batch of errors. In all other cases these batched error instances are included in the success respon. For future growth I think we still want the awserr.BatchedError interface to be a generic container for batched errors like this.  But I think you're correct that using a custom error unmarshaller in Route53 is good idea.\nAlso it would be great to have some test cases of the custom error unmarshaller.\n. Thanks for the update @abustany no rush, we'll keep this PR open for you.\n. Looks pretty good, and thanks for adding the tests also!\nI agree getting rid of the duplication would be helpful. I know this is done for S3 and another service, but not adding additional duplication would be great. I'm curious what using a []byte instead of io.Reader would look like to remove the duplication.\nIn other places where we've needed to rewind a stream bytes.Reader was used to wrapped in an ioutil.NopCloser. This worked well, but isn't very flexible since the bytes.Reader is lost when its wrapped. I'm thinking that it makes sense to create a new type in the aws package which is composed of a bytes.Reader and also an nop closer. Something like:\ngo\ntype ByteReadCloser struct {\n    *bytes.Reader\n}\nfunc (b *ByteReadCloser) Close() error { return nil }\nThis would more cleanly allow us to share the logic used for multi tiered error parsing. \n. Thanks for the update @abustany. The benefit of using the ByteReadCloser type is the SDK's unmarshaller could rewind the body when multiple parsing attempts is needed for the body. Specifically this occurs when the request fails, and an error response is returned. Specifically like in this case where the error could be of multiple types.\n331 highlights this issue where multiple error unmarshalling are helpful.\n. Replacing with #593  which is synced with the tip of master.  Thanks again @abustany for taking the time create this PR and and investigate the issue.\n. Thanks for reporting this error @dogfoodhead I think this may be related to #302.\n. Thanks for the great details about the issue you're seeing. I'm working on creating a unit test case were i can reproduce the problem. I'll update once I have a fix for the issue.\n. Hi @dogfoodhead I pushed a fix to the request pagination logic to correctly handle this case where not all the tokens are required, but some are optional.  Let us know if you have any issues, or feedback with the paginators.\n. Thanks for catching the broken link @pcorliss , pulling in your change.\n. Thanks for the pr @rjeczalik I made a minor tweek and submitted it in 82c88c7.\n. Thanks for reporting this issue @DavyC I'm working on a fix now that will correct the way waiters matches are verified.\n. @DavyC I submitted a fix to how the waiters perform their matching. Please let us know if you run into any other issues with the waiters.\n. Nice catch @DavyC I'll make a pass over all the waiters to see if we have any more conditions like this. I think we just need to update the pathAll case to handle path as well.\nThanks for the suggestion of handling the error case too. You're suggestion looks pretty good there, will incorporate it into my fix.\n. Looks like there are a couple types which need to be handled that aren't\n\"error\",\n\"path\",\n\"pathAll\",\n\"pathAny\",\n\"pathList\",\n\"status\",\n. Thanks for your help @DavyC Waiters should now be fully functional in tip.\n. Great I'm glad to know its working for you. Let us know if you have any questions, or feedback about how the waiters work.\n. Hi @atedja Number DataTypes should be stored in the StringValue field. With DataType set to Number you can format your number to a string based on the type of number.  SQSMessageAttributesNTV provides some more information which DataType to use.\n``` go\n// Integer\nStringValue: aws.String(fmt.Sprintf(\"%d\", myInt))\n// Floating points\nStringValue: aws.String(fmt.Sprintf(\"%f\", myFloat64))\n// Big number type\nStringValue: aws.String(myBigNum.String())\n```\n. Hi @atedja, thanks for contacting us. I'm going to close this issue as it looks to be resolved. Please let us know if you have any other questions or feedback.\n. Thanks for reporting this issue @mattes. It looks like this issue is attributed to the REST header map unmarshaller. We can test it, but I think this method can be updated to extract the original header casing/format instead of always returning the canonical header key form.\nWill need to also verify that setting the metadata headers via the SDK don't mutate the casing also.\nYou could workaround this issue in the interim by manually extracting the metadata headers from the Request.HTTPResponse.Header field.\n. @mattes The SDK will automatically case the metadata fields to Go net/http's standard header canonical formating. The issue we're seeing here is the service's implementation details of the metadata fields being transmitted via headers bleeding into how the SDK returns them.\nThis issue could be addressed and maintain backwards compatibility by introducing an option to ensure the metadata parameters maintain their original casing. Instead of using Go's canonical header key format.\n. Hi @itachi3 thanks for contacting us. The Go SDK does not yet have a utility to batch delete objects of a given pattern. Currently to perform this functionality you need to use ListObjects or ListObjectsPage and implement the pattern locally determining which objects to delete.\nWe would be more than glad to take a look at a pull request which implements it.\n. Hi @itachi3 thanks a lot for taking the time to create the PR. I'll review and provide feedback in the PR thread.\n. applying feedback from PR here. \n@itachi3 thanks for taking the time to create and submit this PR. It is a great place to start adding batch delete all objects from S3 feature.\nWe probably should move this logic to the service/s3/s3manager package since the api.go and example.api files are generated. Because of this it may make sense to follow the s3manager's Uploader and Downloader functionality. In this case we could create a BatchDelete which uses an iterator to determine which object's should be deleted from S3. Taking advantage of S3's ListBucketObjectPages we can do an unlimited number of objects to delete.\nIf we use an iterator pattern the utility would be extensible to multiple sources, and filtering methods.\nSomething like:\n``` go\npackage s3manager\ntype BatchDeleteObject struct{\n    Key, Version string\n}\n// A BatchDeleteIterator provides a way to enumerate the S3 objects to be deleted.\ntype BatchDeleteIterator interface {\n    Next() bool\n    Object() BatchDeleteObject\n    Err() error\n}\n// A FilteredBatchDeleteItor provides filtering for a BatchDeleteIterator\ntype FilteredBatchDeleteItor struct {\n    Itor BatchDeleteIterator\n    FilterFn func(BatchDeleteObject) bool\n}\nfunc (itor FilteredBatchDeleteItor) Next() bool               {}\nfunc (itor FilteredBatchDeleteItor) Value() BatchDeleteObject {}\nfunc (itor *FilteredBatchDeleteItor) Err() error               {}\n// A ListObjectsItor provides an iterator for objects from an S3. Will use\n// pagination of the S3 Objects if the MaxKeys ListObjectsInput parameter\n// is provided.\ntype ListObjectsItor struct {\n    S3       s3iface.S3API\n    Params   s3.ListObjectsInput\n}\nfunc (itor ListObjectsItor) Next() bool               {}\nfunc (itor ListObjectsItor) Value() BatchDeleteObject {}\nfunc (itor ListObjectsItor) Err() error               {}\n// DefaultBatchDeleteSize is the default batch size.\nconst DefaultBatchDeleteSize = 1000\n// A BatchDelete provides the configuration for batch deleting objects from S3\ntype BatchDelete struct {\n    S3        s3iface.S3API\n    BatchSize int\n}\n// NewBatchDelete creates and initializes the BatchDelete object.\nfunc NewBatchDelete(svc s3iface.S3API, options ...func(BatchDelete)) BatchDelete {\n    d := &BatchDelete{\n        S3:        svc,\n        BatchSize: DefaultBatchDeleteSize,\n    }\n    for _, o := range options {\n        o(d)\n    }\nreturn d\n\n}\n// DeleteWithIterator deletes all objects from S3 provided by the BatchDeleteIterator.\nfunc (d *DeleteObjects) Delete(itor BatchDeleteIterator) error {\n    return batchDeleteObjects(d.S3, d.BatchSize, itor)\n}\nfunc batchDeleteObjects(svc s3iface.S3API, batchSize int, itor BatchDeleteIterator) error {\n    queue := make([]BatchDeleteObject, 0, batchSize)\nfor itor.Next() {\n    queue := append(queue, itor.Object())\n\n    if len(queue) >= batchSize {\n        if err := deleteBatch(svc, queue); err != nil {\n            return err\n        }\n        queue = queue[0:0]\n    }\n}\nif err := itor.Err(); err != nil {\n    return err\n}\n\nreturn nil\n\n}\nfunc deleteBatch(svc s3iface.S3API, batch []BatchDeleteObject) error {\n    // TODO call DeleteObjects ...\n}\n``. Closed #451 in favor of further discussion based on iteration approach listed above.. Hi @j7b thanks for contacting us. What Issue are you encountering when you rungo install ./...? In my local environment I haveGO15VENDOREXPERIMENT=\"1\"and do not encounter an issue when running go install. Are you running the go install ./... from the root of yourGOPATH`?\nI was only able to reproduce a failure if I run go install from the root of GOPATH because the dependencies of the SDK test dependencies of the library are not also retrieved.  The runtime dependencies of the SDK are vendored within the vendor directory. If go install ./... is run from the SDK's root directory it will install without error. I also received this same error with the vendoring experiment disable.\nTo reduce the size of the SDK we choose to only vendor runtime dependencies as the testing dependencies are not generally required to use sdk. We'd be open to reconsider this if it is causing usability problems.\n. @j7b thanks for the update. Part of the issue with doing go install ./... from the root of a project is that go get x will only retrieved the explicitly used dependencies of a package.  For example doing go get golang.org/x/net/context will only retrieve the dependancies of golang.org/x/net/context. But golang.org/x/net is the actual repo, so all the other net code is added to your GOPATH also, but none of its dependancies are.\nIn order to support this in the SDK many additional dependencies would be needed to be added to the vendor folder. The following is a minimum of the dependencies that would be needed. Most likely there are more since this pattern would include all dependencies of each dependency.\n\u251c\u2500\u2500 github.com\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 davecgh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 go-spew\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 go-ini\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ini\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 golang\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 protobuf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 jmespath\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 go-jmespath\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 jtolds\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 gls\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 lsegal\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 gucumber\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pmezard\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 go-difflib\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 shiena\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ansicolor\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 smartystreets\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 assertions\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 goconvey\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 stretchr\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 objx\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 testify\n\u251c\u2500\u2500 golang.org\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 x\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 crypto\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 net\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 oauth2\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 text\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 tools\n\u2514\u2500\u2500 google.golang.org\n    \u2514\u2500\u2500 api\n    \u251c\u2500\u2500 appengine\n    \u2514\u2500\u2500 cloud\n    \u2514\u2500\u2500 grpc\n. Thanks for contacting us about adding more vendored dependencies to the SDK. Due to the scope of vending such a large tree of non runtime dependencies we would like to limit the vendored dependencies to just those needed to use the SDK. make get-deps can be used to retrieve all unit, integration, and doc dependencies that can be used for developing and testing the SDK itself.\nAs Go's package management evolves and matures we should take that into account and adjust our vending strategy.\n. Hi @seiffert Thanks for contacting us. I think the issue you're experiencing is default.Config() returns a Config instance without any credentials. With a minor tweak to your code I think you'll be up and running.\nIf you replace default.Config() with aws.NewConfig() I think it will fix this issue you're running into.\n``` go\nimport (\n    \"github.com/aws/aws-sdk-go/aws/defaults\"  \n    \"github.com/aws/aws-sdk-go/aws/session\"\n    \"github.com/aws/aws-sdk-go/service/dynamodb\"\n)\nc := aws.NewConfig().WithEndpoint(\"http://127.0.0.1:8000\")\nd := dynamodb.New(session.New(), c)\n```\nThe aws.Config value provided to the service client's New() is OK to only include the Endpoint value set. When the Dynamodb service client is being created, any additional aws.Config values are merged with the aws.Config inside the session.Session. session.New() will return a session with the default configuration, credentials, and handlers already set.\nIn your using, default.Config() actually clears the credentials. If you're wanting to use the SDK's default config you should use default.Get().Config. I think this is a great place that could use some more documentation.\n. Great glad that worked for your code.\nCorrect, session should the prefered way to share configurations between multiple clients. You can also create sub sessions by using the Copy() method to create a copy of a session so you can modify parts of it without needing to specify the full configuration and handlers again.\nI suggest using aws.NewConfig() when you're wanting to pass instance specific configurations to a client. If you use defaults.Get().Config you will actually be overwriting many of the shared Config values in the session. \ndefaults package exists as a separation between a generic session and the specific SDK default configuration values. I suggest not using the defaults package at all unless you need to reset a client's config to the defaults. In that case its probably best to use session.New() instead, to save confessions.\nIn v0.10.0 we created the session package replacing the previous global defaults. I think more documentation in the defaults package and wiki will help clear this up.\n. Hi @seiffert I pushed an update to the session and defaults package to more clearly state when sessions and defaults should be used.  Thanks for pointing out this, and let us know if you have any additional feedback or questions.\nThe API reference documentation will be updated in our next published release\n. @itachi3 thanks for taking the time to create and submit this PR. It is a great place to start adding batch delete all objects from S3 feature.\nWe probably should move this logic to the service/s3/s3manager package since the api.go and example.api files are generated. Because of this it may make sense to follow the s3manager's Uploader and Downloader functionality.  In this case we could create a BatchDelete which uses an iterator to determine which object's should be deleted from S3.  Taking advantage of S3's ListBucketObjectPages we can do an unlimited number of objects to delete.\n. If we use an iterator pattern the utility would be extensible to multiple sources, and filtering methods.\nSomething like:\n``` go\npackage s3manager\ntype BatchDeleteObject struct{\n    Key, Version string\n}\n// A BatchDeleteIterator provides a way to enumerate the S3 objects to be deleted.\ntype BatchDeleteIterator interface {\n    Next() bool\n    Object() BatchDeleteObject\n    Err() error\n}\n// A FilteredBatchDeleteItor provides filtering for a BatchDeleteIterator\ntype FilteredBatchDeleteItor struct {\n    Itor BatchDeleteIterator\n    FilterFn func(BatchDeleteObject) bool\n}\nfunc (itor FilteredBatchDeleteItor) Next() bool               {}\nfunc (itor FilteredBatchDeleteItor) Value() BatchDeleteObject {}\nfunc (itor *FilteredBatchDeleteItor) Err() error               {}\n// A ListObjectsItor provides an iterator for objects from an S3. Will use\n// pagination of the S3 Objects if the MaxKeys ListObjectsInput parameter\n// is provided.\ntype ListObjectsItor struct {\n    S3       s3iface.S3API\n    Params   s3.ListObjectsInput\n}\nfunc (itor ListObjectsItor) Next() bool               {}\nfunc (itor ListObjectsItor) Value() BatchDeleteObject {}\nfunc (itor ListObjectsItor) Err() error               {}\n// DefaultBatchDeleteSize is the default batch size.\nconst DefaultBatchDeleteSize = 1000\n// A BatchDelete provides the configuration for batch deleting objects from S3\ntype BatchDelete struct {\n    S3        s3iface.S3API\n    BatchSize int\n}\n// NewBatchDelete creates and initializes the BatchDelete object.\nfunc NewBatchDelete(svc s3iface.S3API, options ...func(BatchDelete)) BatchDelete {\n    d := &BatchDelete{\n        S3:        svc,\n        BatchSize: DefaultBatchDeleteSize,\n    }\n    for _, o := range options {\n        o(d)\n    }\nreturn d\n\n}\n// DeleteWithIterator deletes all objects from S3 provided by the BatchDeleteIterator.\nfunc (d *DeleteObjects) Delete(itor BatchDeleteIterator) error {\n    return batchDeleteObjects(d.S3, d.BatchSize, itor)\n}\nfunc batchDeleteObjects(svc s3iface.S3API, batchSize int, itor BatchDeleteIterator) error {\n    queue := make([]BatchDeleteObject, 0, batchSize)\nfor itor.Next() {\n    queue := append(queue, itor.Object())\n\n    if len(queue) >= batchSize {\n        if err := deleteBatch(svc, queue); err != nil {\n            return err\n        }\n        queue = queue[0:0]\n    }\n}\nif err := itor.Err(); err != nil {\n    return err\n}\n\nreturn nil\n\n}\nfunc deleteBatch(svc s3iface.S3API, batch []BatchDeleteObject) error {\n    // TODO call DeleteObjects ...\n}\n``\n. Thanks for contacting us @jwhitcraft This item is still outstanding. I think it would be better to close this PR and open it as a github issue. From the github issue we can discuss the implementation. As it is I don't think we could accept the delete iterator proposed in PR for reasons mentioned above.. @jwhitcraft awesome! please create a PR if with your ideas. I've updated #448 with the feedback from this PR. I think our implementation should be based on an iteration based approach. This will later can be applied to the S3 Upload/Downloaders also.. Hi @rjeczalik thanks for reporting this documentation issue, and suggested change. Adding a defaultEC2Metadataclient inside toec2rolecreds` package would create circular dependencies  though:\nec2rolecreds -> session -> defaults -> ec2rolecreds\nSo we wouldn't be able to create a default EC2Metadata client within ec2rolecreds or ec2metadata pacakges. Since this circular dependencies, and the common needed to create EC2Metadata default clients or simple creation of the client with Ec2RoleProvider, I think the defaults package might be a good place for this.\nWe could add something like the following to the defaults package.\ngo\nfunc EC2MetadataClient(...)  *ec2metadata.EC2Metadata {}\nCalling defaults.EC2MetadataClient() would give you access to a default ec2 metadata client. Alternatively an additional package would need to be created to return a default ec2 metadata client under ec2metadata package.\nHow would this work for your use case?\n. @rjeczalik thanks for trying that out.  I don't think we'll be able to provide a default instance of the ec2metadata client for the ec2rolecreds package automatically, due to the circular references. I think the SDK's package layout will require users to pass a EC2Metadata client instance to EC2RoleProvider explicitly.  We could add a default EC2Metadata instance to the defaults package, but I'm not sure if there is much benefit here over creating your own instance since it would need to be manually passed to EC2RoleProvider.\n. Hi @bpot thanks for getting this PR together. I compared the differences locally and using _ has nearly the same allocs/bytes improvements vs the interface approach. But your method also has a minor improvement on improves on ns/op and is simpler to read I think.\nI don't think there are any issues with breaking changes here since metadata was already private so switching it to _ won't cause an issue.\n. Looks good thanks for your time, idea, and submitting the PR.\n. Thanks for the feature request @vancluever. With this and #455 it may make sense to expose the waiter so that hooks can be added.\nOne idea is the private/waiter#Waiter could be published, and expanded to take a delay handler func.  From here a default back-off delay handler could be created while also supporting custom delay handlers.\nThis probably would entail adding a new WaitUntilX method for each of API waiters, which would return a waiter.Waiter similar to the Request API functions. \n. I could see this possibly being exposed as callbacks or two interfaces. It would be interesting to play around with this idea by breaking the current waiter logic apart into smaller chunks and reassembling them in a stack of handlers.\n. Hi I just merged PR #1132 that adds the ability to use context with API operations, paginators and waiters. Let us know if you run into any issues, or have additional feedback.  This change will be apart of our next release.\nUsing the request.Option variadic arguments added to the WithContext WaitUntil methods you'll be able to inject request handlers into the waiter's request lifecycle to retrieve information on the individual requests if needed.\nIn addition request.WaiterOption values were also added that give you the ability to configure back off with the request.WaiterDelay type.. Thanks for the request @vancluever. I think the solution to this will be similar to #454 since we cannot update WaitUtilX to return an additional parameter. Having an additional parameter which would return a waiter.Waiter and calling Waiter.Wait() will return both response and error. Response probably would be the last response while polling.\n. This one is a bit tougher. If we updated Wait() like the following and expose it i think it would give you the option of receiving the body. In the general case the code probably wouldn't care what the response was. \nThough its worth thinking about what would happen if waiters ever depended on two or more API operations to trigger their WaitUntil condition. This might be a little award to expose just with a response return value. But, I think its valuable to consider what designs we could implement in order to solve this problem because there is in some cases knowing the response of the API calls.\n. Hi @vancluever  I just merged PR #1132 that adds the ability to use context with API operations, paginators and waiters. Let us know if you run into any issues, or have additional feedback.  This change will be apart of our next release.. Hi @abalakersky thanks for contacting us. The ListObject.String() method's string representation is not JSON, but Key/Value paired representation for debug output of a operation's struct.\nI think the feature you're looking for of clean JSON output would be implemented by the fix for #271. If custom JSON encoding marshaller was exposed, or (Un)MarshalJSON() methods added to the shapes we could support this.\n. @abalakersky is your code being used in a chain where the output needs to return a JSON object that will be parsed by another cli tool?\nAlso are you looking to iterate over all objects in your bucket using the prefix for additional queries? Or only iterate over select subset of prefixes?\n. Ah I see, great.  There are a few ways you could approach this.  If you're wanting to list objects in multiple buckets concurrently you could take an approach like our sample listS3EncryptedObjects. Where channels are used to enumerate over the objects in the buckets.\nAlternatively to split based on a prefix within a bucket I suggest using a pushing the prefixes to a channel and have your parallel worker goroutines reading from that channel making ListObject's with the provided prefixes.  This would look something like the following:\n``` go\npackage main\nimport (\n    \"fmt\"\n    \"sync\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\"github.com/aws/aws-sdk-go/service/s3/s3iface\"\n\n)\nfunc main() {\n    svc := s3.New(session.New())\nbucket := \"mybucket\"\nnumWorkers := 5\n\nprefixCh := make(chan string, numWorkers)\nobjCh := make(chan *s3.Object, 100)\nvar wg sync.WaitGroup\nfor i := 0; i < numWorkers; i++ {\n    // Spin up each worker\n    wg.Add(1)\n    go func() {\n        listObjectsWorker(objCh, prefixCh, bucket, svc)\n        wg.Done()\n    }()\n}\ngo func() {\n    // TODO ListObjects in bucket, and use CommonPrefixes\n    // Add each prefix to prefixCh and close it so workers will\n    // know when to stop\n    close(prefixCh)\n}()\ngo func() {\n    // Wait until workers are finished then close object channel\n    wg.Wait()\n    close(objCh)\n}()\n\nfor obj := range objCh {\n    // TODO Do something with the objects.\n    fmt.Println(\"Object:\", *obj.Key)\n}\n\n}\nfunc listObjectsWorker(objCh chan<- *s3.Object, prefixCh <-chan string, bucket string, svc s3iface.S3API) {\n    for prefix := range prefixCh {\n        // TODO ListObjects\n        // TODO Push each Object Contents to the objCh\n    }\n}\n```\n. Ah i see, i think you just need to iterate over the CommonPrefixes slice and Prefix is a field off of that.\ngo\nfor _, commonPrefix := range result.CommonPrefixes {\n    fmt.Println(\"Prefix:\", *commonPrefix.Prefix)\n}\n. Here is a full example:\n``` go\npackage main\nimport (\n    \"fmt\"\n    \"sync\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\"github.com/aws/aws-sdk-go/service/s3/s3iface\"\n\n)\nfunc main() {\n    svc := s3.New(session.New())\nbucket := \"mybucket\"\nnumWorkers := 5\n\nprefixCh := make(chan string, numWorkers)\nobjCh := make(chan *s3.Object, 100)\nvar wg sync.WaitGroup\nfor i := 0; i < numWorkers; i++ {\n    // Spin up each worker\n    wg.Add(1)\n    go func() {\n        listObjectsWorker(objCh, prefixCh, bucket, svc)\n        wg.Done()\n    }()\n}\ngo func() {\n    // Wait until workers are finished then close object channel\n    wg.Wait()\n    close(objCh)\n}()\n\ngo func() {\n    if err := getBucketCommonPrefixes(prefixCh, bucket, svc); err != nil {\n        fmt.Println(\"error getting bucket common prefixes\", err)\n    }\n    // Close prefixCh so workers will know when to stop\n    close(prefixCh)\n}()\n\nfor obj := range objCh {\n    // TODO Do something with the objects.\n    fmt.Println(\"Object:\", *obj.Key)\n}\n\n}\nfunc listObjectsWorker(objCh chan<- *s3.Object, prefixCh <-chan string, bucket string, svc s3iface.S3API) {\n    for prefix := range prefixCh {\n        result, err := svc.ListObjects(&s3.ListObjectsInput{\n            Bucket: &bucket, Delimiter: aws.String(\"/\"),\n            Prefix: &prefix,\n        })\n        if err != nil {\n            fmt.Println(\"failed to list objects by prefix\", prefix, err)\n            continue\n        }\n        for _, obj := range result.Contents {\n            objCh <- obj\n        }\n    }\n}\nfunc getBucketCommonPrefixes(prefixCh chan<- string, bucket string, svc s3iface.S3API) error {\n    result, err := svc.ListObjects(&s3.ListObjectsInput{\n        Bucket: &bucket, Delimiter: aws.String(\"/\"),\n    })\n    if err != nil {\n        return err\n    }\nfor _, commonPrefix := range result.CommonPrefixes {\n    prefixCh <- *commonPrefix.Prefix\n}\n\nreturn nil\n\n}\n``\n. Glad to help @abalakersky let us know if you have any additional questions, or is there is anything we can help you with.  I'll add this code example to our section of example to make it available for others looking how to use the SDK concurrently.\n. @abalakersky overall it looks OK. Here are a couple comments though. Were you able to resolve the pausing your code was running into? Based on your description it sounds like your code is being throttled by S3 for so many requests at once.\n- I'd suggest running your code with the-raceflag to detect if there are any race conditions.\n- In the obj for loop yourw.WriteString(obj.Key + \"\\n\")can be replaced withfmt.Fprintln(w, obj.Key)and it will save you a few allocations and performance since it doesn't need to create the temporary strings.\n- Thefor _, commonPrefix := range topLevel.CommonPrefixesmight be producing unexpected prefixes. Since the evaluation ofcommonPrefix` doesn't occur until after goroutine starts, and a for loop's range reuses values it returns. https://github.com/golang/go/wiki/CommonMistakes#using-goroutines-on-loop-iterator-variables\n. Thanks for the suggestion and code sample @george-makerbot this will help a lot investigating the problem. Initially looking at this issue it looks like the problem might be related to the way the waiter determines the condition is satisfied.\nUsing the following expression:\nservices | [@[?length(deployments)!=`1`], @[?desiredCount!=runningCount]][] | length(@) == `0`\nIt looks that there might be a confusion in the way the SDK is detecting the condition is met.\n. Hi @george-makerbot  would you mind using this updated sample. It has debug logging turned on. Specifically I'm curious about what the wire HTTP body response of the ECS DescribeInstances you're seeing.\nhttp://play.golang.org/p/JEKuG5lh-r  Adds the following to line 96, and set the WaitUntilServicesStable to use the logging service\ngo\necsSvcDebug := ecs.New(sess, &aws.Config{LogLevel: aws.LogLevel(aws.LogDebugWithHTTPBody)})\nSpecifically I'm curious if you're debug log's DescribeInstances response body contains a service[].events[].message, and what the message(s) are.\n. Great find @george-makerbot, I think the binary jpgo vs jmespath.Search differences are attributed to map[string]interface{} vs concrete struct.  I'm working on adding a test case to github.com/jmespath/go-jmespath to reproduce this issue. Once we have that we can create a issue with the upstream repo.\n. Linked this issue to the upstream jmespath/go-jmespath#14 We can work with jmespath to resolve this issue.\n. I'm working to verify a fix for the go-jmespath for this expression. It looks like the way typed slices were being handled was preventing the comparison from working.\n. Thanks for all the information and assistance in addressing this issue. We identified the problem and got a fix pushed upstream to go-jmespath. If you're using the SDK with Go 1.5 vendoring experiment all you need to do is sync the latest version of the SDK.  If you're not using vendoring you can update the SDK and jmespath with:\ngo get -u github.com/aws/aws-sdk-go/...\ngo get -u github.com/jmespath/go-jmespath\n. @rfielding thanks for creating this issue. The SDK will use any header added to the request when calculating the signature. You can add the headers to Request.HTTPRequest.Header and they should be used by the SDK when calculating the signature. The only headers that should be ignored by the SDK are those defined in v4.go#ignoreHeaders. (Authorization,Content-Type,Content-Length,User-Agent).\n``` go\nsvc := s3.New(sess)\nreq, out  := svc.GetOjbectRequest(&s3.GetObjectInput{...})\nreq.HTTPRequest.Header.Set(\"Nonce\", \"a9d9395\")\nerr := req.Send()\n```\nIf you're looking to add a nonce automatically to each request instead of in code, you can use the request.Handlers to inject the nouce automatically.\n. Thanks for the clarification, that was very helpful.  It sounds like the best path to add this feature would be to add a method of registering which headers should be required, and not leaked to the URL. This would allow you to set which headers will stay in the signature, but not be hoisted to the URL.\nSince this issue is limited to Presigning, we could explore the idea of creating a new Presigning method, or consider adding a variadic optional argument of additional options.\nTagged this as an enhancement so we can get this functionality added. We're also more than glad to take a look at PRs if you'd like to prototype something.\n. @rfielding we'd be more than glad to look at your PR if you would like to put one together.\nHoisting headers to the URL during pre-signing is helpful for some services. For S3 though policies only inspect headers and not query string parameters. Because of this, I think we need to create an additional way of generating a presigned url which does not hoist any headers to the URL. In addition changing the functionality of Request.Presign would be a breaking change for those that relied on it.\nI think support for this idea could be added as a new method off of aws/request#Request, PresignHTTPRequest which would return a net/http#Request instance. The benefit of returning a http.Request is the URL and map of headers are easily retrieved. With the http.Request you could share the presigned URL and the header map as desired.\nWhat do you think of this pattern? It is similar to how the AWS SDK for Ruby exposes both presign with hoisting and generic presign http request without hoisting.\n. Thanks for the feedback @rfielding  I've updated this issue to be a feature request so we can add a way to generate presigned requests without hoisting headers to the query string.\n. Hi we didn't update this issue, but this is now fixed. Release v1.1.0 fixes how headers are hoisted to the query string, in addition adds a pre-sign request that does minimal and returns a list of headers that were signed.\n. Looks pretty good @psankar thanks for taking the time to create this example. Us getting examples like this into the SDK will help other users get started quicker using the SDK. \nTo keep our example's together it would be great to put this in the github.com/aws/aws-sdk-go/example/dynamoDbCreateUpdate folder.\n. Hi @psankar thanks for posting this PR, and example app for DynamoDB. We're glad to take additional examples into the SDK. For this example specifically. It would be helpful to add more documentation explaining what is going on and why.\nI'm marking this PR to be closed in the next few days. We're glad to look at this again in the future if you'd like to add to it.\n. Hi @psankar I'm going to close this PR for now. Please reopen to continue the discussion on this example.\n. Looks good @bpot it would be great to have a small test case for this addition to cover the conditional length values.\n. Thanks for putting the PR together @bpot I added some tests, and pushed in, 80dd495. There was a minor bug in the make([]byte, length) call, the initial length of 0 bytes needed to be set, or the buffer might have extra bytes allocated causing serialization errors.\n. Thanks for finding this @sburke-at-connectivity I've updated the wiki, and the code samples are now in their correct locations.\n. Hi @relvacode the SDK doesn't currently posse the ability to download concurrently from a presigned URLs. Since the original presigned URL is generated for a specific operation and request it cannot be split into multiple byte ranges.\nIf you're the one generating the presigned URLs you could use the S3.GetObject operations with a range requests to generate your presigned URLs for the object you're wanting to download concurrently. You could distribute these multiple presigned URLs to your client and they would be able to split the request into multiple parts.\n. HI @relvacode I'm going to close this issue for now. Please open it back up if you have any more question, or we can help clarify the issue you're facing.\n. Hi @ananddhandhania the best way to get a presigned URL for the GetObject API call is to use the  Request.Presign method of a Request.\n```go\nsvc := s3.New(sess)\nreq, _ := svc.GetObjectRequest(&s3.GetObjectInput{\n    Bucket: aws.String(myBucket),\n    Key: aws.String(myKey),\n})\nurlStr, signedHeaders, err := req.PresignRequest(15 * time.Minute)\n```\nOnce you have the URL string you can share the presigned URL and any HTTP client can use the URL to make a the request to the S3 service until the 15 minutes have expired.\nIn addition the signedHeaders returned need to also be set by the client that will make the request with the URL to ensure the request's signature is validated correctly.. @ananddhandhania Each SDK should have a way to create presigned URLs for API operations. For the AWS SDK for Java checkout this document, http://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURLJavaSDK.html. For download using a single presigned URL multiple times is a great way to go. The only consideration to make is that the presigned URL's expiration time is long enough that for large objects the presigned URL will not expire.. Hi @tjumlani, yes, a presigned URL is just a reference a client can use to get/put an object. There is no limit to the number of presigned URLs that can reference a single object.. Thanks for contacting us @voutasaurus. During Developer Preview the SDK did apply inflection logic to the APIs defined by the service in order to normalize the inflection casing to Go's standard. This logic became unmaintainable as new services APIs and enumerables were added. Specifically we were finding our selves needed to maintain effectively a dictionary of words sorted in the order inflections should be applied in. In some cases those inflection rules conflicted between API operation and enumerables.\nIn 96cfbbc50 we removed the reflection logic to only apply inflections to concat _ separated names with camel case, and title casing the first word of the API's operation, fields, or enumerables.\nThis would of been a nice feature to have conforming to the Go inflection standard, but since the service clients are generated based on models defined by the services the SDK is limited on its methods for applying inflections to the SDK.\n. HI @jdnurmi thanks for contacting us about this. The generator code is something that would be cool to expose outside of the SDK and make it generic for general use. At the moment its design and usage is very specific to the SDK.\nExpanding the generator code logic outside of the SDK would be a cool feature and would allow people do generate some interesting logic for their applications using the SDK.\nMarking this as an enhancement so we can track the progress of this enhancement.\n. @jdnurmi I've committed a change which will allow you to specify the base import path for service clients.  The client still expects to live in it's own folder/package but this will allow you to generate service clients based on your API Gateway APIs in your own package paths.\n. Hi @ybogdanov thanks for contacting us. I think this makes sense. If the network connection fails while the body copy is occurring the SDK should retry the chunk. With a limit on retry attempts similar to other operations.\nWe may need to be conscious of errors which occurred due to writing the chunk to the destination. Since this type of error would most likely be caused by a file IO error.\nYour outer retryer is a good workaround until S3 download manager supports retrying IO stream errors.\n. @eldondevcg are you seeing that even with retrying failed parts you're still not able to download an object from s3?\n. Thanks for the feedback. This will help us prioritize this feature in our backlog. We're also always glad to review pull request if anyone is interested in submitting a change.\n. In release v1.4.11 we released the feature that will allow S3 downloads to retry if the connection is broken while downloading the object. Let us know if you have any issue, or feedback on this update. Thanks!\n. @fermin-silva they uploader should be retrying each part if the part fails. The retry logic in internal to the SDK's UploadPart API operation will automatically retry the part upload if a failure occurs.  \nAre you seeing the uploader not attempting to retry? . @fermin-silva thanks for the update. You can also configure the SDK to retry each part request more times by setting the SDK's MaxRetries config attribute.\n```go\nsess := session.Must(session.NewSession(&aws.Config{\n    MaxRetries: aws.Int(retryCount),\n}))\nuploader := s3manager.NewUploader(sess)\n```. Glad to help, let us know if you have any questions, issues, or feedback with the SDK @fermin-silva . Hi @rayrutjes thanks for contacting us. Using presigned urls with the SDK is straightforward and only requires a few extra steps compared to using the API operations directly. A good strategy would be for your CLI client to send your service the file key, and MD5 checksum of each file to be uploaded to S3. Your service would then generate and return presigned URLs for each file.\nAt a minimum the Bucket and Key must be provided to put an object to S3. You may also want to consider adding a MD5 checksum for each file to ensure the presigned URL is used with the correct file. This also ensures that if the presigned URL is leaked it cannot be used to upload other unexpected content to your S3 bucket. Checkout this example from the SDK wiki on how to use presigned URLs.\nIn your service the presigned URL will be generated for the bucket, key, and MD5 of the file to be uploaded. Your CLI should provide these fields to your service. The service would then return a presigned URL.\n``` go\n    svc := s3.New(session.New())\n    r, _ := svc.PutObjectRequest(&s3.PutObjectInput{\n        Bucket: aws.String(\"myBucket\"),\n        Key:    aws.String(\"myKey\"),\n    })\n    r.HTTPRequest.Header.Set(\"Content-MD5\", checksum)\n    url, err := r.Presign(15 * time.Minute)\n    if err != nil {\n        fmt.Println(\"error presigning request\", err)\n        return\n    }\nfmt.Println(\"URL\", url)\n\n```\nThe CLI client would receive the URL and make a PUT request with the file content to upload.\n``` go\n    req, err := http.NewRequest(\"PUT\", url, fileReader)\n    if err != nil {\n        fmt.Println(\"error creating request\", url)\n        return\n    }\nresp, err := http.DefaultClient.Do(req)\nif err != nil {\n    fmt.Println(\"failed making request\")\n    return\n}\n\n```\n. Great, glad to help. Let us know if you have any other questions, feedback, or ideas how we can improve the SDK.\n. Thanks for getting back in touch with us @rayrutjes. Is the Transfer-Encoding header being added by your app by chance? The following is the example I tried running to reproduce the error you reported, But wasn't able to. I also tried setting Transfer-Encoding header explicitly and wasn't able to reproduce it that way either.\nAny additional script or example code you have would be very helpful in investigating this issue.\n``` go\npackage main\nimport (\n    \"bytes\"\n    \"crypto/md5\"\n    \"encoding/base64\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n    \"os\"\n    \"time\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\n)\nfunc main() {\n    buf := bytes.NewReader(make([]byte, 1010241024))\n    h := md5.New()\n    io.Copy(h, buf)\n//\n// Generate presigned PutObject URL\nsvc := s3.New(session.New())\nr, _ := svc.PutObjectRequest(&s3.PutObjectInput{\n    Bucket: aws.String(os.Args[1]),\n    Key:    aws.String(os.Args[2]),\n})\nr.HTTPRequest.Header.Set(\"Content-MD5\", base64.StdEncoding.EncodeToString(h.Sum(nil)))\nurl, err := r.Presign(15 * time.Minute)\nif err != nil {\n    fmt.Println(\"error presigning request\", err)\n    return\n}\nfmt.Println(\"URL\", url)\nbuf.Seek(0, 0)\n\n//\n// Use the presigned URL to put a object to S3\nreq, err := http.NewRequest(\"PUT\", url, buf)\nif err != nil {\n    fmt.Println(\"error creating request\", url)\n    return\n}\n\nresp, err := http.DefaultClient.Do(req)\nif err != nil {\n    fmt.Println(\"failed making request\")\n    return\n}\ndefer resp.Body.Close()\n\n//\n// Print out the response.\nfmt.Println(\"Status\", resp.StatusCode, resp.StatusCode)\no := &bytes.Buffer{}\nio.Copy(o, resp.Body)\nfmt.Println(o.String())\n\n}\n``\n. The SDK intentionally will not hoist anyx-amz` header (which is what the ACL is send as) to the presigned URL. This is a blanket operation because those fields are not valid in the query string.\nOne way we've considered improving this experience is to add an additional way to generate a presigning. But instead of just generating a URL a net/http#Request would be returned. This would include all headers used in the signing so sharing the information would be a little bit easier.\n. Thanks for letting us know about the panic @phinze. I would expect r.HTTPResponse to always be populated with a valid object.  Are you seeing the r.HTTPResponse.Body being nil, or just r.HTTPResponse?\nI'm curious if the revision your user is using is at or near 37ff15f2. This revision matches up with the panic line mentioned in the stack trace for handler_functions.go#L91.  In this revision SendHandler wasn't correctly setting the r.HTTPResponse if an error occurred, but the error was not an URL error.\nThe user should be ok with at least edd888623 (the version of your first link)\n. Glad to help, let us know if you run into any other issues, feedback, or questions.\n. @glasser thanks for contacting us. The SDK doesn't have a quick way to specify the per API operation request timeout/retry rules. Using Retry request handler is the best way to inject a per operation retry handling.\n\n(a) It looks like the only way to make the HTTP request timeout is to set a Timeout on the HTTPClient in the config... which requires creating a whole new service client object for each request with a different timeout. Is that true? (Or otherwise one can do a complex dance of running the request in a goroutine, sending the result across a channel, selecting on the channel and a time.After, and somehow canceling the request.)\n\nCorrect, creating a new client is the only way to set a connection timeout. Improving how the SDK handles retry would be a nice enhancement to add. Using channels combined with canceling the request would probably how this would manifest.\n\n(b) If one does go and set a timeout on the HTTP request, it will get interpreted by core.SendHandler as a retryable network error and try again. This seems likely to not be what you actually wanted: if you said \"I want this operation to timeout in 10 seconds\" that probably doesn't mean \"I want to run a series of HTTP operations each of which time out in 10 seconds\"?\n\nThis could go both ways I think and it may make sense to pass the retry onto the user to determine if they want to retry a timeout connection. We should consider if changings this process is a breaking change or not. We may need to add a flag so users can opt in to no retry of HTTP timeouts.\nWe are more than glad to review PRs. For this issue since its not obvious yet how we should improve this let's start with with discussing how we could implement this and check out some prototypes to get a feel for them.\nSince the SDK's supported min version is 1.5 we wouldn't be able to directly take advantage of the golang.org/x/Context added to net/http in 1.6/1.7. We may be able to through reflection, but we would have access to the Transporter.CancelRequest for canceling the request if it is timing out.\n. Hi @b6g SDK release v1.8.0 added the ability to use context.Context with API operations.  This will greatly improve the usability of setting timeouts and deadlines.  Let us know if you have any feedback or additional ideas we can add to the SDK.. Hi @ThomasChambonDev thanks for contacting us. I think the bug you're experiencing is due to:\n var session = new(session.Session)\nThis line should be:\nvar session = session.New()\nWhen creating a section work needs to be done to load the default configuration, and initialize the session correctly.\n. Great, glad to help. Let us know if you have any other questions, feedback, or ideas how we can improve the SDK. Especially if there is any documentation we could of added to make this easier.\n. Hi @conorgil @puredistortion We just released support for NAT Gateway in our latest release v1.0.6 You'll be able to use NAT Gateway with the SDK now.  Let us know if you have any questions or feedback. Thanks!\n. @geekifier thanks for creating this feature request.  The SDK does have a stscreds.AssumeRoleProvider which allows using external IDs for assuming roles and can be used as a credential provider in the SDK.  This could be used if your client has access to the role arn and external ID.\n``` go\nsess := session.New()\nsess.Config.Credentials = &aws.Config{Credentials: stscreds. NewCredentials(sess, roleArn, func(p * AssumeRoleProvider) {\n    p.ExternalID = externalAccountID\n})\n// Use sess to create service clients and make API operation calls with.\n```\nTo simplify this the SDK would need to support a portion of the CLI's config file. (#384) Portions of the config file are specific to how the CLI functions and may not translate well to the SDK. Having parity where possible would be helpful. To implement the config file more fully this may also require #414 for service specific configuration injected into service clients.\n. Linking to #384 which tracks the SDKs support general support of the ~/aws/config file. Once the SDK supports the config file we should be able to support assume role. \nThanks for voting for this issue, it helps drive our backlog priority.\n. Thanks for the feedback, and feature request. In release v1.3.0 we added support for the using assume role with the shared config's source_profile and role_arn fields.  This feature is an opt-in and needs to be enabled. The easiest way to enable the functionality is via the AWS_SDK_LOAD_CONFIG environment variable. See the session package docs for more information.\nLet us know if you have any additional feedback, questions, or issues.\n. Thanks for the feedback @geekifier, we're talking about this idea driven by @awood45 between our SDKs based on customer feedback.\n. Hi everyone, based on the feedback we received, along with the discussion over on aws/aws-sdk-ruby#1257 we've decided to enable the functionality by default. This will remove the needed for the AWS_SDK_LOAD_CONFIG env var as it would no longer do anything. A new flag AWS_SDK_CONFIG_OPT_OUT. \nPR #788\n. Per the discussion in PR #788, we've held off on switching this functionality from opt in to opt out while determining the viability, determining impact of opt in to opt out change, and demand of this feature.  With that said, we're listening for feedback on this issue.\n. Thanks for reporting the missing endpoint mapping @catsby. This looks good. I'll regenerate the endpoints_map.go and merge in this change.\n. Hi @catsby after taking a deeper look into this issue the SDK's intentionally do not provide this functionality for Opsworks and similar service that are only present in us-east-1. Adding this override would prevent OpsWorks from expanding to additional regions in the future.  Because of this we won't be able to pull in this change.\n. Thanks a lot for the PR and sorry we cannot take pull it in.  But to maintain forward compatibility with potential service expansion we don't want to add logic automatically aliasing the region for this service.\n. @catsby thanks for your feedback. Improving the way these error message to be more specific would definitely clarify why the request failed, and how to resolve it be very nice.\nI think this might be a good feature to add better error messaging in this case.  Off hand I think we could accomplish this by combining a list of known supported regions with this error message. We wouldn't want to outright block the unsupported region request in case the service has added support for that and the user hasn't updated their SDK.  The list of known regions would be needed to ensure that if there actually are routing or connection errors false negative errors won't be reported.\n. Thanks for the suggestion @phbcanada. Are there any specific questions, examples, or particular problems you would like to see answered?\n. Hi @toanctruong thanks for the request. I'd like to hear more about your request. Amazon Aurora DB Clusters have multi-AZ enabled by deploying to three AZ with one being the primary and two failovers with a autoconfigured cluster endpoint. The cluster endpoint will automatically switch between the primary and failover instances as needed.\nIf you're looking to create a Aurora DB not as a cluster but as a single instance or instance with fallover you can use the CreateDBInstance API operation which will allow you to enable or disable multi-AZ for these instance.\nHere and here is a good guide to how clusters are a collection of instances.\n. @toanctruong thanks for the update and clarification. I did some digging into this issue, and I don't see any way of creating a non-multi AZ cluster with the public RDS API. The RDS web console is doing something internally which does not look to be exposed via the public API. I'll reach out to the RDS team  if creating a ClusterDB without Multi AZ replicas is possible with the public API.\nIn addition I suggestion posting a thread on the RDS AWS forums.  There may be others with this issue, and more request for it will help encourage implementation.\n. Hi @toanctruong I did some digging and have some information about how Aurora RDS instances and clusters work with Multi-AZ.\nSpecifically a Aurora RDS deployment is always actually a cluster.  A database storage cluster (Aurora) will always have to exist in three AZ's for redundancy, but you don't have to create a database head node  or Aurora instance in more than one AZ.If you were to create an Aurora cluster through the console and choose multi-az, it would create you a primary head node in one AZ and a read replica in another (eg. multi-az). When selecting \"No\" for the multi-az deployment in the console, it will simply create you one head node in the particular AZ you choose. \nTo create a single AZ deployment with the RDS API you can call CreateDBCluster to create the underlying storage cluster and another call to CreateDBInstance to create the database head node to connect to. For all intensive purposes, that is an Aurora Single AZ deployment.\nLet us know if there if there is any additional information we can provide, or help with.\n. Thanks for contacting us about this issue @toanctruong. I'm going to go ahead and close this issue. Please let us know if you have any other questions, or feedback.\n. Thanks for finding the missing configuration @chrismrivera. Looks good, and merging in.  I'll add a test case to make sure this doesn't break in the future.\n. Thanks for the PR to correct these @gws. Looks good.\n. Thanks for the typo fix @shawnps.\n. Thanks for lettings us know @RichardKnop. I've updated the gzip content example.\nCreating an issue and letting us know about the update/fix is the best way since Github does not allow wiki PRs.\n. Hi @catsby thanks for letting us know. The DatabaseName is modeled as being an output parameter of DescribeDBCluster's list of DBClusters.  It looks like there is an issue not returning the name of the database. The support ticket you opened with RDS is a great start, is the best way of correcting this issue.\n. @zshenker Thanks for submitting this PR. I'll take a look at it and post any feedback on the PR.\n. Thanks again for creating this issue and submitting the PR @zshenker I've merged it in.\n. Hi @m3alibusra thanks for contacting us.  I think the issue you are experiancing was fixed in v1.0.0 specifically 7130969.  I think if you update to the latest or at least v1.0.0 the issue you're experiencing with pagination should go away.\n. Looks good @zshenker. Thanks for putting this PR together and adding the interfaces for the Upload and Download S3 managers. Spot on with the naming and design.\n. Hi @justinsb thanks for contacting us. The SDK currently will keep the original timestamp until the request is resigned. Which only occurs when credentials expire and are refreshed.\nGenerally the delay between retry attempts starts at 30ms and increases exponentially. Up to three retry attempts. With that said, if the request takes a very long time to fail, or the max number of retries is very high it could in theory create a condition where time offset may be large enough to cause signature failures.\nIn most cases the SDK probably shouldn't need to resign the request for all retried requests since the time since last signing would of been should be safe. Even a signed time vs current time of several seconds should not impose a problem with expired signature errors. We should research if there is a benefit to resigning retried request which are being retried a significant duration after the request was last signed.\n423 covers the idea of updating the SDK to account for clock skew.\n. Hi @zshenker, thanks for posting this issue. It looks very similar to the aws/aws-sdk-js#401 issue you linked. I'm curious where the 20160106T204043Z time came from as it is nearly an hour offset from 20160106T211334Z.\nI'm interested to see more information about these errors you're seeing. If you enable debugging you'll be able to see the retry attempts and what time the headers are being signed at.\ngo\nsvc := sqs.New(sess, aws.NewConfig().WithLogLevel(aws.LogDebugWithHTTPBody | aws.LogDebugWithRequestRetries)\nsvc.ReceiveMessage(...)\n. Hi @zshenker Thanks again for contacting us. I'm going to close this issue, but please reopen it when you're able if you're still experiencing this issue. \n. Thanks for reporting this. I'm looking into this issue. I think you're right that the problem was caused by the change in go 1.5.2 to include go/types in the standard repo.\nI'll need to research a bit how to fix this as it doesn't look like simply changing the import path because that would not be backwards compatible with 1.4 - 1.5.1.\nhttps://groups.google.com/forum/#!topic/golang-dev/V0n9dBpPSIA may shed some light on this issue and a path forward.\n. Thanks @dcelasun, This is the pattern I've added in gotypesfix branch. I'm running a few more tests with and will merge into master once i've verified it's working correctly.\nThe change to the go/importer also looks to of caused issues with the source code generation logic, but that can be fixed separate from this issue.\n. Thanks for reporting this issue. You should now be able to update your SDK using your normal method.\ngo\ngo get -u github.com/aws/aws-sdk-go/...\n. Hi @smile-on thanks for contacting us.  Correct #384 covers the feature request of adding support for the AWS CLI's config file to the Go SDK.\nI've updated the README.md to be more clear that the AWS CLI's ~/aws/config file is not currently supported by the Go SDK and only the ~/aws/credentails shared credentails file is supported.\nClosing this issue as the readme is updated, and #384 covers adding support for the aws/config file.\n. Hi @seandevs thanks for contacting us.  I think the issue you're seeing is the value for GrantRead is incorrect.  This forums thread and the S3 docs highlight that GrantRead is actually a URI, and the canned ACL policies are prefered to use is possible.\nThe following example shows creating bucket with the GrantRead and ACL attributes.\n``` go\npackage main\nimport (\n    \"fmt\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\n)\nfunc main() {\n    svc := s3.New(session.New(), aws.NewConfig().WithLogLevel(aws.LogDebugWithHTTPBody))\n    res, err := svc.CreateBucket(&s3.CreateBucketInput{\n        Bucket: aws.String(\"myBucket\"),\n        //      GrantRead: aws.String(uri=\"http://acs.amazonaws.com/groups/global/AllUsers\"),\n        ACL: aws.String(\"public-read\"),\n    })\n    if err != nil {\n        fmt.Println(\"failed to create bucket\", err)\n        return\n    }\nfmt.Println(res)\n\n}\n``\n. Hi @seandevs Thanks again for contacting us.  I'm going to close this issue for now since it looks like your question is answered. Please reopen if we can provide more assistance.\n. Hi @stgleb thanks for contacting us. The AWS SDK for Go does not yet have support for resource types similar to goamz's [Bucket`](https://godoc.org/launchpad.net/goamz/s3#Bucket). The best way to add this idea into your app would be to create a Bucket wrapper type which uses the SDK's S3 API operations to perform the Bucket resource tasks.\nUntil the SDK adds resource support this will be the best way of wrapping your Bucket in the form you're looking for.\n. Thanks @stgleb let us know if you have any other questions, or feedback. We're glad to help.\n. Thanks for reporting this issue @eicca.  I was able to reproduce the issue you're seeing. I'm taking a look into what is going on here and update this issue when I have a workaround or solution.\n. @eicca I've been working with the service team. We're investigating and working on a fix for this issue and will post and update when we have a fix together and available.\n. Hi @brianfaull thanks for contacting us.  I've verified IoT data is now working as expected. The issue causing the SDK to be unable to communicate correctly with the IoT data service has been corrected.  The following script exercies this.\n``` go\npackage main\nimport (\n    \"fmt\"\n    \"os\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/iot\"\n\"github.com/aws/aws-sdk-go/service/iotdataplane\"\n\n)\n// Usage: AWS_REGION= AWS_PROFILE= go run iotdata.go  \nfunc main() {\n    if len(os.Args) != 3 {\n        fmt.Println(\"Usage: AWS_REGION= AWS_PROFILE= go run iotdata.go  \")\n        return\n    }\nsess := session.New(aws.NewConfig().\n    WithLogLevel(aws.LogDebugWithHTTPBody))\n\niotSvc := iot.New(sess)\nres, err := iotSvc.DescribeEndpoint(&iot.DescribeEndpointInput{})\nif err != nil {\n    fmt.Println(\"describe endpoint err\", err)\n    return\n}\nfmt.Println(\"Endpoint:\", res)\n\nsvc := iotdataplane.New(sess, &aws.Config{Endpoint: res.EndpointAddress})\nresult, err := svc.Publish(&iotdataplane.PublishInput{\n    Payload: []byte(\"my payload\"),\n    Topic:   &os.Args[2],\n})\nfmt.Println(\"publish\", result, err)\n\nshadow, err := svc.GetThingShadow(&iotdataplane.GetThingShadowInput{\n    ThingName: &os.Args[1],\n})\nfmt.Println(\"shadow\", shadow, err)\n\n}\n```\n. Thanks for catching this typo @msull92. Looks good the failing test is unrelated to your README change.\n. Thanks for posting this @AntiPaste. It looks like a change was made to the testify library. I'm taking a look into fixing this now. Thanks for reporting it.\nRelated to stretchr/testify#262\n. I've added a change which will provide a workaround until testify is updated in  stretchr/testify#263.\n. Thanks a gain @ernesto-jimenez :)\nI reverted the shim around assertTestingT's usage by gucumber, and there should not be any more built issues. Let us know if you run into any issues.\n. @mwhooker thanks for reporting this issue. I was able to reproduce the issue you're seeing. I think you're correct. It looks like this condition should allow error responses to be considered for both status and error matcher types.\n. Thanks @mwhooker, I just posted a PR which should fix this issue. The existing test case for status waiter did not include an error like it should of causing this issue to be missed. The associated test cases were also updated to correctly reflect what error and status are capable of. It also fixes the error message you included that wasn't being populated correctly.\n. @mwhooker I've pulled in the fix for S3 WaitUntilObjectExists not waiting. Let us know if you have any issue, or feedback. Thanks again for reporting this bug.\n. Hi @wmh thanks for contacting us. I'd like to hear more about the issue you're having.  I tried to run a simple experiment below, but i was unable to reproduce the problem you're seeing.  If you can give us any more information, or even better  and example it would a lot investigating this issue.\n``` go\npackage main\nimport (\n    \"fmt\"\n\"github.com/aws/aws-sdk-go/service/dynamodb/dynamodbattribute\"\n\n)\nfunc main() {\n    v, err := dynamodbattribute.ConvertTo(map[string]interface{}{\n        \"hello\": \"world\",\n        \"foo\":   \"\",\n        \"\":      \"blah\",\n    })\n    fmt.Println(v, err)\n}\n```\nOutput:\n{\n  M: {\n    hello: {\n      S: \"world\"\n    },\n    foo: {\n      S: \"\"\n    },\n    : {\n      S: \"blah\"\n    }\n  }\n} <nil>\n. Thanks for the clarification @wmh. You're correct the DynamoDB API explicitly does not allow empty strings for values.\nThe best workaround for this is to explicitly strip out empty string values from the converted to AttributeValue map. An added utility  could be added to the dynamodbattribute.ConvertX as an option to automatically do this for you.  in addition optional validation logic of AttributeValue maps could be helpful such as no empty keys, no empty strings, or other conditions.\n. Hi @wmh This issue has been resolved by 135604874, included in release v1.1.10.\nThis change deprecated the existing ConvertTo/ConvertFrom methods and adds a Marshal and Unmarshal. These additional functions work similar to the old Convert functions with the exception json encoding is not used, and they have full support for binary data and  Number/Binary/String sets. In addition they have the option to omit empty string or other zero values.\nThis resolves the issue you were looking for of a way to remove empty strings from the AttributeValue marshaling.\nThe dynamodbattribute package docs provide more details and usage examples.\nLet us know if you have any issues, or more questions.\n. @wmh While reviewing the dynamodbattribute docs I noticed the wrong struct tag was being references. dynamodbav is the correct tag not dynamodb. #597 corrects the docs.\n. Thanks for contacting us @stgleb. I'm going to close this issue since It looks like your question is answered. Please let us know if you have additional questions, or feedback.\n. Congrats on being the one to opening the 500th by the way.\n. Thanks for contacting us @purohit. In this case 200 status code is valid. For long running operations S3 needs to send back the a response or clients will timeout. In these cases the initial API request was successful, such as auth, resource found, ect.  Then S3 sends back the status code to prevent the client from disconnecting due to read deadlines. If these long running operations fail after the status code is sent there is no way to send a new status code, so just the error body is returned.\nIt is interesting no Exception code and message are being included in that error message. Could you enable debug logging (aws.LogDebugWithHTTPBody)so we can see the body of the S3 response. Which region are you using?\ngo\nsvc := s3.New(session.New(), aws.NewConfig().WithLogLevel(aws.LogDebugWithHTTPBody))\n// make requests\nThere doesn't appear to be any outages so I suggest contacting  customer support.\n. Also, what size of file are you uploading, and are you specifying the PartSize, Concurrency, or MaxUploadParts ?\n. @purohit thanks for the update! that was very helpful.  I wasn't up to date locally here so missed the change that caused this issue.  Looks like the fix to error handling in a3ecfb458 may of caused this bug. \nSpecifically changing the service/s3/unmarshal_error.go's r.HTTPResponse.ContentLength check to <=1`. from == 0.  This looks to be a conflict between 200 status code successful request with no body nor content length set, and a 4xx status code which is an error and no body also\n. Hi @purohit thanks again for reporting this. This was a regression in the change I mentioned above. 6811074 fixes this by correcting the ContentLength check and adds additional test cases to cover these conditions in the future.\n. Hi @jkakar thanks for sending us the PR. Was this the only occurrence of the trailing slash that was causing issues. Out of curiosity what does your tool do when it encounters URI's with trailing slashes.\nIn this case its possible the trailing slash doesn't impose a meaning, but other APIs may intentionally have trailing slashes and give meaning to that distinction. Though with that said its generally considered best practice to be consistent, and only use one style of trailing slashes.\nI'll pass this request along to the Lambda team since their the owners of this model.\n. Thanks for the update @jkakar. I think handle the trailing slashes in your tool is a good interim solution.  I've forward your request onto the Lambda team requesting consistency in the URI pattern used.\nSince your tool is now compatible with trailing slashes and I've forwarded your request to Lambda I think we can close this PR. Please let us know if you have additional issues, or feedback. Thanks again!\n. This issue was incorrectly closed.\n. @rayrutjes Thanks for creating this issue. I think it points out a generally issue with the current way the SDK generates pre-signed URLs. Content-Type is actually being explicitly ignored when calculating the signature even though the sig v4 spec states it as a valid optional parameter to include in the signature.\nI think part of the issue is the SDK's normal request signing verse rules for pre-signing are getting conflated. The ignoredHeaders in v4.go are used to excluded headers from both the query string for pre-signing, and normal signature for SDK requests.\nMarking this as a bug. We should take a look at this and see how changing this method will impact backwards compatibility. Or potentially the correct solutions is the idea mentioned in #458 to not hoist any headers but auth related, and removing/trimming the blacklist down.\n. I think to start here we need to clarify and document which headers can and cannot be hoisted to the URL path for presigning. In addition a pass over which headers are blacklisted from (pre)signing will be helpful. Specifically we can blacklist headers that can by modified by proxies.\nIn this we should also include a list of headers which \"could\" be hoisted to the query string but probably shouldn't specifically these are some of the S3 headers around policy and encryption. Bucket policy filters do not inspect query string parameters, and only verify against headers.\n. Hi @pwaller thanks for contacting us. The condition you found is limited to the EC2Metadata client used by EC2RoleProvider. The SDK did not expect the default http client to be the one modified. But clients to create new http clients based on their need.\nThe deadlines are only intended to be used if the HTTP client is not the default client. This was done with the intention that the default client wouldn't be modified. If there is any other type of check if the that would be helpful for the SDK to use I'd definitely be up to consider it. \n. Digging into this a bit deeper the SDK could be updated to determine if the passed in client has been modified from the zero state http.Client. Which is the original form of http.DefaultClient. If the HTTP client was modified in anyway from the default http client state the SDK should not make any additions and use the passed in client as is.\ngo\nreflect.DeepEqual(*cfg.HTTPClient, http.Client{})\n. @pwaller you are correct. Since the EC2RoleProvider is created when a session.New() call is made any changes to the HTTPClient or http.DefaultClient would not be reflected in the EC2RoleProvider.\n509 was opened which touch on this issue, and the data race that was introduced because of this change.\n. Hi @purohit since the SDK's CloudFront Signer only provides support signed URLs not Cookie signing the single policy statement restriction made sense.  Adding support for cookie signing would be a great addition to the SDK also.  In this context the Policy could be expanded to contain the additional fields used for cookie signing.\nWith that said the SDK's URL signer is overly restrictive. We should be able to remove this restriction. Adding documentation to the SignWithPolicy stating a policy statement is required.\n. Hi @purohit I've pushed a change d3a3e63 in our latest release v1.0.9 which loosens the restrictions on policies so multiple policy statements can be provided if you need.\nLet us know if you have any questions or feedback.\n. @purohit you're correct the documentation is a big vague on this issue. I've reached out to CloudFront team asking them, the limits of the policies, and requesting documentation be updated.\nYou also might be able to get a faster response, if you create a thread on the CloudFront forums. Others may of had this issue also.\nI think the correct action for the SDK to take is be pervasive and future compatible. Which means keeping the allowance for multiple policies in place. Since CloudFront may enable them in the future and users would need to update their SDK in order to take advantage of this new functionality. Updating the documentation to state the requirements and limits would help clear up this issue I think.\n. Hi @stgleb thanks for contacting us. I'm digging into the issue you're seeing, but having difficulty in reproducing it. Do you have any sample code which reproduces the error using the SDK.\nIs your code setting the Config.Endpoint that the SDK is using?\nThe example code i'm using to try to reproduce this is:\n``` go\npackage main\nimport (\n    \"fmt\"\n    \"os\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\n)\nfunc main() {\n    cfg := aws.NewConfig()\n    if len(os.Args) >= 4 {\n        cfg.WithEndpoint(os.Args[3])\n    }\nsvc := s3.New(session.New(cfg))\nresult, err := svc.GetObject(&s3.GetObjectInput{\n    Bucket: &os.Args[1],\n    Key:    &os.Args[2],\n})\nif err != nil {\n    fmt.Println(\"failed\", err)\n    return\n}\n\nfmt.Println(\"success\\n\", result)\n\n}\n```\nI'm using the following to execute this script:\nsh\n$ env AWS_REGION=eu-central-1 go run main.go myEUCentralBucket mykey\n. Great, thanks for the update!  let us know if there is anything else we can help with and if you have any other feedback, or questions.\n. Thanks for reporting this @imkira This looks to be caused by the change implemented for #504. I think you're correct the SDK should not be copying http.DefaultTransport. All this is done in order to provide shorter timeouts for the EC2RoleProvider in the CredentialsChain. Without the shorter timeout, using the SDK's default CredentialChain on a Non-EC2 host would take a over a minute to fail and return no credentials due to retries.\nI think the #504 change and previous implementation is approaching the problem incorrectly. Instead of modifying the Transport the SDK should set the net/http#Client.Timeout instead. This will simplify the concept of EC2 Metadata custom timeouts, and only use this functionality if the client hasn't been modified. Adding a flag to turn off this logic all together is also probably needed.\n. I've posted a PR addresses this data race and improves how the timeout override is configured. The user now has the option of disabling the override completely, in addition to using safe method of configuring the timeout if not disabled.\n. Looks like there is a timing issue in the code which is causing the tests to fail in Travis.  I'm able to run the tests locally without error.\n. @radeksimko thanks for taking the time to put this PR together. I like the idea of adding a Name to the credentials.Value type so that it's easier to see where your credentials came from at runtime.\nI think we make this change without creating a breaking change to the Providers interface.  If instead of adding a Name func to each Provider. The providers could be responsible for setting the fields themselves. Since the providers are solely responsible for filling the credentials.Value's value it would make sense for them to also populate this field.\nIts possible that a provider implementor forgets to provide this field, but it would also not require a breaking change, nor add additional logic to the credentials.ChainProvider and credentials.Credentials types.\n. Thanks for the feedback @radeksimko. I agree avoiding the breaking change would be great. We'd like to go down this path of ensuring no breaking changes.\n\nIf we were to rely on a struct string field, we'd also have to rely on the user to either always use New*() constructors (no way to enforce this in go) or make sure that all provider methods are consistently setting the provider name, or just accept the fact that the name will only be available once you call Retrieve().\n\nI was thinking that the individual Providers would have a const provider name which is used by default when the Provider returns a credentials Value via Retrieve. Optionally we could add a way of setting a custom provider name which will override the default in Retrieve if set. \n\nI think it's necessary to add some logic to the credentials.ChainProvider as it's responsible for choosing the real provider\n\nIf the chain provider just passes the credential object return by the nested provider then the ProviderName field would already be set and reflect the value that was set by the nested provider before it returned the name. Chain provider wouldn't need to mutate the received credentials.Value it would just pass it along. If instead we went with the Name() method we would need to implement it via an optional NamedProvider like interface that Providers could optionally support. In that case we could expect Providers to not also be named.\n\nThe ultimate question is then, do we want to make this work? awsCredentials.StaticProvider{Value: awsCredentials.Value{...\n\nIn this case I would expect the StaticProvider's Retrieve() function to be responsible for setting the ProviderName. Its up for discussion if the StaticProvider should ignore, passion or augment the passed in credentials.Value.ProviderName field.\n. Awesome!, and thanks for your time and help getting this feature implemented. \n. @radeksimko thanks for making the updates, and glad we can help :) We'll review your changes and get back to you with feedback soon. Thanks again for taking the time to put this PR together.\n. Looks good @radeksimko thanks again for taking the time, and working with us to add this feature to the credentials.\n. Thanks for the PR @radeksimko Merging the two test dependencies looks good. Merging this change in.\n. @pwaller, The points you mention are similar to ones we will need to consider in the future how the SDK should introduce custom timeout logic across services and operations for conditions like ServiceUnavailable.\nModifying http.DefaultClient and http.DefaultTransport at runtime seems to easily run the risk of introducing data races. It is preferable to create a custom client and pass that around as needed to ensure data races are not introduced. It is a data race bug to be mutate shared client's parameters while being used to make requests concurrently.\nTo address this in the most generic way ensuring that the client isn't modified at all. I'm taking a look at if we can take advantage of http.Request.Cancel. This would allow the SDK to cancel a request, of the client's transport supported it. Which actually is similar to http.Client.Timeout's functionality.\n. @pwaller Correct, originally this timeout was introduced in order to reduce the timeout delay (about 2min) when running on a non EC2 instance. It provided balance between quicker failures on non-EC2 instances. We could adjust the default timeout to something more appropriate, and the aws.Config.EC2MetadataDisableTimeoutOverride options is available to disable this modification all together. \nI agree this shorter timeout should be relaxed or removed once a connection was made.\n. Thanks for the update @arvenil, That was going to be my first question is you'd sync'ed yet with the latest version. the usage of http.DefaultClient is is actually sourced from aws/defaults#GetConfig which a session.New() will automatically call. This ensures a http client is always provided to the service clients. In addition this is why the SDK wouldn't be able to check for nil of HTTPClient, because in practice it will never be nil.\n. We've reviewed this issue and think that leaving the EC2Metadata service client mostly as it is. The best way to control the timeouts would be to set the Timeout field on the http.Client passed into the SDK's config, set the timeout on http.DefaultClient, or to disable to timeout override all together with aws#Config.EC2MetadataDisabletimeoutOverride.\nThe following statement shows how to disable the timeout all together.\n``` go\nsess := session.New(aws.NewConfig().WithEC2MetadataDisabletimeoutOverride(true))\ns3Svc := s3.New(sess)\n```\nModifying the http.DefaultClient should be done prior to creating a session. Modifying the http.DefaultClient after sessions, or service clients have been created could introduce race conditions if the SDK were to ever make background request post initialization.\n. We decided not to modify the timeouts for now due to the need to add locking around usage of the Config.HTTPClient. When the SDK looks at adding more comprehensive request timeout logic generically for a per-API operation level I think this will be re-evaluated since how the SDK uses timeouts will most likely change.\n. Hi @arvenil Thanks for contacting us. The SDK allows configuration of non-service specific values via the environment such as Region, Shared credentials profile, and Credentials values.  We have a configuration wiki that will help answer some of your questions as well. In addition the SDK also has shared sessions to share configuration and request handlers between multiple service clients.\nTo set the region via environment variables you can use something like AWS_REGION=us-east-1\nAlso checkout the configuration section for setting up credentials with the SDK. It will guide you on how you can specify credentials with the SDK via environment variables, shared credentials file, and ec2 instance roles. By the default the SDK will search for credentials via the environment variables, then in the shared credentials file, and finally in EC2 Instance role. Additional credentials providers can be added such as STS Assume Role too.\nThe endpoint is not available to set via the environment variable because it generally is only useful if you're working with a single service. As each service has its own unique endpoint. If you'd like to use the an endpoint environment variable with your application you can add it via:\n``` go\npackage main\nimport (\n    \"os\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/dynamodb\"\n\n)\nfunc main() {\n    sharedSessionCfg := session.New()\nvar serviceCfg\nif endpoint := os.Getenv(\"DYNAMODB_ENDPOINT\"); endpoint != \"\" {\n    serviceCfg = &aws.Config{Endpoint: aws.String(endpoint)}\n}\n\nsvc := dynamodb.New(sharedSessionCfg, serviceCfg)\nresult, err := svc.PutItem(&dynamodb.PutItemInput{ /*...*/ })\n// ...\n\n}\n```\n. Thanks for reporting these @client9 Lets hold off on a PR for now. I recently noticed these a few days ago with the Go Report Card http://goreportcard.com/report/aws/aws-sdk-go tool, and roughly looking at it they look to be similar lists. I'll get these forwarded upstream to the service owners. \n. Thanks again @client9 i've reported these to the service teams so we can get the models updated. thanks for lettings us know about your utility and the spelling errors!\n. Thanks for pointing this out @arvenil. Taking a look at our interfaces it looks like Paginators are being added to the interfaces but Waiters are not. Off hand I cannot think of a reason why we wouldn't want to interfaces to also include the waiters.\nIt looks like this will be slightly more complicated to add to the interfaces than Paginators were because the waiters are written as separate file, and managed separately than the APIs and Paginators. private/model/api/waiters.go is responsible for generating the waiters for each service\nIf you're looking to make a change to the SDK we'd be more than glad to review your PR.\n. --reviewed by Ben\n. Correct, checking the length of resp.Item is the best way to know if a GetItem request returned a result. Are you seeing in issues where an empty map for Item causes conflicts?\n. Thanks for the feedback @arvenil I'll make sure to forward that along to the DynamoDB team. Glad to help. Let us know if you have any other questions, feedback, or ideas/features to improve the SDK.\n. Hi @hhk1989 thanks for contacting us. The AWS SDK for Go does not have support for Elastic Searches' data plane, or \"Document APIs\" like those you've listed. The SDK only provides support for the control plane operations which includes those such as CRUD domains, tags, and tags.\n. Thanks for your inquiry @hhk1989. I'm going to go ahead and close this issue since the SDK only supports the control plane of ElasticSearch Service. \n. @tj you could use the SDK to generate signatures for non-supported services by using the Request type to build the request and private/signer/v4 to sign the request.\nThis is pretty complex like you mentioned because you'd need to marshal the input parameters and body your self instead of relying on the SDK's service marshalers. But once the request is fully built, and the body bytes are written on aws/request#Request.HTTPRequest it is safe to call the v4 signer. \nSimplifying the ability to sign an arbitrary http request with the SDK would make this process much easier. If you'd like to create a new issue focusing on this we can add it to our feature request list.\n. @marcosnils pushed our release v1.0.11 which adds ACM service support.  Let us know if you run into any issues with using the SDK with the ACM service.\n. @mengbiping Thanks for submitting this PR to fix []byte in the dynamodb.AttributeValue conversion tool. The change looks good overall. While reviewing it I noticed that if ExampleConvertTo is updated to include a []byte field also it incorrectly gets converted to a string for some reason.\nI think this is caused by the existing logic which attempts converts typed structs to map[string]interface{}. In theory this should be similar to passing in your own maps[string]interface{}. In the case convertToUntyped is converting a []byte to a base64 string.\nI created #529 to track improving the ConvertTo utility's handling of typed structs vs untyped map[string]interface{}.\n. @mengbiping thanks for your feedback. I think we can pull in this change. I'll add some documentation to ConvertTo that the conversion only works for map[string]interface{} forms.\n. Thanks for submitting this PR @mengbiping I've merged it in f36dd26c and added documentation stating []byte is limited to map[string]interface{}.\n. @wolfgangmeyers thanks for contacting us. The example in the readme is still valid, but you're right a more verbose example on how to use the SDK with sessions would be helpful.  I'll update the README to provide more details.\nThe original method was the following with an AWS config or nil value as the only parameter\ngo\nsvc := ec2.New(&aws.Config{Region: aws.String(\"us-west-2\")})\nin v0.10.0 this was updated to require a session with an additional optional aws.Config parameter. Like in the README example.  Adding more information to the README about sessions would be helpful.\nWe also have a section in our wiki, but a bit out of the way. Reading through this wiki example I think it can be improved as well.\n. Looks like the bug you're seeing is related to a recent change (jmespath/go-jmespath#15) in a upstream dependency of the SDK. If you are using Go 1.5 or greater you can set the GO15VENDOREXPERIMENT environment variable to 1. This will cause your go build to use the SDK vendored version of go-jmespath. Alternitivily you can checkout go-jmespath commit c01cf91b01 to before the jmespath change was implemented. \nI think we should raise this issue with go-jmespath and see if we can get it resolved on their end.\n. glad that helped @chadgrant, I created jmespath/go-jmespath#20 to track this issue upstream.  Will leave this issue open for visibility in case any others run into this issue.\n. The go-jmespath package has been updated to revert the change which was causing values to be returned instead of the values original type.  If you sync the go-jmespath package with:\ngo get -u github.com/jmespath/go-jmespath\nLet us know if you're still having any issues, or feedback.  Thanks for all of the information helping identify this issue and getting it fixed.\n. Thanks for the performance report @koenbollen, it is very helpful in tracking down this performance issue. From the frame graph it looks like the SDK's marshallers use of fmt is taking a significant portion of the total time.\nApproximately what was the size of the emails sent? Also any additional information about the complexity of the emails sent would be helpful targeting this perf issue.\n. Reviewing this method it looks like building the paths within validateAny is causing a significant impact for the API operations CPU performance.  The function is recursive since it walks the input API structures searching for fields to validate.\nThe smallest change we could make is to change how the validation error path is constructed. To something that isn't using fmt and string concat.\nAlternatively a more comprehensive change would be to add methods into each API input struct which are generated and know how to explicitly validate each field in the struct.  This would both remove the unnecessary recursion, but also increase the source code and binary size of the SDK.\nSomething like the following would be generated code. Nested validation would call Validate() recursively on shapes where their children have validation requirements.\ngo\ntype GetObjectInput struct {\n     Bucket *string `required:\"true\" type:\"string\"`\n     Key    *string `required:\"true\" type:\"string\"`\n     ACL    *string `type:\"string\"`\n     // ...\n}\nfunc (s *GetObjectInput) Validate() []error {\n    var errs []error\n    if validation.Require(s.Bucket) {\n        errs = append(errs, fmt.Println(\"GetObjectInput.Bucket not set\"))\n    }\n    if validation.Require(Key) {\n        errs = append(errs, fmt.Println(\"GetObjectInput.Key not set\"))\n    }\n    return errs\n}\n. Hi @koenbollen I've created a PR #649 which should address the performance issue you're seeing with validation. Specifically this update changes the SDK to generate its validation instead of using reflection at runtime. This improvement basically removes the impact of validation on requests.\n. Thanks for the update @koenbollen. We are glad to take a look at any additional funtionality or design updates that would provide the flexibility and performance. In addition are there other updates to the SDK which would of made using it easer or more performant?\n. Looks really good, thanks for taking the time to put together this PR.  The only comment I have is a minor tweak to the method used to determine if the value is a []byte\n``` go\n// precached byte slice reflect type for comparison\nvar byteSliceType = reflect.TypeOf([]byte(nil))\n// ...\nfunc (v *validator) validateAny(value reflect.Value, path string) {\n// ...\n    case reflect.Slice:\n        if value.Type() == byteSliceType {\n            // Skip byte slice types since there is no validation on content\n            // inside the byte slice.\n            return\n        }\n// ...\n```\nThis method I think is a little easier to read, and allocates the same memory. Over several benchmark runs it seems to be about 4000 ns/op faster, but that could easily be just my system.\n. Looks good thanks for taking the time to create the PR. This will improve the resource usage of the SDK on any API operation with []byte as input parameters.\n. @ChrisFernandez thanks for the update. The SDK does allow a no input values to be provided, but this is accomplished by passing in nil or, in this case, &opsworks.DescribeStacks{} to the operation function. The last example you provided should do this because svc.DescribeStacks(nil) is not being given any additional input parameters.\nThe documentation about not providing the parameter is referring to the StackIds field of opsworks.DescribeStacks struct. In the AWS SDK for Go all API operations take a struct pointer as input, or nil for no input.  The fields that the API operation uses for its input are contained within the input structs.  In this case if you did want to describe the stacks for specific stack Ids you would do something like the following:\ngo\nresp, err := svc.DescribeStacks(&opsworks.DescribeStacksInput{\n    StackIds: []*string{ aws.String(\"id1\"), aws.String(\"id2\") },\n})\nWe have a making service request wiki which includes some additional helpful tips on how to use the SDK.\n. I see thanks for clarifying. The example you listed in 180498284 is actually the best way to get a list of all of your stack Ids.\nTo retrieve the individual stack Ids from the response you use the fields of opsworks.DescribeStacksOutput struct. This struct has a field Stacks which is a list of opsworks.Stack structs. The StackId is a field on this struct.\n``` go\nresp, err := svc.DescribeStacks(nil)\nif err != nil {\n    return err\n}\nfor _, stack := range resp.Stacks {\n   stackName := aws.StringValue(stack.Name)\n   if stackName == owstackname {\n       fmt.Printf(\"found %s stackid\\n\", stackName)\n       fmt.Println(aws.StringValue(stack.StackId))\n   }\n}\n```\nThe aws.StringValue functions are helpers to convert the string pointer types of the SDK's input/output structs to values. It also protects against the value being nil where it will return empty string.\n. Thanks for catching that. I typed the for loop wrong, and fixed the example above. I did not include the range in the for loop.\n. glad to help, @ChrisFernandez  let us know if you run into any other issues, or anything else we can help with.\n. > Naw, they weren't causing any issues.\nProbably can just removed the [7:] then to not cause unexpected issues in the future.\n. LGTM\n. In generally I think the reason S3 does not provide a header is to explicitly prevent an user agent from automatically following the redirect if the bucket requested is being requested from a request to the wrong region. Specifically because a request would need to be redirected, and the request would need to be resigned for the target region. \nAn alternative approach if you don't know the region which the bucket exists in is to do a HTTP HEAD request to the bucket's url. It doesn't matter which region the request is made to the x-amz-bucket-region header will always provide the actual location. With this you can use the region to make other operation requests to the region for the bucket.\n``` go\nresp, err := http.DefaultClient.Head(\"http://s3.amazonaws.com/lidar_fema\")\nregion := resp.Header.Get(\"X-Amz-Bucket-Region\")\ns3Svc := s3.New(sess, aws.NewConfig().WithRegion())\n// Operate on bucket in region\n```\nThis is the preferred method of determining the region of a bucket without authentication.\n. Good catch my example was meant for domain style but typed it as path style. But that doesn't really help if you need to use path style. \nDue to the way Go's net/http#Client's transporter handles redirects the only workaround for this I can see is to use a custom Transport/RoundTripper for the http client making the head request so that redirects are controlled and the headers can be captured, and not discarded. Since path style http requests this is quite complicated we might want to provide a utility for this in the SDK.\n. Basically, but i think we can minimize the customization by just using the transporter's RoundTrip() instead of creating a whole new client.\n``` go\n    req, err := http.NewRequest(\"HEAD\", \"http://s3.amazonaws.com/jasdel-bucket02\", nil)\n    if err != nil {\n        fmt.Println(\"Failed to create request\", err)\n        return\n    }\ntr := http.Transport{} // or aws#Config.HTTPClient.Transport, or http.DefaultTransport\nresp, err := tr.RoundTrip(req)\nif err != nil {\n    fmt.Println(\"failed to do http transport request\", err)\n    return\n}\nfmt.Println(\"headers\", resp.Header)\n\n```\nThe down side of this is the SDK would lose connection pooling for this request, but this method bypasses the http.Client redirect logic.  The response generated will still be the same, but may loose out on connection pooling and any other built in redirect logic.\n. Hi @radeksimko thanks for contacting us. I'm taking a look at this feature. I'm curious to know more about the background on this change. Where there situations where apps were mistakenly setting themselves up on that host:port? It doesn't look like any of the other AWS SDKs implement this feature.\nI ask because adding this might prevent users from running local proxy's caches in front of ec2metadata service.\n. Thanks a lot for the extra information @radeksimko  and @catsby. I like the idea of being able to filter if the metadata connection is to the intended service, but I'm concerned that the Server: EC2ws header doesn't seem to be an officially supported or documented feature. Adding a dependency on the header into the SDK could create a reliance on a feature which is not official documented or supported. If this header were to be removed, or changed in the future it could break existing users of the SDK.\nIn the general case the SDK relying on this feature could introduce unexpected outcomes if the header was removed or changed in an unexpected way. Though this is a feature users could explicitly opt into by adding the check to the EC2 Metadata client's ValidateResponse request handler.\nWith that said the following code could be added to a code base to enable this validation.\n``` go\nsess := session.New()\nmetadataSvc := ec2metadata.New(sess)\nmetadataSvc.Handlers.ValidateResponse.PushBack(func(r *request.Request) {\n    if r.HTTPResponse.Header.Get(\"Server\") != \"EC2ws\" {\n        r.Error = awserr.New(\"ValidationException\",\n            \"EC2 metadata service does not appear to be a valid EC2 metadata service\",\n            nil)\n    }\n})\n// Use metadataSvc client throughout your application\n```\n\nbut shouldn't such proxy be proxying HTTP headers too?\n\nYou're right, proxies really aren't that significant of an issue. Though hypothetically if a user had custom caching service between ec2 metadata and the SDK that didn't forward the headers, and the SDK introduced this change it would be considered a breaking change.\n. I'll reach out to the EC2 service team to see if there is any other prefered method for verifying the API endpoint.\nIn addition I suggest also posting to the EC2 Forums you might be able to get a quicker answer reaching out to them directly.\n. Hi @radeksimko I was reached out to EC2 and they pointed me to the Instance Identity Documents endpoint. This process does require an extra logic to first verify the EC2 Metadata host with the the identity docs.\nAlternatively to be absolutely sure the EC2 metadata endpoint is a valid ec2 metadata host, you can verify the instance's signature as mentioned in the above doc.  This is quite a bit more of an involved process though.\n. @radeksimko we'd be more than glad to take a look at a PR adding a convenience method to verify the validity of the metadata endpoint. Were you thinking this functionality would be automatic or an opt in operation that users would explicitly call, returning if the metadata endpoint appears to be an EC2 Metadata service?\n. Per our discussion on #590 the simplest way that is table to detect a host is the ec2 instance is to use the EC2Metdata#Available method.  This method will detect if the ec2metadata endpoint is valid and the endpoint returns an instance id.\n. I've pulled in the PR for adding ident doc support. Since we have the available method and it looks to be good for this use case I'm going to go ahead and close this issue.  Thanks for brining the issue up, and creating the associated PR.\n. Thanks for the extra information @Shervanator. I'm taking a look at this issue. If you're able to would you mind adding extra debug logging to your application? You can do this by setting the SDK's log level. This log level will give us the wire request/response data.\nYou can set the log level just for the EC2 instance role, and will only produce wire logs of the requests for credentials from the ec2metadata service.\n``` go\nsess := session.New()\nsess.Config.Credentials = ec2rolecreds.NewCredentialsWithClient(\n     ec2metadata.New(sess, aws.NewConfig().WithLogLevel(aws.LogDebugWithHTTPBody),\n)\n// use session within code\nsvc := s3.New(sess)\n```\nAlternatively you can set the logging globally add debug logging to your application's usage of AWS requests.\n``` go\nsess := session.New(aws.NewConfig().WithLogLevel(aws.LogDebugWithHTTPBody))\nsvc := s3.New(sess)\n//...\n``\n. It looks like the EC2Metadata client is actually just [discarding the body](https://github.com/aws/aws-sdk-go/blob/master/aws/ec2metadata/service.go#L103-L110) of the errored response. This should be a pretty simple change to add the body response as apart of ther.Errorvalue\n. Going to close this feature request as this change would require a breaking change to the SDK to change theRecord` type to an interface. In addition this would prevent the SDK being forward compatible with the service if the service were to add additional optional fields to the Record type as the interface would not be able to represent them.\nThis logic could better exist as a higher level abstraction on top of the Kinesis service.. Hi @raunak-agarwal-flipkart thanks for contacting us. The SDK does not implement the v3 authentication method.  In general the SDK only supports v4, with the exception of v2 for SimpleDB.\nThe best references to the V3 authentication i could find is implementations in the AWS SDK for Ruby. There are the ruby signing implementations for HTTP and HTTPS protocols.\n. @raunak-agarwal-flipkart Going to close this issue, please let us know if you still have any additional questions about this issue, or feedback.\n. Hi @Rauk the AWS SDK for Go is capable of using v2 signature for services and regions which support it, but v4 is prefered and will work in all AWS regions. Is there an AWS service you need to use V2 signing with, but unable to use v4? The signers are in the private/signers package path. If you would like to use a service with v2 you can modify the Sign request handler list to use the v2 signer instead.\nOr are you looking for a standalone v2 request signer? The both SDK's signers are specific to the SDK's request structure.\n. LGTM\n. Thanks for contacting us @artemnikitin, glad you were able to resolve your issue. Let us know if there is anything else we can do to help prevent this issue in the future, or if you have any other questions/feedback.\n. @jeanlaurent thanks for pointing out that comment. I updated it to be more clear.\n. @tj Agreed, one idea to approach this is to simplify the published v4 signer to operate just on an http.Request and some generic parameters instead of request.Request. This would go a long way in allowing custom v4 request signing.\n. Hi this is now available in release v1.2.0. Let us know if you run into any issues or feedback or questions.\n. Based off wrong root branch\n. Hi @pmbapat thanks for pointing this out. The SDK will automatically convert blob AWS service API fields from base64 to a []byte the Content value will be the decoded raw bytes of the credentials report.\nFor fields which the service models as blobs the AWS SDKs will automatically apply special logic to decode the base64 encoded string into raw bytes as a convenience when unmarshalling the api operation response.\n. Hi @charlie-ht thanks for contacting us. I think you're on the right path to implementing the custom provider. To address your concern of periodically refresh outside of Credentials.Get()'s call you could have you custom provider periodically refresh the credentials it self instead of relying on the Retrieve method being called by the Credentials stored in the Session's Config.\nNow to note with this pattern you do run the chance of leaking the timer channel if you find your self not using a provider for the life of the app.\n. Here is a an example provider which could be used to refresh your custom provider in the background within needing Retrieve() to be called periodically. I could see functionality similar to this being useful generically within the SDK also. We could consider adding this functionality generically to the SDK, as an optional Provider layer.\n``` go\npackage main\nimport (\n    \"log\"\n    \"sync\"\n    \"time\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/credentials\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\n)\nfunc main() {\n    sess := session.New()\n// call credRefreshTicker.Stop() to stop the cred refresh\n// useful when the custom credential provider will not\n// be used for the lifetime of the app.\ncredRefreshTicker := time.NewTicker(1 * time.Hour)\ns3Svc := s3.New(sess, &aws.Config{\n    Credentials: credentials.NewCredentials(&RefreshProvider{\n        Provider: customProvider,\n        Ticker:   credRefreshTicker,\n    }),\n})\n\nfor {\n    s3Svc.GetObject(&s3.GetObjectInput{\n        Bucket: aws.String(\"bucket\"),\n        Key:    aws.String(\"key\"),\n    })\n\n    <-time.Tick(5 * time.Hour)\n}\n\n}\ntype RefreshProvider struct {\n    // Nested Provider to call Retrieve and IsExpired on\n    credentials.Provider\n    // How often the credential provider should be refreshed.\n    Ticker *time.Ticker\ncreds      credentials.Value\nerr        error\nmux        sync.RWMutex\ninitRunner sync.Once\n\n}\nfunc (p *RefreshProvider) Retrieve() (credentials.Value, error) {\n    p.initRunner.Do(func() {\n        p.creds, p.err = p.Provider.Retrieve()\n        go p.periodicRefresh()\n    })\np.mux.RLock()\ndefer p.mux.RUnlock()\n\nreturn p.creds, p.err\n\n}\nfunc (p *RefreshProvider) periodicRefresh() {\n    for {\n        _, ok := <-p.Ticker.C\n        if !ok {\n            break\n        }\n    p.mux.Lock()\n    // Probably want to log the returned error\n    creds, err := p.Provider.Retrieve()\n    if err != nil {\n        if p.Provider.IsExpired() {\n            // If the credentials have expired, and still unable\n            // to retrieve new credentials set the error\n            p.err = err\n        }\n        p.mux.Unlock()\n\n        log.Println(\"ERROR: failed to refresh credentials provider,\", err)\n        continue\n    }\n    p.err = nil\n    p.creds = creds\n    p.mux.Unlock()\n}\n\n}\nfunc (p *RefreshProvider) IsExpired() bool {\n    p.mux.RLock()\n    defer p.mux.RUnlock()\nreturn p.Provider.IsExpired()\n\n}\n``\n. Hi @davisford thanks for contacting us.  I think the issue you're seeing is with the usage of abucket, andkey`. A bucket contains many objects, and all objects have a name which is its key. To reference a object you need to provide both the bucket, and the key to object.\nIf we have a bucket named mybucket and an object with the key my/object/namespace/log.txt, the elements my, object, namespace are not nested buckets, but portions of the object key's name. The S3 Web console will represent these parts as folders but this is a convenience method to make viewing buckets with many objects easier. You can use any form of delimiter in key names you'd like, or none at all if that makes sense in your use case. / is commonly used since it mimics how unix filesystems are delimited. Take a look at BucketBasics which goes over this topic in more detail.\nWith that said it sounds like you'd like to list buckets and only list a subset of the objects in the bucket. To do this checkout the ListObjectsInput.Prefix which will allow you to provide the key prefix to filter the list of objects by.\nIn addition I would suggest using the S3.ListObjectsPages func since it will provide pagination, which is especially helpful if you have many objects in that bucket.\n. Generally the response body is closed by the service's protocol unmarshal or unmarshalError. In the case of SQS this is the Query protocol. query.Unmarshal and query.UnmarshalError. Which if either are called will close the Request.HTTPResponse.Body.\nAlso like @xibz mentioned, custom request handlers or removing/clearing/modifying the service client's default Unmarshal or UnmarshalError might also cause response bodies not be closed.\nr.Handlers.Retry isn't set directly by the SDK and is intended as a place where user's of the SDK can add their own custom handlers to determine or alter the retry rules. r.Handlers.AfterRetry is the handlers which will perform the request retry. I think this issue is related to the url.Error case if an error occurs in SendHandler and isn't properly closed the connection would leak. Especially since its stomped on in the SendHandler logic.\nurl.Error also seems like a likely culprit since err doesn't mean a resp wasn't also returned. There could be an situation where an err is returned along with a resp, and the SDK is incorrectly stomping on top of the existing response body. I think at the minimum SendHandler should check if r.HTTPResponse is nil  and if not close the body prior to stomping on the value in the url.Error check.\n. Thanks for taking the time to submit this PR @ekechi the change, and test looks good. We'll need to update our lint filter to ignore the ID vs Id case for this unit.\nThis issue does shine light on the fact the uploader's version of this struct is not generated like the the S3 package version. We should take a look at how to prevent this issue in the future.\n. Thanks for taking the time to submit this PR @ekechi I've taken the change here and added additional test in #575. I'll merge your change in that PR.\n. Hi @xrl thanks for contacting us. The SDK itself does not persist the account credentials or ARN. With that said there is the stscreds.AssumeRole credentials provider. Are you using this provider for your credentials? In addition the $ACC_ID is this the AssumeRole External ID that you're looking for?\n. Thanks for contacting us @xrl , let us know if you have any additional questions, or if we can provide more information.\n. LGTM\n. Looks good, I'd suggest also adding a small test for offsetReader which spawns a handful of goroutines calling the reader's Read method in a tight loop. Then after a period of time call Close or CloseAndCopy (maybe case for each?). The goroutines reading could exit when their Read call returns EOF. And with the -race test flag this should show if the offsetReader creates a race condition.\n. LGTM\n. Hi @jcmcken thanks for creating this feature request. Expanding the configuration of the regions endpoint map would be helpful for users with custom endpoints. For most services The SDK shouldn't need to be updated with new regions are added. though changing of endpoints is definitely a different issue.\nWe've been considering how we could expose this feature so that users can provide custom overrides in addition to or on top of the current SDKs endpoint definitions. I could see this feature still code-gen/compiling in a default set of endpoints which could be overridden via a func call, and maybe env.\nThe current format of the endpoints.json file probably would need to be updated to support this pattern and further endpoint growth for AWS services.\n. This feature request is related to #108 and #384.\n. Thanks for the feedback, and suggestion @adamcrosby I'll forward your feedback along. In addition, you can leave feedback for additional features directly about Regions and Endpoints with AWS using the Feedback feature in the bottom right hand corner on the AWS doc pages\n. Hi @jesusjjf thanks for taking the time to submit this PR, the change looks good. Going to pull this in. \n. Looks good, just a couple comments.\n. Dropping this for now because of the Go version differences with template.\n. Hi @radeksimko thanks for submitting this. This change seems to blacklist the example flag for 1.6, 1.5, 1.4 instead of just 1.5, 1.4. In Go 1.6 vet added the -example flag.\nAre you seeing the vet fail with with Go 1.6 and tip?  The intention of the shells script is a blacklist against 1.4 & 1.5, and allow new versions of Go to vet correctly.\n. @radeksimko  Do you have a custom version of golang.org/x/tools locally?  This is where vet is built from. if so we actually saw this internally where the golang.org/x/tools was out of date and did not yet include the changes for the -example flag added in 1.6.\nThis issue might be resolved by updating vet from the tools repo.\nsh\ngo get -u golang.org/x/tools/cmd/vet\n. Thanks for submitting this PR @radeksimko I'll review the change, and check if there is a standard for the metadata endpoint.\n. Good point, I think its fair to expect Available's check for instance-id is valid and stable.  If this changes in the future via a version rev Available could be updated to correspond to that new method. But In either case the way available checks for instance-id is expected to be stable.\nAdding the retrieval for the ident doc is a good addition too :) so not a waste of time.\nWhen initially created I was thinking you were looking for a more detailed way of verifying the ec2's host's identity. I should of mentioned available. But i think your right Available probably is enough to verify ec2 instance in this usecase.\n. The change looks good, thanks for taking the time to investigate this and implement the PR. Even though  Available also exists, this will be a helpful update.\n. Thanks for finding this issue, and putting together the PR @ppknap. The change looks good and it simplifies the process of generating the encoded string.\n. I was considering how best to represent BatchedErrors.OrigError() other than just another batched error. I'm thinking as a error type which also has a OrigErrs method. What do you think.\nFor the leak test, not sure what to test there, there are no io.Read/Writers nor channels. What were you thinking should be tested?\n. Ah I see, for some reason I was thinking you meant for the error change. Yeah that makes sense for the route53 unmarshaller. I'll add that.\n. - Added more robust baseError.OrigErr handling so it can return more comprehensive values.\n- Added leak tests for custom unmarshaller\n. Thanks for the feature request @jasonrichardsmith. I'd like to hear more about the shard iterator management you were thinking of. Are there any specific implementation's you think stand out? I found nieksand/gokinesis and sendgridlabs/go-kinesis. Or are their libraries in other languages you think model the KCL better that an implementation could glean from?\n. Thanks for the PR @radeksimko, and adding the helpers. I'm curious about the need for HasIAMInstanceProfile though. What use case do you see in identifying if IAM info is available but not using it?\nI think having two methods could create confusing users and might cause them to call HasIAMInstanceProfile then IAMInfo without knowing that they are duplicating the request.\n. I think your use case makes sense. Thanks for the clarification. I agree manually parsing the account ID out of the ARN could be hacky. A tool which automatically parses the various ARN formats would be the best way, but I don't think that doesn't exist at the moment.\n. I think Available has a more generic usage whereas its purpose is to state that at this point in time the EC2 metadata instance endpoint was known to exist and return some value for the instance id. Though there is no validation that the value return is, or even looks like, an instance Id. (Maybe this should be added?)\nHasIAMInstanceProfile is similar in the vein that it is stating if Identity document was retrievable, but I also agree that it also implies that the IAMInstanceProfile is available, and there might be a situation where it isn't separate from the metadata service not being available.\nI am partial to just calling IAMInfo and letting the user determine the meaning of error based on their usecase. What do you think about dropping HasIAMInstanceProfile and use IAMInfo instead?\n. Thanks for letting us know of about this issue @flawedmatrix. I'm checking with the service team to see if this value changed recently.\n. As a short term workaround you can add a custom UnmarshalError request handler to your code in order to workaround the issue until it is resolved.\n``` go\npackage main\nimport (\n    \"fmt\"\n    \"os\"\n\"github.com/aws/aws-sdk-go/aws/awserr\"\n\"github.com/aws/aws-sdk-go/aws/request\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/ec2\"\n\n)\n// Usage: go run main.go \nfunc main() {\n    sess := session.New()\n    svc := ec2.New(sess)\n// Custom handler to correct InvalidInstanceID error code\n// code missmatch found in, https://github.com/aws/aws-sdk-go/issues/599\nsvc.Handlers.UnmarshalError.PushBack(func(r *request.Request) {\n    if r.Operation.Name != \"DescribeInstances\" || r.Error == nil {\n        return\n    }\n    if ae, ok := r.Error.(awserr.Error); ok {\n        if ae.Code() == \"InvalidInstanceID.NotFound\" {\n            r.Error = awserr.New(\"InvalidInstanceIDNotFound\",\n                ae.Message(),\n                ae.OrigErr(),\n            )\n        }\n    }\n})\n\nres, err := svc.DescribeInstances(&ec2.DescribeInstancesInput{\n    InstanceIds: []*string{&os.Args[1]},\n})\nfmt.Println(\"describe instances:\", res, err)\n\nerr = svc.WaitUntilInstanceExists(&ec2.DescribeInstancesInput{\n    InstanceIds: []*string{&os.Args[1]},\n})\nfmt.Println(\"waited for instances:\", err)\n\n}\n``\n. HI @flawedmatrix, @calebamiles thanks for letting us know about this issue. I've just pushed a fix in PR #606 which fixes these waiters. You should be good to use the waiter without issue now.  Let us know if you have any additional questions, or feedback.\n. Thanks for the update @weidewang , glad to know this is working correctly for you now.  Let us know if you have any additional issues, or feedback.\n.DOCSmight be a little too geneeic. It could be  more specific like ,AWS_DOC_GEN_TOOL, or similar. \n. LGTM \n. Change looks good, it would be nice to have a test for this in therequestpackage.\n. lgtm\n. Hi @qhenkart thanks for submitting this PR and correcting the example code. The change looks good. going to go ahead and pull this in.\n. Thanks for reporting this issue @freshvolk I created PR #613 which should resolve this issue. Also thanks for the detailed wire log.\n. Hi @qhenkart thanks for taking the time to investigate this feature and put the PR together. We'd definitely like to have the ability to sign cookies for CloudFront. I'll review the change and get back to you any updates needed.\n. Thanks for the updates @qhenkart will review your changes and get back to you.\n. Thanks a lot for the updates @qhenkart I've taken your changes and made a few minor changes, and merged them in  #626.  It takes your idea of putting theCookieOptionson theCookieSigner, but also allows overriding the options for theSign,SignWithPolicy` methods. So best of both worlds. \nIt also adds a example to the /example/signCloudFrontCookies folder.\n. I've merged your changes in #626. Let us know if you have any feedback, or additional questions. Again thanks for taking the time to get this feature implemented.\n. @charles-at-linknext In addition, what version of Go are you using? The SDK's minimum supported version is Go 1.5.\nSince golang.org/x/tools/go/types no longer supports Go 1.4 using go get github.com/aws/aws-sdk-go/... will fail. To work around this and still stay on Go 1.4 you can split retrieving the dependancies from the SDK it self.\ngo\ngo get -d github.com/aw/aws-sdk-go\ngo get -u github.com/aws/aws-sdk-go/aws/...\ngo get -u github.com/aws/aws-sdk-go/service/...\nThis will retrieve the SDKs runtime dependancies, skipping all those that are involved in tests, or deprecated tools. .\nIf you're using Go 1.5 having the GO15VENDOREXPERIMENT=1 environment variable set will ensure the vendored dependancies of the SDK are available frozen and known good versions. Go 1.6 will do this automatically without the need for the GO15VENDOREXPERIMENT environment variable.\n. is the html file just a place holder? if so a .gitkeep file will work better\n. Thanks for the request @viktorbenei, Content-Length is originally not included since an Content length of 0 may be replaced with an empty string, breaking the request's signature. \nWhich API operation are you pre-signing where not being able to enforce Content-Length is causing issues? \n. Thanks for the update. I think we could enable Content-Length being included in the signature if the length is not 0.\nThough this might still be problematic in your use case, since users could send a Content-Length of 0 and the SDK would skipping signing the Content-Length. A minor issue, but would need to be handled outside of the SDK prior to requesting the pre-signed URL.\n. After a bit of investigating we might be able to remove Content-Length from the black list all together. I'm investigating this further to see how possible this is.\n. The change I made is not complete and needed to be reverted because a empty content-length was being included for some requests which is invalid.  \nReopening this to track a future change.\n. So it turns out that Content-Lenght:0 is replaced by AWS's front end with Content-Length:. We'll need to consider an alternative path for restricting Content-Length.\n. Hi @viktorbenei I've pushed a fix which correctly allows the content-length to be included in the signature if its a valid value greater than 0. The Go http client will automatically strip out the content length header if the value is 0 or less. This change works with that by ensuring the signature generated will not include the content-length if it will be stripped by the http client.\n. Hi @dyhuan123 thanks for contacting us. The SQS service client are safe to use concurrently and do not have any interdependencies that would cause threading issues. You shouldn't experience any problems by using the same svc instance for all your goroutines.\nHave you tried to run your application with go build -[race](http://blog.golang.org/race-detector) flag? This would help shed light on where race conditions may be occurring.\nThe SignatureDoesNotMatch error I think might imply a different issue. Does your system experience time skew where the system time might not match wall clock time?\n. Correct, the request the SDK sends to the SQS service includes a date that the request was signed for. The request is only valid for a short time, so if your application's local time is significantly different than what the SQS service expect the signature would be invalid.  Generally though i think this is more of a expired signature error.\nIn the error message you get does the message include any additional information?  You can also turn on  debugging to receive more information. In addition are you able to reproduce this with a single goroutine?\n``` go\nsess := session.New(aws.NewConfig().WithLogLevel(aws.LogDebugWithHTTPBody))\nsvc := sqs.New(sess)\n//... use the SQS session instance in your code.\n``\n. I cannot see any reason those two lines of code being separated would cause any issue. Unless the params value is being used by multiple goroutines, or being reused. Is it possible that the params value is being reused, or modified once it is created?\n. Thanks for the update @dyhuan123 This is very helpful.  I've identified the case where theAuthorizationheader is being included with the signature on retries. It occurs in an small retry case where the credentials are expired. I'll update once I have a fix out which corrects this and the tests.\n. @dyhuan123 thanks for the update. I think returningtruefor everIsExpired()` call might be a bit excessive. If you have a code sample I'd be glad to take a look at it and give more specific suggestions.\nA credentials.Credentials will only request the credential.Provider's to get credentials if the provider's IsExpired() method returns true.  By making IsExpired always returning true, it increases the work a little that needs to be done because the Credentials will need to call Provider.Retrieve in addition to Provider.IsExpired for every API request. This also means an additional mutex.Lock call.\ncredentials.Credentials does guaranteed that the credentials.Provider's Retrieve and IsExpired method will be called safely so you shouldn't need additional mutex within Provider for most cases.\nI suggest that any determination of credentials logic being done in IsExpired, and Retrieve only does retrieval an updating when the credentials should be expired.\n. Glad to help, let us know if you have any other questions, or you have questions about the implementation.\n. Hi @skyleelove, which example are you trying to run? \nCheck out our wiki on getting started, and configuration. It should point you in the right directly . Let us know if there is anything confusing or we can help with.\n. Thanks for the update.\nThe example needs your AWS account credentials to run.  This can be done via environment variables, shared credentials file, other methods.\nFor example to do this with environment variables you can do the following on linxu/osx. What platform are you using?:\nsh\nAWS_ACCESS_KEY_ID=YOUR_AKID AWS_SECRET_ACCESS_KEY=YOUR_SECRET_KEY go run main.go\nWhere main.go is the example saved to main.go file.\n. Great glad you have the credentials working.  Do you have a code example using S3 that you can paste/link? This would be very helpful in identifying the issue you are experiencing.\n. Hi @skyleelove I'll go ahead an close this issue since you're able to get the credentials to work now. We can continue the discussion on using S3 in #630 that you opened.\n. Looks good, minor suggest for early exit. What do you think about adding a test case for this concept?\n. Hi @skyleelove I suggest using the Page functions instead of implementing the pagination your self.  The page function perform the pagination for you, and simplify the process. Taking out the need to use Marker out completely.\nThe following example includes all object keys in a bucket.\n``` go\npackage main\nimport (\n    \"fmt\"\n    \"os\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\n)\n// Lists all objects in a bucket using pagination\n//\n// Usage:\n// go run main.go \nfunc main() {\n    sess := session.New()\nsvc := s3.New(sess)\n\ni := 0\nerr := svc.ListObjectsPages(&s3.ListObjectsInput{\n    Bucket: &os.Args[1],\n}, func(p *s3.ListObjectsOutput, last bool) (shouldContinue bool) {\n    fmt.Println(\"Page,\", i)\n\n    for _, obj := range p.Contents {\n        fmt.Println(\"Object:\", *obj.Key)\n    }\n    return true\n})\nif err != nil {\n    fmt.Println(\"failed to list objects\", err)\n    return\n}\n\n}\n```\nExample output\nPage, 0\nObject: myKey\nObject: mykey.txt\nObject: resources/0001/item-01\nObject: resources/0001/item-02\nObject: resources/0001/item-03\nObject: resources/0002/item-01\nObject: resources/0002/item-02\nObject: resources/0002/item-03\nObject: resources/0002/item-04\nObject: resources/0002/item-05\n. You can find more information about this and other methods in our API Reference.  In addition we have a more detailed example for listing objects in a bucket concurrently in our example folder.\n. Let us know if you have any additional questions.\n. Thanks for the update @skyleelove, let us know if you have any additional questions.\n. Thanks for contacting us. I'd like to learn more about the  issues you are seeing with the SDK vendoring its dependancies?\nIf there were a generic pattern the SDK could follow supported by the base Go tooling it would be great to investigate how the SDK could use this. Regrettably there is no champion pattern accepted by the community or Go tooling. This is why the SDK uses vendoring to ensure the upstream dependancies needed by the SDK will not cause users to break if the upstream dependancies makes a breaking change. Which has happened.\n. Thanks for your feedback, and suggestion of tools. We're taking a look at the options, but currently we don't think there is a good solution thats widely accepted by the Go community. With multiple competing solutions like Glide, Godep, and multiple vendor spec formats, we think its too soon to choose a specific format and be stuck with it. The /vendor folder is supported by all Go 1.5 with 1.5 vendoring experiment and +1.6 out of the box.\nThe downside of go get -u github.com/aws/aws-sdk-go/... is that users would be downloading several more dependancies than those needed to build applications against the SDK. Specifically, the testing libraries used by the SDK. Along with the golang.org/x/tools dependancies used by the deprecated awsmigration tool.\nInstead of requiring users of the SDK to use a non-standard tool, we'd prefer to provide the most generic support possible, while also providing dependency versioning locking. With the vendoring folder we can guarantee that the SDK will work out of the box when downloaded without worrying about vendoring conflicts.\nIn addition, the SDK's README.md can use some updates to include more detailed information how retrieve the SDK, and what the different methods of retrieval will entail.\n. @e-dard I verified that retrieving the SDK with go get -u github.com/aws/aws-sdk-go/... will still download some of the testing libraries because they are imported from utility packages within the SDK's repository. Specifically the awstesting package.\nWith that said the simplest way to retrieve the SDK with only decencies needed for runtime is: go get -u github.com/aws/aws-sdk-go/service/...\n. Thanks for the feedback, I've updated the README.md to be clearer on what to expect when retrieving the SDK.\n. I created a PR #1544 which adds the dep metadata files to the SDK. This will allow applications to correctly manage the SDK's dependencies in the context of the application.\nI do not think we can safely remove the vendor folder from the SDK without breaking users that do not vendor, and not using dep. In a future version of the SDK a vendor folder would not be needed.. @roytanmoy Are you using the latest version of the SDK?  v1.16.2 updated the SDK's dependency on go-jmespath to be the latest version.. if the request terminates because Cancel is closed, the SDK probably shouldn't retry the request.\n. Looks good\n. Looks good\n. go vet changed the -example flag to -test in go tip (1.7+). https://github.com/golang/go/commit/6e6637bdb478e98d32dd10659ea1975a00aeda0a\n-example was added to go1.6 in. https://github.com/golang/go/commit/7454f53604b9953b5b7a3897c6b855957f911e66\n. Hi @stack72 we are working with the CloudWatch Logs team to get a better understanding of what the expected functionality of this issue is. We'll update this issue once we have more information from the service team. \n. looks good with the comment above\n. looks good\n. Minor comment about Input vs Type, but other than that change looks good.\n. Thanks for contacting us @gwatts and submitting the example code. I'm digging into this issue. I'll get back to you with an update once I've figured out how the SDK should be handling these key values.\nThe encoding/json will encode simple\\x00withnull as simple\\u0000withnull it seems the SDK should do something similar and use UTF8 encoding instead of hex encoding. %q isn't really a good formatter for json make keys since its too generic and escapes as ASCII not UTF8. encoding/json's formatting of these strings is much more robust, https://golang.org/src/encoding/json/encode.go#L787\n. The following shows how JSON encoding will encode the null character.\ngo\n    b, _ := json.Marshal(\"simple\\x00withnull\")\n    fmt.Println(string(b))\n    // Output:\n    // \"simple\\u0000withnull\"\n. Thanks for the update @gwatts, glad that the change is working for you. Let us know if you have other issues, or questions.\n. Thanks for the heads up @mweagle, it looks like something change with the way the docs are generated causing the way the types are organized to change. \n. Hi @mweagle thanks for reporting the issue. I've updated our docs to be generated how they were before. All types for service clients are now listed correctly under their associated packages. On the type's detail page.\n. Thanks for contacting us @gaffo. We would really like to add this feature, and have an issue in our backlog #555 that covers this idea. I'm going to close this isssue as a dup of #555 and we can continue discussion there.\nWe're also always glad to review PR if anyone is interested in contributing.\n. thanks, fixed that :)\n. Looks good with the change in comment I added.\n. Hi @dlsniper thanks for contacting us.  The naming and fields for the services and their API operation input/outputs are provided to us by the service teams. We closing work with them to help ensure more consistent naming across the services, and within a service's API.\nIf you have some cases i can forward on to the service team as feedback on naming in the future I'd be glad to do so.\n. Thanks for the breakdown @dlsniper. This is very helpful, and I'll forward this information along to the service so that in the future we can help improve the consistency of the AWS services' API.\nPlease let us know if you have any additional feedback, questions, or issues.\n. There are a few significant reasons the SDK is unable to take a change which renames these method names and input/output fields.\nFirstly to change the operation names and field names of service client's API would introduce breaking changes to the SDK to all users. We do not want to introduce any breaking changes into the SDK within a major version. \nSecondly the field names and operation names are provided to the SDK from model files defined by the service teams. We use these model files to generate the service client methods and types you see under the service package.\nLastly the SDK relies on the field names of types to know what to (un)marshal it as in the API operation call to and from the service. If these field names were changed additional mapping is needed to lookup the name to be used in the request/response sent across the wire. In some cases this is already done, but generally only for special cases defined by the service team in the model files.\nLet us know if there is anymore information i can provide that will help clear this issue up. \n. Thanks for creating a PR for this @gaffo. One reason we haven't moved the signer out of private yet is because its interface is very SDK centric. Specifically it is pretty difficult to use a signer today on a generic net/http.Request. To support this the signers need a little refactoring to decouple the aws/request.Request from the net/http.Request part of signing.\nThis refactor work should expose a generic signer operating on the low level net/http.Request, and the SDK would include a wrapper on top of the generic request signer.\nWe haven't moved the signer's out of private yet because of this, and this work may introduce breaking changes to the signer's package. Which may occur if we move signer out of private prior to creating the generic signer.\n. Yeah this is what we're looking for in this functionality.\nI think something like the following can be used that would provide generic functionality and work with the SDK's needs for a signer.\n``` go\npackage v4 // aws/signer/v4/v4.go\n// ...\ntype Signer Struct{\n    Creds *credentials.Credentials\n    // Config options\n}\nfunc (s Signer) Sign(r *http.Request) error {\n    // ...\n}\n```\n`` go\n// This could also be in theaws/signer/v4 package as well\npackage corehandlers // aws/corehandlers/signers.go\n// ...\nvar V4SignHandler = request.NamedHandler{ \n    Name: \"core.V4SignHandler\",\n    Fn: func(r *request.Request) {\n        s := v4.Signer{\n            Creds: r.Config.Credentials,\n            // Config options\n        }\n    s.Sign(r.HTTPRequest)\n},\n\n}\n``\n. Hi @nicolai86 thanks for taking the time to create your branch. Would you mind creating a separate PR for it?\n. I'm going to close this PR @gaffo for now. Please reopen if you have any updates.\n. Thanks for contacting us @bkeroackdsc. Did you receive any error or log describing the error that you received when running the instance?  I of would expect self base64 encoding the file would of worked.\n. Thanks for the update, this makes sense.  For this value we wouldn't be able to change the type from*stringto[]byte` because it would be a breaking change.  With that said It would be interesting to have a functionality that would automatically load a file's content and populate it into an input field. \n. Thanks for contacting us @bkeroackdsc. Since the SDK does support binary data from this field I'm going to go ahead and close this issue. Please let us know if you have any other questions, issues or feedback.\n. Hi @dlsniper Reviewing the API I'm not seeing the ability to retrieve when the stream was created. I'll reach out to the Kinesis service team. In addition I suggest also asking in the Kinesis forums as others may of seen your issue and have a quicker answer for you.\n. Thanks for the update @dlsniper, I'll reach out to the Kinesis team for information on the issue you're seeing.\n. Hi @dlsniper the Kinesis team got back to us. The feature you're looking for isn't supported by Kinesis. Though with that said, they did let us know that similar functionality can be obtained using CloudTrail if enabled.\nIf enabled, CloudTrail will log CreateStream, DeleteStream, SplitShards and MergeShards operations, and you can view the timestamp in the CloudTrain log.\n. Hi @ncw thanks for contacting us and submitting this PR. I don't see anything wrong with doing this validation on the dereference. In this case I'm curious if the the SDK should of return an error instead of an empty string. Because Uploading without a valid Location currently would be invalid, and hide any upstream errors.\n. Thanks for the feedback, makes sense. We'll leave it as empty string.  Thanks for submitting the PR, the change looks good. \n. Thanks for contacting us @eriklott and creating this feature request. We'd be glad to take a look at adding this feature into the SDK. If you're interested we're always eager to review PRs and discuss implementation ideas.\nInitially it looks like we should limit this feature to S3 and create it as a customization for the S3 service client similar to the SDK's s3manager's Uploader and Downloader. \n. Thanks for contacting us @insasho with this feature request. I think adding the ability to stream paginations would be helpful to have.\nI think the ideas you proposed in 1,2 would most likely flow pretty well with how the SDK would implement them. 4 takes advantage of a DSL, but like you mentioned its string based, which could cause a significant lose of type safety the other two methods would preserve. For idea 1 we'd need to figure out how to ensure ensure goroutines and channels would not be easily leaked. Because of that option 2 might be the strongest. Also thanks for the iterators link, I'll read that over\nIdea 3 could actually build separate on top of 1 or 2 fairly straightforwardly I think. Providing both a streamer and an optional filter functionalities. \nWe're also always  glad to review PRs and discuss designs for how this could be implemented. \n. Thanks for information @insasho, and looking into this further. The AWS SDK for Go will use the paginators-1.json file to determine how to generate paginators for their associated service client APIs. Currently the SDK does not use the result_key but may need to if streaming pagination where to be used. Only the AWS CLI and AWS SDK for Python currently use this field. result_key is complicated because it is not required to be a single field of the response. There are a few conditions where APIs' paginators have multiple fields are grouped or merged together. In the SDK this may require generating custom shapes for these multi field result_key.\nI think streamed paginators in the SDK probably would like very similar to what you described.\nFor the results_key fields, correct the field is the name in the context of the output type of the API operation. For input/output tokens the SDK uses a bit of reflection to search for this field when processing  if the request should be paged again. To do this for generated code with the results_key the SDK would have to search the associated API's output type for the fields. This is generally available via a ShapeRef's Shape.MemberRefs field, which enumerates all fields of a struct. Context is a good pattern to follow, but the actual net.Context won't be introduced until 1.7 and if the SDK were to implement streams we'd want it to be compatible with at least 1.5 and 1.6 also.\nThere are several APIs where streams make a lot of sense, like the ones you mentioned, and others like dynamodb queries, and listing S3 objects. \n. Hi In PR #1132 I merged a change that adds a request.Pagination type to the SDK. This type allows you to control pagination much more easily your self in a pattern similar to bufio.Scanner. . Thanks for the detailed information @janeczku , and dive into correcting the retry backoff. I think this makes a lot of sense, and will be a great improvement to the SDK's retry logic.  I've taken a look at your PR and the general idea looks good with a few minor comments.\n. Merged in #670, Fixing this bug. \n. Thanks for the PR @janeczku, and associated in depth information about the lack of proper randomality in the retry logic.\nI like the idea of updating the object used by the default retryer to have a better seed. Though I don't think the SDK should modify the global rand seed, because this will impact any consumer of the SDK changing their rand calls in unexpected ways.\nI suggest instead of mutating the global rand I suggest create a instance of rand.Random that the SDK's default retryer will use and initialize that to a proper seed. The Go stdlib doesn't export a concurrency safe rand.Source so we'll need to use our own to prevent race conditions.\n. Actually this is exactly how you implemented the change, thanks!  That's what I get for trying to review code my phone.\n. This change looks good. Thanks for the detailed investigation, and taking the time to submit this PR.  Going to pull this in.\n. Looks good :)\n. Hi @simonwistow we identified what needs to happen in order to support the X-Amz-Server-Side-Encryption-Context header value. Specifically the SDK needs to support blob (base64 encoded []byte) header values. Customization needs to be added to the SDK to allow this feature.\nDue to the customization work needed to the SDK, we won't be able to implement the field directly in the service model without the blob header support. Because of this, I'm going to close this PR, an create a new feature request Issue that will track the discussion, and scheduling of adding this feature.\n. #750 tracks the work of adding support for the blob type. This work is in our backlog, but we are always glad to review PRs.\n. I think this might be related to #466. It looks like if the object's part were retried it might correct the issue. I think this is something we can improve to retry failed copies between the part and writer at.\n. HI @eldondevcg @mwhooker I'm going to close this issue in preference of #466. We can continue the conversation on supporting retries on failed read's post HTTP request being made. \nWe're also always glad to review PRs.\n. Hi @muratsplat where you able to reproduce the issue? If so having a code example would be very helpful in reproducing this issue.\n. Hi @muratsplat let us know of you're able to reproduce this issue, and please reopen this issue if so. \n. Thanks for letting us know about this issue @eldondevcg What version of Go are you using, and which version of the SDK?  This will help us reproduce the issue. In addition, what size is the object that the code above is downloading?\n. I haven't been able to reproduce the issue you're seeing @eldondevcg, but looking through the code i see a possibility that the aws/request/http_request.go's copy of the http request's header could potentially cause a race condition because it copies the Header value. I'm investigating this further, but I think we can do a deep copy here of the header values.\n. Hi @eldondevcg I've identified a condition that could cause the race you identified. I've pushed a fix which addresses this issue. Let us know if you've anymore question, or feedback. \n. Thanks for submitting the PR @GregorioDiStefano, In general the change looks good. \nI am curious why you chose to use a closure for isASCII := func() bool instead of a typed function. Why was this done?\n. Thanks for making the update @GregorioDiStefano. i was taking a deeper look at the CloudFront signer and I'm curious if we should put this logic of ascii into Policy.Validate This would ensure the isASCII validation is done from both URL and Cookie Signing.  This would be on the Policy.Statements[n].Resource field.\nin addition I think it would be helpful to expand on the reason for the error, something like:\ngo\n\"unable to sign resource, [%s]. Resources must only contain ascii characters. Hostnames with unicode should be encoded as Punycode, (e.g. golang.org/x/net/idna), and URL unicode path/query characters should be escaped.\"\n. Thanks for the updates @GregorioDiStefano.  The best way i found to test the cookies is to create the signed cookies and add them to an http.Request's CookieJar. then use that request to make requests to cloud front for the resource the cookies were created for. \nsignCookies example is a simplified version of what I used when testing the sign cookie functionality.\n. The change looks good, and I verified the cookies are not impacted negativity with this change using the above example code I linked. \n. Hi @sethjback thanks for contacting us. Setting NULL for empty maps/slices, and nil pointers was intentionally to follow the existing pattern of the ConvertTo functions.  Because of this we also added the ability to omitempty fields.  This setting can be applied to individual struct fields via struct tag omitempty. This won't encode empty maps/slices but prevents the fields being encoded at all.\nWith that said, I can see the reason for wanting the ability to encode empty maps and slices as {} and [] respectivily. I think we should be able to add this to the encoder's configuration options, and possibly as a struct tag for maps and slices\n. A possible workaround for this until the functionality is implemented would be to implement the dynamodbattributes.Marshaler interface on your map/list type that could be empty. The returned dynamodb.AttributeValue would be an empty list value.\nThis item is in our backlog and haven't begun work to address it yet. with that said we're more than glad to review community PRs, and help with any contributions people would like to make.\nIn this case I could see this be implemented as another struct tag. Maybe something like zeroempty. When a slice or map field is found to be empty and has that tag an empty dynamodb.AttributeValue for Map or List would be set as needed.  It would be nice if we could provide this functionality in more configurable way, but I'm not positive how that would look.\n. Agreed, i think it makes sense to include an optional flag in the MarshalOptions also for this functionality. Not sure yet if two flags or a single would be better. Two may make more sense to follow a pattern of NullEmptyStrings\n. Hi @marcato15 this feature is still in feature request phase. If you're looking to create a PR we're glad to review it. A new flag option in MarshalOptions is probably the best way to make this an opt in feature.. Thanks for the update. in this context I think you are correct the best way to solve the non-boolean path is with custom marshalers.\nOne potential alternative is to add a new struct tags that instructs the marshaler to render the field as an empty value. Kind of the opposite to omitempty for maps and slices. Maybe something like includeempty  the naming seems odd though since \"include\" is the default functionality.  . Looks good, thanks for submitting the PR @astropuffin, and adding the increment.\n. Hi @RazzyP thanks for contacting us and creating this PR. I'd like to know more about the bug you encountered also. Do you have an example code that reproduces the issue you're seeing? Also what API operation(s) where you using when you encountered this issue?\n. Thanks for the update @RazzyP I think I understand the issue better. It looks like the problem is the int64 value should be of time in milliseconds. Time.UnixNano() returns time in nanoseconds, and Time.Unix() returns it in seconds.\nSince time.Time doesn't have a built in utility method for milliseconds, I don't see a problem with the SDK providing a utility for this. This utility probably should be pretty generic and just take a time.Time value and return a int64 for the time in milliseconds. Something like the following:\ngo\nfunc TimeUnixMilli(t time.Time) int64 {\n    return t.UnixNano() / int64(time.Millisecond / time.Nanosecond)\n}\nThis would allow custom time.Time values to be provided other than Now() and would return the time value in milliseconds.\n. Updates are more than fine, we can squash the multiple commits without a problem.\n. Thanks for taking the time to add this PR @RazzyP I've pulled it in, adding a little bit of documentation.\n. LGTM\n. Hi @Kedarnag13, are you looking to use the AWS SDK for Go AWS IoT service client to send publish and subscribe messages to IoT or are you looking to create your own client using MQTT?\nThe SDK provides the API operations needed to send publish and subscribe requests to IoT using the github.com/aws/aws-sdk-go/service/iotdataplane API operations.\nThe SDK retrieves its endpoint from the iot.DescribeEndpoint API call of the IoT control plane API. This API will return a unique endpoint for the AWS account making the API call.\n. Thanks for the update @Kedarnag13, let us know if you have any additional questions, or feedback.\n. Hi @samber We've reached out to the ElasticCache service team, to clarify the documentation for this request field. When we get more information we'll update you here.\n. Hi @samber I got in touch with the service team and they clarified that while the field is a list, only a single element is allowed. As @xibz mentions this allows for future growth and expansion of the API and single cluster nodes.\nSince this is intended functionality I'm going to go ahead and close this issue. Please open if you have additional questions, or feedback.\n. Thanks for contacting us @radeksimko I'll work with the service team to get this update included in the SDK's RDS model definitions.\n. Hi @radeksimko I've updated the SDK model for RDS in our latest release v1.1.30. It now includes the ModifyDBClusterSnapshotAttribute API.\nLet us know if you have any feedback, comments, or questions.\n. Hi @sanathp thanks for contacting us. You are correct the SDK does not currently provide an utility method to unmarshal a slice of map[string]*dynamodb.AttributeValue. The best way to unmarshal this is to iterate through the returned list calling dynamodbattributevalue.UnmarshalMap() for each element in the slice.\n. Marking this as a Feature Request to make receiving items from query API operations easier to unmarshal into Go value types. We're also always glad to review PR if you're looking to contribute this feature to the SDK.\n. Thanks for the feedback @twoism. I'll forward your feedback along to the DynamoDB team to help improve the API. We wouldn't be able to change the API due to the breaking change without the service doing a major version bump.\nTo resolve this feature request I think would solve it similar to the example you posted, same for encoding. The SDK already does this to a more limited degree for marshal of list and maps. So updating to unmarshal a list of maps is pretty straightforward, but like you mentioned has more complexity.\nI'm not sure yet how the marshaler side of this would work yet though. Not sure its even needed though, but may make sense for parity.\n. Thanks for bringing up this issue. Change #897 Fixes this issue by adding a UnmarshalListOfMaps utility to the dynamodbattribute package. This will make using Scan and Query easier since you'll be able to unmarshal Items now without extra logic.\n. Thanks @nicolai86 I'll review and get back to you with feedback.\n. HI @nicolai86 I think we'll want to completely remove the private/signer/v4 package, but I'm ok with this PR not doing that yet since the service client files will add a lot of noise. We can make a second pass removing the private package once we have the v4 signer public.\n. Since Signer would be published we probably want to take a different look at the options provided. Right now the struct is a grab bag of options for different use cases. There are two specific use cases for using the signer normal request, and pre-signed URL.\nI think it might make sense to split apart presigning and request signing into two different methods. This would make it more explicit what the intended action is instead of relying on Signer.ExpireTime as a flag if the signer should presigning a URL. This would simplify some of the fields of Signer too since, many are only there because request.Request wrapper.\nMaybe something like:\n``` go\nfunc New(creds credentials.Credentials) Signer {}\nfunc (s Signer) Sign(r http.Request, service, region string) (http.Header, error) {\n    return signedHeaders, nil\n}\nfunc (s Signer) Presign(r http.Request, service, region string, exp time.Duration) (string, http.Header, error) {\n    return presignedURL, signedHeaders, nil\n}\n```\nIf we were to use this form we'd want to come up with a way to pass in optional configuration values. Generally we've been using functional options for this idea in other parts of the SDK.\n. Thanks a lot for taking the time to update this PR, and the continued discussion. We'll get back to you with feedback soon.\n. Hi @n054 would you mind submitting this change without updating the service clients. Basically leaving the private/signer/v4 like in your PR was originally. This will make our review process simpler, reducing noise.\n. Thanks for updating the PR @nicolai86 , taking a look at it and will post feedback soon.\n. Thanks a lot for your updates @nicolai86. I'd really like to find a way to not have presignWithBody and signWithBody duplicate so much code.\nIn addition the ioutil.ReadAll in Sign and PreSign might be problematic especially for services like S3 where large objects could be sent. The full size of the object would need to be loaded into memory, potentially exhausting the available memory. I'd like to find another way of doing this so the content doesn't have to all be loaded into memory. A possibility is to export the SignWithBody method so that the body can easily be passed in as an io.ReadSeeker. There probably is another way we can represent this idea cleaner. Maybe SignWithBody would set the Body to the http.Request.Body.\n. Thanks updating the PR @nicolai86, and making these changes. One idea we could investigate if it makes sense to create an additional Sign method that takes a r *http.Request, sha256 string, and contentLength int. Without a body parameter. If the sha256 and content length is provided the signer will have no reason to read the body.\nI think this will allow us to provide signing for both a io.ReadSeeker body and a io.Reader body. This additional method would allow you to precompute the request body's sha hash and not have the signer re-read the body's content into memory just to generate the request's signature.\nTo simplify the implementation the Sign method that takes a io.ReadSeeker could compute the sha256 and content length from the passed in body, and call out to our new nobody Sign method passing in the sha256 and content length parameters. The nobody Sign would then be the primary entry point into request signing. In addition if the http.Request already has the Content-SHA256 header it should pass that value on instead of performing the hash.\n. Hi @nicolai86 the rest of the change looks really good. I think the only outstanding issue is to add a Sign without body method. I can add this, prior to merging in the change too if that is helpful.\n. Sure if you'd like to do a squash that would be great, thanks!\n. Thanks a lot for all of your work getting this PR together. I've created feature/publishV4Signer branch for this change with a few updates. Once the travis tests pass I'll merge it in and mark this PR as closed. \nWe ended up not adding the additional Sign without body methods I noted earlier. There didn't seem to be a great need to add it yet. But we can always add them in later if the community would find them useful.\n. Going to close this PR, and will merge in this code change in with #735 which includes your changes and a few doc updates along with regenerating the service client code.\n. LGTM\n. Looks good thanks for correcting this.\n. Thanks for submitting this pr @rjocoleman the change looks good.\n. LGTM\n. Hi @bfosberry I've made a few minor updates to your PR and pushed it up as a new PR #742. Thanks for pointing out this issue, and taking the time to create the PR. \n. Hi @scohen28 I'm having difficulty reproducing the error your seeing. What version of Go  are you using, and do you see this issue if you do the go get from a clean (empty) GOPATH? \nIn attempting to reproduce this I created a new go workspace, and retrieved the SDK, but did not experience any problem. I ran this test with both Go 1.6 and 1.7beta1\nsh\ncd /tmp\nmkdir golang\nGOPATH=`pwd` go get -u github.com/aws/aws-sdk-go\nTo start knowing the Go version you're using would be helpful. In addition were there any customizations made to your version of the Go stdlib? Also does your GOPATH env variable include multiple go workspaces?\n. Great, glad that worked. Let us know if you run into any addition issues.\nSometimes with older Go version's i've seen the artifacts in the $GOPATH/pkg folder not get rebuilt properly when updating versions of Go.\n. Going to go ahead and close this issue, please re-open if you are still experiencing this issue.\n. Hi @vgarg, Correct the AWS SDK for Go does not include support for ES data plane APIs. Using the ElasticSearch provided SDKs is the preferred method for communicating with the service's data plane APIs.\nI agree it looks like none of the community provided Elastic Search SDKs for Go also support signed requests. I'm not seeing an easy way to add signed requests on top of those SDKs either. The AWS SDK for Go has #555 open which is looking at exposing the signer it uses for requests. Once this is implemented you would be able to use the AWS SDK for Go to sign your ElasticSearch request if you had a utility which built and exposed the http.Request instead of just sending the request.\n. Tracking that issue should help you work with ElasticSearch clusters.\nI'm going to close this issue now, and suggest tracking #555. Let us know if you have any additional questions, comments, or feedback.\n. HI @cvanderschuere, Thanks for letting us know about this issue. I'm going to try to reproduce this locally. Initially I think we can improve this performance by removing the string concats.\nWhat version of Go, and the SDK where you using when you saw this issue?  I'm also curious which API operation fields you were using. The stripExcessSpaces should of only been used with headers, and i wouldn't of expected there to be enough headers where it posed an issue.  Does your query request include many headers?  You can log the full wire log of your request by including the aws.LogDebugWithHTTPBody Log level in your configuration.\n``` go\nsess := sesison.New(aws.NewConfig().WithLogLevel(aws.LogDebugWithHTTPBody))\nsvc := dynamodb.New(sess)\nres, err := svc.Query(...)\n```\n. Thanks for the update @cvanderschuere, this will help us investigate the issue. I think  we can improve the performance of this utility by just removing the string concat and use a byte buffer instead. This by it self should make a significant boost.\n. Thanks for bringing this issue up @cvanderschuere this has been fixed on tip, and will be included in our tagged release. Let us know if you encounter additional issues, or have any feedback.\n. Hi @captncraig, thanks for contacting us. I'm investigating this issue. I think the SDK could provide a utility that would unescaping for record set name. I'm not sure if we could do this by default without potentially introduce breaking changes.\n. @captncraig in reviewing this issue I don't think the SDK would be able to make a change to how it handles the response value. A utility would be the best bet to add this functionality. We'd be glad to review a PR if your looking to add this.. LGTM\n. In addition @rasky, #108 tracks adding constants for each region generically in the SDK. Some of the other AWS SDK's have migrated to a new method of modeling which regions services are available in, which allow the AWS SDKs to use a comprehensive model of the AWS regions and endpoints. AWS SDK for Java was the first SDK to ingest the enhanced regions model.\nWe'd like to implement support for this model in the AWS SDK for Go and the work is in our backlog.\n. @matthew-andrews I've been trying to reproduce this issue. I'm only able to encounter the AccessDesigned error when I don't have access to the bucket. For example I try to head your s3up-test bucket I get the error regardless of which region I make the request from.  When I do a GetBucketLocation on a bucket I have permission to access I can call it the API successfully from any region.  Do you have any policies setup on this bucket restricting the the region a request can be made from?\n. Hi @matthew-andrews are you still encountering this issue? It may be helpful to ask on the S3 AWS forums.  Other users may of experienced the issue you are encountering, and will help get more visibility with your issue with S3.\n. Hi @matthew-andrews I think this is a S3 service API issue, not SDK. This is why I was suggesting the S3 forums.\nGetBucketLocation requires the requester to be the owner of the bucket.\nThe Writing IAM Policies: How to Grant Access to an Amazon S3 Bucket blog post goes through the process needed to grant permission to a bucket, including permission to get a bucket's location.\n. I think that its a possibility, especially since you're able to get the bucket in one region but not another. I'm thinking that it is possible that there is a policy on the bucket that is limiting permissions to a specific region.\n. Hi @matthew-andrews any luck identifying if this issue was caused by a policy rule?\n. Hi @matthew-andrews We's escalated this issue to the S3 Service team, and they are currently investigating. We'll update once we have more information.\n. Hi @matthew-andrews The S3 team got back with me and suggested the best API to use is HEAD bucket. GetBucketLocation uses a more complex permissions model where HeadBucket can be called by anyone. The bucket's region will be returned as a X-Amz-Bucket-Region header.\nThe HEAD bucket request can be made regardless of request signing, so anonymous (unsigned) requests can be made requesting a bucket's region.\nI'm going to close this issue with this info. Please let us know if you have additional feedback, or issues.\n. Thanks for the feedback @tve. XML marshaler definitely has a lot of room for improvement. Ultimately we'll looking for #377 to fix the memory issues for all of the protocols with a goal of replacing the duplication and reflection with code generation. Doing this work is our current priority now.. Marking this as a duplicate of #377. Thanks for contacting us @maxekman. The SDK receives the definition of the dynamodb.AttributeValue type from the DynamoDB model, and uses that definition to generate the service client's API operation methods, and associated Input/Output types. In the case of DynamoDB Streams the SDK does have customization in place to share dynamodb.AttributeValue definition between the two service client's API operation methods.\nWith that said though it would be a breaking change to the SDK to split out the dynamodb.AttributeValue from the dynamodb package. \nOne idea to help reduce the dependency is to have a DynamoDb specific Marshaler type that wraps your generic type. When the application goes to (un)marshal to DynamoDB the generic type could be wrapped with a dynamodbattribute.Marshaler/Unmarshaler. This would remove the dynamodb.AttributeValue dependency propagating throughout the rest of the code base the generic type is used. This tactic would easily apply to a top level type, but may be more complicated if nested types need custom (un)marshaling.\nOut of curiosity do you see many types needing custom (un)marshaling?\n. Marking as question since we cannot make this change directly without introducing breaking changes to the SDK, but we might be able to figure out an alternative solution for you're use case.\n. Thanks for the extra information @maxekman. Taking a deeper look at dynamodbattribute.Marshal It looks like it does not handle type aliases the way it should. I think this should be a bug in the (un)marshaler. \nThis script exercises this issue.\n``` go\npackage main\nimport (\n    \"fmt\"\n    \"encoding/json\"\n    \"github.com/aws/aws-sdk-go/service/dynamodb/dynamodbattribute\"\n)\ntype MyString string\ntype MyInt int\ntype Struct struct {\n    Str string\n    Int int\n}\ntype AliasStruct struct {\n    Str MyString\n    Int MyInt\n}\nfunc main() {\n    b, err := json.Marshal(AliasStruct{\"abc\", 123})\n    fmt.Println(\"json.Marshal (with alias)\", string(b), err)\n    as := AliasStruct{}\n    err = json.Unmarshal(b, &as)\n    fmt.Println(\"json.Unmarshal (with alias)\", as, err)\nv, err := dynamodbattribute.Marshal(AliasStruct{\"abc\", 123})\nfmt.Println(\"dynamodbattribute.Marshal (with alias)\", v, err)\n\nv, err = dynamodbattribute.Marshal(Struct{\"abc\", 123})\nfmt.Println(\"dynamodbattribute.Marshal (no alias)\", v, err)\n\ns := Struct{}\ndynamodbattribute.Unmarshal(v, &s)\nfmt.Println(\"dynamodbattribute.Unmarshal (no alias)\", s, err)\n\nas = AliasStruct{}\ndynamodbattribute.Unmarshal(v, &as)\nfmt.Println(\"dynamodbattribute.Unmarshal (with alias)\", as, err)\n\n}\n```\n. The best workaround for this issue is to define the (Un)Marshaler interfaces for the aliased types until this issue is fixed.\n. HI @maxekman thanks for submitting this issue. I pushed a fix which adds support for aliased types to DynamoDB AttributeValue (Un)Marshalers.  Let us know if you run into any issue, or have additional feedback.\n. Thanks for contacting us @dlsniper. We'd like to hear more about the specific issues you're seeing using the SDK.\nThere are areas we are looking at in order to improve the performance of the SDK, and are always glad to review PRs for feature changes, performance improvements, and refactors. We need to be careful of breaking changes though. Specifically we think the way the SDK performs operation (un)marshaling can be improved.. The work we recently did with code generating the SDK's request validation (#649) showed how much overhead was being caused by reflection walking parameters of API operations. Switching to code generation reduce the overhead of request validation by 100% for memory allocations, and 95% for CPU time. We are investigating if this tactic can be applied to the API operation's (un)marshaling, and what impact on the memory and CPU overhead it can be improved by.\nFor this work I think we need focused issues with associated PR that target a specific area of the SDK that can be improved. A large refactor PR would be untrackable. The more specific information we get about areas the SDK is causing performance overheads the more focused we can make improvements.\n. Thank you for assisting to improve the SDK @dlsniper. We're always glad to review community submitted PRs. With that said though PRs should be focused on a specific granular topic. Large grab bag PRs are easy to let unintended changes through since the changes are not focused on a specific topic.\nMoving forward, it would be great to continue these topics in their own specific issue thread. This will allow us to track, and discuss the items individually and in their own context. Trying to do this in a single PR will cause comments to be missed, skipped, and forgotten. In addition generic PRs greatly inhibit meaningful discussion.\n. Hi @dlsniper thank you for your feedback. We agree that the SDKs can be improved by removing reflection, and simplifying some of its logic. So we can move forward with these changes I'm going to close this PR, and ask that you please open individual issues for the specific topics so that we can discuss them individually and create PRs with targeted fixes.\nPlease create issues for the items you outlined:\n- Using the error returned by http.NewRequest to determine an invalid URL was provided instead of an extra call to url.Parse.\n- Creating a utility function for waiters to check int, bool, and strings without reflection prior to falling back to awsutil.DeepEquals this utility function probably should be specific to the waiters and reside in that package.\nWe'd like to drive changes like this using performance analysis of the SDK determining what targeted areas we can improve in without making breaking changes to the SDK.\n377 tracks the work we need to do with the API operation (un)marshalers's poor performance. There is a lot of room for improvement here. Mostly focused around the (un)marshaler's use of reflection and type walking. This is where we are experimenting with code generating a portion of the (un)marshaling logic. In the input parameter validation refactor (#649) work we saw significant improvements in the time to validate parameters. To the point validation is now nearly zero overhead of a request.\n. Going to go ahead and close this as a duplicate since #519 covers the work needed to add waiters to the service client interface definitions.\n. Thanks for pointing out this issue @AnvilStriker #736 should fix it once it is pulled in.\n. I've updated the generated docs for the SDK. The Pages and Request go doc should now show up correctly. Thanks for bringing this issue up to us.\n. Hi @skyleelove which service are you looking to set the endpoint for? The best way to set the endpoint for a service client is to set the value in aws.Config.Endpoint.\ngo\nsess := session.New()\nsvc := s3.New(sess, aws.NewConfig().WithEndpoint(\"https://example.com\"))\nLet us know if there is more we can help with.\n. hi @skyleelove, the HeadObjectOutput is the response type that will be returned when the S3.HeadObject API operation method is called. To set the input parameters for this request use the s3.HeadObjectInput struct.  Generally the input request parameter types are suffixed with Input and all the response containers are suffixed with Output.\n. Thanks for the update @skyleelove, I'll close this issue now, please let us know if you have any additional questions or feedback.\n. Thanks for taking the time to submit this PR @dlsniper. I don't think this will be a great change to take. The performance difference in negligible, and arguable hurts readability. \nRight now these sections are a little gnarly to read. I'm not sure if a fmt.Sprintf would make them any easier to read.\n. @dlsniper, thanks for the update. I think the SDK would be better served using pprof/profiling to identify big fish within the SDK that can make significant improvements. Once we've knocked out those items I think that then is a good time to consider micro optimizations vs the cost of readability and maintainability.\nOne item I'm working on right now is to collect metrics for performance analysis of the SDK so we can identify areas to target for further improvements. Please continue to investigate and bring up areas you find where the SDK's performance could be improved.\n. Hi @dlsniper thanks for contacting us. The v4 is currently being refactored in #698. WE're in the process of moving the signer out of the private package and into a sub package of aws.\n. We're more than glad to review changes to the SDKs code. In the initial phase we're mainly looking to update the Signer's interface so it is usable outside of the SDK's service client context, and can be used with other services link ElasticCache and ElasticSearch where the AWS SDK for Go does not provide API operation functions itself, but other libraries do (but without their own request signing).\nImproving the readability of the signer will be helped I think with this current refactor as it creates a little bit cleaner separation between pre-signing and regular request signing. Though with that said, if there are sections of the signer (and sdk in general) that could still be improved for both readability, stability, and performance please let us know.\n. Hi @dlsniper with the update the the signer in v1.2.0 I'm going to close this issue, as it addresses some of the complication and readability issues with the sign vs presign distinction. Please let us know if you have any additional feedback, or questions. \n. Thanks for creating this PR @dlsniper this change is actually one I was planning on including before pulling in the #698. I think we can go head and include this change now, and will rebase the other PR on top once its merged in.\n. Hi @dlsniper you can now rebase against master #735 has been merged in exporting the v4 signer.\nWe probably don't need to export the EmptyStringSHA256 value and can keep it private as emptyStringSHA256 until there is a need to export it.\n. Hi @dlsniper I've pulled this change into #738, with minor update to be compatible with the refactored V4 signer. Thanks for taking the time to create this PR.\n. Thanks for taking the time to put this PR together @j7b. I think this idea is good, and we'd only need ot make an additional changes.\nThe Makefile's unit tests should be updated to use the -tags testing value. In addition the SDK uses integration for some of its integration tests under awstesting/integration/customizations/. I think this idea should be continued over to all go files in the awstesting/integration path, which includes the smoke tests.  The smoke tests are a little bit more complicated because the SDK uses thegucumber` tool to run the integration tests. We need to find a way of exporting the tags to this tool.\nFurther looking, using tags for the SDKs internal code generation command files should be tagged also to prevent polluting user's bin folder.\n. @j7b I took your changes, and made a few tweeks to update the Makefile, and updated gucumber test runner. Closing this PR in favor of #739 which includes these updates. Thanks again for putting together this PR.\n. While reviewing this change and with the testing tags in place, I'm concerned this change may break some users of the SDK if they use the awstesting/* package for any reason. There are a few exported tools in there that users may of used.\nWith that said this latest update to this PR does update the integration and performance tests to not pull in their dependencies.\nThis brings the go get ./... closure down to:\nsh\n\u2514\u2500\u2500 github.com\n    \u251c\u2500\u2500 aws\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 aws-sdk-go\n    \u2514\u2500\u2500 stretchr\n        \u2514\u2500\u2500 testify\nPreviously it was:\nsh\n\u2514\u2500\u2500 github.com\n    \u251c\u2500\u2500 aws\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 aws-sdk-go\n    \u251c\u2500\u2500 lsegal\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 gucumber\n    \u251c\u2500\u2500 shiena\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 ansicolor\n    \u2514\u2500\u2500 stretchr\n        \u2514\u2500\u2500 testify\nThere is still some work to remove github.com/stretcher from the ./... download.\n. Added extra section pointing to the SDK example folder.\n. In addition, the header x-amz-content-sha256 is added by the signed request via the signature's canonical string. If the hash header is different or modified after the request is sent the following error SignatureDoesNotMatch: The request signature we calculated does not match the signature you provided. Check your key and signing method will be returned.\n. I forgot to mention the issue you're seeing here is because S3 v4 signed requests are slightly different than AWS v4 signed requests.\n. With that said, it does look like the SDK should sign the x-amz-content-sha256 header for S3 requests to match the S3 v4 spec.\n. Hi @jafalas I think updating the V4's signer to sign the header for S3 service API operations would be pretty straightforward.\nBasically the bodyDigest is what generates the hash, but it currently is executed after the signed hearder's portion of the canonical string is constructed. I think all that would be involved is to move the bodyDigest to be executed earlier on, so the sha header can be included in the list of signed headers. The hash probably will need to be cached in the context struct until its needed (where bodyDigest is current being called). We're always glad to review PRs if you're looking to implement this.\n. Thanks for the update @jafalas, I agree this should be a bug. This I think is a bug because the SDK is relying on S3 to not enforce its requirement of a signed sha256 header. You're correct the spec is more strict than the actual S3 service's implementation. It does require the header value be present and its value matches what the request signature's body digest value. \nFor correctness the SDK should sign this header to ensure the SDK won't run into issues if S3's implementation does become more strict.\n. Thanks for reporting this issue @jafalas I've updated the V4 signer to ensure the X-Amz-Content-Sha256 header is signed for S3 requests. Let us know if you have any additional questions or feedback.\n. Thanks for the update @dlsniper, in 1.2.1 we saw a minor improvement to requests across the board due adjusting how headers are sanitized. This most likely is apart of the improvement you saw.\nFor Static vs other credential sources I'd like to learn more about the alternate credential sources the test application was using. In generally Id expect static to always perform better because there is no IO file(shared credentials file) or network request (EC2 roles) involved for retrieving the credentials. Though this hit should only occur once within a large window. For shared credentials this will be once for the lifetime of the session, and EC2 Role for the duration of the role's credentials. Which is usually at least an hour.\n. Are your tests sharing the same session and dynamoDB service client or creating a new session/client for every request? The credentials are shared and cached within a session's config. so as long as the session is shared all the service client instances they would be sharing the same cache. \n. I'm going to close this issue. Please feel free to re-open it if you have additional information, or feedback. Thanks.\n. Hi @eldondevcg thanks for contacting us. Are you looking to copy an object to another object within the same account, or between different accounts? Or are you looking to copy just the parts of an object to another destination? I'd like to learn more about the use case you're looking so solve..\n. Thanks for the update @eldondevcg. S3.CopyObject is available if an account has read access to the source object and write to the destination. This is handled internally in S3. This API operation has the benefit of not needing a download and upload on your part.\nThough if you're looking to do the copy with two different accounts, correct the S3 Manager does not currently support this functionality. The best workaround currently us to chain the S3 Manager Download and Upload with a temp file to store the object locally before it can be uploaded.\nMarking this as a feature request so we can prioritize it in our backlog.\n. No, you're correct S3 limits the usage of CopyObject to 5GB. With that said it does look like you can use the UploadPartCopy API operation to create multipart copy of an object in S3 using byte ranges.\nwhen this feature is implemented in the S3 manager it probably would use a combination of CopyObject and UploadPartCopyas needed.\n. Thanks for posting the sample @andrewarrow. This would be a good feature to add to the S3 Manager package.. Updated to also fix type aliases for byte slices and sets.\n. Thanks for submitting this PR @jcapron, I don't think there is anything wrong with the idea of switching this API operation to use the POST form method instead of GET since the service supports both for this operation. But i think we'd want take advantage of the SDKs customization logic when generating the code for this API operation.\nSpecifically, the SDK has a set of customization passes that are available when code generating service models. If a customization was added for cloud search domain's Search operation to convert the method to a POST, and apply customizations to the associated SearchRequest input shape the SDK can limit the scope of the customization.\n. Thanks for contacting us @AHaymond. I'm taking a look into this to see what is going on. I think this might be a bug in the V4 signer's calculating the request's expiration time.\nIn the URL that are being generated, what is the value of the X-Amz-Expires query string value?\n. Thanks for the update, @AHaymond this is the same expiry value i would expect for 120 hours.  I've created a signed URL locally and using it to tests this issue.\nDo you always see the request expiring after 24 hours exactly or is it random for the duration?  I'll reach out to the S3 team to see if they have any insight into this issue. According to the S3 v4 auth spec X-Amz-Expires should be valid between 1 second up to 7 days.\nIn addition I suggest asking on the AWS S3 forum. Other may of experienced the issue you're seeing, and might have additional insight.\n. Hi @AHaymond I've not been able to reproduce the issue you're seeing. I created a pre-signed URL a couple days ago, but still am able to use it. Any additional feedback you have might help investigate this issue further.\n. Hi @AHaymond i'm going to close this issue for now. Please reopen it if you are still experiencing the issue.\n. Thanks for the feedback @seiffert. I think we can consider this a bug in the SDK's attribute marshaler, because with the current functionality it would be very difficult to provide zero value for pointer type values.\nI think we can change this behavior to only do an omit empty check for the pointer and not for the underlying value. To fix this, I think we need to how Encoder.encode determines if a value should be skipped or not.\nLocally I added the following test to service/dynamodb/dynamodbattribute/encode_test.go that highlights this problem.\n`` go\ntype testOmitEmptyScalar struct {\n    IntZero       intdynamodbav:\",omitempty\"IntPtrNil     *intdynamodbav:\",omitempty\"IntPtrSetZero *intdynamodbav:\",omitempty\"`\n}\nfunc TestMarshalOmitEmpty(t testing.T) {\n    expect := &dynamodb.AttributeValue{\n        M: map[string]dynamodb.AttributeValue{\n            \"IntPtrSetZero\": {N: aws.String(\"0\")},\n        },\n    }\nm := testOmitEmptyScalar{IntPtrSetZero: aws.Int(0)}\n\nactual, err := Marshal(m)\nassert.NoError(t, err)\nassert.Equal(t, expect, actual)\n\n}\n``\n. Thanks for contacting us @mattpollard. The SDK was recently updated moving theprivate/signer/v4package toaws/signer/v4, effectively making it public. Changes like this might be causing conflicts in your Go workspace'spkgcache.  Older ( < 1.5 ) versions of Go have difficulties sometimes with the pkg cache if packages are moved, or renamed. In this case I suggest clearing out your$GOPATH/pkg/`.\nThough I would of expected this case to be consistent. In the OpsWorks environment does the GOPATH get reused between runs or is it supposed to be a fresh environment each time Chef is run? If the environment is reused it might explain why you're seeing the issue only in OpsWorks.\nDoes you're OpsWorks environment use a base image with the build environment already setup? If so it explains why you'd always need to rerun the go get a second time. The first time might be invalidating the pkg cache, and the second type correctly rebuilds the resources.\nI don't think you'd be experiencing these issues if your Go version was updated to 1.5. I believe these issues were corrected in later version of go. If you're able to update 1.5 and 1.6 also have significant improvements to the Go GC that your application might benefit from.\n. Thanks for submitting the issue and PR @grepory. I'm working on getting v1.2.5 out for the SDK, which should contain these model updates, and should be available shortly. The update will also  include support for the SDK to read credentials from task defined roles for apps running within the container.\n. Hi @grepory v.1.2.5 has been released, and you're now able to take advantage of the ECS Task defined roles updates. Let us know if you run into any issues, or if we can help answer any questions.\n. Thanks for contacting us @thomshutt, and glad you found the issue.  I'd suggest not setting the endpoint at all, and just use the region instead.  The SDK will automatically select the endpoint based on the region value provided.\n. Looks good overall, with a few bits I think we can update. \nIn general I noticed several cases of using interfaces as anonymous fields within structs. This can cause odd runtime issues if the struct isn't always initialized correctly and can hide bugs. Generally, interfaces should be named fields.\n. :shipit: \n. Fixed loading environment variable for the shared config file, and updated New error handling to not panic, and be inline with how errors would of been reported when requests are made.\n. examples will be updated in another PR\n. LGTM\n. Looks good with. just need to update the comment.\n. Thanks for contacting us @gonber. This looks to be a bug in the way the SDK determines the error code for S3 HeadObject.\nThe following reproduces this problem.\n``` go\npackage main\nimport (\n    \"fmt\"\n    \"os\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\n)\n// Usage:\n// AWS_PROFILE= AWS_REGION= go run ./s3HeadErrDiff.go  \nfunc main() {\n    svc := s3.New(session.New(), aws.NewConfig().WithLogLevel(aws.LogDebug).WithS3ForcePathStyle(true))\n    _, err := svc.HeadObject(&s3.HeadObjectInput{\n        Bucket: &os.Args[1],\n        Key:    &os.Args[2],\n    })\n    fmt.Println(\"HeadObject\", err)\n_, err = svc.GetObject(&s3.GetObjectInput{\n    Bucket: &os.Args[1],\n    Key:    &os.Args[2],\n})\nfmt.Println(\"GetObject\", err)\n\n}\n```\nProvides an error value, but no code or message:\nHeadObject :\n    status code: 404, request id: <reqid>\n2016/07/21 10:18:17 DEBUG: Request s3/GetObject Details:\nI'll investigate this further, but I don't think the SDK will be able to distinguish if the 404 is because the bucket doesn't exists, or because the key doesn't exist.\nInvalid Key:\n``` text\n2016/07/21 10:22:11 DEBUG: Response s3/HeadObject Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 404 Not Found\nTransfer-Encoding: chunked\nContent-Type: application/xml\nDate: Thu, 21 Jul 2016 17:22:09 GMT\nServer: AmazonS3\nX-Amz-Id-2: \nX-Amz-Request-Id: \n```\nInvalid Bucket:\n``` text\n2016/07/21 10:22:54 DEBUG: Response s3/HeadObject Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 404 Not Found\nTransfer-Encoding: chunked\nContent-Type: application/xml\nDate: Thu, 21 Jul 2016 17:22:52 GMT\nServer: AmazonS3\nX-Amz-Id-2: \nX-Amz-Request-Id: \n```\nGetObject API returns the specific error in both cases, NoSuchBucket vs NoSuchKey.\n. Hi @gonber thanks again for reporting this issue. I've submitted a fix for this, and all other s3 API's response that do not respond with a specific error message.\n. Need to update to correct error testing. The unmarshal error tests don't do a great job of reflecting the actual errors and status messages that are produced by the http library.\n. Updated with correct error code handling. Error codes will consistently be returned based on the status code, if there is no error code available in the response's body. For example status code 404 would become the NotFound error code.\n. lgtm\n. Thanks for the feedback @catsby. Are you looking for the SDK to expose typed Triggers for lambda, or a mechanism to automatically link both sides of a  lambda function <=> IoT device without needing to do both processes separate?. Thanks for the update. It looks like the CLI/boto implemented these custom steps using the API. I think all of these steps could be achieved in the SDK, but customizations would need to be added.\nI'm curious what customizations you're looking to add?\n. Thanks for taking the time to create a PR for this feature @ledor473. These service clients are generated from models provided by the service teams. We're working on getting these models integrated into the SDK soon.\n. Closing this since we'll be generating the update from models. Thanks again!\n. You can follow #779, which will be updated when the SDK includes the service model update\n. Thanks for contacting us. PR #778 Also covers this. We'll be including the Elasticsearch version update soon. We're working on getting theses service client updates out.\n. I'll update this issue, when we have the service model updated, and released.\n. also requested in hashicorp/terraform#7836\n. The Elasticsearch version update has been released, and is included in v1.2.9. Let us know if you run into any issues, or have feedback thanks!\n. Thanks for the update @kayaklee. The script is very helpful. What version of the SDK are you using? Post v1.0.0 versions of the SDK use a session.Session for the service client parameter.\nHere is an example of using the latest version of the SDK with the S3 Upload manager.\nUpdated version of script for current version of the SDK:\n``` go\npackage main\nimport (\n    \"fmt\"\n    \"io\"\n    \"os\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\"github.com/aws/aws-sdk-go/service/s3/s3manager\"\n\n)\nfunc upload(s3Svc s3.S3, bucket string, key string, body io.Reader) {\n    uploader := s3manager.NewUploaderWithClient(s3Svc, func(o s3manager.Uploader) {\n        o.PartSize = 256 * 1024 * 1024\n        o.Concurrency = 4\n        o.LeavePartsOnError = false\n    })\n    if uploader != nil {\n        _, err := uploader.Upload(&s3manager.UploadInput{\n            Body:   body,\n            Bucket: aws.String(bucket),\n            Key:    aws.String(key),\n        })\n        if err == nil {\n            fmt.Printf(\"upload succ, bucket=[%s] key=[%s]\\n\", bucket, key)\n        } else {\n            fmt.Printf(\"upload fail, err=[%s] bucket=[%s] key=[%s]\\n\", err.Error(), bucket, key)\n        }\n    }\n}\ntype incompleteReader struct {\n    finish bool\n}\nfunc (self *incompleteReader) Read(p []byte) (n int, err error) {\n    if self.finish {\n        return 0, io.ErrUnexpectedEOF\n    } else {\n        self.finish = true\n        return copy(p, []byte(\"Hello world!\")), nil\n    }\n}\nfunc main() {\n    sess := session.New()\n    s3Svc := s3.New(sess)\n    upload(s3Svc, os.Args[1], os.Args[2], &incompleteReader{})\n}\n``\n. With that said it does look like there is a bug in the S3 Manager's Upload that intentionally ignoresio.ErrUnexpectedEOF` for some reason.\nThis occurs is in both uploader.upload() and in multiuploader.upload().\n. Thanks for reporting this issue @kayaklee. I've pushed a change which should correct how the SDK handles unexpected EOF error during upload. This will be included in our next release also.\n. Thanks for the update @mpmlj, glad you've discovered the the solution the the problem you were solving. I'll forward your feedback along to the DynamoDB team. In addition you can use the AWS DynamoDB Forums to provide this or any additional feedback you have on the service APIs.\n. Thanks for contacting us with this bug @kaareatcolourboxdotcom. I'm able to reproduce this issue. This bug looks to only impact use cases where the tree hash is (pre) computed manually and added to the input request.\nIf the hash is computed by the SDK's Glacier customization, which occurs when the Checksum field is not set, the bug does not occur because the Glacier customizations will set by the SHA 256 and tree hash headers automatically.\n. I've merged the PR, and again thanks for reporting the bug.\n. Thanks for creating the PR @ColourboxDevelopment. The change looks good and I verified it locally.\nThe other customizations are specific S3. The URI path escaping is specific to S3, and Glacier doesn't support the \"UNSIGNED-PAYLOAD\" body hash substitution for pre-signed requests.\n. Tests running into race/timing issues, working to fix this. \n. Updated test cases to be predictable.\n. Taking a deeper look at this change, because the impact a using an opt out pattern will have on users which do not provide a config profile and the SDK falls back to default. The opt out approach will cause AWS_DEFAULT_PROFILE to be read causing the SDK to potentially use an alternate config profile introducing unexpected behavior changes.\n. To prevent a breaking configuration change the loading of the AWS_DEFAULT_REGION and AWS_DEFAULT_PROFILE values will be gated behind the AWS_SDK_ENABLE_CLI_ENV_VAR_FALLBACK environment variable flag. AWS_SDK_LOAD_CONFIG is deprecated and is now an alias for the fallback variable.\n. Dropping this PR for now, has there have been no demand for an opt out process, and adding additional env configuration to opt in selectively to AWS_DEFAULT_REGION AWS_DEFAULT_PROFILE is confusing.  If there is interest we can bring this issue back up \n. Thanks for the feedback @temujin9, and sorry for the confusion, I'll update #472 to reflect dropping the idea of to opt out vs opting in. \nWith the change to add support for the SDK shared config, assume role is only supported if the shared config support is enabled with the AWS_SDK_LOAD_CONFIG flag.  With that said, we're listening for feedback on this issue.\nWith the Opt In functionality you could use the following code when creating a session for your app to not need the environment variable.\ngo\n// Force enable Shared Config support\nsess, err := session.NewSessionWithOptions(session.Options{\n    SharedConfigState: SharedConfigEnable,\n})\n. @jbergknoff-rival  this difference is mostly a historical problem. The aws cli supported the shared configuration file from the beginning. SDKs didn't add support for the config until later. The AWS SDK for Go has not enabled it by default due to the change in behavior this would cause, potentially breaking user's applications.. @temujin9 correct, the V2 SDK github.com/aws/aws-sdk-go-v2 (in Dev preview) reads the shared config file by default. The V2 SDK is still in development and not ready for production, but it is a good version to check out the new changes with.. Thanks for fixing these typos @ericsperano. Looks good.\n. Thanks for the update, @pjebs. Let us know if you run into any additional issues, or have feedback about the SDK.\n. Hi @shwarzes89 thanks for contacting us.  I'm taking a look at this issue. Are you able to reproduce this issue outside of GAE?\nIn addition it might be helpful to enable debug logging for these requests. This can be done with:\n``` go\nsess := session.New()\nsvc := s3.New(sess, &aws.Config{LogLevel: aws.LogLevel(aws.LogDebug)})\n// make service call.\n```\n. Hi @shwarzes89 Let us know if you still are running into this issue, and reopen the issue if so.\n. Looks good, just comment about link's scheme.\n. :shipit: \n. Thanks for contacting us @5k3105 could you also include the specific error message you're receiving?\n. Thanks, I think we need to enable debug logging to see more information. Could you update your session's config to add debug like the following.\ngo\nsvc := s3.New(session.New(&aws.Config{Region: aws.String(\"us-west-2\"),\n        Endpoint:   aws.String(\"x.x.x.x:xxxx\"),\n        DisableSSL: aws.Bool(true),\n        LogLevel: aws.LogLevel(aws.LogDebugWithHTTPBody),\n}))\nThis will log the full request and response headers and body, and might give us some insight into whats failing.\n. Thanks for the update. You're using the SDK with a third party cloud storage platform, correct? Do you know what version of request signing this cloud platform uses?  The SDK only support S3 v4 request signing for S3 requests. Some third party cloud platforms only support V2.\n. Hi @radeksimko this feature has been added to our v1.4.1 release. Let us know if you run into any issues,.\n. Thanks for reporting this issue @cristim. We're investigating the issue that is causing the ec2iface target to not be present. The ec2iface type still exists in the SDK, but docs are not correctly being generated for it.\n. Thanks again for reporting this issue @cristim. The docs have been updated to correctly include the *iface pages.  Let us know if you run into any issues, or have feedback.\n. Hi @tleyden thanks for reaching out to us. This is a great place to ask questions, and contribute. With v1.3.0 the SDK now supports loading assume roles derived from the Shared Config files ~/.aws/config and ~/.aws/credentials. See the SDK's sessions wiki page for more information on this change.\nAssuming a role from the shared config is great if you have access to those files, or want to create a session from them.\nAlternatively if your use case needs the sessions to be created at runtime dynamically the SDK provides the stscreds.AssumeRoleProvider type that should make using assume roles much easier.\nThe way a session will setup assume role from the shared config is a good example how to setup a stscreds.AssumeRoleProvider.\n. Hi @krishnasrinivas thanks for reaching out to us. I'm looking into this question. I'll get back to you once I find the answer.\n. Hi @mitchellh thanks for contacting us. Is SharedCredentialsProvider being used explicitly, as in set to the aws.Config.Credentials field? Or is the SDK's default credential chain being used where Config.Credentials is not manual set at all?\nI ask because a user setting the Config.Credentials field when creating a session, or service client signals to the SDK that it should ignore any default configuration and use only that credential provider instead. The credentials.ChainProvider can be used to chain multiple providers together. Skipping providers which fail, or are unable to load credentials, and going to the next provider in the chain.\nIn your use case, what would you look for the SDK to do when the SharedCredentialsProvider is unable to load the credentials file?\n. Thanks for the update. I'm not sure if we could change this behavior without a breaking change. With that said I think it would be a good for the SDK to expose what the file to load will be if none are passed in. The SDK has this information encapsulated, but needs to be exposed.\nThe SDK could expose these two functions from aws package making it simple to get the expected \"default\" config file to be loaded. The SharedCredentialsProvider would be updated to use these helpers instead of duplicating the logic.\n. Updated this issue to a feature request. I think export the default paths helpers. the aws package is probably the best place to expose these helpers.This is a pretty small, change and if anyone is looking to make a PR we're more than glad to review it.\n. Hi @mitchellh Sorry i never updated this feature request when the SDK added helper functions to return the SDK's default shared configuration and shared credential filepaths.  This feature was added in v1.8.33. The defaults package was updated with two new functions, SharedCredentialsFilename and SharedConfigFilename.\n. Thanks for bringing up this issue. Change #897 Fixes this issue by adding a UnmarshalListOfMaps utility to the dynamodbattribute package. This will make using Scan and Query easier since you'll be able to unmarshal Items now without extra logic.\n. :shipit: \n. Hi @mengbiping Thanks for taking the time to submit this PR.  Would you be able to provide more context about the situation leading up to the panic?\nIn addition if you could provide a description of the fix that would also be very helpful.\n. Hi @mengbiping thanks for the update, and thanks for the idea of how to merge the two functions into one. I think something like this is good since it will make reading the code easier, and explain why a local version of FileByIndex was needed.\n. @mengbiping, I think either encode.go or decode.go is fine within the service/dynamodb/dynamodbattribute package as an un-exported function. I don't think we need to export the function at this point in time.\n. Thanks for the update @mengbiping The change looks good.\n. :shipit: \n. Thanks for contacting us, @AMeng. The SDK should automatically attempt to retry requests that fail because of expired credentials.\nIt looks like the error code returned by the Route53 service API called is different than those expected. I'll take a look at how the other SDKs handle expired requests for Route53. Naively it looks like the error message will need to be parsed because the error code SignatureDoesNotMatch is too generic.\nI think if we can solve the request signature resigning issue on failed request we can solve the issue.  Though I agree the signer should be updated to allow custom request expiration to be set.\nWith that said I don't see any issue with checking if the request's signature will expire \"soon\" and proactively resign it.\n. Thanks for the update @AMeng. This morning I realized in my previous comment I mixed up expired credentials and expired request signatures. With that said, there is no work for us to do on the retry requests I linked in the previous post.\nIn addition is the request being used a pre-signed URL request that the Terraform library vends to your application, or is Terraform making a request on behalf of your application? Or is Terraform providing a AWS SDK for Go request.Request value back to your application?\nI ask because the use-case between the two are fairly different. The 10 minutes only applies to resigning normal request signatures, not pre-signed URLs. In the case of the X-Amz-Expires header, that value is only set when the signed request is for a pre-signed URL. Not regular API calls made with the SDK.\nIs the SDK just taking a very long time to create the signature of the request?\n. Thanks for the additional information @AMeng, this is very helpful.  This helps clarify what my be going on. I don't think there is any issue in the way Terraform is using the API, and I don't think there is a need for them to handle the error on their end.\nI'll dive deeper into how the retries are being done in this context. Its possible that a race condition is going on, but I'm not sure if the change here is the best approach, or if it should be more significant.\n. @AMeng, sorry for the delay, I'm looking into this issue.\nThe more I review this condition I think it might be better for the SDK to not try to determine if the request needs to be resigned at all, and just resign it. Would need to ensure the digest of the request's Body is persisted between retries since this can be costly for large requests. But will not change between retries.\n. Syncing with the other AWS SDKs, neither AWS SDK for Java nor AWS SDK for Javascript employ logic to not resign the request. They do implement various levels of caching though. AWS SDK for Java will cache the signing key, and AWS SDK for Javascript will cache the body digest.  Since the Go SDK will already cache the body's digest through the X-Amz-Content-Sha256 header I think we can remove the is signed check.\nWith that said, we might want to consider adding the expiry check in the request's Send handlers just before the HTTP request is sent. Resigning the request if needed. I have a feeling that the issue you're experiencing is because the request was created and signed, and a point later in time is sent. The check we've been talking about were moved to this new request handler it would help ensure request's sent are signed with a valid signature.\nSomething like the following added to the core handlers, and default handlers\n``` go\nfunc RefreshSignature(r *request.Request) {\n    signingTime := req.Time\n    if !req.LastSignedAt.IsZero() {\n        signingTime = req.LastSignedAt\n    }\n// 10 minutes to allow for some clock skew/delays in transmission. Would be improved\n// with aws/aws-sdk-go#423\nif time.Now().Before(signingTime.Add(10*time.Minute)) {\n    return\n}\n\nr.Sign()\n\n}\n``\n. Hi @AMeng I merged in #876  that should resolve the expired signature you're seeing. Let us know if you're still having any issues, or have feedback.\n. In additionio.ReadSeeker` is used so the SDK can include the request body's digest in the request's signature. This prevents the need for for always requiring the content to be loaded into memory for each request, given that S3 requests generally are multiple megabytes in size.\nIf you have a type which implements the io.ReadSeeker interface you'd be able to use that also, so you wouldn't be restricted to temp files. Something like bytes.Reader.\n. Hi @allspace, I think you can still use the s3manager.Uploader for this task.  As long as your stream is terminated with an EOF the uploader will attempt to read up to part size chunk at a time from the stream.\nThe uploader also handles the case where the object to upload is less than the min part size. This uploader does this by reading up to the part size. If an EOF is encountered in this first read the uploader will switch modes and use the normal s3.PutObject instead of multipart.  If the uploader is able to read the full part size of the stream, and no EOF is encountered the uploader will use the multipart upload mode.\nThis process allows files of all sizes to be uploaded to S3 in a streaming way. Though it also does mean that those file contents will be buffered in memory at part size chunks until the stream is fully read.\n. Alternatively, you could consider the concept of vending pre-signed URLs to your clients. This would allow a client to control its own upload, and retries, bypassing need for the additional load on your service. By your service vending the pre-signed URLs to the client you still maintain control of where the content is uploaded, and how long before that pre-signed URL expires.\nIf size of the uploaded object is a concern a common technique is to have the client provide the file  size , and potentially an MD5 or SHA256 digest to your service. Your service would then generate the pre-signed URL with those values. This ensures the client uploads only the expected content to your s3 bucket.\nThis does put the burden of request retry on the client, but they most likely also have more complete information about what content they are uploading than your service would in this context. \n. Hi @ItsRanveer Thanks for contacting us. JSON marshaling is not supported with the Unmarhsal and Marshal functions due to the complexity and limitations of using JSON marshaler for DynamoDB AttributeValue types.\nThe new functions will look for a Marshaler and Unmarshaler. These interfaces replace the Convert's JSON marshaler usage.\n. hI @ItsRanveer Thanks again for contacting us. Let us know if you have any additional questions, or feedback.\n. looks good\n. Hi @skyleelove, by default the SDK will use connection pooling and keep alive for all requests made to S3. These connections may be closed server side if they are not reused within a short duration though.\n. Thanks for the update @skyleelove. Are you getting the TIME_WAIT for waits when putting just a single object to S3, or only with multiple file uploads? In addition, are you using the S3 Upload Manager or the S3.PutObject() api call? What version of Go and the SDK are you using?\nIf you have any sample code that reproduces the issue it may be helpful identifying the issue you're seeing.\n. Thanks for the update. How does your code initialize the S3 client, and is the s3 client instance shared between all of your upload requests? Or is a new client created for each upload?\nThe SDK's service clients are safe to shared among gorouties, so you shouldn't need to create a new client for each goroutine.\n. Do you see a TIME_WAIT for each sock for each put or only some of the puts?  Its possible that S3 server is closing the connection because it has not been reused within a short enough period. Possibly within the period your code is using. Have you tried to upload the requests in parallel with a smaller delay between the uploads? This might help take advantage of the reused connection before S3 closes it.\n. The theory behind the change I suggest is that by using a shorter delay period between upload your increase the probability of reusing a previous HTTP TCP connection.\nAre you running into an issue where you're running out of sockets/handles that is related to the TIME_WAIT? If so I think it would be good to check to make sure that your code is closing all files that are being put to S3. If given an os.File for the io.ReadSeeker the SDK will not automatically close the file when the upload is complete.\nIf this isn't the issue you could try setting the aws#Config.S3ForcePathStyle configuration field to true. This will change the way your request is being made to S3 to not use the bucket in the domain name but in the URL path. This may help with the closed connections you're seeing.\n. Hi @skyleelove, the SDK does not directly close the HTTP TCP connections. This is managed by the Go std libraries http.Client. if the connection is being closed, I think it is being closed server side.\nWe're you able to set the S3ForcePathStyle config value to see if that helped in this case?\n. @skyleelove, Increasing the number of idle connections will improve the retention of connections in the pool. There looks to be a known issue (golang/go#13801) in Go's http.Client that causes it to more aggressively close connections as idle than really needed. The fix didn't make it into Go 1.7, but looks to be slated for 1.8.\nHow many requests at once is your application making?\nTo just specify a custom http.Client for the SDK I suggest passing it in to the SDK's config, when creating the service client. Something like the following will create a custom http.Client for the S3 service client to use when making requests.\n``` go\nsess := session.New()\nsvc := s3.New(sess, &aws.Config{HTTPClient: &http.Client{\n    Transport: &http.Transport{\n        Proxy: http.ProxyFromEnvironment,\n        DialContext: (&net.Dialer{\n            Timeout:   30 * time.Second,\n            KeepAlive: 30 * time.Second,\n        }).DialContext,\n        MaxIdleConns:          100,\n        IdleConnTimeout:       90 * time.Second,\n        MaxIdleConnsPerHost:   100,\n        TLSHandshakeTimeout:   3 * time.Second,\n        ExpectContinueTimeout: 1 * time.Second,\n    },\n}})\n```\nThe IdleConnTimeout probably doesn't need to be so long. In addition setting ExpectContinueTimeout to such a low value may cause timeout errors if the connection becomes congested or S3 doesn't respond with 100 continue soon enough.\n. Thanks for contacting us about the RDS service API. Correct, this feature is not available yet in the AWS SDKs. We are working with the RDS team to to add support for the cluster reader endpoint feature update. \n. Thanks for the PR @jackbot, and fixing the incorrect doc strings. The change looks good.\n. Thanks for the update @jrevillas, the change looks good. \n. Thanks for creating this Pr @ryanfowler. I agree the SDK should be maintaining the context between requests retries.\n. Thanks for the update @ryanfowler I'm taking a second look at the SDK and if a shallow copy would be sufficient. I think you're correct that the only place body is assigned other than during retry is the debug logger. \n. I'm not able to trigger a race condition with the SDK and this change. I think we are safe to take this change. In more investigation it looks like the conflict between the Body and Transport is resolved because of the usage of the offsetReader, that provides protection against the RouteTripper potentially still reading from the Body after the request call has returned, i.e golang/go#12796.\n. Thanks for taking the time to create the PR, and proposal @pwaller. Adding MFA to the SDK is something we'd like to do. I'm hesitant to use the AWS CLI's cache though. This resource is owned and used only by the CLI. It can can change at any time, and potentially breaking the SDK's usage. In addition there are no write locks on the files so it is possible for race conditions reading/writing these files between the CLI and SDKs.\nWhat about, instead of using the CLI to provide MFA credentials, and token refresh, we integrate this concept of inputing the MFA token into the SDK. Lets continue the discussion over on #842, and discuss on how the SDK could be updated to ingest MFA token directly.\n. Thanks for opening the proposal @pwaller. We'd like to add MFA support to the AWS SDK for Go. This process might help us drive that design, and implementation.\nI think we can break this issue down into two parts, default credentials, and STS credential provider. Adding MFA to default credentials I think will depend on a modification to the STS credential provider.\nCurrently the only way to get a MFA token into the STS credential provider is to set the TokenCode field when creating the provider. This works fine for the first time the credentials are retrieved but most likely will not work for any credentials refresh calls. I think we should investigate a way to modify the STS credential provider to take some kind of additional input for the AssumeRoleProvider type that will allow users to provide the updated token's asynchronously.\nOnce we have the STS credential provider updated, I think we can start taking a look at how this can be expanded to the default credential chain, and shared config. Ideally it would be some concept/config we can replicate across the other AWS SDKs.\n. For the STS credential provider's MFA support I'm thinking that we could add an additional field to AssumeRoleProvider, such as TokenProvider. This field would be a type or interface, that the user would need to set when constructing the AssumeRoleProvider. The AssumeRoleProvider would use this field each time its Retrieve method is called. The value returned from the field would be added to the STS AssumeRole request.\nMaybe something like:\ngo\ntype TokenProvider interface {\n    Token() (string, error)\n}\nI'm not sure if this type's method would need to take any input values or not.\n. Thanks for the clarification @pwaller. I agree the existing AssumeRole in not sufficient in this case. I think a new credential provider for the SDK(s) to use will need to be created thats independent, or expands on STS credential provider. With that said, the basic idea of a TokenProvider still applies but would be added to this new session token credential provider instead of, or in addition to the AssumeRoleProvider.\n. @pwaller thanks for getting back with us. This work is on our backlog. We've not started work on this feature yet. Though we are always glad to help guide any PRs people would like to contribute.\nFor this feature I think the discussion here is on the right track. The implementation could start with a session token credential provider from there additional functionality could be added. Additionally work to have a MFATokenProvider that is used by both the new session token credential provider and the existing AssumeRoleProvider.\nOnce we have this working in the SDK we can take a look next how to integrate the shared config file ~/.aws/config.\n. Thanks for the request @oli-g I don't have any additional information to share at the moment. This feature is still on our backlog, but requests like this help us prioritize the tasks we address next. . Hi @pwaller, @oli-g, and @zmalik I've created PR #1088 that adds support for MFA tokens with assume role via the shared config via the Session and via stscreds.AssumeRoleProvider directly.\nFor this change I went with a function func() (string, error) instead of interface out of simplicity.\nI still need to make another pass over the docs to clarify expectations and usage of the MFA support.\nIt would be great if you could take a look at the PR. Any feedback would be very helpful.. HI All I merged in #1088 which adds MFA support to the SDK. Let us know if you have any issues or feedback. thanks!. Updated to switch to for loop to include retry in statement.\n. Thanks for contacting us @bmurtagh we're working with the service team to get the updated service models into the SDKs. We'll update once we have the updated models. \n. Hi @bmurtagh the SDK has been updated to include the RDS change in our latest release v1.4.11.  Let us know if you have any questions, or feedback. Thanks!\n. @iscofield you can also configure your code's usage of the SDK to always enable support for the shared config ~/.aws/config and AWS_DEFAULT_PROFILE. This can be enabled by setting the SharedConfigState when creating a session let the example below. \ngo\n// Force enable Shared Config support\nsess, err := session.NewSessionWithOptions(session.Optons{\n    SharedConfigState: SharedConfigEnable,\n})\nYou only need to do this once and you can share the session.Session (sess) with all of the SDK's service client's your code uses.\n. Thanks for bringing this issue up. PR #854 fixed a bug that was preventing this value from being loaded if the SharedConfigEnable was used. This bug is now fixed on tip, and will be included in our next release.\nThe SDK will still give precedence to AWS_PROFILE if it is set, but will fall back to AWS_DEFAULT_PROFILE if the previous env var is not set.\n. Closing issue, please reopen if you're still experiencing this issue. Any additional information will help investigating the issue you're seeing.\n. @akamensky I think the issue you're running into is because email and password of the user struct are not being exported. The Unmarshaler uses reflection to walk the type's value, and it will not have visibility of un-exported fields in the struct.\nThe first solution would be to export the email and password fields of the user struct. In addition the dynamodbattribute package uses the struct tag dynamodbav for directives on how to name and control marshaling.\ngo\ntype user struct {\n    Email string `email dynamodbav:\"email\"`\n    Password string `password dynamodbav:\"password\"`\n}\nBut, its completely understandable why you might not want to do this. Alternatively, you could add the method UnmarshalDynamoDBAttributeValue to the email type and implement the unmarshaling explicitly. similar to how json.Unmarshaler would be implemented. Though the down side of using this method will be that you'll need to implement   MarshalDynamoDBAttributeValue as well to marshal the content.\n```go\nfunc (u user) UnmarshalDynamoDBAttributeValue(v *dynamodb.AttributeValue) error {\n    if len(v.M) == 0 {\n        // Empty value, nothing to do.\n        return nil\n    }\nif val, ok := v.M[\"email\"]; ok {\n    if err = dynamodbattribute.Unmarshal(mv, &u.email); err != nil {\n        // Probably want a decorator around the error return so its easy to see\n        // this came from the user unmarshaling.\n        return fmt.Errorf(\"failed to marshal user email, %v\", err)\n    }\n}\n\nif val, ok := v.M[\"password\"]; ok {\n    if err = dynamodbattribute.Unmarshal(mv, &u.password); err != nil {\n        // Probably want a decorator around the error return so its easy to see\n        // this came from the user unmarshaling.\n        return fmt.Errorf(\"failed to marshal user password, %v\", err)\n    }\n}\n\nreturn nil\n\n}\nfunc (u user) MarshalDynamoDBAttributeValue(v *dynamodb.AttributeValue) error {\n    // Create a simple container for the values to be marshaled from.\n    m := map[string]string{}{\n         \"email\": u.email,\n         \"password\": u.password,\n    }\nvar err error\n// Marshal the simple map created above into a DynamoDB Attribute Value Map\nv.M, err = dynamodbattribute.MarshalMap(m)\nif err != nil {\n    // Probably want a decorator around the error return so its easy to see\n    // this came from the user unmarshaling.\n     return fmt.Errorf(\"failed to marshal user type, %v\", err)\n}\n\nreturn nil\n\n}\n```. Thanks for contacting us, @Baberrehman, do you have an example input parameters and API call  that you are using? \n. Thanks for the update @Baberrehman. By default security groups are given full access to the network. I suggest reaching out on the AWS EC2 forums with your suggestion. \nFor custom rules those need to be added like you're doing here. The default rule is automatically added when the group is created. You'd need to remove it, and add any other rules if they conflict with the default.  You can use DescribeSecurityGroups API call to see the current configuration for the security groups. \n.  @Baberrehman I copied your message from another thread I think it was a mis-post.\n\n@jasdel Thank you for the response.\nActually my point is on adding duplicate rules, the API shouldn't throw error. If the rule is already added, API should just skip that request instead of throwing the error.\n. Thanks for the feedback. The best way to get this feedback to the EC2 service team is via the their forums. I'll forward the message along also internally. Since this is functionality defined by the service there isn't a lot the SDK can do on our end. But I encourage you to provide feedback to them.\n. thanks for contacting us @misham. The SDK has basic EC2 meta data access with the the aws/ec2metadata#EC2Metadata client. This client can read from any metadata path, but it's limited in what content it has built in Go types for. \n\nIs this what your looking for, or is there a specific metadata path that would be helpful for the SDK to parse?\n. I took a second look at the ec2metadat client and see what was missing. I think to support this a GetUserData method just needs to be added to the EC2Metadata client.  Marking this is a feature request so we can track its change. We're also always glad to review PRs if you're looking to make a change.\n. Hi @misham I merged in PR #872 that adds the user-data support to the ec2metadata client. Let us know if you have any questions or feedback.\n. Thanks creating the PR @tuxlife. I'll verify this locally since it looks like Travis/github is having issues.\n. Thanks for contacting us @stack72 We've forward this information on to the EC2 service team. In addition the console website has a \"Feedback\" button in the bottom left hand corner of the page. Submitting your feedback here will send your feedback to the EC2 and console teams.\nSince this issue seems to be a limitation of the console website and not the SDK I'm going to go ahead and close this issue.\n. looks good\n. In addition the output message logged to the console should include the reason why the service thinks the request signatures do not match up. This normally includes the expected body sha and headers expected to of been included in the request that were signed.\n. Hi @MichaelLiZhou I'm closing this issue. Please reopen if you're still experiencing this issue. In addition in v1.4.15 a fix was made to the SDK that ensure's requests will be signed with fresh signatures if the signature was nearing expiration before being sent.\n. Thanks for creating the issue @seiffert . This will help us investigate and provide a way of using context with the SDK's Paginators. I think this process should also apply to Waiters.\n. Related issue about the waiters and paginators needing to support configuration raised in #1069. Hi @seiffert I just merged PR #1132 that adds the ability to use context with API operations, paginators and waiters. Let us know if you run into any issues, or have additional feedback.  This change will be apart of our next release.. Thanks for contacting us @dcu. Are you seeing this issue with a service client API operation, or is the signer being used standalone by its self?  The Signer will escape query paths containing non alpha numeric characters. Do you have an example of what the request would look like?\n. Thanks for the update @dcu. Could you include a example of the request's URL that is being used in the signer? Specifically looking for what the path looks like. It would also be helpful to see the error message from the service. In addition, what the service is being used?\n. @dcu I'm looking into this issue. Its correct that the signer needs to have fields escaped before they are sent. To dig deeper into what the cause of the problem is though it would be very helpful if you could provide the HTTP wire log of the request(minus body) and service's error response. Specifically we're looking for any information in the service's error message that may provide more insight into why the request signature does not match what is expected.\nThe httputil.DumpRequest and httputil.DumpResponse are good utilities for logging these responses. This is how the SDK uses these utilities.\nIn addition could you enable the debug with the signer.\ngo\n    v4 := NewSigner(req.Config.Credentials, func(v4 *Signer) {\n        v4.Debug = aws.LogDebugWithSigning\n        v4.Logger = aws.NewDefaultLogger()\n    })\n. I investigated this issue some more and it looks like the SDK's signer performs escaping of the * before the request is sent. This is expected per the Elastic Cache Auth docs, and AWS v4 request signing.\nThe reason the signed request needs to be double encoded is because the URI /logs-*/_search will be encoded to /logs-%2A/_search by the Go HTTP client when the request is made to the service. The canonical string must contain the escaped version of this path. Meaning that the path needs to be signed as/logs-%252A/_search in the signature's canonical string. S3 is the only service where additional encoding of the canonical string's path is not needed.\nI think a workaround for this is to add a configuration flag to the v4 signer which will enable double escaping of the URI. I prototyped this locally and it looks to resolve the issue.\n. @dcu I'm experimenting with https://github.com/jasdel/aws-sdk-go/tree/feature/SignEscapeStrategy, but I'm not really happy with exposing the double escaping this way. I think it would be nice to have a better solution would hide this complexity. \n. @dcu I created PR #885 that address this issue. The PR improves both the documentation of the signer, outlying its requirements for pre-escaping the URI path, and adds support for URL.EscapedPath. This should make it so additional escaping in your code is not needed.\n. @dcu The PR #885 was merged in that resolves the issue you were experiencing with the signer and how the signature was being calculated. Additional documentation was also added to clarify the usage of the signer outside of the SDK.\n. This change should be be included in the SDK's next release. I'll update here, when that release goes live.\n. @allspace The PartNumber parameter is an alternative to the Byte-Range header. If you head the object you'll get the number of parts available via the PartsCount response field. The size of each part will match the size that was used when the object was uploaded. \n. Correct, you don't have access to the contents of the part until the multipart upload is completed. With the ListMultipartUploads combined with the ListParts S3 API operations you can get a list of the incomplete pending multipart uploads, and the number of parts for each. But this will not give you access to the contents of the part until its completed.\nLet us know if you have additional questions, or feedback.\n. Hi @keiths-osc and @coopernurse, I just merged #874, and this fixes the issue of the SDK's Request reader potentially encountering race condition. Let us know if you have any issues,  or feedback.\n. Thanks for the feedback, @MikeMangialardi. PR #871, is looking to fix the underlying issue that you encountered. Specifically, that the SDK's retry logic allowed the raw io.ReadSeeker to make its way to the http.Request without being wrapped. Once this bug is fixed you'll be able to control when the src of the passed in io.ReadSeeker is closed, (aka your file). The change makes sure the http.Request won't close the reader unexpectedly.\n. @MikeMangialardi I just merged #874, and this fixes the issue of the SDK's Request reader potentially encountering race condition. Let us know if you have any issues,  or feedback.\nYou should be able to consistently close the file provided to SDK's requests now.\n. @cam-stitt Thanks for creating the issue. I'm working to reproduce this issue.\n. Hi @cam-stitt I'm having difficulties reproducing this issue. I'm using the following example code running on an EC2 instance, and was able to retrieve EC2 Role credentials.\n``` go\npackage main\nimport (\n    \"fmt\"\n\"github.com/aws/aws-sdk-go/aws/credentials/ec2rolecreds\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\n)\nfunc main() {\n    sess := session.New()\n    creds := ec2rolecreds.NewCredentials(sess)\n    fmt.Println(creds.Get())\n}\n```\nWhat instance os type/size are you using? In addition what version of the SDK are you using. I'm testing with v1.4.14\n. Thanks for the update @cam-stitt do you know if the docker container has any routing rules set up that might be impacting the ec2 metadata request?\n. @cam-stitt I think the / issue here is related something that is being performed within the docker router. \nUsing the following example could you take a look at the body log and let us know the error that you're seeing? Specifically, the Response to the GET /latest/meta-data/iam/security-credentials request. Does the 301 not include a location header? Without docker the EC2 Metadata is happy without the trailing / to the /latest/meta-data/iam/security-credentials endpoint. The docs for EC2 Metadata do not directly say if iam/security-credentials needs to be terminated with a / or not, but there is a 301 redirect to the /latest/meta-data/iam/security-credentials/ that is working in my test.\n``` go\npackage main\nimport (\n    \"fmt\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/credentials/ec2rolecreds\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\n)\nfunc main() {\n    sess := session.New(&aws.Config{LogLevel: aws.LogLevel(aws.LogDebugWithHTTPBody)})\n    creds := ec2rolecreds.NewCredentials(sess)\n    fmt.Println(creds.Get())\n}\n```\nIn addition you could try just curl command\nsh\ncurl -v \"http://169.254.169.254/latest/meta-data/iam/security-credentials\"\nThis should return a 301 but with a Location header.\n. Hi @cam-stitt I'm going to close this issue, please reopen if you're still experiencing this issue. In addition let us know what the result of the above tests were.\n. Dropping this PR in favor of #874\n. I just merged #874, and this fixes the issue of the SDK's Request reader potentially encountering race condition. Let us know if you have any issues,  or feedback.\n. Reviewing this PR in place of PR #871.\n. Thanks for contacting us @micahwedemeyer I'm trying to reproduce this issue, but not able to trigger an error with the URL posted. Do you have a small code sample that is reproducing the issue?\nThis is the code snip I'm am trying to reproduce the issue with.\n``` go\npackage main\nimport (\n    \"fmt\"\n    \"os\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/sqs\"\n\n)\nfunc main() {\n    sess := session.New(&aws.Config{\n        LogLevel: aws.LogLevel(aws.LogDebugWithHTTPBody),\n    })\nsvc := sqs.New(sess)\nresult, err := svc.SendMessage(&sqs.SendMessageInput{\n    QueueUrl:    &os.Args[1],\n    MessageBody: aws.String(`http://foo.com?bar=baz%E8qux`),\n    //      MessageBody: aws.String(`http://foo.com?bar=baz\u00e8qux`),\n})\nif err != nil {\n    panic(err)\n}\n\nfmt.Println(result)\n\n}\n```\nAre there any additional content in the Messages that might be triggering the checksum issue?  SQS does require the content of messages to be UTF-8 encoded. If you look at the message in SQS's web console do any of the characters show up as question mark on a black diamond, \ufffd \\xEF\\xBF\\xBD?\n. Hi @micahwedemeyer I'm going to close the issue, please reopen if you're still experiencing this issue. Any additional information you have will be very helpful investigating.\n. Thanks for contacting us @dlapiduz. I'm working with the CloudFront service team to get the updated models for the SDK. I'll update once the SDK is updated with support for CloudFront's IPv6 fields.\n. @dlapiduz Release v1.4.19 includes the service model update that adds support for CloudFront's IPv6 flag. Thanks again for bringing up this issue, and let us know if you have any feedback or issues.\n. Updated to include more friendly error list, and ensure the correct error type vs code string is added to the doc based on protocol.\n_Example output added to Route53's ChangeResourceRecordSets docstring:_\nReturns awserr.Error for service API and SDK errors. Use runtime type assertions with awserr.Error's Code and Message methods to get detailed information about the error.\nSee the AWS API reference guide for Amazon Route 53's API operation ChangeResourceRecordSets for usage and error information.\nReturned Error Codes:\n```\n* NoSuchHostedZone\nNo hosted zone exists with the ID that you specified.\n\n\nNoSuchHealthCheck\nNo health check exists with the ID that you specified in the DeleteHealthCheck\nrequest.\n\n\nInvalidChangeBatch\nThis exception contains a list of messages that might contain one or more\nerror messages. Each error message indicates one error in the change batch.\n\n\nInvalidInput\nThe input is not valid.\n\n\nPriorRequestNotComplete\nIf Amazon Route 53 can't process a request before the next request arrives,\nit will reject subsequent requests for the same hosted zone and return an\nHTTP 400 error (Bad request). If Amazon Route 53 returns this error repeatedly\nfor the same request, we recommend that you wait, in intervals of increasing\nduration, before you try the request again.\n```\n. Thanks for correcting the typeo @minamijoyo . Change looks good.\n. Thanks for taking the time to create the PR @redbaron. The change looks good. \n. Thanks for contacting us @OrZipori, I tweaked your example slightly to be syntax highlighted. Are you using the AWS SDK for Go for your mobile apps. from the code block pasted it looks like the AWS SDK for Javascript.\n\n\nIf you're using AWS SDK for Go I think if we can set your app to perform additional logging we can get a better idea what is causing the failure. To do this I suggest setting the log level of your application's sns client to be aws.LogDebugWithHTTPBody | aws.LogDebugWithSigning.  These log levels will log the Headers and request the SDK is making so that we can see what how the signature is being calculated.\n. Thanks for the update @OrZipori no problem. Glad to help.\n. Looks good thanks for making the update @lionelmessi.\n. Think you'll need to rebase against master. With this you can add the codegen build tags where needed.\n. @nonexu thanks for getting in contact with us. What version of Go are you using. There is a known issue with Go 1.7 where the logic for reaping keep alive connections could be overly agressive. If you bump up the Transports MaxIdleConnsPerHost.\n``` go\nsess := session.New()\nsvc := sns.New(sess, &aws.Config{HTTPClient: &http.Client{\n    Transport: &http.Transport{\n        Proxy: http.ProxyFromEnvironment,\n        DialContext: (&net.Dialer{\n            Timeout:   30 * time.Second,\n            KeepAlive: 30 * time.Second,\n        }).DialContext,\n        MaxIdleConns:          100,\n        IdleConnTimeout:       90 * time.Second,\n    // Only difference between default transport and this example.\n    MaxIdleConnsPerHost:   100,\n\n    TLSHandshakeTimeout:   3 * time.Second,\n    ExpectContinueTimeout: 1 * time.Second,\n},\n\n}})\n``\n. Thanks for the update @nonexu. Thems.svcvalue that is being constructed here. Is it being shared across multiple goroutines, or creating a newsns` client per goroutine? The SDK's service clients are safe to make API operation requests concurrently with the same service client instance.\n. Thanks for the update @nonexu. The change you mention to switching from a per goroutine service client to a shared client is good. It would allow the requests to share the underlying session, and service client.\nThe request being made as quick as possible could be triggering throttling from the SNS service. The throttling response probably will break break the connection. Were you seeing the same issue with the timeouts enabled?\nI suggest a worker pattern when a set of n workers will perform the SNS publish calls. This limits the number of parallel uploads, limiting the chance throttling response. I think this is similar to the 8 goroutine setup you described.\nFrom your netstat log it looks like the 54.240.255.50 address are being keepalive as expected. Is it possible that the TCP connections to 17.188.164.x are from a different system?\n. Thanks for the update @nonexu. Since the difference between the HTTP and HTTPs are so significant I think something is going on outside of the scope of the SDK. The SDK relies on the Go http.Client to handle all round tripping and connection pooling.\nAre there any proxies or gateways that the requests might be traveling through? Its possible they are interrupting TLS connections.\nI'll reach out to the SNS service team to see if there are any suggestions they can provide.\nTo help debug this issue, you could enable SDK debug logging for the messages. This will let us know if the service is intentionally disabling keep-alive with its response. Use this service client to log each request and response header.\n``` go\nsess := session.New()\nsvc := sns.New(sess, aws.NewConfig().WithLogLevel(aws.LogDebug))\n// ...\nsvc.Publish\n```\nOnce that is done you could search the output for the Connection header or misspellings of it. that will be used to disable keep alive.\nIn addition if you could provide the RequestID of some of the responses.\n. @nonexu It also might be helpful to reach out to the AWS SNS forums There may be others that have experienced this issue before and can provide a additional assistance.\n. @nonexu I was able to get some clarity on this issue from the SNS team. SNS does not support log lived keep-alive connections for TLS. SNS will allow multiple requests over one connection for a short period though.\nSending the requests as quickly in the short period will help your client reuse the connection as long as possible. aka don't use sleeps, and send requests in parallel with goroutines. I think this will help your code make the most of the short keep alive window it has available.\nLet us know if you have any other questions, or feedback.\n. Let us know if you have any additional questions, or if there is anything we can help out with.\n. The addition of the build tag was added to ensure user's getting the SDK wouldn't have SDK dev code generation binaries polluting their Go workspace. In the case of the api package I can see it not needing to be wrapped with the build tag. Where as all of the main cli packages are. I don't think there are any negative impacts of removing the build tag from the private/model/api package.\nThough with that said, this package and all packages under private are not stable and can break at any time. We have an Item in our backlog to cleanup and export the API code generation logic, but no work has started on it.. @danehammer in your use case are you using a executable to perform code generation, or are you using Go's built in generate command?  If you're using the built in go generate the easiest way to support this would be to update your generate.go file to specify the codegen build tag.\ngo\n//go:generate go run -tags codegen ./code/gen/logic/file.go. Thanks for contacting us @btai35. Does this issue occur for each batch delete performed, or only periodically? In addition do you have a short sample of how your code of how the DeleteMessageBatch call is being made.\nIn addition is the code calling DeleteMessageBatch using the same message receive ID for the deletion? Effectively calling the delete multiple times for the same message?\n. Thanks for the update @btai35. It is possible that the Receipt handle for the message ID is not the most recent receipt handle the service will return success, but the message won't actually be deleted. Only the most recent receipt handle can be used to delete the message in the SQS queue. It is possible that this is occurring in your case. It would explain why the service responds with success but the message doesn't get deleted server side.\nhttp://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_DeleteMessage.html\n. I would expect both DeleteMessageBatch and DeleteMessage function the same. The link I referenced is for the DeleteMessage API, but also applies to DeleteMessageBatch. It is the expected effect is if the receipt handle is not the most recent the message server side won't be deleted. Once the delete is called with the most recent handle the message will be deleted server side.\nI think this is a good question to ask on the AWS SQS forums. Others there may be able to provide more information on how to handle the SQS messages that are being received by multiple clients.\n. @btai35 I'm going to close this issue since it is related to the Service API for SQS. Posting on the AWS SQS forums is the best way to get a quick response reaching out to the SQS team. In addition other users may of experienced issues similar to yours.\n. Checking SDK out to sandbox and running make generate needed the following changes.\n``` patch\nFrom c14c1b23633c7b6935c884bb5769d943e69ddd9b Mon Sep 17 00:00:00 2001\nFrom: Jason Del Ponte delpontej@gmail.com\nDate: Fri, 21 Oct 2016 10:54:37 -0700\nSubject: [PATCH] Add codegen getter to get deps\n\nMakefile | 10 ++++++----\n 1 file changed, 6 insertions(+), 4 deletions(-)\ndiff --git a/Makefile b/Makefile\nindex 141ace5..61aec80 100644\n--- a/Makefile\n+++ b/Makefile\n@@ -38,13 +38,13 @@ generate: gen-test gen-endpoints gen-services\ngen-test: gen-protocol-test\n-gen-services:\n+gen-services: get-deps-codegen\n        go generate ./service\n-gen-protocol-test:\n+gen-protocol-test: get-deps-codegen\n        go generate ./private/protocol/...\n-gen-endpoints:\n+gen-endpoints: get-deps-codegen\n        go generate ./private/endpoints\nbuild:\n@@ -121,7 +121,7 @@ endif\n vet:\n        ${GO_VET_CMD} ${SDK_BASE_FOLDERS}\n-get-deps: get-deps-tests get-deps-verify\n+get-deps: get-deps-codegen get-deps-tests get-deps-verify\n        @echo \"go get SDK dependencies\"\n        @go get -v $(SDK_ONLY_PKGS)\n@@ -130,6 +130,8 @@ get-deps-tests:\n        go get github.com/gucumber/gucumber/cmd/gucumber\n        go get github.com/stretchr/testify\n        go get github.com/smartystreets/goconvey\n+\n+get-deps-codegen:\n        go get golang.org/x/net/html\nget-deps-verify:\n2.7.3\n```\n. @luopengift I think the EC2.DescribeInstancesPages method is what will resolve the issue you're experiencing.\nThe EC2 service has a limit of the number of results that will be returned in a single DescribeInstances API call. Because of this, the SDK implements Paginators that will request up to MaxResults per page until there are no more resources left. The Pages methods on the EC2 service client are the paginators.\nThe following is an example how to use the paginator methods.\n``` go\nsvc := ec2.New(sess)\nresv := []ec2.Reservation{}\nparams := &ec2.DescribeInstancesInput{\n    DryRun: aws.Bool(false),\n    MaxResults: aws.Int64(1000),\n}\nerr = svc.DescribeInstancesPages(params, func(result ec2.DescribeInstancesOutput, lastPage bool) bool {\n    resv = append(resv, result.Reservations...)\n    return true\n})\nif err != nil {\n    return\n}\nfmt.Println(\"Have\", len(resv), \"reservations\")\n``\n. @luopengift let us know if you're still running into issues, or have additional questions we can help with.\n. @luopengift Sorry I had a typo in my example. I forgot the...` in the append statement, and return. I've updated the example above.\ngo\n    resv = append(resv, result.Reservations)\nshould of been\ngo\n    resv = append(resv, result.Reservations...)\n. Correct DescribeInstancesPages will return a number of instances per page. The callback function that you pass into DescribeInstancesPages will be called for each page.  Are you seeing the callback function only being called once?\nAre all of your instances that you're looking to query in the same region, or are they spread across multiple regions? The DescribeInstances API call is limited to a single region.\n. I think enabling debug logging on the client might help with this.  You can enable logging using the following.\n``` go\nsess := sesison.New()\nsvc := ec2.New(sess, aws.NewConfig().WithLogLevel(aws.LogDebugWithHTTPBody))\n// Make svc.DescribeInstancesPages call\n```\nIn addition could you log the value of the ec2.DescribeInstancesOutput.NextToken.\ngo\nfmt.Println(\"NextToken:\", result.NextToken, aws.StringValue(result.NextToken))\n. Thanks for the update @luopengift. If you change WithLogLevel(aws.LogDebug) to WithLogLevel(aws.LogDebugWithHTTPBody).  Here we're looking for the what NextToken or nextToken is in the response HTTP body.\n. Thanks for getting the debug output. From the output it looks like the service believes there is only 54 instances in the Account.  My first thought it to suggest verifying the account that is being used is the correct account.\nI'll reach out to the EC2 service team to see if they can shed any light on the issue.\n. If you're using both together I'm curious if there is a difference the the profile. Specifically AWS SDK for Python will us the AWS_DEFAULT_PROFILE shared config profile. The AWS SDK For Go only uses the AWS_PROFILE environment by default. If the AWS_SDK_LOAD_CONFIG variable is set 1 the SDK will use AWS_DEFAULT_PROFILE.\nThe SDK has an open discussion to make this the default functionality.\n. Thanks for the update @luopengift i've reached out to the EC2 service team. Once they get back I'll update this issue. If you find any additional information out on your end please let us know.\nIn addition I suggest posting your issue on the AWS EC2 forums. You may get a faster response that way, and other users may of experienced issues similar to yours.\n. @luopengift Thanks for reaching out to us, I reached out to the EC2 team, and got some clarity on what is going on. It turns out that while there are 101 instances (currently) in the region those instances are from 54 reservations.\nDescribeInstances only returns information on the list of reservations. In the output DescribeInstancesOutput.Reservations[i] there is a Instances field list that you can use to integrate over the instances for each reservation.\nI think this will resolve the issue you're experiencing. Please let us know if you have any additional questions, or feedback.\n. Hi @jferrer21 could you execute go env in the same shell that you're using go get with?  Is the GOPATH value set here?\n. I suggest trying the go get command in a new terminal. It sounds like the go get command isn't able to find the GOPATH env val. are you able to go get any other package?\n. I just updated to v1.7.3 but was not able to reproduce this issue n a Ubuntu system? Were you able to find a solution to this?\n. @jferrer21 thanks for the update. Glad you were able to resolve the installation issue.\nUsing the -v flag for go get might help with additional verbose information about what go get is doing and why it is failing.\nsh\ngo get -u -v github.com/aws/aws-sdk-go\nAn additional thing you could try is to create a temporary Go workspace (aka /tmp/golang or some other new temporary folder) try to get the SDK in this fresh workspace. Its possible that something in your $GPOATH/pkg or something else is conflicting with the go get command.\nsh\nmkdir /tmp/golang\ncd /tmp/golang\nGOPATH=`pwd` go get -v -u github.com/aw/aws-sdk-go\nOne additional thing to check, is to make sure the github.com/aws/aws-sdk-go is typed  correctly. I received an error similar to yours above, because I typed the package path incorrectly.\n. @jferrer21 I'm going to go ahead an close this issue since it seems to be related to Go or the workspace. Let us know if there is anything additional we can help out with.\n. @jferrer21 glad you were able to get the go get to work. Let us know if you have any additional issues, or questions.\n. @refaelos let us know if there is anything additional we can help with. I'll close the issue since this issue is related to Go and not the SDK.\n. @samt42 thanks for contacting us. I think the issue you're running into is similar to #837. I'm working to reproduce this. Do you have a small sample that you can share that reproduces this issue?\n. I'm unable to reproduce this issue with the following script. How does it compare to the code you're using?\n``` go\npackage main\nimport (\n    \"fmt\"\n    \"strings\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\n)\nconst (\n    region = my_region\n    bucket = my_bucket\n    key    = my_key\n)\nfunc main() {\n    sess := session.New()\nsvc := s3.New(sess, aws.NewConfig().\n    WithRegion(region),\n)\n\n_, err := svc.PutObject(&s3.PutObjectInput{\n    Bucket: aws.String(bucket),\n    Key:    aws.String(key),\n    Body:   strings.NewReader(\"abc\"),\n    WebsiteRedirectLocation: aws.String(\"https://example.com/something\"),\n})\nif err != nil {\n    panic(err)\n}\n\ngetResp, err := svc.HeadObject(&s3.HeadObjectInput{\n    Bucket: aws.String(bucket),\n    Key:    aws.String(key),\n})\nif err != nil {\n    panic(err)\n}\n\nfmt.Println(\"Website redirect:\", aws.StringValue(getResp.WebsiteRedirectLocation))\n\n}\n```\nOutput:\nWebsite redirect: https://example.com/something\n. @samt42 are you still running into this issue?  Let us know if you have any update.\n. Glad to help. Let us know if you run into any issuers, or have additional feedback.\n. @jferrer21 The issue you're having with http.Client.Timeout not being defined signals that there is an issue with your Go environment. Go v1.3 added the Timeout field. This makes me think that you're using Go v1.2. The SDK's minimum supported version is Go v1.5 but we recommend at least Go 1.7.\nWhat is the output of the command go env I think something is corrupted in your Go environment.  I suggest re-installing Go. In addition it might be a good idea to delete the $GOPATH/pkg  folder and its contents.\nWhat do you see when you execute: go version ?\n. Thanks for contacting us @DavidJFelix, I'd like to learn more about the your use case. In the web service's code does it cache the contents it will be uploading to S3 as a file or in memory? In addition are you referring to physical memory or virtual memory?\nSince the SDK uses AWS v4 signature version a digest hash of the body needs to be computed. In order to do this the SDK must read the contents of the io.ReadSeekers and seek back to the origin of the reader so that the http.Client can send the body in the request. In addition using an io.ReadSeeker allows the SDK to retry failed uploads if a network connection error occurs. An io.Reader would make retries impossible without reading the full contents into memory.\nOne solution for streaming content is to use the s3manager.Uploader. This utility allows uses  S3's multi-part uploads to upload the content in chunks(aka parts) in parallel and it can be used with an io.Reader. The uploader defaults to 5MB parts but this can be adjusted with configurations.\ngo\nuploader := s3manager.NewUploader(sess)\nerr := uploader.Upload(params /*s3manager.UploadInput */)\nAn additional solution could be to use the SDK's Presigner with the S3 PutObject operation. Though, all retry functionality would be on the onerous of the code that would be sending the URL. If just a io.Reader were used for the request's body, retries would be impossible without loading the full contents into memory like mentioned earlier.\ngo\nreq, _ := svc.PutObjectRequest(params)\nurlStr, err := req.Presign(15 *time.Minutes /* URL expiry time*/)\nWith that said, locally I tried to reproduce this with a 300MB file from disk being uploaded to to S3, but the memory usage of the process stayed at about 6MB. The process's virtual memory did go up 300MB, but that's just because the OS creates this for the mapped file, the memory is not actually used.\nI also tried an experiment where I switched to using ioutil.ReadAll to read the file's contents into memory and then wrap the bytes with a bytes.Reader. In this case I did see the memory usage go up significantly. Memory usage of about the size of the file is expected, but I did see a condition were something in the stack was causing the buffer to be duplicated.\n. In addition I didn't mention before, you can control concurrency of the s3manager.Uploader's uploads. By default it is 5 parts at once per file, but you can change this to 1 or 2 in this context. Even if the uploader is uploading in serial it will handle retries of the parts.\n. @DavidJFelix with the s3manager.Uploader you can control how many parallel uploads are done at once. Configuring this will set how much memory is used during the upload.  The following is an update the the code above that configures a single part upload at a time.\n``` go\nfunc handler(w http.ResponseWriter, r *http.Request) {\n    basicPost(r.Body)\n}\nfunc basicPost(r io.Reader) {\nsess, err := session.NewSession(&aws.Config{Region: aws.String(\"us-east-1\")})\nif err != nil {\n    fmt.Println(err)\n}\nsvc := s3manager.NewUploader(sess, func(u *s3manager.Uploader) {\n    u.Concurrency = 1\n})\ninput := &s3manager.UploadInput{\n    Bucket: aws.String(\"bucket\"),\n    Key: aws.String(\"whatever\"),\n    Body: r,\n}\n\n _, err = svc.Upload(input)\n if err != nil {\n      fmt.Println(err)\n}\n\n}\n```\nSetting Concurrency to 1 will do streaming upload per a part at a time.\n. Reviewing the code it looks like part of the memory issue with S3 manager is that the buffer used for reader is recreated for each part. I think if this was updated to use a pool of readers it would reduce the overall memory alloc/frees.\n. @DavidJFelix I'm working to reproduce this. With a benchmark I can see using an io.Reader like bytes.Buffer the uploader will allocate a bit over the 100MB per upload total over the life of the upload. This is split into 5MB chunks (part size). I expected these parts to be GC once no longer used, but I'll do a focused test to see if i can reproduce leaking the memory.\n. Thanks for the update @DavidJFelix. The memory used by the process may not immediately drop as Go runtime will hold onto memory allocated by the process. The s3manger when not using an io.ReadSeeker will allocate additional memory that could be optimized away though.\nIf possible its best to share the managers and session across uploads where possible. Especially if the SDK were to add byte array pools to optimize out some of the allocs with io.Readers.  The uploader and sessions are safe to use concurrently.\nIn your use-case is the upload manager being used to upload multiple 100MB files? Or is a single file is uploaded?\nWith the following bench, the uploader does allocate about 25MB per upload. About 5MB per part. This highlights why the SDK should optimize for a byte buffer pool when using io.Reader.\nsh\ngo test -run NONE -bench=.* -benchmem\n``` go\npackage main\nimport (\n    \"bytes\"\n    \"fmt\"\n    \"io/ioutil\"\n    \"net/http\"\n    \"sync\"\n    \"testing\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/request\"\n\"github.com/aws/aws-sdk-go/awstesting/unit\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\"github.com/aws/aws-sdk-go/service/s3/s3manager\"\n\n)\nfunc BenchmarkS3Manager_Upload(b testing.B) {\n    buf := make([]byte, 10241024*20)\nmockSvc := mockedSvc()\nuploader := s3manager.NewUploaderWithClient(mockSvc, func(u *s3manager.Uploader) {\n    //          u.PartSize = 64 * 1024 * 1024 // 64MB per part\n})\n\nb.ResetTimer()\n\nfor i := 0; i < b.N; i++ {\n    uploader.Upload(&s3manager.UploadInput{\n        Bucket: aws.String(\"jasdel-bucket02\"),\n        Key:    aws.String(\"somekey\"),\n        Body:   bytes.NewBuffer(buf),\n    })\n}\n\n}\nfunc mockedSvc() *s3.S3 {\n    var m sync.Mutex\n    partNum := 0\nsvc := s3.New(unit.Session)\nsvc.Handlers.Unmarshal.Clear()\nsvc.Handlers.UnmarshalMeta.Clear()\nsvc.Handlers.UnmarshalError.Clear()\nsvc.Handlers.Send.Clear()\nsvc.Handlers.Send.PushBack(func(r *request.Request) {\n    m.Lock()\n    defer m.Unlock()\n\n    r.HTTPResponse = &http.Response{\n        StatusCode: 200,\n        Body:       ioutil.NopCloser(bytes.NewReader([]byte{})),\n    }\n\n    switch data := r.Data.(type) {\n    case *s3.CreateMultipartUploadOutput:\n        data.UploadId = aws.String(\"UPLOAD-ID\")\n    case *s3.UploadPartOutput:\n        partNum++\n        data.ETag = aws.String(fmt.Sprintf(\"ETAG%d\", partNum))\n    case *s3.CompleteMultipartUploadOutput:\n        data.Location = aws.String(\"https://location\")\n        data.VersionId = aws.String(\"VERSION-ID\")\n    case *s3.PutObjectOutput:\n        data.VersionId = aws.String(\"VERSION-ID\")\n    }\n})\n\nreturn svc\n\n}\n``\n. Hi all, I think the the best way to address this issue without a breaking change to the SDK's S3 PutObject and UploadPart APIs would be to use the [s3manager](http://docs.aws.amazon.com/sdk-for-go/api/service/s3/s3manager/) package's [Uploader](http://docs.aws.amazon.com/sdk-for-go/api/service/s3/s3manager/#Uploader) utility.  This utility will take aio.Readerinstead of anio.ReadSeekerand internally optimize for theio.ReadSeeker` case.\nIn addition if you're looking to stream content from a client through your server to S3 I recommend your server application using presigned URLs that are sent to the client. This reduces the load on your service and allows the client application to upload the content without the content being streamed through your service.  I'll put together an example that highlights this idea.. Thanks for the clarification @Redundancy. One way to work around this would be to use presigned URLs with S3's PutObject API. I actually just finished a PR which outlines how to use presigned URLs with S3. The example was written in the context of a client & server setup, but I think its still useful for using presigned urls in general. The presigned url's are then executed with a standard http.Client. The example was in the context of a client and server setup, but i think the idea is similar to your use case. Do you think this would help address your usecase?\nhttps://github.com/aws/aws-sdk-go/pull/1260\n. I don't think we could change the API directly as it would require a breaking change. In the general case the SDK needs to use an io.ReadSeeker for APIs such as PutObject were the reader's content needs  a to be included in the \nAWS v4 signature. The SDK would be required to buffer the reader's full contents into memory inorder to calculate the body Sha component of the signature. In addition retrying request automatically by the SDK would be imposible without buffering the reader's content in memory as well.\nRecently the AWS SDKs have started to allow S3 PutObject  to use a magic value of UNSIGNED-PAYLOAD, for the request's body sha signature component when the request is made over HTTPS. This feature can be enable in the SDK as well.\nWith the above we could consider creating a streaming only version of PutObject that uses an io.Reader instead of read seeker. With the magic unsigned payload body Sha component. This customization may not be able to retry any request error. \nWhen streaming content from a client it's generally much better to provide the client with a presigned URL. An application could require it's client provide it with a sha256 hash of the body the client wants to include. With that hash the presigned URL can be generated to only allow a body with that hash to be uploaded to S3. Additional headers such as Content-MD5 can also be added and included in the signature for data verification.. @ejcx I was able to reproduce the issue you're running into. It looks like the SDK is running into an issue parsing the isx:type XML attribute field from the response.\nxml\n<AccessControlPolicy xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\n    <Owner>\n        <ID>id</ID>\n        <DisplayName>name</DisplayName>\n    </Owner>\n    <AccessControlList>\n        <Grant>\n            <Grantee xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:type=\"CanonicalUser\">\n                <ID>id</ID>\n                <DisplayName>name</DisplayName>\n            </Grantee>\n            <Permission>READ</Permission>\n        </Grant>\n        <Grant>\n            <Grantee xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:type=\"CanonicalUser\">\n                <ID>id</ID>\n                <DisplayName>name</DisplayName>\n            </Grantee>\n            <Permission>WRITE</Permission>\n        </Grant>\n        <Grant>\n            <Grantee xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:type=\"CanonicalUser\">\n                <ID>id</ID>\n                <DisplayName>name</DisplayName>\n            </Grantee>\n            <Permission>READ_ACP</Permission>\n        </Grant>\n        <Grant>\n            <Grantee xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:type=\"CanonicalUser\">\n                <ID>id</ID>\n                <DisplayName>name</DisplayName>\n            </Grantee>\n            <Permission>WRITE_ACP</Permission>\n        </Grant>\n    </AccessControlList>\n</AccessControlPolicy>\n. The Type field in Grantee struct has the unmarshalling metadata needed, but it looks like the REST XML unmarshaler is unable to unmarshal the field for some reason.\n``` go\ntype Grantee struct {\n    // ...\nType *string `locationName:\"xsi:type\" type:\"string\" xmlAttribute:\"true\" required:\"true\" enum:\"Type\"`\n\n}\n```\n. one way to workaround the situation, without a breaking change is to gate this with an opt-in functionality. This way existing users would not notice a difference, and continue to use the SDK the way they have been. While those that choose to use un-squashed paths for key names will have the option to do so.\n. In v1.15.85 the DynamoDB API was updated adding support for transactions. . Thanks for the feedback @sihil. Currently the SDK does not take into account the type of credentials when retrying expired credentials.\nAdding this feature should be pretty straightforward. We probably want to do this as satisfying a Refreshable interface. If the CanRefresh returns true then allow the credentials to be refreshed, and request retried. Alternatively, if not the current request should error out.\ngo\ntype Refreshable interface {\n    CanRefresh() bool\n}\n. thanks for the feedback @shatil. The only way the session could fail is if the shared config file is malformed, or AssumeRoles are used with the shared config enabled. Adding more documentation stating session.Must as the preferred wrapper is, good and we can update the docs to reflect this.\n. Thanks for requesting this update @shatil I've pushed change #1099 which improves to SDK's documentation to use session.Must where appropriate.. In addition would you mind adding example build tags to the file and its associated test. Check out the other example files for the pattern.\n``` go\n// +build example\npackage main\n``\n. @GeorgeHosuAdswizz thanks for contacting us. The simplest way to support what you're looking for it to set the env varAWS_SDK_LOAD_CONFIG=1, and then run your code using the SDK. This flag also enables support for assuming roles from the shared config file. This configuration will instruct the SDK to use the shared config file (~/.aws/config`). By default without that flag set the SDK will not use the shared config.\nThe Sessions getting started guide has more information on this.\nYou can also do this in code too like the following:\n``` go\n    sess, err := session.NewSessionWithOptions(session.Options{\n        // enable shared config support.\n        SharedConfigState: SharedConfigEnable,\n    // Optionally set the profile to use from the shared config.\n    Profile: myProfile, \n})\n\n``\n. Hi @GeorgeHosuAdswizz Thanks for reaching out to us, let us know if there is anything more we can answer or help with.. Thanks for creating the update @iain17 I've taking your changes, and made a minor change to not include theREADME.md` updates. we can discuss that in a separate issue. I created PR #1179 as a fixup with the readme changes removed.\n. Hi @scohen28 Thanks for contacting us. What version of Go and the AWS SDK for Go are you encountering this issue on?  I'm on Go v1.7.3 and SDK v1.5.3 and unable to reproduce this issue.\nThe GetBody is define on the request.Request type. This was added in SDK Release v1.4.15\n. @scohen28 Let us know if you're still running into this issue, and anything we can help.. Hi @bobbydeveaux The best way to configure the SDK to connect to a private endpoint is by setting the aws.Config.Endpoint with the HTTP endpoint of the private service.  You'll also need to specify the Region that the private service expects the requests to be signed for.\nIts important to note that the SDK only supports AWS request signature v4. It does not support S3's v2 like request signature. This is meaningful if the private service only supports v2 S3 request signatures.\nThe following is a conversion of the boto configuration for the AWS SDK for Go.\n``` go\nsess := session.Must(session.NewSession())\ns3Svc := s3.New(sess, &aws.Config{\n    Credentials: credentials.NewStaticCredentials(\n         fmt.Sprintf(\"%s/%s\", os.Getenv(\"MY_S3_ID\"), os.Getenv(\"MY_S3_ID2\")),\n         os.Getenv(\"MY_S3_SECRET\"),\n         \"\",\n    ),\n    Endpoint: aws.String(fmt.Sprintf(\"%s:%d\", os.Getenv(\"My_S3_HOST\"), 8443),\n    Region: aws.String(\"the_private_services_region\"),\n})\n// .. Use S3 API\n```\n. Hi @charz this is correct. This is a valid HTTP request, but some server applications do not correctly support the absolute URL in the request's header.\nThe http.Client may use the absolute URL in the request's initial header if the request was built where the URL.Opaque field is set. The SDK does this to ensure no unexpected escaping is performed by the Go http Client that would break the request signature.\n. In Go 1.5+ there is the URL.RawPath which is a better version of URL.Opaque as its just the URI Path and does not include the hostname. Switching to using RawPath instead of Opaque should prevent the need for the http client from using absolute URL.\n. @tbarbugli the first optimization that could be made is to reuse the session, and uploader between files to upload. Something like the following should remove a lot of the over head you're seeing. Especially if your credentials come from EC2 instance roles.\n``` go\npackage main\nimport (\n    \"bytes\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3/s3manager\"\n\n)\nfunc main() {\n// create the uploader and associated session that will be\n// shared for all instances of the uploader\nuploader := S3Uploader{\n    Client: s3manager.NewUploader(\n        session.Must(session.NewSession(\n            &aws.Config{Region: aws.String(\"us-east-1\")},\n        )),\n    ),\n    Bucket: \"bucket name\",\n}\n\nfor i := 0; i < 1000; i++ {\n    // Example calling handle function with some bytes\n    handle(make([]byte, 100), &uploader)\n}\n\n}\nfunc handle(payload []byte, uploader *S3Uploader) {\n    if err := uploader.Upload(payload, uuid.NewV4()); err != nil {\n        // handle error\n    }\n}\ntype S3Uploader struct {\n    Client *s3manager.Uploader\n    Bucket string\n}\nfunc (u *S3Uploader) Upload(b []byte, key string) error {\n    params := s3manager.UploadInput{\n        Bucket: aws.String(u.Bucket),\n        Key:    aws.String(key),\n    // Its preferable to use a Reader instead of Buffer here as the\n    // Uploader can optimize for having a io.ReadSeeker\n    Body:   bytes.NewReader(b),\n}\n\n_, err := u.Client.Upload(&params)\nreturn err\n\n}\n``\n. A further minor optimization would be to uses3.PutObjectdirectly instead of thes3manager.Uploader. Since the size of the file is so small the s3manager.Uploader's multipart capability isn't being used, and is just adding a little bit of overhead.  Though if the size of the file varies widely and you could periodically have large files I'd suggest staying with the s3manager.Uploader.\n. Let us know if you any other issues with this, feedback, or other questions.\n. Thanks for letting us know about this issue, and example. I think you're correct. This change was added to capture users's canceling a request, ensuring the SDK did not attempt to retry a canceled. request. I think we can add additional logic around this correctly allow retries of Timeout, but not cancels.\n. The SDK should be able to take advantage of the net package's Error interface for determining if the request can be retried. https://golang.org/pkg/net/#Error. Thanks for contacting us @malekascha. The easiest way to use IAM roles with temporary keys are with the SDK's shared config~/.aws/configfile. Similar to how you would use IAM roles with the CLI or other SDKs. To enable support for this with the SDK you'll need to either set the environment variableAWS_SDK_LOAD_CONFIG=1or enable support for the shared config via theSessionwith [session.Options.SharedConfigState = session.SharedConfigEnable`](http://docs.aws.amazon.com/sdk-for-go/api/aws/session/#NewSessionWithOptions).\nAlternatively you can use the SDK's IAM provider directly with stscreds.NewCredentials.. @malekascha In the response from the service what does the service think is incorrect with the signature? Specifically, in the body of the HTTP response you'll see what the service expects the signature to of been computed against.  You can compare this to the debug logging of the signer.\nThe only thing that jumps out in the code snip is not checking the error returned by signer.Sign. This error could be related to credentials not being found, or error creating the signature. This would be my first place to check.\nYou can enable debug logging with the signer using the following:\n```go\nsigner := v4.NewSigner(sess.Config.Credentials, func(s *v4.Signer) {\n    s.Debug = aws.LogDebugWithRequestSigning\n    s.Logger = aws. NewDefaultLogger()\n})\n// Build request ...\n// Make sure to check for error, not doing so will create an invalid request\n_, err := signer.Sign(req, bytes.NewReader(postBody), \"es\", \"us-west-2\", time.Now())\nif err != nil {\n    // handle error. Most likely will be a credentials error not found, \n    // If using EC2 Instance Roles you can retry the sign request, but normally this means\n    // that the SDK's credentials could not be retrieved.\n}\n// Do request ...\n``. Great! let us know if there is anything else we can help with or, any feedback you have.. Hi @EricRobert Let us know if there is anything else we can help out with. Using theDisableRestProtocolURICleaning` is the best way to enable the SDK to no attempt to clean object key paths. . Thanks for contacting us @bacoboy. I'll forward your feedback onto the X-Ray's service team. In addition it would help if you sent your feedback directly to X-Ray via their AWS Console page using the \"Feedback\" link.\nAWS SDK for Go has support for the X-Ray API through the service/xray package. This is different than the support in X-Ray's support for languages running in EC2 though.. Great, let us know if there is anything else we can help with, if you have any additional feedback, or questions!. Dropping this, was bug in XML protocol marshaler.. @voutasaurus Do you see this with any other service and API or just ListHostedZones?. I've been able to reproduce this using 1.8beta1 and not on 1.7.4. Tip of Go also does this. Initially the only difference in the requests I can see so far is that the 1.8beta1 version of the request includes a body with a single 0 character in it for some reason. It looks like the server rejects the request if this is received, which is odd because I would of expected a HTTP status code not socket error.. Looks like this is a change in behavior in 1.8. It is a bit odd to make a get request with a zero length body, but before this was fine. Now it looks like the zero length is getting written the request's body. Go server seems to handle this fine, but I have a feeling other HTTP servers like Apache my not. \n```go\npackage main\nimport (\n    \"bytes\"\n    \"fmt\"\n    \"io/ioutil\"\n    \"net/http\"\n    \"net/http/httptest\"\n    \"net/http/httputil\"\n)\nfunc main() {\n    server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        b, _ := httputil.DumpRequest(r, true)\n        fmt.Println(\"Request\")\n        fmt.Println(string(b))\n    w.WriteHeader(http.StatusOK)\n}))\n\nreq, err := http.NewRequest(\"GET\", server.URL, nil)\n\nbody := bytes.NewReader([]byte{})\nreq.Body = ioutil.NopCloser(body)\n\nresp, err := http.DefaultClient.Do(req)\nif err != nil {\n    panic(err)\n}\n\nb, _ := httputil.DumpResponse(resp, true)\nfmt.Println(\"Response:\")\nfmt.Println(string(b))\n\n}\n```\nOutput:\n```sh\nRequest\nGET / HTTP/1.1\nHost: 127.0.0.1:39813\nTransfer-Encoding: chunked\nAccept-Encoding: gzip\nUser-Agent: Go-http-client/1.1\n0\nResponse:\nHTTP/1.1 200 OK\nContent-Type: text/plain; charset=utf-8\nDate: Thu, 08 Dec 2016 21:24:58 GMT\nContent-Length: 0\n```. Arguably the SDK shouldn't be setting the body at all if the length is zero especially for GET, HEAD, and DELETE operations, but fixing this now will still mean users that update to Go 1.8 but not updating the SDK will experience issues.. Marking as bug since we can make some fixes on our end to improve this, but I think the change that introduce this into 1.8 may also be at issue.. Sure glad to. This process is spread over a few places though.\nThe SDK will first create a request based on metadata for the API called.\nThe request's query parameters, headers, and body will be constructed based on the API and protocol. Updating the http.Request value's Headers, URI, and QueryString. The built body is stored in a io.ReadSeeker.\nThe built body of is then wrapped in an concurrency safe internal reader to allow retrying of the underlying reader without risking race conditions with http.Transport before being assigned to the http.Request.Body field.\nThe built request will finally be sent via the configured HTTP client.. Thanks for contacting us about this issue. I pushed a change which corrects this in #991, and this will be included in our next release. Let us know if you run into any issues, or have feedback.. Thanks for the feature request @seanchann. How are you looking for this functionality to be represented? Would the utility perform a local search of the Scan/Query result page, or is there additional search/filtering functionality you're looking for the SDK to create via an expression?\nIs there any existing implementation you think would help the SDK's to be modeled after?. Thanks for the update @seanchann I'll forward this feedback onto the Support service team. In addition I suggest you also provide feedback directly to them. The best way to do this is via the Support AWS forums.\nSince this is a service API feature we cannot make changes directly to the SDK to support this feature. With that said reaching to the service team is the best way to raise visibility of this feature request. \n/edit: missed that this request for Support not Dynamo DB. @pin thanks for testing against this change. I think you're correct. The SDK pre-sets the request to a io.ReadSeeker, and later populates this with connects. The SDK was relying on a undocumented feature in Go where Go's http client would figure out if the request should be sent with a request body or not based on the length of the body.\nIn the case of the SDK, AWS services do not seem to like GET, HEAD, or DELETE HTTP requests with chunked encoding bodies, even if those body chunks are 0 length. The request will fails with a timeout. I'm working on reaching out internally to see if that issue can be addressed in a broader way server side to ignore the body instead of timing out the socket. But, for now this change should fix the SDK's behavior.. Thanks for reaching out to us @btai35. I think you're correct the issue you're experiencing is because of the location of the credentials.  Are your credentials stored in another file with the same format as the ~/.aws/config ? If so you can point the SDK at that file by using the environment variable AWS_CONFIG_FILE.\nThe SDK doesn't support yaml files, but if you use a third party utility to load the file you can set the values for the SDK using the credentials.NewStaticCredentials\n``go\nsess, err := session.NewSessionWithOptions(sessionOptions{\n    Config: aws.Config{\n        Region: aws.String(cfg.S3.Region),\n        Credentials: credentials.NewStaticCredentials(accessKey, secretKey, sessionToken),\n    },\n    SessionConfigState: session.SharedConfigEnable,\n}). Thanks for creating the issue @hajhatten. Marking this as a feature request. I think this could be exposed as a helper utility either off ofPartition` or helper function like the one you mentioned. \ngo\n// helper method on Partition\nfunc (p Partition) RegionsForService(serviceID string) []Region{}\nWhat are your thoughts on list of regions vs map? Currently a map is used to make lookup easier, and order of region is arbitrary.\nI think you could do this with the current functionality like the following. Would additional documentation and examples make this easier to discover?\ngo\n// returns a map[string]Regions\nregions := endpoints.AwsPartition().Services()[endpoints.S3ServiceID].Endpoints()\nThough, there is a minor difference between Endpoint and Region, as endpoint also includes other endpoints such as aws-global and local which aren't really regions.\nIf the above examples works well with more documentation we could add a Regions method to Service that returns just the Regions that a service are in excluding non-region endpoints.. Hi @hajhatten we've merged in #1218 which adds a few utility functions:\n RegionsForService - returns a list of regions for a given service and partition.\n DefaultPartitions - returns the list of partitions removing the need to use the EnumPartitions interface.\n* Service.Regions returns a list of AWS regions a service is in.\n. Thanks for taking the time to find and add the throttle error code to the SDK @kcolemangt. Let us know if you find anything else, or have additional feedback.. Thanks for finding this @kcolemangt and making the PR. The change looks good. . Thanks for reaching out to us @nonexu. You're only seeing this issue periodically, correct? Not for every call to PutRecords.\nIn the loop where the code is retrying on error, do you know what error the code is receiving? The SDK will automatically retry throttling and temporary errors from the service, but other errors may not be retry-able.\nThe call to PutRecords Looks straightforward. My first idea would be to check the self.records value to ensure that it always has all the values that are expected to be put into Kinesis. Are the records of a regular format, and are you able to share what a record might contain?\nIn addition is the ExplicitHashKey value being set in the record?. Thanks for contacting us @alexbrand. Do you see this with any usage of route53's ChangeResourceRecordSets? Do you see it with other Route53 API calls as well? In addition what version of Go are you using?. Thanks for the update, and code link. I've been able to reproduce this with ChangeResourceRecordSets. The root issue might be impacting other API calls as well. It looks like Route53 is sending back errors that are not wrapped with the expected ErrorResponse. \nCan you verify if the credentials you're using to make the API call have write \naccess to the HostedZoneId resource? Though, I seem to be getting the same error message for accounts that should have access to the hosted zone.. @alexbrand If you can provide a request Id I can forward that along to the Route53 team to get more insight into the issue.\nA way to get the request ID quickly for this if your code uses the awserr.Error type for looking at error codes is to update 325 of the code you linked to change it to the following. Alternatively, you could just log out the value of awsReq.RequestID.\ngo\n    awsReq, resp := api.ChangeResourceRecordSetsRequest(req)\n    if err := awsReq.Send(); err != nil {\n        if awsErr, ok := err.(awserr.Error); ok {\n            err = awserr.NewRequestFailure(awsErr, awsReq.HTTPResponse.StatusCode, awsReq.RequestID)\n        }\n        return err\n    }\nI'll create a task to expose the request ID for serialization errors.. Ok, the Request ID isn't need, thanks anyways. I've verified this is only a SDK v1.6.3 bug doesn't exist in v1.6.2.. So the only difference I can see between the 1.6.2 and 1.6.3 requests is that the request's URL being absolute in 1.6.2 and relative in 1.6.3.  I'll reach out to Route53 to see if they can provide any feedback into this.\n```\n---[ REQUEST POST-SIGN ]-----------------------------\nPOST /2013-04-01/hostedzone/hostedzone//rrset/ HTTP/1.1\nHost: route53.amazonaws.com\nUser-Agent: aws-sdk-go/1.6.3 (go1.8beta1; darwin; amd64)\nContent-Length: 278\nAuthorization: AWS4-HMAC-SHA256 Credential=/20161215/us-east-1/route53/aws4_request, SignedHeaders=content-length;host;x-amz-date, Signature=9a184f840a4b1a3069c5b39321c9784b0038e859796b6b39c55285e994ce5747\nX-Amz-Date: 20161215T211118Z\nAccept-Encoding: gzip\nCREATE*CNAME\n---[ REQUEST POST-SIGN ]-----------------------------\nPOST http://route53.amazonaws.com/2013-04-01/hostedzone//rrset/ HTTP/1.1\nHost: route53.amazonaws.com\nUser-Agent: aws-sdk-go/1.6.2 (go1.8beta1; darwin; amd64)\nContent-Length: 278\nAuthorization: AWS4-HMAC-SHA256 Credential=/20161215/us-east-1/route53/aws4_request, SignedHeaders=content-length;host;x-amz-date, Signature=a55918bb194a38ac29e784d07276fb5d2a00d840fc5fe2be77839288c7b825d8\nX-Amz-Date: 20161215T211055Z\nAccept-Encoding: gzip\nCREATE*CNAME\n``. Thought id does look likehostedzone is listed twice in the URL, thats probably it.. @alexbrand thanks for reporting this issue. I've posted PR #1006 with a fix that correctly cleans the Route53 URL path. This change will be available in our next release which is expected to be later today.. Thanks for contacting us @nevins-b and investigating this issue. I think we need to make the change for this service in the code generation. The SDK us incorrectly generating the service.go file with the wrong data for the Application Auto Scaling service.\nI've created #1009 to track this work. Thanks again for taking the time to create the PR. In this case I think we need to do the logic change in the code that generates the service.go file.. Correct, but the ServiceName is the value used to lookup the entry in the region and endpoints metadata which is named application-autoscalling.. Yeah, in this case the SDK code generation is getting mixed up with endpointPrefix and the value that should be used for lookup into regions and endpoints.. * Generate test to verify every service has an entry in endpoints metadata?\n  * Or generate the EndpointsKey based on endpoints.XxxServiceID.\n  * What about iot-data and cloudsearchdomain where there are no endpoints metadata entry. @nevins-b I just pushed a fix (#1011) to tip  that resolves this issue. This change will be included in our next release. Thanks for bringing it to our attention.. Hi @Jamesxql thanks for contacting us. I think the issue you are facing is an configuration issue with the AIM being used, or configuration of the environment. I think it might be similar to this older issue I found on the AWS forums, https://forums.aws.amazon.com/thread.jspa?threadID=123510.\nAre you using an EBS EC2 instance? In addition what AIM are you using?\nFrom the command line do you see the value of home if you do the following command?\nsh\necho \"HOME=$HOME\". @Jamesxql The first suggest I can think of is to verify the AIM is configured correctly and any supervisor configuration used isn't removing or incorrectly setting the HOME variable. I think posting on the 'EC2 Forums' is the best way to get assistance with this issue. In addition other users may of experienced the issue also and will have suggests.\nLet us know if there is anything more we can do to help.. @nonexu you shouldn't need a new client, but you may need to retrieve a new iterator if the current expires.. Hi we've merged in PR ##1166 which adds retrying of connection reset errors discovered during unmarshaling response body, and read timeouts for Kinesis API Get operations. The read timeouts should prevent your application hanging for significant amounts of time.. Thanks for contacting us @Jamesxql. That line will limit the capacity of the byte slice returned. In reviewing the code I think this was done to ensure any appends to the returned slice beyond the slice's current length will not modify the WriteAtBuffer's internal slice buffer.  But, with that said, I don't think this extra logic is needed. . I think we can replace that line with just return b.buf.. One additional check is to update the SDK to the latest I noticed your SDK is v1.1.21 which is several months old. The latest version is v1.6.12. Thanks for contacting us @ror6ax.  I think this the Vault project will be better able to help with configuring the Vault tool to connect to a custom endpoint.  Unless there is a way to pass the aws.Config returned by getClientConfig back to Vault I don't think modifying the config value here will have the impact you're looking for. \nThe best way to contact Vault is via their github repo, #vault-tool on Freenode, or their Google Groups.\nThe shared configuration files (~/.aws/config and ~/.aws/credentials) do not define a endpoint value. The --endpoint-url is specific to the CLI due to its command line nature. To se the SDK's endpoint all you need to do is is set Config.Endpoint to the target prior to creating the needed service client.\n```go\nsess := session.Must(session.NewSession())\ns3Svc := s3.New(sess, aws.NewConfig().WithEndpoint(\"http://eucalyptrusurl\"))\n// Make s3 requests to custom endpoint\n``. @codingzombie is the ARN for your role coming from the shared config file~/.aws/configor the shared credentials file~/.aws/credentials? if the role ARN is coming from there you may need to enable the SDK to read that value from the file by default it does not. You can use the env varAWS_SDK_LOAD_CONFIG=1to enable this feature or in code with, [NewSessionWithOptions`](http://docs.aws.amazon.com/sdk-for-go/api/aws/session/#NewSessionWithOptions).\nCheck out the aws/session package for more details on what and how the SDK determines what configuration to load\nlet us know if this helps solve the issue you're running into.. @codingzombie could you please provide a sample of the error message you are getting? It might be helpful to also enable detailed debug logging.\n```go\nfunc main() {\n    sess := session.New(&aws.Config{\n        LogLevel: aws.LogLevel(aws.LogDebugWithHTTPBody),\n        CredentialsChainVerboseErrors: aws.Bool(true),\n        Region: aws.String(region),\n    }))\n    _, err := sess.Config.Credentials.Get()\nfmt.Println(err)\n\n}\n```\nwhen a request is made you'll get more detailed error and the body of the requests will be logged.. Hi @btai35 by default the SDK uses TLS for all AWS requests. You can optionally disable this for debugging, but it is recommended to leave it enabled for most use cases.\nIf you think you've found an issue where this may not be the case, please let us know so we can investigate it.. Let us know if you have any additional questions or feedback.. hi @shaileshkm Let us know if you still have any additional questions.. Thanks for creating the PR @marclop. This file is generated by the EBS service team. I'll close this issue, but I will take this information back to them about changing the max attempts. \nIn addition I think the SDK needs a way to configure the max attempts and wait delay. Similar to #454. I'll take a look and see if there is a better way we can expose this configuration in a easier way.\nI'll update here with what I hear back from the service team. . Thanks for contacting us @t2y We'd be glad to have contributions.  I'll put together a contribution guide for the AWS SDK for Go and will link to this issue when updated.\nIn the mean time, are there any questions I can answer about contributing to the SDK?. Version naming incorrect.. Hi @orlando Thanks for contacting us. I don't think this functionality is immediately available with the SDK's unmarshaling of the response from the service. The encode from string to base64 occurs automatically when the SDK marshal the request for the API operation. \nThe SDK doesn't currently provide a way to enable this feature. I'm sure how best would be to expose this feature, but the following is a possible workaround. \nCurrently the only way this would be possible is to define a custom input type that defines the field as a string instead of []byte. This does introduce a extra level of complexity on your code since this would prevent your code automatically having available any new fields that were added to kms.DecryptInput, The code would also be relying on internal functionality of the SDK for marshaling input shapes and requests.\n``go\ntype CustomDecryptInput struct {\n    _ struct{}type:\"structure\"`\n// Ciphertext to be decrypted. The blob includes metadata.\n//\n// CiphertextBlob is a required field\nCiphertextBlob string `min:\"1\" type:\"string\" required:\"true\"`\n\n// The encryption context. If this was specified in the Encrypt function, it\n// must be specified here or the decryption operation will fail. For more information,\n// see Encryption Context (http://docs.aws.amazon.com/kms/latest/developerguide/encryption-context.html).\nEncryptionContext map[string]*string `type:\"map\"`\n\n// A list of grant tokens.\n//\n// For more information, see Grant Tokens (http://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#grant_token)\n// in the AWS Key Management Service Developer Guide.\nGrantTokens []*string `type:\"list\"`\n\n}\nparams := &CustomDecryptInput{\n    CiphertextBlob: encryptedBase64,\n}\nreq, output := client.DecryptRequest(nil)\n// Set input parameters on the request value.\nreq.Params = &params\nerr := req.Send()\nif err != nil {\n    return nil, err\n}\n```. Marking as feature request to see if we can better support this in the future.. Added more detailed docs and verified with the stdin token provider is able to assume roles that require MFA.. Thank you for creating this PR @protomouse. I'll forward these upstream to the AWS IoT service team so they can add the paginator's to their model definitions. The AWS IoT service team actually owns this model and the SDK consumes it.  In the mean time I'll close this PR and reference it once the service team adds the additional paginators to their model.\nAgain thanks again for letting us know that these paginators are missing. I'll also work to update the CONTRIBUTING.md and PR templates to help improve awareness of modifying models and generated code.. Thanks for reaching back out to us @protomouse, sorry i do not have a date when this feature will be available. I suggest reaching out on the AWS Forums for AWS IoT directly. This will help improve the visibility about this feature request.. Hi @mcandre thanks for contacting us. The SDK's validation of bucket name only verifies the name string is not empty. The SDK does not validate the bucket exists, or that the request is being made to the correct region. \nBefore the SDK sends the request to S3 it will determine if the bucket name can be used as apart of the request's hostname. e.g mybucket.s3-us-west-2.amazonaws.com. If the bucket name cannot be used in the host name it will be apart of the path instead.. Hi @mcandre i'm going to go ahead and close this issue. Please reopen if you have addition questions about the bucket validation. Thanks!. I think to easily enable this feature we could add an additional option to the session Options that instructs the session to attempt to retrieve the region from ec2metadata automatically if the region is not already set via aws.Config or ENV var.\nProbably should also investigate if this could also be done with a new shared ENV var or shared config field that would instruct the SDKs to get the region from ec2metadata automatically.. @poopoothegorilla And additional level of debuging output might help here. The following will log the body of the HTTP Request/Response.\ngo\ncfg := aws.NewConfig().\n    WithRegion(\"us-west2\").\n    WithCredentials(creds).\n    WithLogLevel(aws.LogDebugWithHTTPBody | aws.LogDebugWithSigning)\nThis will give you a log of everything the SDK is using to sign the request with.\nI notice the ContentLength is being manually set. Is it possible that the value for that field is different than the actual length of the body being sent? Its possible that you're running into a file flush issue. Where the length of the file when stat'ed does not match the length of the content written by gzip. What about trying something like:\n```go\n    w := gzip.NewWriter(file)\n    w.Write(\"hello\")\n    // Move Close from a defer to inline so everything will be written before trying\n    // to stat the file.\n    w.Close()\nfileInfo, _ := file.Stat()\nfileLen := fileInfo.Size()\nfileBytes, _ := ioutil.ReadAll(file)\n\n``. Yeah.. resolving the pending changelog entries i think will need to be manually done :(. Thanks for contacting us @matthinrichsen-wf . Correct it looks like the SDK does not return theHostIDorX-Amz-Id-2` values from an error.\nI think the best way this could be resolved for S3 is to create a S3 error type that will provide access and logging support for the additional ID. Since S3 has custom error unmarshaling I don't think this is a complicated feature to add. Instead of creating a NewRequestFailure a, S3RequestFailure could be returned that satisfies the RequestFailure interface, while also providing access to the second request ID.\nWe'd want the S3 error type exported to be an interface so that concrete typed errors can be created in the future (#394, #722, #377).\n. Hi @matthinrichsen-wf thanks for letting us know about this issue. I've created #1131 to resolve this issue by adding the HostID to the errors returned by S3.. Thanks for contacting us @eric-luminal about the dependency issue. I'm taking a look into this to see if the dependency can be updated. I don't see any immediate impact updating the dependency to the most resent change, but will do some more testing.\nThanks for the feedback @cristim, I think having a continuing the discussion to remove dependencies is worthwhile. The primary concern we have about removing the vendored dependencies is upstream package breaking changes. Lets continue the discussion how/if the SDK should vendor here, #634.. I've created #1126 that updates the version of vendored go-ini to 1.25.4.\nThis fixes the old version of go-ini, but does not address the general desire to remove dependencies from the SDK. Lets continue discussion on #634 to determine how the SDK can improve its dependency references.. Hi @eric-luminal I've pulled in the more recent version of go-ini. This should resolve the issue you were running into.. Would it be worth the extra methods to generate __WithContext API methods for go 1.7 in addition, or instead of __Ex?\nWaiters/Paginators would need to be in addition probably. API operation method would simplify request handler usage.\ngo\nfunc (c *S3) PutObjectWithContext(ctx context.Context, in *S3.PutObjectInput, opts ...request.Option) (*s3.PutObjectOutput, error). After discussing this, Think its better to drop the __Ex methods, and replace with __WithContext methods instead. The SDK probably will need to define its own Context interface. The SDK's Context interface would be a alias for the Go 1.7 context to allow Go 1.6 users to take advantage of the __WithContext API methods also.. Updated methods to be WithContext instead of Ex. Taking a look at API and Waiter retry delay logic.. Need to cleanup new request.Pagination type to have a simpler interface.  HasNextPage is still needed for __Pages callback func signature.\ngo\n    for p.Next() {\n        page := p.Page()\n    }\n    err := p.Err(). Cleanup commits to make reviewing easier.. Thanks for the feedback and suggestions @rhysh. I've updated the PR with an aws.BackgroundContext() instead of using nil value for contexts, and corrected the other issues you pointed out.. Thanks a lot for the feedback. I've updated the docs clarifying the requirement that Context is non-nil and that a panic will occur if it is. This latest update also makes SetContext panic if context is nil creating a stronger contract and expectation.\nCleaned up other documentation throughout the change to clarify functionality and expectations for waiters and paginators. Along with spelling errors in this change.. Thanks for submitting this PR @saquibmian. let us know if you run into any additional situations like this. In addition It would be great to have a benchmark that we could add to the SDK for changes like this in the future.. Thanks for contacting us @crazed , We're actually working on implementing this in #1132. I'll Update the PR to reference this feature request also.\nThe PR introduces the pattern WithContext to the SDK allowing to pass in the context.Context, along with optional SDK Request options.\ngo\nuploader := s3manager.NewUploader(sess)\nresp, err := uploader.UploadWithContext(ctx, params). :) Please let us know if you have any feedback on the PR. Any feedback now is much easier to address, than after the PR is merged.. Hi I've merged #1132 that adds support for context to the SDK's s3manager Uploader and Downloader types.  Let us know any feedback you may have, or issues you run into.. Hi @gwatts thanks for contacting us. Correct, the SDK does not currently retry API operations that fail during unmarshal. In the past the SDK hasn't retried unmarshaling request due to not knowing if the request was truly idempotent and can successfully be retried.\nWith that said, I think we can add a workaround to this condition that will instruct the SDK to retry a specific request. This would need would be an opt-in configuration option on the request.\nThis could appear as a request option on the API operation. I'm not sure if it would make sense across the board to to apply to a service client directly.\nI'll do some research and see if there is any additional idempotent data the SDK can derive from its models, or metadata that can be used to improve this.. In the meantime, retrying the request on \"SerializationError\" is the best workaround.. Thanks for the update @gwatts. I think this is a bug where DynamoDB service client clobbers the retryer where it should only set its own if none was specified in the Config.\nA workaround for this with DynamoDB is to also set the retryer on the dynamodb service client directly\n```go\nsess := session.Must(session.NewSession(aws.NewConfig().WithRetryer(myRetryer)))\ndyn := dynamodb.New(sess)\ndyn.Config.WithRetryer(myRetryer)\n```\nThis is not an ideal solution, but will unblock you I think. We'll need to come up with a fix for the DynamoDB service client to not clobber the configured retryer.\n. Hi we've merged in PR ##1166 which adds retrying of connection reset errors discovered during unmarshaling response body, and read timeouts for Kinesis API Get operations. This means that the SDK will retry requests when a serialization error occurs caused by a connection reset error.. Thank you very much for the in depth information. This is very helpful for us working with the service team to address this. We've forwarded this information on to the Kinesis service team to help address the issue.\nA potential fix for this issue is for the SDK not to wait forever for a complete response from the server. Failing the connection if the server takes too long to respond with the complete response. In this case we should consider if the request can be retried without user intervention. Especially in the cases like Kinesis where the token may not be valid or expired. We'd need to make sure the requests are idempotent.. #1139 would need similar logic for determining if a request can be retried.\nIn the worst case the SDK could fail the request, and not retry. This would require user applications to determine if the request could be retried safely or not.. We'll need to investigate the best way to implement a read timeout on the response body without introducing significant latency. I think we can prototype a few methods of this timeout and go with the best.. Thank you very much for the additional information @twang-rs. We've forwarded this on to the Kinesis team.\nIn addition I think the snip you provided is similar to the idea we'd like to implement. I think we need to profile the impact of using channels/goroutines in a Read method. Since these methods are called many times in tight loops we need to be careful of the overhead of the timer logic.. Thanks for finding this and creating the pr.. Hi @crazed, thanks for finding this and creating the PR. The change looks good. I'll merge it in.. Correct @patrickml, the SDK does not have support for the Affiliate Program service. I suggest reaching out to the Affiliate Program to see if they maintain a SDK for that service.. @patrickml I'm going to close this issue, but suggest reaching out to the Affiliate Program for any SDK they may have available.. @patrickml I'm having a difficulty finding the Affiliate Program support in AWS SDK for Node.js. Could you share a link where you noticed the support?\nI'm looking on the AWS SDK for Javascript's API reference.. Thanks for letting us know about the documentation issue @bmess I've created PR awsdocs/aws-go-developer-guide#7 that should correct this on the developer guide. It may take few days for the change to be visible.. Hi @bmess It looks like the change has been pulled in. The API reference guide page should be updated the next time the api reference guides are deployed.. Thanks for contacting us @bonifaido. I'll take a look at exporting this functionality. I immediately don't see a significant reason for not exporting this.  We're also more than glad to take a look at PRs if you're looking to create one.. Hi @bonifaido I created PR #1161 that exports the tree hash function. . Hi @bonifaido I've merged in PR #1161 which exports the ComputeTreeHash function. Let us know if you have any additional feedback. Thanks!. I've merged in #1161 which exports a ComputeTreeHash function in the glacier package. Let us know if you have any issues, additional feedback, and feature requests :). HI @pinguo-ops thanks for contacting us. Would you be able to provide more description of the error you're seeing?  Specifically any code sample or reproduction case would be very helpful.. Hi @pinguo-ops Were you able to provide any additional information? If so please let us know. I'll close this issue in a few days if we don't hear back.. Hi @pinguo-ops please reopen if you are still experiencing this issue with additional information of the problem you are experiencing.. Thanks for contacting us @linearregression I think there are a couple things the SDK can do to improve this situation. We've made progress in ensuring the SDK's error codes from services are generated constants. This helps knowing what error codes can be returned, and correct matching so typos and string parsing isn't needed. I think we can apply this pattern to the request errors as well.\nIn this case i think exposing the request errors as constants would be a first step.  In this case all of these error codes are from various services, but may not all be modeled. Because of this I think aws/request probably is the best place to generate these error code constants.\nTaking this further, I think we can investigate the best way to expose which error codes will be retried.\nMarking this as a feature request. We're also more than glad to look at PRs.. HI @linearregression I thought about this request some more. I think calling the  Request.IsErrorThrottle is the best way to determine if the service is throttling the request. This method will return true if the SDK determines the service is asking the SDK to throttle the request. \nIn addition, if you're looking to configure custom retry logic I suggest implementing a request.Retryer. Add configuring the API operation to use that retryer. The SDK's uses DefaultRetryer for all requests by default.\n```go\ntype MyRetryer struct {\n    baseDelay      time.Duration\n    maxAttempts int\n}\nfunc (m MyRetryer) ShouldRetry(r request.Request) bool {\n    // Logic to determine if request should be retried.\n    return true\n}\nfunc (m MyRetryer) RetryRules(r request.Request) time.Duration {\n    // Logic to determine retry delay\n    return delay \n}\nfunc (m MyRetryer) MaxAttempts() int {\n    return m.maxAttempts\n}\n//...\nsvc.PutItemWithContext(ctx, params, func(r *request.Request) {\n    r.Config.Retryer = MyRetryer{baseDelay: 500 * time.Millisecond, maxAttempts: 5}\n})\n```\nUsing Request.IsErrorThrottle would be the best way to build on top of the SDK instead of comparing specific error codes for if the SDK will retry the request or not. This would make your code more flexible to the SDK adding additional throttle logic in the future. For example if the SDK ever is updated to look for Throttle in the error code returned by the service instead of doing a direct error code comparison.. Thanks for the clarification @linearregression. The SDK will perform all throttling and retry logic internally to the API operation method call. This generally prevents you from needing to implement additional retry logic for network connection and throttling errors. Because the SDK will handle these internally up to MaxRetries defined in the Config.\nAre you looking to get access to the request retry lifecycle, or notice when a request was retried or throttled?  To get access upstream to the number of times a request was retried you could add code like the following to get the number of request retries.  The SDK doesn't expose this information outside of the underlying SDK Request type.\n```go\nretried := 0\nresp, err := svc.PutItemWithContext(ctx, params, func(r request.Request) {\n    // Add request handler to Complete handlers list to retrieve the number of times\n    // the request was retried.\n    r.Handlers.Complete.PushBack(func(req request.Request) {\n        retried = req.RetryCount\n    })\n})\n``. Thanks for the update @etsangsplk. I think the best way the SDK could expose this functionality would be to export a function to determine if an error is a request throttling error.  This probably would be exposed as exportingIsErrorThrottle` as a function that checked the error codes internal.\nThis probably would provide the best functionality due to how the SDK determines if something was throttled can vary between service, and error code.. Thanks for requesting this feature. I just merged #1202 which exports a set of new utility functions to compare the errors that the SDK considers retryable or throttled.\n IsErrorThrottle\n IsErrorRetryable\n* IsErrorExpiredCreds. Thanks for letting us know about the additional error code. I've created #1158 that adds this functionality.. Hi @bonifaido Thanks for letting us know about this issue. I've merged the change in, and is available on HEAD. This will be included in our next release.. Thanks for letting us know about this issue @jasonmoo. I'll make an update that corrects this issue allowing nil values for the input parameters.. Thanks for the feedback @jasonmoo. The original change was done with the assumption that the SDK's __Request methods generated would have the same default assumptions of empty input parameter type if the input value was nil. In the context of the SKD's current code generation this logic is true, but if anyone were using the API generation code outside of the SDK this may not always be true. \nI have no problem with this change. I can post another CR shortly for this. Making this change  will also simplify any other defaults that might be added in the future.. @jasonmoo I posted PR #1160. I think this addresses the issue you raised. Let me know if you have any additional feedback about the change.. Thanks for the update and feedback!. Thanks updated docs, and tests. Also correct the function name to ComputeTreeHash instead of ComputeHashTree. Thanks for reaching out to us with this issue, and PR @t2y. These model files are provided to the SDK by the AWS service teams. In this case The Amazon S3 service owns this model. I will forward this on to the service team as a feature request for them though, but we won't be able to accept any PRs to the models directly.\nWith that said you can workaround this the headers not being modeled in the S3 API by taking advantage of the SDK's request handlers. I'll put together a workaround that highlights how to retrieve these headers easily in an application.\n. Here is an example how to workaround the header values not being modeled by the S3 API. I think the SDK can add these helper request options to simplify work you'd need to do to retrieve the header values.\n```go\npackage main\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"net/http\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/request\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\n)\n// Installs a request handler into the API request that will assign the header value to \"val\"\n//\n// This should be added to the SDK as a helper.\nfunc WithGetResponseHeader(key string, val string) request.Option {\n    return func(r request.Request) {\n        r.Handlers.Complete.PushBack(func(req request.Request) {\n            val = req.HTTPResponse.Header.Get(key)\n        })\n    }\n}\n// Installs a request handler into the API request that assign \"store\" the map of all\n// headers in the HTTP Response.\n//\n// This should be added to the SDK as a helper.\nfunc WithGetResponseHeaders(store http.Header) request.Option {\n    return func(r request.Request) {\n        r.Handlers.Complete.PushBack(func(req request.Request) {\n            store = req.HTTPResponse.Header\n        })\n    }\n}\nfunc main() {\n    sess := session.Must(session.NewSession())\n    svc := s3.New(sess)\nparams := s3.HeadObjectInput{\n    Bucket: aws.String(\"myBucket\"),\n    Key:    aws.String(\"myKey\"),\n}\n\nctx := context.Background()\n\n// Create a variable to store the id2\nvar id2 string\n\n// Make the API call using the new WithContext API method allowing you to easily pass in\n// request handlers and options to the API call.\nresult, err := svc.HeadObjectWithContext(ctx, &params, WithGetResponseHeader(\"x-amz-id-2\", &id2))\nif err != nil {\n    log.Fatalf(\"unexpected error, %v\", err)\n}\n\nfmt.Println(\"Object Length:\", aws.Int64Value(result.ContentLength))\nfmt.Println(\"Request ID 2\", id2)\n\n}\n```. H @t2y I created PR #1164 updating CONTRIBUTING.md with guidance on creating PRs for model change suggestions.\nI'm going to close this PR for now since we cannot accept the model change directly. I'll also forward this request onto the S3 service team as a feature request.\nIn the mean time I'll create a PR adding the helper functions I mentioned in the workaround example above.\nThanks again for your time investigating and creating this PR.. HI @vaijab would you be able to provide a short reproduction example of how the subnet IDs are being set in the request. In addition, I don't think the API operation will return subnets that are in different regions than the one the API call is being made to. Are all of the SubnetIDs in the region the API call is being made to?\nIs the subnet returned for the the first subnet ID in the input parameters?. Awesome! Thanks for the update and glad you resolved the issue.. Thanks updated. Thanks for the request @markymark. I'll forward this request onto the Lambda team. The more customer request and feedback Lambda sees for customers asking for Go support will help.. Thanks for letting us know @0x1eaf.  This should be added to the SDK in our next release. I'll update here when the update is available.. Hi @0x1eaf v1.8.8 added support for the TreatMissingData parameter in PutMetricAlarmInput.  let us know if you have any additional feedback, or questions.. Thanks for creating this. One thing to consider with tracing API operations is how should the user identify the sequence API operations are made in, and how they are related. I'm not sure the SDK can go down the Java path of enabling a flag and getting support for sequence tracing.\nJava SDK is able to do this because of thread local storage ensures sequence tracing is straightforward. In Go we don't have that option. The nearest comparison would be context.Context. If a user were to manage which context value is given to an API operation method they could achieve a similar sequence tracking. I think this might be good helper functions which create sub-contexts, e.g setting some context Value to identify the sequence.\nContext needs to be used for sequence tracking, because there is no guaranteed association between a service client instance and a different sequences of API operation calls. By this I mean that many API operations may be called from the same service client, but there may not be any direction association between the operations to each other. Since service clients are thread safe and it is encouraged to share service clients as much as possible to reduce overhead. Separate service clients are only needed for differing config such as credentials and region.. Thanks for the update and clarification. I think this is onto the right path using the a context value to keep track of soubriquet requests.\nWith that said I don't think we'd include an X-Ray extension directly in the SDK. This would be great feature built on top of the SDK. By taking advantage of the SDK's request handlers and context a x-ray extension on top of the SDK could be built without needing internal SDK functionality. For comparison the AWS SDK for Javacript's Node.JS X-Ray feature is a separate module that injects into the AWS SDK for Javascript.  Because of this lets close this issue as the feature wouldn't be implemented directly within the SDK.. Thanks for the feedback @dlsniper. I think we still want to keep this issue closed. An X-Ray agent/metric gathering util would be best outside of the SDK similar to the Node.js library. It will allow additional features to be added to the agent separate from the SDK and prevents additional dependencies being added to the SDK's core. With the SDK's Request Handler's functionality all metrics should be available as injected functionality.\nWith that said let us know if there are metric features are you running into that are not available via the SDK's request handler functionality. We want to make sure users are able to get at the metrics they need/want when using the SDK without the SDK being a road block.. Hi @tj  the AWS SDK for Go does not perform any detection or active integration with the X-Ray SDK. The AWS X-Ray SDK for Go will inject the instrumentation into the AWS SDK for Go.. Looks like a03c3b9 is where this bug was introduced.. We've created #1197 to address this issue. Thanks again for bringing this issue to our attention.. Thanks for the update @asac I think we can investigate how to workaround this. But, moving forward I don't think the SDK can guarantee breaking changes won't occur on platforms that do not use the Go standard library, or remove parts of it. The SDK's backwards compatibility guarantee applies to the environments that use the Go standard library. I don't think its possible to guarantee no breaking changes for platforms that remove or modify parts of the standard library. Because of this we cannot prevent breaking changes of this type in the future. Any issues that arise will need to be investigated on a case by case basis.\nIn this isolated case the SDK can probably workaround this issue through alternate compare, or investigate \"// +build !appengine\" build tag workaround.\nRelated stack overflow question.. Thanks for finding this issue and creating a PR for us on it. I'll take this and forward it along to the Amazon S3 service team so they can correct it in their docs. Since the SDK consumes these models from an external source we won't be able to apply the change directly to the SDK. But I'll work with the service team to update their docs.. Thanks for finding this issue, and creating a PR. I think the change is good. I dug into the waiter's test cases and found where this condition was being skipped. I created PR #1195 in place of this one that takes your change, and corrects the waiter tests also.. Thanks for requesting this feature @fxaguessy. Technically this is nothing stopping the SDK from implementing this feature. We'd need to consider that the description of the regions, services, and partitions are not fixed and can change in the future. The SDK probably could address that caveat with docs. . @martonsereg we'd be glad to review a PR if you're looking to put one together. I think the documentation for the feature should also include that the description are not fixed and may changed between SDK releases. It is also possible that some regions may not have metadata descriptions. These descriptions are more of freeform text not official descriptions of the regions.. Thanks for creating this PR, i've merged it in and it will be tagged in the SDK's next release.. Thanks for letting us know about this issue @kahing.It looks like the SDK is not wrapping the connection reset error like it should be. This is a bug in the SDK, and we can correct this to return the correct wrapped error. In addition to this the SDK should identify the connection reset error and retry the request.. Thanks for the update @kahing Just to clarify the key that is getting truncated in the above example is \"linux/linux-4.10.6/drivers/med\" correct?\nI think this might of uncovered a bug in the SDK's XML protocol unmarshaler. Reopening this issue for more investigation..  In a quick review of the code it looks like the call to XMLToStruct by private/protocol/xml/xmlutil#UnmarshalXML ignores any error returned by the function. UnmarshalXML ignoring XMLToStruct's error probably is causing any IO errors to be dropped, and partial ListObjectsOutput value to be populated.\nI'm thinking if the UnmarshalXML is updated to check, and fail on error the issue will be corrected. This theory should be pretty easy to prototype and unit test.\n```go\nfunc UnmarshalXML(v interface{}, d *xml.Decoder, wrapper string) error {\n    n, _ := XMLToStruct(d, nil)\n    if err != nil {\n        return err\n    }\n    if n.Children != nil {\n// ...\n``. Thanks for bringing this issue back to our attention @kahing I've created PR #1219 that address the core issue of the SDK's XML protocol unmarshaler dropping errors. In the tests location this as corrected the issue.  This change should fix the SDK's silently failing on all XML responses.. @kahing thanks for your assistance with this issue. We merged in #1219 which corrects the issue by adding error handling to the XML unmarshaler's logic.  Let us know if you run into any additional issues, or have feedback.. Thanks for the update @kahing, let us know if you run into any other issues or feedback.. Thanks for contacting us @henriquechehad, your change to specify the solution is the correct change to make. It looks like the example linked doesn't include theregionthe SDK will make the SNS request to.  The SDK supports multiple methods for retrieving the region at runtime.  Checkout the [configuration wiki](https://docs.aws.amazon.com/sdk-for-go/v1/developer-guide/configuring-sdk.html) page on how to do this.. Hi @henriquechehad Let us know if your still running into this issue. If not we can close this issue.. Great thanks for the update. Let us know if you run into any additional issues, or have feedback about the SDK.. Thanks for contacting us @bsdlp. It looks like the service is modeling this error incorrectly modeling theNotSuchKey` error code as being returned from this API operation. I'll reach out to the service team to see if they can update this documentation.\nIn the meantime the S3 service will return a 404 (\"NotFound\") when the key does not exist, and 400 (\"BadRequest\") in other cases such as the bucket doesn't exist.\n. consider if a stub region be used if non is set, so the value will be populated?. updated fixing comments.. :( looks like several of the other tests failed. Some of them look to be cause the test are using invalid xml. Making a pass over these.. Fixed the failing tests. In all cases they were failing because of invalid XML strings.. Updated to include closed test and unit testing.. Failing because the doc_custom.go package is adding a package doc without the Package dynamodb ... line.  This appears to be valid for godoc and is appended to the tail of the existing package's doc. Need to see if this is actually valid, or just working unintended. May need to generate these custom docs into the service's main package file.. Example of the new changes:\nNew: \n EC2\n DynamoDB\nOld: \n EC2\n DynamoDB\n. Added package doc for the SDK's root package. https://godoc.org/github.com/jasdel/aws-sdk-go\n. Need to make a pass over the wording to fix that up.. Fixed up the documentations for service packages, and synced the Go package aws-sdk-go docs with the README so we get the content available both in godocs and docs.aws.amazon.com.. Just realized the s3/doc_custom.go file wasn't finished need to go back and fill that in.. Thanks, updated to include rdsutils docs. Thanks for pointing that out again. It looks like the markdown generator the SDK is using to convert the README.md file to an HTML page for \"https://docs.aws.amazon.com/sdk-for-go/api/\" isn't generating lists correctly at all.  I'll take a look at the rules we have setup for that and see about getting this fixed.. It also might be helpful to clear out your $GOPATH/pkg folder. From the description it sounds like the Go package cache is stale, or there is another version of the SDK in your environment that is out of date.. Thanks for contacting us @pjmuller Let us know if you're still running into this issue. I think the above suggestions will help you resolve the issue. Let us know if you have and addition issues, or feedback. @adhikarisandeep could you describe the issue you're running into? This issue seems to be related to a Go Workspace's  reference to the SDK getting mixed up, or the workspace contains multiple conflicting versions of the SDK.\nIn the above case the issue was caused by a library vendoring the SDK and the application using the SDK. Since in Go vendored packages are not the same as non-vendored ones compile errors are thrown when attempting to use the two together.. Thanks for creating the issue @abhay8nitt.  To make sure I understand correctly:\nThe issue is that the EC2 environment needs to use a proxy to make AWS service requests. This excludes the EC2 metadata service for EC2 instance role credentials.  Enabling the proxy will break the credentials, but not having it enabled with break the AWS service requests.   Does that sound right?\nAre you looking to configure the request for EC2 instance role credentials to be made through a proxy, or without the proxy?  Or are only the AWS service requests, e.g S3 made through the proxy?\nFor steps 1 and 2 this makes sense why the succeed or fail. The SDK will use the same proxy for all HTTP clients. This seems to be at the root of the issue, and makes sense why when unsetting NO_PROXY  caused 3 to fail.\nThe Go http.Client default Transport will use the HTTP_PROXY || http_proxy, HTTPS_PROXY || https_proxy, and NO_PROXY || no_proxy environment variables by default.\nCould you share what value you're using with NO_PROXY. Setting this environment variable to a value like NO_PROXY=169.254.169.254.  I think this would workaround the issue you're experiencing.\n. Thanks for the update! I think there are a couple things that you are seeing here. Setting, Credentials to NewStaticCredentials returned value will override and replace the SDK's default credential chain. This is effectively makes the SDK not use the EC2 instance role credentials at all. In addition, the SDK requires a region to be provided in order to make AWS service API requests.\nThe reason the credentials fail when you remove setting Credentials is that the custom http client's proxy is being applied to the EC2 metadata client which makes the request for the instance role credentials. Attempting to proxy the EC2 metadata request is most likely what is causing the failure here.\nWhen an http client has its Transport's Proxy member set that will override any environment variable Proxy configuration. e.g NO_PROXY. You can add logic to the Proxy function to exclude the EC2 metadata endpoint.\ngo\nhttpClient := &http.Client{\n    Transport: &http.Transport{\n        Proxy: func(h *http.Request) (*url.URL, error) {\n            if r.URL.Host == \"162.254.162.254\" {\n                return nil, nil // No Proxy\n            }\n            return url.Parse(os.GetEnv(HTTPS_PROXY))\n        },\n    },\n}\nIn the last code example you provided could you comment what the error message was that you received when Region parameter was removed?  Getting the EC2 Instance Role credentials shouldn't require a region to be set. But, with that said, any SDK API operation to an AWS service will require the region to be set. You can set the region via the environment variable, AWS_REGION, or directly in code like you have.  If you're using the shared configuration file, (~/.aws/config) you also will need to set the environment variable AWS_SDK_LOAD_CONFIG=1 to enable this feature.  See the Session package for more information.. @abhay8nitt, the r.URL.Host can be any hostname or domain name that the request is made with. In this case the IP address was used because that was the EC2 Metadata service's IP.\nIf the URL was http://www.example.com/something/else\" ther.URL.Hostvalue would bewww.example.com`. I think this answers your question. Let us know if this doesn't answer your quest and we can reopen.  In addition, thanks for reaching out to us, and let us know if when you run into any additional issues, or have feedback.. Thanks for raising this issue @cunnie, this definitely looks like a bug in the S3 Manager's Download logic. We're more than glad to review PRs, Our contribution guide can be found here.\nReviewing the code here it looks like the failure occurs because the downloadPart function exits while the download method is waiting to push into the queue. Is this what you were seeing?\nIt looks like the logic needs to be refactored to either drain the download channel, or provide a way for pushing the next chunk to break out. It looks like that if space is freed in the channel the download method's next iteration of its loop will act on the error.\n . Looks like the safest way is for downloadPart to read an item from the ch channel after calling setErr. This should free up the space in the queue for download's channel write to continue, and  on the next iteration of the loop bail out because getErr() != nil.\nsomething like the following should fix the issue I think\n```go\nfunc (d *downloader) downloadPart(ch chan dlchunk) {\n    defer d.wg.Done()\n    for {\n        chunk, ok := <-ch\n        if !ok || d.getErr() != nil {\n            break\n        }\n    if err := d.downloadChunk(chunk); err != nil {\n        d.setErr(err)\n\n\n// Drain an item from the download queue to prevent the downloader\n// from pushing to a full channel, and wait forever because the\n// downloadParts have exited.\nselect {\ncase <-ch:\ndefault:\n}\n            break\n        }\n    }\n}\n```\n\nI think there is a deeper problem here that we'll need to address in the future as well. The general architecture of the download manager needs to be reviewed and refactored. Mainly improving how the download manager uses error as control flow.. Thanks for the update. When creating the PR please reference this Issue as it will be easier to keep the two tracked together. In addition for adding a test. Could you please include a test that will fail without the fix in place, but succeed with the fix.. Hi @cunnie and @cdutra are you still interested in submitting a PR?  jI'm more than glad to spend a little time today to put together a fix for this bug. I have a test cases that reproduces the issue so fixing it should be quick. But i wanted to see if you were still looking to create a PR first.\n. No problem, we're always glad to review PRs. I'll have a PR up later today that should fix the issue. I'll update once its posted. Thanks again for creating this issue and your work to get the issue resolved!. Great glad this is working well for you. Let us know if you run into any additional issues, or have feedback.. Hi @henriquechehad thanks for contacting us. The best place to ask a question like this will be Stack overflow or the SDK's gitter channel. We try to keep our issues focused on the SDK it self, and suggest Stack overflow as the resource for AWS usage questions.. Glad to help, let us know if you have any additional feedback, or issues with the SDK.. You can also find more information on this in the session's documentation. This feature can also be enabled in code by setting the session Option.SharedConfigState to SharedConfigEnable.\ngo\n// Force enable Shared Config support\nsess := session.Must(session.NewSessionWithOptions(session.Options{\n    SharedConfigState: session.SharedConfigEnable,\n})). Thanks for the feedback @jgimenez. You're correct the SDK added support for the config file after the SDK had already been released. We're continuing to watch this issue, because enabling support for the shared config file it self is not breaking, but enabling support for both the config file and AWS_DEFAULT_REGION could be breaking.. One way i think the SDK could implement this functionality is to add support for an intermediate reader that would keep track of the bytes read by the user. When a error occurs internally it could do another GetObject request, but using byte range to start where it had left off.  I imagine that the retry could would be configurable.\nI think this would be the best strategy for implementing this feature.. Hi @mrichman I don't know of any example off hand. The best reference I can find is on using Amazon Cognito User Pools at a higher level cross language.  The examples that are written for Javascript, Android, and IOS may be useful to use as a reference implementation. The aws-sdk-go does not have the customizations implemented like the other SDKs mentioned above.. Hi @bfallik This feature is now available via the WaitUntilSnapshotXXX waiters.  This was added in release v1.12.11. For the testing error: \ncannot find package \"github.com/go-sql-driver/mysql\"\nprobably want to add a skip build flag to the file so it won't be tested because of the external dependency. Something like:\n// +build example,skip. Hi @hongchaodeng thanks for contacting us.  You can configure individual clients to use specific credentials two ways.\nFirst would be to create a new session specifically for the client. The NewSessionWithOptions Session constructor takes an option where you can specify the shared credentials file profile to use. This allows multiple sets of credentials bucketed by profiles to be used by the same application.\n```go\n// Load session with \"profileA\"'s credentials.\nsessA := session.Must(session. NewSessionWithOptions(session.Option{\n    Profile: \"profileA\",\n}\n// Load session with \"profileB\"'s credentials.\nsessB := session.Must(session. NewSessionWithOptions(session.Option{\n    Profile: \"profileB\",\n}\n```\nAlternatively you can create a SharedCredentials credential provider and assign that to the Config when creating the service client. The first option above is preferable if the credentials will be shared by multiple clients.\n``go\ncredsFilename :=~/.aws/credentials`\nsess := session.Must(session.NewSession())\ns3SvcA := s3.New(sess, &aws.Config{\n    Credentials: NewSharedCredentials(credsFilename, \"profileA\"),\n})\ns3SvcB := s3.New(sess, &aws.Config{\n    Credentials: NewSharedCredentials(credsFilename, \"profileb\"),\n})\n```\nIn addition you can actually define your own credentials provider with the Provider interface that the SDK will use to retrieve credentials from custom sources.\n. Thanks for the update @hongchaodeng.\n\nThe multi-tenancy model above is per profile, but not per account.\n\nCould you clarify this? The profile is just a name that is used to group AWS credentials in the shared credentials file. Each shared credentials file can contain multiple profiles. In addition the credentials files can contain profiles for multiple accounts. Profiles do not need to have any association with one another.\n\nThe problem with this is that it couldn't get the \"config\" file, the file referred to by env AWS_CONFIG_FILE.\n\nAh this makes sense, the SDK does not read the shared configuration file by default, only the credentials file. The Sessions from Shared Config section of the session package docs should be helpful here. The AWS_SDK_LOAD_CONFIG=1 environment variable needs to be set, or  NewSessionWithOption used when creating a Session with SharedConfigState set to session.SharedConfigEnable.\ngo\n// Force enable Shared Config support\nsess := session.Must(session.NewSessionWithOptions(session.Options{\n    SharedConfigState: session.SharedConfigEnable,\n}))\n\nCurrently we need to pass in parameters like \"region\" to work around.\n\nCould you go into more details about this? Is this setting the Region parameter of the SDK's Config type? If the SDK's shared configuration support mentioned above, the SDK will read the region from the shared configuration file. If your application is operating in a single environment the AWS_REGION environment variable for setting the region via the environment. The Environment Variables section of the session package docs should be helpful here.. Thanks for the update and additional information @hongchaodeng. Out of curiosity how are the credentials shared with your service? Does User provide them through some interface and they are stored server side?  Are these credentials used for multiple operations, or short lived for a single task?\nI suggest instead of the User providing credentials directly to your application, and your application using those credentials on the User's behalf would be to provide instructions to the users how they can create a cross account IAM role that can be assumed from an account you control. This would allow your users to never provide your application with credentials directly, and it keeps their credentials secure because they are not sharing them.\nThis tutorial on Delegate Access Across AWS Accounts Using IAM Roles should help with this.\nGenerally this will be a much safer and secure means for the customer to give permission to your application to make modifications to their account's resources. The customer can provide find grained permissions as needed. This also reduces the risk your service would need to manage because of the sensitivity of the user's credentials potentially leaking.\nI recommend not storing User credentials in a plain text config file. If your application does need to the user's the credentials, and cross account IAM roles is not an option, storing the credentials with strong encryption should be used.  This guide on How to Manage Secrets for Amazon EC2 Container Service\u2013Based Applications by Using Amazon S3 and Docker  But again, I strongly recommend using cross account IAM roles instead.. Thanks for the clarification. The SDK does not currently export the functionality to load the shared configuration file directly. Only the shared credential's file functionality is exported. The shared configuration file is only available internally when creating a session. But, this makes it difficult to use multiple shared configuration files concurrently.\nI think the best way to add this functionality would be to export the SDK's sharedConfig functionality. We'll need to explore the best way to integrate a passed in or constructed with Session or aws.Config.. One additional way to work around this issue, is a preprocess step that would run before creating a Session which would merge the contents of the multiple shared config files into a single file. You'd need to ensure that the profile names of each file do not clash though. This could probably be accomplished by adding a prefix to the existing profile names. This is probably the least invasive way to accomplish this today until the SDK supports this feature.\nThis suggestion assumes that you that your application is only concerned about multiple shared config files once at startup. If your application deals with multiple shared configuration files throughout the lifetime of the application another solution would be needed.. @hongchaodeng, We'd be glad to review a PR for this feature. I've not started work on it.\nI think the best way to support this is to update the session.Option struct to have an additional optional member which is a list of files the user wants to load the credentials from.\ngo\nsess := session.Must(session.NewSessionWithOptions(session.Options{\n     // Ordered list of files the SDK will load configuration from\n     SharedConfigFiles: []string{file1, file2, file3}, \n}))\nThis would mean the newSession function would use this list instead of the default if it was set.\nIn doing so the we'd probably also want to export the sharedCredentialsFilename and sharedConfigFilename functions.. This will be included in our next release. I don't have an exact date for that, but I expect it to be early/mid next week. I've also updated the pending changelog for this feature, and the release notes will include details about this change.\n. Hi @hongchaodeng This change is now included with the SDK's release that was cut today. 1.8.23. The release should be published a little bit later today.. Thanks for contacting us @takadasho and taking the time to create this PR and investigate the issue. This issue looks to be related to x/net/http2 not handling the http.NoBody value  golang/go#18891 and looks to be being fixed in Go 1.9.\nLets open this as an issue in the SDK's instead of a PR, and track the change there initially. I don't think we can take a change that modifies the value of the noBodyReader to nil. When the Go 1.8 change was made the SDK intentionally did not use nil for no bodies because of the risk of introducing a breaking change to users of the SDK that accessed the SDK's  Request.HTTPResponse.Body value. Since this value is publicly available, users of the SDK could experience nil pointer dereference panics if they were using the value.\nLets continue the discussion on #1264.. Thanks for creating this PR @ncw, and letting us know about plan9 not supporting syscall. This change looks good. \nI think issues like appengine and plan9 highlight an issue the SDK has with using the syscall package in general. I think this highlights the SDK may need to consider removing syscall pkg in general. https://golang.org/s/go1.4-syscall\n. I agree. I think we can do a string comparison on the error instead of using the syscall package's ECONNRESET. I think this change would pretty small really. I created #1263 to help track this.. When the Go 1.8 change was made the SDK intentionally did not use nil for no bodies because of the risk of introducing a breaking change to users of the SDK that accessed the SDK's Request.HTTPResponse.Body value. Since this value is publicly available, users of the SDK could experience nil pointer dereference panics if they were using the value. This would mean changing the nobodyReader value to nil in request_1_8.go could have unintended fallout.. To workaround this issue I think the SDK could approach this problem from a different angle. The SDK could detect that the Request's Body is a noBodyReader within the Send request handler, and swap out the Request.Body value for a nil, and reset the value before existing the Send function.. Thanks for letting us know about this issue @takadasho. I merged in a fix that allows the SDK to send a nil body when the request should not include a body.. Thanks for contacting us @kr Out of curiosity is your code attempting to put to the bucket as soon as the CreateBucket API call returns? I believe there is a small timing issue that can arise because addressing the bucket by its domain name isn't available yet. e.g. mybucket.s3-us-west-2.amazonaws.com.\nThe SDK provides the WaitUntilBucketExists utility on the S3 service client that you can use to ensure that the bucket is fully created and is available to use. Both the CLI and Console may be performing additional logic hiding the need to wait until the bucket exists, or the timing is just enough to not experience the issue. In my experience the time until a bucket is ready is very very small, but its non-zero. Using the WaitUntilBucketExists method will help you ensure the bucket is ready to be used before its used.\n```go\nsess := session.Must(session.NewSession())\nsvc := s3.New(sess, &aws.Config{Region: aws.String(myRegion)})\n// Create the bucket\nresp, err := svc.CreateBucket(&CreateBucketInput{\n    Bucket: aws.String(myBucket),\n})\n// Wait until bucket is setup and ready to use before using it.\nerr = svc.WaitUntilBucketExists(&s3.HeadBucket{\n    Bucket: aws.String(myBucket),\n})\n// Now the bucket is ready to use.\n```\n. Thanks for the update and clarification @kr. Are you only creating the bucket via the AWS console? Then attempting to upload to it from the SDK, or CLI?\nI've not experienced this issue before. I'll try to reproduce this. What region are you using when creating the bucket? Does this happen every time a new bucket is created, or only some fraction of the time?\n. So after a bit of investigation the redirect issue looks to be a known condition of creating buckets outside of the s3.amazonaws.com endpoint. http://docs.aws.amazon.com/AmazonS3/latest/dev/Redirects.html  \nThe SDK's http client will not follow this 307 redirect because the method is a PUT. and the SDK does set the http.Request.GetBody value that was added in Go 1.8 enabling support for following PUT redirects.. Updating this to a bug since the SDK should definitely support these redirects, and there isn't really a good way to work around this issue.. Thanks for reporting this issue @kr I've updated the SDK to enable support for retrying PUT requests that are redirected. This should resolve the errors that you are seeing. Let us know if you run into any additional issues, or have feedback.. Thanks for verifying this corrects the redirect you were running into.. Thanks for creating this PR, the change looks good. Would you mind adding a test to session_test.go that validates this change.. Thanks for contacting us @cvvs This feature request needs to be implemented by the Amazon S3 service team. The SDK consumes the model produced by the service team. I'll reach out to the service team directly, but it would also be very helpful if you request that the service add this on the Amazon S3 Forums.\nUntil S3 adds support for this to their public API that the SDK consume you can use the following workaround to add the header your self.\n```go\nfunc WithIfNoneMatch(conditions ...string) request.Option {\n    return func(r *request.Request) {\n       for _, v := range conditions {\n            r.HTTPRequest.Header.Add(\"If-None-Match\", v)\n       }\n    }\n}\nsvc := s3.New(sess)\nsvc.PutObjectwithContext(ctx, &s3.PutObjectInput{\n    Bucket: aws.String(myBucket),\n    Key:      aws.String(myKey),\n    Body:    reader,\n    // Other parameters...\n}, WithIfNoneMatch(\"etag\")\n```. I'll close this issue since this is a service API feature, but I'll forward this request onto the S3  service team.. Thanks for letting us know about this issue @lukaf  I'm looking into how to fix this.. Thanks for bringing this issue to our attention @lukaf I've corrected the docs and put validation in place to ensure broken docs won't be deployed again.. Thanks for creating this issue @grubernaut I'm investigating this issue. You're correct that the SDK is treating the error as a throttling which is causing the significant delay before failure. This error shouldn't be treated as a error.. After a bit of investigation it looks like this exception is returned for both throttling rate limiting conditions as well as total number of tables crated reaches a limit.  I think this needs more investigation into how the SDK should handle this error, because I don't think the SDK knows if it can or cannot retry this request without parsing the error message. Paring the error message would be fragile. I'll reach out to the DynamoDB service team to get a better idea how this error should be handled.\n. Hi @grubernaut I created #1276 to address this issue by removing LimitExceededException from the list of error codes that should be throttled.\nSince this throttling on +10 create/delete/update concurrent DynamoDB tasks could still fail due to the limit exceeded exception I do not think this would be a breaking change for users as they would need to have logic handling the error code anyways.. Thanks for the update @grubernaut  Let us know if you find any additional cases, or have feedback on the SDK!. Thanks for correcting the doc reference @slai. The change looks good. Thanks for correcting the typo @BlueMonday. . Thanks for the heads up on this @RendijsSmukulis I think this is an extension of logic added a few weeks ago to retry connection resets.  I think we can add a case to the SDK's retrier logic to catch this case.. Hi @RendijsSmukulis Thanks for letting us know about this issue. I just merged in #1289 which adds support for retrying temporary error such as Client.Timeout that occur when the SDK is reading the response body.  Let us know if you run into any additional issues, or have feedback.. Thanks for reaching out to us @anudeepakumar. This is expected behavior of the DynamoDB Marshalers. In Go lower case struct members are not visible outside of the package that they are defined in. Attempting to access a lower case (non-exported) member or type via reflection will cause a panic.  You'd see similar outcome with other marshaling libraries such as the Go standard library's JSON marshaler.\nOut of curiosity what is the reason you're looking for the fields to be lower case? Is this to ensure the marshaled output keys are lower case as well? If so the best way to achieve this is to take advantage of the struct tags supported by the dynamodbattribute marshalers.\ngo\ntype Record struct {\n    Ad   string `dynamodbav:\"ad\"`\n    Test string `dynamodbav:\"test\"`\n}\nThis is a similar pattern that you'd do for marshaling JSON.\nAlternatively, if the goal is not to have lower case key output there are two ways to work around this issue. the first is like you did in your example. Make the fields exported (title case). The second is to implement the Marshaler interface on the Record type.  Implementing the Marshaler interface is the best way to handle this if you do not want to export the fields. Though if you do implement the marshaler you probably will want to implement both Unmarshaler and Marshaler if you end up reading table records as well.\n```go\nfunc (r Record) MarshalDynamoDBAttributeValue(av dynamodb.AttributeValue) error {\n    av.M = map[string]dynamodb.AttributeValue{\n         \"ad\":   {S: aws.String(r.Ad)},\n         \"test\": {S: aws.String(r.Test)},\n    }\nreturn nil\n\n}\n``\n. Hi @anudeepakumar  i'm going to close this issue as I think it answers the question you had.  Please reopen if you're still having an issue with the marshaler.. @phbcanada Thanks for reaching out to us. You are correct s3manager's Downloader does not return the Metadata, or other non-object body data. The best way to get the Metadata or other attributes is to use theHeadObjectAPI operation if using the S3 download manager to download the object.. Hi @phbcanada Does this answer the question you were having? Let us know if we can help further.. Hi @phbcanada Let us know if you're still having this issue, or we can provide more assistance.. Thanks for the feedback @phbcanada lets open this back up as a feature request. We cannot change the existing method signature without a breaking change. Though I think it is possible to expand the downloader to provide a way at getting the DownloadOutput. . We're also always glad to review PRs. I think in this case, I could see the implementation going one of two ways. Either provide a callback when configuring the downloader that will be executed when the downloaded object's metadata is available, or create a new set ofDownload` methods that return a DownloadOutput value.\nCallback has the benefit of not needing an extra set of methods reducing noise, but not as natural of an interface compared to Uploader and GetObject.. Thanks for the heads up @radeksimko. I'll reach out to the service teams to see if this exception should automatically be retried by the clients. I don't think any of the AWS SDKs currently retry.\nBased on the error message returned I think this makes sense, but will sync up with service team and in other SDKs.. Hi @radeksimko i got feedback from the EBS service team on this exception.  While this exception is a rate limit exception that the clients should self impose throttling for, the minimum time the server recommends for throttling retries starting off with at least 15 seconds. The SDK's base retry throttle delay is 500ms.\nI'll reach out to the other SDK teams to see how they would handles this longer delay. . Hi @radeksimko Sorry for the long delay getting back to you with this. We're investigating more generic ways the SDKs can work with service teams to more accurately respond to service directed throttling delays. We're looking for ways to make this more generic and not require special cases per service and exception.\nSince this exception requires special logic we think handling this by the user of the SDK I think having that customization in code using the SDK is best.. Thanks for the feedback @dlsniper. I think the use case you mentioned is that this API operation would be used by a X-Ray agent(s) built with the public API model.\nI'll forward this onto the X-Ray team about clarifying this usage of this API operation, and if they are able to mark it as internal.. Thanks again for letting us know about this API operation @dlsniper Since the X-Ray service team owns the API model the SDK ingests I've forwarded this request onto them internally. . Thanks for letting us know about this issue @dlsniper. It looks like all of the X-Ray API doc links are resolving to the generic documentation page. I'll forward this onto our documentation team and loop in the X-Ray service team to see if they can fix this. . After a bit of review it looks like this issue applies to all SDKs that generate API reference links in their docs to the X-Ray service API reference.. @dlsniper thanks again for letting us know about this issue. I've forwarded this onto our internal documentation team to address the issue. It \"looks\" like the SDKs are generating these doc links correctly.\nI'll keep this open to track if the SDK(s) need to make any changes for this issue.. Thanks for letting us know about the broken links @dlsniper We've worked with the X-Ray team to get this issue resolved.  The dock likes should all be working now.. Hi @FrenchBen I think the issue that you are seeing is related to the question in #1278.  The SDK is unable to (un)marshal struct members which are not exported. In the example you provided the myField value cannot be (un)marshaled because the lowercase m.  This is the same expectation as Go's JSON marshaling, https://play.golang.org/p/uMR-c85Uet.\nIf your use case requires non-exported members I suggest implementing the suggestion in #1278 for this functionality.. Glad to help, let us know if you run into any additional issues, questions, or have feedback about the SDK :). You can also set the  \"Accept-Encoding\" header to \"gzip\" on the request which will prevent the default transport from decompressing the object. \n```go\nfunc WithAcceptEncoding(e string) request.Option {\n    return func(r *request.Request) {\n        r.HTTPRequest.Header.Add(\"Accept-Encoding\", e)\n    }\n}\nsess := session.Must(session.NewSession())\nsvc := s3.New(sess)\nresp, err := wsvc.GetObjectWithContext(ctx, &s3.GetObjectInput{\n    Bucket: aws.String(myBucket),\n    Key:    aws.String(myKey),\n}, WithAcceptEncoding(\"gzip\"))\n```. @phbcanada Correct the default transport is decompressing the content on the fly as it downloads the object from s3.. Here is an example that highlights what is going on.\n```go\npackage main\nimport (\n    \"bytes\"\n    \"compress/gzip\"\n    \"context\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n    \"net/http/httptest\"\n    \"strconv\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/endpoints\"\n\"github.com/aws/aws-sdk-go/aws/request\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\"github.com/pkg/errors\"\n\n)\n// Example of mock request to s3. Uses mock gzip response.\n//\n// Usage: go run main.go\nfunc main() {\n    server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        b := &bytes.Buffer{}\n        gw := gzip.NewWriter(b)\n        gw.Write([]byte(\"hello there stranger\"))\n        gw.Close()\n    w.Header().Set(\"Content-Length\", strconv.Itoa(b.Len()))\n    w.Header().Set(\"Content-Encoding\", \"gzip\")\n    w.Header().Set(\"Content-Type\", \"plain/text\")\n    w.WriteHeader(http.StatusOK)\n\n    io.Copy(w, b)\n}))\n\nsess := session.Must(session.NewSession())\n\nsvc := s3.New(sess, &aws.Config{\n    Region:           aws.String(endpoints.UsWest2RegionID),\n    DisableSSL:       aws.Bool(true),\n    S3ForcePathStyle: aws.Bool(true),\n    Endpoint:         aws.String(server.URL),\n})\n\nresult, err := svc.GetObjectWithContext(context.Background(),\n    &s3.GetObjectInput{\n        Bucket: aws.String(\"abucket\"),\n        Key:    aws.String(\"akey\"),\n    }, func(r *request.Request) {\n        // Comment out to have transport decode the object.\n        r.HTTPRequest.Header.Add(\"Accept-Encoding\", \"gzip\")\n    })\nif err != nil {\n    panic(errors.Wrap(err, \"get object failed\"))\n}\ndefer result.Body.Close()\n\nb := bytes.Buffer{}\nif _, err := io.Copy(&b, result.Body); err != nil {\n    panic(errors.Wrap(err, \"read response body failed\"))\n}\n\nfmt.Println(result)\nfmt.Println(b.String())\n\n}\n```. Hi @zmarois thank you for finding this bug and putting this PR together. In reviewing this I realized that this bug also applied to the SDK's default credential chain's shared credentials provider on windows as well. \nI created #1308 that takes the changes you made in this PR, and exposing it in a generic way fixing both the SDK Session's usage of SharedCredentialsProvider and using SharedCredentialsProvider standalone.. Thanks for creating this issue @sandoracs. I agree this looks to be an oversight with S3 Download Manager. I think it makes sense for the downloader to use this value.\nSince the value was being ignored before we'd need to figure out how the SDK should handle this field if it is not formatted properly.. In the meantime @sandoracs the S3.GetObject API call will work correctly with the Range parameter. This could be used as a workaround until the SDK's download manager issue is fixed.. In taking a look at this issue, it looks to support this feature the S3 Download Manager would need a pretty significant refactor. To address the issue of taking a slice of an Object I think the downloader can be updated to short circuit multipart download functionality and fallback to using GetObject directly.\nWe can then see if there is demand for full byte-range support within the download manager and prioritize that as a feature request.. Hi @sandoracs I've just merged in #1311 . This basic support for Byte-Range with the S3 manager. Setting this field will disable S3's concurrency support. We can take a look at adding further support for byte range with multipart get object download as a feature request.. HI @timk10 Thanks for contacting us with this issue. I think the best place to help with the filtering issue you're experiencing is via Stack Overflow, or our Gitter channel. In addition the Gopher Slack channel #aws is great for AWS related questions.. Thank for contacting us, and let us know if you run into any issues with the SDK or have feedback.. Thanks for contacting us @myshen. Are you running into an issue with Lambda in GovCloud not being accessible?  The SDK will automatically derive the hostname based on the metadata information in the endpoints file.\nI'm having difficulties reproducing Lambda endpoint not resolving for the GovCloud region. Could you provide more information about the issue you're seeing?\n```go\npackage main\nimport (\n    \"fmt\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/endpoints\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/lambda\"\n\n)\nfunc main() {\n    // Resolve the region's endpoint directly\n    endpoint, err := endpoints.DefaultResolver().EndpointFor(endpoints.LambdaServiceID, endpoints.UsGovWest1RegionID)\n    if err != nil {\n        fmt.Println(\"failed to get endpoint\", err)\n    }\n    fmt.Println(\"Lambda Resolved Endpoint:\", endpoint.URL, \"Region:\", endpoint.SigningRegion)\n// Create a session and service client configured for the region.\nsess := session.Must(session.NewSession(&aws.Config{\n    Region: aws.String(endpoints.UsGovWest1RegionID),\n}))\nsvc := lambda.New(sess)\nfmt.Println(\"Lambda Client Endpoint:\", svc.ClientInfo.Endpoint, \"Region:\", svc.ClientInfo.SigningRegion)\n\n// Output:\n// Lambda Resolved Endpoint: https://lambda.us-gov-west-1.amazonaws.com Region: us-gov-west-1\n// Lambda Client Endpoint: https://lambda.us-gov-west-1.amazonaws.com Region: us-gov-west-1\n\n}\n```. Thanks for the update @myshen. Let us know if your run into any additional issues, or have feedback.. Thanks for the letting us know about this @pwaller We're working with our doc teams to get these links fixed. . Thanks for fixing this @prasad-shinde  the change looks good.. Thanks for contacting us @jadekler. The Java SDK includes this feature as a customization.\nAre you looking for aws-sdk-go to support the MQTT protocol? We have two issues in our backlog tracking this support #820, and #706.. Thanks for the update @jadekler I'll go ahead and close this issue as a duplicate. We'll use the other two feature requests to track these issues. Please feel free to upvote/comment on those issues as this will help us judge demand for the feature.. Thanks for pointing this issue out @dmowcomber. It looks like the model from the Athena service has incorrect documentation.  The best way to get this fixed is to submit feedback on  QueryExecutionStatus API doc page.\nI'll also forward this internally to the service team, but receiving the feedback from you will help prioritize getting these docs fixed.. Because this is a service decimation model issue not the SDK it self i'm going to close this issue. When the service documentation is updated it should be included in a future SDK release.\nThanks again for taking the time to submit this issue.. Hi @anupavanm closing this issue. Please let us know if you're still running into this issue, or have additional questions.. @fxaguessy thanks for reaching out to us. You are correct the aws-sdk-go does not implement support for a cross-process credentials cache. The only caching of credentials the SDK includes is in process via a Session, via the Config's Credentials.\nBeing able to cache credentials cross process for a CLI makes a lot of sense, especially for MFA. I could imagine that being painful to use with MFA and STS. The AWS SDKs do not support the AWS CLI's file based credential cache as that cache is owned as internal functionality of the CLI. None of the SDKs use this cache for that reason.\nI think the best approach to this problem would be to create a cache that is specific to the awless use case. Creating a standalone cache separate from the AWS CLI's cache would be good to avoid being impacted by any breaking change the CLI may make to its cache in the future.\nI think there is some work we can do on the aws-sdk-go to make injecting a cache easier. Today adding a cache on top of the SDK, while also maintaining default credential chain functionality created by the Session, is the following:\n```go\ntype FileCacheProvider struct {\n    Creds *credentials.Credentials\n}\nfunc (f FileCacheProvider) Retrieve() (credentials.Value, error) {\n    // TODO check file credential cache before looking at nested credentials.\n    // Fall back to underlying credentials, and repopulate the cache.\n    return f.Creds.Get()\n}\nfunc (f FileCacheProvider) IsExpired() bool {\n    // TODO check file cache is expired? Fall back to underlying credentials\n    return f.Creds.IsExpired()\n}\nfunc main() {\n    sess := session.Must(session.NewSession())\n// Inject cache able credential provider on top of the SDK's credentials loader\nsess.Config.Credentials = credentials.NewCredentials(&FileCacheProvider{\n    Creds: sess.Config.Credentials,\n})\n\n// create service clients with sessions and make API calls.\n\n}\n```\nThe way the SDK supports the shared config makes wrapping injecting the FileCacheProvider prior to creating the Session very complicated. The above example is probably the best way to do this today after the Session is created.\nThis is an area the SDK can be improved I think. At the minimum if the Credentials type exposed its Provider you'd be able to easily wrap the underlying provider. Wrapping the Provider instead of the Credentials adds a significant benefit as it removes the synchronous locking overhead the Credentials type uses to ensure it is safe to use service clients across multiple goroutines.\n. Thanks for the update @fxaguessy, glad that was helpful.\n\nI'm not sure to have fully understood this:\n\nWrapping the Provider instead of the Credentials adds a significant benefit as it removes the synchronous locking overhead the Credentials type uses to ensure it is safe to use service clients across multiple goroutines.\n\n\nSure, let me clarify that. This is an error the SDK could improve with a minor optimization by exposing the credentials Provider built by the Session instead of needing to wrap the Session's Credentials value. The Credentials type implements locking so it can be used across multiple goroutines safely. By wrapping the Session's Credentials value with another credentials Provider and wrapping that provider in a Credentials it duplicates the locking overhead by the SDK retrieving credentials, and checking if they are expired.. Hi @fxaguessy I created PR #1320 a earlier this week that adds support for Go 1.8's plugin to retrieve AWS credentials from. I think there are use cases where users of awless might find it useful to provide custom credential sources for the CLI to use, without needing to modify the CLI's code. If you get a chance to look at it let us know what you think of the API and its usefulness.\nThe PR includes example and documentation on how to use this new feature. This feature is opt in and needs to be explicitly configured by the application using the SDK.. From a Credentials value you should be able to call IsExpired method to determine if the underlying credentials are expired or not.  Your FileCacheProvider wrapping the SDK's build Credentials value will be able to do this.\nYour application won't be able to know what time the credentials will expire on, it will know if they are expired or not. To handle this when credentials expire you'll need a way to lock the file cache so only a single cli instance will attempt to refresh the credentials from AWS.. Oh i see part of the problem is determining if the cached credentials are expired correct?  . hmm so I don't think the SDK's Credentials or Provider expose the value that is needed here. I think to support this the SDK's Credentials type needs to be updated so Provider is an exported field. If Provider can be retrieved from credentials this would provide the chance at getting an expiry time from the underlying provider. \nWe'd probably want to create a new interface Expirer in the credentials package that a Provider could satisfy if it has the ability to state when its credentials will expire.\ngo\ntype Expirer interface {\n    ExpiresAt() time.Time\n}\nIn this case the stscreds Provider would be updated to satisfy the Expirer interface. . I updated your above example removing the access key and secret key from the example. If those were real credentials please invalidate them immediately. As malicious automation bots will scrape github for credentials.. Are there specific API operations to sa-east-1 or ap-southeast-2 that seem to fail more often than others?  \nA couple comments that might help.\n\nIn v1.8.0 of the SDK added support for context in all API requests. This means you can update the code like the following.\ngo\n                ctx := context.Background()\n                svc := ec2.New(session, aws.NewConfig().WithRegion(region))\n                req, resp := svc.DescribeInstancesRequest(nil)\n                req.HTTPRequest = req.HTTPRequest.WithContext(ctx)\n                req.RetryCount = requestAttempts\n                req.RetryDelay = requestDelay\nCan be updated to be:\n```go\ntype MyRetryer struct {\n     client.DefaultRetryer\n     RetryDelay time.Duration\n}\nfunc (m *MyRetryer) RetryRules() time.Duration {\n    return m.RetryDelay\n}\n\n// ...\n                ctx := context.Background()\n                svc := ec2.New(session, aws.NewConfig().\n                        WithRegion(region).\n                        WithRetryer(&myRetryer{\n                            DefaultRetryer: client.DefaultRetryer{\n                                NumMaxRetries: requestAttempts,\n                            },\n                        }))\n                // Can be updated to\n                resp, err := svc.DescribeInstancesRequestWithContext(ctx, nil)\n```. One thing that could help debug this issue is to turn on debugging for request retries and see if the requests are being retried several times, and eventually fail with an EOF.\nSetting the LogLevel of the Config to the following should log request retries and errors.\ngo\ncfg := aws.NewConfig().\n    WithRegion(region).\n    WithRetryer(myretryer).\n    WithLogLevel(aws.LogDebugWithRequestRetries | aws.LogDebugWithRequestErrors). Thanks for pointing that out @stevenh it looks like the WithRetryer function is not defined on Config like it should be.\nAn alternative way of setting Retryer on the Config is:\ngo\ncfg := &aws.Config{\n    Region:   aws.String(region),\n    Retryer:  myRetryer,\n    LogLevel: aws.LogLevel(aws.LogDebugWithRequestRetries | aws.LogDebugWithRequestErrors),\n}\n. Thanks for reporting this issue @bflad. I investigated this issue and it looks like this API requires the xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/ attribute on MetricsConfiguration. This seems to be more strict than other S3 APIs which no not require this namespace such as PutBucketAcl.\nIt looks like the SDK's code generation for S3's protocol drop's the modeled xmlNamespace attribute for API types. Marking this as a bug so the SDK can prioritize getting this issue fixed in the S3 service client.\nAs a rough workaround you could write the request body directly in a build handler to workaround this issue until the SDK fixes this bug.\ngo\n// PUT S3 Bucket Metrics Configuration\n_, err := svcDebug.PutBucketMetricsConfigurationWithContext(context.Background(), &s3.PutBucketMetricsConfigurationInput{\n    Bucket: aws.String(bucket),\n    Id:     aws.String(metricsID),\n    MetricsConfiguration: &s3.MetricsConfiguration{\n        Id: aws.String(metricsID),\n    },\n}, func(r *request.Request) {\n    r.Handlers.Build.PushBack(func(req *request.Request) {\n        req.SetReaderBody(bytes.NewReader([]byte(`<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<MetricsConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\n<Id>EntireBucket</Id>\n</MetricsConfiguration>`)))\n    })\n})\nif err != nil {\n    log.Printf(\"Error putting S3 bucket metrics configuration: %s\", err)\n}. Hi @bflad thanks for reporting this issue. I've merged in a fix that corrects this issue for S3 API operations. This change is available on tip and will be included in the SDK's next release.. Thanks identifying this bug @deedweird. I'll review this but I think you are correct the error code here should of been MinLen not MinValue.. The change looked good @deedweird. Let us know if you run into any additional issues, have questions, or feedback.. Changes look good @jhwang09 thanks for correcting this example.. Thanks for creating this pr @andrewarrow. The UploadPartWithContext API operation call will by default automatically retry 3 times within the context of the call it self. This pr would effectively make that up to 30 times per part.\nAnother way to extend the number of retries per part is to update the SDK's MaxRetries value. This value will be used for any API operation call to retry that api operation with jitter and exponential back off. \nTo configure the Upload manager to use a larger number of retries per part you can adjust the configuration directly when creation a Session, or alternatively configuring the S3 service client directly. NewUploaderWithClient.\n```go\nsess := session.Must(session.NewSession(&aws.Config{\n    MaxRetries: aws.Int(10),\n}))\nuploader := s3manager.NewUploader(sess)\nuploader.Upload(/.../)\n```\nAlternaitively configure the S3 client instead of creating a new session.\n```go\nsvc := s3.New(sess, &aws.Config{\n    MaxRetries: aws.Int(10),\n})\nuploader := s3manager.NewUploaderWithClient(svc)\nuploader.Upload(/.../)\n``. glad to help, let us know if you have any questions or feedback. also if any parts of the docs could be clarified.  In addition, we have a [Gitter channel](https://gitter.im/aws/aws-sdk-go) for the SDK that is a great place to ask questions/suggestions from others working with the AWS SDK for Go.. Hi @andrewarrow thanks for reaching out to us. The s3manager doesn't support renaming of S3 objects. to do this you'd need to use the CopyMultipart API.  We have feature request #745 to address this issue of renaming large objects in S3. This feature is currently in our backlog.. #745 also describes how you can workaround the SDK not having this feature.. Hi @siredwin thanks for reaching out to us. I think Stack Overflow will be the best place to get suggestions for the use case you're looking to create a solution for.  There is the tag [aws-sdk-go`](https://stackoverflow.com/questions/tagged/aws-sdk-go) on Stack Overflow for questions like this.. HI @siredwin I'm going to close this issue as Stack Overflow is the best place for this question. Let us know if you have any feedback, or issues with the SDK.. Thanks for submitting this issue @stevenh. The change looks good. I think it makes sense for the SDK's signer to give preference to the request.Host over that of the URL if set.. Thanks for submitting and pointing this issue out @stevenh. I think the change makes sense and looks good. . Hi @zach5410 This sounds like your Go workspace's pkg cache is out of date, or needs to be cleaned. Also if your using any vendoring tool make sure the full SDK's content is synced to the same version of the SDK.\nTo clear out your Go package class you can delete the contents of the pkg folder. .e.g rm -r $GOPATH/pkg/*. Hi @zach5410 Thanks again for reaching out to us. I think this issue is solved by correcting the issue in the Go workspace's pkg cache. I'm going to go ahead and close this issue. Please let us know if you have any addition issues, questions, or feedback.. Thanks for reaching out to us @ejholmes I'll bring this request up with our SDK and CLI teams. \nTo clarify your request are you looking to being able to assume a IAM role using the .aws/config to define nested profiles?. Such as In order to get the IAM role creds for dev the SDK would need to get the IAM role creds from ops which would entail getting the credentials from the default profile.\nIn this example the temporary flag would instruct the SDKs to get a session token instead of IAM role credentials, correct?. Thanks for reporting this issue @TalLevAmi I'll convert this use case into a unit test for the  stripExcessSpaces to see if i can reproduce this issue.. Hi @cbroglie Thanks for reaching out to us about this issue. You're correct the ambiguity of this exception prevented the SDK from retrying this error on a global bases. Also an additional ready this exception was removed is none of the other SDKs retry this exception for the reasons mentioned.\nFrom reading the description of the DescribeStream documentation suggests that this exception is used for both too many resources used, and too many concurrent requests for a resource. I'll reach out to the Kinesis team also to get their feedback on the retryability of this exception in the context of the DescribeStream API call.\nAlternatively the following request handler could be added to the Kinesis service client that will enabling retrying of LimitExceededException. \n```go\nsess := session.Must(session.NewSession())\nsvc := kinesis.New(sess)\nsvc.Handler.Retry.PushBack(func(r *request.Request) {\n    if r.Operation.Name != \"DescribeStream\" {\n        return\n    }\n    if aerr, ok := r.Error.(awserr.Error); !ok || aerr == nil {\n        return\n    }\n    if aerr.Code() == kinesis.ErrCodeLimitExceededException {\n        r.Retryable = aws.Bool(true)\n    }\n})\n// Use the Kinesis service client.\n``. Hi @cbroglie Thanks for getting back with us. I've talked about this with other SDKs and service team and the ambiguity of this exception makes it risking for the SDK to retry directly. I think performing this logic directly by the component using the SDK will have the most insight into the context of the request, and if it should be retryable.. Thanks for the feedback @cbroglie. We're investigating how the SDKs can better determine in what circumstances a request can be retried. Currently this is mostly inconsistent tribal knowledge per SDK. We're working with the AWS service teams to provide a better way retries to be more definitive, explicit, and service driven.. Thanks, I'll close this. Let us know if you run into additional issues, have feedback, or feature requests for the SDK. . Opening this issue back up due to #2530. The SDK should be able to support applying retry behavior to specific AWS Services. Its possible that the best solution will be to implement this flexible behavior in the V2 SDK, but it would be good to investigate what options we have for implementing per service retry behavior in the v1 SDK.. In the HTTP Body response you'll see a section from S3 which says why the signature didn't match. Normally this is because the expected headers to be included in the signatures differed, or a value of one of the headers S3 received was not the same value that the signature was calculated with. Once you're able to get the logs for the error we should be able to get some more insight into the issue.. Thanks for the update @sethcleveland let us know if you run into any additional issues, or have suggestions for the SDK.. Thanks for reaching out to us @bboreham. I'll need to do a bit of research on what the impact of adding a.to the end of the fqdn. I don't think any of the SDK's do this today.. Could you go into more detail about how adding the.to the fqdn would help the DNS lookups? I'm not overly familiar with DNS, but I was under the impression that the trailing.was optional, and assumed if not present.. This [doc](http://www.dns-sd.org/trailingdotsindomainnames.html) goes into the details of trailing dot on.` fqdn to make them absolute.. Thanks for the update @bboreham I'm reaching out internally to our service teams to to investigate the impact of this on these services. I'll update once I get more feedback.  Initially it looks like this would be a good change for the AWS SDKs to implement.\nSince a few AWS services use DNS with their load balancing, e.g. S3, I want to make sure this won't have any negative impacts on those services. I don't expect there to be, but want to verify just in case.. I did a bit of research and testing, and it looks like using absolute FQDN across the board isn't compatible with all services out of the box. Of the services tested via the SDK's integration tests S3, SES, IoT, and APIGateway encounter errors which looks to be due to issues parsing the request's Host header value.\nWith that said an immediate workaround you can do is implement a endpoints.Resolver interface and configure the SDK to use that custom resolver to append the trailing dot to the SDK's endpoint hostnames.\n```go\nsess := session.Must(session.NewSession())\nsvc := ec2.New(sess, &aws.Config{\n    EndpointResolver: endpoints.ResolverFunc(func(service, region string, opts ...func(*endpoints.Options)) (endpoints.ResolvedEndpoint, error) {\n        e, err := endpoints.DefaultResolver().EndpointFor(service, region, opts...)\n        if err != nil {\n            return endpoints.ResolvedEndpoint{}, err\n        }\n        e.URL += \".\"\n        return e, nil\n    }),\n})\n```\nThe Config's EndpointResolver can also be applied to the session directly and used for multiple AWS service clients, but I wouldn't suggest using this across the board because not all services support trailing dots in the request's Host header value.\nCheck out our blog post on using the endpoints, and endpoints package for more information. . Hi @bboreham could you please go into more details about how significantly this issue is impacting your application? Are you seeing DNS lookups against the search list for ever API request?\nIn addition, do you know if the DNS lookup results are being cached or not?  I don't believe Go's HTTP client caches DNS lookups and instead rely on connection keep alive.  If you're seeing a very small number of keep-alive connections you could try to configure a custom http.Client with a larger number of keep-alive connection's per host. By default this pool is 2 connections.. Thanks for the update @bboreham. In the mean time with the example above you should be able to instruct the SDK to use FQDN for service's at your choosing. . Hi @bboreham since not all of the services support this feature, and can trigger signature errors We cannot add this to the SDK. The example provided above is the best way to implement this feature locally if it is needed by your application.. Thanks for reporting this documentation issue @dowlingw we've sent this to the service team, and they will work on correcting the documentation. When the doc updates are available the SDK will automatically pick them up.. The S3 Upload manager will automatically switch to multipart mode when the content to upload is larger than Uploader.PartSize which defaults to 5 MB. \n. I created #2453 PR to address this issue of inconsistent URL locations being returned. The URL value returned is updated to use the same pattern as the Single part upload URL.. Thanks for the update @the1337beauty and @BastienM it looks like the CognitoIdentityProvider service does not define paginators for their service in the models provided by them to the SDKs. This lack of information is why the request.Pagination type isn't working. The request.Pagination type requires additional information to know how to paginate the APIs, which in the case of this service is not modeled.\nI've reached out to the service team asking them to add information to their model that provides pagination information.\nThe following is an workaround you can use which will add pagination metadata to the ListUsers API so you can use the SDK's provided request.Pagination utility. The example adds a NewPageableListUsersRequest helper which augments the request returned by ListUsersRequest to include pagination metadata. The getUsersFromPool function was also updated to use this helper.\n```go\nfunc getUsersFromPool(pool cognitoidentityprovider.UserPoolDescriptionType) (cognitoidentityprovider.ListUsersOutput, error) {\n    var users []*cognitoidentityprovider.UserType\nparams := cognitoidentityprovider.ListUsersInput{\n    UserPoolId: &*pool.Id,\n    Limit:      aws.Int64(50),\n}\nctx := context.Background()\n\np := request.Pagination{\n    NewRequest: func() (*request.Request, error) {\n        req := NewPageableListUsersRequest(CognitoClient, &params)\n        req.SetContext(ctx)\n        return req, nil\n    },\n}\n\nfor p.Next() {\n    page := p.Page().(*cognitoidentityprovider.ListUsersOutput)\n    users = append(users, page.Users...)\n}\n\noutput := &cognitoidentityprovider.ListUsersOutput{}\noutput.SetUsers(users)\n\nreturn output, p.Err()\n\n}\n// NewPageableListUsersRequest returns a new AWS SDK request constructed for the ListUsers API which can be paged over.\nfunc NewPageableListUsersRequest(svc cognitoidentityprovideriface.CognitoIdentityProviderAPI, in cognitoidentityprovider.ListUsersInput) request.Request {\n    req, _ := svc.ListUsersRequest(in)\n    if req.Operation.Paginator != nil {\n        return req\n    }\nreq.Operation.Paginator = &request.Paginator{\n    InputTokens:  []string{\"PaginationToken\"},\n    OutputTokens: []string{\"PaginationToken\"},\n    LimitToken:   \"Limit\",\n}\n\nreturn req\n\n}\n```. I'll make the testing updates in a follow up PR, thanks again for fixing this issue.. Thanks for reaching out to us @senthilmk. Could you provide a short code sample where you can reproduce the issue?\nFrom the error message it looks like your code is defining a variable named s3 that conflicts with the S3 package. Check the return variable name of the s3.New function to see if it's being set to s3. Doing this will prevent using any type defined in the S3 package, which I believe is the error message you are seeing.. Thanks for the update, glad that worked for you.. Hi @artong0416, thanks for reaching out to us.\nIn reviewing your sample I think the issue can be resolved by changing the contentType value to text/html; charset=utf-8. The. contentEncoding value should no longer be needed then. The trailing utf-8 portion of contentType should provide the outcome your looking for.. Thanks for the update. Let us know if you run into additional issues, or have feedback, please let us know.. Thanks for reporting this issue @amerine. Where you able to reproduce this with the latest v1.10.10?  Several bug fixes have gone into correcting the SDK's functionality with retries, and error handling. there were a few cases in older SDKs where in some cases the HTTPResposne value would be nil. This should no longer be the case.. Thanks for the update. Let us know if you still encounter this issue after updating.. Thanks for reporting this bug @bennycao. Looked to be a bug with the stripToIdx value being computed wrong. The value should be based on the original size of buf before cut, not the idx.. Thanks for the suggestion @bennycao. I've modified the PR to use this with a minor tweak to stripExcessSpaces. Interestingly enough doing the field join doesn't seem to actually have any negative performance hit.\nAlso updated the stripExcessSpaces to modify the passed in array inline to remove the additional allocs for the new string slice. Since this is an internal function and only used in a specific use case I think this makes sense.. Actually I take that back, I had the benchmark setup incorrectly. fields join performances about twice ( as many objects as doing the replace manually. sorry for the noise.\nBenchmarkStripExcessSpaces-4             1000000          1793 ns/op         368 B/op         13 allocs/op\nBenchmarkStripExcessSpacesSimple-4        300000          4734 ns/op         976 B/op         28 allocs/op. Thanks for reporting this @bennycao I've submitted a changed and merged into master. This will be included in our next release.. Thanks for reaching out to us @cdelguercio. I'm taking a look at this test case. Do you experience the issue if the PutObjectInput's ContentLength value is not set at all?  I would expect this to be the same as setting it to 0.. Thanks for the update I've been able to reproduce the issue and trying to figure out if this is just an issue in how the presigned URL is being used or a bug in the SDK's signing.. After a lot of experimentation I realized the error I was \"duplicating\" was because I was using POST instead of PUT in the HTTP method of my new request.\nThe following code example is what i did to create and send a pre-signed request.\n```go\n    svc := s3.New(sess)\nsdkReq, _ := svc.PutObjectRequest(&s3.PutObjectInput{\n    Bucket: aws.String(bucket),\n    Key:    aws.String(key),\n})\n\nu, header, err := sdkReq.PresignRequest(15 * time.Minute)\nif err != nil {\n    panic(err)\n}\n\nreq, _ := http.NewRequest(\"PUT\", u, nil)\nfor k, vs := range header {\n    for _, v := range vs {\n        req.Header.Add(k, v)\n    }\n}\n\nb, _ := httputil.DumpRequest(req, true)\nfmt.Println(\"Request\")\nfmt.Println(string(b))\n\nresp, err := http.DefaultClient.Do(req)\nif err != nil {\n    panic(err)\n}\n\nb, _ = httputil.DumpResponse(resp, true)\nfmt.Println(\"Response\")\nfmt.Println(string(b))\n\n```. Thanks for clarifying that @cdelguercio. It looks like S3 require's the ContentLength value to be set if a body is used. If the body is empty the ContentLength does not need to be set.\nNot setting the content length when creating the pre-signed URL just prevents it from being included in the signature. Not being included allows the user of the pre-signed url to provide a PUT with any length content.. Oh also, make sure to set Request.ContentLength not the Content-Length header.  I noticed that setting Content-Length header is ignored for these kind of requests.. Ah ok I think i figured out the issue. In Go 1.8 if a content length is not set the request will be sent with Transfer-Encoding: chunked. S3 does not support chunked transfer encoding. To prevent this you must set the http.Request.ContentLength value to the length of the content you want to put to S3.. Great!, let us know if you have additional questions or feedback about the SDK.. Hi @tejasmanohar the will not prebuffer the GetObject's body response. The io.ReadCloser is going to be the read closer of the http.Response.. Thanks for putting this change together @stevenh. The change looks good. We'll pull this in, and will be included in our next release.. The best way to add AssumeRole credentials to a session is to set the Session's Config.Credentials value to the assume role provider credentials.\ngo\nsess, err := session.NewSession()\nsess.Config.Credentials = stscreds.NewCredentials(sess, myRoleARN)\nThe service clients created with the session will now use the STS AssumeRoleProvider to retrieve credentials.\nThis pattern assumes that the NewSessionis able to load credentials from the shared credentials file. (default ~/.aws/credentials). The AssumeRole will use these credentials.  If you need to control the profile that is loaded you can use the environment variable AWS_PROFILE or specify the profile with NewSessionWithOptions session constructor via the Options.Profile member.\nThis pattern does not use the shared config file though. (default ~/.aws/config).  Both can be used together, but you'd need a way to optionally do the sess.Config.Credentials = stscreds.NewCredentials(sess, myRoleARN)  If the ARN needs to be explicitly provided.. Thanks in this case i think we can close this issue, and use #1436 for further discussion. How does that sound?. Thanks for reaching out to us @Baberrehman. I'm a bit unsure by the example used presented. Are you using a utility to load JSON into the [ChangeResourceRecordSetsInput](http://docs.aws.amazon.com/sdk-for-go/api/service/route53/#ChangeResourceRecordSetsInput) type before making the API call?\nTake a look at a similar issue reported with the boto/boto3#612 library.  It looks like TTL is an optionally required parameter in this context.\nThe ChangeBatch field of ChangeResourceRecordSetsInput of type Change has a list of the fields that are required for the action types.\n. Also this is a great topic to ask on Stack overflow, as you'll have access to a wider audience. That may be able to provide a more specific answer.. Hi @etsangsplk since this is a service API question, i'm going to close this issue. Check out the stack overflow link above for service API questions.. Hi @Baberrehman thanks for reaching out to us. I'd check that the version of the SDK you're using is correct.  These two API operations were added to the SDK in v1.0.3\nCould you go into more details about the issue you're experiencing.. Thanks for the update I'll go ahead and close this issue, and take a look at the other one.. Hi @TylerBrock thanks for putting this PR together. I agree having this utility would be helpful for the int64 -> time use cases.. Thanks for putting the PR together @TylerBrock its been pulled into master the next release tag will include this change.. Thanks for updating the names. Would you mind adding a test case for each helper utility to the convert_types_test.go file.\nIn addition one minor tweak to the wording of the functions' doc comment would be to include that seconds and milliseconds are since Epoch. Just to make sure thats clear.. Hi @etsangsplk I think the best way to approach this issue is with what is described in aws/aws-sdk-go#1427 to optionally create a stscreds.AssumeRoleProvider and configure the session to use that when the ARN is precent in an external location.  It looks like the GetSessionByRole function should be doing this.. I noticed the GetSessionByRole's NewSessionWithOptions doesn't set SharedConfigState like getSessionByProfile is this intentional?. Thanks for the update. If the Session returned by GetSessionByRole should use the shared config (default ~/.aws/config) file for configuration then yes SharedConfigState should be set. But if the Session returned by GetSessionByRole doesn't need any information out of the config file then setting the shared config option is not needed.. @etsangsplk I noticed from the sample code you provided the line \ngo\n    // Create the credentials from AssumeRoleProvider to assume the role\n    // referenced by the role ARN. Prompt for MFA token from stdin.\nI think the line should also set the SerialNumber and TokenProvider values in order to work correctly with MFA.\ngo\nnewCreds := stscreds.NewCredentials(sess, roleArn, func(p *stscreds.AssumeRoleProvider) {})\nNeeds to be updated to:\ngo\nnewCreds := stscreds.NewCredentials(sess, roleArn, func(p *stscreds.AssumeRoleProvider) {\n        p.SerialNumber = aws.String(serialNum)\n        p.TokenProvider = stscreds.StdinTokenProvider\n})\nThe SDK will not automatically get the MFA token from stdin without setting the TokenProvider directly.. I believe the The security token included in the request is invalid is in relation to the MFA token being invalid, old or not matching the serial number the AssumeRoleProvider was configured to use. Do you receive this error for both the testprovider and admin or only one of them? Are both profile IAM roles setup to require MFA?. Thanks for the update @etsangsplk. In your original test example does the following error condition occur with sessB or sess2?\ngo\n// Following lines also errored out about invalid security token\ncredVal, err = (*s.Config).Credentials.Get()  <-- FAIL \nassert.NoError(t, err)\nIn addition to help understand the issue, Is it correct that calling GetSessionByRole will initialize a session with iamProfile as the AWS shared config/credentials profile\nIn reading through the code I would expect 's call to getSessionByProfile to of created a Credentials with the  IAM role if role_arn is in the profile. Is the purpose of GetSessionByRole to use the credential provider from the profile with an additional IAM role specified by roleArn?  If the role_arn in the AWS shared config file is not valid this would cause the session created in getSessionByProfile to have an invalid Credential profile which will fail to assume the role. When GetSessionByRole is called for an iamProfile that has the role_arn the stscreds.NewCredentials will be created with a Session with invalid credentials. This would explain why the assume role error is being received.. To work around this your code would need to disable using the SharedConfigState state for a profile that has an invalid role_arn value in the AWS shared config. Setting SharedConfigState to session.SharedConfigDisable will prevent the SDK from using the assume role. This is needed because assume role credentials has precedence over the shared credentials file.. Hi @etsangsplk are you still running into this issue?  Did the above description help the use case you were looking to implement?  Let us know if there is additional information we can provide.. Please reopen this issue if you're still running into issues with implementing the shared config with iam roles across multiple profiles.. Hi @meirf thanks for contacting us. Could you provide an example of the keys that you're running into an issue with?  This would greatly help is investigating the issue if you're able to provide an example of object key's that seem to be problematic.\nThe only thing I think could be causing an issue with the example code above is the path.Join. This function will strip off trailing / characters from the joined value. If any of the objects in your bucket have a trailing / this would be stripped by path.Join and could be the cause of the NoSuchKey error. In addition to cleaning duplicate side-by-side / characters.. In addition I suggest checking how the CopySource value is built. From the CopyObjectInput docs the CopySource value must be URL-encoded. I don't think the SDK will do this automatically for this field.  I'd need to investigate to verify this though.. Thanks for the additional information. I think part of the issue for those is that the CopySource value must be URL-encoded.. Glad to help, let us know if you have any additional feedback, issues, or questions with the SDK.. @zbintliff In addition the SDK also has a higher level helper in the s3manager package called GetBucketRegion that simplifies the work of getting a bucket's region. This utility also simplifies working with the different error/response messages S3 can provide base on the bucket's location.. @Bankq Is the account that you're using the owner of the S3 bucket, or just has read permissions on the bucket?  From your description it sounds like there might be a policy that is preventing the GetBucketRegion utility from getting the bucket's region. The GetBucketRegion utility uses the HeadBucket API call. If there is a policy that restricts the access of HeadBucket for anonymous requests this would block GetBucketRegion from working.\nCould you use the following example to identify how the API request is being made with the service response:\n```go\npackage main\nimport (\n    \"context\"\n    \"fmt\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3/s3manager\"\n\n)\nfunc main() {\n    sess := session.Must(session.NewSession(&aws.Config{\n        LogLevel: aws.LogLevel(aws.LogDebugWithHTTPBody),\n    }))\nctx := context.Background()\nregion, err := s3manager.GetBucketRegion(ctx, sess, \"<yourBucket>\", \"<regionHint>\")\nfmt.Println(region, err)\n\n}\n``. @zbintliff a lot of times misconfigured cross-account policies can cause GetBucketLocation on to work. I suggest using thes3manager.GetBucketRegionhelper utility to get a bucket's region. It has custom handling for the cases S3 sends back a 301 message without a location name. Which breaks the default Go http client's transport if follow redirects is enabled).. Thanks for putting this PR together @lostick and correcting type typos. Would you mind trimming this PR down to justaws/context_1_6.goandaws/session/env_config.go. The other files modified are code generated and I'll create a separate PR with those changes.. Thanks for making the update. I'll create a separate PR that includes the generated code typos.. Thanks for pointing this out @etsangsplk. Yeah this value should definitely be a constant. If you're looking to create a PR we'd be glad to review it.  In this case constant name ofEnvProviderNameprobably is good.  We probably can do this same for the static credential provider in the same package as well if its doesn't already use a constant.. Hi @etsangsplk I think theSessiondocs on [Environment Variables`](http://docs.aws.amazon.com/sdk-for-go/api/aws/session/#hdr-Environment_Variables) will help with this question on environment variables and configuration file support parameters.\nIn addition we also have a gitter channel specifically to the aws-sdk-go that is good to ask questions like this.. With that said I'll take a look at the share credential provider to see why the difference is there.. I think the confusion is because there are multiple environment variables that are supported like you mentioned for backwards compatibility, but the shared credentials and shared config files only support aws_secret_access_key. Shared Config Fields outlines the fields that are supported by the SDK.. Thanks for getting in contact with us @etsangsplk. This issue looks to be resolved. Please let us know if you have additional questions, or feedback for the SDK.. Thanks for putting this PR together @glasser. Will be good to add support for fractional seconds. Will take a look at the PR and post feedback.. Thanks for the update :). Thanks for contacting us @dfuentes77 I've been experimenting with the dep project for annotating which dependencies the SDK needs. Before we do this we're looking at ways of simplifying the SDK's code generation dependencies to not pollute the SDK's dependence list.. I created a PR #1544 which adds the dep metadata files to the SDK. This will allow applications to correctly manage the SDK's dependencies in the context of the application.  I think this will resolve the issues experiences with the un-versioned vendor folder.. Thanks for reporting this @mwhooker we've let the EC2 service team know about this issue. We can go ahead and close this issue. When the EC2 release this feature it will be included in a future release of the SDK.. Thanks for putting these changes together, and using the SDK's code generate to update the associated files.. Hi @lucas-rudd thanks for reaching out to us. I think this is a great question for the Lambda AWS Forums. This will be the best place to share your comments about the Lambda API, and get feedback from other users of the service.\nI'm going to close this issue, as the best place to give feedback on the Lambda API will be its AWS Forums.. Hi @etsangsplk Thanks for reaching out us with this question. I think the best place to get more details on the expectations for ListPolicy is stack overflow and the IAM AWS forums. Hi @etsangsplk since this is a service API question, i'm going to close this issue. Check out the stack overflow and AWS forums links above for service API questions.. Thanks for the update @etsangsplk. The best way to request this feature would be on the IAM AWS forums. This is where issues with the IAM API are aggregated and reported to the teams responsible for the service. The SDK is built from models defined by the AWS services.. Hi @sharabash Thanks for contacting us. If you still are running into this issue, please reopen. It would be great to also provide additional detail information about the issue.. Thanks! Would you mind adding a // +build go1.7 build tag to the top of the test file so the t.Run call can stay in. This should let the test succeed for Go 1.6 and 1.5.. Thanks for reaching out to us @mscansian. Let us know if you have additional questions or feedback for the SDK.. Hi @alexbilbie thanks for letting us know about this issue. It looks like the metadata is being added to the URL, but it is also being included in the signature of the request. This should be investigated and see if this is a bug in the signer for presigned requests.\nA workaround in the short term is that the down stream client that will use the presigned URL can add the \"x-amz-meta-user-id: 12345\" header in addition to the header \"x-amz-content-sha256: UNSIGNED-PAYLOAD\" and that is already being added by the down stream client. This workaround still allows you to specify additional data, but at the cost of the down stream client specifying the fields also. The field specified by the client must match the value in the header.. @alexbilbie after investigating this issue a bit the issue is caused by using Presign vs PresignRequest.  In your case I think using Presign would accomplish what you're looking for.\nPresignRequest will return a presigned URL, but all of the requests headers will be signed and not added to the URL.  Looks like the docs for this method need to be improved.. Thanks for the update. For Presign method of generating the URL are you seeing signature errors response from s3?\nIn investigating this we did discover an issue with sending the metadata over as query parameter with title casing causes their values to not be set on the S3 object's metadata. e.g X-Amz-Meta-User-Id vs x-amz-meta-user-id.. Thanks for the update I'm investigating the x-amz-meta- issue. But for not this seems to be a bug with S3 not handling metadata values hoisted to the query strings for presigned URLs correctly.  I created #1469 which will resolve the need for the downstream client to add x-amz-content-sha256 in its usage of the presigned URLs.. Hi @alexbilbie I've submitted a change in #1469 that removes the need of setting X-Amz-Content-Sha256 header in a presigned URL.\nThis does not solve the problem of X-Amz-Meta-* headers needing to be included in headers though. Specifically because of the issue with S3 not accepting metadata fields correctly via the query string. Only via headers can metadata data be reliably set without modifying casing of the key in the query string.\nI'll reach out to S3 about the metadata query string issue letting them know. I don't think the SDK should be modified if the service can correct this issue.. Removed the bug tag as #1469 corrects the issue with the content sha. Metadata should be included as a header for now, until S3 is able to correct the query string issue.. The AWS SDKs for PHP and JS look to be sending the metadata name as lower cased as the query string key. This is how I discovered the issue where S3 does not correctly handle mixed case metadata name query string keys.\nIn addition, while also investigating this issue I discovered that if unicode characters are used for metadata values the value stored on the S3 object is the URI escaped value not the unescaped value the user intended to send. I'm reaching out to S3 to see how these issues can be resolved.. It looks like S3 does handle non-lowercase querystring keys. This is one thing preventing to SDK from sending metadata tags via the querystring.  In addition we discovered that UTF-8 metadata values are represented differently in S3 if they are sent via Header vs querystring. We recommend only sending metadata via the Header which is what the SDK currently does.\nBecause of these items I don't think the SDK can move the metadata from the Header to querystring, as S3's handling those would institute a breaking change in functionality.\nLet us know if you have any feedback, or additional questions about this issue.. @stack72 The SDK does not perform any of validation on these fields, other than to ensure that the field is set with a value. But, it does not validate the content of the field.\nFrom the service's official API reference docs aurora is the only valid value at the moment.  I don't thinks this API can be used to create postgres as a cluster. It looks like only CreateDBInstance can create postgres databases.. I heard back from RDS and they said to use aurora-postgresql as the engine value. They will work to improve the documentation.. Thanks for the update @stack72. I've forward this to the service team. Hopefully this will help them clarify if, or how, a postgres db can be used with RDS cluster.. Ah ok, thanks! This feature probably requires your AWS account be whitelisted to enable it. I don't expect any change to the SDK unless there are new API operations or parameters needed for this feature. In that case RDS's preview process should be able to provide you with preview builds of the SDKs including features.. This feature will be enabled automatically in the SDK once RDS releases the feature for general availability. I'm going to close this issue as there is no change needed to the SDK to support this future RDS update.. @kylegato thanks for the clarification. I think the best way to accomplish your use case is to use the SDK's request handlers. All SDK Requests have a Handlers value that can be used to add additional functionality to the request's processing.\nIn this case you'd want to add a handler before the v4 signature is computed, and additional handler after the signature is computed.\nIf the proxy endpoint, and S3 endpoint will always be the same this logic will be needed for all S3 requests you can apply these handlers directly to the service client.\n```go\nsess := session.Must(session.NewSession())\nsvc := s3.New(sess) \nsvc.Handlers.Sign.PushFront(func(r request.Request) {\n    r.HTTPRequest.URL.Host = realS3EndpointHost\n})\nsvc.Handlers.Sign.PushBack(func(r request.Request) {\n    r.HTTPRequest.URL.Host = myProxyEndpointHost\n})\n// Use S3 service client\n```\nIf the proxy endpoint is dynamic, or you want to ensure the SDK's built S3 endpoint is used you could do the following per request with an anonymous function passed in as the variadic arguments to the __WithContext methods of the service client\n```go\nsess := session.Must(session.NewSession())\nsvc := s3.New(sess)\nresult, err := svc.GetObjectWithContext(ctx, &s3.GetObjectInput{}, func(r request.Request) {\n    var origHost string\n    r.Handlers.Sign.PushFront(func(r request.Request) {\n        r.HTTPRequest.URL.Host = realS3EndpointHost\n   })\n   r.Handlers.Sign.PushBack(func(r *request.Request) {\n       r.HTTPRequest.URL.Host = origHost\n   })\n})\n```\nThe anonymous function could be moved to a helper function to reduce code duplication. request.Option \n```go\nfunc RequestSignWithHost(newHost string) func(r request.Request) {\n    return func(r request.Request) {\n       var origHost string\n       r.Handlers.Sign.PushFront(func(r request.Request) {\n           r.HTTPRequest.URL.Host = newHost\n       })\n       r.Handlers.Sign.PushBack(func(r request.Request) {\n           r.HTTPRequest.URL.Host = origHost\n       })\n    }\n}\n//...\nsess := session.Must(session.NewSession())\nsvc := s3.New(sess)\nresult, err := svc.GetObjectWithContext(ctx, &s3.GetObjectInput{}, RequestSignWithHost(realS3EndpointHost)). Hi @kylegato are you still running into this issue? The solution provided i think is the best way to support this use case. I'm going to go ahead and close this issue, but please re-open if you're still having issues setting up a proxy with the SDK.. Thanks for letting us know about this issue. The best way to update the docs for the AutoScaling service is to submit feedback on the services API reference guide. The bottom right corner of the page has a \"Feedback\" button that will submit your feedback directly to the AutoScaling service.\nThe SDK is built from models provided by the services. We don't have direct control over the documentation of the services or their APIs. I can forward this internally to the AutoScaling service, but you submitting feedback on the API reference page probably will have a quicker resolution.. Thanks for letting us know about this issue @stack72 AutoScaling team has been updated, and alerted to the issue with their documentation. The updated docs will be included in the SDK in a future update.. Thanks for updating the CR @piusranjan I've taking your changes with minor tweaks to #1492. I'll close this PR and merge in the new one.. In addition @matthewmueller once the endpoint is constructed its generally treated as a black box by the SDK. Primarily, because there is no strict format that is required. Some services multiple different endpoint formats such as S3 like you referenced in the example. But other services have no region component and are considered global such as sts.amazonaws.com.\nWhat is the use case that you're looking to solve? I'm curious if there is another way to address the problem without parsing the endpoints URLs.. Thanks for the update @matthewmueller. I strongly recommend not passing the AWS access key and secret in the URL. These credentials could easily be logged by the proxy or intermediate host unexpectedly leaking the credentials.\nWhat is the motivation behind wanting to do a remote signer with credentials passed to it? Would it be viable for the proxy to maintain the credentials internally? This would reduce the risk of credentials being leaked in transit.. Hi @matthewmueller since parsing the endpoint URL is not supported by the SDK, and there isn't a definitive format for the URL hostname I don't think it would be a good idea for the SDK to bake in this functionality which may break based on service hostname patterns.. @matthewmueller thanks for the feedback. At 106 services most will follow the general pattern, but the SDKs do not have a contract on how these endpoints in their various forms and level's of opaqueness could be deconstructed. The endpoints are initially treated opaquely once they are constructed allowing service teams the flexibility to create endpoints based on the service's use case and needs.\nServices can create endpoints that do not match the general format. In some cases such as IoT DataPlane the endpoint can be completely opaque. IoT DataPlane uses an endpoint provided by IoT's DescribeEndpoint API call. The format of this endpoint is opaque. Other services such as S3 have several components that optionally can make up the endpoint such as <bucket>, accelerate, dualstack, and optionally no region for case us-external-1.\nIn a proxy/remote signer usecase would it be feasible to extract the region and service name from the SDK's request before it is sent to the proxy?  Including the region as a query parameter.\n. Glad to help. You might also want to checkout the SDK's gitter channel where several users of the Go SDK will ask questions and discuss designs with each other. In addition there in the Go slack #AWS channel which commonly will discuss AWS use cases. If you're not already apart of the Go slack you can join via https://invite.slack.golangbridge.org/. Thanks for putting this PR together, verifying the v4 signature, and correcting the test case.. Hi @sleavitt Thanks for reaching out to us. This PR was reverted in v1.10.36 due to the undocumented expectation of the v4 signature to sort both query string key and value.\nAre you running into an issue with the SDK's current version (v1.14.29)? If so please open a new issue.. Thanks @stevenh. I would of expected the spec and test suite to align, but in cases where they do not I can work with our internal teams to investigate and clarify the discrepancies.\nI think a big area the SDK is lacking is that it does not run tests against the official test suite. We need to correct this and add the test suite validation to the SDK.\nFor #1494 I'll take a look that again today to see if it needs to be reverted. Did you discover services/APIs that break if the query values are not sorted as well?. I'm going to revert #1491 for now @stevenh until a clearer picture of the signing requirements are available. This work should also include integrating the test cases into the SDK.. Thanks for the additional information @stevenh. I'll raise the duplicate query parameters issue with S3 as well. Even if none of the APIs are lists, I wouldn't expect the service to invalidate the signature because of it.. After reviewing this the SDK is going to continue to sort the query values instead to the query fields when calculating the AWS Sig v4 Signature. While the Sigv4 signing spec does not explicitly state that the query values need to be sorted in addition to the keys, the test suite does exercise this expectation.\nFor the duplicate query parameters in S3, there doesn't seem to be any API operation which could trigger this from the SDKs.  I suggest raising this issue further on the AWS S3 Forums.. Thanks for letting us know about the typo @Ninir. The best way to get this fixed is to submit feedback on the Elastic Search API reference guide documentation. In the bottom right corner of the page there is a \"feedback\" link where you can submit documentation corrections.\nOnce the service corrects their documentation the SDKs will be updated as well in a future release.. Thanks for the information @ramvasanth. Do you have a small sample snip-it that is able to reproduce the issue? Also, are you able to reproduce this in isolation from the database component?\nIn addition, it would be very helpful if you could provide the configuration the application is using for the http.Client. Is a http client being provided to the SDK via config? If not the SDK will use the http.DefaultClient value.. thanks for the update @ramvasanth. I'll take a look at see if I can reproduce this issue. . @ramvasanth if you could provide a small sample that reproduces the problem without the SQL database it would be very helpful. I created the following sample below but wasn't able to reproduce any issue with outstanding ports on an OSX system.\n```go\npackage main\nimport (\n    \"flag\"\n    \"fmt\"\n    \"os\"\n    \"time\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/sqs\"\n\n)\nfunc main() {\n    sess := session.Must(session.NewSession())\nvar queueURL string\nflag.StringVar(&queueURL, \"q\", \"\", \"The `queue` URL the script will poll from.\")\nflag.Parse()\n\nsvc := sqs.New(sess)\n\nt := time.NewTicker(1 * time.Second)\nfor {\n    resp, err := svc.ReceiveMessage(&sqs.ReceiveMessageInput{\n        MaxNumberOfMessages: aws.Int64(10),\n        WaitTimeSeconds:     aws.Int64(1),\n        QueueUrl:            aws.String(queueURL),\n    })\n    if err != nil {\n        fmt.Fprintf(os.Stderr, \"failed to get messages, %v\", err)\n        os.Exit(1)\n    }\n    <-t.C\n\n    fmt.Println(\"Responses:\", len(resp.Messages))\n}\n\n}\n```\nWhen I output losf for the sqsGetMessages process the example was run as I only see a single entry for SQS, with no pending TCP connections.\nCOMMAND     PID   USER   FD     TYPE             DEVICE  SIZE/OFF     NODE NAME\n...\nsqsGetMes 49269 jasdel    3u    IPv4 <device>       0t0      TCP <mylocalhost>:64089->us-west-2.queue.amazonaws.com:https (ESTABLISHED)\nsqsGetMes 49269 jasdel    4u    unix <device>       0t0          -><device>\nsqsGetMes 49269 jasdel    5u  KQUEUE                                       count=0, state=0xa\n.... Hi @ramvasanth I've not been able to pre-produce this issue. If you're still encountering this issue, it would be very helpful to have a sample code using the SDK in isolation.. Thanks for the update @ramvasanth, let us know if you run into any additional issues, or have questions/feedback.. Thanks for putting this PR together @rgarcia. The change looks good. I made a minor tweak to this PR to cache thereflect.TypeOf(time.Time{}) calls as a minor performance improvement.  I'll pull this change in via #1520.. thanks for reaching out to us @atsushi-ishibashi. Did the WaitUntilTasksStopped API call never return, or does it take a few seconds later?  I would expect it to exit after a specific expired time if the status of the task has not been determined to be successful. \nSometimes these status can be cached by the service, and can take several seconds longer to show up to the SDK using the APIs.\n. We've seen cases in the past were multiple calls to an ECS task state will cause the response to be cached server side. In some cases I believe up to of minute, if several requests to the state are made within a short period of time. In this case are there multiple clients checking for the state of this task concurrently, or only a single client checking?\nAre you able to reproduce this condition in a standalone example. Starting a single task then using the waiter to wait for it to finish? . Thanks for the update @atsushi-ishibashi Looks like i was thinking for EC2 where the state responses could be cached (#364), but in that case it was without using the waiter. \nIf possible would you be able to enable debug logging of the response. You can do this with the following change to https://github.com/atsushi-ishibashi/influencer/blob/master/svc/ecs_client.go#L102.\ngo\nreturn ec.WaitUntilTasksStoppedWithContext(context.Background(), input,\n    request.WithWaiterRequestOptions(request.WithLogLevel(aws.LogDebugWithHTTPBody)))\nThis will cause the application to log to the SDK's default logger the HTTP request and response messages for the WaitUntilTasksStopped call. In this case the DescribeTasks API operation.\nI'll try to reproduce this locally. It looks like the check for waiter state is pretty simple, tasks[].lastStatus values of the response all being STOPPED.\nIs it possible that some of the taskARNs passed into WaitUntilTasksStop are not being stopped?  The debug output show provide this information though.. Thanks for the update @atsushi-ishibashi. it looks like the logging wasn't captured in the output. do you know if the SDK's client ec has an alternative logger configured? By default the SDK will use aws.NewDefaultLogger which writes log entries to stdout.\nWhat we should see are lines such as request and response headers with the HTTP body logged. This is what would be helpful to see. It sounds like the response the SDK is getting back from the DescribeTasks API call does not match any of the predefined waiter conditions.. Thanks for the update @atsushi-ishibashi. Do you know the configuration or additional details of the task that is being started? That would be very helpful to identify If the SDK is missing a condition where the task is started.\nIn addition do you know about how long the task takes to run? The SDK's WaitUntilTasksStopped call will check the tasks's status every task defined in the DescribeTasksInput value has a lastStatus of STOPPED.\nAre you able to run the task outside of the context of influencer?. Hi @atsushi-ishibashi are you still having this issue?  Please re-open this github issue.. Hi @cloudaice thanks for reaching out to us. In this case does your application receive an error from the GetObject API call? If you could include the error message it would help debug this issue.. Hi @cloudaice I used the following example to make an GetObject API call to for an object with IfModifiedSince set to get a 304 status back.  In my example I am seeing a NotModfiied error code being returned in the err value.  Are you not seeing this?  If you could provide a debug output with the HTTP request and response values that would be helpful.\n```go\npackage main\nimport (\n    \"context\"\n    \"flag\"\n    \"fmt\"\n    \"time\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\n)\nfunc main() {\n    var bucket, key string\n    var timeout time.Duration\nflag.StringVar(&bucket, \"b\", \"\", \"The `bucket` name.\")\nflag.StringVar(&key, \"k\", \"\", \"Object `key` name.\")\nflag.DurationVar(&timeout, \"d\", 10*time.Second, \"Upload `timeout`.\")\nflag.Parse()\n\nsess := session.Must(session.NewSession())\nsvc := s3.New(sess, &aws.Config{\n    LogLevel: aws.LogLevel(aws.LogDebugWithHTTPBody),\n})\n\nctx := context.Background()\nif timeout > 0 {\n    ctx, _ = context.WithTimeout(ctx, timeout)\n}\n\nreq, resp := svc.GetObjectRequest(&s3.GetObjectInput{\n    Bucket:          aws.String(bucket),\n    Key:             aws.String(key),\n    IfModifiedSince: aws.Time(time.Now().Add(-20 * time.Minute)),\n})\nreq.SetContext(ctx)\nreq.HTTPRequest.Header.Add(\"Accept-Encoding\", \"application/x-gzip\")\n\nerr := req.Send()\n\nfmt.Println(\"Response:\", resp)\nfmt.Println(\"Error:\", err)\n\n}\n```\nDebug HTTP Output:\n```\n2017/09/15 10:06:45 DEBUG: Request s3/GetObject Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nGET /somekey HTTP/1.1\nHost: .s3-us-west-2.amazonaws.com\nUser-Agent: aws-sdk-go/1.10.45 (go1.8.3; darwin; amd64)\nAccept-Encoding: application/x-gzip\nAuthorization: AWS4-HMAC-SHA256 Credential=/20170915/us-west-2/s3/aws4_request, SignedHeaders=accept-encoding;host;if-modified-since;x-amz-content-sha256;x-amz-date, Signature=\nIf-Modified-Since: Fri, 15 Sep 2017 16:46:45 GMT\nX-Amz-Content-Sha256: \nX-Amz-Date: 20170915T170645Z\n\n2017/09/15 10:06:46 DEBUG: Response s3/GetObject Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 304 Not Modified\nDate: Fri, 15 Sep 2017 17:06:46 GMT\nEtag: \"1f6e74cd9c05ea97e186953ad620739b\"\nLast-Modified: Wed, 13 Sep 2017 18:55:32 GMT\nServer: AmazonS3\nX-Amz-Id-2: \nX-Amz-Meta-123: 123value\nX-Amz-Meta-Abc: abcvalue\nX-Amz-Request-Id: \n\n2017/09/15 10:06:46\n```\nOutput of resp, and err values\nResponse: {\n  Body: <nil>,\n  ETag: \"\\\"1f6e74cd9c05ea97e186953ad620739b\\\"\",\n  LastModified: 2017-09-13 18:55:32 +0000 UTC,\n  Metadata: {\n    Abc: \"abcvalue\",\n    123: \"123value\"\n  }\n}\nError: NotModified: Not Modified\n    status code: 304, request id: <requestID>, host id:<hostID>=. Hi @cloudaice Are you still running into this issue? I've not been able to reproduce it locally my self. Let us know if you still are having issues. If you're able to create a sample that reproduces the issue please let us know and reopen the issue.. Thanks for the update @vedhavyas, Let us know if you have any feedback or run into issues with the SDK.. Hi @razvanm thanks for contacting us. The SDK should be fully functional with Go 1.9. There are no known issues that we are aware of. If you're running into any issue please let us know.\nI believe request_1_8.go is working as intended with the // +build go1.8 statement applying to go 1.8 and above. request_1_7.go's build take limits the go version the file will be included in to Go 1.7 and below.. Thanks for letting us know about the API needing better documentation @fxaguessy. The best way to get this updated is to use the Feedback button in the lower right hand corner of the https://docs.aws.amazon.com/IAM/latest/APIReference/API_PolicyVersion.html page. Select Typo, grammar, or spelling issue option along with describing the documentation issue.\nI'll do this as well, but it will be helpful if you do as well to raise visibility.. I've submitted feedback to the IAM documentation team. Thanks again for letting us know about the lack of documentation here.. Thanks for reaching out to us @harshavardhana I think the best way to get answer to you question is via the Amazon S3 forums. There others who use S3 and may be able to help you with your question.\n\nWhy is ListParts never used in UploadManager? Is resuming never really supported? - I understand why true resuming cannot be implemented here, but wanted to know if you have more insights.\n\nThe S3 Upload Manager does not use ListParts because the uploader has full knowledge of the parts as they are uploaded. These parts numbers are only used in an upload failure condition to remove stale parts preventing them from taking space in your S3 bucket. Once a terminal failure occurs, and retries have been exausted the uploader will return the list of parts that were uploaded on error if LeavePartsOnError is true on Uploader.\n. I would not recommend using ListParts when performing a CompleteMultipartUpload. While today it may work it is documented that the part numbers returned should not be used for completing a multipart.\nThe recommended way is for the application to maintain the list of parts that were uploaded and use that list for completing the multi-part upload.. Sorry I do not know that off hand. This would be my assumption that ListParts is not guaranteed to be consistent. Whereas maintaining the list locally is always consistent because the application has knowledge of all the parts it uploaded.. Thanks for reaching out to us @harshavardhana . Let us know if you have any additional questions or feedback with the SDK. Since it looks like this question is answered i'm going to close it. Please reach back out if you have further questions.. Thanks for reaching out to us @the1337beauty. The best place to get an answer to your question will be via Stack Overflow, or the AWS Data Pipeline Forums. Both of these will have a broad audience which may be able to help you with your question.\nLet us know if you have any questions, and feedback with the SDK.. Thanks for the update @etsangsplk, glad that was cleared up. Let us know if you have any additional questions with the SDK or feedback. In addition the AWS EC2 Forums is also a great place to get information about supporter service APIs.. Thanks for reaching out to us @zakhark This looks like a shortcoming in the Console vs service API definition. The EC2 RequestSpotInstances API operation does not support subnets to be concatenated like this.\nI suggest reaching out to EC2 directly via the Amazon EC2 Forums. Or, Stack Overflow. This will be the best way to get feedback on the API.\nLet us know if you have questions, or feedback on the SDK.. Hi @zakhark Thanks for contacting us. Asking on the EC2 forums or stack overflow will be the best place to get the information you're looking for. Let us know if you have additional questions, or feedback.. Thanks for putting together this PR @e-beach. The change looks good. I'll go ahead and merge this in.. Hi @sateeshyandapalli thanks for reaching out to us. Could you please provide more information about the error that your seeing. Are you seeing additional DynamoDB Attribute Keys in your DynamoDB records when viewed from the AWS console, or are you seeing additional keys in the response when getting a record from DynamoDB.. Thanks for the update @sateeshyandapalli. Do you have a sample of the code that is inserting the records into DynamoDB that would help determine where the issues is coming from. In addition are you using the dynamodbattribute.Marshaler utilities to marshal the dynamodb.AttributeValue types into DynamoDB, or are the AttributeValues being constructed by hand?\nThe dynamodbattribute package doc's example may be helpful is using the SDK to put items into a DynamoDB table.. Hi @sateeshyandapalli are you still running into issues with AttributeValue's fields showing up in DynamoDB items responses?  Please reopen this issue if so.. Hi @trung We reviewed this issue with the service team and their guidance was that this message is intentionally encoded this content. The service team has this as a feature request in their backlog. Since this issue could be resolve by a service API update Is suggest commenting on AWS EC2 Forums. Thanks for reporting this issue let's use #1540 to tack this issue.. Thanks for posting this issue. Reviewing the change in the PR and repo it seems only the model folders Greengrass item is at fault. I'll investigate this more and correct this issue. It looks like the greengrass folder had it's casing changed some how. Will update this GitHub issue when a fix is available. Temporary fix will be correcting the casing locally, but need to fix this upstream as well so it doesn't happen again.. I've posted a PR #1541 that should correct the issue. We'll need to make some internally changes to how the release is generated too. As apart of these changes I think when the SDK's models are generated it should fail if two models target the same service's package. This is what occurred here both, Greengrass and greengrass were used to generate the greengrass service client. With the latter stomping over the former.. Thanks again for pointing this issue out and alerting us to it.  The PR has corrected the issue, and will be apart of our next tagged release.. Thanks for reaching out to us I'll take a look at this. Would you be able to enable logging with aws.LogDebugWithHTTPBody this will help identify if this issue is related to the change today.\nyou can do this with\ngo\nawsConfig := aws.Config{\n    Region: aws.String(\"us-west-2\"),\n    Credentials: creds,\n    LogLevel: aws.LogLevel(aws.LogDebugWithHTTPBody),\n}. Thanks for the update @andrewmeissner i've been able to reproduce this on my end, and and working to correct the bug.. I've created #1551 reverting the new marshalers in the short term until this bug can be fixed. I'll create a new version and recall v1.11.0 once the PR is merged in.. In a future release we'll that add back in the code generated API marshallers after more verification is done to make sure there are no additional edge cases the new marshallers are missing.. I have a fix locally that correct the Route 53 issue. It turns out this specific condition was not included in the SDKs protocol tests that I used to verify the changes. Causing the bug to get released.\nBefore releasing the marshallers again I think a utility is needed verify they marshaled output/request matches the current SDKs output. Protocol tests should also be updated.. Thanks again @andrewmeissner for posting this issue. Since the RESTXML change as been reverted and Route 53 request are working correction I'm going to close this issue as fixed. I have PR #1554 that will reenable the generated marshalers that can be accepted once more verification is completed.. Hi @gurleen-gks thanks for reaching out to us. The setContentMD5 is a customization of the AWS SDK for Java, as that S3 client is hand written. The AWS SDK for Go S3 client is built from the S3 service API model. It looks like the ContentMD5 header as an input parameter is not defined as apart of this model.\nTo work around this issue you can explicitly set the header using the SDK's request handlers. Usign the PutObjectWithContext form of the PutObject API operation.\ngo\nsvc.PutObjectWithContext(ctx, params, func(r *request.Request) {\n    r.HTTPRequest.Header.Set(\"Content-MD5\", objectsMD5)\n})\nRelated to #965, #530. @gurleen-gks which API operations are you looking to set the Content-MD5 on? Are these the PutObject API?. Thanks for the update @gurleen-gks This looks like an issue where the SDK is mistakenly removing ContentMD5 from the PutObject API operation when generating the Go PutObjectInput type.\nI'll dig into this more, but initially it seems like the logic removing ContentMD5 from the S3 models should be updated to not remove ContentMD5 from PutObjectInput, and maybe PutObjectPart. More verification is needed to ensure there are no other missing rules from the marshaler before this PR can be accepted.. Using the script to exercise the entire surface area of the SDK's APIs highlighted several issues with the generated marshalers that were not captured in the protocol tests. Notably the way handling of RESTJSON Content-Type, and header time values.. Dropping this old PR as the work is outdated. aws/aws-sdk-go-v2#76 includes additional work on this issue.. Thanks for contacting us @danielnelson. From the sample code you linked it looks like the number of connections to keep alive are not a problem.\nI see from the sample that LogDebug had been enabled. Did you notice any connection close responses coming back from cloudwatch in the logs?\n. Running on an OSX system I'm having difficulty reproducing this issue. Monitoring via netstat, and wireshark i'm not seeing any closed connections. Only a single connection created at startup, with that connection closed on ctr+c of the sample.\nIn netstat i'm seeing a single entry for cloudwatch staying established until the process is terminated.\ntcp4       0      0  <my ip addr>.60005    monitoring.us-we.http  ESTABLISHED. Though I noticed my test was with http. It looks like the cloudwatch TLS endpoint is where the reconnecting for each request occur.\nI'll forward this onto the cloudwatch team and see if they have any insight into this issue. I've not noticed this before with other services.\n. Reproduction sample with httptrace showing connection status.\nhttps://gist.github.com/jasdel/ef4d52e94ad91e84eddef6452c53f4d4. Hi @danielnelson I've heard back from the CloudWatch service, and verified that HTTPs connections  do support keep-alive but with a limit and a relatively short timeout.\nI've updated the gist linked above with a shorter timeout, between API requests. This shows connections being keep-alive. The previous version of the example was showing no keep-alive due to the long sleep between API requests.. I think this resolves this issue. Let us know if you run into any additional issues, or feedback. Thanks again for reaching out to us.. Thanks for the update @danielnelson. The SDK will always close the the http.Response body after reading the response payload. If the server chooses not to response with keep alive I think the client will close the socket. In these cases it is the Cloudwatch server which is choosing if connections should be keep alive to it. These generally are not documented as they can change at any time. From talking with the service team they currently have a low and very short timeout. Which may approach no keep alives, but this can change at any point in the future.. Thanks for the update. I missed that in your previous post.  I think monitoring.us-west-1.amazonaws.com:80 item you're seeing in the log isn't from a redirect. This looks to be an artifact of the SDKs request logging. No request to port 80 is actually being made. These lines go away when the SDK's logging is left off.\nI verified in wireshark that the SDK only makes a connection to the 443 port. No connection is ever made to 80 unless DisableSSL is set.\n. Thanks for the feature request @andrewmeissner. The AWS EC2 Forums would be a great place to post this feature request. A change like this would be best implemented as an API update to the Ec2 service.. Glad to help. Let us know if you run into any additional issues or have feedback.. Hi @atsushi-ishibashi thanks for reaching out to us. The technique that your looking for is very similar to Optimistic Locking. With this technique you can use a field in your dynamodb table to signify the version, or date in your application's case.\nWe have this utility in the SDK's feature backlog (#409).. Thanks for the update @atsushi-ishibashi and clarification. Check out the expression package this package was added a few weeks ago which through Builders remove the complexity of building dynamodb expressions. Along with their associated name and value aliases.\nDoes this help with the issue that you're running into?. I don't think it would be good for the SDK to add murexes around these API operations. Mostly because it would only partially workaround the issue. In a distributed system environment a mutex within a single application instance won't be much help if two or more instances try to make the change close enough to each other.\nIf this is an issue with the service restrict any resource change to only a one at a time. I think this is feedback we should forward on to the service. Especially in the distributed environment, this issue should be handled either by the service allowing multiple concurrent changes, or the client application infrastructure limiting the changes to only a single one at a time.\nI'm curious if there is anyway to identify that a resource change is pending. This might be a good place to have a waiter that will wait for a pending resource.. Thanks for creating this PR @radeksimko, but I don't think the SDK should retry for this API call. Since this is a API resource limit of the service API a waiter might be a better option, or for the application making the API call to manage the concurrent resource updates.\nThe AWS EC2 Forums is the best place to reach out to the Autoscaling and request the service support a higher rate of resource updates.. Hi @crazytan thanks for reaching out to us. Correct, the Ec2Metadata client only reads the document it does not validate the documents signature. This feature is in our backlog. If you're looking to contribute to a PR we'd be glad to review a change adding support for this feature. . Linking this to the SDK's backlog item of typed error handling. #798.. Thanks for correcting the doc for this package. I think this is actually apart of the SDK's code generation logic.\nIf we update the template in api.go this fix can be applied to all SDK service docs.. Thanks for creating this PR @bboughton i've taken this change and regenerated the. SDK's services in PR #1616 with the generated code.. Thanks for the feature request @theherk. The expression package does not currently provide a way to parse and unmarshal a given expression.\nAre you looking for such a utility to validate the correctness of the expression? . Thanks for reaching out to us @damienstanton. The issue in differing functionality here that you're seeing is an artifact of the CLI's legacy implementation. The CLI hides some of the details of a S3 bucket and object.\nBased on the the example it looks like damienstanton-test is the bucket name. Is this correct?\nS3 does not natively the concept of founders. This is an artifact of the CLI's representation of folders due to the nature of using a CLI on a in a command shell.  All objects in S3 are referenced by a key. These keys are unique within a bucket. The way the CLI simulates folders is to add additional meaning to the / character, by creating empty Objects at that key. In addition to mixing the Object's bucket and key value into a CLI specific s3:// protocol.\nIn this case the example is adding a path to the bucket name. This is incorrect. Instead of prefixing the key. A prefix (aka sub folders in this example) can only exist in the Key api field.  The SDKs do not perform any additional parsing on these values.\nA correct API request would be the following with the Bucket name field separate from the Object's Key. If the object key's in your bucket are prefixed with / you'll need to add that to the Key as well. (e.g childpath/test.txt vs /childpath/test.txt).\ngo\nuploader := s3manager.NewUploader(sess)\n    _, err := uploader.Upload(&s3manager.UploadInput{\n        Bucket: aws.String(\"damienstanton-test\"),\n        Key:    aws.String(\"childpath/test.txt\"),\n        Body:   file,\n    })\n. Thanks for the update @damienstanton. Let us know if you run into any additional issues or have feedback for the SDK.. Thanks for reaching out to us. Glad you were able to resolve the issue. This sounds like the best way to deal with the SDK's S3 bucket host vs path calling styles.\nLet us know if you have feedback, or issues in the future.. Thanks for putting this change together @dvrkps, and updating the travis builds to use the latest patch version for each Go point release.. Thanks for the update and feedback @crazytan. I think this feature would be helpful for users for Ec2 instances. The concern I have is adding an additional  third party library, pkcs7, auditing it, and the per partition Certs only available on a webpage. I'm not too concerned about testing the functionality, since this probably can be addressed via refactoring the GetInstanceIdentityDocument method's internals.\nSpecifically the concern with the signatures and certs is that there is no trust chain on the validity of the public certs. The only source of the public certs currently is text on a webpage.\nReading over this feature, and requirements I'm thinking this feature might be better as a feature external to the SDK. The library could take a EC2InstanceIdentityDocument value and validate the document's signature. I think the library should require the cert to be provided to it by the application instead of baking the certs into the library. If the certs are baked into the library the library will break when the private/public certs are rotated.. Thanks again for taking the time to create this PR @crazytan I don't think this change would be good to bake directly into the SDK. Primarily because it would require bundling the public certs (which hinders rotation), and additional dependency.. Thanks for reporting this issue @moitias. We've been able to reproduce this issue locally. It looks like this is just a matter of adding array handling to the decoders logic.. Thanks for reaching out to us @SCKelemen. This example requires an AWS region to be specified. In this example that could be accomplished easliy by putting the region in the environment when calling the example.\nAWS_REGION=us-west-2 go run withContext.go -b mybucket -k myKey -d 10m < myfile.txt\nIf you're using the example with a shared config file ~/.aws/config the SDK does not load this file by default. You can enable this feature via the environment or in the Session. Check, out the Session doc for more information.. Let us know if you're setting the region and still getting the error.. @SCKelemen The region can be added via the environment or hard coded when creating the Session or s3 service client.\ngo\nsvc := s3.New(sess, &aws.Config{\n    Region: aws.String(\"us-west-2\"),\n}). Hi @itskingori thanks for reaching out to us. The SDK will only read the shared configuration's region parameter if the session is loaded with SharedConfigEnabled.  The SDK's session package documents the best way to enable this feature.\nWith that said It looks like the GetBucketRegion requires theregionHintto be provided ignoring the region that may of existed in theSession.  With this functionality the regionHint is required. I'll take a look at see if this functionality can be updated.. I put out PR #1804 which hopefully will address this issue by adding additional wording to the GetBucketRegion function doc, and adding support for reading the region hint from thesession(aka, ConfigProvider) if aregionHintparameter is not provided.. Thanks for the feedback @ljfranklin The best place to provide this feedback will be the S3 API reference documentation. Feedback there will be forwarded to the S3 service team.  You can do this by clicking theFeedback` button in the bottom right hand corner on the S3 CreateBucket API doc page.. @ppetko Thanks for the update, and link. The SDK's API type's String and GoString functions output is not JSON. It is a formatted string of the type, for human readability.\nIn your usecase I think it would be better to pass the result value returned from the API call to a json.Marshaler instead of using fmt.Println(result).  The SDK does not have the feature to output API types as JSON.. @atsushi-ishibashi Thanks for the feedback. The best place to provide feedback about an service API is on the service's API Reference documentation. Clicking the Feedback button in the bottom righthand of the page will send the feedback to the service.. Thanks for posting this feature request @mrichman. It looks like the AWS SDK for Ruby already supports this functionality as a AWSRB_DEBUG env var. I think this is also relevant to the AWS SDK for Javascript PR aws/aws-sdk-js#1739.. Thanks for putting this change together @jsgv.. Hi @dneralla Let us know if you're still running into this issue. . Thanks for creating this PR @s12v. I'm looking into the v4 signing spec to verify that this will not go against any of the v4 specification.. From the V4 signing specification and test suite i'm not seeing any mention of excluding port from the Host header when computing the signature's canonical string. And per the HTTP RFC the port should be included in the header if its non standard.\nI'm curious if the failure you're seeing because the a \"standard\" port of 443 (TLS) is being included in the hostname.. Related to aws/aws-cli#2883. I was thinking about this as well. I think the original request must be modified because if the HTTP request sent doesn't use the exact Host header value the signature will fail. I don't think the Go HTTP client automatically strip off the port in the Host header.  At least httputil.DumpRequest shows it with the port on the host header. I've not looked at this in wireshark yet though.\nhttps://play.golang.org/p/Z7opUaTA-F\nIf the Go HTTP client doesn't automatically strip off the \"standard\" port form the Host header I think this needs to be performed by the SDK.\nMaybe a better place for this logic would be when the request is created instead of the signer. The signer wouldn't need any special logic if the Host value on the HTTPRequest is already formatted \"correctly\".. Thanks for the update @s12v I think with ES you're right the port is probably being removed automatically server side before signature verification. We probably cannot rely on this for all services though. I've heard mentions about standard port in host header causing failures for some services.\nThanks a lot for taking the time and putting together this PR by the way!\n\n\nMaybe a better place for this logic would be when the request is created instead of the signer.\n\nNot sure, the signer accepts http.Request.\n\nGood point. This logic would be required in the signer anyways to handle the case a user uses the signer directly with a http.Request. So we know the logic needs to per performed in the signer for the Sign and Presign maybe case.  From a library structural point of view it would also be good if the SDK's internals generated the request.Request's Host header correctly.\n. To Resolve this how about we move the stripDefaultPort et al functions/logic to the request package, and export stripDefaultPort named as SanitizeHostForHeader. The SDK's request.New could use this when building a new SDK request. Setting the Request's HTTPRequest.Host member.\nFor the use case where a user manually calls v4.Signer.Sign|Presign, the methods should use request.SanitizeHostForHeader to update the http.Request's Host member.\nI think this would allow the logic to be performed in both request building, and ensure the Host header is constructed correctly for anonymous requests, (aka unsigned).. Thanks for putting this PR together @s12v. the change looks good. I'll go ahead and merge this in. It will be tagged in our next release.. Thanks for letting us know about this issue in the v2 SDK @s12v. Would you mind creating a issue in the v2 for this.  We'll get that issue ported over.. Hi @rarguelloF do you have any additional questions? If so please reopen this issue as setting the Config's CredentialsChainVerboseErrors will cause the SDK to log the chain of credentials used when determining which credentials to use.. Hi @Aprimit The best way to avoid using the dynamodb.AttributeValue (aka dynamodb json) is to use the SDK's dynamodbattributes package. This package provides (un)marshaling of Go types to and from dynamodb attribute values. Checkout the Attribute Value Marshaling example in the package.. Thanks for reaching out to us with this problem @stevenh and @Luizm. Any additional information you provide would be very helpful. Are there any specific EC2 Query parameters your application uses that seems to trigger this failure?  In addition is this limited to sa-east-1 or are you seeing this in other regions also?. @patthehuman also try to clear your Go pkg cache this can sometimes get stale with old versions of content.\nthis is in the $GOPATH/pkg/* path.. Thanks for the update @patthehuman let us know if you run into any issue in the future or have feedback for the SDK.. Thanks for reaching out to us @vaibhavkewl. I'd like to hear more about this feature. How would it manifest in the SDK? \nI'm not very familiar with the AWS SDK for Java's linked feature. but will read up on it. I think Java relies heavily on Java Annotations for meta-programming. \n. Thanks for reaching out to us @justone We're taking a look at this, and will update once we reproduce the issue.. Was able to reproduce this easily. Thanks for the sample.\nResponse body from service.\njson\n{\n  \"FormatVersion\": \"aws_v1\",\n  \"NextToken\": \"nwgM6jfEMURvqUBTOCtXvw==:0x2+NWRDC5l8tzDFYdtRI7sv1ua8wr8gulXULluS8OBMjd2UvaVhWVCIOyIhZnLtefLKeduMMIcajIpmk+niXFXR0k+iKhmtLRtW528QNyUR1FrLzrbP8Zru8tJaXMvB/JJe1I3Qoc+aMeGtF0LjXNJuLFqtu6wFohQvX9RyQTGL/gQHabQbYcKmTiOGhAkh\",\n  \"PriceList\": [\n    \"{\\\"product\\\":{\\\"productFamily\\\":\\\"Storage\\\",\\\"attributes\\\":{\\\"storageMedia\\\":\\\"SSD-backed\\\",\\\"maxThroughputvolume\\\":\\\"320 MB/sec\\\",\\\"volumeType\\\":\\\"Provisioned IOPS\\\",\\\"maxIopsvolume\\\":\\\"20000\\\",\\\"servicecode\\\":\\\"AmazonEC2\\\",\\\"usagetype\\\":\\\"APS1-EBS:VolumeUsage.piops\\\",\\\"locationType\\\":\\\"AWS Region\\\",\\\"location\\\":\\\"Asia Pacific (Singapore)\\\",\\\"servicename\\\":\\\"Amazon Elastic Compute Cloud\\\",\\\"maxVolumeSize\\\":\\\"16 TiB\\\",\\\"operation\\\":\\\"\\\"},\\\"sku\\\":\\\"3MKHN58N7RDDVGKJ\\\"},\\\"serviceCode\\\":\\\"AmazonEC2\\\",\\\"terms\\\":{\\\"OnDemand\\\":{\\\"3MKHN58N7RDDVGKJ.JRTCKXETXF\\\":{\\\"priceDimensions\\\":{\\\"3MKHN58N7RDDVGKJ.JRTCKXETXF.6YS6EN2CT7\\\":{\\\"unit\\\":\\\"GB-Mo\\\",\\\"endRange\\\":\\\"Inf\\\",\\\"description\\\":\\\"$0.138 per GB-month of Provisioned IOPS SSD (io1)  provisioned storage - Asia Pacific (Singapore)\\\",\\\"appliesTo\\\":[],\\\"rateCode\\\":\\\"3MKHN58N7RDDVGKJ.JRTCKXETXF.6YS6EN2CT7\\\",\\\"beginRange\\\":\\\"0\\\",\\\"pricePerUnit\\\":{\\\"USD\\\":\\\"0.1380000000\\\"}}},\\\"sku\\\":\\\"3MKHN58N7RDDVGKJ\\\",\\\"effectiveDate\\\":\\\"2017-10-01T00:00:00Z\\\",\\\"offerTermCode\\\":\\\"JRTCKXETXF\\\",\\\"termAttributes\\\":{}}}},\\\"version\\\":\\\"20171026015458\\\",\\\"publicationDate\\\":\\\"2017-10-26T01:54:58Z\\\"}\"\n  ]\n}. Thanks for reporting this issue @justone I've merged in a fix that will correct the SDK's behavior. This will be included in the next release.. Thanks for reaching out @trung This feature is actually supported in the latest version of the SDK. An issue in the SDK's doc deployment prevented the documentation being updated.\nWe're working to fix the docs.. Thanks for letting us know about this issue. The SDK's docs have been updated.  Let us know if you run into any additional issues or have feedback for the SDK.. @Aprimit thanks for reaching out to us. Correct the marshaler assumes a string slice is a list because a string set implies that there are no duplicate values within the list. Since only the application providing the slice to the marshaler knows if the data is actually a set or arbitrary list the application needs to signal the marshaler to process the value as a string set.. It looks like the marshaler doesn't document this condition very well.  We should update the documentation to make this more clear.. Sure the dynamodbav:\",stringset\" struct tag is the best way to tell the marshaler that the field should be treated as a string set instead of a list.\ne.g\ngo\ntype MyType struct {\n    // Field will be marshaled as a string set\n    Field []string `dynamodbav:\",stringset\"`\n}. Hi @Aprimit is there any additional information or assistance with this pattern that would help your usecase?. Closing this issue. Please reopen if you're still experiencing this issue.. Hi @etsangsplk Thanks for reaching out to us. Do you have additional information about the field that is missing? I didn't find a metadata operation that returns a fileSystemId, but might be missing it. Do you know which EC2 metadata operation that should return this field? \n. Thanks for the update. @etsangsplk. Could you please provide more details about the feature you are looking for?\nAre you looking for additional fields to be added to the EFS API? If so the best way to make that feature request to EFS  is vai the API docs page. \nOr are you looking for additional content to be made available via the EC2 Metadata service?. Hi @etsangsplk if you could provide more information on the feature you're looking for it would help us investigate this issue further.. Closing this issue. Please reopen with more details for the feature request.. Hi @atsushi-ishibashi  we heard back from the service team. This is a documentation bug. The minimum number of targets is 1, not 0. The service team will get the documentation for the Targets field, and it will be included in a future release.\nIn addition you can provide feedback directly to the SSM service's teams documentation on the RegisterTargetWithMaintenanceWindowInput page. Using the Feedback link in the bottom right corner of the page.. Hi @atcrawford thanks for reaching out to us. I don't have any additional information when a DAX Client SDK for Go would be available, sorry. Posting your feedback on the AWS DynamoDB forums would be the best place to voice your support for client in Go.. Thanks for reaching out to us @slapec93 It looks like the issue you're running into is how the AWS SDK for Ruby marshals the ACL field into the query string. The AWS SDK for Go puts the ACL field in the header.\nI'll look into why there is this difference.\nAs a workaround you can use the PresignRequest method on Request type that will return both the URL and a http.Header for all header values that must be included in the HTTP request that was presigned. The headers returned must be sent with the request or the signature will be invalid. Check out our PresignURL for a runnable example.. Thanks for the update. I've investigated this issue and it looks like the URL that is being generated by the AWS SDK for Ruby is actually not restricting the presigned URL to the parameters that is being specified in the SDK's a input parameters. Specifically the Content-Length header isn't being signed allowing users of the presigned URL to upload any content length of their choosing.\nIn addition for ACL, if the x-amz-acl field is not in the header the ACL canned policy is ignored. So if the policy is in the query string it will be ignored and the object will have the bucket's default policy applied. Since private is generally the default policy having x-amz-acl=private in the query string is hiding the fact its not actually working. S3 only reads the ACL if it is included as a header. You can validated this by changing private to public-read and inspect the object's permissions in the AWS S3 Console.\nHere is a reference to where this issue was fixed in the AWS SDK for Ruby v2.1.31 and related Github issue, aws/aws-sdk-ruby#874. I think nearly all of the S3 header values need to be preserved as headers for presigned URLs.\n. Let us know if you run into any addition issues with the SDK or have feedback.. Thanks for reaching out to us @dseichter, The best way to delete multiple items in a DynamoDB table is to use the BatchWriteItem API.\nThe following example is a a good place to get started.\ngo\nsvc.BatchWriteItem(&dynamodb.BatchWriteItemInput{\n    RequestItems: map[string][]*dynamodb. WriteRequest{\n        \"<TableName>\": []*dynamodb. WriteRequest{\n            {\n                 DeleteRequest: &dynamodb.DeleteRequest{\n                    Key: // dynamodb.AttributeValue of key to delete,\n                 },\n             },\n            {\n                 DeleteRequest: &dynamodb.DeleteRequest{\n                    Key: // dynamodb.AttributeValue of key to delete,\n                 },\n             },\n            {\n                 DeleteRequest: &dynamodb.DeleteRequest{\n                    Key: // dynamodb.AttributeValue of key to delete,\n                 },\n             },\n        },\n    },\n})\nIf you have additional questions about the DynamoDB API, a great place to find answers is via Stack Overflow, or the DynamoDB forums.. Thanks for reaching out to us on this issue, @lorengordon. This looks like the service updated their API, but those update didn't make it to any of the SDKs. I'll reach out to the services internally, but I also suggest that you reach out on the AWS Directory Service's forums directly.. Hi @lorengordon I heard back from the service team, and the API reference documentation is incorrect. The service does not support a Size parameter.  The service is working to get their documentation updated.\nThanks for letting us know about this issue.. Bad merge :( will have to fix up this PR. Thanks for reaching out to us @matthewmueller for a service usage question I suggest asking on Stack Overflow, the AWS Firehose Forums page, or in our Gitter channel.\nIn addition I suggest voicing your feedback on the Firehose API reference documentation directly. You can do this with the \"Feedback\" button in the bottom right hand corner of the PutRecordBatch operations API doc page.\nFor FailedPutCount my understanding of the documenation is that the FailedPutCount is the total number of Records that failed to be put to the Kinesis Firehose. The RequestResponses member is a list of Put status metadata for all records that were submitted in the batch. For each PutRecordBatchResponseEntry if ErrorCode and ErrorMessage are set the record failed to be Put. If the error code and message fields are not set, there was no error putting that record.. Thanks for reaching out to us @meirf. The best way to get the Object's website redirect location's value is to use the HeadObject API to get detailed metadata about the object. This API will provide the WebsiteRedirectLocation field. The listObjects API does not include this detailed information.\nLet us know if you're still running into issues. In addition I suggest asking questions like this on the StackOverflow.. Thanks for fixing this README typo @tkawachi . The change looks good.. Found another case:\nmodels/protocol_tests/generate.go:332: Sprintf format %b has arg v.(bool) of wrong type bool. Thanks for the update @bwhaley. Let us know if you run into an additional issues, or have feedback.. Hi @beard1ess are you still running into this issue? If so any additional information would be helpful.. Hi @beard1ess I think the issue you're running into is because the S3 API operation GetBucketLocation will intentionally return an empty string \"\" for us-east-1. This is the service's expected response.\nThe utility method provided by the SDK NormalizeBucketLocation in the s3 package attempts to provide a better experience for this API operation via an opt in added helper.\nWithout the helper the S3 API call GetBucketLocation will return response values for the LocationConstraint as documented in the Response Elements section of the doc liked above.\n. Using the NormalizeBucketLocation is the best way in the V1 SDK to get a consistent region value when using the GetBucketLocation API. Due to S3's design of this API the GetBucketLocation API's default behavior cannot be changed in the v1 SDK without a breaking change. The NormalizeBucketLocation request handler helper should assist with this issue.. Thanks for creating this issue @brikis98 The release v1.12.42 yesterday included an updated for SageMaker service which was committed with an alternate naming scheme incorrectly.\nThis was fixed in, #1686. A release has not been cut yet that includes this fix. A release will be cut for this fix today.. Thanks for the suggestion. We're working to improve the reliability of how these model folder names are generated and sourced to be more stable.\nRelease v1.12.43 should be cut shortly once the travis checks finish.. Thanks for reporting this issue @brikis98, @Atanas-Kanchev, and @sudhirj. This issue is now tagged in release v1.12.43.. Hi @MathieuMailhos I think the issue you're running into is that the version of the SDK your using needs to be updated. Go 1.8 introduced a change that modified the way a undocumented feature of net/http#Request's body worked. \nYou should upgrade to a minimum of v1.6.3 of the SDK, though I suggest migrating to the latest release as several features and bug fixes have been added since v1.6.3\nRelated to #984. Thanks for the update and feedback @hwh33. The goal of the builders are to provide immutable expression building functions and methods. This pattern allows users to build multiple expressions from a common components. In addition, this pattern allows partial expressions components to be reused, and composed into more complex expressions.\n. Hi @nsagnett thanks for reaching out to us. The best place to get an answer to your question will be   Stack Overflow, or the AWS ElasticCache Forums. In addition we have our [gitter channel] for SDK specific questions, and there is the Gopher Slack group's #aws channel for general AWS questions.\nThe SDK is a Go representation of the AWS service APIs, but the best way to get answers to specific service API questions is to reach out to the service forums, AWS API reference documentation, or community such as Stack Overflow.\nIf you think you've encountered an issue with the service API not functioning correctly please use the AWS forums, in addition you can submit feedback on the service's API reference page with the \"Feedback\" button in the bottom righthand corner of the page.. In addition could you describe the error that you're receiving in more details.. Hi @forensicsguy20012004 and @Amitgb14  We're looking into verifying this issue. Could you please enable logging with the SDK and provide us with the X-Amz-Id-2 and X-Amz-Request-Id of the GetBucketLogging response headers.\nWe've been able to reproduce the issue for bucket's that have not been configured for logging, which is the expected outcome, but it would be helpful to have more reproduction steps to reproduce this issue when logging is expected to be enabled.  In addition could you describe how logging was enabled. The following is an example of the code I'm using to put the bucket logging, and read it. If you could describe your expected bucket logging that would be helpful also.\nYou can enable this output with the following:\n```go\n    sess := session.Must(session.NewSession(&aws.Config{\n        Region:   aws.String(\"ap-south-1\"),\n        LogLevel: aws.LogLevel(aws.LogDebugWithHTTPBody | aws.LogDebugWithSigning),\n    }))\nsvc := s3.New(sess)\n\n```\nThe following is an example of the code I'm using to put the bucket logging, and read it. If you could describe your expected bucket logging that would be helpful also.\n```go\npackage main\nimport (\n    \"flag\"\n    \"fmt\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\n)\n// Usage: go run  -b  [-r region]\n// Credentials sourced for SDK default credential provider. e.g ~/.aws/credentials\nfunc main() {\n    var region, bucket string\n    flag.StringVar(&bucket, \"b\", \"\", \"The bucket to use.\")\n    flag.StringVar(&region, \"r\", \"ap-south-1\", \"The region to use.\")\n    flag.Parse()\n    if len(bucket) == 0 {\n        panic(\"bucket name required\")\n    }\nsess := session.Must(session.NewSession(&aws.Config{\n    Region:   aws.String(region),\n    LogLevel: aws.LogLevel(aws.LogDebugWithHTTPBody | aws.LogDebugWithSigning),\n}))\n\nsvc := s3.New(sess)\n\nif err := putBucketLogging(svc, bucket); err != nil {\n    panic(\"put failed, \" + err.Error())\n}\n\nif err := getBucketLogging(svc, bucket); err != nil {\n    panic(\"get failed, \" + err.Error())\n}\n\n}\nfunc putBucketLogging(svc *s3.S3, bucket string) error {\n    input := &s3.PutBucketLoggingInput{\n        Bucket: aws.String(bucket),\n        BucketLoggingStatus: &s3.BucketLoggingStatus{\n            LoggingEnabled: &s3.LoggingEnabled{\n                TargetBucket: aws.String(bucket),\n                TargetPrefix: aws.String(\"myloggingprefix\"),\n            },\n        },\n    }\nresult, err := svc.PutBucketLogging(input)\nfmt.Println(result, err)\nreturn err\n\n}\nfunc getBucketLogging(svc *s3.S3, bucket string) error {\n    input := &s3.GetBucketLoggingInput{\n        Bucket: aws.String(bucket),\n    }\nresult, err := svc.GetBucketLogging(input)\nfmt.Println(result, err)\n\nreturn err\n\n}\n``\n. Thanks for the clarification @Amitgb14. I think this is an issue with the GetBucketLogging and its expected output. When logging is not enabled you can expect effectively an empty response withLoggingEnabledfield ofGetBucketLoggingOutput` being nil.. Thanks for the clarification @forensicsguy20012004. Does your bucket have logging enabled? From the console screenshot it looks like logging is disabled.\nA GetBucketLogging API call to S3 with a bucket without logging enabled should return a GetBucketLoggingOutput value with the LoggingEnabled member having a nil value. If the GetBucketLoggingOutput value is printed to the terminal. The empty response is the expected result.. HI @forensicsguy20012004  Thanks for the feedback, we've forward this along to the S3 team. The SDK isn't adding the LoggingEnabled member to the string output of GetBucketLoggingOutput because the API does not returning the value. S3 is using a unset state for this field to signify that no logging is enabled.. Thanks for reaching out to us @tnclong I'll try to reproduce this issue locally. The code same you gave looks pretty straight forward. By chance is your application adding any SDK request handlers, or additional headers to the request?\nIn addition, if you could answer some of the following questions it would be very helpful.\n Did you experience this with any other services or APIs?\n was this working in the past and recently broke?. @tnclong I'm having difficulties reproducing this issue. Could you check that the topic ARN was copied correctly?\nI created a topic, SomeCoolTopic, copy the ARN from the console, pasted it into my sample application, and was successfully able to make the request.\n```go\npackage main\nimport (\n    \"fmt\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/endpoints\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/sns\"\n\n)\nfunc main() {\n    sess := session.Must(session.NewSession())\n    sess.Config.Region = aws.String(endpoints.UsWest2RegionID)\n    sess.Config.LogLevel = aws.LogLevel(aws.LogDebugWithHTTPBody)\nsvc := sns.New(sess)\n\nresp, err := svc.Publish(&sns.PublishInput{\n    TopicArn: aws.String(\"arn:aws:sns:us-west-2:<accountID>:SomeCoolTopic\"),\n    Message:  aws.String(\"my message\"),\n})\n\nfmt.Println(resp, err)\n\n}\n```\n```\n2017/12/28 10:44:05 DEBUG: Request sns/Publish Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nPOST / HTTP/1.1\nHost: sns.us-west-2.amazonaws.com\nUser-Agent: aws-sdk-go/1.12.53 (go1.9.1; darwin; amd64)\nContent-Length: 120\nAuthorization: \nContent-Type: application/x-www-form-urlencoded; charset=utf-8\nX-Amz-Date: 20171228T184405Z\nAccept-Encoding: gzip\nAction=Publish&Message=my+message&TopicArn=arn%3Aaws%3Asns%3Aus-west-2%3A%3ASomeCoolTopic&Version=2010-03-31\n2017/12/28 10:44:06 DEBUG: Response sns/Publish Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 200 OK\nContent-Length: 294\nContent-Type: text/xml\nDate: Thu, 28 Dec 2017 18:44:05 GMT\nX-Amzn-Requestid: e5e0f05b-9725-53bc-9669-a2e4aa93e50d\n\n2017/12/28 10:44:06 \n\ne67a31e2-564d-5195-9595-635d6bfbd724\n\n\ne5e0f05b-9725-53bc-9669-a2e4aa93e50d\n\n\n{\n  MessageId: \"e67a31e2-564d-5195-9595-635d6bfbd724\"\n} \n```. Thanks for the update @tnclong . Let us know if you run into any additional issues, have feedback, or questions with the SDK.. Thanks for the update @varroba  that you're still running into this issue. I'm going to reach out to GuardDuty again, and see what the issue with this API is.\nAlso, @varroba are you seeing this issue only with the Asn parameter?. Hi @varroba I'm trying to reproduce this issue, but not having any luck.  If you're still experiencing this issue could you provide a sample of the HTTP response body you're receiving with the offending asn value. along with the response headers.\nExample response wire trace:\n2018/04/20 14:29:22 DEBUG: Response guardduty/GetFindings Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/2.0 200 OK\nContent-Type: application/json\nDate: Fri, 20 Apr 2018 21:29:22 GMT\nVia: 1.1 a3c7cc30af6c8465e695a3c0d44793e0.cloudfront.net (CloudFront)\nX-Amz-Apigw-Id: FqND6HEnvHcFWww=\nX-Amz-Cf-Id: hv4B_X9hwlNbVo4_j2ejmdruh6LLNJBT3CzS_ESrRs64fhVoW3Caow==\nX-Amzn-Requestid: e47679e7-44e1-11e8-bbdf-47f7b0920c8c\nX-Amzn-Trace-Id: sampled=0;root=1-5ada5bb2-bc43e8777451c92dee884cc7\nX-Cache: Miss from cloudfront\nYou can enable this output by setting the SDK's Config.LogLevel to aws.LogDebugWithHTTPBody\nThe following is the example program I used to try to reproduce this issue with GuardDuty's example Findings.\n```go\npackage main\nimport (\n    \"fmt\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/guardduty\"\n\n)\n// Usage: AWS_REGION= AWS_PROFILE= go run main.go\nfunc main() {\n    sess := session.Must(session.NewSession(\n        &aws.Config{\n            LogLevel: aws.LogLevel(aws.LogDebugWithHTTPBody),\n        },\n    ))\nsvc := guardduty.New(sess)\n\ndetectorID := \"<detectorID>\"\nlistInput := &guardduty.ListFindingsInput{\n    DetectorId: aws.String(detectorID),\n    MaxResults: aws.Int64(50),\n}\n\nerr := svc.ListFindingsPages(listInput, func(p *guardduty.ListFindingsOutput, lastPage bool) bool {\n    resp, err := svc.GetFindings(&guardduty.GetFindingsInput{\n        DetectorId: aws.String(detectorID),\n        FindingIds: p.FindingIds,\n    })\n    fmt.Println(\"GetFindings\", resp, err)\n    return true\n})\n\nfmt.Println(\"failed list findings\", err)\n\n}\n```. Thanks for the update @varroba let us know if you're still encountering this issue. If possible include Request Ids of the HTTP response header if so.. Thanks for providing the additional information and wire log @varroba. I've forwarded these onto the service team. In addition would you also be able to provide the X-Amz-RequestId header value(s) as well. These may help the service team track down the request via logs on their end.\n(sorry @'ed the wrong person in original comment.). @varroba as a workaround for this issue I think you can apply the following patch change to your local version of the SDK. This should allow you to workaround the GuardDuty issue until the service is able to fix the bug.  This workaround updates the SDK's JSON unmarshaler to allow unmarshaling JSON numbers into String types.\n```patch\ndiff --git a/private/protocol/json/jsonutil/unmarshal.go b/private/protocol/json/jsonutil/unmarshal.go\nindex 037e1e7..c61f3c0 100644\n--- a/private/protocol/json/jsonutil/unmarshal.go\n+++ b/private/protocol/json/jsonutil/unmarshal.go\n@@ -7,6 +7,7 @@ import (\n        \"io\"\n        \"io/ioutil\"\n        \"reflect\"\n+       \"strconv\"\n        \"time\"\n    \"github.com/aws/aws-sdk-go/aws\"\n\n@@ -209,6 +210,9 @@ func unmarshalScalar(value reflect.Value, data interface{}, tag reflect.StructTa\n                case time.Time:\n                        t := time.Unix(int64(d), 0).UTC()\n                        value.Set(reflect.ValueOf(&t))\n+               case string:\n+                       ds := strconv.FormatFloat(d, 'f', -1, 64)\n+                       value.Set(reflect.ValueOf(&ds))\n                default:\n                        return errf()\n                }\n```. Thanks for the update @varroba from your response and talking with GuardDuty I think GuardDuty resolved this issue on their end. The string ASN change in behavior is the expected, and should no longer encounter the integer error case. Let us know if you're still running into this issue.. Hi @jammerful thanks for letting us know about this issue. Contacting the service team via support is the best way to raise this issue to their attention. The SDK won't have any ability to correct this issue as it is apart of the service's response.. Thanks for fixing this typo @earlonrails . @Amitgb14 It sounds like you're looking to upload the same file N number of times to different keys. Is this correct?\nI  think the issue you're running into is that the fileReader value is being shared across the goroutines. Where each goroutine is trying to read bytes from the file. A io.Reader basically has a cursor to keep track of where in the file it is currently reading/writing. If multiple goroutines try to read from the same reader concurrently the cursor will be moved around unexpectedly. This would explain why each goroutine opening the file independently does not encounters the issue.\nRunning your application or tests with the go -race flag will help identify where race conditions occur in your application.\nIf your application will be putting a S3 object into multiple key's I suggest doing a PutObject API call initially once, then using the CopyObject API call to copy it to the additional keys.\n  . @Puneeth-n correct since the InvalidParameterValueException exception isn't a throttle exception the SDK will not attempt to retry. We've reached out to the EC2 team to rase the issue with them. In addition it may help raise the issue more if you were to post on the EC2 AWS Forums with the issue of EC2 returning the wrong error code.. @Puneeth-n let us know if you're still experiencing this issue and we can assist in reaching out to the EC2 team.. Hi @dahankzter thanks for reaching out to us. Could you clarify how the presigned URLs are being generated by your application. Specifically, is the Session being created for every presigned URL, or created once, and shared across all presign URL generators? Is the same being done with the S3 service client?\n\nCompare with not presigning at all.\n\nCould you clarify what you mean. Does sending S3 API requests not incur the same overhead as presiging a URL? Or is there a different setup that your application is using that we should test against?\nIn addition could you clarify which S3 API your application creating the presigned URLs for?. I created #1735 which exercises the Put and Get object presigning. Are these benchmarks comparable to the way your application is using the S3 presign URLs?\nFrom investigating it looks like the restxml.Build you're seeing is the SDK injecting the bucket, and key into the request's URI. The majority of the cpu time seems to be split between the 1/3 protocol marshalering the bucket and key into the URL, and 2/3 v4 signing it self. Coming out to about 40us per request presign on my system.\nHow do these numbers compare with what your application is seeing?\n\nrest.buildLocationElements and signingCtx.build probably are a good targets for improving the performance of presigned signatures.. Removed the BenchmarkPresign_PutObject_WithUnsignedPayload as all S3 presign request are unsigned payload by default. This bench wasn't doing anything unique.. Hi @luoxiaoxun could you provide more information bout the issue that you're running into, or the error that your application is receiving.\nYou can find examples how to use the SDK in its example folder, and documentation linked of the SDK's README's Reference Documentation. Thanks for reporting this issue @nirhaas. It looks like the SDK is incorrectly applying the CloudHSMv2 service name as the signing name if a singing name is not modeled in the SDK's endpoint models. This is causing the signing name modeled in the CloudHSMv2 API model to be ignored.\nWe need to investigate the modeled signing name expectation, but initially i'm thinking the SDK's logic should be updated so that deriving the signing name from the service name doesn't happen until after the API modeled signing name value is considered.\nA workaround for this issue until a fix is submitted without editing the SDK is to use a custom endpoints.Resolver.\n```go\npackage main\nimport (\n    \"github.com/aws/aws-sdk-go/aws/endpoints\"\n    \"github.com/aws/aws-sdk-go/aws/session\"\n    \"github.com/aws/aws-sdk-go/service/cloudhsmv2\"\n)\nfunc main() {\n    // Initialize Session the way your application does today, e.g.\n    sess := session.Must(session.NewSession())\n// When initializing the CloudHSMv2 service client add the additional configuration.\nsvc := cloudhsmv2.New(sess, &aws.Config{\n    EndpointResolver: endpoints.ResolverFunc(ResolveCloudHSMv2Endpoint),\n})\n\n// Use CloudHSMv2 service client APIs. The SDK should be use the correct signing region now.\nresp, err := svc.DescribeBackups(nil)\n\nfmt.Println(resp, err)\n\n}\nfunc ResolveCloudHSMv2Endpoint(service, region string, optFns ...func(*endpoints.Options)) (endpoints.ResolvedEndpoint, error) {\n    endpoint, err := endpoints.DefaultResolver().EndpointFor(service, region, optFns...)\n    if err != nil {\n        return endpoints.ResolvedEndpoint{}, err\n    }\nif service == cloudhsmv2.EndpointsID {\n    endpoint.SigningName = \"cloudhsm\"\n}\n\nreturn endpoint, nil\n\n}\n```. HI @dseichter thanks for reaching out to us. I'm not aware of any support for server side encryption in DynamoDB. Reaching out directly to the DynamoDB team on the AWS Forums would be the best place to propose and ask that feature.\nFor client side encryption there are a couple Java examples, and an experimental Java repository which provides client side encryption.\nI'll mark this a a feature request for the SDK to provide client side encryption for DynamoDB content.. Thanks for finding and letting us know about this bug @caseylucas. I'll take a look at the PR you created.. Thanks for putting this PR together @caseylucas, we've merged in the fix.. Hi @Puneeth-n thanks for reaching out to us. Could you provide more information about the issue you're seeing?\nReading of the service's API reference guide it looks like description is not apart of the GetParameters response. To get the parameter metadata such as description you use use the DescribeParameters API. The response of this API return the parameter metadata. Thanks for reporting this issue. We're looking into ways to validate the SDK's release process doesn't introduce additional errors like this in the future.. Thanks for putting this PR together @micheldlebeau. the changes look good to pull in.. Thanks for reaching out to us @bohdantrotsenko with this request. Could you describe more about the requested Preallocate method that would assist in the issue you're seeing?\nThe NewWriteAtBuffer function takes a buffer of arbitrary size. Would it make sense to use this to create a preallocated buffer?  \ngo\nw := aws.NewWriteAtBuffer(make([]byte, 1024*1024*70)) // 70MB buffer\nIn addition, if your application is performing many S3 downloads with the S3 Manager it may make sense to preallocate multiple WriteAtBuffers into a pool. This would allow your application to share the grown WriteAtBuffers limiting the total number of allocs/copies done. With that said care needs to be taken when releasing the buffer back to a pool so no outstanding references to the buffer are maintained by any part of your application.\n. Thanks on the feature request @ash2k. The SDK's error constants are based on values modeled by the AWS service, CloudFormation in this case. If a service does not model the exceptions the SDKS have no knowledge of them and are unable to generate error codes or types for them. \nOne possible workaround for this is for the SDK to add customizations for these \"common\" errors, but that can become stale and out of date across the SDKs pretty easily with 132 services. I think a more scaleable way to address this issue is for the SDK teams to work with the services to more completely model their exceptions that can be returned.. Thanks for creating this issue @cristim. Is similar to #127 in the SDK's backlog? Or are you looking for an option to track the API calls that are made with the SDK?\nI think a feature similar to this can be enabled today with the SDK's request handlers. The Complete request handler is probably the easiest one to use for this tracking.\nThe following example will add a logger of service api calls for all APIs made from a client created from a session.\n```go\nsess := session.Must(session.NewSession())\nsess.Handlers.Complete.PushBack(func(r request.Request) {\n    fmt.Printf(\"region:%s, service:%s, api:%s\\n\",\n        r.Config.Region,\n        r.ClientInfo.ServiceName,\n        r.Operation.Name,\n})\n// make service api calls, \n``. Thanks for reporting this issue @nickrobinson. I think this is related to the idea that the leading slash  of/records` will be squashed by the SDK's URL encoding. The SDK provides the configuration flag DisableRestProtocolURICleaning. This flag would instruct the SDK to ignore its cleaning URL rules which causes the topic's leading slash to get squashed with the previous URL path elements trailing slash.\nSince IoT Publish's Topic value is not URI encoded the SDK doesn't know if the leading slash is intentional or accidental.  We should explore if it would be a breaking change if the SDK can enable this DisableRestProtocolURICleaning by default for IOT. . Hi @nickrobinson I'm trying to reproduce this issue, but it looks like I'm unable to create a thing name with a leading / On the console I see the error, Must contain only alphanumeric characters and/or the following: _-. \nI think the correct naming for this thing should of been records. Thanks for the clarification. Iw as thinking this was the thing, instead of the topic. an example would be very helpful.. Thanks for the update @nickrobinson! I've been able to reproduce this issue.  After reviewing the issue and trying a couple alternatives I don't think the SDK can make a change to how it processes this value by default. If the SDK were to change its default users of the SDK already publishing to topics may see their topic names change without any warning. This change in behavior would be a breaking change to those users.\nThe best way to address this issue in the v1 SDK is for users to set the DisableRestProtocolURICleaning configuration flag. In the v2 SDK we can change this behavior for the SDK in general to not perform any cleaning. aws/aws-sdk-go-v2#120.. I've verified and reproduced this bug. The SDK's session.New was not using the SDK's default filepaths for the shared configuration files that were updated in 5f43689.\nPR #1770 fixes this bug by moving the shared configuration file default paths to the envConfig loading to be used if not set.. Thanks for reaching out to us @bhmorse. The best way to allow a streaming interface for the lex-runtime's PostContent API is by wrapping your io.Reader in the SDK's ReadSeekCloser. The aws.ReaderSeekerCloser type is used by the SDK to allow streamable content to be used. This type is only usable with APIs which do not require the body to be signed.\nIn addition, using the aws.ReaderSeekerCloser, the SDK will not be able to retry the request without errors. Looking through the SDK's codebase it doesn't look like it correctly disables API retry of streamed content.\nThe SDK's InputStream is an io.ReadSeeker so the SDK can retry failed requests automatically. Generally the SDK needs to be able to seek the body in order to support the AWS Sigv4 request signature, but API calls like lex-runtime's PostContent explicitly disable including the body hash in the request signature. . @bhmorse thanks for the update could you describe, or paste the error that you're reciving?\nI'm using the following code as a test and unable to reproduce the issue you're seeing. Is your application's buf being written to by a separate goroutine?  If so that might be the issue. I think you need to use an io.Pipe so the Go HTTP Request won't thing the buf has been fulling read.\n```go\npackage main\nimport (\n    \"bytes\"\n    \"fmt\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/endpoints\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/lexruntimeservice\"\n\n)\nfunc main() {\n    sess := session.Must(session.NewSession(&aws.Config{\n        Region: aws.String(endpoints.UsEast1RegionID),\n    }))\nsvc := lexruntimeservice.New(sess)\n\nbuf := bytes.NewBuffer([]byte(\"hello hello hlello\"))\n\nreq, resp := svc.PostContentRequest(&lexruntimeservice.PostContentInput{\n    BotAlias:    aws.String(\"alias\"),\n    BotName:     aws.String(\"TestBotName\"),\n    ContentType: aws.String(\"text/plain; charset=utf-8\"),\n    UserId:      aws.String(\"user\"),\n    InputStream: aws.ReadSeekCloser(buf),\n})\n\nerr := req.Send()\n\nfmt.Println(resp, err)\n\n}\n``. Also in my testing I also found that the SDK's handling of debug logging doesn't play nicely with a streamed payload. #1778 Fixes most of these issues, but needs to be updated to work with the logging bug.. The following is adds usage of anio.Pipe` to the above example.\n```go\npackage main\nimport (\n    \"fmt\"\n    \"io\"\n    \"time\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/endpoints\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/lexruntimeservice\"\n\n)\nfunc main() {\n    sess := session.Must(session.NewSession(&aws.Config{\n        Region: aws.String(endpoints.UsEast1RegionID),\n    }))\nsvc := lexruntimeservice.New(sess)\n\npr, pw := io.Pipe()\n\ngo func() {\n    for i := 0; i < 5; i++ {\n        pw.Write([]byte(\"hello there \"))\n        time.Sleep(10 * time.Millisecond)\n    }\n    pw.Close()\n}()\n\nreq, resp := svc.PostContentRequest(&lexruntimeservice.PostContentInput{\n    BotAlias:    aws.String(\"alias\"),\n    BotName:     aws.String(\"TestBotName\"),\n    ContentType: aws.String(\"text/plain; charset=utf-8\"),\n    UserId:      aws.String(\"user\"),\n    InputStream: aws.ReadSeekCloser(pr),\n})\n\nerr := req.Send()\n\nfmt.Println(resp, err)\n\n}\n``. In #1776 also found that the SDK's debug logging clobbers the unseekable body.  I think this is starting to show there are potential other edge cases trying to shim a streamable content into the SDK when it was built  forio.ReadSeeker`. \nIt is possible full support for this feature will be needed for v2 to prevent unintended bugs, and allow correcting of the SDK's underlying library.. Could you update the description to state which class of fields where being left out. Current is a little vague. From changes looks just like nested lists, but update description would clarify.. Thanks for creating this issue @a-h. I think the Content-MD5 value is required to be in the header for S3 API requests. Though it is in the querystring with the URL @edwardbrowncross  posted, the request's signature is still calculated with the Content-MD5 value as a header.\nFrom testing I'm not seeing Content-MD5 being used by S3 if included as a query parameter. It only is used by S3 if it is a header in the request.\nIn my test I modified the SDK to put the Content-MD5 in the query and generated a resigned URL. With the Content-MD5 in the querystring i'm able to upload arbitrary content different from the MD5 the presigned URL was created with.. Thanks for creating this issue @elegos. Do you consistently see this error, or is it seemingly random? I think the SDK could do a better job handling invalid message bodies from the services. Ideally the service shouldn't be sending HTML messages, but i think in rare cases these can come back from the load balancers.\nIf you're able to provide any kind of reproduction or example case that would greatly help us identify this issue.. Thanks for the update, do you have a code sample that reproduces this issue or a debug message with the HTTP body response?\nyou can enable SDK debugging with the aws.LogDebugWithHTTPBody LogLevel\ngo\naws.Config{\n    LogLevel: aws.LogLevel(aws.LogDebugWithHTTPBody),\n}. Thanks for reporting this issue in v1.15.30 the SDK released a change that will ensure the SDK retries these serialization errors. If the requests continue to fail after the retries are exhausted, the error returned will also include the request ID for working with AWS support.. Hi @doertedev The EC2 service team got back to us and their feedback was that you should use the ec2.ResourceTypeSnapshot for this API call. In the context of the CreateSnapshot call the resource must be snapshot. See if changing to this resource type corrects the issue you're experiencing.. Thanks for letting us know about the documentation clarity with DownloadWithIterator @random9s. We'll put out an update for this doc example to correct the issue.. @Quentin-M This functionality actually had to be disabled while we resolve a issue with Content-Encoding and the SDK failing to validate the trailing checksum correctly, #1843. Once content encoding is fixed the SDK will do end-to-end MD5 validation for:\n\nS3 Manager Uploader\nS3 Manager Downloader\nS3 GetObject\nS3 PutObject\nS3 UploadPart. Hi @timduhenchanter thanks for reaching out to us. The best place to request this feature would be directly to the CloudWatch team's AWS forums. The service's AWS forums will help alert the service about this feature.. Thanks for letting us know about this feature request @cboss24. I believe the process credentials provider is currently a AWS CLI only feature. We'll add this feature to our backlog. Also, we'd be glad to review a PR if you are interested is submitting an implementation.. Thanks for reporting this issue. This looks like a unexpected condition where the content length is not set but trailing MD5 is still provided.  We'll need to investigate how to fix this issue. The fact a content-length isn't being set but the trailing MD5 checksum still provided will require more investigation and a fix to this scenario.\n\nI'm not yet sure where the GetObjectOutput.ContentLength value is coming from, because response wire log doesn't seem to have this value as the Content-Length header.\n@patrobinson Do you always receive this error for these objects, or is the error only received a % of the times GetObject is called?. @patrobinson an immediate workaround that you can do is to disable the content MD5 validation. Setting the Config.S3DisableContentMD5Validation flag to true will disable this validation.\n```go\n// Update configuration for S3 service client.\ns3Svc  := s3.New(sess, &aws.Config{\n    S3DisableContentMD5Validation: aws.Bool(true),\n})\n// Your existing code\nparams := &s3.GetObjectInput{\n    Bucket: aws.String(bucket),\n    Key:    aws.String(key),\n}\nresponse, s3_err := s3svc.GetObject(params)\nif s3_err != nil {\n    return \"\", s3_err\n}\n``. Thanks for the feedback. We're investigating how the SDKs can handle the case where a trailing checksum is present, but the SDKs won't have the ContentLength.. Discovered the root cause of this issue. The S3 object is agziparchive and has theContent-Encoding: gzipS3 object header/metadata set. When the Go HTTP Client is used to request this content the HTTP client's default transport will automatically start to wrap thegzip'ed content with agzip.Reader. In doing so, the HTTP client also removes theContent-Length` response header. This is what the SDK is triggering the error with.\nThe SDK for GetObject API request preemptively set the Accept-Encoding: gzip header which will prevent the HTTP client's transport from automatically handling the gzip decoding. In place of this the SDK would need to wrap the gzip'ed content with its own gzip.Reader. Verifying the trailing MD5 checksum after the gzip'ed content is read.. Closing this issue as the SDK has been updated, disabling checksum validation errors. We will have future work to re-enable checksum validation in the future that correctly handles content-encoding. Hi @alwindoss  are you able to use the DynamoDB client's QueryPages pagination method to paginate the responses?  The SDK should automatically be using the LastEvaluatedKey fields to paginate the query response.. For the SDK's paginators that Boolean is only if you want your application to continue within the scope of the available pages. The QueryPages will automatically stop paging when it knows it reached the last page.\nThe returned bool is only if your application wants to stop paging early.  This is a confusion that we are looking to correct in the v2 sdk. It uses an iterator pattern instead of callbacks for pagination.. Thanks for the feedback on the documentation of the pagination. We'll look into cleaning that up.\nEach iteration of QueryPages will return a new QueryOutput. It should be safe to store and mutate this value as needed without worry of the paginator mutating QueryOutput also.. Thanks for bringing this issue up to us @swt2c are you seeing IoT services fail with signature errors? or is this error only being encountered with Presigned URLs?. Thanks for the update and feedback for this issue. I'm updating this to be marked as a feature request. I think the best way for the SDK to handle the case we're the value must be used in the signed header or query should be driven by a flag in the signer.  Hopefully we can come up with a way to model this behavior per service, e.g iot, where the feature is needed.. Hi @arthurpaimarnold from the error message you're receiving it looks like SwiftStack is sending back an HTML document not an XML on. The SDK requires that the error response be XML.. Hi @arthurpaimarnold i'm not familiar with S3 Browser. but from the original error message you posted Swift is returning an HTML page not XML. The SDKs don't understand how to process HTML responses. Amazon S3 uses an XML base protocol. Given that the HTTP status code is 403 (aka, not authorized) I'm betting that Swift is returning some kind of sign in page, or error page about the client's credentials not being valid.\nYou can view the contents of the full body response by enabling debug logging with the SDK by setting the LogLevel configuration value of the SDK when a S3 client is created.\n```go\ns3Svc := s3.New(sess, &aws.Config{\n    LogLevel: aws.LogLevel(aws.LogDebugWithHTTPBody),\n})\n// use S3 client as normal\n```. Thanks for the feedback and additional information.  I also updated your comment @valenlovii cleaning up the replied content copied into GitHub.\n@5k3105 I think part of the issue you're seeing is because the SDK's default behavior is to put th bucket name into the request's hostname instead of the URI Path. Since this service requires the bucket name to be in the URI path you can use the aws.Config.S3ForcePathStyle value in the config to instruct the S3 client to keep the hostname in the URI path.\nYou're previous example updated with the config option set.\n``go\nfunc get_exist() {\n    op :=2018041814_P2C1/L0/Ancillary/2018041814_P2C1_Extraction.pngcreds := credentials.NewStaticCredentials(\"abc\",xyz, \"\")\n    nsession, err := session.NewSession(&aws.Config{\n        Region:      aws.String(region),\n        Endpoint:    aws.String(https://test-s3.data.asd.org:443`),\n        Credentials: creds,\n        LogLevel: aws.LogLevel(aws.LogDebugWithHTTPBody),\n        S3ForcePathStyle: aws.Bool(true),  // Added force path style\n    })\n    if err != nil {\n        println(op, \"new session\", \"getExist\", err.Error())\n    } \n    svc := s3.New(nsession)\nparams := &s3.HeadObjectInput{\n    Bucket:   aws.String(\"bucket1\"),\n    Key:    aws.String(op),\n}\n_, err = svc.HeadObject(params)\nif err != nil { \n    println(op, \"getExist\", \"svc.HeadObject\", err.Error())\n}\nprintln(\"found\")\n\n}\n```. Thanks for the example @5k3105  I think the reason you're seeing a zerobyte upload is because \ngo\nif _, err := io.Copy(hash, f); err != nil {\nThis line will read until the end of the file. The offset position in the file is now at the end of the file, and there are 0 bytes renaming. To reset the offset position in the file you can do f.Seek(0,os. SEEK_SET) which will seek the file back to the beginning.. Thanks for the update. I would not of expected the cross complie to change the behavior of the program. Unless something is getting coppied that impacts the outcome of how the fileoffset is handled. If your able to reproduce the file offset not being consistent without the SDK I suggest creating a issue in the Go languages GitHub repo as this behavior should be consistent.\nWith that said the expected behavior requires the seek since the file offset is at the end of the file after the copy.. Hi @5k3105  Closing this issue since it looks like performing the seek resolved the issue you were experiencing. Let us know if you're still running into this issue, or have additional questions/feedback with the SDK.. Thanks for taking the time to investigate and create this PR @omeid. The change looks good.\nI couldn't find explicit documentation that states that a call for a resource within the same region must not include a presignedURL. I'll forward this to the service's team to help improve the documentation of this feature.. Hi @josselin-c Thanks for reporting this issue. I'll forward this information onto the service team for feedback.\nIn the short term you can manually disable usage of authentication for these requests by updating the request configuration's Credentials to be  credentials.AnonymousCredentials. \nThe following will instruct your applications client for this service to never sign InitiateAuth API requests.\ngo\nc := idp.New(sess)\nc.Handlers.Build.PushBack(func(r *request.Request) {\n    if r.Operation.Name != \"InitiateAuth\" {\n        return\n    }\n    r.Config.Credentials = credentials.AnonymousCredentials\n}). @tomvachon Thanks for reporting this issue. I think this is part of a mixup in the metadata the SDKs have from a few services which is causing this issue. While investigating this a workaround you should be able to use is:\ngo\nsvc := mediastoredata.New(sess)\nsvc.ClientInfo.SigningName = \"mediastore\". Related to #1751 and #1838. @tomvachon thanks for the update could you provide more details on the other services that are failing for you?. Thanks!. Updated codebase to use a new flag SigningNameDerived to ResolvedEndpoint instead of changing the values that EndpointFor's ResolvedEndpointcould return for the signing name when the signing name was not modeled.. Any endpoint which is specified by the user via the \"Endpoint\" config parameter requires the empty signingName check to ensure that the service modeled signing name is used. Needed for services like iot data plane and media store data.. Thanks for creating the issue for this bug @radeksimko. It looks like the SDK's model parser and code generation is stripping off theWAFfrom the error name due to the SDK's removal ofstuttering` prefixes. e.g waf package with waf type prefix.\nWe should investigate if the SDK can change its code generation logic to correctly render the string value of the error code with the WAF prefix without a breaking change to the const variable name. \nThere is a decent chance this error is impacting other service code gen cases as well.. Hi @tomvachon  we heard back from the service team. This API incorrectly includes the MaxResults as an API parameter. This parameter is not valid to use, as the API does not support pagination. We'll continue to work with the service team to improve this situation.. Thanks for reaching out to us @bensont1, It looks like the host's EC2 Metadata service is timing out. Does your application generally only run into this issue as startup, or are you seeing this issue also periodically during the execution of your application? \nIf you're running into this issue at startup you could take a look at the connection timeout being used by your application. By default the SDK will use the http.DefaultClient if none is provided when a Session is created. I think the default transport timeout is is 30 seconds. It is possible that your application is starting before the EC2 Metadata service is available. Increasing this timeout or increasing the number of MaxRetires in the Config might be able to resolve the issue.. Thanks for creating the PR @TheGUNNER13  I don't think that change fixes the issue as the order of expressions being evaluated doesn't change. Though the PR does improve readability of the method, so we can merge it based on that.\nThe EC2 Instance Role doc has a good explanation on the steps needed to assign an IAM Role to an EC2 Instance. \nWith a role assigned correctly the SDK should not need any additional configuration in order to retrieve the credentials for that role from EC2's Instance Metadata Service.\nThe EmptyEC2RoleList: empty EC2 Role list in the error message means either the EC2 Instance is not configured with an IAM role as expected. \nAre you running your application directly within EC2 Instance, or are you using some kind of container such as Docker, or proxy within the EC2 instance?. Thanks for the update @TheGUNNER13 I'm having difficulty reproducing this issue. I'm using the latest version of the SDK (v1.18.5). I see you're using v1.17.14, but that shouldn't cause and issue, but maybe updating to the latest version of the SDK might resolve the issue?. Thanks for fixing this typo @davidkretch. The change looks good. Thanks for reporting this issue @dav009. It looks like the SDKs' models for endpoint data is out of sync with the released regions. I'll reach out to our services and model providers to see why these are missing.. Hi @dav009 I researched this issue, and this region is intentionally excluded for all of the SDKs and Tools as ap-northeast-3 requires access request.\nYou can still configure the SDK to use this region by specifying the region as ap-northeast-3 when configuring the SDK in your application.\n. Thanks for reaching out to us @seongju, I think you're seeing this error because the writeToS3 function in your example is using session.Session value type, where it should be taking the pointer, *session.Session.  I think making this change will remove the issue you're seeing.\ngo\nfunc writeToS3(sess *session.Session, bucket, key string, data []byte) error {. Thanks for reporting this issue @eikenb. This is a known issue with the Documentation page that seems to be being caused by the way the pages are hosted. I agree this is very annoying experience. We're working with the appropriate internal teams to identify how this issue can be fixed.. Thanks for creating this example @xinst . Thanks for the feedback, @ibrt  would you mind creating a Github issue for this bug? I think we should update the original upload progress example to use similar output as the new download progress PR, #2456 . Thanks for creating this PR @micahhausler. We'll review this and get back to you with feedback.\nInitially my feedback is that the ProcessProvider's reading of the configuration file duplicates the logic in the session package's sharedConfig type. I don't think we want this file to be read multiple times. I think if we were going to add this feature, the session.sharedConfig and co types should be exported (probably to an external package similar to v2) instead of duplicating the logic. This was done for the V2 SDK's refactor of  configuration loading specifically for this reason.\nIn addition, I think we'd want to remove the auto loading of configuration file from the ProcessProvider, requiring the command to be set when the ProcessProvider was created. With this the ProcessProvider's role would be limited to executing the command and reading its input. The ProcessProvider wouldn't need to reach out to the configuration file directly. If a user wants to use the ProcessProvider outside of the default credential chain (i.e. session.NewSession) they would be able to use the would-be exported SharedConfig type to read the credentials file, and pass the command to execute to the ProcessProvider.\nSession's configuration currently reads the configuration file's contents and determines which credential provider to use based on configuration of the Session's Options and content of the configuration file. The defaults.CredChain only really exists for backwards compatibility for users if they want to original features, but it is not used by the SDK.. Thanks for taking the time to create this PR @micahhausler. Since we have two PRs for similar features. I think we should consolidate, and continue this discussion on #2217.. Thanks for reporting this issue @jcoyne. I think this is a duplicate of the outstanding feature request #682. Generally empty lists are invalid DynamoDB attribute values. This is why the SDK uses NULL. Though #682 highlights this is not always the the case. +1'ing that feature request will help us prioritize it in the SDK's backlog.. Thanks for reporting this issue @kmcampott and providing a runnable example. I'll see if I can reproduce this issue. Do you see this issue consistently regardless of the metric namespaces used? Is the issue consistent or transient?. I was able to reproduce the issue. It looks like the SDK is not parsing the Timestamps tag from the response as we would of expected it to.\n```go\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 200 OK\nContent-Length: 608\nContent-Type: text/xml\nDate: Fri, 13 Apr 2018 17:21:46 GMT\nX-Amzn-Requestid: 24c415c0-3f3f-11e8-9c07-596789d07656\n\n2018/04/13 10:21:46 \n\n\n\n\n2018-04-13T00:00:00Z\n\n\n6.48820321E8\n\nm1\nBucketSizeBytes\nComplete\n\n\n\n\n24c415c0-3f3f-11e8-9c07-596789d07656\n\n\n```. Hi @kmcampott I put out fix #1894 which address this issue. We're going to be working to verify this fix, and make sure it doesn't have any regression impact on other XML based services. In the meantime you could use the version of the SDK in the PR as a workaround until the changed is merged into master.. Hi @kmcampott we've merged this change in. It will be tagged in the SDK's next release. Let us know if you run into addition issues. Thanks again for reporting this issue.. Merge pending until we're able to regression test this with the other REST XML protocol services.. Hi @wolfeidau correct the S3Select API requires the EventStream feature that is not yet available in the SDK. We're working on adding this feature. Once implemented the SDK will remove the S3 Select suppression.\nI'll update this to a feature request to track S3 Select + event stream support in the SDK.. Thanks it would be great to have more test points of this once I get a PR  up. I'll update this issue once I get that PR ready. Hi @wolfeidau I created PR #1941 which will add support for the S3 SelectObjectContent API. If you get a chance to take a look at the proposed API usage in the PR description it would be great to get your feedback on the change.. Thanks for the feedback @wolfeidau. the EventStream implementation I included with the PR should read the entire message payload (e.g JSON) from the service's response message.  It uses the EventStream Message's length prelude values to determine how many bytes to read from the wire. This implementation will always read full the payload content into memory since the maximum size of a payload must be 16MB or less. This implementation reuses a byte slice buffer between events reducing the overall impact on memory.\nFor S3 SelectObjectContent I'd expect the RecordsEvent's Records byte slice to be a complete JSON document. Are you seeing this not being the case?. Thanks for creating these paginators @aditya87 These paginators are modeled by the service team. We'll need to get them to make the change on their service models. If we apply this change the next  API update by AWS Batch will stomp over these changes.\nI'll reach out to the service team about adding this feature to their service model.. @aditya87 Thanks for posting this PR I've passed along this request to the service team, and it is in their backlog.. Since this is a change that will need to be performed by the AWS Batch team I'm going to close this PR. Thanks again for your time creating this PR, and please let us know any feedback about the SDK you have, or other suggested changes and additions.. HI @AkhterAli thanks for reporting this issue. It sounds like this is an issue with the GovCloud version of SQS service not the SDK. I can forward this feedback internally. With that said if you're able to submit a support ticket I suggest doing that as the support person may be able to get more details on the issue for you. Alternatively, you could submit feedback on the SQS TagQueue API docs directly. This feedback will go directly to the SQS service teams.. I heard back from the SQS service team. Tagging is not yet available for the GovCloud regions, but it is on their roadmap.. @AkhterAli, We'd prefer to close this issue as the missing feature is with the service and not the SDK. I don't think the SDK will have visibility/notification when SQS Tagging in GovCloud becomes available. Reaching out directly to SQS team via the AWS Forums is the best way to help the hear the need for this feature.. @tosh001 are you still running into this issue? . Thanks for the update @tosh001, let us know if you run into additional issues, or have feedback for the SDK.. Thanks for creating the PR @varroba. The API code is actually generated from models provided to the SDK from the service teams. In this case of GuardDuty it looks like the service is either incorrectly sending an integer value, or the modeling of the API is wrong. From #1716 it looks like we reached out to GuardDuty in the past, but i doesn't look like an update was provided. I'll reach out to guard duty again and see what is the issue with this API.. Closing this PR per the discussion, and pending reproduction in #1716.. Thanks for creating this issue @joestump. Correct, the API code is generated from models in the SDK's models/apis folder. The SDK sources these models directly from the service teams. I'll reach out to the CodePipeline team to see why this API is not modeled. . Thanks for the update. With the API feature now available I'll close this issue. Let us know if you run into additional issues, or have feedback about the SDK.. Thanks for reporting this issue @fkerlach. Do you consistently see this error, or is it transient? We'll try to reproduce the issue. From your description of the error it sounds like the Service is sending back a XML or HTML document instead of a valid JSON response message.\nIt is possible this issue is related to #1811 where SQS service would sometimes send back an HTML document instead of XML.. Hi @lookstar thanks for reaching out to us. I'll look back and verify this, but I think the SDK is using a commit due to a feature or change that is not captured by the 0.2.2 tag.  We can reach out to the go-jmespath repo to see if they can tag their latest release.. @lookstar thanks for taking the time to create this issue. I've reached out to the go-jmespath maintainer asking for an updated version tag. In the meantime I've also created a PR #2284 to update the SDK to the latest version of go-jmespath.. Thanks for creating this PR @johanneswuerbach, was this a recent issue that started to fail?  I noticed the Ruby issue is back in 2015 so i'm curious if this is a new regression with CloudWatchLogs, or a bug that has always existed.\nIs there a specific condition where this error occurs with CloudWatchLogs, does this error occur for any pagination of the CloudWatchLogs GetLogEvents API?. Thanks for taking the time to create this PR @martonsereg. The change looks good, and thanks for adding the addition documentation to the description accessor method.. @ken5scal Reaching out to GuardDuty service team directly via the AWS Forums like @xibz  mentioned is the best way for the service team to look at adding these constants to their service.\nSince this issue is a service API issue and not an SDK issue I'm going to close this issue. Let us know if you have additional questions or feedback about the SDK. The AWS Forums is a great place for service specific questions, and feedback.. Hi @forensicsguy20012004 thanks for reaching out to us. For General API questions the S3 AWS forums are generally the best place to get quick answers to API specific questions.\nI think most of those property groups come from specific API calls.\n GetBucketVersioning -> Versioning\n GetBucketLogging -> Logging\n GetBucketWebsite -> Static Website Hosting\n GetBucketEncryption -> Default encryption. Hi @alexd765 thanks for reaching out to us. Yes the SDK should be working in the AWS China region. The SDK includes the region information for the AWS China regions.\nIf your running into any issues please let us know. I'll forward the need to add the SDK to the AWS China document pages. Thanks for taking the time to fix these typos in the SDK @tbroadley. I'll merge these changes in.. Thanks for taking the time to report this issue @shabbyrobe. Did you experience an error with one of the AWS services due to the SDK's usage of GMT instead of MST? If so which service API(s) did you encounter an error with.\nThe SDK uses GMT as the format to encode date time as per the HTTP RFC2621. With that said the SDK may be able to change the Parse format to not be fixed to GMT and be more flexible using something like MST.\nI'll update aws/aws-sdk-go-v2#178 to be a tracking issue of this.. @jazzyarchitects Does your use case require the body to be passed through the json.Marshal? If body is a series of ascii characters the json.Marshal actually won't do anything other than quote the body's content turning it into a JSON String. The data stored in Kinesis is a binary blob so additional quoting shouldn't be needed in storage. Have you tried setting the body value to the record directly?\n```go\nfunc sendLogToFirehose(firehoseSession *firehose.Firehose, body []byte, deliveryStream string) {\n    putRecordInput := &firehose.PutRecordInput{}\n    record := &firehose.Record{}\nlog.Println(\"Read Body \", string(body))\n\nrecord.SetData(body)\n\nif err := record.Validate(); err != nil {\n    log.Panicln(\"Record Validation error \", err)\n}\n\nputRecordInput.SetDeliveryStreamName(\"test-delivery-Stream\")\nputRecordInput.SetRecord(record)\n\nres, err := firehoseSession.PutRecord(putRecordInput)\nif err != nil {\n    log.Println(\"Error sending to firehose\")\n    log.Panicln(err)\n}\n\n// if false {\nlog.Println(\"Sent to firehose\", res)\n// }\n\n}\n```. @pwaller thanks for letting us know this wasn't in the release notes. We'll make sure these are all included, and will go back and update the associated release notes.\nThe best way to preserve the functionality of looping forever is to use the API directly, or you can use the SDK's Pagination type. This is what is being done within the GetLogEventsPages method.\n```go\nsvc := cloudwatchlogs.New(sess)\ninput := cloudwatchLogs.GetLogEventsInput{\n    // Input parameters ...\n}\n// Create a Pagination for the API to automatically handle paginating each page. \np := request.Pagination{\n    // Comment out to rely on CloudWatch Log's loop forever behavior.\n    // EndPageOnSameToken: true,\n// Request builder to provide a new SDK request per page. Takes a copy of\n// the input parmeters.\nNewRequest: func() (*request.Request, error) {\n    inCpy := *input\n    req, _ := svc.GetLogEventsRequest(&inCpy)\n    // TODO Set request options as needed\n    return req, nil\n},\n\n}\n// Iterate through the pages calling Page() to get the current page. Next requests the next page.\nfor p.Next() {\n    page := p.Page().(*cloudwatchlogs.GetLogEventsOutput)\n}\n// Check for pagination error.\nif err := p.Err(); err != nil {\n    return err\n}\n``. Thanks for submitting this request. I tagged this as a feature request so we can priotorize it in our backlog. We want to add support for the S3 manager to use thegithub.com/aws/aws-sdk-go/service/s3/s3crypto` library for client side encryption.\nI think some tweaks are needed to the s3crypto libraries implementation to correctly encrypt the parts so that the full object can be downloaded and decrypted. Right now the s3crypto package only support PutObject, support for UploadPart is needed.. Thanks for reaching out to us @dlsniper was this example in the SDK or SDK's docs?  It looks like the awslabs instead of aws github.com org is being used.\nDoes the problem go away when changing the imported packages to the following?\ngo\nimport (\n    \"github.com/aws/aws-sdk-go/aws/session\"\n    \"github.com/aws/aws-sdk-go/service/s3\"\n)\nThe github.com/awslabs/aws-sdk-go repo should no longer be valid, and was the SDK's home prior to its initial 1.0 release.. Thanks for reporting this issue @vanniszsu I'll reach out to the service team to get this modeled information correct.  In the short term you can define a custom endpoint resolver which will workaround this issue.\n```go\ncustResolverFn := func(service, region string, optFns ...func(*endpoints.Options)) (endpoints.ResolvedEndpoint, error) {\n    if service == applicationautoscaling. EndpointsID && strings.HasPrefix(region, \"cn-\") {\n        cnEndpoint := \"https://autoscaling.{region}.amazonaws.com.cn\"\n        cnEndpoint = strings.Replace(cnEndpoint, \"{region}\", region)\n        return endpoints.ResolvedEndpoint{\n            URL:           cnEndpoint,\n            SigningRegion: region,\n        }, nil\n    }\nreturn defaultResolver.EndpointFor(service, region, optFns...)\n\n}\nsess := session.Must(session.NewSessionWithOptions(session.Options{\n    Config: aws.Config{\n        Region:           aws.String(\"cn-north-1\"),\n        EndpointResolver: endpoints.ResolverFunc(custResolverFn),\n    },\n}))\n// Create service client from the session.\nsvc := applicationautoscaling.New(sess)\n// Make API calls to the service.\n``. Thanks for reaching out to us @peterabarry could you clarify the Go type for thefilevariable?  is this anos.Fileor some other type?  When the file is uploaded is theContentType` of the file set correctly?. Thanks for the update @peterabarry. I'm having difficulty reproducing this issue.  I'm using the following code to upload an object to S3.\n```go\npackage main\nimport (\n    \"bytes\"\n    \"flag\"\n    \"fmt\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\"github.com/aws/aws-sdk-go/service/s3/s3manager\"\n\n)\nfunc main() {\n    sess := session.Must(session.NewSession(&aws.Config{\n        LogLevel: aws.LogLevel(aws.LogDebug),\n    }))\n    if sess.Config.Region == nil {\n        sess.Config.Region = new(string)\n    }\nvar bucket, key string\nflag.StringVar(sess.Config.Region, \"region\", *sess.Config.Region, \"The `region` to get the object from.\")\nflag.StringVar(&bucket, \"bucket\", \"\", \"The name of the `bucket` to get the object from.\")\nflag.StringVar(&key, \"key\", \"\", \"The S3 object `key` name to get.\")\nflag.Parse()\n\nuploader := s3manager.NewUploader(sess)\n\nbody := bytes.NewBuffer(make([]byte, 0, 1024*1024*20))\nbodySize := body.Cap()\nfor i := 0; i < bodySize/len(\"hello\"); i++ {\n    body.WriteString(\"hello\")\n}\n\nresp, err := uploader.Upload(&s3manager.UploadInput{\n    Bucket:               &bucket,\n    Key:                  &key,\n    ContentType:          aws.String(\"image/jpeg\"),\n    ServerSideEncryption: aws.String(\"AES256\"),\n    Body:                 body,\n})\nif err != nil {\n    panic(err)\n}\n\nfmt.Println(\"Upload Response\", resp)\n\nsvc := s3.New(sess)\nobjResp, err := svc.HeadObject(&s3.HeadObjectInput{\n    Bucket: &bucket,\n    Key:    &key,\n})\nif err != nil {\n    panic(err)\n}\n\nif v := *objResp.ContentType; v != \"image/jpeg\" {\n    panic(\"expect content type \" + v)\n}\nif v := *objResp.ContentLength; v != int64(bodySize) {\n    panic(fmt.Sprintf(\"expect content length %d\", v))\n}\n\n}\n```\nI can verify that the parts are being uploaded with the logged statements, and verifying the object's content is S3.\n2018/06/07 14:57:04 DEBUG: Request s3/UploadPart Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nPUT /myKey?partNumber=3&uploadId=EYrMnDXu4K1xI.jNzMl1KbM.12KmoKeLX1s7w8ICBxbs9S7rRYBDxSviWDEShcmZCMbruXkFzCnz2tt331GpVFoy4KpFY1w2T_y2qj0VmVqd_WzjZwwSdMUvWc8uOmob HTTP/1.1\nHost: myBucket.s3.us-west-2.amazonaws.com\nUser-Agent: aws-sdk-go/1.14.1 (go1.10.2; darwin; amd64) S3Manager\nContent-Length: 5242880\nAuthorization: \nContent-Md5: IrDWri0GeI7atmW0vCwROQ==\nExpect: 100-Continue\nX-Amz-Content-Sha256: 50c5711f72196fb29755b590d503d7772c8e7953b37f30ff5d7baf7445091490\nX-Amz-Date: 20180607T215704Z\nAccept-Encoding: gzip. Thanks for the update @peterabarry. We can leave open for a little bit sure.\nThe ACL shouldn't impact the body as that is a header value. And defer close won't have any impact because that is just a clean up after the PutObject API is finished.\nWhat I think may actually be the fault here is reusing the file. In general it is invalid to reuse and io.Reader value between operation. Especially in this case the first use of the file will upload the content, but all other probably are uploading a zero object to S3. This would be my first bet for why you are seeing zero byte object.\nI'd take a closer look at how that file value is being used. Reusing the same file between requests without special logic to seek to the beginning in of the file before the next usage (file.Seek(0,0)). If this is the case there is a good chance this could break in the future due to some other usage.. Thanks for the update @kks32. In your use case were you intentionally uploading files with zero bytes, or was that an unexpected error that was occurring? If you have a code example that would be very helpful to help us investigate this issue further.  Is the content that your application uploaded, offset somewhere within a source io.Reader?\nThe SDK's Upload manager should of always started from the io.Reader's current offset. If the SDK was seeking back to the head of a file this would of been a bug, and unexpected behavior. If the io.Reader is also an io.ReadSeeker the SDK will attempt to seek within its content to determine the content's size and compute the AWS request signature. Seeking within the io.ReaderSeeker's content should always go back to the original offset the io.ReadSeeker was at when provided to the SDK.\nI think this was a bug fixed in v1.12.77 e926b11. Regretfully it looks like this change wasn't added to the SDK's change log. I've gone back and fixed this.. It looks like seeking the buffer after it was read prior to uploading with the SDK resolved the issue you were experiencing. Let us know if you run into future issues with the SDK or have questions.. Hi @jney thanks for reaching out to us. The best place to get an answer to your question is over at the Dax AWS Forums.\nThe SDK is built from models defined by the services teams. The specific service teams control which APIs are available. Reaching out to the Dax support via the AWS forums will help answer the question your asking.. Thanks for reaching out to us @discordianfish. The issue you're running into sounds like a usability issue with the SQS service API. The best way to reach out to the SQS service is via the AWS Forums, or submitting feedback in the lower right hand corner via their API reference docs.. Thanks for the clarification. This sounds like a bug in the API or its docs. The SQS service team is the best place to reach out to let them know that the API isn't operating as expected. The best place to do this is via the AWS Forums or the API reference docs linked above. The Feedback button on the API reference docs pages will send the feedback directly to the service team's doc maintainers.. Hi @songpengwei thanks for reaching out to us. It looks like the SDK should not be including this value for UNSIGNED-PAYLOAD presigned URLS. S3 requires the X-Amz-Content-Sha256 value to be a header if its value is not UNSIGNED-PAYLOAD.. Thanks for letting us know about this bug @songpengwei. We've committed a fix for the issue. It is available now on tip of HEAD, and it will be included in the SDK's next tagged release.. Thanks for making the updates @kalafut . The changes look good. I'll get this merged in and it will be included in the SDK's next release.. Hi @EduardShaid thanks for reaching out to us. I think the AWS SDK for CPP's aws-cpp-sdk-states is the same as the other SDK's sfn or AWS Step Functions\nIs the AWS Step Functions service the thing you are looking for?. Thanks for making that update @davidkretch. Change looks good.. Thanks for letting us know about this issue. These models are provided by the API Gateway service to the SDK teams. I'll reach out to the API Gateway service team to let them know that the API.. Thanks for reaching out to us @kundankumarjha. Could you clarify which AWS service you're experiencing this issue with? I think the DescribeTags API is actually used by a few AWS services.\nIn addition if you could provide an example of how you're using the API and if possible a RequestID of the response you expected to see multiple pages that would be very helpful.. Thanks for the update @kundankumarjha  I'm running into difficulty reproducing this issue. I'm using the example below to paginate the DescribeTags API calls.  In my case i have a filesystem with 21 tags. My sample gets all 21 tags in three API calls with 10, 10, 1 tags in the response.\nDoes your application set the MaxItems parameter of DescribeTagsInput? This could be limiting the number of tags returned.  In addition make sure theNextMarker of DescribeTagsOutput us used as the value for Marker of DescribeTagsInput's next API call.\n```go\npackage main\nimport (\n    \"fmt\"\n    \"os\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/efs\"\n\n)\nfunc main() {\n    sess, err := session.NewSession()\n    if err != nil {\n        panic(err)\n    }\nsvc := efs.New(sess)\n\nparams := &efs.DescribeTagsInput{\n    FileSystemId: aws.String(os.Args[1]),\n}\n\nresp, err := svc.DescribeTags(params)\nif err != nil {\n    panic(fmt.Sprintf(\"failed, initial describe, %v\", err))\n}\nfmt.Println(len(resp.Tags), resp.Tags)\nfor resp.NextMarker != nil {\n    params.Marker = resp.NextMarker\n    resp, err = svc.DescribeTags(params)\n    if err != nil {\n        panic(fmt.Sprintf(\"failed, describe, %v\", err))\n    }\n    fmt.Println(len(resp.Tags), resp.Tags)\n}\n\n}\n```. Thanks for the update @kundankumarjha . I'm going to close this issue for now. Let us know if you run into additional issues, or have any feedback, ideas for the SDK.. I think this is related to #1639 for using Sets with the SDK. By default the SDK's attribute value marshaler from go type has no information if a slice is a set or list. The SDK defaults to list.\nYou can override this functionality with the stringset struct tag on the member:\ngo\ntype Foo struct  {\n    Field []string `dynamodbav:\",stringset\"`\n}\nOr implementing the Marshaler interfere.\ngo\ntype ExampleMarshaler struct {\n    Value int\n}\nfunc (m *ExampleMarshaler)  MarshalDynamoDBAttributeValue(av *dynamodb.AttributeValue) error {\n    n := fmt.Sprintf(\"%v\", m.Value)\n    av.N = &n\n    return nil\n}\nDocs for the marshaler used by the expression builder. https://docs.aws.amazon.com/sdk-for-go/api/service/dynamodb/dynamodbattribute/\n. Added an example to #2360 defining a string set custom marshaler.. @mikkeloscar thanks again for reaching out to us with this feature request. We decided that while this feature can be useful, bundling it into the SDKs introduces the potential for race conditions, and cross platform compatibility support. Performing atomic file locking and change notification do not have feature parity across platforms, and can introduce unexpected instability.\nI think there are a couple alternative ways your IAM Controller for Kubernetes could solve the EC2 Metadata request timeout (<1s) issue you're seeing.\nGeneric HTTP Credential provider\nThe SDK includes a Generic HTTP Credential Provider which will attempt to retrieve credentials from an HTTP endpoint on the 169.254.170.2 IP. This credential provider supports expiration just like EC2 Instance roles. Your application(s) can use this credential provider directly via code, or rely on the SDK's default credential chain using the environment variable, AWS_CONTAINER_CREDENTIALS_RELATIVE_URI to retrieve credentials from an HTTP endpoint similar to what your Kubernetes Controller is doing with the EC2 Metadata proxy. The AWS_CONTAINER_CREDENTIALS_RELATIVE_URI environment variable's value is a URI path that the SDK will make an HTTP request to. Even better, this credential provider is supported by the default chain across all of the AWS SDKs.\nIn the following example the SDK will make an HTTP request to http://169.254.170.2/MyCreds, if no other credentials are found in the credential chain, (i.e no environment, nor file credentials)\nAWS_CONTAINER_CREDENTIALS_RELATIVE_URI=/MyCreds ./MyApp\nThe HTTP response body is a JSON value containing the credentials the SDK should use. See the IAM Roles for Tasks documentation for more details of this JSON value.\njson\n{\n    \"AccessKeyId\": \"ACCESS_KEY_ID\",\n    \"Expiration\": \"EXPIRATION_DATE\",\n    \"RoleArn\": \"TASK_ROLE_ARN\",\n    \"SecretAccessKey\": \"SECRET_ACCESS_KEY\",\n    \"Token\": \"SECURITY_TOKEN_STRING\"\n}\naws.Config.EC2MetadataDisableTimeoutOverride\nAlternatively, setting the aws.Config.EC2MetadataDisableTimeoutOverride configuration value to true within your Go application will prevent the SDK from using a significantly shorter request timeout, in comparison to the <1s timeout used by default when making EC2 Metadata requests.\nFor this solution I think we can also consider adding support for an environment variable which uses non-shortened EC2 Metadata request timeout by default without modifying the application(s) running in the pod directly.. Thanks for the feedback and update @mikkeloscar.\nIn the context of Kubernetes our concerns with the refreshing from a file are probably manageable. Given that Kubernetes would own the environment be responsible for writing the file as needed. Our biggest concern with this file being refreshed is two factor. Usage outside of Kubernetes, and that the shared configuration and shared credential files are interchangeable for the most part. This means that there are non-credential related information that would be refreshed. The additional information in the files doesn't make sense to be refreshed during the lifetime of an application.\nA http endpoint for retrieving credentials per pod provides the best flexibility for applications of users. The issue with only being able share a single environment variable for AWS_CONTAINER_CREDENTIALS_RELATIVE_URI is problematic. Especially, if pods should have different roles they assume. But if all Pods are able to use the same environment variable using this method of setting up the HTTP endpoint is the best. In addition SDKs will not apply short timeouts for the endpoint since it must be explicitly enabled unlike EC2 Instance roles which is automatic.\nI think further discussing EC2MetadataDisableTimeoutOverride as an environment variable makes absolute sense.  Ideally this environment variable would be usable across all the SDKs.\n. Thanks @mikkeloscar we'll be glad to review changes when we get closer to having a design for this feature. We'll be working with the other AWS SDKs in our designs to create a consistent experience and support.\nUsing a JSON file instead of the ini-like shared config file addresses the majority of our concerns about partial reads. The Go structure with a JSON file most likely would be similar to the endpointcreds.getCredentialsOutput type. The Expiration member would be serialized from a string formatted as RFC3339 time, e.g. (2006-01-02T15:04:05Z).\ngo\ntype RefreshableCredentials struct {\n    Expiration      *time.Time\n    AccessKeyID     string\n    SecretAccessKey string\n    Token           string\n}\nIn you're example you included a RoleArn member. Could you go into details more about the usage of this value? In general SDKs load this value from the shared config file (~/.aws/config) when loading configuration. Currently most SDKs don't support assuming a role with credentials which are not defined in the shared config/credential file (#1019), but I think also updating the SDK to support assuming a role with additional credentials would be the best way to support the Assume Role use case the RoleArn looks to facilitate.. Thanks for the clarification. I missed that when making my post. Agreed, assume role case would be a nice to have, but would probably not be a blocker.. Thank for reporting this issue @shabesoglu  could you please provide more details about what lead up to this panic? A sample of the code that encountered this would be very helpful in figuring out what happened here.  Also if you can provide a description of what the application did leading up to this failure would be very helpful.. Looking at the code for uploader.initSize  It looks like the panic is due to an invalid PartSize or MaxUploadParts specified in the configuration of the Uploader.\nPartSize is allowed to be '0' and falls back to DefaultUploadPartSize if its value is 0. \nWhen creating an Uploader the MaxUploadParts member is set to a default of the package const MaxUploadParts.  The MaxUploadParts member does not get automatically set to a default value if is zero in uploader.init.\nHow is your application initializing the Uploader? Is it using the provided constructor functions like NewUploader, or is the application using something else to create an Uploader value.. Updated S3 Upload manager's documentation in #2077, and provided a default value for MaxUploadParts being and invalid value of zero.. Change to RESTXML not complete, fails for S3 CompleteMultipartUpload. fixed RESTXML's handling of unnamed implicit payload marshaling.. Thanks for creating this PR @stefansundin. One solution to solving the path.Join issue would be to wrap the path.Join call detecting if last element has a trailing / and re-append it to the string.\nAlternatively, we could investigate the usage of URL.ResolveReference. This utility will preserve the trailing slash of resolved references. Though it wasn't added until Go 1.8, so a backported version would need to be embedded in the SDK for older versions of Go. Thanks for updating this PR @stefansundin I took your changes into #2037, and added using a helper utility which preserves the trailing slash when joining elements. This is done to ensure the SDK does not change in behavior if a user's code passes in a GetMetadata parameter that was previously being cleaned.. Thanks for posting this PR @canni. The SDK does have a backlog feature, #423 to perform clock skew correction automatically comparing local clock time and service time.\nIf an application where to set the ClockSkew field, I'm guessing this field would be static, and the application would no longer need/want to adjust the field through the life time of the program?. I think we'd prefer for the SDK to implement clock skew automatically similar to the AWS SDK for C++'s design, aws/aws-sdk-cpp#285. The SDK would need a way to synchronize the time offsets across request if it clock skew were to be performed automatically.. Thanks for reporting this issue @danielwhite I think you're correct the SDK's XML marshaler is intentionally excluding elements from the XML document if that structure does not have nested values. I know the SDK's JSON marshaler are able to handle this case to marshaler empty objects.\nPR #1998 adds additional tests from XML marshaling that we can use for validating this functionality. The SDK's xml marshaler is pretty fragile, additional tests need to be included with the fix for this.. Hi @ggaaooppeenngg have you tried setting the s3.PutObjectInput.ContentLength member? This should allow you to create a presigned URL with the content length header set.\n```go\nsess, err := session.NewSession()\nsvc := s3.New(sess)\nreq, _ := svc.PutObjectRequest(&s3.PutObjectInput{\n    Bucket: aws.String(\"bucket-name\"),\n    Key: aws.String(\"key-name\"),\n    ContentLength: aws.Int64(1234), // Content length value\n    ACL: aws.String(s3. ObjectCannedACLPublicRead),\n})\nurl, headers, err := req.PresignRequest(15*time.Minute)\n```. I think what you are looking for is the policy document for an upload. https://aws.amazon.com/articles/browser-uploads-to-s3-using-html-post-forms/ I think to use this feature the SDK needs additional customization it does not yet include.\n. Thanks for taking the time to create this PR @leonsim. I think this PR would create a breaking change in the SDK's s3manager and s3crypto packages. The Interface packages were created with the expectation that they will break as a service adds new APIs. The s3iface package does have an example how tests can be built with these interfaces which will not break when the service adds new APIs. Though, when the application uses the new APIs the test type would need to be updated.\nWith that said I think this is a feature that we can explore for the v2 SDK, in developer preview. I think it makes sense to explore this design for the V2 SDK's usage of service clients.. Thanks for the feedback @leonsim. In the general case most code using the SDK would not experience the breaking change. But, code using function's signature as a type would break because the signatures of the functions would no longer match. The breaking change would be the change of type of input parameter from s3iface.S3API to getObjectRequester.\nExample user code. The NewListIterFn member would no longer be compatible with the proposed s3manager.NewDeleteListIterator and similar changes.  https://play.golang.org/p/szaltzYybR2\ngo\ntype MyType struct {\n    NewListIterFn func(s3iface.S3API, *s3.ListObjectInput, ...func(*s3manager.DeleteListIeterator) BatchDeleteIterator\n}\nIn addition the unexpected interface definition will prevent godoc from documenting the listObjectsRequester type and required methods.\nI don't think we can make this change for the v1 SDK, but we should explore it for V2 to remove the dependencies on the *iface packages, and the associated added bloat the entail.\n. Hi @kevineaton thanks for reaching out to us. I'm having difficulties reproducing the issue you're running into.  I'm using the make sandbox-go19 to create a sandboxed container for testing, create a pkg which depends on the SDK, and able to do dep init and dep ensure without error.\nI think dep can use your working GOPATH for sourcing some of the dependencies. I'd check to make sure that if there is a version of the SDK in your GOPATH it is uptodate.\n```\n$> make sandbox-go19\n$> cd $GOPATH\n$> go get -u github.com/golang/dep/cmd/dep\n$> mkdir src/mypkg\nedit src/mypkg/main.go to depend on sdk\n$> dep init\n$> dep ensure\n```\nmypkg/main.go\n```go\npackage main\nimport (\n    \"fmt\"\n    \"github.com/aws/aws-sdk-go/aws\"\n)\nfunc main() {\n    fmt.Printl(\"blah\", aws.String(\"string\")\n}\n```\n```\n$> cat Gopkg.toml\ncat Gopkg.toml\nGopkg.toml example\n\nRefer to https://golang.github.io/dep/docs/Gopkg.toml.html\nfor detailed Gopkg.toml documentation.\n\nrequired = [\"github.com/user/thing/cmd/thing\"]\nignored = [\"github.com/user/project/pkgX\", \"bitbucket.org/user/project/pkgA/pkgY\"]\n\n[[constraint]]\nname = \"github.com/user/project\"\nversion = \"1.0.0\"\n\n[[constraint]]\nname = \"github.com/user/project2\"\nbranch = \"dev\"\nsource = \"github.com/myfork/project2\"\n\n[[override]]\nname = \"github.com/x/y\"\nversion = \"2.4.0\"\n\n[prune]\nnon-go = false\ngo-tests = true\nunused-packages = true\n[[constraint]]\n  name = \"github.com/aws/aws-sdk-go\"\n  version = \"1.14.21\"\n[prune]\n  go-tests = true\n  unused-packages = true\n```\n```\n$> cat Gopkg.lock\nThis file is autogenerated, do not edit; changes may be undone by the next 'dep ensure'.\n[[projects]]\n  name = \"github.com/aws/aws-sdk-go\"\n  packages = [\n    \"aws\",\n    \"aws/awserr\",\n    \"aws/credentials\",\n    \"aws/endpoints\",\n    \"internal/sdkio\",\n    \"internal/shareddefaults\"\n  ]\n  revision = \"16cb9f62f8a8b7af4f38725567f5ce762e1d562b\"\n  version = \"v1.14.21\"\n[[projects]]\n  name = \"github.com/go-ini/ini\"\n  packages = [\".\"]\n  revision = \"06f5f3d67269ccec1fe5fe4134ba6e982984f7f5\"\n  version = \"v1.37.0\"\n[solve-meta]\n  analyzer-name = \"dep\"\n  analyzer-version = 1\n  inputs-digest = \"041307ae303c224e2d5ceee5d0d6007a06766f8a50ac160dd40a658c2fba3b73\"\n  solver-name = \"gps-cdcl\"\n  solver-version = 1\n```. Thanks for creating this CR @usrenmae. Since this would be a behavior change I think this needs to be opt in via a flag in the unmarhsaler. I think the behavior is good, and we should integrate this as the default behavior for v2 if the response is an empty list is received from an API request.\nI think this PR is also related to #2105.. Thanks for taking the time to create this PR, i've created #2419 merging in the implementation of #2105, and also adds a MarshalOption.NilAsEmpty option that applies to all members (un)marshaled.. I think there might be something else at play here. I updated the s3manager.Uploader to persist a sync.Pool across all Upload calls, but was not able to see a meaningful difference.\n@rsm10 could you provide more information about how the Uploader is being used, e.g the type of value used for the UploadInput.Body parameter. Is the upload type a os.File, bytes.Buffer, bytes.Reader, etc? If you're able to profile the issue you're experiencing in more detail it would significantly help investigate this issue further.\nI used the following to benchmark the Upload: (I realized the previous version of my tested had a bug causing it to have a empty body for all but first request, but after fixing still not a noticeable difference)\n``go\n// go test -bench=BenchmarkUpload -benchmem -benchtime 10s -run NONE -cpu 1,2,4 ./service/s3/s3manager/\nfunc BenchmarkUpload(b *testing.B) {\n    createUploadBody := []byte(\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\nBucket\nKey\nuploadid\n\n`)\ncompleteUploadBody := []byte(`\n\n\n\n\n\n`)\nsvc := s3.New(unit.Session)\nsvc.Handlers.Send.Swap(corehandlers.SendHandler.Name, request.NamedHandler{\n    Name: \"mock.send\",\n    Fn: func(r *request.Request) {\n        io.CopyN(ioutil.Discard, r.HTTPRequest.Body, r.HTTPRequest.ContentLength)\n        req := r.HTTPRequest\n        var body []byte\n        switch {\n        case strings.Contains(req.URL.RawQuery, \"uploads\") && req.Method == \"POST\":\n            body = createUploadBody\n        case strings.Contains(req.URL.RawQuery, \"uploadId\") && req.Method == \"POST\":\n            body = completeUploadBody\n        }\n        req.Body.Close()\n        respHeader := &http.Header{}\n        respHeader.Set(\"Content-Length\", strconv.Itoa(len(body)))\n        r.HTTPResponse = &http.Response{\n            Status:        \"OK\",\n            StatusCode:    200,\n            Header:        http.Header{},\n            ContentLength: int64(len(body)),\n            Body:          ioutil.NopCloser(bytes.NewReader(body)),\n        }\n    },\n})\nu := s3manager.NewUploaderWithClient(svc)\n\nbodyBuf := make([]byte, 30*1024)\nb.ResetTimer()\n\nb.RunParallel(func(pb *testing.PB) {\n    for pb.Next() {\n        _, err := u.Upload(&s3manager.UploadInput{\n            Bucket: aws.String(\"Bucket\"),\n            Key:    aws.String(\"Key\"),\n            // Use a Bytes Buffer since it is not seekable.\n            Body: bytes.NewBuffer(bodyBuf),\n        })\n        if err != nil {\n            b.Fatalf(\"expect no error, got %v\", err)\n        }\n    }\n})\n\n}\n``. Thanks for the update @stefansundin I created aws-sdk-go-v2#198 to track this work being ported to the V2 SDK.. Thanks for contacting us with this feature request @zbintliff. Exposing a list ofProvidersin thedefaults` package.  We'd be glad to review a PR with a helper that returns a list of credential providers.. Thanks for putting this PR together @zbintliff. The change looks good. I'll merge this in and it will be included in our next release.. Thanks for reaching out to us @agorman. I think you're looking for the SetIdentityPoolRoles API operation of the Cognito Identity Service.. Thanks for reaching out to us @cah-andrewfitzgerald. This sounds like a feature request for the RDS service's API. The best way to reach out to the RDS service team is via the AWS Forums.\nIn addition on the RDS RestoreDBInstanceFromDBSnapshot API reference page the bottom right hand corner has a feedback button that you can submit feedback on the API.. Thanks for reaching out to us @mikaelrandy. Correct, the SDK does not currently support assuming a role from another role. The SDK only supports assuming a role with source_profile using static credentials. We have #1019 to expand this functionality allowing the SDK to assume a role using alternative credential providers.\nI'll mark this issue as a duplicate and keep #1019 as the tracking issue for this feature request.. Closing as duplicate of #1019. I think this feature is a feature the SDKs will need to support in the future and is apart of the AWS SDK for Go's backlog, but we've not specifically planned when the feature would be implemented though.. Thanks for reporting this bug @kundankumarjha. This looks to be an issue with how the SDK is generating its clients, and normalizing the API input/output API parameter type names. I'll update this issue once we have a bug fix for this issue.\nUpdateFileSystemOutput should of never renamed FileSystemDescription types.. This issue is a top priority for us fixing. I don't have an exact ETA, but we'll update this issue as soon as we have a pending fix.. Hi @kundankumarjha correct a fix is out in PR. Once it is reviewed and approved we'll merge it into the SDK. It will be included in the next release after the fixed is merged in.. @kundankumarjha thanks again for reporting this issue to use. I've merged in #2073 which fixes this issue, and it will be included in the SDK's next release.. Thanks for reporting this bug  in the SDK's endpoints.json file @x6j8x. The entry in the file should be autoscaling.{region}.amazonaws.com.cn.\nIt looks like we already have a issue for this bug #1957. Lets use that issue to track the bug. I'll reach out again to the service team about fixing this endpoint. In the meantime we'll look to see if we can add a customization to the SDK to fix this bug.. @danielvaughan thanks for the update. The error you're running into sounds like a Go package cache or import path conflict. Does the AWS SDK for Go exist in your GOPATH in multiple locations. Potentially as a vendored dependency of your application, or a library used by your application?  . @delitescere thanks for the feedback. The reason the change in behavior is the SDK looks at AWS_REGION first and only AWS_DEFAULT_REGION next if the SDK's support for the shared config is enabled via the session or AWS_SDK_LOAD_CONFIG=1 env is set.\nThe SDK requires an option support for the shared config files since its feature were added after the SDK initial released. Automatically supporting the shared config would of potentially change the behavior of user applications that were running in the same environment as the AWS CLI.. I think we need to combine the efforts of #2105 and #2032. We'll want to provide both the encoder/decoder configuration flag, and struct tag flag styles for enabling this feature. I think having  both config and struct tag support is the only piece missing from this PR.. I've pulled  down this change locally to take a look at adding the configuration flag.. Thanks for all the work putting this PR together. I've created #2419, pulling in these changes and added a NilAsEmpty marshaling option as well to apply the nilasempty pattern to all maps/slices by default.. Thanks for creating this issue @atsushi-ishibashi the #2115 PR looks good. We'll investigate to make sure there are no unintended consequences since the code base didn't use the decoder from the beginning. We'll update this issue once the PR is approved.. Thanks for posting this PR @maniankara. The service client packages are modeled by the respective service teams. We'll forward this request upstream to the service team. In addition I suggest you also give feedback on the service's API reference documentation directly. Feedback on this docs will be sent directly to the service team.. Thanks for creating this PR @deedubs. This example is modeled by the service team. We'll forward this fix to the service team so they can update their example.. Thanks for reaching out to us @arjantop. The SubscribeToShard feature is currently only available in the AWS SDK for Java. Once Amazon Kinesis supports the other AWS SDKs we'll be able to add support for this feature for the AWS SDK for Go.  Sorry, I don't have a timeline on when Amazon Kinesis will make this feature available for the non-Java SDKs.  I will pass along the feedback that the release notes for this feature were not as clear as they could of been.. Thanks for reaching out about SubscribeToShared support. We have PR #2402 out which provides this feature. We have some final integration testing to complete prior to merging in this feature support. . Hi @xiaosongdhls thanks for reaching out to us. Are you referring to the colon in the Region: aws.String(REGION), line in your example? If so, the colon character is the notation the Go language uses to assign fields in a structure during struct initialization.\nIf this is not the case could you please clarify your questions.. Thanks for the update, let us know if you have additional questions, issues, about the SDK or suggestions or features.. Hi @zhejiananzhuren  thanks for reaching out to us. Could you post the full error message that you receive. e.g the result of the fmt.Println(\"send err    \",err) line.\nIn addition could you enable debug logging. and include the log information as well. You can do this by adding a LogLevel to your config with a value of aws.LogDebugWithHTTPBody.\ngo\nsession, err := session.NewSession(&aws.Config{\n        Region:      aws.String(Region),\n        Endpoint:    aws.String(Endpoint),\n        Credentials: credentials,\n        LogLevel:    aws.LogLevel(aws.LogDebugWithHTTPBody),\n    })\n. Hi @wking thanks for reaching out to us. JMESPath expressions are only used as an internal runtime component of the SDK's handling locally of API operation parameters, e.g Waiters and Paginators. JMESPath is a client only utility.. The AWS CLI will perform all JMESPath logic locally. The client might also be performing additional customization locally depending on service. JMESPath is only a client side concept the client and SDKs might use for added client side functionality.. Hi @rogchap  thanks for reporting this issue. From reading the PR and this issue, not HTML escaping the JSON block sounds like a correct action. . Thanks for the update, restarted the build. Thanks for fixing this typo @procrypt, The changes looks good.. HI @warmchang  thank you for letting us know about these spelling issues in the API docs. We cannot accept changes directly to the api.go files as these are code generated from model files provided by the service teams.\nWe can forward these upstream to the service teams so they can correct their documentation on their end.. I've forward these misspellings to the respective service teams. When their API docs are updated the SDKs will automatically reflect the change.  Thanks again for taking the time to find this typo and submit this PR.. @turtleDev are you using vgo or go 1.11 support for modules? In my local testing I've only been using Go 1.11 mods.. Hi @sectorsize512, thanks for reaching out to us. The amount of data read from the socket looks to be an internal implementation detail of io.Copy. Specifically, io.copyBuffer will create a 32KB buffer to read from the source with. The SDK's downloader uses io.Copy to copy the HTTP response body to the io.WriterAt your application provided when starting the download.\nIn Go 1.5 CopyBuffer was added to the io package. The SDK's downloader probably could use this function to optionally allow a user to specify the size of bytes to be copied from the HTTP response body to the target io.WriterAt. Doing this, the downloader probably could also have a minor optimization reducing overall allocs by reusing this buffer.. We'd be glad to review a change to the S3 manager download. Thanks for posting this PR, but #2178 covers this issue. We'll use that one for this change. Hi @vkrishs thanks for reporting this issue. Could you Update the version of the SDK you're using to the latest version v1.15.47. The SDK recently was updated to provide a more useful error message for these request serialization errors that includes the RequestId and HTTP status code from the service's response.\nIf you have any request Ids and region associated with these errors I think it will help us work with the service team to investigate the issue. . Thanks for reporting that you're also seeing this. It looks like in a few rare cases some services can send back HTML responses with a 400 status code. This issue was original resolved because the SDK will retry 50x status codes. Though you could still see the error if the service responds to all retry attempts with HTML and 50x, exhausting the SDKs retry attempts.\nWe can use #2315 to track this even though that issue was original created for Kinesis, i think the issue is similar to what you're experiencing.. Thanks for creating this PR @saravanan30erd regretfully this example is defined by the Lambda service, and is generated from a model. We can upstream this issue to the Lambda service team to correct the issue with the model.. also should update the go.mod file dependencies.. Looks like the build failure was an upstream golang.org/x/tool failure. Will restart tests.. @YakDriver thanks for letting us know about the build failure. We'll need to update the unit tests and code generation so that no test with x/tools dependency will be run. The SDK's core and client unit tests should all still be valid to run.. Thanks for the additional feedback. Our biggest concerns about enabling this feature by default is that users may unknowingly be opening them applications up to exploits by the SDK executing arbitrary executables from the shared config file. This is the primary reason we are concerned about enabling this feature in the default credential chain automatically within an application.\nWe understand the benefit of it should just work out of the box, enabling this feature automatically opens the application up to being exploited.\nThere are a couple use cases where we think the automatic behavior could be exploited:\n User's application has its ~/.aws/config or ~/.aws/credentials files compromised.\n User's application is instructed to use an alternative compromised shared config/credential file. e.g AWS_CONFIG_PATH and similar environment variables.\nThe following is the most obvious use case I immediately think of that combines these two vulnerabilities together. But, I have no doubt that there are others.\n\nAn given Go application using the SDK with automatic support for credential_process feature is spawned as a CGI \"script\". The web app allows external users to upload a file to be processed. That file is stored in a predictable pattern in the web app's host storage.\nA nefarious user uses the web app to upload a shared config file with a malicious command in the credential_process field. The nefarious user also constructs a request to the web service that includes a AWS_CONFIG_PATH header value. The value of this header points to uploaded shared config file with the malicious command. The CGI caller will execute the Go app as a CGI \"script\" with the AWS_CONFIG_PATH header as a environment variable. This will instruct the SDK within the app to load the uploaded shared config with the malicious command, and execute the command automatically with the permissions of the CGI script.\n\nBecause of these risks, we think that something in the user's application code needs to explicitly enable this feature, beyond the shared config file, is needed to prevent unexpected execution of arbitrary executables. We see the value and need being able to define a way to source credentials from an external process, but we do not think it is appropriate for an automatic default feature within the SDK.. Thanks for updating this PR @YakDriver. I'll work to review these changes, and provide feedback.\nAlso thanks for the extensive breakdown of AWS CLI vs AWS SDK for Go breakdown of behavior. I agree, the SDK's behavior performs the most consistent experience without the unexpected edge cases. I think the AWS SDK for Ruby behavior matches the SDK's.. Thanks for working with us to create this PR, @YakDriver. The updates look good, and we'll be able to merge this PR in nearly next week after re:Invent.\n@xibz and I are not at re:Invent this year, though there are a few AWS SDK team members at the developer tools booth this year.. :( looks like Go 1.7 and 1.8 no longer build  golint due to golang.org/x/tools update. We'll need to update SDK's CI tests to only perform unit tests of core and  clients.. Thanks for the update. The issue that you're running into seems to be directly related to using an io.Reader with the S3 Upload Manager.  The S3 Upload Manager will attempt to optimize for an io.Reader that is also an io.ReadSeeker. If the uploader is able to do this optimization very little additional memory should be created by the uploader. The uploader is not able to make this optimization for an io.Reader. Instead the io.Reader will create a new byte slice of PartSize (normally about 5MB unless changed) for each chunk uploaded to S3.\nMost likely the memory error that you're running into is due to the Uploader's default number of concurrent uploads (5) combined with the minimum part size (5MB), a 100MB fill will at a minimum consume at least 25MBs of memory just for buffering of the 5MB parts to be uploaded. \nOne potential way to limit the impact of this is to set the Uploader's Concurrency parameter to 1.  I'm not positive if this will still be within the 10MB window though.\ngo\n    // Perform an upload.\n    resp, err := uploader.Upload(upParams, func(u *s3manager.Uploader) {\n        u.Concurrency = 1\n    }). Another way you could solve this issue with such a restricted memory is to use the SDK to generate a presigned URL for the request, then use the stdlib http.Client to stream the object to S3.  Presigned URLs support streaming. Currently the AWS SDK for Go does not support streaming uploads with the PutObject S3 API operation.  The SDK's S3 Upload Manager does support streaming, but the uploader's memory usage should be improved.\nHere is an example of using a presigned URL for uploading an S3 Object. https://github.com/aws/aws-sdk-go/tree/master/example/service/s3/presignURL This pattern will consume the least amount of memory. \nWe have #2036 tracking refactoring the S3 Upload Manger's performance, but either reducing the concurrency, or using presigned URLs should work for your memory threshold.. Thanks for the feedback, sure we can leave the issue open. I'll make it as pending feedback. Let us know how the change goes.. @iselind  leave the ContentLength value unset in your PutObjectRequest. In addition, your http.Request needs to set the Content-Length header to the length of the content you'll be putting. S3 does not support chunked transfer-encoding PutObject requests.\nIn addition I suggest also using PresignRequest instead of Presign.  PresignRequest will return a http.Header value with all headers that were signed, and must be included with the presigned URL is used. Generally this value is empty, but depending on what parameters you use for PutObjectInput there may be headers that need to be populated, e.g. SSE.. @ncw I think for the SDK to support streaming we need to add additional functionality to the SDK's internals to allow an API parameter to be an io.Reader. Currently, a significant portion of the SDK's internal behavior expects a io.ReadSeeker. The ReadSeeker is used by both the protocol marshaler for serialization, and signer for computing the request signature.\nS3's PutObject and similar methods are special, and support the magic value UNSIGNED-PAYLOAD for the request body's digest. We should investigate how viable it is to update the SDK to allow S3 PutObject to take an io.Reader instead of an io.ReadSeeker.. Looks good just need to be rebased against master to pull in the fixed unit tests.. Hi @anasanzari the SDK's clients are all safe to use concurrently. You should be able to call ReceiveMessage from multiple goroutines without error.. @jritsema  thanks for letting us know about this issue. we'll add this to a test case and work to fix the issue from there.. Thanks for the heads up @dideler we'll forward this  upstream to the SageMaker team. \nIn addition, the Metadata.SigningName field within the api-2.json file your client is generated/defined from also provides the signing name (aka CredentialScope) a client should use if that name is not provided via the standalone enpoints.json file.. I've forward this issue to the SageMaker team. Closing this issue since the AWS SDK for Go is using the service model's Metadata.signingName field it will correctly make the API requests to the service.\nthe Elixir library should also be updated to use the Metadata.signingName field to avoid this issue in the future for other services.. Thanks for the feedback @emirb. We think maintaining CI tests for the core SDK and service clients to ensure users old older Go versions are able to upgrade their SDK for the latest service API support.\nAs new features and capabilities of both the SDKs and Go language are released we might need to reevaluate the CI testing matrix.. Hi @Hendra-Huang thanks for letting us know about this issue. I created #2284 which updates the SDK's usage of JMESPath to be the latest version.  I think this will resolve the issue that you are experiencing.. Hi @Hendra-Huang  thanks for creating this PR. Why was d6516e1 chosen instead of the most recent go-jmespath of c2b33e8?. Lets track this change in #2284, that PR updates the SDK to use the latest version of go-jmespath which includes the .gitignore.. Thanks for creating this issue @WIZARDISHUNGRY. I think this issue is similar to #1842. I think we should track this feature in #1842. I'll close this as issue as a duplicate.. Thanks for reporting this issue @xiaozhu36. Could you please add a test that validates that S3's endpoint host is valid with and without the port suffix.. Thanks for creating this PR @dvelitchkov. The UploadWithIterator and DownloadWithIterator methods where added to the S3 Uploader and Download manager after the s3manageriface interfaces were created.\nThe SDK generates iface packages for each service's client. These interfaces are documented as being unstable and will break in the future when the service team adds additional API operations.\nSince, the S3 Upload and Download managers are hand written, the s3manageriface package doesn't have this documentation. I think it would be better to add a new interfaces for the UploadWithIterator and DownloadWithIterator methods that users can compose into an interface of their own.\ne.g.:\n```go\ntype UploadWithIterator interface {\n    UploadWithIterator(aws.Context, s3manager.BatchUploadIterator, ...func(*s3manager.Uploader)) error\n}\ntype DownloadWithIterator interface {\n    DownloadWithIterator(aws.Context, s3manager.BatchDownloadIterator, ...func(*s3manager.Downloader)) error\n}\ntype Deleter interface {\n    Delete(aws.Context, s3manager.BatchDeleteIterator) error\n}\n``. Hi @thepudds, this PR updates the SDK'sgo-jmespath` dependency to the latest version. When creating the PR i'm unable to reproduce an error you experienced with golang/go#28396. Does this PR resolve the issue you were experiencing getting the SDK?. Thanks for the update @shaunc I'm having difficulties reproducing this issue. Could you try cleaning your Go cache, and modcache?  With a clean cache and getting the latest, or specific commit go-jmespath i'm unable to reproduce any error.\n```sh\ngo clean -cache\ngo clean -modcache\ngo get github.com/jmespath/go-jmespath@latest\n```\n. Great, glad that resolved the issue!. Hi @a4abhishek thanks for creating the fix for this documentation. The change is good and we'll be able to merge it. We're holding off merging non-critical updates during re:Invent. We'll be be able to merge in this change after re:Invent at the latest.. Thanks for the update I'll take a look at the PR and get back with feedback.\nAlso the SDK includes a Go 1.5 dockerfile in it's awstesting/sandbox folder. You can use the make command make sandbox-go15 to get a interactive docker container for the SDK and go 1.5. Thanks for the ping @azr I updated the discussion on nil case within shouldRetryCancel. I think all we need here is some extra docs to state how this case could happen.. Thanks for heads up. Is this during shutdown, or just when no credentials are available? The SDK should retry the ec2 metadata request until Max retries because even on an ec2 instance the SDK needs to try multiple times in case the metadata instance is not available at startup. The SDK doesn't have a reliable way to know if it is on an ec2 instance or not. So, the SDK has to always return the endpoint, unless disabled by application. Depending on the error. The SDK considers errors such as connection refused as retryable but the Go HTTP Client does not. #2416 corrected this action so refused connections are retried. I'm still investigating to make sure this is complete If not it would be better to error on the side of allowing retries instead of being restrictive. . Yeah :/ running any go command will update the go.mod go.sum files on the user's system. These will only add additional testing entries to the module files. Users using the SDK in their applications will not see this change. Only when they are making changes to the SDK it self will their mod file update.\nThough it does look like the go.sum is missing the following line:\ngithub.com/jmespath/go-jmespath v0.0.0-20160202185014-0b12d6b521d8 h1:12VvqtR6Aowv3l/EQUlocDHW2Cp4G9WJVH7uyH8QFJE=. Thanks for reporting this issue @bflad we're working on getting the docs updated. We'll update this issue when the docs are updated.. Hi @WIZARDISHUNGRY  thanks for reaching out to us. The expires time is only relevant for the S3 service. Other services have their own fixed expiration time. Generally this is 15 minutes, but it looks like IoT data service uses a 5 minute expiration time.. Thanks for the feedback @WIZARDISHUNGRY. We're glad to review a PR if you're looking to add this warning to the  presign. We'd probably want to log an warning message that the expiration might not be used by the service. The SDK doesn't have any metadata data available providing which services do or do not use the expiry value.. Thanks for the feedback @ouroboros8. The AWS SDK for Go added support for the shared configuration file about an year after the SDK released. The AWS_SDK_LOAD_CONFIG flag exists to opt into support, due to the concern automatic opt in would cause sudden unexpected behavior to user's applications if the SDK were to enable the feature automatically. The v2 SDK, currently in developer preview, has support for the shared config file by default.\nI think requiring the token provider to be explicitly specified by the user prevents numerous error or unexpected behavior conditions that could cause significant harm to a user's application. Returning an error upfront that required information is missing provides a better experience.\nThe bundled StdinTokenProvider is a way to get the MFA token, but it provides no protection for concurrency, or the user's application creating multiple sessions. These can best be solved by a user's domain knowledge to configure the MFA token retrieval based on their use case.. The error message \"should\" never remove content, but could remove content. With that said though, I've very hesitant to keep this PR's changes because it relying on parsing the error message string instead of type assertions. I'd like to find an alternative way to detect connection reset that relies on types, not strings.. Thanks for reporting this issue @roy-michael. It looks like in rare cases the Kinesis service will return a HTML response instead of the expected json response. We'll forward this to the Kinesis team, but I think there are some improvements to the SDK we can make.\nWe need to investigate if the SDK can safely rely on the Content-Type response header to determine if the response is usable or not, and and use that to provide a better error message. Also, if you update the version of the SDK that you are using to the latest version the SDK will retry these errors automatically. Release v1.15.30 fixes the SDK to retry these errors.. Thanks for the udpate @markphelps. Are the 500 errors SerializationErrors as well where HTML payloads are being received, or is the SDK receiving another 500 error?. Thanks, correct that error is because the server is sending HTML responses. Most likely coming from a front end, e.g. load balancer due to a server error.. Investigating a potential timing bug in the updated request send.  The Go 1.9 test timedout in what looks like the S3 package.. The 1.9 failure looks to be a transient failure, unrelated to this change.. Thanks for the feedback @bodhi. A 1024 default buffer seems reasonable. We can do more research and see if there is a more appropriate size.. Separate commit is fine, the PR will be squashed when its merged into master so no worries about the separate commits in the PR.. Thanks for putting this fix together. The change looks good.. HI @gazoakley thanks for reporting this issue. I've published #2340 to fix the SDK's handling of this unexpected response from the service. I'll also reach out to the SecurityHub service team to let them know about this unexpected payload.. Thanks for reporting the additional issue with SecurityHub. Could you open another issue for the other API issues?\nI've merged the PR fixing the unexpected response with the SDK.. Thanks for reporting this issue @jmassara. We've merged in a fix in, #2329. Thanks for creating this PR @jmassara. This change is fixed by #2329.  Lets use that PR to track this change.. Thanks for the update @rnzsgh, I'll go ahead and close this issue.\nThe only thing i can  think that could be an issue here is with vendoring of the SDK, a bad go build cache, or go module cache.\nI'm unable to reproduce this issue. I cleaned out my GOCACHe and GOPATH/pkg folders.\n rm -rf $(go env GOCACHE)/*\n rm -rf $(go env GOPATH}/pkg\nI created a new go module for testing:\nsh\nmkdir sdkTest\ncd sdkTest \ngo mod init \"jasdel/sdkvertest\" \ngo get github.com/aws/aws-sdk-go@v1.16.3\nI created a main.go with a reference to the SDK:\n```go\npackage main\nimport (\n    \"fmt\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\n)\nfunc main() {\n    sess := session.Must(session.NewSession())\n    fmt.Println(sess)\n}\n```\nI was able to go build the test project for v1.16.3 without issue.. Hi @roytanmoy  thanks for reaching out to us. The version of the SDK you're using is fairly old. Do you still experience this issue with the latest version of the SDK v1.16.4? \nv1.16.2 also updated the SDK's dependency on go-jmespath to be the latest version.\nIn addition are you using Go 1.11's modules, dep, or any other tool to manage dependencies?. HI @ehan1990 thanks for reaching out to us with this issue and example. I think the issue you're running into is related to #1990. \nI posted a comment with example on how you can work around the list vs set issue.. Sorry, missed you were using the stringset struct tag attribute in your example.  The struct tag isn't working in your use case because the struct member is being passed into the expression.Value. Struct tags are only accessible when in the context of a value of the struct, not its member values.\nTo work around this issue you could define an alias type for the CarModels member, and define the MarshalDynamoDBAttributeValue method. \n```go\ntype CarModelSet []string\nfunc (cs CarModelSet) MarshalDynamoDBAttributeValue(av dynamodb.AttributeValue) error {\n        av.SS = make([]*string, 0, len(cs))\n        for _, v := range cs {\n             av.SS = append(av.SS, &v)\n        }\n        return nil\n}\n```\nYou could either update your Garage struct so CarModels member uses this type alias: \ngo\ntype Garage struct {\n    ID string `json:\"ID\"`\n    CarModels CarModelSet `json:\"CarModels\"`\n}\nOr update your DynamoDB expression code to cast the CarModels member to the alias when used in an expression.\ngo\nupdate := expression.Set(\n    expression.Name(\"CarModels\"), expression.Value(CarModelSet(garage.CarModels)),\n). Thanks for reporting this issue @jamie-digital. The test case needs to be updated to work consistently. If you're looking to create a PR we're glad to review it, otherwise we'll schedule some time to fix this issue.. Thanks for the updates, I'll review these.  \nThe forceRefresh flag is used by the SDK to mark current credentials as no longer valid. This can happen when credentials have been revoked. When credentials are revoked, the service will send an specific error code. The SDK identifies the error code and marks the credentials as expired so that the next API request will refresh the credentials.. The SDK's built in request handler AfterRetryHandler will call Expire if the service's API response has an error that signifies the credentials are no longer valid.. Will your application be caching the credenitals.Value or credentials.Credentials value?  I suggest using the credentials.Credentials value as it will always be up-to-date. In addition the Credentials provides synchronous locks for usage concurrently.\nUsing the forceRefresh flag in ExpiresAt will help ensure any entity using the Credentials value to cache AWS Credentials will correctly know when to expect the credentials to expire, or if they are forced expired by another entity such as SDK's API requests.. Thanks for creating this PR. I've merged it into master, and it will be included in the next tagged release. I expect a tagged release to be created this week.. Thanks for submitting this issue. We'll reach out to the service team to help address this issue. I think the service should be setting the X-Amzn-Errortype header with the error code value as well, which isn't being done.. As a short term workaround, until the service corrects the error format returned, you could temporarily swap out the SDK's error unmarshaler for this service replacing it with custom logic to extract the error code. (I'll look at adding this as a customization to the SDK as well for the RAM client)\nDefine a custom request unmarshal error request handler function. This custom unmarshaler will be compatible with the service when the fix is made on their end.\n```go\nfunc custUnmarshalError(r *request.Request) {\n    defer r.HTTPResponse.Body.Close()\nvar jsonErr struct {\n    Code string `json:\"code\"`\n    Type string `json:\"__type\"`\n    Message string `json:\"message\"`\n}\n\nif err := json.NewDecoder(r.HTTPResponse.Body).Decode(&jsonErr); err != nil {\n    r.Error = awserr.NewRequestFailure(\n        awserr.New(\"SerializationError\", r.HTTPResponse.Status, nil),\n        r.HTTPResponse.StatusCode,\n        r.RequestID,\n    )\n    return\n}\n\ncode := r.HTTPRequest.Header.Get(\"X-Amzn-Errortype\")\nif len(code) == 0 {\n    code = jsonErr.Code\n}\nif len(code) == 0 {\n    code = jsonErr.Type\n}\n\nr.Error = awserr.NewRequestFailure(\n    awserr.New(code, jsonErr.Message, nil),\n    r.HTTPResponse.StatusCode,\n    r.RequestID,\n)\n\n}\n```\nWhen creating the client for RAM swap the restjson.UnmarshalErrorHandler with your custom handler.\ngo\nsvc := ram.New(sess)\nsvc.Handlers.UnmarshalError.Swap(\n    restjson.UnmarshalErrorHandler.Name,\n    request.NamedHandler{\n        Name: \"custom.unmarshalError\",\n        Fn: custUnmarshalError,\n    },\n). HI @dmolesUC3 thanks for reaching out to us.  The best way to prevent the SDK's default credential provider from attempting to use EC2 instance roles is to set the AWS_EC2_METADATA_DISABLED=true environment variable for the application your using.  This environment variable will prevent the SDK's ec2metadata client from making requests to the EC2 instance metadata endpoint.\nhttps://docs.aws.amazon.com/sdk-for-go/api/aws/ec2metadata/. Calling EC2Metadata.Available is expected to incur the same slowdown because its trying to make a HTTP request to the EC2 Instance metadata endpoint. The SDKs don't currently have a consistent, cross platform way of detecting if they are running on an EC2 instance or not. This is the reason the AWS_EC2_METADATA_DISABLED environment variable was created, directing the SDK's to ignore the EC2 instance metadata endpoint calls.. Thanks for taking the time to create this PR. The change is good. We'll also work with the service team on our end to help fix for this on their end.. Thanks for the feature request @jadn123. Adding the ability to compute the size of a DynamoDB AttributeValue makes sense. If you're looking to create a PR we'll review it and provide any feedback so it can be merged in. . Pending integration test validation of this feature with the kinesis service. . Thanks for the ping @timmersuk. I'll bring this up at our next planning and get the integration work prioritized so we can get this PR merged.. Hi @Zebradil thanks for reaching out to us. The SDK environment variables accept 1,t, T, TRUE, true, True as a true value and 0, f, F, FALSE, false, False as false. We can improve the SDK's documentation to clarify this behavior.. @rayjlinden you can specify a custom Go HTTP client instance with TLS cert verification turned off. Be cautious using this technique as disabling TLS cert verification opens your client up to connecting to malicious hosts.\n```\n    tr := &http.Transport{\n        TLSClientConfig: &tls.Config{InsecureSkipVerify: true},\n    }\n    client := &http.Client{Transport: tr}\nkc := kinesis.New(s, &aws.Config{\n     HTTPClient: client,\n})\n\n```. Thanks for reaching out to us @rayjlinden. Do you have a more complete example of the code that you're encountering this issue with. I was not able to reproduce the issue with the following code snip.\nAlso, if your application is providing a custom endpoint that is not HTTPS, you'll most likely also need to set the DisableSSL aws.Config flag to true as well.\n```go\npackage main\nimport (\n    \"fmt\"\n    \"os\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/kinesis\"\n\n)\nfunc main() {\n    sess, err := session.NewSession(&aws.Config{\n        Endpoint: aws.String(\"http://kinesislite:4568\"),\n    })\n    if err != nil {\n        fmt.Println(\"err\", err)\n        os.Exit(1)\n    }\nsvc := kinesis.New(sess)\nresp, err := svc.ListStreams(&kinesis.ListStreamsInput{})\n\nfmt.Println(resp, err)\n\n}\n```. @rayjlinden i'm still running into issues reproducing this error sorry. Can you ensure that you're using AWS SDK for Go v1.16.18, and Go 1.11?  What operating system is the code running on?\nUsing the code pasted, I'm able to get the expected RequestError, because kinesislite is not a valid host name for my system.\nRequestError: send request failed\ncaused by: Post http://kinesislite:4568/: dial tcp: lookup kinesislite: no such host \nHere is the HTTP request wire traces using the custom endpoint.\n```\n2019/01/14 14:58:22 DEBUG: Request kinesis/ListStreams Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nPOST / HTTP/1.1\nHost: kinesislite:4568\nUser-Agent: aws-sdk-go/1.16.18 (go1.11.3; darwin; amd64)\nContent-Length: 2\nAuthorization: \nContent-Type: application/x-amz-json-1.1\nX-Amz-Date: 20190114T225822Z\nX-Amz-Target: Kinesis_20131202.ListStreams\nAccept-Encoding: gzip\n```. Thanks for the update @rayjlinden Glad you found the cause of the issue. Let us know if you run into any addition issue, or have feedback for the SDK.. Thanks for the request @rayjlinden. There currently is not an Amazon KPL for Go library. A feature to decode the protobuffer would best be in a KPL for Go instead in the base SDK. I think this is related to #594.\n. Individual part upload will use exponential, back off retrying the part put unto MaxRetries, (3 by default).  If a part fails to upload, and uses all retries, the upload manager will fail the entire upload.. Pulling this change in to make the SDK's usage of aws.Context with the standardlibrarie's context.Context. Since the SDK doesn't need its own context type, using Go 1.9's type alias provides the correct definition.. The test is indeed unstable. Restarting the Travis CI job for Go 1.5 linux looks to succeed this time.. Fixed in #2417. While adding documentation for this feature, I came across a few areas that I think might be confusing for this feature. Specifically the difference between nil vs empty of a slice/map, along with the marshaler's behavior on when elements are set or not set on the user's side during marshal.\nTo help clarify this I think renaming NilAsEmpty to KeepEmpty to be more inline with the common, omitempty.  Will review if a nil vs empty of a list/map needs additional separation, but a keepempty I think covers the nil and empty usecases. Need to see if or how keepempty applies to non map/slice members.. Hi @tj thanks for reaching out to us. We don't have any specific plans for WASM with the SDK. Have you tried to use the SDK with WASM? If so what issues have you run into?. Thanks for reaching out to us @rafaelkperes. Specifying a Endpoint config value forces the SDK to use that endpoint for API requests. The Region value is used during request signing, and its value must be valid for the Endpoint provided.\nIf no Endpoint is provided during config the SDK will derive the endpoint based on the the service's name, and the Region provided.\nIn your use case, is the bucket name being provided upstream in addition to the endpoint, and region? Does your use case require the upstream user to provide the endpoint in addition to the bucket?  The SDK is able to compute the URL for a S3 bucket with just the region and bucket name.. Thanks for the update. I agree the error message is confusing. To make sure I understand this correctly.  The bucket is in the us-west-2 region, correct?\nI think you're seeing this error message because the request is actually being sent to the us-east-1 region, but the request was signed with the us-west-2 region as apart of the signature. The error is valid because the bucket doesn't actually exist in the us-east-1 region, but it looks like S3 API is using the wrong value in the error message.  I can forward this onto the S3 service team to let them know about the error, that the error message should refer to the region the API request was made to, not the region the request as signed for. Though in general the two values should match up.\n. HI @rafaelkperes  after more digging this issue seems to be localized in the SDK. the SDK handles the 301 error message from the service and creates the error message using the region provided in the request.\nI created, #2451 to add additional information to the error message, such as endpoint the request was made to along with the actual region the bucket is in if that information was provided by the S3 error response.. Thanks for taking the time to create this PR @Quasilyte  I've pulled in the change locally, and created PR #2476 to fix this test.. Thanks for taking the time to create this PR @buroa. The endpoint's default.go file is code generated from the contents in the ./models/endpoints/endpoints.json file. The contents of this file are provided by the AWS service teams.\nI can forward this request upstream to the Amazon SQS team to see their plans on updating this definition. I'm not positive if the SDKs can be updated though as the regional endpoints may have different behavior from the \"global\" endpoint. . I've let the Amazon SQS team know about this issue. SQS and STS services have similar issue where regional endpoints were added or changed.\nWould you mind opening a Github Issue about the endpoints not being modeled as expected? This would be a good place to track this.. Thanks for making this update @limitusus the change looks good.. Thanks for the feedback @radeksimko. We have reservations that removing the Gopkg.* files might negatively impact users depending on the Go dep tool's functionality. \nWe would like to remove the vendor folder and Gopkg.* files, but the SDK doesn't yet have a story for deprecating old Go versions. Currently. the SDK supports all the way back to Go 1.5 and later. We'd like to have a comprehensive story on how the SDK could deprecate Go versions, but this has not be done yet. The supported Go versions will prevent the SDK from removing the vendor folder and Gopkg.* files for now.\nWith all of that said, are you seeing issues using the SDK and its multiple dependency metadata with your use case?\n. Thanks for reaching out to us @bflad. ECR service team changed their endpoints from ecr to api.ecr in v1.16.25. The SDKs were updated to reflect this change. While the ecr endpoint is still valid, the SDKs no longer will use it, and use the api.ecr endpoint instead. This is also why the SDK deprecated its provided service endpoint id generated as Go constants, because it was discovered the values are not immutable.\nWe're investigating how we can improve searching for these endpoint metadata with stable keys. \n. Thanks for reaching out to us @cbarraford. From the error message, it looks like the SDK is reading some file, but fails to continue because the file doesn't parse as INI. Is there a file at /root/.aws/credentials ?. Thanks for reaching out to us @takeyourhatoff. It looks like the SDK doesn't have the pagination model for the DescribeDBClusterSnapshots RDS API operation. This looks to be the only missing piece. . Thanks for the update and feedback. The double read is triggered by the SDK's need to compute a hash of the payload to upload, so that it can be included in the request's signature, along with determining the length of the payload to send.\nWe're looking at optimizations for the request signature's payload hash for S3, that we might be able to implement in the future removing the need to compute the hash for a payload over TLS, but this work is still outstanding and I don't have a timeline on it sorry.\nWith that said I think the example can workaround this double read of the payload by changing when the progress tracker is injected. Instead of wrapping the reader to be read from, we can inject a request handler into the Uploader that will keep track of the payload chunks as they individually are sent to S3.\nI put together PR #2456  that fixes this.. Thanks for taking the time to create this PR @sanketplus. The AWS SDK for Go doesn't distinguish between the shared config file (~/aws/config) and shared credentials file (~/aws/credentials). To the SDK they are the same file.\nWith that said the SDK will not read the additional information such as region and assume role fields out of the file unless the Session's Shared Config mode is enabled.\nIf Session Shared Config support mode is not enabled, the SDK might be able to drop all entries in the cfgFiles slice, (from your PR change). We need to evaluate this to not be a breaking change, and add tests to ensure the behavior is maintained in the future.. Thanks for the update @sanketplus. I'm suggesting, that if the Session's Shared Config support is not enabled it maybe be safe for the SDK to remove all filenames from that slice. The name of the credentials file can be customized and specified via other means. In addition, from the SDK's perspective there is no difference between the config and credentials file. This is mostly true for the AWS CLI as well. The SDKs suggest credentials only exist within the credentials file, but the SDK and CLI all support the same behavior from the config file as well. The credentials file is just read first.  This is why I suggest if Session Shared Config support is not enabled the full list of files to load might be safe to clear.. Thanks for creating this PR @mikecook  would you mind updating the PR to exclude the /model/**/*.json files. These are provided upstream by the service teams, and will be overritten on the next update. \nI'll see if i can upstream cleanup to our tooling to remove trailing whitespace from the model files.. Thanks for creating this issue, @ks2211. Are you seeing the RequestCancelled as the error returned by the caller of the errgroup.WaitContext?\nI think the reason you're seeing RequestCancelled error within the API operation response is because the errgroup Will cancel the group's context if any group function returns an error. I expect the RequestCancelled to occur based on a race condition chance in the other group's functions.\nFrom my experiments, I'm seeing a high chance that one of the two requests will be canceled when the other group member completes first failing with the InvalidParameterValue error.\n```go\npackage main\nimport (\n    \"context\"\n    \"fmt\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/ec2\"\n\"github.com/aws/aws-sdk-go/service/ec2/ec2iface\"\n\"golang.org/x/sync/errgroup\"\n\n)\nfunc main() {\n    sess, err := session.NewSession(&aws.Config{\n        LogLevel: aws.LogLevel(aws.LogDebug),\n    })\n    if err != nil {\n        panic(err)\n    }\nfmt.Println(\"Error?\", doCall(ec2.New(sess)))\n\n}\nfunc doCall(ec2Svc ec2iface.EC2API) error {\n    g, gctx := errgroup.WithContext(context.Background())\n    for i := 0; i < 2; i++ {\n    id := i\n    g.Go(func() error {\n        _, err := ec2Svc.CreateSubnetWithContext(gctx, &ec2.CreateSubnetInput{\n            CidrBlock: aws.String(\"hi\"),\n            VpcId:     aws.String(\"123\"),\n        })\n        if err != nil {\n            fmt.Println(\"group error: \", id, err)\n            return err\n        }\n        return nil\n    })\n}\n\nif err := g.Wait(); err != nil {\n    return err\n}\nreturn nil\n\n}\n```\n```sh\n$> AWS_REGION=us-west-2 AWS_PROFILE=default go run main.go\n2019/03/06 16:32:27 DEBUG: Request ec2/CreateSubnet Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nPOST / HTTP/1.1\nHost: ec2.us-west-2.amazonaws.com\nUser-Agent: aws-sdk-go/1.17.11 (go1.12; darwin; amd64)\nContent-Length: 61\nAuthorization: \nContent-Type: application/x-www-form-urlencoded; charset=utf-8\nX-Amz-Date: 20190307T003227Z\nAccept-Encoding: gzip\n\n2019/03/06 16:32:27 DEBUG: Request ec2/CreateSubnet Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nPOST / HTTP/1.1\nHost: ec2.us-west-2.amazonaws.com\nUser-Agent: aws-sdk-go/1.17.11 (go1.12; darwin; amd64)\nContent-Length: 61\nAuthorization: \nContent-Type: application/x-www-form-urlencoded; charset=utf-8\nX-Amz-Date: 20190307T003227Z\nAccept-Encoding: gzip\n\n2019/03/06 16:32:27 DEBUG: Response ec2/CreateSubnet Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 400 Bad Request\nConnection: close\nTransfer-Encoding: chunked\nDate: Thu, 07 Mar 2019 00:32:27 GMT\nServer: AmazonEC2\n\ngroup error: 1 InvalidParameterValue: Value (hi) for parameter cidrBlock is invalid. This is not a valid CIDR block.\n    status code: 400, request id: db902dab-fce1-4958-83df-f8c8fac0d062\ngroup error: 0 RequestCanceled: request context canceled\ncaused by: context canceled\nError? InvalidParameterValue: Value (hi) for parameter cidrBlock is invalid. This is not a valid CIDR block.\n    status code: 400, request id: db902dab-fce1-4958-83df-f8c8fac0d062\n``. Thanks for the update @ks2211. I think the best alternative is to either not doing the requests in parallel, or performing cleanup after the fact ifg.Wait()` returns an error. \nThe RequestCancelled is triggered because the passed in Context is canceled, which looks to be happening after the service has already received the operation call's HTTP Request. 1b's request is terminated before the SDK has had an opportunity to unmarshal the response. This is why the nil response is being returned for that call.\nFor your usecase do you want 1b to succeed regardless the success of 1a? Or is the expected behavior 1b should fail, and not create an ec2 resource? If the former, not using errgroup probably is the best approach and manage the loops your self. But if the latter, cleanup after error occurs is the best option.. Thanks for taking the time to create this PR @TheGUNNER13. I don't think this PR fixes the linked issue. Though we can still take this change as it does improve the readability of this code.\nThe Send method will be invoked prior to the return's parameter values being captured and returned.  https://play.golang.org/p/PhAaJyi-8ff and https://golang.org/ref/spec#Order_of_evaluation. Thanks for the update @TheGUNNER13 I think this change working was by coincidence to something else that changed on the EC2 Instance. Potentially the a different instance was used for testing?\nI'll get this change merged in, and we can continue discussion on #1861. Thanks for reaching out to us @klische. The SDK will automatically unmarshal the original JSON response from the DynamoDB service into the dynamodb.AttributeValue type. The original JSON string is no longer available at that point.\nThe best way to get the JSON form of the AttributeValue is to use the json.Marshaler to marshal the AttributeValue as a JSON object.  The SDK's String and GoString methods are helper methods that render the value as a human readable string, but this string is not JSON.\n```go\nresult, err := dynamoDb.GetItem(&dynamodb.GetItemInput{ / ... / })\n// handle request error\nb, err = json.Marshal(result.Item)\n// handle json err\nfmt.Println(string(b))\n```. @klische could you clarify the behavior you're looking for? Are you having difficulties storing a raw JSON object within a member of the Item?  Or are you looking to get the full Item as a JSON string back through a method?\nBased on your example it sounds like the jsonString should be available from result.Item[\"Item_Name\"].S. Does this return what you're looking for?. I'm thinking this logic might be bettered suited to a BeforeRetry handler.  We already have AfterRetry and Retry handlers so I'm wondering if in this case it would make sense to create a BeforeRetry. What do you think?\n. This logic could be split into their own function similar to the internal/protocol/json/jsonutil's build.go and unmarshal.go pattern of xxxAny, xxxStruct, xxxList, xxxMap, xxxScalar, but instead of tags using reflect.  With this pattern The json Marshal/Unmarshal wouldn't be needed, and would facilitate splitting out the logic from each of these case statements into their own functions.\n. If this and Marshal used rv.Elem() instead of just Value the code shouldn't need to do IsNil() checks simplifying it a little.\n. This should take an optional client *sts.STS parameter instead of Region so that users can provide their own customly configured sts client.  client is nill then we can default to client = sts.New(nil)\nThis would mean Region field doesn't need to be exposed, and can be replaced with a client *sts.STS field.\n. With credentials.Expiry this can be removed, and credentials.Expiry added as anonymous struct field. Same for IsExpired below.\nThe code setting s.ExpiryWindow in Retrieve() can be updated to be just\ngo\ns.SetExpiration(*roleOutput.Credentials.Expiration, s.ExpiryWindow)\n. There are a few different ways STS can provide roles, AssumeRole and GetSessionToken.  Both are very similar with the exception of the RoleARN. It would be great if the single Provider could be built for both types cleanly. but if not it may make sense for this PR code to be just a AssumeRoleProvider instead of generic STS provider.\n. > // Expire 10 seconds before the actual expiry to prevent exceptions.\nwould you mind correcting this comment.\n. While technically you can create a private interface for the assumeRoler it would be better if it was public. I suggest a public AssumeRoler interface in this package so that it will be documented and easily discoverable to people using the STS role provider\n. We can probably remove this if check since it's not apart of the v2 signing.\nWhile true we only plan to use the signer internally for SimpleDB support. If in the future someone wrote their code to connect to an s3 compatible service with the v2 signer they would only be able to make POST requests.\n. Would you mind changing these to use \"AKID\", \"SECRET\", \"SESSION\" for mock key ids and secrets. Its easier when reading the code to make sure no valid credentials are leaked.\n. This looks similar to the S3's error handling. Was it the same?  We don't have to do it here, but it might be a good idea in the future to refactor this custom error handling to use the same code.\n. Can assertEquivalentJSON be replaced with awstesting.AssertJSON? , they look similar.\n. Would you mind adding a package doc header here?\n. It would be great to update this section to use the SDK's waiters. DynamoDB.WaitUntilTableExists will provide this functionality and will return when the table is created. Which is great if it were to take more than 5 seconds in this case.\n. could you expand this comment stating why 416 might be received?\n. This should use a runtime type assertion or can panic.\ngo\nif e, ok := d.err.(awserr.RequestFailure); ok && e.StatusCode() == 416\nAnd replace the 416 with http.StatusRequestedRangeNotSatisfiable to easier to read\n. Might make sense to move this logic into the download function. Being its own method accessing data like this could introduce data races if this method is called somewhere else unexpectedly.\n. Early exit would be preferable here to wrapping the block\n. can this be just an else?\n. the extra = here isn't really needed since fin will be set back to the same value.\n. probably should be http.Header\n. http.Header also instead of map\n. I'd suggest publishing allow, Allow(...\n. might be better as something like allowWhitelist\n. Might want these to satisfy the allower interface.\n. might want to reconsider the name, allower isn't very descriptive\n. might be more descriptive as something like allowedQueryHoisting\n. http.Header\n. I'd suggest getting rid of the header delete and query delete and just do:\ngo\nv4.Query[k] = urlValues[k]\nSince a url.Values is returned\n. These (allowed) should be allowers\n. For notHoist we can skip buildQuery all together. can be done outside in build.\n. I think we can drop this v4.headers\n. suggest Headers be SignedHeaders for both values.\n. v4.headers should be http.Header, and maybe named v4.signedHeaderVals\n. I'd add tests for the x-amz-* in query string\nand verify x-amz-meta-* are good in query string \n. is this still needed since there is no longer any mock data?\n. This can be just the follow. Since no other values are being sent don't need to create a Response.\ngo\nw.WriteHeader(http.StatusOK)\n. I think this list of services can be simplified. Also I suggest the list of services be in their own file with nothing else so the file which searches through the services is easier to read.\nUnless this list is generated it will be easy to forget to add new services to this list when they are released. Even more so if SDK were to ever support versioning of services.\nIf a []reflect.Value of each service client's New func pointer value was created. Both creation of service client, and inspection of service client's operations could easily be done in the same method by iterating over the items in the list instead of hard coding each service client type buildAnArrayOfClients()\ne.g:\ngo\nvar clients = []reflect.Value{\n    reflect.ValueOf(acm.New),\n    reflect.ValueOf(apigateway.New),\n}\n//...\nfor i, c := range clients {\n    v := c.Call([]reflect.Value{reflect.ValueOf(mock.Session)}\n    findAndGetMethod(v[0].Interface{})\n}\n. prefer early exist for conditionals instead of large wrapping blocks. \ngo\nif method.Type.NumIn() != 2 {\n   continue\n}\n. I think a newBenchmarkLogger could returns a *benchmarkLogger or outputer instead of using an init() func would be a little easier to follow.\nAlso, if there are multiple types of benchmark loggers I suggests split them into different types that share a common interface. It looks like they kind of already do with outputer and logger  Could an outputter be a logger?\n. I'm guessing you're running this on osx. Must because the flags supported bwteed the platforms are different, or I need to try different quoting scheme. \nGood catch, only LAN this on Linux systems so far\n. I'd add a comment here about why the body is being closed. Probably similar to what you have in aws/corehandlers/handlers.go\n. Should clarify this is the upper limit on retry delay not number of retry attempts.\n. :+1:  like the Pow 2 short hand\n. Need to do runtime type assertion here and only call CloseCopy if the Body is a offsetReader\n. What do you think of CloseCopy being renamed to CloseAndCopy?    CloseCopy could imply that calling this method will close a copy, not return a copy and close the current.\n. What do you think about removing this lock calls in CloseCopy and just call Close. Because the close method already has the lock and logic to close the offset reader. In addition I don't think the copy needs to be included in the Lock scope.\n. This is an optimization to only look if the type implements the Marshaler interface if there are methods on the type. The check for num methods allows us to skip the type assertion if we know it will never match.\n. I'll try that, and see if I can use the Sprintf, the benefit of the strconv.Format methods are they don't need to do the extra processing Sprintf performs. Minor optimization in this case since we know the types. \n. Will look at that. Used two iterations because an exact match should have priority over an folded match. In this case the fields [abc,Abc,ABC] and searching for Abc should return the second element. Since the names are derived from tags and falling back to structure feild name. it is possible to creat a list of marshaled feild names like these.\n. That is what i was thinking it seemed redundant, but didn't fully understand why the encoding/json code used this additional check.\nAfter a bit of research I found the condition which it looks like this is catches. http://play.golang.org/p/2NThBfWeTU . Specifically if a type is defined as an alias for the pointer value of another type this code prevents the dereferencing too far. Interestingly enough I don't think this is replicated in the decoding side of encoding/json's indirect func. I'll add more tests to capture this case.\n. cool i think that will work just as well.\n. What do you think about changing this to not rely on the indexes?\ngo\ncopy(b, []byte(`<B><A>`)\n. Similar here:\ngo\nconst xmlTerm = `</A></B>`\ncopy(b[delta-len(xmlTerm):], []byte(xmlTerm))\n. Same idea here as above metioned for xml\n. line 24 and 23 should be swapped. Need to drain the body before its closed.\n. Line 25 and 26 should be reversed also, same as above.\n. Ah cool, sounds good\n. yup, thanks will fix that.\n. Probably can drop the fmt.Sprintf since not needed. Here and the error case below.\n. I'm thinking the GetIdentityDocument might be better in the ec2metadata package. Since the contents of the identity doc are not really specific to the EC2RoleProvider. And it might be useful generically to those using the EC2Metadata client.\nProbably can also return the value of EC2InstanceIdentityDocument instead of its pointer. Is the pointer value needed?\n. What do you think about calling GetMetadata instead of generating the request directly? Maybe similar to Region?\nIts counter to the generated service clients, but in this case it simplifies the code and is possible since these requests to metadata similar.\n. Ah thanks for pointing that out. I missed the fact that dynamic is not under the meta-data path.  This implementation here is good. Thanks!\n. Name's value should be GetDynamicData in this case. Its not directly used in the HTTP request, but is useful for logging.\n. Thanks will update that.\n. Interesting didn't know you could do that with XML tags, ServiceUnavailableException isn't an attribute, but a different xml tag than ErrorResponse.\nI'm  not sure if xml can do that. I didn't see any mention of it in the godoc, and this play example hits an error, http://play.golang.org/p/NCECBedwc5\n. Removing the name tag all together might work if the only tags were ErrorResponse or ServiceUnavailableException i think. If there were any other tag before the two error tags that we are expecting I think the xml will not unmarshal the error we're actually looking for.  The additional unmarshal should be an extremely rare event and only occurs if ErrorResponse failed to parse the error body.\n. Can leave the third parameter of awserr.New nil. Since its an optional error param that available to set if there was an underlying error that awserr.Error is wrapping.\nIn addition EC2RoleError would be more descriptive as EC2MetadataError.\n. Generally would prefer lower case error messages. \nGetting IAM Info was unsuccessful...\ncould be\nfailed to get EC2 IAM Info...\n. Just realized IAMInfo and GetInstanceIdentityDocument use EC2RoleRequestError as their error code.  We probably should standardize error code for metadata requests as , EC2MetadataRequestError\n. shouldn't ProvisionedThroughputExceededException be included in the throttleCodes\n. Could you expand the doc for CookieSigner similar to URLSigner\n. This example comment would be great inside of an sign_cookie_example_test.go file so it can be validated by the compiler, and could even add test output.\n. This probably should follow the URLSigner's pattern and be SignWithPolicy.\nIn addition what do you think about adding a Sign method similar to URLSigner's that takes a resource string and a expires time.Time instead of a Policy? The Sign method could then use the NewCannedPolicy func to created a policy from a resource and expires values.\n. What do you think about using a []*http.Cookie instead of explicit values? All cookies returned are required to be added, so a list that can be iterated over might be easier to use.\n. If instead of o *CookieOptions set to nil if not provided a functional option pattern could be used like opts ...func(*CookieOptions).  The user would not need to provide the optional opts argument at all instead of setting nil.\n``` go\nc.SignWithPolicy(myPolicy)\n// or with opts\nc.SignWithPolicy(myPolicy, func(o *sign.CookieOptions) {\n    o.Path = \"/\"\n})\n``\n. I'd suggest breaking these tests out into multiple Test functions. This will make tracking down future bugs easier and tests would be more focused.\n. Instead of using randReader directly in tests, this should be replaced with the test rand reader so predictable signatures can be tested similar toTestSignURL`\ngo\n    origRandReader := randReader\n    randReader = newRandomReader(rand.New(rand.NewSource(1)))\n    defer func() {\n        randReader = origRandReader\n    }()\n. What do you think of doing early exist instead?\n``` go\nif r.Body == nil || r.HTTPRequest.Header.Get(\"X-Amz-Sha256-Tree-Hash\") != \"\" {\n    return\n}\nh := ComputeHashes(r.Body)\nhstr := hex.EncodeToString(h.TreeHash)\nr.HTTPRequest.Header.Set(\"X-Amz-Sha256-Tree-Hash\", hstr)\nhLstr := hex.EncodeToString(h.LinearHash)\nr.HTTPRequest.Header.Set(\"X-Amz-Content-Sha256\", hLstr)\n``\n. Should document thatCancelwas added in 1.5\n. Suggest passing body in instead of copingr.Bodyvalue. also prevents need setting it outside of copy function.\n. would be helpful to have an additional test that closes theHTTPRequest.Cancelchannel in a goroutine after Send is called.\n. Would be nice ifInputwas a little more descriptive. What do you think of something like aType` field. This would be more extensible.\n``` go\ntype TestSuiteType int\nconst (\n    TestSuiteTypeInput = iota\n    TestSuiteTypeOutput\n    // Could add other categories later. \n)\ntype TestSuite struct {\n    //...\n    Type TestSuiteType\n}\n``\n. I don't think these methods are necessary. Also I don't think they capture if theTransport.CancelRequestwas the reason the request being canceled. They provide a false sense of information the SDK may not have.\n. Instead of making this check. It might be better to just document the expectation, inaws/request:Request.Send`.  There are many stages in the chain of making a request where a request could of been canceled that this logic would not be able to capture.\nI think a more complete fix would be documentation the expectation, and updating the tests to reflect this. \n. Why change the current 30 to minTime?\n. This clear will actually also remove the retry handler also, so this test case could never fail. I think we only want to clear theValidate,Unmarshal,UnmarshalMeta, andUnmarshalErrorhandlers.\n. Thanks for the the update. Doing 100% jitter spread makes sense.  \n. would prefer for this to useunicode.MaxASCIIinstead of127` for clarity.\ngo\nif c > unicode.MaxASCII {\n. Suggest using crypto/rand#Reader instead of math/rand, this should be able to replace this whole generateRandBytes functions.\n. why bytes.Reader instead of a more generic io.ReaderX ?\n. should document where the padding is defined for AES. Will make understanding the logic easier later.\n. should meta just be an annon struct in Envelope instead of its own type? since its not exported\n. I think having a PutObjectInput with a  S3PutObjectInput might be a little confusing.\nWhat do you think about splitting the parameters out into the method instead of wrapped in a struct? Especially since the signatures are not duck-type compatible with the s3.PutObject API operation`\ngo\nfunc (client *Client) PutObjectRequest(strat SaveStrategy, materialDesc string, in *s3.PutObjectInput) ...\n. Probably want to make CredValues non-exported so it doesn't cause confusions with Credentials.\n. May want to consider using functional options instead of pointers here for optional parameters. This allows us to grow the options later own, and have the power to merge optional parameters without dealing with pointers. This would remove the need for the nil check below in Save\n. Instead of splitting options into a Config struct could consider merging Config into Client and have the functional option operate on Client after it is constructed.\nIn addition if MasterCipher is required, it should be a input parameter on NewClient not a config option.\n. Probably can go with just New instead of NewClient, but debatable because other types within this package.\n. We probably want to log the fact that a MessageId was not present in the response. In addition the RequestID from request.Request should be included in the message so that the user is aware of the issue.\n. can remove this commented line?\n. I think we can compress the get metadata into the crypto client's GetObject class. Since the request's response will include the headers as well, can just read them prior to deprecating the content.\n. If the reading metadata is merged into crypto client's GetObject should be able to remove usage of the regular s3 client. Though looks like the s3 client is used for other parts of the tests like ListObjects. Would be interesting to see if it makes sense to make s3 crypto client be composed of a s3 client\n. should this be \"(.+?)\" and \"(.+?)\" ? or am i reading this wrong?\n. should this be \"aes_ecb\" and \"aes_cbc\" ? or am i reading this wrong?\n. same comment as above\n. Lets go with DisableHeaderHoisting instead of NotHoist its a bit more descriptive. there isn't much we can do about request.Request.NotHoist but can clarify it here in the signer.\n. The signWithBody and presignWithBody seem to reuse a significant amount of their code. It would be great to have a union between the two so they will be easier to maintain.\n. Should include that this is a request.Handler to sign the requests made by the SDK's service clients in the comment for this function.\n. The Signer methods can be value instead of pointer since no value of the Signer's fields are modified.\n. What do you think about putting the build method on the signingCtx type instead of Signer. It doesn't look like any Signer value is being used here other than NotHoist. This could be passed in as a configuration param instead.\nAlternatively we could change build to sign and keep the method on Signer.\n. I'm not sure if this can exit early here. It is possible for user's custom request handlers to depend on the request.Request.HTTPRequest value expecting it to be set. Returning early without the other fields could cause breaking changes because fields like HTTPRequest are no longer set. Its still an error condition, but exiting early may cause applications to panic instead of returning a API operation error.\nI don't see any problem with getting ride of the url.Parse and letting NewRequest validate the URL. Both will report the error in Go 1.6. Neither 1.4 or 1.5 would return an error for invalid URLs such as http://example.com:80. For 1.4 and 1.5 the request would fail at transmission time instead of being caught early. #689.\n. The check for nil of Param and Data were to insure that a interface with a nil type value wasn't provided. Since r.Param != nil may fail with a false positive in the situation where a nil type value was set to the interface [1] [2].\nThough in practice this could only happen if a user where to set the value of the r.Param or r.Data them selves since all operation's should always provided a valid value for two fields.\nAre you seeing these two calls impacting the performance of the SDK? In the pprof's we've been running this hasn't registered.\nThe portion of the overhead which is impacted by reflection will be focused more on the (un)marshaling of the API operation's request/response data. As visible in #722.\n. The change to be type specific is a fine change instead of reflection, but an a.Expected can be many more types than string and int. It can actually be any value type. This is the reason awsutil.DeepEqual is used. The service's waiter will define a field that indicates a specific state of the waiter has occurred. The type of this field is pretty open ended. Such as, the a.Expected could contain complex value such as structs, maps or lists. \nI don't think off hand there are any that actually use complex types, so a primitive type matching might be used without reflection. Though at any time a service's waiter could use a complex type, so the SDK would need to use the DeepEquals as a fallback.\nIn either case any functionality like this should be in its own independent function so its not duplicated through the code, and can be tested.\n. These copy's are actually needed. HandlerList is a struct, but it contains a slice of Handlers.  This slice needs to be independently mutable at each stage of a request's pipeline without impacting any upstream functionality. \nSpecifically this change will cause any modifications made to a request handler's list to be propagated upstream to the service's request handler list. This means that If a customization needs to modify a request handler to perform some functionality for a specific API operation, the mutation of the handler list will be propagated upstream to the service, and now all API operation made will now also include the request handler customization. A common pattern is to share session.Session between multiple service clients, and this would polute all other service client instances with the customized request handlers.\nThis also introduces race conditions in SDK. Which is the panic shown in the failed Travis test. Since the backing arrays of handlers would be shared between multiple instances of service clients, and API operations being made concurrently, any mutation to the handler list would introduce race conditions.\nThis Go blog post provides a good rundown on how Arrays vs Slices could break if down stream changes make unintended mutations to a upstream owned slice.\n. Correct, the only time Copy will be called is when a service client is created from a session.Session, and then each time an API operation is called. Outside of that it should be very rare of a HandlerList to be copied.\nAn alternative approach might be to implement lazy copy of the handlers, but this would require mutexes to wrap the HandlerList functions, since concurrent mutations/operation need to be supported. As a guess adding the critical section check will have more overhead over the life time of the request than the copy of slices of function pointers.\n. Users are always free to add or remove request handlers from the various HandlerLists based on  their use cases. This can be performed both on the service client instance, and the individual request such as with, dynamodb.CreateTableRequest. All of the xxxRequest methods return a request.Request that can be modified based on the user's preferences. This is commonly done for debugging, or metrics reporting.\nIn addition there are customizations the SDK implements for some of the service client's and their API operations, and these are applied to the individual operation requests as they are called.\nThe HandlerList could be created once the first time each API operation is called by the service client, but mutex's would need to be added around the HandlerLists. This is due to API operations are allowed to be called concurrently. With that said, the handler lists still must be a unique copy per API operation request call instance, because users could choose to augment an operation's request Handlers uniformly. An upfront, or first time use copy would prevent this functionality.\n. Just to note 3000ns is only 3us or 0.003ms not 3ms. Since requests can take multiple ms to cross the wire 0.003ms once per request is not significant.\nHaving user modifiable handlers is important, and some users depend on this functionality today. Removing this would be a breaking change to the SDK.\n. Both cases are an error, but if the SDK were to start panicking it would break user's applications where before only an error would of been returned.\nThe SDK's min supported version is 1.5, and plans to continue to support this version in the future. Go 1.4 is not officially supported by the SDK, but many users still use this version of Go, and we'd like to continue unofficial support for this version as long as possible. The SDK supporting these additional Go versions does not impact the overall performance of the SDK. This does introduce some limitations on what new stdlib features the SDK can use, but this has not introduced any issues as of yet.\n. AutoScaling's waiters are a good example of bool expected types. e.g. WaitUntilGroupInService. In generally only bool, string, and int values are used as Expected.\nIf the fields were split out they would be ExpectedString, ExpectedInt, ExpectedBool. This would also grow if the Expected for each additional type that were ever added such as float, slice, and maps. With that said, this would be a breaking change to the SDK because the waiter.WaitAcceptor type is an exported type. User's most likely would have no reason to use the type or its fields directly, but changing the fields would break them if they did.\nThere is nothing wrong with updating this section to use non-reflection for for the known cases int, string, bool, and falling back to the awsutil.DeepEquals for all other cases. In addition to the change should not be hardcodeed and duplicated inside of the waiter code, but implemented in a standalone utility function that is testable. In addition this should be discussed in its own Issue, with its own PR.\n. This could be just a const since the value won't change.\ngo\n// EmptyStringSHA256 is a SHA256 of an empty string\nconst EmptyStringSHA256 = `e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855`\n. thanks, updated to only set if there was no signing error.\n. This reuses the source bytes array. Needs to be a copy not set.\nhttps://play.golang.org/p/6qRA_W3Pu0\n. yeah in this case because if v.Type() == byteSliceType { only checks for a type being []byte the following switch statement for at line 203 will capture types that are aliases of []byte.\nif v.Type() == byteSliceType { is more of an early exit, not absolutely required until before the switch statement.\n. At this point the code doesn't know the destination reflect.Value is a []byte or not yet. The only thing that is known is the source value is a dynamodb.AttributeValue.B (aka []byte). We the code below needs to figure out how to copy the source byte slice into the destination's value.\n. Not sure, But with that said not sure it would even need to because a slice has a len internal field on its data structure. So it won't need to calculate value each call.\n. need to finish comment :)\n. @s3 probably should be something like @s3encclient to be specific to these integration tests. @client is generally reserved for the generated service client too.\n. I think examples need to be indented or they won't show up in the godocs as examples\n. Should also document that AES GSM is in memory  and not streamed.\n. Why pass in the address of the slices?\nI think this could be simplified with a bytes.Buffer as the block buffer to store extra out of block byes.\n. Why does dst need to be a pointer?\nI think this function can be replaced with a bytes.Buffer using the read and write methods.  Something like https://play.golang.org/p/D6kqywHMsW I think will remove the need for the copy method in both decrypt and encrypt.\n. 0 probably should be returned for number of bytes read if there was an error.\n. Same here len(b) bytes where not read because there was no data to be decrypted\n. I think assert.Equal can be used here to compare byte slices, this might simplify these assertions a little bit.\n. Looking over this i think it makes sense for EncryptContents to follow the same pattern as DecryptContents. This puts it more in line with the stream patterns used by the standard library such as compress/gzip. The io.Copy pattern is kind of unique where as the stream pattern is more general.\ngo\nEncryptContents(io.Reader) (io.Reader, error)\n. Updating EncryptContents to follow the streaming pattern would simplify this function a little bit removing the need for the io.Copy. The io.Copy effectively would get moved into newSHA256Writer I think and,  updated to take a reader instead of being written to.\n. How would additional cypher's for AuthenticationMode be added in the future?\n. Nit: the order of the equal parameters are reversed. The \"expected\" should be second, and actual is third.\n. would be nice to have a different name for CipherDataIface. What do you think about something like CipherMetadata?\n. Nit I'd rename modeFactory to something like modeForEnvelope. Only because its more descriptive.\n. Nit same as above, I'd rename keyProviderFactory to keyProviderForEnvelope. Only because its more descriptive.\n. same for cekFactory -> cekFor Envelope\n. I'd suggest a more specific error code, maybe something like InvalidWrapAlgorithmError. Similar more details about the context of the error n the message.\n. AES/GCM/NoPadding should be a const\n. Similar comment as above abut more info in error message add code.\n. Should GetContentLength be a int64? int on 32 bit systems will limit the content to 2GB.\n. int64 instead of int\n. With the above change for content length to int64, this probably can be just\ngo\nvar contentLength int64\n. In what case could the hash reader be nil?\n. nit, strconv.FormatInt probably would be a better option that fmt.Sprintf. Prevents the need for the extra reflection/type check calls that would occur in fmt.\n. Is this function expected to initialize and envelope, or just augment an existing envelope with data? If its creating its initializing the envelope would it be better for this to return the envelope instead of creating it?\n. can these hex.DecodeString just be the following?\ngo\niv := make([]byte, 32)\nkey := make([]byte, 64)\n. Use the ``` for formatting JSON literal strings. Easier to read and less prone to bugs.\ngo\nMatDesc: `{\"kms_cmk_id\":\"\"}`,\n. nit, Probably want to use assert.NoError for error, and then do the cast to AESGCM after that assert. \n. Generally try to use assert.Error or assert.NoError for error checks.\n. Nit We aren't doing anything, the type's methods do the work :)\n. What do you think about moving these header strings to consts. protect against string typos. In addition the Header keys should use title casing, . e.g. X-Amz-Meta-X-Amz-Key-V2\n. Is this method complete? Based on its name I would of expected it to do something.\n. Nit, what about DefaultInstructionKeySuffix ?\n. Nit InstructionFileSuffix should be InstructionKeySuffix. Alternatively, what if I, as a user, want to use a completely different instruction key of my own choosing?\n. Is this TODO still valid?\n. This will panic if input.Key is not set. Probably should return a InvalidParameter error if key isn't set. This will be captured by the validation, but thats too late for this case.\n. use `` for to prevent need for escaping quotes.\n. Nit, Both styles are OK, but preferable to only use make when needed.input.Matadata = map[string]*string{}can be used here.\n. int64\n. TODO still valid?\n. This interface does a lot. Interfaces should be as small and concise as possible. What do you think about splitting this interface up? Initially like the generation of key/iv from the get/set methods?  In looking at the usage it doesn't look like the GenerateKey|IV are done in a different context from get/set. \n. can remove?\n. Still needed?\n. Instead of taking asession.Sessionthis should useclient.ConfigProviderImportingsession` could risk causing can cause circular references in the future.\n. TODO still valid?\nDoes material description need to be an interface since the values must always be key value pairs? Seems like a typed aliased map[string]*string with (Un)Marshal methods on it would be sufficient.\n. nit,\ngo\nreturn &JSONMatDesc{\n    data: map[string]*string{},\n}\n. Nit, Instead of EncodeDescription what do you think about just Marshal returning a ([]byte, error), Similar for DecodeDescription\n. Same as above.\nIn addition naming of this function vs the one above is confusing. Both take material descriptions, but one takes it differently, and it looks like they duplicate code, but in slightly different ways. Are both needed? or can just the previous one be used?\n. should be client.ConfigProviders\n. I'm not seeing where the MasterKey is used. Is this still needed?\n. it not we :)\n. Is SaveStrategy intended to be an anonymous field? Interfaces as anon fields can hide missing implementations.\n. ./ may fail on non *nix systems, e.g. windows. This should be filepath.PathSeperator. just ..\nBut with that said why do we not want to use the default OS temp directory, those will be cleaned up by the OS eventually? Which is especially helpful if this process crashes. Either way there probably should be an config option to specify the directory for temporary files.\n. EncryptContents updated to be a stream vs copy pattern, should only need minor changes to the sha and md5 read/writers.\n. Could the sha util type include this encoding as a method., e.g. GetValueHexEncoded\n. should use named returned values, easy to cause bugs by relying on naked returns.\n. We probably only want to create the temp file if the input.Body's content length is greater than X size. 5MB maybe?, In addition this should be a configurable threshold.\n. Whats the impact of doing the close and file delete in both send and ValidateResponse. Shouldn't it only be done once?\n. Can the S3 be the s3iface.S3API interface instead? this will allow users to mock out the SDK's low level service client.\nIn addition should Client statisfy the s3iface.S3API interface? If we want to use it as a drop in for the S3 Manager's service client it will need to.\n. does this method need a pointer receiver?  since its not modifying anything, and the headerSaveStrategy type has  no values?\n. if this were just headerSaveStrategy{} without the New it would prevent the need for an alloc.\n. New should take a client.ConfigProvider not create its own session. Similar pattern to the s3manager's Upload/Download clients.\n. If this handler fails should the out.Body be closed? since users may not close a body if an API call fails. Generally they'd only close the body if the API call was successful.\n. Good point that makes a lot more sense. I'll update the naming.\n. Suggest Updating DecodeMeta to be receiver method on Envelope that is just a decrypter of cipher key and iv. e.g\ngo\nfunc (env *Envelope) Decrypt(kp KeyProvider) (cipherKey, iv []byte, error error)\n. If cekForEnvelope is updated to just take the algorithm and not the full envelope won't need to pass in env.\n. Thanks, I agree, I'm not happy with the panic either. Logging the error at this point is the only alternative I can think of.\n. That sounds fine, can remove this method. It has a nitch usage and can add it back later if there is demand for it.\n. Thanks I'll take a look at refactoring these methods to see if I can create a better naming scheme for them, that fits in.\n. Updated the panic to follow the existing functionality of New where errors are not reported when request is made.\n. Need to update integ tests for correct way to create session.\n. Sounds good was thinking about converting these to typed errors for simplicity anyways, so this would work out well.\n. This is actually intended. The files are read in order and each file read has the chance to stomp over the previous file's values. This simplified the logic for reading/setting values, since we support some partial data loaded from each file. e.g credentials from first file, and region from second file.\n. could the first param be b[:0] so that the byte array will be reused for output? If seal needs to grow the array it can, and the returned []byte  (b) will be updated as needed. But gives Seal a chance to reuse the byte array.\nhttps://golang.org/pkg/crypto/cipher/#AEAD\ngo\nb = reader.encrypter.Seal(b[:0], reader.nonce, b, nil)\n. Seems dirty having a function on a type that returns it self. This is a pretty small function and probably can be on the users of the value instead of embedding.\n. include err in samples.\n. same as above for reusing the byte buffer for Open as suggested for Seal.\ngo\nb = reader.decrypter.Open(b[:0], reader.nonce, b, nil)\n. little odd naming here NewEncryptor func probably should be ContentCipher\n. func named, ContentCipher\n. CipherData can stay a value an not a pointer since it does't need to be mutated.\n. This should return a value not pointer\n. Probably shouldn't embedd this unless really needed, \n. not embedded\n. This interface requires the underlying type to be safe concurrently.\n. Does mat description need to be embedded?\n. Should this be method be just return md ?\n. Debug line\n. Nit would go with Client or S3Client  for the field name.\n. Nit would go with Client or KMSClient for the field name\n. Would a aws.WriteAtBuffer or similar be good to use here instead of a bytes buffer? Implementing Seek, but disabling its functionality could cause hard to track down errors.  It works in the use case here, but if any other usages of seek are added such as seeking to get lenght, tracking down the bug will be pretty hard.\n. I suggest moving all of the temp file logic into a helper function. This will simplify reading this function.  In addition if the in memory buffer satisfies the same ReadWriteSeekCloser interface then special case logic is not needed at line 147\n. This will panic if dst isn't a os.File. Making sure in memory buffer satisfies the same ReadWriteSeekCloser interface will remove the need for special case logic.\n. Don't need to attempt to clean up twice. I suggest just do it on Send. Doing cleanup on both could be source of errors in future.\n. can this be https for the link's scheme?\n. The section in encode and decode look very similar it would be helpful to reduce the duplicated logic between the two.\n. thanks, fixed.\n. updated\n. This was originally done as an explicit copy because of race conditions that could occur between the original request's Body and the underlying client's Transport could still have an outstanding call to r.Body.Read. The Go race checker flagged this as a race condition. This is why the individual fields were copied.\nThis is also why the Request wraps the original body reader in a offsetReader. So that the underlying buffer can be rewound for the next retry request attempt. \n. Because of the above comment, regrettably we still need to keep this version around.\n. would be nice to document the input parameters here also in addition to the readme.\n. How about just concatObjects for shorter name.\n. Also would be great to describe what is going on and how UploadPartCopy can be used to concat two objects already in S3 into a new object. And mention if you can do A = A + B, or only C = A + B\n. leading spaces, and were tags changes intended?\n. I think we can just change this to Split instead of SplitN. I don't think we need to limit the number of parts in the tag. \n. This function is really big, can it be broken down into smaller chunks?\n. could you add docs describing what the new parameters do\n. go\n// +build 1.6\nWill fix the build errors.  The generation code can only run with Go v1.6+ because of the newer template features used. - to trim template white space.\n. Would it make sense for li tags be prefixed with a * as a bulleted list instead of just code block.\n. Looks like the stack is being recreated twice here  with info above. Is that needed?\n. cloud level be replaced with len(stack) or does the two need to be separate?\n. Replacing size with len(stack) in usages will probably help prevent bugs since we'd not have to worry about extra variables to maintain.\n. I suggest breaking these deep nested functionality into their own function, and using early exits to prevent such deep nesting of conditionals.\n. In the diff I noticed, that the change removes the indention from the item below. Are these arn lines paragraphs as well in the doc model?\ndiff\n        // String that contains an ACM Certificate ARN. The ARN must be of the form:\n        //\n-       //  arn:aws:acm:region:123456789012:certificate/12345678-1234-1234-1234-123456789012\n+       // arn:aws:acm:region:123456789012:certificate/12345678-1234-1234-1234-123456789012\n        //\n        // For more information about ARNs, see Amazon Resource Names (ARNs) and AWS\n        // Service Namespaces (http://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html).\n. Another case in applicationdiscoveryservice/api.go are these paragraph also, not pre tags?\ndiff\n        // Tags that you want to associate with one or more configuration items. Specify\n        // the tags that you want to create in a key-value format. For example:\n        //\n-       //  {\"key\": \"serverType\", \"value\": \"webServer\"}\n+       // {\"key\": \"serverType\", \"value\": \"webServer\"}\n        //\n        // Tags is a required field\n        Tags []*Tag `locationName:\"tags\" locationNameList:\"item\" type:\"list\" required:\"true\"`\n. Need to take a look at how to prevent the golang.org/x/net/html being included as a runtime dependency. I have PR #891 to add codegen build tags to the /private/mode/api packages to prevent them causing their dependencies to leak into the runtime.\nThough I think this is a strong argument for splitting the code generator logic out of the SDK. Doing so would prevent polluting the SDK's runtime environment with code gen deps.\n. ah i think i misread the code, was thinking the info := stack[len(stack)-1] and stack = stack[:len(stack)-1] were the same subslice, missed that info was an element, not slice.\n. The PR #891 will also fix this too for users in general. but i think both are needed for these functions because they use 1.6+ features.\n. would it make sense to do strings.TrimSpace on the txt and tag fields then check if they are empty strings?\n. I thought of this, but didn't because its a bit harder to read. The ! is really easy to miss.\n. thanks fixed.\n. The // +build example should be the first line of a file followed by a empty line. After that you can start the package docs.\n``` go\n// +build example\n// Package unitTest demonstrates how to unit test, without needing to pass a\n// connector to every function, code that uses DynamoDB.\n``\n. An empty  line is needed after the build tag of a file before any other docs.\n. can useaws.BoolValue(r.Config.DisableRestProtocolURICleaning)instead of needing to use&&here\n. Thanks, this was a bug those fields should of been set.. updated to leave as string.. I thought about that, but did it as just generic list of functions since it was simpler and more generic. I can put them all in an array if that helps.  Customizations are not required to be service specific. They could be anything, or partition wide. Doing it as a generic function integrated over for all partition simplifies this.  Thoughts?. Yeah i think we can drop theendpoint != \"\"from this. It was moved from an old implementation that did some other logic, that is not needed. here.. Should usefilepath.Join` here instead of hardcoded separators.. Would it make sense to use:\nname == namespace + \":\" + a.Name.Local\nor\nname == fmt.Sprintf(\"%s:%s\", namespace, a.Name.Local). Probably want code block ``around this. Remove copy/paste error handling?. Update error messages to be more obvious what failed. This and other tests added.. Ended up refactoring this logic simplifying this. will add. updated. yeah included this in the refactor\n. updated thanks. probably want to refer to these as noCrossLinkServices little bit longer, but adds context. blacklistedServices is too generic.. same here for too generic with BlacklistedServices. This probably should take just the string, instead of the whole API value.. Thank! I keep forgetting to fix that :). thanks, fixed.. redirectURL should be added to the template data not injected into the string like this.. redirectURL should be added to the template data not injected into the string like this.. \ud83d\udc4d . are the $. needed? generally we haven't used that notation for the templates before. Unless its needed we probably want to be consistent.. Suggest something like origVal or similar so its clearer the difference between value and v.. Suggest a comment what scratch is for and why.. Intentional to switch from indirected to original value here?. Should the var $errDoc be moved here instead of being a variable above?\ngo\n{{ if $err.Docstring -}}\n{{ $errDoc := $err.IndentedDocstring }}\n{{ end -}}. This will cause the MissingEndpoint error to be returned. Services like CloudSearchDomain and IoT-Dataplane use this functionality to alert users that they need to manually provide the endpoint.. hmm I'll look at simplifying.. thanks. Always resolving would be a break expectations with services that the endpoint should not be resolved such as CloudSearchDomain and IoT-Dataplane.\nThe boolean option really is there for the SDK's internal endpoint resolving via the sessions. Users using the endpoint resolver directly will generally not need this option, but it is there if needed.. The switch here was focused on simplicity. Since an isolated piece of logic was being provided an function pointer was simpler and provided the same functionality. . How about rename to serializeMap since Instantiate implies creating a map, but its really creating a string serialized version of a map.. Still needed?. Instead of adding an optional ShapeRef. Is it possible to add this decoration somewhere else? Mainly because this is an optional parameter that is only used in a narrow use case.\nMaybe compare this logic to how the SDK generates io.ReadSeeker, vs []byte payloads.. Should bool be added to this as well?. Could you comment on checking for case \"jsonvalue\" both here and in the case map[string]interface{}: case above? When would both cases be needed?. If the shape type is a jsonvalue does it still need to do the traverseMap? Might make sense to just create a serialized for JSONValue directly instead of reusing map, and not transversing.  In the case of JSONValue, while it is rendered as a map type the type is opaque, and there is never any transversing needed. Basically it is the same as a scalar as far as serializing here.. Probably want to create a new Shape here or reuse a shared JSONValue shape, because just renaming the existing shape the ref points to could easily cause other refs that point to the same shape to be rendered incorrectly.\nIn the following model, the shape pointed to by the refs, A, and B will be converted to a jsonvalue type if I'm understanding this code correctly.\njson\n\"MyShape\": {\n    \"members\": {\n         \"A\":  {\"shape\": \"String\"},\n         \"B\": {\"shape\": \"String\", \"JSONValue\": true}\n    }\n}\n. Would this cause a JSONValue type to be rendered as a interface{}?\ngo\ntype MyType struct {\n    A interface{}\n}\nIt looks like it would here. If the shape's type is a jsonvalue we can probably treat it as an opaque type and not transverse it at all, and just render as map[string]interface{}. This would probably save some additional conditionals/checks that were added.. Previous comment covers this where I think we can not treat jsonvalue as map at all ad more similar to a scaler or opaque type that is not transversed.. is this still needed if the shape(Ref).Type is a jsonvalue? I wouldn't expect it to be.. if the shape's type:\"jsonvalue\" struct tag is used, is this field tag still needed?. I'm not sure this is really needed. JSONValue type can be used the same as a map as far as operators go, including range. I'd leave this out for now unless there is a strong reason to keep it.. Still needed? Is there still a need to check for jsonvalue in if shape.Type == \"map\" || shape.Type == \"jsonvalue\" { now?. Shouldn't this be \"aws.JSONValue\" as the returned string.. I think this should be aws.JSONValue{} since moving to alias instead of map directly.. Need to fill out comment?. is this helper still needed?. Is this condition still valid since the type will be a aws.JSONValue not a map[string]interface{}. Or is is it difference in this case?. Maybe expand the example to include example key/value pair?\n.e.g:\ngo\naws.JSONValue{\"key\": \"value\"}. yeah... Look at this i'm not sure why the io.EOF check is here. I'll dig a bit to see if i can find a reason. I think the EOF condition can be removed.. so... it looks like xml.Decoder.Decode can actually return an io.EOF when the reader runs out of bytes, or was empty to begin with.. yeah... realized that was missing. Adding tests. Can the unsignedBody option be used to replace the check for s3 here?. Would be great if this could be a option also.  Is it always true that be cause unsignedBody is true, the X-Amz-Content-Sha256 header is also set? These seem like they might be two different conditions, especially in the future.. Probably don't want to clear all Signing handlers here. Could be custom signer handlers added upstream, e.g service, and session levels. This could instead probably remove the default signer handler(s?) by name.\nWill also probably want to generate docs for the API operation that are unsigned stating that operation will ignore the signing operation applied at the service level, and overwriting it when the request is created.. What do you think about making this a bit more generic.  Instead of having a predefined handler for this specific option, what about a builder that takes as input a config, and will return the built the request handlers for that configuration?  Could solve some of the other configuration issues such as single vs double encoding, and when the Sha256 header is added. Makes it easier to combine configurations in the future too.. makes sense.. Custom request handlers that are added at the service or session level would of been applied to the request when c.newRequest( is called.  A Clear() here would remove any  of those custom request handlers added to the Sign list.  Only custom request handlers added after the request is created would be preserved. This was the reasoning for explicitly removing the known signers. \nThough fun to consider if this trait were to be applied to a service like S3 anyone using a custom v2 signer that the SDK has no knowledge of would be removed on a Clear(), or incorrectly left in on a remove by name. I'm not sure the best way to handle this. Situations like this is where the concept of the request handlers being a list breaks down, and a singular function call is needed.\nI'm not sure Clear() can be used without additional logic. Partly because custom handlers added by the user, and handlers like BuildContentLengthHandler are added to the Sign list.\n. What do you think about something like WithWaiterDelay? I think it mixes the best of both suggestions.\nWaiter Option examples:\n WithWaiterDelay\n WithWaiterMaxAttempts\n* WithWaiterLogger\nRequest Option examples:\n WithRetryer\n WithLogLevel. finish sentence, need to mention it lets you use the WithContext methods when using Go 1.6.. Instead of taking a cfg struct, i'd suggest functional options that modify Signer directly setting the options they want. This unifies the SDK's services configuration options and using the Signer standalone. This also simplifies the changes that would be needed to configure single vs double encoding for services like S3 instead of hardcoded here in signSDKRequestWithCurrTime.\ne.g\n```go\n// Can a Option type be created?  Might conflict with NewSigner. If not all the Option below would need to be replaced with func(Signer).\ntype Option func(Signer)\nfunc (s *Signer) ApplyOptions(opts ...[]Option) {\n    for _, opt := range opts {\n        opt(s)\n    }\n}\nfunc WithUnsignedBody(b bool) Option {\n    return func(s *Signer) {\n        s.UnsignedBody = b\n    }\n}\nfunc withTimeNow() Option {\n   return func(s *Signer) {\n        s. currentTimeFn = time.Now\n   }\n}\nfunc BuildNamedHandler(name string, opts ...[]Option) request.NamedHandler {\n    // ...\n   signSDKRequestWithCurrTime(req, append([]Option{withTimeNow}, opts...))\n   // ...\n}\nfunc signSDKRequestWithCurrTime(req request.Request, opts ...[]Option) {\n    // ...\n   // Add SDK service API op customizations to signer via appending to func options.\n   // TODO: Maybe invert with if service API op customizations are first or last in list.\n   opts = append(opts, func(v4 Signer) {\n       // ...\n   })\nv4 := NewSigner(req.Config.Credentials, opts...)\n   // ...\n}\n```\nThis also allows the curTimeFn to be moved to an unexported Option also.\n. would be great to have test for this.  . could make the WithUnsignedPayload declarative instead of toggle. I think thats fine too.. Thanks for pointing this out. I missed this during the initial conversion.. This makes sense I can switch this to be a time.Timer directly.. Thanks for pointing this out. I think that makes sense. I'll update these new methods to require a context.. Will clean up these lines to be more inline with context contract.. removing these checks.. I think creating a aws.BackgroundContext() like you suggested is the best way to extend the existing functionality without the need for nil contexts. I'll put an update together that includes this and remove the nil support.. yeah... not really that clear. I'll fix this.  Was supposed to convey that the numbering starts at 1 as in attempts made, like first, second, ect. This is apposed to the \"standard\" numbering of starting at 0.\nMaybe instead of attempts plural change to attempt singular. Would that be more clear?. sure can change that.. see above. SleepDelay is a testing function made available via aws.Config.  For backwards compatibility, if the sleep delay function is set, we need to respect it.\nThough pointing this out does show that I do need to update the docstsring for SleepDelay to state that setting SleepDelay will prevent the SDK using context to interrupt retry delays. Context would still interrupt in flight connections.. Makes sense, I'll add the check and panic to SetContext.. Fixed spelling issues. thanks. I don't see any issue making this change. I'll update Background to source the background based on Go version.. Thanks clarified this statement for the operations.. The emptCtx.String method seems to be primarily used by logging and building a string representation of the chain of contexts created. The golang.org/x/net/context used the String method for this well.. Thanks, I missed this one. I'll update the waiter and request retry delay flow to share the same logic.. suggest merging ok if check up to the assignment above since both value are used within the scope of the if. For these error statements it would be helpful to use also log k.\nIn addition should use t.Fatalf instead of a separate nested Sprintf. Both here and line below.  Also probably only want them to be t.Errorf so if one fails it won't prevent seeing the other errors.. Suggest doing a select instead of wait after\n```go\nfunc (r *timeoutReadCloser) Read(b []byte) (int, error) {\n    t := time.NewTimer(r.dur)\n    defer t.Stop()\nc := make(chan readResult)\ngo func() {\n    n, err := r.reader.Read(b)\n    c <- readResult{n, err}\n}()\n\nselect {\ncase <-t.C:\n    return 0, timeoutErr\ncase res := <-c:\n    return res.n, res.err\n}\n\n}\n``.ErrCode` prefix should be used for these error codes is what were used for the service clients error codes.\nWe probably should also define these in the request package. That is where the other request errors are enumerated.. Probably want a more specific error code such as ResponseReadTimeoutError. . AfterFunc no longer used. Suggest describing that the timeout duration is per Read call. Its probably per packet, but I don't think Go http.Client guarantees that.  Also should mention what error code will be returned if timeout occurs.\nAlso this function should probably be called WithResponseReadTimeout  To clarify what the expectation is.. I don't think this will do the right thing.  Request Options are added at the beginning of a request lifecycle. This will cause a nil pointer panic because HTTPResponse isn't a valid value until after the Send handlers are executed.\nI suggest this be changed to:\ngo\nfunc WithResponseReadTimeout(duration time.Duration) Option {\n    return func(r *Request) {\n        r.Handlers.Send.PushBack(func(req *Request) {\n            req.HTTPResponse.Body = &timeoutReadCloser{\n                reader:    req.HTTPResponse.Body,\n                duration: duration,\n            }\n        })\n    }\n}. Should also add test for WithResponseReadTimeout. With the suggestion above of fixing, WithResponseReadTimeout  This line becomes the following, No need for the addTimeoutReadCloserOption function.\ngo\nr.ApplyOptions(request.WithResponseReadTimeout(readDuration)). Should use the constant instead of string for RequestTimeout. Current the awserr package has no errors codes in it. All current request related error codes are defined in the request package. \nError codes in request package were miss-named and not caught before release of the feature :(. Nit adaptError function can be declared outside of the WithResponseReadTimeout since it doesn't use any values from this scope right?. With the addition of the adaptError function should add test that verifies that functionality as well.. Nit, this function's docstring should include what error will be returned if the timeout occurs. In addition clarify that the timeouts are per reading fromhttp#Response.Body. Yeah append will increment the len of the slice making room for the new element. The shift will then move all the elements in the slice by one, effectively creating space as the head of the slice for the handler to be inserted into.\nhttps://github.com/golang/go/wiki/SliceTricks#insert . pretty minor, but will allow the request handler to be GC'd if the element is set to a zero value because the array still will reference the request handler. \nThis does bring up the point that the clear function should zero the array before resetting the slice len for the same reason as above.  I'll make a change to update clear.. I'm not sure i'm following. append is needed to increase the len of the slice. The underlying array already has enough capacity for the insert.  Append will just extend the len of a slice if there is enough capacity. append only creates new array if the slice's backing array doesn't have enough capacity.\ncap == len means there is not enough space to add a new element without allocating a new backing array. this is why the first condition needs to allocate a new array, and the second doesn't.. I think the error string compare should be lower case, connection reset. \n. Nit would be nice to merge these two if statements into one.\ngo\nif err, ok := r.Error.(awserr.Error); ok err.Code() == ErrCodeSerialization {. Can we take advantage of,  net.OpError and \noperr.Err.Error() == syscall.ECONNRESET.Error()\nhttp://stackoverflow.com/questions/19929386/handling-connection-reset-errors-in-go. Good point. will update to be error when request credentials.. Adding a small blurb about why someone would want to enable this flag would help clarify what it is for. \nSuch as something about when providing a custom retry handler and howShouldRetry will be handled with and without the flag enabled.. can these use the ErrCodeReadError constants?. can these use the ErrCodeReadError constants?. Sure, I don't see any reason not to do that.. The apply options is intentionally done after the function sets up the request so modifications can be applied. If ApplyOptions were done ahead of time it would be very difficult to modify the handlers added by the function.. \ud83d\udc4d . Sounds good, added the bool return parameter.. For these i suggest just removing the api.go files from the PR. We can upstream these doc issues to the service teams. I think Source would be a better name for this parameter.  Body isn't obvious that this is the source the underlying tee reader's content comes from.\nIn addition it would be nice to document the members so its quick to understand what this type is for and what the members mean.. Would prefer to use named fields here not implicit.  Its easier to read and don't need to lookback at the teeReaderCloser source when browsing the code.. Can this just return instead of the else ?. Not sure this should overwrite an SDK API error if one existed. I think this defeats the move to using the TeeReader.  Both here and DumpResponse usage above.. need // +build example  build tag . probably don't want to set or override the error here as well since this is the logger. Having this here is to ensure that the exposer of the nil HTTPRequest.Body value doesn't escape where it is absolutely needed. Doing this in request.Send would allow the nil value to be exposed to other request handlers within the Send handlers list.. good callout I'll update the docs.. the tabbing of the template directives here makes it a little hard to read. I'd suggest something like:\ngotmpl\n        switch aerr.Code() {\n        {{ range $_, $ref := .Operation.ErrorRefs -}}\n            {{ if not ($.HasVisitedError $ref) -}}. For the indexes, what about XX(e.g 00, 01, 02) instead of a, b, ect.. This can be replaced with the generateExampleInputs mentioned below.. I think some of this function can be apart of the example template above. Especially the input := ..., and result, err := svc.... portions.\nI suggest making a helper function only for the input parameters type it self.  With this you could add the following to the example template in place of the {{ generateFunction . }} call.\ngotmpl\n    input := &{{ .API.PackageName() }}.{{ .Operation.InputRef.Shape.ShapeName }} {\n        {{ generateExampleInputs . }}\n    }\n    result, err := svc.{{ .OperationName }}(input)\n. Suggest doing the basic input and API call template here instead of in code.   See the generateExampleInputs mention below.. How does this get replaced for customizations? I'm not seeing this redefined. or replaced anywhere.. I think this function can probably be split into a few components. specifically the components based on type of the shape value.\n. This is a really good example of these template strings should be a const or template and not defined inline.\n. All of these WriteString calls should write a single const string or from a template. Doing each line a separate calls will make it really easy to introduce errors.. these comments still needed?. ditto. I noticed the buildComplex doesn't handle slices is that just because we didn't have any list data in example model?. empty conditional. would be helpful to document the isMap parameter and what its expectations are. still needed?. I my PR #1308, I created the utility SprintExpectActual in awstesting package. that takes care of this. This should make the testing out put easier to read. Should be able to apply it to all of these large blocks.. Using () here instead of wrapping quotes will remove the need for the escaping the double quotes. Same for\n other test cases in this file.. I don't think this file should of changed.. Where these changes to this file intentional?. I think these are missing nested values from the example model. Based on the description it sounds like there should be additional fields.. Should these have fields's AttributeValue have values?. I think something in the example generator is preventingAttributeValuefields from getting rendered.  Or is these just missing from the model?. good point, i'll add validation of the length.. but bees are fun too :). could you add a doc that describe what this function is and why it is needed?. Thanks fixed these tests.. I suggest just a emptyselect{}for this instead of creating aWaitGroup.  Adding a comment line stating why this should wait forever would also help clear up any confusion.\n.Nextshould increment theindexif there is no error. iserrneeded since this is just a list of objects, and cannot error?.Nextshould be the function to increment the index.DeleteObjectshould just return the value at the current index.. Could this type be exported publicly as justDeleteObjectsIteratoror similar?  This would remove the need for theNewconstructor.. Next should increment the current index, up to the max if no error. How should the batch size be specified?. This should be NewBatchDeleteWithClient, the NewBatchDelete should take a ConfigProvider similar to s3 upload and download manager. . This looks like it deletes the list objects twice. Is this correct?. Is this type still needed if Batch download is a method on the s3 download manager?. This looks like it downloads the objects twice?. It doesn't look like batch size does anything for download.. It doesn't look like this is really a batch download but an iterator based delete. Are these deletes concurrent? Id didn't see that if they are..objectswould be good to use the plural form.. This is pretty fragile. I suggest not trying to do this logic here, but relay on the [request.Pagination](http://docs.aws.amazon.com/sdk-for-go/api/aws/request/#Pagination) utility instead, while caching thePagination` value between calls. Similar to how ListObjectsPages is implemented.\nThe following simplifies this i think and removes the duplication pagination low level logic.\n```go\ntype DeleteListIterator struct {\n    paginator request.Pagination\n    bucket    *string\nobjects []*s3.Object\nerr     error\n\n}\nfunc NewDeleteListIterator(svc s3iface.S3API, input s3.ListObjectsInput) DeleteListIterator {\n    return &DeleteListIterator{\n        Bucket: input.Bucket,\n        Paginator: request.Pagination{\n            NewRequest: func() (request.Request, error) {\n                var inCpy ListObjectsInput\n                if input != nil {\n                    tmp := *input\n                    inCpy = &tmp\n                }\n                req, _ := c.ListObjectsRequest(inCpy)\n                return req, nil\n            },\n        },\n    }\n}\nfunc (iter *DeleteListIterator) Next() bool {\n    if iter.err != nil {\n        return false\n    }\nif len(iter.objects) > 0 {\n    iter.objects = iter.objects[1:]\n}\n\nif len(iter.objects) == 0 {\n    return iter.Paginator.Next()\n}\n\nreturn true\n\n}\nfunc (iter *DeleteListIterator) DeleteObject() BatchDeleteObject {\n    var obj BatchDeleteObject\nif len(iter.objects) == 0 {\n    p := iter.Paginator.Page().(*s3.ListObjectOutput)\n    iter.objects = p.Contents\n}\n\nreturn BatchDeleteObject{\n    Object: &s3.DeleteObjectInput{\n        Bucket: iter.bucket,\n        Key:    iter.objects[0].Key,\n    },\n}\n\n}\n``. No need to check for error for this type because there never can be an error. This seems to be attempting to delete these objects twice. In addition what is the purpose of the \"BatchSize\" value since only one object is deleted at once?. Don't need to check for error here because there never can be an error. Don't need to check for error here because there never can be an error. Theopts` value is not used.. It looks like these are trying to upload twice. What is the purpose of the BatchSize value since these uploads are all serial?  This is a little confusing design I'd imagine this could be simplified to something like the download with iterator method:\ngo\nfunc (u Uploader) UploadWithIterator(ctx aws.Context, iter BatchUploadIterator, opts ...func(*Uploader)) error {\n    for iter.Next() {\n        if _, err := svc.UploadWithContext(ctx, iter.UploadObject().Object); err != nil {\n             return err\n        }\n   }\n   return iter.Err()\n}. We should consider if this will fail on the first error or continue until the iterator drained, and report all errors at the end.. Ignores the error that can happen with the iterator.\ngo\nreturn iter.Err(). Link upload probably should consider how errors should be handled, either continue until iterator is drained reporting all error(s) or fail on the first error.. I don't think BatchSize is actually used for any batching.. not needed.. opts not used it should be passed into the DownloadWithContext call. not needed. Not needed. This probably should just return the paginator's error since no error state is being keep in DeleteListIterator type.\ngo\nreturn iter.Paginator.Err(). Probably want the construction of the DeleteListIterator's paginator to be done in a NewDeleteListIterator function, similar to the example mentioned earlier. User needing to do this each time they want to use the delete iterator would be too much.. If returning a list of errors of files that failed to upload, probably want to create a custom error type that wraps the individual error with a type which includes a field for the object key of the upload that failed.\n. Similar to Upload probably want a custom error type that provides a way to get the object key that failed to upload. print lines still needed?. It would be helpful for the Error message to include the bucket and object key that the error failed on.. I'll clarify this statement to stay after the object has been attempted to be uploaded. Regardless of success/failure.\nSame for the other After functions. should create a new iterator for each test case to prevent pollution. Sharing the iterator value can hide or introduce bugs in the test.. Nit this will only wouldn't this be similar to just do something like the following, or change from an index, to a boolean flag.\ngo\nfunc (iter *testAfterIter) Next() bool {\n    defer func() { iter.index++ }\n    return iter.index == 0\n}. This would be nice to export.. need to capture/report object.After() error.. I think this function can be significantly simplified if checking for field parity is done in Delete method.  This would prevent the need for duplicate logic to send the DeleteObjects API.. DeleteObjects API will actually return successful, but with a list of errors if some of the deletes fail. http://docs.aws.amazon.com/sdk-for-go/api/service/s3/#DeleteObjectsOutput. It would be nice for this for loop to be included in the deleteBatch function, or as its own function so the logic isn't duplicated in here, and at the end of this function.. I think only doing the append here will cause the after of the object that fails the parity check not to be called, because it will never be added to the objects slice.. I don't think hasPairty will return the correct value if o1.MFA is not nil but o2.Object.MFA is nil. It looks like this will ignore the differing values. Is this correct?\nI suggest something like the following to simplify this.\ngo\nif aws.StringValue(o1.MFA) != aws.StringValue(o2.Object.MFA) {\n    return false\n}. I suggest making this the default option if the SleepWithContext is nil.  This would prevent needing to modify all of the generated waiters.. Suggest merging these two together\ngo\n        delay := w.Delay(attempt)\n        if sleepFn := req.Config.SleepDelay; sleepFn != nil {\n            // Support SleepDelay for backwards compatibility and testing\n            sleepFn(delay)\n        } else {\n            sleepFn := w.SleepWithContext\n            if sleepFn == nil {\n                sleepFn = aws.SleepWithContext\n            }\n            if err := sleepFn(ctx, delay); err != nil {\n                return awserr.New(CanceledErrorCode, \"waiter context canceled\", err)\n            }\n        }. discussed.. This was so that the cost of creating an API  input parameter was included in the benchmark results. The Build benchmarks try to narrow the focus just to building overhead.. Suggest this being:\n```go\ntype OperandExpression struct {\n    OpType\n    Key string\n    Value *dynamodb.AttributeValue\n}\ntype OperandBuilder interface {\n    BuildOperand() (OperandExpression, error)\n}\n```\n. It would be helpful for this to specify the resolution the int64is expected to be. In this case it looks to be millisecond -> second. Is this correct?\nAlternatively dropping the Int64 part all together and just SecondsTimeValue and MillisecondTimeValue with int64 input param. Bit more verbose, but clear on the expectations.. Since i don't think there is any requirement required for services to return a specific resolution of time when used in number format I think having a set expectation would be the best approach.\nMilliseconds would provide the most general usage. The downside to this would be that i think it would defeat the purpose of this utility for second based int64 fields as the user would need to dereference the *int64 value shift for milliseconds, and grab the pointer again before using the utility. This is where the idea of resolution specific methods came from.. Examples should be indented. I think we generally use 4 spaces.. This function might be better as And(left, right ConditionBuilder,  others ...ConditionBuilder) Since And requires left and right hand side components.. Spacing is off with these examples lines. Initializations of these ExprNode and getting children probably can be moved to a helper since the setup is always the same.. This for statement can be simplified by using strings.Repeat\ngo\nret.fmtExpr = \"($c)\" + strings.Repeat(\" AND ($c)\", len(c.conditionList)-1). Suggest putting these validations into ConditionBuilder's buildCondition methods.. Suggest using t.Run(c.name, func(t *testing.T) {... for nested test cases like this.. these probably can be simplified to use input like the other test methods instead of lhs and rhs This also removes the EqualsCond.. Spacing. Should indent these to make them easier to read.. Would suggest not exporting these. As the end user is unable to use the values or type.. The extraDynamoDBAttributeType` are not needed on each line. its assumed by the compiler that all types are the same as the parent per const block.\ne.g.\ngo\nconst (\n    String DynamoDBAttributeType = \"S\"\n    StringSet  = \"SS\". If for some strange reason a service decides to write the time value as an exponent, which JSON allows, I think this would parse the incorrect value. e.g 1.23456e+10.\nInstead of processing the value as a string could we used something like math.Modf to extract the whole(seconds) and fractional components from the value?. These tests are actually generated based on a file in the model/protocol_tests/output/json.json file.  To test the timeFromUnixString function it would be great to have a unit test in private/protocol/json/jsonutil/unmarshal_test.go.  We can later add a protocol test in for fractional time support to the protocol model.. Nice part about Modf is that it preserves the sign of the value for both whole and decimal components returned. So adding the two components together should equal the original value.\nFor precision we really have to primary options, 1.)  add the float precision error into the resulting nano seconds. 2.) pick a predefined maximum precision and truncate decimal component to it. e.g. milliseconds.\nOption 2 probably is the best bet overall to prevent adding complexity of switching based on exponent usage.\n```go\nfunc timeFromUnixFloat(f float64) (time.Time, error) {\n    if math.IsInf(f, 0) || math.IsNaN(f) {\n        return time.Time{}, fmt.Errorf(\"invalid number value for Unix time, %v\", f)\n    }\nw, f := math.Modf(f)\nn := int64(f * 1e3)\n\nreturn time.Unix(int64(w), n*1e6), nil\n\n}\n`. Adding this to the `model/protocol_tests/input/json.json` is pretty straightforward but it looks like the code generation for this requires some tweaking because it will attempt to render into a field which takes an int64, and will cause a compile failure because of truncating the value.\njson\n  {\n    \"description\": \"Fractional Timestamp members\",\n    \"metadata\": {\n      \"protocol\": \"json\"\n    },\n    \"shapes\": {\n      \"OutputShape\": {\n        \"type\": \"structure\",\n        \"members\": {\n          \"TimeMember\": {\n            \"shape\": \"TimeType\"\n          },\n          \"StructMember\": {\n            \"shape\": \"TimeContainer\"\n          }\n        }\n      },\n      \"TimeType\": {\n        \"type\": \"timestamp\"\n      },\n      \"TimeContainer\": {\n        \"type\": \"structure\",\n        \"members\": {\n          \"foo\": {\n            \"shape\": \"TimeType\"\n          }\n        }\n      }\n    },\n    \"cases\": [\n      {\n        \"given\": {\n          \"output\": {\n            \"shape\": \"OutputShape\"\n          },\n          \"name\": \"OperationName\"\n        },\n        \"result\": {\n          \"TimeMember\": 1501917228.309,\n          \"StructMember\": {\n            \"foo\": 1501917228.309\n          }\n        },\n        \"response\": {\n          \"status_code\": 200,\n          \"headers\": {},\n          \"body\": \"{\\\"TimeMember\\\": 1501917228.309, \\\"StructMember\\\": {\\\"foo\\\": 1501917228.309}}\"\n        }\n      }\n    ]\n  },. Suggest instead of using method for each expression type use a single getter taking the expression type as input.\nhttps://play.golang.org/p/txrOvLujQW. suggest substr be prefix. Suggest this be a BuildOperand which returns a Operand type which could be something like the following.\ngo\ntype Operand struct {\n   Name string\n   Value dynamodb.AttributeValue\n   Expression string\n}\nThis will allow you to make ExprNode private, \n. This can be private since TreeBuilder can be a private internal interface.. Suggest instead of a factor builder being constructed by things like Condition switch it around and define an ExpressionBuilder which as helper methods to add Condition, Update ect.\n```go\ntype Builder struct {}\nfunc NewBuilder() Builder {}\nfunc (e Builder) WithCondition(builder ConditionBuilder) ExpressionBuilder {}\nfunc (e Builder) WithUpdate(builder UpdateBuilder) ExpressionBuilder {}\n...\nfunc (e Builder) Build() Expression {}\ntype Expression struct {}\nfunc (e Expression) Names() map[string]string {}\nfunc (e Expression) Values() map[string]dynamodb.AttributeValue {}\nfunc (e Expression) Get(expType Type) string {}\n// or \nfunc (e Expression) Condition() string {}\n``. Should be consistent with errors that are returned. in theunsetCondcase aawserr.Erroris used but genericerror` is used here.\nFor errors can consider what action the receiver is expected to take, or do with the error. In most cases (all?) these errors are fatal, and inform the user that their using invalid input.  Because of this the awserr.Error may be overly onerous.\nAlso consider that the buildTree part of the error message isn't helpful to a user. What about error messages something like, \"build condition failed, unsupported mode %v\", cb.mode\n. probably want error default case for known condition mode. This will also help adding additional cases in the future.. probably want default case also.. nit, Expression here generally isn't talked about as argument. In this context it is more similar to a type that has methods on it. e.g If the Expression does not have a condition expression this method will return nil.. This should be done inside of Builder.Build so that any errors can be surfaced to the user. In addition, this will reduce the duplicate logic that is occurring in each getter. I think it makes sense to see the Expression type as a read only view into the already built/constructed data created by Builder. This logic should be apart of the Builder.Build method.. This logic should be apart of the Builder.Build method.. Should state that a Name is a DynamoDB Operand to create that clear relationship.. Nit it would be helpful for this to be suffixed with ValueMode to keep these constants clear. The _shared00 part isn't needed for these docs.. future tense :)\n`The following is an example using a condition builder to create the expression where DynamoDB record's field name \"Artist\" equals the value \"No One You Know\".. nit, audience is the reader, not a third party the reader will instruct on how to use the library.\nIn this case can replace users must call, with just call the.. target you, instead of Users.  Or consider reword to more declarative such as, Additional expressions can be added to the Builder via theWith___methods. To build an Expression, call Build.. Nit Users :). Not really sure this section adds very much value to the user to the documentation.\nTo explain expression specific requirements/expectations would be better in the Builder types themselves.. This file's content should be apart of the doc.go file . This file as it is will never be visible to users when using godoc, or similar tools.\nA new file added to the https://github.com/awsdocs/aws-go-developer-guide repo would be a good place to put extra higher level documentation, concepts, and detailed example based information. But About DynamoDB Expressions and Using the Package sections should be in the doc.go file regardless.. ah no I'll remove that. Changing the signature of this function would be a breaking change.  I think this change also removes requesting the identity document as well.. The certificates are not consistent across AWS partitions. For example AWS uses this certificate, and AWS GovCloud (US) uses a different certificate. I think each partition uses a separate certificate. . I don't think it would be good to pull in an additional dependency into the SDK.. What do you think about the following. I think it might make this statement easier to read.\ngo\nif !(v.Kind() == reflect.Slice || v.Kind() == reflect.Array) {. I might be reading this wrong, but this seems that this case would be triggered if V is an Array, and Cap is less than len(b).  I think this would trigger a panic by trying to set a slice to array value.. I'm not sure this case is correct. If v is of type [3]string  I think SetString on v should panic. Should this be a copy per rune into v's array, or a set of v's first index?  Is SetString doing a copy? I didn't see that documented in the reflect package.. reflect.TypeOf([]byte(\"\") already exists as a cached var at pkg initialization, byteSliceSlicetype.. reflect.Copy would be better than iterating manually.. If kind isn't used, can squash, into switch. nit, should include the around the package.. did you run into situations where thectx.Requestvalue was nil? I think this value is required for the signer to function.  In that case i think it would be better to fail loudly instead of silently bypass the nil value.. Same question as above insanitizeHostwith nil request value.. could you update this to mention directly that this was copied for the Go standard libraryurlpackage.  Would also be also to include the go version too.. ditto as above. definitely want to return errors from these functions. If the protocol marshaler is squashing them then those marshalers should be updated not to squash the errors.. Thanks for the update @glasser. Using the Go.1.10 function would be the best option I think. The SDK can copy the function into the/private/protocolpackage with attributing the Go 1.10 stdlib as the source. It would be nice if the SDK used the actual stdlib implementation for Go 1.10 and higher. Using build flags is how the SDK generally does this for other version specific utilities.. How about this do a switch on the known types and return an error if the escape mode is a value that is unknown.. Ah you mean errors on write, sorry missed that.  Those should be captured... I don't think the SDK does that for any writes currently like this since the type is a bytes.buffer.. nit extra(in Pr link . Same, extra(and)here. Makes sense, will break out of the loop on first occurrence.. thanks, yeah thats not needed.. sure will update to maintain the error.. Would be good to move this specific rename logic within the this for loop into its own function.  Then the logic can be made more generic to just rename colliding reference, instead of duplicated logic for Set prefix, and Colliding reference name.. Would be helpful if this function name was a little bit more specific. Collides is a little to generic.  Something likeIsGeneratedMethod`.  Let the decision on what to do with the fact a name is a generated method left up to the user of the function.. Nit, this would be nicer as the following. This format puts all names into a single case and easier to read.\ngo\nfunc IsGeneratedMethod(name string) bool {\n    switch name {\n    case\n        \"String\",\n        \"GoString\",\n        \"Validate\":\n        return true\n    default:\n        return false\n    }\n}. Instead of a fmt.Sprintf in this case i think it would be better just to do v.MemberRefs[\"Set\"+name].  Not really gaining anything with the fmt. This is the pattern used for the actual renaming logic.. thanks added the logging.. Nit, should be logged to stderr. we probably want a build tag here so this test cannot run outside of our environment since its reaching out to git cli.\nmaybe use awsinclude for the build tag, maybe codegen, but awsinclude seems to fit better.. could also add a \"stub.go\" file to the models/apis folder, and i don't think this change would be needed.. moved to the Request.WillRetry method to ensure it will always be the case.. Maybe, I'll try that. The types are different so not sure what that will do with ahfallthrough. :( io.SeekCurrent require's go 1.7.. This is used by client pkg right? Lets just move this type into the SDK's /internal/ folder under some sync package maybe /internal/sdkrand. If these two cases are the same this should use a fallthrough instead of duplicate the command.. Is this right?  i think this should be 3 for EC2.. Would be good to also have a test case to validate the customization does not replace a custom retryer provided by the user.. A table testing here would be a great way to covers these different cases easily.. Simple solution for the vet error for this is just to defer done().  This is the standard way of making sure this func is always called. Its safe to call multiple times.. in this case returning error is preferred.. Sounds good will create a helper for this.. This this block is only performed when there is no header present, the check was further up in the function. I'll add docs to clarify this.. Thanks for posting this I'll go back over this and see if this is a bug. This probably should be using time.Millisecond instead of 1e9 anyways.. Is it really valid for reading a header's value type to ignore the EOF?  Need to re-review this part and decodeHeaders' decodeHeaderValue call above. For the HeaderName it makes sense as there may not be another header, but not sure if this is valid for value type.. thanks for pointing this bug out. Ended up the unit test data for timestamp was also incorrect.  . does having a nil for the value impact the test case?. this is probably for all cloudwatchlogs APIs, but surprised this issue never came up before if its all APIs.  I suggest verifying this change with the API directly.. Since the scope of this change was narrowed to cloudwatchlogs can revert this change.. Consider using a completely new HandlerList so that EventStream APIs wouldn't use Unmarshal at all. If customers have customizations that are blanket adding to the Unmarshal HandlerList they could break EventStream event messages.  Using new HandlerList for initial response would prevent this.\nsomething like: (find better names)\n UnmarshalEventStreamAPIResponse - would unmarshal the initial HTTP response of the API operation\n UnmarshalEventStreamMessagePayload - would unmarshal an individual event payload. EventStream format only allows single header values, unlike HTTP.. the check for protocol should be removed as that is now caught in API.setupEventStreams.\nAs for the rest of the function. It only adds the location name to a Event shape's struct. For Events with explicit modeled payloads the LocationName needs to be set. If the event stream event uses implicit payload (no payload modeled directly) no tagging will be added.\ngo\ntype ProgressEvent struct {\n     _ struct{} `type:\"structure\" payload:\"Details\"`\n     Details *Progress `locationName:\"Details\" type:\"structure\"`\n}\nvs\ngo\ntype ProgressEvent struct {\n     _ struct{} `type:\"structure\"`\n     Details *Progress `type:\"structure\"`\n}. the old protocol will still be referenced in the file because of the swap out of the old unmarshaler for the rest one. So both packages will always be referenced.\ngo\nreq.Handlers.Unmarshal.Swap(restxml.UnmarshalHandler.Name, rest.UnmarshalHandler). probably don't want to return a serialization error as this is embedded within the eventstream's debug logger. This code is only used by the eventstream unit tests for decoding the eventstream test data files.  I can update it to return an error instead of panic though.. How about this be a utility method on the API type. There are a few reasons were ServiceID can be used for a few other things in the future.. would be helpful to make these a package level constant so they are accessible to the user. similar to Service Name.  Not directly related note, v2 SDK ServiceName should be renamed to ServiceEndpointPrefix.. Would be helpful to have a example_test.go  that shows these usages. Those would help also ensure the examples compile.. Since this lock isn't isn't used by the reporter it would be helpful to define the variable in aws/csm/enable.go file.. With the InjectHandlers function how is the csm.Start called?  What happens if the the handlers are injected, before Start is called.  Then start is called later?  that probably will be a race condition it think.. I think if InjectHandlers is called prior to Start it will not work as expected since the function pointer for the sender methods.. I think it will be helpful to have a race condition test with multiple writers to the metric chan in parallel, pausing the chan, and unpausing it.  Would also be really helpful to make sure that there are no blocked writers to the channel if Pause is called.. This value definition should live in the reporter since that is where it is used.  Also why a var instead of const?. Can use the technique similar to the request_pagination.go's using atomic counters so a log message is only recorded once. Same for the TODO below in writing to the UDP connection.\nProbably want to pass in the aws.Logger when this is first created.. In this context what about just a return instead of break Loop ?. Would be helpful to render this as a const within the server.go file similar to ServiceName. Are there any services without a service id?. Is it possible to use the logger at this point instead of fmt.Print?. Does this need to be a string? A boolean would be much more natural. like EnableSharedConfig.. Need package docs which describe this package and how its used.. The value needs docs, but Is there anyway to not make this a global value, can this be configured via the Start. Those cleanup should just be in defers then i think.  Thats the pattern that is already being used in the function and is the common pattern for these types of loops.. This test should compare the sender values to ensure the second call to Start did not reset sender to a new value.. Stop should probably be called Pause, as it pauses the reporting.  Need to document that this will panic if Start has not been called.. Nit this doc format should be in the form of\ngo\n// Package csm provides the Client Side Monitoring .... The package doc should also include description of the Get method for getting a pointer  later.. Leaving the unmarshal handler list the way they are in current implementation. the concept of an unmarshal for the response is too ingrained into the v1 SDK. . added back in defers.. This should also set the Err member of UnmarshalTypeError.\nhttps://github.com/aws/aws-sdk-go/pull/1978/files#diff-7b4de96174d722928e311dc8ec32d2fbR744. Regrettably changing the name of this handler will be a breaking change in the SDK.. This example naming is causing the tests to fail.  To have the example paired with the godocs this should be named. ExampleBuildAuthToken. lw wouldn't be initialized here when it should of been if LogDebugWithHTTPBody isn't set.. Forgot tests do actually disable the parameter validation in the eventstreamtest.SetupEventStreamSession helper.. Doesn't look like it.. Moved to eventstream template generation, needed for a few error messages there.. Fixed. Prefer to use typed errors that satisfy the awserr.Error interface instead of using the awserr.New directly. . Suggest using this with go-fuzz to validate for errors. does this need to be exported? and move the literalToken's type accessor methods to this type.. This method should use the UnionValue.String method. Should return an error if the byte's are not a newline. Nit use typed error for ParserError instead of awserr.New. Nit have comment here sating that resetting negative is for the exponent's value. (which could later become negative). Suggest using a type of number to switch between later instead of multiple bools. suggest using strconv.Itoa just for readability.. nit might help readability if Epsilon to be MarkComplete.. Nit need support the : as an equality operator aswell.. nit document the base int value returned. Wouldn't refer to SharedConfig here something more like Default would be better. with the accessors being preferred way of getting the section, moving the map[string]section to a unexpected field would be good.. nit don't need to cast to map here.. nit values probably can be unexpected as it shouldn't be exposed. and move outside of the Sections group of methods. should be unexported. Nit I think may be better for readability for UnionValueType to be ValueType.  Same with UnionValue to be Value. Consider using the ok pattern as a return instead of a NoneType. Minor, but more common usage.. Would be helpful for this method to be grouped with the SharedConfigVisitor type. ditto on grouping.. Suggest using, unicode.IsSpace if able.. nit suggest moving the ini to the repo's root internal folder. tab/spaces are off.. I don't think this comment is valid anymore. I don't think this comment is valid anymore. I don't think this comment is valid anymore. nit would prefer these cases all return their string since the str var isn't being used other than return a value.. Enabling YAML by default would be a breaking change in behavior for some applications if the struct's used by that application already include YAML tags, but their application has been (un)marshaling DynamoDB Attributes based on the struct name.. Couldn't be enabled by default, and would want to document which YAML tags are supported and what they mean.. Would need to document the precedence of yaml replacing associated fields of the json and dynamodbav tags if both are set.. Should add comment saying checking for error isn't needed because the fuzz data includes invalid data, and that this test is validating against crashes.. Don't think this commit is valid any more. Nit should use t.Run instead of prints.. Nit, I think switch statement would be nicer to read that if/else if. doc for this function needs updated.. It would be better to use filepath.Ext here instead of doing the split manually.. nit for consistency with rest of input parameters this should just take the address of mimeType, instead of aws.String\ngo\nContentType: &mimeType,. yeah we can ignore these diffs Go 1.11 changed the format.. Did you find a case where the io.EOF was being returned, and it wasn't a parsing error?  I wouldn't of expected Decode to return an EOF error if it successfully read the content.\nI think the if err == io.EOF { block can be removed.. Need to populate the RequestID field for these errors.  This can be obtained from r.RequestID i think, but need to make sure. The r.RequestID should of been populated from the UnmarshalMeta handler list.. nit this file probably shouldn't change. would you mind moving the process_provider.go and _test files to a processcreds package. Having the credential provider in their own package allow them to depend on the aws package if needed. the credentials package cannot use the aws package since aws imports `credentials.. For these error messages could you please create consts to make comparing against them easier.\ne.g:\ngo\nconst (\n     ErrCodeProcessProviderLoad = `ProcessProviderLoad`\n). Loading the shared config file each time the credentials are retrieved have issues that I don't think is good for the SDK to perform. per #1993. I think we need to look at exporting aws/session/shard_config.go or at least put it under the SDK's root internal package so it can be reused by the process provider. There are modifications to the shared config logic that will be important for this utility.. I think ideally the ProcessProvider doesn't need to read/load the shared config file at all. The loading of the contents of the shared config's credential_process  value should be passed in via an input parameter.  Similar to the stscreds's credential provider.\nIf the shared_config.go logic is exported it would make it easier for the ProcessProvider to have access to the credential_provider value.. Can Expiration not be a time.Time?. the exec.Command should already be pulling the application's environment. is this not the case, or is there an issue that requires it being set explicitly?\nhttps://golang.org/pkg/os/exec/#Cmd.Env. I think this split will have issue with quoted strings within the argument list of the credential_process.  I don't think the following will be handled correctly:\ncredential_process=ls \"my cool/path with/spaces\"\nwill be split into:\ngo\n[]string{\"ls\", \"\\\"my\", \"cool/path\", \"with/spaces\\\"\"}. It is probably best to let exec.Command fail and capture any error with the command not being valid/executable.. Should this capture stderr as well. I don't remember if the err returned by cmd.Output includes stderr or not. This might only include the exit code.. Using a pointer for resp.Expiration would simplify receiving the time value. This is done for the SDK's endpoint credential provider. This information should come from the SDK's session similar to stscreds  There are several locations, and various env var names that can drive the config file to load within the SDK. Having loading the shared config file utility do this work, will help reduce duplication.. The SDK does not use the AWS_DEFAULT_PROFILE by default. this was a purely CLI environment variable that still exists for backwards compatibility.  The SDK's session and env_config. go provides the logic to determine which environment variable to use.. The SDK is looking to move away from testify and use pure go testing. It would be great to the pure go testing package for these tests.. The sharedConfig utility is the utility that should be solely responsible for loading the shared config file. . The error messages here are too specific, probably should be more generic since these are passed in values.. nit path would be better as tokenFilePath. nit path would be better as tokenFilePath. This is probably pretty unique, but not sure if it will be unique enough. Since field is required, might need more specific value, and optionally allow user provided.. Is it possible for the Expiration to be nil?  If so these credentials will expire immediately since the TimeValue will be zero time.  If no Expiration is a valid use case might need to only SetExpiration if Expiration isn't nil.. Probably want to set the SessionToken value as well incase the AssumeRoleWithWebIdentity call returns it.. Also Might be good to make the Error codes be InvalidParameter or something similar to differentiate between that and the API error failed.. Should this also validate that the RoleArn env var is provided, or just let the creds fail?. don't think this file actually changed?. Could you document why this check is for.. I think this new task should test just the public SDK and its runtime. Can drop the -tags ${UNIT_TEST_TAGS} and switch to ${SDK_ONLY_PKGS} for the packages to test.\nIn addition maybe look at renaming this task to reflect the tests is for the public SDK. . I think we should update this matrix to include all 1.5 - 1.9 only test the public SDK's runtime. All code generation should only be tested with the current supported version of go.. thanks will fix.. good point in that case we can probably drop it. . Yeah can make it required. Not having the doc model doesn't prevent code generation, but does mean modeled docs which isn't good.. nit spacing. probably didn't want the double slashes. probably don't need this comment: \"Currently the weight of addresses is set to 1.0.\". nit these will get changed back during next release.. Nit what do you think bout naming this file something like cust_paginator_test.go since its hand written and the other tests under codegentest are codegen.. Currently there is no other way of specifying the alternative filenames. Could consider that as a future feature.. @YakDriver  could you comment on the purpose of the AlreadyPreparedCommandKey env var?  Would this be a value the user would provide?  How would they use it?  This seems like a way to bypass the user's credential_process in the shared config file.. @YakDriver  could you comment on why the OS's shell needs to be added to the command arguments, as apposed to just executing the command directly with exec.Command? If there are specific shell environment needed I think the user would be responsible for providing it.. Why do the system folders need to be added to the path? Wouldn't the command execute in the same environment as the parent?. Temporary check is good, but I think this wasn't added until Go 1.6. Since the SDK still supports back to Go 1.5 we'll need a way of detecting to  cancel for retry... Thanks, I'll add a better error message here.. system32 ist required to be on the C:\\ drive, nor is the win dir required to be \"Windows\". :)  \nShould use SYSTEM32 env var instead for windows.. Nit changing this to the style below. Go doc doesn't provide any highlighting for markdown bold syntax.\n```\nWARNING: The following describes a method of sourcing....\n````. Its good to export the code, but I'd prefer not to export the error messages. Mainly, not to support comparing error message bodies.. I don't think we need to export this value.\nIdeally all the windows/other behavior are built in via Go build os functionality.. What do you think about merging DefaultMaxBufSize and DefaultInitialBufSize together into a DefaultBuffSize=512? The difference between the two values are fairly minor. . Having the command publicly accessible i think is risky. If a user constructs a command with additional configuration set that the process credential provider isn't aware of they could receive unexpected behavior the second+ time the credentials are retrieved.\nCould you change this member to not be exported.. Thanks for the update @azr, it looks like the unit test on Go 1.5 are failing, because the SDK is unable to determine the request was canceled, and is retrying.. Since the feature is enabled by default, a negative flag is needed to disable it. Like you mention the SDK uses the negative pattern to disable functionality that is enabled by default.. This test never actually was used, and did not compile. The SDK's make integration cmd never executed this unit since it was in the service folder. The test looks to exist in order to test the flatten modeled trait, but this is tested via the SDK's protocol tests.. I was wrong with the original understanding of the order these operations needed to occur in. The previous implementation also sent the apiCallAttempt metric prior to the RetryAfter's handler was executed, because the csm reporter's apiCallAttempt handler was added to RetryAfter handler list before the SDK's retry delay.\nIn the case of a succesful request the RetryAfter handler would of never executed.. Looks like this was just a visual bug with Github's diff view.. Thanks, using the cached creds value's ProviderName would be preferred to reflection here.. I think accessing the expirer.ExpiersAt needs to be protected with the mutex similar to Credentials .IsExpired on line 163. Otherwise, there could be a race updating the expires time and reading it.. In addition I think the Credentials's forceRefresh flag should be considered here. If the credentials have been flagged as as needing to be refreshed then I think they should also be considered as already expired.\nThis could be done by first checking isExpired() after grabbing the mutex and returning time.Now() if the credentials are flagged as expired. Otherwise return the actual expiry time.. I don't think this local definition of the interface is needed since the unit test is within the request package. The temporary type should be accessible.. Would prefer the r function be renamed to newRequest to prevent confusion/shadowing with the r variables below.. Could you update these tests names to camel case like: TestShouldRetryCancel  having a _<condition> suffix is fine for specific sub cases. e.g. TestShouldRetryCancel_timeout. Is it possible for a nil value to be provided to shouldRetryCancel? \nThis test should also assert the default behavior of nil error. I'm guessing a nil call to shouldRetryCancel should be false return value. But we should take a look at any code path that could provide nil to shouldRetryCancel too to see if that can be prevented.. Is it possible for shouldRetryCanceled to be called with a nil error? If so, did the request actually fail? I'm not sure returning true instructing the SDK a request should be retried if there was no error is correct.. Thanks, that makes sense. Yeah adding r.Error is needed in the Send handler if the StatusCode is not a 200.  This is done by the SDK's built in Send handler.  . Thanks for looking into this. It looks like the SDK's test status is a little fragile. You're correct that the mocked send handler should not be setting the RequestFailure. This error will be set via the UnmarshalMeta handler after the request is sent.\nI think the reason the panic in shouldRetryCancel is occurring is because the return shouldRetryCancel(err.OrigErr()) on line request.go:574 of the PR. It is possible for err.OrigErr() to return nil, if there was no underlying error. In your pr the nil check case of shouldRetryCancel would effectively be catching this. \nWe can keep the nil case, but it would be helpful to add docs to this case stating, that a nested error could be nil within a parent error.\n. One additional change, the appengine, plan9 version of this file should be also be updated with these additional vars as well. Adding the following to connection_reset_error_other_test.go should be fine.\ngo\nvar (\n     stubConnectionResetErrorAccept = errors.New(\"accept: connection reset\")\n     stubConnectionResetErrorRead = errors.New(\"read: connection reset\")\n)\n. \nLooks like this assert was broken, and failing on the wrong thing. think this code block would be better suited as the following. I think this change will also fix the broken unit tests in CR for this PR.\ngo\nif e, a := \"part size must be at least\", aerr.Message(); !strings.Contains(a, e) {\n    t.Errorf(\"expect %v to be in %v\", e, a)\n}\nThis style also brings this assert inline with the SDK's preferred assert test pattern.. Adding example build tag to the top of this fill will prevent it being built under \"normal\" use cases. We use this to separate the examples from the rest of the SDK when building.\n```go\n// +build example\npackage main\n``. could you update this usage example to include the-tag exampleGo build tag as well.. Nit, would you mind running goimport on this file. It will help sort the imports into two groups standard library, and external. The imports are also sorted alphabetically.. I think using the [HeadObject](https://docs.aws.amazon.com/sdk-for-go/api/service/s3/#S3.HeadObject) API operation would be better instead of list object. This will also ensure that no other objects are returned, and additional error checking isn't needed..\"kMGTPE\"[exp]is an interesting way to capture the size suffix, I would not of thought to do a literal string index lookup.. Could you add documentation to theprogressWritertype about how it works, and how the progress is printed?. Curious why define the credential chain inline, as apposed to using the SDK's default chain provided within the Session.. Prefer to use [session.NewSession](https://docs.aws.amazon.com/sdk-for-go/api/aws/session/#NewSession), theNewfunction is deprecated.. Probably should check thattemp.Close()doesn't return an error. Files written to disk, may encounter an error when being closed.. Nit, would prefer named struct members, instead of anonymous, or a constructor function instead.. Could you add a comment to describe the 304 http status code case?. I think theos.Remove, should only be called if the file fails to download. Willtemp.Name()return the correct (e.g. \"getObjectWithProgress-tmp-*\") name or, thefilenamevalue?  I think either caching thetemp.Name()returned value before the defer is defined will prevent any confusion.. I think the change adding the// +build example` build tag might of gotten lost in the update. . Would you mind updating these to:\n```go\nimport (\n    \"fmt\"\n    \"io\"\n    \"io/ioutil\"\n    \"log\"\n    \"os\"\n    \"regexp\"\n    \"strings\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\"github.com/aws/aws-sdk-go/service/s3/s3manager\"\n\"gopkg.in/cheggaaa/pb.v1\"\n\n)\n``. The SDK doesn't currently use any of the CLI'ss3://URI style to refer to objects. I think it would be helpful to keep this consistent in the example by passing inbucketandkeyas separate parameters into the example function.. Should make sure to handle theerrreturned fromNewSession. How about update theCloseto be called before the temp file is renamed?. I think we'd prefer not adding this dependency to the example. Having the dependency in the example will cause usersgo getthe SDK to also retrieve this dependency, regardless if they have theexample` build tag specified.\nI think this example should implement a custom WriterAt, like being done on line 30, but instead of using a progress bar just print out the percentage bytes downloaded. Similar to the PutObject example, except without the divide. (there is a bug in that example, #2468)\nThe example can have a comment about how a progress bar could also be used, but I don't think we want to add the direct dependency on the progress bar's package to the SDK.. ",
    "mitchellh": "We went through and manually updated all the fields just to verify this is indeed the problem and it worked :D So yep, should be an easy fix!\n. \n. Either way is fine with me. Just a question: if you're moving away from the wrappers, what are you moving to? \n. @lsegal Oh you mean regular pointers? (Sorry didn't look in branch). Yeah, in that case you'd definitely add more work on your side of getting those pointers to not be hex values during debug printouts... so that is an argument for them. I honestly don't mind it either way, but pragmatically speaking it is harder to debug when they're raw pointers.\n. Yeah, for us that would work just as well. \nFor now I've worked around it with some reflection magic. \ngo\n    // If the value is a pointer to a non-struct, get its value and\n    // use that. This allows Set to take a pointer to primitives to\n    // simplify the interface.\n    reflectVal := reflect.ValueOf(value)\n    if reflectVal.Kind() == reflect.Ptr {\n        if reflectVal.IsNil() {\n            // If the pointer is nil, then the value is just nil\n            value = nil\n        } else {\n            // Otherwise, we dereference the pointer as long as its not\n            // a pointer to a struct, since struct pointers are allowed.\n            reflectVal = reflect.Indirect(reflectVal)\n            if reflectVal.Kind() != reflect.Struct {\n                value = reflectVal.Interface()\n            }\n        }\n    }\n. @bmatsuo That's what we've done is implement it ourselves, but in two different projects we've needed it. If it is a common need, I believe it should be in the lib itself. It doesn't bother me anymore, we've solved it.\n. I don't think this issue is asking anything, however I can give my feedback on pointers.\nI completely understand the need for pointers. With the recent helper methods to print the values of structs with pointers within this library, we're actually quite happy with this in Terraform and Packer. \nI'm actually not a big fan of \"required fields are values and optional fields pointers\" since it ties the structure of an API call pretty closely to the validation semantics, which seems odd to me. i.e. What if a field becomes optional later? That wouldn't break BC at an API level, but it would at an SDK level. Or, what if a field is conditionally optional/required? Pretty weird to put that knowledge into the memory layout of a thing. \nOverall, I judge an API (using \"API\" in this context as SDK API, not HTTP API) by my ability to be productive with it. A major blocker was the issue I reported where we couldn't log.Printf(\"%#v\") values because of pointers. This seriously hampered productivity in debugging. However, that was resolved. And now we're shipping software, and we're shipping it fast. Pointers aren't in the way of that. They're weird sure, but practically speaking, we don't care. Everything is consistent: everything is a pointer. We expect it, we're used to it, we can debug it. We're happy.\nThe only practical problem we've had with the pointers is that it certainly has increased the surface area of crash-ability into our software since you have to nil check every output value. I recommended it in some other issue, but a ValueOrZero type method would be helpful: return the value it is, or return the zero value if it doesn't have a value. That would get rid of a lot of if conditionals for us. \nOverall, I'm more supportive of maintaining as much stability as possible. The very regular/consistent approach of the SDK currently is fantastic, and we're very happy with the speed of new API calls becoming available in the lib (it is regenerated often!). We'd be very upset if there was a sudden major backwards incompat change. \nKeep up the great work. \n. It is being specified directly with &SharedCredentialsProvider{} since we want strict control over the order that the chain provider looks at things. However, we aren't passing any config into it directly.\nAlternately, if we could expose a way to get the default filename from it so that we could do that validation, we'd be happy to do so if you feel the behavior change on your side is incorrect.\nI think the thing that made me feel like this was wrong was that we aren't specifying the filename so we're relying on aws-sdk-go to choose its default. I was very confused about the error message until I realized I had that path as a directory XD.\n. I don't think it'd be a \"breaking\" change since at best right now it is returning an error anyways and at worst its simply not working as people expect (its probably not common to a have a folder named that).\nHowever, I'm completely fine with just exposing the path!\n. ",
    "jvehent": "I'm afraid that did not fix the issue. \nError: The request signature we calculated does not match the signature you provided. Check your key and signing method.\nAre any other field required?\n``` go\n    fi, err := os.Stat(os.Args[1])\n    if err != nil {\n        fmt.Printf(\"Error: no input file found in '%s'\\n\", os.Args[1])\n        os.Exit(1)\n    }\n    fd, err = os.Open(os.Args[1])\n    if err != nil {\n        panic(err)\n    }\n    defer fd.Close()\n// create a bucket upload request and send\nobjectreq := s3.PutObjectRequest{\n    Bucket:        aws.String(bucket),\n    Body:          fd,\n    ContentLength: aws.Integer(int(fi.Size())),\n    ContentType:   aws.String(contenttype),\n    Key:           aws.String(fd.Name()),\n}\n\n```\n. Are you able to reproduce it using my code?\nhttps://gist.github.com/jvehent/94cd0720985b04e5fd0f is exactly the code I used, with my credentials in ~/.awsgo in the format:\n[credentials]\n    accesskey = \"A...\"\n    secretkey = \"m...\"\n. And now it start working. It may have been an issue with fd.Name() as the key instead of fi.Name(), which only contains the basename of the file.\nThanks for the help, and sorry for the false positive...\n. I like it so much, I wrote a blog post about it :)\nhttps://jve.linuxwall.info/blog/index.php?post/2014/12/15/Stripe-s-AWS-Go-and-uploading-to-S3\n. ",
    "nelhage": "Hm, one awkward thing about the Credentials API is that there's no safe way to handle expiration currently \u2013 without making assumptions about the call sequence of the different methods, you can't guarantee a call sequence where the caller calls AccessKeyID, the provider refreshes, and then the caller calls SecretAccessKey, and you have a mismatched set of credentials.\nSo maybe Credentials should instead have a single method that returns a struct.\n. ",
    "ciela": "Thank you very much for your quick response.\n. ",
    "fumin": "@stripecodahale great, this is indeed the right fix and it works for us, thanks!\n. ",
    "streadway": "We are interested in generating both signed S3 and CloudFront URLs with canned and custom policies.\nI'm not certain aws.Context is the best place for S3, CloudFront and other service specific signing concerns.  I read it now that the aws package has the AWS API signing concern, where individual services would have service specific signing concerns.  Could the packages representing services in gen contain hand written code for service specific signing?  How do other AWS libraries organize signing concerns?\n. ",
    "nathany": "\nHow do other AWS libraries organize signing concerns?\n\n@streadway gopkg.in/amz.v2 just has a SignedURL method for a Bucket. There is some request signing stuff in their aws package but I think that's something different.\nI would like to do pre-signed posts from Go, and so far undecided on which library to use.\n. Thanks @lsegal. So I should check out the develop branch? I noticed that the develop branch is both ahead and behind master.\n. Okay. Thanks.\n. related: https://github.com/awslabs/aws-sdk-go/issues/81#issuecomment-72667434\n. Thanks Loren.\nThe 100 MB figure comes from this statement in the AWS documentation: \n\nWe encourage Amazon S3 customers to use Multipart Upload for objects greater than 100 MB.\n\nIt's the only recommendation I had found before.\nI did some experiments with Multipart uploads last month, but using https://github.com/kr/s3 rather than this library. In my tests, I actually didn't see any issues from AWS when uploading a single-part file < 5 MB.\nHowever, I was buffering 5 MB parts, not for V4 signing, but because I didn't have the Content-Length at the time. 5 MB buffers is much more than I would like. (5 MB * 200 concurrent uploads, where uploads can take a few minutes = 1 GB)\nI don't understand how this upload manager will make non-seekable streams possible? Unless the intention is to buffer 5 MB parts to do the V4 signing calculation? A ReadSeeker could avoid that overhead by reading through the data twice instead, right?\nTo receive the benefits of parallel uploads and recovery, I would need to write the request.Body to disk and then read it back again to get a ReadSeeker. Writing a temp file to disk is something I've been trying to avoid so far, but perhaps I need to re-evaluate that decision (at least for larger files).\n. I agree that chunked uploads are complex, but a chunk size of 8 KB or even 64 KB is much more appealing than a 5 MB part size. \nIf chunked uploads are added to the upload manager later, how would that be done? Would there be a chunked variant of PutObject? Or of the aws request Send?\n. Good point with regards to SSDs. You've given me a fair bit to chew on. Thanks.\n. That seems fair. Thanks.\n. Glad to hear that's landed, but sorry, I don't have time at the moment.\nI ended up writing my own little S3 library that uses v2 signing and is specific for the purpose I need (which is perhaps a bit unusual).\n. ",
    "zshenker": "@lsegal Using your snippet above, I get out an S3 URL that starts with \"https://s3..amazonaws.com/\" which is not going to work. Is this because I am not passing in a config object that specifies the region? \n. @lsegal By passing in a config object and setting the region, or setting the AWS_REGION environment variable I was able to get a valid signed S3 URL out.\nsvc := s3.New(&s3.S3Config{&aws.Config{\n        Credentials: aws.DefaultCreds(),\n        Endpoint:    \"\",\n        Region:      \"us-west-2,\n        DisableSSL:  false,\n        ManualSend:  false,\n        HTTPClient:  http.DefaultClient,\n        LogLevel:    0,\n    }})\nI have run into another issue with the Expiry time, it appears that regardless of the value I pass into the Presign method, the URL always has X-Amz-Expires=300.\n. The signed URL that comes out here with the Body set does appear to then work when performing a PUT requests, I get a SignatureDoesNotMatch error.\n. Setting the following produces a signed URL that works for PUT requests,\nreq.Operation.OperationBindings.InPayload = \"\"\n. Any roadmap or plans for this anytime soon? @lsegal \n. We are going to need this soon, so is something that I might look at taking on and submitting a PR. Have you had any thoughts as to the right way to approach this work, and integrate it with the SDK?\n. Thanks @lsegal. Are there any recommendations as to when one should be using Path style addressing vs virtual hosted bucket style addressing? And was there a particular reason the default has now changed to the virtual hosted bucket style?\n. @jasdel Just opened PR #485 taking a shot at adding interfaces for s3manager.Downloader & s3manager.Uploader.\n. ",
    "evanphx": "I don't even see a develop branch anymore, did this get merged? I don't see the code anywhere... /cc @lsegal \n. Nevermind! Found it! :kissing_heart: \n. ",
    "sclasen": "Thanks!\n. looks like the test response needs wrapping with a<ErrorResponse> for the xml failure.\nwe should probably look at if only the text/xml responses are like that.\nthe json failure is a similar thing, not sure if they are actually wrapped in an {\"ErrorResponse\": <theError> }\n. the client impls are already named for the service name, so thats 'taken'. Also considered <Service>Operations or <Service>Ops.  \nrenaming the client impls seemed likely to break any current users.\n. Closing since the aws.Handlers in the develop branch will let you accomplish similar functionality\n. Nice @lsegal any idea on timelines for getting the handlers release?  \nCan probably kill https://github.com/awslabs/aws-sdk-go/pull/79 since handlers address the same thing\n. Hmm was assuming there would be a handler type that would allow you to intercept the request/input object and return properly typed response, but looking closer it doesnt appear that way so far...maybe #79 is still of use?\n. Aha so Params is an interface{} and the operation gives you the info needed to assert it to the right type?  \nIn the example above, had there been a non nil ListTablesInput we could have asserted  Params to be a ListTableInputt? And then wrote tests against it?  If so :+1:\n. One less desireable property of the handlers is  having to know how the\noperation name maps to a type. If you have interfaces that are mocked you\nare recieving a properly typed object and dont have to do any type\nassertion based on the operation.\nBut having one way to do things may trump the extra boilerplate?  Is there\nany sugar that could be generated to make mapping operation to type easier?\nOn Tue, Feb 10, 2015 at 10:35 AM, Loren Segal notifications@github.com\nwrote:\n\n@GeneticGenesis https://github.com/GeneticGenesis the handlers\narchitecture should work as a superset of #79\nhttps://github.com/awslabs/aws-sdk-go/pull/79. Can you provide an\nexample for what the interface enables that the handlers do not?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/awslabs/aws-sdk-go/issues/88#issuecomment-73755673.\n. FWIW I have been using hand build interfaces that contain the subset of operations I need in my projects as well, and not just for testing.\n\nAs you are doing in the example, code that needs to talk to dynamo gets a DynamoDBer which is satisfied by either dynamodb.Dynamodb or a mock.  The test versions of DynamoDBer that I am using are probably more correctly called stubs, but at the end of the day let you write tests that dont actually hit aws.\nI think I'll have to actually use the Handlers impl to have a strong opinion one way or the other, there is still definitely a tension in my mind between the handlers being 'the one way' to do things with aws-sdk-go and more easily accomodating using gomock/testify/etc...\n. +100 Please!\n. Relevant template snippet\nhttps://github.com/awslabs/aws-sdk-go/blob/1355efa4e35cd429590baeaef5e84d2dfbc44efc/model/templates.go#L119-L123\n. Fixes #155\n. @lsegal using the ExportableName method yields some enums that are not go style guide approved, is this ok? I think the exportable func is more stylistically correct?\nFor instance\n```\n-       ConfigurationItemStatusOK         = \"Ok\"\n+       ConfigurationItemStatusOk         = \"Ok\"\n\nResourceTypeAWSEC2VPNconnection    = \"AWS::EC2::VPNConnection\"\nResourceTypeAWSEC2VPNgateway       = \"AWS::EC2::VPNGateway\"\n        ResourceTypeAWSEC2Volume           = \"AWS::EC2::Volume\"\nResourceTypeAWSEC2Vpnconnection    = \"AWS::EC2::VPNConnection\"\nResourceTypeAWSEC2Vpngateway       = \"AWS::EC2::VPNGateway\"\n\n``\n. @lsegal got rid of the map. Still seems to be an issue with ExportableName though. any advice?\n. https://github.com/sclasen/aws-go/pull/1  shows the diff of using the originalexportablefunction instead ofExportableName`, which results in much better code imo.\n. @lsegal do you mean not set up properly for enums alone somehow or in general not set up correctly?\nis there a setup func that one can call on shape.API?\n. Aha missed that comment. wired it in passes instead, and make complains about a ton of missing inflections, I added them and things are looking better.\n. @lsegal looking better, though now I am noticing that the generator is picking up older schemas, for example cloudfront has 2 schemas. Is there an incantation to make the generator pick the later schema?\n. @lsegal ok, I deleted the older cloudfront schemas and ran the generator and it is of course correct. Should older shcemas be deleted?\n. Alright, is there a mechanism for using only the latest generator other than explicitly enumerating the go:generate calls in service/generate.go ?\n. moved the old ones to the side for generation for now\n. @lsegal deduped inflections, were a ton!  let me know if anything else is needed.\n. squashed commits\n. @lsegal anything needed here? we have a pile of code that uses the pre develop-merge code, and we use quite a ton of the consts, so we cant switch over till this goes in.\n. Thanks @lsegal  \nfwiw I have found the type prefixes on the constants pretty helpful, especially from an autocomplete perspective, you type  swf.EventType<autocomplete keystroke> and get a  dropdown of all the correct consts. \nMy fear is that if we use type aliasing instead of prefixes, the tooling will have to be much smarter to give you a reasonable dropdown. Long names are not a problem with tooling.\nDo you think adding a enum generation blacklist mechanism would help? Unhelpful enums such as route53domains country code could simply be skipped. Let me know if you'd like me to add that feature.\n. @lsegal another question here, but more general. \nIs it sane to consider building other (external to this project) generators, based on internal/model/api/API.go\nIf it were fairly well documented/stable it would be pretty easy to show folks how to generate arbitrary code that you dont necessarily want in this project.  For instance\n```\npackage main\nimport(\n    \"github.com/awslabs/aws-sdk-go/internal/model/api\"\n    \"os\"\n    \"fmt\"\n)\nfunc main(){\n    service := \"swf\"\n    date := \"2012-01-25\"\n    gopath := os.Getenv(\"GOPATH\")\n    api := new(api.API)\n    api.Attach(fmt.Sprintf(\"%s/src/github.com/awslabs/aws-sdk-go/apis/%s/%s.normal.json\", gopath, service, date))\n    for , o := range api.ShapeList(){\n        if len(o.Enum) > 0 {\n            for , e := range o.Enum {\n                fmt.Printf(\"%s->%s\\n\", o.ShapeName, e)\n            }\n        }\n    }\n}\n```\n. @jasdel so basically the two initial places I have looked at  generating code in my projects, rather than using code in this project are exemplified in these PRs\nhttps://github.com/awslabs/aws-sdk-go/pull/79  <- client interfaces\nhttps://github.com/awslabs/aws-sdk-go/pull/158 <- enum generation\nthe enum generation one is most exemplary atm. This PR has been open for 26 days, and I'm unsure if/when it will get merged.  If the model packages are not internal, then I can easily throw a generator in my projects and generate things I want, rather than wait till/hope something gets merged into this project.\n@lsegal advised in the PR\n\"In the meantime, while we look at the inflections and think about how to improve visibility in documentation, you should be able to generate these constant names in a separate package in your own codebase (i.e. copy paste the string consts you use) and import that for now. That should unblock you until this gets merged in while still allowing you to switch over the rest of your code.\"\nI would love to have the ability to do that without hacking on a branch of this repo and cutting and pasting code around.  I would guess there will be more situations like this where people could much more easily unblock themselves if the model was not internal-only.\n. Cool, your last commit actually fixes #241 too, so im fine either way.\nMight want to have a spec that does some crazy json inside json inside json inside json and have it actually hit some services. Never know what people will try and shove in dynamodb.\n. :+1:  thanks @lsegal \n. :+1: \n. :+1: \n. @lsegal thanks, If I push that UnmarshalMeta Handler onto non jsonrpc services, should I expect it to work? or are the headers named differently on different services?\n. @lsegal  the error responses coming back from dynamo look like \n```\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 400 Bad Request\nContent-Length: 240\nContent-Type: application/x-amz-json-1.0\nDate: Tue, 14 Jul 2015 04:30:44 GMT\nX-Amz-Crc32: 3356068745\nX-Amzn-Requestid: XXX\n{\"__type\":\"com.amazonaws.dynamodb.v20120810#ProvisionedThroughputExceededException\",\"message\":\"The level of configured provisioned throughput for the table was exceeded. Consider increasing your provisioning level with the UpdateTable API\"}\n```\nthe test for retries doesnt look like it parses that type?  https://github.com/aws/aws-sdk-go/blob/master/aws/request_test.go#L97\nreqs := []http.Response{\n        {StatusCode: 400, Body: body(`{\"__type\":\"Throttling\",\"message\":\"Rate exceeded.\"}`)},\n        {StatusCode: 429, Body: body(`{\"__type\":\"ProvisionedThroughputExceededException\",\"message\":\"Rate exceeded.\"}`)},\n        {StatusCode: 200, Body: body(`{\"data\":\"valid\"}`)},\n    }\nIs there more logging to turn on?  I had logLevel = 2 and logHttpBody = true and AddDebugHandlers() called on the client.\nam i missing somewhere where the __type is parsed such that only the stuff after the anchor is used?\n. If I change the TestRequestRecoverRetry4xxRetryable like so, it fails.\nreqs := []http.Response{\n        http.Response{StatusCode: 400, Body: body(`{\"__type\":\"Throttling\",\"message\":\"Rate exceeded.\"}`)},\n        http.Response{StatusCode: 429, Body: body(`{\"__type\":\"com.amazonaws.dynamodb.v20120810#ProvisionedThroughputExceededException\",\"message\":\"Rate exceeded.\"}`)},\n        http.Response{StatusCode: 200, Body: body(`{\"data\":\"valid\"}`)},\n    }\n. hrm, probably cause its not using the jsonrpc UnmarshalError, which does do the split on the error code\nhttps://github.com/aws/aws-sdk-go/blob/5af114c4d0fd0d5d24a5764bca1153e075722b36/internal/protocol/jsonrpc/jsonrpc.go#L87\n. yep, test passes if I use the jsonrpc unmarshalError. grr.\nalso wrote a test in dynamodb/customizations_test.go and it does appear to retry.\nMaybe I am somehow breaking retries? I do pushFront and pushBack send handlers that do metrics.\n. Aha. was using an &aws.Config{} which was picking up MaxRetries as 0, instead of -1.\n. thanks :+1: \n. :+1: \n. \\O/\n. Oh interesting on the 500s, we've been seeing tons of these post-retries so I assumed they must not be retrying.  \nWhy does it only retry 3 times? just a default? \n. @jasdel @lsegal I think some of the confusion is coming from here\nhttps://github.com/aws/aws-sdk-go/blob/master/internal/protocol/jsonrpc/jsonrpc.go#L91\nthe request id is being set to \"\", but shouldnt it be present, and set to req.RequestID since the jsonrpc.UnmarshalMeta grabs it? \n. @jasdel jsonrpc services do already popluate RequestID. #311 is for all the other types of services.\nIf line 91 was changed to req.RequestID, today, it would work properly afaict.  If line 91 is not changed, then request id wont be populated in the errors, though it will be present in the req.\nOr is line 91 to be fixed as part of #311?\n. alrighty then!\n. :+1: \n. @lsegal here is a bad one. \n. Assume these 2 should be VPNConnection and VPNGateway @lsegal ?\n. MFAEnabled ?\n. MFADevices ?\n. VMWare ?\n. ",
    "ncw": "Here is my first draft - let me know if you want any changes\n. Updated to include a message for empty bodied errors\n. The same problem applies to various Content-Length fields in s3 too.\n. goamz does this, where req.signpath is a non path escaped version of the URL.\ngo\nreq.signpath = (&url.URL{Path: req.signpath}).String()\nreq.headers[\"Host\"] = []string{u.Host}\nreq.headers[\"Date\"] = []string{time.Now().In(time.UTC).Format(time.RFC1123)}\nsign(s3.Auth, req.method, req.signpath, req.params, req.headers)\n. Above you'll find my attempt to fix this issue.  I re-wrote the logic from  https://github.com/mitchellh/goamz/blob/master/s3/s3.go#L660 as it wasn't under a compatible licence :-(\nThis passes my tests with rclone too.\n. Hmm, having to have a seekable stream will break rclone - it will stop it streaming data  to s3 which is unfortunate.\nIs it possible to send the signed hash as an http trailer?  If so then you could use a TeeReader  to write it to the hash as you go along, then send the hash as a trailer at the end of the upload. \nYou shouldn't need to do reflection to sort out what sort of io.Reader you have, just do type assertions to the various interfaces and give an error if you can't find one you need.  Ideally it would fall back to not sending the hash if it was just a plain io.Reader.\n. > The problem is listed in your panic() error, but in my tests, wrapping even a seekable stream (os.Open()) inside of a ioutil.NopCloser will make it so the checked assertion fails with the above check. You have to actually indirect the interface to get at the underlying type, but I may be doing something wrong. I'm still looking into how we can make this work, ideally I want to be using ReadClosers too.\nI don't think indirecting it is the right approach.  You can get layers upon layers of io.Readers - that is the nice thing about that interface it is so easy to compose.\nIf an io.ReadSeeker is required then you should declare that as the type.  If not then you can use an io.Reader and upgrade it with a type assertion or type switch to an io.ReadSeeker to get one of those where it would be helpful, falling back to the plain io.Reader.\n\nUnfortunately checksumming the body is part of the version 4 signature protocol itself-- in other words, you must sign the body when using sigv4, so it's not possible to fall back.\n\nThat seems a bit of an oversight to me - ruling out streaming like the previous signing protocols.\nIn the specific case we are talking about here, object upload, would it be possible to break the stream up into chunks which can be buffered in memory, signed then uploaded?\n\nThat's why a seekable reader is required here. That is, if the checksum is not already computed into the X-Amz-Content-Sha-256 header by another handler-- meaning you could pass a non-seekable if you pre-computed this header yourself, but it still must be computed somehow, and your code will run into the same problem on any non-rewindable stream.\n\nIf I have to pass in a seekable reader it will break the internal design of rclone - all the other cloud storage providers support streaming uploads and downloads.\nMy choices would be\n- use goamz instead (I was trying to get away from it, but it is working now)\n- implement some sort of chunking scheme\n- buffer files to disk (undesireable)\nAny other ideas?\nThanks\nNick\n. Thank you very much for your detailed reply.  Yes I'm sure you are right about rclone already not working in some s3 regions.\nrclone will have a length and an MD5 sum of the content at the point it starts its upload - that is how rclone does its integrity checking which fits in very nicely with the old signing methods. It checks the returned ETag against what it expects.\nI thought s3 supported chunked transfer encoding like openstack/swift but I see from a bit of research that I'm wrong and s3 indeed doesn't support a true streaming upload.\nI'll investigate multpart uploads for rclone - that seems to be the best solution.\nI shall continue to watch the development of this branch with interest!\nThanks\nNick\n. Thanks!  I'll take a look.\n. @lsegal Bit late with feedback sorry!  I think making the Body type an io.ReadSeeker is the correct choice.  I'm working on a chunked upload for rclone to work around the streaming issues, so I think we are all good now!\nThanks for your help and patient explanation\nNick\n. I've just noticed this too.  ListBuckets takes no parameters, so just call it to reproduce!\nHere is a possible fix (might be completely wrong)\n``` diff\n--- a/aws/core.go\n+++ b/aws/core.go\n@@ -227,7 +227,8 @@ func (s *Service) AddDebugHandlers() {\n }\nfunc (r *Request) ParamsFilled() bool {\n-   return reflect.ValueOf(r.Params).Elem().IsValid()\n+   value := reflect.ValueOf(r.Params)\n+   return value.IsValid() && value.Elem().IsValid()\n }\nfunc (r *Request) DataFilled() bool {\n```\n. @ezbercih - thanks for trying that.  I ran rclone's test suite with that patch, and it was a long way from working so I'm not surprised it doesn't actually work properly!\n. Using @ezbercih reproducer, that doesn't find any of my buckets?\n. @rchakra1 did you ever come up with a v2 signer? I'm facing the same problem trying to use this library with Ceph which only supports v2 signing, not v4.\n. @rchakra1 Ha!  I'm trying to move away from goamz because it doesn't have all the functionality I need! Thanks\n. @jasdel I can confirm that the issue above is fixed by your fix.\nHowever when I updated I found that ListBuckets has stopped working, giving me a 404 error.\n2015/08/04 16:24:48 S3 bucket : Couldn't list buckets: NoSuchBucket: The specified bucket does not exist\n    status code: 404, request id: []\nIf I revert fd5f50d04c01c1019ee94936e13692396e54e238 then ListBuckets starts working again at the expense of this problem.\n. @jasdel I managed to narrow that down slightly - ListBuckets only doesn't work with the new code if you are using .WithEndpoint(\"https://s3.amazonaws.com/\") in your config. Removing the trailing slash from the endpoint to make it .WithEndpoint(\"https://s3.amazonaws.com\") works fine.\nQuite why that changed I don't know, nor whether the endpoint should end with a / or not!\n. @jasdel I've tested and yes it is all working now - thank you very much for fixing it\n-- Nick\n. @jasdel thanks for the explanation and fix.  I'll update my docs to indicate the above.\nPS http://rclone.org is now using the new SDK - the managed multipart upload was a real timesaver thanks!\n. I haven't observed this directly, but a user of my rclone program reported the problem and we debugged it together.  See ncw/rclone#415\nI think this error is very unlikely to occur normally, however my user could make it happen reliably with their ceph cluster.\nAccording to the thread I linked ErrUnexpectedEOF can happen when using https.\nI've tested the patch as has my user with the problem and it passes the unit tests, though travis seems to be broken at the moment (probably because of this https://groups.google.com/forum/#!topic/golang-announce/JMs4_CGbXWk)\n. I've updated and rebased the PR - let me know what you think!\nThanks\nNick\n. For my own usage within rclone I don't look at the Location returned so if it is empty rclone doesn't care.\nThe fact that this was returned as nil from a ceph cluster, not AWS probably indicates a non-conformance there, but in the testing my user did, the files were uploaded correctly even though the Location field was returned as nil.\nMy preference would be to let the Location be returned as an empty string.\n. > I think issues like appengine and plan9 highlight an issue the SDK has with using the syscall package in general. I think this highlights the SDK may need to consider removing syscall pkg in general. https://golang.org/s/go1.4-syscall\nsyscall is annoyingly inconsistent.\nI don't think the usage of syscall is getting out of hand yet though\n$ git grep syscall\nCHANGELOG.md:  * Remove syscall error checking on appengine platforms.\naws/request/connection_reset_error.go:  \"syscall\"\naws/request/connection_reset_error.go:                  return sysErr.Err == syscall.ECONNRESET\naws/request/connection_reset_error_test.go:     \"syscall\"\naws/request/connection_reset_error_test.go:var stubConnectionResetError = &net.OpError{Err: &os.SyscallError{Syscall: \"read\", Err: syscall.ECONNRESET}}\ngolang.org/x/sys/unix is quite a big dependency to and you would still have to do the conditional compilation as it only covers unix platforms.. @jasdel\n\nCurrently the AWS SDK for Go does not support streaming uploads with the PutObject S3 API operation.\n\nWould it be possible to bring back streaming uploads for PutObject?  The SDK used to support this (many iterations ago ;-) and the underlying API supports it just fine.\nSupplying the content MD5 is optional so if the user is willing to live without this protection then it would be great if there was a PutObject variant which could stream.\nI'd like to use this in rclone for uploading files with the minimum memory usage.  I know the MD5SUM in advance so I'd like to set that and stream the content.\nI'll try to see if I can get the presigned URL uploader to work.. > I'll try to see if I can get the presigned URL uploader to work.\nI did get it to work - thanks for the hints above :-). @jasdel \n\nS3's PutObject and similar methods are special, and support the magic value UNSIGNED-PAYLOAD for the request body's digest. We should investigate how viable it is to update the SDK to allow S3 PutObject to take an io.Reader instead of an io.ReadSeeker.\n\nThat would be nice.  In the mean time the code above plus your hints makes for a working uploader provided you know the size of the object in advance.  For rclone's use this is fine.  For objects for which the size isn't known I'll just use s3uploader as before.\nI note that Backblaze have solved the hash problem in a different way to allow streaming - they allow the last 20 bytes of the upload stream to be the sha1 hash.  One could also supply hashes in http trailers.\nThanks for your help :-). What happens is that after the change it does one less iteration of the loop exiting at the first io.ErrUnexpectedEOF instead of iterating again and exiting at the io.EOF.\nI moved it to make the unit tests pass!\n. My reading of the code is that it is correct now. \nnum starts on 1, because one part gets sent before the loop starts.\nSay MaxUploadParts is 2.\nFirst time around the loop num gets incremented to 2 before the check against MaxUploadParts.\nThe num > MaxUploadParts check succeeds and we send that part (the second).\nSecond time around the loop num gets incremented to 3 - the check fails and the loop is aborted before sending the 3rd part.\nThe test that fails without this change is TestUploadOrderMultiBufferedReaderExceedTotalParts.  Looking at that code I think that is correct.\nIt worked before because the loop did one extra iteration it shouldn't have done.\nI could be wrong though so a second opinion would be good!\n. I think before my change it sent one part too many\n- send first part outside loop\n- num = 1\n- if num > MaxUploadParts -> no\n- num++ (2)\n- send second part\n- if num > MaxUploadParts -> no\n- num++ (3)\n- send third part\n- if num > MaxUploadParts -> yes, break\n. ",
    "pges": "Please have a look at signer V4 in https://bitbucket.org/go-aws/aws-sdk/src/ecb0077e4f46c2eec46ea27cd478a96f0393c0aa/awscore/signer.go?at=master\nWorks well and test suite is already built to run some of the Amazon tests:\nhttps://bitbucket.org/go-aws/aws-sdk/src/ecb0077e4f46c2eec46ea27cd478a96f0393c0aa/awscore/signer_test.go?at=master\nSigner generates a signature like this and adds the date header as well:\n```\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIDEXAMPLE/20110909/us-east-1/iam/aws4_request, SignedHeaders=content-type;host;x-amz-date, Signature=ced6826de92d2bdeed8f846f0bf508e8559e98e4b0199114b84c54174deb456c\n```\n. See an example from https://bitbucket.org/go-aws/aws-sdk/src/7372572b25f43f45a456b292eecb361ed62ae3c4/dynamodb/datadatatypes.go?at=master\n``` Go\ntype Region string\n// Returns the domain for this region; ex: \"amazonaws.com\".\nfunc (r Region) Domain() string {\n    return \"amazonaws.com\"\n}\n// The unique system ID for this region; ex: \"us-east-1\".\nfunc (r Region) Name() string {\n    return string(r)\n}\n// Returns the endpoint for the service given.\n// Example: \"dynamodb.us-east-1.amazonaws.com\" for \"dynamodb\"\nfunc (r Region) ServiceEndpoint(service ServiceAbbreviation) string {\n    var endpoint string\nswitch service {\ncase Dynamodb, SQS:\n    endpoint = service.Name() + \".\" + r.Name() + \".\" + r.Domain()\ncase S3:\n    if r.Name() == US_EAST_1.Name() {\n        endpoint = \"s3-external-1.\" + r.Domain()\n    } else {\n        endpoint = \"s3-\" + r.Name() + \".\" + r.Domain()\n    }\n}\n\nreturn endpoint\n\n}\n// Returns whether the given service support the http protocol in this region.\nfunc (r Region) HasHttpEndpoint(service ServiceAbbreviation) bool {\n    return false\n}\n// Returns whether the given service support the https protocol in this region.\nfunc (r Region) HasHttpsEndpoint(service ServiceAbbreviation) bool {\n    return false\n}\n// Returns whether the given service is supported in this region.\nfunc (r Region) IsServiceSupported(service ServiceAbbreviation) bool {\n    return false\n}\nconst (\n    US_EAST_1      Region = \"us-east-1\"\n    US_WEST_1      Region = \"us-west-1\"\n    US_WEST_2      Region = \"us-west-2\"\n    EU_WEST_1      Region = \"eu-west-1\"\n    EU_CENTRAL_1   Region = \"eu-central-1\"\n    AP_SOUTHEAST_1 Region = \"ap-southeast-1\"\n    AP_SOUTHEAST_2 Region = \"ap-southeast-2\"\n    AP_NORTHEAST_1 Region = \"ap-northeast-1\"\n    SA_EAST_1      Region = \"sa-east-1\"\n    DEFAULT_REGION Region = US_EAST_1\n)\n// As seen in: http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/regions/ServiceAbbreviations.html\n// Abbreviations for looking up information about a specific service. Used in Region.ServiceEndpoint() and related methods.\ntype ServiceAbbreviation string\n// Abbreviation name\nfunc (s ServiceAbbreviation) Name() string {\n    return string(s)\n}\nconst (\n    Dynamodb ServiceAbbreviation = \"dynamodb\"\n    S3       ServiceAbbreviation = \"s3\"\n    SQS      ServiceAbbreviation = \"sqs\"\n)\n```\n. Hi kidoman, it can be improved for sure.\n. I think that using interfaces would avoid the need of using pointers in this case too.\nSee https://github.com/awslabs/aws-sdk-go/issues/125 for a solution for this same case.\nUsing pointers to basic data types is prone to errors in my opinion, and adds an extra layer of complexity.\n. @lsegal , my suggestion is to take a different approach. Instead of using null pointers for null values to set a default value, I suggest using constructors to set default values and initialize structures that otherwise you have to do manually.\nFor example, for CreateTableInput type for CreateTable function, I have crated a constructor and added some methods to help in initialization.\nAPI for creating the DynamoDB request struct would be as follow:\n``` Go\n    svc := dynamodb.New(nil) // Taken from documentation reference for illustration\nvar request *CreateTableInput\n\nrequest = NewCreateTableInput(\"test_table_name\").\n    WithAttributeDefinitions(\n    AttributeDefinition{\"name_1\", \"S\"},\n    AttributeDefinition{\"name_2\", \"N\"},\n    AttributeDefinition{\"name_3\", \"B\"},\n).WithKeySchema(\n    KeySchemaElement{\"uuid\", \"S\"},\n)\n\nresp, err := svc.CreateTable(request)  // Taken from documentation reference for illustration\n\n```\nThe API will create a struct which produces a result Json like this:\njson\n{\"AttributeDefinitions\":[{\"AttributeName\":\"name_1\",\"AttributeType\":\"S\"},{\"AttributeName\":\"name_2\",\"AttributeType\":\"N\"},{\"AttributeName\":\"name_3\",\"AttributeType\":\"B\"}],\"GlobalSecondaryIndexes\":[],\"KeySchema\":[{\"AttributeName\":\"uuid\",\"KeyType\":\"S\"}],\"ProvisionedThroughput\":{\"ReadCapacityUnits\":2,\"WriteCapacityUnits\":1},\"TableName\":\"test_table_name\"}\nConstructor enforces to set a table name wich is compulsory. See that the constructor has initialized every list to an empty list, and has assigned a default value to ReadCapacityUnits and WriteCapacityUnits, which can very useful to initialize boolean values too.\nI think that this above makes the need of pointers to basic types to crate null values irrelevant.\nQuite likely it makes testing far easier as well.\nAnd this is how the API above works:\n``` Go\n// http://godoc.org/github.com/awslabs/aws-sdk-go/service/dynamodb#CreateTableInput\npackage main\nimport (\n    \"encoding/json\"\n    \"fmt\"\n)\ntype CreateTableInput struct {\n    AttributeDefinitions   []AttributeDefinition  type:\"list\" required:\"true\"\n    GlobalSecondaryIndexes []GlobalSecondaryIndex type:\"list\"\n    KeySchema              []KeySchemaElement     type:\"list\" required:\"true\"\n    ProvisionedThroughput  ProvisionedThroughput  type:\"structure\" required:\"true\"\n    TableName              string                 type:\"string\" required:\"true\"\n}\n// Handy method to create lists of elements\n// Do the same for GlobalSecondaryIndexes, KeySchema, and others\nfunc (c CreateTableInput) WithAttributeDefinitions(ad ...AttributeDefinition) CreateTableInput {\n    c.AttributeDefinitions = ad\n    return c\n}\nfunc (c CreateTableInput) WithKeySchema(ks ...KeySchemaElement) CreateTableInput {\n    c.KeySchema = ks\n    return c\n}\n// Constructor builds the request with default values\nfunc NewCreateTableInput(tableName string) *CreateTableInput {\n    return &CreateTableInput{\n        AttributeDefinitions:   []AttributeDefinition{},\n        GlobalSecondaryIndexes: []GlobalSecondaryIndex{},\n        KeySchema:              []KeySchemaElement{},\n        ProvisionedThroughput: ProvisionedThroughput{\n            ReadCapacityUnits:  2, // Default value\n            WriteCapacityUnits: 1, // Default value\n        },\n        TableName: tableName,\n    }\n}\ntype AttributeDefinition struct {\n    AttributeName string\n    AttributeType string\n}\ntype GlobalSecondaryIndex struct {\n    //\n}\ntype KeySchemaElement struct {\n    AttributeName string\n    KeyType       string\n}\ntype ProvisionedThroughput struct {\n    ReadCapacityUnits  uint // If not int, better uint or uint64 than int64: negative value is no sense\n    WriteCapacityUnits uint\n}\nfunc main() {\nvar request *CreateTableInput\n\nrequest = NewCreateTableInput(\"test_table_name\").\n    WithAttributeDefinitions(\n    AttributeDefinition{\"name_1\", \"S\"},\n    AttributeDefinition{\"name_2\", \"N\"},\n    AttributeDefinition{\"name_3\", \"B\"},\n).WithKeySchema(\n    KeySchemaElement{\"uuid\", \"S\"},\n)\n\nb, err := json.Marshal(request)\nif err != nil {\n    fmt.Println(\"error:\", err)\n}\n\nfmt.Println(string(b))\n\n}\n```\n. @drombosky @ngauthier please give me your feedback on the below:\nWhen you want to make a difference when values are set and unset, this is better achieved with a dictionary. In this case because this is are url values, Go type url.Values may be the best candidate.\nFor ModifyVpcAttribute I have written the following example whose API would be as follows:\n``` Go\n    query1 := NewModifyVpcAttribute().\n        WithEnableDnsHostnames(false)\n    fmt.Println(\"With one parameter, others omitted: \", query1.UrlValues().Encode())\nquery2 := NewModifyVpcAttribute().\n    WithEnableDnsHostnames(false).\n    WithEnableDnsSupport(true).\n    WithVpcId(\"vpc-1a2b3c4d\")\nfmt.Println(\"With all the paremeters: \", query2.UrlValues().Encode())\n\n```\nThis would produce the following output:\nWith one parameter, others omitted:  EnableDnsHostnames.Value=false\nWith all the paremeters:  EnableDnsHostnames.Value=false&EnableDnsSupport.Value=true&VpcId=vpc-1a2b3c4d\nFull example code:\n``` Go\n// Example for omited values\npackage main\nimport (\n    \"fmt\"\n    \"net/url\"\n    \"strconv\"\n)\n// UrlValuer is an interface for creating URL values\ntype UrlValuer interface {\n    UrlValues() url.Values\n}\n// ModifyVpcAttribute\n// Do not create the struct directly. Use the constructor instead.\n// http://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_ModifyVpcAttribute.html\ntype ModifyVpcAttribute struct {\n    values url.Values\n}\nfunc NewModifyVpcAttribute() *ModifyVpcAttribute {\n    return &ModifyVpcAttribute{\n        values: url.Values{},\n    }\n}\nfunc (m *ModifyVpcAttribute) UrlValues() url.Values {\n    return m.values\n}\nfunc (m ModifyVpcAttribute) WithEnableDnsHostnames(v bool) ModifyVpcAttribute {\n    m.values.Set(\"EnableDnsHostnames.Value\", strconv.FormatBool(v))\n    return m\n}\nfunc (m ModifyVpcAttribute) WithEnableDnsSupport(v bool) ModifyVpcAttribute {\n    m.values.Set(\"EnableDnsSupport.Value\", strconv.FormatBool(v))\n    return m\n}\nfunc (m ModifyVpcAttribute) WithVpcId(v string) ModifyVpcAttribute {\n    m.values.Set(\"VpcId\", v)\n    return m\n}\nfunc main() {\n    query1 := NewModifyVpcAttribute().\n        WithEnableDnsHostnames(false)\n    fmt.Println(\"With one parameter, others omitted: \", query1.UrlValues().Encode())\nquery2 := NewModifyVpcAttribute().\n    WithEnableDnsHostnames(false).\n    WithEnableDnsSupport(true).\n    WithVpcId(\"vpc-1a2b3c4d\")\nfmt.Println(\"With all the paremeters: \", query2.UrlValues().Encode())\n\n}\n```\n. Thank you for sharing your feedback and raising your concerns; it gives me the opportunity to explain better what my proposition is.\nIn my experience with the AWS API, three types of data encapsulation are used: Json or XML, headers, and URL parameters. These are better served using the following Go types respectively: struct types, http.Header, and url.Values.\nMy proposition is not to avoid structs when it is natural to use them, but making initialization easier and safer. I think you may have missed the struct inizialization example above. Struct constructor creates and initialize a struct that you can manipulate later on like any other struct.\nAs logic is contained in the struct and its constructor, it is easy to test and extend safely if future API would require so.\nFor the other two use cases, url.Values example would be a better fit. For users the API is pretty similar, and in my opinion it is a better fit than using structs for this too.\n. Hi @lsegal \nLet me rethink this as it doesn't work as is. I would like to find a way where it is not needed to workaround the request with nilable values.\n. @lsegal see the example below on the implementation of AttributeValue via an interface to avoid the need of nulable values.\nKey is using interface AttributeValuer as AWS AttributeValue\n``` Go\n// awssdkgo\npackage main\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"strconv\"\n)\n// Demonstration on how to user AttributeValuer\n// http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_PutItem.html\ntype MockRequest struct {\n    TableName string                     json:\"TableName\"\n    Item      map[string]AttributeValuer json:\"Item\"\n}\n// AttributeValuer is the interface for DynamoDB basic types\ntype AttributeValuer interface {\n    DataType() string\n    TryItemValue(map[string]interface{}) bool\n}\n// A Boolean data type\ntype AttributeValueBOOL struct {\n    BOOL bool json:\"BOOL\"\n}\nfunc (a *AttributeValueBOOL) DataType() string {\n    return \"BOOL\"\n}\nfunc (a *AttributeValueBOOL) TryItemValue(m map[string]interface{}) bool {\n    i, exists := m[a.DataType()]\n    if exists {\n        a.BOOL = i.(bool)\n    }\n    return exists\n}\n// A Number data type.\ntype AttributeValueN struct {\n    N string json:\"N\"\n}\nfunc (a *AttributeValueN) DataType() string {\n    return \"N\"\n}\nfunc (a *AttributeValueN) TryItemValue(m map[string]interface{}) bool {\n    i, exists := m[a.DataType()]\n    if exists {\n        a.N = i.(string)\n    }\n    return exists\n}\nfunc (a *AttributeValueN) FromInt64(i int64) {\n    a.N = strconv.FormatInt(i, 10)\n}\nfunc (a AttributeValueN) WithInt64(i int64) AttributeValueN {\n    a.FromInt64(i)\n    return a\n}\nfunc (a *AttributeValueN) ToInt64() (i int64, err error) {\n    i, err = strconv.ParseInt(a.N, 10, 0)\n    return\n}\nfunc (a *AttributeValueN) FromUint64(n uint64) {\n    a.N = strconv.FormatUint(n, 10)\n}\nfunc (a AttributeValueN) WithUint64(n uint64) AttributeValueN {\n    a.FromUint64(n)\n    return a\n}\nfunc (a *AttributeValueN) ToUint64() (n uint64, err error) {\n    n, err = strconv.ParseUint(a.N, 10, 0)\n    return\n}\nfunc (a *AttributeValueN) FromFloat64(f float64) {\n    a.N = strconv.FormatFloat(f, 'f', -1, 64)\n}\nfunc (a AttributeValueN) WithFloat64(f float64) AttributeValueN {\n    a.FromFloat64(f)\n    return a\n}\nfunc (a *AttributeValueN) ToFloat64() (f float64, err error) {\n    f, err = strconv.ParseFloat(a.N, 64)\n    return\n}\n// A Number Set data type\ntype AttributeValueNS struct {\n    NS []*AttributeValueN json:\"NS\"\n}\nfunc (a *AttributeValueNS) DataType() string {\n    return \"NS\"\n}\nfunc (a AttributeValueNS) TryItemValue(m map[string]interface{}) bool {\n    i, exists := m[a.DataType()]\n    if exists {\n        a.NS = i.([]AttributeValueN)\n    }\n    return exists\n}\nfunc main() {\n    fmt.Println(\"Hello World!\")\nrequest := &MockRequest{\n    TableName: \"testTable\",\n    Item: map[string]AttributeValuer{\n        \"OneValue\":     new(AttributeValueN).WithInt64(123456),\n        \"OtherValue\":   new(AttributeValueN).WithFloat64(123456.78),\n        \"boolean_test\": &AttributeValueBOOL{true},\n        \"number_set\": &AttributeValueNS{\n            NS: []*AttributeValueN{\n                new(AttributeValueN).WithInt64(-123456),\n                new(AttributeValueN).WithFloat64(123456.78),\n                new(AttributeValueN).WithUint64(123456),\n            },\n        },\n    },\n}\n\nb, err := json.Marshal(request)\nif err != nil {\n    fmt.Println(\"error:\", err)\n}\n\nfmt.Println(string(b))\n\n}\n```\nThis will produce the following Json output. See that we have the flexibility needed to create different AttributeTypes and values in a clean and safe way.\njson\n{\"TableName\":\"testTable\",\"Item\":{\"OneValue\":{\"N\":\"123456\"},\"OtherValue\":{\"N\":\"123456.78\"},\"boolean_test\":{\"BOOL\":true},\"number_set\":{\"NS\":[{\"N\":\"-123456\"},{\"N\":\"123456.78\"},{\"N\":\"123456\"}]}}}\nLet me know if I can help you any further.\n. ",
    "ian-kent": "Narrowed it down a bit using AWS CLI...\nThe error from aws-go is strconv.ParseInt: parsing \"1.420642418989E9\": invalid syntax\nUsing AWS CLI I get this response (some stuff obfuscated), which indicates its related to the event timestamps...\n{\n    \"previousStartedEventId\": 0, \n    \"workflowExecution\": {\n        \"workflowId\": \"6\", \n        \"runId\": \"22Ru........\"\n    }, \n    \"startedEventId\": 3, \n    \"workflowType\": {\n        \"version\": \"v1\", \n        \"name\": \"convert-....\"\n    }, \n    \"events\": [\n        {\n            \"eventId\": 1, \n            \"eventType\": \"WorkflowExecutionStarted\", \n            \"workflowExecutionStartedEventAttributes\": {\n                \"taskList\": {\n                    \"name\": \"default-.....\"\n                }, \n                \"parentInitiatedEventId\": 0, \n                \"taskStartToCloseTimeout\": \"3600\", \n                \"childPolicy\": \"TERMINATE\", \n                \"executionStartToCloseTimeout\": \"3600\", \n                \"input\": \"\", \n                \"workflowType\": {\n                    \"version\": \"v1\", \n                    \"name\": \"convert-......\"\n                }\n            }, \n            \"eventTimestamp\": 1420642538.771\n        }, \n        {\n            \"eventId\": 2, \n            \"eventType\": \"DecisionTaskScheduled\", \n            \"decisionTaskScheduledEventAttributes\": {\n                \"startToCloseTimeout\": \"3600\", \n                \"taskList\": {\n                    \"name\": \"default-......\"\n                }\n            }, \n            \"eventTimestamp\": 1420642538.771\n        }, \n        {\n            \"eventId\": 3, \n            \"eventType\": \"DecisionTaskStarted\", \n            \"eventTimestamp\": 1420642538.85, \n            \"decisionTaskStartedEventAttributes\": {\n                \"scheduledEventId\": 2\n            }\n        }\n    ], \n    \"taskToken\": \"AAAAKgAAAAIAAAAAAAAAA.....\"\n}\n. Think it might possibly be related to this, since SWF also appears to use a floating point date format - https://github.com/stripe/aws-go/blob/master/model/model.go#L431\n. ",
    "nightlyone": "What happens, when I break the loop?\n. @lsegal Yes, but the goroutine feeding the channel will hang then in the channel send. Multiple of those calls of the pagination api will pile up goroutines and thus leak memory that cannot be collected by the garbage collector. This will include all data still in the channel and on their stacks. See http://play.golang.org/p/MwXEgRcldr for illustration.\n. Just opened an issue there, too. https://github.com/aarzilli/sandblast/issues/1\n. Why not simply use the Go logging package? Every Gopher knows how to use, tune and disable it.\nAnd it logs to stderr per default, so you can have meaningful output at stdout.\nWould you accept a PR for that until your full blown logging solution is designed and developed?\n. @michaeljs1990 even simpler: use io.Copy and let it handle all the buffering\n``` go\n    f, err := os.Create(file)\n    if err != nil {\n        fmt.Println(err)\n    }\n_, err = io.Copy(f, resp.Body)\nif err != nil {\n    fmt.Println(err)\n}\n\n```\n. If you already are on an EC2 machine and want to use services from the same region, you can query the availability zone via \"http://169.254.169.254/latest/meta-data/placement/availabilty-zone\" and derive the region from that by stripping the last lowercase letter.\nAnd might be useful to allow auto-configuration of the region this way.\nBut when you are outside an EC2 machine, e.g. using a service on a machine in your office, there is no way knowing which region you want to talk to.\n. The linked Wikipedia article actually says: \"Some implementations allow a colon (:) as the name/value delimiter (instead of the equals sign).\" and this implementation doesn't. So I think, this looks OK as is.\n. Please put _ struct{} to the front instead of to the end of the struct, see  https://github.com/golang/go/issues/12884 for the reason.\n. That sounds like a great idea to me, too. Data could be replaced that way and the firehose.PutXX functions could pass in a pre-allocated buffer to a method Data(w io.Writer) which can be re-used.\n. Documentation disagrees with return code. \ngo\nfunc Time(t time.Time) TimeValue { return TimeValue(&t) }\ninstead?\n. You might also want to check the input types here:\n- takes two values\n- the first one must be ???\n- the second one must be a boolean\n. I would check for \"exactly one output parameter\" (valfn.Type().NumOut() == 1) to enforce stronger typing.\nWhile think about it, the valid type for this function is actually\ngo\nfunc (data SomeType, more bool) bool\nOnly SomeType cannot be represented properly by Go at the moment.\nAn alternate design might be to use \ngo\nfunc (data interface{}, more bool) bool\nThat would make this function much safer to use, but maybe a little less convenient. \nBut it will also get rid of reflection here.\nIt might be possible to hide this better by providing type asserting helpers for the specific use cases and/or more code generation. \n. @lsegal I looked at the results and the manual coded parts got easier, the hard parts are luckily generated by the api scripts (which I didn't anticipate). So it looks awesome now :heart_eyes: \n. ",
    "cbroglie": "The use of callbacks does match the idioms from the std lib: http://golang.org/pkg/path/filepath/#Walk\n. I can look into porting over https://github.com/AdRoll/goamz/pull/292 if there is interest in a similar solution\n. Cool, let me know. My implementation is based completely off the logic from the Java SDK.\n. My version doesn't support sets either as the intention was to map to and from structs representing JSON documents, and there is no built in representation of sets.\nIt supports all the JSON struct tags since it leverages encoding/json to do some of the marshalling.\nOverall the implementations look fairly similar to me.\n. Closing since #231 was merged\n. Ok, that makes sense. I started https://forums.aws.amazon.com/thread.jspa?threadID=171238.\nI don't expect a fast turnaround time for DynamoDB Local supporting query string authentication, and it's critical to my development workflow, so I may add a new function to v4.go called something like SignWithHeader which uses the old Authentication header logic.\n. I planned on doing something like:\ngo\ndb := dynamodb.New(nil)\ndb.Handlers.Sign.Clear()\ndb.Handlers.Sign.PushBack(v4.SignWithHeader)\nSo the functions in v4.go could be shared. But just adding an additional handler which sets the header based on the query string sounds better.\nThat approach would definitely let me implement it outside of the library code, but I would argue there is still some value in putting in the library as others will likely have the same use case. Maybe in a separate debug package?\n. I opened up PR #99 which adds a signer which works as discussed, and I confirmed it works with DynamoDB Local.\nThis doesn't need to live in the core library, but I think it will be useful to anyone using DynamoDB Local (or the equivalents for other services). Happy to move it elsewhere if there is another package structure which makes more sense.\n. Thanks\n. :+1: \n. Note that the tests included expect DynamoDB Local to be running on port 8000\n. @lsegal Curious if you have any thoughts on this change?\n. Fair point. I agree the main value comes from the marshaller. I'll look into resource mappings if/when I get a chance.\n. @lsegal Would you be interested in a separate PR which includes just the marshaling code?\n. This won't help with integer types, but you can leverage the omitempty json struct tag to omit empty strings from serialization. Since DynamoDB doesn't allow empty strings, there is no need to disambiguate. Not sure if other services need to allow for strings to be present but empty.\n. I was primarily thinking of empty strings being omitted from request serialization, since that's where the pain point around pointers comes from. Responses could easily remain as pointers. And I was thinking of the DynamoDB requirement being a contract rather than a current implementation detail. But if there is any chance of that changing, then it certainly shouldn't be relied upon.\n. FYI, you can save from doing renames like this by explicitly cloning your fork (instead of using go get) to a local path under awslabs/aws-sdk-go. Works with tools like Godep as well.\n. Actually, it's only the test code that depends on those struct tags. I'll fix that.\n. Fixed, will open a fresh PR\n. @jasdel That sounds reasonable to me. I don't have a lot of free time at the moment, but will try to get back within a couple weeks.\n. @jasdel I renamed the methods to ConvertTo/ConvertFrom.\nHowever, I'm unsure about this comment:\n\nAn additional note AttributeValues returned from dynamodb can also be scalars and slices in addition to maps.\n\nSince the hash/range keys are part of the data, you will always have a map at the top level, never a scalar or list. The input and output structs reflect this. \n. Ah, ok\n. @lsegal @jasdel Implemented the ConvertToMap/ConvertToList/ConvertTo functions as requested. Still utilizing the json package for handling typed, but that be changed later if necessary. I'm not sure about the cause of the build failure, but it doesn't appear related to my changes.\n. Where do you see that error? All I see in travis is The command \"go get -v -t ./... && make deps\" failed and exited with 2 during .\n. Ok, I see the error locally now after merging upstream. Looking at it.\n. Fixed\n. Woohoo, thanks for merging\n. @jasdel @lsegal It might be a good idea to add a wiki or FAQ page documenting the reasons for the use of pointers. It's clearly a hot issue for newcomers to the library, and specific documentation you can point people to would be easier to consume than having to read through the discussions in different github issues. I think the post about go-github is a great starting point.\n. Thanks @jasdel. I'm encountering this issue in the context of running Terraform, and I can add the custom retrier you suggested there if necessary. I just want to confirm whether or not it can be addressed by the SDK first, so please let me know when you hear back from the service team.. @jasdel Any updates on this?. @jasdel Thanks for following up. I understand the technical reasons why it's hard for the SDK to solve this generically, and as you can see from the linked PRs, it was fairly trivial to address in Terraform with your sample code.\nThat said, this isn't a fully satisfying outcome. As a user of the SDK I would like it to be authoritative about which errors should be retried, and encapsulate this centrally, rather than having to maintain a bunch of customizations that need to be ported to each each application. I feel like users would be better served by including these customizations in the SDK.. Sounds good. Feel free to close out this issue if you'd like.. That's true. The json encoding has always made me feel a little icky, but I haven't spent time to remove it because it just works.\nAlso, I haven't benchmarked it, but one nice thing about using json to convert is that it caches the reflection results. Though I guess in practice this isn't likely to be a hot spot either way, the data types are usually pretty small.\n. Wouldn't returning error be more appropriate than panicing?\n. That argument makes sense, and I agree with the differentiation between programmer and runtime errors. But the downside of this form of runtime type checking is that you only catch the code paths you execute in your testing. 99% of the time you have a simple case and you hit the panic in development and fix it right away. But a common issue when accepting an interface{} argument is someone passing a pointer when they shouldn't have, or vice versa. And if this happens in an edge case that escaped your testing, you start panicing when releasing to production. Or worst case, code has been live and stable and some external change causes a behavior change which starts executing this bad path.\nOf course the dream is 100% test coverage, and recoverer middleware can often stop panics from bubbling to the top for web servers (unless the panic is in a goroutine). But in reality edge cases can go untested, and it's nice when library code is more forgiving.\nThe Go authors seem to support that stance:\n\nThe convention in the Go libraries is that even when a package uses panic internally, its external API still presents explicit error return values.\n\nThe above excerpt is from http://blog.golang.org/defer-panic-and-recover.\nAll that said, I think it's extremely unlikely anyone would hit these particular panics in production, for the reasons you mentioned. But I'm always a little uncomfortable when I see the possibility of panics escaping libraries.\n. An option would be to use a recoverer, like the encoding/json package.\nBut do I agree that this would be extremely rare in production.\n. ",
    "jszwedko": "Awesome, that is looking good @lsegal.\nOne thing I like about some of the other AWS SDKs, e.g. Ruby, is that it provides a hook into the polling process.\n. :clap: thank you @jasdel !\n. @lsegal thank you very much for clarifying. I don't have as much experience with the other SDKs as this one so I never noticed that behavior.\nLooking forward to seeing what you find!\n. @jasdel thanks for the additional explanation! It makes sense from a compatibility standpoint -- just a minor annoyance/suprise.\n. @jasdel that'd be great :+1: \n. ",
    "georgyo": "Has any progress been made on this,  is there a plan to merge this, and what else needs to be done for it?\nThe last commits to the waiters branch were on Mar 28. \n. ",
    "bracki": "Any news on this? Really would love to have this.\n. Is this issue really solved? \nI'm still getting the following using https://github.com/awslabs/aws-sdk-go/commit/d67a47db2971c58a865e3ed3d5d4ee317477ac10:\n```\nT 127.0.0.1:63833 -> 127.0.0.1:8000 [AP]\nPOST /?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAAXXXXXXXXXXXX%2F20150421%2Feu-west-1%2Fdynamodb%2Faws4_request&X-Amz-Date=20150421T075501Z&X-Amz-Expires=300&X-Amz-SignedHeaders=host%3Bx-amz-target&X-Amz-Signature=xxxxxxxxxxxxxxxxxxxxxxx HTTP/1.1.\nHost: localhost:8000.\nUser-Agent: aws-sdk-go/0.5.0.\nContent-Length: 21.\nContent-Type: application/x-amz-json-1.0.\nX-Amz-Content-Sha256: ab92c2bbb935dcf9db6b76ea026a523873c16015c2a74250e9ed26b8c4c8b438.\nX-Amz-Target: DynamoDB_20120810.DescribeTable.\nAccept-Encoding: gzip.\n.\n{\"TableName\":\"confd\"}\n\nT 127.0.0.1:8000 -> 127.0.0.1:63833 [AP]\nHTTP/1.1 400 Bad Request.\nContent-Type: application/x-amz-json-1.0.\nx-amzn-RequestId: fd2582c8-9a5f-4b5d-8555-7b4739fa3afb.\nContent-Length: 173.\nServer: Jetty(8.1.12.v20130726).\n.\n{\"__type\":\"com.amazonaws.dynamodb.v20120810#MissingAuthenticationToken\",\"Message\":\"Request must contain either a valid (registered) AWS access key ID or X.509 certificate.\"}\n```\nThis runs against a local DynamoDB. I assumed that authentication now should use header based authentication again.\n. Sorry my mistake. GOPATH/Godeps confusion \u00e0 la carte.\n-- Jan\n\nAm 21.04.2015 um 15:02 schrieb Loren Segal notifications@github.com:\n@bracki it should be resolved. Are you sure you've updated your dependency?\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "luck02": "How would you go about setting this while using the kinesis client.\nI would have assumed you would set it on the httpclient object on config:\nconfig := &aws.Config{Region: \"us-west-2\"}\n    svc := kinesis.New(config)\nBut changing that doesn't seem to have any impact.\n. Just to be clear, I'd like an overview of how other people are using this / testing this.  Ideally I'd like to be able to contribute to the library.\n. Oh nice, thanks allot.\n. @jasdel and @lsegal  thanks, I'll be checking this out shortly.\n. I'm seeing the stream become active, it's the empty response that's confusing me.\nI think no response would be more intuitive.\n. ",
    "marceldegraaf": "Thanks, we'll look into submitting a PR for this.\n. Hi @stripecodahale, my colleague @mrdg and I have discussed a possible implementation for this feature. We want to quickly run that by you before we start hacking on it :-).\nSo, our proposal is to add an overrides map to the endpoints package, containing tuples of service name (e.g. \"ec2\") and endpoint (e.g. \"http://localhost:1234\"). This overrides map has precedence over the existing code in endpoints.Lookup(), allowing one to override one or more service endpoints for local testing.\nIf you're OK with this, we'll submit the PR as soon as possible.\n. I'm not sure I understand what you mean. Can you elaborate on what the region specifier should do in the context of a local service endpoint override? Thanks :smile: \n. Heh, no problem!\nAlright, sounds good. We'll probably have a PR for this in the coming days.\n. Ah, that file is generated as well? I thought it wasn't, because it doesn't have the telltale comment at the top :-).\u00a0\nI can also update the PR with a correct implementation if you want?\nOn Tue, Jan 20, 2015 at 11:36 PM, Coda Hale notifications@github.com\nwrote:\n\nOh, sorry. I'm going to have to back this out. I didn't see that it's a change to a generated file. I'll merge this functionality in myself.\nReply to this email directly or view it on GitHub:\nhttps://github.com/stripe/aws-go/pull/65#issuecomment-70748822\n. Awesome, thanks!\n. \n",
    "brycekahle": "Thanks @stripecodahale \n. ",
    "gronpipmaster": "http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_UpdateTable.html\n1) No AttributeDefinitions you can not upgrade the current key for the table.\n2) It describes methods create and delete http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_GlobalSecondaryIndexUpdate.html\nI can create for botocore's pulrekves with these same changes.\nSo as these methods are tested on a real table.\n. See https://github.com/boto/botocore/pull/439\n. Hi, is they have already prepared a commit, see here, merget into master.\nhttps://github.com/boto/botocore/commit/672c69033c4a365f1b5099c7f0a7edd6ba41d3f2\n. ",
    "drombosky": "Looks like the fix proposed above does work. I'll try to get the PR in tomorrow. Thanks again for managing this project!\n. I tried replicating your problem and couldn't using Go 1.4 on OSX. I had to modify the filter a little bit to get the example to run.\nresp, err := e.DescribeInstances(&ec2.DescribeInstancesRequest{\n        Filters: []ec2.Filter{ec2.Filter{aws.String(\"image-id\"), []string{\"ami-b66ed3de\"}}}})\nWhere ami-b66ed3de is the Amazon Linux AMI in us-east-1. The command worked and only returned information on the proper instances.\nIf you are trying to filter by Security Groups, take a look at this page: http://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInstances.html . You'll probably want to use the instance.group-id or instance.group-name filters.\nHope this helps!\n. @guregu There are some cases where pointers are needed because there is a very big difference in the AWS API between non-zero, zero, and omitted. Take ModifyVpcAttribute for example. If EnableDneHostnames and EnableDnsSupport where both literal booleans that were unconditionally marshaled, you would need to specify both values all the time or risk the chance of a true value flipping. Likewise, using omitempty prevents false from being sent over the wire.\n@lsegal I do still wonder about []*string and *map[string]*AttributeValue. I'm not sure what those buy technically over []string and map[string]AttributeValue. I'm just getting around to moving my code over from Stripe's implementation and finding it a bit tiring to constantly convert []<type> to []*<type>.\n. I'll echo the points above. Stuct initialization is one of the better idiomatic features of Go. \nBuilding the data structures via function calls would look significantly different compared to standard struct initialization. Also, and this is a personal preference, the justification gofmt provides for stuct initialization makes the code feel cleaner.\nThe Go compiler also prevents double initialization of struct fields. I'm not sure if that would be possible in this case.\nThe solution does look like it would work nicely at a higher level. The first thing I thought of when I saw this was chaining together parameterized data transformations in a concise way.\nI don't see a problem with continuing to pass pointers to non-nillable types and using helper functions to get the job done (e.g., aws.String and aws.Boolean). My headaches come from transforming []<type> to []*<type> where most of the cases <type> is string or a filter type but may also be another aws-sdk-go type that is passed in a list.\nHope this helps as well!\n. @lsegal I'm a bit confused on the plugin use case. I skimmed through the code for hooks and didn't see any. I'm not sure how a plugin wouldn't be able to modify a request in the []Type case, expect if it was trying to act across the API, which I'm not sure is a good design pattern to support.\nI think Foo: aws.String(\"x\"), Bars: []string{\"not-a-ptr\"} is less confusing than Foo: aws.String(\"x\"), Bars: []*string{aws.String(NotAPtr[0]), aws.String(NotAPtr[1])} or having to add additional boiler plate to convert from []Type to []*Type before every call.\nIf []*Type is a requirement, I would request helper functions for converting between []Type and []*Type, then your example becomes Foo: aws.String(\"x\"), Bars: aws.Strings(NotAPtr). This would be especially useful since there are many types that follow this pattern, not just string.\n. ",
    "moonpolysoft": "Looks like this actually got fixed in https://github.com/stripe/aws-go/commit/f2b70851d1a16fff694a745bde93303af1733855 and go get had cached an old version on me.  Thanks for looking into this.\n. ",
    "alexaandru": "Same here, can't get it to work.\n``` go\npackage main\nimport (\n    \"fmt\"\n    \"github.com/awslabs/aws-sdk-go/aws\"\n    \"github.com/awslabs/aws-sdk-go/gen/sdb\"\n)\nfunc main() {\n    cred, err := aws.EnvCreds()\n    if err != nil {\n        fmt.Println(1, err)\n        return\n    }\ncli := sdb.New(cred, \"us-west-2\", nil)\nresp, err := cli.ListDomains(nil)\nif err != nil {\n    fmt.Println(2, err)\n    return\n}\n\nfmt.Println(resp.DomainNames)\n\n}\n```\n. Would you accept a patch? Are there any documented guidelines for contributing to this project?\n. Pushed https://github.com/awslabs/aws-sdk-go/pull/245  Cheers!\n. I just checked and I can confirm it works perfectly. We have the custom headers, the SSE turned on, etc. Thanks again! :) Cheers!\n. Reflection as in locally to just populate the exiting fields on the request automatically? Something like:\nreflect.ValueOf(&opts).Elem().FieldByName(key).SetString(val)\n(with proper error checking of course) ? Or something else, at higher level across the entire SDK maybe?\n. Awesome, thank you :)\n. I gave the reflection a try (including the awsutil.Copy) but no luck. It just lead me from one error to another so far. I must admit I have zero experience with writing reflection based code in Go.\nAny chance you could accept my patch until someone offers a reflection based alternative? At least we could start using this feature right away. Thank you.\n. That sounds great then, thank you! :)\n. That is awesome news, thank you so much! :)\n. +1 for the current pointer approach -1 for anything based on interface{}\n. ",
    "jjeffery": "Is there any progress on this issue? Is SimpleDB likely to be supported any time soon? I understand from the comments above that it is not the highest priority, but I was hoping for some idea of when SimpleDB support might become available. (Or if it is going to ever be available, I suppose).\nThanks\n. When I authored this PR i thought it made sense to add a new package for the version 2 signer (internal/signer/v2) alongside the existing v4 package. I also modified the API template generator (internal/model/api/api.go) to use the v2 signer for services where the JSON API metadata specified a version 2 signature.\nI know that SimpleDB is not a big priority for AWS, and although to my knowledge it is not officially deprecated, Amazon are not encouraging people to use SimpleDB. For this reason, would it be better to:\n1. Revert the changes made to the API generation code\n2. Move the v2 signer into the services/simpledb package as a private\n3. Customize the SimpleDB service code to use v2 signing inside the initService() function.\nThe advantages of this approach would be that:\n- The legacy version 2 signer would not be visible to any other package other than the simpledb package.\n- Including SimpleDB in the SDK would involve a single, one-line change to internal/model/cli/gen-api/main.go, and all other implementation would be confined to the service/simpledb package itself.\n- There is no additional complication or complexity to the SDK by including support for SimpleDB.\nI'm only too happy to make these changes there is a view that this would be a better implementation.\n. @andrewgaul: So does this mean you think I should leave the code as is, and not move the v2 signer into the simpledb package as per my previous comment?\n. Hi @jasdel thanks for reviewing the PR. All good comments. I have actioned your comments and responded to them above. I have also put a little bit more work into testing the V2 signer.\nI wouldn't mind putting a bit more work into the V2 tests. I'd like to find a test suite for version 2 similar to the V4 test suite (http://docs.aws.amazon.com/general/latest/gr/signature-v4-test-suite.html), but I have not been successful so far. If I do get my hands on some good test suite data I can upgrade the tests and include in a separate PR.\n. The V2 signing was originally included as part of PR #316 to support SimpleDB, which only works with V2 signing in all AWS regions. At the time I thought it might be a good idea to keep all the V2 signing code private to the SimpleDB package, because it was specific to SimpleDB. (As noted earlier in this conversation, V2 signing is slightly different for S3). \nThe decision was made to keep the V2 signer in its own package, next to V4 in the package hierarchy. See #316 for details.\nNot sure how much work is required to the V2 signer to get it working for both S3 and SimpleDB.\n. Fixed -- good point. I have implemented GET signatures. My reading of the V2 signature spec is that only GET and POST methods are supported, so I've changed the test to check for those methods. I have also modified the signature code to support GET, which was pretty straightforward.\n. Fixed. Interestingly enough, I could not find any test suites for V2 signature, so I captured a SimpleDB request using fiddler and used that as the basis for this test. Of course I invalidated the credentials after doing this. I take your point though. It is difficult to check that these credentials are no longer valid and it is better to use mock credentials.\n. You are spot on, I did base this code on the S3 code, with the main difference being that the SimpleDB error responses is different from the S3 error responses. (Note that the SDB code has xmlErrorDetail and xmlErrorResponse types, while the S3 code only has the xmlErrorResponse type).\nThere are also some other differences:\n- the S3 code has some S3-specific checks for incorrect region\n- the SDB error response can contain multiple error codes\n- there are extra serialization error conditions when checking the SDB error response\nIt just seemed a bit too tricky to make the same bit of code work for both S3 and SDB, and then there was the issue of what package to put the common code in.\nSo  I have not made any changes to this code at this time, but am happy to in future if you want.\n. ",
    "nbari": "How to import/user sdb, while using:\nimport \"github.com/awslabs/aws-sdk-go/gen/sdb\"\nor \nimport github.com/aws/aws-sdk-go/gen/sdb\nI get: \ncannot find package\nI did:\n$ go get -u github.com/aws/aws-sdk-go/...\nBut still can't figure out how to work with simpledb\n. ",
    "justonia": "I haven't done a deep comparison between my solution and your goamz one. Barring bugs I am unaware of, mine supports the same semantics as encoding/json -- eg nested types, pointer fields, etc. I do not support int or string sets yet mapping into attribute values. How does that compare to what you have implemented so far?\n. The context pattern isn't itself intimately tied to goroutines. Cancellation of one is an obvious example they provide, but it can be useful with or without them. You will no doubt as part of this library need to support exponential backoff for various Amazon APIs that require it. If the code that implements the backoff has access to a context object, it would be able to prematurely exit the backoff loop if the context were cancelled. Here is an extremely rough example that requires no goroutines:\n```\nfunc awsBackoffRequest(ctx context.Context, .......) error {\n    for i := 0; i < maxBackoffAttempts; i++ {\n        select {\n        case <-ctx.Done:\n            return ctx.Err() // e.g. ErrCancelled\n        default: // context not cancelled, continue with attempt\n        }\n    // make connection attempt\n    ...\n}\nreturn nil\n\n}\n```\nIt is very important to understand the context object is meant to be a request-scoped object only. I may have one instance of an AWS service instantiated that I intend to share across many frontend requests, but each request itself would have its own context object that would be passed down all the way to any calls into the AWS lib. It would be a mis-use of the pattern to make the context object part of a Config struct passed once to a \"New*\" method of a service, since this would now prevent per-request cancellation of an API call.\nBesides cancellation, internally we use the context object for dependency injection of request scoped parameters. For instance, our context object is a required argument for our logging interface. This allows us to inject parameters that the logging system wants to decorate messages with, like an HTTP request ID, without intermediate systems having to have this information or even know they are part of an HTTP request cycle. We also inject call stack information into the context object so that our log messages can provide useful debug information we can run through analytics.\n. Yea I agree, the context pattern is not just for concurrency and this is a great example of a use case for it.\n. The changes are better than writing to stdout by default, but still far from ideal when it comes to capturing logging information in a meaningful way. Additionally, by just writing to a io.Writer how is the user of the library supposed to know when a single \"log message\" has been written? In the v4 signing code for instance, a single \"message\" is actually 7 separate write calls with no way of knowing what indicates the begin or end of the message.\nThere are already many conventions for logging APIs, the Go standard library even has one. So something as simple as:\ntype Logger interface {\n    Debug(message string)\n    Info(message string)\n    // etc\n}\nWould be both simple to use and simple to implement inside of the library.\n. Have you had a chance to look at the context pattern case? I updated it with more information about usage patterns. That would pretty heavily impact any interface I would propose. \n. Our logger interface requires a context object because we inject request-scoped parameters into it that we then use to create very detailed log information. For example:\n18:57:23.450 3ms    3ms    main:frontend:setup:core:aws:dynamodb:connection:query - [ERROR]\n{\n    \"type\": \"*url.Error\",\n    \"message\": \"Post http://192.168.59.103:8000/: dial tcp 192.168.59.103:8000: connection refused\",\n    \"url\": \"http://192.168.59.103:8000\",\n    \"request_id\": \"de5c2b0858e94caf80cd8e47295ab190\"\n}\nThis example is from our stdout log backend for one log message from a failed DynamoDb connection attempt (using our internal library). It contains the time, the elapsed time since the request started, the call stack string, the log level, and a series of k/v pairs describing in more detail what the log message actually is. The only parameters that our low-level DynamoDb library provided to the log interface were \"type\", \"message\", and \"url\". Everything else was injected and stored within the request-scoped context. We have other backends we output rich JSON data to, such as Loggly. We can then run detailed analytics against a huge collection of log messages to make meaningful decisions on what is going on in our system.\n. Again, for the same reasons I have mentioned it is impossible to get request scoped information into even the default Go logger without having to have every single method take every single parameter you want logged. If there are other solutions you can think of I'm open to them, it's the best we could come up with internally and the context object seemed to be a very natural fit.\n. Sounds good to me.\n.  I forgot that I have not implemented the set types yet,  it'd be easy enough to add at some point.\n. ",
    "cdunn": "+1 on this add. currently using the marshaling on goamz\n. ",
    "tgross": "@cbroglie given my comment in https://github.com/AdRoll/goamz/issues/308#issuecomment-73341675, I'd be happy to help out on this ticket here in any way I can.\n. ",
    "aarzilli": "I changed those imports.\n. ",
    "kidoman": "+1 to this! Go is standardizing around golang.org/x/net/context \ngRPC (http://www.grpc.io/) recently open sourced is also heavily built around context package. It would be a shame for one of the most important libraries in Go (aws !) to not be inline with the trend.\n. I always thought that \"golang.org/x\" was home for packages which the team does not want to add to the stdlib but is a critical part of the golang eco-system. While I cannot comment about the experimental nature of the other packages under \"golang.org/x\" but looking at the widespread usage of golang.org/x/net/context (https://godoc.org/golang.org/x/net/context?importers) I feel that betting on this package would be sound.\nWriting code using contexts (particularly when controlling the deadline/timeout of a subtree of calls) is quite elegant. I would rather use it everywhere vs looking for API specific cancellation mechanisms.\nFor example, this things like this: https://godoc.org/google.golang.org/api/storage/v1#ObjectsInsertCall.ResumableMedia\nHere, the cancellation can be dictated by the parent caller (since everyone is passing the parents ctx around, or a ctx dervied from the parents) quite nicely.\n. @lsegal:\nPlease have a look at this: https://groups.google.com/forum/#!topic/golang-dev/cQs1z9LrJDU\nTL;DR: net/context is moving into the stdlib.\n. @freeformz I would really expect the client (user of this lib) to generate purpose build interfaces. *Impl classes really remind me of bad times!\n. Woud it not be better to have idiomatic Go constant names. i.e. USEast1 vs US_EAST_1?\n. ",
    "jharlap": "Unfortunate to see this getting closed. An example where context support would be really nice is when using SQS ReceiveMessage with non-zero WaitTimeSeconds. In this case, cancelling a context while the ReceiveMessage request is just waiting idly would be a very reasonable thing to do.\n. ",
    "seiffert": "I've spent some time on this topic and thought about different ways to solve this.\nMy problem is mainly with DynamoDB and throttled requests. When one misses to increase the provisioned capacity of a table, this leads to throttled requests, which in turn causes the SDK to retry its requests with exponential backoff. This causes long request times which cause timeouts in our system. By using context objects, we have more control over the time to fail in such situations.\nFirst I thought that implementing a custom request.Retryer would be enough. Most API requests are really quick anyway and this way one could prevent new requests being started after the context was cancelled. The problem is, that a request.Retryer is really not capable of deciding whether to perform further retries. It has a method ShouldRetry(*request.Request) bool which I would expect to be consulted for this decision. However, it turns out that it is not called in most cases and that the decision is mostly done in a different place.\ngo\ntype Retryer interface {\n    RetryRules(*Request) time.Duration\n    ShouldRetry(*Request) bool\n    MaxRetries() int\n}\nThe next thing I evaluated was performing all SDK operations asynchronously and stop waiting for the responses as soon as the surrounding context was cancelled.\n``` go\nvar (\n    errCh = make(chan error)\n    outCh = make(chan *dynamodb.ScanOutput) \n)\ngo func() {\n    out, err := client.Scan(&dynamodb.ScanInput{...})\n    if err != nil {\n        errCh <- err\n        return\n    }\n    outCh <- out\n}()\nselect {\n    case <-ctx.Done():\n        return ctx.Error()\n    case err := <-errCh:\n        return err\n    case out := <-outCh:\n        return doStuffWithOutput(out)\n}\n```\nThis turned out to be a bad idea pretty quickly. For safe operations (speak \"read-only\") this is ok, for operations that change the state of a resources however, one cannot be sure whether the operation actually was performed or not.\nDigging deeper in the SDK, I read more about handlers. Apparently everything related to the request/response cycle is done in handlers. One signs the request, another one validates its parameters, one validates the response and so on. In order to modify something in the lifecycle of a request, one can add or modify a handler.\nNormally when performing requests to a backing service via HTTP, I prefer to use the x/net/context/ctxhttp package which provides helper methods to perform cancellable HTTP requests. So why not use them in the AWS SDK's \"send handler\"? This is what I did and until now I'm quite confident about this approach.\nBy replacing the default corehandlers.SendHandler with a very similar implementation that uses ctxhttp.Do(...) instead of the default HTTP client's Do method, all API requests respect the lifetime of a context. It is important to note that one should not do this with the handlers configured in a SDK session object. Contexts are most of the times only valid during one request and therefore need to be configured on an SDK operation basis.\nI created a really small library (ctxaws) that does exactly this. Does this approach make sense to you? I'd really like to get some feedback on this. \n. Thank you @lsegal for your detailed response!\nWhat I extract from it are the following thoughts:\n- cancelling HTTP requests is usually not what we want (however I think that HTTP timeouts basically cause the same issues - in both cases you don't know whether the server got the message)\n- I really should go back to implementing a custom Retryer in order to stop retrying when the context has exceeded\n- In order to do this properly, one would need to adapt the whole handler chain. Here, I still don't know how to improve my solution. If I understand you correctly, you suggest to set sensible HTTP timeouts and rely on a custom Retryer otherwise, correct?\nThanks again, feedback like this was exactly what I was looking for!\n. @jasdel it seems like the section you've added to the documentation was not moved to https://github.com/awsdocs/aws-go-developer-guide. Could you please re-add it?\n. Do you have an idea how to use contexts best for operations requiring pagination? \n. Ok, I opened a feature request: https://github.com/aws/aws-sdk-go/issues/861. \nThanks!\n. Hi @jasdel,\nthank you for your quick answer! Changing the code to use aws.NewConfig or defaults.Get().Config made it work again! Prior to the update we just used defaults.DefaultConfig everywhere and modified it according to our needs.\nSo in my current understanding, I'd put configuration that applies to multiple clients into the session's configuration and pass defaults.Get().Config.With[...] with client specific configuration as the second argument to every client constructor. Is that correct? And is it the recommended practice? I would love to see some more documentation about best practices.\nOne more question: What is the use case for defaults.Config()?\nThanks again!\n. Thanks for the fast response and fix!\n. Sure, I've added this formatting in f6e943c. I just removed the word Details because it doesn't actually log any request/response details.\n. You're completely right. I added return statements in 703ff7a.\n. fixed\n. ",
    "ryanfowler": "For anyone still interested in using context with aws-sdk-go, the solution appears quite simple with Go 1.7 now that you can set the context in an HTTP request:\n``` go\nctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\ndefer cancel()\n// SQS ReceiveMessage\nparams := &sqs.ReceiveMessageInput{ ... }\nreq, resp := s.ReceiveMessageRequest(params)\nreq.HTTPRequest = req.HTTPRequest.WithContext(ctx)\nerr := req.Send()\n```\n. @jasdel Sounds good, I'll submit a PR in the next couple days.\nDefinitely a good idea to add to the SDKs wiki, too \ud83d\udc4d \n. @muhqu @jasdel You're right. I've just submit #839 to take a shallow copy of the HTTP request (before assigning the URL, body, and Headers) when retrying. This will carry the context field over in Go 1.7, yet still be compatible with Go 1.x.\n. @jasdel thanks for reviewing!\nWith regards to the race condition inBody, it appears that the only way this would introduce a race condition is if the Body field in the initial request was being assigned to at the same time as the copy operation. I'm not totally sure where this is happening other than in this function.\nThat's too bad, I was hoping to avoid creating another version specific file for this. However if you'd like to proceed this way, I can update the current http_request.go to http_request_1_5.go with build tags:  // +build go1.5,!go1.7, and create a new file that copies the request's context? In that case, all of these commits should be squashed, if accepted.\n. ",
    "muhqu": "@ryanfowler @jasdel It looks like the req.HTTPRequest = req.HTTPRequest.WithContext(ctx) only works for the first attempt, but not if the request is automatically retried (due to expired credentials).\nI guess the SDK needs to be adjusted to propagate req.HTTPRequest.Context() in case of retries. WDYT?\n. ",
    "rem7": "Are you talking about an SQL insert? You should be using COPY. Inserts are discouraged in Redshift, especially bulk inserts. Inserts are extremely slow because of the columnar storage.\nhttp://docs.aws.amazon.com/redshift/latest/dg/r_INSERT_30.html\n. ",
    "hakejam": "Why add the +API to the interface name?  Why not leave it as the service name?\n. ",
    "joshrose": "+1. This is also an issue for me.\n. I'm not sure if this is helpful, but at https://github.com/awslabs/aws-sdk-go/blob/master/aws/v4.go#L162-L170 you'll see that the query is encoded (166), but the path is not (165). This is interesting given that the spec here: http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-query-string-auth.html appears to suggest that the path must be uri-encoded as well.\n. ",
    "CheRuisiBesares": "Hey I took a brief look into this and it looks like the api is changing a lot on the develop branch. On the current master the aws/rest.go does the following:\nreq.URL.Opaque = EscapePath(req.URL.Path)\nbefore signing the request. The signer looks only for the URL.Path\nfunc (v4 *signer) buildCanonicalString() {\n    v4.canonicalString = strings.Join([]string{\n        v4.Request.Method,\n        v4.Request.URL.Path,\n        v4.Request.URL.Query().Encode(),\n        v4.canonicalHeaders + \"\\n\",\n        v4.signedHeaders,\n        v4.bodyDigest(),\n    }, \"\\n\")\n}\nWhich won't pick up the modifications made to the Opaque path. I think that modifying the above function to:\nfunc (v4 *signer) buildCanonicalString() {\n    v4.canonicalString = strings.Join([]string{\n        v4.Request.Method,\n        v4.Request.URL.RequestURI(),\n        v4.Request.URL.Query().Encode(),\n        v4.canonicalHeaders + \"\\n\",\n        v4.signedHeaders,\n        v4.bodyDigest(),\n    }, \"\\n\")\n}\nCould solve the problem but it breaks the dynamodb test case. I have included the diff in the expected string below as well as the debug output:\nAWS4-HMAC-SHA256\n     Credential=AKID/19700101/us-east-1/dynamodb/aws4_request,\n     SignedHeaders=content-type;host;x-amz-security-token;x-amz-target,\n-    Signature=4662104789134800e088b6a2bf3a1153ca7d38ecfc07a69bff2859f04900b67f\n+    Signature=2a7e89071c6876ffc419f9d3fb7e09800da5bc980059608e24e6e9fd4aca3370\nThe signer with debug toggled shows this:\n```\n---[ CANONICAL STRING  ]-----------------------------\nPOST\n/\ncontent-type:application/x-amz-json-1.0\nhost:dynamodb.us-east-1.amazonaws.com\nx-amz-security-token:SESSION\nx-amz-target:prefix.Operation\ncontent-type;host;x-amz-security-token;x-amz-target\n44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a\n\n---[ STRING TO SIGN ]--------------------------------\nAWS4-HMAC-SHA256\n19700101T000000Z\n19700101/us-east-1/dynamodb/aws4_request\nf5773a3ef130214404b7a6b1fae0576dbcc14166a4b1b35b3168e197a6463e7c\n-----------------------------------------------------\n```\nIm wondering how you generated the Signature for this test and if the new value is still valid. I also noticed that the signature value changes in the develop branch version to this af788cd33b3a72bc770b83f5ee0a59adb8fe6971bb690fea2d2009e14e04d01d\nI would be interested in pushing some additional test up following the examples here http://docs.aws.amazon.com/general/latest/gr/signature-v4-test-suite.html if your interested but Im guessing it might make more sense to focus that effort on the develop branch. For now I would love to get this bug out of the way so I don't have to compile against my own fork.\nAnyway I can submit the above changes as a pull request against master in the interim if they seems alright. If they don't let me know and I can take a deeper look.\nThanks for the great project.\n. Absolutely and thanks for looking into this. In this case I was trying to use RequestURI as a little bit of a hack because it returns Opaque path if its defined and Path if not. I guess however that path would be escaped in that case... I can't seem to find a way you could get to that part of the code without having an Opaque but I guess i didn't look closely at the ec2 stuff. Sorry to pull you off the trial. Thanks for looking into that for all of us.\n. ",
    "catsby": "@rmenn I found the problem here to be this line in gen/ec2/ec2.go:\n- https://github.com/awslabs/aws-sdk-go/blob/master/gen/ec2/ec2.go#L3169\ngo\nInternetGatewayIDs []string         `ec2:\"InternetGatewayIds\" xml:\"internetGatewayId>item\"`\nIt should be ec2:\"InternetGatewayId\", matching VPNGatewayIDs in  DescribeVPNGatewaysRequest, et. al. \nLooking at the API json, I'm not sure why gen/ec2/ec2.go got generated that way, though I'm probably not understanding something about the code generation process. \nFor what it's worth, changning that to ec2:\"InternetGatewayId\" resolves this issue\nUpdate: fixed typos\n. Sorry \u2013\u00a0ignore this, wrong repo :frowning: \n. Thanks @cbroglie , good suggestion. I'm not sure that would work for other people pulling terraform and trying to do builds unless we had that workaround in the documentation. \nThe fork and mass renaming is only a temporary measure, I eagerly away destroying the fork :) \n. The documentation in the code implied I needed to:\n\n// The public key. You must base64 encode the public key material before sending\n    // it to AWS.\n    PublicKeyMaterial []byte locationName:\"publicKeyMaterial\" type:\"blob\" required:\"true\"\n- https://github.com/awslabs/aws-sdk-go/blob/master/service/ec2/api.go#L11375-L11377\n\nIs that not intended for people using the SDK? \nEdit: not encoding the key myself does seem to work, thanks \n. Thanks! :+1: \n. Hey @lsegal \n\nif your intent is to omit the parameter\n\nMy intent is to specify the parameter. I do not want those devices to be present in the AMI, I want them suppressed \n. Thanks @lsegal \u2013\u00a0I did try the string true, but I get an error:\nInvalidRequest: The request received was invalid.status code: 400, request id: []\n(that's from Packer, not the script I wrote and shared, but I'm out the door so it will be a few hours before I can try that script again)\n. Hey @lsegal, I apologize for nagging, do you have any further thoughts here? The impetus of this is in Packer: https://github.com/mitchellh/packer/issues/2398 if you need additional backstory. Thanks!\n. Hey @lsegal \u2013\u00a0Yes, I am still hitting this.\nI've updated the gist above (link), it still demonstrates the problem. Running that script with a running, EBS backed instance ID specified, will create an AMI that fails to boot. Using the CLI I see this for the state reason:\n||||                                                     State                                                    ||||\n|||+---------------------------------------+----------------------------------------------------------------------+|||\n||||  Code                                 |  48                                                                  ||||\n||||  Name                                 |  terminated                                                          ||||\n|||+---------------------------------------+----------------------------------------------------------------------+|||\n||||                                                  StateReason                                                 ||||\n|||+---------+----------------------------------------------------------------------------------------------------+|||\n||||  Code   |  Client.InvalidParameterCombination                                                                ||||\n||||  Message|  Client.InvalidParameterCombination: Could not create volume with size 0GiB from snapshot 'null'   ||||\n|||+---------+----------------------------------------------------------------------------------------------------+|||\nSpecifying true or false strings results in invalid request errors \n. For what it's worth, https://github.com/mitchellh/packer/issues/2398 has been resolved with a workaround , basically only specifying NoDevice and nothing else in the image create...\n. Amazing, thanks @jasdel! \n. I ran the recommended migration tool on the Terraform project and everything seems to work as expected. Big thanks for including that! \n\n. I'm glad this is being considered. In Terraform we've added our own retry/waiting behavior, some of which is more complex and specific to Terraform, but others are simply \"wait for this condition\". I would like to use the native Waiter here, however, for things like RDS this can take considerable time. Terraform emits logs with our current waiting behavior, and it seems like without any hooks in the SDK Waiters, the CLI would appear to simply hang while waiting. If there was a hook to provide a function which simply emits a log statement (and could possible check the response and return error, etc), I think we would convert to these Waiters pretty quickly.\nThanks for all the great work!\n. Success :+1: \n. Fair enough, I understand the reasoning to not merge it. \nAs a counter point, it is less than ideal that I can rely on the sdk to automatically pick up my credentials from the environment (including default region), only to learn that in order to use OpsWorks I had to explicitly configure the connection to use us-east-1. \nLess than ideal, but not the end of world :smile: \nThanks!\n. Follow up:  My specific less than ideal comment is mostly because the error message I got was dial tcp: lookup opsworks.us-west-2.amazonaws.com: no such host, which at first read made me think there was a problem with the service. Only after some debugging did I think to check AWS's endpoints documentation and decipher that I was hitting a bad / nonexistent endpoint.  \nStill, not a huge deal I suppose. Thanks again \n. Understandable @jasdel , thanks :smile: \n. For what it's worth, the aws cli gives me the same results (no name returned), so it's likely not an SDK issue. ~~I'll see about opening a support issue with AWS~~ I opened a support issue, but I don't think it's public :/ \nAWS CLI:\n$ aws rds create-db-cluster --engine aurora --master-user-password ****** --database-name specialname --db-cluster-identifier aspecialidentifier --master-username someusername`\nSame results\n. We concluded it was an API issue, so, thank you for following up. I'll close this then, thanks!\n. You seem spot on, @radeksimko . The background issue/pr for Terraform is here:\n- https://github.com/hashicorp/terraform/pull/4254\nBasically, if we're running on AWS, we want to try and use the EC2RoleProvider. \nWe're currently unaware of any reliable means to answer the question \"are we running on AWS?\", so we check this metadata url. The server header is more of a safety net in the event that something is listening on the same ip:port but not actually the metadata service. The odds of that are small, but small odds have bitten me in the past :smile: \nOur Nomad project does a similar check (minus the header check)\n. This is likely a godep problem and not an issue with the SDK. I've seen this before but I honestly don't recall what the issue was, other than my godep path was messed up. \n- Are you using Go 1.6 and vending, with the experiment flag set? (I forget if you want that, or not). \n- Have you modified the AWS SDK in your vendor path?\n- What does your Godeps.json file say you're using, in terms of the aws sdk commit? \n- Are you re-writting import paths? \nI apologize for the barrage of questions, I honestly can't recall what the trouble was when I hit this but it's 99.9% a problem with godep and your project\nEDIT: What's your import like in your source file? Are you importing from vendor? ( you should not )\n. Hello! In github.com/hashicorp/terraform we're starting to build out our support of IoT and in doing so I noticed that I cannot setup a lambda function and link it to a IoT button without being able to set the Lambda Trigger programmatically through the API. I can create the IoT rule and link it to the Lambda Function, but it seems I need to do the reverse as well, create a Trigger on the Lambda side, to make things actually work.\nExposing Lambda triggers in the SDK would help greatly \ud83d\ude04 . Both, I believe \ud83d\ude04 sorry, been a bit since I looked at this problem. I recall having to manually enable the IoT action from the Lambda console. With Terraform I was able to add the rule in IoT but it wouldn't work unless I added a reciprocal rule/trigger in Lambda. The amount of deletion has certainly increased since February, however on a fairly steady, gradual climb, as we got more and more tests running in TravisCI. I re-ran the failing tests and things were basically back to normal (not getting that LimitExceeded error), I'll let you know how tonights full-test-suite run goes.\n\nFrom what I know there hasn't been any updates to the service\n\nLess interested in updates and more curious about any disruption. \nDo you think this is a thing we can add to the Retryer for automatic retrying? \n. Thank you @xibz ! I'll close for now and re-open if something comes up. \n. huh... I'm not seeing it in the SDK nor the AWS CLI output (using aws rds describe-db-clusters)\n. Thanks @jasdel \ud83d\ude04 \n. Thank you!\n. Hey @xibz I apologize if I'm just not understanding. \nFor ImportKeyPair, the SDK and docs suggest I need to base64 encode the string myself:\n\nhttps://github.com/aws/aws-sdk-go/blob/master/service/lightsail/api.go#L7194-L7197\n\nso I was expecting to use something like base64.StdEncoding.EncodeToString:\n\nhttps://play.golang.org/p/j9UZsQ88WI\n\nDoing so gives me an error; sending the string unmodified succeeds. \nIn the EC2 API for RunInstances, UserData is also said to be base64 encoded, but it explicitly says that the encoding is done for you. \nIs the SDK doing the encoding and decoding for us here? . I am not modifying the string, it's formatted like so:\nssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAoHHarC545C[..keythings..]hGInw== clint@computer.com\n\nI show above that in the HTTP response, both public and private key material don't *look* to me like they've been base64 encoded \ud83e\udd14 , but I admit that I could just be mistaken. . I'm just literally using the contents of my ~/.ssh/id_rsa.pub in the uploads, the example above has been scrubbed to obscure it though. I generated them from simple ssh-keygen. . My mistake! Thanks for clarifying \ud83d\ude04 . Thanks @xibz ! I forgot to mention that I am doing this in us-west-2, in the default VPC . excellent, thank you for the fast fix!. ",
    "jvilhuber": "Seems the API is returning expected (though to me wrong) values:\nJans-MacBook-Air:~ jan_vilhuber$ aws --profile nachocove --region us-east-1 cognito-identity describe-identity --identity-id us-east-1:XXXXX\n{\n    \"LastModifiedDate\": 1421516513.393,\n    \"CreationDate\": 1421516513.393,\n    \"IdentityId\": \"us-east-1:XXXXX\"\n}\nSo it appears the go code is at fault for not being able to correctly parse datetimes.\n. Looks like this ought to do the trick:\napis/cognito-identity/2014-06-30.normal.json:\n@@ -6,6 +6,7 @@\n     \"serviceFullName\":\"Amazon Cognito Identity\",\n     \"signatureVersion\":\"v4\",\n     \"targetPrefix\":\"AWSCognitoIdentityService\",\n+    \"timestampFormat\":\"unixTimestamp\",\n     \"protocol\":\"json\"\n   },\n. see https://github.com/awslabs/aws-sdk-go/pull/85\n. Good to know. Thanks.\nJan.\nOn Feb 3, 2015 7:15 PM, \"Loren Segal\" notifications@github.com wrote:\n\nNote that in the future development happens in the develop branch, we make\nfew changes in master and aren't accepting PRs to master currently.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/awslabs/aws-sdk-go/pull/85#issuecomment-72777892.\n. \n",
    "GeneticGenesis": "I think this is a valid approach (and the reasoning is sound, and what I was expecting), and I'd say consistency with the AWS APIs here is probably more valuable than being \"go idiomatic\". Thank you for taking the time to discuss it in that much detail though.\nIts worth noting that some golang SDKs actually still populate err, but also give you back the ability to get the extra properties that you refer to. An example of this is the github SDK for golang, which has API calls which return a triplet of object, response, err - See https://godoc.org/github.com/google/go-github/github#GistsService.Get for example.\nCheers,\n. I'm happy for this to be closed.\nCheers,\n. So this works very well in practise, but I can't see any way to combine this with expectations...\nIE, I can't actually assert that the DynamoClient.Query was called, and what parameters it was called with.\nAny suggestions on achieving this?\nAlso, would you consider this bad encapsulation since it means that anyone with access to the dynamodb client can affect its behaviour?\n. Would it be acceptable to have #79 too, in the interests of allowing consumers of the AWS-SDK to decide how they want to test within their applications?\n. The primary use case would be for people who want to use tools like gomock or testify/mock rather than using library-specific approaches for changing behaviour in test scenarios.\nI'm not saying the hander approach is particularly bad, what I am saying is that lots of people are already using a standard set of tools and approaches. IMO it would be sensible for this library to allow people to continue to do that.\n. So here's an example repo I put together to explore this: https://github.com/GeneticGenesis/aws-sdk-go-mock-demos - I'd like to expand it to cover more approaches. It actually turned out to be more elegant than I was expecting. I think a lot of my concerns were around still using the type definitions from within the AWS package, but this turned out to be just fine.\nIt uses Testify and Mockery to achieve a mock of the DynamoDB Query() call. I hand generated an interface for the one call I required, and used that in my tests. I've used a compile time check to enforce that the interface I declare fits the one in DynamoDB. In practise, this compile time check is really useful, because it will compilation failures if the DynamoDB API changes, forcing me to regenerate my interface.\nI'm still not hugely comfortable with me owning this responsibility though. If I want to have a DynamoDB encapsulated in any of my structs, I have to declare it as my interface type (DynamoDBer in the case of my example), in order to be allowed to replace it with a mock at test time - I'd really prefer that to exist in one place (ideally with the real declaration), so I don't litter every usage of AWS I have with small, incomplete interfaces.\nI'd have no problem with the interfaces being in a different package at all, but yes, I'd prefer AWS to own the process of generating, versioning and releasing them. Happy to chip in on modifying #79 to change the generation location and add the interface verification as I had in my example.\nThoughts?... @sclasen?\n. ",
    "euank": "I understand the concern, but I feel having to know about a helper method to use a reader is slightly less usable.\nI think if someone's uploading large objects that aren't seekable (e.g. reading a file through gzip) they should be using multipart uploads anyways and providing an easier path to using those transparently will go a lot of the way to assuage your concern.\nI'll give an implementation that is more explicit about that tradeoff a go when I get the chance and see how it looks.\n. This is no longer really relavent with the current develop branch.\nPerhaps we should revisit this in the future, but for now it doesn't make sense to leave this open I think.\nI did do a bit more work around this, expressed in this commit.\nBest,\nEuan\n. I'd argue that there's some value in keeping it usable with the default json marshaller/unmarshaller as well unless there's some significant reason to move to the custom one you mention.\nSticking with 'json' helps the structs interop with other software more cleanly.\nMy case is certainly a very unusual one, but I'm using an alternate transport and it's convenient to be able to write these structs as json over a transport with the go std library as opposed to a custom aws marshaller.\n. Good call on the churn.\nI was already thinking about running the final files all through goimports as part of the generation process (basically a second GoFmt that only runs on fully complete files, not just fragments). This would ensure that import ordering is always consistent.\nI'd be happy to go back and improve the json tagging to be only for json as well.\nUnfortunately, I don't have the spare cycles to do this right this moment so I expect it to take a couple days (no promises though).\nI think it does still make sense to bump the generated files. \nI'm not really concerned about resolving the churn since it would happen the next time someone generated the service files anyways and it doesn't really change the readability (lack of) for this commit IMO. If you think that's a blocker I can come back to it, or if you want it sooner feel free to regenerate them appropriately yourself.\n. I would like to vendor an unmodified version/commit-hash of the sdk and I would not like to make changes within my 'Godeps' folder as part of my build if I can avoid it.\nCurrently I have a go:generate block that looks something like..\n//go:generate go build -o gen-api github.com/awslabs/aws-sdk-go/internal/model/cli/gen-api\n//go:generate ./gen-api ./api/*.json\nI'm relying on the gopath having been set by invoking 'godep go generate' or otherwise setting up the path correctly.\nI hope that clarifies my use-case a little more. If you insist, I could do something akin to what you say, but it won't be pretty.\nI'd also be willing to take another approach like an extra -flag that adds inflection files to merge if you're just concerned about this implementation.\n. I feel like it should be the concern of the gen-api tool to generate an api regardless if it's one of the ones you have filled in inflections for or not. That's why I felt my change made sense in this package.\nAll the *-core SDKs, I believe, can load an arbitrary json model that was not included in the repository with no trouble. This isn't a strictly necessary behavior and is not exercised by those SDKs either, but it's a nice feature.\nSimilarly, I think a user of this SDK being able to use the code within this package \u2014 with no alterations \u2014 on an arbitrary json model is useful, and within the scope of it, even if not strictly necessary.\nI also was not suggesting that the behavior I added should become an unchanging contract of the tool. I completely understand I'm doing something that I cannot rely on being stable, and I'm vendoring the dependency for that reason. I'm doing this expecting that it's a stop-gap to a more full-featured generator that I'll have to switch to at an unknown time in the future.\nThank you for suggesting how I fix my code simply, but that suggestion makes too many assumptions which are not necessarily true. It assumes GOPATH is a single directory, not a colon separated list (as mine often is), it assumes that I know the path to gen-api.go easily, not that it could be in the Godeps workspace (which I think should be of an opaque format anyways probably), or in the user's non-godeps GOPATH because they ran godep restore. It assumes that the aws-go-sdk's .git folder is present, which is not true within a Godeps workspace (though that's within my .git repo so it would still work I s'pose).\nThe above is part of the reason I used go build over go run; go run requires I know where the target is within the GOPATH, but I also needed to use build because go run gives you a working directory in /tmp where it would not find my added inflections file, only the sdk-included one.\nOf course, I can resolve all the above issues with no code changes in this repo if needed, but it is not quite so trivial as you suggest.\nIf you still feel that being able to work with an arbitrary json model should be outside the scope of the current gen-api tool, I'll happily close this issue and proceed to see how correct I am about the assessment of complexity for doing it in my build system :grin:\n. Thinking more about it, my last comment was petty of me. I apologize for that. I'll close this issue and just go ahead with doing it on my end.\nThanks for the discussion! I look forwards to further maturation around this and I'll work around it in the mean-time.\n. This fails to generate as is because it's missing the inflection \"Docker\".  Applying the following diff makes this successfully generate for me:\ndiff --git a/internal/model/api/inflections.csv b/internal/model/api/inflections.csv\nindex ad14ca6..78f5be0 100644\n--- a/internal/model/api/inflections.csv\n+++ b/internal/model/api/inflections.csv\n@@ -1532,3 +1532,4 @@ Color:\n Space:\n Descending:\n Charged:\n+Docker:\n. I went back and fixed the tests that were failing too, though I don't see how the tests were related to my original changes so I'm not sure why they weren't failing before.\n. Actually, on looking at it a moment more, I noticed that some of the generated code I got from fixing that test looks wrong. Specifically, I noticed InputService18TestShapeInputShape changed to InputService18TestShapeInputService18TestShapeInputShape... Looks like it might lengthen each time I run generate.\nThese test issues aren't actually related to the original commit I think. I'll look into it a little more.\n. Found the map traversal that was leading to the non-deterministic test generation I noticed.\nIf you'd rather me split the testing changes out into a different pull-request I could.\nIn addition, I noticed that #204 has a test change that might make my second commit redundant (didn't look too closely) and I'd be happy to rebase on top of his change if it does address that issue.\n. I rebased out the region change and regenerated; it wasn't really part of the scope of this change anyways.\nIt makes sense to not bind the tests to regions, but requiring a user to have the environment variable just changes where that region is dictated and makes tests less consistent between developers.\n. Any reason for preferring SettableBool or triState over just a default nil *bool?\nThe reason for triState in the cli tool, based on the comment, is that flag.Bool does not distinguish, but since this isn't a command line flag parsed with the flag package I see no reason not to just use a nillable bool. That also, imo, makes it easier for someone unfamiliar with the api otherwise to reason about / understand how to deal with the type.\n. @josharian The 'Set' method requires a pointer receiver to mutate it, and so you get an allocation for using the given struct as well, right? See playground.\nWith *bool, your editor will tell you about the type and you know you have to de-reference it to compare. You know it can be nil. With tristate, you have to know about the third state or you'll have logical errors (if true else /* false */ type).\nThat's my reasoning for why I'd prefer *bool anyways.\nEdit: I realized that you probably meant the tirState option doesn't require an alloc. Oops, my misunderstanding! I'll still argue for something I feel is more understandable and type-safe over the cost of an alloc.\n. @josharian Yeah, per my edit I realized you meant tristate while typing my first paragraph and that my comment didn't actually apply. Sorry for the confusion there :confused:\nI recognize that it's type safe, but I still am concerned about the possibility for logic errors. If I type request.Retriable. and see my editor suggest the function .Bool() bool, I'm likely to not realize that there's a third value and not handle it.\nBecause there's not a language-level construct of Maybe<T> or similar, I can't be expected to be familiar with this construct already so I think such a mistake is quite possible.\nOn the other hand, nil and pointers are core to the language and a typical Go developer will understand that a pointer might be nil, thus making logical errors far less likely.\n@jasdel The pointer requires nil checks, which I see as an argument for it, not against it. The tristate masks the third value somewhat, so you don't have to check for it, but you also might not realize you should check for it, leading to logically incorrect code.\nI do see the argument for tristate though and my opposition is only minor. \nThanks for the discussion!\n. :+1:, I ran into this same issue with query.Encode in the past as well and had the same fix.\nI actually recall trying to test if the SDK did it right then, but my methodology must have been flawed because I thought it worked. Good catch, I think, @oremj \n. Awesome, thanks!\n. I think this should be filepath.Join (which joins with os.PathSeparator) vs path.Join (which joins with '/')\n. ",
    "freeformz": "FWIW I would really prefer that aws-sdk-go provide interfaces and default implementations of those interfaces. Taking Kinesis as an example I would expect something like this:\n``` go\ntype Kinesis interface {\n  AddTagsToStream(AddTagsToStreamInput) error\n  DescribeStream(DescribeStreamInput) (*DescribeStreamOutput, error)\n  ...\n}\ntype KinesisImpl struct { ... }\nfunc (ki KinesisImpl) AddTagsToStream(req AddTagsToStreamInput) (err error) {\n  ...\n}\nfunc (ki KinesisImpl) DescribeStream(req DescribeStreamInput) (resp *DescribeStreamOutput, err error) {\n...\n}\n```\nNote: This is a deviation from the idiomatic *er interface convention in Go, but IMO it makes more sense here. But I'd be fine with the interface being called Kinesiser and the implementation being Kinesis or modulo whatever naming makes people not cringe or think of java.\nYes this means I still need to mock stuff, but for the most part that's just managing the responses of concrete response structs, but that's what I'd want anyway.\n. @kidoman Generally speaking I agree. But, every project that I've seen using this lib ends up having to re-create the interfaces at least for the purposes of testing. Maybe that's fine though and it's a mistake to try to optimize for this use case.\nIn any case this now seems moot as the owners of the lib have gone down the path of allowing users to supply a mock Endpoint (probably via httptest), which is a fine, if slightly clunky alternative, at least upon initial examination.\n. ",
    "brycefisher": "For posterity's sake, here's the current tests showing how to use the handler system:\nhttps://github.com/aws/aws-sdk-go/blob/7e05bc442d2ff1b846da71d9b913b606d743f1e3/aws/request/handlers_test.go\nNote the last test which shows how to add and remove \"named\" handlers.\n. ",
    "cristiangraz": "This works great with a single handler, but when I use multiple handlers each handler runs n times where n is the number of handlers I've added. For example with this:\n``` go\nr53 := route53.New(session.New(), nil)\nr53.Handlers.Clear()\nr53.Handlers.Build.PushBack(func(r *request.Request) {\n    log.Printf(\"first: %T\\n\", r.Params)\n})\nr53.Handlers.Build.PushBack(func(r *request.Request) {\n    log.Printf(\"second: %T\\n\", r.Params)\n})\nr53.ListResourceRecordSets(&route53.ListResourceRecordSetsInput{\n    HostedZoneId: aws.String(\"abc\"),\n})\nr53.ChangeResourceRecordSets(&route53.ChangeResourceRecordSetsInput{\n    HostedZoneId: aws.String(\"abc\"),\n    ChangeBatch:  &route53.ChangeBatch{},\n})\n```\nI get this output:\n2016/03/21 18:23:21 first: *route53.ListResourceRecordSetsInput\n2016/03/21 18:23:21 second: *route53.ListResourceRecordSetsInput\n2016/03/21 18:23:21 first: *route53.ChangeResourceRecordSetsInput\n2016/03/21 18:23:21 second: *route53.ChangeResourceRecordSetsInput\nbut I expect to get this:\n2016/03/21 18:23:21 first: *route53.ListResourceRecordSetsInput\n2016/03/21 18:23:21 second: *route53.ChangeResourceRecordSetsInput\nIf I add a third handler, it will execute each of the three functions three times even though I'm only calling it once.\nAm I misusing?\n. Thank you @lsegal and @xibz. Both replies were very helpful!\n. ",
    "xibz": "Hello @cristiangraz - Thank you for coming here and asking this. When you add handlers to the package they affect all the operations in the package. So, let's define n to be the number of handlers and m be the number of operations called. We should expect n * m messages.\nFor more clarification, you aren't calling it once. You are calling each handler every time you call an operation. I hope that makes sense. Please let me know if you have further questions.\n. @lsegal - Absolutely! Thank you for a more clarified answer.\n. Hello @cristim, that sounds like a great idea! I'll put that in our backlog so we can plan it in one of our sprints. Thank you for the feedback!\n. Hello @shatil, thank you for reaching out to us. We are doing our best to have the best examples and up to date documentation for our users. I'll raise the priority on this in our backlog. Do you have any particular issue that you are trying to test?\n. Sure, an Init bootstrapping function works there. We typically just assign them at the global level when mocking out sessions.\nFor instance,\ngo\npackage mocktest\nvar Session = session.New(&aws.Config{})\nBut either is fine. Was there something you did not like with the Init function?\n. @shatil - I think the best way is the interface.go files. Can you give me an example of what you are trying to test?\nAlso, if I understood your questions correctly, it sounds like you want to do something like this:\n``` go\ntype MockDynamoDB struct {\n    dynamodbiface.DynamoDBAPI\n}\nvar mock MockDynamoDB\n```\nNow your tests will look like this\ngo\nfunc foo() {\n    err := mock.PutItem(/* put item params */)\n    // assert err\n}\n. @shatil - Are you casting as *dynamodbiface.DynamoDBAPI? Try dynamodbiface.DynamoDBAPI.\nHere's an example:\n``` go\npackage main                                                                                                               \nimport (                                                                                                                 \n  \"fmt\"                                                                                                                    \n\"github.com/aws/aws-sdk-go/aws/session\"                                                                                \n  \"github.com/aws/aws-sdk-go/service/s3\"                                                                                 \n  \"github.com/aws/aws-sdk-go/service/s3/s3iface\"                                                                         \n)                                                                                                                          \nfunc main() {                                                                                                            \n  svc := s3.New(session.New())                                                                                           \n  ifaceClient := s3iface.S3API(svc)                                                                                      \n  fmt.Println(ifaceClient)                                                                                               \n} \n``\n. @shatil - Awesome, glad we could get that clarified for you :). Also, if you want to submit this example in theexamplesfolder as a PR, we'd be more than glad to take it! Additionally, if you have any further questions, please let us know.\n. Hello @lkv123, thank you for reaching out to us. Sometimes people will accidentally swap the keys. Are you sure you are using the right keys for the right environment variable? In addition, how are you loading your credentials and what error are you receiving? What is the Golang and SDK version you are using?. @crezam - Thank you for reaching out to us. The service that you are using uses a different v2 signer. The SDK does not support that, currently.. Hello @psankar, what I think @jasdel means by that, is to have a builder function for some sort of LockEntity that has a lock and data object on it. If you could please clarify @jasdel.\n. Hello @integrii, thank you for bringing this to our attention. Can you enable logging and paste the logs here that illustrate thenilfield whenIsTruncatedistrue`?\ngo\nl := aws.LogDebugWithHTTPBody\nsess := session.Must(session.NewSession(&aws.Config{\n    LogLevel: &l,\n})). @integrii - I was able to reproduce this on my end. Looks like using ListObjectV2, however, behaves properly. I'll reach out to the service team, but in the mean time it would probably be best to just use the newer API.\nI am going to close this as ListObjectsV2 works properly and this is a service issue. I would also reach out to the service team via forums to raise this issue with them.. @itachi3 - We have just implemented the batch feature in #1333. Please let us know if you have any questions on the feature.. Hello @geekifier, we are doing our best to ensure that customer demanded features are of highest priority. I can definitely see this being on our roadmap, but when, I don't know. I will let you know when we are looking to implement this feature.\n. Hello @csyangchen, thank you for reaching out to us. We try to leave client side validation to a minimum and rely on the services to throw detailed exceptions. The reason for the lack of client side validation is it makes backwards compatibility more difficult if the service wants to change something that we validated against. I understand it can sometimes be difficult to debug, but ensuring your code will work for many versions is something we hold at a very high standard. If you have any further questions, please let us know!\n. Fixed in v1.1.0. Let us know if any other issues or feedback regarding the change.\n. Looks good.\n. Looks fantastic\n. --reviewed by Jason\n. --reviewed by Jason\n. --reviewed by Jason\n. Closing because it was merged in release-v1.1.0\n. Hello @fl0cke, \nI believe this is related to #503. We are currently reviewing the fix and the progress can be viewed here, #526. Thank you for confirming this bug.\n. Fixed in v1.1.0. Let us know if any other issues or feedback regarding the change.\n. Yea, I think it is because Content-Md5 is an optional header, which is why I think it isn't shown, but should be. We should show how to correctly set headers like that to limit confusion. After reading the wiki, it is also ambiguous. I will go ahead and provide a more clear summary and example. Thank you again @fl0cke. \n. Thank you for fixing the example @rosenhouse\n. Looks good besides the typo, :ship: \n. Hello @chadgrant, thank you for bringing this to our attention. You can't produce this locally? I've been trying to reproduce this, but haven't been successful. What version of the SDK are you using? What does your input look like?\n. Hello @wangkuiyi, thank you for that! I narrowed it down to a library that we use. I'm currently working on a fix.\n. Hello @ChrisFernandez, thank you for bringing this to our attention. Looking at your posted code, I noticed you didn't have a parameter for DescribeStacks. Did this complain to you in the go compiler? If not, what version are you using?\nI have tried a mixture of input and cannot reproduce your error. Could you give post the top section of the stack call? \n. Naw, they weren't causing any issues.\n. Oh they weren't causing issues because of that. If I removed it, it won't run\n. Hello @kahing, thank you for reaching out to us and looking into this. Are you talking about setting the redirection property on the S3 item as the bucket to redirect to? So, this would be the correct bucket location set in the meta. Additionally then, trying to get at that information?\n. Thank you for clarifying @kahing. In this case, you'll need to write an unmarshal meta handler. Here is an example of writing your own handlers.\ngo\nr.Handlers.UnmarshalMeta.PushBack(handler)\nerr := r.Send()\n`` go\ntype xmlErrorResponse struct {\n  XMLName xml.Namexml:\"Error\"Code    stringxml:\"Code\"Message stringxml:\"Message\"Region  stringxml:\"Region\"`\n}\nfunc handler(req *request.Request) {\n  resp := &xmlErrorResponse{}\n  err := xml.NewDecoder(req.HTTPResponse.Body).Decode(resp)\n  if err != nil {\n    req.Error = awserr.New(\"SerializationError\", \"failed to decode S3 XML error response\",\n  }\nfmt.Println(resp.Region)\n  // Do whatever with region\n}\n```\nIs this what you are looking for?\n. That's a good question. I believe it is because we know S3 will always return the XMLName, Code, and Message, but unsure about Region and others. So, with that said, if users want to grab very specific data from the XML, that may only happen in one or two requests, we can provide handlers for them to grab what they know will be in them. Let us know if you have any more questions, I'll be glad to help. Thank you, @kahing, again for reaching out\n. Unfortunately, that would mean it is an actual redirect from S3. When it is an invalid bucket, it doesn't redirect but throws an error with status 400, bad request. OrigErr() returns nil, because when you pass an invalid bucket, the original error is generated by parsing S3's response and set to req.Error.\nI hope that clears up the confusion.\n. When you send that request, it returns with status 400, if a invalid region is supplied, and not a 301. S3, the server itself, is setting status code to be a 400, instead of 301. The s3 error handler looks at the status code of the response and if it is a redirect, provides the necessary error. However, since it isn't a 301, it skips it.\nHow is your connection to s3 defined? Did you specify the right region in the session? If you did, you wouldn't have gotten an error. If a user using your library calls this with a different region, it'd print the XML.\n. Sorry, I wasn't clear. I meant using the Go SDK. The Go SDK doesn't hit the URL, by default, with a region in it. It hits the bucket directly. bucketname.s3.amazonaws.com/itemname. The credentials are passed into the header under Authorization. That is where region lives. Which doesn't throw a 301, but a 400.\n. Since I assumed you weren't using S3ForcePathStyle = true, s3 only returns a basic 301 response with no information. If you set it to false, s3 will return an xml response.\ngo\nsvc := s3.New(                                                                                                           \n    session.New(),                                                                                                         \n    &aws.Config{                                                                                                           \n      Region:           aws.String(\"us-east-1\"),                                                                           \n      LogLevel:         aws.LogLevel(aws.LogDebug),                                                                                                                                                          \n    },                                                                                                                     \n  )\nThe log level will show you the request and responses. If you set path styles to true, you'll see a very basic http response.\n. I can reach out to s3 and see why they only return such basic responses for S3ForcePathStyle = true. I feel like this is a bug with S3. They should be providing a header with the right location. So I will go ahead a do that.\n. No problem! I appreciate the information. Also, I tracked down some information. Go's default HTTP client will return an error if location isn't provided in the header when a 301 occurs. So, we make a blank response object with very little info, because of the the http error when performing the request. I'll see if I can build a more verbose error that contains header info\n. So, it looks like with go, it forces the HTTP spec and if you are receiving a 301, it will always return an error if the server does not provide a location in the header. https://golang.org/src/net/http/client.go line 398. Which is why when you don't set S3ForcePathStyle to true, it works. That is because it returns a 400. So, I see two ways of getting this to work, set S3ForcePathStyle to false or see why isn't S3 providing a location on a redirect. I'll ask S3 anyways, because I am really curious to why they don't set a location, and perhaps, they should be. \n. @jasdel - Unfortunately, the Transport/Roundtripper isn't going to solve the issue. In the Golang http client, if it is a redirect, it does an explicit check for location in the response's header. If it doesn't exist, it returns an error and a nil response is returned. I think we may need to write a custom client if we want to support having header's without location for redirects.\nhttps://golang.org/src/net/http/client.go line 398\n. @kahing Awesome, and I'm glad you found a work around! Let us know if you have any additional questions or feedback. \n. Hello @paultyng, thank you for creating this. Tagged this as a feature request.\n. Thank you @lox! @paultyng - Was there something in particular that you were looking for that isn't supported? \n. @paultyng - Thank you for bringing that to our attention. I'll look into that.\n. @paultyng - We've updated the documentation. Again, thank you for bringing this to our attention. Let us know if there is anything else we can do for you. \nhttp://docs.aws.amazon.com/sdk-for-go/api/\n. Hello @harlow and @nightlyone, thank you for suggesting this! I am tagging this as a feature request. We are more than happy to look at PRs. Unfortunately, we cannot mutate the Record object directly, as this would be a breaking change. However, We can add another layer on top of the service api. This layer would have some sort of interface. We can then call an interface method to adapt the interface to the Record object. If you have any further questions, let us know.\n. Looks good! :shipit: \n. Hello @nickw444, the function, ConvertFrom*, does not support string sets. Here's an alternative solution\ngo\nfunc makeData() map[string]*dynamodb.AttributeValue {                                                                      \n  return map[string]*dynamodb.AttributeValue{                                                                              \n    \"MyField\": &dynamodb.AttributeValue{                                                                                   \n      L: []*dynamodb.AttributeValue{                                                                                       \n        {S: aws.String(\"a\")},                                                                                              \n        {S: aws.String(\"b\")},                                                                                              \n      },                                                                                                                   \n    },                                                                                                                     \n  }                                                                                                                        \n}\nThe reason for this is when we convert to a map, if the field on the struct is a slice, we default to a list attribute.  Then if we want to reverse the process, we have to also use the list attribute. Let us know if there is anything else you need!\n. Thank you for taking the time to post this PR. We investigated the usage of AWS_DEFAULT_PROFILE and only the CLI uses this value. None of the other SDKs use this. If we were to add this, it'd be a breaking change. If customers were relying on AWS_PROFILE for the SDK, but was using the AWS_DEFAULT_PROFILE for the CLI, then this, would cause their profile to change unexpectedly. AWS_DEFAULT_PROFILE is a legacy environment variable supported only by the CLI. docker/machine#3045, in this issue, if the user uses AWS_PROFILE, this issue will be removed.\n. Hello @tj, thank you for taking the time to request this feature. I will mark this as a feature request. Also, we more than welcome any PRs!\n. Yea, I am not too sure why the demand for this isn't higher either. Let us know if you need to bounce some ideas. \n. :shipit:\n. :shipit:\n. Hello @albrow, thank you for bringing this to our attention. I am going to try to reproduce this. What request are you making? Any custom handlers or http clients?\n. @albrow - I have a PR, #572, with a fix in the retry and in the core handlers, based on your recommendations. Thank you for that. If you have any more issues, please let us know.\n. @albrow Thank you for the update, and that is great to hear! \n. Hello @stack72, thank you for bringing this to our attention. I bet if you check the status, it would say the stream is updating, StreamStatus: Updating. It does take a few moments! Let us know if it still hasn't updated.\n. Hello @robbiet480, thank you for contacting us. Is your body an expected pem cert? Print out the URL to ensure that it is also correct. Looking at the RSA VerifyPKCS1v15 function, there are only two spots that returns that error. If the public key is incorrect in size or if the verification fails. If it has failed the verification, then more digging needs to be done.\n. The newline could break it, if it is included in the verifying. Any difference whatsoever in signed/signature or public key will break the verification. The SHA1WithRSA is just used to choose the correct algorithm. It has no affect on the verification besides which algorithm to choose. Have you tried including the newline to see if that corrects the problem? It may be a good idea to print the signedand newSignString as byte arrays in both PHP and Golang and see if there is any difference. Lastly, writing the same unit tests as php/js and see which tests fail. This would probably give some nice information.\n. @robbiet480, interesting that Golang is behaving that way. I am going to do some small test and see if I can figure something out. I really would like to prevent needing to use external libraries.\n. @robbiet480 Awesome! I am really curious to what the issue was. Please let us know!\n. Nvm, found it. Change this to this.\ngo\ncert.CheckSignature(x509.SHA1WithRSA, buffer.Bytes(), base64decodedsignature)\nThe CheckSignature does what you were doing. Let us know if that works! Closing this. If there are anymore questions or issues, we can reopen.\n. @robbiet480, my pleasure. Yea, I don't know if it would make sense to place this in the SDK. It may be a good idea to publish this as an external library since I can see people possibly wanting to just verify the request and not needing the SDK. \nThank you for posting detailed attempts and solutions. I can see people wanting to know how to do this!\n. Hello @Zariel, thank you for the information on the data race. I am curious to how this is being caused. I can see this happening if a goroutine is closing the request body while another might try to read from it. Can you give a little more detail on how your recursive algorithm works?\n. @Zariel - What I think is happening in the SDK is when there is a retry we fill the body with nops. This causes the race with the transport like you stated. Instead, we should create a new HTTP request and set the body from the request.Request.Body wrapped nop closer. I will first test this and then push out a fix. \n. @Zariel - We were able to reproduce what you were seeing. So, it looks like the Seek is conflicting with the transporter. What the plan is is to create a custom ReadSeeker that would lock on a Read and when we want to copy the request to send, it'd call a new Copy method. This method would use the same lock and call Seek on the body. Finally returning a new ReadSeeker for the new request.\n. @Zariel - We merged in a change that should fix the data race. If you have any more issues, please let us know.\n. @voutasaurus - Thank you for taking the time to fix the example\n. @tomwans, thank you for taking the time to do this PR. One suggestion is to add a function pointer to the WriteAtBuffer structure, GrowthFn. This could be then replace this line newBuf := make([]byte, expLen, 2*expLen) with newBuf := make([]byte, expLen, b.GrowthFn(expLen, pLen)). If the GrowthFn is not set then default to 2 * pLen to 4 * pLen. This will allow people to choose how they want to grow. The reason for this is if someone was to pass a buffer that is fairly large in size, 1.5  gigs, this would allocate a buffer of 3 gigs. Does that sound reasonable?\n. @tomwans - No worries! Also, thank you for the research.\nAnother solution could be to pass your own buffer and we just use that with some growth rate with the limit of the buffer you passed in.\nOr pass a growth rate/max size value, where growth rate of zero defaults to one.\nWhat do you think of these? The first would allow for no buffer reallocation and the ceiling would be the size of the buffer. The second would just allow for users to at least specify how fast they want their buffer to grow or cap. What do you think of these?\n. @tomwans - Thank you for the update. Changes look good.\n. Hello @jesusjjf, thank you for the feedback. There is an example in the setting up credentials portion in the wiki. After clicking examples in the wiki, there's a link to SDK Configuration.\nCurrently, I do not think there is a process to contributing to the wiki other than posting an issue in github. I am going to close this. If there is anything else, please let us know and we can reopen.\n. Added a race test for offsetReader and removed the race test in request_test.go since that was more of a functional test. Made it more clean.\n. Hello @yissachar, thank you for bringing this to our attention. I have reached out to the documentation team to see Enabled is indeed suppose to be true by default. \n. @yissachar http://docs.aws.amazon.com/ses/latest/APIReference/API_ReceiptRule.html was updated and sorry for the confusion. If you have any other issues, please let us know.\n. Hello @josh-padnick, thank you for reaching out to us. I am curious to how you are attempting to grab the verbose errors. Are you grabbing the verbose errors by calling Get on the Credentials? We do not log these errors. This just enables chaining of errors.\n. Sorry, yes, I meant Get. Updated old message to be correct. Unfortunately, we cannot have it return an error, because this would be a breaking change for people. In addition, a panic doesn't seem right here; killing someone's program for this seems a little harsh.\nWe can update the docs to be more explicit and add this to the backlog for the future. Thank you for the feedback and we'll do our best to make the use of the SDK more intuitive. \n. I've updated the github wiki to include an example of checking if credentials have been loaded properly.\nAt the bottom of Specifying Profiles. Going to close this issue. If you have any feedback or requests, please let us know.\n. LGTM, :shipit:\n. Hello @pjebs, thank you for posting your findings!\n. Hello @derekwaynecarr, thank you for reaching out to us. You can disable retries by setting MaxRetries, in the config, to 0. Going to close this issue. If you have any more questions, please let us know.\ngo\nsess := session.New(&aws.Config{\n    Region:      aws.String(\"us-west-2\"),\n    MaxRetries: aws.Int(0),\n})\nIn addition, this would apply to all requests. You can add this also to the client itself.\ngo\nsvc := ec2metadata.New(sess, &aws.Config {\n    MaxRetries: aws.Int(0),\n})\n. Hello @mdonnellyli, thank you for the PR. :+1: \n. Hello @stack72, thank you for bringing this to our attention. It looks like no SDK provides this as input. I am going to reach out to the documentation team and see what is going on.\n. @stack72 - The docs team is still looking into it! I'll post my findings once I find something out. Sorry for the inconvenience. \n. @stack72 - Yep, they were just waiting on a new version and looks like they were added to SDKs end of last week. I am going to close this and if you have any further questions, let us know!\n. @rodlogic - No worries! \n. Hello @topmuffin, thank you for reaching out to us. It does continue with u.totalSize as -1 and doesn't return an error. So, that isn't causing the error. In addition, in the errors that were logged looks like it failed at sending the Put request. I'll dig around and see what is going on.\n. LGTM, :shipit:\n. Was the BatchedErrors.OrigError() not going to concatenate the error strings? Or just return the first one in the list?\n. Should probably add a leak test to the unit tests\n. Yea, that makes sense, was just curious because it seems there would be more use in OrigErrs than in the OrigErr. So, I was thinking of concate the error messages together and returning that as a single error. However, after some thought, it may not be useful, since you can just call OrigErrs.\nYea, I agree for the leak test there wouldn't be much to test there, but was thinking of having complete coverage of the services that have custom marshalers may be nice.\n. LGTM, :shipit:\n. LGTM, :shipit:\n. Hello @radeksimko - the HasIAMInstanceProfile seems a little misleading, because it is tied to whether or not there was an error. I could definitely see a possibility of an error returning and a profile existing.This method is more of a does a profile exist in the current response, which may be true or false. I saw your discussion with @jasdel, so maybe he can weigh in some thoughts too.\nI think it may be a good idea to remove HasIAMInstanceProfile and just let the users define how they want to handle checking whether an instance profile is there. Perhaps passing the response into the function would clarify this better?\n. @radeksimko - Yes, you've highlighted my concerns. So, with that said, the name is what is misleading. I was trying to figure out whether passing the response would clarify what the method is doing or finding a better name. \nMaybe document it more like?\n// HasIAMInstanceProfile returns if the response\n// succeeds in getting an IAM profile\nAlso, I disagree with them even meaning close to the same thing. When something times out it doesn't mean it doesn't have them. It means it was unable to get them. They were unavailable. Which is why I think the Available one is fine. I hope that clears up what I was trying to say. \n. @radeksimko - Thank you, again, for taking the time to put this together! \n. LGTM :shipit:\n. Looks good, :shipit:\n. Yea, that makes sense. I'll change that\n. LGTM, :shipit:\n. Hello @pottava, thank you for taking the time to put this PR together. Just a few comments.\n. @pottava - Awesome! Thank you again for your contribution.\n. @JReuling - Thank you for taking the time to put together this PR.\nThis would imply that the convert* method support string sets. Unfortunately, they do not. We do have a a Marshaler that supports sets. \ngo\nb, err := dynamodbattribute.Marshal(&obj)\n// Code\ndynamodbattribute.Unmarshal(b, &newObj)\n. LGTM, :shipit:\n. Hello @sclasen, we do not change our retry mechanism when being throttled as you described. I will mark this as a feature request. Thank you for bringing this to our attention!\n. @sclasen - #609 resolves this. If you have any other suggestions, please let us know! Again, thank you.\n. Hello @rodzzlessa24, thank you for bringing this to our attention. I'll go ahead and see if I can reproduce it and find out what's going on.\n. @rodzzlessa24  - Change US Standard to us-east-1 and it should work. If your contents are not there try us-west-2. Let me know if that works. Going to contact the S3 team to update the comments to reflect this. Going to close this for now.\n. @jasdel - Changed a couple things you may want to look over. First I changed minTime to not be on the jitter but on the floor of the retry time. Additionally, I capped the retry at ~five minutes if throttled. Before it was 2^13 seconds, which is way too long to retry.\n. LGTM, :shipit:\n. LGTM, :shipit:\n. @v4n, thank you for the PR. The changed look good!\n. Hello @DaniloMoura1, thank you for putting this PR together. I have a comment on the changes. Golang's http. CanonicalHeaderKey only support alphanumeric and the dash. Making it way more stricter. I am open to discussing how we want to support the other characters. Any questions, please let us know!\n. @DaniloMoura1 - Fixed travis. If you could resync with master, that'd be great!\n. Hello @4ydx, thank you for reaching out to us. You can add boolean logic to your ConditionExpression. A good place to start is here, ConditionExpression. \nHave you tried something like this?\nConditionExpression: aws.String(\"attribute_not_exists(CompaniesId) AND attribute_not_exists(UtcTimestamp)\"),\nIf you have any more questions, please let us know.\n. Hello @charles-at-linknext, thank you for bringing this to our attention. Ill look into this. What version of golang are you using?\n. @charles-at-linknext - if you look in the awsmigrate/awsmigrate-renamer/vendor/ folder you will see that tools has the exact dependency that you are looking for. If you run the renamer it should import the correct tooling. Let me know if you have any additional questions. \n. @charles-at-linknext - thank you for the detailed information. I will take a look as to why Go v1.5.2 doesnt build properly. \nNo worries on the whole Go version stuff! You have pointed out an issue!\n. @charles-at-linknext - glad we were able to fix that for you. If you have any other issues, please let us know!\n. Hello @ncw, thank you for taking the time to put this PR together. I am going to reach out to S3 and see when an io.ErrUnexpectedEOF would occur and how they want that handled. Also, what are you doing to produce this error?\n. @ncw - Okay, I'll forward this to S3. \nYea, cmd/vet was deleted from tools today. \n. @ncw - Fixed travis. If you could resync with master, that'd be great!\n. @ncw - Okay, your changes make sense. Once you add the other unit test and sync with master I can merge it in\n. @ncw - Awesome! Thanks for the update! The changes look good. \n. LGTM\n. @fabiokung - Thank you for taking the time to submit this PR. Changes look good!\n. LGTM, :shipit:\n. Made the early exit change. Also, will add the test\n. LGTM! :shipit:\n. LGTM, :shipit:\n. :ship:\n. LGTM, :shipit:\n. LGTM, :ship:\n. Hello @skyleelove, thank you for reaching out to us. Here is our documentation for getting the ACL of the bucket, GetBucketAcl. In addition, for changing the ACL, PutObject. There are examples at the bottom of those operations. Please let us know if you have any issues.\n. Hello @b6g, thank you for submitting this PR. Go 1.4 doesn't support the Cancel field. It may be a good idea to separate the struct to its own file and add a build tag for 1.5+. \nGoing to run some race tests to ensure that it doesn't have any.\n. @b6g - if you could. We officially support 1.5+. However, we'd like to support 1.4 as best as we can.\n. no package in the new files.\nAlso, instead of newHTTPRequest maybe copyHTTPRequest. What do you think?\n. @b6g - Hey, I have a PR, #651, with your changes and tests that verify they work. Thank you again for taking the time to put this PR together\n. Hello @mweagle, thank you for letting us know that the documentation is lagging behind. We've looked into this and are looking to update the documentation soon.\n. @mweagle - Just updated the documentation. This should populate shortly. Thank you, again, for bringing this up! Closing this.\n. Hello @ryanbrainard, thank you for the PR! These changes look good. I'll merge them in once the checks pass\n. Hello @SergeyTsalkov, thank you for reaching out to us. I am going to try to reproduce this on my end and see what is going on. I will get back to you once I have some more information.\n. Hello @SergeyTsalkov, we currently have a PR to get this fix. Again, thank you for letting us know!\n. Hello @sstarcher - Thank you for putting this PR together. These changes look good and I will merge them in.\n. Hello @saml, thank you for bringing this to our attention. I'll take a look into this.\n. @saml - The two are comparable. In addition, I can reach out to S3 and see why they return the location url encoded. We could url encode the single part one to represent them to be the same. Would that work for you?\n. @saml - Alright! If you have any additional questions, please let us know. Again, thank you for reaching out to us!\n. Hello @Jalle19, we havent gotten around to it yet! However, we do have it in our backlogs! If you want to submit a PR for this, we are more than happy to take a look. In addition it may be a good idea to URL encode the single part upload location field. If you have any additional questions, please let us know.\n. Hello @agonzalezro, thank you for taking the time to put this PR together. This would be a breaking change for people using the CLI with the Go SDK. The best option is to just use AWS_SESSION_TOKEN.\n. @agonzalezro, yes, I think closing would be best. No problem and I appreciate the recognition! Again, thank you for taking the time to submit this PR. This further shows a demand for some union between configs.\n. LGTM, :ship:\n. LGTM, :shipit:\n. Awesome change, and it looks good. :shipit:\n. Hello @stack72, thank you for reaching out to us. It would seem if the request succeeds, then could you not wait the specified time? Let me know what concerns you have with this idea\n. @stack72, what is it that you are trying to do with the RetentionInDays? I imagine you are looking to check the state of the LogGroup, like PENDING, that way you don't have to keep track of all the numbers. Let me know if this isn't the reasoning!\n. @stack72 - Unfortunately, there is no state associated with the LogGroups. I was curious if that was more of what you were trying to do. Sorry if I wasn't clear\n. Okay, I am asking Cloudwatch if there is a way to do this. It seems the SDKs do not allow for this. So, once they get back to me, I'll update you!\n. @stack72 - CloudWatch Logs states it happens immediately. Are you doing something that has unexpected output?\n. @stack72 - Thank you again for reaching out to us. If you have any issues, please reopen so we can properly get this handled.\n. LGTM, :shipit:\n. LGTM, :shipit:\n. Hello @nicolai86, thank you for your interest in the Go SDK. I have reached out to @gaffo and I will let him tell us if there are any updates. I, too, agree that it would be great to get the v4 signer exposed. \n. Hello @nicolai86, here is the github link for the current work for the public signer. \n. Hello @spaceweasel, thank you for reaching out to us and finding this. I agree with your reasoning of switching on the time.Time struct and do the appropriate marshing/unmarshaling from there. We are more than happy to look at PRs as well!\n. Resolved via #672 \n. Hello @spaceweasel - These changes look good. Going to go ahead and merge the changes. Again, thank you for taking the time to make the changes and composing this PR.\n. Hello @simonwistow, thank you for taking the time to compose this PR. Most of the code you have modified is auto generated. I can reach out to S3 and see why they haven't included that field in their operations.\nanother way to accomplish this:\n``` go\nreq, err := svc.PutObjectRequest(&s3.PutObjectInput{\n// input field\n})\nreq.HTTPRequest.Header[\"X-Amz-Server-Side-Encryption-Context\"] = \"a3fbe\" // base64 string\nreq.Send()\n``\n. @simonwistow - Im still waiting on S3 to get back to me on theX-Amz-Server-Side-Encryption-Contextnot being there. In addition, if I don't hear word from S3, I can go ahead and add it to our models. I can then submit a code review to S3 and see what they say. Thes3managercode would be fine by itself, because that is all handwritten. So, if you want to remove thes3code and keep thes3manager, I'll relook at the PR.\n. Hello @simonwistow, sorry for the long duration. We haven't heard any word froms3as of yet. However, I've reached out to a couple people last and this week. So, once we get word back from them, I'll be sure to keep you updated! Thank you for your patience. \n. Hello @simonwistow, we just heard back from the service and it turns out the model does need to be updated with a new blob type. So, we are going to go ahead and do that, and in addition the SDKs need to be able to support blob types. So some coordination has to happen there. I'll post back here with any updates. Thank you for your patience! \n. @mwhooker - Thank you for reaching out to us. Have you tried just hittings3directly without using your library? In addition, can you add to this [line](https://github.com/wercker/mhook/blob/master/mhook.go#L369) what type of error it is? For instance, whether or not it is anawserr. Furthermore, we noticed that your remove and close is backwards, [here](https://github.com/wercker/mhook/blob/master/mhook.go#L214-L215)\n. @mwhooker - My pleasure. So this does occur even when hittings3` directly? Once we get more information, we will be able to figure out what is going on.\nThe reason I asked if it were an awserr is because Download only returns awserr. \n. @mwhooker - After looking at the s3.Download code, it would seem that this could return the error you are looking at, here. I bet it's the progressWriter. I bet it is still trying to write when it is being removed. Remove I don't think waits for the io.WriterAt to finish. So I bet you are removing the file and that causes the buffer to stop writing. Since we resolved the Remove and Close order, you shouldn't see this problem anymore.\n. @mwhooker - Thank you again for the outreach. I believe I have the answer above. I'll close this for now. Feel free to reopen if the problem arises again.\n. Reopening because I can't reproduce it with Remove or Close. It looks like this occurs when it is trying to read something into a buffer that is bigger than the actual size of the remaining data. Im not 100%, but I'll try to reproduce this on my end.\n. @mwhooker - I have been trying to reproduce this all week. I haven't been able to reproduce this error with S3. So, with that said I am leaning towards this isn't an issue with the SDK. If you can provide more information, I can definitely give some input. \n. @eldondevcg - Thank you for the information. That seems very feasible. We do have something in our backlog to deal with retrying on ErrUnexpectedEOF. I will update the issue with the information found here. Again, thank you.\n. Hello @muratsplat - Thank you for reaching out to us. I will take a look at this issue, can you supply what you are trying to do? A code example would help. Is it exactly what #185 is trying to do? And what version of Go are you using?\n. @muratsplat - I am able to use sqs.DeleteMessageBatch without any issues. Can you please provide a code snippet? \n. @muratsplat - Yes, the async methods could be causing this. Depends on how the rest of the code base has been architected, but I can definitely see some potential for the async methods to cause duplication. It seems you have found a solution though! Was there any other questions that you had?\n. Hello @muratsplat - Thank you for reaching out to us. Are you calling sqs.ReceiveMessage multiple times? If so, setting the VisibilityTimeout and then deleting may be what you want. How does that sound?\n. Hello @muratsplat, have you tried what I suggested above with setting the timeout?\n. @muratsplat - if you are getting these messages concurrently within the window of visibility being set, you will have duplicates. It may be a good idea to have one go routine get/delete the messages and send them to the other go routines. Sorry for the confusion there! If you have any additional questions please let us know! \n. LGTM, :ship:\n. LGTM, :shipit:\n. Hello @jlafon, thank you for bringing this to our attention. What version of the Go SDK are you using? The MessageId shouldn't be nil if  a message was received. I will go ahead and try to reproduce this, but I believe this isn't an issue with the SDK. If the MessageId is nil, I can forward this to SQS to see what is going on.\n. Hello @jlafon, a few more questions! Are you using any custom handlers? How large are the messages? Where are the messages coming from? Is there a VisibilityTimeout set on any of these messages? Can you provide any details on how and what you are doing for us to forward to the service team?\n. @jlafon - Thank you for the detailed information. I've sent this off the service team. I also recommend going to their forums and posting your findings there. When we hear back from them, I'll update this issue. For now, going to close this as it isn't a problem with the SDK. If you have any other information or questions, please reach out to us.\n. Hello @jlafon - Can you please provide the request id, region, and queue name of the request?\n. Hello @jlafon, have you ran into this issue recently? The service is asking for a requestID.\n. @jlafon - Okay, please let us know if you see that error again! Thank you for keeping us informed with this issue.\n. Hello @sergeyfd, thank you for reaching out to us. The Go SDK only supports AWS V4 for services except for SimpleDB. We do not support S3 V2. If you have any other questions, please let us know!\n. @sergeyfd - SigV4 is the preferred method of signing and since S3 V2 isn't supported in all regions, we have not prioritized that work. Going to close this for now. If you have any additional questions, please let us know.\n. Hello @sergeyfd, thank you for bringing this to our attention and the detailed log. I'll go ahead and try to reproduce it.\n. @sergeyfd - A current workaround is to Trim the url for now. We are looking into a fix. Again, thank you for reaching out to us.\n. Hello @sergeyfd, I have a PR #690 that addresses this issue! Going to close this for now and once merged in, if there are any additional problems, then please reach out to us. \n. Hello @tmaiaroto, thank you for bringing this to our attention. I do not believe this is an SDK issue but a service issue. I will try and reproduce this and forward this to the service team, if it is a service issue. Again, thank you!\n. @tmaiaroto - no worries! May I ask what the issue was? Havent had the chance to try to reproduce it yet, but it may be good for others who are running into the same issues. Thank you!\n. Hello @samber, thank you for asking this! This was done to allow for additional groups to be added, if needed, in the future. Sorry for the confusion there! In addition, if it were a bug, changing it would break customers, so it'd need to remain the same in either situation.\n. LGTM, :shipit:\n. Hello @insasho, thank you for the feedback. We are using java as a very basic baseline, but the overall design is our own. This is a quick prototype and will likely change. However, compatibility between SDKs encryption clients is our goal.\n. @jasdel - Do we want to move from aws-godocs to aws-godoc?\n. Hello @krishnasrinivas, thank you for bringing this to our attention. Looks like there is an error in one of our models. Forwarding to the service team to resolve!\n. Hello @celkins, thank you for bringing this to our attention. These files are maintained by the service teams. I will go ahead and forward this to Firehose. Again, thank you.\n. Hello @agonzalezro, thank you for reaching out to us. Are you using the latest SDK version? In addition, can you set the Logger to get more detailed information.\ngo\nsvc := route53.New(session.New((&aws.Config{\n/*config params*/}).WithLogLevel(aws.LogDebugWithHTTPBody))\n. @agonzalezro - I went ahead and was able to reproduce this error. It looks like it is a service side error, because it is valid xml. I will go ahead and forward this to route53.\n. @agonzalezro - After some investigation, it looks like PrivateZone is meant to only be set on the response of the request. I have clarified that this is confusing, and should be stated in documentation that this field is only settable in the response. Sorry for the confusion there. Going to close this. If you have any more questions, feel free to reach out to us again.\n. @agonzalezro - http://docs.aws.amazon.com/Route53/latest/APIReference/API_CreateHostedZone.html, looks like it is only on the response from the services documentation. The reason why it is in boto3 is because we code generate from the same models. I can do some investigation on how to create a private zone.\n. Hello @agonzalezro, looks like to set it as private you just set the vpc structure.\n``` go\n  out, err := svc.CreateHostedZone(&route53.CreateHostedZoneInput{                                                    \n    CallerReference: aws.String(name),                                                                                \n    Name:            aws.String(name),                                                                                \n    HostedZoneConfig: &route53.HostedZoneConfig{                                                                      \n      Comment: aws.String(fmt.Sprintf(\"Private%szone\", name)),                                                        \n    },                                                                                                                \n    VPC: &route53.VPC{                                                                                                \n      VPCId:     aws.String(id),                                                                          \n      VPCRegion: aws.String(region),                                                                             \n    },                                                                                                                \n  })                                                                                                                    \nfmt.Println(out, err)                                                                                               \n} \n```\nIf you have any additional questions, please let us know\n. No problem @agonzalezro. I also reached out to SQS letting them know that it is very confusing that PrivateZone can only be set on the response but is provided also on the response shapes. And thank you for reaching out to us!\n. @agonzalezro - Documentation has been updated. Thank you again for bringing this to our attention!\n. Hello @ci-iotsyst, thank you for reaching out to us. The JS library you are referring to is something IOT maintains. If you wanted to do something similar, you'd have to look at their code itself and mimic the logic. If you do make the library, it may be better to submit it as a new standalone project. In addition, it may be a good idea to browse the IOT forums for feedback. If you have any other questions, please let us know! I will reach out to IOT letting them know that there is interest in a library for Go. Going to go ahead and close this.\n. @ci-iotsyst, that makes sense. We code-generate a lot of the code for services based off of the models. IOT has not added anything as of yet to allow the SDKs to do this. However, this would be a pretty big refactor in terms of how we deal with credentials. I will reopen this and mark it as a feature request and see why we had not done that with IOT from the start.\n. @ci-iotsyst - So, it seems that there is very little reason to use the SDK on embedded object, as the binaries can be quite large and there will be very little to no use of the SDK, but only IOT's SDK. This SDK is the SDK that IOT maintains So, with that said, does the alternative of submitting a project to IOT seem reasonable?\n. Yes, IOT is one of AWS' services. In addition, I have expressed your concerns in a message for IOT to get feedback. For now, I am leaving this as a feature request. I definitely understand why you would want to be able to have IOTs custom credential interface. So, if you have any additional feedback or questions, please let us know\n. Hello @bfosberry, thank you for reaching out to us. MessageId should never be nil. We have brought this issue with SQS before. I am waiting for word back from them. In addition, do you have the RequestId and Region? Any additional information for SQS will help them find out what is going on.\n. @bfosberry, I understand your concern! I agree that letting panics happen is bad practice. Let me write a mock functional test and see the best course of action is.\n. Hello @bfosberry, have you ran into this issue recently? I am curious because the service is asking for a requestID.\n. Hello @joe2far, thank you for taking the time to put this PR together. The changes look good!\n. LGTM, :shipit:\n. LGTM :shipit:\n. Hello @raphtheb thank you for reaching out to us. The SDK does not do validation on max. That error is being returned from the service, ELB. In addition, there is no current way to get the current max amount from the ELB service. Why are you looking to do this? Here is the ELB forums. Others may have had the same concerns and ELB will be able to give a response, while making them aware of the demand for this operation! If you have any more questions, please let us know.\n. @raphtheb, awesome news! I wonder if it is natural text that you have to parse. I imagine that if the service ever changes it, it could fail pretty badly. I do not know if that'd be a good place for it in the SDK, but maybe a wrapper in your service may be a better place. Perhaps @jasdel has a different opinion on whether that it would make sense to have in the SDK.\nThe reason why you are getting strange errors is because ELB, the service, has default max set to 20. You may have to change that limit by submitting a limit increase. That way you don't get those errors from the service.\n. @raphtheb - Really? Interesting. That sounds like a bug on the service's end! I now understand your confusion! Do you have any information on the request that failed? Such as region and requestId\n. Hello @raphtheb - I have gotten some feedback from the service and they'd like to know what API calls are you making that is producing this error?\n. That is great work on your side @raphtheb! Thank you for your patience! In addition, if you have any additional questions, please reach out to us!\n. Hello @rasky, thank you for reaching out to us. We have Validate handlers that allow for users to do their own custom validation!\ngo\nreq, out := svc.PutObjectRequest(&s3.PutObjectInput{\n// Parameters here\n})\nreq.Handlers.Validate.PushBack(func(r *request.Request) {\n    // ValidRegions is a custom array of valid regions\n    for _, region := range ValidRegions {\n        if r.Config.Region == region {\n            return\n        }\n    }\n    r.Error = awserr.New(\"ErrInvalidRegion\", \"Invalid region: \" + r.Config.Region, nil)\n})\nerr := req.Send()\nIf you have any additional questions, please let us know!\n. @rasky - We do have them defined for each service, but they are not in an array or list. For instance in S3 if you look at the constants you'll see some defined like BucketLocationConstraintUsWest2. For now, you'll need to add those into an array. If you have any additional questions, please let us know!\n. Hello @Nosajool, thank you for taking the time to compose this PR. These changes look good!\n. Hello @matthew-andrews, thank you for bringing this to our attention. I cannot reproduce this. I've tried creating a bucket in eu-west-1 and still it succeeds. What Go and SDK versions are you using?\n. Hello @matthew-andrews, I've reached out to s3 to further investigate this issue. Please let us know if there is anything else we can do for you.\n. Hello @tmichel, thank you for the feedback. We do have an internal util package under private/util. I think moving it there may be fine. We would be glad to take a look at your PR! If you have any additional questions, please let us know.\n. Hello @ChristianLohmann, it looks like the Unmarshal method is the culprit here. I wouldn't say it is a memory leak, but it definitely is a memory hog. So, we've went ahead and added it to our backlogs to fix the memory consumption of that method. I'll leave this as a feature request for now. If you have any additional questions, please go ahead and reach out to us.\n. @tmichel - Thank you for taking the time to compose this PR. These changes look great! Merging\n. Hello @jabalsad, thank you for the feedback. The issue you linked is tracking this, and in addition, we are always willing to take a look at PRs! \n. Hello @AnvilStriker, thank you for the feedback. The information that was removed looks like it was removed from the previous docs as well. We grab that information from the comments from the function. I will take a look to see where that information went.\nedit: Looks like our old doc generator used a custom template for specific operations. We will go ahead and add that information to the newer docs. If you have any additional feedback on how we can improve these docs please let us know!\n. Hello @AnvilStriker, just wanted to reach back out and see if you've run into any additional issues with the SDK's documentation. We are always looking to improve, so please follow up with us and let us know how we're doing!. LGTM, :shipit:\n. LGTM, :shipit:\n. LGTM, :ship:\n. LGTM, :ship:\n. LGTM, :ship:\n. Hello @jafalas, thank you for reaching out to us! The buildCanonicalHeaders uses the blacklist of ignoredHeaders. The requiredSignedHeaders is only used for hoisting. So, it should be signed. Is there some sort of behaviour that you are seeing that is not signing that header?\n. :ship:\n. Changes look good. :ship:\n. Hello @jcapron, I've went ahead and created a new issue marked as a feature request to better track this. #1204. If you have any more feedback about this, please post them there!. @simonwistow @tolidano - Thank you for following up on this issue. We currently have this in our backlog and I'll bring this up next sprint.. @seiffert - No problem and thank you for finding this! \n. Hello @stack72, thank you for reaching out to us. I am going to go ahead and try to reproduce this. In the meantime can you give us any information regarding the request, like request id and region?\n. @stack72 - Can we get a full debug log of the request with including the requestID? Thanks!\n. @stack72 - No worries there! This is what we are here for. Please let us know if you have any additional questions. Thank you\n. Hello @mpmlj, thank you for reaching reaching out to us. What are you doing to marshal and unmarshal this object?\n. Awesome, thank you for the extra information. \nI do see what you are talking about. It looks like if we are decoding an object, we will always expect it to be in time.RFC3339 format and this is because that is what json expects it to be. You are absolutely right that we should document that! \n. PR #762 to document this\n. Hello @mpmlj, thank you for reaching out to us. This does look like a bug. I'll try to reproduce this and then see what is going on internally in the SDK/service.\n. Hello, @mpmlj - it looks like I cannot reproduce this error.\n``` go\n  put, err := svc.PutItem(&dynamodb.PutItemInput{                                                                        \n    TableName: aws.String(\"TestTable\"),                                                                                  \n    Item: map[string]*dynamodb.AttributeValue{                                                                           \n      \"Test.Item\": &dynamodb.AttributeValue{                                                                             \n        N: aws.String(\"1\"),                                                                                              \n      },                                                                                                                 \n      \"TestKey\": &dynamodb.AttributeValue{                                                                               \n        S: aws.String(\"Foo\"),                                                                                            \n      },                                                                                                                 \n    },                                                                                                                   \n  })                                                                                                                     \n  fmt.Println(put, err)                                                                                                    \nout, err := svc.UpdateItem(&dynamodb.UpdateItemInput{                                                                  \n    TableName: aws.String(\"TestTable\"),                                                                                  \n    Key: map[string]dynamodb.AttributeValue{                                                                            \n      \"TestKey\": &dynamodb.AttributeValue{                                                                               \n        S: aws.String(\"Foo\"),                                                                                            \n      },                                                                                                                 \n    },                                                                                                                   \n    UpdateExpression: aws.String(\"SET #ATTR = #ATTR + :val\"),                                                            \n    ExpressionAttributeValues: map[string]dynamodb.AttributeValue{                                                      \n      \":val\": &dynamodb.AttributeValue{                                                                                  \n        N: aws.String(\"1\"),                                                                                              \n      },                                                                                                                 \n    },                                                                                                                   \n    ExpressionAttributeNames: map[string]*string{                                                                        \n      \"#ATTR\": aws.String(\"Test.Item\"),                                                                                  \n    },                                                                                                                   \n  }) \n```\nDoes your code look something like this? In addition, can you provide the requestID? If you call the operation UpdateItemRequest and look at the request after calling Send, you'll be able to get the requestID.\n. @mpmlj - Awesome job finding this! And no worries with taking our time, that's why we are here! If you have any more questions, please let us know. \n. Awesome, thanks for the great feedback @jasdel. I'll go ahead and update this when I get the time.\n. Looks good with the few comments I have. \n. @jasdel - Looks pretty good! I have added few comments.\n. LGTM, :shipit:\n. Hello @gonber, thank you for reaching out to us! It seems like the object's were put without the type being set. The best way is to do what you're doing. Just place that in a helper method and you should be good!\n. Hello @gonber, if you are still having issues, please reach out to us. I am going to go ahead and close this for now.\n. @gonber - The issue here doesn't seem to be an SDK problem, but a problem with uploading the objects without a type. The type was never set when putting the objects in s3, but should have been set, based on how you are getting the objects. I hope that makes sense, or maybe I am misunderstanding something here. If I am, please expand!\n. Hello @gonber, thank you for clarifying. What's weird is when I call PutObjectAcl followed by GetObjectAcl, I get the type. So, I am curious to see how you are calling PutObjectAcl.\nIn addition, adding this to GetObjectAcl would be a breaking change. The SDKs represents the service APIs over the wire and this util function is best placed above the SDK.\n. Oh! That makes way more sense! @gonber, in that case that's a console issue. What I suggest is clicking the feedback button at the bottom left corner in the console. They should be able to address the issue there!\n. @jasdel - were you adding more tests? Or is this PR ready to be reviewed?\n. LGTM, :shipit:\n. Hello @xmwilldo, thank you for reaching out to us. The policy document is a JSON string representing the policy. Here is an example in the IAM documentation of what a policy document would look like. It is at the very bottom. If you have any additional questions, please let us know!\n. I am going to go ahead and close this @xmwilldo. If you have any further questions, please let us know.\n. Hello @Sjeanpierre, thank you for reaching out to us and taking the initiative to solve this issue. I have looked at your PR and had a couple comments. \n. Hello @Sjeanpierre, thank you for taking the time to put this together! The more documentation the better! I have a couple comments, but other than that, it looks good.\n. Hello @Sjeanpierre, thank you again for taking the time to put this PR together. I've went ahead and made one small change to the PR and resubmitted, #773. Again, thank you! If you have any more examples, we won't mind taking a look!\n. Hello @krish7919, thank you for reaching out to us. We are looking into the best solutions for the SDK to implement. Again, thank you! If you have any additional comments or questions, please let us know.\n. hello @krish7919, I've created a feature request #798 to address this issue. Im going to go ahead and close this. If you have any additional comments or questions, please see the other issue. Thank you for bringing this to our attention!\n. LGTM, :shipit:\n. Hello @d-smith, thank you for reaching out to us. This would be an excellent feature! Currently the services do not model that explicitly. I am going to mark this as a feature request and decide if it would better for us to implement a model for this or the services. Thank you again!\n. LGTM, :shipit:\n. Hello @gracedo, thank you for reaching out to us. Have you tried using Hive's JDBC driver to create a jar and just add a custom jar with that? \n. :shipit:\n. Hello @kayaklee, thank you for reaching out to us. Can you provide a sample of what you're doing? In addition, are you getting an error? What is the part size of the object?\n. Hello @MathieuMailhos, thank you for reaching out to us. There are some documentation in the above comments in the api.go file. Also the api reference has some documentation about the operation. This was just updated. If you have any other questions, please let us know.\n. LGTM, :shipit:\n. Hello @MathieuMailhos, thank you for reaching out to us. This seems it may be a question with the JWT api. I'll take a crack at this. It seems once you Decode you can generate the signingString by calling SigningString on the token. Let me know if that works.\n. Have you tried passing the key as a []byte instead of string?\n. Oh, you just need to make sure the key you are passing in is a *rsa.PublicKey. If it isn't a pointer, it'll fail\n. @MathieuMailhos - Looking into this some more. I am wondering when you parse the token, does your token raw contain a .? Because if you look at the rsa_test.go file, it does a split on the . of the token value, then the two parts is the signingSignature. \nAnd if you look further, the SigningSignature and SignedSignature join the parts with .\n. Hello @MathieuMailhos, I am going to go ahead and close this, since this isn't related to this SDK. I think the better option here is to submit an issue with them asking for assistance, since I don't have too much knowledge of that API. Thanks again for reaching out to us! And if you have any other questions, please let us know!\n. LGTM, :shipit:\n. LGTM, :shipit:\n. @RoarkeRandall, thank you for reaching out to us. If you do not create an empty MessageAttributes, it'll not error. We do not send empty or nil data over the wire.\nMeaning if something is set to nil we ignore it. However, if a list or map is used and it is empty, we will through an error. If you have any additional questions, please let us know!\n. LGTM, :shipit:\n. Hello @shwarzes89, can you strip out the APIs and only use the SDK, and then try to reproduce this?  I'd suggest using the svc := sns.New(sess, &aws.Config{LogLevel: aws.LogLevel(aws.LogDebug)}) to get more verbose errors. I would also stub any data you've been using like the /wassup/76.\n. @5k3105 - The Go SDK has only supported v4. #400 and #291 has some, I believe, implementing the different v2 signing protocols or references one. Also, you can take a look at the v2 spec.\nI'm going to go ahead and close this. If you have any questions regarding the spec, please let us know!\n. Hello @xoraes, thank you for reaching out to us. Can you provide some example code of what it is that you are trying to do? From the looks of it, couldn't you just use a regular time.Time object then call time.Time.Unix() on it to get unix time, instead of storing it?\n. Where is the jsonStr coming from?\n. I see, thanks for the information, @xoraes. Yes, I believe this is a bug. I'll mark it as service API for now, until I fully test it for verification.\n. @xoraes - the issue there is that would be a breaking change if it were added to the StreamRecord structure. We are always willing to take a look at PRs and really encourage them, but we need to make sure we aren't breaking existing customers using the feature. Since no one can use this feature, that change may be fine, except we autogenerate most of the services code.\n. @xoraes, it looks like the reason why this is failing there is a tag that defines it as unix and we handle that internally. So, if you use json marshallers, it will throw an error. So your wrapper for now is the best option. However, if we have autogenerated marshallers, I definitely can see that solving this issue. I'll mark this as a feature request for autogenerated marshallers.\n. Hello @harlow, the current solution is to wrap the object and provide a UnmarshalJSON method on it. We have this marked as a feature request and it is something we want to implement.  If you have any additional questions, please reach out to us.. Hello @junlong-gao, thank you for reaching out to us. This is a customization of the CLI and I think this would also be a nice feature for the Go SDK to have as well. I will mark this as a feature request.\n. Hello @krishnasrinivas - We have placed the test cases under the Apache 2.0 license. So, if you download the file, you should see the LICENSE file in there.\n. Hello @stack72, thank you for reaching out to us. This sounds like a service feature request. I would suggest reaching out to them on their forums! I am going to go ahead and close this. If you have any additional questions or features for us, please let us know!\n. @seiffert - Thank you for reaching out to us and taking the time to submit the PR! I'll go ahead and review it when I have some time. \n. Thank you, again, for taking the time to compose this PR!\n. Hello @catsby, thank you for reaching out to us! From what I know there hasn't been any updates to the service. Let me reach out to them and see. \n. @catsby - In addition, has the amount of deletion occurring increased since February?\n. @catsby - It looks like IAM was having some issues this morning and late last night. I believe things should be fine now, but keep me posted.\nIf it does happen again, we can think about the best course of action \n. Hello @voutasaurus, thank you for reaching out to us. I'll try to reproduce this and see what's going on.\n. @voutasaurus - Can you run the operation with svc := s3.New(sess, &aws.Config{LogLevel: aws.LogLevel(aws.LogDebug | aws.LogDebugWithHTTPBody)})? \nWhen I try to reproduce this, I get NotFound when calling err.(awserr.Error).Code().\nWhat version of the SDK are you running?\n. @voutasaurus - Yea, I remembered fixing something related to errors recently. So, I am going to go ahead and close this. If you have any additional questions, please reach out to us. And again, thank you!\n. Hello @kfir-stratoscale, thank you for reaching out to us. I will take a look into this. \n. @kfir-stratoscale - What version of the SDK are you using? I have just tried to reproduce this and cannot. What are you doing that is causing an error?\n. @kfir-stratoscale - I see what you're talking about. I will mark this as a feature request. Thank you again for reaching out to us to improve the SDK! If you have any additional questions or feedback, please reach out.\n. Hello @liamjbennett, thank you for reaching out to us. It would be best to post this to the service team's forum. Anything you see in the service folder is service defined. The only thing that is not is customizations. I also think this would be a great feature to have! \nIm going to go ahead and close this. If you have any more questions, please reach out to us!\nAlso, if you think there is something we can do to accomplish this from the SDKs point of view, feel free to reopen!\n. reviewed by @trevorrowe\n. Hello @djdenv, I see you have already commented on the PR, and thank you for finding this. I will get this merged in as soon as I get it reviewed.\n. @djdenv - Just merged the changes. Please let us know if you run into any additional issues. Thank you again!\n. Hello @liamjbennett, thank you for reaching out to us. Each service owns their operations. I would suggest reaching out to the forums about adding the new operation to the SDKs. I am a little confused by the PR that you linked. What is it the reasoning that you are blocked? It seems the service does not provide a way, other than the console, to register a directory. Also, are you talking about registering a domain or directories? \nIf you are talking about domains, then you probably have the wrong service. Maybe route53domains?\n. @liamjbennett - I am going to go ahead and close this since this is a service specific question. If the service does release the operation, it'll be automatically generated into the SDK. If you do have any additional questions or feedback, feel free to reach back out to us! \n. Looks good. Thank you again @utilum for taking the time to improve the SDK!\n. Hello @yusinto, thank you for reaching out to us. This looks like to be a service specific question. Here is their forums to which they may be better suited for this question. Pasting that log in their forums will give them the necessary information to figure out what is going on! Again, thank you, and if you have any additional questions or feedback, please reach out.\n. Hello @dhubler, thank you for reaching out to us. Those are service specific SDKs, the JS and Java SDK, and currently are only supported by those two SDKs. This would be a great feature and I will mark this as a feature request.\n. Hello @tcolgate, thank you for reaching out to us and taking the time to compose this PR. This is not an SDK issue. It looks like the models, to which we generate our code from, has been incorrectly entered. I have reached out to the service team to fix.\n. @tcolgate - I've created a new issue, #845, to monitor this. The service team is aware of the bug. Thank you again for notifying us of this bug!\n. Hello @allspace, thank you for reaching out to us. You can set the part size to whatever you want, as long as it is bigger than 5mb. So, there is only that limitation and that is by the service, not us. If the part size is less than 5mb, we will fall back to a single part upload.\nWe use the io.ReadSeeker to get the file size by seeking to the end and back. I hope that clears things up. If you have any additional questions, don't hesitate to ask!\n. Helloo @eikenb, thank you for reaching out to us and bringing this to our attention. I currently have a PR that addresses this. If you have any other additional questions or concerns, please reach out!\n. Hello @ItsRanveer, thank you for taking the time to correct this. Everything looks good. I'll go ahead and merge this in.\n. Hello @ljfranklin, thank you for reaching out to us. Please rerun your request with LogLevel set to LogDebugWithHTTPBody\ngo\nsvc := s3.New(session.New((&aws.Config{}).\n    WithLogLevel(aws.LogDebugWithRequestErrors | aws.LogDebugWithHTTPBody),\n))\nThis will help us a little bit see what is going on.\n. Hello @ljfranklin, thank you for providing the log. Nothing looks suspicious there. Can you also provide a code snippet? If that also looks fine, I'll go ahead and try to reproduce this.\n. -- nvm -- This isn't what I thought it was.\n. @ljfranklin  - after taking a look, I don't think the recommended use of sign will work. Can you also set WithLogLevel(aws.LogDebugWithRequestErrors | aws.LogDebugWithHTTPBody). to WithLogLevel(aws.LogDebugWithRequestErrors | aws.LogDebugWithHTTPBody | aws.LogDebugWithSigning)?\n. Hello @cunnie and @ljfranklin, I've reached out to the service team and asked them how the SDKs are suppose to sign requests. The logs are showing the host is changing. That makes sense, but becomes an issue with signing. Thank you for your guys' cooperation!\n. Hello @cunnie @ljfranklin, can you guys provide your DistributionID? \nIn addition, they are saying the setting of the endpoint is happening in the wrong place.\nTry this,\n``` go\npackage main                                                                                                               \nimport (                                                                                                                 \n  \"fmt\"                                                                                                                  \n  \"net/url\"                                                                                                              \n  \"strings\"                                                                                                                \n\"github.com/aws/aws-sdk-go/aws\"                                                                                        \n  \"github.com/aws/aws-sdk-go/aws/credentials\"                                                                            \n  \"github.com/aws/aws-sdk-go/aws/request\"                                                                                \n  \"github.com/aws/aws-sdk-go/aws/session\"                                                                                \n  \"github.com/aws/aws-sdk-go/service/s3\"                                                                                 \n)                                                                                                                          \nfunc main() {                                                                                                            \n  awsConfig := &aws.Config{                                                                                              \n    Region:      aws.String(\"us-east-1\"),                                                                                \n    Credentials: credentials.NewEnvCredentials(),                                                                        \n  }                                                                                                                        \nsess := session.New(awsConfig)                                                                                         \n  client := s3.New(sess, awsConfig)                                                                                        \nreq, out := client.PutObjectRequest(&s3.PutObjectInput{                                                                \n    Bucket: aws.String(\"dy5fxXX7XXXX\"),                                                                                  \n    Key:    aws.String(\"foo\"),                                                                                           \n    Body:   strings.NewReader(\"bar\"),                                                                                    \n  })                                                                                                                       \nvar err error\n  // Signing has already occurred, now we should change the endpoint.                                                                                                          \n  req.Handlers.Send.PushFront(func(r *request.Request) {                                                                 \n    req.HTTPRequest.URL, err = url.Parse(\"https://cloudfront.net\" + r.Operation.HTTPPath)                                \n    if err != nil {                                                                                                      \n      panic(err)                                                                                                         \n    }                                                                                                                    \n  })                                                                                                                       \nerr = req.Send()                                                                                                       \n  if err != nil {                                                                                                        \n    panic(err.Error())                                                                                                   \n  }                                                                                                                      \n  fmt.Printf(\"%#v\", out)                                                                                                 \n}         \n```\n. @ljfranklin and @cunnie  - So I got this working and had no issues. I hacked this together fairly quickly to test. If you guys have any other issues, please let me know.\n``` go\npackage main                                                                                                            \nimport (                                                                                                              \n  \"fmt\"                                                                                                               \n  \"strings\"                                                                                                             \n\"github.com/aws/aws-sdk-go/aws\"                                                                                     \n  \"github.com/aws/aws-sdk-go/aws/credentials\"                                                                         \n  \"github.com/aws/aws-sdk-go/aws/request\"                                                                             \n  \"github.com/aws/aws-sdk-go/aws/session\"                                                                             \n  \"github.com/aws/aws-sdk-go/service/s3\"                                                                              \n)                                                                                                                       \nfunc main() {                                                                                                         \n  awsConfig := (&aws.Config{                                                                                          \n    Region:           aws.String(\"us-west-2\"),                                                                        \n    Credentials:      credentials.NewEnvCredentials(),                                                                \n    S3ForcePathStyle: aws.Bool(false),                                                                                \n  }).WithLogLevel(aws.LogDebugWithSigning)                                                                              \nsess := session.New(awsConfig)                                                                                      \n  client := s3.New(sess, awsConfig)                                                                                     \nid := \"DISTRUBTION ID\"                                                                                              \n  req, out := client.PutObjectRequest(&s3.PutObjectInput{                                                             \n    Bucket: aws.String(id),                                                                                           \n    Key:    aws.String(\"foo\"),                                                                                        \n    Body:   strings.NewReader(\"bar\"),                                                                                 \n  })                                                                                                                    \nvar err error                                                                                                       \n  // Signing has already occurred, now we should change the endpoint.                                                 \n  req.Handlers.Send.PushFront(func(r *request.Request) {                                                              \n    if req.HTTPRequest.URL.Opaque != \"\" {                                                                             \n      fmt.Println(req.HTTPRequest.URL.Opaque)                                                                         \n      fmt.Println(req.HTTPRequest.URL.RawQuery)                                                                       \n      urlParts := strings.SplitAfter(req.HTTPRequest.URL.Opaque, \"/\")\n      // Pull off query parameters to append to the new URL                                               \n      query := \"\"                                                                                                     \n      size := len(urlParts)                                                                                           \n      for i := 3; i < size; i++ {                                                                                     \n        query += urlParts[i]                                                                                          \n      }                                                                                                               \n      req.HTTPRequest.URL.Opaque = fmt.Sprintf(\"//%s.cloudfront.net/%s\", id, query)                                   \n      req.HTTPRequest.URL.Host = fmt.Sprintf(\"%s.cloudfront.net\", id)                                                 \n    } else {                                                                                                          \n      req.HTTPRequest.URL.Host = fmt.Sprintf(\"%s.cloudfront.net\", id)                                                 \n    }                                                                                                                 \n  })                                                                                                                    \nerr = req.Send()                                                                                                    \n  if err != nil {                                                                                                     \n    panic(err.Error())                                                                                                \n  }                                                                                                                   \n  fmt.Printf(\"%#v\", out)                                                                                              \n}\n``\n. @ljfranklin - The reason why we don't use theEndpointhere is because that is signed with the request. Since we want to forward this request tos3it needs to be thes3endpoint when signing. However, by the time we need to send it it needs thecloudfrontendpoint. Hence the change of the endpoint in theSend` handler. \nYour settings look fine. What does your bucket permissions look like?\n. @ljfranklin - Can you use a different bucket with full permissions for everyone? Let me know if this works.\n. @ljfranklin - Yea, I agree that signing it anonymously solves this. Here is some information on permissions. Take a look at the granting permissions to cloudfront. It looks like credentials and permissions behave differently with the service!\n. Hello @ljfranklin, I'll go ahead and reach out to the service team with this question. Once I have an answer I'll post back here!\n. So, according to the service team:\n\"you could acheive this with two distributions pointing at the same origin. one allows read-only methods for downloading and no trusted signers. the other allows uploads but requires authentication. \"\nDoes this fit your use case?\n. Hello @erikswanson, thank you for the time to reach out to us. I need to take a look at the package. In addition, I can see what we can do. Until then, I will go ahead and mark this as a feature request.\n. @erikswanson, the service team has just got back to me, and they currently have no plans on integrating with the SDK, because they believe it is a strictly different use case. However, they did say if customers have a specific use case where both are needed, they would do what is best for the customers. Please let me know what the use case is, and I can get back to them.\n. @erikswanson - I am going to go ahead and close this. If you are interested in getting this merged into the SDKs please let us know what the use cases are, so we can forward that to the service team! Thank you again for reaching out to us! And please let us know if you have any additional questions\n. Hello @radeksimko, thank you for reaching out to us! I will reach out to RDS and see when they plan on releasing this feature. Please hold tight! If you have any additional questions, you can always reach back out.\n. Hello @bigkraig, @catsby, and @radeksimko, we just released v1.4.10 which contains this change. Closing this issue and thank you all for waiting patiently!\n. Hello @allspace, the SDK does not have copy operations in multipart upload. I will go ahead and mark this as a feature request since I do see that Ruby has this feature. Also, did you receive this idea from this SDK?\nAlso, how big are the objects you are trying to concatenate?\n. @allspace, yes I see UploadPartCopy. I thought you were trying to use the s3manager. \nI got it working. Check the example below.\n``` go\npackage main                                                                                                               \nimport (                                                                                                                 \n  \"fmt\"                                                                                                                  \n  \"net/url\"                                                                                                              \n  \"strings\"                                                                                                                \n\"github.com/aws/aws-sdk-go/aws\"                                                                                        \n  \"github.com/aws/aws-sdk-go/aws/session\"                                                                                \n  \"github.com/aws/aws-sdk-go/service/s3\"                                                                                 \n)                                                                                                                          \nvar client *s3.S3                                                                                                        \nvar bucket = \"testBucket\"                                                                                                  \n// concatenate will contenate key1's object to key2's object under the key testKey                                                                                                                \nfunc concatenate(key1, key2 string, uploadID string) (string, *string) { \n  // The first part to be uploaded which is represented as part number 1                                              \n  foo, err := client.UploadPartCopy(&s3.UploadPartCopyInput{                                                             \n    Bucket:     aws.String(bucket),                                                                                      \n    CopySource: aws.String(url.QueryEscape(bucket + \"/\" + key1)),                                                        \n    PartNumber: aws.Int64(1),                                                                                            \n    Key:        aws.String(\"testKey\"),                                                                                   \n    UploadId:   uploadID,                                                                                                \n  })                                                                                                                     \n  if err != nil {                                                                                                        \n    panic(err.Error())                                                                                                   \n  }                                                                                                                        \n// The second part that is going to be appended to the newly created testKey\n  // object.                                                                                                                       \n  bar, err := client.UploadPartCopy(&s3.UploadPartCopyInput{                                                             \n    Bucket:     aws.String(bucket),                                                                                      \n    CopySource: aws.String(url.QueryEscape(bucket + \"/\" + key2)),                                                        \n    PartNumber: aws.Int64(2),                                                                                            \n    Key:        aws.String(\"testKey\"),                                                                                   \n    UploadId:   uploadID,                                                                                                \n  })                                                                                                                     \n  if err != nil {                                                                                                        \n    panic(err.Error())                                                                                                   \n  }     \n  // The ETags are needed to complete the process                                                                                                               \n  return foo.CopyPartResult.ETag, bar.CopyPartResult.ETag                                                                \n}                                                                                                                          \nfunc main() {                                                                                                            \n  awsConfig := &aws.Config{                                                                                              \n    Region: aws.String(\"us-west-2\"),                                                                                     \n  }                                                                                                                        \nsess := session.New(awsConfig)                                                                                         \n  client = s3.New(sess, awsConfig)                                                                                         \n// We let the service know that we want to do a multipart upload                                                                                                            \n  output, err := client.CreateMultipartUpload(&s3.CreateMultipartUploadInput{                                            \n    Bucket: &bucket,                                                                                                     \n    Key:    aws.String(\"testKey\"),                                                                                       \n  })                                                                                                                     \n  if err != nil {                                                                                                        \n    panic(err.Error())                                                                                                   \n  } \nfoo, bar := concatenate(\"foo\", \"bar\", output.UploadId)                                                                   \n// We finally complete the multipart upload.                                                                                                                        \n  _, err = client.CompleteMultipartUpload(&s3.CompleteMultipartUploadInput{                                              \n    Bucket:   &bucket,                                                                                                   \n    Key:      aws.String(\"testKey\"),                                                                                     \n    UploadId: output.UploadId,                                                                                           \n    MultipartUpload: &s3.CompletedMultipartUpload{                                                                       \n      Parts: []*s3.CompletedPart{                                                                                        \n        &s3.CompletedPart{                                                                                               \n          ETag:       foo,                                                                                               \n          PartNumber: aws.Int64(1),                                                                                      \n        },                                                                                                               \n        &s3.CompletedPart{                                                                                               \n          ETag:       bar,                                                                                               \n          PartNumber: aws.Int64(2),                                                                                      \n        },                                                                                                               \n      },                                                                                                                 \n    },                                                                                                                   \n  })                                                                                                                     \n  if err != nil {                                                                                                        \n    panic(err)                                                                                                           \n  }                                                                                                                      \n}\n``\n. Hello @allspace, if you are still having issues or have additional questions pertaining to this, please let us know!\n. No worries, closed! Thank you again @allspace for reaching out to us. If you have any additional questions, please reach out to us again.\n. @allspace - This example just uses A + B = C. However, you can concatenate A to B. Just renametestKeyto whatever.\n. Hello @LBailly, thank you for reaching out to us. I believe thex-amz-meta-is correct. Why do you need the header to bex-amz-`?\n. @LBailly - It looks like this needs to manually be passed in as a header.\ngo\nparams := &s3.PutObjectInput{\n       Bucket:             aws.String(bucket),\n       Key:                aws.String(objectKey),\n}\nreq, out := svc.PutObjectRequest(params)\nreq.HTTPRequest.Header.Set(\"x-amz-website-redirect-location\", location)\nerr := req.Send()\nIf you have any additional question, please reach out!\n. @LBailly - Perfect! That is the best solution :). Let us know if you need any additional help or have any questions. \n. Hello @andrewarrow, thank you for reaching out to us. There is no way to do this in one operation call. The best way would be to upload the new object to the service, then concatenate the two objects together. Here is an example of concatenation, #832.\nFurther, reaching out to the service team's forums and asking for this feature may be a great idea. I think this could be very useful for multiple customers.\n. No problem, and if you have any additional questions or feedback, please reach out to us!\n. Fixed in release v1.4.14\n. LGTM, :shipit:\n. Hello @oremj, thank you for taking the time to compose this PR. These changes look good! I'll go ahead and merge this.\n. Hello @iscofield, the AWS_DEFAULT_PROFILE is only read if AWS_SDK_LOAD_CONFIG is set. This environment variable only pertains to the shared config file though.\nAre you trying to use the SDKs shared config? If you aren't, and only using ~/.aws/credentials, then setting AWS_PROFILE is probably what you are looking for.\n. @iscofield - If you are only using the credentials file, use AWS_PROFILE, you shouldn't need any additional intervention or configuration. Additionally, the CLI and Boto is the only SDK that support AWS_DEFAULT_PROFILE.\nWhat fits your usecase? Is it the credentials file only or the shared configuration environment variables?\n. @iscofield - It looks like there was some confusion on what was being said here. Does the merged PR #854 address your issue?\n. Hello @Ehekatl, thank you for reaching out to us. string should work. Mind posting an updated snippet? I have not been able to reproduce this on my end.\n. Hello @mbowBC, thank you for reaching out to us. This would be great functionality for the SDK. I will mark this as a feature request. If you have any additional questions, please reach out to us again!\n. @mbowBC - Just to clarify, are you looking for a CLI tool that uses Go or a helper function within the SDK like the s3manager or an extension of the s3manager?\n. @mbowBC @colinmutter - We have just implemented this feature in #1333. Please let us know if you have any questions on the feature.. Hello @stack72, I'll reach out to the service team and let them know. Thank you for reaching out to us with this find. \n. Hello @stack72, sorry for the delayed response and thank you for being patient. It looks like the service team had just gotten back to us. According to them, the docs states:\n\"Important \nYou cannot create more than one customer gateway with the same VPN type, IP address, and BGP ASN parameter values. If you run an identical request more than one time, the first request creates the customer gateway, and subsequent requests return information about the existing customer gateway. The subsequent requests do not create new customer gateway resources.\".\nFurther, the console uses some validation of input to guide customers in choosing correct values. Lastly, the service team states that this the behavior is safe to use and is idempotent.\nIf you have any additional questions @stack72, please let us know!\n. Hello @MichaelLiZhou, thank you for reaching out to us. I'll take a look and see if I can reproduce this.\n. So, I copied your code snippet above and was not able to reproduce it. I received the list of Lambda functions as expected. \nCan you log the request with debugging and post the output here?\ngo\nsvc := lambda.New(session.New((&aws.Config{\n    LogLevel: aws.LogLevel(aws.LogDebugWithHTTPBody | aws.LogDebugWithSigning),\n})\n. Can you also provide the request log output? Also, can you also provide how you create the client and any configurations?\n. I am going to forward this off to the service team. I cannot reproduce it with your region or at all. \nBefore I do that, how many functions are you trying to list? \nWhat is the request ID of the failed request? \nAre these requests being retried at all? \nCan you write a simple program that only hits the service to see if it reproduces the same error?\n. @MichaelLiZhou - In the logged request, there is some information that could help us debug this issue.\n``` go\n---[ CANONICAL STRING  ]-----------------------------\nGET\n/2015-03-31/functions/\nhost:lambda.us-east-1.amazonaws.com\nx-amz-date:20160929T165134Z\nhost;x-amz-date\ne3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n---[ STRING TO SIGN ]--------------------------------\nAWS4-HMAC-SHA256\n20160929T165134Z\n20160929/us-east-1/lambda/aws4_request\n385684da0a9a08fc6c2da4fd37f9f979f36b7f6a938e45bb195fceb891b84d20\n```\nIt looks like that. Could you please provide that portion of the log? In addition, can you check for clock skew? This can be done by looking at the date in the log above and then compare it to your machine's clock.\n. LGTM, :shipit:\n. Hello @keiths-osc and @coopernurse, thank you for reaching out to us. We have taken a look at this and see where the issue is arising from. I think the fix requires us to wrap the request.Body as an offsetReader in an internal Request field. So after the build handler, we can set up this field. Then when we get to copyHTTPRequest for retries, we can set Body to this internal field. Additionally, we are more than happy to take PRs! \n. @keiths-osc and @coopernurse, PR #871 should resolve this! If you have any additional questions, please let us know!\n. Hello @MikeMangialardi, thank you for reaching out to us. I will take a look at this and see what is going on. Thank you for providing some code to test with!\n. @MikeMangialardi, it looks like in net/http, the WriteBody method will always close the body. It looks like os.File cannot have multiple calls to Close. I don't believe this is an issue with the SDK, but a side effect of the net/http's transport.\n. @MikeMangialardi - After reading my response, it made it seem like we weren't going to do anything about it. Yes, with the PR #871, this should resolve this issue. Sorry for the unclear response. It wasn't my intention!\n. Hello @calv, you can add it through the request.Handlers.\n``` go\nreq, out := svc.GetCallerIdentityRequest(&sts.GetCallerIdentityInput{})\nreq.Handlers.Send.PushFront(sendHandler)\nerr := req.Send()\n```\nLet me know if that solves your issue, as I, too, cannot reproduce this.\n. The response in your example is the out variable in the above example.\n. Try the req.Handlers.Build.PushFront instead of Send. Let me know if that works.\n. Awesome, thanks for the code snippet @cam-stitt. \nThat should do the same as the above code snippet @calvn. Hopefully this resolves your issue!\n. @calvn - Can you post your code snippet that you're using and the output? \n. @calvn - using the wrong client with svc. You have to push to the ec2metadata client handler list.\n``` go\nfunc IdentityHandler(w http.ResponseWriter, r *http.Request) {\n    sess, err := session.NewSession()\n    if err != nil {\n        log.Println(\"failed to create session,\", err)\n        w.WriteHeader(http.StatusInternalServerError)\n        return\n    }\n// code modified here\nmetadata := ec2metadata.New(sess,aws.NewConfig().WithLogLevel(aws.LogDebugWithHTTPBody))\nmetadata.Handlers.Send.PushFront(sendHandler)\n// end of modification\nsess.Config.Credentials = ec2rolecreds.NewCredentialsWithClient(metadata)\nsvc := sts.New(sess)\n\n\nresp, err := svc.GetCallerIdentity(nil)\nif err != nil {\n    log.Println(err.Error())\n    w.WriteHeader(http.StatusInternalServerError)\n    json.NewEncoder(w).Encode(&ErrorResponse{err.Error()})\n    return\n}\n\nresponse := IdentityResponse{\n    Identity: resp,\n}\njson.NewEncoder(w).Encode(response)\nreturn\n\n}\nfunc sendHandler(r *request.Request) {\n    log.Printf(\"request path: %s\", r.HTTPRequest.URL.Path)\n    r.HTTPRequest.URL.Path = fmt.Sprintf(\"%s/\", r.HTTPRequest.URL.Path)\n}\n```\nLet me know if that works!\n. @calvn - I am trying to reproduce this within docker. I am making the assumption that this docker instance is on ec2. Is this correct? Also, did you follow these instructions?\n. @calvn - Thank you for providing all this information. I will continue to try and reproduce this and see what the issue is.\n. @calvn - Awesome job figuring it out! If you have any additional questions, please let us know.\n. :shipit:\n. LGTM\n. Thanks for the feedback @jasdel. Made a lot of changes which had cleaned up the code quite a bit. Ill update the PR when I get a chance.\n. LGTM, :shipit:\n. hello @danehammer, thank you for reaching out to us. What do you mean you can't build the package? The // +build codegen is used for code genning the SDK. Shouldn't affect your builds.. edit: Nvm, I see you are including private/* in your godeps. See above @jasdel's comment. LGTM, :shipit:\n. Hello @refaelos, thank you for reaching out to us. I believe this is a Go and OSX Sierra issue. I would suggest posting this issue in Golang. Here is a similar issue.\n. Thank you @rickard-von-essen for taking the time to do this. Your changes look good!\n. LGTM, :shipit:\n. @jferrer21 - Thank you for reaching out to us. I believe there is something wrong with your environment. I am not sure what it is, but looking at Go 1.7.3's source, I see that the http client has a Timeout field.\nIf you set up a new Go workspace and run go get -u github.com/aws/aws-sdk-go. Do any errors occur? Additionally, from there can you make a few calls to see if the SDK is working properly?\n. @jferrer21 - Try running \nbash\ngo get -u github.com/aws/aws-sdk-go/aws/...\ngo get -u github.com/aws/aws-sdk-go/service/...\nand see if that properly installs the missing dependencies.\n. I just ran that exact command in a new workspace and I am not running into any issues. It installed all the dependencies properly.\nAlso, you shouldn't have to modify your workspace. You should just run those two commands and it should properly grab everything.\nDid it at least install the dependencies in the vendor folder?\n. Yea, I am using 1.7.3.  Looking at your error means there's something up with your go installation. http.Client is from golang's standard library, net/http.\nWhen you run go version what do you get? Also is it possible you have more than one go version installed?\n. Awesome, that is great to hear @jferrer21! Please let us know if you have any additional questions or feedback.\n. LGTM, :shipit:\n. Hello @andrefsp, thank you for taking the time to compose this PR. This was modeled after encoding/json. The use of UseNumber is behaving properly. If you are looking to unmarshal directly into a typed field, then creating a structure with that type should give the desired results. I'm going to go ahead and close this, as this is behaving as intended.\n. Hello @ejcx, thank you for reaching out to us. I am unable to reproduce this.\nHere is the code snippet I used,\n``` go\npackage main                                                                                                            \nimport (                                                                                                              \n  \"fmt\"                                                                                                                 \n\"github.com/aws/aws-sdk-go/aws\"                                                                                     \n  \"github.com/aws/aws-sdk-go/aws/session\"                                                                             \n  \"github.com/aws/aws-sdk-go/service/s3\"                                                                              \n)                                                                                                                       \nfunc main() {                                                                                                         \n  sess := session.New()                                                                                               \n  config := &aws.Config{                                                                                              \n    Region:   aws.String(\"us-west-2\"),                                                                                \n    LogLevel: aws.LogLevel(aws.LogDebugWithHTTPBody),                                                                 \n  }                                                                                                                   \n  svc := s3.New(sess, config)                                                                                         \n  out, err := svc.GetBucketAcl(&s3.GetBucketAclInput{                                                                 \n    Bucket: aws.String(\"bucket\"),                                                                              \n  })                                                                                                                  \n  fmt.Println(out, err)                                                                                               \n}\n```\nCan you post your snippet?\n. Hello @ejcx, I've put together a PR, #954, that should fix this. If you have any additional feedback or questions, please reach out to us.. Hello @ejcx, thank you for reaching out to us.Canonical User should be CanonicalUser. If you have any additional questions, please reach out to us.\n. @ejcx - Yes, you can find it in the service's docs under constants. The Grantee is a enum that is named Type. So, if you search for that, you should find it. I'll go ahead and close this. If you have any additional questions, please reach out to us.\n. LGTM, :shipit:\n. LGTM, :shipit:\n. Hello @xtudouh, thank you for reaching out to us. I'm going to try and reproduce this and see what is going on.\n. @xtudouh - I think the issue here is we aren't using Opaque or RawPath when it should be used. Even though you can't do this in the console, it is still a valid request. I do appreciate your feedback and the information you found. \n. @xtudouh - I know the console does some validation of the input to help guide customers in correct values. I can go ahead and ask the service team and, as well, as the console team to see what the preferred behavior should be. That way we can clear up this confusion and have consistency. \n. @xtudouh - So, it looks like the console will create empty folders, like //foo//bar. So, this seems to be a bug in the Go SDK as we are the odd duck in this.\n. @xtudouh - I have created a PR, #935, with a fix. Please let us know if you see any issues!\n. Hello @aarthi184, thank you for reaching out to us. I'll go ahead and mark this as a feature request. I don't know in terms of time, but we are more than happy to go over designs or look over PRs!\n. Hello @shatil, thank you for taking the time to do this! We really appreciate the PR. It looks like travis failed due to linting errors. If you can fix that, I wont mind merging this in!\n. @shatil - The build tag needs to be at the very top of the file, and that should fix that problem.\n. Awesome, @shatil. Thank you for taking the time to do this. Changes look good. Merging it in!\n. LGTM, :shipit:\n. Hello @jwdeitch, thank you for reaching out to us and taking the time to submit this PR. This code is autogenerated. I can, however, reach out to the service team to add some better documentation of these fields. If you have any questions, please reach out to us!\n. LGTM, :shipit:\n. Hello @fsenart, thank you for taking the time to compose this PR. While I can definitely see why someone would want to do this, I could also see users also wanting to view the binary data. \nI think having an additional function that prints this condensed version may be another way of doing this without removing the binary that may be useful to some.\n. Since buffer already does this, this seems to be the preferred choice. Ill go ahead and merge this. Again, thank you for taking the time to compose the PR!\n. Hello @tbarbugli, thank you for reaching out to us. Can you provide some code so I could give this a test?\nAre you using the s3manager? Or just using PutObject?\n. Hello @mbh621, I have a PR, #981, out that addresses this issue. Please let us know if you have any more issues!. Hello @ThomasAlxDmy, thank you for reaching out to us. I am wondering if a presign URL may be a better fit. Have you explored this option?\nIn addition, what are you needing from ACL specifically?. In the documentation above you still need to use credentials:\n{\"x-amz-credential\": \"AKIAIOSFODNN7EXAMPLE/20151229/us-east-1/s3/aws4_request\"},\nSo, I'm not too sure if that is what you are looking for or maybe I'm misunderstanding something here? \nThe above documentation can be done with the PutObject operation using the AccessControlPolicy field in the SDK.. @ThomasAlxDmy - I see. How about AssumeRole? This gives you everything you need. It will generate a secret key, but that secret key will be for a given operation. Additionally, you can put an duration that that role is assumed for.\nHere is the documentation for that.. Hello @ThomasAlxDmy, our signer does not support the documented feature listed above. We will mark this as a feature request. Also, we are more than happy to look at PR and/or discuss designs.\nThis change will need to bypass most of the sigv4 signing with building the canonical string and just set the string to sign as the base64 encoded policy.. Hello @sangameshb15, thank you for reaching out to us. In your s3.New you need to pass a session.\ns3.New(session.New(cfg)). Hello @xring, thank you for reaching out to us. The LocationConstraint is an enum. Here is the documentation for that operation. If you look at the LocationConstraint field it will show all valid values. This can also be found in services/s3/api.go file. Here is the documentation for the list of enums that are supported for LocationConstraint.. Oh, now I understand the confusion. Yea, that service is actually a global service. So, the bucket names must be unique for all regions. The reason why a region must be specified when getting data from that bucket is that it is an optimization on the service's end to try and get that data as close to your customers as possible.\nThere is a bug currently in the SDK where we do not follow the service's redirect. We currently have that in our backlog to fix.\nDoes that clear up the confusion?. Hello @GarethLewin, thank you for reaching out to us. Unfortunately now that it is done like that, we cannot add it to aws due to circular dependencies. The proper place should have been aws.\nI don't think this is the most optimal solution, but we could create another Retryer interface in a different package. It'll copy the Retryer in the request package. This would at least bypass the circular dependency.. Hello @GarethLewin, I am going to go ahead and close this. If you still have any questions regarding this topic, please reach back out.. Hello @seanchann, thank you for reaching out to us. I believe what you want to use is the dynamodbattribute.Marshal instead of the json marshaler. Let me know if you run into any more problems with that!. Hello @seanchann, thank you for reaching out to us. Can I have a code snippet of what you are trying to do?. not_contain is not a valid filter expression. Here is a list of them. Your filter should probably be \"NOT contains(<some expression>)\". Going to close this. If you have any additional issues, please let us know . Hello @seanchann, thank you for reaching out to us. Take a look at this. This should give you some guidance on how to use the Scan operation.\nIf you have any additional questions, please reach out to us again.. Looks like this needs the latest endpoints file. LGTM, :shipit:. LGTM, :shipit:. Hello @tjchu, thank you for reaching out to us. You can set the MD5 in the the request handlers. This can be done through the client.\ngo\nsvc.Handlers.Build.PushBack(buildFn)\nuploader := s3manager.NewUploader(cfg, func (uploader *s3manager.Uploader) {\n    uploader.S3 = svc\n})\nIm curious to why you want to be able to set the Content-MD5 though. If you are using the uploader and it is splitting your payload into parts, each one will have a different md5.\nPlease let us know if you have any additional issues.. You can do that with PutObject, but if you are looking to do concurrent uploading, then md5 with our multipart upload is a little more tricky.\n```go\nreq, out := svc.PutObjectRequest(&s3.PutObjectInput {\n    // input fields\n})\nreq.HTTPRequest.Header.Set(\"Content-MD5\", md5Value)\nerr := req.Send()\n```\nI think you are talking about this. This has the content-MD5 that you mentioned, and not the PutObject. \nCurrently that isn't apart of the Uploader, because if the Content-MD5 fails, that would mean we'd have to roll back or return an error. I feel this is far more complicated and I think it is better for this to live a layer above the SDK. For instance, you can wrap the io.Reader that is used in the Uploader to calculate the running md5 and use that to check.\n```go\ntype md5Reader struct {\n    md5 hash.Hash\n    body io.Reader // assumes that this is concurrent safe\n}\nfunc (reader *md5Reader) Read(b []byte) (int, error) {\n     n, err := reader.body.Read(b)\n     if err != nil {\n          return n, err\n     }\n     return md5.Write(b[:n])\n}\n// in some func where uploader is called\nif reader.md5.Sum(nil) != md5Value {\n     // Handle by either rolling back or returning error\n}\n``\nLet me know if this helps!. LGTM, :shipit:. Hello @tejasmanohar, thank you for reaching out to us. Currently, you have to inspecterr.Error()`. We definitely want to model errors at some point, which would allow for type comparisons. I'll mark this as a feature request and a duplication, since this has been asked before.\nIf you have any additional questions and/or feed, please let us know!. Yes, that method will return the original error. Missed the SerializationError part. So, yea, that should suffice! Ill go ahead and close this then. If you have any additional questions, please let us know!. Hello @EricRobert, thank you for reaching out to us. We cannot change the default behavior as that will break customers.\nHowever, we are open to suggestions on how to make this smarter. Mind you, changing something in the protocol, will affect all services. Not just s3. We hold backwards compatibility as a very important goal, because of that, we opted with this as the best solution.. Hello @gsg822, thank you for reaching out to us. I believe this may be an issue with the service team. I've used older version of the SDK and still run into the issue. Let me reach out to them and see what is going on.. Hello @gsg822, taking a look further, and it turns out that some of the enums have required fields. For your Group type you need to specify a URI instead of an ID.\nGoing to go ahead and close this issue. If you have any more questions, please feel free to reach back out.. @gsg822, I've created a PR with a README.md that will, hopefully, clarify the PutObjectAcl operation. Please let me know what you think! #979 . Hello @dtan4, thank you for reaching out to us and taking the time to compose this PR. This looks good to merge in.. Hello @malekascha, thank you for reaching out to us. In your NewSigner method you need to set the Logger.\ngo\nsess, err := session.NewSession()\n  if(err != nil){\n    return err\n  }\n  newSigner := v4.NewSigner(credentials.NewStaticCredentials(\"KEY\", \"SECRET KEY\", \"\"), func (signer *v4.Signer){\n    signer.Logger = req.Config.Logger   \n    signer.Debug = aws.LogDebugWithSigning\n  })\n  signer = newSigner\nLet me know if that solves your issue!. Hello @catsby, thank you for reaching out to us. Correct me if I am wrong, but the value in the PublicKeyBase64 is base64 encoded, no? So, I believe you do not need to do any of the encoding yourself. Hence why sending the unmodified string results in a success. Please let me know if that clears up the confusion.. Thank you for the clarification. We do not base64 encode/decode that field. I am wondering if that documentation is wrong. Let me reach out to the service team to get their input. Is the cert already base64 encoded?. The above response is base64 encoded or at least looks like it.\nssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAoHHarC545C[..keythings..]hGInw== clint@computer.com\nThis also looks base64 encoded. The ssh-rsa is not base64 encode, but the value seems to be, AAAAB3....\nI think we just have poor documentation on the service's end. I'll reach out to them and let them know they should have a more detailed explanation of their fields and operations.. @catsby in the ssh-rsa spec, https://tools.ietf.org/html/rfc4716, it states that the body is base64 encoded. So, you shouldn't need to do anything if you are generating the keys with ssh-keygen. I am going to go ahead and close this since this is documented in the spec. If you have any further questions, please let us know!. LGTM, :shipit:. Hello @voutasaurus, thank you for reaching out to us. I'll definitely take a look at 1.8 to see if it is behaving funny. Looking at the 1.8 milestone, I don't see anything that sticks out to why it would fail to send requests.. LGTM, :shipit:. \ud83d\udc4d  :shipit: . looks good minus the one print. :shipit:. Hello @aultokped, thank you for reaching out to us. Can you provide a code snippet of this behavior with logging turned on and the output?\ngo\n  svc := dynamodb.New(session.New((&aws.Config{                                                                            \n    Region: aws.String(\"us-west-2\"),                                                                                       \n  }).WithLogLevel(aws.LogDebugWithHTTPBody | aws.LogDebug | aws.LogDebugWithSigning))). @aultokped, thank you for providing those logs. It looks like this may be a service issue. I've reached out to them. Once I know something, I'll post back.. @aultokped - According to the service team, ItemCollectionMetrics will only be returned if the table has one or more local secondary indexes, and deleted items were in one or more of these indexes such that deleting the items from the base table would propagate a delete to indexes. \nDoes your table satisfy this?. @aultokped, according to the service team to use BatchWriteItem, the table must have at least one local secondary index. So, I think the solution is to do just that, setting secondary index on the table.\nIf you have any additional questions, please reach out to us. I will go ahead and close this.. Hello @nonexu, can you provide the logs and some description pasted inline, rather than the tar?\nOnce I have that information, I can see if anything sticks out to me.\nIf it is quite large, you can use gist.. @nonexu, it seems that may be a check. In addition, the service returns all records that were used with put under Records.\nRecords contain error information. So even if the response was a 200 ok, it could still fail to upload individual records it looks like. Inspecting that field may give more context to why a Put would have failed.\nHere is the documentation. Let us know if that helps!. @nonexu, that is great! Merry Christmas and Happy New Year to you as well!. :shipit: . :shipit:. I think the autoscaling is correct. I believe it is just isn't using the correct SigningRegion.. Ohh okay, thanks for the clarification.. \ud83d\udea2 . Hello @MatTarantini, thank you for reaching out to us. With Golang dropping support for 1.4, it has made it difficult for us to keep support for 1.4. This is due to a few reasons with one of them being issues with HTTP2 and earlier version of Golang. I think locking to the SDK's version 1.6.2 is a temporary solution, but finding a way to upgrade golang would probably be the best solution.\nCould setting the docker file to install from tar be an option?\nWhy is it that you can't lock to a specific version? I am trying to better understand the limitations.. Yes, I think the solution you proposed may be the best bet, as far as I know at least. I have reached out to the service team letting them know of our issues with Golang 1.4 and have asked them if they have any solutions to provide existing customers, using 1.4, an easier transition to later version of Go. I will go ahead and close this issue. I'll update this issue once I get word back from the service team.. The service team has gotten back to me and are aware of this issue. The solution they provided is to use the native golang container. If you have any further questions, please reach out to us.. Hello @alasdairnicol, thank you for reaching out to us. What version of the SDK are you using? In addition, can you provide a small code sample that is failing for you? I have tried to reproduce this, but haven't been able to.. Yea, after reading the issue, I was able to figure out what you were trying to do. I am still trying to reproduce this.\nWe support AssumeRole on the shared config. Going to dig around a little more to see if I can replicate this.. According to the python team, they also do not support assuming instance profiles. Here is their issue to track that. Since there is extra work to support that, the Go SDK also doesn't support it currently.\nI will mark this as a feature request. If you have any additional questions and/or feedback, please feel free to reach out to us.. Hello @smugcloud, thank you for reaching out to us. This was a bug that has been addressed in later version of the SDK. Can you try updating to latest and see if this resolves the issue?. @tmaiaroto, interesting, we haven't changed anything regarding requests in the last week or so. I wonder if this is a service issue. I'll reach out to them and see if anything looks suspicious. Are you also using the service/apigateway package?. Awesome @smugcloud @tmaiaroto, great to hear! If you guys have any additional questions, please let us know.. @rabbbit - Are you still running into the issue? From what I remember this was on the service's side and was not related to #874.. @rabbbit - Could please enable HTTP2 debugging? This can be enabled by using GODEBUG=http2debug=2. @rabbbit - Looks like the RST_STREAM is being set which is canceling that stream. I am curious if this would be resolved by updating Golang. I know the language made several changes to the HTTP2 library since then. Please let us know if this resolves the issue.. @rabbbit - There was a bug regarding HTTP2 in an earlier version of the SDK. I noticed you were using a very early version of the SDK. So, updating that would be a good idea as well.. Hello @mmacdermaid, thank you for reaching out to us. It looks like something funky is going on the service side. Can you enable logging to see what exactly is being sent back?\ngo\nuploader.S3 := s3.New(session.New((&aws.Config{                                                                            \n    // Config params                                                                                    \n  }).WithLogLevel(aws.LogDebugWithHTTPBody | aws.LogDebug | aws.LogDebugWithSigning)))\n. @mmacdermaid, I agree that you definitely don't want to post that sensitive information here. However, all we need is the bit that pertains to the panic. I would then also remove any sensitive information in the logging and then post it here.\nWhat I need minimally is:\n RequestID of the failed response\n Failed response headers and body\nWith that I think we can at least diagnose what is going on.. Okay @mmacdermaid, can you also see if the body contains the correct data? Like missing any fields. That panic is a result of a field not being populated. So my guess is, perhaps, the body may be empty.. Sorry, I was talking about the body on the response is missing from the service.\nWe need something to narrow the search of possibilities.\nSo, you got the output for the panic, could you not just look at the last thing that way logged?\nIf this isn't an option, perhaps a custom logger to log to a buffer and clear the buffer when a new request is started by using the build handler. If a panic occurs, write to a file. Does that sound feasible?\nPlease let me know\n. @mmacdermaid , instead of enabling logging, you can use the handlers.\ngo\n  req.Handlers.Send.PushBack(func(r *request.Request) {                                                                    \n    dumpedBody, err := httputil.DumpResponse(r.HTTPResponse, true)                                                         \n    if err != nil {                                                                                                        \n      fmt.Println(\"Error:\", err)\n      return                                                                           \n    }                                                                                                                      \n    fmt.Println(string(dumpedBody))                                                                                        \n  })\nLet me know, if that helps!. Hello @mmacdermaid, I am going to go ahead and close this. If you are still having issues, please feel free to reopen this. Cheers!. Hello @disq, thank you for reaching out to us. Is this pretty consistent? Enabling logs may give us a better idea to what is going on.\ngo\nuploader.S3 := s3.New(session.New((&aws.Config{                                                                            \n    // Config params                                                                                    \n  }).WithLogLevel(aws.LogDebugWithHTTPBody | aws.LogDebug | aws.LogDebugWithSigning))). Hello @donileo, thank you for reaching out to us. I cannot reproduce this on my end. What Golang and SDK version are you using?\nAnd what does your code look like? Are these requests concurrent? I believe this is failing on the client being nil, if you are on latest. . @donileo - The output is nil when passed into NewRequest, but is only used to set the Data field , which we do again in the lines below. We can move that above and get rid of the setting of Data below the NewRequest call. That shouldn't be causing the panic though. \nPlease keep us updated!. @donileo - I agree completely. #1025 addresses this and does exactly what you suggested.. Hello @AA33, thank you for reaching out to us. I was just writing some examples for service/sqs and did not run into this issue. Can you enable logging and post the logs back here? That'll at least give us an idea of where it is breaking.\ngo\nsvc := sqs.New(session.New((&aws.Config{                                                                            \n    // Config params                                                                                    \n  }).WithLogLevel(aws.LogDebugWithHTTPBody | aws.LogDebug | aws.LogDebugWithSigning)))\n. @AA33, the serialization error is masking another error. The service isn't returning a proper error response, hence the serialization error. I'll reach out to the service team with the logs and see what is going on.. @AA33 - Glad you were able to get that problem sorted out. The issue is the error is a little cryptic, so I went ahead and contacted the service team and had let them know that we need to provide better error messages to our users.\nIf you have any additional questions or feedback, please let us know!. The only issue I have is how massive service/ec2 is in the docs. WIth all the setters and operations, it is massive. It is just as massive before, but better organized.\nNeed to think of a way to be able to better explore such massive amounts of operations and/or structs.. Hello @stack72, thank you for reaching out to us. Yes, the SDK currently does not support that. Looking at the customization in the CLI, I can see it is really only two calls and it choses which one(s) to call based off the args passed in. First being SetVisibleToAllUsers and the second being SetTerminationProtection. So, I am wondering if it would make sense to just make those calls instead of implementing a customization.\nPlease let me know on whether making those calls will help!. Hello @shawnps, thanks again for taking the time to fix this. I've already reached out to the service team to have this fixed! Cheers and please reach out with any additional fixes and/or corrections.. Hello @shawnps, thank you for taking the time to correct this. The api.go files are auto-generated. So, this is a typo on the service team's model. I'll reach out to them to get that fixed. . Hello @morissette, thank you for reaching out to us and reporting this. Have you tested what is the proper scope? If not, I can take a look. Either way, it looks like something is incorrect with the endpoints.json file. I'll reach out to the service team to see what is going on.. @morissette - The endpoints file now contain the necessary changes. If you have any further issues or questions please reach out to us!. @morissette, thank you for taking the time to correct this. This file, however, is autogenerated by the services. I've asked the service team to correct this. If you have any further questions or feedback, please let us know.. Sorry, I was really unclear there. The endpoints.json file is also generated. This is generated by the service teams. I have notified the service team to make the necessary fix to include your change.\nIf you have any further questions or feedback, please let us know!. Hello @chrisledet, thank you for taking the time to compose this PR. Those comments are correct. There is the credentials file, .aws/credentials, or the shared config file, .aws/config. In the shared config you can specify your credentials there if you choose. Please let me know if that helps clarify things!  . Hello @graham-ring, thank you for taking the time to do this. Can you add a test to test the addition of this feature?. Awesome, thank you @graham-ring. These changes look good.. Hello @nonexu, thank you for reaching out to us. Just looking at your small snippet, are you only sleeping if there is an error? How often do you make a call to the service if there isnt an error?\nI am wondering if you are making too many calls too quickly. Try sleeping at the end of the loop to see if that fixes it. And try some different sleep durations. Let me know if that helps any.. Hello @nonexu, I am going to go ahead and close this. If you are still having issues with this, please let us know.. @nonexu - We log immediately when we have the body, IF you set the logger to log the body. Perhaps, profiling using pprof to see where the bottleneck is occurring may give us a better insight to what is going on.. @nonexu , I was wondering about those times. Those are sent back from the service. \nAdding your own timer at the end of the send handler to check if the service is sending the correct time back would be at least let us know it isn't the SDK. If it isn't, I would mention that in the ticket. \ngo\nreq, out := svc.GetRecordsRequest(&kinesis.GetRecordsInput{\n// values here\n})\nreq.Handlers.Send.PushBack(func(r *request.Request) {\n    fmt.Println(\"Time:\", time.Now().UTC())\n})\nIf the times are correct, I'd profile next.\nHere is a good blog that shows you how to profile your go applications.\nPlease let me know if you have any issues with the profiling and/or with the snippet above.. @nonexu - Looks like the service is returning the incorrect time. I would mention that in the ticket.  I believe this is a service issue, but I think we should leave this issue open until it is solved. Please let us know if you need anything and/or information.. @nonexu - The service team just got back to me and is asking for the instance id of the ec2 instance that this is occurring on. Can you please provide that?. @nonexu - Thank you for getting that to us. I've relayed this information to the service team. If you look at #301, the solution that other customer provided was to implement an HTTP timeout of 1 minute with retries. That may be a good solution for now, while we investigate what is going on.. @nonexu - Additionally, how long does it take, typically, to experience a long delay?. @nonexu - The service team is looking into to it, but requires some information. I'll let them know how often this occurs. Thank you for providing the necessary information and no worries on the delayed response!. @nonexu - I have asked them to check. I will let you know when they get back to me.. Hello @nonexu, the service team just got back to me. So, it looks like it is what we suspected earlier. I am unsure if the sleep I suggested in the loop is enough, but that instance was running at 100% CPU. I would profile and observe the instance to see what is causing the high CPU utilization.. @nonexu - Sorry, I think I may have been unclear here. The issue here isn't the service or SDK, but how it is being used. I suggested a sleep would help. If it hadn't, I suggest profiling to see why this your code base is utilizing 100% CPU. Please let us know if there is any more that we can do.. @nonexu - Yea, that is suspicious to why it only affects certain instances. Without being able to see all the instances and how they are configured, won't lead me down any conclusions. I suggest looking into why the spike to 100% CPU usage and that may give more context to why it affect some instances and not others.. @nonexu - You are only sleeping if there is an error or if the record length is 0. You need to add a sleep in your loop that is always hit. I just moved it to the beginning.\n```go\n    for{\n                        time.Sleep(time.Second)\n            params := &kinesis.GetRecordsInput{\n            ShardIterator: aws.String(shardIterator), // Required\n            Limit:         aws.Int64(1000),\n            }\n        fmt.Println(\"start get data, shardId:\", shardId)\n        result, err := this.client.GetRecords(params)\n        fmt.Println(\"api return\")\n\n        if err != nil{\n            fmt.Println(\"get record error:\", err.Error())\n            continue\n        }\n\n        recordLen := len(result.Records)\n        fmt.Println(\"get records length:\", recordLen)\n        if recordLen > 0{\n            fmt.Println(result.Records[0])\n        }\n        shardIterator = *result.NextShardIterator\n}\n\n```\nTry that and let us know if that helps!. @nonexu - Thank you for keeping us up to date. What was the CPU load? And how long was the delay?. Thank you @nonexu for all the information. I've forwarded your graph to the service team. I will try to reproduce this on my end using your code and see if I can reproduce it.. @nonexu -  The service team has asked for sar metrics of the instance during use to capture this delay. Can you please provide that?. Hello @nonexu, if you are still having issues with this, please feel free to reopen this. I am going to close this for now until we have further data.. Hello @palaiyacw, thank you for reaching out to us. Can you paste some logs in here?\ngo\nsvc := s3.New(session.New((&aws.Config{\n    // Config values\n}).WithLogLevel(aws.LogDebugWithHTTPBody | aws.LogDebug | aws.LogDebugWithSigning)))\nI suspect it is something with the key. What is the key that you are trying to fetch?. @palaiyacw - Does your key have a leading /? If it does, enable in the config DisableRestProtocolURICleaning.\ngo\nsess := session.New(&aws.Config{\n    DisableRestProtocolURICleaning: aws.Bool(true),\n})\nLet me know if that resolves the issue.. @palaiyacw - The logs do not have anything that sticks out. I believe this may be a service problem. I'll go ahead and reach out to them and see if they can be of assistance.. Hello @palaiyacw, I am going to close this issue. If you are still having problems, please reach out to us.. Hello @jefferai, thank you for reaching out to us and filing this. I think the user has the keys backwards?\nI cannot reproduce this with the SECRET_KEY containing a slash and the issue linked to hadoop is saying the SECRET_KEY contains a slash, it'll fail. But in the user's example it is the other way around. \nIf the user's keys are correct, can we get more information?\n Where did the user get those keys?\n What operations are being called?. Cheers @jefferai and @RichardKnop , glad we got that managed for you guys. No worries on the noise. If you guys have any other issues or questions, please feel free to reach out.. hello @rinetd, thank you for reaching out to us. I do not think this is a SDK issue. I have no issues with go get. Have you tried git clone to see if you are even able to clone the repo? Also what version of Golang are you using?. @rinetd - If you are still running into this issue, please feel free to reopen this issue.. @chrisledet - Was reading the comments and noticed you were right about the typos! Cheers!. Hello @ror6ax, you can enable logging to inspect the contents of the body.\ngo\nsts := sts.New(session.New((&aws.Config{                                                                            \n    // Config params                                                                                    \n  }).WithLogLevel(aws.LogDebugWithHTTPBody | aws.LogDebug | aws.LogDebugWithSigning)))\nPlease let us know if that helps.. Hello @ror6ax, if you are still experiencing issues, please feel free to reach out to us on our gitter channel.. Hello @codingzombie, thank you for reaching out to us. Here is a general example. Please let me know if that works.. Hello @codingzombie, you shouldn't need to add that to the config. Was that example above not sufficient for your use case? Please let us know!. Hello @codingzombie, forwarding this to the service team may allow for them to look into adding this as a feature. Here is the forums to which you can reach them! I am going to close this, but if you have any additional issues, please reach out to us. Cheers!. Hello @codingzombie, thank you for reaching out to us. Can you please provide a code snippet of what you are trying to do?. Hello @codingzombie, I am going to go ahead and close this. Please reopen this if you are still having issues.. Hello @gbougeard, thank you for reaching out to us. Here is what I did:\n```go                                                                                                       \n  svc := autoscaling.New(session.New((&aws.Config{}).WithLogLevel(aws.LogDebugWithHTTPBody | aws.LogDebug | aws.LogDebugWithSigning)))\nname := \"foobarbaz\"                                                                                                      \n_, err := svc.CreateAutoScalingGroup(&autoscaling.CreateAutoScalingGroupInput{                                         \n    AutoScalingGroupName: &name,                                                                                         \n    MaxSize:              aws.Int64(2),                                                                                  \n    MinSize:              aws.Int64(1),                                                                                  \n    InstanceId:           aws.String(\"id\"),                                                                              \n  })                                                                                                                       \nif err != nil {                                                                                                        \n    return err                                                                                                           \n  }                                                                                                                        \nsvc.WaitUntilGroupInService(&autoscaling.DescribeAutoScalingGroupsInput{                                               \n    AutoScalingGroupNames: []*string{                                                                                    \n      &name,                                                                                                             \n    },                                                                                                                   \n  })                                                                                                                     \n  resp, err := svc.DescribeAutoScalingGroups(&autoscaling.DescribeAutoScalingGroupsInput{})                              \n  if err != nil {                                                                                                        \n    return err                                                                                                          \n  }                                                                                                                        \nfmt.Println(resp)                                                                                                      \n```\nI got the expected results. Please let me know if you have any issues with that example.. Hello @gbougeard, if you are still having issues with this, please reach out to us again.. :shipit:. LGTM, :shipit:. Hello @leelynne, thank you for reaching out to us. We've addressed this issue in the later versions of the SDK, #991. Can you update and see if that fixes the problem?. @leelynne no worries. If you run into any other issues, please let us know!. Hello @Mchau2, thank you for reaching out to us. This definitely sounds like a firefox issue. Please reach out to them with the issue here. If you have questions regarding the SDK, please let us know!. :shipit:. :ship:. Hello @shaileshkm, thank you for reaching out to us. After you create the queue, it returns the QueueUrl in the output shape. Is there a reason why you can't pull it off the output shape?. @shaileshkm - That makes sense. Yea, doing either that, caching, or calling the GetQueueUrl operation is another way. I do not know of a way to get the URL from the ARN. Please let me know if that helps.. LGTM, :shipit:. Hello @KilleR, thank you for reaching out to us. Can you give an example for this behaviour with the response being logged? It sounds like a malformed response is being sent. I do think an awserr.Error should be returned.. @patsoffice - That makes sense. The InvalidSequenceTokenException could be different depending on service and the message could be different. So, having a generic interface doesn't seems to be not the best answer. Also, having the SDK do the parsing seems like the wrong direction. Then doing the other solution of calling another operation to get that info seems a little fragile. I think the solution called for here is to allow for exceptions to be modeled and have the service return all necessary information in that structured exception. Please let me know what you guys think!\nWe do have modeled exceptions in our backlog and requests like this will prioritize that. Ill mark this as a feature request.. Going to close this out. Let's continue discussion here, #798.. Closing pull request since this is outdated now. Hello @dtan4, thank you for taking the time to do this. I think the reason why this isn't checked is that will always be set through bootstrapping requests. A request would be invalid if it was not set. One could make the argument to why should Operation be a pointer. Additionally, this would still fail if ResetBody is called.\nThe fix here I think is just update your mock client to bootstrap all request with some sort of operation. Please let us know if you have any further issues or PRs! We always welcome PRs and will be glad to walk through designs or go through reviews.. :shipit:. Good point, will update. Hello @etsangsplk, thank you for reaching out to us. A perfect place for an example would be to reach out to StackOverflow or gitter. We also do not mind looking at PRs for examples. In addition, the SDK team tries to cover as many examples as we can in the SDK. Letting us know which examples are needed will help us prioritize these.\nI am going to go ahead and close this, but if you have any features or bugs, please file them here.. :shipit:. Hello @zKai, thank you for reaching out to us. I've gone ahead and submitted a PR, #1097, that fixes this. If you have any additional issues, please feel free to reach out to us.. @zKai, we just merged in #1097. If you have any issues with the latest changes, please let us know. . @jasdel, naw, because we do a copy of the input shape directly when downloading. The uploader builds a new struct and does not call awsutil.Copy. I believe the reasoning for this is due to the uploader shape being a little more complex and is not of the same type.. Hello @stack72, thank you for reaching out to us. I'll take a look into this. Once I am able to replicate it, I'll post back here.\nCan you post what your input looks like?. @stack72 - I am unable to reproduce this, and behaves properly from what I can tell. Here is my source\n```go\npackage main                                                                                                               \nimport (                                                                                                                 \n  \"fmt\"                                                                                                                  \n  \"time\"                                                                                                                   \n\"github.com/aws/aws-sdk-go/aws\"                                                                                        \n  \"github.com/aws/aws-sdk-go/aws/session\"                                                                                \n  \"github.com/aws/aws-sdk-go/service/rds\"                                                                                \n)                                                                                                                          \nfunc main() {                                                                                                            \n  sess := session.New((&aws.Config{                                                                                      \n    Region: aws.String(\"us-east-1\"),                                                                                     \n  }).WithLogLevel(aws.LogDebugWithRequestRetries | aws.LogDebugWithRequestErrors))                                         \nsvc := rds.New(sess)                                                                                                   \n  req, out := svc.CreateDBInstanceReadReplicaRequest(&rds.CreateDBInstanceReadReplicaInput{                              \n    DBInstanceIdentifier:       aws.String(\"fooinstance\"),                                                              \n    SourceDBInstanceIdentifier: aws.String(\"arn to instance\"),                        \n    SourceRegion:               aws.String(\"us-west-2\"),                                                                 \n    KmsKeyId:                   aws.String(\"us-east-1 key arn\"),            \n  })                                                                                                                     \n  url, err := req.Presign(time.Minute * 5)                                                                               \n  fmt.Println(url)                                                                                                       \n  fmt.Println(err)                                                                                                       \n  fmt.Println(out)                                                                                                       \n} \n```\nPlease let me know if you any issues with that example.. @stack72 - If you are just looking to use the presigned URL, you do not need to send the request :).. @stack72 - Yea, that just returns the URL. You must wget or call http.Get on the url. Let me know if that helps!. @stack72 - What go version are you using? Can you provide what your input looks like when calling CreateDBInstanceReadReplicaRequest? Im wondering if it is something that is being specified in the opts like you had eluded to.. The output is not going to be populated due to it not being unmarshaled from the http.Get. You would need to read the response body from the http.Get and unmarshal into the out shape. So, Im certain if you look at your instances, you'll see a new instance being created. . Yep! No problem @stack72. If you run into any other issue, please feel free to reach out :). :shipit:. Hello @elbuo8, thank you for taking the time to fix this and bringing this to our attention. I've submitted another PR #1102 that regenerates the SDK and the code generation portion. Going to close this for now. Again, thank you. If you find anything else, please let us know!. Hello @antoniomo, thank you for reaching out to us. There are multiple ways of providing region, not through code, that may achieve what you are looking for. You can specify either in ~/.aws/credentials or ~/.aws/config your region. If using  the shared config,~/.aws/config, you must set AWS_SDK_LOAD_CONFIG to some value to enable it. Another option is to provide it through the AWS_REGION environment variable. Please let me know if that is a better usage for you.. @antoniomo - After some thought, a redesign of the credential package to not need a region, if on an instance, could be a potential. I'll take some time thinking about this and let you know what we come up with. For now I'll mark this as a feature request. Please let us know if you have any questions!. Hello @diamondap, thank you for reaching out to us and thank you for laying out the issue. Yea, I believe you are correct since there isn't a limit to how many time the go routine is created. We should probably have a pool of buffers or goroutines that we would pull from. I am going to mark this as a feature request. If you have any additional questions, please let us know. We also are more than glad to go over any PRs!. Hello @diamondap, after looking at the code a little bit, I do not think it is a buffering issue. There is a field in the Uploader called Concurrency which will dictate the amount of goroutines to use. Since nextReader is called per goroutine, this limits the buffer pool by Concurrency. I think the issue may be the Marshallers. We have another issue here that may be related, #722. Running pprof should show us what exactly is allocating the memory usage. I am going to mark this as a duplicate.\n. @diamondap - What Golang version and OS are you using? In addition, I can do some pprof metrics to see where the memory is going to with a 4gb io.Reader. What is the io.Reader, if it isn't a file, a socket? Going to try to reproduce this behavior. So any information of what you are using will be helpful!. @diamondap - I was unable to reproduce this with the tar.Reader. My application only ended up using ~400 MBs for a 4GB tar file. Here is what my application looks like:\n```go\npackage main\nimport (\n    \"archive/tar\"\n    \"fmt\"\n    \"log\"\n    \"os\"\n    \"runtime/pprof\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\"github.com/aws/aws-sdk-go/service/s3/s3manager\"\n\n)\nfunc createTarFile(size int) {\n    f, err := os.Open(\"random.txt\")\n    if err != nil {\n        panic(err)\n    }\n    defer f.Close()\n    file, err := os.Create(\"temp.tar\")\n    if err != nil {\n        panic(err)\n    }\n    defer file.Close()\n// Create a new tar archive.\ntw := tar.NewWriter(file)\nif _, err := f.Stat(); err == nil {\n    // now lets create the header as needed for this file within the tarball\n    hdr := &tar.Header{\n        Name: \"random.txt\",\n        Mode: 0600,\n        Size: int64(size),\n    }\n    // write the header to the tarball archive\n    if err := tw.WriteHeader(hdr); err != nil {\n        panic(err)\n    }\n\n    b := make([]byte, 1024*1024*100)\n    for i := 0; i < size; i += 1024 * 1024 * 100 {\n        fmt.Println(i)\n        n, err := f.Read(b)\n        if err != nil {\n            panic(err)\n        }\n\n        if _, err := tw.Write(b[:n]); err != nil {\n            panic(err)\n        }\n    }\n}\n\n}\nfunc main() {\n    writeFile := false\n    if writeFile {\n        createTarFile(1024 * 1024 * 1000 * 4)\n        return\n    }\n    f, err := os.Open(\"temp.tar\")\n    if err != nil {\n        panic(err)\n    }\n    defer f.Close()\n    tarReader := tar.NewReader(f)\n    cpu, err := os.Create(\"mem.pprof\")\n    if err != nil {\n        log.Fatal(err)\n    }\nsess := session.New((&aws.Config{\n    Region: aws.String(\"us-west-2\"),\n}))\n\nclient := s3.New(sess)\n// Set Concurrency to 5 and instantiate a new uploader\nsvc := s3manager.NewUploaderWithClient(client, func(u *s3manager.Uploader) {\n    u.Concurrency = 5\n})\n\n\n_, err = svc.Upload(&s3manager.UploadInput{\n    Bucket: aws.String(\"bucket\"),\n    Key:    aws.String(\"key\"),\n    Body:   tarReader,\n})\nfmt.Println(err)\npprof.WriteHeapProfile(cpu)\ncpu.Close()\n\n}\n```\nThis doesn't seem to be an issue with the SDK. I am going to close this issue. This seems to be a perfect question for gitter or stackoverflow. If you are able to determine that this is an SDK issue, please feel free to reopen this and we will get it addressed.. Hello @poopoothegorilla, thank you for reaching out to us. By other calls, are you saying this function works with other objects?\nCan you enable logging and post back the data? Also what Go and SDK version are you using?\n``go\ncfg := aws.NewConfig().WithRegion(\"us-west-2\").WithCredentials(creds).WithLogLevel(aws.LogDebug). @poopoothegorilla , the logs look correct as far as I can tell. Can you post a request that succeeds?. Should update theCHANGELOG_PENDING.mdwith the PRs we have. I'll do the same forjsonvalue. Other than that, looks good. @muly, thank you for reaching out to us. The Predix example does not return an error, so I would modify the sample and have it return an error. In addition, I would turn on more debugging to see what your key is. If your key is empty during aGetObject, I believe, it'll perform aListBucket`. It sounds like that sample just needs more fine tuning.\nPlease enable more logging and paste back the logs here. I think this will give us a better idea on to what is going on. \ngo\n&aws.Config {\n    LogLevel:    aws.LogLevel(aws.LogDebugWithHTTPBody | aws.LogDebugWithSigning | aws.LogDebugWithRequestErrors | aws.LogDebugWithRequestRetries),\n}. @muly - Can you reach out to Predix and post your issue there? This doesn't seem to be  a bug with the SDK. If you have any additional questions, please feel free to reach out to us.. Hello @catsby, thank you for reaching out to us. I'm going to do a little diving into this and try to reproduce this.. @catsby - I have a PR that addresses this, #1128. This will be in our next release. If you have any issues with this, please let us know. Thank you, again, for bringing this to our attention. :shipit:. :shipit:. Hello @haonumen, thank you for reaching out to us. What version of Golang are you using?\nIf you are using an older version of Go, updating will fix this.. @haonumen, can you try upgrading to a newer version of Go? I think this will solve the issue you are having!. You would need to uninstall your current version of Go and install the later version, 1.8. Currently you are on version 4.x.x, which is quite old.. Hello @gwatts, #1166 is adding some logic to handle broken TCP connections. If this is doing something similar to your custom retryer, then we can expand this change also to other services. This would get rid of that customization that we have in service/dynamodb.. @gwatts - This does not address idempotent requests. That will still need to be handled on a per request basis. And yes, you should not need a custom retryer as this will automatically retry serialization errors that were causes by connection reset. If you have any questions or feedback, please let us know!. @gwatts - We just merged in the PR. Please let us know if you have any additional issues.. Hello @twang-rs, thank you for reaching out to us and providing all this information. I've relayed this information to the service team. If you find anything else that may be helpful, please reach out.. @twang-rs, I have a PR, #1166, that fixes this for GetRecords. Please let us know if any other methods needs this or if the duration for the read is too small. We can always expose readDuration, but I felt 2 minutes was more than enough time for a Read.. @twang-rs - We just merged in the fix for this. If you have any additional questions or concerns, please don't hesitate to reach out. Feedback is always welcomed. Hello @DocMerlin, thank you for reaching out to us. #1141 is also affected by this. I'll mark this as a feature request. If you have any questions and/or feedback, please let us know.. Due to this being a duplicate I am going to close this and we can continue discussion in #1141. @petermbenjamin, thank you for finding and fixing this! Looks good :+1:. Looks good. Minor grammatical comment.. Looks good! :shipit:. @twang-rs - The timeout can be set by using request options. Please see the WithResponseReadTimeout.. @gwatts - Should be fine on windows. The ECONNRESET is defined by architecture. Ill have a PR that fixes the bad equality check.. @gwatts - Here is the updated PR #1181. Please let us know if you have anymore issues!. :eagle: . Hellon @kahing, thank you for reaching out to us. I am trying to reproduce this on my end, but I am unable to. Are you adding any custom handlers? In your case, it looks like the HTTPResponse is nil.. Thanks @kahing. I was able to reproduce this. I'm going to take a look into this and mark this as a bug.. Hello @kahing, thank you for reaching out to us. Taking a look, I noticed your bucket names are different. Is that what was intended? I cannot reproduce this one either :(.. Hello @rasecoiac03, thank you for reaching out to us. I cannot reproduce this on my end with the \"old code\" sample. I just get a valid response. Can you provide the full code block of what you were trying to do?. @rasecoiac03 - It sounds like everything has been correct. If you are still running into issues, please let us know!. @stefansundin, thank you for taking the time to correct this! This looks good.. Hello @asac, thank you for reaching out to us. I understand why they don't want all of the syscall package due to security. However, since they are already modifying the standard library why didn't they just support the error constants from syscall? I'll take some time to think about various solutions. This also should be addressed in appengine.  Again, thank you for letting us know about this.. @asac - I have a PR out that fixes this, #1199. Please let us know if you have any more issues!. @asac - This has been merged into master. Going to go ahead and close this. If you are still running into any issues, please reach out to us again. . @kahing - I have  PR that fixes this. You can track it here, #1212. If you have any concerns, please feel free to voice them!. @kahing - If an error is not being returned, but the response is empty, then I would think maybe this is a service issue.\nWhat version of the SDK is being used? \nWhat version of Go is being used?\nWhat are the steps to reproduce this error?. Looks good. Really like the refactor of the Send handler. Have a couple comments. Hello @pjmuller, thank you for reaching out to us. How are you importing this package? It should look something like,\ngo\nimport (\n\"github.com/aws/aws-sdk-go/<packages>\"\n). Hey @dixudx, these changes look good. Thank you for taking the time to do this. I'll go ahead and merge this in.. Hello @jgimenez, thank you for reaching out to us. When using the shared config you have to set AWS_SDK_LOAD_CONFIG to a truthy value. Please reopen this if you are still having issues!. Hello @ByteFlinger, currently only kms is supported. We have this in our backlog, but I'll mark this as a feature request. You can also implement your own key wrap handler, if needed. I will bring this up in our next planning meeting. Cheers!. @ByteFlinger, yea that is a good high level explanation. The RSA algorithm would be RSA/ECB/OAEPWithSHA-256AndMGF1Padding. As long as you implement the key wrapping interface, it shouldn't be any different to when we decide to support it.\nThe EncryptedKey should be the encrypted symmetric key that was randomly generated and shouldn't be ignored. What I would do is write a wrapper for RSA that supports the key wrapping interface. I'd then write unit tests to ensure everything is being properly encrypted and decrypted. I would even ensure cross compatibility between SDKs by encrypting with one SDK and decrypting with another.\n. @ByteFlinger - The label that is supported for RSA is RSA/ECB/OAEPWithSHA-256AndMGF1Padding. That algorithm, RSA/ECB/OAEPWithSHA-256AndMGF1Padding, is a Java cipher, and I believe it is apart of one of their crypto libraries. You will need to mimic their implementation to ensure you have cross compatibility for our future implementation.\nCurrently the crypto client does not support being used in s3manager. However, we do have it in our backlog along with implementing RSA for a key wrap algorithm.. For paragraph 1, yes, that is the label that is used for decryption. Here is the Java SDK label.\nParagraph 2: Yes, Golang's OAEP will use MGF1 padding.\nParagraph 3: Yea, I don't see any issues with using the crypto client with s3manager.. @ByteFlinger - With encryption and decryption it is going to be pretty difficult to see whether or not this is encrypting/decrypting properly. Do you have some tests in place to see if it is encrypting and decrypting properly? I would also run those test vectors in Java to ensure you are getting the appropriate values as expected. Have you pushed this code up? I want to take a look at the overall implementation to see if I can see anything.. @rogaha - Currently there is a limitation in Go's RSA crypto library which doesn't allow specification of the MGF1 hash separate from the cipher hash. See here. The cleanest solution would be to wait until Go's standard library has support for the distinction of which hashes to use for the padding and cipher.\n@ByteFlinger @tfeng - This is the reason to why you guys were receiving that Java error. Java uses SHA-1 for its MGF1 padding when specifying RSA/ECB/OAEPWithSHA-256AndMGF1Padding, but SHA-256 for the cipher. When you pass the SHA-256 hash.Hash to the DecryptOAEP, it will use SHA-256 for both the cipher and the MGF1 padder.. Hello @mbh621, thank you for reaching out to us. I would suggest using the s3manager package. This will retry on connection resets. Please let us know if you have any issues there. Meanwhile I'll make this as a feature request for the service/s3 package.. Hello @alwindoss, thank you for reaching out to us. I am not familiar with how those SDKs deal with auth challege. Can you elaborate on what exactly it is that is wanted?. @alwindoss - It looks like the functionality you are referring to is part of the service team's SDK which is built on top of the AWS SDK for JavaScript.  I will keep this as a feature request, and I completely agree that this should be in the SDK as a higher level function to help users get these values.. @alwindoss - The best place to see how these values are calculated is to look at this SDK. This feature is undocumented and private. Meaning that this is not guaranteed to be stable and can be subject to change. Since the algorithms do not pertain to this SDK, it may be a good idea to reach out to the service team's forums. They should be able to better help you in getting the specifics of the algorithms.\nStackoverflow discussion\n. @alwindoss - It looks there is documentation for the SECRET_HASH value. Please let us know if that helps!. @alwindoss - The AWS SDK for JavaScript is different then the one that's listed there. I imagine this is true for mobile as well. The service team maintains those specific SDKs. The best course of action is to reach out to the forums here. I've also forwarded this request to the service team.. @alwindoss - We don't track individual service team features. The best way to get at this information is through their forums. I will ping the service team to let them know there is more demand for this feature.. @bfallik, thank you for taking the time to correct this. LGTM!. Hello @bfallik, thank you for feedback. We code generate these from the waiter files found in the models/api package. I'll go ahead and mark this as a feature request.. Hello @jhwang09, thank you for reaching out to us. When the service team tested our implementation, they said it worked. I would ask on their forums to see if they can spot anything that may not look right. In addition, questions like this may be best suited for stackoverflow or gitter. However, if there is an issue with the implementation, please reopen this ticket.. @jhwang09, I am looking at the docs and I see the confusion. Let me try to test this on my end and see if the example is incorrect.. @jhwang09, I have a feeling your DSN string is incorrect. The example does not include [ ] around the user:password.\nWhy is there [ ] around the DSN string? Just making sure there isn't some information I may be missing.. jhwang09 - What is the error you are seeing when you use that?\nI ran into no issues using the example.. @jhwang09 - Please make sure your db endpoint is correct. It should match up with what is in the console. I believe your DSN construction is correct, but your endpoint may be wrong since you are manually constructing it. Can you please verify that it is correct? It should match in the AWS console.. I would use the port one, because that's what I tested with. And yes, the DSN looks correct. Can I have the errors of when you tried to use the scheme http or https? Also, may want to try static credentials to see if that has anything to do with it.. Okay, that is pretty important information. Let me try replicating this with an IAM role. If it succeeds, then it may be something in your middleware. Can you write a simple Go program that eliminates jenkins and all your dependencies and only tests the SDK? Or has that been what you've been doing?. @jhwang09 - I just tried this with STS credentials, and it worked. Is this failing on opening a connection with sql? Or when performing a query?\nSo, yea, I cannot seem to replicate this. What mysql driver are you using? I am using this one github.com/go-sql-driver/mysql. @jhwang09 - you shouldnt need to be on an Amazon EC2 host. What is the error when trying to assume the role? Have you have given permissions the correct permissions to Amazon RDS? What does that look like?. @jhwang09 - I have a PR that contains an example of using the rdsutils #1256. I am going to close this. If you are still having issues, please try  stackoverflow or gitter.. Hello @sujunzhu, what issues are you running into? Can you provide a code sample?. The work around would be the first design. The first design was to write to stdout and not use a buffer at all. I personally like this approach.. Hello @nextdimension, thank you for reaching out to us. It looks like the Acl header is not being signed on the service's end. I am going to reach out to the service team and see why that is.. @nextdimension - It looks like you have to set the header explicitly in the request.\n```go\npackage main                                                                                                               \nimport (                                                                                                                 \n  \"fmt\"                                                                                                                  \n  \"net/http\"                                                                                                             \n  \"time\"                                                                                                                   \n\"github.com/aws/aws-sdk-go/aws\"                                                                                        \n  \"github.com/aws/aws-sdk-go/aws/session\"                                                                                \n  \"github.com/aws/aws-sdk-go/service/s3\"                                                                                 \n)                                                                                                                          \nfunc main() {                                                                                                            \n  svc := s3.New(session.New(&aws.Config{Region: aws.String(\"eu-west-2\")}))                                               \n  req, _ := svc.PutObjectRequest(&s3.PutObjectInput{                                                                     \n    ACL:    aws.String(\"public-read\"),                                                                                   \n    Bucket: aws.String(\"BUCKET\"),                                                                                 \n    Key:    aws.String(\"MY KEY\"),                                                                                        \n  })                                                                                                                       \nstr, headers, _ := req.PresignRequest(15 * time.Minute)                                                                \n  httpreq, _ := http.NewRequest(\"PUT\", str, nil)                                                                         \n  for k, values := range headers {                                                                                       \n    for _, value := range values {                                                                                       \n      httpreq.Header.Add(k, value)                                                                                       \n    }                                                                                                                    \n  }                                                                                                                      \n  fmt.Println(http.DefaultClient.Do(httpreq))                                                                            \n}\n```\nIf you have any additional issues, please let us know!. I am going to go ahead and close this issue. If you are still having issues, please reopen this.. Hello @grubernaut, thank you for reaching out to us.  Looks like this is an issue with the endpoint.json file not being up to date. I have reached out to the service team to get an update to that file.. @grubernaut - So, depending on what region is being specified in the client will limit what AvailabilityZones can be used. For instance, if you create a session in us-east-1, your AvailabilityZones can only be us-east-1a, us-east-1b, and so on.\nSo, it looks like the client that is being created is us-east-1 but the AvailabiityZone is eu-west-1a, which is invalid. You can get a list of valid regions by calling the GetRegions operation.\nIf you have any other questions, please feel free to ask. I will go ahead and close this as this seems to be not an SDK issue.. I wrote a quick example and cannot reproduce this:\n```go\nimport (                                                                                                                 \n  \"fmt\"                                                                                                                    \n\"github.com/aws/aws-sdk-go/aws\"                                                                                        \n  \"github.com/aws/aws-sdk-go/aws/session\"                                                                                \n  \"github.com/aws/aws-sdk-go/service/lightsail\"                                                                          \n)                                                                                                                          \nfunc main() {                                                                                                            \n  sess := session.New((&aws.Config{                                                                                      \n    Region: aws.String(\"eu-west-1\"),                                                                                     \n  }))                                                                                                                      \nsvc := lightsail.New(sess)                                                                                               \nresult, err := svc.CreateInstances(&lightsail.CreateInstancesInput{                                                    \n    AvailabilityZone: aws.String(\"eu-west-1a\"),                                                                          \n    BlueprintId:      aws.String(\"redmine_3_3_2\"),                                                                       \n    BundleId:         aws.String(\"micro_1_0\"),                                                                           \n    InstanceNames: []*string{                                                                                            \n      aws.String(\"foo\"),                                                                                                 \n    },                                                                                                                   \n  })                                                                                                                       \nif err != nil {                                                                                                        \n    panic(err)                                                                                                           \n  }                                                                                                                      \n  fmt.Println(result)                                                                                                    \n} \n```\nCan you provide steps to reproduce this?. No worries @grubernaut! If you have any more issues or questions, please reach out to us :). Hello @phbcanada, thank you for reaching out to us. The decompression is handled by the http.Transport. We use the default HTTP client, but you can set the transport to not handle (un)compressing the data. Please let us know if you have any additional issues with that!. Cool, good comments. I'll go ahead and make those changes. @pwaller - In the mean time here is the correct link to the starting guide. Please let us know if you have any other issues!. Hello @pwaller, looks like this just got fixed. Please let us know if you have any more issues.. Hello @sandoracs, thank you for reaching out to us. How big is the object you are downloading? And additionally, the aws.WriteAtBuffer is all in memory.. @sandoracs - I think what is happening here is that we know that both unmarshaling and marshaling takes a lot of memory. We have this as a feature request, #377, that we can continue discussion on there. Additionally, this also has been mentioned in #722. I am going to go ahead and close this as I am pretty sure this is the issue. However, if you do some profiling and find that this is not the case, please reopen this.. Hello @anupavanm, thank you for reaching out to us. The endpoint URL should look like this, https://mturk-requester-sandbox.us-east-1.amazonaws.com without the ending /. Please let us know if you have any additional issues.. @stevenh -WithRetryer is a function off of the request package. See here. Please try setting that and see if that helps.. Hello @davyzhang, thank you for reaching out to us. Are you sure you are getting a valid task arn back? I noticed you were squashing errors. Was an error being returned from the ListTasks operation?. @fxaguessy - Thank you for the suggestion. Please try that solution, @davyzhang. If you are still having issues, please feel free to reach back out.. @davyzhang  - I am going to close this issue. If you are still having still having problems, please reopen!. Hello @xanderdunn, thank you for reaching out to us. What region is your object in? And what region is your key in?. @xanderdunn - After looking more closely at this, I see that one is using service side encryption and the other is using client side encryption. The s3crypto package is for client side encryption.\nI think what you are looking for is the GetObject operation in the s3 package. There is a few fields regarding SSE. I would look at this documentation to set that up properly!. Hello @jamesdbowman, thank you for taking the time to make this change. Looks good. I'll merge it in after the tests pass.. Hello @nanjekyejoannah, thank you for reaching out to us. The SDK does not validate against max values.\nAlso It looks like to be failing during creation of objects? Which hasn't used the maxkeys yet in your code. Please let us know if that clarifies or sheds light on what you are testing!. @nanjekyejoannah - I think the SDK should not generically do client side validation. Sure, max negative values may not make sense a lot of the times, but max is not constrained to positive numbers. The SDK would then be applying a limitation for future services. I hope that clarifies a little to why we do not do that sort of validation.. @nanjekyejoannah - I am going to go ahead and close this issue. If you have any other questions or feedback regarding this, please reopen this!. Hello @nanjekyejoannah, thank you for reaching out to us. The SDK will not keep track of previous requests. So, this is completely valid from the SDK's point of view. That sort of validation should be happening on the service's side, imo. Please let us know if that helps or if you wanted to discuss this further.. @nanjekyejoannah, yea you would have to cache that somewhere and check against that cache to see if you've already created that bucket, or you can call HeadBucket and see if it already exists by checking against this error code ErrCodeNoSuchBucket. @nanjekyejoannah - No, we only do a very small set of validation. Does HeadBucket fit your use case? If it doesn't, can we move this to gitter or stackoverflow? You'll be able to get answer from the community, and as well as us! In the mean time, I am going to go ahead and close this. If you have any additional issues, please feel free to reopen or create a new issue.. It was more of I didn't realize the aws.ReadSeekCloser would not sign properly if a non-io.Seeker was used.. Hello @TalLevAmi, we just merged #1372. Please let us know if you have any other issues!. Hello @TalLevAmi, I have another PR #1373 that addresses this. It looks like it is a logical bug. Made the change and should not be an issue any further. Cheers!. Hello @sethcleveland, thank you for reaching out to us. Can you please enable logging so we can see what is going on? Additionally could you also paste the code? This seems that it may be a service issue, but with more information we may be to figure out what is going on.\ngo\n// enables logging\nsess := session.New((&aws.Config).WithLogLevel(aws.LogDebugWithHTTPBody | aws.LogDebugWithSigning)). @sethcleveland  - Interesting, could you also just test the SDK rather than the abstraction? I want to remove as much as we can when determining what the issue is. And yes, signing is idempotent. Why do you ask?. @sethcleveland - Yea, I don't think I've seen this problem myself. However, doesn't mean there isn't one! Once we have the logs we can at least route it to the service team to see if they can see anything weird with the request. How often would you say this happens?. @sethcleveland - You can remove any sensitive information. If there is too much information that is needing to be censored, I can go ahead and let you know what the process is for sending us that information. Please let us know what you'd like to do. @sethcleveland - Sure, I think the best way would be hop in gitter and send me a private message. . Hello @amongil, thank you for reaching out to us. This is a known issue with the service using 1.6.2 of the SDK or earlier. You just need to update your SDK and things should work! Please let us know if you have any additional issues. Hello @dowlingw, thank you for reaching out to us and repoting this. I'll forward this along to the service team! . Hello @mtestrot, thank you for reaching out to us. We are aware of this issue and currently have it in our backlog, #645. We are more than willing to take PRs too! I will go ahead and bring this up in our next team meeting. For now, I'll mark this as a feature request.. @mtestrot - What is your use case for the Location field? Are you doing anything with that field?. @mtestrot - Other than the inconsistency of the escaping, does the URL work?. @mtestrot - Thank you for the information. I will go ahead and try to reproduce this on my end as well. Can you provide a code snippet trying to reuse the Location value? In particular, the section that makes the request and the section that replaces the %2F.. @mtestrot thank you for the example. I'll go ahead and discuss with the other SDKs to have some consistency on dealing with this issue.. @mtestrot - Taking a look at the code that you provided, can you instead just call get object instead of trying to build the request yourself? Why is it that you are trying to do this? What's the specific use case?. Hello @the1337beauty, thank you for reaching out to us. We've went ahead and marked this as a feature request. If you find anymore of these, please let us know!. Hello @etsangsplk, thank you for reaching out to us. Please take a look at the release release it says to ignore v.1.9.0 and v1.9.00. We cannot remove the other tags because of the potential of breaking people. If you have any other issues, please let us know!. Hello @bfosberry, thank you for reaching out to us. Does previous versions of the SDK behave correctly?. Here is a working example:\n```go\npackage main                                                                                                               \nimport (                                                                                                                 \n  \"fmt\"                                                                                                                  \n  \"strings\"                                                                                                                \n\"github.com/aws/aws-sdk-go/aws\"                                                                                        \n  \"github.com/aws/aws-sdk-go/aws/session\"                                                                                \n  \"github.com/aws/aws-sdk-go/service/s3\"                                                                                 \n)                                                                                                                          \nfunc main() {                                                                                                            \n  l := aws.LogLevelType(aws.LogDebugWithSigning | aws.LogDebugWithHTTPBody | aws.LogDebugWithRequestRetries | aws.LogDebugWithRequestErrors)\n  svc := s3.New(session.New(&aws.Config{                                                                                 \n    Region:   aws.String(\"us-west-2\"),                                                                                   \n    LogLevel: &l,                                                                                                        \n  }))                                                                                                                      \nresp, err := svc.PutObject(&s3.PutObjectInput{                                                                         \n    Bucket:       aws.String(\"bucket\"),                                                                           \n    Key:          aws.String(\"key\"),                                                                                     \n    Body:         strings.NewReader(\"HELLO\"),                                                                            \n    StorageClass: aws.String(\"REDUCED_REDUNDANCY\"),                                                                      \n  })                                                                                                                     \n  fmt.Println(resp, err)                                                                                                   \nr, err := svc.HeadObject(&s3.HeadObjectInput{                                                                          \n    Bucket: aws.String(\"bucket\"),                                                                                 \n    Key:    aws.String(\"key\"),                                                                                           \n  })                                                                                                                     \n  fmt.Println(r, err)                                                                                                    \n} \n```\nThen this prints the correct storage class when heading the object.. @bfosberry - I think the best bet here is to enable logging on a staging environment and seeing what is being sent. You can post the logs back here and we can try to debug this together. Other than that, I think something may be messing with the values. I can't say for certain without having some code to look at.. @bfosberry -Awesome! Good job finding that. I will go ahead and close this. If this is an issue with the SDK, please feel free to reopen!. Hello @colinfaulkingham, thank you for reaching out to us. I cannot reproduce this. Here is the code I used to try to reproduce this:\n```go\npackage main                                                                                                            \nimport (                                                                                                              \n  \"fmt\"                                                                                                               \n  \"strings\"                                                                                                             \n\"github.com/aws/aws-sdk-go/aws\"                                                                                     \n  \"github.com/aws/aws-sdk-go/aws/session\"                                                                             \n  \"github.com/aws/aws-sdk-go/service/s3\"                                                                              \n)                                                                                                                       \nfunc main() {                                                                                                         \n  svc := s3.New(session.New(&aws.Config{                                                                              \n    Region: aws.String(\"us-west-2\"),                                                                                  \n  }))                                                                                                                   \nbucket := \"bucket\"                                                                                                    \n_, err := svc.PutObject(&s3.PutObjectInput{                                                                         \n    Bucket: &bucket,                                                                                                  \n    Key:    aws.String(\"//test.txt\"),                                                                                 \n    Body:   strings.NewReader(\"foobar\"),                                                                              \n  })                                                                                                                  \n  if err != nil {                                                                                                     \n    panic(err)                                                                                                        \n  }                                                                                                                     \nresp, err := svc.DeleteObject(&s3.DeleteObjectInput{                                                                \n    Bucket: &bucket,                                                                                                  \n    Key:    aws.String(\"//test.txt\"),                                                                                 \n  })                                                                                                                  \n  if err != nil {                                                                                                     \n    panic(err)                                                                                                        \n  }                                                                                                                   \n  fmt.Println(\"RESP\", resp)                                                                                           \n}\n```\nCan you try that and see if you are still running into the issue?. @colinfaulkingham - Ah I see what you are wanting. There is a config field that gives you the desired behavior.\ngo\n  svc := s3.New(session.New(&aws.Config{                                                                                                                                                       \n    DisableRestProtocolURICleaning: aws.Bool(true),                                                                     \n  }))\nPlease let me know if this addresses your issue!. Hello @etsangsplk, thank you for reaching out to us. By easily, what were you thinking? The code that you provided looks like it uses an abstraction above the SDK. To better understand the issue, can you please elaborate on the pain points on using the temporary credentials?. @etsangsplk - I am going to go ahead and close this one in favor of #1436. If you feel this should be reopen, please go ahead and do so.. Hello @meirf, thank you for reaching out to us. I will take a look at this and try to reproduce this.. @meirf - Can you please provide a code sample? I've tried to reproduce it using this code and cannot. The key does not contain a +\n```go\npackage main                                                                                                               \nimport (                                                                                                                 \n  \"fmt\"                                                                                                                  \n  \"strings\"                                                                                                                \n\"github.com/aws/aws-sdk-go/aws\"                                                                                        \n  \"github.com/aws/aws-sdk-go/aws/session\"                                                                                \n  \"github.com/aws/aws-sdk-go/service/s3\"                                                                                 \n)                                                                                                                          \nfunc main() {                                                                                                            \n  svc := s3.New(session.New(&aws.Config{                                                                                 \n    Region: aws.String(\"us-west-2\"),                                                                                                                                                         \n  }))                                                                                                                      \nbucket := \"bucket\"                                                                                              \n_, err := svc.PutObject(&s3.PutObjectInput{                                                                            \n    Bucket: &bucket,                                                                                                     \n    Key:    aws.String(\"foo bar\"),                                                                                       \n    Body:   strings.NewReader(\"foobar\"),                                                                                 \n  })                                                                                                                     \n  if err != nil {                                                                                                        \n    panic(err)                                                                                                           \n  }                                                                                                                        \nerr = svc.ListObjectsPages(&s3.ListObjectsInput{Bucket: &bucket}, func(output s3.ListObjectsOutput, ok bool) bool { \n    for _, object := range output.Contents {                                                                             \n      fmt.Println(\"KEY\", object.Key)                                                                                    \n    }                                                                                                                    \n    return output.IsTruncated != nil && !*output.IsTruncated                                                             \n  })                                                                                                                     \n  fmt.Println(err)                                                                                                       \n}\n```. @meirf - Great. Please let us know when you've found the issue!. Hello @zbintliff, thank you for reaching out to us. We've have received this feedback before. I think the main reason why we do not do this is that we want the user to be explicit in the choice of the bucket. We want to give full control and make sure that the SDK isn't doing anything magical, like automatically attempting to redirect.\nHere is an example of getting the region for a specific bucket:\n```go\npackage main                                                                                                               \nimport (                                                                                                                 \n  \"fmt\"                                                                                                                    \n\"github.com/aws/aws-sdk-go/aws\"                                                                                        \n  \"github.com/aws/aws-sdk-go/aws/session\"                                                                                \n  \"github.com/aws/aws-sdk-go/service/s3\"                                                                                 \n)                                                                                                                          \nfunc main() {                                                                                                            \n  svc := s3.New(session.New(&aws.Config{                                                                                 \n    Region: aws.String(\"us-east-1\"),                                                                                     \n    DisableRestProtocolURICleaning: aws.Bool(true),                                                                      \n  }))                                                                                                                      \nbucket := \"bucket\"                                                                                              \nresp, err := svc.GetBucketLocation(&s3.GetBucketLocationInput{                                                         \n    Bucket: &bucket,                                                                                                     \n  })                                                                                                                     \n  if err != nil {                                                                                                        \n    panic(err)                                                                                                           \n  }                                                                                                                        \nfmt.Println(*resp.LocationConstraint)                                                                                  \n}\n```\nPlease let us know if that helps!. @zbintliff - No worries. There are a lot of operations, so I can understand why things like this can get missed. This just means we need a good example illustrating on how to get a bucket region somewhere. You are correct, this will still work even if the client is configured with a different region.. @Bankq - It sounds like you are having some permission issues. I would take a look at your policies/permissions. I cannot reproduce this and the example @zbintliff provided works fine.. Hello @piusranjan, thank you for reaching out to us. I am not able to reproduce this using your code. I get the desired results back. Are you sure you are using the right credentials and region? In addition, a question like this is perfect for stackoverflow or gitter. You get a wider audience who may be able to answer a lot more quickly and have dealt with this sort of problem in the past. If you do have any other issues or questions, please reach out to us!. @piusranjan - Interesting! Please let us know if you figure that out. I am glad you got that working. If you have any additional questions, please let us know!. Hello @mwhooker, thank you for reaching out to us. I think this waiter hasn't been created yet. I'll go ahead and ping the service team!. Hello @mwhooker, thank you for pointing this out! We are more than happy to take this as a PR!. Hello @jlburkhead, thank you for reaching out to us. The documentation is a little unclear and it should be stated that the max concurrent goroutines is per invocation. I will go ahead and add this in the backlog to better document that.. Hello @sharabash, thank you for reaching out to us. Could you please provide a code sample that reproduces this with logging enabled?. Hello @mscansian, thank you for reaching out to us. It looks like this long polling isn't performed by the SDK, but is kept alive by the service. See here. In addition, you can cancel the request with a context by calling GetActivityTaskWithContext. Please let us know if that solves your problem!\ngo\nctx, cancel := context.WithCancel(aws.BackgroundContext())\nresp, err := svc.GetActivityTaskWithContext(ctx, params)\n// this will cancel the request\ncancel()\n. Hello @stack72, thank you for reaching out to us. Yea, it look like the documentation needs to be updated there. I'll forward this along to the service team.. Hello @s-maj, thank you for reaching out to us. I am going to go ahead and forward this to the service team as this isn't an issue with the SDK. The best place for these types of issues is the service team forums. Please let us know if you have any additional issues!. Hello @kylegato, thank you for reaching out to us. Out of curiosity would modifying the proxy at the transport level work for you? Or http proxy?. @kylegato - Thank you for the clarification. Have you tried the two suggestions listed above? I believe this will fix your issue.. @TylerBrock - Ah! Yes, you are correct. It looks like it does the check for you to see if there are pages.\nLet me check if the filtering is working. I'll try to reproduce this on my end! Totally misunderstood your question. Thanks for the clarification.\n. @TylerBrock - I am unable to reproduce this :(.\n```go\npackage main                                                                                                               \nimport (                                                                                                                 \n  \"fmt\"                                                                                                                    \n\"github.com/aws/aws-sdk-go/aws\"                                                                                        \n  \"github.com/aws/aws-sdk-go/aws/session\"                                                                                \n  \"github.com/aws/aws-sdk-go/service/cloudwatchlogs\"                                                                     \n)                                                                                                                          \nfunc main() {                                                                                                            \n  input := cloudwatchlogs.DescribeLogStreamsInput{}                                                                      \n  input.SetLogGroupName(\"groupname\")                                                                         \n  var streamNames = make([]string, 0)                                                                                   \n  cw := cloudwatchlogs.New(session.New(&aws.Config{                                                                      \n    Region: aws.String(\"us-west-2\"),                                                                                     \n  }))                                                                                                                    \n  cw.DescribeLogStreamsPages(&input, func(out cloudwatchlogs.DescribeLogStreamsOutput, lastPage bool) bool {            \n    for _, stream := range out.LogStreams {                                                                              \n      streamNames = append(streamNames, stream.LogStreamName)                                                            \n    }                                                                                                                    \n    return lastPage                                                                                                      \n  })                                                                                                                       \nin := &cloudwatchlogs.FilterLogEventsInput{}                                                                           \n  in.SetLogGroupName(\"groupname\")                                                                            \n  in.SetLogStreamNames(streamNames)                                                                                      \n  fmt.Println(in.LogStreamNames)                                                                                         \n  output, err := cw.FilterLogEvents(in)                                                                                  \n  if err != nil {                                                                                                        \n    panic(err)                                                                                                           \n  }                                                                                                                      \n  fmt.Println(output)                                                                                                    \n}  \n```\nI am getting all the necessary results. Can you try this small code sample to see if it works? If it does, it looks like it is something happening on your end.. Ah! Good catch. So, did that solve your problem? Im going to go ahead and close this. If you run into any other issues, please reopen!. Hello @michaelstewart, thank you for taking the time to correct this. This looks good! I'll go ahead and merge this.. Hello @TylerBrock, thank you for reaching out to us. You can set the region by default in the shared config. I am going to go ahead and close this. If you have any questions doing that, please let us know!. Hello @matthewmueller, thank you for reaching out to us. Questions like these are best suited for gitter and stackoverflow as you'll get feedback from the overall community. You can iterate through regions with the example here. You may be able to use that to your advantage without needing to parse the endpoint directly. Let me know if you any issues with that!. Hello @wolfgangmeyers, thank you for taking the time to write your feedback. The SDK currently doesn't have that high level functionality. In addition, this is something the service team could add to the SDKs, hence why asking on the service team's forum is the correct answer. That is if you don't care about have text to view of what has finished but instead could wait until a given job has finished. By asking the service teams to model this would allow all of the SDKs to have this functionality.\nStackoverflow would be a good place to see how to properly do this without the higher level functionality until the service teams add waiters for that given operation.\nI hope that explains why jasdel recommended those two places!. Hello @tharun06, thank you for reaching out to us. It looks like you are using the AWS Command Line Interface. Please refer CLI issues here.\nI am going to go ahead and close this. If this isn't a CLI issue, please reopen this and some clarity to why this is an AWS SDK for Go issue would be very helpful.. Hello @trung, thank you for reaching out to us. For clarification, it looks like you want the DecodedMessage json string to be returned as a type?. @trung - Thank you for that clarification. That message is pretty useless in logs... Let me reach out to the service team to see it is encoded in the first place. With that said, I'll bring this up in our sprint and see what is the best path forward for this.. hello @brendanjerwin, thank you for reaching out to us. So, from the sound of things, it sounds like the main issue here is that when signing, we are including the port, which is incorrect. Does that sound right?\nI'll need to do a little bit of research to see why we choose to go with req.Host and req.URL.Host before we can make the change. Also we are always happy to take a look at PRs :). @brendanjerwin - I have set up a reverse proxy running this command\nmitmdump -R https://s3-us-west-2.amazonaws.com -p 9091\nThen I try to hit it using this code\n```go\n  svc := s3.New(session.New(&aws.Config{                                                                                 \n    Region:     aws.String(\"us-west-2\"),                                                                                 \n    Endpoint:   aws.String(\"localhost:9091\"),                                                                            \n    DisableSSL: aws.Bool(true),                                                                                          \n    LogLevel:   aws.LogLevel(aws.LogDebugWithHTTPBody),                                                                  \n  }))                                                                                                                      \nresp, err := svc.ListBuckets(&s3.ListBucketsInput{})                                                                   \n``\nAnd looks like I am still getting a signature is invalid with that change ofHostname(). I want to make sure that I am setting this up the same as you. Please let me know.. @atsushi-ishibashi - Following up to see how the above suggestions are working out for you, and if there is any other questions regarding this you may have.. @atsushi-ishibashi: Have you taken a look [here](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ExpressionAttributeNames.html)? You should be able to referenceTimestampI imagine. A good place to ask this is the [dynamodb forums](https://forums.aws.amazon.com/forum.jspa?forumID=131) as this isn't really an SDK question. I am going to go ahead and close this. If you have any other questions regarding the SDK please feel free to contact us.. @radeksimko - I don't think this should be retryable. The only real concern I have is that operations would be nondeterministic. What is your use case? Is it for the describe calls?. @radeksimko - Yea, the first option I think is the better way of going about this. Is there any issues with the first approach?. @jasdel - Agreed, I was thinking client side above the SDK as clarification. I did not see a waiter for this particular operation, unfortunately.. Hello @mariusae, thank you for reaching out to us. Adding anything to theawserr.Error` interface would be a breaking change, so that is something we could not do. However, we could change it up a little and have helper functions like\ngo\nfunc IsTemporary(err error) bool {\n    _, ok := err.(TemporaryError)\n    return ok\n}\nWe'd need to figure out how to do this without introducing any breaking changes mind you. But overall, would this be something that you would to see?. @theherk - Thank you for clarifying. I can definitely see value in being able to be able to go to and from builders.  We would definitely appreciate the PR, and also submitting just the Names/Values map first since it is far simpler.. I'd be curious to see how you tackled this. There is a TODO in the expression package to eventually use an AST instead of formatted strings. If an AST were used, you would only need to get it into the AST form then the builder should be able to construct an expression from there. This could be an option going forward.. @theherk - We have it in our backlog, but we haven't prioritized it yet. So no real time frame. If you have any other issues or questions, please feel free to ask :). Hello @Phlamethrower, thank you for taking the time to construct this PR. Having this use json.Number by default would be a breaking change. Another potential solution would be to use the json and dynamodbattribute marshaling interfaces.\n```go                                                                                                         \ntype Foo json.Number                                                                                                    \nfunc (foo Foo) MarshalJSON() ([]byte, error) {                                                                                                                                                                                \n  return json.Marshal(json.Number(foo))                                                                               \n}                                                                                                                       \nfunc (foo Foo) MarshalDynamoDBAttributeValue(av *dynamodb.AttributeValue) error {                                                                                                                                   \n  av.S = aws.String(json.Number(foo).String())                                                                          \nreturn nil                                                                                                          \n} \n``. @Phlamethrower - As you have alluded to, allowing more of these types to be introduced in thedynamodbattributepackage is not ideal. Out of curiosity what is your use case for doing this? Additionally, how much overhead are you looking at to loop over the keys/values? . @Doug-AWS - Thank you for this! LGTM. @moitias - Thank you again for reporting this issue. I have just merged in this [PR](#1606). If you run into any more issues, please feel free to reach out to us.. @SCKelemen - I went ahead and closed this issue. If you are still having issues, please feel free to reopen this.. Hello @Arnold1, thank you for reaching out to us. Please see [here](http://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html). @Arnold1 - The link is the best I have. However, questions like these are best asked in [gitter](https://gitter.im/aws/aws-sdk-go) and [stackoverflow](http://stackoverflow.com/). Please ask there and let us know if you have any other issues.. @Arnold1 - I am going to go ahead and close this. If you have any other questions about the SDK, please let us know.. Hello @ppetko, thank you for reaching out to us. Are you getting a serialization error from the SDK? If not, then I suspect this is an issue with the stringer. . @ppetko - After re-reading I think what you are asking is to transform the output toJSON. That string that is printed is notJSON, but just the output from the stringer,String(). You would need to usejson.Marshal, but you will have a bunch ofnilvalues in there. Please let us know if that answers your question.. @ppetko - How are you trying to parse the output? Are you using theUnmarshal` handlers? Can you provide an example of your code doing the parsing?. Hello @atsushi-ishibashi, it looks like the docs have not been updated since then. I will go ahead and look into this and thank you for reporting this!. @atsushi-ishibashi - Our docs have been updated. Please let us know if you have any other issues! Going to close this out.. Hello @dneralla, thank you for reaching out to us. To better assist you can you provide some logs, the error, and the code snippet to reproduce this.\nTo enable logging\ngo\nlevel := aws.LogDebugWithHTTPBody\nsess := session.New(&aws.Config{\n    LogLevel: &level,\n}). @dneralla - I am going to go ahead and close this issue. If you are still have issues, please reopen this!. Hello @echlebek, thank you for reaching out to us. I have gone ahead and marked this as a bug, and the correct solution should be to return an error rather than a panic.. Hello @rarguelloF, thank you for reaching out to us. Verbose messaging is disabled, so setting aws.Config.CredentialsChainVerboseErrors to true will give you a better idea to what is failing. Please let us know if you have any more issues.. Hello @fanminshi, thank you for reaching out to us. The best way to probably handle this is to use your own custom type for the Body. Taking a look at the s3crypto package will show you how we calculate a running md5 by doing this. Please see here.. Awesome, thanks. Will go ahead and close!. @stevenh @danielfm @Luizm - I've went ahead and reached out to the service team letting them know that there is some issue with that region, specifically. . Thanks for everyone's patient. Looks like the issue has been solved :). I'll go ahead and close this. If anyone else if having issues, please feel free to reopen.. @stevenh - That's great to hear :).\nThe best way to get a service issue handled is to reach out in the service team's forums and/or AWS support. If it is unknown whether or not it is a service issue or an SDK issue, reaching out here first is a good option also.. Hello @patthehuman, thank you for reaching out to us. It looks like there's a package conflict. Meaning that there is a local space SDK and a vendored SDK. I would take a look at your workspace to see if something like that sticks out.. You could take a look at the aws/version.go file. This has the SDKVersion constant.. You have competing SDKs, regardless of version. Meaning you have to either use the non-vendored SDK or the vendored SDK. Does that help clarify the confusion?\nedit: I don't think Go merged vendored content. So, there may be something internal that is conflicting.. @vaibhavkewl - I can definitely see this as a higher level abstraction. I don't think this should live on the client, as the client should just be the contract of the service's API, and any custom features should live in a layer above the SDK. So maybe something like a KeyBuilder or a function that does key building would be nice.. @vaibhavkewl - I think we are saying the same thing. However, the only difference with our way of implementing would be that we would add a helper function or some sort of key builder for users. Rather than just sticking a prefix field on the client. I hope that clarifies what I meant.. @simar7, thank you for reaching out to us. It is exposed, but as a field. Having getters and setters isn't really idiomatic to Go. So by following Go's standard we expose it as a field. See here for ContentMD5 and here. I am going to go ahead and close this since this is exposed. If you are having any other issues, please let us know.\nedit: It looks like we don't have the Content-Md5 on the response output, but I am curious if you need that if you have the ETag? Reopening since there needs to be some clarification.. @simar7 - From the service team's docs:\n\nObjects created by the PUT Object, POST Object, or Copy operation, or through the AWS Management Console, and are encrypted by SSE-S3 or plaintext, have ETags that are an MD5 digest of their object data.\nObjects created by the PUT Object, POST Object, or Copy operation, or through the AWS Management Console, and are encrypted by SSE-C or SSE-KMS, have ETags that are not an MD5 digest of their object data.\nIf an object is created by either the Multipart Upload or Part Copy operation, the ETag is not an MD5 digest, regardless of the method of encryption.\n\nSo depends on the type of SSE that was used.. @simar7 - I am going to go ahead and close this. If you are still having issues, please feel free to reopen this.. Hello @Aprimit, thank you for reaching out to us. Im confused at what you are trying to do. Could you please clarify? If you need to filter out of a string set, couldn't you just remove the elements you do not need and call the Scan operation again?. @Aprimit - Using our high-level abstraction should make this a lot easier for you.\nHere is an example pulled from the docs:\n```go\n// keyCond represents the Key Condition Expression\nkeyCond := expression.Key(\"someKey\").Equal(expression.Value(\"someValue\"))\n// proj represents the Projection Expression\nproj := expression.NamesList(expression.Name(\"aName\"), expression.Name(\"anotherName\"), expression.Name(\"oneOtherName\"))\n// Add keyCond and proj to builder as a Key Condition and Projection\n// respectively\nbuilder := expression.NewBuilder().WithKeyCondition(keyCond).WithProjection(proj)\nexpression := builder.Build()\nqueryInput := dynamodb.QueryInput{\n  KeyConditionExpression:    expression.KeyCondition(),\n  ProjectionExpression:      expression.Projection(),\n  ExpressionAttributeNames:  expression.Names(),\n  ExpressionAttributeValues: expression.Values(),\n  TableName: aws.String(\"SomeTable\"),\n}\n```. Hello @Razz, thank you for reaching out to us and taking the time to correct this issue. The waiters are all code generated. I will go ahead and mark this as a feature request and close the PR.. Hello @atsushi-ishibashi, thank you for reaching out to us. I have reached out to the service team to see what is the proper minimum size for that list. . Hello @fly1028, thank you for reaching out to us. Normal retries are capped at roughly ~5minutes. And it looks like throttles ~5 minutes.\nPlease see my example\nedit: nvm, there is an issue with the if statement not looking that it is throttled first.. Thanks for finding this @fly1028. I have a PR, #1654, for this. Please let us know if you have any other issues with this!. Hello @otto-md, thank you for taking the time to correct these issues. This looks good!. @ascendbruce - Thank you for taking the time to correct these issues :)! Since aws.Context is needed by default, it should be in the main function signature. I'll go ahead and merge this!. Hello @bwhaley, thank you for reaching out to us and reporting this issue. Im going to try and reproduce this on our end.. @bwhaley - Also, when you curl that request, what does the response look like?. @bwhaley - Sorry, I think I may be confused to what the issue is. Could you please clarify? What I am confused with is there is an Expiration field in the json from the curl request. Yet, I think the question is why is it in the response of the SDK? . @bwhaley - According to @hflamboauto1, regarding https://github.com/grafana/grafana/issues/9309,  it looks like the SDK is behaving properly.. Hello @beard1ess, thank you for reaching out to us. You can directly add the normalize region handler to your S3 client.\ngo\n  sess := session.New(&aws.Config{})                                                                                       \n  sess.Handlers.Unmarshal.PushBackNamed(s3.NormalizeBucketLocationHandler)                                                 \n  svc := s3.New(sess)\nYou can also call NormalizeBucketLocation.\nThis should correct the empty region. Another option, if you only need the region, is to use the GetBucketRegion in the s3manager package. Please let us know if you have any further issues.. Hello @beard1ess, can you provide the updated code snippet? If you could write a small test that only tests the SDK, something like this:\n```go\npackage main                                                                                                               \nimport (                                                                                                                 \n  \"fmt\"                                                                                                                    \n\"github.com/aws/aws-sdk-go/aws\"                                                                                        \n  \"github.com/aws/aws-sdk-go/aws/awserr\"                                                                                 \n  \"github.com/aws/aws-sdk-go/aws/session\"                                                                                \n  \"github.com/aws/aws-sdk-go/service/s3\"                                                                                 \n)                                                                                                                          \nfunc main() {                                                                                                            \n  sess := session.New(&aws.Config{})                                                                                     \n  sess.Handlers.Unmarshal.PushBackNamed(s3.NormalizeBucketLocationHandler)                                               \n  svc := s3.New(sess)                                                                                                      \ninput := &s3.GetBucketLocationInput{                                                                                   \n    Bucket: aws.String(\"examplebucket\"),                                                                                 \n  }                                                                                                                      \n  result, err := svc.GetBucketLocation(input)                                                                            \n  if err != nil {                                                                                                        \n    if aerr, ok := err.(awserr.Error); ok {                                                                              \n      switch aerr.Code() {                                                                                               \n      default:                                                                                                           \n        fmt.Println(aerr.Error())                                                                                        \n      }                                                                                                                  \n    } else {                                                                                                             \n      // Print the error, cast err to awserr.Error to get the Code and                                                   \n      // Message from an error.                                                                                          \n      fmt.Println(err.Error())                                                                                           \n    }                                                                                                                    \n    return                                                                                                               \n  }                                                                                                                        \nfmt.Println(result)                                                                                                    \n} \n```\nPlease let us know if the region is still empty for us-east-2. @beard1ess - Just adding the handler should fix your issue. closing this PR in favor of #2066 . @crwgregory, thank you for reaching out to us. Based off your last comment, I think this is probably something with the docker container as you eluded to. I think a better place for this type of question is stackoverflow. That'll give you a wider audience potentially some that use docker and the SDK :). I'm going to go ahead and close this as it seems this isn't an SDK issue. If you do find that this is an SDK issue, please feel free to reopen!. Hello @hwh33, thank you for reaching out to us. Generally the builder pattern doesn't modify the underlying thing you are building and returns a new object every time. So, we are following that pattern :). As you noted, this is a breaking change, meaning we won't make this change. Maybe Set isn't the best name. We could deprecate Set and replace it with something more obvious.. @hwh33 - Ah, I think the confusing part here is that Set isn't like a setter, but a reflection of Set in dynamodb. With that, I think it makes sense that it is called Set. It is also highlighted in the docs that this is the query language of the service Set and not a setter.. @Amitgb14, thank you for reaching out to us. Out of curiosity, what is that endpointURL?. @Amitgb14, thanks for the information. I went ahead and ran your example and was able to reproduce this. It looks like part of the xml response has a comment coming back which is excluding most of the contents. So, I'll reach out to the service team and explain the issue. . @yujinis - Any user should be able to do this today by overriding the http client the SDK uses. So, with that said, is introducing an environment variable meaningful here?. Awesome! Thanks for the example :) @yujinis . Hello @philm, thank you for reaching out to us and reporting this issue. I'll go ahead and forward this to the service team.. @philm - it sounds like that is an API inconsistency between the services. My guess is you are trying to use the asn field interchangeably but they are of different types. Did I understand that correctly?. @philm - Thank you for the clarification. I will report this back to the service team!. @philm - The service is looking into. Will follow up when I have more information.. Hello @Amitgb14, thank you for reaching out to us. Can we have a bigger code sample? It seems like this may be a race condition, but I can't be 100% sure without seeing the rest of the code. You may want to take a look at how you are using the fileReader and see if there is something that is racing with setting that when it is being used by another go routine.. Hello @0xSeven, thank you for reaching out to us. Are those credentials from a role? If they are, you need to setup a policy and enable IAM Database Authentication. Please see the service docs regarding contingencies of using the DB token.. Hello @bkatrenko, thank you for reaching out to us. I am able to reproduce this issue on my end and it looks like the service team isn't pulling the algorithm value out when generating the canonical request. I've went ahead and contacted the service team and let them know what the issue is. Once I hear back from them, I'll reply back here with any necessary information.. Hello @Puneeth-n, thank you for reaching out to us. I noticed that it is returning an InvalidParameterValueException, but the message suggests it's a throttling issue. How often is this lambda being hit? I'm assuming it's being hit in parallel?. Hello @cwedgwood, thank you for reaching out to us. AWS Transcribe is currently in preview. You can sign up for their service, here. They should be able to help you out. If you have any additional questions, it may be best to reach out to the service team directly. I am going to go ahead and close this. If you are having anymore issues or questions, please feel free to reopen.\nI have also asked the service team to make it clear on how to get started with the SDKs.. Hello @jeffb-stell, thank you for reaching out to us. I'm trying to determine the correct behavior here in that which SDK has this right. What tag is Ruby trying to read? If we put tag length of zero would Ruby still crap out? Because that is completely valid, imo. That is to say CBC has a tag length of zero. We could make the change on our end that if TagLen is \"\" set it to 0. Ruby however would need to update their logic as well. Have you tried this with Java? \nEdit: I'll have a PR to ignore that header if TagLen is \"\" as that seems to be the easiest path forward :). @jeffb-stell - I have a  PR set up to fix this, #1743. Please let us know if you have any issues with the change!. @hori-ryota  - We just release a new version of the SDK, v1.12.69, and deprecating v1.12.68. Please let us know if you are having anymore issues.. @hori-ryota - With PR #1758 will help catch future collisions. :). Thanks @hori-ryota! I'll go ahead and merge this once I take a look at the travis failure.. Hello @ronnylt, thank you for reaching out to us. This is due to the duplicate service folder mentioned here, #1753. Im going to go ahead and close this. Please continue discussion there!. @ronnylt - We just release a new version of the SDK, v1.12.69, and deprecating v1.12.68. Please let us know if you are having anymore issues.. @sdboyer @ronnylt  - With PR #1758 will help catch future collisions. :). Hello @sunilmaiya, thank you for reaching out to us. This repo is for the AWS SDK for Go. Could you please file this issue here instead. Im going to go ahead and close this. If this turns out to be an issue with the AWS SDK for Go, please feel free to reopen.. Hello @atsushi-ishibashi, thank you for reaching out to us and reporting this issue. That file is maintained by the service team and we would probably just want to update our example code generation to add the #. I'll mark this as a bug.\nedit: looks like this is invalid for all other SDKs as well. I'll go ahead and forward this to the service team.. Hello @yizha, thank you for reaching out to us. Can you try using the s3manager.Downloader? That will retry connection resets. Please let us know if you have any issues with that.. @yizha - How is the memory on that machine when you are running this? My first thought is since these are just firing off, you are loading everything into memory almost all at once.. @yizha - Thank you for the detailed information. I suspect you may be getting throttled by the service. Can you enable logging by setting LogLevel to aws.LogDebug and paste the logs here? More particularly getting the RequestId of the failed request will be very helpful.. @yizha - I've reached out to the service team, as I believe this is an issue on their end. The error you are seeing suggests that they may be closing the connection. I will let you know once I have more information.. @toobrien, thank you for reaching out to us. Looks like the example is incorrect. Can you please reach out to the service team as the service team maintains that example file. Im going to close this for now. If you are still having issues, please let us know.. Hello @ddcprg, thank you for reaching out to us. Could you request this change to the service team? Here is their forum. Cheers, :).\nI am going to go ahead and close this. If you have any feedback or other request regarding the SDK please feel free to reach out and open an issue.. :shipit:. Hello @hmcgonig, thank you for taking the time in constructing this PR. It looks like some of the unit tests have failed with this change.. @hmcgonig - Taking a look at the code, I am not seeing why this would improve memory efficiency. Could provide benchmarks and compare the two? Something similar to this. There looks to be also a couple critical sections. I know we can get rid of one of them by changing the method signature on processInputBody. But I want to first see how much more efficient this new implementation would be. Cheers!. Hello @veqryn, thank you for reaching out to us. We are encoding it because that is part of the HTTP RFC. Additionally, Golang's url.URL structure has a Path field that is unencoded. So, the SDK will escape and Amazon S3 will unescape it on their end. I am going to go ahead and close this as this is more of a question about the HTTP protocol than an issue.. Hello @nzoschke, thank you for reaching out to us. Can you give us the panic output?\nedit: Sorry, missed the arrow. What is errorString? That isn't in our SDK.. @nzoschke - I have a PR #1792 that addresses the client issue of not utilizing the context being passed. This still does require you to pass in a functional option to set a Paginator that utilizes a context.. Made a slight refactor to the BuildList method to instead call buildListElements. This cleans up the code with no need for the extra parameter in BuildList. Hello @sjeandeaux, thank you for reaching out to us. I am pretty sure you wouldn't need to set runtime.GOMAXPROCS. Are you seeing something that leads you believe you need to?. @sjeandeaux, yea, you shouldn't need to set it if you are using Golang 1.5 or above. I will go ahead and close this issue. If you have any additional questions, you can always reach back out to us.. Hello @prats226, thank you for reaching out to us. I would go ahead and file this issue in their forums as we code generate the docs from the models they hand to us. I am going to go ahead and close this as this isn't an issue with the SDK but the service providing either incorrect values from their service or the docs need to be updated.. @prats226 - Yes, the SDK uses code generation and generates documentation, the comments, for the services. Please file the issue within that forum, cheers :).. Hello @bharath-srinivas, thank you for reaching out to us. The s3manager client is different from the batching clients, and the s3manageriface is to highlight the functionality in the s3manager client. The batching clients take iterator interfaces which allow you to do the mocking and testing there. Here is a unit test that uses a mocked interface. If you have any questions regarding this, please let us know.. Hello @doertedev, thank you for reaching out to us. I was easily able to reproduce with the code you provided. Thank you for that. I have went ahead and asked the service team for clarification on their tagging documentation. Once I hear back from them, I'll reach back out.. Hello @btsteve, thank you for reaching out to us. I see the operation here. Is this what you're looking for?. Hello @tlawrence, thank you for reaching out to us. I think the best way to handle this is to write a custom retryer and look for NotImplemented error code and not retry. If it is not that error code, fall back to the default retryer. Please see here for the Retryer interface.. Hey @bflad, thanks for the information and suggestion. I think that makes sense to not retry on 501 HTTP status codes. Let me reach out to the team and have a discussion around this and information to why we retry on this is needing to be investigated.. @bflag, we have a PR out, #1826, that will remove retrying from 501 status codes. :). Shouldn't we also strip off the content length of the hash once we remove it?. Hello @Broham, thank you for reaching out to us. The SDK provides an io.Reader back. So, you should be able to use it as you see fit unless I am misunderstanding your question. Does that help answer your question? If not, could you elaborate on what you mean?. Hello @jdlehman, thank you for reaching out to us. This looks like an issue with the signing name not being specified in the endpoints file. I'll go ahead and reach out to the service team to get this fixed.. @jdlehman - I've went ahead and merged in a customization for the service that will correctly sign with the proper name. Please let us know if you run into any other issues!. Hello @patrobinson, thank you for reaching out to us. Can you enable logging and paste that information back here?\ngo\nl := aws.LogDebugWithHTTPBody\nsess := session.New(&aws.Config{\n    LogLevel: &l,\n}). Hello @pwaller, thank you for reaching out to us. Try getting the original errors from the BatchError type. That may give you more information. Please let us know if the errors are still empty after doing the proper casting.\ngo\nbErr, ok := err.(*s3manager.BatchError)\nif !ok {\n    panic(\"should have been a batched error\")\n}\nfor _, e := range bErr.Errors {\n    fmt.Println(e.OrigErr)\n}. @pwaller - Absolutely. I've went ahead and created a #1851 to alleviate the confusion on the error string. I'll do a little digging to see why OrigErr is empty.. @pwaller - I added another commit. Looks like error messages that were being returned by the service would get lost. Please let us know if you have any other issues.. Hello @svenwltr, thank you for reaching out to us. Try setting DisableRestProtocolURICleaning in the aws.Config. That should preserve the extra /. Please let us know if that resolves your issue.. Hey @bflad, thank you for reaching out to us. It seems this may have been resolved as I cannot reproduce this. If you are still running into this issue, please reopen this.. Hello @tomvachon, thank you for reaching out to us. I was able to reproduce this and have reached out to the service team to see what's going on. Once I have more information, I'll update the issue.. Hello @lzbpythoner, thank you for reaching out to us. Can you provide a code sample to help better diagnose the problem?. Hello @tahoward, thank you for posting your interesting on this feature. Out of curiosity what is the use case? I'm a little concerned for allowing reading of a file to fork and run a process. Sends a lot of red flags towards my way when I hear that.. Hello @benjafire, thank you for reaching out to us. I agree that it is awkward to having to set one field as base64 encoded and the other as not. While we can't change this currently as it would be a breaking change in the service package, we could add maybe add a helper function similar to aws.String or a struct tag signifying when something needs to be base64 encoded automatically. While the struct tag would do this automatically, it would not do this for pre-existing APIs. . Hello @ellerbrock, thank you for pointing that out! I have went ahead and created a PR to address this issue.. Hello @AndreaM16, thank you for reaching out to us. There is already a conversion abstraction here. If you have any other questions please reach out to us!. Hello @tosh001, thank you for reaching out to us. I can't seem to reproduce this. Are you still having issues?. Hello @haines, thank you for reaching out to us. This would be a great feature to have in the SDK and we would be glad to accept any PRs :)!. Thanks @hoshsadiq!\nAs @hoshsadiq already mentioned, #2201 now supports credential_source. Please let us know if there are any issues or questions with #2201.. Hello @johanneswuerbach, I've went ahead and expanding on your PR to allow for users to opt into this functionality in addition to fixing the paginator for the service. Please see #1945! And thank you again for taking the time to do this. I'll go ahead and close this PR.. Hello @artyom, thank you for reaching out to us. This seems to be an issue on the service's end. What's funny about that key is it will prevent that bucket to be deletable and that object is unable to be deleted either. So I've reached out to the service team to see what they can do on their end to resolve this issue, which in turn may resolve this issue.. Hey @artyom, thank you for your patience on this, but it looks like that is the preferred way, as the service team put, to deal with illegal XML characters. As this is documented, regarding what characters are safe to use, and the work around is to use the EncodingType, I am going to go ahead and close this. If you have any other questions, please reach out to us!. Hello @ken5scal, thank you for reaching out to us. This is a service issue. Can you please reach out to the service team via the forums?. Hello @akskap, thank you for reaching out to us. Looking at the documentation, it looks like this is the correct behavior. However, if I set my owner ID in the list of OwnerIds, it'll return the correct amount for that region. Have you tried that? The owner ID would be your AWS account number.. Awesome! That's great to hear @akskap. I am going to go ahead and close this. If you have any additional questions, please reach out to us again.. Hello @jazzyarchitects, thank you for reaching out to us. I took your sending logic and was not able to reproduce this. Can you enable logging? That will tell us what your contents look like. Also, what does your stream look like? Does it use any sort of transformers or anything?\ngo\nl := aws.LogDebugWithHTTPBody\nsess := session.New(&aws.Config{\n    LogLevel: &l,\n}). hey @jazzyarchitects, can you provide logging with a failed request? Cause it looks like the request you sent worked correctly? Or did it return an error?. @jazzyarchitects - I wrote a quick example to see if I could reproduce this, but it looks like my stream successfully uploaded to Amazon Elasticsearch Service (ES).\nMy stream uses direct input and no transformations. Very basic stream to simply forward data to ES.\n```go\npackage main\nimport (                                                                                                                 \n  \"encoding/json\"                                                                                                        \n  \"fmt\"                                                                                                                    \n\"github.com/aws/aws-sdk-go/aws\"                                                                                        \n  \"github.com/aws/aws-sdk-go/aws/session\"                                                                                \n  \"github.com/aws/aws-sdk-go/service/firehose\"                                                                           \n)                                                                                                                          \nfunc main() {                                                                                                            \n  sess := session.Must(session.NewSession(&aws.Config{                                                                   \n    Region: aws.String(\"us-west-2\"),                                                                                     \n  }))                                                                                                                      \nobj := map[string]string{                                                                                              \n    \"foo\": \"bar\",                                                                                                        \n  }                                                                                                                        \nb, err := json.Marshal(obj)                                                                                            \n  if err != nil {                                                                                                        \n    panic(err)                                                                                                           \n  }                                                                                                                        \nsvc := firehose.New(sess)                                                                                              \n  resp, err := svc.PutRecord(&firehose.PutRecordInput{                                                                   \n    DeliveryStreamName: aws.String(\"stream_name\"),                                                                       \n    Record: &firehose.Record{                                                                                            \n      Data: b,                                                                                                           \n    },                                                                                                                   \n  })                                                                                                                     \n  if err != nil {                                                                                                        \n    panic(err)                                                                                                           \n  }                                                                                                                        \nfmt.Println(resp)                                                                                                      \n} \n```\nDoes the example work for you with your stream name?. @jazzyarchitects - I think @jasdel is correct here. It seems you may be already calling json.Marshal. Meaning you are json encoding your body twice, which is unneeded. It is why Amazon Elasticsearch Service is unable to parse the json. Please let us know if the example above works for you.. Hello @astronoka, thank you for taking the time to fix this bug! The changes look great.. Hello @mitchlloyd, thank you for reaching out to us. I believe theSET operand needs to be used in this case. Have you tried that?\nPlease take a look at the docs here regarding lists.\nThere is also an example of appending to lists. Please let us know if you have any other questions.. @mitchlloyd - Ah! Forgot about the sets. Yea, this may be a bug. Let me try to reproduce it on my end.. @mitchlloyd - I think jasdel's comment regarding adding string sets is going to be the best way to do that. The expression package looks to lack some functionality for specifying string sets. I'm going to mark this as a feature request for the expression package. Please see jasdel's example to get unblocked. If you have any other questions, please let us know. Hello @mikkeloscar, thank you for reaching out to us and proposing this. We have a very similar concept proposed. I'll mark this as a feature request and bring this up in our next sprint.. Hello @mikkeloscar, thank you for pointing out some various points on this feature. I've went ahead and started working with EKS to see what solutions may be possible. Once we land on something concrete, I'll update this issue and hopefully have a PR with the implementation.. Hello @jbergknoff-rival, thank you for creating this. I've went ahead and used your PR to build upon the other missing values for credential_source: EcsContainer and Environment. Going to close this in favor of #2201. Any feedback is most welcome!. Hello @utahcon, thank you for taking the time to fix this! I'll go ahead and merge it!. Hello @shatil, thank you for reaching out to us. The SDK does very little client side validation, ie) required fields. We generally rely on the service to handle validation. I am going to go ahead and close this as the SDK is behaving as designed. If you have any further questions, please let us know.. @shatil  - Yes, we validate against minimum and required, but nothing else. Also, the documentation is autogenerated from what the service team gives us. However, I totally understand why this can be frustrating since there is no comment or docs on about this limit! It may best to reach out to the service in the forums about changing the docs to include the limits.. Hello @leonsim, thank you for taking the time to compose this PR. We are always looking for ways to improve the SDK! However, this change would be breaking existing users if they were using any of those fields. I am going to go ahead and close this PR. If you find any further improvements, we would definitely like a PR or issue! . Hello @rsm10, thank you for reaching out to us. I believe there is a couple reasons to why a lot of memory is being used. The first being that the marshalers take a lot of memory due to the use of reflection, and secondly the lack of pooling as you had stated. I'll go ahead and mark this as a feature request. If you have any additional questions, please let us know!. Hello @ngsw, thank for the correction! Much appreciated :). Hello @paulnivin, thank you for reaching out to us. I think the best fix here is to add a waiter option instead of changing the waiters in general. I think that'd way the control is left up to you to dictate on how long you want to wait. What're the thoughts there?. @paulnivin, @mwhooker - I'll go ahead and reach out to the service team and see if they can update the documentation to give users a better idea on how long each of the instance types roughly take to initiate. I believe i3.metal is the only instance type that doesn't satisfy the default waiter implementation, is that right?. Hello @bashtoni, thank you for reaching out to us and filing this issue. I am going to go ahead and mark this as a bug. I think the SDK should be omitting time.Time if it is a zero value.\nedit: we may have to introduce a config option, something like OmitZeroTime, to have the omission be opt in. This would prevent breaking changes for users who may be relying on this.. @bashtoni - After looking it looks like this isn't a bug as this code mimics json's marshaling. We see that json does not distinguish an empty value for time. We are going to go ahead and close this as this is behaving as intended. Please let us know if you still have questions!. Hello @jbmchuck, thank you for fixing that! Looks good. Going to go ahead and merge this :).. Hello @bpot, these changes look good! Going to go ahead and merge this in.. Hello @bpot, thank you for taking the time to construct this. I imagine the data that is being benchmarked here is small. I am curious if you benchmarked against a larger file, couple gigs, if that delta goes down. If so, I don't know if we want the makeSha256Reader return an error as that might cause some unexpected behavior in user's application. Let's first benchmark against a larger file and see if this makes sense to potentially pull in, especially since we are talking about ns.. @bpot - Ah, assumed all of the first column was cpu efficiency. Yea, I'll publish some feedback and see what we can do about potential breaking changes areas. Thank you @bpot for taking the time to make those fixes. I just merged those changes in :). edited:\nHello @nwalke, thank you for reaching out to us. It looks like this is actually lacking on our end as the scoping of source_profie was misunderstood. I'll go ahead and mark this as a feature request.. Hey @joelddiaz, thank you for taking the time to create this PR. I think the initial thought here was that you could call Err on the iterator to return the specific error, but I think adding it to the batch error list makes sense as the first element. I'll go ahead and merge this.. Hello @tomelliff, thank you for reaching out to us. I've went ahead and contacted the service team as they are sending the incorrect timestamp format for their protocol. I will let you know when I hear back form them.\nAlso, the files you've edited are code generated, and while that is a way of bypassing the issue, I would wait until the service team fixes it on their end.. @tomelliff - The service team has been made aware that this is an issue on their end. They've also informed me they are working on a fix but have not given me a timeline. And yes, once they fix it on their end, the SDK doesn't need to be updated and should start working as intended. We will keep you updated once we have more information!. @JGalego - The service team has the fix completed and is currently testing it. Once we have news on when this is rolled out, I'll post back here. . @ryan-dyer-sp @JGalego @tomelliff - The service team has released a fix for this. Please let us know if you guys are still having issues. Going to go ahead and close this.. Hello @delitescere, thank you for reaching out to us. This would be a breaking change for the SDK and something we would not be able to support without introducing another environment variable. This is just an inconsistency of how SDKs handle environment variable. In addition, AWS_DEFAULT_REGION was introduced later, which is why the behavior is the way that it is. Since this is a breaking change, I will go ahead and close this. I suggest using AWS_REGION for both the CLI and the SDK. If you have any other issues, please let us know!. Hello @akamensky, thank you for the feedback. Your opinion absolutely matters to us! After taking a look at this, I agree. The documentation MUST match the API call. The service team has been made aware of the issue. This also seems like a breaking change if it went from idempotent to not.If this has always behaved as so, then the documentation need to be fixed. Regardless the service team knows that they need to resolve this.\nIf you have any more feedback, please share them :).. Hello @YakDriver, thank you for taking the time to create this PR. We have reviewed this internally and there are some concerns we have. I've decided to pull this in and refactor some of this based on the concerns and ensure that we get this reviewed internally as well. I just wanted to let you know that we have looked at this and we have decided to only allow the credential process provider to be enabled via code and not the shared config. So, I'll go ahead and make those changes and if you have any questions or feedback, please let us know.. @lorengordon - Out of curiosity, why is this a major issue? Would it be difficult to not have a predefined location of said credential process, ie /usr/bin and have you code just reference that location? Why having it in shared config such a need, I guess is what I am asking? What is your use case?\nReading the AWS CLI docs it explicitly states,\nWarning\nThe following describes a method of sourcing credentials from an external process. This can \npotentially be dangerous, so proceed with caution. Other credential providers should be preferred\nif at all possible. If using this option, you should make sure that the config file is as locked\ndown as possible using security best practices for your operating system.\nFound here. This is a major reason why we are potentially deciding this to only be allowed via code.. @lorengordon - Thank you for the feedback, and I want to let you know that we are not finalized on a decision as we want more feedback from users. However, some things to consider we want this to behave similarly to how the spec is defined internally. This is why we want to pull this down and make the changes. Also, having this only code enabled shouldn't be very different than how the other SDKs do it today. In addition, we aren't making a decision until we've received feedback from users and we have a meeting with a user tomorrow to ensure that we can implement this safe, secure, and easy to use. I believe we can achieve this and your feedback will help with that. \nHere's what we were thinking,\n```go\ncmd, err := processprovider.NewCommandFromConfig(\"/path/to/config\", \"profile\")\nif err != nil {\n    return err\n}\np := processprovider.NewProcessProvider(cmd)\ncfg := &aws.Config{\n    Credentials: credentials.NewCredential(p),\n}\nsess := session.Must(session.NewSession(cfg))\n```\nPlease let us know if this will work for you. After much discussion, we've decided to enable this through a field on the session construct. \ngo\nsess, err := session.NewSessionWithOptions(session.Options{\n    EnableCredentialProcess: true,\n})\nThis allows for users to enable and opt into the feature that has potential security holes rather than having this be a security risk to all users who may not be aware of the feature. I will go ahead and take some time to construct a PR and any additional comment or feedback is most welcome.\n@lorengordon - Other than the security risks associated with enabling it automatically via the shared config, this would also be a backwards compatible change for users that may be using the CLI and the SDK. For instance, if the CLI and SDK were being used and the configuration would fail in the SDKs chain and find the next provider to use and potentially succeed. However, suddenly having this enabled would be a change of behavior and may be unwanted.. After going back and forth internally, we've decided to have this behavior behave like the CLI.\n@YakDriver - Can we get this PR rebased? In addition, we need to document that this is a unsafe feature to have in the config, similarly to how the CLI documents theirs. I also believe the CLI has the credential process provider last in the chain. Can we ensure we behave similarly through some tests?. Hello @ktravis, thank you for reporting this. You can mitigate this for now by putting \" around the credentials until we get #2240 merged.. @ktravis - We just merged in #2240. Please let us know if you have any other issues!. Hello @jritsema, I believe this has already been fixed, #2247, but hasn't been released yet. Please try using HEAD to see if you are still running into the issue. If you are not, the next release should contain the fix. I am going to go ahead and close the issue. If you are still having issues, please let us know and we will reopen.. Hello @rafaelfc-olx, thank you for reaching out to us and reporting this. The easy mitigation is to wrap your string in quotes and I'll take a look at fixing this.. @rafaelfc-olx - We just merged in #2247. Please let us know if you have any additional issues.. Hello @ngsw, thank you for reaching out to us and reporting this. I've went ahead and created a PR, #2255, that addresses this. I don't think we should be limiting this check to just a specific key. Please let me know if you have any additional issues.. @ngsw - We just merged #2255 in. If you have any additional issues, please let us know!. Hello @ngsw, thank you for taking the time to fix this. I have create a PR that addresses this, #2255, as I did not want a magic value of aws_secret_access_key. I am going to go ahead and close this. If you have any other feedback, please take a look at #2255.. Hello @alienth, thank you reaching out to us and reporting this. This is a bug and I will go ahead and mark this as such.. Hmm, I can't seem to reproduce this. Here's what I have as a test profile and it seems to work properly,\n[hyphen-profile-name]                                                                                                      \nregion = \"foo-region\". @alienth - Good to hear that it was a simple mistake! I will go ahead and close this. Cheers.. Going to reopen this as this is still a bug and should either parse or return an error and not silently fail to read the section.. @alienth - Just merged in #2265. Please let us know if you have any other issues!. Hello @cullenmcdermott, thank you for reaching out to us and reporting this. I've went ahead and created a PR, #2282, that fixes this issue. Please let us know if you have any additional question or issues.. *character\n. Shouldn't this be just echo \"-example\", this should be fine because it is in the shell script so the carriage return and newline shouldn't affect this. The main issue is echo is echoing out -n -example, which breaks the vet tool\n. This needs to be a defer immediately after the lock\n. Possibly combine these three funcs, encodeInt encodeFloat encodeUint, and just use fmt.Sprintf(\"%v\", val) and change to encodeNum?\n. Is this if statement needed? Wont the next if statement fail and return the same thing?\n. if encodeNum is used can replace all casting with just encodeNum(typed)\n. Im guessing this is another optimization, but compressing these for loops into one is also possible.\n. The ft.Name() == \"\" seems redundant. Isn't pointers always unnamed types? \nunnamed type full def\nunnamed types typelit def\nDon't know why the golang source also does this also.\n. What I was thinking was\n``` go\nfunc fieldByName(fields []field, name string) (field, bool) {\n    foldExists := false\n    foldField := field{}\nfor _, f := range fields {\n    if f.Name == name {\n        return f, true\n    }\n    if !foldExists && strings.EqualFold(f.Name, name) {\n        foldField = f\n        foldExists = true\n    }\n}\n\nreturn foldField, foldExists\n\n}\n```\n. Yea, that'll make that look a lot better\n. It is pushed to a stack. So, drain will happen first.\nhttp://go-vim.appspot.com/p/5JrUe61K6N\n. What is meant by \"will handle them\"? Is this suppose to be \"will not\"?\n. Odd spacing on comment\n. :+1: like the separation.\n:shipit:\n. Wouldn't it be possible to just update\ngo\ntype xmlErrorResponse struct {\n    XMLName   xml.Name `xml:\"ErrorResponse,ServiceUnavailableException\"`\n    Code      string   `xml:\"Error>Code\"`\n    Message   string   `xml:\"Error>Message\"`\n    RequestID string   `xml:\"RequestId\"`\n}\nthen check the name rather than unmarshal twice?\n. You are right. We could just get rid of the ErrorResponse and ServiceUnavailableException and just check the name that way? But I don't know if I like that, what do you think?\nOther than that, :shipit:!\n. Okay, thanks for the clarification. LGTM\n. Change to pointer since optional\n. Same here\n. Check for nil instead\n. Add policy field too\n. hrm, unsigned headers now won't be included.\n. It would seem that if some header that didn't satisfy any of the rules would not pass through.\ngo\nif !r.IsValid(canonicalKey) {\n    continue // ignored header\n}\nWhich means this header never gets added to the v4.signedHeaderVals and then would panic\ngo\nstrings.Join(v4.signedHeaderVals[k], \",\")\nsince it doesn't exist. I think a new method with less strict rules than http.CanonicalHeaderKey needs to be implemented. I hope that clears that up!\n. Oh! Didn't see the pluralization! Okay, yes, then this should be fine.\n. Add one more test here\nreq.Header.Add(\"X-amz-Meta-Other-Header_With_Underscore\", \"some-value=!@#$%^&* (+)\")\nI think this test would fail. But should check to be sure\n. Why was this moved?\n. Wouldn't this always cause one less iteration? It may be best to update the unit tests. \n. The previous change would increment to 3 and then break on an io.EOF since there are no more parts. In this case it would start at 1, increment to 2 and upload the next part with no errors. Finally, increment to three and set the err to the excess error, but we want to break on an io.EOF and have nothing set to the error. Does that sound right? Let me know if I am missing something\n. - send first part outside loop\n- num = 1\n- if num > MaxUploadParts -> no\n- num++ (2)\n- send second part\n- if num > MaxUploadParts -> no\n- num++ (3)\n- nextReader() return err\n- err is io.EOF\n- break\nLemme see what that unit test is doing\n. Okay, nvm. Drew it out. I see what you are saying. Can you add one more unit test to test for EOF?\n. same here\n. copyHTTPRequest\n. same as above\n. Should add a test testing the cancel functionality\n. I can write this on my end and add a test. I will send a PR and cc you on it.\n. This is never set anywhere in the cli/gen-api folder. Should it be?\n. We do not want to call to JSON. We want to decouple from JSON.\n. Same as comment above\n. Thanks @spaceweasel. I believe Parse and Format will make for more readable code.\n. Update comment not to refer JSON\n. same as above\n. Will remove once all is complete\n. Yea, I think if we want to specify a default. I think I may put the Encrypter and Decrypter on the client. That way when calling the NewClient users can specify which algorithm they want to use. Does that sound adequate? The only downside I see to that is users cannot specify per request. Might have to do some sort of config for each operation or something. How does that sound?\n. Yea, that was for specifically cbc, that's going to change with the comment above.\n. Ooo, I like this idea. I will prototype a small design. \n. @dcoker - Thank you for the feedback. Our goal is to make it very easy to use! So the information here is very valuable. I think having the algorithm part of the client-level config may be the best bet and providing variadic configs. \nIn addition, some objects may not have the crypto algorithm in the metadata. In particular v1's implementation of the Java's client side encryption. Maybe if V2 metadata is encountered decrypt with proper crypto algorithm, else use the Decrypter and attempt to decrypt it. Let me know what you think!\n. Should we only set this if err was nil?\n. What is this if statement checking? If it is a []byte slice already, do we need to check that it is a slice?\n. if the if statement above exists, does this if statement also need to be present?\n. Is the go compiler smart enough to cache the len(b) calls? \n. It isn't meant to create but decode the data that resides in the envelope.\n. Copy pasta, will fix!\n. Naw, just grabs the V1 envelope from the headers. However, we no longer support v1 reads due to padding attacks. So, may be best to remove the method\n. Way better name. Will fix!\n. I don't think any SDKs behave that way. I think that was the purpose of the suffix, was allow the use of your own key.\nBut will rename!\n. Yea, I was walking down get also generating values if none was called. However, as I started thinking reusing of the client, it could potentially not generate a key or iv as a bug. Further it made the logic a little wonky. So I decided to just separate the two. If that is what you were talking about\n. Yes, that makes sense. I'll go ahead and change that.\n. Yea, I can definitely see the confusion there. The difference here is that the material description is populated with needed data. The other one is populated with whatever the user wants.\n. Great point. Yea, I'll add that to the config\n. Worry is that users may be clearing one or the other for whatever reason.\n. That makes sense. I'll change that\n. This was a little confusing to me by the name. I was thinking this was exporting to some environment variable. Perhaps getEnvValue(key []string) string may make more sense or something along those lines?\n. *will not set fields\n. What is the provider name here? Do we care about empty ProviderName's? \n. No longer an issue with bytes.Buffer\n. A new mode would be created. Each mode is tied to a cipher. If a user wants to add their own cipher, they'd simply add a new mode. The reason for this design is so we didn't need some sort of enum or construction function to be passed around.\n. Path is misspelled, example/service/ec2/*filterInstances*\n. instances misspelled here\n. Should just remove this and hit some region instead of iterating through all of them.\n. Can we also add the same comment in the README.md about running the file here?\n. Yea, that makes sense. We want our examples to be as simple as possible. That way when users who want to add their own customizations to them, can. So, I think the change would at least be able to benefit all users without them needing to remove code, but only add it.\n. Remove this, see comment below\n. for loop is not needed. Just define a new variable region := \"us-east-1\"\n. they are lowercase when getting the data. Originally this was using casing but was finding nothing.\n. Naw, removed the TODO\n. One last change. Add this comment above main for consistency.\ngo\n// go run filter_ec2_by_tag.go <name_filter>\n. Removed due to bytes.Buffer\n. I originally tried it with type aliasing. The issue here is we can't cast a type aliased thing to a map[string]*string\n. Add as method Envelope\n. Update comment\n. Change this to be aliased to map[string]string and keep marshaling/unmarshaling private\n. SharedConfigProfileNotExist probably should be a const, since used in multiple places\n. this keeps iterating if an ini file has been found. Do we want to break if err == nil?\n. f.Name() is more of the issue, but yea, I cleaned it up by moving 147 into useTempFile if statement. I'll move this to a helper method.\n. Now that it is moved into the if statement above it will always be an os.File\n. Alright, may make unit tests a pain. Clearing send is done quite a bit. So I'll need to update that.\n. Yea, I would still need to wrap the aws.WriteAtBuffer due to it not supporting Write when using io.Copy\n. go\nconst logDumpErrorMsg = `DEBUG ERROR: Request %s/%s Details:\n---[ REQUEST DUMP ERROR ]----------------------\n%s\n-----------------------------------------------------`\nSomething of this nature, that way we have some consistency with the current logging.\n. Do we want to continue on logging the dumpedBody, if an error was thrown? It is probably an empty string. This would make the log after this one pretty much useless.\n. Same as above\n. This, before, doesn't really test anything. Was testing that you can assign a new pointer to another pointer. Which is always not true when compairing they are the same. So this test would always pass\n. Have to check *time.Time explicitly, because doing this generically would be a breaking change.\n. We should still document that the New function has been deprecated.\n. Get rid of only this line instead.\n. Spacing is off here.\n. Add usage example here\n. go\nfor retry := 0; retry < d.partBodyMaxRetries; retry++ {\n}\n. With the logic moved into the for expression, perhaps breaking early makes sense?\ngo\nif err == nil {\n     break\n}\n. Then here upon for exit\ngo\nreturn err\n. Can get rid of this if block\n. Removed spaces. Also, the tags reflect the shared feature file.\n. Agreed!\n. Yes, those arns were in a paragraph. \n. Yea, the reason why it does that is due to how this parser works. I am working on a modification to extend it to be smarter about nested tags. \n. Yes, already changed. Was because the stack came later when I realized I needed one\n. Yep! Had the same thought and went ahead and changed it.\n. Yea, completely agree, one of the todos is to clean this code up and make it more maintainable.\n. Are you talking about the append?\n. Yep, going ahead and doing so\n. Ah, that makes sende. Ill go ahead and fix that. Thanks!\n. couldn't we just return return !(s.Streaming || s.Shape.Streaming)?\n. no getters\n. Should point to our docs - http://docs.aws.amazon.com/sdk-for-go/api/service/dynamodb/dynamodbattribute/#Marshal\n. I think we should also educate on how to add credentials without the CLI as well. Linking to the dev guide would probably be good here.\n. You can leave the Data tag out\n. I am also wondering if this is the right place to put this. Perhaps, take this out and maybe adding it to our dev guide may be a better option.\n. Can we also add the length of the slice here? Some like <binary> len %d\n. what do you think about somehow getting rid of those two empty strings somehow? It seems that NewClient doesn't use those two parameters anymore, since the resolver is going to handle all of that. Maybe deprecate NewClient and add a new method?. Why does version need to be an int? Leaving it as a string seems better. How do you feel about adding a map[string][]func(*Partition) here? The key would be the service and it could iterate through all the customizations.\nThen for things like ec2metadata, we can just leave that the way it is.. Seems odd to check for empty string and also check for length before calling AddScheme.. if len(ps) == 2 wouldn't the resulting string be foo, and bar when it should be foo and bar?. Hrm, yea, if anything if there is every a need to change it we can. \nI can't think of any good reasons for the change at least for now.. don't need to do the last iteration\ngo\nfor i := 1; i+1 < len(parts); i++ {\n// ...\n}. xmlns is duplicated?. Print?. seekable := true. Can get rid of this. I'm thinking adding a util function that does all this and returns the size of the body.. How do we feel about returning an error? I feel like when we get an error in here, it'll at least clean it up some, rather than mutating the request's error.. Do we have tests to test the non-seekable case?. :+1: this looks a lot better. !(v4.DisableRequestBodyOverwrite || ctx.isPresign)\nDon't know if the Go compiler does De Morgan's laws under the hood.. Constant name conflict. Link to AWS Support page would be nice from here too.. What about linking here to https://github.com/aws/aws-sdk-go/issues/new?. Yea, good point. Fixed. How do you feel about merging this into a single if block?\ngo\nif !(hasService || opt.ResolveUnknownService) {\n}. Support*. We are defaulting this to true?\nI wonder if we should just always resolve it and not have a boolean. I understand that adding it will allow users to disable if needed, but I just can't think of any meaningful use cases for this.\nThoughts?. So, if endpoint isn't specified, it'd just be an empty URL?\nDoes this get populated at some point during the endpoints chain?. Oh cool, thanks for the clarification. Definitely like this addition. +1. oh yea, that totally makes sense.. +1, way better name . Is there a reason why this isn't an interface but a function pointer?. Cool, yea. Better name +1. Yep, just caught that myself. Yea, that's a way better spot. Will make the change. Yea, apparently during one of the traversals it walks down to the map value. However, if we don't need to walk the map, it might fix this.. Yea, that's a good point.. will change. Yea, I'll bootstrap a JSONValue type when going through the shapes passes.. For this, we don't even need to do that. We can just return a string map[string]interface{} and whether or not that is required. This is only used in example code.. Woops, the comments aren't needed. Gah, forgot to commit the private/model folder it looks like. Different case. Do we want to squash a potential io.EOF? I feel this could be meaningful information.. ?. same here. Same here. WaiterWithDelay is misleading. I assumed this would return a waiter with a delay. Perhaps a better name? WaiterOptionDelay?. same here. +1. Same here. Perhaps log that a deprecated method is being used. Same here. Here too. deprecated. deprecated. *deprecated. I think we don't want to remove the s3 check until we know the trait has been added there. That may need to be a separate PR.. The spec says that when using UNSIGNED_PAYLOAD that header needs to be set.. Yea, that seems like a good approach. I'll rework that to be more generic.. The issue here is we don't know which signer is really on there. It could be v#. Let's say we version bump to v5, we wouldn't know which signers were in the handler list, v2, v4, or v5. I mean we could just remove v2, v4, and v5 signers and just add whatever was specified.\nWouldn't the custom handlers come afterwards anyways? Im trying to think of a case where it is added upstream, but how the SDK is currently constructed, I don't see how that's possible with this implementation.\nCould you please elaborate?. Oh yea! Good catch. I'll think about this some, and come back with the change once I think about some solutions.. Yea, I like that a lot better. :+1:. Seems weird with the boolean and the name being WithUnsignedPayload. Minor, but I feel this may be a little cleaner\ngo\nfor attempts := 1; ; attempts++ {\n}. It seems like if a SleepDelay function is specified and someone canceled the request, that SleepDelay will always win. Is that what we want?. What do you mean '1' based?. Still confused to why we don't care about cancelling here. :+1: for constants. We *will not. Do we want to make this more generalized where we can swap in a hash algorithm?. Hmm, that's a weird place for it.\nI'll go ahead and rename this to ErrCode*. However, why should a response time error code live in the request package? Just for consistency?\nI think ErrCodes should live in the awserr package, because that's what it is apart of. I think adding it to request will add a lot of unrelated things to that package. Even the error codes that are defined in the request package are not named ErrCode*.. This creates a new timer per read. So, will be fine :). @twang-rs - Yea, if overhead does become a concern, we will look into optimizing other things. We would only keep the timer if that was our last choice. The creation of a timer shouldn't be the bottle neck, at least. I plan on adding some benchmarks to show the overall difference in performance with the changes.\nHowever, comments like this is great discussion and opens up for improvements overall. So any other feedback is welcomed!. append isn't needed since we are less than cap?. Is this line needed?. Is len important though? Because I imagine that it'll look at cap == len to dictate whether or not to allocate new space. I think the slice trick assumes cap == len and since len < cap, I don't think any allocations should be happening. But I may be wrong here.\nWhat I was thinking is if we got rid of the append all together, it'd have the exact same benchmarks. I can check to see if this is the case.. Looks like there is a misunderstanding on how length works with slices on my end. I assumed if its cap > len you could still access those slice elements regardless of length. Now that I think about it, that doesn't make too much sense. Thanks for clearing that up. Cool, this looks good.. Hrm, so if it fails to parse, it'll still continue on. I wonder if it is better to just return and empty Provider or nil.. Can we just call GetBucketRegionWithClient from here? We wouldn't need to set as many arguments in the client, and this method would be simplified.. Only issue I have with this is other handlers could potentially stomp over this value due to the ApplyOptions below. Yea, should it be documented then that setting Retryable in handlers will have incorrect behavior when calling GetBucketRegion?. I think adding steps to reproduce should be a header as well. minor - *the SDK is\nextra space. Should return a bool here as well for consistency and distinguishing a difference between an empty service-region map versus empty map. This is autogenerated.. same here.. Should this exist in a handler or in the request.Send method? Since users can clear the handler list, possibly moving this to request.Send may be a better location? Thoughts?. The one thing I think that is important here to mention is that ResetBody must be called before calling sending this over the wire, because if not, then the GetBody would never get set and redirects would not work. . You override the func map with the customization.. This could panic for someone that passed in an invalid string. Should we document that or check for length?. Yea, that is part of the docstring issue of listing errors. I was going to submit that as a separate PR. Yea, linter was complaining about v being shadowed. be*. Yea, but then I would have to concatenate the escaped newline.. arg is never used in the example. naw, it would, if you look at the !parity in the if statement. It adds o to the slice.. What would this function look like if an int64 was returned instead of a string?. Minor comment\nWhy is s3PutObjectAclInput in this function closure, but above it is being stored in params?. Okay, that's fair. LGTM. Is this TODO still valid? Or are we going to put this somewhere else later?. Would prefer fmt.Sprintf here. fmt.Sprintf here. same here. Why are we starting at 1 here?. Should probably add some comments to this function about what $ means. comment. could this case ever happen i == len(expression)-1?. I wonder if an AST would be better here as opposed a fmt string. we should probably have a default case that returns an error saying case not supported. Can you elaborate a little here on what a condition builder would be used for?\nAlso, providing examples in the documentation will be helpful too. For example, here is the comments on JSONValue\ngo\n// JSONValue is a representation of a grab bag type that will be marshaled                                                 \n// into a json string. This type can be used just like any other map.                                                      \n//                                                                                                                         \n//  Example:                                                                                                               \n//                                                                                                                         \n//  values := aws.JSONValue{                                                                                               \n//    \"Foo\": \"Bar\",                                                                                                        \n//  }                                                                                                                      \n//  values[\"Baz\"] = \"Qux\"\nThis will allow our documentation to signify that there is an example.. recommend renaming boolBuildCondition to compoundBuildCondition since EqualCond is technically a bool as well. Move these checks into a simple validate method which returns an error. This will remove some duplicate lines. Could probably move all this to a function to prevent duplication. Index can be used here.. This can be simplified by removing the if and else statement, as it looks like it isn't really needed, and just always building the child nodes.. add comments about these enums. go\nnoError = iota. go\nif c.err != noError {\n}. the comments are a little weird here.. Another option may be to accept the floating point precision error and allow for overriding if people require more precise values. I think for most cases people will not care too much about the minor precision issues. \n@jasdel, @glasser  - What do you think?. This could bereturn en.buildExprNodes(&aliasList{}). What about testing an empty projection?. May be a good idea to make this a typed error and expose this to the users. How about putting each of these cases in their own functions?. minor, but this could be ret.Names = map[string]*string{}. Use constants here, like ec2.InstanceStateNameRunning. nit, but generally we put stdlib stuff separate from SDK imports\n```go\nimport (\n    \"fmt\"\n    \"os\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n// and so on\n\n)\n``. Remove the unneeded whitespace here for consistency.. Region shouldn't be hard coded, please pass this in via command line. whitespace not properly aligned. Rungo fmtto fix this. Why do we not want to return this error?. whitespace not following Go standard. whitespace. I would either get rid of this else statement or not pass in states. region should be passed in via command line as well. probably want some mock value here like x-000000000000000000. *Making. Since this is testing for empty output, may want to have nothing in the params.. Same here. I just realized we squash all these error. Is that something we want to do?. Do we want to default toNoEscape? We may want to check forNoEscapein a switch case and return an error, if it is of invalid escaping type.. Might want to add tests for these new functions.. Yea, that sounds good. Hrm, what if the name is something likeAmazon AWS FlooforAWSFloof? I thinkAWSFloofshould remain the same and questionable about the first case. Print statement needed?. Should we squash the error here? The error could return too many files open which may be useful information.. extra space beforePutLogEvents. Maybe log that this should be removed here. same here. We probably want to log the!okcase too.. Add test forNoBody.. This shouldn't get stomped over on line 207. The logic here is the same? Would it make sense to have a fallthrough for theaws.ReadSeekerClosercase?. replacing the1withio.SeekCurrentmay be good here. Same here.0withio.SeekStart. Replace the constant numbers withio.. I think we can keep the old code and simply use [sync.Pool](https://golang.org/pkg/sync/#Pool). What do you think? This will get rid of the mutexes and exposed critical sections.. Do we want to return an error or just remove that specific handler?. Should the last sentence beIf no region was foundrather thanspecified?. How do you feel about putting this in a helper method likeIsPresign? I think it'll increase the readability here.. What if this header is already set? Wouldn't this stomp over it?. Same as above. Wouldn't this throw an error, when it got intoparseScalar?. Yep, verified. Works fine :). IfupdateEventPayloadRefreturns early, that would mean thateventStreamisn't supported.  Should the operation be even visible at that point? Or even panic there?. Isn't there a case if there is only one operation and it is event stream that the initial protocol will still be imported and potentially unused?. Do we want to panic here? Why can't this be aSerializationError?. What about duplicate key names with different values? Shouldn't this be returning an array ofValue?. Maybeinject_request_payload.go? And update the struct names?. Prefer to setpathto a const. Yes, the intention is that start would be called prior to anything else. I could have it return an error ifsenderisnil. Yea, want to be changeable by the user. Probably should removeDefaultfrom the name.. Yea, worry with that is if we ever had clean up stuff for this method it may be missed because of it being areturn. So I would prefer thebreak. Ah! Good point. Will make that a util. I can make it a const. Doesn't hurt at least.. would prefer this in a defer due to the potential ofisExpiredever panicking . Move to code genlocationNamefor event stream shapes.. Cast toerrorinstead ofawserr.Error. No need for$fieldIdx. Just use range which already sorts, which is whatfieldIdxwas used for.. Serialization error should be used here.. Comment on why we are chopping off first element. Check event channel to ensure it is closed. Why is this disabled for benchmarks but not unit tests?. Can go away with removal offmt.Errorf. See if the operation is HTTP/2 and if it is able to be determined.. *Params. This template will not generate event message as an exception type.. suppress*. Having this return an error may propose problems. . nit - Probably should rename tolorlengthsincelen` is a keyword in go.\nSquash error here as well.. get rid of the return error and these changes should go away. We may want to have a separate benchmarks and tests for this functionality and leave these benchmarks the same since we want to ensure non-ReadSeeker interfaces do not break.\n. Could wrap the body here in aws.readerSeekerWrapper for benchmarking. same here. remove and should be enabled via flag benchmem. shouldn't this be str[len(str)-1] != '/'?. @atsushi-ishibashi - Oof, I really don't like relying on the io.EOF as it requires internal knowledge on how Decode works. However, I think we can accept this change with one minor tweak\ngo\nerr := json.NewDecoder(stream).Decode(&out)\nif err == io.EOF {\n    return nil\n} else if err != nil {\n    return err\n}\nIt may be a good idea of not creating a new decoder every time either. We can let UnmarshalJSON handle the creation of the stream and pass that into unmarshalJSON. Let me know what you think of that!\ngo\n// or maybe make this a method on an object? That way decoder doesn't need to be passed\n// down\nfunc unmarshalJSON(decoder json.Decoder, v interface{}, stream io.Reader) error {\n    err := decoder.Decode(stream)\n    // same logic as above\n}. nvm! Looks like UnmarshalJSON will ever only be called once! Forget I said anything :D. This will return an error, if role ARN is empty, when Retrieve is called. I figure it'd be best to add it to the credential chain and if it is empty return an error signaling an issue back to the user rather than ignoring it silently.. I am okay with changing it to behaving the other way, if it seems more preferable.. How about this, return an error if only one is set at the session create level rather than at the credential retrieval. That way it fails quickly and let's the user know that they have misconfigured the application. removes unneeded whitespace. Should doc-2.json not be required?. corresponds. Shouldn't the description be \n// OperationForMethod returns the API operation that corrisponds to the. this line technically isn't needed.. Can we add tests for the new functions introduced?. I would prefer not to lazy load these. How do you feel about adding a constructor for ProcessProvider with functional options?. Couldn't we do this logic in executeCredentialProcess? Would be nice to keep the os specific stuff in one place.. go\nif err := json.Unmarshal(out, resp); err != nil {\n}. What's the purpose for Version? . if we call Retrieve twice and let's say the first call does not have an Expiration. This will set staticCreds to true. The second call comes in and has an expiration, however, staticCreds is still true. Probably should just be\ngo\np.staticCreds = resp.Expiration == nil. go\nif length := len(p)+b.buff.Len(); length > b.maxSize {\n    return -1, fmt.Errorf(\"buffer size (%v) exceeded: %v\", b.maxSize, length)\n}. Can we move this goroutine to a function?. How about instead of Process being a string, but a exec.Cmd or some sort of Command type? This will allow for all the OS specific logic to be very contained in the Command portion.. are several. to learn how to interface with AWS services. on how to configure*. If we allow for custom api-2.json why not the others?. Why is this test getting removed?. ",
    "cristim": "Nice discussion. It would be awesome if some of these ideas would be written down into a wiki page dedicated to testing mechanisms for the SDK, since much of this is not really straightforward and much unlike anything I've seen in other golang projects.\n. Looks good, but I would like to have more of the metadata URLs exposed as golang methods, for the sake of editor code completion.\n. Hi,\nI had similar issues when uploading to S3, needed for CloudFormation custom\nresources written in golang.\nThe problem is caused by net/http, which un-escapes the special characters\nfrom the response URL(in my case colons and pipe characters), which is\nbreaking the signature mechanism.\nMy workaround was to undo the un-escaping in the URL, using Opaque URLs:\n```\nreq, err := http.NewRequest(\"PUT\", event.ResponseURL, strings.NewReader(string(responseBody)))\nreq.URL.Opaque = strings.Replace(URL.Path, \":\", \"%3A\", -1)\nreq.URL.Opaque = strings.Replace(req.URL.Opaque, \"|\", \"%7C\", -1)\nreq.Header.Set(\"content-length\", strconv.Itoa(len(responseBody)))\nclient := &http.Client{}\nresp, err := client.Do(req)\nif err != nil {\n    fmt.Println(\"Failed to set CloudFormation state\", err.Error())\n}\ndefer resp.Body.Close()\n```\nI hope this helps you, but if any of you guys can think of a better way,\nI'm all ears :-)\n-Cristi\n. Apparently answering by email is not so well supported by Github, and looks like I can't properly format the answer's code anymore.\n. I had the same issue and I ended up waiting for the new instance from my spot instance request to come up until I can run CreateTags on it.\nI wish there was a better way, but at the moment it looks like the only option because the both the spot instance request and the launch configuration data structures are missing a tags attribute, which would be the natural place where this kind of information would be stored.\nIn case of having it in the launch configuration I see a possible issue with instances that would get replaced in case of tag changes in case the group has a replacement policy, but I don't think that's a common use case.\nI suppose the current state is just for historical reasons, because the tags were added later and someone decided it would be easier to append them on the resources after their creation instead of placing them in the data structures used to generate those resources in the first place.\n. Could this perhaps be combined this with #1031?. Perhaps the SDK should stop vendoring and let the users handle it, as recommended here. @jasdel that issue seems to be about generating policy data structures that can accommodate and compare equally for different formatting of the JSON.\nThis one is just a special kind of logging that would be allow users to generate policy documents for a given program using the SDK.\nMaybe that policy data structure could be used for collecting this logging information throughout the execution of the program and then dump it to a policy json/yaml file right before exiting.\n. ",
    "shatil": "@xibz how's that backlog looking?\n. @xibz great to hear! I'm writing some unit tests for Go code interacting with DynamoDB. I discovered how to mock GetItem by reading https://github.com/aws/aws-sdk-go/blob/master/service/dynamodb/dynamodbiface/interface.go\nI'm still writing the tests. I'm wondering what approach to use for mocking things like an Init function, e.g.,\nfunc Init() {\n    AwsConfig = aws.Config{Region: aws.String(\"us-west-2\")}\n    AwsSession = session.New(&AwsConfig)\n    DynamoDB = dynamodb.New(AwsSession)\n}\n. @xbiz it's taken a while to wrap my head around this.\nHow can I define a variable, or field in a struct, to share among functions without needing to pass along a parameter for the service to every function?\n(DynamoDB's interface.go suggests passing along svc dynamodbiface.DynamoDBAPI param, which feels like a Go anti-pattern.)\n. That's actually what I'd like to avoid since:\ngo\nfunc (self *MyObject) MyMethod(svc *dynamodbiface.DynamoDBAPI) {svc.PutItem(/* ... */)}\nmy_object.MyMethod(svc)  // every method then requires an additional parameter\nI'd like instead to define in a file like lib.go:\ngo\nvar DynamoDB *dynamodb.DynamoDB = dynamodb.New()\nThen\ngo\nfunc (self *MyObject) MyMethod() {lib.DynamoDB.PutItem(/* ... */)}\nmy_object.MyMethod()\nGo doesn't allow me to cast *dynamodb.DynamoDB as *dynamodbiface.DynamoDBAPI or vice versa, so if I simplify my code, I can't then test it.\n. @xibz your suggestion worked perfectly. Thank you! Example main.go for anyone interested:\n``` go\npackage main\nimport (\n    \"github.com/aws/aws-sdk-go/aws\"\n    \"github.com/aws/aws-sdk-go/aws/session\"\n    \"github.com/aws/aws-sdk-go/service/dynamodb\"\n    \"github.com/aws/aws-sdk-go/service/dynamodb/dynamodbiface\"\n)\ntype AmazonWebServices struct {\n    Config aws.Config\n    Session session.Session\n    DynamoDB dynamodbiface.DynamoDBAPI\n}\n// Example configuration block, which runs fine as-is in tests.\nfunc ConfigureAws() {\n    var Aws AmazonWebServices = new(AmazonWebServices)\n    Aws.Config = &aws.Config{Region: aws.String(\"us-west-2\"),}\n    Aws.Session, _ = session.NewSession(Aws.Config)  // New() deprecated\n    var svc dynamodb.DynamoDB = dynamodb.New(Aws.Session)\n    Aws.DynamoDB = dynamodbiface.DynamoDBAPI(svc)  // Thanks for this!\n}\nfunc (self AmazonWebServices) GetFromDynamoDB(key string) {\n    var parameters dynamodb.GetItemInput = // Configure parameters...\n    var response *dynamodb.GetItemOutput\n    var err error\n    response, err = self.DynamoDB.GetItem(parameters)\n    // Do stuff w/ your fancy response! (Or error.)\n}\n```\nThen main_test.go:\n``` go\npackage main\nimport (\n    \"testing\"\n\"github.com/aws/aws-sdk-go/service/dynamodb\"\n\"github.com/aws/aws-sdk-go/service/dynamodb/dynamodbiface\"\n\n)\ntype FakeDynamoDB struct {\n    dynamodbiface.DynamoDBAPI\n    // I piggyback expected payloads here, too.\n}\nfunc (self FakeDynamoDB) GetItem(input dynamodb.GetItemInput) (*dynamodb.GetItemOutput, error) {\n    // Your fake GetItem.\n}\nTestGetFromDynamoDB(t *testing.T) {\n    test_aws = new(AmazonWebService)\n    test_aws.DynamoDB = &FakeDynamoDB{}\n    test_aws.GetFromDynamoDB(\"this particular key\")\n}\n``\n. Sure thing, @xibz! Here's a shot: #929 \n. Awesome, @jasdel! I'd give you a new phone tool icon if I could \ud83e\udd47 . @xibz and @jasdel thanks for the review! Updated with the linter and// +build examplefeedback. Let's see if it passes.\n. @jasdel should it be// +build exampleor should I followexample/service/dynamodb/unitTest/unitTest.go:1:1: package comment should be of the form \"Package unitTest ...\"`?\n. Alright, better problem now:\nno buildable Go source files in /home/travis/gopath/src/github.com/aws/aws-sdk-go/example/service/dynamodb/unitTest\n. OMG it passed\n. @xibz, client side validation exists for cloudwatch.PutMetricDataInput: https://github.com/aws/aws-sdk-go/blob/master/service/cloudwatch/api.go#L4477-L4504\nIt validates whether things like Namespace are present checks every item in MetricData. What it doesn't do is raise an error if the length of a field is unacceptably long.\nSDK documentation for many interactions indicate how many items are returned by a request per \"page\". If Validate() is after all this not the right place to actually validate on the client side, then at the very least, documentation should exist noting that MetricData should contain no more than 20 items, like:\n```diff\ndiff --git a/service/cloudwatch/api.go b/service/cloudwatch/api.go\nindex 38519a2a..b14d8d2a 100644\n--- a/service/cloudwatch/api.go\n+++ b/service/cloudwatch/api.go\n@@ -4452,7 +4452,7 @@ type PutMetricDataInput struct {\n    // The data for the metric.\n    //\n\n\n// MetricData is a required field\n// MetricData is a required field and must not contain more than 20 items.\n        MetricData []*MetricDatum type:\"list\" required:\"true\"// The namespace for the metric data.\n\n```. \n\n",
    "shangsunset": "Hi guys, sorry to post in an old issue here. I couldnt find any solution elsewhere and figured my question is kinda related to this one.\nI have following code for signing a request to s3\n```go\n  req, _ := svc.s3.GetObjectRequest(&s3.GetObjectInput{\n    Bucket: aws.String(\"bucket\"),\n    Key:    aws.String(\"key\"),\n  })\ndst, err := req.Presign(5 * time.Minute)\n``\nI can mock outGetObjectRequestwiths3iface.S3APIinjected in mysvc,  but how can I mockreq.Presign?. @lsegal Thanks for you reply. You had a good point. I didnt really look intoreq.Presign(). I gotinvalid memory address or nil pointer dereferencefrom callingreq.Presign()` and automatically thought to mock that method.\nSo in that case, theres no reason to mock GetObjectRequest() either?\n. ",
    "keyneston": "Sure I can expand.\nSo someone updated the models/endpoints.go which has several problems.\n1. It thinks it is package \"aws\", while being in the service/endpoints folder. This causes conflicts.\n2. It is missing the endpoints.Lookup function which the rest of the generated\n   code is calling.\nSo what happens is someone runs make and the endpoints.go file gets\noverwritten. You can then run git checkout service/endpoints/endpoints.go and\nget it back to the previous hand coded(?) file which allows the other code to be compiled again.\nThis change makes it so that the endpoints file that is automatically generated\nworks with the rest of the code. Specifically it uses the new endpointStruct\nformat.\nIn addition this change changes package aws to endpoints and adds a\ndisclaimer that the file is generated.\n. ",
    "ezbercih": "@lsegal - Here you go:\n``` go\npackage main\nimport (\n        \"fmt\"\n    \"github.com/awslabs/aws-sdk-go/aws\"\n    \"github.com/awslabs/aws-sdk-go/service/s3\"\n\n)\nfunc main() {\n        creds, err := aws.EnvCreds()\n        if err != nil {\n                fmt.Printf(\"Error: %v\\n\", err)\n                return\n        }\n        config := &aws.Config{\n                Credentials: creds,\n                Region:      \"us-east-1\",\n        }\n        cli := s3.New(&s3.S3Config{config})\n        buckets, err := cli.ListBuckets()\n        if err != nil {\n                fmt.Printf(\"Error: --%v--\\n\", err)\n                return\n        }\n        fmt.Printf(\"Buckets: %+v\\n\", buckets.Buckets)\n}\n```\n@ncw - I made the same changes you made (quite similar to what I did but wanted it to be the same for testing purposes) but still no luck. I get an error when I call ListBuckets and the error is empty and Buckets itself is an empty slice.\n. ",
    "schmatz": "@lsegal are you handling this? If so, do you have an estimated timeline? Else, what did you have in mind? I managed to fix the time value issue with a few lines of code, but I'm pretty sure a quick patch is not what you had in mind when creating this issue! (though if you did, should I submit a PR?)\n. @lsegal check out the PR and let me know if you think this is the right solution. I think using a type like TimeValue for every field which isn't guaranteed to be set and using time.Time for every field which must be set might be the best way to go about this.\n. I think that's much more elegant - I originally thought to do that but saw the other types in types.go not using the type alias they had just declared as the return type. Is there any reason why the other types are written that way?\n. ",
    "normj": "One of the things we are working to fix is to make the names consistent for structs on operations. We are going to make it so the root shapes you pass into the operation will have the \"Input\" suffix and the struct returned will have an \"Output\" suffix. Once we complete that there will just be a DescribeStackEventsOutput and there will not be a generated DescribeStackEventsResult\n. ",
    "nuance": "For now, I'm using a hacky fix of manually setting the XMLName attribute on CompletedPart objects:\npart.XMLName = xml.Name{\"\", \"Part\"}\n. ",
    "jrozner": "Would it be possible to get the document abstractions merged in in the mean time and circle back to dealing with the the resources later? I was actually about to start writing a wrapper like this to move away from goamz and would prefer not have to do that only to swap it out when this lands.\n. ",
    "guregu": "Whoa, wish I had seen this earlier. I started something similar here: https://github.com/guregu/dynamo\nIt can't automatically marshal a few types and doesn't cache the reflect data so it's unoptimized and incomplete. \nI ended up going with this for my Marshaler interface:\ngo\ntype Marshaler interface {\n    MarshalDynamo() (dynamodb.AttributeValue, error)\n}\nThis targets specific fields within a struct, not the whole struct. Maybe something like this would also be necessary? \ngo\ntype StructMarshaler interface {\n    MarshalDynamo() (map[string]dynamodb.AttributeValue, error)\n}\nYou could probably use this for individual fields too, but then comes the question of whether to use the map's keys or the struct tags as the column name. Struct tags could take priority, but what happens when the map has multiple entries? \nAnyway, I'd definitely like to contribute to this if possible, but I think it will be pretty difficult to come up with abstractions around DynamoDB complete enough to satisfy everyone's needs and be \"official\". At the very least, an official marshaler would be nice.\n. Let's look at dynamodb.GetItemInput.\nhttp://godoc.org/github.com/awslabs/aws-sdk-go/service/dynamodb#GetItemInput\nAttributesToGet []*string `type:\"list\"`\nWhy is it a slice of *string? AFAIK mixing a nil in there makes no sense and will just result in an error.\nConsistentRead *bool `type:\"boolean\"`\nThe default is false. Nil and false mean the exact same thing. Why not just make this a boolean with omitempty?\nKey *map[string]*AttributeValue `type:\"map\" required:\"true\"`\nWhy is this a pointer? Maps can be nil without making them a pointer. I can think of very few use cases for a pointer to a map in Go and this isn't one of them. Also, why are maps made into pointers but slices (like AttributesToGet) not? \nProjectionExpression *string `type:\"string\"`\nA blank ProjectionExpression is defined to return all results, so this should be a string with omitempty. It's perfectly idiomatic Go to check if a string is blank with str != \"\", there is no need to make this a pointer.\nTableName *string `type:\"string\" required:\"true\"`\nThis is required. There is no reason to make it nullable. \nI understand that not all Amazon APIs work the same way as DynamoDB's, but I think it would be worth it to make everything idiomatic where possible. I would be happy to volunteer effort towards this. At the very least get rid of the map pointers, please. \nOf course, for situations like \"a string that may be null in the response\" (i.e. blank strings and null have a different meaning), I think a pointer is totally fine. \n. @jasdel Thank you very much! I think this change will make a lot of people happy :+1: \n. ",
    "pdalinis": "An \"IsRegionValid(region string) bool\" validation would be very helpful.  Having the regions as a map might allow better utility.\nSame concepts with availability zones. Looking up a region and then getting it's AZ's would be great to have.\n. @lsegal, sounds good. I'll create our own package to do this in the meantime.\nOne use case that I just hit is we have to query the metadata to retrieve region of the instance.  The DefaultConfig makes an assumption of using ENV variable, so it does not work for us. \nSimilar to the DefaultCreds logic, it would be nice to have DefaultConfig use the ENV variable, and if it isn't there, then query the meta-data for the region.\nAnother use case is to detect if the code is being executed on an EC2 instance. We change logic paths for different execution behavior based on where it is being hosted (log to cloudwatch vs syslog).\n. https://gist.github.com/pdalinis/5a3a21b8deec4334cf21\nThis is what we are currently using. I took most of this from the other/older sdk's. The GetRegion is a bit specific to our needs, but could easily be made more generic or omitted.  Some caching of immutable values in memory would be nice so repeat calls do not actually make the http request.\nI can easily move this over to this sdk with a pull request if wanted.\nI have been busy porting over all our packages to the dev branch merge, and am almost done. I can have a PR later today or tomorrow.\n. Confirmed it is working.  Thanks!\n. I'll look into this and get back to you on Friday when I get some free time.\nI agree on keeping complexity low.\n. @jasdel, I really like using pointers of primitive types.  To me, it is more natural and there is less to learn.\nI also really like the builder pattern, especially if you have only one or two settings to set.\n. We are doing a bunch of AWS calls in Go routines, and it is working fine (and was before the development branch merge).\n. diff --git a/apis/sqs/2012-11-05.normal.json b/apis/sqs/2012-11-05.normal.json\nindex a63f1ab..c4cad6a 100644\n--- a/apis/sqs/2012-11-05.normal.json\n+++ b/apis/sqs/2012-11-05.normal.json\n@@ -782,14 +782,14 @@\n       \"type\":\"structure\",\n       \"required\":[\n         \"QueueUrl\",\n-        \"Entries\"\n+        \"DeleteMessageBatchRequestEntryList\"\n       ],\n       \"members\":{\n         \"QueueUrl\":{\n           \"shape\":\"String\",\n           \"documentation\":\"<p>The URL of the Amazon SQS queue to take action on.</p>\"\n         },\n-        \"Entries\":{\n+        \"DeleteMessageBatchRequestEntryList\":{\n           \"shape\":\"DeleteMessageBatchRequestEntryList\",\n           \"documentation\":\"<p>A list of receipt handles for the messages to be deleted.</p>\"\n         }\nWould you like a pull request?  This fixed the problem, but not sure if it is a solution. I really need to spend some time and learn the sdk better.\n. @robbyram yes, I see that too.  Looks like the same issue. A quick grep returned the following places:\n- DeleteMessageBatchRequest\n- ChangeMessageVisibilityBatchRequest\n- SendMessageBatchRequest\n. Just updated, DeleteMessageBatch works as expected.  Thank you!\n. Am on PTO, will resume next week unless you want to take over.\nOn Apr 13, 2015 3:06 PM, \"jsdir\" notifications@github.com wrote:\n\nAny progress on this?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/awslabs/aws-sdk-go/pull/182#issuecomment-92482152.\n. Lets nail down what else needs to be done in order to get this merged in.\n\n@lsegal, what needs to happen before we can get some meta-data retrieval helpers into the sdk?\n. @jasdel, this looks great and meets my needs. Thank you for getting to this.\n. It also is limiting in that only the \"latest\" is returned, no versioning is allowed.\nPerhaps the url prefix should just be http://169.254.169.254/ ?  I am fine w/ just http://169.254.169.254/latest/ .\nWe need to walk that line between usability and flexibility while not bloating the package with a ton of methods.\n. ",
    "jsdir": "+1 for IsRegionValid. I'll be happy to put this in if an agreement can be formed around how to manage the region list.\n. @lsegal Although this functionality has no dependency on the core SDK, it is still a documented part of the API as a whole. Is this library intended to only wrap the core SDK? An implementation of this can already be used here. I'll work on a PR for this feature.\n. Any progress on this?\n. Shouldn't /meta-data be specified by the user in path since the user-data and dynamic endpoints are also hosted? \n. ",
    "jmassara": "+1, this would be tremendously useful.\n. What's the status of this issue? Thanks!\n. Thanks @jasdel.  Is there an ETA on it? This issue is approaching 2 years old.\n. Would love to see this integrated.. That's because the zero-value of time is a real time. You'd have to use a pointer to time.Time.. @jasdel Thanks for the great feedback! Please let me know if I've addressed the issues.. @jasdel sorry for the extra PR, I searched issues and not pull requests when checking early. My bad.. \ud83e\udd26\u200d\u2642\ufe0f . ",
    "ntgsx92": "+1, this would be very helpful !!\n. ",
    "jvatic": "Adding the following to aws/ec2.go#L98 fixes this issue:\ngo\nif casted, ok := value.Interface().([]byte); ok && prefix != \"\" {\n  v.Set(prefix, string(casted))\n  return nil\n}\nhttps://gist.github.com/jvatic/8fcc481664ec4736270c\n. ",
    "meatballhat": "@lsegal sorry about that!  Re-targeting...\n. :heart_eyes_cat: :exclamation:\n. ",
    "ngauthier": ":+1: to @drombosky. Builtin pointers make sense to me, and also we don't need pointers on nillable types.\n. That is a nice solution but I worry it's more complicated and non idiomatic go to have a DSL instead of structs whose unset value is the default. Also I'd imagine this would be harder to generate and maintain, but I am not an expert in go generate.\nI also worry that since ur.Values is a map that one could accidentally set a param that was not actually a valid one per the API. Although if it was private and all methods were generated this probably isn't an issue.\nurl.Values makes sense for many parts of the AWS API since they use query params. But don't others use json and xml payloads? I don't know, but I am not sure we can always omit options. We would need defaults in some cases. Structs are good at that.\nI think in the end I prefer structs with pointers. It is a much clearer interface even if it means you have to do a nil check and deference. With your example one would still need nil checks on responses (and you have not shown how you would handle responses, only requests). IMO it is valuable for the request and response API to be symmetric (i.e. both structs) vs having different slightly optimized APIs (like url.Values for requests, then structs or something else for responses).\nIn my usage, I am more dealing with nils in responses than I am in requests.\nHope that helps!\n. Yes, I do think @pges's solution is very nice at a high level. Reminds me of squirrel. It would be interesting to provide such a package build upon this one. But for the core IMO we should stick to the basics.\n. ",
    "dhawal55": "Awesome!! When can we do the same for slices?\n. Retrieving metadata +1\n. ",
    "tobstarr": "Also: the EC2 tag of \nhttps://github.com/awslabs/aws-sdk-go/blob/master/gen/ec2/ec2.go#L5117\nneeds to be NetworkInterface, not NetworkInterfaces\n. Thank you for getting back to me. Any ETA for the develop -> master merge?\n. ",
    "codemac": "Is there anything I'm missing here? I'd appreciate some feedback on what I should be doing. I really enjoy this library.\n. Awesome! Yay :) Looking forward to converting my stuff to the new api style and I'll double check this in master as well (though I see that you tested it too).\n. ",
    "viglesiasce": "I believe the the first step would be to get the endpoint to be configurable:\nhttps://github.com/awslabs/aws-sdk-go/blob/master/aws/query.go#L32\n. That is great news @lsegal ! Ill give it a look see.\n. @lsegal @wolfspyre  \nI was able to get this working in the develop branch as follows for EC2:\n```\npackage main\nimport \"github.com/awslabs/aws-sdk-go/aws\"\nimport \"github.com/awslabs/aws-sdk-go/service/ec2\"\nimport \"log\"\nfunc main() {\n  creds := aws.Creds(\"AKINXXXXXXXXXXXP\", \"5DWxbEYYYYYYYYYYYYECtoSjtQvHUi\", \"\")\n  svc := ec2.New(&ec2.EC2Config{&aws.Config{\n        Credentials: creds,\n        Endpoint:    \"compute.cloud.home\",\n        Region:      \"eucalyptus\",\n        DisableSSL:  true,\n    }})\n  list, err := svc.DescribeInstances(nil)\n  if err != nil {\n        log.Printf(\"fail: %#v\", err)\n        return\n  }\n  log.Printf(\"%+v\\n\", list)\n}\n```\n. ",
    "fsouza": "@lsegal do you have an ETA on the merging of the develop branch?\n. Awesome! :-D\n. Oh, cool. I will have a look in the develop branch, and if double-overriding doesn't work, is it fine to send a pull request to the develop branch?\n. Yeah, I just figured that develop is entirely different, and better, so I made a dumb question before even looking at code, I apologize for that.\nI look forward to see the work in the develop branch merged into master. Thank you!\n. ",
    "bmatsuo": "I'm partial to the initial suggestion from @mitchellh.\nBut, it's also the kind of function that is trivial for one to write themselves on an as needed basis. It's even possible for people to define them in an internal helper package if they want.\nIs this a problem the package needs to solve?\n. Yes. It appears that the canonical request is not formatted correctly.\nWith LogLevel: 1 in the config I see the canonical request:\n```\nGET\n/example-bucket?delimiter=%2F\nhost:s3.amazonaws.com\nx-amz-date:20150323T213630Z\nhost;x-amz-date\n...\n```\nThe correct canonical request should have the query on a separate line like this,\n```\nGET\n/example-bucket\ndelimiter=%2F\nhost:s3.amazonaws.com\nx-amz-date:20150323T213743Z\nhost;x-amz-date\n...\n```\nI was able to work around things with a fairly minor change in \"internal/protocol/rest\". But I'm not sure if that was the right thing to do (e.g. whether or not that will break other services).\nIt is also a little weird because it seems like the godoc for URL.String does not match its behavior with respect to the Opaque struct field. I opened an issue against the Go repo about this. https://github.com/golang/go/issues/10227\n. Fantastic. Thank you for the advice @lsegal. But I am having trouble implementing things using initRequest. I get the following panic when running my program built off the commit https://github.com/bmatsuo/aws-sdk-go/commit/ae031cb559a4d39b35b5766612d3e48e9ebc1380\n```\npanic: interface conversion: interface is nil, not func(*aws.Request)\ngoroutine 6 [running]:\ngithub.com/awslabs/aws-sdk-go/aws.(HandlerList).Run(0xc20824a538, 0xc20824a500)\n    /home/bmatsuo/src/github.com/awslabs/aws-sdk-go/aws/handlers.go:62 +0x6b\ngithub.com/awslabs/aws-sdk-go/aws.(Request).Build(0xc20824a500, 0x0, 0x0)\n    /home/bmatsuo/src/github.com/awslabs/aws-sdk-go/aws/request.go:101 +0xc1\ngithub.com/awslabs/aws-sdk-go/aws.(Request).Sign(0xc20824a500, 0x0, 0x0)\n    /home/bmatsuo/src/github.com/awslabs/aws-sdk-go/aws/request.go:109 +0x3a\ngithub.com/awslabs/aws-sdk-go/aws.(Request).Send(0xc20824a500, 0x0, 0x0)\n    /home/bmatsuo/src/github.com/awslabs/aws-sdk-go/aws/request.go:119 +0x3a\ngithub.com/awslabs/aws-sdk-go/service/s3.(S3).DeleteObjects(0xc20802c098, 0xc20825a660, 0xc208261140, 0x0, 0x0)\n    /home/bmatsuo/src/github.com/awslabs/aws-sdk-go/service/s3/api.go:384 +0x5c\nmain.(BulkDelete).worker(0xc20803d320, 0xc20803a360, 0xc20803a3c0, 0xc20803a420)\n    /home/bmatsuo/src/github.com/bmatsuo/sandbox/s3rm/s3rm.go:181 +0x4b4\ncreated by main.(*BulkDelete).run\n    /home/bmatsuo/src/github.com/bmatsuo/sandbox/s3rm/s3rm.go:112 +0xe9\n```\nI am still debugging things. But I believe the problem is in the implementation of the HandlerList type. Specifically, it doesn't seem safe ~~to me~~ to handle a list.List directly (as opposed to a pointer *list.List).  See this snippet on the playground\nhttp://play.golang.org/p/vXAjUnnXeQ\nYou will see that PushBack is called twice and that the list thinks it has two elements. But the loop iterates three times. The behavior has to do with list.Element values having a pointer to the list.List they belong to.\nIn this case specifically, it seems HandlerList.copy() is responsible for breaking the list.\n. I don't have any reference for that assertion. It is only based on observed behavior.\n\nI'm not seeing anything wrong with the copy semantics, specifically the list doesn't seem broken by the copy implementation (the one type assertion removed):\nhttp://play.golang.org/p/MNVBTEGVY5\n\nThat snippet you provided does not exhibit strange behavior because you do not call PushBack on the result of h1.copy().\nI've modified your snippet by calling h2.PushBack(2).\nhttp://play.golang.org/p/XHc-GtSg_p (alteration on lines 36 & 37)\nThis causes the final loop to iterate one more time than it should.\n```\nh2 len after copy: 3\n\n0\n1\n2\n\n```\n. > items are being pushed in reverse order after the copy.\n\nOh wow. I didn't realize that.\nI'm not sure this would be considered a bug in the implementation of list.List. But the documentation you referenced does seem problematic. Certainly new(list.List) produces a valid list and will not have problems when the list is mutated. But the docs imply something more general.\n. @lsegal, I rebased and things seem to work. Thanks for the help.\n. ",
    "LukeMauldin": "Yes, definitely need some solution to this issue\n. ",
    "zpatrick": "Executing this example:\n```\npackage main\nimport (\n    \"github.com/awslabs/aws-sdk-go/aws\"\n    \"github.com/awslabs/aws-sdk-go/gen/s3\"\n    \"log\"\n)\nfunc main() {\n    creds, err := aws.EnvCreds()\nif err != nil {\n    log.Fatal(\"Could not find aws env creds\")\n}\n\ns3i := s3.New(creds, \"us-west-1\", nil)\nbucketname := \"zp-cloudtrail-3\"\n\npolicy := \"{ \\\"Statement\\\": [ { \\\"Effect\\\": \\\"Allow\\\", \\\"Principal\\\": \\\"*\\\", \\\"Action\\\": \\\"s3:GetObject\\\", \\\"Resource\\\": \\\"arn:aws:s3:::zp-cloudtrail-3/*\\\" } ] }\"\n\nerr = s3i.PutBucketPolicy(\n    &s3.PutBucketPolicyRequest{\n        Bucket: aws.String(bucketname),\n        Policy: aws.String(policy)})\n\nif err != nil {\n    panic(err)\n}\n\n}\n```\nReturns \n```\npanic: Policies must be valid JSON and the first byte must be '{'\ngoroutine 1 [running]:\nmain.main()\n        C:\\main.go:31 +0x352\ngoroutine 9 [runnable]:\nnet/http.(persistConn).readLoop(0xc082010210)\n        c:/go/src/net/http/transport.go:928 +0x9d5\ncreated by net/http.(Transport).dialConn\n        c:/go/src/net/http/transport.go:660 +0xca6\ngoroutine 10 [select]:\nnet/http.(persistConn).writeLoop(0xc082010210)\n        c:/go/src/net/http/transport.go:945 +0x424\ncreated by net/http.(Transport).dialConn\n        c:/go/src/net/http/transport.go:661 +0xcc3\nexit status 2\n```\n. ",
    "radeksimko": "This would help us a lot in Terraform. We have quite a few outstanding issues being constantly reported due to the fact we're not able to fully parse & reconstruct the IAM JSON structure.\nhttps://github.com/hashicorp/terraform/issues/3634\nhttps://github.com/hashicorp/terraform/pull/3124\nhttps://github.com/hashicorp/terraform/pull/4278\nhttps://github.com/hashicorp/terraform/issues/3519\nand many others.\nIf this doesn't get implemented, we may just end up creating the IAM structs ourselves.\n. > Syncing with how the other SDKs handle policy type definitions will also be helpful.\n@jasdel Did you have any chance to sync with other SDK teams?\n. > Since policies very pretty widely from service to service, is there a specific subset that would be more helpful that others to have?\nI'd say IAM resources (Users, Roles, Groups, ...) are currently causing most of the pain for Terraform users since these always require/expect the user to define some kind of policy.\nOther policies for other services (S3, SNS, Lambda, ECR, ES, SQS, ...) can be optional in many cases (or default to sensible policy which doesn't need to be diff'ed and cause confusions).\nThis is why I believe that policies for IAM resources should be given priority, but eventually Terraform users/developers would appreciate structs for all policies. \ud83d\ude09 \n\nHow would you use the utility? Would you expect it to be a type that you set fields on, or more of a builder pattern?\n\nBoth marshal & unmarshal - pretty much the same thing we do today for all JSON-based fields in Terraform:\ngo\nfunc normalizeJson(jsonString interface{}) string {\n    if jsonString == nil || jsonString == \"\" {\n        return \"\"\n    }\n    var j interface{}\n    err := json.Unmarshal([]byte(jsonString.(string)), &j)\n    if err != nil {\n        return fmt.Sprintf(\"Error parsing JSON: %s\", err)\n    }\n    b, _ := json.Marshal(j)\n    return string(b[:])\n}\nNo matter what format user used we'd like to be able to construct a canonical version of the policy to figure out if there's any change (during the dry-run) and eventually in the future allow some fancy user-friendly diff-ing too.\nBasically we're looking for something like this: https://github.com/hashicorp/terraform/blob/master/builtin/providers/aws/iam_policy_model.go in AWS SDK, managed & updated by AWS.\n. > Out of curiosity, why is the do does Terraform need to normalize the JSON strings? is this to ensure the string is encoded correct?\nIn simple terms each time user runs terraform plan or terraform apply Terraform compares the real state (returned from Describe*/Get*/... API methods) with definitions in HCL configs to a) provide a diff to the user as part of dry-run functionality and b) decide which part of the infrastructure to update\n- Comparing primitive data types, maps, sorted and unsorted (in cases where AWS returns items in random order) lists is fairly easy\n- Comparing predictable JSON structure as string would be probably work fine, but we don't do it as we want whitespace changes to be noop and prevent noop update API calls. i.e. {\"k\":\"v\"} should be treated as equal to { \"k\" : \"v\" }\n- Comparing unpredictable JSON structure is impossible without the structs we're requesting here because JSON sent to API != JSON returned back from API. This is why Terraform generates unnecessary diffs today and performs basically noop Update API calls of many IAM policies, purely because the JSON structure (unmarshalled to a simple nested map) doesn't match.\nTL;DR Terraform can't tell if \"Resource\": [\"arn:*\"] is the same as \"Resource\": \"arn:*\". For AWS it's equal.\n. Ah, my bad, it's already done... maintainers are quicker than I expected. :smiley: https://github.com/aws/aws-sdk-go/commit/d1a54ab53d17453da862eca771ce55946ac0766d\n. That's interesting... it did not fail under your Travis build...\nHave a look here:\nhttps://travis-ci.org/radeksimko/aws-sdk-go/jobs/73390026\nBefore I submitted this PR, I tried building it in Travis too, just from my fork and it did fail there.\npackage github.com/aws/aws-sdk-go/internal/endpoints\n    imports github.com/aws/aws-sdk-go/internal/endpoints: use of internal package not allowed\nAlso I've seen it failing many times in hashicorp/terraform Travis builds from tip... but latest builds seem to be green...\n. It is using a different build of go:\nThe one which is failing\ngo version devel +7904946 Thu Jul 30 15:55:25 2015 +0000 linux/amd64\nwhereas build that your Travis jobs use are green:\ngo version devel +7cabade Thu Jul 30 12:42:18 2015 +0000 linux/amd64\nI reckon it will be best to wait for 1.5 stable release and then test against that and make any conclusions/actions based on that.\n. The Go team is apparently just looking into this:\nhttps://github.com/golang/go/issues/11960\n. :+1: \n. Thanks, I will have a play.\nOut of interest - is there any reason why is it called elasticsearchservice and not just elasticsearch ?\n. > Since the providers are solely responsible for filling the credentials.Value's value it would make sense for them to also populate this field.\nI was actually playing with this version before I submitted the PR. The reason I have chosen the implementation with Name() func is because this would work at any time in any context, no matter how would you use the provider.\nIf we were to rely on a struct string field, we'd also have to rely on the user to either always use New*() constructors (no way to enforce this in go) or make sure that all provider methods are consistently setting the provider name, or just accept the fact that the name will only be available once you call Retrieve().\nThe ultimate question is then, do we want to make this work?\ngo\np := &awsCredentials.StaticProvider{Value: awsCredentials.Value{\n    AccessKeyID:     key,\n    SecretAccessKey: secret,\n    SessionToken:    token,\n}}\np.Name // this would be always empty, because no method was called to set it\n\nIts possible that a provider implementor forgets to provide this field, but it would also not require a breaking change, nor add additional logic to the credentials.ChainProvider and credentials.Credentials types.\n\nAgreed that no changes would have to be made to credentials.Credentials.\nI think it's necessary to add some logic to the credentials.ChainProvider as it's responsible for choosing the real provider, otherwise we'd be getting ChainProvider as the name of the provider, which isn't very useful (depends on context, but most of the time you'd probably want to see the chosen provider name from the chain).\nI'm fine with implementing the changes you suggested, after all I've been in that stage already a few days ago :smiley: , can you just confirm you're ok with all the pros (no breaking change) and cons (providerName likely to be empty in many cases)?\n. Constant is a great idea (I'm actually now embarrassed I didn't come up with this :smiley: ) and solves the problems I raised! I will make those changes asap.\n. @jasdel Modifications done.\nI thought I could make this even cleaner by moving each provider into a special package from credentials, but that would introduce breaking change, so I decided not to do that.\nOT: Thank you (and the rest of the team behind AWS SDK) for changing my perception of Amazon & OSS. Nothing personal against your colleagues & maintainers of Boto, but PRs w/out any response for 1year+ (I am able to find even 4 years old PRs with 0 responses) don't usually leave any positive feelings. :wink: \n. I think this was more of a check for non-EC2 environments. We can (almost 100%?) safely assume that this IP address will always point to the metadata API on running EC2 instances unless I (as the VPC/subnet architect) decide to choose this CIDR or happen to peer with VPC which uses that CIDR or use Direct Connect to connect to a network which happens to use this CIDR. Maybe even then I'd expect the AWS API to warn me and/or still try to route the traffic to the metadata API.\nHowever there's no way we can make any assumptions about networks of users which are connecting to AWS using other methods (env variables, shared creds file, hard-coded creds). For any reason those auth methods may fail and it will eventually fall through to EC2Role provider. 169.254.0.0/16 is APIPA range which means that pretty much any machine may eventually assign such address itself according to RFC3927 if it lost connection to the gateway.\nIn our context (Terraform), many ops/devs runs the tool directly from their laptops and they change the network very often, which also increases the chance for being part of 169.254.0.0/16 CIDR.\n\nI ask because adding this might prevent users from running local proxy's caches in front of ec2metadata service.\n\nI may be naive, but shouldn't such proxy be proxying HTTP headers too?\nI may be also misinterpreting @catsby's intentions (it was originally his idea) - want to add any comments or further explanations here, Clint?\n. @jasdel If we come up with a PR implementing this check, is there any chance of getting such PR merged or is your concern about proxy caches not forwarding headers too strong?\n. > Though this is a feature users could explicitly opt into by adding the check to the EC2 Metadata client's ValidateResponse request handler.\nThat looks a lot cleaner than the current solution we have in place. :+1: \nIf Server: EC2ws is officially undocumented, is there any documented feature we could use to verify connection to AWS metadata API then?\n. > In addition I suggest also posting to the EC2 Forums you might be able to get a quicker answer reaching out to them directly.\nhttps://forums.aws.amazon.com/thread.jspa?threadID=226140\n. @jasdel Understood, thanks for reaching out to EC2 team and pointing me to the right direction.\nWould you prefer to have this logic in the SDK or do you expect the consumer (projects using the SDK) to implement this? I'm happy to send the PR in the first case.\n. > Were you thinking this functionality would be automatic or an opt in operation that users would explicitly call, returning if the metadata endpoint appears to be an EC2 Metadata service?\nThat is a good question, I was  originally thinking of \"opt-out model\", but opt-in where the consumer/user would just call one extra method (isEndpointValid() bool or something equally easy) is ok with me too.\nIt is still much cleaner than anything we have or discussed so far (manually calling IP address or metadataSvc.Handlers.ValidateResponse with custom header-checking logic).\nI may get to this in a few days, so any suggestions regarding the interface are welcomed.\n. See https://github.com/aws/aws-sdk-go/pull/590\nThe usage is not as simple as I originally thought it would be, but I didn't want to confuse people with naming conventions - i.e. making isEndpointValid() responsible for checking existence & parse-ability of the instance identity document would be IMO misleading.\n. I can see -example flag in the documentation, but I was not able to find it locally:\nsh\n$ go version\ngo version go1.6 darwin/amd64\nsh\n$ go tool vet 2>&1 | grep example | wc -l\n       0\nI have installed Go via Homebrew.\nI can keep digging to find out why it's missing in my installation.\n@jasdel I assume the above grep returns some results on your local machine? If that's the case, then I guess we can close this PR and rather think about opening a PR in Homebrew :smiley: \n. You're right, this part of the Homebrew formula needs fixing:\nruby\n  resource \"gotools\" do\n    url \"https://go.googlesource.com/tools.git\",\n    :revision => \"d02228d1857b9f49cd0252788516ff5584266eb6\"\n  end\nthe revision is from go1.5: https://go.googlesource.com/tools.git/+/release-branch.go1.5\nSo anyone who installed go1.6 from Homebrew will face the same problem.\nI will send a PR to homebrew/homebrew. Thanks for helping me to debug this.\n. https://github.com/Homebrew/homebrew/pull/50101\n. Updated according to feedback.\nAlso when I'm now looking into ec2metadata, I'm thinking there's already one handy function which may be good enough for the use-case we discussed in https://github.com/aws/aws-sdk-go/issues/543\nWould something like this be considered \"stable\" or recommended way to verify EC2 metadata endpoint?\ngo\nmetadataSvc := ec2metadata.New(session.New(cfg))\nif metadataSvc.Available() {\n    // it is verified\n}\nwhich doesn't mean the time implementing this PR was wasted as I believe it may be useful anyway, but probably not in the context I thought it would be.\n. > I think having two methods could create confusing users and might cause them to call HasIAMInstanceProfile then IAMInfo without knowing that they are duplicating the request.\nThe original idea behind this was:\n1. Call Available() -> if false then fail\n2. Call HasIAMInstanceProfile -> if false then fail\n3. Get account ID via doc := GetInstanceIdentityDocument(); doc.AccountID - you could say I should have used IAMInfo() and parse it out from Role ARN, but that would look a lot more hacky\n4. Check if account ID is expected/allowed, if not then fail - we do this kind of check in Terraform and Cumulus does the same thing to prevent human mistakes\n5. If all is ok, add EC2RoleProvider to the provider chain and carry on\nDoes the use-case make sense or you think it's rare?\n. @jasdel Updated according to your comments.\n. @xibz While I may share your concerns, I should say I was mostly inspired by the existing method Available() which does exactly the same thing. :wink: \n``` go\n func (c *EC2Metadata) Available() bool {\n    if _, err := c.GetMetadata(\"instance-id\"); err != nil {\n        return false\n    }\nreturn true\n\n}\n```\nCan you explain the situation where an error may occur and instance may be attached?\nIf the API returns error or erroneous response (i.e. Code != Success) or times out, it kind of means the same thing for the user as if there was no instance profile IMO.\nThe reason a user would be asking about this is because they will need temporary credentials from the API and if the API returns error, I could assume it is very likely to return an error when asking for credentials -> i.e. HasIAMInstanceProfile() should return false in such case, whether it actually means there is no instance profile or AWS API has got an outage for some reason. Maybe the name is just misleading?\n. I will update the PR tomorrow or over the upcoming weekend.\nI think I understand all the reasons you both mentioned why you don't want HasIAMInstanceProfile to be there and work the way I proposed.\nI still appreciate that IAMInfo along with the struct & JSON mapping will get there, so that I can remove this code from Terraform and let the SDK deal with such thing. :smiley: \n. Updated.\n. To be fair it would be best if the API was just capable of handling more than 1 request at a time \ud83d\ude1c\nSince that's not the reality there are IMO two ways of dealing with this:\n\nImplement mutex on the API client side\nRetry - which is what I'm proposing\n\nAdmittedly the first option may be a bit more \"correct\" - is that what you're suggesting?. My only issue is that it feels a bit dirty to do this. My feeling is that API maintainer (AWS in this case) is pushing the responsibility for dealing with the internal problem up to the user and it should be either hidden in the API (best case) or SDK.\nAs far as I'm aware the SDK doesn't have any concept of mutexes for any particular methods? Would there be any interest in adding those to the SDK or do you expect users of the SDK to implement this themselves?. > I'm curious if there is anyway to identify that a resource change is pending. This might be a good place to have a waiter that will wait for a pending resource.\nThat would be awesome.\n\nIn a distributed system environment a mutex within a single application instance won't be much help if two or more instances try to make the change close enough to each other.\n\nIndeed - which is actually argument for retry \ud83d\ude42  although I agree with @xibz that it's not a nice solution, to blindly retry.. > Unfortunately we aren't quite prepared to drop support for godep at this time, as this would be a breaking change for the SDK.\nI understand the cautiousness \ud83d\udc4d but if the migration is done well it should not result in any breaking change for the end user. There will (obviously) be changes in how dependencies are managed, is that what do you mean?\n\nthis change is something we will consider in the future, once Go modules are supported in a more official capacity.\n\nDo you mind expanding your definition of \"official capacity\" a bit more? Go modules are reaching non-experimental phase in Go 1.12 (RC is already out and final is to be released in coming weeks) and the new behaviour will become opt-out (as opposed to opt-in in <=1.12) in 1.13.\nFWIW I haven't noticed major issues when migrating smaller SDKs and libraries (similar size of dependency tree of this SDK) myself.. Thanks for your detailed explanations. That's reasonable.\n\nWith all of that said, are you seeing issues using the SDK and its multiple dependency metadata with your use case?\n\nI do not at this point, my question/request is merely precaution. I'm involved in migrating around 90+ repositories all of which vendor this SDK (mostly as a transitive dependency though) and I'm hoping I won't run into version conflicts as a result of missing metadata.\nI will let you know if I do.. Agreed, that's useless there, I will remove it.\n. Well, the problem here is that instance identity doc has different base path and cannot be obtained via GetMetadata since GetMetadata effectively calls http://169.254.169.254/meta-data whereas instance identity doc is available under http://169.254.169.254/dynamic. That is why I created this extra function.\nShould I create another level of abstraction and built GetDynamicData & GetMetadata on top of it?\n. I'm tempted to agree, I just need to find out how does the interface change for the user of the SDK and how more/less complicated it will be... I think it should be ok, just one extra import... I guess.\n. Or we could make GetMetadata call http://169.254.169.254/ and leave the implementer to define /meta-data... but that would introduce breaking change, so I'm not sure that's a good idea.\n. Will do.\n. ",
    "pmoust": "Hello, just pointing out that this is a blocker for our projects as well.\nIt would greatly help if this could get in sync with the status of other SDKs. \n. @jasdel anything related to s3/ec2/lamda for us. A builder pattern would be a nice generic solution, that said we'd be happy with just exposed structs as well.\n. Also reproduced it with tip on linux.\n. Oh @jasdel, I completely missed that since I was reading the SDK/API examples I figured 'wth?'\nThanks for clarifying and fixing the godoc/examples.\n. ",
    "etdebruin": "Cool, thanks.\nOn Tue, Mar 10, 2015 at 12:16 AM, Loren Segal notifications@github.com\nwrote:\n\nThank you for reporting. This issue should be fixed in the develop branch\n(where we are currently doing development work) and will eventually be\nmerged into the master branch. I will leave this open until we make the\nmerge in case anyone else comes across this issue.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/awslabs/aws-sdk-go/issues/129#issuecomment-78006084.\n. \n",
    "mbrevoort": "@lsegal Thank You! It's working now!\n. ",
    "awinstan": "If the service model is regenerated, won't this revert b95c70b? Shouldn't this be added as an case in ServiceGoCode?\n. And thank you for the quick interim fix!\n. It does. Thank you!\nOn Mar 12, 2015 1:05 PM, \"Loren Segal\" notifications@github.com wrote:\n\n@awinstan https://github.com/awinstan service files are only\nregenerated when -force is used in the code generator. Or at least they\nshould be-- it looks like that's not hooked up correctly, I'll go ahead and\nfix that. Basically we intend to apply many other customizations to S3 and\nother services by hooking into service.go-- the initial file is simply\nboilerplate for future customizations.\nHope that explains things.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/awslabs/aws-sdk-go/issues/135#issuecomment-78593411.\n. @euank Fixed!\n. I agree, good catch! I'll update the PR this evening.\n. \n",
    "c4milo": "@lsegal sorry for commenting on a closed issue but it wasn't totally clear for me what exactly we are loosing by using the aws.ReadSeekCloser(io.Reader) trick.  As far as I understand we lose the following:\n- Request signing\n- Retries \nIs there anything else? \n. wrapping aws.ReadSeekCloser(io.Reader) doesn't seem to work: \nfailed uploading part 1 to S3: XAmzContentSHA256Mismatch: The provided 'x-amz-content-sha256' header does not match what was computed.\n    status code: 400, request id: 7F90A244BC4B3C98\n. I'm giving up on using this SDK, it was not thought out to stream multipart requests from client -> server -> S3 and requires buffering an entire file in the server in order to send it to S3, makes no sense.\n. Ok, I went back and implemented it using s3manager it works as expected for uploading, memory usage is very stable too, unlike https://github.com/rlmcpherson/s3gof3r which seemed to keep allocating memory every time I did an upload.  \nFor downloading I had to use the regular S3 API as http.ResponseWriter does not implement io.WriterAt. But, that is not a problem for my use case.\nThanks for @jasdel for commenting back! \n. > @c4milo can you provide the code that you are using with the SDK that is causing this issue?\nI provided the link to the section of the code involved above but here it goes again: https://github.com/managedbyq/terraform-1/blob/aws_dhcp_options/builtin/providers/aws/resource_aws_vpc_dhcp_options.go#L85-L95\n\nI will say that if the service is accepting your request with a 200 successful response, it's unlikely that the SDK is the cause of any issues here--\n\nYes, the response is 200 and what it is creating can be seen in the screenshot I originally posted above.\n\nthis looks like a configuration issue with an EC2 instance itself.\n\nThe configuration looks OK to me. The EC2 instance is using the VPC that has associated the involved DHCP Option Set.\n\nYou can also verify that the SDK is sending the correct request by enabling logging (LogLevel: 1 in the service constructor).\n\nI will look into this. Thanks!\n. > @c4milo it looks like you are using the SDK correctly, but you may not be using the API correctly (i.e. sending the right values to EC2)\nWhat values should I be sending? I tried sending only one value with a string containing the list of name servers and it didn't work either. \n\nI'm not entirely sure what your intended goal is, but this doesn't look like a problem with the SDK unless you can show wire logs of incorrect data being sent to the service.\n\nUnfortunately, I ran out of time chasing this issue. I'm going to close this for now and re-open it later if it makes sense.\n. It's ok from the SDK side, dhclient is just too weird. \n. There is no issue in Terraform.\nOn Sun, Jun 28, 2015 at 5:01 PM Aris Pikeas notifications@github.com\nwrote:\n\nI'm not sure what that means - is there currently an issue in Terraform,\nor was it resolved?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-go/issues/210#issuecomment-116333746.\n. I'm hitting this issue too. I need to stream multipart requests coming from clients to my server and up to S3 without too much buffering. The SDK, as it currently stands, does not seem to be thought out for this use case and requires buffering an entire part before being able to push it up to S3. \n. I ended up switching to https://github.com/rlmcpherson/s3gof3r which does this correctly. \n. \n",
    "rcway": "Thanks, works great with the new API\n. ",
    "iNickGo": "do you have sample code that will get this error?\n. ",
    "ando-masaki": "Following way, I caught error \"The provided 'x-amz-content-sha256' header does not match what was computed.\".\ngo\n        bucket := \"foo\"\n        key := \"bar\"\n        req := s3.PutObjectInput{\n                ACL:             aws.String(\"private\"),\n                Body:            aws.ReadSeekCloser(buf),\n                Bucket:          aws.String(bucket),\n                ContentEncoding: aws.String(\"gzip\"),\n                ContentLength: aws.Long(int64(buf.Len())),\n                ContentMD5: func() *string {\n                        h := md5.New()\n                        h.Write(buf.Bytes())\n                        return aws.String(base64.StdEncoding.EncodeToString(h.Sum(nil)))\n                }(),\n                ContentType: aws.String(\"application/x-gzip\"),\n                Key:         key,\n        }\n        if _, err = cfg.S3.PutObject(&req); err != nil {\n                log.Fatal(err)\n        }\nI solved it to change Body parameter and not use ContentLength.\ngo\n        bucket := \"foo\"\n        key := \"bar\"\n        req := s3.PutObjectInput{\n                ACL:             aws.String(\"private\"),\n                Body:            bytes.NewReader(buf.Bytes()),\n                Bucket:          aws.String(bucket),\n                ContentEncoding: aws.String(\"gzip\"),\n                ContentMD5: func() *string {\n                        h := md5.New()\n                        h.Write(buf.Bytes())\n                        return aws.String(base64.StdEncoding.EncodeToString(h.Sum(nil)))\n                }(),\n                ContentType: aws.String(\"application/x-gzip\"),\n                Key:         key,\n        }\n        if _, err = cfg.S3.PutObject(&req); err != nil {\n                log.Fatal(err)\n        }\n. And I used ContentLength, I caught following error.\nA header you provided implies functionality that is not implemented\nTherefore, I don't use ContentLength.\n. ",
    "leelynne": "As a work around I ended up setting the body to nil if presigning - https://github.com/talio/aws-sdk-go/commit/fa59979e5ca9a545b77b88569c4151c1a44141cf\n. Not a committer but you don't want to import that code directly.  In fact the main purpose of internal is for code that is purposely not supported outside the package.  Starting with Go 1.5 this will be enforced - https://docs.google.com/document/d/1e8kOo3r51b2BWtTs_1uADIA5djfXhPT36s6eHVRIvaU\nYou could copy it instead into a new library instead\n. Ugh, my apologies. I thought I was on the latest version but actually had it locked on 1.5.1. All good. Thanks!. ",
    "dthuering": "Looks like it is still broken. At least for PUT requests.\n`// expires := time.Now().Add(30 * time.Minute)\n    input := &s3.PutObjectInput{\n        Bucket:      aws.String(s.Bucket),\n        Key:         aws.String(key),\n        ContentType: aws.String(contentType),\n        // Expires:     &expires,\n    }\nreq, _ := s.s3.PutObjectRequest(input)\n\nurlStr, err := req.Presign(30 * time.Minute)\n\n`\ngenerating the URL with the JS SDK works.. ",
    "kshinn": "Delimiter is not specified in the above mentioned requests. According to the AWS documentation, NextMarker is only returned from the server when delimiter is specified. This seems to be working consistently with the API documentation. \nFeature Request: Always fill in NextMarker when IsTruncated is true.\n. @lsegal This makes sense. I figured that was the intent. In that case, this is not an issue and can be closed. Looking forward to pagination!\n. ",
    "pas256": "A +1 from me too.\n. Fantastic. Thank you.\n. Oh, the docs don't list:\nhttp://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html\nonly InvokeAsync.\n. I hit refresh on the docs page manually. It is there now. Crisis over :)\n. ",
    "mdub": "+1 for defaulting region from instance meta-data\n. ",
    "bmatsuo1": "Thanks for the info. For the time being I altered restxml.Build() so it adds Content-MD5 to every request with a body. Pretty lame, and probably not entirely correct. But it is working for the calls in my toy program. I'd be willing to write a patch to help fix this for real if someone had know-how/thoughts on the right way to approach the problem.\nFor example, it seems like the API spec documents (under directory apis/) are the right spot to put these requirements. Is there already enough information there to determine when the header needs to be added? It doesn't seem so to me.\n. Thanks for the explaination, @lsegal. Both points make sense.\n. ",
    "danp": "Thanks @lsegal and @jasdel!\n. :clap: \n. @lsegal any thoughts on @sclasen's responses? Would be great to get some movement on this. Looks like it needs a rebase as well.\n. Maybe fixed by #227?\n. Is this what you're after?\n. ",
    "foresmac": "@lsegal I'll open issues if I run into anything specific; mostly I'm just having trouble generating the right options struct to make it work because (and this is a complain I have about AWS in general) the documentation is so obtuse.\nThanks.\n. @jasdel Thanks for the pointer; once I create a job in the console, is there a way to get the config copied so I can use it to build a struct programmatically? If so, it would be a nice pointer to add to the docs. ;)\n. @jasdel Cool, will do on the feature request. :stars: \nI guess if I could describe my particular goal at the moment, you might have a specific suggestion. I feel this is probably the 80% case if I had to guess:\nI have an S3 bucket where all the original files are uploaded. I have an S3 bucket where all the transcoded files go. I have a pipeline already set up for this. And I'm using a pre-set for transcoding options. I think the only other thing I need to specify is the output key format. I believe other than that I just want ET to just do its thing. What would that look like?\n. Just adding some notes as I work my way through the docs and the source to figure out how to using the SDK:\nIn the example to create a job, nil is passed to New() when creating the service. As best as I can tell, at a minimum I need to pass an *aws.Config with at least credentials set up. Probably the 99% case is doing so via ENV, which it appears is done automatically by the default config. But, you need to pass a region name in as well, and there's no default for this, so you'd need something like what is detailed here:\ngo\nsvc := elastictranscoder.New(&aws.Config{Region: \"us-east-1\"})\n. All over the example, every string is wrapped in a custom type like so:\ngo\naws.String(\"Example String\")\nBut the \"Getting Started\" docs show just passing strings into structs. Can this be safely ignored?\n. @lsegal I missed the part after \"not set by default\" where it says \"reads in a ENV variable\". Sorry.\n. What I think the minimal params are:\n- JobInput\n- PipelineID\n- Output (or Outputs, but we're talking one file to another)\nWhat is minimally needed for JobInput:\ngo\n    Input: &elastictranscoder.JobInput{ // Required\n        AspectRatio: \"auto\", // Should this be \"Auto\" or \"auto\"?\n        Container:   \"auto\",\n        FrameRate:  \"auto\",\n        Interlaced: \"auto\",\n        Key:        \"s3_filename.mov\",\n        Resolution: \"auto\",\n    }\nIf it's all auto, is this the default? Or would &elastictranscoder.JobInput{Key: \"s3_filename.mov\"} work?\nWhat is minimally needed for Output:\ngo\n    Output: &elastictranscoder.CreateJobOutput{\n        Key:             \"transcoded_filename.mp4\",\n        PresetID:        \"1351620000001-000010\", // Generic 720p H.264\n        Rotate:          \"auto\",\n        ThumbnailPattern: \"-{count}-{resolution}\",\n    },\nThis is the best I could discern from the console settings. I did request the options to copy the configuration from a job to be added to the console as requested.\n. Oh, JobInput might require setting the JobContainer, but I haven't been able to confirm what this setting is. Maybe the console infers it from the input key from S3?\n. @lsegal Yes, it looks like this example more or less confirms what I outlined above. When you say \"should be in the docs themselves\", which docs are you referring to? Separate Elastic Transcoder API documentation?\nAs I noted on the confusion on whether a Go string literal or aws.String was required, the Getting Started guide shows examples of aws.Config using string literals as a counter example. So, maybe it's ok there but not in params for services?\nI'm really not trying to be obtuse, but you can see how when I need to look at four different places (Getting Started guide, SDK godocs, example projects for other SDK languages, and I presume Elastic Transcoder API documentation) for documentation it gets hard to be sure if I understand things correctly.\n. I've been making successful requests with the following minimal parameters:\ngo\n    params := &elastictranscoder.CreateJobInput{\n        Input: &elastictranscoder.JobInput{\n            AspectRatio: aws.String(\"auto\"),\n            Container:   aws.String(\"auto\"),\n            FrameRate:   aws.String(\"auto\"),\n            Interlaced:  aws.String(\"auto\"),\n            Key:         aws.String(key), // the \"filename\" in S3\n            Resolution:  aws.String(\"auto\"),\n        },\n        PipelineID: aws.String(\"09583757687362-xxxxxx\"), // Pipeline can be created via console\n        Output: &elastictranscoder.CreateJobOutput{\n            Key:              aws.String(key + \".mp4\"),\n            PresetID:         aws.String(\"1351620000001-000010\"), // Generic 720p H.264\n            Rotate:           aws.String(\"auto\"),\n            ThumbnailPattern: aws.String(key + \"{count}-{resolution}\"),\n        },\n    }\n. > Getting Started Guide - this document should be used to get a general idea of how to configure and make basic requests with the SDK. If it's your first run with the SDK, you probably want to be following this in addition to the API documentation, since this doc focuses solely on the syntax and behavior of the library itself, and not on the behavior of our AWS services.\n\nSDK godocs (API docs) - this should be your primary stop for reading about the service you're trying to use. Everything in Elastic Transcoder's API reference is in our own API documentation, including what values can be passed in to which structures. For example, JobInput mentions using \"auto\" in a number of places, and these are the same places that you would find them in the regular API docs. Our example operations show all required fields with a // Required comment, so if you do not see that then the field is optional and has a server-side default. We could do a better job of pointing this out in the textual documentation (the non-example portions), but for now you can identify required params with the required:\"true\" tag on the member.\nService Developer Guide (Elastic Transcoder for example) - this document extends far beyond the scope of the SDK. If you're getting started with a service and want to understand the concepts, you should probably give this doc a read since it will give you a basis for vocabulary and other terminology. You should not need this document for coding, though in some cases, services may supply helpful samples that you might want to look out for.\n\nThe above would be perfect to put into the main README, I think.\n. I agree with the point here; if the SDK abstracted some of the complicated configuration set up, it would alleviate a lot of the need for explanations requested in #262, for instance.\nI think part of this is a result of the verbose XML origins of the configuration parameters for the AWS API and the automated code generation of the SDKs. The later makes it easier to keep the SDKs up-to-date with the API, but the trade-off is that the official SDKs seem to be harder to use.\n. @lsegal Agree with all your points and I understand why it is the way it is.\nI'm happy to make my own library of convenience functions and accept the responsibility of keeping up to date for my own use; and that's probably what others would do as well.\n. Once I understand enough from #262, I'm happy to! :+1:\n. I agree with @conslo that the official SDK is very un-Go-like. Like I said earlier in the thread, I understand that this is a result of the Java origins of the AWS APIs and the auto-generation of code from the API. But the side effect is that non-Java SDKs are always going to \"feel\" wrong because they just don't really follow common idioms or patterns for those other languages.\nI'm not sure there's a \"good\" way to address this other than to build an idiomatic library on top of the official SDK. Otherwise users are left to choose between an official SDK that conforms perfectly to the API but is difficult to use, or something like goamz that's easy to use but doesn't always conform to the current API.\n. I think the helper function to build common job input scenarios would go a long way to address the pain points I mentioned regarding Elastic Transcoder, for instance. There are really only like 5 things that I think would honestly be passed in in the 80% use case, while about 10 things would be default values and the remaining 30 or so can generally be ignored.\n. ",
    "richardbowden": "@lsegal @pdalinis thanks for your input, I was previous using hailocab/goamz and SQS, having 4 routiens polling a queue and deleting the messages once complete, the delete seemed to delete messages that were being consumed on another routine...\nI am going to switch to this SDK, and as you say you are using it, good enough for me...cheers\n. ",
    "robbyram": "I am having a similar problem with SendMessageBatch.  This worked before this recent spate of changes.\n. Awesome, thank YOU\n. ",
    "jostyee": "huh, after go get it working now.\n. oic, thanks for responding.\n. @lsegal Still not working here.\n%!(EXTRA string=InvalidParameterValue, string=The message attribute 'Key' has an invalid message attribute type, the set of supported type prefixes is Binary, Number, and String.)\n. Is there a more direct way to interact, such as Slack/IRC/Gitter? It's too inconvenient to discuss via issues.\n. It can be split to 2 parts of problems:\nthe first is the duplicate sent messages, logs:\nhttps://gist.github.com/iyee/cc96147eaf9d167267ca\nReceiveMessage get same msgs multiple times.\nthe second is the batchdelete func not working, logs:\nhttps://gist.github.com/iyee/a942c3d6b25782b91679\nafter invoke it, msg still can be reached at the next receive loop.\n_[edited]_I've looked into the logs myself, seems the delete action is fine, but SQS continues to produce more same msgs to consume.\n. Checked again, sent from both aws-sdk-java and aws-sdk-go client are normal, 'messages available' number is equal to sent ones.\nBut the aws-sdk-go ReceiveMessage doesn't deal with them correctly, will repeatedly consumes same msgs:\nI0407 18:30:43.679225   32532 worker.go:24] worker: Received 3 messages\nI0407 18:30:43.679320   32532 worker.go:39] worker: Spawned worker goroutine\nI0407 18:30:43.679346   32532 handler.go:21] **msg_id:0caf0f3e-17a0-4326-825c-ff0d840d7f4a**, msg:Send text from Java: 5\nI0407 18:30:43.679441   32532 worker.go:39] worker: Spawned worker goroutine\nI0407 18:30:43.679455   32532 handler.go:21] **msg_id:0caf0f3e-17a0-4326-825c-ff0d840d7f4a**, msg:Send text from Java: 5\nI0407 18:30:43.679513   32532 worker.go:39] worker: Spawned worker goroutine\nI0407 18:30:43.679534   32532 handler.go:21] **msg_id:0caf0f3e-17a0-4326-825c-ff0d840d7f4a**, msg:Send text from Java: 5\n@lsegal Tried to use DeleteMessage instead of DeleteMessageBatch without luck\ncodes:\nfunc handleMsg(q *gaea.Sqs, h *Handler, m *sqs.Message) error {\n    if err := (*h).Handle(m); err != nil {\n        return err\n    }\n    err := q.Delete(m.ReceiptHandle)\n    if err == nil {\n        glog.Infof(\"DeleteMessage okay: \", *m.ReceiptHandle)\n    } else {\n        glog.Errorf(\"DeleteMessage fail: \", *m.ReceiptHandle, err)\n    }\n    return err\n}\noutput:\nLog file created at: 2015/04/07 18:46:25\nRunning on machine: worktop\nBinary: Built with gc go1.4.2 for darwin/amd64\nLog line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\nI0407 18:46:25.951708   33239 worker.go:15] Start consumer:https://sqs.cn-north-1.amazonaws.com.cn//alerting_slack\nI0407 18:46:25.976558   33239 worker.go:24] worker: Received 3 messages\nI0407 18:46:25.976611   33239 worker.go:39] worker: Spawned worker goroutine\nI0407 18:46:25.976624   33239 handler.go:21] msg_id:b2efa2a7-f3d4-4859-8e22-17ca5d4d6b6b, msg:Send with \u2665 through SQS10\nI0407 18:46:25.976903   33239 worker.go:39] worker: Spawned worker goroutine\nI0407 18:46:25.976954   33239 handler.go:21] msg_id:b2efa2a7-f3d4-4859-8e22-17ca5d4d6b6b, msg:Send with \u2665 through SQS10\nI0407 18:47:14.838530   33239 worker.go:64] DeleteMessage okay: %!(EXTRA string=AQEBy+KwAUmjb6FwKsBq5y1JvDeeT4sWViuGmkGy3LgSfwzjwCbNt2P1HB2uWV6APl7CnHkMij175Yhm3xZR9dQoRyzZeen0LWYJqFKJyWm5JKF4rfT02c0KrjfY0TI5sEVubPMaaBdLJnH8YEmxDSMESLzvZ2SN2ZmFHd+pchBYXKXpCdw8XyjWnUgqCJybSttpP7lhf6lgUddMmdeTn5LkrABrBEokXwy/3Zwz+jmgXuN/dWIl90ksJGg9zczgfO/==)\nI0407 18:47:14.977719   33239 worker.go:64] DeleteMessage okay: %!(EXTRA string=AQEBy+KwAUmjb6FwKsBq5y1JvDeeT4sWViuGmkGy3LgSfwzjwCbNt2P1HB2uWV6APl7CnHkMij175Yhm3xZR9dQoRyzZeen0LWYJqFKJyWm5JKF4rfT02c0KrjfY0TI5sEVubPMaaBdLJnH8YEmxDSMESLzvZ2SN2ZmFHd+pchBYXKXpCdw8XyjWnUgqCJybSttpP7lhf6lgUddMmdeTn5LkrABrBEokXwy/3Zwz+jmgXuN/dWIl90ksJGg9zczgfO/==)\n12 sent but 31 received, no error occured.\nPS: Through the SQS Management Console, I can find the messages consumed slowly \n. ReceiveMessage will always continue to receive a message until it's been deleted from the queue, and marked as such by a successful 200 response from a Delete operation request (simply sending a request does not imply that it was necessarily deleted yet).\n@lsegal My problem is not only receive a message multiple times, but also receive multiple same message at one time.\nI doubt there is any race condition, since I just start a thread to infinitely for loop result from ReceiveBatch(), send msgs to a goroutine to consume, and then sleep 2 seconds to void the undeleted msgs.\n. figured out. I've reference a defferent msg obj to consume.\ninvoke DeleteMessege() after Receive one by one is OK, but DeleteMessegeBatch() would still get duplicate messages.\n. ",
    "magegu": "m.barcode is a string (eg \"C00295116\" ) as part of the following struct\ntype Mailing struct { \n    id int\n    barcode string\n    received time.Time\n    customerId sql.NullInt64\n    db *sql.DB\n    s3 *s3.S3\n}\nno problem when using it in log like here\nlog.Printf(\"file not found at %v with barcode %v \\n\", filepath, m.barcode)\n. Thanks for the suggestion! I did not have the time to setup the debugging properly today. I ll push it next week!\n. hi there,\ndoe does that log of an failed upload help?\nhttps://gist.github.com/magegu/36e0f2350a22b7cfcdfb\n. quick update: \nmy barcode string is created by scanning a SQL row via the go-sql-driver from a mysql latin1 table. Since you mentioned encoding issues as a possible cause, I converted the barcode to a UTF-8 String via the fromISO88591 function mentioned at http://stackoverflow.com/questions/24555819/golang-persist-using-iso-8859-1-charset\nUnfortunately, this did not fix the \"The request signature we calculated does not match the signature you provided. Check your key and signing method.\"\nmaybe also worth noting: my PUTs are happening in multiple go routines since i'm uploading many files concurrently.\nI'm still digging around to find the issue\n. ok, fixed.\nafter comparing envs etc I tried to use different credentials for the code. now it works. \nI'm not sure at which point it stopped working and why, but with new credentials its working.\nthanks for the support anyway.\n. @lsegal thanks for your detailed answer! \nyou have been right, once i reduced the number of workers the failures came later .. i'm still trying to find the sweet spot where i get maximum thoughput without throttling :)\n. @lsegal i might need a bit of support here.\ni'm trying to implement Error Retry behaviour following suggestions i found here: http://docs.aws.amazon.com/general/latest/gr/api-retries.html After each try i'm waitng 200ms*number_of_tries.\nI'm using 32 goroutines each responsible for one or more uploads. Unfortunately, I'm getting not a single \"success\" log.\nI'm killing my upload app when I see that the first goroutine logs are at try > 50. so he already tried 50times to PUT one single file. \ndo you have any suggestion how to improve the retry behaviour in go? \nthis is my snipped now. \ntry := 0\nfor err != nil || try == 0 {\n  try++\n  _, err = s3client.PutObject(&s3.PutObjectInput{\n    Body:   fi,\n    Bucket: aws.String(envelopes_bucket),\n    Key:    aws.String(keyName), //fmt.Sprintf(\"%v.jpg\", barcode)\n  })\n  if err != nil {\n    time.Sleep(time.Millisecond * time.Duration(try*200))\n    log.Printf(\"envelope: %v\\n\", err)\n    log.Printf(\"failed envelope... trying %v again: %s to %s\\n\", try, envelopeFilepath, envelopes_bucket)\n  } else {\n    if try > 1 {\n      log.Printf(\"envelope success after try  %v\\n\", try)\n    }\n  }\n}\nthe error messages are all like: 2015/04/23 11:48:28 envelope: NotImplemented: A header you provided implies functionality that is not implemented\nthanks!\n. have you tried more than 12k files? i'm hitting the errors somewhere around this threshold.\n. @jasdel @lsegal \nI just uploaded the same amount of files (400k) with a node.js apllication using a similiar pattern, except go routines i'm using eachLimit (https://github.com/caolan/async). it works like a charm. so i guess there is really a bug hidden in the go sdk OR its about goroutines (nodejs is single threaded after all)\n. ",
    "geraldstanje": "is that even possible with the SDK? i would like to write an auto deploy script for my Docker...\n. ok cool. i started reading the documentation...\nhow can i set the predefined configuration using docker?\nPredefined configuration: Docker (AWS Elastic Beanstalk will create an environment running Docker 1.5.0 on 64bit Amazon Linux 2014.09 v1.2.1.)\nEnvironment type: Single instance \nim new to aws. is there an golang example available which i could use for my adaption?\n. @lsegal do you prefer ssh into AWS Elastic Beanstalk + run a deploy script or use the aws sdk?\n. ",
    "pires": "Any progress on this?\n. Any progress on this?\n. ",
    "grahamc": "Any progress? I'd like to use this to identify the instance-id being run against.\n. ",
    "Iouns": "I forgot to mention other resources (VPC, instances, eip, security groups... ) are working fine.\n. Thank you for your help, I'll do this.\n. ",
    "hallas": ":+1: \nLooking for this my self however it shouldn't be to hard to PR and add custom to ones own project until it's added in?\n. I worked it out in Go tonight. I'll share tomorrow.\n. @zshenker \nSomething like this https://gist.github.com/hallas/6a8d3a297dc481301fed should do the trick for you. You need to fill in the constants and the PEM path. It worked for me. I hope it works for you.\n. Great stuff, thanks @jasdel \n. ",
    "apremalal": "This way we need to have credentials available (KeyID, privateKey). Is it possible to use EC2 IAM roles for signing?. ",
    "defunct73": "The data race does go away but doesn't that change effectively serialize all API calls? Pulling AMIs from all regions took less than a second before and now takes around 2 to 3 seconds.\n. Agreed, I re-ran my tests as well and saw times much improved. Transient network issues, I suppose. :)\nAnyway, thanks for the quick turnaround on the fix.\n. ",
    "ejholmes": "+1 on adding this in, but go generate fails with the following using this api definition:\nError generating ../apis/ecs/2014-11-13.normal.json\nruntime error: invalid memory address or nil pointer dereference\n/Users/ejholmes/go/src/github.com/awslabs/aws-sdk-go/internal/model/cli/gen-api/main.go:92 (0x33b3)\n    func.001: fmt.Fprintf(os.Stderr, fmtStr, file, r, debug.Stack())\n/usr/local/Cellar/go/1.4.1/libexec/src/runtime/asm_amd64.s:401 (0x39745)\n    call16: CALLFN(\u00b7call16, 16)\n/usr/local/Cellar/go/1.4.1/libexec/src/runtime/panic.go:387 (0x15118)\n    gopanic: reflectcall(unsafe.Pointer(d.fn), deferArgs(d), uint32(d.siz), uint32(d.siz))\n/usr/local/Cellar/go/1.4.1/libexec/src/runtime/panic.go:42 (0x1443e)\n    panicmem: panic(memoryError)\n/usr/local/Cellar/go/1.4.1/libexec/src/runtime/sigpanic_unix.go:26 (0x18830)\n    sigpanic: panicmem()\n/Users/ejholmes/go/src/github.com/awslabs/aws-sdk-go/internal/model/api/passes.go:89 (0x717ce)\n    (*API).renameToplevelShapes: switch n := len(v.InputRef.Shape.refs); {\n/Users/ejholmes/go/src/github.com/awslabs/aws-sdk-go/internal/model/api/load.go:43 (0x6dfe0)\n    (*API).Setup: a.renameToplevelShapes()\n/Users/ejholmes/go/src/github.com/awslabs/aws-sdk-go/internal/model/api/load.go:26 (0x6de42)\n    (*API).Attach: a.Setup()\n/Users/ejholmes/go/src/github.com/awslabs/aws-sdk-go/internal/model/cli/gen-api/main.go:28 (0x20ca)\n    newGenerateInfo: g.API.Attach(modelFile)\n/Users/ejholmes/go/src/github.com/awslabs/aws-sdk-go/internal/model/cli/gen-api/main.go:96 (0x36b7)\n    func.002: if g := newGenerateInfo(file, svcPath); g != nil {\n/usr/local/Cellar/go/1.4.1/libexec/src/runtime/asm_amd64.s:2232 (0x3b801)\n    goexit:\n. Did this in https://github.com/awslabs/aws-sdk-go/pull/193\n. I just ran into this as well. It appears that even a more trivial Argument for the waiter fails:\nservices | [@[?runningCount!=`0`]][] | length(@) == `0`\nGiven the input:\njson\n{\n  \"failures\": [],\n  \"services\": [\n    {\n      \"clusterArn\": \"arn:aws:ecs:us-west-2:012345678910:cluster/telemetry\",\n      \"deploymentConfiguration\": {\n          \"maximumPercent\": 200,\n          \"minimumHealthyPercent\": 100\n      },\n      \"deployments\": [\n        {\n          \"createdAt\": 1432829320.611,\n          \"desiredCount\": 0,\n          \"id\": \"ecs-svc/9223370604025455196\",\n          \"pendingCount\": 0,\n          \"runningCount\": 0,\n          \"status\": \"PRIMARY\",\n          \"taskDefinition\": \"arn:aws:ecs:us-west-2:012345678910:task-definition/hpcc-t2-medium:1\",\n          \"updatedAt\": 1432829320.611\n        }\n      ],\n      \"desiredCount\": 0,\n      \"events\": [],\n      \"loadBalancers\": [],\n      \"pendingCount\": 0,\n      \"runningCount\": 0,\n      \"serviceArn\": \"arn:aws:ecs:us-west-2:012345678910:service/bunker-buster\",\n      \"serviceName\": \"bunker-buster\",\n      \"status\": \"ACTIVE\",\n      \"taskDefinition\": \"arn:aws:ecs:us-west-2:012345678910:task-definition/hpcc-t2-medium:1\"\n    }\n  ]\n}\nIt returns false with go-jmespath, but true at http://jmespath.org. It appears that the value of runningCount in this case is actually the location in memory, since it's an *int64, not int64 so it fails when compared against 0.\nIt looks like fieldFromStruct doesn't attempt to obtain the actual value from a pointer to a basic type.\n. I opened a PR at https://github.com/jmespath/go-jmespath/pull/15 that adds a failing test for this as well as a potential implementation. Although, I can't help but feel like passing in req.Data is going to be problematic since it's basically relying on the struct fields to match the JSON exactly, which isn't always the case. Maybe it would be better to just pass in the raw response body instead?\n. > In this example the temporary flag would instruct the SDKs to get a session token instead of IAM role credentials, correct?\nCorrect. At the moment, ~/.aws/config doesn't provide a method to define a profile that uses GetSessionToken to obtain credentials, and that's what I'd be looking for.. ",
    "willejs": "Closing this in favour of #193\n. :+1: \n. ",
    "jen20": "Is this expected to be accepted at some point? If not I'll rework the stuff in mitchellh/packer#2034 to retry etc.\n. ",
    "epipho": "@lsegal any chance one of these will be merged soonish?\n. ",
    "lilirui": "```\nError: SignatureDoesNotMatch The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\nThe Canonical String for this request should have been\n'POST\n/\nhost:cloudformation.ap-southeast-1.amazonaws.com\nx-amz-date:20150416T000129Z\nhost;x-amz-date\ncfa97c292558adbe821bd4025f3cf2b976791586857e64868d0aea874269027e'\nThe String-to-Sign should have been\n'AWS4-HMAC-SHA256\n20150416T000129Z\n20150416/ap-southeast-1/cloudformation/aws4_request\n5e32441595579025110271974484d9b7c2b682219c4e0f69e274f1d241616f8b'\n{\n}\nLiruis-MacBook-Pro-2:aws-test liruili$ go run cf_cli.go \n=> [2015-04-15 17:08:02.584944146 -0700 PDT] cloudformation.ListStacks(&{NextToken:0xc20800ad80 StackStatusFilter:[0xc20800ad90] metadataListStacksInput:{SDKShapeTraits:false}})\n---[ REQUEST PRE-SIGN ]------------------------------\nPOST / HTTP/1.1\nHost: cloudformation.ap-southeast-1.amazonaws.com\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\nUser-Agent: aws-sdk-go/0.5.0\nAction=ListStacks&NextToken=NextToken&StackStatusFilter.member.1=CREATE_COMPLETE&Version=2010-05-15\n---[ CANONICAL STRING  ]-----------------------------\nPOST\n/\nhost:cloudformation.ap-southeast-1.amazonaws.com\nx-amz-date:20150416T000802Z\nhost;x-amz-date\ncfa97c292558adbe821bd4025f3cf2b976791586857e64868d0aea874269027e\n---[ STRING TO SIGN ]--------------------------------\nAWS4-HMAC-SHA256\n20150416T000802Z\n20150416/ap-southeast-1/cloudformation/aws4_request\n0100756baa044e84d66757384b843f3fa7101f06d800490b08b2c7e4e70b32b4\n---[ SIGNED URL ]--------------------------------\nhttps://cloudformation.ap-southeast-1.amazonaws.com/\n-----------------------------------------------------\n---[ REQUEST POST-SIGN ]-----------------------------\nPOST / HTTP/1.1\nHost: cloudformation.ap-southeast-1.amazonaws.com\nUser-Agent: aws-sdk-go/0.5.0\nContent-Length: 99\nAuthorization: AWS4-HMAC-SHA256 Credential=.../20150416/ap-southeast-1/cloudformation/aws4_request, SignedHeaders=host;x-amz-date, Signature=54826b51bbde018b6e240e8a5fa3e4a73d377063c52ee7d045602388839f8ba9\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\nX-Amz-Content-Sha256: cfa97c292558adbe821bd4025f3cf2b976791586857e64868d0aea874269027e\nX-Amz-Date: 20150416T000802Z\nAccept-Encoding: gzip\nAction=ListStacks&NextToken=NextToken&StackStatusFilter.member.1=CREATE_COMPLETE&Version=2010-05-15\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 403 Forbidden\nContent-Length: 857\nContent-Type: text/xml\nDate: Thu, 16 Apr 2015 00:08:03 GMT\nX-Amzn-Requestid: a68b2dc8-e3cc-11e4-a296-c194fcb03423\n\n\nSender\nSignatureDoesNotMatch\nThe request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\nThe Canonical String for this request should have been\n'POST\n/\nhost:cloudformation.ap-southeast-1.amazonaws.com\nx-amz-date:20150416T000802Z\nhost;x-amz-date\ncfa97c292558adbe821bd4025f3cf2b976791586857e64868d0aea874269027e'\nThe String-to-Sign should have been\n'AWS4-HMAC-SHA256\n20150416T000802Z\n20150416/ap-southeast-1/cloudformation/aws4_request\n0100756baa044e84d66757384b843f3fa7101f06d800490b08b2c7e4e70b32b4'\n\n\na68b2dc8-e3cc-11e4-a296-c194fcb03423\n\n\nError: SignatureDoesNotMatch The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\nThe Canonical String for this request should have been\n'POST\n/\nhost:cloudformation.ap-southeast-1.amazonaws.com\nx-amz-date:20150416T000802Z\nhost;x-amz-date\ncfa97c292558adbe821bd4025f3cf2b976791586857e64868d0aea874269027e'\nThe String-to-Sign should have been\n'AWS4-HMAC-SHA256\n20150416T000802Z\n20150416/ap-southeast-1/cloudformation/aws4_request\n0100756baa044e84d66757384b843f3fa7101f06d800490b08b2c7e4e70b32b4'\n{\n}\n```\n. This is weird, I don't have proxy installed but after I reboot my laptop, the issue is gone.\nAnyway thank you Loren!\n. ",
    "julienvey": "Thank you @lsegal for the quick fix !\n. ",
    "richarddbarnett": "Looking into the Travis test failure, I can't reproduce it locally.\n. Thanks for merging into #211, will try it soon.\n. ",
    "sacheendra": "I changed the table name to a dummy name and the method still works. Shouldn't it return an error if table does not exist?\n. The problem was caused because the region was not set. Setting the region fixed the problem. \n. Even after setting the region, still get the same output.\n&{Attributes: ConsumedCapacity: ItemCollectionMetrics: metadataPutItemOutput:{SDKShapeTraits:false}}\nNow I also get an error object and after doing aws.Error(err).Error() on it, an empty string is the result. \nI am loading credentials using EnvCreds() and the credentials are valid. Using the same creds, dynamo is accessible from the CLI. \nAny idea why this is happening? I can confirm that the app is connecting to dynamo due the latency. Around 2 secs for the first request and 300ms for the subsequent requests. \n. Setting LogLevel: 1, the responses are printed and the response is \n{\"__type\":\"com.amazon.coral.service#SerializationException\"}\nShouldn't this be returned as an error. The error object I am getting is empty. \nThe result of fmt.Println(err == nil, err.Error() == \"\") is\nfalse, true\nAfter looking at my request again, I realized that I am using the \"null\" character in some of my strings and that is causing the problem. Doesn't dynamodb support UTF-8 completely in strings? Then having nulls in the string should not be a problem right. \nThe request when it is printed out has this string\ntest_collection\\x00test_id\n. I can confirm that the same thing happens with other non-printable characters like \\001, \\002, etc...\nI want to do namespacing using a character which can never occur in a user entered string. Any other ideas than non-printable characters?\n. Thank you for fixing it.\n. ",
    "josharian": "Thanks for the speedy review.\nUsing a BeforeRetry handler sounds reasonable. Updated: dc19fa0. If you want a clean commit history, I am happy to squash all the commits if/when you're ready to merge.\nRegarding #196, I'm not really sure I understand what is going on there, and I'd like this fix to be orthogonal to it. (And hey, maybe this PR will make the build pass for #196.)\n. In case it helps, here's a script to test retries by hitting API limits: https://gist.github.com/josharian/b77539174c0ac5a5ca13\n. Superceded by #211.\n. LGTM. I'd be tempted to go for an int instead of SettableBool -- see e.g. https://github.com/golang/tools/blob/master/cmd/vet/main.go#L56 -- but it is minor, and immaterial to the question at hand.\nIt'll take a bit to get this integrated into our broader system, but if we still encounter problems then, I will diagnose...so full speed ahead!\nThanks so much for the help with this. I look forward to the merge. :)\n. bool requires an alloc. Also, accidentally copying a bool can make different values alias.\n. Glad to see this merged! Thanks, @jasdel.\nFWIW, @euank you don't use a Set method, you just assign a new value: http://play.golang.org/p/ZaGvE-9y_A. Tristate is pretty type-safe--attempting to use a Tristate where a bool is required will fail to compile.\n. ",
    "mateusz": "Neither of these PRs affects it. On a hunch I tried putting the client creation into the for loop so that it's recreated every time, but the result is the same. Always something like this:\n2015/04/28 02:49:32 Sending successful.\n2015/04/28 02:49:38 Sender failed to put record: Post https://kinesis.ap-southeast-2.amazonaws.com/: EOF, Post https://kinesis.ap-southeast-2.amazonaws.com/: EOF, &url.Error{Op:\"Post\", URL:\"https://kinesis.ap-southeast-2.amazonaws.com/\", Err:(*errors.errorString)(0xc20800a0e0)}\n2015/04/28 02:49:45 Sending successful.\n2015/04/28 02:49:51 Sender failed to put record: Post https://kinesis.ap-southeast-2.amazonaws.com/: EOF, Post https://kinesis.ap-southeast-2.amazonaws.com/: EOF, &url.Error{Op:\"Post\", URL:\"https://kinesis.ap-southeast-2.amazonaws.com/\", Err:(*errors.errorString)(0xc20800a0e0)}\n2015/04/28 02:49:58 Sending successful.\n2015/04/28 02:50:04 Sender failed to put record: Post https://kinesis.ap-southeast-2.amazonaws.com/: EOF, Post https://kinesis.ap-southeast-2.amazonaws.com/: EOF, &url.Error{Op:\"Post\", URL:\"https://kinesis.ap-southeast-2.amazonaws.com/\", Err:(*errors.errorString)(0xc20800a0e0)}\n. Here you go.\n```\n=> [2015-04-28 03:55:56.738735534 +0000 UTC] kinesis.PutRecord(&{Data:[97] ExplicitHashKey: PartitionKey:0xc2081c9520 SequenceNumberForOrdering: StreamName:0xc2081c9530 metadataPutRecordInput:{SDKShapeTraits:false}})\n---[ REQUEST PRE-SIGN ]------------------------------\nPOST / HTTP/1.1\nHost: kinesis.ap-southeast-2.amazonaws.com\nContent-Type: application/x-amz-json-1.1\nUser-Agent: aws-sdk-go/0.5.0\nX-Amz-Target: Kinesis_20131202.PutRecord\n{\"Data\":\"YQ==\",\"PartitionKey\":\"1\",\"StreamName\":\"playpen-metrics\"}\n---[ CANONICAL STRING  ]-----------------------------\nPOST\n/\nhost:kinesis.ap-southeast-2.amazonaws.com\nx-amz-date:20150428T035556Z\nx-amz-target:Kinesis_20131202.PutRecord\nhost;x-amz-date;x-amz-target\n\n---[ STRING TO SIGN ]--------------------------------\nAWS4-HMAC-SHA256\n20150428T035556Z\n20150428/ap-southeast-2/kinesis/aws4_request\n\n---[ SIGNED URL ]--------------------------------\nhttps://kinesis.ap-southeast-2.amazonaws.com/\n-----------------------------------------------------\n---[ REQUEST POST-SIGN ]-----------------------------\nPOST / HTTP/1.1\nHost: kinesis.ap-southeast-2.amazonaws.com\nUser-Agent: aws-sdk-go/0.5.0\nContent-Length: 65\nAuthorization: AWS4-HMAC-SHA256 Credential=/20150428/ap-southeast-2/kinesis/aws4_request, SignedHeaders=host;x-amz-date;x-amz-target, Signature=\nContent-Type: application/x-amz-json-1.1\nX-Amz-Content-Sha256: \nX-Amz-Date: 20150428T035556Z\nX-Amz-Target: Kinesis_20131202.PutRecord\nAccept-Encoding: gzip\n{\"Data\":\"YQ==\",\"PartitionKey\":\"1\",\"StreamName\":\"\"}\n---[ RESPONSE ]--------------------------------------\nPost https://kinesis.ap-southeast-2.amazonaws.com/: EOF\n-----------------------------------------------------\n2015/04/28 03:55:56 Sender failed to put record: Post https://kinesis.ap-southeast-2.amazonaws.com/: EOF, Post https://kinesis.ap-southeast-2.amazonaws.com/: EOF, &url.Error{Op:\"Post\", URL:\"https://kinesis.ap-southeast-2.amazonaws.com/\", Err:(*errors.errorString)(0xc20800a0e0)}\n```\n. I was thinking that perhaps it has something to do with the subsequent request coinciding with a keep-alive connection being closed by the server on the other side, and the timing being just right?.. \n. I've tested buffering requests and sending them no more frequently than every 10s, and that did not result in any errors after 24 hours of doing requests. I'm not using any cache, it's pretty much a basic debian wheezy box (and aws cli tool doesn't seem to have that problem, so that'd rule out a proxy problem). \nI've just retested this bit of go code on darwin/amd64 on my local to rule out EC2-related issues too - same problem.\nHere is the bash script I used from that EC2 machine for doing the same thing with aws cli, which did not show this problem - tried both with sleep 6 and sleep 7:\n``` bash\n!/bin/bash\nfor i in {1..20}; do\n    (aws kinesis put-record --stream-name playpen-metrics --data \"a\" --partition-key \"1\" --region ap-southeast-2) &\n    sleep 6\ndone\nwait\n```\nI guess we'd be looking for that magic timeout number of \"about 5 or 6\" somewhere in the aws sdk or go socket code? Do you know how to disable keep-alives on these aws sdk requests? Maybe force connection closure after every request just to see if that would fix it?\n. Sorry, that was a typo. Instead of proxy I said cache - no, I'm not using any proxy.\n. Heh... disabling keep-alive seems to be fixing it - no failures after 30 sends. Here is the test code. Removing line 21 from this gist reintroduces the issue - fails within the first few sends again.\nI might be completely misled, but here is another guess: according to this it seems one needs to close the response Body when done with it. I have added some debug output to all occurrences of \"Body.Close\" in this SDK, and none seems to have been hit. \nDo you think it could possibly be missing Body.Close?\n. :+1: Thanks for confirming the issue - happy to hear it's not my specific set-up going crazy.\n. Thanks, we'll wait until it's fixed upstream then.\n. Thanks @jasdel for exposing the Expiry - will make a use of that, move the code to aws/credentials/stscreds and give it a go to fix it up as suggested.\n. Note: this PR requires #270 and is rebased against it. Still outstanding: tests, GetSessionToken. Will continue tomorrow.\n. Thanks for your advice - updated the interface to be public AssumeRoler. \nThe tests are now added. I didn't add the expiry testing, because that feels like it should be done in the credentials module.\nAlso, I decided to rename this as AssumeRoleProvider. GetSessionToken is not the single other way of obtaining credentials - there is also AssumeRoleWithSAML and AssumeRoleWithWebIdentity, so it feels each one of these should get a provider, or something generally more flexible needs to be cooked up.\n. The test failures seem unrelated: TestUploadOrderReadFail2-2 or TestUploadOrderMultiBufferedReaderExceedTotalParts... \n. Great, thanks for help!\n. Good point, done.\n. :+1: done\n. I'll have a think how to incorporate it (or rename the provider if I run out of time).\n. That's already gone thanks to your Expiry patch.\n. @jasdel I can't really stub the STS client because it's an autogenerated struct. Do you think I could use a new private assumeRoler interface like that?\n. (and then the stubSTS here implements the assumeRoler and mocks the AssumeRole call)\n. ",
    "itsjamie": "Not knowing what the types are used for internally, it just seemed awkward to me. I took a quick look and saw that Part had the necessary exposed fields for CompleteMultipartUpload, so I wasn't sure why a different type was used. Externally, I believe it means that you need to convert the Part into a CompletedPart.\n```\n//parts = []Part\nmpUpload := &s3.CompletedMultipartUpload{\n    Parts: make([]s3.CompletedPart, len(parts)),\n}\nfor idx, part := range parts {\n    mpUpload.Parts[idx] = &s3.CompletedPart{\n        ETag:       part.ETag,\n        PartNumber: part.PartNumber,\n    }\n}\n```\nIf this is incorrect, please let me know.\n. Right, we don't manage the part uploads on the server. Essentially, we validate the user should have the ability to upload to Amazon, and initiate the request. We sign the parts as they request it, and send the necessary headers down for them to upload. We sign each part request and pass that information to the client who is uploading from the browser using XHR2 and the File API.\nSeems like the best thing I will be able to do is make a convenience function that converts Part to CompletedPart if I find myself needing to do it a lot.\nMy suggestion would be, since CompletedPart is entirely a subset of Part with a different field name for the unexposed metadata (prefixed with complete), I'd ask you to reconsider changing the API before freezing it to take Parts instead. It seems wasteful to me to force an external user to loop potentially ten thousand times essentially allocating a bunch of tiny structs'o'pointers that will need to be cleaned up later..\n. Yes, we currently use s3gof3r to do parallel uploading when we do it from the server. \nIt implements the internal book-keeping so a ListParts isn't necessary.\nAt this point, since we are doing some client-side uploading and just request signing server-side, I understand that this is an edge-case for the SDK.\nThank you for the consideration and your time.\n. ",
    "jeffw-wherethebitsroam": "Ok, brilliant. I know the SDK is in development, so the docs are a bit thin at the moment, but this would be good information to have on the http://godoc.org/github.com/awslabs/aws-sdk-go/service/s3 page for example.\n. ",
    "Colin-Murphy": "Guess who can't read code in the morning!\n. ",
    "pikeas": "@c4milo This issue is linked from www.terraform.io/docs/providers/aws/r/vpc_dhcp_options.html. Is this still a problem, or did you resolve it? If there's still a problem, could you please link to the new open issue?\n. I'm not sure what that means - is there currently an issue in Terraform, or was it resolved?\n. ",
    "michaeljs1990": "Heh. 3AM and my brain must not be functioning. I just noticed that you are passing back an io.ReadCloser in resp.Body. So i just needed to do this...\n```\n    f, err := os.Create(file)\n    if err != nil {\n        fmt.Println(err)\n    }\nw := bufio.NewWriter(f)\n_, err = w.ReadFrom(resp.Body)\nif err != nil {\n    fmt.Println(err)\n}\nw.Flush()\n\n```\nthis was with get object as you suggested.\n. It looks like what i would want is multipart download but I don't see it any place in the current API. I am guessing you could implement it fairly easily using goroutines and the Range header? I would be willing to do this although I am not sure if multipart download has an official spec it needs to meet?\n. I implemented multipart download locally if that is something that is on the roadmap for this sdk.\n. Awesome, I'll clean it up over the weekend and submit a PR.\n. ",
    "forty8bits": "I'm wondering if this is confirmed working, and if so are there are any concrete examples of correct usage? Granted, it may be an issue with ECS and not this SDK, but I'm having no luck connecting through IAM instance roles within a Go app being hosted on ECS. The appropriate IAM instance roles are all set up.\nThe code I'm using is:\ngo\nDB = dynamodb.New(&aws.Config{                                  \n    Region: \"eu-west-1\",\n    Credentials: credentials.NewEC2RoleCredentials(\n        &http.Client{\n                Timeout: 10 * time.Second,\n        },\n        \"\",\n        0,\n    ),\n})\n. @lsegal thanks for the info.\n\nOnly if you need to set an explicit Timeout should you ever need to pass in credentials explicitly-- I assume that's what you're trying to do here.\n\nAbsolutely, I was just trying to see if a longer timeout would do the trick.\n\n...note that Docker might need to map the metadata host somehow? I've never tested this under Docker, so that might be related. Perhaps it's unable to route that endpoint.\n\nFrom what I've read, it seems like using IAM roles from within Docker containers on ECS should work in exactly the same way as running on a standard EC2 instance, thanks to some forwarding within the ECS agent on each instance, although I'm not 100% on the inner workings there.\nI'll do some more troubleshooting, check the forums, see if I can get it sorted and report back if it seems to be an issue in the SDK.\n. ",
    "ohookins": "It's not super clear how #197 relates to this but if you say Code and Message should not be nil in any case, I would agree with that.\n. OK, got it. Thanks!\n. OK, that makes sense. Thanks!\n. I think you are right. Thanks!\n. ",
    "oremj": "It looks like it applies to all services. http://docs.aws.amazon.com/general/latest/gr/sigv4-create-canonical-request.html\n```\nAdd the canonical query string, followed by a newline character. If the request does not include a query string, set this value in the canonical query to an empty string (essentially, a blank line). The example query does not contain a query string.\nSample request: canonical query string\nempty string\nTo construct the canonical query string, complete the following steps:\nURI-encode each parameter name and value according to the following rules:\n\n    Do not URL-encode any of the unreserved characters that RFC 3986 defines: A-Z, a-z, 0-9, hyphen ( - ), underscore ( _ ), period ( . ), and tilde ( ~ ).\n\n    Percent-encode all other characters with %XY, where X and Y are hexadecimal characters (0-9 and uppercase A-F).\n\n    For example, the space character must be encoded as %20 (not using '+', as some encoding schemes do) and extended UTF-8 characters must be in the form %XY%ZA%BC.\n\n```\n. ",
    "justincampbell": "Understood, thanks!\n. Thanks! It's now working.\n. ",
    "chpapa": "Thanks. Though it sounds kind of weird to me that the SDK won't handle the escape. Will close this issue. Any chance if you can point me to any reference of how to implement the encode properly? Seems there are nothing compliance to the encode CF invalidation need in the standard library.\n. Thanks a lot for the clarification @lsegal \n. ",
    "stephen-mw": "Thank you both for the feedback. @nightlyone that seems like a very reasonable workaround to me. I'm going to close this since the default behavior does appear to be the best. Thanks again.\n. ",
    "anacrolix": "Thank you!\n. In code calling either HeadObject you can imagine that the intention is to determine what the result would be were the call a GET. I understand your wanting to completely abstract away the HTTP, that makes sense. So my issue is that I have to write the same code twice. Once for HeadObject, and once for GetObject. Setting all the fields identically, if I want to achieve the result described above.\nHaving said that, I've now thought of the ideal solution in Go. If you retain your existing HeadObjectInput and GetObjectInput structs, but merge the shared fields in a new structure that is embedded in both, the interface remains the same for existing users, but I'm able to refer to the shared structure. I hope this explains:\n```\ntype GetObjectInput struct {\n  ObjectInput\n}\ntype HeadObjectInput struct {\n  ObjectInput\n}\n```\nand conversely:\n``\ntype GetObjectOutput {\n  ObjectOutput\n  Body io.ReadClosertype:\"blob\"`\n}\ntype HeadObjectOutput {\n  ObjectOutput\n}\n```\nA GetObjectOutput is a superset of HeadObjectOutput.\nNow I'm able to call both HeadObject and GetObject with the same setter code.\nfunc myHandler(w http.ResponseWriter, r *http.Request) {\n  if r.Method == \"GET\" {\n    input := GetObjectInput{}\n    initObjectInput(&input.ObjectInput)\n    s3svc.GetObject(&input)\n    ...\n  } else if r.Method == \"HEAD\" {\n    input := HeadObjectInput{}\n    initObjectInput(&input.ObjectInput)\n    s3svc.HeadObject(&input)\n    ...\n  }\n}\nOf course this is particular to my use-case, but it is a very common one. It's a similar story for response handlers. Abstracting the HTTP is good, but the influence of the underlying HTTP API is still present, and it's desirable to maintain some of the assumptions it provides.\nThanks for your patience.\n. Thanks for the great work.\n. ",
    "cespare": "It's not an error channel, it's a quit signal chan, which is the usual pattern for cooperative cancellation.\nIn my fork I'm using github.com/cespare/wait which replaces wg, quit, closed, and err, and simplifies the code quite a bit.\nChecking an error on every loop (instead of using a select) is probably going to always be broken.\n. @lsegal Done, and I rebased both and updated the PR title.\n. @lsegal I implemented this a few days ago and I ended up retrying each chunk up to 5 times, with exponential backoff, on any *url.Error. This may be a hack but it has been working successfully for a large amount of data for a several days now.\n\nIf the SDK is not retrying parts, it's due to an error that is not currently marked as retryable\n\nWell, I showed two of the errors I'm getting above; both of them go away on retry, so maybe consider this a bug report then?\n. Replying to your updated comment:\n\nSeeing EOF / timeout errors usually implies that you are getting throttled by S3. If you want to improve resiliency, the best idea is to simply slow down.\n\nIt would be helpful if the error exposed to the programmer would indicate throttling, then. (I don't know whether this is an API or SDK limitation). It seems like my workaround is the best solution if throttling can manifest as TCP connections getting dropped.\nOr to put it another way: wouldn't you expect that one could upload a 50GB file to S3 using s3manager, using the default settings? (I was unable to do so in multiple tests.)\n. OK cool, this is the info I needed, thanks @lsegal.\n. See previous discussion on #226.\n. Yes, I discovered this deadlock when it occurred for me. I killed it and examined the stack trace to find it.\nI don't have time right now to put together a reproducer but there wasn't anything special about my code. I just opened a giant file (60GB if I recall correctly) and passed it to s3manager.Upload().\nHow it happens is pretty straightforward. Here's an example scenario:\n- The dispatch goroutine checks for an error on line 318 and finds it is nil\n- The network blips and all the readChunk workers get an error and break out of their loop on line 362\n- Now when the dispatch goroutine tries to put the next chunk onto ch on line 323, nothing is pulling from the other end of the channel, so there is a deadlock\nThings you can do to increase the likelihood of the deadlock happening if you want to reproduce it:\n- Set GOMAXPROCS to something > 1\n- Add in a sleep on line 320 to increase the race window size\n- Kill s3manager's TCP connections randomly (or otherwise cause the upload to error)\n. I have a related question, too. (Maybe this should be a separate issue.) It seems that S3 won't accept multipart uploads of more than 10k parts, as I discovered when uploading some large files. So in my own uploader, I checked the total number of parts and if it's >10k, I bump the part size up such that there will be exactly 10k parts.\nIn s3manager, for arbitrary io.Readers it won't know the size, so I think uploads will fail if they're too large (larger than 50GB for the default PartSize of 5MB). This raises two questions:\n- After generating 10k parts, should s3manager immediately abort and report the failure to the user rather than trying to complete an invalid upload?\n- In the new proposed code path for Seekers, should we do the PartSize adjustment I did in my code? (A different option would be immediately reporting some \"part size too small\" error after discovering the size of the file.)\n. > Failing the upload at 10k parts would be unhelpful, as the user would have no way to complete the upload and they would get stuck with having to restart from scratch. If we can fail early, it would save quite a bit of time.\nWell, failing and aborting at that point would be more helpful than allowing the user to upload another 50k chunks :) And this still might be a case we want to handle if the user doesn't provide an estimated size (or if the estimated size was very wrong).\n. > In order for an io.Seeker to work, you need the data in a seekable storage, i.e. the data needs to be in RAM.\nSeeking files and reading sections of them should generally only pull in the relevant pages to the buffer cache. It's definitely not the case that only data in RAM is seekable.. > Firstly; how can you in any way or respect be confident that an io.Reader is in fact a file? In our case we get our data from a socket.\nWe can't. That's why my uploader code takes a file-like interface instead of a plain io.Reader.\n\nSecondly; even if the io.Reader is a file, that's still not page based until the file has been either memory mapped or read into RAM. Either way you end up in RAM again.\n\nHmm, I'm not quite sure what you're getting at here. The pages of the file may be on disk only or they may be on disk and in the page cache (memory). It's true that if you seek to the middle of the file and then read some data (to upload it) then those pages are now in page cache, but the important things are (1) the kernel may drop those pages from the cache later after we're done reading that chunk of the file, and (2) the data for a chunk doesn't need to all be copied into the Go process's heap at once. That's why my uploading code has much lower memory usage than s3manager.Uploader.. ",
    "cmdrkeene": "For that matter, legacy fields like KeyConditions are marked as required. Does this file simply need to be regenerated from a more current schema?\n. Thanks @jasdel !\n. ",
    "suzuken": "@jasdel I got it! Thanks!\n. ",
    "ghost": "Awesome!\nOn Wed, May 20, 2015 at 10:07 AM, Jason Del Ponte notifications@github.com\nwrote:\n\nHi @squirkle https://github.com/squirkle thanks for the feedback. I\ndon't think awserr.Error type shouldn't have any issue assigning to an\nerror type value since awserr.Error is an interface which can satisfy the\ngeneric error interface. But I'd love to see an example if that is the\ncase. Though, It would introduce an issues if other function calls returned\nan error in the same scope and the value was set to the same err an SDK\nawserr.Error was set to. That would cause a conflict because err is of type\naws.Error interface, not generic error.\nto use @b6g https://github.com/b6g's example\noutput, err := s3manage.Upload(svc, input, opts)\n_, err = io.Copy(dst, src) // * we cannot do this *\nBecause of this, I'll make an update which switches the return types back\nto error, but keep the underlying interface awserr.Error which can be cast\nto in order to retrieve extra information.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/awslabs/aws-sdk-go/issues/238#issuecomment-103963127.\n. Does it make sense to modify Request.Send to copy the r.HTTPRequest.Cancel? Suppose I have a context ctx, before I call r.Send, I can set r.HTTPRequest.Cancel = ctx.Done(). When ctx is done, the HTTP request will be cancelled.\n\nYes, it is still a bit awkward to set r.HTTPRequest.Cancel on every request, but at least we have a quick and simple solution to time out the request. Thanks,\n. https://github.com/aws/aws-sdk-go/pull/638\n. According to https://github.com/aws/aws-sdk-go/wiki/setting-up, \n\nThe AWS SDK for Go requires Go 1.5 or later\n. But I can separate it if you strongly prefer.\n. make sense. let me do it. thanks,\n. done, please take a look. thanks!\n. done. compiled and tested in 1.4, 1.5 and 1.6.\n. Agree that the SDK shouldn't retry when the request is canceled.\n. great! thanks!\n. \n",
    "LuqmanSahaf": "@lsegal, here is the simplified version of what I am doing. It works partially, i.e., the instance is created but the volumes are not attached when the instance is started. Hope it helps.\n``` go\nfunc main(){\n  creds := aws.Creds(\"abcd\", \"xyzef0-0\", \"\")\n  svc := ec2.New(&aws.Config{Region: \"us-east-1\", Credentials: creds})\n// volumes\n  volume_count := 2\n  var blockMappings []*ec2.BlockDeviceMapping\ndevice_suffix := 'a'\n  for i:= 0 ; i< volume_count ; i++ {\n    device_suffix += 1\n    device_name := \"/dev/sd\" + string(device_suffix)\n    virtual_name := \"ephemeral\" + strconv.Itoa(i)\n    blockMappings = append(blockMappings,&ec2.BlockDeviceMapping{\n      DeviceName: &device_name,\n      VirtualName: &virtual_name\n    })\n  }\nfmt.Println(\"The block device mappings are:\") \n  for i,b := range blockMappings {\n    fmt.Println(strconv.Itoa(i), \"-\", b.DeviceName, \":\", b.VirtualName)\n  }\n  var no_of_nodes int64 = int64(1)\n  request := &ec2.RunInstancesInput{\n    MinCount: &no_of_nodes,\n    MaxCount: &no_of_nodes,\n    BlockDeviceMappings: blockMappings\n  }\n  ins := \"c3.xlarge\"\n  request.InstanceType = &ins\n  img := \"ami-d6033bbe\"\n  request.ImageID = &img\n  ssh := \"some_key\"\n  request.KeyName = &ssh\nreservation, err := svc.RunInstances(request)\nif err !=nil {\n    panic(err)\n  }\n  fmt.Println(\"  > Number of instances: \", len(reservation.Instances))\n  for _, inst := range reservation.Instances {\n    fmt.Println(\"    - Instance ID: \", inst.InstanceID)\n    fmt.Println(\"    - Status: \", inst.State.Name, \" \", strconv.FormatInt(*inst.State.Code, 10))\n    fmt.Println(\"    - Block Device Mapping: \", inst.BlockDeviceMappings)\n  }\n}\n```\nHere is the result for the program and request:\n``` stdout\nThe block device mappings are:\n0 - /dev/sdb : ephemeral0\n1 - /dev/sdc : ephemeral1\n\nNumber of instances:  1\n  - Instance ID:  i-9cf1f44a\n  - Status:  pending   0\n  - Block Device Mapping:  []\n```\n\nAs you can see the array for device mappings is empty.\n. I followed this document to give device mapping through aws cli. I ran the following command to run an instance:\nsh\naws ec2 run-instances --image-id \"ami-d6033bbe\" --key-name depl --block-device-mappings '[{\"DeviceName\": \"/dev/sdf\",\"VirtualName\":\"ephemeral0\"},{\"DeviceName\": \"/dev/sdg\",\"VirtualName\":\"ephemeral1\"}]' --count 1 --instance-type c3.xlarge\nSame thing happens that the instance is created but the volumes are not attached or created.\n. Yes, I waited and did the necessary checks to find if the volumes were attached or not.\n. I have also asked this question on AWS forum: https://forums.aws.amazon.com/thread.jspa?threadID=181493\n. ",
    "oblitum": "Thanks for this, already working in production :-)\n. +1\n(in need for this or similar to land)\n. ",
    "rlcomte": "I will make the changes for the logging asap.\nI did check the table hash/range key, and the table I try to access only uses a hash.\nI also tried the ProjectionExpression stuff, to see if that matters, but that threw some other errors.\n. Hi @jasdel, I ran the service with the additional logging. Below you see the full trace. Some auth stuff is removed.\n---[ CANONICAL STRING  ]-----------------------------\nPOST\n/\naccept-encoding:identity\nhost:dynamodb.eu-west-1.amazonaws.com\nx-amz-date:20150526T062318Z\nx-amz-target:DynamoDB_20120810.GetItem\naccept-encoding;host;x-amz-date;x-amz-target\n6e68c5e4245cc293c96ac7a8cd97e5dd64ba8873f5a19a118c5fed5102bbf9d6\n---[ REQUEST POST-SIGN ]-----------------------------\nPOST / HTTP/1.1\nHost: dynamodb.eu-west-1.amazonaws.com\nUser-Agent: aws-sdk-go/0.5.0\nContent-Length: 52\nAccept-Encoding: identity\nContent-Type: application/x-amz-json-1.0\nX-Amz-Date: 20150526T062318Z\nX-Amz-Target: DynamoDB_20120810.GetItem\n\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 400 Bad Request\nContent-Length: 0\nContent-Type: application/x-amz-json-1.0\nDate: Tue, 26 May 2015 06:23:18 GMT\nX-Amz-Crc32: 3485231410\nX-Amzn-Requestid: KIAQ8PO2E7NI9MC68FDADN6SOJVV4KQNSO5AEMVJF66Q9ASUAAJG\n\nValidationException The provided key element does not match the schema \nValidationException The provided key element does not match the schema 400 \n. Hmm, I checked again and it was a typo at my side ;(.\nThanks for the help.\nRine\n. ",
    "pungoyal": "@jasdel How is the AWS console able to do a query just on the HASH key then?. @jasdel so you are saying query with the hash key is not a public method, but DynamoDB is 'cheating' to enable that on the AWS console? \ud83e\udd15 . ",
    "ddollar": "It will take me some time to reproduce on master. I'm currently on dcbb30018ec1d54e6127cf3f2cf900b35235b1b1\n. ",
    "purohit": "It's happened to me a couple times since but infrequently. The client is\ninitialized, but I've flooded EC2 with requests and I get these nil\npointers when I use the client in parallel. I haven't created a\nreproducible test case.\nOn May 28, 2015 5:51 PM, \"Jason Del Ponte\" notifications@github.com wrote:\n\nHi @purohit https://github.com/purohit thanks for reporting this. From\nthe stack trace it looks like the Service pointer is nil. Were you able to\nreproduce this with any other service or operations?\nHow is your code initializing the EC2 service struct? New() is the best\nmethod to create and initialize a service before it is used. Calling a\nmethod on a nil *struct actually won't panic until something on that struct\nis accessed. Example: http://play.golang.org/p/8BnCwUiECt\nIs it possible when an error is received the code uses an incorrect or\nuninitialized service instance?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/awslabs/aws-sdk-go/issues/251#issuecomment-106631008.\n. I figured out why it was happening. Sometimes when I do:\n\nclient := ec2.New(&aws.Config{Region: region})\nI get a nil pointer, and I should check for that. There is no error returned (\u00e0 la client, err :=) so I must create one myself.\n. Yes, I think that is likely. Is the recommended usage to instantiate a new ec2 client for each goroutine?\n. OK, thanks for the tips! I don't see anywhere in the code it would be possible that I'm using the client prior to it being created, but if I discover I was doing something stupid, I'll post it.\n. I understand. Thanks for the fast response.\n. Hi @jasdel,\nRegion: us-west-2.\nThanks for the fast response! It turns out the problem only happens on our Travis-CI builds, not local ones -- which was strange. That indicated that while previously err == nil on a 200, perhaps the behavior was updated in some commit in aws-sdk-go that changed it to report an error. Travis-CI pulls the latest aws-sdk-go, while locally we run an older version.\nIt initially was hard to get the logs because Travis-CI stops logging after 4MB of data, and S3 requires a minimum of 5MB for a multipart upload! But I just modified the client on the fly right before the CompleteMultipartUpload request, and here are the results\nWith Travis (err != nil):\n2016/01/14 02:26:20 DEBUG: Response s3/CompleteMultipartUpload Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 200 OK\nTransfer-Encoding: chunked\nContent-Type: application/xml\nDate: Thu, 14 Jan 2016 02:26:21 GMT\nServer: AmazonS3\nX-Amz-Id-2: hfyBiKFUbdt+qZlyPI+R0CzPR9CuzZQUgXMx6GrEFPf8XgVyh9PoGJSIhEdZCvFCGVw0kqLcDiI=\nX-Amz-Request-Id: 440D3953A7724A06\n185\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<CompleteMultipartUploadResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"><Location>https://our-test-files.s3-us-west-2.amazonaws.com/s3-test-1452738376130177164</Location><Bucket>our-test-files</Bucket><Key>s3-test-1452738376130177164</Key><ETag>&quot;db3d92caa11d0fe265f5752d43f9fcf0-5&quot;</ETag></CompleteMultipartUploadResult>\nLocally (err == nil):\n```\n2016/01/13 19:02:03 DEBUG: Response s3/CompleteMultipartUpload Details:                                                                                                                         \n---[ RESPONSE ]--------------------------------------                                                                                                                                           \nHTTP/1.1 200 OK^M                                                                                                                                                                               \nTransfer-Encoding: chunked^M                                                                                                                                                                    \nContent-Type: application/xml^M                                                                                                                                                                 \nDate: Thu, 14 Jan 2016 02:02:27 GMT^M                                                                                                                                                           \nServer: AmazonS3^M                                                                                                                                                                              \nX-Amz-Id-2: h6FtAB55A4gFLWPb3b9AK0FDxF9jd2Qg/hVdTmAqcoim/0trOVXO1j11G6fI9I3cOZqNPaPwxNg=^M                                                                                                      \nX-Amz-Request-Id: 801850193878E42C^M                                                                                                                                                            \n^M                                                                                                                                                                                              \n185^M                                                                                                                                                                                           \n<?xml version=\"1.0\" encoding=\"UTF-8\"?>                                                                                                                                                            \nhttps://our-test-files.s3-us-west-2.amazonaws.com/s3-test-1452736919985166867our-test-filess3-test-1452736919985166867\"39f6fbf954e0596a853b128b2cc35ba3-5\"\n```\nThe file is there from either version, so it seems clear that nothing is actually wrong -- the multipart upload works. I will investigate tomorrow with bisect to see if I can find which commit in aws-sdk-go is causing me problems, if that is the cause. \nThanks for the tip on the WithLogLevel(aws.LogDebugWithHTTPBody), that helped a lot.\n. I can confirm this also fixed the regression on my end. Thanks for taking care of it.\n. HI @jasdel,\nAlthough the library doesn't give me errors anymore, CloudFront does when I try to have multiple policy statements. The policy is signed correctly, but when accessing CloudFront:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?><Error><Code>MalformedPolicy</Code><Message>Malformed Policy</Message></Error>\nIt's hard to affirm that no, you cannot have multiple statements per policy, because the error message is vague. But, anecdotally it only occurs if I add more than one policy statement.\nIt seems like there is an \"unofficial\" policy that requires one and only one statement per policy, as the library previously enforced.\nHowever, as you know, there is no official documentation stating one way or the other. Is there any way you can get someone from Amazon to update the documentation to make it clear? That would prevent headache in the future and provide grounds for the code in the library, whichever way.\n. @jasdel I agree about the code policy in the SDK -- leave it open for future changes. Thanks for forwarding this request to your colleagues.\nI've posted on the forum as you suggested: https://forums.aws.amazon.com/thread.jspa?messageID=696691, and hopefully we can get an answer soon.\n. @kenju Unfortunately, we did not -- we found some workaround but since it's been over 18 months I can't recall what it is :/. @kenju  I just recalled what we did -- my use case was to protect a certain subset of images in an iPython notebook using a CloudFront policy. However, since it wasn't possible to declare multiple resources (and they weren't all in the same subfolder -- think 10,000 images with basically random filenames, so not capturable by regex), I wrote code that would take the iPython notebook, and replace every image URL with a new cloud signed version (aka sign every single image URL separately), using the Golang SDK. It surprisingly didn't take that long to run.. ",
    "sanathp": "Sorry for that , the issue is resolved there was some bug in my code.\n. ",
    "ThisGuyCodes": "@lsegal \n\n~stevenh the big problem is that although these structures may have a single field now, it's very possible they won't in the future. It's also very possible that even required parameters become conditionally optional with the introduction of new fields, so you can't avoid the possibility of a nil on any field. In any case where we add a singular argument method replacement, it would have to be a separate method name so as to not break compatibility in an update, and even that isn't even a great experience, since those methods would effectively become deprecated (on a case-by-case basis) when this happened.\n\nIn the compatibility situation you describe, when you introduce new fields, aren't you breaking compatibility anyway? The function signature may not need to change, but if additional fields are required, or response types change, that's still a breaking change. I still need to change my code to add this field to the struct. I disagree with the return types as well, but have a less concrete argument against them.\nBut this whole \"declare a request object before making every request\" thing this library makes us do feels... dirty, and unlike what I'm used to expecting when using Go. Actually, it feels like java. As does the extreme verbosity of types/field names:\nGo\nstatuses, err := route53Svc.GetHealthCheckStatus(req)\n// handle errors\nfor status := range statuses.HealthCheckObservations {\n    // code\n}\nI just called a method named GetHealthCheckStatus, what else could I possibly expect as output than a list of statuses? If there's an insistence on this single-return-type model, maybe:\nGo\nstatuses, err := route53Svc.GetHealthCheckStatus(req)\n// handle errors\nfor status := range statuses.Observations {\n    // code\n}\nIs more prudent? https://github.com/golang/go/wiki/CodeReviewComments#variable-names\n. @lsegal actually the two examples I gave show it is the case. I just omitted the declaration of the req variable. In fact all/most (haven't actually read every page) of the documentation uses this methodology in examples: http://godoc.org/github.com/aws/aws-sdk-go/service/route53#pkg-examples (though you're using the variable name params instead of req).\nI would continue to argue against the verbosity, and I'll say that when I use a method with HealthCheck in it it's reasonable to expect the developer to assume non-explicit values are related to HealthChecks, or a method with the name AutoScaling is assumed to have output related to AutoScaling unless expressly stated otherwise. But that's a less concrete argument, more emotional and style driven, so I'll drop it.\n. It's not literally required, enforcement of that level isn't possible. And yes I could do it all in one line, but does the line of code you just gave me really not look absurdly verbose and cumbersome to you?\nGo\nresp, err := route53Svc.GetHealthCheckStatus(&route53.GetHealthCheckStatusInput{...})\nThis issue, and others (#284, #114, #124), I feel are all comments on the style of this library. I tend to come off fairly aggressive, so upfront I want to apologize if that is/becomes the case, but this library does not seem to fit with what my experience with the Go ecosystem is so far. I see the arguments in favor of most of the complaints, and these arguments make sense, but I would personally rather deal with upgrading the version of my sdk to get new optional/required parameters rather than deal with the syntactic salt my code receives from trying to use the officially supported library (that is a specific example, but I think you can see what I'm getting at). It feels... clunky, and incredibly verbose, and when I'm working on/reading my code that uses it the input and output of things feels disjointed, and I don't feel productive.\nArguments in favor of infinite flexibility (which I feel is exactly what's going on here) I find are difficult to argue against. Not because I agree with them, but because I've not yet found a way to succinctly communicate the costs associated with designing towards that. But there are costs. I want to use the officially supported library, but these design decisions have made every time I sit down with this library frustrating.\n. I second this comment. everything is a pointer, why? In fact the style guide (technically \"common code review comments\") pushes you towards using values: https://github.com/golang/go/wiki/CodeReviewComments#pass-values\n. I would also like to add that using pointers everywhere forces people to use the helper methods you declared in the aws package (aws.Long(10)) which just feels silly. (also, Go calls them int64, not long)\n. @lsegal if you're passing a pointer to the struct around (which you are) doesn't that solve the behavioral effects issue you describe?\nAnd I did read through that, and would like to reraise a some examples that were given:\nguregu specifically raised 5 instances where the zero-value vs omitted requirement (which I do understand) doesn't make sense. I do actually agree pointers are a clean way to allow distinguishing between omitted and zero values (most relevantly with singular struct values), but there are many cases where it is not required (or realistically ever required, like a list of ID's I'm requesting information on, or the list of information I get in response).\nI argue that using pointers vs values selectively where it's needed (rather than everywhere) would actually add to code clarity. If it's a value I know I actually have to put something there, if it's a pointer, I know the API will accept (possibly only under qualifying circumstances) the value being omitted.\n. @jasdel could you give an example of a service that could return sparse arrays; specifically where the existence of said null members is important information for the user? (meaning: transparently removing them as part of the encoding/decoding process would break correct applications)\nRelated to this, there's an explicit change that may be coming up in 1.5 (if they don't decide to revert) that is related to this idea of null members in encoding/decoding, discussed here: https://groups.google.com/forum/#!topic/golang-dev/3w2QlLDBEjc\nThough I don't know if this will actually affect this library (this is talking about json, and I don't know if a similar change was made to xml, and off the top of my head I think you guys are using xml?), it's worth looking at.\n. @lsegal Seems odd to me that you'd have a null values as header fields, but it does make sense that the TrustedAdvisorResourceDetail type that said headers are for might need to be able to do null values.... except in this case this api call is for a service is specifically mirroring an Excel download (and I may be reading it wrong but it looks like that field is documented as a list of strings and required), so I highly doubt empty string vs null will come into play at this endpoint in the future.\nI feel like a broken record, but I didn't suggest substituting with empty strings, I suggested omitting them. However tabular data is an instance where omitting these nulls would be incorrect, when you don't manage these header-data associations for the user (if this isn't a concept you're familiar with, I'd suggest checking out the DictReader in python's csv parser, it allows you to iterate over a csv and deal with fields by their header name rather than their position, sorta like how you can deal with values when ranging over a slice instead of doing pointer arithmetic on the location of the first array member) and instead simply mirror the raw data output exactly; but it does also seem like an instance where empty strings would actually be more correct to return (that's more subjective).\nMore importantly, did you view the discussion on CL 9376 / have you tested this library against this sort of data output on Go 1.5 yet? The decision for that CL seems to directly go against this philosophy of maintaining nulls, and weather you agree with my interpretation or not it may break your implementation.\n. > We don't use encoding/json. I believe jasdel has already tested against Go 1.5, but he can confirm that.\nGood to hear, though the idea of the standard library treating nulls as non-data for encoding/decoding purposes may be something to keep an eye on.\n\nThe issue here isn't about the specific API...\n\nNo, but I did request a specific example, to which you provided one that had a correct alternative. Though I wasn't aware the API served any data in a tabular fashion like this (seems... archaic, to refer to fields by number rather than name, hence my reference to pointer arithmetic), but I accept this library's responsibility is to mirror the API rather than provide a productive abstraction. The fact that endpoints that serve tabular data like this exist is valid enough.\n. This is incredibly disappointing. As I've said before, the arguments in favor of this design make sense, but basically seem centered around \"We don't want to have to change the SDK if the API changes\". Personally Required fields value and optional fields pointer would have been my prefered approach, but that's not the point I want to make, so instead I'll make said point:\nI interpret this post as this: We have heard your complaints about the design of this API, but all these suggestions require us to actually maintain the SDK when the API it's used for changes, so we don't care, deal with it.\nThat may sound childish, but I see this response as childish. You've heard fairly substantial arguments and complaints that this doesn't fit into the ecosystem, and have responded stubbornly with no compromise.\n. @lsegal yes, I'll be more constructive, that was largely an emotional response. There is an idea I've been toying with:\nhttps://gist.github.com/conslo/497f985dd74fafaffdaa\nUsing a request constructing paradigm you could accomplish a very simple interface for the vast majority of requests: svc.Request(param1, param2).Send() and if someone needs newer features (like another optional field being null) they can do something like this: svc.Request(\"\", param2).SetFirst(nil).Send()\n. I don't think a builder pattern needs to be strictly based on required vs optional, which would, as discussed, cause regular breaking changes. Using the sort of pattern I suggested you could have:\nreq := ec2Svc.RunInstances(count int, imageID string, instanceType string)\n(there could be different parameters decided on, and yes I'm mixing usage and declaration syntax, work with me for a second. In addition I'm also posing that in most instances for someone both max and min count are the same)\nreq would be a type with helper methods for modifying each of its fields, and returning itself, which would allow you to do this:\nreq := ec2Svc.RunInstances(count int, imageID string, instanceType string).UserData(userData *string)\nand if you somehow needed to set the initial parameters to the null values (why we use pointers):\nreq := ec2Svc.RunInstances(count int, \"\" string, instanceType string).ImageID(nil *string)\nthen (and this is my favorite part) the request type would have a method to send the request inline:\nresp, err := ec2Svc.RunInstances(count int, imageID string, instanceType string).Send()\nIf the constructor method is too verbose or gets in the way you can declare the struct type it creates literally:\nreq := &ec2Svc.RunInstancesRequest{\n    // some values and stuff here, pointers, not literals\n}\nresp, err := req.Send()\nwhich looks very similar to the current syntax.\nThe one thing I want to stress here, is that the arguments to the constructor method are decided on by usage not requirements. This is asking \"how are most people using this when starting out\" not \"what are people allowed to do with this.\" And as such, the arguments to the constructor would not need to change when the api changes a field requirement, and this would be ok because both the helper methods and struct declaration allow (and would always have allowed) users to set these fields to null values.\nIf this approach is used, I'd be happy with both 'common sense' approach to decisions for what arguments to present in the constructor, and a data centric approach (something I'm sure AWS could dig up from their logs ;)) so long as it's aggregated on a per-customer basis, not a per-call basis, as the problem a constructor would be trying to solve is for people starting out and/or most users, not angled towards power-users (who I imagine are making the most raw calls, but I could be wrong).\n. @jasdel I don't see how your post is a response to my suggestion at all. At no point did I suggest static methods for every possible combination, nor did I suggest additional base methods be provided for new functionality (in fact, I expressly said you shouldn't). Are you not familiar with implementing a dot-chaining syntax for constructors? Taking your example:\nGo\nfunc (*foo) Bar(id JobID, name string) *BarRequest {}\nmethod Bar returns a request type. Which has some neat methods:\n``` Go\nfunc (BarRequest) Send() (BarOutput, error) {}\nfunc (BarRequest) SetIdentity(ident Identity) *BarRequest {}\nfunc (BarRequest) SetName(name string) *BarRequest {}\nfunc (BarRequest) SetJobID(id JobID) *BarRequest {}\n```\nSo then to send a request I can do:\nGo\nresp, err := foo.Bar(id, name).Send()\nThen, new feature comes along, I want to use an identity struct concept that's available to me now, I have two types of options:\nGo\nresp, err := foo.Bar(id, \"\").SetIdentity(&ident).Send()\n// Assuming the presence of an identity overrides an empty name, which seems sane to me, or:\nresp, err := foo.Bar(id, \"\").SetIdentity(&ident).SetName(nil).Send()\nor\nGo\nreq := &BarRequest{\n    Id: &id,\n    Identity: &ident,\n}\nresp, err := req.Send()\nAnother new feature comes around, I can now bake the job id concept into the identity! (not sure why we're moving concern around so frequently and loosely, but ok):\nGo\nreq := &BarRequest{\n    Identity: &ident,\n}\nresp, err := req.Send()\nor\nGo\nresp, err := foo.Bar(0, \"\").SetIdentity(&ident).Send()\n// Again, assuming sane interpretation, or:\nresp, err := foo.Bar(0, \"\").SetIdentity(&ident).SetName(nil).SetJobID(nil).Send()\nThe default methods are just that, defaults, sane defaults, like disabling SSLv2 on a webserver program by default instead of requiring anyone who uses it to be versed in every aspect of the technology before they can do anything. They're meant to assist with the simple case where one doesn't need advanced functionality or fine grained control, or even necessarily wants to know about it; but not get in the way when, as the developer learns more functionality over time, they want to do more in-depth things, at which point they would move away from the helper syntax and begin constructing requests on their own, because you've given them that option.\nIt's very idealistic to expose 100% of the raw functionality of something to people off the bat. Fact of the matter is, most people just don't care about most of what you're throwing at them.\nGive us sane defaults. Give people the ability to get something simple now, and the tools to become power users later.\n. @lsegal How is that a breaking change? Did you read the comment in the code block you copied? Or the second line of code under it where I set said field to nil? Or the second syntax I provided under it? I've seen the statement about \"empty vs nil\" a dozen times already and agree with it, and I accounted for its necessity in both forms of syntax I provided (as you may notice, the actual struct fields are pointers, as are the arguments to the modifying methods). Here's the full syntax of one of the methods to be as clear as possible:\nGo\nfunc (br *BarRequest) SetName(name *string) *BarRequest {\n    br.Name = name\n    return br\n}\nAgain, I iterate the name field is a pointer. See where I did .SetName(nil)?\nAnd I'm not talking about default values, but default functionality. Which is why I said default methods, not default values. That entire paragraph was trying to say that the current syntax is useful for power users, but frustrating for those looking to get something simple done. But most users are by definition not power users, and do not need access to every possible combination of parameters (as @jasdel seemed to think I was asking for). I am not suggesting that access to that flexibility be removed, but it doesn't need to be made easier than it is currently because people who need that are already power users, and the syntax/mental overhead/whatever isn't going to be noticed much by them.\nI'm asking for a simple syntax for the more common ways things are done (which again, could likely be backed up by data). Yes, the API may now, or in the future, support a (*EC2) DeleteVolume command without a VolumeID being sent, and the SDK should be capable of doing that should I need it, but considering all these methods do is form a request and send it:\nGo\nfunc (c *EC2) DeleteVolume(input *DeleteVolumeInput) (*DeleteVolumeOutput, error) {\n    req, out := c.DeleteVolumeRequest(input)\n    err := req.Send()\n    return out, err\n}\nSome refactoring of the request type (returning things instead of writing to pointers by moving the return value allocation out of the request formation and into the .Send() method), could turn it into something like this:\nGo\nfunc (c *EC2) DeleteVolume(input *DeleteVolumeInput) (*DeleteVolumeOutput, error) {\n    req := c.DeleteVolumeRequest(input)\n    return req.Send()\n}\nwhich at this point is just:\nGo\nfunc (c *EC2) DeleteVolume(input *DeleteVolumeInput) (*DeleteVolumeOutput, error) {\n    return c.DeleteVolumeRequest(input).Send()\n}\nWhich seems... a bit silly to put into it's own method?\n(as a slight aside, I just had a journey through the request abstraction here in an attempt to more specifically suggest how to change to the syntax I just did above, and I must say that at a high level, it's quite beautiful. Further down I started having a bit of difficulty following the execution path, but the way you set the Data interface{} to a pointer to a static type to get type safety is pretty cool, if a bit magical)\n(but I did figure out how to do it):\n``` Go\ntype DeleteVolumeRequest struct {\n    c     EC2\n    input aws.request\n}\nfunc (r DeleteVolumeRequest) Send() (DeleteVolumeOutput, error) {\n    op := &aws.Operation{\n        Name:       opDeleteVolume,\n        HTTPMethod: \"POST\",\n        HTTPPath:   \"/\",\n    } // This could be statically declared outside the method\n    // I saw the way it previously was lazy loaded, which required a mutex, but\n    // really you can just declare it statically outside the function scope without a mutex.\nif r.input == nil {\n    r.input = &DeleteVolumeInput{}\n}\n\nreq = r.c.newRequest(opDeleteVolume, r.input, output)\noutput = &DeleteVolumeOutput{}\nreq.Data = output // is there a reason this isn't:\n// output = &DeleteVolumeOutput{}\n// req = r.c.newRequest(op, r.input, output)\n// without the need to do \"req.Data = output\" after?\n\nerr := req.Send()\n\nreturn output, err\n\n}\nfunc (c EC2) DeleteVolumeRequest(input DeleteVolumeInput) *DeleteVolumeRequest {\n    return &DeleteVolumeRequest{\n        input: input,\n        c:     c,\n    }\n}\n```\nUsing a syntax like this would pave the way to (possibly) replace the current (*EC2) DeleteVolume with something like:\nGo\nfunc (c *EC2) DeleteVolume(volume string) (*DeleteVolumeOutput, error) {\n    return (&DeleteVolumeInput{\n        VolumeID: volume,\n    }).Send()\n}\nI'm not actually suggesting DryRun is a value worth omitting, but the example I'm working with only has two fields. It'd be a qualitative (or again, data driven) analysis of each request to decide on a signature. A more concrete suggestion:\nGo\nfunc (*EC2) DescribeAddresses(DryRun bool, filters ...Filter) (*DescribeAddressesOutput, error)\nYes, being able to only search certain allocations or IPs is useful, and important, but are likely not nearly as common a use. (the whole slice of pointers thing is still confusing though, I'm also interested in a response to @drombosky's post in this regard).\nDoes this make sense yet? I acknowledge that you may not agree with my idea, but I've not yet gotten a response that communicates a disagreement with the design, and instead they all seem to misconstrue what I'm trying to say.\n. ",
    "stevenh": "@conslo makes a very good point here, in that dealing with upgrades when the parameters change would indeed be much more preferable than having to deal with all the unnessaray syntactic sugar thats currently required.\nFrom a support perspective using tags and branches for each version that introduces incompatible changes would work just fine.\nAs @foresmac says the un-Go-like nature of the library is a huge turn off, and currently we've chosen to not use this library due to that even though helping maintain competing libraries is quite a large overhead, its still preferable, which gives you an idea the overhead that this library introduces.\n. Thanks @jasdel the debug example was from LogLevel set to aws.LogDebugWithRequestRetries | aws.LogDebugWithRequestErrors.\nThe output as you see says \"not retrying\" I'll try adding in the retrier as per example and see if that changes things.. Its always the DescribeInstances method that fails, however we're not using many method in this particular code base ATM, as its just a little auditing daemon.. Hmm tried to do the WithRetryer(..) but there doesn't seem to be any such option.. Ok restructured with all your suggested changes, figured out where that method was.\n```go\npackage main\nimport (\n        \"context\"\n        \"log\"\n        \"time\"\n    \"github.com/aws/aws-sdk-go/aws\"\n    \"github.com/aws/aws-sdk-go/aws/client\"\n    \"github.com/aws/aws-sdk-go/aws/credentials\"\n    \"github.com/aws/aws-sdk-go/aws/request\"\n    \"github.com/aws/aws-sdk-go/aws/session\"\n    \"github.com/aws/aws-sdk-go/service/ec2\"\n\n)\nconst (\n        accessKey       = \"keyID\"\n        secretKey       = \"secretKey\"\n        region          = \"ap-southeast-2\"\n        requestAttempts = 10\n)\nfunc main() {\n        creds := credentials.NewStaticCredentials(accessKey, secretKey, \"\")\n        session, err := session.NewSessionWithOptions(\n                session.Options{\n                        Config:            aws.Config{Credentials: creds},\n                        SharedConfigState: session.SharedConfigDisable,\n                },\n        )\n        if err != nil {\n                log.Println(err)\n                return\n        }\n    for i := 1; ; i++ {\n            ctx := context.Background()\n            cfg := request.WithRetryer(aws.NewConfig().\n                    WithRegion(region).\n                    WithLogLevel(aws.LogDebugWithRequestRetries|aws.LogDebugWithRequestErrors),\n                    client.DefaultRetryer{NumMaxRetries: requestAttempts},\n            )\n            svc := ec2.New(session, cfg)\n            j := 1\n            resp, err := svc.DescribeInstancesWithContext(ctx, nil)\n            if err != nil {\n                    log.Printf(\"describe instances attempt %v, %v for %v failed: %v\", i, j, region, err)\n                    return\n            }\n\n            instances := make([]*ec2.Instance, 0, len(resp.Reservations))\n            for {\n                    for _, r := range resp.Reservations {\n                            instances = append(instances, r.Instances...)\n                    }\n\n                    if resp.NextToken == nil {\n                            break\n                    }\n\n                    j++\n                    resp, err = svc.DescribeInstancesWithContext(ctx, &ec2.DescribeInstancesInput{NextToken: resp.NextToken})\n                    if err != nil {\n                            log.Printf(\"describe instances attempt %v, %v for %v failed: %v\", i, j, region, err)\n                            return\n                    }\n            }\n            log.Printf(\"attempt %v succeeded\\n\", i)\n            time.Sleep(time.Second)\n    }\n\n}\n```\nWe're now seeing retries happening, which is good.\nlog\n2017/06/09 23:15:56 DEBUG: Send Request ec2/DescribeInstances failed, will retry, error RequestError: send request failed\ncaused by: Post https://ec2.ap-southeast-1.amazonaws.com/: EOF\n2017/06/09 23:15:56 DEBUG: Retrying Request ec2/DescribeInstances, attempt 1\n2017/06/09 23:15:56 DEBUG: Request ec2/DescribeInstances Details:. We've been monitoring the retries for the last few hours and only those two regions are seeing issues.\nThe DefaultRetryer is now preventing the errors from being reported to the app, so I'm happy that the go aws sdk is now doing what I would expect.\nWhile there does seem to be an issue with those regions is not an sdk bug, so closing.\nFinally for reference #957 explains why WithRetryer is where it is.\nThanks for everyone's feedback \ud83d\udc4d . Travis CI failure looks totally unrelated.. Yep I agree with that, thanks for your time.\nTo answer your question no we haven't come across any services that break with unsorted values but I definitely think there is a risk of that.. Hmm while investigating it suddenly stopped happening, so more like this was an infrastructure issue in the zone?\nWhat I found out before it was fixed is it was net/http built in gzip support which was throwing the error, so the server was reporting a Content-Encoding gzip but the returned body was invalid gzip as far as compress/gzip was concerned.. Grr its started erroring again :(. We have ec2 instances it pretty much all regions and sa-east-1 is the only\none we\u2019re seeing this error from. In our case just a plain describe\ninstances call triggers it when it\u2019s happening\nOn Fri, 3 Nov 2017 at 20:43, Daniel Martins notifications@github.com\nwrote:\n\nThis is happening consistently here in Terraform v0.10.7 with aws provider\nv1.2.0 (that apparently uses aws-sdk-go v1.12.19) while trying to manage\nresources in the sa-east-1 region.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-go/issues/1628#issuecomment-341821876,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAGXL22et8qP5p-ktd6NsgLd9Kl2QQSCks5sy3rtgaJpZM4QRV4M\n.\n. Thanks guys, I can confirm from our side that the error has not been reported again, so thanks for your help!\n\nGiven this wasn't a SDK code issue and was an infrastructure problem in that zone, if we do see something like it in the future it would be good to know the best way to raise such an issue?. ",
    "mrekucci": "There is no magic behind this PR. I just run the gofmt -s -w command at the top of the repository. \n. Oh, I didn't know that. Is there a good reason to keep such a generated files in the repository?\n. I fixed the commit. I took away the files which are generated. I hope it's ok now.\n. ",
    "fatih": "Additional data:\ngce: https://godoc.org/google.golang.org/api/compute/v1#ImageList\ndo: https://godoc.org/github.com/digitalocean/godo#Image\nAs seen types of other respective API's all have omitempty. \n. Yes, I want to create JSON outputs of certain types, so I can pass them to other services who consume JSON. Another use case is displaying them (think of it as a CLI), like:\naws ec2 describe-images --owners self --output json\nHowever the types are not JSON friendly. I've checked gce and do and they both already supports it. This is btw very common in Go community and JSON is very popular so everyone would benefit from it in the long term.\n. ",
    "clbanning": "http://play.golang.org/p/MODtdRa6ng\n. ",
    "marcosnils": "@DavidHuie I had the same concern while trying to upload some content to S3. If you look into the source code the library uses a ReadSeeker to calculate to content MD5 (https://github.com/aws/aws-sdk-go/blob/ea83c25c44525da47e8044bbd21e4045758ea39b/service/s3/content_md5.go#L25) which is about to send. It doesn't matter if the interface is changed to an io.Reader because either way the library will still read the whole content to do this calculation. It's cleaner from an API perspective to use ReadSeeker as resetting the buffer is provided by the Seeker API.\n. @lsegal totally right. I knew that some sort of hash was calculated because I looked in the code before. Just got confused between MD5 and Checksum. \nThanks for the clarification.\n. @diptanu seems like your post is going to localhost:8080 according to the Host header. Do you have a dynamodb listening there?\n. @diptanu seems like something is not working with your dynamo local. What do you get if you issue a curl localhost:8080/ ?\n. BTW: I'm also using dynamo local in my projects and this is how initialize my dynamo connection:\n``` golang\nfunc Server() *dynamodb.DynamoDB {\n        server := dynamodb.New(config.Current.DynamoConfig)                                                                                                                                                    \n    return server\n\n}\n```\nWhere DynamoConfig is:\ngolang\n        Current.DynamoConfig = &aws.Config{Credentials: Current.DynamoAwsCredentials,\n                Endpoint: Current.DynamoHost,\n                Region:   Current.DynamoRegion,\n        }\nYou need to specify credentials even though those are fake.\n. +1. \n. @phstc try changing the LogLevel in the config object to see more info. I'm able to use dynamodb-local without any problems\n. @jhspaybar documentation is generated automatically using ruby scripts located in the repo. Specifically what you want to modify is located here: https://github.com/aws/aws-sdk-go/blob/5b1076b5b29299f1e92786e5a4113e168418cd40/doc-src/plugin/plugin.rb#L46-L54\n. @jasdel ?\n. cool, thx! :)\n. @dragonfly90 I'd recommend to check the API doc instead: \nhttp://docs.aws.amazon.com/sdk-for-go/api/service/ec2/EC2.html#RunInstances-instance_method\nThere are updated examples there.\n. ",
    "DavidHuie": "The Uploader abstraction doesn't quite fit my usecase because I'm managing my own uploads via the multipart API (the server is proxying a large upload for a client). Would it be possible to create a similar abstraction for uploading a multipart part?\n. Correct, but in my usecase the server never has the entire payload -- it's on the client. The server, which would use this library, would be proxying for the multipart API.\n. @lsegal The cacheing on disk solution is probably good enough for my purposes. Thanks for the quick response.\n. ",
    "phinze": "Whoops typo - fixing.\n. Okay remaining failure looks unrelated :+1:\nLet me know if there's anything else you need from me here!\n. Sounds like a plan to me - thanks!\n. Just chiming in here that I spent a few minutes during an ELB wire debugging session trying to figure out when the RequestID field of Request is populated, and it seemed to be perpetually blank.\nEnded up falling back to full HTTPResponse dumping which served my purposes just fine.\n``` go\nsess.Handlers.UnmarshalMeta.PushBack(func(r *request.Request) {\n    dump, err := httputil.DumpResponse(r.HTTPResponse, true)\n    if err != nil {\n        log.Printf(\"Response Dump Error: %s\", err)\n        return\n    }\n    log.Printf(\"Response Dump: %s\", dump) // <-- has X-Amzn-Requestid\n    log.Printf(\"RequestID: %s\", r.RequestID) // <-- never populated\n})\n```\n. Great - thanks @jasdel!\n. > I'm curious if the revision your user is using is at or near 37ff15f. This revision matches up with the panic line mentioned in the stack trace for handler_functions.go#L91. In this revision SendHandler wasn't correctly setting the r.HTTPResponse if an error occurred, but the error was not an URL error.\n@jasdel aha! I'm sure that's it - thanks for the research / context!\n. ",
    "diptanu": "Here is what I am seeing in the logs with LogLevel set to 1\n```\n---[ CANONICAL STRING  ]-----------------------------\nPOST\n/\naccept-encoding:identity\nhost:localhost:8080\nx-amz-date:20150608T183625Z\nx-amz-target:DynamoDB_20120810.PutItem\naccept-encoding;host;x-amz-date;x-amz-target\n2f23609c1bc208b32ac6a2a23f21b7e7d35d0b6bf5ffb6b569bba75b7b5d97d6\n---[ STRING TO SIGN ]--------------------------------\nAWS4-HMAC-SHA256\n20150608T183625Z\n20150608/us-east-1/dynamodb/aws4_request\nae1068d59cba3179e22e92ac7adf92cf42817058398596ad3a7d780dbbb05e20\n-----------------------------------------------------\n---[ REQUEST POST-SIGN ]-----------------------------\nPOST / HTTP/1.1\nHost: localhost:8080\nUser-Agent: aws-sdk-go/0.6.1\nContent-Length: 82\nAccept-Encoding: identity\nAuthorization: AWS4-HMAC-SHA256 Credential=.../20150608/us-east-1/dynamodb/aws4_request, SignedHeaders=accept-encoding;host;x-amz-date;x-amz-target, Signature=a25d8746a5aa0d0458ef117ddf081dd48d2ec1a5e2096c38b5ffe493268d5168\nContent-Type: application/x-amz-json-1.0\nX-Amz-Content-Sha256: 2f23609c1bc208b32ac6a2a23f21b7e7d35d0b6bf5ffb6b569bba75b7b5d97d6\nX-Amz-Date: 20150608T183625Z\nX-Amz-Target: DynamoDB_20120810.PutItem\n{\"Item\":{\"Foo\":{\"S\":\"value\"},\"Bar\":{\"S\":\"value\"}},\"TableName\":\"DataBaseName\"}\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 404 Not Found\nContent-Length: 19\nContent-Type: text/plain; charset=utf-8\nDate: Mon, 08 Jun 2015 18:36:25 GMT\n404 page not found\n\nUnmarshal failed decoding JSON RPC error response invalid character 'p' after top-level value\n```\n. Yes it should go to localhost:8080, since I am using dynamodb local.\n. Well, the ports were messed up, sorry I got rat-holed into believing the client was not able to parse the response from Dynamo Local. Closing this issue.\n. @marcosnils I thought the SDK could use on-instance keys if I don't specify any credentials. It would be really nice if that's how it worked even if it doesn't work like that today.\n. Thanks!\n. ",
    "mischief": "you can already do this if you make your own io.Reader that tracks bytes read.\n. @jasdel i believe i was having a problem with incorrectly specified region, which i fixed, but i thought i would report this unmarshalling error anyway. i was not using UserBucket, only URL.\n. ",
    "avitex": "@jasdel Uploads via the s3manger.Uploader.\n@mischief I had already tried this, but seemed to 'upload' instantly, perhaps my code was bugged?\n. ",
    "benhinchley": "I have just been looking around about how to do this nicely, If I find anything I will post it here, but this would definitely be a nice feature to have\n. ",
    "fadiasamara": "Is there any update on this, we have tried to implement such a feature but its impossible with the current upload manager.\n. ",
    "krishnakhandagale": "Is upload progress feature implemented yet? . ",
    "oleynikd": "No workaround for this?. ",
    "maccam912": "In my fork I ended up just adding in a progress bar (https://github.com/cheggaaa/pb) and as each part in a multipart upload is completed it adds that progess to the bar. It's not perfect, but it works for me. Maybe a good starting point?\nIt just adds another channel for each uploader to send back the number of bytes uploaded and each time something is put in that channel the progress bar updates.\nhttps://github.com/maccam912/aws-sdk-go/blob/master/service/s3/s3manager/upload.go#L712\nhttps://github.com/maccam912/aws-sdk-go/blob/master/service/s3/s3manager/upload.go#L673. ",
    "xinst": "I have just create a pull request, and add an example for upload with progress \nhttps://github.com/aws/aws-sdk-go/pull/1868\n. I think @yizha this is reached bandwidth limit, you can test with \"Network Linker Conditioner\" on MAC or some other tools like that to limit your network speed, and then try again. > Thanks for the feedback, @ibrt would you mind creating a Github issue for this bug? I think we should update the original upload progress example to use similar output as the new download progress PR, #2456\nthat's awesome could display a progress bar in the sample  . > @xinst FYI the example is broken... What happens is that the uploader reads the whole thing twice, haven't dug into why but I think it has to do with determining the number of parts. The first read is very fast as the contents are discarded. The second actually seems to be piped to the upload.\n\nThe sample reports progress by dividing the number of bytes read by two, so what I'm seeing is that it jumps straight to 50%, and then proceeds to 100% at half the actual \"speed\". I \"fixed\" it by initializing read to -size, starting to report when read passes 0.\n\nI was test with some small(less than 5MB) and large files when I wrote this sample, it was worked for me at that time when divided by two, it was strange.  I didn't met the progress straight jumps to 50%.  . I was confused before but now I think I got the answer.\nwhen we upload a file use the API \noutput, err := uploader.Upload(&s3manager.UploadInput{\n        Bucket: aws.String(bucket),\n        Key:    aws.String(key),\n        Body:   reader,\n    })\nfirst step:  it will construct a chunk, it includes a SectionReader which will call the interface ReadAt, and this will increase the number of read,  after that, the chunk will be send a channel.\nsecond step: there are some go routines (the number you have set) read from the channel, and then it just to start sending data to S3, during the request you can find a Body                   io.ReadSeeker the Request structure, and the HTTP request reads data by this ReadSeeker, it will call the interface Read again, that's why length of read is double of file's length.\nmaybe the first step very fast, and you will see the progress jumps straight to 50%(if the read initializing to 0),  so I think there is no obvious difference  between by initializing read to -size and  by dividing the number of bytes read by two. Personally, set the read to -size is a little strange.\nHere is the code in the SDK\naws/aws-sdk-go/service/s3/s3manager/upload.go\n```\n// nextReader returns a seekable reader representing the next packet of data.\n// This operation increases the shared u.readerPos counter, but note that it\n// does not need to be wrapped in a mutex because nextReader is only called\n// from the main thread.\nfunc (u *uploader) nextReader() (io.ReadSeeker, int, []byte, error) {\n    type readerAtSeeker interface {\n        io.ReaderAt\n        io.ReadSeeker\n    }\n    switch r := u.in.Body.(type) {\n    case readerAtSeeker:\n        var err error\n    n := u.cfg.PartSize\n    if u.totalSize >= 0 {\n        bytesLeft := u.totalSize - u.readerPos\n\n        if bytesLeft <= u.cfg.PartSize {\n            err = io.EOF\n            n = bytesLeft\n        }\n    }\n\n    reader := io.NewSectionReader(r, u.readerPos, n)\n    u.readerPos += n\n\n    return reader, int(n), nil, err\n\ndefault:\n    part := u.bufferPool.Get().([]byte)\n    n, err := readFillBuf(r, part)\n    u.readerPos += int64(n)\n\n    return bytes.NewReader(part[0:n]), n, part, err\n}\n\n}\naws-sdk-go/aws/request/request.go\n// A Request is the service request to be made.\ntype Request struct {\n    Config     aws.Config\n    ClientInfo metadata.ClientInfo\n    Handlers   Handlers\nRetryer\nTime                   time.Time\nOperation              *Operation\nHTTPRequest            *http.Request\nHTTPResponse           *http.Response\nBody                   io.ReadSeeker\nBodyStart              int64 // offset from beginning of Body that the request body starts\nParams                 interface{}\nError                  error\nData                   interface{}\nRequestID              string\nRetryCount             int\nRetryable              *bool\nRetryDelay             time.Duration\nNotHoist               bool\nSignedHeaderVals       http.Header\nLastSignedAt           time.Time\nDisableFollowRedirects bool\n\n// A value greater than 0 instructs the request to be signed as Presigned URL\n// You should not set this field directly. Instead use Request's\n// Presign or PresignRequest methods.\nExpireTime time.Duration\n\ncontext aws.Context\n\nbuilt bool\n\n// Need to persist an intermediate body between the input Body and HTTP\n// request body because the HTTP Client's transport can maintain a reference\n// to the HTTP request's body after the client has returned. This value is\n// safe to use concurrently and wrap the input Body for each HTTP request.\nsafeBody *offsetReader\n\n}\n```\n. ",
    "jyehbrightcove": "I noticed that the library is located at:\ngithub.com/aws/aws-sdk-go/service/dynamodb/dynamodbattribute\nIt looks like the package declaration is dynamodb, which is the same as the parent folder.  Was this intentional?  If I try to import both this and the parent, I get a complaint that dynamodb is being redefined.\nI figured I might be missing something so I didn't want to raise an issue at first.\n. My apologies for not looking at the outstanding PRs.\n. ",
    "pkazmierczak": "@jasdel thanks, I think we can close this one. If I find something else I'll file another issue (afaik there's no way I can make a pull request for the wiki). \n. ",
    "mwhooker": "\nIf you're worried about the object being concurrently written to while you are downloading it then the initial HEAD request will be the least of your concerns, as the downloader itself is likely making multiple concurrent GET requests-- and the object could theoretically change in between those requests as well.\n\nyou're right. thanks for putting that into perspective.\n. awesome, thanks @jasdel! let me know if I can help\n. :+1: cheers, thanks\n. @xibz thanks for the quick response. I am able to hit s3 directly. in fact, I can run this command and have it work. I only get the unexpected EOF errors occasionally, but haven't detected a pattern yet.\nThanks for noticing the remove/close order. I will fix that and add the error type, and let it run over the weekend.\nI know it's not an awserr (I'm fairly certain it's an io.ErrUnexpectedEOF) because the line you linked only runs if it's not an awserr\n. I haven't been able to reproduce it when just hitting s3 directly. We run this command in response to user requests, which happen very infrequently (this is in a development environment).\nthat's interesting that Download only returns awserr. I'm not sure where this could be coming from, then.  Perhaps I'll change the other downloadToFile error conditions to panics.\n. @xibz thanks so much for looking in to this! I really appreciate it.\nI will continue to investigate and try to pinpoint the exact circumstances.\n. Hi @paulnivin,\nI agree it would be best if we got some guidance about how long to wait for specific types of instances, but I can see the logic of not wanting to encode that in the SDK. \nFor the Packer use case, my advice would be to adjust the AWS_MAX_RETRIES and AWS_POLL_DELAY_SECONDS environment variables appropriately. It may also be worth filing an issue to get higher defaults when the instance type is bare metal, and that kind of change should have more discussion.. Will keep eyes peeled for other reports of timeouts, but we've also seen this with WaitUntilAMIAvailable. However, that's not surprising to me since it's such a variable process.. ",
    "xunchangguo": "Feature added?\n. ",
    "petems": "With a little help from @mwhooker, I made an example of how to do this for an S3 download.\nInspired by the PR to show this for upload, I added an example PR here: https://github.com/aws/aws-sdk-go/pull/2456. @jasdel Awesome, thanks for your help getting it over the line! \ud83d\ude04 . Done! \ud83d\udc4d . Done! \ud83d\udc4d. Yeah this logic was left over from my CLI implementation, that checks if the file is downloaded first, removing for this example \ud83d\udc4d . Done! \ud83d\udc4d . Done! \ud83d\udc4d . Nice, I was repurposing some of my existing ls code, but for a single files, makes sense to use HeadObject. Yeah, I can't take credit for this, it was from a helper method in a library I was using \ud83d\ude04 . Yeah, I was re-interpreting this from some CLI code that used it's own env variables but good point, since we're using the default credentials I can remove all of this. . Fixed \ud83d\udc4d . Done! \ud83d\udc4d . Good idea, I moved this lower and stuck some error logic around it, does that work for you?. Done! \ud83d\udc4d . @jasdel I moved this lower and stuck some error logic around it, does that work for you?. Ah nvm, got it working \ud83d\udc4d \n```go\n// progressWriter creates a progressbar using the cheggaaa/pb library\n// whilst also following the progress using the bytes written to the\n// writerAt object to track its progress\ntype progressWriter struct {\n  written int64\n  writer  io.WriterAt\n  size         int64\n}\nfunc (pw *progressWriter) WriteAt(p []byte, off int64) (int, error) {\natomic.AddInt64(&pw.written, int64(len(p)))\n\nlog.Printf(\"File size:%d progress:%d \\n\", pw.size, pw.written)\n\nreturn pw.writer.WriteAt(p, off)\n\n}\n```\nI'll see if I can move this to percentages/chunks as right now it's very noisy!. Fixed. Yep, looks like i missed that, fixed. Done, but removed pb and added atomic/sync. Done! \ud83d\udc4d . Makes sense, removed this code and switched to bucket, key as args. ",
    "lkv123": "Even I got the same issue, but the removal of extra space after the access key didn't solve the problem. \nPlease help\nThanks . ",
    "jitendrawebclues": "@lsegal, Thanks work for me.!!. ",
    "r03": "Good point. I agree\n. I had the same problem with the code below.\nUsing go or gb didn't matter.\nAfter disabling the antivirus (bitdefender) it seemed to be solved.\nI guess the antivirus checks new exe's or something.\nsvc := s3.New(&aws.Config{Region: \"eu-central-1\", LogLevel: 1})\nresult, err := svc.ListBuckets(&s3.ListBucketsInput{})\nif err != nil {\n    fmt.Printf(\"ERROR %v\", err)\n}\nfor _, bucket := range result.Buckets {\n    fmt.Printf(\"%s \\n\", *bucket.Name)\n}\n. It worked a couple of times after disabling the auto pilot mode in bitdefender, but now I have the same problem again. But I will leave this ticket closed because this seems like a local problem.\n. ",
    "rchakra1": "Thanks very much @jasdel. Is there a util that can build the v2 authorization strategy header or I have to do that manually in the func(r *aws.Request)\n. Thanks again @jasdel . That helps\n. Hi @ncw we ended up using https://github.com/mitchellh/goamz which works\n. ",
    "crezam": "Hi @jasdel, Im trying to use the S3 SDK with a S3 compatible 3rd party that doesnt support v4 signatures. Is this v2 signer implemented now?\nI'm trying your suggestion above:\ngo\nsvc.Handlers.Sign.Clear()\nsvc.Handlers.Sign.PushBackNamed(corehandlers.BuildContentLengthHandler)\nsvc.Handlers.Sign.PushBackNamed(v2.SignRequestHandler)\nI'm getting a status code 403 and using correct credentials. Not sure if an issue with the way I'm setting up the handlers.. Thanks @xibz, I just confirmed the signature strategy that works for my current case in the Java SDK  is this. Comparing the headers set are not the same. I'm curious if that is even older than v2 signatures or a special subset of v2 for S3. Thanks. For the record, found that S3 service supported 'custom' V2 signatures that used HMAC-SHA1\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html\nIn contrast, for other services V2 signatures are different than S3 and using HMAC-SHA256\nhttp://docs.aws.amazon.com/general/latest/gr/signature-version-2.html. ",
    "kahing": "FWIW, goofys hacks in its own copy of v2 signer (https://github.com/kahing/goofys/blob/master/internal/v2signer.go). It works with path-based requests and had an issue when S3ForcePathStyle is not true (this was awhile ago and I don't remember the details).. @andrewgaul this may solve your problem\n. I am unsure why I get this error to begin with. Here's some log for this error:\n```\n2015/10/27 22:02:15 DEBUG: Request s3/UploadPart Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nPUT http://goofys.s3-us-west-2.amazonaws.com/test_dir/largefile?partNumber=29&uploadId=figScXN8Bs_SZCdsosYLepPjje7tCKufqjErL4x3irjOa5e1oxE7ZG3VhW.3sr47v_SEYXGBAbBVYLxU37MaKcldukqi9P21eAcqMjAgAWMSp_pmd4iqmBXYnUAqIQny HTTP/1.1\nHost: goofys.s3-us-west-2.amazonaws.com\nUser-Agent: aws-sdk-go/0.9.17\nContent-Length: 5242880\nAuthorization: \nX-Amz-Content-Sha256: c036cbb7553a909f8b8877d4461924307f27ecb66cff928eeeafd569c3887e29\nX-Amz-Date: 20151027T220215Z\nAccept-Encoding: gzip\n...\n2015/10/27 22:03:03 DEBUG: Response s3/UploadPart Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 400 Bad Request\nConnection: close\nTransfer-Encoding: chunked\nContent-Type: application/xml\nDate: Tue, 27 Oct 2015 22:02:42 GMT\nServer: AmazonS3\nX-Amz-Id-2: T3xI0zSTjXYdr+BieOZL8P+TaNCLIZga5wbSdGpU5hMWTVk2RVZUg1ILLJPjikxctDoKQ3GEkJY=\nX-Amz-Request-Id: 78438C4C74F7C9AD\n2015/10/27 22:03:03 DEBUG: Validate Response s3/UploadPart failed, will retry, error UnknownError: unknown error\n2015/10/27 22:03:03 DEBUG: Request s3/UploadPart Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nPUT http://goofys.s3-us-west-2.amazonaws.com/test_dir/largefile?partNumber=29&uploadId=figScXN8Bs_SZCdsosYLepPjje7tCKufqjErL4x3irjOa5e1oxE7ZG3VhW.3sr47v_SEYXGBAbBVYLxU37MaKcldukqi9P21eAcqMjAgAWMSp_pmd4iqmBXYnUAqIQny HTTP/1.1\nHost: goofys.s3-us-west-2.amazonaws.com\nUser-Agent: aws-sdk-go/0.9.17\nContent-Length: 5242880\nAuthorization: \nX-Amz-Content-Sha256: c036cbb7553a909f8b8877d4461924307f27ecb66cff928eeeafd569c3887e29\nX-Amz-Date: 20151027T220215Z\nAccept-Encoding: gzip\n```\nThis is run within ec2 to s3 bucket in the same region. Noticed how it took 48 seconds to get this error uploading a 5MB part.\n. Ah yes, the log is with this PR included. It wasn't retrying before. The original error looks something like:\n```\n2015/10/28 22:22:53 DEBUG: Response s3/UploadPart Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 400 Bad Request\nConnection: close\nTransfer-Encoding: chunked\nContent-Type: application/xml\nDate: Wed, 28 Oct 2015 22:22:33 GMT\nServer: AmazonS3\nX-Amz-Id-2: R/aUkurBzKrh4KosjiRw248a4TPF42ZvzfuWEtxlJM2Z/xD/M3PWG+xJ/0ax+Xm45iOqDWXN9NU=\nX-Amz-Request-Id: 290F8AB76C12C5D2\n2015/10/28 22:22:53 DEBUG: Validate Response s3/UploadPart failed, not retrying, error RequestTimeout: Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.\n        status code: 400, request id:\n```\nI am still very baffled why this error happens at all.\n. I see this very often. Initially I omitted the Content-Length and just relied on the sdk to figure it out (the Body is a bytes.NewReader([]byte)), but that and manually setting ContentLength to len(buf) had the same results.\nIf I provide my own reader and log from WriteTo(), I can see that net/http/Client reads from my buffer 4KB at a time, and at some point would stop, and then usually after another 43 seconds or so I will get the error.\n. I should add that my buffers are always 5MB and usually this stops after a few hundred KB.\n. This is running on ec2 c4.xlarge in us-west-2a connecting to a bucket in us-west-2\n. I do not expect the delay either and the buffer is a regular []byte. It seems to happen more on https and less (or not at all) on http. Just got a tcpdump and will look into it soon.\n. I believe in this case the bucket is at a different region than awsconfig is set to. I am trying to autodetect region so my users don't need to specify one. \n. Thanks. This is indeed a way for me to get the region information and I will try that. Is there a reason this isn't done in the SDK? \n. The linked unmarshal code already handles redirect and return a different error, maybe it makes sense to parse region there then. Also awserr.Error is specified to have OrigErr but that's never set, which is odd.\nThat said I will do the unmarshaling as you suggested in my own application.\n. I don't quite understand what you are saying about 400 and how is that related to 301. But there's another problem with always trying to parse the xml output: if it's a HEAD request there's no body. For some reason even if it's a GET I can't read the body either:\n``` go\n    head := s3.GetObjectInput{Bucket: &bucket, Key: aws.String(\"a\")}\n    req, _ := fs.s3.GetObjectRequest(&head)\n    req.Handlers.UnmarshalMeta.PushBack(func(req *request.Request) {\n        defer req.HTTPResponse.Body.Close()\n    //req.HTTPResponse.Header['x-amz-bucket-region']\n    buf, err := ioutil.ReadAll(req.HTTPResponse.Body)\n    fmt.Printf(\"err: %v\", string(buf))\n    return\n})\nreq.Send()\n\n```\nthe error is always empty string.\n. 301 is sometimes returned:\n```\n$ curl -v http://s3-us-west-2.amazonaws.com/lidar_fema\n Hostname was NOT found in DNS cache\n   Trying 54.231.168.208...\n* Connected to s3-us-west-2.amazonaws.com (54.231.168.208) port 80 (#0)\n\nGET /lidar_fema HTTP/1.1\nUser-Agent: curl/7.35.0\nHost: s3-us-west-2.amazonaws.com\nAccept: /\n< HTTP/1.1 301 Moved Permanently\n< x-amz-bucket-region: us-east-1\n< x-amz-request-id: B5FC5FB76C296B0C\n< x-amz-id-2: NwgQomdCNMCA8Z8+GGqiJDMJaXEGHBDUaubymsOWtRyX7DcObdd5GtEIPIpvrdmuCzRB2yjY3Dg=\n< Content-Type: application/xml\n< Transfer-Encoding: chunked\n< Date: Sun, 07 Feb 2016 00:56:07 GMT\n Server AmazonS3 is not blacklisted\n< Server: AmazonS3\n< \n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n Connection #0 to host s3-us-west-2.amazonaws.com left intact\nPermanentRedirectThe bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.lidar_femas3.amazonaws.comB5FC5FB76C296B0CNwgQomdCNMCA8Z8+GGqiJDMJaXEGHBDUaubymsOWtRyX7DcObdd5GtEIPIpvrdmuCzRB2yjY3Dg=\n```\n. Perhaps I should have specified that I am using S3ForcePathStyle = true.\n. If I HEAD the bucket from curl the region is returned from the header. And as you can see from the curl GET output above, an XML output is returned for GET. However I haven't figured out a way to get either one. \n. BTW really appreciate that you are looking into this even on a weekend! I may not have time to debug this further until next week. \n. The problem with that is if the bucket is not at us-east-1, doing a HEAD request would still result in 301. In the above case, when substituted the bucket with one in us-west-2, resp is nil and err.String() is \"Head : 301 response missing Location header\".\n. on top of that, s3.HeadBucket() uses path style even if S3ForcePathStyle is not set to true, so I can't even temporarily disable that to check the bucket region.\n. I don't know if this kind of workaround is necessary for most use cases, and I was able to come up with a workaround. Thanks @jasdel and @xibz !\n. There's no custom handler in this case, here's a reduced testcase:\n\n```go\n    awsConfig := &aws.Config{\n        Region:           aws.String(\"us-east-1\"),\n        S3ForcePathStyle: aws.Bool(true),\n        LogLevel:         aws.LogLevel(aws.LogDebug | aws.LogDebugWithRequestErrors),\n    }\nsess := session.New(awsConfig)\nsvc := s3.New(sess)\nbucket := \"goofys-eu-west-1.kahing.xyz\"\n_, _ = svc.HeadObject(&s3.HeadObjectInput{Bucket: &bucket, Key: aws.String(\"foo\")})\n\n```. looks like this one is indeed related to my custom hacked v2 signing handler. sorry about that. Understood that mine is not a correct fix. Happy to test this once the fix is available.. #1212 doesn't appear to fix the problem, when LogDebugWithHTTPBody is not set or when logging is disabled. go 1.8.1 and aws-sdk-go v1.8.14. The response is not empty, but I get things like (when dumped to debug myself):\n...\n    {\n      ETag: \"\\\"0951f27a7eaff745a10eec71a0eeea9c\\\"\",\n      Key: \"linux/linux-4.10.6/drivers/media/dvb-frontends/zl10039.c\",\n      LastModified: 2017-03-27 06:06:52 +0000 UTC,\n      Owner: {\n        DisplayName: \"kahing\",\n        ID: \"31aae647457d132c0f31fd72bbb86394153797f65ab8af38fecfb5c178040636\"\n      },\n      Size: 7192,\n      StorageClass: \"STANDARD\"\n    },\n    {\n      Key: \"linux/linux-4.10.6/drivers/med\"\n    }\n  ],\n  Delimiter: \"/\",\n  IsTruncated: false,\n  Marker: \"\",\n  MaxKeys: 1000,\n  Name: \"goofys\",\n  Prefix: \"linux/linux-4.10.6/drivers/media/dvb-frontends/\"\n}\nNotice that the last key is truncated. I don't have an easily reproducible step beyond what's in https://github.com/kahing/goofys/issues/161 , but that's just a way to induce the connection resets.. Ideally IO errors would be differentiable from XML parsing errors, but I am not picky. . the latest fix seems to work. thanks!. ",
    "shamiq": "Thanks for the speedy response! Looking at the S3 examples, I assumed I could just use the outputted URL. So, anything annotated with location:\"header\" must be included in the header then. \nIf i have multiple such headers, will I need to have them in a set order in the curl command?\n. ",
    "jeremybradbury": "Thank you @shamiq for this issue... the JS sdk is so poorly documented. \nThe key for the header is: \"ServerSideEncryption\" which matches x-amz-server-side-encryption isn't very obvious and the only AWS docs that had this data were in Go. Java SDK uses a different key to identify this header. But the only docs that explain how to do this use Java snippets. \nSeems like AWS loves to write wordy and confusing docs that don't actually get to the point and when they do, it's in C# or Java rather than something people actually use. I wouldn't mind so much if the APIs had matching keys for the object properties but they are wildly different.\n. ",
    "fantyz": "Sorry for taking a while to get back to you.\nEach worker \"own\" a shard and as such aren't sharing anything like iterators or the likes. I'm fairly confident that the two issues are the one and same (eg. a GetRecords request taking very long to finish causing either the http request being reset by peer or actually finishing but taking longer to do so than the 5 minutes which is the maximum allowed age of an iterator).\nIt is running on a m3.medium instance in EC2. It has no problems keeping up with the load (<20% CPU usage, plenty of memory left). I've tried a few different instance types (including much larger instance types as well) but the problem persists.\n. Thanks- I hope to find time to investigate soon. I'll be back! :)\nOn Thu, Aug 6, 2015 at 1:03 AM, Jason Del Ponte notifications@github.com\nwrote:\n\nHi @fantyz https://github.com/fantyz With our latest changes to\naws.Config the best way to see the request and response wire data.\nsvc := kinesis.New(aws.NewConfig().WithLogLevel(aws.LogDebug))// ... use the service.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-go/issues/301#issuecomment-128177127.\n\n\nBest regards,\nKasper Middelboe Petersen\nLead Backend Developer\nSYBO Games ApS\nJorcks Passage 1A, 4th.\n1162 Copenhagen K\n. Yea, we still have it. I've just not had the chance yet to dig further into it. I'll be back :)\n. @jasdel I don't have permissions to reopen the issue.\nI'm using aws-sdk-go v1.0.11.\nI've had a chance to spend some time on this now.\nI've made a very simple Kinesis reader and had it running for a short while. As it is a transient problem its not easy to catch it happening. The closest has been a 30 second delay. As shown below the actual delay is happening after the response from the Kinesis API has been received..?\nAny ideas how to proceed from here?\n...\nJul 05 13:17:06 pipeline.log:  2016/07/05 11:17:06 DEBUG: Response kinesis/GetRecords Details:\nJul 05 13:17:06 pipeline.log:  ---[ RESPONSE ]--------------------------------------\nJul 05 13:17:06 pipeline.log:  <skipping body>\nJul 05 13:17:06 pipeline.log:  -----------------------------------------------------\nJul 05 13:17:06 pipeline.log:  2016/07/05 11:17:06 [DEBUGGER] Read 9 records from Kinesis (took=26.05296ms, lag=2016-07-05 11:17:06 +0000 UTC)\nJul 05 13:17:06 pipeline.log:  2016/07/05 11:17:06 [DEBUGGER] Sleeping for 973.94704ms\nJul 05 13:17:07 pipeline.log:  2016/07/05 11:17:07 DEBUG: Request kinesis/GetRecords Details:\nJul 05 13:17:07 pipeline.log:  ---[ REQUEST POST-SIGN ]-----------------------------\nJul 05 13:17:07 pipeline.log:  POST / HTTP/1.1 \nJul 05 13:17:07 pipeline.log:  Host: kinesis.us-east-1.amazonaws.com \nJul 05 13:17:07 pipeline.log:  User-Agent: aws-sdk-go/1.0.11 (go1.6; linux; amd64) \nJul 05 13:17:07 pipeline.log:  Content-Length: 268 \nJul 05 13:17:07 pipeline.log:  Authorization: AWS4-HMAC-SHA256 Credential=ASIAJ3ZTEIMWNPP6ZTRQ/20160705/us-east-1/kinesis/aws4_request, SignedHeaders=host;x-amz-date;x-amz-security-token;x-amz-target, Signature=7e8845e185a8604c23c68d653595c9da9fdaee252d4afe0d1140ad3efa120b13 \nJul 05 13:17:07 pipeline.log:  Content-Type: application/x-amz-json-1.1 \nJul 05 13:17:07 pipeline.log:  X-Amz-Content-Sha256: 8f03fc179d188c84ff902eac26fd43a717507ee881ea42b9d245cb5467bad255 \nJul 05 13:17:07 pipeline.log:  X-Amz-Date: 20160705T111707Z \nJul 05 13:17:07 pipeline.log:  X-Amz-Security-Token: FQoDYXdzEIv//////////wEaDOa/V5X5iX9RQ/lqNSKcAyfpYz91GDFn5mDadwdxseZ6faPdxyljA9lvieptBicUM7Pw91WAbC1Aa+yqXUmjA90KaOUst8zhYmY6KZOgJ+hbI1OJMDJcmOY0kRbjca/Xf7B8n3Mh4ukTKTsymidrn0FfkKctvzv/VBoFYBM7cpdAon2HKSOqk7AeeCPfAn9D+BiGtvQUd0gW4h/k5ZU0qTLoeN1d/fnMcGu+PsxosVVfgSyoNKOMI45A77KsazjFbym3fqFb4UVPoL+1UEMcGJzwtIPozYKTPPISsBUEYoH1SxN++XTynJvjAhlZsuVb4GuKMMwB51NS1CsRcghYTenIqHMwOXe7lSzY4HX3zvLJRs2mD+4vVvoqDuaBaiM5Fx08Pa1IXkhU80lf68/rxR/uvYtGKzzkW323rpDlg9toSckd0OGRwHMiFCZfR3s9u563EJx2E4CRTry7uRzFmW5FWFON4l4PKkAsLYSO14lgbDT/ol0HiFL/Nmo5HpECX8aONIvgMhGyJf5xQjEroRXYfPJOWPFv4QpIb381nrW0J9t/ipMirgcuovoor4XuuwU= \nJul 05 13:17:07 pipeline.log:  X-Amz-Target: Kinesis_20131202.GetRecords \nJul 05 13:17:07 pipeline.log:  Accept-Encoding: gzip \nJul 05 13:17:07 pipeline.log: {\"ShardIterator\":\"AAAAAAAAAAHHHk8OCHycfvVztfWnpG0G23yT/4e0q+ULejP0Y2PPKdTw4CJAFZbHpWYfmtfpkLjnCJDyKK1rWGR3GpGaguggr5Y1Mzgb3PfQ3boK7LWYT4Zztn+OJLXWKD5s/4wkWClyKPihUiGmXYTwoTi0zkpcXzwkIM4ofpbkYsBFSZDbymyvZ/fawst1PnpgfOCeH15KwSHqFGrGT5qU38VwVefy3fXkKClKl+vbyHJpC7BIFQ==\"}\nJul 05 13:17:07 pipeline.log:  -----------------------------------------------------\nJul 05 13:17:07 pipeline.log:  2016/07/05 11:17:07 DEBUG: Response kinesis/GetRecords Details:\nJul 05 13:17:07 pipeline.log:  ---[ RESPONSE ]--------------------------------------\nJul 05 13:17:07 pipeline.log:  <skipping body>\nJul 05 13:17:07 pipeline.log:  -----------------------------------------------------\nJul 05 13:17:38 pipeline.log:  2016/07/05 11:17:38 [DEBUGGER] Read 2 records from Kinesis (took=30.79373588s, lag=2016-07-05 11:17:07 +0000 UTC)\n...\nI modified aws-sdk-go/aws/client/client.go to allow using aws.LogDebugWithHTTPBody without actually printing the succesful responses as aws.LogDebugWithRequestErrors is outputting a lot of UnknownError: unknown rather than the actual error.\n``` go\nfunc logResponse(r *request.Request) {\n    var msg = \"no response data\"\nif r.HTTPResponse.StatusCode == 200 {  \n    // Temporary hack to avoid outputting the body of a succesful GetRecords\n    msg = \"<skipping body>\"\n} else if r.HTTPResponse != nil {\n    logBody := r.Config.LogLevel.Matches(aws.LogDebugWithHTTPBody)\n    dumpedBody, _ := httputil.DumpResponse(r.HTTPResponse, logBody)\n    msg = string(dumpedBody)\n} else if r.Error != nil {\n    msg = r.Error.Error()\n}\nr.Config.Logger.Log(fmt.Sprintf(logRespMsg, r.ClientInfo.ServiceName, r.Operation.Name, msg))\n\n}\n```\nMy kinesis reader application looks like this:\n``` go\npackage main\nimport (\n    \"fmt\"\n    \"log\"\n    \"time\"\n\"github.com/aws/aws-sdk-go/aws\"\n//  \"github.com/aws/aws-sdk-go/aws/credentials\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/kinesis\"\n\n)\n/\nvar kc = kinesis.New(session.New(aws.NewConfig().\n    WithRegion(\"us-east-1\").\n    WithLogLevel(aws.LogDebugWithRequestErrors | aws.LogDebugWithHTTPBody).\n    WithCredentials(credentials.NewSharedCredentials(\"\", \"prod\")),\n))\n/\nvar kc = kinesis.New(session.New(aws.NewConfig().WithRegion(\"us-east-1\").WithLogLevel(aws.LogDebugWithRequestErrors | aws.LogDebugWithHTTPBody)))\nfunc GetKinesisTrimHorizonIterator(streamName, shardId string) (string, error) {\n    getShardIteratorInput := &kinesis.GetShardIteratorInput{\n        StreamName:        &streamName,\n        ShardId:           &shardId,\n        ShardIteratorType: aws.String(\"TRIM_HORIZON\"),\n        //ShardIteratorType: aws.String(\"LATEST\"),\n    }\n    resp, err := kc.GetShardIterator(getShardIteratorInput)\n    if err != nil {\n        return \"\", fmt.Errorf(\"Failed to get kinesis shard iterator (TRIM_HORIZON) (streamName=%v, shardId=%v, err=%v)\", streamName, shardId, err)\n    }\n    return *resp.ShardIterator, nil\n}\nfunc GetKinesisRecords(iterator string) ([]*kinesis.Record, string, error) {\n    getRecordsInput := &kinesis.GetRecordsInput{\n        ShardIterator: &iterator,\n    }\n    resp, err := kc.GetRecords(getRecordsInput)\n    if err != nil {\n        return nil, \"\", fmt.Errorf(\"Failed to get records from kinesis (iterator=%v, err=%v)\", iterator, err)\n    }\nreturn resp.Records, *resp.NextShardIterator, nil\n\n}\nfunc main() {\n    minReadDuration := 1 * time.Second\n    streamName := \"\"\n    shardId := \"\"\nouter:\n    for {\n        log.Printf(\"[DEBUGGER] Getting kinesis iterator\")\n        iterator, err := GetKinesisTrimHorizonIterator(streamName, shardId)\n        if err != nil {\n            log.Printf(\"ERROR [DEBUGGER] Unable to get kinesis shard iterator: %v\", err)\n            return\n        }\n        log.Printf(\"[DEBUGGER] Got iterator %v\", iterator)\n    var ts_start, ts_last, lag time.Time\n    var records []*kinesis.Record\n    ts_last = time.Now()\n\n    for {\n        // get records\n        ts_start = time.Now()\n        records, iterator, err = GetKinesisRecords(iterator)\n        if err != nil {\n            log.Printf(\"[DEBUGGER] Unable to get data (requestTime=%v, lastIterationRequestTime=%v, err=%v)\", time.Since(ts_start), ts_start.Sub(ts_last), err)\n            continue outer\n        }\n        ts_last = ts_start\n\n        if len(records) > 0 {\n            lag = *(records[len(records)-1]).ApproximateArrivalTimestamp\n        }\n\n        took := time.Since(ts_start)\n        log.Printf(\"[DEBUGGER] Read %v records from Kinesis (took=%v, lag=%v)\", len(records), took, lag)\n        if took > 1*time.Minute {\n            log.Printf(\"[DEBUGGER] Request took more than 1 minute!\")\n        }\n        sleepDuration := minReadDuration - took\n        if sleepDuration > 0 {\n            log.Printf(\"[DEBUGGER] Sleeping for %v\", sleepDuration)\n            time.Sleep(sleepDuration)\n        }\n    }\n}\n\n}\n``\n. I left the debug application running over night. It contains a handful of examples of theGetRecords` taking more than 1 minute to respond. The worst one caught here is more than 12 minutes.\n...\nJul 05 17:03:40 pipeline.log:  2016/07/05 15:03:40 [DEBUGGER] Sleeping for 974.869046ms\nJul 05 17:03:41 pipeline.log:  2016/07/05 15:03:41 DEBUG: Request kinesis/GetRecords Details:\nJul 05 17:03:41 pipeline.log:  ---[ REQUEST POST-SIGN ]-----------------------------\nJul 05 17:03:41 pipeline.log:  POST / HTTP/1.1 \nJul 05 17:03:41 pipeline.log:  Host: kinesis.us-east-1.amazonaws.com \nJul 05 17:03:41 pipeline.log:  User-Agent: aws-sdk-go/1.0.11 (go1.6; linux; amd64) \nJul 05 17:03:41 pipeline.log:  Content-Length: 268 \nJul 05 17:03:41 pipeline.log:  Authorization: AWS4-HMAC-SHA256 Credential=ASIAJ3ZTEIMWNPP6ZTRQ/20160705/us-east-1/kinesis/aws4_request, SignedHeaders=host;x-amz-date;x-amz-security-token;x-amz-target, Signature=319589d86e3dd9dc3ecc015078bb68f403782f5b6158d7c2e73ce1455d0812ea \nJul 05 17:03:41 pipeline.log:  Content-Type: application/x-amz-json-1.1 \nJul 05 17:03:41 pipeline.log:  X-Amz-Content-Sha256: 2830397eb712ad24bf5685487bb6f2c873c355217dc42ed9029c380f826a00dc \nJul 05 17:03:41 pipeline.log:  X-Amz-Date: 20160705T150341Z \nJul 05 17:03:41 pipeline.log:  X-Amz-Security-Token: FQoDYXdzEIv//////////wEaDOa/V5X5iX9RQ/lqNSKcAyfpYz91GDFn5mDadwdxseZ6faPdxyljA9lvieptBicUM7Pw91WAbC1Aa+yqXUmjA90KaOUst8zhYmY6KZOgJ+hbI1OJMDJcmOY0kRbjca/Xf7B8n3Mh4ukTKTsymidrn0FfkKctvzv/VBoFYBM7cpdAon2HKSOqk7AeeCPfAn9D+BiGtvQUd0gW4h/k5ZU0qTLoeN1d/fnMcGu+PsxosVVfgSyoNKOMI45A77KsazjFbym3fqFb4UVPoL+1UEMcGJzwtIPozYKTPPISsBUEYoH1SxN++XTynJvjAhlZsuVb4GuKMMwB51NS1CsRcghYTenIqHMwOXe7lSzY4HX3zvLJRs2mD+4vVvoqDuaBaiM5Fx08Pa1IXkhU80lf68/rxR/uvYtGKzzkW323rpDlg9toSckd0OGRwHMiFCZfR3s9u563EJx2E4CRTry7uRzFmW5FWFON4l4PKkAsLYSO14lgbDT/ol0HiFL/Nmo5HpECX8aONIvgMhGyJf5xQjEroRXYfPJOWPFv4QpIb381nrW0J9t/ipMirgcuovoor4XuuwU= \nJul 05 17:03:41 pipeline.log:  X-Amz-Target: Kinesis_20131202.GetRecords \nJul 05 17:03:41 pipeline.log:  Accept-Encoding: gzip \nJul 05 17:03:41 pipeline.log: {\"ShardIterator\":\"AAAAAAAAAAEeoL/YYr2CCrFAWl2N+UrspY+TGDxNGwq6pmWhHA85F4wbXQW3WZeE4F+nEcbpt4uGzjMZJDn6EsO7eZ4uSOHsZB7jsJKl891zoSLW7/px0TpBFye0th8WGv/Xy2NLNwU0R0FhdUA/rj2vVgH7o8BaNXhQn61yOSsj2sGj5hcsXG6lTw6GKBR+jm7DJtThN3DVCkzP0VDr5rYxxmW+bN6voqXK95YhsbXg9GpSdU/qrA==\"}\nJul 05 17:03:41 pipeline.log:  -----------------------------------------------------\nJul 05 17:03:41 pipeline.log:  2016/07/05 15:03:41 DEBUG: Response kinesis/GetRecords Details:\nJul 05 17:03:41 pipeline.log:  ---[ RESPONSE ]--------------------------------------\nJul 05 17:03:41 pipeline.log:  <skipping body>\nJul 05 17:03:41 pipeline.log:  -----------------------------------------------------\nJul 05 17:15:46 pipeline.log:  2016/07/05 15:15:46 [DEBUGGER] Read 8 records from Kinesis (took=12m4.798487323s, lag=2016-07-05 15:03:41 +0000 UTC)\nJul 05 17:15:46 pipeline.log:  2016/07/05 15:15:46 [DEBUGGER] Request took more than 1 minute!\n...\n. @jasdel yes, but it is the minor hack I did to the SDK shown in my reply above. It's putting the <skipping body> if and only if the status code is 200.\nThe GetRecords that takes a very long time succeeds and returns actual records and not an error (which matches the single request to kinesis from the SDK and the subsequent <skipping body> on the 200 response. I have no idea what could be taking time after that.\nI'm using Kinesis streams. Our records vary a lot in size average of around 10kb but does have cases of records of 1 mb (analytics coming in from clients that potentially could have been offline for a while batching up a lot of analytics). Currently I think the throughput is around 10-15 mb/minute total on this system.\n. @jasdel these are two unrelated issues though!\nThis issue is about the seemingly random hang I'm getting on GetRecords. That one has no errors involved with it. The logs shows the request succeeds but takes an inordinary large amount of time to complete.\nThe other issue you list popped up while trying to debug the first one. I think a new issue is in its place for that one! :)\n. According to the log timestamps the SDK have the full body at hand and is able to print it out (although I bypassed that to avoid flooding the logs with data) before the delay incurs. I don't think it has anything to do with the actual connection?\nYou can see the jump from 17:03:41 to 17:15:46 is after getting the response.\nJul 05 17:03:41 pipeline.log:  ---[ RESPONSE ]--------------------------------------\nJul 05 17:03:41 pipeline.log:  <skipping body>\nJul 05 17:03:41 pipeline.log:  -----------------------------------------------------\nJul 05 17:15:46 pipeline.log:  2016/07/05 15:15:46 [DEBUGGER] Read 8 records from Kinesis (took=12m4.798487323s, lag=2016-07-05 15:03:41 +0000 UTC)\n. Retrying logging was not explicitly enabled, but does it have to be when request logging is? I've seen many examples clearly being able to see the retries from the requests being logged with the current settings- the individual requests are shown (including the failing ones). The above example only have one single request that succeeds, but the SDK does not return from the function call for a significant amount of time afterwards.\nHow do I find the request id? I'll dig it out tomorrow then, although I'm not sure what the Kinesis service team can do as the GetRecords request is responded to in a timely manner (as shown by the logs)?\nIt looks to me like it is printing the body of the succesful GetRecords API request around 12 minutes before returning the function call or am I missing something obvious here?\n. It varies a lot how often it happens. Not more than a couple of times a day normally though. However I updated my debugger with getting the status code and content length as well as printing the size of the records earlier today. I think its been running for ~8 hours and have one occurrence:\nAs shown, the jump in time happens after the printing of the response but before the function returns.\nJul 08 19:03:52 pipeline.log:  2016/07/08 17:03:52 [DEBUGGER] Read 5 records from Kinesis (took=38.532997ms, lag=2016-07-08 17:03:52 +0000 UTC)\nJul 08 19:03:52 pipeline.log:  2016/07/08 17:03:52 [DEBUGGER] Sleeping for 961.467003ms\nJul 08 19:03:53 pipeline.log:  2016/07/08 17:03:53 DEBUG: Request kinesis/GetRecords Details:\nJul 08 19:03:53 pipeline.log:  ---[ REQUEST POST-SIGN ]-----------------------------\nJul 08 19:03:53 pipeline.log:  POST / HTTP/1.1 \nJul 08 19:03:53 pipeline.log:  Host: kinesis.us-east-1.amazonaws.com \nJul 08 19:03:53 pipeline.log:  User-Agent: aws-sdk-go/1.0.11 (go1.6; linux; amd64) \nJul 08 19:03:53 pipeline.log:  Content-Length: 268 \nJul 08 19:03:53 pipeline.log:  Authorization: AWS4-HMAC-SHA256 Credential=ASIAJX3ZO3GO2HYO6E7A/20160708/us-east-1/kinesis/aws4_request, SignedHeaders=host;x-amz-date;x-amz-security-token;x-amz-target, Signature=bf252509dca02b933b71360faff9361428a65983cc3a0eae0bbe95cf1e2f1903 \nJul 08 19:03:53 pipeline.log:  Content-Type: application/x-amz-json-1.1 \nJul 08 19:03:53 pipeline.log:  X-Amz-Content-Sha256: b7326a3657919307e21c8012ffaddcc4533a7dc5f9489569efe2e29db1d7ed91 \nJul 08 19:03:53 pipeline.log:  X-Amz-Date: 20160708T170353Z \nJul 08 19:03:53 pipeline.log:  X-Amz-Security-Token: FQoDYXdzENf//////////wEaDN5V67Ir7x/AAehQBSKcAyvnugTclVN25and56pVD5jIerpza+Hgovo1ih2oNa0U5BwjtsBsfmbY7lG+JrR505MNpXv+bTcS0mf4+WglBf+r017YQhJ9xg1CjDpDglDnqFNwy1FVpP7EXamKO1sIDu/9A/XlrsQMCf6kAowaEeYxwZbJnsqW5qjH9PsV33vd7rrXzPNmx7WPVuj7uiFnTxuZp+V+PypYM4Q/xYDBS7OeObVbEjMHDR3KHVWYPUyAwIjqHcmFnOPwJP0PR2qtBAvbgSB9pUNBXXMuUSt7SBaHG1qBIoLjHZ4zmTapiNp2AhyDgTEsjRc//m8D0oxhPslt6xABwBFvM2XVwpTPDWoLlyz2HjgbrJmPNVlzBnUcuMxHaUlio1ZmZSsonAJ/vfDUYyLfoFqZvTEkxDJJub+gJ4LUdmuTJ8nj4bt00lsYYFeNLUerpCXigPTg9rnThT7VGKHF3hfxUdlEJkmEZTH7tnscoSi0nw5/q3ipBuafvPGyLL8XDQBlGP9KXbxgzet2ulAYY+ZbwLt1xPaS5xj/uukHGU4AhOKRGH8optz+uwU= \nJul 08 19:03:53 pipeline.log:  X-Amz-Target: Kinesis_20131202.GetRecords \nJul 08 19:03:53 pipeline.log:  Accept-Encoding: gzip \nJul 08 19:03:53 pipeline.log: {\"ShardIterator\":\"AAAAAAAAAAHprr6wW7jVknGCzR4EDZJBoEzT360fUy2PMMxEye7ibI7wAQ3ybnNRpcCbi6zMZSfATOqyP33dNnbnfisvoMJW8flafcyuoeUjynEjIujDDyg9j3HaracaaXf0235vK4BEwqapmP36MvcccRHo58FA8bOTQ/lk32R/w6bR04vXUV/JANW6bTnvEiijg4Q5Yo5U0k4I/GUarUscf7GIl1iECRV84vZWWUB3p+kUGHs97A==\"}\nJul 08 19:03:53 pipeline.log:  -----------------------------------------------------\nJul 08 19:03:53 pipeline.log:  2016/07/08 17:03:53 DEBUG: Response kinesis/GetRecords Details:\nJul 08 19:03:53 pipeline.log:  ---[ RESPONSE ]--------------------------------------\nJul 08 19:03:53 pipeline.log:  HTTP/1.1 200 OK \nJul 08 19:03:53 pipeline.log:  Content-Length: 49633 \nJul 08 19:03:53 pipeline.log:  Content-Type: application/x-amz-json-1.1 \nJul 08 19:03:53 pipeline.log:  Date: Fri, 08 Jul 2016 17:03:52 GMT \nJul 08 19:03:53 pipeline.log:  Server: Apache-Coyote/1.1 \nJul 08 19:03:53 pipeline.log:  X-Amz-Id-2: 6z52FP2ur7fSTxaTtaoIqkHiKvD1GFG5/iZoxzJBjWqlZubJAmc4M2cqydK3KazYUquPV8BlkhA2Q2mpusChYaiWAoiA3ipq \nJul 08 19:03:53 pipeline.log:  X-Amzn-Requestid: fb6566fb-6468-eeaf-b49c-b9c2d6f38b0f \nJul 08 19:03:53 pipeline.log:  -----------------------------------------------------\nJul 08 19:10:01 cron:  Jul  8 17:10:01 ip-172-31-150-165 CROND[23747]: (root) CMD (/usr/lib64/sa/sa1 1 1)\nJul 08 19:11:41 pipeline.log:  2016/07/08 17:11:41 [DEBUGGER] Read 8 records from Kinesis (took=7m48.178352365s, lag=2016-07-08 17:03:52 +0000 UTC)\nJul 08 19:11:41 pipeline.log:  2016/07/08 17:11:41 [DEBUGGER] Request took more than 1 minute!\nJul 08 19:11:41 pipeline.log:  2016/07/08 17:11:41 [DEBUGGER]   -> Record size: 535.00B\nJul 08 19:11:41 pipeline.log:  2016/07/08 17:11:41 [DEBUGGER]   -> Record size: 5.92KB\nJul 08 19:11:41 pipeline.log:  2016/07/08 17:11:41 [DEBUGGER]   -> Record size: 3.99KB\nJul 08 19:11:41 pipeline.log:  2016/07/08 17:11:41 [DEBUGGER]   -> Record size: 5.91KB\nJul 08 19:11:41 pipeline.log:  2016/07/08 17:11:41 [DEBUGGER]   -> Record size: 766.00B\nJul 08 19:11:41 pipeline.log:  2016/07/08 17:11:41 [DEBUGGER]   -> Record size: 592.00B\nJul 08 19:11:41 pipeline.log:  2016/07/08 17:11:41 [DEBUGGER]   -> Record size: 11.49KB\nJul 08 19:11:41 pipeline.log:  2016/07/08 17:11:41 [DEBUGGER]   -> Record size: 6.75KB\n. I've done slight modifications to get your version to run in our system. You can see the source here.\nModifications done:\n- Hardcode stream name\n- Added .WithLogLevel(aws.LogDebugWithRequestRetries | aws.LogDebugWithRequestErrors | aws.LogDebugWithHTTPBody)\n- Added if time.Since(start) > 30*time.Second { fmt.Printf(\"GetRecords took more than 30 seconds!\") } right after GetRecords to easily grep it in the papertrail logs\n- Removed putRecords to allow it to run against prod where we see the issue happening\nTo be continued once it hopefully catches the issue.\n. Caught one!\nJul 12 15:03:25 pipeline.log:  Starting GetRecords build/sign request, took 13.904\u00b5s\nJul 12 15:03:25 pipeline.log:  2016/07/12 13:03:25 DEBUG: Request kinesis/GetRecords Details:\nJul 12 15:03:25 pipeline.log:  ---[ REQUEST POST-SIGN ]-----------------------------\nJul 12 15:03:25 pipeline.log:  POST / HTTP/1.1 \nJul 12 15:03:25 pipeline.log:  Host: kinesis.us-east-1.amazonaws.com \nJul 12 15:03:25 pipeline.log:  User-Agent: aws-sdk-go/1.0.11 (go1.6; linux; amd64) \nJul 12 15:03:25 pipeline.log:  Content-Length: 282 \nJul 12 15:03:25 pipeline.log:  Authorization: AWS4-HMAC-SHA256 Credential=ASIAJACNTZW3NQXOLAXQ/20160712/us-east-1/kinesis/aws4_request, SignedHeaders=host;x-amz-date;x-amz-security-token;x-amz-target, Signature=eb842fd894a667916bb4653012e9472f3d0a3834ceaac4ef0aae936279b0639e \nJul 12 15:03:25 pipeline.log:  Content-Type: application/x-amz-json-1.1 \nJul 12 15:03:25 pipeline.log:  X-Amz-Content-Sha256: d45ee2996ba45b8b09f719264c94b418cf5c738efa6d28ea363ff73b749255dd \nJul 12 15:03:25 pipeline.log:  X-Amz-Date: 20160712T130325Z \nJul 12 15:03:25 pipeline.log:  X-Amz-Security-Token: FQoDYXdzEEYaDGG2zMo1+mUcP+sZiSKcA3macZ233hc6PYH54Ln5hBZszt09jwM7LoMzVZx2X60LIwf66ZcC9nFwi0/s0C2nN8JsxcOgSZ/Yb8w4wdqBEZG6zuokkfkHUfliyrfaDZEL/aPNwMe16HPDaN5KwWDJP10VbYKwNDRA+rv8EuJoyf2/eoaiJU5udvVksDiqTxUmnIa/2TXTOix883maCYaWlDMoW1PRyFGHDxQMiepM7j4uoaXsoK7d4KdehMIb78YKOL/rjYMDT7jgrB8jNUWzGXywMVzX6c1YIiOZGLhTfD5VahY18oCw72NfS5UHgnXLUiJ2vbvF9rqeCisHHhoX/yLujNWSLKbnMSGwvLu6CLRctn63SRItXVC7iYWl1OGUJ/RaXqum8m9dILAvJB/jN1niYPpBlDVHR6mOdF65F62G1oKKwA+2CmcJPE49NYjbn7BKW/U9ut5Wo1LdlgNjF1o49HU/SHRe2uaRd8+FtqTjuZ+wSReCYvxIlDPeKrmEXHxQYZITgaksTg7K8xxT/zrc6kxURr/DhUnxc2A5DFU0uPiHLGbSf5tWPcMowsWTvAU= \nJul 12 15:03:25 pipeline.log:  X-Amz-Target: Kinesis_20131202.GetRecords \nJul 12 15:03:25 pipeline.log:  Accept-Encoding: gzip \nJul 12 15:03:25 pipeline.log: {\"Limit\":10000,\"ShardIterator\":\"AAAAAAAAAAFD6ICsKC0nix2BvU9x9UTtQ/1sPPXhNCsJy9D3xmvzU9l/qw2WTs5QNTwNgQqZ84x+HSHSmNq0F0wVCXyGz9KGp6BSBsD+kGaznRa1FRuVFkmJKXA7QHQ4618ksAgkIzCplHWqH09qNSmtIVErLoX9jVROlugEMqMqx3ZYfN6uDGbORaAhtdHR/KAU54+PxVBPtV9wPKWxxTnTODfdDuNuv7g1EOAJPdfcB5N+TzZ4AA==\"}\nJul 12 15:03:25 pipeline.log:  -----------------------------------------------------\nJul 12 15:03:25 pipeline.log:  2016/07/12 13:03:25 DEBUG: Response kinesis/GetRecords Details:\nJul 12 15:03:25 pipeline.log:  ---[ RESPONSE ]--------------------------------------\nJul 12 15:03:25 pipeline.log:  HTTP/1.1 200 OK \nJul 12 15:03:25 pipeline.log:  Content-Length: 417677 \nJul 12 15:03:25 pipeline.log:  Content-Type: application/x-amz-json-1.1 \nJul 12 15:03:25 pipeline.log:  Date: Tue, 12 Jul 2016 13:03:24 GMT \nJul 12 15:03:25 pipeline.log:  Server: Apache-Coyote/1.1 \nJul 12 15:03:26 pipeline.log:  X-Amz-Id-2: Xp32ChP4khmreGN5rivkJljIOd7MNUVFe06m6hXywQ81NVM3f5QNesBrQ6ZigkrJXGguKMBbQEPOHGJIVNR4AMNuC7+52HVS10tEPVbv1TE= \nJul 12 15:03:26 pipeline.log:  X-Amzn-Requestid: efca0e47-2989-1995-a034-e29a0e72b502 \nJul 12 15:03:26 pipeline.log:  -----------------------------------------------------\nJul 12 15:03:26 pipeline.log:  Finished GetRecords Send, took 60.192006ms\nJul 12 15:03:26 pipeline.log:  Started GetRecords Unmarshal, took 60.240635ms\nJul 12 15:03:26 pipeline.log:  2016/07/12 13:03:25 DEBUG: read 512 bytes, took 25.831982ms\nJul 12 15:03:26 pipeline.log:  2016/07/12 13:03:25 DEBUG: read 1024 bytes, took 11.141\u00b5s\nJul 12 15:03:26 pipeline.log:  2016/07/12 13:03:25 DEBUG: read 2048 bytes, took 654ns\nJul 12 15:03:26 pipeline.log:  2016/07/12 13:03:25 DEBUG: read 512 bytes, took 556ns\nJul 12 15:03:26 pipeline.log:  2016/07/12 13:03:25 DEBUG: read 3584 bytes, took 22.321\u00b5s\nJul 12 15:03:26 pipeline.log:  2016/07/12 13:03:25 DEBUG: read 512 bytes, took 714ns\nJul 12 15:03:26 pipeline.log:  2016/07/12 13:03:25 DEBUG: read 7680 bytes, took 2.036\u00b5s\nJul 12 15:03:26 pipeline.log:  2016/07/12 13:03:25 DEBUG: read 512 bytes, took 1.148\u00b5s\nJul 12 15:03:26 pipeline.log:  2016/07/12 13:03:25 DEBUG: read 697 bytes, took 29.214\u00b5s\nJul 12 15:03:26 pipeline.log:  2016/07/12 13:03:25 DEBUG: read 15175 bytes, took 8.580319ms\nJul 12 15:03:26 pipeline.log:  2016/07/12 13:03:25 DEBUG: read 1209 bytes, took 2.009\u00b5s\nJul 12 15:03:26 pipeline.log:  2016/07/12 13:03:25 DEBUG: read 1024 bytes, took 12.625\u00b5s\nJul 12 15:03:26 pipeline.log:  2016/07/12 13:03:25 DEBUG: read 16384 bytes, took 8.423524ms\nJul 12 15:03:26 pipeline.log:  2016/07/12 13:03:25 DEBUG: read 1024 bytes, took 42.287\u00b5s\nJul 12 15:03:26 pipeline.log:  2016/07/12 13:03:26 DEBUG: read 13127 bytes, took 8.190222ms\nJul 12 15:03:26 pipeline.log:  2016/07/12 13:03:26 DEBUG: read 3257 bytes, took 2.653\u00b5s\nJul 12 15:03:26 pipeline.log:  2016/07/12 13:03:26 DEBUG: read 1024 bytes, took 14.178\u00b5s\nJul 12 15:03:26 pipeline.log:  2016/07/12 13:03:26 DEBUG: read 16384 bytes, took 165.79\u00b5s\nJul 12 15:03:26 pipeline.log:  2016/07/12 13:03:26 DEBUG: read 1024 bytes, took 16.645\u00b5s\nJul 12 15:03:26 pipeline.log:  2016/07/12 13:03:26 DEBUG: read 16384 bytes, took 8.220345ms\nJul 12 15:03:26 pipeline.log:  2016/07/12 13:03:26 DEBUG: read 1024 bytes, took 13.322\u00b5s\nJul 12 15:03:27 pipeline.log:  2016/07/12 13:03:27 DEBUG: read 16384 bytes, took 1.473254589s\nJul 12 15:03:27 pipeline.log:  2016/07/12 13:03:27 DEBUG: read 1024 bytes, took 16.335\u00b5s\nJul 12 15:03:52 pipeline.log:  2016/07/12 13:03:52 DEBUG: read 9031 bytes, took 25.250034682s\nJul 12 15:03:52 pipeline.log:  2016/07/12 13:03:52 DEBUG: read 7353 bytes, took 4.84\u00b5s\nJul 12 15:04:19 pipeline.log:  2016/07/12 13:04:19 DEBUG: read 1024 bytes, took 26.959723302s\nJul 12 15:15:02 pipeline.log:  2016/07/12 13:15:02 DEBUG: read 16384 bytes, took 10m43.033927967s\nJul 12 15:15:02 pipeline.log:  2016/07/12 13:15:02 DEBUG: read 1024 bytes, took 17.575\u00b5s\nJul 12 15:27:05 pipeline.log:  2016/07/12 13:27:04 DEBUG: read 1024 bytes, took 17.762\u00b5s\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 16384 bytes, took 2m0.319726961s\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 1024 bytes, took 18.595\u00b5s\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 16384 bytes, took 124.755\u00b5s\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 1024 bytes, took 11.984\u00b5s\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 16384 bytes, took 573.078\u00b5s\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 1024 bytes, took 12.093\u00b5s\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 16384 bytes, took 3.029766ms\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 1024 bytes, took 16.442\u00b5s\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 16384 bytes, took 1.670321ms\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 1024 bytes, took 18.579\u00b5s\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 839 bytes, took 1.671051ms\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 3257 bytes, took 1.754\u00b5s\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 12288 bytes, took 2.942\u00b5s\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 1024 bytes, took 35.459\u00b5s\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 16384 bytes, took 929.825\u00b5s\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 1024 bytes, took 23.253\u00b5s\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 16384 bytes, took 1.488066ms\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 1024 bytes, took 21.32\u00b5s\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 16384 bytes, took 654.209\u00b5s\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 1024 bytes, took 19.064\u00b5s\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 16384 bytes, took 1.310997ms\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 1024 bytes, took 43.762\u00b5s\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 16384 bytes, took 648.015\u00b5s\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 1024 bytes, took 20.848\u00b5s\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 16384 bytes, took 644.039\u00b5s\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 1024 bytes, took 31.212\u00b5s\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 16384 bytes, took 500.04\u00b5s\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 1024 bytes, took 45.292\u00b5s\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 16384 bytes, took 360.311\u00b5s\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 1024 bytes, took 18.789\u00b5s\nJul 12 15:29:05 pipeline.log:  2016/07/12 13:29:04 DEBUG: read 212 bytes, took 99.948\u00b5s\nJul 12 15:29:05 pipeline.log:  Finished GetRecords body read, took 25m39.11891238s\nJul 12 15:29:05 pipeline.log:  Finished GetRecords Unmarshal, took 25m39.119042824s\nJul 12 15:29:05 pipeline.log:  Finished GetRecords request, 67 records from shard 1, took 25m39.11912334s\nJul 12 15:29:05 pipeline.log:  GetRecords took more than 30 seconds!\n. Thanks, let me know what they come up with!\nIn the mean while I'm implementing client side retries on the GetRecords failures and is adding a timeout of a minute by setting my own http.Client.\n. ",
    "dmac": "Sure, the length of tokens is 3 and the last token is nil (corresponding to the ListResourceRecordSetsOutput.NextRecordIdentifier, so i == 2).\nI think what happens is in r.nextPageTokens(), the tokens slice is initialized to the length of r.Operation.OutputTokens, but the if statement can prevent some of the tokens from being populated, leaving them nil, which produces the panic downstream.\nhttps://github.com/aws/aws-sdk-go/blob/496a3ed061799c7b39384e47424e556af575c71a/aws/request.go#L257-L265\nHappy to work with you to debug further if it would be helpful.\n. Thanks for the clarification @lsegal. However, the behavior of clearing a value at a given path sounds potentially confusing to me, because it would change the contract of the function from always setting the pointed-to value of a pointer, to sometimes setting the value and sometimes setting the pointer itself.\n``` go\ntype S struct {\n        Name *string\n}\nvar s S\nstr := \"bar\"\n// Current behavior\nSetValueAtAnyPath(s, \"Name\", \"foo\") // equivalent to *s.Name = \"foo\"\nSetValueAtAnyPath(s, \"Name\", nil)   // panics\nSetValueAtAnyPath(s, \"Name\", &str)  // panics\n// Proposed behavior\nSetValueAtAnyPath(s, \"Name\", \"foo\") // equivalent to *s.Name = \"foo\"\nSetValueAtAnyPath(s, \"Name\", nil)   // equivalent to s.Name = nil\nSetValueAtAnyPath(s, \"Name\", &str)  // ?\n```\nI think this complexity stems from the fact that rValuesAtPath returns the pointed-to values rather than the pointers themselves, and will automatically create a value if the pointer is nil when called by the SetValueAt*Path functions.\n. ",
    "jehiah": "Reference docstrings for GetObjectOutput.ETag\n. Thanks for the quick reply. I was using v0.6.4 tag. I'll work on a script to reproduce and will check back in soon. \n. me culpa! this turns out to be an error on my side.\n. ",
    "bkeroackdsc": "Thanks for explaining some of the rationale behind the decisions. It brings up another point about the pervasive use of pointers in this library--any insight into why that was done? It makes using this library pretty cumbersome.\n. @jasdel on further investigation, it turns out that I was base64-encoding the data twice (gzip -> b64 -> b64) which could explain the failure (SSH never came up on the instance).\n. Meh. My mistake.\n. ",
    "porjo": "\nSome code review should be done to make this library more Go-community friendly before freezing the API.\n\n+1 to this. I've just had a quick look over the API for the first time and it feels bloated. Just to take one example cloudformation.CreateStackInput vs cloudformation.UpdateStackInput are virtually identical. Is that duplication really necessary?\n. ",
    "andytzeng": "Thanks for promptly fix the case. It works well for me at least one week. :)\n. ",
    "gaul": "@jjeffery aws-sdk-go should include an optional v2 signer.  Some third-party S3 implementations like Ceph and S3Proxy only support v2 signing.\n. @jjeffery I think you should leave the code as-is.  Once upstream merges this I will submit a PR for configurable S3 v2 signing.\n. I have started work adapting the SimpleDB v2 signer for S3.  However, the signing methods differ between the two providers, with SimpleDB using HmacSHA256 and S3 using HmacSHA1 and signing different headers in the request.\n/cc @kahing @jjeffery \n. Sorry I have abandoned working on this as I have added v4 signing to S3Proxy.  If anyone else takes this issue up, note that the S3 v2 signer is slightly different than the SimpleDB signer.\n. ",
    "DamonYellow": "Such a good news!\nAnd could pls show us a code example to implement the request using V2 signing? @jjeffery . so there is no solution yet? so sad.... correct, this is a object storage service of our company with S2 signature, for python, they can use boto3 library to connect easily.... ",
    "phstc": "Awesome! Tks :beers: \n. @lsegal not sure if it's working with dynamodb-local. \nIt's returning this error:\nbash\nstartingRequestError: send request failed\ncaused by: Post http://127.0.0.1:8080/: dial tcp 127.0.0.1:8080: connection refused\ndynamodb-local is up & running and I was able to connect to it using aws-sdk-ruby.\n``` go\npackage main\nimport (\n    \"fmt\"\n    \"github.com/aws/aws-sdk-go/aws\"\n    \"github.com/aws/aws-sdk-go/aws/credentials\"\n    \"github.com/aws/aws-sdk-go/service/dynamodb\"\n)\nfunc main() {\n    fmt.Printf(\"starting\")\ncreds := credentials.NewStaticCredentials(\"123\", \"123\", \"\")\nawsConfig := &aws.Config{\n    Credentials: creds,\n    Region:      \"us-east-1\",\n    Endpoint:    \"http://127.0.0.1:8080\",\n}\n\ndynamodbconn := dynamodb.New(awsConfig)\n\nreq := &dynamodb.DescribeTableInput{\n    TableName: aws.String(\"my_table\"),\n}\n\nresult, err := dynamodbconn.DescribeTable(req)\n\nif err != nil {\n    fmt.Printf(\"%s\", err)\n}\n\ntable := result.Table\n\n   // some code\n\nfmt.Printf(\"done\")\n\n}\n```\nIs there anything I can fix in my end? Do I need to set something else in the aws.Config?\n. \ud83d\udc34 ... I was testing against the wrong port, it should be 8000 and not 8080.\n. ",
    "Aprimit": "Hi,\nI understand the issue is about connecting to dynamodb-local. Does anyone here know how to point to dynamodb tables of different AWS account? Is it done in similar fashion as its discussed for dynamodb-local? Please explain.. Is there some wayout to avoid using dynamodb json. If yes, Is it also supoprted for Golang? Please explain how?\nThanks & Regards\nAprimit Garg. It works pretty good. Thank you.. @jasdel thanks for the quick response. Well, could you please provide an example of how to signal the marshaler to process the slice as stringset ?. @jasdel the suggested solution works for me. Appreciate your help here. Thank you \ud83d\udc4d . Hi,\nI see the issue updated with labels. Is it assigned to someone? Is there any suggested solution for this issue?\nThanks & Regards\nAprimit Garg. Hi @xibz , I believe the ExpressionAttributeValues accepts a dynamodb attribute value of string type only. Is there a way we can pass a List/StringSet i.e collection of string values to ExpressionAttributeValues?. ",
    "vincent6767": "Are the credentials need to be provided in the local DynamoDB? . @jasdel  Ah I see, thank you!. ",
    "bertabus": "@lsegal right you are. My permissions were wide open. I locked them down and couldn't upload anything at all. I was following a snippet I had found in the comments to issue #26 that suggested something similar to the code above. I have since noticed that there is a \"PutObjectRequest\" and \"PutObjectInput\". I updated my code and everything works exactly as I was expecting. Thanks\n. ",
    "kciredor": "Thanks very much for your swift and kind answer @lsegal - I'll proceed and close this issue.\n. ",
    "parkr": "Because go get doesn't allow for versioning, it'd be suuuuper dope if breaking changes were kept to a minimum. This caused a lot of frustration today. Thanks so much for your consideration.\n. Thanks, @jasdel! We'll look into moving to some external tool like godep or similar.\n. ",
    "rina-sleeping": "Yes, I use S3. \nFollowing is the code I tried.\n```\nservice := s3.New(&aws.Config{\n        Credentials: credentials.NewStaticCredentials(cq.kKEY, cq.kSECRET_KEY, \"\"),\n        Region:      kAWS_REGION,\n    })\nreq := &s3.PutObjectInput{\n            Bucket: aws.String(BUCKET_NAME),\n            Key:    aws.String( \"sample/\"),\n        }\n        _, err = service.PutObject(req)\n```\nThis creates a sample object.\nBut I want a sample folder object which is the object named \"sample/\".\n. ",
    "dlsniper": "I could replicate this consistently, at least in Terraform. Same issue happened with Go 1.5 beta 2 and a previous build from tip. Should I open an issue on Go? Maybe it's a bc break change that should be fixed before 1.5 release (I rekon I haven't had enough time to investigate this properly, I'll try to get more information in the next hours). Thank you. \n. Hi, I think this can be marked as solved as it's related to how the dependencies are fetched in Terraform not in AWS, see https://github.com/hashicorp/terraform/pull/2900 for futher details. The PR itself, build against tip however is good I guess to help in the future.\n. Hi, thanks for the awesome fast reply.\nIn the case of @juRiii the problem is that even if this project vendors the code, the tool he's using is not aware of the the fact that the library already has the dependencies vendored and it tries to copy them from GOPATH (which will fail since they are not there, would be interesting to know how he go getted the package, by doing go get aws-sdk-go or go get aws-sdk-go/...).\nMaybe a simple fix could be to tell the users to do the later, with appending /... to the package when go getting it (needs to be tested) (cc @juRiii, see this).\nOn the large side of the issues, this is still pretty much a yet to be solved problem in Go but the general consensus is to not vendor the dependencies in case the project is a library and not an application. Fortunately enough, you are doing the nice thing and not also rewriting the import path (which a bigger problem). Of course, you could use a tool which has a file in which it saves the revision for each dependency and have only that added to the repository (but that means a big transition and forcing users to use that specific tool + the tool should be aware of the vendor/aws/vendor revisions)\nThis should also be asked / discussed on the mailing list / with the Go team as we can either have a community driven solution, like @StabbyCutyou mentioned vendor-spec, or we can have the Go team creating a standard for this.\n. @jasdel I completely agree with your points.\nI would suggest raising the issue with the Go team on the mailing list https://groups.google.com/d/forum/golang-dev to see what's their opinion on this as come Go 1.7 the current solution will be here to stay (for better or worse) and feedback like this could help them out to understand the issue and either have them or the community come to an agreement on what needs to be done moving forward. I've raised a issue as well, unrelated to this, and there are several other threads with different problems as well, most recently (and somewhat similar) https://groups.google.com/forum/#!topic/golang-dev/4FfTBfN2YaI\nThank you!\n. I see.\nTo give you a concrete case:\nec2.Instance.LaunchTime\nelb.LoadBalancerDescription.CreatedTime\ndynamodb.TableDescription.CreationDateTime\nBoth express the same time: when was the resource created.\nI haven't had a chance to look into how you (un)marshal the resources but the marshaling name can be different than how we name the fields in the structs. Also, the naming of the structs is a bit strange (ec2.Instance and elb.LoadBalancerDescription) even if they come from different parts of the API.\nThis is how I would see the things named:\nec2.Instance.CreatedAt `json:\"LaunchTime\"`\nelb.Instance.CreatedAt `json:\"CreatedTime\"`\ndynamodb.Table.CreatedAt `json:\"CreationDateTime\"`\nThis would introduce massive breaking code everywhere, but hopefully it could be migrated by using an automated fix tool, like the Go team used to do (iir there's a tool from them which supports this kind of usage).\nAt least then the user experience should be greatly improved by having standard, expected / common names across packages.\nThank you.\nEDIT I've updated this with the DynamoDB example\n. There are also issues on how one would expect the to list resources (sometimes you have to use List* other times you use Describe*) so it's not only field names (but those are a start).\n. Well I can work on a patch if you want, but I don't understand why this was closed. It is still a valid issue. Or am I missing something? Thanks.\n. Unfortunately: Your account is not ready for posting messages yet. Please try again later.\n. To give a bit more context, I've tried posting on the forum as indeed it doesn't seem to be a way to get this from the API but for some reason I couldn't finish the action. The error is the one above, the message was:\n```\nCreation date for Streams and Shards in the API\nHi,\nI was using the Go SDK for AWS and I tried to get the Creation date for Kinesis Streams and Shards. Unfortunately, as per the reply in the [issue|https://github.com/aws/aws-sdk-go/issues/664] it seems this is not available in the DescribeStream call from AWS itself, http://docs.aws.amazon.com/kinesis/latest/APIReference/API_DescribeStream.html\nI would really appreciate if this would be fixed fast (especially since it seems like a pretty easy fix and it's not breaking the API).\n```\n. @jasdel thank you for your patience with this, I appreciate the position you are in and the fact that my out of the blue PR is rather aggressive.\nTo give you more details, I was testing out a DynamoDB client in both NodeJS and Go. The NodeJS client, without any optimizations, can do about 4.5k req/s while the Go client does 1.5k req/s at best, This happens on a c4.2xlarge instance nonetheless. What's more frustrating is that the latency in the 99% percentile for NodeJS is well under the one of the Go client and a lot more stable. And here's the code: https://play.golang.org/p/YeezG9sV9T (the table contains more keys but that's just a simple poc, the schema for the poc is: id string; status: string ) there's really not much that I can do on my side (userland for the SDK) to make it faster.\nLike @vburenin said here: https://github.com/aws/aws-sdk-go/pull/725#discussion_r67594884 in his case, the function which does the copying of handlers takes 3 ms (not to mention all the garbage it creates in the process)!!! That's a lot! I can query and reply from RDS / Postgres in < 2 ms using jsonb.\nBasically the whole SDK can not be used in any high performance environment when it comes to DynamoDB. And by the looks of it, all the other operations/services where you'd want high performance suffer from these issues.\nYes, it might not matter when uploading to S3, or when creating an EC2 instance, but when I need to get data asap from Dynamo and my app spends more time in the SDK than on the network, something is definitely wrong.\nTo further understand the issue this: [I'll update with a profile over 15 minutes] . Notice how the network portion is 1/3 the whole time being displayed. Even if you remove the client.Do part, then the whole non-network bit is still 2x slower than the network. And that's to fetch a single item of two fields.\nI've initially started this hoping that I'll be able to improve the speed without breaking anything in the userland but the more I go and do changes, the more I bump into issues and assumptions from the SDK. Simply the fact that there's a Request factory function (because I can't name it in a better way) speaks volumes on how much work needs to be done to get this in a decent state.\nI'm currently at a stage where I'm torn into multiple options:\n- scrap everything and write a better SDK myself\n- abandon all hope to use the AWS SDK for any serious work any time soon and just roll my own functions for the API calls I need (partially same as previous)\n- use another language to do high-performance work with AWS services\nThe interface{} / reflect dance is doing so much magic that I don't think refactoring this is a viable option anymore. I was looking at this initially, https://github.com/aws/aws-sdk-go/issues/712, and I've reverted the SDK to a time before the https://github.com/aws/aws-sdk-go/issues/642 was merged, I get 10-15% speed improvements. Yes, many other things have changed meanwhile, and it might not be only from that specific change, but simply because a user can't sanitize its data, everybody suffers now. Or the issue with the white spaces at the end of the address, thus everybody gets addresses sanitized now. I can supply the address correctly myself, thank you, so do 99% of the programmers.\nFinally, speaking about the SDK architecture, there are many many many places where the SDK should simply avoid doing extra work on the hot-path of a request but this doesn't appear to be the case. For example, using named handlers so that they can be removed. Why would those operations be ever needed? Or why do we need those to begin with? Is it ever the case when adding a new middleware handler won't need that middleware to be written then called upon? More concrete, from the code sending the requests: https://sourcegraph.com/github.com/aws/aws-sdk-go/-/def/GoPackage/github.com/aws/aws-sdk-go/aws/request/-/copyHTTPRequest How did we even come to this? Why would there even be a need to do this? I understand there's the so called case where users might want to provide some handlers to take care of some specific issues. Are we sure is this the best course of action we have? Can we do better? Can we tell the user to simply wrap the call to the said service in a function on it's own and be done with vs breaking things out for the other users? That would seem like a great alternative to me imho.\nThe worse is that I'm not the only one which faces this issue, the whole community is suffering from this and the AWS SDK is always given as an example of how not to do things. And that's really sad.\nI appreciate your efforts, as well as the team behind this, I really do, I can only imagine the effort that was put behind this, but this is Go, not Java, Javascript, C, C++, D, Python or even Ruby, we simply do things differently here and we care more about readability and performance than copy-pasting an extra line of code to inject some convenience. It might feel similar to developers from other languages coming to Go, but for gophers this is wrong. It's just like the apps which don't want to go respect the design guidelines for mobile apps because they want a consistent look'n'feel for their users, which funny enough will be more confused since their whole OS/other apps look'n'feel are different.\nI'm sorry that I'm giving you such a bad time/feedback, but that's simply the reality as it is now.\n. Hi, sorry for the delayed reply. For many of the usecases yes, this might make no difference. I do think that for high perf environments, even a few ns shaved of while generating the key would be nice but up to you. I can rebase if you want or close this. Please let me know which direction you'd prefer. Thank you.\n. @jasdel thanks for the speedy reply. Can you please take this into account in the said refactoring? Thank you.\n. Will do, thanks!\n. I could also wait for that one to be merged so that there's a easy way to remember about it.\n. Thank you!\n. On a different note, the same ~30% performance gains can be observed when using a NodeJS application and hardcoded credentials, please let your colleagues @ AWS / NodeJS sdk team know about this. Thank you.\n. > For Static vs other credential sources I'd like to learn more about the alternate credential sources the test application was using. In generally Id expect static to always perform better because there is no IO file(shared credentials file) or network request (EC2 roles) involved for retrieving the credentials. Though this hit should only occur once within a large window. For shared credentials this will be once for the lifetime of the session, and EC2 Role for the duration of the role's credentials. Which is usually at least an hour.\nThat was my expectation as well, but I can consistently see a bump from 1.1k (ec2 metadata) to 1.4k (hardcoded) rps when running a simple GetItem request to Dynamo which is rather strange. I would expect only the first request, which is outside of the benchmark anyway to have this issue.\nI've also noticed that if I do a heavy test against Dynamo on a c4.large instance, then I start to get things like the response body not being closed and file descriptors leaks (so I guess request leak) when using the metadata provider but not with the static hardcoded credentials.\nI'll continue to investigate this as the difference between doing a dynamodb and not doing it is huge (Dynamo replies w/ an average of <4ms)\n. I think one option that would allow the SDK to behave as close as possible to a \"Go way\" of doing this would be to have a middleware function in the SDK that the users can use in order to allow the SDK to control the values from the Context. \nThen, by overwriting the context of the request and calling the next step in the chain, you would require the users to pass that context or a descended of it.\nThe SDK could also provide a function to give the user of the context back the value of trace header so that the it can be used in places like custom tracing in \"userland\", calls to other services, etc.\nIf the users don't want to use the custom middleware then it should be their responsibility to make sure the corect value are sent in the context at the time of the call (the SDK could provide a helper method to inject the X-Ray trace in the context maybe.\nWhat do you think?. I think this needs to be reopened.\nI understand the need to have this in a separate repo, and I agree with it, but as it is right now, the SDK is unusable.\nI've created this repository https://github.com/dlsniper/rontgen/blob/master/rontgen.go as we've previously discussed, but it turns out that the SDK does not have a clear way to provide any of the elements needed. I also cannot find a way provided by the SDK to send the data to the local running daemon, which the documentation about X-Ray clearly makes it as a desirable use-case.\nI'm happy to talk with the X-Ray team / you (as SDK maintainers) to see how this could be solved, but I think that all of this should be rather part of the SDK not a separate SDK for Go and X-Ray. That way the current SDK could be instrumented and also provide wrappers for http handlers as well as http clients that can be instrumented with X-Ray.. Cool, thank you for such a quick reply!. Awesome! Thank you. My take on this is the following:\n- the function itself should have been returning a *Request, error pair from the beginning. It doesn't do so so I'd rather not break it yet.\n- regarding the issue in #689, I don't see why this would make any difference to be honest. In both cases if there's an error there, the API call can't continue. If the developer can't provide a proper URL then it's ok for this to crash. We are also pretty close to Go 1.7 so there should be little to no incentive to support Go 1.4 (or really anything under 1.6 at this stage).\n. Under which condition do you see this happening? I've searched the SDK and all I could find was that the waiter waits for either a string or an int. I'm rather tempted to split this into two fields actually, ExpectString and ExpectInt.\n. Like @vburenin said, this happens a lot.\nI believe this is a good case where the so called convenience of user trying to add a metric or a logging as you say should be taken away and have the user add the code for monitoring manually.\nIn @vburenin's case, this alone takes 3ms to happen for every single time. It's a third of the \"single digit millisecond\" \"feature\" of DynamoDB.\n. Good point, my math is wrong in the morning.\n. Good point.\nChanged, thanks.\n. I suggest renaming this to something more obvious like newSessionFallback or similar.\n. I would not introduce this at all, if the client consumes the result of session.Build() then it should handle the error accordingly, be it with panic() or other means.\n. Since the previous behavior is not a panic, at least as far as I can see, I wouldn't introduce it here either. Old behavior should be used until such time a 2.0 bc break version is done in which case this should be changed to return *Session, error.\n. ",
    "dstokes": "@jasdel good point. i noticed it was already declared in some api schemas but handling it generically would resolve it oce and for all, i imagine\n. ",
    "jitcompile": "I updated the version of sdk from 0.7.0 to 0.7.1 and it seems to work. Weird. Also it could be a combination of issues due to for-range and pointer of values. I've updated to 0.7.1 and using aws.String implementation, it works as expected. Thanks. I'm closing the issue.\n. Hi @jasdel,\nI've tried to log the http body and here's the printed message. The last line is from my own logging where I log the *aws.config.region value to screen.\n```\n2015/09/16 13:59:58 DEBUG: Request s3/GetBucketLocation Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nGET http://test-sydney-files.s3.amazonaws.com/?location= HTTP/1.1\nHost: test-sydney-files.s3.amazonaws.com\nUser-Agent: aws-sdk-go/0.9.6\nAuthorization: AWS4-HMAC-SHA256 Credential=xxx/20150916/us-east\n-1/s3/aws4_request, SignedHeaders=host;x-amz-date, Signature=4b232a21998cd4e6882\nec59b5e93726a956ffb536987f8352dfdb7dbb40888ec\nX-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b78\n52b855\nX-Amz-Date: 20150916T125958Z\nAccept-Encoding: gzip\n\n2015/09/16 14:00:00 DEBUG: Response s3/GetBucketLocation Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 400 Bad Request\nConnection: close\nTransfer-Encoding: chunked\nContent-Type: application/xml\nDate: Wed, 16 Sep 2015 13:01:36 GMT\nServer: AmazonS3\nX-Amz-Id-2: 0MT1cYHUfyZBuoI0nRHyphzT9dB3WIij5ywRuacXhaCDjKjrprNa5QaPxkl8zkEjruwo\nRsKIUm4=\nX-Amz-Request-Id: 34964AD6A7A64A4A\n177\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\nAuthorizationHeaderMalformedThe authorization heade\nr is malformed; the region 'us-east-1' is wrong; expecting 'ap-southeast-2'ap-southeast-234964AD6A7A64A4A0MT1cYHUfyZBuoI0nRHyphzT9dB3WIij5ywRuacXhaCDjKjrprNa5QaPxkl8zkEjruwoRsKIUm4=<\n/HostId>\n0\n\nClient configuration region:  us-east-1\n```\n. Thanks @jasdel for confirming the problem and the workaround.\nI've a ugly hack in my code where I'm parsing the error using regex and getting the correct bucket location.  I'll use the workaround that you suggested.\nI'll wait for the final fix. Thank you and fantastic work with the sdk :+1: \n. ",
    "hngkr": "I'm still thinking in a SQL fashion - explicit schemas isn't a thing in DynamoDB.\nGot this helpful pointer to the DynamoDB DataModel\n. ",
    "wari": "That actually works (on my test code) and it's a lot cleaner than the confusing JavaSDK example, due to the ceremony around creating expiry information.\nI missed GetRequestObject() because it's way down the API list when sorted by name.\nThanks @lsegal \n. ",
    "jedi4ever": "Thanks cristim! You put me on the right track with the opaque trick.\nI've had to add the raw_query too\n```\n  fileToUpload := \"a.ipa\"\n  file, err := os.Open(fileToUpload)\nif err != nil {\n    fmt.Println(err)\n    os.Exit(1)\n  }\ndefer file.Close()\nfileInfo, _ := file.Stat()\n  var fileSize int64 = fileInfo.Size()\nbuffer := make([]byte, fileSize)\n// read file content to buffer\n  file.Read(buffer)\nfileBytes := bytes.NewReader(buffer) // convert to io.ReadSeeker type\nreq, err := http.NewRequest(\"PUT\", upload_url, fileBytes)\n// Splitting the url by string function because golang URL parsing tries to be too clever\n strippedUrl := strings.Split(strings.Replace(upload_url, \"https://prod-us-west-2-uploads.s3-us-west-2.amazonaws.com/\", \"/\", -1), \"?\")\n// The actual path\n req.URL.Opaque = strippedUrl[0]\n// the part with AWS_KEY , Expire, ...\n req.URL.RawQuery = strippedUrl[1]\nreq.Header.Set(\"Content-Type\", \"application/octet-stream\")\n req.Header.Add(\"Content-Length\", strconv.FormatInt(fileSize, 10))\nclient := &http.Client{}\nresp, err := client.Do(req)\n\nif err != nil {\n    fmt.Println(\"Failed to set Upload To Devicefarm\", err.Error())\n}\n\ndefer resp.Body.Close()\n\n```\n. For more info on using the devicefarm API - I've made a devicefarm-cli.\nThe upload part can be found at\nhttps://github.com/jedi4ever/devicefarm-cli/blob/master/devicefarm-cli.go#L443\n. ",
    "hopkinsth": "Not sure how much of a priority this is, but it would be nice to have support for it to automate Lambda + API gateway in particular.\nFor those looking for a (temporary) alternative, you might want to check out halgo. I don't yet know if it will do all that you need outside signing requests to Amazon, so YMMV!\n. Excellent, @jasdel! Hoping to build this into an application ASAP. Thanks!\n. ",
    "swenson": "Some of our dependencies still use awslabs. :(\n. Thanks. I think it might just be easier to submit a pull request to update our dependency's dependencies. :)\n. ",
    "masonoise": "Good thinking -- I can confirm that it works with that removed. Thank you! I would recommend, though, a change to make it work with profile in there, because that is legal according to the AWS docs and it does work with their CLI tool (who knows why, but it does). It's strange because their docs show it both ways on this page: http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-multiple-profiles\nIn any case, I'm unblocked, and again thanks very much. Just the sort of thing I might have stared at for hours without seeing it.\n. Ok, I read more details and I see. It's the difference between the config file -- which is what I've got set up -- and the credentials file, which is what the sdk is looking for. Would be great to have it accept either one, but if not then this is more of a documentation note than anything else, I guess.\n. Terrific, looks great. Thanks!\n. ",
    "davidsonff": "Thanks!!! That fixed it! Also, I was not using base64 to encode the ciphers... Now I just need to figure out how the encryption actually works!!!\n. ",
    "sjwhyte": "@jasdel I was using the ec2 service.\n. @jasdel I believe that I see the issue in my code that was causing the problem. I will close this issue. Thanks for your help.\n. ",
    "SergeyTsalkov": "Guys, thanks for the quick and thoughtful responses! Yes, my complaint is entirely about DreamObjects compatibility. Clearly I was wrong about HTTP/1.1, so thanks for that lesson!\nI'd hoped that fixing the problem on this end would be faster and easier than getting DreamObjects changed, but it sounds like this would be impractical.\n. ",
    "pilt": "Glad to be of help. Thanks for the fix @jasdel!\n. ",
    "chadgrant": "Still busted\n. Thanks for getting back to me.\nI'm trying to use it on ecs from within docker and getting a panic, using WithCredentials(credentials.NewCredentials(&ec2rolecreds.EC2RoleProvider{})) \nTrying to download a file from s3 that the parent ec2 machine's IAM role has access to.\nusing the scratch docker image described here: http://blog.xebia.com/create-the-smallest-possible-docker-container/\n. I'm using latest, deploying in a scratch docker image, using this pattern: http://blog.xebia.com/create-the-smallest-possible-docker-container/\nI changed some html files and redeployed. Something must have changed in my build chain. :/\n. thanks jasdel, that did fix my issue\n. ",
    "gyuho": "Thanks a lot for the migration tool!\n. ",
    "aibou": "I have a same problem.\nI think this statement affects adversely: \nhttps://github.com/aws/aws-sdk-go/blob/e9380f7f9dbc347719a748187d98d440dfcf27cd/aws/credentials/ec2rolecreds/ec2_role_provider.go#L162\nThe EC2 instance meta-data returns this response on my instance (as a sample):\n$ curl http://169.254.169.254/latest/meta-data/iam/security-credentials/[role-name]\n{\n  \"Code\" : \"Success\",\n  \"LastUpdated\" : \"2015-08-13T03:03:43Z\",\n  \"Type\" : \"AWS-HMAC\",\n  \"AccessKeyId\" : \"[key]\",\n  \"SecretAccessKey\" : \"[secret]\",\n  \"Token\" : \"[token]\",\n  \"Expiration\" : \"2015-08-13T09:35:12Z\"\n}\nCurrently, requestCred function calls meta-data url and returns an empty ec2RoleCredRespBody struct as error when ec2RoleCredRespBody.Code is not blank string.\nBut the attribute is not blank on that response.\nCould you confirm it?\n. ",
    "ajaybc": "@jasdel \nI tried updating the SDK even then it fails. \nNext I tried the code snippet you suggested. I got an error in the line resp, err := cog.GetOpenIDTokenForDeveloperIdentity(params) which just said 'Success : ' and resp was nil\n. @jasdel It is working fine now. Thank you :)\n. ",
    "mariusgrigaitis": "I confirm that @jasdel commit fixes the problem with \"NoCredentialProviders: no valid providers in chain\"\n. ",
    "linusthe3rd": "For anyone that lands on this issue, is using some sort of package manager (e.g. godep) to manage the version they are using, and also doesn't want to update to >=0.9.0 just yet, this bug only affects v0.7.4\nThis bug does not affect 0.7.3 as it was introduced in 0.7.4.\nFor the maintainers, it would be nice if the fix could be backported to the 0.7.x versions, but I totally understand if the project maintainers don't want to do that and push forward with >=0.9.\n. ",
    "glasser": "Heh, I just stumbled into this too :)\n. (Just FYI, I'm going on vacation for a few weeks \ud83c\udf1e \ud83c\udf15  so will probably fall silent here soon, but I'll pick it up when I get back :) ). So, I tried that originally \u2014 I hate writing so much string parsing!\nBut here's the question: what should nsecs be for 1501917228.309?  I'd assume it should be 309000000.  However, if you just parse as a float, Modf, multiply by 1e9, and convert to integer, you get 309000015 due to floating-point precision issues: https://play.golang.org/p/ytEH62Alfp\nI alluded to this in the PR description (though I incorrectly said \"rounding\" rather than precision) but should have been more explicit.\nI'm not sure what the best step from here is:\n- Ignore precision issues\n- Do normal float parsing but also count the number of post-decimal digits and round based on that?\n- Apply the logic here if ^\\d+(\\.\\d+)?$ and do something different if exponent is applied?\n- Extend the logic here to handle exponents (though it's not super clear to me what that should mean)?\nThoughts?\nAlso I guess I should handle a + prefix (and a - too, for pre-1970 times?). Ah, I missed that \u2014\u00a0I had seen the note in CONTRIBUTING.md about model but didn't realize this file was generated from there, though it makes sense in retrospect. Maybe the CONTRIBUTING.md note could mention the other files that are generated?. The code you have there is not good, because float->int conversion rounds towards zero, so if the float approximation of the input millisecond is slightly less than the written value, this will subtract a whole ms, which isn't great.  https://play.golang.org/p/Ud_u9sFcS2\nBut in general, the idea of rounding to ms seems reasonable to me.  You could use this function which is likely to be math.Round in Go 1.10\nI don't suppose that (given that this library specifically targets AWS) there's something in the AWS protocol specs that specifies what precision is used for timestamps in each service?. OK, I've now read https://www.cockroachlabs.com/blog/rounding-implementations-in-go/ which fills me with fear.\nI think we should either use the function which will be math.Round in Go 1.10 (copied into the codebase), which requires a multiply/divide by 1e3 step, or https://github.com/montanaflynn/stats/blob/41c34e4914ec3c05d485e564d9028d8861d5d9ad/round.go#L5 which does \"to 3 places\" built-in.\nIs it OK to vendor in the montanaflynn file? Or just copy that one file (it's MIT Licensed)?. ",
    "asemt": "Hi Jason,\nthanks for updating and clarifying the wiki page!\nIt's important to have good and up-to-date documentation (especially for new users of the SDK), that's why I raised this issue in the first place.\nBest,\nAndreas\n. ",
    "nlamirault": "OK.\nNo i don't set AccountId variable. \ndo you know why it works with call to ListVaults and not to DescribeVault ?\n. Strange it works now. Sorry for noise.\nBut i don't set AccountId. Only VaultName\n. ",
    "mattes": "Hmm. I think I understand. Wouldn't it be nicer to use interfaces for things like that though? Like:\nfunc foo(stringOrNil interface{}) {\n  ...\n}\n. I've been working with the API today and I understand the reason behind using pointers. Honestly, I am not a fan though. It creates very weird constructs of switching back and forth between pointers while integrating with the rest of the code base. Debugging isn't fun neither. I am not saying it's hard. It's just not very aesthetic. Which I think is definitely an argument. :-) \nI still think something like map[string]interface{} would work for inputs. Yes, it would remove compiler type checking, but I think it's an reasonable approach in this situation. Return types could still be structs. Not sure how others think about this. But it seems as if I am not the only one who is a little unhappy with the current pointer approach.\n. So I guess this problem is unrelated from this SDK, but again, here is what happens:\n- Call AWS API to allocate new Elastic IP\n- API returns immediately with successful response (always works)\n- Now try to query for this (successfully) allocated Elastic IP ...\n  - Most of the time, this works.\n  - Sometimes it doesn't and AWS pretends not to know anything about this Elastic IP.\n    Via the API or even via the Web Console.\n    The AWS API will still prevent you from creating Elastic IPs over your limit though.\n- Immediately try to allocate a new Elastic IP (always works)\n. Using these versions:\ngithub.com/aws/aws-sdk-go ce51895e994693d65ab997ae48032bf13a9290b7\ngithub.com/go-ini/ini 060d7da055ba6ec5ea7a31f116332fe5efa04ce0\ngithub.com/jmespath/go-jmespath 3433f3ea46d9f8019119e7dd41274e112a2359a9\n. ",
    "omeid": "Why not use methods then?\ngo\nb := s3.NewObject(bucket, key, map[string]string{\"key\": key});\nerr := b.SetACL(\"ObjectCannedACL\");\n//Check ACL et al.\n. @jasdel Thanks for the through response. I did read through the mentioned issues and most everything discussed here.\nI don't think trying to be extra backward-compatible at the cost of a non-idiomatic API is a good trade-off; further more, if APIs changes, then shouldn't the users need update their code anyway? but if that is not the case, with vendoring now being an official experiment and few other solutions for reproducible builds becoming somewhat widespread, let people who don't want to get update to date stay in the dark. ;)\n. Well, in that case, the solution would be to use idiomatic API and let the users use a reproducible build solution like Go 1.5 Vendoring Experiment, Godep, or one of the many other.\nBackward compatibility is great, but I personally think it shouldn't cost idiomatic and clean API.\n. Well, if you want new features then you're probably already updating your code; as for security, they can always be back-ported. Too much emphasize on back-compatibility ends up with something a la PHP.\n. I think anything more than the official Go 1 compatibility promise is too much, specially if it means an arguably ugly and non idomatic API.\n. > If I have 10,000 lines of code in my existing system that rely on SDK features, and I am \"updating my code\" to add 20 lines of code that rely on new SDK features, I don't want to be liable for potentially having to upgrade and rewrite 10,020 lines of code.\nThis example is absurdly unrealistic. Just because some APIs may have breaking change doesn't mean you would have to rewrite your whole application.\n\nPHP is a successful product, I don't see a problem here. \n\nPHP Is successful despite it's inconsistencies and flaws not because of them, PHP is a success because of first-mover advantage, cheap hostings, and now survives because of big players like Facebook, wordpress, et al.\n\nThat said, if you don't like PHP, you could just as easily replace that statement with...\n\nWhile there maybe many other products that suffer from negative consequences of overzealous backward-compatibility, PHP is the most famous one that I know.\nAnyway, the matter of fact here is that the API as it stand is non-idiomatic and arguably a hack. I would have preferred if it was designed better and didn't feel like I am writing not-go.\n. @jasdel What needs to be done to get this merged? This is blocking creating read replicas in the same region with terraform :(. Thanks for the follow up.\nUnfort I am not immediately familiar with the test setup and it may take a while for me to fix it up, so I am keen to take your suggestion of adding tests before merging this in.\nCheers. ",
    "endophage": "Given the desirability of more idiomatic go (path of least surprise yada yada yada...), are there cases where the empty string, \"\", would functionally mean something different to nil?\n. Incidentally, I'm also seeing plenty of places a *string has been used where a []byte would be more appropriate (like the body of SQS Messages). Slices can be nil and that would solve your problems in some cases.\n. @jasdel JSON also uses unicode as an encoding however you'll note that Go's encoding/json, and encoding/xml packages take encoded input in the []byte type and output encoded content in the []byte type. \nUsing []byte to represent the raw form of an encoding in Go is very standard across the board. As I would guess most people are sending their own encoded content through SQS and other AWS services, you are simply forcing people to write additional, ugly typecasts by not following the convention used in the core language.\n. The allocation doesn't overly concern me, my data isn't large, but it would be nice to avoid. It's more that the chosen style of the AWS libraries is inconsistent with the core Go libraries. \nMy use case is certainly JSON contained in the MessageBody. I know I'm not alone in that. I would hazard a guess that a large majority of users of many AWS services (S3 probably being the main exception, though []byte applies even more appropriately there and the SDK appropriately uses an io.ReadCloser), are using some form of encoding that will need to be parsed/\"Unmarshalled\" when received by a consumer.\n. ",
    "stevvooe": "@jasdel \n\nThis is done because it is not possible to distinguish between a never set value from a value set to the type's Zero value. The SDK uses pointers for all primitive and struct type fields. In many cases the type's Zero value of a field has meaning, and is different from unset, and a non zero value. All API fields in Input and Output structs are pointers, because even though a field is required today, it may not be required in the future. A breaking change would be required to allow not setting the once required field.\n\nThe use of *string and others is extremely non-idiomatic, making use of this package more challenging when integrating with existing Go code. Many existing Go packages cope with this just fine and I implore you to study them.\nOther than the verbosity required, the main issue with this is the unnecessary performance penalty of the pointer redirect and the extra garbage generated.\nEffectively, to use your package, a Go developer must depart from the style commonly used throughout the standard library and every other package.\n. > Since many fields within the the API operations are optional a concept is needed to identify if the field was set to a non-zero value, not set at all, or set to the zero value for the type. When using by value there is no way to distinguish the different between set to zero value, and not set at all.\nI disagree with this premise. Zero-valued types are commonly used without adding such complexity across the Go community. I'm confused as to why this doesn't work for this project. Please examine this premise and work from there.\nCould you give some concrete examples in the API where an empty string, zero-value number or other zero-valued type needs to be distinguished? Looking at Config, as an example, not one field cannot be correctly detected via zero-valued types. A cursory look over other types shows this to be unnecessary, as well.\nTake a peak at this quote from Best practices for a new Go developer:\n\nYou\u2019ll work with the strengths of the languages more easily if you don\u2019t try to do C-like things (allocation on the heap, pointers to everything) and treat the stack and pass-by-value as cheap operations for most purposes. I\u2019ve seen big performance improvements just by simplifying heap usage and replacing it with stack usage. Go is designed to do the right thing for programs that look very straightforward, so if you feel like you\u2019re doing something clever, it\u2019s potentially working against both you and the runtime.\n\nWhile this quote isn't perfectly apropos, please examine whether or not approaching the problem this way is truly necessary. Especially when multiple experienced Go developers have suggested this might not be the right approach.\n. ",
    "yasker": "@jasdel This time it continues for around 150 seconds. I got the log:\n```\ntime=\"2015-09-01T18:41:00Z\" level=debug msg=\"Snapshot snap-ab8bdeef process 99%\" pkg=ebs\n2015/09/01 18:41:01 DEBUG: Request ec2/DescribeSnapshots Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nPOST / HTTP/1.1\nHost: ec2.us-west-2.amazonaws.com\nUser-Agent: aws-sdk-go/0.9.4rc5\nContent-Length: 176\nAuthorization: AWS4-HMAC-SHA256 Credential=xxxx/20150901/us-west-2/ec2/aws4_request, SignedHeaders=host;x-amz-date, Signature=6d4727869ec71a7700222e274d95c1947dedc21cacf8089dbae067bf162c6ab6\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\nX-Amz-Content-Sha256: 40cbfd7838e6ea080000db333ab0698a21a90db494bcac4a858123ae5f1d15b0\nX-Amz-Date: 20150901T184101Z\nAccept-Encoding: gzip\n\n2015/09/01 18:41:01 DEBUG: Response ec2/DescribeSnapshots Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 200 OK\nTransfer-Encoding: chunked\nContent-Type: text/xml;charset=UTF-8\nDate: Tue, 01 Sep 2015 18:41:06 GMT\nServer: AmazonEC2\nVary: Accept-Encoding\n\ntime=\"2015-09-01T18:41:01Z\" level=debug msg=\"Snapshot snap-ab8bdeef process 99%\" pkg=ebs\n2015/09/01 18:41:02 DEBUG: Request ec2/DescribeSnapshots Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nPOST / HTTP/1.1\nHost: ec2.us-west-2.amazonaws.com\nUser-Agent: aws-sdk-go/0.9.4rc5\nContent-Length: 176\nAuthorization: AWS4-HMAC-SHA256 Credential=xxxx/20150901/us-west-2/ec2/aws4_request, SignedHeaders=host;x-amz-date, Signature=1d98ce1590ef2816af8a8479667699b42e96c9e23134afcb843c82744dfad72f\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\nX-Amz-Content-Sha256: 40cbfd7838e6ea080000db333ab0698a21a90db494bcac4a858123ae5f1d15b0\nX-Amz-Date: 20150901T184102Z\nAccept-Encoding: gzip\n\n2015/09/01 18:41:02 DEBUG: Response ec2/DescribeSnapshots Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 200 OK\nTransfer-Encoding: chunked\nContent-Type: text/xml;charset=UTF-8\nDate: Tue, 01 Sep 2015 18:41:07 GMT\nServer: AmazonEC2\nVary: Accept-Encoding\n\n```\nAnd here is the last one and it finally success:\n```\ntime=\"2015-09-01T18:43:35Z\" level=debug msg=\"Snapshot snap-ab8bdeef process 99%\" pkg=ebs\n2015/09/01 18:43:36 DEBUG: Request ec2/DescribeSnapshots Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nPOST / HTTP/1.1\nHost: ec2.us-west-2.amazonaws.com\nUser-Agent: aws-sdk-go/0.9.4rc5\nContent-Length: 176\nAuthorization: AWS4-HMAC-SHA256 Credential=xxxx/20150901/us-west-2/ec2/aws4_request, SignedHeaders=host;x-amz-date, Signature=cfb6932f6f9df623da54e65ecca0f44e63bb66b44448e55b9103de1e77a754f2\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\nX-Amz-Content-Sha256: 40cbfd7838e6ea080000db333ab0698a21a90db494bcac4a858123ae5f1d15b0\nX-Amz-Date: 20150901T184336Z\nAccept-Encoding: gzip\n\n2015/09/01 18:43:36 DEBUG: Response ec2/DescribeSnapshots Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 200 OK\nTransfer-Encoding: chunked\nContent-Type: text/xml;charset=UTF-8\nDate: Tue, 01 Sep 2015 18:43:40 GMT\nServer: AmazonEC2\nVary: Accept-Encoding\n\ntime=\"2015-09-01T18:43:36Z\" level=debug msg=\"Created snapshot snap-ab8bdeef\" pkg=ebs\n```\nIn the meantime:\n~$ date && aws ec2 describe-snapshots --snapshot-id=snap-ab8bdeef\nTue Sep  1 18:42:01 UTC 2015\n{\n    \"Snapshots\": [\n        {\n            \"Description\": \"Test snapshot\",\n            \"Encrypted\": false,\n            \"VolumeId\": \"vol-d3844627\",\n            \"State\": \"completed\",\n            \"VolumeSize\": 1,\n            \"Progress\": \"100%\",\n            \"StartTime\": \"2015-09-01T18:41:04.000Z\",\n            \"SnapshotId\": \"snap-ab8bdeef\",\n            \"OwnerId\": \"605812595337\"\n        }\n    ]\n}\n~$ date && aws ec2 describe-snapshots --snapshot-id=snap-ab8bdeef\nTue Sep  1 18:43:05 UTC 2015\n{\n    \"Snapshots\": [\n        {\n            \"Description\": \"Test snapshot\",\n            \"Encrypted\": false,\n            \"VolumeId\": \"vol-d3844627\",\n            \"State\": \"completed\",\n            \"VolumeSize\": 1,\n            \"Progress\": \"100%\",\n            \"StartTime\": \"2015-09-01T18:41:04.000Z\",\n            \"SnapshotId\": \"snap-ab8bdeef\",\n            \"OwnerId\": \"605812595337\"\n        }\n    ]\n}\n. @jasdel BTW, it's not happening all the time, but almost definitely would happen 1 out of 5 times.\nEdited: In fact it happens 5 out of 5 times now...\n. Thanks @jasdel ! That's explained a lot, though I am not sure why it's still not working for me... I've tried to adjust interval to 5 seconds or 15 seconds, still the response won't update for minutes... I am using us-west-2, don't know if that's related. Or probably due to I was issuing too many API request recently? But only DescribeSnapshot has this problem for me now...\nDEBU[0031] Snapshot snap-2cbc626b process 0%             pkg=ebs\nDEBU[0046] Snapshot snap-2cbc626b process 0%             pkg=ebs\nDEBU[0061] Snapshot snap-2cbc626b process 0%             pkg=ebs\nDEBU[0076] Snapshot snap-2cbc626b process 0%             pkg=ebs\nDEBU[0091] Snapshot snap-2cbc626b process 0%             pkg=ebs\nDEBU[0106] Snapshot snap-2cbc626b process 0%             pkg=ebs\nDEBU[0121] Snapshot snap-2cbc626b process 0%             pkg=ebs\nDEBU[0136] Snapshot snap-2cbc626b process 0%             pkg=ebs\nDEBU[0151] Snapshot snap-2cbc626b process 0%             pkg=ebs\nDEBU[0167] Snapshot snap-2cbc626b process 0%             pkg=ebs\nDEBU[0182] Snapshot snap-2cbc626b process 0%             pkg=ebs\n. @jasdel Seems things are getting better now. Currently in my case CopySnapshot() takes long time still(with state is complete in CLI), but taking snapshot is much faster after waiting longer. Thanks!\n. @jasdel I've double checked the CopySnapshot() case, seems it's really \"Pending\" at the time. Sometime EC2 web console shows green, but refresh the website would revert it to \"Pending\" status. So that should be fine.\nThough I haven't able to figure out why the first couple of times when I tried to increase timeout it didn't work. Don't quite understand what's changed, but I would set retry time to 15s now.\nThank you! I would close this issue for now.\n. Seems the transition is like this based on what I found:\na. Attach volume:\n1. volume.state = \"Available\"\n2. volume.state = \"InUse\"\n3. attachment.state = \"Attaching\"\n4. attachment.state = \"Attached\"\nb. Detach volume:\n1. volume.state = \"InUse\"\n2. attachment.state = \"Detaching\"\n3. attachment.state = \"Detached\"\n4. volume.state = \"Available\"\nAdd attachment seems happened at the a2 or a2.5. Remove attachment seems happen at b3 or b3.5.\nSo when volume is \"InUse\", it may still \"attaching\". So it would be only case have to check attachment state to know if the transition is done or not.\nPlease correct me if that's not the case. Also if we can know exactly when attachment is added to volume that would be very helpful, otherwise I am not sure when volume is InUse but without attachments is a legal state or not.\n. Thanks @jasdel . So I think it's safe to assume if a Volume don't have attachments, definitely it hasn't been not attached yet. Check for VolumeAttachmentStateAttached should be good enough.\nBut check for VolumeAttachmentStateDetached can be tricky because it seems sometime after volume detached, attachments array of volume object would be removed totally? Or either VolumeAttachmentStateDetached or VolumeStateAvailable with no attachments means volume has already been detached?\n. I see. Thanks!\n. ",
    "abhiofdoon": "I am seeing this issue with the Ruby SDK.\nI am using snapshot.wait_until_completed with the default interval and retries (15s and 40 retries).\nThe snapshot completes very quickly (I can see it 100% done and green in aws console), however the wait_until_complete() takes about 3 minutes to return. \nAny suggestions ?\n. @jasdel Sure. I just posted the same issue in ruby sdk issues.\nYes I am seeing this issue every time I create a snapshot. The time it takes is also pretty consistent. It takes about 2 minutes and 40 seconds to return.\n. ",
    "seanarnold": ":+1: \n. ",
    "satyenr": "@nightlyone: While I'd normally agree with you, the fact that boto and AWS CLI work just fine with the colon delimited form makes it confusing/irritating for people who are switching to Go or use both simultaneously - especially if the credentials file is being auto-generated by a tool they don't control.\nLooking at the source code, seems like a third party INI file parser is being used. It may be worthwhile to replace it by another parser that supports colon delimited files. Do you disagree?\n. @jasdel: goconfig seems like a good alternative INI parser. \n. Thanks!\n. ",
    "n-boy": "@jasdel, Thanks a lot! Now it works fine for me.\n. Thanks for reply.\n\nIs the file you're trying to upload being read across a network filesystem, or from a slow medium (CD/USB/ect)?\n\nNo, file is located locally on SSD. And one more detail - I've tested this on two different windows machines, with different equipment and internet provider.\n\ngo\nfileReader := bufio.NewReaderSize(origFileReader, 128 * 1024)\n\nThis won't work because glacier.UploadArchiveInput.Body is 'ReadSeeker'.\nWhy am I think that the issue could be somehow fixed in AWS library:\nif using simply http.defaultClient (which is used in aws-go-sdk, yeah?) to upload the same file somewere, the speed is just fine (100Mbit to the nearby server). \nThe code I used for test:\ngo\nclient := &http.Client{}\nfileReader, _ := os.Open(filePath)\nreq, _ := http.NewRequest(\"POST\", \"http://example.com\", fileReader)\nresp, _ := client.Do(req)\nI will be trying to find solution by myself, but if you'll have some ideas - please let me know.\n. ",
    "nariyu": "Oops... I close this PR and think again. Thank you for your comment.\n. ",
    "caarlos0": "Just realized that the client was using another zone, my bad, sorry.\n. ",
    "eriklott": "@jasdel, I think @murrekatt might be referring to a browser based POST, rather than a server POST. The aws sdk for ruby, for example, has a PresignedPost()  method which generates the url, params and credentials necessary to construct a browser base upload form:\nhttp://docs.aws.amazon.com/sdkforruby/api/Aws/S3/PresignedPost.html\nWe're setting up browser based s3 uploads right now in our GO app, and we're sorely missing this feature. \n. ",
    "bpot": "This is the same technique that the json standard library uses (https://github.com/golang/go/blob/932c1e3dd32f636ab3f25b23d9dcef194a577bca/src/encoding/json/encode.go#L1025) to check if a field is export so it should be correct.\n. Thanks!\n. @jasdel The interface style looks good to me.\nI have one other idea. Would it be possible to hang the payload tags off of the blank identifier instead of using the unexported metadata* field/SDKShapeTraits combination? For example,\ntype UploadPartInput struct {\n  _ struct{} `type:\"structure\" payload:\"Body\"`\n  ...\n}\nWe would still need to use FieldByName(\"_\") to fetch the payload tag but because the field we're looking for is on the top level struct it would take the fast path instead of the path the triggers all of the allocations.\n. Made some move improvements. The benchmark is a little bit misleading because it doesn't capture the optimizations in stdlib json for cases where the same struct is encoded multiple times (e.g. DynamoDB batch write) but it's definitely an improvement!\nBenchmarkBuildJSON-4     1000000              9362 ns/op            1344 B/op         58 allocs/op\nBenchmarkStdlibJSON-4    1000000              7340 ns/op            1200 B/op         13 allocs/op\n. Deltas for make bench-protocol against master:\n```\nbenchmark                                                                      old ns/op     new ns/op     delta\nBenchmarkEC2QueryBuild_Complex_ec2AuthorizeSecurityGroupEgress-4               49414         50128         +1.44%\nBenchmarkEC2QueryBuild_Simple_ec2AttachNetworkInterface-4                      12972         13064         +0.71%\nBenchmarkBuildJSON-4                                                           8984          7713          -14.15%\nBenchmarkStdlibJSON-4                                                          7071          6934          -1.94%\nBenchmarkJSONRPCBuild_Simple_dynamodbPutItem-4                                 18702         16294         -12.88%\nBenchmarkJSONUtilBuild_Simple_dynamodbPutItem-4                                18531         15594         -15.85%\nBenchmarkEncodingJSONMarshal_Simple_dynamodbPutItem-4                          6596          6326          -4.09%\nBenchmarkRESTJSONBuild_Complex_elastictranscoderCreateJobInput-4               273698        271139        -0.93%\nBenchmarkRESTBuild_Complex_elastictranscoderCreateJobInput-4                   8876          9031          +1.75%\nBenchmarkEncodingJSONMarshal_Complex_elastictranscoderCreateJobInput-4         40054         38537         -3.79%\nBenchmarkRESTJSONBuild_Simple_elastictranscoderListJobsByPipeline-4            13949         14221         +1.95%\nBenchmarkRESTBuild_Simple_elastictranscoderListJobsByPipeline-4                10198         9991          -2.03%\nBenchmarkEncodingJSONMarshal_Simple_elastictranscoderListJobsByPipeline-4      1064          1093          +2.73%\nBenchmarkRESTXMLBuild_Complex_cloudfrontCreateDistribution-4                   414674        407649        -1.69%\nBenchmarkRESTXMLBuild_Simple_cloudfrontDeleteStreamingDistribution-4           19058         18549         -2.67%\nBenchmarkEncodingXMLMarshal_Simple_cloudfrontDeleteStreamingDistribution-4     5207          5172          -0.67%\nbenchmark                                                                      old allocs     new allocs     delta\nBenchmarkEC2QueryBuild_Complex_ec2AuthorizeSecurityGroupEgress-4               237            237            +0.00%\nBenchmarkEC2QueryBuild_Simple_ec2AttachNetworkInterface-4                      67             67             +0.00%\nBenchmarkBuildJSON-4                                                           58             45             -22.41%\nBenchmarkStdlibJSON-4                                                          13             13             +0.00%\nBenchmarkJSONRPCBuild_Simple_dynamodbPutItem-4                                 105            79             -24.76%\nBenchmarkJSONUtilBuild_Simple_dynamodbPutItem-4                                103            77             -25.24%\nBenchmarkEncodingJSONMarshal_Simple_dynamodbPutItem-4                          8              8              +0.00%\nBenchmarkRESTJSONBuild_Complex_elastictranscoderCreateJobInput-4               1131           1131           +0.00%\nBenchmarkRESTBuild_Complex_elastictranscoderCreateJobInput-4                   59             59             +0.00%\nBenchmarkEncodingJSONMarshal_Complex_elastictranscoderCreateJobInput-4         7              7              +0.00%\nBenchmarkRESTJSONBuild_Simple_elastictranscoderListJobsByPipeline-4            80             80             +0.00%\nBenchmarkRESTBuild_Simple_elastictranscoderListJobsByPipeline-4                57             57             +0.00%\nBenchmarkEncodingJSONMarshal_Simple_elastictranscoderListJobsByPipeline-4      2              2              +0.00%\nBenchmarkRESTXMLBuild_Complex_cloudfrontCreateDistribution-4                   1832           1832           +0.00%\nBenchmarkRESTXMLBuild_Simple_cloudfrontDeleteStreamingDistribution-4           88             88             +0.00%\nBenchmarkEncodingXMLMarshal_Simple_cloudfrontDeleteStreamingDistribution-4     9              9              +0.00%\nbenchmark                                                                      old bytes     new bytes     delta\nBenchmarkEC2QueryBuild_Complex_ec2AuthorizeSecurityGroupEgress-4               10920         10919         -0.01%\nBenchmarkEC2QueryBuild_Simple_ec2AttachNetworkInterface-4                      3696          3696          +0.00%\nBenchmarkBuildJSON-4                                                           1344          1136          -15.48%\nBenchmarkStdlibJSON-4                                                          1200          1200          +0.00%\nBenchmarkJSONRPCBuild_Simple_dynamodbPutItem-4                                 4289          3873          -9.70%\nBenchmarkJSONUtilBuild_Simple_dynamodbPutItem-4                                4225          3809          -9.85%\nBenchmarkEncodingJSONMarshal_Simple_dynamodbPutItem-4                          808           808           +0.00%\nBenchmarkRESTJSONBuild_Complex_elastictranscoderCreateJobInput-4               41276         41276         +0.00%\nBenchmarkRESTBuild_Complex_elastictranscoderCreateJobInput-4                   2800          2800          +0.00%\nBenchmarkEncodingJSONMarshal_Complex_elastictranscoderCreateJobInput-4         4905          4905          +0.00%\nBenchmarkRESTJSONBuild_Simple_elastictranscoderListJobsByPipeline-4            4368          4368          +0.00%\nBenchmarkRESTBuild_Simple_elastictranscoderListJobsByPipeline-4                3328          3328          +0.00%\nBenchmarkEncodingJSONMarshal_Simple_elastictranscoderListJobsByPipeline-4      120           120           +0.00%\nBenchmarkRESTXMLBuild_Complex_cloudfrontCreateDistribution-4                   103386        103351        -0.03%\nBenchmarkRESTXMLBuild_Simple_cloudfrontDeleteStreamingDistribution-4           9104          9104          +0.00%\nBenchmarkEncodingXMLMarshal_Simple_cloudfrontDeleteStreamingDistribution-4     4656          4656          +0.00%\n``\n. Deltas formake bench-protocolagainst master+thebuild_test.go` change:\n```\nbenchmark                                                                      old ns/op     new ns/op     delta\nBenchmarkEC2QueryBuild_Complex_ec2AuthorizeSecurityGroupEgress-4               51114         50054         -2.07%\nBenchmarkEC2QueryBuild_Simple_ec2AttachNetworkInterface-4                      12863         13034         +1.33%\nBenchmarkBuildJSON-4                                                           14813         9617          -35.08%\nBenchmarkStdlibJSON-4                                                          7061          7014          -0.67%\nBenchmarkJSONRPCBuild_Simple_dynamodbPutItem-4                                 18704         14767         -21.05%\nBenchmarkJSONUtilBuild_Simple_dynamodbPutItem-4                                18671         14005         -24.99%\nBenchmarkEncodingJSONMarshal_Simple_dynamodbPutItem-4                          6377          6395          +0.28%\nBenchmarkRESTJSONBuild_Complex_elastictranscoderCreateJobInput-4               273837        215841        -21.18%\nBenchmarkRESTBuild_Complex_elastictranscoderCreateJobInput-4                   8867          8984          +1.32%\nBenchmarkEncodingJSONMarshal_Complex_elastictranscoderCreateJobInput-4         37970         39407         +3.78%\nBenchmarkRESTJSONBuild_Simple_elastictranscoderListJobsByPipeline-4            14139         13099         -7.36%\nBenchmarkRESTBuild_Simple_elastictranscoderListJobsByPipeline-4                10115         10440         +3.21%\nBenchmarkEncodingJSONMarshal_Simple_elastictranscoderListJobsByPipeline-4      1096          1089          -0.64%\nBenchmarkRESTXMLBuild_Complex_cloudfrontCreateDistribution-4                   405664        405967        +0.07%\nBenchmarkRESTXMLBuild_Simple_cloudfrontDeleteStreamingDistribution-4           18864         18716         -0.78%\nBenchmarkEncodingXMLMarshal_Simple_cloudfrontDeleteStreamingDistribution-4     5132          5217          +1.66%\nbenchmark                                                                      old allocs     new allocs     delta\nBenchmarkEC2QueryBuild_Complex_ec2AuthorizeSecurityGroupEgress-4               237            237            +0.00%\nBenchmarkEC2QueryBuild_Simple_ec2AttachNetworkInterface-4                      67             67             +0.00%\nBenchmarkBuildJSON-4                                                           98             62             -36.73%\nBenchmarkStdlibJSON-4                                                          13             13             +0.00%\nBenchmarkJSONRPCBuild_Simple_dynamodbPutItem-4                                 105            78             -25.71%\nBenchmarkJSONUtilBuild_Simple_dynamodbPutItem-4                                103            76             -26.21%\nBenchmarkEncodingJSONMarshal_Simple_dynamodbPutItem-4                          8              8              +0.00%\nBenchmarkRESTJSONBuild_Complex_elastictranscoderCreateJobInput-4               1131           798            -29.44%\nBenchmarkRESTBuild_Complex_elastictranscoderCreateJobInput-4                   59             59             +0.00%\nBenchmarkEncodingJSONMarshal_Complex_elastictranscoderCreateJobInput-4         7              7              +0.00%\nBenchmarkRESTJSONBuild_Simple_elastictranscoderListJobsByPipeline-4            80             71             -11.25%\nBenchmarkRESTBuild_Simple_elastictranscoderListJobsByPipeline-4                57             57             +0.00%\nBenchmarkEncodingJSONMarshal_Simple_elastictranscoderListJobsByPipeline-4      2              2              +0.00%\nBenchmarkRESTXMLBuild_Complex_cloudfrontCreateDistribution-4                   1832           1832           +0.00%\nBenchmarkRESTXMLBuild_Simple_cloudfrontDeleteStreamingDistribution-4           88             88             +0.00%\nBenchmarkEncodingXMLMarshal_Simple_cloudfrontDeleteStreamingDistribution-4     9              9              +0.00%\nbenchmark                                                                      old bytes     new bytes     delta\nBenchmarkEC2QueryBuild_Complex_ec2AuthorizeSecurityGroupEgress-4               10922         10922         +0.00%\nBenchmarkEC2QueryBuild_Simple_ec2AttachNetworkInterface-4                      3696          3696          +0.00%\nBenchmarkBuildJSON-4                                                           3136          1408          -55.10%\nBenchmarkStdlibJSON-4                                                          1200          1200          +0.00%\nBenchmarkJSONRPCBuild_Simple_dynamodbPutItem-4                                 4289          2992          -30.24%\nBenchmarkJSONUtilBuild_Simple_dynamodbPutItem-4                                4225          2928          -30.70%\nBenchmarkEncodingJSONMarshal_Simple_dynamodbPutItem-4                          808           808           +0.00%\nBenchmarkRESTJSONBuild_Complex_elastictranscoderCreateJobInput-4               41276         25287         -38.74%\nBenchmarkRESTBuild_Complex_elastictranscoderCreateJobInput-4                   2800          2800          +0.00%\nBenchmarkEncodingJSONMarshal_Complex_elastictranscoderCreateJobInput-4         4905          4905          +0.00%\nBenchmarkRESTJSONBuild_Simple_elastictranscoderListJobsByPipeline-4            4368          3936          -9.89%\nBenchmarkRESTBuild_Simple_elastictranscoderListJobsByPipeline-4                3328          3328          +0.00%\nBenchmarkEncodingJSONMarshal_Simple_elastictranscoderListJobsByPipeline-4      120           120           +0.00%\nBenchmarkRESTXMLBuild_Complex_cloudfrontCreateDistribution-4                   103354        103383        +0.03%\nBenchmarkRESTXMLBuild_Simple_cloudfrontDeleteStreamingDistribution-4           9104          9104          +0.00%\nBenchmarkEncodingXMLMarshal_Simple_cloudfrontDeleteStreamingDistribution-4     4656          4656          +0.00%\n```\n. Cool! I definitely prefer the method solution too.\nI'll check the performance differences.\n. The hanging test is part of our integration test suite. I can try and make a minimal test case but that may not happen until Monday.\nI doubt it will help much but on the off chance it does here is a backtrace of what's happening when the test timedout:\n00:07:46.043 goroutine 15 [runnable]:\n00:07:46.043 net/http.(*persistConn).roundTrip(0xc820224000, 0xc82033cf30, 0x0, 0x0, 0x0)\n00:07:46.043    /usr/local/go/src/net/http/transport.go:1164 +0xef6\n00:07:46.043 net/http.(*Transport).RoundTrip(0xd07de0, 0xc8200a9420, 0x41176b, 0x0, 0x0)\n00:07:46.043    /usr/local/go/src/net/http/transport.go:235 +0x6b4\n00:07:46.043 net/http.send(0xc8200a9420, 0x7f9ad93b37a8, 0xd07de0, 0x10, 0x0, 0x0)\n00:07:46.043    /usr/local/go/src/net/http/client.go:220 +0x73e\n00:07:46.043 net/http.(*Client).send(0xc8201f2270, 0xc8200a9420, 0x10, 0x0, 0x0)\n00:07:46.043    /usr/local/go/src/net/http/client.go:143 +0x1f8\n00:07:46.043 net/http.(*Client).doFollowingRedirects(0xc8201f2270, 0xc8200a9420, 0xb474c8, 0x0, 0x0, 0x0)\n00:07:46.043    /usr/local/go/src/net/http/client.go:380 +0x105a\n00:07:46.043 net/http.(*Client).Do(0xc8201f2270, 0xc8200a9420, 0xc8205e6a80, 0x0, 0x0)\n00:07:46.043    /usr/local/go/src/net/http/client.go:178 +0x1e3\n00:07:46.043 github.com/ReturnPath/godzilla/Godeps/_workspace/src/github.com/aws/aws-sdk-go/aws/corehandlers.glob.func3(0xc8205e6840)\n00:07:46.044    /go/src/github.com/ReturnPath/godzilla/Godeps/_workspace/src/github.com/aws/aws-sdk-go/aws/corehandlers/handlers.go:62 +0x95\n00:07:46.044 github.com/ReturnPath/godzilla/Godeps/_workspace/src/github.com/aws/aws-sdk-go/aws/request.(*HandlerList).Run(0xc8205e6960, 0xc8205e6840)\n00:07:46.044    /go/src/github.com/ReturnPath/godzilla/Godeps/_workspace/src/github.com/aws/aws-sdk-go/aws/request/handlers.go:110 +0xd9\n00:07:46.044 github.com/ReturnPath/godzilla/Godeps/_workspace/src/github.com/aws/aws-sdk-go/aws/request.(*Request).Send(0xc8205e6840, 0x0, 0x0)\n00:07:46.044    /go/src/github.com/ReturnPath/godzilla/Godeps/_workspace/src/github.com/aws/aws-sdk-go/aws/request/request.go:225 +0x798\n00:07:46.044 github.com/ReturnPath/godzilla/Godeps/_workspace/src/github.com/aws/aws-sdk-go/aws/request.(*Request).EachPage(0xc8204d7080, 0xc820309848, 0x0, 0x0)\n00:07:46.044    /go/src/github.com/ReturnPath/godzilla/Godeps/_workspace/src/github.com/aws/aws-sdk-go/aws/request/request_pagination.go:88 +0x4f\n00:07:46.044 github.com/ReturnPath/godzilla/Godeps/_workspace/src/github.com/aws/aws-sdk-go/service/dynamodb.(*DynamoDB).QueryPages(0xc820200020, 0xc8202075e0, 0xc820309bd0, 0x0, 0x0)\n00:07:46.044    /go/src/github.com/ReturnPath/godzilla/Godeps/_workspace/src/github.com/aws/aws-sdk-go/service/dynamodb/api.go:543 +0xa4\nThat failure is on https://github.com/aws/aws-sdk-go/commit/2fc0e818c4a658aa141d67574d115b40e4b29aa8\n. This is kind of a shot in the dark but it seems like this might be failing for dynamo because LastEvaluatedKey is a map and not a string (which I assume it is for most other services). \n. @jasdel thanks for the quick turn around! I've verified that it fixes our issues.\n. Thanks @nightlyone, I've moved it to the top and verified that it takes no space now.\n. That makes sense. To clarify I'm mostly interested in reducing the amount of memory allocated by this method in cases where the request body is relatively small. I wouldn't expect this PR to change behavior for cases where the body is larger than 32KB.\nAs an example, we have an operation which updates a single field on a DynamoDB item. makeSha256Reader is responsible for about 66% of the space allocated by that operation.\nI'll try to see if I can make this a less risky change.. @xibz rebased and updated in response to comments.. Looks like gofmt issue. I can back this out if we don't formatting changes mixed into this.. ",
    "tve": "Is there any work in progress on the XML front? The EC2 DescribeImages call seems like a good test case ;-). Having the same problem here, EC2 DescribeImages takes up >2GB. Looking at the code, it seems that the very foundations of the XML unmarshaling of the response uses reflection to inspect each token used by the stdlib xml decoder and then reallocates a fresh object for it. No wonder...\nI'm not sure how it would integrate into the API, but being able to handle each resource record in turn as it's decoded would help my use-case and would avoid having to build up the entire response array in-memory. That would not help the CPU burn caused by all the objects allocated and subsequent GC, though.. ",
    "jonaskint": "Any upgrade on the status of this issue? We're currently running against the same problem as issue #1300 . . ",
    "teastburn": "Hello, thanks for the hard work! Any updates on this? \nI am seeing ~9x memory growth using WriteAtBuffer from a Lambda. Here I am downloading a file that is 50949808 bytes and printing memory stats via runtime.MemStats:\n```go\nAlloc = 1 MiB TotalAlloc = 1 MiB Sys = 3 MiB NumGC = 0\nbuffer := aws.NewWriteAtBuffer([]byte{})\nn, err := downloader.Download(buffer, &s3.GetObjectInput{\n    Bucket: aws.String(s3Bucket),\n    Key:    aws.String(s3Path),\n})\nAlloc = 250 MiB TotalAlloc = 25973 MiB Sys = 438 MiB NumGC = 346\nNote the `Sys` memory, which is what Lambda seems to care about, went up 435 MiB.. @jasdel thanks for the quick response! I did also try that after looking at the WriteAtBuffer code.go\nAlloc = 1 MiB TotalAlloc = 1 MiB Sys = 5 MiB NumGC = 0\nb := make([]byte, 50000000)\nAlloc = 49 MiB    TotalAlloc = 49 MiB    Sys = 55 MiB    NumGC = 1\nbuffer := aws.NewWriteAtBuffer(b)\nAlloc = 48 MiB    TotalAlloc = 49 MiB    Sys = 55 MiB    NumGC = 1\nn, err := downloader.Download(buffer, &s3.GetObjectInput{\n    Bucket: aws.String(s3Bucket),\n    Key:    aws.String(s3Path),\n})\nAlloc = 153 MiB    TotalAlloc = 5522 MiB    Sys = 525 MiB    NumGC = 55\n``\nInterestingly, the Lambda did not run out of memory (Memory Size: 512 MB   Max Memory Used: 509 MB), and theNumGCis far lower when pre-allocating, along withTotalAlloc. However, theSys` memory used appears to be roughly equivalent or higher.\nI'm using go version go1.x (lambda) and aws-sdk-go version v1.15.72.. I switched to using \nresp, err := svc.GetObject(&s3.GetObjectInput{\n        Bucket: aws.String(s3Bucket),\n        Key:    aws.String(s3Path),\n    })\nwhich uses a constant amount of memory if used with an io.Reader on resp.Body.. @jasdel do you have a recommendation there? I am wrapping it in a bufio.NewReader so that I can easily read by lines via a Scanner. One semi annoying issue with my solution is the caller to my struct receiver function still has to call Close() on the body via a struct method. The good thing, however, is that even loading a 256mb file takes a max memory of 50mb.\n```go\n// Allows us to loop through s3 file line by line\ntype ByteReader interface {\n    io.Reader\n    ReadBytes(byte) ([]byte, error)\n}\n// Mockable interface\ntype S3FileGettable interface {\n    Get(string, string) (ByteReader, error)\n    io.Closer\n}\ntype S3Getter struct {\n    body io.ReadCloser\n}\nfunc (s3g *S3Getter) Close() error {\n    return s3g.body.Close()\n}\n// Must call S3Getter.Close() after done with ByteReader\nfunc (s3g *S3Getter) Get(s3Bucket, s3Path string) (ByteReader, error) {\n    // ...\n    svc := s3.New(session.Must(session.NewSession(s3Cfg)))\n    resp, err := svc.GetObject(&s3.GetObjectInput{\n        Bucket: aws.String(s3Bucket),\n        Key:    aws.String(s3Path),\n    })\n    // ...\n    s3g.body = resp.Body\n    reader := bufio.NewReader(resp.Body)\n    return reader, nil\n}\n```. @jasdel That's great feedback. Thanks for the help!. ",
    "rfielding": "Thanks for that useful debugging tip.  I got a similar looking error while writing to CloudWatch.  Missing x-amz-content-sha256.  I'm not sure why yet.\nodrive_1      | 2016/08/23 00:21:58 DEBUG: Request monitoring/PutMetricData Details:\nodrive_1      | ---[ REQUEST POST-SIGN ]-----------------------------\nodrive_1      | POST / HTTP/1.1\nodrive_1      | Host: s3.amazonaws.com\nodrive_1      | User-Agent: aws-sdk-go/1.4.3 (go1.6; linux; amd64)\nodrive_1      | Content-Length: 437\nodrive_1      | Authorization: AWS4-HMAC-SHA256 Credential=***/20160823/us-east-1/monitoring/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date, Signature=4bd84824c6eb8d8ef42bae82cd23c0bcac47eb0a1206c63ffc186e7682b36237\nodrive_1      | Content-Type: application/x-www-form-urlencoded; charset=utf-8\nodrive_1      | X-Amz-Date: 20160823T002158Z\nodrive_1      | Accept-Encoding: gzip\nodrive_1      | \nodrive_1      | Action=PutMetricData&MetricData.member.1.Dimensions=&MetricData.member.1.MetricName=RequestLatency&MetricData.member.1.StatisticValues.Maximum=1&MetricData.member.1.StatisticValues.Minimum=1&MetricData.member.1.StatisticValues.SampleCount=1&MetricData.member.1.StatisticValues.Sum=1&MetricData.member.1.Timestamp=2016-08-23T00%3A21%3A58Z&MetricData.member.1.Unit=Count&MetricData.member.1.Value=1&Namespace=objectDrive&Version=2010-08-01\nodrive_1      | -----------------------------------------------------\nodrive_1      | 2016/08/23 00:21:58 DEBUG: Response monitoring/PutMetricData Details:\nodrive_1      | ---[ RESPONSE ]--------------------------------------\nodrive_1      | HTTP/1.1 400 Bad Request\nodrive_1      | Connection: close\nodrive_1      | Transfer-Encoding: chunked\nodrive_1      | Content-Type: application/xml\nodrive_1      | Date: Tue, 23 Aug 2016 00:21:57 GMT\nodrive_1      | Server: AmazonS3\nodrive_1      | X-Amz-Id-2: 0E18nfsemgrQbQW5m4Jec6WPhC206JGoXaOzVEhrmRqLSYtMhNM8Nq94X0OXjgXYdzj/4U1qJPs=\nodrive_1      | X-Amz-Request-Id: 5401BD0D502B4542\nodrive_1      | \nodrive_1      | 126\nodrive_1      | <?xml version=\"1.0\" encoding=\"UTF-8\"?>\nodrive_1      | <Error><Code>InvalidRequest</Code><Message>Missing required header for this request: x-amz-content-sha256</Message><RequestId>5401BD0D502B4542</RequestId><HostId>0E18nfsemgrQbQW5m4Jec6WPhC206JGoXaOzVEhrmRqLSYtMhNM8Nq94X0OXjgXYdzj/4U1qJPs=</HostId></Error>\nodrive_1      | 0\nodrive_1      |\n. I definitely have new information.  It is now returning ErrorResponse.... (but not quite writing correctly to CloudWatch yet.\nI removed from my code the bit that explicitly sets endpoint to \"s3.amazonaws.com\" (because we need to override endpoint in government deployed setting):\nsessionConfig := &aws.Config{\n            Credentials: credentials.NewEnvCredentials(),\n            Region:      aws.String(region),\n            //Endpoint:    aws.String(endpoint),\n        }\nIt was (explicitly set!):\nHost: s3.amazonaws.com\nAnd I was sharing that session object for use in S3 as well as CloudWatch.\nodrive_1      | HTTP/1.1 400 Bad Request\nodrive_1      | Content-Length: 397\nodrive_1      | Content-Type: text/xml\nodrive_1      | Date: Tue, 23 Aug 2016 00:45:36 GMT\nodrive_1      | X-Amzn-Requestid: e890925c-68ca-11e6-9f5a-cbec9914f6f8\nodrive_1      | \nodrive_1      | <ErrorResponse xmlns=\"http://monitoring.amazonaws.com/doc/2010-08-01/\">\nodrive_1      |   <Error>\nodrive_1      |     <Type>Sender</Type>\nodrive_1      |     <Code>InvalidParameterCombination</Code>\nodrive_1      |     <Message>The parameters MetricData.member.1.Value and MetricData.member.1.StatisticValues are mutually exclusive and you have specified both.</Message>\nodrive_1      |   </Error>\nodrive_1      |   <RequestId>e890925c-68ca-11e6-9f5a-cbec9914f6f8</RequestId>\nodrive_1      | </ErrorResponse>\nSo, host was \"s3.amazonaws.com\" for the S3 case.  But I was sharing the session.  Now that I am not setting it explicitly, it's\nHost: monitoring.us-east-1.amazonaws.com\nSo, I think first I need to just not be sharing that session between S3 and cloudwatch.  I'm not sure what's going to happen when somebody deploys this into gov cloud.  I guess I need a separate endpoint override for that one too.\n. ok!  that did it!  I just commented out the StatisticSet, and I'm publishing to CloudWatch.  \nMy guess:  S3 returns errors wrapped in Error tags, while monitoring returns errors wrapped up in ErrorResponse.  And the endpoint hosts... s3 doesn't qualify by region, but monitoring does.\n. Thanks @jasdel  for taking a look at this, and getting to it so quickly.  I apologize for my post being too vague.  It's a problem with Presign(), not Send().  The problem is that when I do this the download works without the nonce (a security hole -- the URL alone is sufficient to get it, even though the nonce is supposedly required).  The point of the nonce is that if a user scrapes the URL out of the browser and sends it to somebody else, he may not realize that the other person isn't actually authorized for it.  So part of the required info must be in a header, since headers and cookies are not so easily leaked.  This is the client code being suggested:\n```\nfunc main() {\n    account := \"default\"\n    bucketName := aws.String(\"decipherers\")\n    svc := generateSession(account)\npicture := aws.String(\"Grumpy-Cat-6.jpg\")\nexpiresWithin := 60 * time.Minute\nreq, _ := svc.GetObjectRequest(\n    &s3.GetObjectInput{\n        Bucket: bucketName,\n        Key:    picture,\n    },\n)\n\nreq.HTTPRequest.Header.Set(\"nonce\", \"9999\")\nurlStr, _ := req.Presign(expiresWithin)\nlog.Println(urlStr)\n\n}\n```\nThe problem is that the pre-signed GET url works without any headers being set (security hole):\nRobs-MacBook-Pro:uploadtest rfielding$ go run main.go \n2015/12/08 10:32:12 https://decipherers.s3.amazonaws.com/Grumpy-Cat-6.jpg?Nonce=9999&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJIOAGQ2S42PSW6QQ%2F20151208%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20151208T153212Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=40fd74c4ac5c127d735b0a0599464cdd7ad517a41e9891cbc0264a0aa3452e7a\nRobs-MacBook-Pro:uploadtest rfielding$ wget 'https://decipherers.s3.amazonaws.com/Grumpy-Cat-6.jpg?Nonce=9999&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJIOAGQ2S42PSW6QQ%2F20151208%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20151208T153212Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=40fd74c4ac5c127d735b0a0599464cdd7ad517a41e9891cbc0264a0aa3452e7a'\nThe get should have failed, when called like this, then succeeded when wget has the parameter --header=\"nonce: 9999\" in it.\nThe nonce appearing in the GET is itself a security problem, as that's supposed to be the missing information required to ensure that the unexpired url is not leaked.  So I did a Proof Of Concept fix in my code to do this (awful hack).  First, I just removed the Header.Set from my code.  I then hardcoded the header set at the earliest point I can (with passing the nonce value in somehow TBD):\n//Comment out my explicit Header.Set in main for now....\n    //req.HTTPRequest.Header.Set(\"nonce\", \"9999\")\nSo I am at github.com/aws/aws-sdk-go/private/signer/v4/v4.go:\n```\nfunc (v4 *signer) buildCanonicalHeaders() {\n    var headers []string\n    headers = append(headers, \"host\")\n    headers = append(headers, \"nonce\")\n    for k := range v4.Request.Header {\n        if _, ok := ignoredHeaders[http.CanonicalHeaderKey(k)]; ok {\n            continue // ignored header\n        }\n        headers = append(headers, strings.ToLower(k))\n    }\n    sort.Strings(headers)\nv4.signedHeaders = strings.Join(headers, \";\")\n\nif v4.isPresign {\n    v4.Query.Set(\"X-Amz-SignedHeaders\", v4.signedHeaders)\n}\n\nheaderValues := make([]string, len(headers))\nfor i, k := range headers {\n    if k == \"host\" {\n        headerValues[i] = \"host:\" + v4.Request.URL.Host\n    } else {\n        if k == \"nonce\" {\n            headerValues[i] = \"nonce:9999\"\n        } else {\n            headerValues[i] = k + \":\" +\n                strings.Join(v4.Request.Header[http.CanonicalHeaderKey(k)], \",\")\n        }\n    }\n}\n\nv4.canonicalHeaders = strings.Join(headerValues, \"\\n\")\n\n}\n```\nSo, now it behaves correctly.  The download FAILS without the header as it should now.  The nonce is not leaked into the GET url:\nRobs-MacBook-Pro:uploadtest rfielding$ go run main.go \n2015/12/08 10:56:39 https://decipherers.s3.amazonaws.com/Grumpy-Cat-6.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJIOAGQ2S42PSW6QQ%2F20151208%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20151208T155639Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host%3Bnonce&X-Amz-Signature=c326c0ec82652d93210d94abc4d431612c444255354341a281fddd3225f41e80\nRobs-MacBook-Pro:uploadtest rfielding$\nAnd it fails when we try to use the URL alone:\n```\nRobs-MacBook-Pro:uploadtest rfielding$ wget 'https://decipherers.s3.amazonaws.com/Grumpy-Cat-6.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJIOAGQ2S42PSW6QQ%2F20151208%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20151208T155639Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host%3Bnonce&X-Amz-Signature=c326c0ec82652d93210d94abc4d431612c444255354341a281fddd3225f41e80'\nThe name is too long, 291 chars total.\nTrying to shorten...\nNew name is Grumpy-Cat-6.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJIOAGQ2S42PSW6QQ%2F20151208%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20151208T155639Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host%3Bnonce&X-Amz-Signature=c326c0ec8.\nThe name is too long, 291 chars total.\nTrying to shorten...\nNew name is Grumpy-Cat-6.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJIOAGQ2S42PSW6QQ%2F20151208%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20151208T155639Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host%3Bnonce&X-Amz-Signature=c326c0ec8.\n--2015-12-08 10:57:48--  https://decipherers.s3.amazonaws.com/Grumpy-Cat-6.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJIOAGQ2S42PSW6QQ%2F20151208%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20151208T155639Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host%3Bnonce&X-Amz-Signature=c326c0ec82652d93210d94abc4d431612c444255354341a281fddd3225f41e80\nResolving decipherers.s3.amazonaws.com... 54.231.10.233\nConnecting to decipherers.s3.amazonaws.com|54.231.10.233|:443... connected.\nHTTP request sent, awaiting response... 403 Forbidden\n2015-12-08 10:57:48 ERROR 403: Forbidden.\nRobs-MacBook-Pro:uploadtest rfielding$ \n```\nAnd if we redo the request with --header=\"nonce: 9999\", it succeeds.\n```\nRobs-MacBook-Pro:uploadtest rfielding$ wget --header=\"nonce: 9999\" 'https://decipherers.s3.amazonaws.com/Grumpy-Cat-6.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJIOAGQ2S42PSW6QQ%2F20151208%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20151208T155639Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host%3Bnonce&X-Amz-Signature=c326c0ec82652d93210d94abc4d431612c444255354341a281fddd3225f41e80'\nThe name is too long, 291 chars total.\nTrying to shorten...\nNew name is Grumpy-Cat-6.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJIOAGQ2S42PSW6QQ%2F20151208%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20151208T155639Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host%3Bnonce&X-Amz-Signature=c326c0ec8.\nThe name is too long, 291 chars total.\nTrying to shorten...\nNew name is Grumpy-Cat-6.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJIOAGQ2S42PSW6QQ%2F20151208%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20151208T155639Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host%3Bnonce&X-Amz-Signature=c326c0ec8.\n--2015-12-08 10:59:27--  https://decipherers.s3.amazonaws.com/Grumpy-Cat-6.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJIOAGQ2S42PSW6QQ%2F20151208%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20151208T155639Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host%3Bnonce&X-Amz-Signature=c326c0ec82652d93210d94abc4d431612c444255354341a281fddd3225f41e80\nResolving decipherers.s3.amazonaws.com... 54.231.11.57\nConnecting to decipherers.s3.amazonaws.com|54.231.11.57|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1643642 (1.6M) [image/jpeg]\nSaving to: 'Grumpy-Cat-6.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJIOAGQ2S42PSW6QQ%2F20151208%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20151208T155639Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host%3Bnonce&X-Amz-Signature=c326c0ec8'\nGrumpy-Cat-6.jpg?X- 100%[=====================>]   1.57M  1.32MB/s   in 1.2s   \n2015-12-08 10:59:29 (1.32 MB/s) - 'Grumpy-Cat-6.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJIOAGQ2S42PSW6QQ%2F20151208%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20151208T155639Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host%3Bnonce&X-Amz-Signature=c326c0ec8' saved [1643642/1643642]\nRobs-MacBook-Pro:uploadtest rfielding$ \n```\nOnly now does it work.  It does appear that X-Amz-SignedHeaders should just work for this purpose, though there are some signed headers that cannot appear in the GET url that gets returned.\n. I was trying to figure out how to write a clean fix (how do I at least\nproperly pass in the header value and keep it out of the URL?) so that I\ncould make a pull request.  I will gladly make a pull request if I manage\nto figure it out first.  Thanks!\nOn Tue, Dec 8, 2015 at 11:45 AM, Jason Del Ponte notifications@github.com\nwrote:\n\nThanks for the clarification, that was very helpful. It sounds like the\nbest path to add this feature would be to add a method of registering which\nheaders should be required, and not leaked to the URL. This would allow you\nto set which headers will stay in the signature, but not be hoisted to the\nURL.\nSince this issue is limited to Presigning, we could explore the idea of\ncreating a new Presigning method, or consider adding a variadic optional\nargument of additional options.\nTagged this as an enhancement so we can get this functionality added.\nWe're also more than glad to take a look at PRs if you'd like to prototype\nsomething.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-go/issues/458#issuecomment-162941360.\n\n\nhttp://rfieldin.appspot.com\nhttp://rrr00bb.blogspot.com\n. I have a more general question, because I think this is possibly a general security bug.  Why is any signed header value ever in the GET URL?  (host isn't in there, so why would nonce be?). \nIf the client knows the correct values, then they go into the headers.  If they are both in the GET parameters and the headers, and the values contradictory, why should I be allowed access if they are contradictory (between header and URL), and if the URL tells me what they are, then why does S3 want them plugged into a header?.  I am very new to AWS (a few days), but I can't think of a case where the header value should be spelled out in the URL.  It's like there's a difference between what the REST APIs mean, versus what the SDK means.\n. This particular experiment to go directly to S3 to (rather dramatically) speed things up got shot down by other issues (related to 2way tls, gatekeeper, client-side encryption, etc) - so I will end up intercepting big file transfers with a Go program for now.  \nIf I do go back to using pre-signed urls for some reason, I at least know how to plug the security hole in my own copy of the sdk.\n. I have the same problem.  I am building a DropBox like application where I routinely upload 4k video files in excess of 4GB (a DJI Phantom 4 drone produces 8GB for every 20min battery of flight, reduced to about 4GB of high quality mp4 later).  Now imagine streaming such a file through a Raspberry Pi to S3 (the application stream encrypts and stores ciphertext only) that might have a disk that's actually smaller than the file.  In my Go server that did this (before we included a MySQL backend for metadata) never used more than a few hundred kilobytes of RAM regardless of the file size.  If I was able to use an io.Reader and io.Writer to send it up, I would not have had to cache the file on disk at all before writing it to S3 unless there was a network outage to S3.  (Imagine a battery powered cluster taking custody of files streaming off of a drone, possibly during an internet outage).\nI may already know the sha256 checksum because the file is sitting on disk.  But I need to stream it.  I actually may not know the checksum because the file never really ends until the sender dies.  \nI looked into s3manager, seeing how it worked, because I was trying to avoid needing the whole file on disk before sending (or having to cut the file into a bunch of pieces manually and re-assemble later).  If I recall, it only uses the ReadSeeker features to seek to the end to check the size and to look ahead one char.  In any case, s3manager looks like it's designed to cut up the file and send it in parallel, but in the streaming case, you just need to stream it using a minimal amount of memory - ideally with an io.Reader - (os read is usually somewhere between 4k and 32k... just enough to cover packet jitter, and there's no point in buffering more than that).\nSo my solution for now is to upload the entire file into an ec2 instance (and drain to S3 in the background) - which can take an hour to get the whole file into the ec2 and begin the drain into S3.  If the reverse proxy round-robins, it's unlikely that the cached file has made it to S3 yet if you query  very soon after uploading into the ec2 (because it was uploaded into a different instance behind nginx).  That means very large latency.  You may need to wait an hour to start watching the movie; at which point, I allow range requesting (required to skip around through mp4 video - round-robining between all instances behind the reverse proxy).\nTo handle live-streaming, I may need to just name a latency I can tolerate (assume it's 2sec), and write a bunch of 2sec files up to S3, and when a viewer wants to watch, the segments need to be transparently concatenated back together to fit into the range requests that the client asks for.\n. I didn't know what the motivation was for having big parts in memory (along with a number of parts limit - which if you think about it imposes a file size limit given that you are sticking to a small amount of memory).  Doing 4k video requires somewhere around 20Mb/s rate of moving things, but it requires almost no memory (once I don't need to fetch from S3), because the buffers are only a few kb in size.\nIf tcp packets are 1500 bytes, and you pass them on at the same rate (an ideal case for illustration) that you get them, then you need a 1500 byte buffer.  You only need a larger buffer in situations where your inbound rate is exceeding your outbound rate - and queueing happens; such as when moving data outbound gets stalled.  Outbound data needs to enqueue (and fixed overhead per packet).  The buffering is there to absorb jitter and amortize overhead, and increasing buffer sizes has diminishing returns because the number of sessions you can handle is correlated with how little memory is used per session (the whole motivation for goroutines).  \nIf you manually use os.Read and take statistics on how much data you get per read, you find that the amounts you get from single reads are generally small.  One measurement regularly gave me 4kb no matter what I did, and when I was doing multi-gigabyte files I could sometimes get back a 32kb read.  When moving data from io.Reader to io.Writer manually, I was measuring almost no benefit to making the buffers any larger.\nRequiring the whole file with the intention of sending files in parallel also uses quite a bit of memory because of an increased number of un-acked packets that need to hang around in the kernel; where halving the latency (by buffering less) is as useful as doubling throughput once the tcp window manages to get full.  (I haven't measured how often I was setting that limit, but I was easily uploading a 1.5GB file into a RaspberryPi2B+ in about 45mins over wifi... a 1.5hr movie, which is 2x the frame rate to watch it.  That took 200kb total memory for the whole Go binary while that upload happened.)\nBtw... an io.Reader where you pass in a content length and perhaps even a hash separately isn't unreasonable.  Using the io.Reader is for the sake of streaming, so that the amount of memory used is absolutely minimal... throughput times resident time for the data passing through.\n. Correct.  That's why I brought up latency in the context of live-streaming.  If you are streaming at 5MB/s, then those are 1s chunks.  And if you are using an io.Reader, then you don't need to buffer incoming data to disk no matter how large the incoming data is - because your io.Read buffers will be small, like 4k or 32k.  \nMaybe this is why chunks use 5MB in memory?  Taking into account writes out to S3 being stalled while still draining incoming data?\nNote that you can start writing the part before you have 5MB if you could write to it with an io.Writer.  You just keep writing to it until 5MB go in, even if that takes 30s; then that part is done.\nWhen I was looking at details of the APIs for writing into S3, if I recall (it was a long time ago), it all seemed backwards.  This is because I'm in the middle of using an io.Reader to read out of a multi-part mime part, and want an io.Writer destined for S3, where I bucket-brigade 4k chunks from the user through the cipher and into S3.  But I ended up having to write the whole file to disk so that I could get an io.ReadSeeker instead.\n. Use case (download side):  streaming the ciphertext of 4k video (mp4 files).  \nThe ciphertext is in S3 in files that are about 4GB in size.  Presume that you get the video in chunks.  Video must download chunks within a timing deadline to the EC2 instance that decrypts the stream for the user.  So the chunks coming from S3 must be small because you can't read the first byte out of the chunk until all bytes of the chunk have come down.  That means that you end up making a huge number of requests to S3 for chunks.\nThe alternative is that you just pass in an io.Reader that only reads forward.  Now the call to S3 should return immediately with the io.Reader still open.  That means that when we get range requests from the browser, we just keep reading more out of the io.Reader.\nWe have a similar issue with uploads.  If uploading allowed the multipart mime part's io.Reader to be used as a parameter to S3 manager, then if the browser uploaded a quarter of the file, then a quarter of the ciphertext is already uploaded into S3.  When the whole file is uploaded, the whole ciphertext file exists now in S3.  \nWhat we are doing now as a workaround is to let the user just upload the file into the EC2, and promise to get the ciphertext of the file into S3 as soon as possible.  When I was investigating this, I think I supplied my own io.ReadSeeker to log what it was doing... but what I discovered was that it seeked to the end to get a file size before it began, and it looked like it seeked one byte ahead while doing something.  \nWith video, the stream is often unbounded.  You won't know the content length or be able to compute a hash of it until you have an EOF; so you end up just having to upload it first.  \nLive-Streaming is the act of downloading a file (video, logs) that is still uploading.  If everything was using forward-only IO, it becomes straight-forward; other than the issue of not knowing the content length or hash until the stream actually ends.\n. A byte array of 256 random bits is 32 bytes long.  It's not valid ASCII, because it has embedded nulls.  It has embedded carriage returns, etc.  When base64 encoded and first 32 bytes taken of that, that string is 256*3/4 bytes long (192 bits of the original key).  \nI tried using an aws.String(v[:]) of 32 truly random bytes and it seemed to \"work\".  But that is not a valid string for use in an http header, and probably is even more limited because it passes through a header.  \nThat means that it's actually a 192-bit key passed in to AES256.  It has 192 bits of entropy because you know that if it's not a valid base64 encoding, then you don't try it in a brute-force check.  This means that the space to check is reduced by a factor of 2^64 (!!!)\nie: If you want to brute-force attack it, then you strip 64 bits off the search space by ignoring keys that are not valid base64.\nCan somebody verify that this is correct?  Can aws.String(v[:]) be used when the byte array is true random bytes, or does it need to be valid ASCII?. Ok, I take it that we agree on this, but there is nothing we can do about it now?  The problem is that we don't support every character being a random digit from 0..255 uniformly distributed.  Because of this restriction, it can't possibly have 256 bits of entropy.\n\nIn Go, the customer key argument is a 32-char string. In Java it's a base64 encoding of the 32-char string. 32 chars * 8 bits per char = 256 bits needed for AES256\n\nI take it to mean that this string cannot have ASCII chars with the high bit set, embedded zero, bell character, carriage return, space, etc?  An AES key is not just 32 bytes.  It is 32 bytes of entropy.  As an example, suppose that the 32 byte key is restricted to be an ASCII string of binary digits:\n\"01000101010101010100010101000\"\nGiven that we are restricted to representing this string as zeroes and ones, it's 32 characters long, but it cannot have more than 2^32 bits of entropy.  This is an inefficient encoding of a 32 bit key that takes the space of 256 bits.  It is NOT a 256 bit key if we know that each char can only take on 2 distinct values.  We would need a 256 char string to properly store 256 bits of entropy; and it would take 256*8 bits to store it.\nBase64 is similar.  If each character can only take on 64 different values, then each char has not 8 bits of entropy (0..255), but 6 bits (0..63).  6*32 is 192.  So, these keys are turning an AES256 cipher into something with the strength of an AES192 cipher, because you only need to try keys that are valid base64.  \nThe fact that 64 of the bits (to bring it to 256) are effectively missing means that it is 2^64 easier to brute-force the key-space.  I would say that for government Top Secret data for instance, that a 192 bit AES key is still acceptable; though people using AES256 think they are getting true 256 bit keys.. Note: if attempting to fix this, you could look at the length of the string to determine how to handle it:\nIt's supposed to be a 256-bit AES key:\n- len(k) == 32 means that you literally cast the ASCII to bytes, to be backwards compatible.  If it's base64, then it is known to have up to 192 bits of entropy\n- len(k) == 64 could signify to expect hexadecimal chars.  It can hold the full 256 bits of entropy.\n- len(k) == 44 if it is valid base64 with a last char being =, then this is actually a full 32-byte random key encoded to base64.\nThe encoding matters, because it reduces entropy! . @xbiz where would I file a more general security bug to be noted against the general use of SSECustomerKey ?\nCasting 32 bytes of printable ASCII to a 32 byte array\n\ncannot represent all possible AES256 keys\nis somewhere between 32 and 64 bits short of the entropy required for an AES256 key\nie: 32 random bytes converted to base64, then truncated is a 192 bit key, encoded as 256 bits\n32 random bytes converted to base128, then truncated is a 224 bit key, encoded as 256 bits\n\nIt can be fixed by allowing keys that are longer than 32 bytes to be recognized as a PROPER encoding of 32 bits of entropy.  Notably:\n\nif it's length 32, it's backwards compatible, and must be a bit short on entropy.  Probably 192 bits\nif it's length 44 and valid base64, then decode the base64 to recover the full random 32 bytes\nif it's length 64 and valid hex digits, then decode from hex to recover the full random 32 bytes.\n\nKeep this in mind: If it only has 192 bits, when it should be 256 bits, it is 2^64 times faster to brute-force the keys (2^64 ... because 64 bits are truncated).\n2^64 is 18446744073709551616. workaround here?  how could you use this from anywhere? (aws s3api, java, go, etc):\nhttps://forums.aws.amazon.com/thread.jspa?messageID=811033&tstart=0\n```\nI figured it out!\nI needed to remove the \n\"x-amz-server-side-encryption\" = AES256;\nHeader.\nThen \n\"x-amz-server-side-encryption-customer-algorithm\" = AES256;\n\"x-amz-server-side-encryption-customer-key\" = \"at1TMx82nEy7SoAK8jHYanMQDVZMSLayXaaUvTc6CP0=\";\n\"x-amz-server-side-encryption-customer-key-MD5\" = \"LWkBoT3psNdTYez70TVHUQ==\";\n``\nWhat is the trigger to deduce thatx-amz-server-side-encryption-customer-keyis base64 encoded?  Theaws s3api` seems to reject base64 encoded keys outright (which will be 44 bytes):\naws s3api put-object --acl public-read --sse-customer-algorithm=AES256 --sse-customer-key=MUwg/uvG1kIIuByJUISHnAm8TK4sA+PR --body ~/Downloads/copied/CNC.mp4 --bucket decipherers --key gmdata-dev/hugeVideo.mp4. workaround for now:\n\nGo SDK takes advantage of the fact that strings can contain embedded null pointers, so they are (length, ptr) pairs.  So, you can cast an arbitrary []byte to a *string.\nIn the http call, it is a full base64 of what you supplied (44 bytes long).\nOther environments that need real AES keys handle the binary input of the key differently.  --sse-customer-key=fileb://theKey can read a file off of disk that was created like dd if=/dev/random of=theKey bs=32 count=1.  But that writes keys to disk, so avoid that if at all possible.  I am guessing that the workaround varies among language SDKs.. \n",
    "jeskew": "That comment refers to the javascript file included on the next line. This is a standard practice for properly attributing included JS files that may or may not include an in-line attribution.\n. ",
    "onlyjob": "Scope of the comment is very ambiguous. It is not clear at all where it applies (to included file above the comment or below it or to the whole footer.erb...\n. Thanks, @jasdel. As for https://media.amazonwebservices.com/js/sitecatalyst/s_code.min.js, it is minified obfuscated code which is very hard to review and nearly impossible to modify. There are general concerns about safety of such code because of difficulties to review it. Lately it was demonstrated how minification itself can be exploited to produce an attack vector. Finally minification is generally unnecessary and even harmful -- here you can read some arguments against minification: https://wiki.debian.org/onlyjob/no-minification.\nFinally from open source software prospective it is a bad practice to rely upon or use proprietary components even if they are not a part of SDK.\n. But it is part of the SDK as the file in question ships with SDK.\nAttribution is nice but it does not make non-free any less evil.\nNaturally I had to drop fragment of this file from Debian package and I wish that wouldn't be necessary... :(\n. ",
    "wt": "Here's the doc for the CLI tool:\nhttp://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-multiple-profiles\nAny chance that those env vars could be consulted as well?\n. ",
    "phoolish": "I might add that it would be helpful to have cross account role support as defined in this blog post.\n. Don't know too much go, but I was looking for a holiday project.\n. ",
    "smile-on": "@jasdel This issue with sdk ignoring shared config may be very annoying to anyone who uses aws sdk in the project that more than just a toy. It seams natural for integration tests to use connectivity config stored at the server in shared location and that would run into the limitation issue.\nI'd like to make simple patch to aws-sdk-go that would allows sdk to read region from shared config if no value is given explicitly in code (as mentioned in #489). This is much simpler functionality and easy to test scope that this #384. The most importantly it helps in running practical integration test that share both credentials and config with CLI tools on the server. Would such simple patch be considered as a pool request or there is a plan for something bigger?\n. There is similar issue #384 where this is stated as limitation rather than bug. Would be nice to make this \"limitation\" documented, well at least be clear in describe-instances sdk example and instead of vague\n// Note that you can also configure your region globally by\n// exporting the AWS_REGION environment variable\nwrite clearly\n// Note that if you want configure your region globally \n// due to current limitation, you must configure it by\n// exporting the AWS_REGION environment variable\n// setting in ~/.aws/config file is ignored.\nSame way as programmer does \"AWS_REGION must be configured to run integration tests\"\n. ",
    "Bowbaq": "Ran into this a few days ago, so I wrote a provider that reads from the CLI config file\n. ",
    "temujin9": "@jasdel Have any decisions been made on priority for this? Its lack blocks a rather large chunk of my upcoming work.\n. https://github.com/Bowbaq/profilecreds has worked as a workaround: I would recommend it as a stepping-off point.\n. Unfortunately, abandoning this appears to have left the requirement to set AWS_SDK_LOAD_CONFIG. Attempting to load an assumed role without that set results in\nNoCredentialProviders: no valid providers in chain. Deprecated. \n        For verbose messaging see aws.Config.CredentialsChainVerboseErrors\nPer discussion in https://github.com/aws/aws-sdk-go/issues/472 that flag should be removed, as allowing loading of assumed roles should be standard behavior.\n. As discussed in the prior thread, making this functionality opt-in, rather than the default, is an idea that was considered and quickly abandoned by the Ruby SDK. I have worked around this already in code, so I don't have much of a dog in the fight, but I agree with that assessment.\n. So update to the next major version for this library, and make the change there. Version pinning will allow preservation of existing behavior for old applications.\nPreserving bad semantics indefinitely for historical reasons is poor practice.. ",
    "mtibben": "Hey thanks @jasdel, to be honest I can't remember the exact use-case I was having issues with when reporting this bug, but these days I'm using https://github.com/99designs/aws-vault which automatically sets all the potential environment variables like AWS_REGION and AWS_DEFAULT_REGION based on the ~/.aws/config. This has been working quite nicely for some time. \nThis work is certainly appreciated however, hopefully it leads to a smoother UX when using the SDK! Thanks!\n. ",
    "gregory-m": "@jasdel thank you for quick fix.\n. ",
    "mjibson": "Yes. Thanks.\n. ",
    "vito": "woops, a few issues with this, sorry; it probably panics on unversioned buckets, and doesn't handle the single-part case. fixing now.\n. Great, thanks!\n. ",
    "reedobrien": "Are you on windows?\n.  I would make that guess because a windows file path separator would be \"\\\" which doesn't appear as a \"folder\" in s3. In fact there are no \"folders\" in s3, but AWS console makes paths with \"/\" appear hierarchical. Try joining with \"/\". I.e. 'path.Join'. I don't have windows so I don't know if that will use \"\\\" or \"/\". \n. I don't know about \"design\" intentions. Probably more like cognitive impedance. There is no hierarchy in s3. Each object has a key, this/is/not/a/path/but/a/key.txt could just as easily be this\\is\\not\\a\\path\\but\\a\\key.txt, however those are two different keys representing two different objects. \nThings can share a prefix... but there is no hierarchy. The s3 console does a bit of a disservice to users making it appear as a file system hierarchy, but I suppose that lowers the cognitive load for new users. \n. Does this example help?\nhttps://github.com/aws/aws-sdk-go/blob/master/service/s3/examples_test.go#L327\nOn Wed, Jan 13, 2016, 02:22 Gleb notifications@github.com wrote:\n\nI'm wonder that i didnt found any mentions in docs how to delete file by\nkey from bucket\nIs it possible? Because bucket struct is perry lightweight\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-go/issues/500.\n. NP\n. \n",
    "aweick": "Sorry, yes this is occurring on Windows.\n. That did it, thanks! I am going to have to code around this depending on the OS. Seems like this should be handled by the UploadManager or am I mistaken and this is as designed?\n. Hi @jasdel. I understood that S3 does not really have folders. The mistake I made was thinking that the SDK would handle the slashes appropriately for S3. As the example code on the wiki yields different results depending on the platform I would recommend that it get updated to something agnostic as well.\n. ",
    "phemmer": "Ah, ok I think I see why this was set to GMT. RFC822 uses GMT, not UTC. But golang's time formatter uses the string UTC. So the SDK is hard coding GMT in the time format and manually ensuring that all times are converted to UTC before formatting.\nThe problem this is causing is that if a date header in a response comes back with a time zone other than the literal GMT, the parser throws an error.\n. ",
    "rosenhouse": "@jasdel Yeah, that makes sense.  Right now I'm only using the template package in some exploratory work, so go ahead, I can fix up import statements as needed.\n. @jasdel: Any update here?   Would you be open to merging this PR as it currently stands?  We could follow up later on with the generated resource & resource-property structs.\n. Thanks for the review.  I've rebased against master, replaced my assertEquivalentJSON with awstesting.AssertJSON and added a package doc header.\n. Great, thank you!\n. ",
    "Skarlso": "So... um. This is an open issue and the related repo has not been touched in a year. \nI see the help wanted sign. So there is This Guys template parser: https://github.com/crewjam/go-cloudformation which looks promising. Care to chip in @crewjam?. Yep, agreed. :-). I think the waiter is fairly fine. Writing something which hooks into it is pretty easy with Go. Consider this example:\n```go\nfunc WaitForFunctionWithStatusOutput(state string, freq int, f func()) {\n    var wg sync.WaitGroup\n    wg.Add(1)\n    done := make(chan bool)\n    go func() {\n        defer wg.Done()\n        f()\n        done <- true\n    }()\n    go func() {\n        counter := 0\n        for {\n            counter = (counter + 1) % len(Spinners[config.SPINNER])\n            fmt.Printf(\"\\r[%s] Waiting for state: %s\", yellow(string(Spinners[config.SPINNER][counter])), red(state))\n            time.Sleep(time.Duration(freq) * time.Second)\n            select {\n            case <-done:\n                fmt.Println()\n                break\n            default:\n            }\n        }\n    }()\nwg.Wait()\n\n}\n```\nWhich is a very simple way if outputting a spinner for example on every tick. I would call this like this:\ngo\nfunc (cf *CFClient) waitForStackComplete(stackname string) {\n    describeStackInput := &cloudformation.DescribeStacksInput{\n        StackName: aws.String(stackname),\n    }\n    utils.WaitForFunctionWithStatusOutput(\"CREATE_COMPLETE\", config.WAITFREQUENCY, func() {\n        cf.Client.WaitUntilStackCreateComplete(describeStackInput)\n    })\n}. Never mind. I had to update all my packages with go get -u all after updating go to 1.8. I always forget that... :/. I also had GOROOT defined to an older install whilst using brew for newer versions. Ahh.. :). ",
    "crewjam": "I wasn't aware of the work in the repo you mention. One of the design goals of go-cloudformation was to leverage the go type system to avoid errors, admittedly at the expense of a slightly more arcane syntax, i.e. cfn.Ref(\"Whatever\").String() etc.\nThe next piece of effort were go-cloudformation could be improved is to ditch the documentation scraper and replace it by the schema AWS have published, as described in https://github.com/crewjam/go-cloudformation/issues/19.\nMy $.02 \n. ",
    "vtapaskar": "Jsaon,\nJSON to error-code, with const is good start, but I was hoping that common\nerror will be abstracted and moved out in awserr level. One example of this\ncould be auth related errors, they are independent of which service is\nactually trying use it.\nIts shouldn't that every service has Const, that is redefining the same\nerror.\nAlso just curious, does the httpStatusCode needs to be propagated out may\nbe as well...because depending if there are corp proxies in the path then\nhttp errors might be useful as well.\nThanks,\nVijay\nOn Mon, Sep 28, 2015 at 10:40 AM, Jason Del Ponte notifications@github.com\nwrote:\n\nThanks for creating this issue @vtapaskar https://github.com/vtapaskar\nI'll add this to our backlog.\nTo implement this I think we need update our API generation code (\ninternal/model) to extract the error codes from the API model json docs.\nMore investigation is needed, but it may make sense to collect shapes which\nhave an error.code property, and write the error shape name and error\ncode string as const for each service api.\nAuto Scaling group error example:\n\"LimitExceededFault\":{\n  \"type\":\"structure\",\n  \"members\":{\n    \"message\":{\"shape\":\"XmlStringMaxLen255\"}\n  },\n  \"error\":{\n    \"code\":\"LimitExceeded\",\n    \"httpStatusCode\":400,\n    \"senderFault\":true\n  },\n  \"exception\":true\n}\nThis would generate something like the following in\nservice/autoscaling/api.go\nconst (\n     // Errors\n     ErrCodeLimitExceededFault = \"LimitExceeded\"\n     // ...\n)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-go/issues/394#issuecomment-143816357.\n. \n",
    "polds": "Just adding my +1 to this. It's nice the docs show the potential error codes the methods return but we just ran into an issue where we were trying to catch a ResourceNotFoundException and after some debugging found out it had been misspelled ResourceNotFoundExeption (missing the 'c').\nI was noticing that ResourceNotFoundException is thrown all over the place so something like:\nif aerr, ok := err.(awserr.Error); ok {\n   if aerr.Code() == awserr.ResourceNotFoundException {\n   }\n}\nwould be a very welcome addition.. ",
    "nickschuch": "Awesome!\n. ",
    "caitlin615": "Hey @jasdel, here's a sample where we're running into the 2 minute timeout:\n``` go\npackage main\nimport (\n    \"fmt\"\n    \"time\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/service/elasticbeanstalk\"\n\n)\nfunc main() {\n    start := time.Now()\n    fmt.Println(\"starting\", start.String())\nconfig := aws.NewConfig().WithRegion(\"us-east-1\")\nclient := elasticbeanstalk.New(config)\n\nparams := &elasticbeanstalk.DescribeApplicationsInput{}\nresp, err := client.DescribeApplications(params)\n\nif err != nil {\n    fmt.Println(err.Error())\n}\n\nfor _, app := range resp.Applications {\n    fmt.Println(aws.StringValue(app.ApplicationName))\n}\n\nelapsed := time.Since(start)\nfmt.Println(\"took\", elapsed)\n\n}\n```\nOutput:\nStarting now 2015-10-09 10:25:16.579294564 -0400 EDT\nNoCredentialProviders: no valid providers in chain\ntook 2m0.371379647s\n. We use the default behavior so our developers can run it with environment variables or shared credential file and our ec2 instances can use the ec2 instance role. We don't want to restrict which credential it uses. \nWe noticed this behavior when it can't find any. It works great if it can find at least one, but if it can't find any, it takes 2 minutes to error out.\n. Works great, thanks so much!\n. ",
    "kstokoz": "@andrewgaul Any updates on your sign v2 for S3?\n. ",
    "SpectralHiss": "@andrewgaul @jasdel we're starting a spike on v2 s3 signing too, as we could really benefit from this in order to interface with Ceph. Please let us know if you're close to completion!\n. ",
    "tumiao": "S3 v2 signing is not supported in current version, right?\nWhat if I had to use v2 signing? Should I modify the source code of v2.go?\nTHX~\n. ",
    "pauldtong": "+1\n. ",
    "wmh": "Got it, thanks. :+1: \n. The error message apears when I use dynamodb.DynamoDB.PutItem()\n. I'm looking forward to seeing dynamodbattribute.ConvertX :+1:\n. :+1: \n. ",
    "brunoksato": "@jasdel  Thanks for the fast reply ! \nthanks for the tips, I'll try to apply here\nbut AssumeRoleWithWebIdentity not have in documentation right ?\nwhat params AssumeRoleWithWebIdentity ? \nthe first validation generate token with GetOpenIdTokenForDeveloperIdentity\nhow to validate this token again ? \n. Hi @jasdel have problem in AssumeRoleWithIdentity, generate token public but I do not know validate this token in all request api\nwhich api user to validate the token is still valid ?\n. resolved\n. ",
    "tskinn": "The list of instances I was getting from another aws source (ecs) included ec2 instances that were terminated and no longer existed. That is why the DescribeInstances() was not returning more instances.\n. ",
    "lixingwang": "Got it, thanks @jasdel \n. ",
    "psankar": "If someone wants to contribute this, are there any pointers for where to begin ? Thanks.\n. @jasdel Thanks. I will look at the docs. I glanced over the docs and found that they use decoration / annotation feature in Java, .NET and Python. What do you think will be the best way to get such a feature in Go ? AFAIK, Go does not support decorators.\nAlso, it will be good, if we could create a google group or some such mailing list for these discussions. I prepared one test program yesterday after plenty of hardwork which will update items in a dynamodb table. I wanted to make that an example program, but have no easy way to ask for comments due to the lack of a mailing list.\n. The main problem with using Gitter is that the conversations are not google searchable. I feel it will be much more useful to have a users mailing list. As we get more adoption, we will have more requests for this. Let us leave that debate aside for now :)\nI will send a PR with my sample program.\nRegarding this issue, I really am clueless as to what will be the right way to implement this. May be we can ask in golang nuts mailing list ?\nMy current project has a big need for having version protected locks, to prevent conflicting parallel writes. It is infact the reason for choosing Dynamodb, as otherwise we might have went with mysql and implemented a request queue in front to prevent parallel updates. So, I am not sure how feasible it may be to use the Go SDK for my current project (which is in the initial stage). Are there any workarounds ?\n. > we'd could consider a builder pattern\nCan you elaborate on this a bit ?\n. Sure. I will create the file in the right folder. This PR is just to seek code review comments. I did not even know about the WaitUntilTableExists until the review comments. Thanks for the comments. Please keep them coming. I am travelling and so will not be able to address the review comments before next week. I will fix them and will give a new PR then. Thank you so much.\n. ",
    "ablewhiskey": "Thanks!\n. ",
    "salmanbukhari": "Okay i will post it there as well and the expection name is also confusing. InvalidParameterCombination looks like the combination of values of EnableDnsHostnames and EnableDnsSupport is not valid somehow. \n. ",
    "jsgv": "No problem :+1: \n. ",
    "philpennock": "thanks for the quick fix.\n. ",
    "integrii": "I just hit this today.  It is very confusing and tricky that the request requires a Delimiter property.  As a developer, I need to be able to assume that if IsTruncated is true, then I can fetch the NextMarker... Instead you're throwing me a nil and exploding my program.  Please re-open and fix?. Put a valid bucket name in here.  The program will explode when the nil NextMarker is returned.  Uncomment the delimiter property in the request and the program will work.\n```go\npackage main\nimport (\n    \"fmt\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\n)\nfunc main() {\nsess := session.Must(session.NewSession(&aws.Config{\n    Region: aws.String(\"us-east-1\"),\n    CredentialsChainVerboseErrors: aws.Bool(true),\n}))\n\nclient := s3.New(sess)\nvar params = &s3.ListObjectsInput{\n    Bucket:  aws.String(\"a.valid.bucket.name\"), // put a valid bucket name here\n    MaxKeys: aws.Int64(5),\n    // Delimiter: aws.String(\"-\"), // Uncomment this line to fix the program\n}\n\nfor {\n    // send the request\n    resp, err := client.ListObjects(params)\n    if err != nil {\n        panic(err)\n    }\n\n    // if results truncated, keep paginating\n    if *resp.IsTruncated {\n        fmt.Println(\"Results truncated... paginating.\")\n\n        // list outputs\n        for _, k := range resp.Contents {\n            fmt.Println(k.String())\n        }\n\n        fmt.Println(\"Next Marker:\", *resp.NextMarker)\n\n        params.SetMarker(*resp.NextMarker)\n        continue\n    }\n    break\n}\n\n}\n```. ",
    "stack72": "@jasdel thanks forgetting back to me. I'm not getting an error response - I just cannot change the value of MeasureLatency once it is set\n. Thanks @jasdel - thought that may be the case :) Thanks for getting back to me here!\n. @xibz this indeed was the issue - thanks so much!\n. Hi @xibz, any thoughts on this?\n. nps :) Thanks for the follow up\n. Hi @xibz \nMy apologies I wasn;t clear here. I mean that when i query the DescribeLogGroups that the value of 7 is not returned for RetentionInDays. that value of 7 is only available after a short (indeterminate) amount of time. My thought here is that the LogGroup takes some time for the change to be applied. But how can i know when the LogGroup has been updated? \nP.\n. @xibz \nThanks for the repo here. I will just check the state of the LogGroup to make sure that it isn't in a pending state. If it doesn't help then I will re-comment\nThanks\nPaul \n. yeah that was what i was trying to achieve. How do I know when a retention period has been added to the LogGroup? It's not an immediate thing\n. thanks so much for all the help here @xibz :)\n. Hi, any thoughts on the issue here?\nthanks \nPaul\n. Awesome :) Thanks!\n. Hi @xibz \nI am trying to get this working now. If the Cloudwatch Logs say it works as so then I will find the fix on my side. I will close the issue until it has been proven otherwise\nThanks for all the work here\nPaul\n. Hi @xibz \nI cannot seem to replicate this at all now. I have just ran the tests like 5 times and each time it came back as expected.\nI should close this and not take up any more of your time! Sorry if I have wasted any effort\nPaul\n. \ud83d\udc4d \n. Thanks for keeping the Terraform issue updated @jasdel :)\n. So is his something will be acted on, something I need to raise elsewhere or just ignore? This feels a pretty serious bug as infrastructure can be easily modified without any intention to. Closing it without any idea of what happens next seems to be an odd choice\n. @xibz thanks for the fast response - this will indeed work :) thanks\nPaul. @xibz \nthanks for this - so there is no need to run the req.Send()?\nP.. got it - let me try that now\nWill report back within the hour or so\nThanks for this. @xibz you are correct - that fixes my issue :) Thanks so much!. @xibz ok, this code works but it doesn't actually create the replica - we just go into a waiting pattern for it to actually get deployed and nothing happens:\nThe code looks as follows now:\nreq, out := conn.CreateDBInstanceReadReplicaRequest(&opts)\n_, presign_err := req.Presign(time.Minute * 5)\nif presign_err != nil {\n    return fmt.Errorf(`provider.aws: aws_db_instance: %s: error encountered calling Presign on request: %s `, identifier, presign_err)\n}\nlog.Printf(\"[DEBUG] CreateDBInstanceReadReplicaRequest Options: %#v\", opts)\nlog.Printf(\"[DEBUG] CreateDBInstanceReadReplicaRequest Request: %#v\", req)\nlog.Printf(\"[DEBUG] CreateDBInstanceReadReplicaRequest Output: %#v\", out)\nThe responses are as follows:\n2017/02/24 01:04:22 [DEBUG] CreateDBInstanceReadReplicaRequest Options: {\n   CopyTagsToSnapshot: false,\n   DBInstanceClass: \"db.t2.large\",\n   DBInstanceIdentifier: \"tf-replica-db-1\",\n   DBSubnetGroupName: \"foobarbaz-test-1\",\n   DestinationRegion: \"us-west-2\",\n   KmsKeyId: \"arn:aws:kms:us-west-2:187416307283:key/86185527-b4b7-47e6-9740-bd853ea52945\",\n   PubliclyAccessible: false,\n   SourceDBInstanceIdentifier: \"arn:aws:rds:eu-central-1:187416307283:db:foobarbaz-test-terraform-1\",\n   SourceRegion: \"eu-central-1\",\n   Tags: [{\n       Key: \"Name\",\n       Value: \"tf-replica-db-1\"\n     }]\n }\n```\n[DEBUG] CreateDBInstanceReadReplicaRequest Url: \"https://rds.us-west-2.amazonaws.com/?Action=CreateDBInstanceReadReplica&CopyTagsToSnapshot=false&DBInstanceClass=db.t2.large&DBInstanceIdentifier=tf-replica-db-1&DBSubnetGroupName=foobarbaz-test-1&DestinationRegion=us-west-2&KmsKeyId=arn%3Aaws%3Akms%3Aus-west-2%3A187416307283%3Akey%2F86185527-b4b7-47e6-9740-bd853ea52945&PubliclyAccessible=false&SourceDBInstanceIdentifier=arn%3Aaws%3Ards%3Aeu-central-1%3A187416307283%3Adb%3Afoobarbaz-test-terraform-1&Tags.member.1.Key=Name&Tags.member.1.Value=tf-replica-db-1&Version=2014-10-31&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAI2TTU7QGVZGXTG4Q%2F20170223%2Fus-west-2%2Frds%2Faws4_request&X-Amz-Date=20170223T231812Z&X-Amz-Expires=300&X-Amz-SignedHeaders=host&X-Amz-Signature=ef25d5c566a0d70b9e0594eab7c85ad6372243c2a2e0554a858bab14803c7c30\"\n2017/02/24 01:04:22 [DEBUG] CreateDBInstanceReadReplicaRequest Output: {\n}\n```\nThe output is empty\nDoes your code actually create the replica? or just happen to do the same thing?. Hi @xibz \nWhen i do the following now:\nreq, out := conn.CreateDBInstanceReadReplicaRequest(&opts)\npresignUrl, presign_err := req.Presign(time.Minute * 5)\nif presign_err != nil {\n  return fmt.Errorf(`provider.aws: aws_db_instance: %s: error encountered\n  calling Presign on request: %s `, identifier, presign_err)\n}\nresp, err := http.Get(presignUrl)\nif err != nil {\n  return err\n}\nlog.Printf(\"[DEBUG] CreateDBInstanceReadReplicaRequest Url: %#v\", presignUrl)\nlog.Printf(\"[DEBUG] CreateDBInstanceReadReplicaRequest Resp: %#v\", resp)\nlog.Printf(\"[DEBUG] CreateDBInstanceReadReplicaRequest Options: %#v\", opts)\nlog.Printf(\"[DEBUG] CreateDBInstanceReadReplicaRequest Output: %#v\", out)\nThe response from the http.Get returns the following:\n[DEBUG] CreateDBInstanceReadReplicaRequest Resp: &http.Response{Status:\"400 Bad Request\", StatusCode:400, Proto:\"HTTP/1.1\", ProtoMajor:1, ProtoMinor:1, Header:http.Header{\"X-Amzn-Requestid\":[]string{\"1196037c-fa20-11e6-b950-2db38d0d26b9\"}, \"Content-Type\":[]string{\"text/xml\"}, \"Content-Length\":[]string{\"292\"}, \"Date\":[]string{\"Thu, 23 Feb 2017 23:30:36 GMT\"}}, Body:(*http.bodyEOFSignal)(0xc4210b1d40), ContentLength:292, TransferEncoding:[]string(nil), Close:true, Trailer:http.Header(nil), Request:(*http.Request)(0xc4210b2270), TLS:(*tls.ConnectionState)(0xc420f619e0)}\nAny reason why the presigned url above would return a 400? Is it because my opts being passed to the Request have more than the 4 fields you have above?. % go version                                                                                                     \ngo version go1.7.4 darwin/amd64\nok, so by only supplying the 4 options as follows:\nopts := rds.CreateDBInstanceReadReplicaInput{\n  SourceDBInstanceIdentifier: aws.String(v.(string)),\n  DBInstanceIdentifier: aws.String(identifier),\n  KmsKeyId: aws.String(kms_attr.(string)),\n  SourceRegion: aws.String(sourceRegion.(string)),\n}\nI then get a URL that returns a 200 response\n[DEBUG] CreateDBInstanceReadReplicaRequest Url: \"https://rds.us-west-2.amazonaws.com/?Action=CreateDBInstanceReadReplica&DBInstanceIdentifier=tf-replica-db-1&DestinationRegion=us-west-2&KmsKeyId=arn%3Aaws%3Akms%3Aus-west-2%3A187416307283%3Akey%2F86185527-b4b7-47e6-9740-bd853ea52945&PreSignedUrl=https%3A%2F%2Frds.eu-central-1.amazonaws.com%2F%3FAction%3DCreateDBInstanceReadReplica%26DBInstanceIdentifier%3Dtf-replica-db-1%26DestinationRegion%3Dus-west-2%26KmsKeyId%3Darn%253Aaws%253Akms%253Aus-west-2%253A187416307283%253Akey%252F86185527-b4b7-47e6-9740-bd853ea52945%26SourceDBInstanceIdentifier%3Darn%253Aaws%253Ards%253Aeu-central-1%253A187416307283%253Adb%253Afoobarbaz-test-terraform-1%26Version%3D2014-10-31%26X-Amz-Algorithm%3DAWS4-HMAC-SHA256%26X-Amz-Credential%3DAKIAI2TTU7QGVZGXTG4Q%252F20170223%252Feu-central-1%252Frds%252Faws4_request%26X-Amz-Date%3D20170223T234328Z%26X-Amz-Expires%3D300%26X-Amz-SignedHeaders%3Dhost%26X-Amz-Signature%3D6f8ca48d7ffb5ff18f8e75ecbd04b984e154cc50dc42c04159a2591b1f33b670&SourceDBInstanceIdentifier=arn%3Aaws%3Ards%3Aeu-central-1%3A187416307283%3Adb%3Afoobarbaz-test-terraform-1&Version=2014-10-31&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAI2TTU7QGVZGXTG4Q%2F20170223%2Fus-west-2%2Frds%2Faws4_request&X-Amz-Date=20170223T234328Z&X-Amz-Expires=300&X-Amz-SignedHeaders=host&X-Amz-Signature=7ad992cea3fae626ed9c8a61af63e6d462b61c91f463c98180f9add36e5fcd50\"\n[DEBUG] CreateDBInstanceReadReplicaRequest Resp: &http.Response{Status:\"200 OK\", StatusCode:200, Proto:\"HTTP/1.1\", ProtoMajor:1, ProtoMinor:1, Header:http.Header{\"X-Amzn-Requestid\":[]string{\"e0f6e40d-fa21-11e6-99e0-5d0069e0435b\"}, \"Content-Type\":[]string{\"text/xml\"}, \"Vary\":[]string{\"Accept-Encoding\"}, \"Date\":[]string{\"Thu, 23 Feb 2017 23:43:34 GMT\"}}, Body:(*http.bodyEOFSignal)(0xc4210d0900), ContentLength:-1, TransferEncoding:[]string{\"chunked\"}, Close:false, Trailer:http.Header(nil), Request:(*http.Request)(0xc421028410), TLS:(*tls.ConnectionState)(0xc420e04e40)}\nBut the output from the request is still empty at this point so, no replica is being created:\n[DEBUG] CreateDBInstanceReadReplicaRequest Output: {\n }\n. yes i see it being created now - thanks!. Thanks @xibz - I tried this and it wasn't allowed so I think there is validation in there to only allow aurora - maybe the SDK needs to change?. Hi @jasdel \nthanks for the response here. Unfortunately, this doesn't work either:\nthis is my request:\n```\n2017/08/17 07:50:43 [DEBUG] [aws-sdk-go] DEBUG: Request rds/CreateDBCluster Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nPOST / HTTP/1.1\nHost: rds.us-west-2.amazonaws.com\nUser-Agent: aws-sdk-go/1.10.25 (go1.8.1; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.10.0-dev\nContent-Length: 401\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\nX-Amz-Date: 20170817T045043Z\nAccept-Encoding: gzip\nAction=CreateDBCluster&AvailabilityZones.AvailabilityZone.1=us-west-2c&AvailabilityZones.AvailabilityZone.2=us-west-2b&AvailabilityZones.AvailabilityZone.3=us-west-2a&BackupRetentionPeriod=5&DBClusterIdentifier=aurora-cluster-demo&DatabaseName=mydb&Engine=aurora-postgresql&MasterUserPassword=bar&MasterUsername=foo&PreferredBackupWindow=07%3A00-09%3A00&StorageEncrypted=false&Tags=&Version=2014-10-31\n```\nNotice that the ending is set to aurora-postgresql\nThe response I get is:\n```\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 400 Bad Request\nConnection: close\nContent-Length: 269\nContent-Type: text/xml\nDate: Thu, 17 Aug 2017 04:50:44 GMT\nX-Amzn-Requestid: a09ec9b1-8307-11e7-9140-054a3c85b027\n\n2017/08/17 07:50:45 [DEBUG] [aws-sdk-go] \n\nSender\nInvalidParameterValue\nInvalid DB engine\n\na09ec9b1-8307-11e7-9140-054a3c85b027\n\n```\nPaul. Thanks @jasdel \nJust a FYI, this is a preview feature so it may just not have hit the SDK / API yet\nP.. ",
    "wolfgangmeyers": "Glad I'm not the only one who ran into this. Can someone please update the documentation in the readme to reflect this? It still shows the old method of creating service clients.\n. Sorry, not sure how I missed that. The signature in the readme is correct.\nThanks!\n. I disagree, this seems like the best place to ask the question. I think the answer is that it's not currently implemented, because I can't find anything in the api resembling the words \"list runs\".. For what it's worth, I found a similar question regarding the boto library, and it looks like multiple apis were being called under the covers by the CLI: https://github.com/boto/boto3/issues/1029. One important note: you need to use Sphere: aws.String(\"INSTANCE\") in the QueryObjects call, then pass the IDs returned into the DescribeObjects call.. Yes that makes sense. I was under the mistaken impression that it was a service-level API. Thanks!. ",
    "tfutada": "The said issue seems to persist in cloudwatch package.\nsvc := cloudwatch.New(session.New(), &aws.Config{Region: aws.String(\"us-east-1\")})\nThis causes the following error.\n./charge.go:13: cannot use session.New() (type *session.Session) as type \"github.com/aws/aws-sdk-go/aws/client\".ConfigProvider in argument to cloudwatch.New:\n    *session.Session does not implement \"github.com/aws/aws-sdk-go/aws/client\".ConfigProvider (wrong type for ClientConfig method)\n        have ClientConfig(string, ...*\"github.com/tfutada/vendor/github.com/aws/aws-sdk-go/aws\".Config) \"github.com/tfutada/vendor/github.com/aws/aws-sdk-go/aws/client\".Config\n        want ClientConfig(string, ...*\"github.com/aws/aws-sdk-go/aws\".Config) \"github.com/aws/aws-sdk-go/aws/client\".Config\n./charge.go:13: cannot use \"github.com/tfutada/vendor/github.com/aws/aws-sdk-go/aws\".Config literal (type *\"github.com/tfutada/vendor/github.com/aws/aws-sdk-go/aws\".Config) as type *\"github.com/aws/aws-sdk-go/aws\".Config in argument to cloudwatch.New\nI am using go1.6 darwin/amd64 and the latest AWS GO SDK tagged as v1.1.31\n. Oh! It works! I totally forgot to have it in vendor folder as well. Really appreciate it.\n. ",
    "AbhishekSaha": "This is on a university Linux server,so I didn't install go. The version of Go is: go1.2 linux/amd64.\nThe latter instructions you mentioned results in these errors when calling \"go install ./...\":\n/usr/lib64/golang/src/pkg/golang.org/x/tools/go/loader (from $GOROOT)\n/ilab/users/as1695/src/golang.org/x/tools/go/loader (from $GOPATH)\n /ilab/users/as1695/src/github.com/aws/aws-sdk-go/awsmigrate/awsmigrate-renamer/rename/rename.go:13:2: cannot find package \"golang.org/x/tools/go/types\" in any of:\n/usr/lib64/golang/src/pkg/golang.org/x/tools/go/types (from $GOROOT)\n/ilab/users/as1695/src/golang.org/x/tools/go/types (from $GOPATH)\n  /ilab/users/as1695/src/github.com/aws/aws-sdk-go/awstesting/assert.go:16:2: cannot find package   \"github.com/stretchr/testify/assert\" in any of:\n/usr/lib64/golang/src/pkg/github.com/stretchr/testify/assert (from $GOROOT)\n/ilab/users/as1695/src/github.com/stretchr/testify/assert (from $GOPATH)\n/ilab/users/as1695/src/github.com/aws/aws-sdk-go/awstesting/integration/smoke/shared.go:13:2: cannot find package \"github.com/lsegal/gucumber\" in any of:\n/usr/lib64/golang/src/pkg/github.com/lsegal/gucumber (from $GOROOT)\n/ilab/users/as1695/src/github.com/lsegal/gucumber (from $GOPATH)\n. What was the old way of setting the client Config up in 0.9.14? The session stuff won't work for 0.9.14\n. Fix for \"undefined: aws.DefaultRetries\" ? This is with 0.9.14\n. go run routes.go\ngithub.com/aws/aws-sdk-go/aws/defaults\n/ilab/users/as1695/src/github.com/aws/aws-sdk-go/aws/defaults/defaults.go:36: undefined: aws.DefaultRetries\n. I do, what should I do to fix this error?\n. ",
    "upccup": "hi @jasdel Thank you for your answer. But I think it does not solve my problem.  I use  jsonutil.UnmarshalJSON to automatically parse message when a message is received. so i do not want to use 'switch' and just add the type to a type list like https://github.com/aws/amazon-ecs-agent/blob/master/agent/acs/client/acs_client_types.go#L32.  I want to know whether I can do.\nThanks.\n. @jasdel  I add a new function like this\n```\nfunc unmarshalRawMessage(value reflect.Value, data interface{}) error {\n    if data == nil {\n        return nil\n    }\nmapData, ok := data.(map[string]interface{})\nif !ok {\n    return fmt.Errorf(\"JSON value is not a map (%#v)\", data)\n}\n\nbytes, err := json.Marshal(mapData)\nif err != nil {\n    return err\n}\n\nl := len(bytes)\nif value.IsNil() {\n    value.Set(reflect.MakeSlice(value.Type(), l, l))\n}\nvalue.SetBytes(bytes)\nreturn nil\n\n}\n```\nand noe my problem is resolved Thank you very much\n. ",
    "rjeczalik": "\nI'd like to find a way to ensure at compile time RequestRetryer satisfies the client.Retryer interface without circular dependencies.\n\nIt won't be possible with current aws-sdk-go. The session.Session is not accessible within client.New; even if it was there's another problem to solve - typically session is exposed to services as client.ConfigProvider, there's no guarantee user will not use his own type; and the problem you noted, cluttering the API with different configuration objects.\nIn order to have compile-time type checking we can add helper methods:\n``` go\npackage request\nfunc WithRetryer(cfg aws.Config, retryer Retryer) aws.Config {\n    cfg.Retryer = retryer\n    return cfg\n}\nfunc WithHandler(cfg aws.Config, fn func(Handlers)) *aws.Config {\n    cfg.Handlers = append(cfg.Handlers, fn)\n    return cfg\n}\n```\nHaving those in place I'd be able to write:\n``` go\npackage sharedpkg\nvar Config = request.WithRetryer(aws.NewConfig().WithHTTPClient(client), Retryer)\n```\ngo\nsvc := route53.New(session, sharedpkg.Config)\n. So the ctor for each service looks the same:\ngo\nfunc New(cfg client.ConfigProvider, mixins ...*aws.Config) *Service\nFrom my experience as a user the cfg is usually a value that holds authorisation settings (key, secret, token etc.) and is shared among all services, where mixins are properties local to the particular service (e.g. I would use different retryer and http client for s3 and ec2).\nMaking the retryer/handler and any other field, that can't be imported directly by the aws package, part of session.Session would not make sense as the user would need then to merge manually different session.Session values.\nIf the Retryer is part of aws.Config then a service with custom Retryer can be created simply:\ngo\nfunc NewService(auth *session.Session) *Service {\n  return &Service{\n    client: ec2.New(auth, sharedpkg.ResiliantRetryerConfig),\n  }\n}\nIf the Retryer is part of session.Session then I'd need to write:\ngo\nfunc NewService(auth *session.Session) *Service {\n  authCopy := *auth // or deep-copy?\n  authCopy.Retryer = sharedpkg.ResiliantRetryer\n  return &Service{\n    client: ec2.New(&authCopy),\n  }\n}\nIf the latter is introduced I would probably just stick to overwriting the .Client.Retryer field directly as it is right now. :)\nIf the higher aim is to have compile-time safety over composability, I'd merge aws and aws/request packages, so all the configuration types sit in one package. But probably this is so big change for a so small gain that it does not warrant the breakage and unhappy users :-)\n. @jasdel Hey, let me know if you're ok with my initial proposal, if yes I'll send PR for it. Or if not and we probably need different solution.\n. @jasdel: Yay, thanks!\n. @jasdel I tried adding ec2metadata to defaults package, it failed due to one more cycle in ec2rolecreds tests:\nec2rolecreds_test -> ec2rolecreds -> defaults -> session -> ec2rolecreds\nThen I tried adding ec2metadata/defaults package, and importing it within ec2rolecreds, also a cycle. It seems like there's not an easy way breaking out of it.\nDo we have any other options? If not, I'm going to send PR against terraform instead.\n. @jasdel I agree, thanks for helping me out :)\n. @jasdel Oh I need to be faster next time, wanted to do the same :D\n. ",
    "rking788": "hi @jasdel, just to clarify, in your example solution the last line says \nsess.Config.Credentials = creds\nbut it seems like creds is not defined in your example. did you mean this instead?\nsess.Config.Credentials = credentials\n. thanks for clarifying, this fixed my problem with empty metadata in the RoleProvider\n. ",
    "scrivy": "I'm sure others would appreciate some additional documentation in http://docs.aws.amazon.com/sdk-for-go/api/aws/credentials/ec2rolecreds/\nI made the same mistake.\n. ",
    "abustany": "(for the record: I based this PR on https://github.com/aws/aws-sdk-go/commit/51ee3809fa0ef2f387e00e6c07478907486869e7)\n. Woooops, hadn't run make unit that runs the linter, fixed the struct names, CI should now hopefully pass.\n. ok, thanks a lot for the guidance, I'll cook a patch based on that. Looking at the APIs I know from AWS (and quickly grepping through the JSON API specs), I can't really find any other kind of batched errors (S3 multi-delete for example still returns HTTP 200 even if some deletes fail). There are many AWS APIs I'm not familiar with though, so it's quite likely that I missed some.\n. I wrote a quick and dirty go script to check errors structures across the various APIs: http://paste.fedoraproject.org/291392/76716614 (you can just go run it in the root of the directory). The output for the current HEAD is at http://paste.fedoraproject.org/291394/14477673 . Unless the script is doing something wrong, it looks like InvalidChangeBatch is the only structure of its kind in the API. I would therefore tend to argue that we should do a route53 specific error, instead of having the code in the restxml parser... If you still prefer to have it in the restxml package, I can of course also go ahead with that.\n. apologies about the delay on that one, I still intend to fix that issue, but have higher priorities at work at the moment.\n. OK, here is a new version of the branch that adds a standard type to awserr for batched errors, and tests for route53 error unmarshalling. I'm still a bit unhappy about the unmarshalling code since it duplicates some of the code in the restxml package. If you're fine with me adding an additional method there to unmarshal an error from a given []byte payload instead of the body of the HTTP response (which we can't use since we already consumed it in the custom unmarhsaller), we can get rid of the code duplication (ie the standardXMLErrorResponse struct).\n. So I updated the branch again, it looks like in that case I can get away by simply \"monkey patching\" the Body of the http.Request with a bytes.Reader as you suggested. I'm not fully sure what would be the use of the ByteReadCloser type you're suggesting (are there cases where we need to access the bytes.Reader again?), but then I haven't seen much of the rest of the code base :-)\n. ",
    "dogfoodhead": "Thank you for the reply. I have run my test case against the latest code and the problem has been resolved. Thanks again.\n. ",
    "DavyC": "Thanks for your quick response!\nThat fixes the Expected and I just realized fmt.Println was already removed in the latest commit. However, the following still wouldn't work:\nwaiterCfg := waiter.Config{\n        Operation:   \"DescribeTable\",\n        Delay:       20,\n        MaxAttempts: 25,\n        Acceptors: []waiter.WaitAcceptor{\n            {\n                State:    \"success\",\n                Matcher:  \"path\",\n                Argument: \"Table.TableStatus\",\n                Expected: \"ACTIVE\",\n            },\n            {\n                State:    \"retry\",\n                Matcher:  \"error\",\n                Argument: \"\",\n                Expected: \"ResourceNotFoundException\",\n            },\n        },\n    }\n(originated from dynamodb/waiters.go)\nMatcher:  \"path\" case doesn't exist in the waiter's switch statement. That probably involves either modifying dynamodb/waiters-2.json or handle that new Matcher.\nAs for Matcher:  \"error\", I was thinking about having the following:\n```\ndiff --git a/private/waiter/waiter.go b/private/waiter/waiter.go\nindex 335af91..1bbad80 100644\n--- a/private/waiter/waiter.go\n+++ b/private/waiter/waiter.go\n@@ -47,10 +47,8 @@ func (w Waiter) Wait() error {\n                res := method.Call([]reflect.Value{in})\n                req := res[0].Interface().(request.Request)\n                req.Handlers.Build.PushBack(request.MakeAddToUserAgentFreeFormHandler(\"Waiter\"))\n-               if err := req.Send(); err != nil {\n-                       return err\n-               }\n\nerr := req.Send()\n                for _, a := range w.Acceptors {\n                        result := false\n                        switch a.Matcher {\n@@ -76,6 +74,14 @@ func (w *Waiter) Wait() error {\n                        case \"status\":\n                                s := a.Expected.(int)\n                                result = s == req.HTTPResponse.StatusCode\ncase \"error\":\nif req.Error != nil {\nif awsErr, ok := req.Error.(awserr.Error); ok {\nif awsErr.Code() == a.Expected {\nresult = true\n}\n}\n}\n                        }                if result {\n\n@@ -94,6 +100,9 @@ func (w *Waiter) Wait() error {\n                            break\n                    }\n            }\n+               if err != nil {\n+                       return err\n+               }\n        time.Sleep(time.Second * time.Duration(w.Delay))\n}\n\n```\n\n\nIt may not be right. just my 2 cents :)\n. Working like a charm! Thanks a lot!\nNow I can replace my waiter-like code with the official aws waiter :smile:\n. ",
    "gwatts": "To add a datapoint to this issue; I tripped over the same thing while retrieving metadata i'd set as lowercase keys and finding that i needed to fetch them using canonical header keys.\nI think even adding an accessor method to GetObjectOutput such as GetMetadataValue(k string) string which applies the key normalization would help avoid this kind of confusion down the line.. Confirmed fixed - Thanks very much!\n. To follow up on this, i attempted to create a custom retryer, which would retry on SerializationError.. very simple, of course..\nI then tried to use that:\ncfg := aws.NewConfig()\ncfg = request.WithRetryer(cfg, myRetryer)\ns, _ := session.NewSession(cfg) \ndyn := dynamodb.New(s)\nHowever i'm finding my custom retryer is being clobbered by https://github.com/aws/aws-sdk-go/blob/master/service/dynamodb/customizations.go#L35\nIs there a better way I should be passing that retryer in such that it won't be overwritten on the client by initClient ?. Hi\nMy custom retryer literally just wraps the DefaultRetryer's ShouldRetry method and puts in a catch for SerializationError such that it'll return true in my limited use case for requests i know are idempotent.\nUnfortunately supplying that retryer via WithRetryer doesn't work due to the issue i noted above; i'm not sure the pattern you've linked to would resolve that or not. Thanks for the suggestion.  Needed dyn.Retryer = myretryer in the end as Config doesn't have a WithRetryer method due to that being part of the request package, but after that the workaround works.\nLooks like your PR will render it moot soon though :-). Thanks very much @jasdel - I assume this renders my custom retryer unnecessary.\nYou previously mentioned the difficulty of auto-retrying broken connections due to not implicitly knowing whether the request is idempotent; did you resolve that problem with this PR?. Just tried this with my own program, and it failed to trap the error.\nDumping the error at https://github.com/aws/aws-sdk-go/blob/master/aws/request/retryer.go#L85  shows it trapped:\ngolang\nOpErr.Err &os.SyscallError{\n    Syscall: \"read\",\n    Err:     syscall.Errno(0x68),\n}\nOpErr.Err.Error() then returns read: connection reset by peer\nsyscall.ECONNRESET.Error() on the other hand, returns connection reset by peer so the equality test returns false.\nTesting OpErr.Err.Err == syscall.ECONNRESET seems to do the right thing, however. I also wonder if this even works on Windows?. Thanks @xibz - Tested and it correctly retries for me now; very much appreciate all your work on this SDK!. ",
    "MasterCarl": "Please note that this issue still persists more than two years after it was originally opened. I would consider this a bug rather than a Feature Request - different casing is unexpected behavior and requires additional code as a workaround.. ",
    "NeoyeElf": "the problem still not solved, using version: v1.16.26 \ud83d\ude15. ",
    "itachi3": "@jasdel Thanks for the reply. Please have a look https://github.com/aws/aws-sdk-go/pull/451 and let me know if some changes are required. \nMax objects is hardcoded as \"1000\" for now, i am sure somewhere this constant is mentioned. \n. ",
    "j7b": "Yes, it fails from root of gopath, with GO15VENDOREXPERIMENT being able to go install ./... from root of gopath is path of least resistance for a project with multiple main packages that don't share a common directory immediately under src, it's the default build target in at least goclipse and probably in other tools, and the trend seems to be away from multiple entries in gopath (such as https://github.com/golang/go/issues/13225 change) in general. \n. ",
    "johnzeng": "hi, I can see it's 1.34 now, but , ah, I can't use local dynmaodb to do integration testing now. It throws me a MissingRegion: could not find region configuration when I just try as you commented above.But if I add region into the setting like:\ngo\nconfig := aws.NewConfig().WithRegion(\"us-east-1\").WithEndpoint(\"http://127.0.0.1:8000\")\nIt throws me another error:\nInternalFailure: The request processing has failed because of an unknown error, exception or failure.\n    status code: 500, request id: ab46ed80-2a7b-4845-a67c-880597815e1b\nI didn't find any useful info in Offical documents\nPls let me know you are still supporting local dynamodb, thanks\n. Sorry, ignore my comment on above, \nconfig := aws.NewConfig().WithRegion(\"us-east-1\").WithEndpoint(\"http://127.0.0.1:8000\")\nThis code works, the problem happens on the query parameters. My bad.\n. ",
    "jwhitcraft": "@itachi3 any update on this, It would be really helpful to be in the core lib.. @jasdel if you want to do that, i might have some time to work on implementing it.. ",
    "vancluever": "The Ruby SDK waiters accept blocks and Procs in the before_wait and before_attempt parts of the poll. Not too sure how that translates into go (especially since they also hook waiters into the Resource class), but it might be a good place to set the bar for near-parity.\n. @catsby I actually opened these issues based on the stuff I mentioned in\nhashicorp/terraform#4103 :)\nOn Wed, Dec 2, 2015 at 7:44 AM, Clint notifications@github.com wrote:\n\nI'm glad this is being considered. In Terraform\nhttps://github.com/hashicorp/terraform/blob/master/builtin/providers/aws/resource_aws_instance.go#L386-L393\nwe've added our own retry/waiting behavior, some of which is more complex\nand specific to Terraform, but others are simply \"wait for this condition\".\nI would like to use the native Waiter here, however, for things like RDS\nthis can take considerable time. Terraform emits logs with our current\nwaiting behavior, and it seems like without any hooks here, the CLI will\nappear to simply hang while waiting. If there was a hook to provide a\nfunction which simply emits a log statement (and could possible check the\nresponse and return error, etc), I think we would convert to these Waiters\npretty quickly.\nThanks for all the great work!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-go/issues/454#issuecomment-161338821.\n. Thanks @jasdel - I agree it might be tough to get this in and from what I see the Ruby SDK (the one I'm used to working with) only exposes responses in the callback, from what I've seen.\n. \n",
    "abalakersky": "Thank you for your quick reply.\nSo, is there any other way you could recommend to iterate through the ListObject output and use Prefix: \"Dir1/\", etc. as a param for new ListObjects command?\nThank you\n. @jasdel I am using the code in GO only. I am actually looking to iterate over all objects using prefixes to split concurrency. I have run the same using Ruby to multi-thread the iteration. Was trying to migrate to GO and got stumped on this point.\n. @jasdel Thank you very much. I am just starting with Go so this is great help. One thing through I am still struggling with, and that's where my original question led me to, is how to get Prefixes separated from CommonPrefixes so they can be pushed to the prefixCh? I was trying to Unmarshal results from my original ListObjects but that did not want to work. So that's where I got stuck. With a seeming no way to get prefixes separated sufficiently to be usable for a followup command.\n. Ah, this is fantastic. Thank you for all your help. I definitely got ways to go with go :) but I love it so far.\n. @jasdel Thank you again for all your help. Here is today's final version :smiley: that I came up with. Please see if that would be useful. I also incorporate the option to save a manifest and basic case insensitive search. BTW, if you do know of a better way to do both I would greatly appreciate any advice.\n``` go\npackage main\nimport (\n    \"time\"\n    \"strconv\"\n    \"flag\"\n    \"fmt\"\n    \"github.com/aws/aws-sdk-go/aws\"\n    \"github.com/aws/aws-sdk-go/aws/credentials\"\n    \"github.com/aws/aws-sdk-go/aws/session\"\n    \"github.com/aws/aws-sdk-go/service/s3\"\n    \"sync\"\n    \"os\"\n    \"log\"\n    \"path/filepath\"\n    \"github.com/aws/aws-sdk-go/service/s3/s3iface\"\n    \"bufio\"\n    \"strings\"\n)\nvar (\n    bucket = flag.String(\"bucket\", \"\", \"Bucket Name to list objects from. REQUIRED\")\n    region = flag.String(\"region\", \"us-east-1\", \"Region to connect to.\")\n    creds = flag.String(\"creds\", \"default\", \"Credentials Profile to use\")\n    search = flag.String(\"search\", \"\", \"Search string to find in object paths\")\n    t = time.Now()\n    dir, _ = filepath.Abs(filepath.Dir(os.Args[0]))\n    name = dir + \"/\" + bucket + \"_\" + search + strconv.FormatInt(t.Unix(), 10) + \".log\"\n)\nfunc listObjectsWorker(objCh chan <- s3.Object, prefix string, bucket string, svc s3iface.S3API) {\n    params := &s3.ListObjectsInput{\n        Bucket: bucket,\n        Prefix: &prefix,\n    }\n    err := svc.ListObjectsPages(params,\n        func(page *s3.ListObjectsOutput, last bool) bool {\n            for _, object := range page.Contents {\n                objCh <- object\n            }\n            return true\n        },\n    )\nif err != nil {\n    fmt.Println(\"failed to list objects by prefix\", prefix, err)\n}\n\n}\nfunc CaseInsesitiveContains (s, substr string) bool {\n    s, substr = strings.ToUpper(s), strings.ToUpper(substr)\n    return strings.Contains(s, substr)\n}\nfunc main() {\n    flag.Parse()\n    svc := s3.New(session.New(&aws.Config{\n        Region:      region,\n        Credentials: credentials.NewSharedCredentials(\"\", *creds),\n    }))\nif *bucket == \"\" {\n    fmt.Printf(\"\\n%s\\n\\n\", \"You Need to specify name of the Bucket to scan\")\n    return\n}\n\nf, err := os.Create(name)\nif err != nil {\n    panic(err)\n}\ndefer f.Close()\n\ntopLevel, err := svc.ListObjects(&s3.ListObjectsInput{Bucket: bucket, Delimiter: aws.String(\"/\")})\nif err != nil {\n    log.Println(\"Failed to list Top Level objects\", err)\n    return\n}\nfor _, contentKeys := range topLevel.Contents {\n    fmt.Println(*contentKeys.Key)\n}\n\nobjCh := make(chan *s3.Object, 10)\nvar wg sync.WaitGroup\n\nfor _, commonPrefix := range topLevel.CommonPrefixes {\n\n//      fmt.Println(commonPrefix.Prefix)\n        wg.Add(1)\n        go func() {\n            defer wg.Done()\n            listObjectsWorker(objCh, commonPrefix.Prefix, bucket, svc)\n        }()\n        go func() {\n            wg.Wait()\n            close(objCh)\n        }()\n    }\n//  for obj := range objCh {\n//      fmt.Println(obj.Key)\n//  }\n    w := bufio.NewWriter(f)\n    for obj := range objCh {\n        switch  {\n        case search == \"\" :\n            fmt.Println(obj.Key)\n            w.WriteString(obj.Key + \"\\n\")\n        case search != \"\" :\n            if CaseInsesitiveContains(obj.Key, search) == true {\n                fmt.Println(obj.Key)\n                w.WriteString(obj.Key + \"\\n\")\n            } else {\n                continue\n            }\n        }\n    }\n    w.Flush()\n}\n```\n. @jasdel Thank you again for your help. I've done the changes you mentioned and also run with -race. Found couple of race conditions as well as close on closed channel. Took me a while to figure out the close on closed, but Dave Cheney's article helped http://dave.cheney.net/2014/03/19/channel-axioms\nI think I found everything and testing so far is successful on Linux and Windows.\nI would really appreciate your opinion on the script, if you have few minutes. \nThank you again.\n``` go\npackage main\nimport (\n    \"bufio\"\n    \"flag\"\n    \"fmt\"\n    \"github.com/aws/aws-sdk-go/aws\"\n    \"github.com/aws/aws-sdk-go/aws/credentials\"\n    \"github.com/aws/aws-sdk-go/aws/session\"\n    \"github.com/aws/aws-sdk-go/service/s3\"\n    \"github.com/aws/aws-sdk-go/service/s3/s3iface\"\n    \"log\"\n    \"os\"\n    \"path/filepath\"\n    \"strconv\"\n    \"strings\"\n    \"sync\"\n    \"time\"\n)\nvar (\n    bucket = flag.String(\"bucket\", \"\", \"Bucket Name to list objects from. REQUIRED\")\n    region = flag.String(\"region\", \"us-east-1\", \"Region to connect to.\")\n    creds  = flag.String(\"creds\", \"default\", \"Credentials Profile to use\")\n    search = flag.String(\"search\", \"\", \"Search string to find in object paths\")\n    t      = time.Now()\n    dir, _ = filepath.Abs(filepath.Dir(os.Args[0]))\n)\nfunc caseInsesitiveContains(s, substr string) bool {\n    s, substr = strings.ToUpper(s), strings.ToUpper(substr)\n    return strings.Contains(s, substr)\n}\nfunc main() {\n    flag.Parse()\n    svc := s3.New(session.New(&aws.Config{\n        Region:      region,\n        Credentials: credentials.NewSharedCredentials(\"\", *creds),\n    }))\nif *bucket == \"\" {\n    fmt.Printf(\"\\n%s\\n\\n\", \"You Need to specify name of the Bucket to scan\")\n    return\n}\nvar s string\nif *search != \"\" {\n    s = *search\n}\n\nname := dir + \"/\" + *bucket + \"_\" + s + \"_\" + strconv.FormatInt(t.Unix(), 10) + \".log\"\n\nf, err := os.Create(name)\nif err != nil {\n    panic(err)\n}\ndefer f.Close()\nw := bufio.NewWriter(f)\n\ntopLevel, err := svc.ListObjects(&s3.ListObjectsInput{Bucket: bucket, Delimiter: aws.String(\"/\")})\nif err != nil {\n    log.Println(\"Failed to list Top Level objects\", err)\n    return\n}\nfor _, contentKeys := range topLevel.Contents {\n    switch {\n    case *search == \"\":\n        fmt.Fprintln(w, *contentKeys.Key)\n    case *search != \"\":\n        if caseInsesitiveContains(*contentKeys.Key, *search) == true {\n            fmt.Fprintln(w, *contentKeys.Key)\n        } else {\n            continue\n        }\n    }\n}\n\nvar prefixes []string\nfor _, commonPrefix := range topLevel.CommonPrefixes {\n    prefixes = append(prefixes, *commonPrefix.Prefix)\n}\n\nobjCh := make(chan *s3.Object, 10)\nvar wg sync.WaitGroup\n\nlistObjectsWorker := func(objCh chan<- *s3.Object, prefix string, bucket *string, svc s3iface.S3API) {\n    params := &s3.ListObjectsInput{\n        Bucket: bucket,\n        Prefix: &prefix,\n    }\n    err := svc.ListObjectsPages(params,\n        func(page *s3.ListObjectsOutput, last bool) bool {\n            for _, object := range page.Contents {\n                objCh <- object\n                //              objCh <- fmt.Sprintf(\"%s\", *object.Key)\n            }\n            return true\n        },\n    )\n\n    if err != nil {\n        fmt.Println(\"failed to list objects by prefix\", prefix, err)\n    }\n    wg.Done()\n}\n\nwg.Add(len(prefixes))\n\nfor i := range prefixes {\n    prefix := prefixes[i]\n    go listObjectsWorker(objCh, prefix, bucket, svc)\n}\n\ngo func() {\n    wg.Wait()\n    close(objCh)\n}()\n\nfor obj := range objCh {\n    switch {\n    case *search == \"\":\n        fmt.Fprintln(w, *obj.Key)\n        //              fmt.Println(*obj.Key)\n    case *search != \"\":\n        if caseInsesitiveContains(*obj.Key, *search) == true {\n            fmt.Fprintln(w, *obj.Key)\n            //              fmt.Println(*obj.Key)\n        } else {\n            continue\n        }\n    }\n}\nw.Flush()\n\n}\n```\n. ",
    "george-makerbot": "Hi @jasdel \nHere is the requested debugging information, I took the liberty of running the json output though a prettifier. \nEDIT: Link instead of in lined output to make the comment history easier to read.\nhttps://gist.githubusercontent.com/george-makerbot/404dea1fc90de6f7346f/raw/bcca9f27e37f9cecd23d4f1cd15e056c770ad586/ecs_watch_debug.json\n. I tracked it down to a problem in \"github.com/jmespath/go-jmespath\" but that library is a bit hard for me to traverse.\nIf we don't ignore the error on the Wait call here: https://github.com/aws/aws-sdk-go/blob/master/private/waiter/waiter.go#L64\nWe get\nInvalid type for: <nil>, expected: []jmespath.jpType{\"string\", \"array\", \"object\"\nThe code successfully pulls out and tests the simpler status checks. But fails on services | [@[?length(deployments)!=1], @[?desiredCount!=runningCount]][] | length(@) ==0``\nI am a bit stumped here, using the output we have the the binary version of jmespath (https://github.com/jmespath/go-jmespath/blob/master/cmd/jpgo) I do not hit the error.\n. ",
    "dpetersen": "I've run into this issue as well, so I'll be happy to test any proposed fixes in this library or go-jmespath. Thanks for looking into it!\n. ",
    "xackery": "if conditions; conditions { } are ugly\nsince it's an example, breaking apart each struct (e.g. your svc.CreateTable(&dynamodb.CreateTableInput call) like params : = &dynamodb.CreateTableInput{} and svc.CreateTable(params) helps make it easier to follow along.\nyour struct definition is in the middle of code, that's unusual. (type file struct)\nIntentional to add the two additional map properties (expressionAttributeNames) differently than the converttomap call? also instead of leveraging & a lot, you can use aws.String() etc, not sure which is standard\nXackery 13:29\nlike call on createtableinput: AttributeName: aws.String(\"Filename\"),\nwhile call on m1: m1[\"#a\"] = &aCol //aCol := \"Description\"\ntwo different ways of doing the same thing, a little confusing for an example\n. ",
    "JordonPhillips": "Whoops! Thanks for pointing it out! Everything should be fixed now.\n. What if this never happens? Looks to me like this will never halt, but I'm not sure how that time loop bit works. Even if it does halt after x time, what if the cache still isn't updated? Seems like you could give yourself a better error message in that case.. It's a bit odd to have a \"disable\" boolean, though it looks like you already do that for a few other flags.. ",
    "ananddhandhania": "Hi @jasdel - Reviving this old thread but can you help me understand how I can generate multiple presigned URLs from the GetObject API ? . Thanks @jasdel. Is this something unique to S3 sdk for go ?\nI can't see something similar for java sdk. . Another approach I would like to share is this. As you can see, from a single presigned url, we can provide the bytes range. This means we can do a parallel download with raw curl by passing in the  image bytes range in the http request header. Isn't this better than generating multiple presigned urls? \nf45c89a59d13:~ ananddh$ curl -v -X GET -H \"range: bytes=27-300\" 'https://random-presigned-url-bucket.s3.amazonaws.com/very_large_image.jpg?AWSAccessKeyId=AADDSADASDASDAS&Expires=1493783501&x-amz-security-token=DADAsdasdsaasASDK3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDIRP9SbovN9s5vwG0SLTAaaag1edYamrIqoTZX340jaBixD6x3fyyZVtFKvTD1Gg3U3jornraaFXbyrqBxS0Vu5DeG%2BwWazISVEvyQt7gO6CogaN41KcylAO9y1qAPsGwsrw3P3UZa8u7n3XCmMx%2BOurWnLGeuCgQrdWbVhlOEQwYqBZGW%2BbPx90PR3LBjjuADThD2Qzfno%2B67ciTX3W1YLKCMmuhNxJonORSFE2GrjDlDqSasnD%2FsCS3pt0NTTMW04C8AxSXHP%2FwvQUQIaHk7djrs57ebx2xIUaP3cdN5VL8usoh72jyAU%3D&Signature=9k8a572lTpkipLu%2B8t%2FTLzoUSgE%3D'\n   Trying 52.216.1.64...\n Connected to random-presigned-url-bucket.s3.amazonaws.com (52.216.1.64) port 443 (#0)\n TLS 1.2 connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\n Server certificate: .s3.amazonaws.com\n Server certificate: DigiCert Baltimore CA-2 G2\n* Server certificate: Baltimore CyberTrust Root\n\nGET /very_large_image.jpg?AWSAccessKeyId=ASIAJ7C2CSNM4SJX4PHA&Expires=1493783501&x-amz-security-token=FQoDYXdzEK3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDIRP9SbovN9s5vwG0SLTAaaag1edYamrIqoTZX340jaBixD6x3fyyZVtFKvTD1Gg3U3jornraaFXbyrqBxS0Vu5DeG%2BwWazISVEvyQt7gO6CogaN41KcylAO9y1qAPsGwsrw3P3UZa8u7n3XCmMx%2BOurWnLGeuCgQrdWbVhlOEQwYqBZGW%2BbPx90PR3LBjjuADThD2Qzfno%2B67ciTX3W1YLKCMmuhNxJonORSFE2GrjDlDqSasnD%2FsCS3pt0NTTMW04C8AxSXHP%2FwvQUQIaHk7djrs57ebx2xIUaP3cdN5VL8usoh72jyAU%3D&Signature=9k8a572lTpkipLu%2B8t%2FTLzoUSgE%3D HTTP/1.1\nHost: random-presigned-url-bucket.s3.amazonaws.com\nUser-Agent: curl/7.43.0\nAccept: /\nrange: bytes=27-300\n< HTTP/1.1 206 Partial Content\n< x-amz-id-2: CMU8IDkjFAKF2qR/zqMQ9lP11bHiEMdhChKz2EuxsCcIjo6Nd7khGUrqo0Eq6eyV94+ggY+UH8k=\n< x-amz-request-id: 56F1961D023C59BD\n< Date: Tue, 02 May 2017 19:34:36 GMT\n< Last-Modified: Wed, 09 Nov 2016 01:27:25 GMT\n< ETag: \"7a4c42317eaf4754e5217c1361c70dd5-9\"\n< x-amz-version-id: null\n< Accept-Ranges: bytes\n< Content-Range: bytes 27-300/71590911\n< Content-Type: image/jpeg\n< Content-Length: 274\n< Server: AmazonS3\n< \n* Connection #0 to host random-presigned-url-bucket.s3.amazonaws.com left intact\nOr????(1?2-??'-??'Adobe Photoshop CS5 Macintosh2012:03:28 12:46:48????.?. \n",
    "tjumlani": "Can an object have multiple presigned urls ?. Thanks. I experimented on that a moment ago and has been validated by your answer.. ",
    "ybogdanov": "For now, I made an outer retryer that does its retry logic if it gets non-aws error: https://github.com/grammarly/rocker/blob/master/src/storage/s3/s3.go#L231\nhttps://github.com/grammarly/rocker/blob/master/src/storage/s3/retryer.go#L65\n. ",
    "eldondevcg": "\ud83d\udc4d \nThis is a significant issue/necessity for us.\n. We are going to have to move back to s3gof3r due to these issues. If the number of parts is sufficient, and the traffic on the machine is sufficiently high, it seems that this occurs often enough that no number of retries is sufficient to download the file.\n. Sorry, no longer using this download code, so hard to say. The code in #679  was what was encountering this issue if you want to try to repro. I could speculate about issues within our VPC etc, but since we've changed directions, I can't say for sure. I wanted to try setting KeepAlive here (because I don't believe it was set), but never got around to it.\n. @ljfranklin AWS team doesn't seem to be moving on this ticket much. If you need a lot of multipart upload/download, I highly recomment s3gof3r or similar\n. @xibz this may have to do with contention on the s3 bucket. @mwhooker what kind of load are you pushing here? I believe we have seen this in the past. This link describes some of the workloads that make s3 misbehave and the ways that people address them, but s3manager IMHO should handle these kinds of scenarios gracefully even if the bucket keys are not optimized.\n. My primary purpose at the moment is to copy objects between accounts. I think that it probably shouldn't matter, though (different buckets, different accounts, same bucket different keys), if they are in the same s3 region. The primary purpose would be to have something in s3manager or built into the library that transparently handles the CopyPartRequests for the whole (potentially large) object. Something like what has been implemented here which I found by searching. The aws cli implements this transparently as well.\n. @jasdel As I understand it from this documentation, this option is only available for object sizes of 5GB or less. If that's not the case, my code might be about to get a lot simpler.\n. Yes, that's the strategy the above implementation uses.\n. ",
    "ljfranklin": "\ud83d\udc4d  on fixing this. We're hitting \"error running command: read tcp 10.254.0.206:32961->52.216.65.11:443: read: connection reset by peer\" crazy frequently when downloading from S3 in a flaky network environment.\n. @xibz thanks for taking a look! Debug logs are attached.\nsignature_debug.txt\n. Thanks @xibz, the code in the issue description should reproduce the issue. \n. Our DistributionID is dy5fx5oj7gr1m, feel free to play around with it.\nSetting the endpoint by adding a handler as shown in your example throws this error:\nDEBUG: Send Request s3/PutObject failed, not retrying, error RequestError: send request failed\ncaused by: Put https://cloudfront.net/%7BBucket%7D/%7BKey+%7D: dial tcp: lookup cloudfront.net: no such host\nThe value of r.Operation.HTTPPath is literally /{Bucket}/{Key+}, not the actual bucket name or key. Also CloudFront URLs only support S3 style (e.g. dy5fx5oj7gr1m.cloudfront.net), not path style (cloudfront.net/dy5fx5oj7gr1m).\nHere's the debug output from that run: \nendpoint_handler_debug.txt\n. @xibz afraid we're still hitting the same SignatureDoesNotMatch with your latest code snippet. \nAs far as I can tell, the added request handler has the same effect as setting the Endpoint. If we print URL.Opaque and URL.Host at the end of the new handler, we get:\nOpaque: //dy5fx5oj7gr1m.cloudfront.net/foo\nHost: dy5fx5oj7gr1m.cloudfront.net\nWhich looks correct, but we get the same value if we remove the handler entirely and only set the endpoint:\ngo\nawsConfig.Endpoint = aws.String(\"https://cloudfront.net\")\n...\nreq.Handlers.Send.PushFront(func(r *request.Request) {\n  fmt.Printf(\"Opaque: %s\\n\", req.HTTPRequest.URL.Opaque)\n  fmt.Printf(\"Host: %s\\n\", req.HTTPRequest.URL.Host)\n})\nWe get the same output:\nOpaque: //dy5fx5oj7gr1m.cloudfront.net/foo\nHost: dy5fx5oj7gr1m.cloudfront.net\nIs it possible the CloudFront distribution you tested against has some setting we're missing? We have the following:\n- Allowed HTTP Methods: GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE\n- Forward Headers: Whitelist\n- Whitelist Headers:\nAccess-Control-Request-Headers\nAccess-Control-Request-Method\nOrigin\n- Query String Forwarding and Caching: Forward all, cache based on all\nScreenshot:\n\nAgain, thanks for the help!\n. @xibz thanks for the explanation of the request handler ordering, makes sense.\nThe access keys we're using have \"S3FullAccess\" permission to that bucket. If we remove the CloudFront settings and hit S3 directly, we're able to PUT an object to that bucket. If it helps, our bucket also provides read-only access to anonymous users via this bucket policy:\njson\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AddPerm\",\n            \"Effect\": \"Allow\",\n            \"Principal\": \"*\",\n            \"Action\": [\n                \"s3:GetObjectVersion\",\n                \"s3:ListBucket\",\n                \"s3:ListBucketVersions\",\n                \"s3:GetObject\",\n                \"s3:GetBucketVersioning\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::test-gce-light-stemcells\",\n                \"arn:aws:s3:::test-gce-light-stemcells/*\"\n            ]\n        }\n    ]\n}\n. As a sanity check, here's the exact code we ran:\n``` go\npackage main\nimport (\n    \"fmt\"\n    \"strings\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/credentials\"\n\"github.com/aws/aws-sdk-go/aws/request\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\n)\nfunc main() {\n    awsConfig := (&aws.Config{\n        Region:           aws.String(\"us-east-1\"),\n        Credentials:      credentials.NewEnvCredentials(),\n        S3ForcePathStyle: aws.Bool(false),\n    }).WithLogLevel(aws.LogDebugWithSigning)\nsess := session.New(awsConfig)\nclient := s3.New(sess, awsConfig)\n\nid := \"dy5fx5oj7gr1m\"\nreq, out := client.PutObjectRequest(&s3.PutObjectInput{\n    Bucket: aws.String(id),\n    Key:    aws.String(\"foo\"),\n    Body:   strings.NewReader(\"bar\"),\n})\n\nvar err error\n// Signing has already occurred, now we should change the endpoint.\nreq.Handlers.Send.PushFront(func(r *request.Request) {\n    fmt.Println(req.HTTPRequest.URL.Opaque)\n    fmt.Println(req.HTTPRequest.URL.RawQuery)\n    if req.HTTPRequest.URL.Opaque != \"\" {\n        urlParts := strings.SplitAfter(req.HTTPRequest.URL.Opaque, \"/\")\n        // Pull off query parameters to append to the new URL\n        query := \"\"\n        size := len(urlParts)\n        for i := 3; i < size; i++ {\n            query += urlParts[i]\n        }\n        req.HTTPRequest.URL.Opaque = fmt.Sprintf(\"//%s.cloudfront.net/%s\", id, query)\n        req.HTTPRequest.URL.Host = fmt.Sprintf(\"%s.cloudfront.net\", id)\n    } else {\n        req.HTTPRequest.URL.Host = fmt.Sprintf(\"%s.cloudfront.net\", id)\n    }\n})\n\nerr = req.Send()\nif err != nil {\n    panic(err.Error())\n}\nfmt.Printf(\"%#v\", out)\n\n}\n```\nOutput: \nreq_handler_output.txt\n. @xibz It works if you give anonymous users \"s3:PutObject\" on the bucket and set Credentials: credentials.AnonymousCredentials in the AWS config. Kind of side steps the signing error rather than fixing it though :)\n. @xibz while giving upload permission to anyone with the CloudFront URL might be usable in certain environments, we're looking to solve this in the general case. We'd like users with proper access keys and permissions to be able to upload, everyone can download and list. Is there no way to fix the Signing Error?\n. @xibz While we were hoping to solve this with a single distribution, if that's not possible then no worries. For context, we were adding a CloudFront integration to our CI system: https://github.com/concourse/s3-resource.\nWe'll close this out for now, thanks for the help!\n. @jasdel thanks, I copied my issue into the docs feedback form. Closing this out.. ",
    "ctaymor": "We're seeing this quite frequently as well.\n. ",
    "fermin-silva": "Is it me or the uploader does not have retry logic implemented yet?\nCouldn't see any retry logic in upload.go\nThanks. Thanks @jasdel for the quick reply. I jumped into conclusions too quickly apparently, because we are trying to upload a very large (close to 1TB) file, so you can imagine the size of the log files.\nWill dig deeper into the log files and get back to you.\nI saw retry logic in https://github.com/aws/aws-sdk-go/blob/master/service/s3/s3manager/download.go\nbut couldn't see anything retry-related in https://github.com/aws/aws-sdk-go/blob/master/service/s3/s3manager/upload.go\nThanks again . Thanks again @jasdel. I was creating it with awsConfig.WithMaxRetries(5), but the upload process was failing for another reason.\nI simply saw this issue and saw no retry logic in the uploader and thought it was not implemented, my bad.\nSorry for the noise :$. ",
    "rayrutjes": "@jasdel thank you so much, this is actually just what I needed.\n. @jasdel I have implemented the suggested solution and also the md5 control but I keep getting the following error message back from s3:\nxml\n<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\n<Error>\n    <Code>NotImplemented</Code>\n    <Message>A header you provided implies functionality that is not implemented</Message>\n    <Header>Transfer-Encoding</Header>\n    <RequestId>0A6E12B1B237E050</RequestId>\n    <HostId>gnpz4TUIqaWp67TsNZUpJu80Ll+XK+G55P+/qkLU/kiMH1yK/h2gUbsb9YIAtYqr12a9jSwGE98=</HostId>\n</Error>\nAny clue? \n. @jasdel thank you again for your time, you put me in the right direction. I finally got it working.\nThe Transfer-Encoding was indeed added dynamically by \"net/http/transfer.go\" because I was not using a fixed sized type as body. \nThe solution was simply to manually set the req.ContentLength, and it now works like a charm.\nI am very grateful for your help.\nCheers.\n. @jasdel one last note for anyone trying to figure out the same kind of implementation. If you try to add ACL to the presigned url, you will also have to pass that exact same header to the request processing the actual upload. Is this actually a bug or a feature?\n. @jasdel I have to agree with you, a little helper function like the one available for adding the checksum to a request service/s3/content_md5.go would be nice to have. By the way, the official doc should also mention an example in go: http://docs.aws.amazon.com/AmazonS3/latest/dev/PresignedUrlUploadObject.html\nTell me if I can be of any use on those matters, would love to help you in return!\n. ",
    "brydavis": "Hotdog, @rayrutjes, thanks for the tip on setting req.ContentLength... Had given up on this a few weeks ago and then came across this! Works perfectly \ud83d\udc4d . ",
    "ThomasChambonDev": "Thank you very much for your answer !\nI changed it and it works just fine !\n. ",
    "dalethestirling": ":+1:  I am also a Terraform user and keen to see this functionality get added to the Go SDK so it can flow into Terraform.\n. ",
    "conorgil": "hot diggity, that was quick! Thanks!\n. ",
    "geekifier": "I came across another OSS project, TerraForm, which is also affected by this issue. Unfortunately, this is a bit of a show stopper when it comes to using these useful tools. We do not want to create/store IAM credentials for each client's account, as that is contrary to AWS best practices. Any chance of this request being on a roadmap in the near future?\n. @jasdel Thanks for the implementation. One thing that is worth mentioning, is that the \"opt-in\" feature you have mentioned was implemented, and promptly abandoned by the aws-sdk-ruby team. See https://github.com/aws/aws-sdk-ruby/issues/1257.\nIt might make sense for various SDKs to have feature parity/predictable behavior, but I am not sure about the dynamics of the various teams at AWS, so I am not suggesting either way, just referencing the other discussion :).\n. ",
    "kquinsland": "@xibz If it's any help, I'd like to add a +1 to this because two tools that I use\n- Terraform\n- cli53 \nare both effected by this issue.  Lots of indirect Go SDK users are effected by this :/\nSo does that count as a +3? ;)\n. ",
    "aaroncaito": "Need here as well, hashicorp tools, cli53 and every other aws targeted go project would benefit.\n. ",
    "freefood89": "+1\nThis affects the awslogs driver on https://github.com/docker/docker/issues/23698\nAs for tools like Terraform and Ansible, the current work around I use is to wrap Terraform commands using a python script that uses boto3 to retrieve the AWS secret/key/sessiontoken and then set them as process variables prior to calling the wrapped command: ./wrapper.py terraform plan\nThis may quickly get annoying unless you're running terraform from an automation tool like Jenkins or Drone\n. @bmurphy1976 no guarantees that it's bullet proof \ud83d\ude1b \nSuggestions for improvement are also welcome\nhttps://gist.github.com/freefood89/7446e54dfc486f420a386a895ae1c49f\n. ",
    "bmurphy1976": "@freefood89 would you mind sharing that script?\n. Thanks!  Doesn't have to be bullet proof, I just need a good starting point.  We were already planning to do something similar so if it saves us some effort, great!! \n. ",
    "5330": "Would use this feature daily to support AWS using Terraform (tf) instead of using a wrapper. (wtf)  \n:+1: \n. ",
    "STRogers": "This would also be of use to us. (Using cli53).\n. ",
    "johnrengelman": "My thoughts on this:\n1) if the AWS docs at any point say that source_profile etc is a valid field for the shared credentials file then the SDKs should support it. It's not realistic to expect people to know the difference (i.e I can use these settings when using the aws CLI, but not when using an application that uses the SDK because that causes an error)\n2) This allows for centralizing credentials configuration via profiles and the shared credential file. Tools that use the SDK would no longer need to implement there own configuration/credential chain to handle things the AWS CLI does natively. \n3) The only caveat to this when enabling MFA. I'm not sure the SDK could effectively implement prompting the user for the MFA code b/c it was configured in the profile.\n. ",
    "dprime": "I've recently run into this limitation and not only is the default behaviour confusing, it took me an hour to find this issue and realise what was actually happening. Default enabled gets my vote, for sure.. ",
    "mcfedr": "Found similar problem when trying to use amazon-ecr-credential-helper for docker, and it cannot read my profiles that use source_profile, I have had to manually call assume role, and export the AWS_ACCESS_KEY etc. ",
    "phbcanada": "Well, in my case I'm just ramping up on dynamodb so more api examples there would help me. But I realize that's just a small part of a big api.\n. Sorry, long weekend here in Canada...   Well, the answer gives a work around but from my point of view it's both a flaw and an inconsistency in the interface. The Upload method appropriately returns an UploadOutput object, so it's somewhat confusing to see that Download does not return a similar DownloadOutput object.. So does that mean that the decompression is actually happening on the client side and the transfer itself is compressed? If so then we're fine with that.. Ok. Thanks for the detailed response. Glad to hear that it's possible and even happier to know it's all taken care of. Sorry for the false alarm. . ",
    "toanctruong": "Hi @jasdel thanks for following up. As far as I can tell (I may be wrong), creating a new Aurora DB Cluster utilizing the AWS SDK, the cluster gets created with the multi-az property set to \"No\". My assumption is that since that option is not exposed through \"CreateDBClusterInput\", AWS will default to \"No\" which is a property that cannot be changed after the fact.\n\n. ",
    "stgleb": "Canonical request:\nGET\n/\nhost:s3.eu-central-1.amazonaws.com\nuser-agent:Boto3/1.2.3 Python/2.7.6 Linux/3.16.0-57-generic Botocore/1.3.15\nx-amz-content-sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\nx-amz-date:20160105T131503Z\nhost;user-agent;x-amz-content-sha256;x-amz-date\ne3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n2016-01-05 15:15:03,653 botocore.auth [DEBUG] StringToSign:\nAWS4-HMAC-SHA256\n20160105T131503Z\n20160105/eu-central-1/s3/aws4_request\na716e14d4440d6668ee9e3c9e0e56a80315decee20eff7ab3c67954be197d5c6\n. Thanks\n. Thanks!\n. Sorry it was my fault\n. ",
    "csyangchen": "I got into the same issue. Then I find out s3:// prefix in the bucket name caused the SignatureDoesNotMatch error.\nIt would be better to check Bucket name and return errors like Bucket name must match the regex \"^[a-zA-Z0-9.\\-_]{1,255}$\", as in aws-python-sdk.\n. ",
    "m3alibusra": "Hi @jasdel thanks for the pointer. It fixes my problem.\n. ",
    "dcelasun": "Same here. Full output below.\n```\n$ go get -u github.com/aws/aws-sdk-go/...\ngithub.com/aws/aws-sdk-go/awsmigrate/awsmigrate-renamer/rename\n../../aws/aws-sdk-go/awsmigrate/awsmigrate-renamer/rename/rename.go:92: impossible type switch case: v (type \"go/types\".Object) cannot have dynamic type \"golang.org/x/tools/go/types\".Func (wrong type for Parent method)\n        have Parent() \"golang.org/x/tools/go/types\".Scope\n        want Parent() \"go/types\".Scope\n../../aws/aws-sdk-go/awsmigrate/awsmigrate-renamer/rename/rename.go:98: impossible type switch case: v (type \"go/types\".Object) cannot have dynamic type \"golang.org/x/tools/go/types\".TypeName (wrong type for Parent method)\n        have Parent() \"golang.org/x/tools/go/types\".Scope\n        want Parent() \"go/types\".Scope\n../../aws/aws-sdk-go/awsmigrate/awsmigrate-renamer/rename/rename.go:104: impossible type switch case: v (type \"go/types\".Object) cannot have dynamic type \"golang.org/x/tools/go/types\".Var (wrong type for Parent method)\n        have Parent() \"golang.org/x/tools/go/types\".Scope\n        want Parent() *\"go/types\".Scope\n``\n. I think something similar to what [gomail](https://github.com/go-gomail/gomail) did could work.mime/quotedprintable` became part of the stdlib in 1.5, so they have a Go 1.5 version and an everything else version, both guarded by build tags.\n. ",
    "schoenobates": "I'm getting the same thing today as well:\nEl Capitain: go version go1.5.2 darwin/amd64\n```\ngithub.com/aws/aws-sdk-go/awsmigrate/awsmigrate-renamer/rename\n../../aws/aws-sdk-go/awsmigrate/awsmigrate-renamer/rename/rename.go:92: impossible type switch case: v (type \"go/types\".Object) cannot have dynamic type \"golang.org/x/tools/go/types\".Func (wrong type for Parent method)\n    have Parent() \"golang.org/x/tools/go/types\".Scope\n    want Parent() \"go/types\".Scope\n../../aws/aws-sdk-go/awsmigrate/awsmigrate-renamer/rename/rename.go:98: impossible type switch case: v (type \"go/types\".Object) cannot have dynamic type \"golang.org/x/tools/go/types\".TypeName (wrong type for Parent method)\n    have Parent() \"golang.org/x/tools/go/types\".Scope\n    want Parent() \"go/types\".Scope\n../../aws/aws-sdk-go/awsmigrate/awsmigrate-renamer/rename/rename.go:104: impossible type switch case: v (type \"go/types\".Object) cannot have dynamic type \"golang.org/x/tools/go/types\".Var (wrong type for Parent method)\n    have Parent() \"golang.org/x/tools/go/types\".Scope\n    want Parent() *\"go/types\".Scope\n```\n. Thanks for fixing so quickly Jason \n. ",
    "charles-at-linknext": "Thank you very much for fixing quickly!\n. Hi @xibz and @jasdel,\nThank you very much for quick responses. My Go version was: \"go version go1.3.3 linux/amd64\". After upgrading to \"go version go1.5.2 linux/amd64\", I could get rid of the first error (i.e. imports golang.org/x/tools/go/types: no buildable Go source files in /aGoProject/go/src/golang.org/x/tools/go/types).\nBut I am still having error of finding package \"exact\". Messages are below.\npackage github.com/aws/aws-sdk-go/awsmigrate/awsmigrate-renamer/vendor/golang.org/x/tools/go/types\n    imports golang.org/x/tools/go/exact: cannot find package \"golang.org/x/tools/go/exact\" in any of:\n    /usr/local/go/src/golang.org/x/tools/go/exact (from $GOROOT)\n    /aGoProject/src/golang.org/x/tools/go/exact (from $GOPATH)\nI will try to amend GOPATH or do some research first. I am very sorry to be so careless regarding the Go version impact. Will be more careful next time before dropping an issue. Thank you very much again.\nSincerely yours,\n. I am able to build successfully by upgrading Go to \"go version go1.6 linux/amd64\". Just dropped a message to let you know. Many thanks to @xibz and @jasdel.\n. Hi @xibz - I have no problem of building projects for now. Many thanks again!\n. ",
    "seandevs": "Thank you. Your recommendation worked perfectly.\nOn Thu, Jan 14, 2016 at 6:46 PM, Jason Del Ponte notifications@github.com\nwrote:\n\nHi @seandevs https://github.com/seandevs Thanks again for contacting\nus. I'm going to close this issue for now since it looks like your question\nis answered. Please reopen if we can provide more assistance.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-go/issues/491#issuecomment-171819253.\n. \n",
    "brianfaull": "The iotdataplane.GetThingShadow() seems to be working now, but I only did a quick test. Is this issue closed, or are there remaining functional issues or testing to be completed? It bugs me (just a little bit :) every time I see my MQTT-based workaround scroll by in logs... but I don't want to be too hasty if it's not ready for primetime! Thank you.\n. ",
    "ernesto-jimenez": "@jadel, I updated testify to remove FailNow() from assert.TestingT, so you can remove your workaround. PR: https://github.com/stretchr/testify/pull/264\nSorry for the issue!\n. ",
    "jkakar": "There were actually a few others (in the lib/aws/lambda.ex changes in this pull request):\n/2015-03-31/event-source-mappings\n/2014-11-13/functions/#{URI.encode(function_name)}/invoke-async\n/2015-03-31/functions\nThose all had trailing slashes that were removed.  I modified my code generator to drop trailing slashes and things seem to work now.\n. ",
    "kenju": "@purohit We are facing the same problem. Did you find an answer for this?\nThe forum post you created does not seem to receive any helpful answers yet.. @purohit \n\nwe found some workaround but since it's been over 18 months I can't recall what it is :/\n\nThanks for your reply. I'd be glad if you drop a line once when you recall the workaround :)\nMaybe we shall set the broader regex like this:\njson\n{  \n  \"Statement\":[  \n    {        \"Resource\":\"https://qssnkgp0nnr9vo.cloudfront.net/*\",    },\n  ]\n}\nInstead of declaring multiple resources, which are invalid syntax:\njson\n{  \n  \"Statement\":[  \n    {        \"Resource\":\"https://qssnkgp0nnr9vo.cloudfront.net/foo/*\",    },\n    {        \"Resource\":\"https://qssnkgp0nnr9vo.cloudfront.net/bar/*\",      }\n  ]\n}\nThis is the only thing I have come upon so far.. @purohit Thanks for the detail :). FYI\nI have contacted with the official AWS support team, and got the reply yesterday.\nDetails: https://stackoverflow.com/a/45647858/2775013. ",
    "gadelkareem": "I think it is a misleading error. The error message should show the bucket name because usually that's the problem. \nBucketRegionError: incorrect region, the bucket 'nothing!' is not in 'eu-central-1' region. ",
    "imkira": "Thanks a lot for the prompt support @jasdel \n. ",
    "arvenil": "... and here I'am receiving DATA RACES :/ :D\nAlso it doesn't make sense to me to modify http.DefaultClient.\nA lot of libraries uses http.DefaultClient so a) this introduces data races (as I don't see a way to safely lock http.DefaultClient while modifying it) and b) messes up configuration for other libraries and c) doesn't allow root application to modify defaults.\nRemoving everything and leaving cfg.HTTPClient = http.DefaultClient gets rid of data races.\nWould you accept PR that removes this block of code? (it's worth to try nicely get rid of those data races - if not I will stay with a fork:/)\nHere is a part of example data race I've received.\n```\nWARNING: DATA RACE\nWrite by goroutine 8:\n  sync/atomic.CompareAndSwapInt32()\n      /private/var/folders/bn/49l0ffts381_wqsh02663nr40000gn/T/workdir/go/src/runtime/race_amd64.s:279 +0xb\n  sync.(Mutex).Lock()\n      /private/var/folders/bn/49l0ffts381_wqsh02663nr40000gn/T/workdir/go/src/sync/mutex.go:43 +0x4d\n  net/http.(Transport).getIdleConn()\n      /private/var/folders/bn/49l0ffts381_wqsh02663nr40000gn/T/workdir/go/src/net/http/transport.go:440 +0xc2\n  net/http.(Transport).getConn()\n      /private/var/folders/bn/49l0ffts381_wqsh02663nr40000gn/T/workdir/go/src/net/http/transport.go:512 +0x96\n  net/http.(Transport).RoundTrip()\n      /private/var/folders/bn/49l0ffts381_wqsh02663nr40000gn/T/workdir/go/src/net/http/transport.go:228 +0x62a\n  net/http.send()\n      /private/var/folders/bn/49l0ffts381_wqsh02663nr40000gn/T/workdir/go/src/net/http/client.go:220 +0x73d\n  net/http.(Client).send()\n      /private/var/folders/bn/49l0ffts381_wqsh02663nr40000gn/T/workdir/go/src/net/http/client.go:143 +0x1f7\n  net/http.(Client).doFollowingRedirects()\n      /private/var/folders/bn/49l0ffts381_wqsh02663nr40000gn/T/workdir/go/src/net/http/client.go:380 +0x1059\n  net/http.(*Client).Do()\n      /private/var/folders/bn/49l0ffts381_wqsh02663nr40000gn/T/workdir/go/src/net/http/client.go:178 +0x1e2\n(...)\nPrevious read by goroutine 12:\n  github.com/a/b/vendor/github.com/aws/aws-sdk-go/aws/ec2metadata.NewClient()\n      /Users/kamil/go/src/github.com/a/b/vendor/github.com/aws/aws-sdk-go/aws/ec2metadata/service.go:54 +0xb5f\n  github.com/a/b/vendor/github.com/aws/aws-sdk-go/aws/defaults.CredChain()\n      /Users/kamil/go/src/github.com/a/b/vendor/github.com/aws/aws-sdk-go/aws/defaults/defaults.go:94 +0x245\n  github.com/a/b/vendor/github.com/aws/aws-sdk-go/aws/defaults.Get()\n      /Users/kamil/go/src/github.com/a/b/vendor/github.com/aws/aws-sdk-go/aws/defaults/defaults.go:34 +0xb14\n  github.com/a/b/vendor/github.com/aws/aws-sdk-go/aws/session.New()\n(...)\n```\n@jasdel Thank you for finding this before me!\n. To be honest I really don't get why set this timeout on default client. It's application decision to set it on http.DefaultClient. I also see you can pass your own httpClient through cfg.HTTPClient so I really don't see a reason why simple\n``` go\n    if cfg.HTTPClient == nil {\n        cfg.HTTPClient = http.DefaultClient\n    }\n```\nisn't a better solution\n\n@jasdel, only the application itself (and not libraries) should modify the default timeout\n\nWhy then this library is different?:P\n. Also the problem is not that other libraries writes, but that they might read http.DefaultClient while AWS library is writing (which also is happening for me).\n. Wow, looks like I was referring to older version (still had older one) that was without #511 \nMy sincere apologies. \nSo, now I don't have data races but as the main topic says http.DefaultClient is not used at all.\nMaybe something like this could be a better solution that fits everyone.\ngo\nif httpClientZero(cfg.HTTPClient) {\n    if aws.BoolValue(cfg.EC2MetadataDisableHttpDefaultClientOverride) {\n        cfg.HTTPClient = http.DefaultClient\n    } else {\n        // If the http client is unmodified and this feature is not disabled\n        // set custom timeouts for EC2Metadata requests.\n        cfg.HTTPClient = &http.Client{\n            // use a shorter timeout than default because the metadata\n            // service is local if it is running, and to fail faster\n            // if not running on an ec2 instance.\n            Timeout: 5 * time.Second,\n        }\n    }\n}\n. ... doh, nvm my previous comments, with new version, cfg.EC2MetadataDisableHttpDefaultClientOverride works for me, thanks guys! You are amazing! :D\n. ... or... the question is... can I set credentials with aws.Config{}?\n. Indeed I've missed AWS_REGION variable. Thanks! :)\nAnd from the example I understand that something like AWS_DYNAMODB_ENDPOINT I need to implement by myself?\nAlso https://github.com/aws/aws-sdk-go/wiki/configuring-sdk#hard-coded-credentials-in-an-application-not-recommended solves my problem for providing credentials in tests. Thank you!\n. The only issue I had was that I first didn't had the check if len(resp.Item) == 0 and so my code was panicking when I tried to get data with resp.Item[\"json\"].\nI think this a bit unusual approach, I would rather expect GetItem() returning a package defined error like ErrNotFound (example from sql world https://golang.org/pkg/database/sql/#pkg-variables). Harder to make mistake and end up with panic.\nThanks for answer!\n. ",
    "client9": "ha!  great!   I wrote the tool that goreportcard is using https://github.com/client9/misspell  Ok good to know it and goreportcard and getting noticed and being useful.\nIt's fast and easy to integrate into your build / test / lint process see the readme at client9/misspell .  Feedback most welcome and thank you!\ncc @shawnps \n. ",
    "jabalsad": "Related; https://github.com/aws/aws-sdk-go/issues/726\n. Seems I was unawares of how Lambda gains access to AWS, which is through environment variables (that the nodejs shim didn't export to the child process)\n. ",
    "0xdevalias": "Just ran into this.. strangest behaviour, would not have expected it. At the very least some extra documentation somewhere on the function would be useful, it seems ambiguous at best:\n\nIf there is no matching item, GetItem does not return any data\n// and there will be no Item element in the response.\nErrCodeResourceNotFoundException \"ResourceNotFoundException\"\nThe operation tried to access a nonexistent table or index. The resource\nmight not be specified correctly, or its status might not be ACTIVE.\n\nThanks for this post existing though.. If you're ok with hacks in the interim, this was the simplest way I found to workaround this:\n```golang\nav, err := dynamodbattribute.MarshalMap(user)\nif err != nil {\n  return nil, err\n}\n// Hack to work around https://github.com/aws/aws-sdk-go/issues/682\nemptyMap := make(map[string]*dynamodb.AttributeValue)\nav[\"foo\"] = &dynamodb.AttributeValue{M: emptyMap}\n```. ",
    "tj": "@jasdel is there an api you'd recommend in the SDK for building up the signature? Or is https://godoc.org/github.com/aws/aws-sdk-go/aws/request the way to go? It seems a little low level, I'm looking at the source for other services and it seems like way too much to implement. If that is what's recommended it seems like something that could be vastly improved for the end-user.\n. Great thanks, I'll see if I can get the low-level solution working for now.\n. Sounds good, if I come up with something that is generic enough I'll send it over. Am I missing something, shouldn't everyone using AWS ES need this kind of functionality? Unless you have it wide open or tied to ip(s). I'm surprised there's not more issues open for it.\n. Even the v4 signer seems really coupled to request.Request, and request.Request is coupled to services since most of that is unrelated to just making a regular ES request. Not sure how to approach that, I think I'm just going to find/write a v4 signer. I won't lie, it's things like this that I find very frustrating about AWS, you can't even use their products without wasting a bunch of time.\nIf it can be done with request.Request, an example would be nice, it's pretty confusing as-is.\n. Ah I see, at least v4.Sign() references Request fields, might be usable then, if it wasn't private. I don't have time to work on this unfortunately, I need to unblock myself from Amazon's weird idea of UX and get back to building my products. I'd love to talk to a project manager, I think the SDK is really well built but they should reconsider adding support for things like this, not being able to use a product out of the box is terrible UX, and extremely frustrating when you're paying for a service.\n. Yep, what's there now makes sense for internals but for someone unfamiliar it just creates a huge surface area. I was poking around following the methods/fields around but it's definitely not clear how it should be used for custom requests. Sorry for being the bitchy customer :p\nOn the bright side I just tried the js AWS SDK and it works fine, so I can use that for now.\n. For anyone who hits this as well github.com/smartystreets/go-aws-auth works (so far) and has a pretty clean API. The forks of it are outdated / broken for es though so make sure to use this one. Something similar in the SDK with credentials integration would be great!\n. ah interesting, thanks for the heads up @mwek \n. Is the SDK supposed to detect the parent trace and instrument itself automatically? The X-Ray docs made it sound like that was the case.\ncheers. thanks!. should be trying it soon, I imagine all of the network stuff will fail since there are on shims, but I'll make notes of what I run into!. I ended up abandoning Go WASM for now, feel free to close!. ",
    "Shervanator": "It actually still seems to happen on 727e1f5 :( I might close this issue till we know more. One change we did make since we started getting this issue is lowering the network MTU on the boxes from 9100 to 1500. This was because docker pull's were constantly failing and we found lowering the MTU seemed to fix things up.\n. This was our original ticket just for reference: https://github.com/aws/aws-sdk-go/issues/527\n. ",
    "mengbiping": "Yes, it's due the error in json encoding/decoding a []byte field. I don't think json encoder is proper approach to perform the conversion from typed struct to map[string]interface{}. But I think it's basically another issue that is not blocking my PR. And this PR can work as expected for untyped data. How do you think?\n. Thought the test cases added are the best example that illustrates the crash and fix. \ud83d\ude04 \nAnyway I'm moving the example to the description.\n. @jasdel PIng!\n. @jasdel again, where would you suggest me putting the function to?\n. @jasdel please take a look, thanks!\n. Ack. Will add it soon.\n. @jasdel, I'm thinking of something as follows:\n```\n// It breaks the loop if OnEmbeddedNilStruct returns false\nfunc FieldByIndex(v reflect.Value, index []int, func OnEmbeddedNilStruct(*reflect.Value) bool) reflect.Value {\n    fv := v\n    for i, x := range index{\n        if i > 0 {\n            if fv.Kind() == reflect.Ptr && fv.Type().Elem().Kind() == reflect.Struct {\n                if fv.IsNil() && !OnEmbeddedNilStruct(&fv) {\n                    break\n                }\n                fv = fv.Elem()\n            }\n        }\n        fv = fv.Field(x)\n    }\n    return fv\n}\n// Here in decode.go:\nfv := FieldByIndex(v, func (v reflet.Value) bool {\n    v.Set(reflect.New(f.Type().Elem()))\n    return true // to continue the loop.\n})\n// And in encode.go:\nfv := FieldByIndex(v, func (v reflet.Value) bool {\n    found = false\n    return false // to break the loop.\n})\n```\nNot sure if this is what you want (it sounds a bit obscure to me indeed).\nIf so, where would you suggest me putting the function to.\n. ",
    "fl0cke": "Hey, \nThanks a lot for the quick fix!\nI was under the assumption that s3 automatically computes the md5 hash of the client's PUT request body and compares it to the hash in the presigned url. I had to dig through all the s3 documentaion to find out that you actually have to set the content-md5 header on the PUT request, too.\nFor anyone who might read this issue, looking for a solution, here it is:\n```\nsvc := s3.New(session.New(&aws.Config{Region: aws.String(\"us-west-2\")}))\nreq, _ := svc.PutObjectRequest(&s3.PutObjectInput{\n    Bucket: aws.String(\"myBucket\"),\n    Key:    aws.String(\"myKey\"),\n})\ncontent := strings.NewReader(\"EXPECTED CONTENTS\")\nh := md5.New()\ncontent.WriteTo(h)\nsum := h.Sum(nil)\ncontentMd5 := make([]byte, base64.StdEncoding.EncodedLen(len(sum)))\nbase64.StdEncoding.Encode(contentMd5, sum)\nreq.HTTPRequest.Header.Set(\"Content-Md5\", string(contentMd5))\nstr, err := req.Presign(15 * time.Minute)\n...\nput, _ := http.NewRequest(\"PUT\", str, strings.NewReader(\"EXPECTED CONTENTS\"))\nput.Header.Set(\"Content-Md5\", string(contentMd5)) // IMPORTANT!!\nhttp.DefaultClient.Do(put) // returns 200 OK\n```\nYou should probably update the example on the wiki page, because it is wrong and suggests that you can just PUT without setting the content-md5 header on the request. Also, adding a body field to the PutObjectInput does nothing. I'm not sure if that is a bug or a feature, though.\n. ",
    "wangkuiyi": "I have the same problem with the following code snippet:\nresp, _ := svc.CreateBucket(&s3.CreateBucketInput{\n        Bucket: aws.String(\"images\"), // Required\n    })\nI changed all local variable bucket in updateHostWithBucket defined in github.com/aws/aws-sdk-go/service/s3/ from type *string to string, and it cured the panic.\n```\nfunc updateHostWithBucket(r *request.Request) {\n    b, _ := awsutil.ValuesAtPath(r.Params, \"Bucket\")\n    if len(b) == 0 {\n        return\n    }\nif bucket := b[0].(string); bucket != \"\" && hostStyleBucketName(r, bucket) {\n    r.HTTPRequest.URL.Host = bucket + \".\" + r.HTTPRequest.URL.Host\n    r.HTTPRequest.URL.Path = strings.Replace(r.HTTPRequest.URL.Path, \"/{Bucket}\", \"\", -1)\n    if r.HTTPRequest.URL.Path == \"\" {\n        r.HTTPRequest.URL.Path = \"/\"\n    }\n}\n\n}\n```\nIt seems that s3.CreateBucketInput.Bucket when used as type interface{}, it requires to be assigned a *string, but will be treated by Go as string.\n. ",
    "techjanitor": "Same exact thing\npanic: interface conversion: interface {} is string, not string [recovered]\n    panic: interface conversion: interface {} is string, not string\ngoroutine 24 [running]:\ntesting.tRunner.func1(0xc82007a090)\n    /usr/local/go/src/testing/testing.go:450 +0x171\ngithub.com/aws/aws-sdk-go/service/s3.updateHostWithBucket(0xc82010a000)\n    /data/pram/src/github.com/aws/aws-sdk-go/service/s3/host_style_bucket.go:53 +0x340\ngithub.com/aws/aws-sdk-go/aws/request.(_HandlerList).Run(0xc82010a110, 0xc82010a000)\n    /data/pram/src/github.com/aws/aws-sdk-go/aws/request/handlers.go:115 +0x9f\ngithub.com/aws/aws-sdk-go/aws/request.(_Request).Build(0xc82010a000, 0x0, 0x0)\n    /data/pram/src/github.com/aws/aws-sdk-go/aws/request/request.go:194 +0x106\ngithub.com/aws/aws-sdk-go/aws/request.(_Request).Sign(0xc82010a000, 0x0, 0x0)\n    /data/pram/src/github.com/aws/aws-sdk-go/aws/request/request.go:206 +0x31\ngithub.com/aws/aws-sdk-go/aws/request.(_Request).Send(0xc82010a000, 0x0, 0x0)\n    /data/pram/src/github.com/aws/aws-sdk-go/aws/request/request.go:222 +0x54\ngithub.com/aws/aws-sdk-go/service/s3/s3manager.(_uploader).singlePart(0xc82006e640, 0x7fa5df40f8a8, 0xc820118ed0, 0xc82000c110, 0x0, 0x0)\n    /data/pram/src/github.com/aws/aws-sdk-go/service/s3/s3manager/upload.go:449 +0x1fb\ngithub.com/aws/aws-sdk-go/service/s3/s3manager.(_uploader).upload(0xc82006e640, 0xc82006e640, 0x0, 0x0)\n    /data/pram/src/github.com/aws/aws-sdk-go/service/s3/s3manager/upload.go:355 +0x5d8\ngithub.com/aws/aws-sdk-go/service/s3/s3manager.Uploader.Upload(0x500000, 0x5, 0x0, 0x2710, 0x7fa5df40f498, 0xc8200280e0, 0xc8200d6300, 0x0, 0x0, 0x0, ...)\n    /data/pram/src/github.com/aws/aws-sdk-go/service/s3/s3manager/upload.go:329 +0x159\ngithub.com/eirka/eirka-libs/amazon.(*Amazon).Save(0xc8200280d0, 0xc8200e3de0, 0x20, 0xc820116320, 0x15, 0xa0ac80, 0xa, 0x0, 0x0, 0x0)\n. Ahh @jasdel is correct, setting GO15VENDOREXPERIMENT fixes it on compilation, but any test will be broken since they don't appear to use the vendored version.\n. ",
    "koenbollen": "We are currently sending to > 300k users. But we are in te progress of migrating/warming to SES.\nThe email is highly customized for all users (rendering each mail separately).\nThe function that is taking a while is github.com/aws/aws-sdk-go/aws/corehandlers.(*validator).validateAny (being really recursive)\n. Good thing that this is on the radar. We've switched to manually creating the http request and doing the retry mechanism (we already had that in place).\n. Hey @jasdel. Great news, good thing it's fixed now.\nWe have, however, already switch to manually constructing the HTTP request to SES. Which actually gave us some extra flexibility and performance (we can completely prepare our http.Request objects and retry them without reconstructing).\nThanks!\n. We're happy using our custom bit of code to generate the HTTP request. We fire around 1500 req/s so we need all the flexibility to tweak every bit of the request.\n. ",
    "rhysh": "Yes, that's a lot easier to read. Thanks!\n. \ud83d\udc4d . Part of the contract of the context package is that context.Context values are never nil. From https://golang.org/pkg/context/,\n\nDo not pass a nil Context, even if a function permits it. Pass context.TODO if you are unsure about which Context to use.\n\nNot checking for nil here (and panicking if the caller incorrectly passes nil) is correct; panicking is allowed in the case of programmer error. Callers without a context.Context from their parent can use context.Background() (or the equivalent aws.BackgroundContext()).. This *time.Timer will remain on the runtime's timer heap until it expires, in effect leaking (for some amount of time). Memory used in this way can be significant when timeouts are large or request rates are very high. I recommend using time.NewTimer so the timer can be stopped if the context is canceled.. context.Background() (or context.TODO()) is the context that callers pass when they want the value to be ignored. They never time out, and contain no Values.. Checks like this should not be required. When a context is provided, it should be non-nil (per https://golang.org/pkg/context/).. The method suffix here is incorrect; it should be AddTagsToCertificateWithContext rather than AddTagsToCertificateEx.. Period between \"context\" and \"If\".. Should \"cancelization\" instead be \"cancellation\"?. Probably \"cancellation\", as in request_context.go. net/http.Request.WithContext explicitly checks for nil, panicking if that's what the caller provided. I suggest taking the same approach here, since 1) the context package's docs say to never pass a nil Context, 2) it might provide handy invariants for this package to use, and 3) it will be impossible to make this change later and maintain compatibility.. \"equivalent\". The old package at golang.org/x/net/context passes through to the stdlib context with Go 1.7+ to the greatest extent that it can. In particular, it uses the same value for its Background() and TODO() return values as the stdlib's version\u2014it's not just that they have the same behavior, they are ==.\nChecking that a provided context is == to context.Background() isn't very common, but it can be helpful.\nIs that an option for this package for Go 1.7+? How is this type's String method used?\nhttps://github.com/golang/net/blob/a6577fac2d73be281a500b310739095313165611/context/context.go#L141\nhttps://github.com/golang/net/blob/a6577fac2d73be281a500b310739095313165611/context/go17.go#L16\n. Since there's a chance of a panic (and the caller is forbidden from passing a nil context), it may be better to 1) say \"The Context provided must not be nil\", and 2) for aws.request.Request.SetContext to explicitly check for nil and to panic (as net/http.Request.WithContext does). When the panic is reliable the user will learn how to use the API very quickly, rather than \"getting lucky\" for a while and only encountering the panic once their app is deployed.\n(And same for *PagesWithContext and WaitUntil*WithContext). This time.Timer will leak for a short period when the context is canceled. It could use time.NewTimer as in AfterRetryHandler, but with canceling the timer in the <-ctx.Done() case rather than calling defer since this is in a loop.. ",
    "ReK2Fernandez": "no I dont want to have a parameter this is why I link to my question in stackoverflow all the details are there.. I want a List of ALL my stacks.. so in the documentation says.. do not pass a parameter if you want a list of all of them... this is only to get familiar with the AWS-SDK im used to the ruby one.. and Im trying to find a way to get the stackid from the stackname I do this in ruby a lot but in go I just cant find the way.. so if I get the whole stacklist and use regex eventually to search for name from the results I may be able to get the stackID so later on with this stackID in the program I can do other things.\n```\npackage main\nimport (\n        \"fmt\"\n    \"github.com/aws/aws-sdk-go/aws\"\n    \"github.com/aws/aws-sdk-go/aws/credentials\"\n    \"github.com/aws/aws-sdk-go/aws/session\"\n    \"github.com/aws/aws-sdk-go/service/opsworks\"\n\n)\nfunc main() {\n    svc := opsworks.New(session.New(&aws.Config{\n            Region:      aws.String(\"us-east-1\"),\n            Credentials: credentials.NewSharedCredentials(\"\", \"development\"),\n    }))\n\n    resp, err := svc.DescribeStacks(nil)\n    if err != nil {\n            fmt.Println(err.Error())\n            return\n    }\n    fmt.Println(resp)\n\n}\n```\n. hello, maybe Im reading wrong.. but I do NOT want to pass stackID's I want to FIND out stackID or a list of ALL the stacks... I know already how to pass stackids' but if you don't know the stackID's then what? thats my point.. in ruby I can find out the stackid by stackname... \nexample:\ndef get_stackid()\n  begin\n  puts \"Getting stack id\"\n  stacks_resp = $opsworks.describe_stacks\n  stacks_resp.stacks.each_with_index do |stack, idx|\n    if stack.name.eql? \"#{$owstackname}\"\n      puts \"found #{$owstackname} stackid\"\n      $owstackid = \"#{stack.stack_id}\"\n      puts $owstackid\n      break\n    end\n    end\n  rescue Aws::OpsWorks::Errors::ResourceNotFoundException => error_ida\n    puts \"Resource not Found: #{error_ida}\"\n  rescue Aws::OpsWorks::Errors::ServiceError => error_idb\n    puts \"Service Error: #{error_idb}\"\n  end\nend\n. Thanks I think that will work.. but I think the for loop may be wrong? with your example I get \n\nexpected boolean or range expression, found simple statement (missing parentheses around composite literal?) \n. np, thanks to you for helping this time it worked like a charm!!! I needed this so I can proceed with other tasks, is easy for a user to enter the name of the stack as an argument than to remember the stackid.. \nthanks!\n. \n",
    "lox": "https://github.com/aws/aws-sdk-go/tree/master/service/cloudwatchevents ?\n. ",
    "paultyng": "Ah I guess the service is just not listed in the docs yet.  http://docs.aws.amazon.com/sdk-for-go/api/\n. ",
    "Rauk": "@jasdel \nIs there any other SDK for Go using v2? Sorry for the inactivity though.\n. ",
    "artemnikitin": "Also, in my code I'm waiting for all uploads to be processed as well.\n. Sorry for disturbance. This issue can be closed. Reason: I specified wrong type for CreateUpload step.\n. ",
    "nickw444": "Makes sense! Thanks for the quick response!\n. ",
    "jeanlaurent": "I was mislead by https://github.com/aws/aws-sdk-go/issues/384#issuecomment-148812184. Fair enough, let's close this then.\n. ",
    "mwek": "@tj: FWIW there is a discrepancy between go-aws-auth and AWS ES service about double-encoding special characters when signing the requests. See https://github.com/smartystreets/go-aws-auth/issues/28 for more info.\n. ",
    "nzoschke": "I would like this also. I have a similar want as #522 but for CodeCommit.\nThe CodeCommit HTTP git service uses http basic authentication where the password is based on sig v4. The aws-sdk-go only manages the CodeCommit control plane, and doesn't offer any of the high-level http git credential helpers.\nI ported the credential helper implementation in the Python awscli to Go, and was surprised to learn that the aws-sdk-go couldn't really help with the signing bits.\nI found similar problems with the signing implementations in https://github.com/smartystreets/go-aws-auth and https://github.com/goamz/goamz. \nThe low level signing bits are protected or private and not extensible enough to implement the CodeCommit credential helper. \nEven the go-aws-auth http.Request based implementation makes too many assumptions about signing a request vs generating a signature for the CodeCommit password.\nI ended up having to generate signatures from scratch which was easiest by following the goamz library.\nMaybe the CodeCommit password is a total special case, but it would be nice if the aws-sdk-go exposed more low-level signing utilities. \nIdeally the single SDK can also help broker the high-level access to CodeCommit, ElasticSearch and more.\n. @xibz thanks! I believe errorString is coming from https://github.com/aws/aws-xray-sdk-go.. ",
    "gaffo": "I've got a PR in the works\n. @jasdel LOL SIM :)\n. Okay so you want to refactor out a signer that works on http request with a credential and settings?\n. Sounds good. Working on this.\n. ",
    "sha1sum": "@tj In anticipation of the merging of #698 for this issue, I've created a wrapper for *http.Client that will use the new signing from the PR to ensure that all outgoing requests are signed before sending. Hopefully this will help.\n. This is awesome. I'm literally searching for exactly this just as it seems the PR is being wrapped up. Thank you so much for your work, @nicolai86 and @jasdel -- I was beginning to frustrate myself trying to sign requests for the Elasticsearch data plane!\n. @nicolai86 I'm currently working on a signing client that takes in an *http.Client (or creates one if nil is provided) and wraps its RoundTripper (or http.DefaultTransport) with signing done by your code and returns a new *http.Client for use with Elastic (or anything needing a service signature). I should be done with that today. I'll keep you posted.\n. @nicolai86 The first written version without having been tested has been pushed up to my repository. Usually I would ensure things are working before pushing, but no one is using it yet and I'd like to get your feedback on the code itself if you'd be willing.\nThanks again!\n. I've added tests for the client to ensure that both requests with and without bodies are now being signed before being sent off to the RoundTripper, in case anyone here runs into an issue with needing an *http.Client that automates the signing of requests. I've also changed the code to make use of aws.ReadSeekCloser() for the passing of request bodies, which should help with larger *http.Request bodies.\n. @vgarg in anticipation of the merging of #698 for issue #555 I've created a wrapper for *http.Client that will use the new signing from the PR to ensure that all outgoing requests are signed before sending. Hopefully this will help.\n. ",
    "charlie-ht": "Hi @jasdel, thanks a lot for your suggestions. It looks like this will work for my use-case. I'll have some more experience with the approach after a week or so of use. I'll close the issue for now, because my initial question is answered.\nThanks again for your time, much appreciated.\n. I'm afraid I didn't get to the bottom of it. I reset my Go environment and started from scratch, the worst kind of resolution I know. It's certainly a bit of Godep weirdness. It appeared to have no grabbed all the AWS SDK deps it needed to. Thanks again for the pointers.\n. ",
    "davisford": "Thanks, @jasdel for the detailed response.  That makes sense.\n. ",
    "albrow": "Thanks for the quick reply.\n\nWhat request are you making? Any custom handlers or http clients?\n\n@xibs, we are primarily calling SendMessage and ReceiveMessage. We're sending and receiving approximately 2 million SQS messages per day.\nWe are not using any sort of custom handlers and we're sticking with the default HTTP client.\n. Thank you @xibz. We're trying out the latest version now.\n. @xibz I can confirm we've seen a drastic decrease in the number of file descriptor errors on our server. There have been none in the past 5 days :)\n. ",
    "marcin-zbijowski": "It looks like there was a problem in another place. File writer has had some data buffered and not written to file yet.\n. ",
    "robbiet480": "Hey @xibz I confirmed the URL is correct and when I downloaded the PEM I was able to successfully open it with OSX's Keychain Access. I confirmed that the PEM that Go downloads exactly matches what I download with Chrome. The only difference is absence of a newline character at the very end of the file, but I don't believe that would cause an issue.\n. When looking at the algorithm map in x509, I noticed that SHA1WithRSA maps to SHA1-RSA but in the JavaScript implementation we use RSA-SHA1. I know little about this level of crypto, so maybe it's nothing but I thought i'd point it out.\n. @xibz I did just test it with and without an extra newline but nothing changed :(. I also had Go write out the exact PEM file it's receiving and diff'ed it against the one downloaded via Chrome and found no differences.\nI don't have a handy PHP setup anywhere, so I'll test the byte array suggestion in Javascript and get back to you shortly.\n. @xibz Okay, I tested with both PHP and Javascript. I built some really simple Go tests to confirm everything that's coming in is the same when it comes out (just in case there was something weird happening like when filling the struct). All of the tests are passing for me. I don't see this being an issue in any of the above code, so everything points to something different or wrong with the way Go is doing the certificate check.\nI'm considering ripping out all the Go crypto code and implementing OpenSSL directly using one of the available libraries.\nLet me know if you have any other pointers on how to proceed.\n. @xibz I may have fixed it actually. Testing and retesting right now\n. I think that CheckSignature may have been a red herring. We should have instead been using VerifyPKCS1v15 from the rsa package directly. If we don't care about the output of CheckSignature and just use VerifyPKCS1v15 the hashes match without issue. Be warned though, my head is kind of spinning with all the acronyms and functions going around, so CheckSignature may be an important step, but it seems to me that VerifyPKCS1v15 fulfills all requirements. I realized all of this after finding this Gist.\nAlso note I changed from manually coding in the string generation to a method more like the PHP/JS libraries. \nHere's the code I have, let me know if I got it right:\n``` go\nfunc (sns *SNSData) verifySNS() (bool, error) {\n    var buffer bytes.Buffer\n    signableKeys := []string{\"Message\", \"MessageId\", \"Subject\", \"Timestamp\", \"TopicArn\", \"Type\"}\n    for _, key := range signableKeys {\n        r := reflect.ValueOf(sns)\n        f := reflect.Indirect(r).FieldByName(key)\n        keyString := f.String()\n        if keyString != \"\" {\n            buffer.WriteString(key + \"\\n\")\n            buffer.WriteString(keyString + \"\\n\")\n        }\n    }\nbase64decodedsignature, err := base64.StdEncoding.DecodeString(sns.Signature)\n\nif err != nil {\n    log.Errorln(\"Base64 decoding error!\", err)\n}\n\nresp, err := http.Get(sns.SigningCertURL)\nif err != nil {\n    log.Errorln(\"HTTP GET error!\", err)\n}\ndefer resp.Body.Close()\n// print(\"URL\\n\", sns.SigningCertURL)\nbody, err := ioutil.ReadAll(resp.Body)\nif err != nil {\n    log.Errorln(\"IOUtil error!\", err)\n}\n\np, _ := pem.Decode(body)\ncert, err := x509.ParseCertificate(p.Bytes)\n\nif err != nil {\n    log.Errorln(\"Certificate parsing error!\", err)\n}\n\n// This returns false for some reason\ncheckErr := cert.CheckSignature(x509.SHA1WithRSA, base64decodedsignature, buffer.Bytes())\nif checkErr != nil {\n    log.Println(\"CheckSignature error!\", checkErr)\n}\n\npub := cert.PublicKey.(*rsa.PublicKey)\n\nh := sha1.New()\nh.Write(buffer.Bytes())\ndigest := h.Sum(nil)\n\nfinalVerifyErr := rsa.VerifyPKCS1v15(pub, crypto.SHA1, digest, base64decodedsignature)\nif finalVerifyErr != nil {\n    log.Println(\"verify:\", finalVerifyErr)\n    return false, finalVerifyErr\n} else {\n    return true, nil\n}\n\n}\n```\n. @xibz Works for me! Thanks so much for helping out with them. I would offer to submit cleaned up code to do verification but I'm unsure where this would fit. Otherwise, i'll just build this into an external library which i'll release.\n. @xibz Just published first cut of the library, it's available here.\nThanks again!\n. ",
    "Zariel": "Its not itself recursive but is generating a lot of gourtines, when listing the bucket with objects laid out like /year/month/day/hour/objects.... It appears that the http.Request is not thread safe, according to the documentation that http.Transport is responsible for closing the body of the request, which is what it is doing here. I think that instead of trying to change the body on the request to try a new request needs to be created each time.\n. @xibz thanks!\n. ",
    "tomwans": "Apologies for the radio silence here, been out on vacation for a bit.\nI didn\u2019t include the ability to change the growth rate here because the WriteAtBuffer is essentially a concurrent dynamic array. I  initially picked 2m because I thought it was ~optimal with respect to the number of times we had to copy old elements after a resize. When the buffer is at its maximum size m, m/2 of the elements would have been copied only once, m/4 only twice, and so on. On average, every element would be copied twice. This would mean that the work to manage the internal buffer is about linear (O(2n) = O(n)) to the number of elements in the array, which I figured was pretty good.\nI did some googling to see if there is a good rule of thumb around the growth rate for dynamic arrays, and as you may expect I came across a ton of different answers. I\u2019ve been convinced by https://github.com/facebook/folly/blob/master/folly/docs/FBVector.md that it\u2019s probably best to just go with 1.5m for now.\nWhat do you say to that? If y\u2019all still want the ability to define the growth rate via a passed in func, I\u2019m happy to do that, but just trying to keep things simple here since it seems like 1.5m should be enough for most cases.\n. Hey @xibz I updated this PR with your suggestions. Let me know if you have any other feedback.\n. Sure, will update.\n. ",
    "ekechi": "Awesome. Thanks.\n. ",
    "adamcrosby": "This would be a great feature.  It'd also be nice to expose a more dynamic interface, like the Java SDK: \nhttp://docs.aws.amazon.com/java-sdk/latest/developer-guide/java-dg-region-selection.html\nOn a more meta level, it'd be nice if AWS exposed this information through an API, rather than a web page.  Building tools that have to be recompiled and redistributed to add support for a (effectively identical) new region is a drag, and AWS is adding new regions more rapidly than in the past...\n. I'll do that, thanks.\n. ",
    "yissachar": "Not the resolution that I was expecting but that works for me :+1: \nThanks for the quick response!\n. ",
    "josh-padnick": "Ok, just saw #413, but looks like that's been rolled back in favor of #517.  But when I use:\n```\nsvc := ec2.New(session.New(), aws.NewConfig().WithRegion(region).WithCredentialsChainVerboseErrors(true))\nparams := &ec2.DescribeAvailabilityZonesInput{\n    DryRun: aws.Bool(false),\n}\n```\nI still don't receive any logging output?\n. I was using Credentials.Get() and catching the err.  I actually don't see a Retrieve() method on Credentials?\nHere's my full code:\n```\nsvc := ec2.New(session.New(), aws.NewConfig().WithRegion(region))\n    _, err := svc.Config.Credentials.Get()\n    if err != nil {\n        log.Fatalf(\"Failed to open EC2 session: %s\\n\", err.Error())\n    }\n```\nThis works fine, but it's non-obvious to have to do this test.  It seems like ec2.New should panic or at least return an err if it can't successfully authenticate.\n. Ah, got it. Yeah, docs would be a big help since I kept thinking there was something I was missing by not seeing any errors/logs/panics.  Thanks for the prompt responsiveness!\n. ",
    "pjebs": "Is there some way I can switch the package to use urlfetch which is required in GAE to make http requests.\n. FYI: https://cloud.google.com/appengine/docs/go/urlfetch/\n. Got it to work:\n``` go\nimport (\n    \"google.golang.org/appengine\"\n    \"google.golang.org/appengine/urlfetch\"\n)\nctx := appengine.NewContext(r)\nclient := urlfetch.Client(ctx)\nsess := session.New(&aws.Config{\n    Region:      aws.String(\"us-west-1\"),\n    HTTPClient:  client,\n})\n```\n. I'm pretty sure it's due to this build tag:\n// +build go1.5 in  aws-sdk-go/aws/request/http_request.go\nPlease fix this ASAP for Google App Engine.\n. $ goapp version\ngo version go1.6 (appengine-1.9.35) darwin/amd64\n. I updated to latest version of GAE-GO and it works!\ngoapp version\ngo version go1.6.2 (appengine-1.9.40) darwin/amd64\n. The error said it could not find copyHTTPRequest function in the package github.com/aws/aws-sdk-go/aws/request/request.go:253.\nThere were files with build tag // +build go1.5 and // +build !go1.5. \nBoth files had the copyHTTPRequest function inside and since the tags are mutually exclusive, one of them should have been detected.\nIt was obviously a Google App Engine SDK issue which has now been corrected.\n. I suspect it was related to this: https://code.google.com/p/googleappengine/issues/detail?id=12867\nhttps://groups.google.com/forum/#!msg/google-appengine-go/HZr_cHT-bk0/yPqccgyzBgAJ\n. ",
    "derekwaynecarr": "@xibz thanks for the prompt assistance.\n. ",
    "rodlogic": "Wrong repo :-)\n. ",
    "jasonrichardsmith": "Actually nieksand/gokinesis is the perfect example but it requires the the python KCL helper libs to run, where as python, nodejs, ruby and .net have their own.  It is also not an official aws lib, or even written in a Go fashion, or even have tests.\nAs far as I can tell sendgridslab does not support application level brokering for sharditerators, like the KCL and Kafka consumer groups do.  If I am wrong about this, I am more than willing to use this library.\nAt this point I have crash coursed myself on Maven to build the KCL myself and run it directly.  I may end up packaging that up in a deb so people can quickly get the KCL running without maven, and without the complicated startup scripts.\n@jasdel\n. ",
    "calebamiles": "Thanks for the information @jasdel. Thankfully the waiter was used only in test so we can safely retry and thankfully we're not too prideful to do so.\n. ",
    "weidewang": "@jasdel sorry ,my fault. \nI forget to update the SDK on my some ec2 instance.\nv1.1.15 is good.\n. ",
    "pottava": "Hi @xibz, thanks for your feedbacks! I've changed code and squashed commits.\n. ",
    "JReuling": "@xibz Thank you for the comment. I missed the marshal functions but they work perfectly, thanks.\n. ",
    "threeaccents": "@xibz Thank you!\n. ",
    "qhenkart": "@jasdel Thank you for the review! That was really great. I think I got it now. For the extra options, I opted to just make them part of the CookieSigner struct, I think it makes more sense this way, but I am happy to add the embedded function if you prefer.  \nI also moved the example into a new file like you requested, although I wasn't entirely sure how you wanted me to handle it. I just did a simple test to make sure there are 3 cookies, since the other tests are testing the values. I opted to keep the example in the comments as it slightly represents more of a real world usage (prepending the package name etc), but I'll remove it if you prefer\nAnyway I think I got pretty much everything you asked for, but please don't hesitate to continue to critique or request a refactor. \nThank you for your patience!\n. my bucket policy was not configured properly.. ",
    "mourad": "Looks like the last failure is due to an issue with Travis CI and not with the changes:\npackage golang.org/x/tools/cmd/vet: cannot find package \"golang.org/x/tools/cmd/vet\" in any of:\n    /home/travis/.gimme/versions/go1.6.linux.amd64/src/golang.org/x/tools/cmd/vet (from $GOROOT)\n    /home/travis/gopath/src/golang.org/x/tools/cmd/vet (from $GOPATH)\nmake: *** [get-deps-verify] Error 1\n. @xibz: thanks, I've merged master\n. Could you elaborate? I am sure that I am just simply not understanding it.\nThe problem I hit is that lowercasing the key, then calling http.CanonicalHeaderKey on the result does not necessarily get you back to the original key in order to retrieve the header value.  The value, however, has already been stored using the lowercased key in signedHeaderVals so it seemed that we would be able to simply retrieve it without needing to make the route trip of obtaining the original key.\nI am glad to look at an alternative solution.\n. It seems that the headers that do not satisfy the rules (and therefore, are not passed through) would also not get added headers slice which is used to iterate through on the second loop.\nIt seems that it is guaranteed that every k within the second loop would be in strings.Join(v4.signedHeaderVals[k], \",\").\nNote that in the second loop, we are iterating through headers slice which seems to be the subset of header that we care about.\n. Added this test and it failed as you expected due to generating the wrong signature.\nI've added a commit and tested it again AWS to make sure it did calculate the correct signature.\n. ",
    "4ydx": "Thanks!  That looks to be working.  I think I was originally trying to place multiple attributes into one attribute_not_exist statement.\n. Rather than doing the above, I switched to calling AuthorizeSecurityGroupIngress with\n  p2 := &ec2.AuthorizeSecurityGroupIngressInput{\n    GroupId:    aws.String(*securityGroupId),\n    FromPort:   aws.Int64(25),\n    ToPort:     aws.Int64(25),\n    IpProtocol: aws.String(\"tcp\"),\n    CidrIp:     aws.String(cidr.Value),\n  }\n\nDoing this one at a time works.\n. ",
    "fabiokung": "np, that was a quick turnaround!\n. ",
    "viktorbenei": "We'd like to use it as part of an API, to skip uploading to our server. Basically: the client sends a request for our server requesting a file upload/storage URL. The request includes the file size, which is validated (accepted or rejected). If the request is deemed as valid, the client will get a pre-signed S3 URL to upload the file to.\nWe'd like to enforce the file size for the upload, just like we do with the Ruby SDK, so that the client can't just send us a smaller size initially and then upload a larger file to the provided URL.\n. That would be fantastic!\n\nThough this might still be problematic in your use case, since users could send a Content-Length of 0 and the SDK would skipping signing the Content-Length.\n\nOur API rejects this, the same way if the size would not be specified in the request, but I can see that there's a chance that someone forgets to validate the Content-Length and just passes it to the SDK directly.. Although, I believe, the same thing applies for any other parameter if it comes from user input, it have to be validated before passed to the SDK.\nThanks again for your time!\n. Thanks for the updates & for your time!\n. Fantastic, thank you very much, I'll test it right away! :)\n. @jasdel thanks for the quick reply! I'm a team mate of @slapec93 :)\nPresignRequest is not an option unfortunately as we have to be able to share a single URL, without additional Header requirements. At least that's what we did so far, e.g. with the Ruby SDK (as mentioned by @slapec93 ). Got it, thank you @jasdel ! :). ",
    "dyhuan123": "Hi @jasdel, thanks for your follow up. Can you talk more about the time skew and how time is used to verify the signature? I can see that the request we sent to SQS server contains a timestamp. Does SQS verify signature using that timestamp? Or using the local server timestamp when the request is received? If it just uses the request timestamp to recompute the signature and do the verification, how does time skew matter? \n. @jasdel: In the error message it also shows what the canonical string and string-to-sign should have been. It looks working well for a single goroutine, but maybe I see the error if I run single goroutine for long enough time. Not sure about that. \nDo you know how long is the request valid? Like a few seconds or minutes? And if it is like an expired signature error because of some error, what would you recommend? Shall I just ignore it given that it only fails a small fraction of requests? \nAlso, given the following way to init receiveMessage request, would that be a problem if the goroutine sleeps for say several seconds between first line and second line? Are we expected to use params asap once we create it? \n///////////////\nparams := &sqs.ReceiveMessageInput{}\nresp, err := svc.ReceiveMessage(params)\n////////////////////\n. Hi @jasdel , it looks that my \"SignatureDoesNotMatch\" problem will be fixed if I add \"Authorization\" to the \"ignoredHeaders\" in https://github.com/aws/aws-sdk-go/blob/master/private/signer/v4/v4.go#L29 \nI think what is happening in my case is, \n- We try to send one request, in the first attempt it does not contain authorization header. At the end of first attempt, authorization header is set. (https://github.com/aws/aws-sdk-go/blob/master/private/signer/v4/v4.go#L255)\n- This first attempt fails and aws sdk retries. \n- In this second attempt, aws sdk computes the new signature and this signature contains authorization header (https://github.com/aws/aws-sdk-go/blob/master/private/signer/v4/v4.go#L242) and that causes \"SignatureDoesNotMatch\"\nI noticed that someone removed \"Authorization\" from \"ignoreHeaders\" a few months back (see the commit below). Can I ask what is the reason for that? Is that a bug? \nhttps://github.com/aws/aws-sdk-go/commit/93b30eabd3cfb0ddc1491370cf3f8b64ad92d2a0\n. @jasdel That is exactly our case. For our credential, we use credential provider and currently we always return true for IsExpired() just in case we want to update the AWS key and secret later. Please let me know if this is not the recommended way and we are open for better suggestions. \nThanks for working on the fix and let me know once the fix is out. \n. @jasdel Thanks for your suggestion. We will move the determination of credentials to IsExpired. Basically we will record the old credentials and in IsExpired fetch the new secrets and keys and compare with the old values. At first I thought this is the same as having IsExpired always return true. But given that calling Provider.Retrieve requires an additional mutex.Lock, putting these logic in IsExpired should be better. \n. ",
    "skyleelove": "https://github.com/aws/aws-sdk-go \nthe example of this case\n. My platform is Linux.And how to run the AWS account credentials.Thanks you very much.\n. I can run the example.But I have a problem. I get the object  count of one bucket is not accurate.I think it's the problem of paging.So what can I do to finish this problem. \n. Thank you.But I don't have a code example using s3 that i can paste.Do you have one to use?\n. How to use the Marker to ListObject?\n. Thanks you very much!I finish it.\n. Thanks,\n one more question,such as HeadObjectOutput{}, where to assignin.\n. OK,Thanks a lot!\n. Hi,jasdel\nThanks for your answer.I get so many TIME_WAIT when I put object to S3 using the sdk for go.How can I do for solve the problem\uff1f\n. Thanks a lot .\nThe version of go is go version go1.6 darwin/amd64\nThe version of sdk version v1.4.5\nI use the S3.PutObject() and put 200 object to s3 one second\n. Now I update the sdk to v1.4.8 .It still has so mang TIME_WAIT\n. var s3Client *s3.S3\nI just use one client for all goroutine\n. func init() {\n    config = &aws.Config{\n        Endpoint:    aws.String(AWS_ENDPOINT),\n        Region:      aws.String(AWS_REGION),\n        Credentials: credentials.NewStaticCredentials(AWS_ACCESS_KEY, AWS_SECRET_KEY, \"\"),\n    }\n    s3Client = s3.New(session.New(config))\n}\nHere are init client\n. Thanks a lot .\nI set the /etc/sysctl.conf like this:\nnet.ipv4.tcp_syncookies = 1\nnet.ipv4.tcp_tw_reuse = 1\nnet.ipv4.tcp_tw_recycle = 1\nnet.core.somaxconn = 65535\nnet.ipv4.tcp_fin_timeout = 2\nBut it still has so many TIME_WAIT. Should I change my code to use the sdk to solve the problem with your suggestion.\n. Thanks a lot\nI will try\n. Hi, I try it ,but it does not work.\n. Hi,jasdel\nI find that every connection is disconnected by client,so which function of the sdk to break the connection.I can not find it.\n. Hi,jasdel\n   I got the way to solve this case.I rewrite this function in aws/config.go 246\nthis is old:\ngo\nfunc (c *Config) WithHTTPClient(client *http.Client) *Config {\n    c.HTTPClient = client\n    return c\n}\nthis is new that I change:\ngo\nfunc (c *Config) WithHTTPClient(client *http.Client) *Config {\n    ct := &http.Client{}\n    ts := &http.Transport{\n        Proxy: http.ProxyFromEnvironment,\n        DialContext: (&net.Dialer{\n                Timeout:   30 * time.Second,\n                KeepAlive: 30 * time.Second,\n        }).DialContext,\n        MaxIdleConns:          100,\n        IdleConnTimeout:       90 * time.Second,\n        MaxIdleConnsPerHost:   100,\n        TLSHandshakeTimeout:   3 * time.Second,\n        ExpectContinueTimeout: 1 * time.Second,\n    }\n    ct.Transport = ts\n    c.HTTPClient = ct\n    return c\n}\nI set MaxIdleConnsPerHost to 100 in order to increase the connection number of same host to solve.And I want to kown the suggestion of you about this change.Thanks a lot. \n. Thanks a lot ,I will change the code.\n. ",
    "jurij": "Related Use Case:\njkmb:server juRiii$ godep save ./...\ngodep: Package (github.com/jmespath/go-jmespath) not found\njkmb:server juRiii$ grep -R --include=\"*.go\" \"github.com/jmespath/go-jmespath\" /Users/juRiii/dev/go/src/\n/Users/juRiii/dev/go/src//github.com/aws/aws-sdk-go/aws/awsutil/path_value.go:    \"github.com/jmespath/go-jmespath\"\n/Users/juRiii/dev/go/src//github.com/aws/aws-sdk-go/vendor/github.com/jmespath/go-jmespath/fuzz/jmespath.go:import \"github.com/jmespath/go-jmespath\"\njkmb:Godeps juRiii$ go version\ngo version go1.6 darwin/amd64\n. @dlsniper I have used go get -u github.com/aws/aws-sdk-go because of go1.6. I have not thought about that vendoring feature of any dependency I am using.\n@jasdel Since most users are using this as a Library and not contributing, at least an Information in the README.md would be helpful. \n. ",
    "StabbyCutyou": "Not sure how widely supported it is, but the author of govendor has been maintaining a spec for dependency files in hopes to get it adopted by more of the tools.\nSomething to consider https://github.com/kardianos/vendor-spec\n. ",
    "e-dard": "@jasdel \n\nThe downside of go get -u github.com/aws/aws-sdk-go/... is that users would be downloading several more dependancies than those needed to build applications against the SDK. Specifically, the testing libraries used by the SDK.\n\nThat shouldn't be the case. As far as I'm aware go get has never fetched test dependencies; you have to explicitly provide the -t flag to go get to fetch those.\n. ",
    "roytanmoy": "Is this issue resolved? I am still getting the similar error with Go version 1.11.2.\n../aws/aws-sdk-go/aws/awsutil/path_value.go:9:2: cannot find package \"github.com/jmespath/go-jmespath\"  . ",
    "mweagle": "Looks good - thanks!\n. ",
    "kelcecil": "+1.\n. ",
    "snicko": "+1\n. ",
    "bcarpen6": "+1\n. ",
    "mattnenterprise": "+1\n. ",
    "awestbro": "+1\n. ",
    "masneyb": "+1\n. ",
    "jpfielding": "not the paren we deserve, but the paren we need right now...\n. ",
    "jdavis92": "+1\n. ",
    "saml": "It looks like AmazonS3 indeed does not distinguish / and %2F unlike other http servers like apache httpd.\n```\n\u279c  /tmp curl -sI http://www.allthingsdistributed.com/2011/08/Jekyll-amazon-s3.html          \nHTTP/1.1 200 OK\nx-amz-id-2: cmbJSficK5ohoB8wFy9d4sD3sqQk5ch6YqtV6pQr4mu1y3S0WKTVIr7cAKEekV3W\nx-amz-request-id: 3928102F8A9EA1AA\nDate: Fri, 22 Apr 2016 03:16:12 GMT\nLast-Modified: Tue, 07 Feb 2012 21:37:36 GMT\nETag: \"8de398a992b316becac109dca818398e\"\nContent-Type: text/html\nContent-Length: 16258\nServer: AmazonS3\n\u279c  /tmp curl -sI http://www.allthingsdistributed.com/2011%2F08%2FJekyll-amazon-s3.html          \nHTTP/1.1 200 OK\nx-amz-id-2: mFUHvcJIgIVWcbz3BLnGVWDMoUNBsCjwkNrbfbLGH3Efpri0Z07tdS1iYpPKG7DLvYc4vlN5Z1I=\nx-amz-request-id: 3201E883595B1EB6\nDate: Fri, 22 Apr 2016 03:16:16 GMT\nLast-Modified: Tue, 07 Feb 2012 21:37:36 GMT\nETag: \"8de398a992b316becac109dca818398e\"\nContent-Type: text/html\nContent-Length: 16258\nServer: AmazonS3\n\u279c  /tmp curl -sI https://httpd.apache.org/docs/2.4/  \nHTTP/1.1 200 OK\nDate: Fri, 22 Apr 2016 03:23:08 GMT\nServer: Apache/2.4.7 (Ubuntu)\nContent-Location: index.html.en\nVary: negotiate,accept-language,accept-charset,Accept-Encoding\nTCN: choice\nLast-Modified: Wed, 24 Feb 2016 12:04:31 GMT\nETag: \"234a-52c82de54eede;385-4f1aa8c240810\"\nAccept-Ranges: bytes\nContent-Length: 9034\nContent-Type: text/html\nContent-Language: en\n\u279c  /tmp curl -sI https://httpd.apache.org/docs%2F2.4%2F\nHTTP/1.1 404 Not Found\nDate: Fri, 22 Apr 2016 03:23:00 GMT\nServer: Apache/2.4.7 (Ubuntu)\nContent-Type: text/html; charset=iso-8859-1\n```\nI prefer / to %2F.\nBut, no further work is necessary. I'm force decoding Path part of UploadOutput.Location.\n. ",
    "Jalle19": "Any chance this could be fixed?\n. That's okay, the URL generated seems to work anyway so it's mostly a cosmetic issue.\n. ",
    "agonzalezro": "Since boto2 seems to be using it as well I though that perhaps it was a good idea.\nAnyway I explained the commit the other way around. It seems that I had an older version and after fetch and rebase it was rightly using AWS_SESSION_TOKEN.\nDo you want me to close the issue or do you want to keep the test at least?\n. Also, thanks for getting back to me so quickly @xibz :)\n. @xibz it's what I was expecting. This is why I said that I was not sure that this was the place.\nThanks a lot for your help. Will we keep this open until route53 fixes it?\nPS: just to be clear:\n- Comment should be optional. In case that it shouldn't the documentation here must be updated.\n- PrivateZone should be expected and it's mandatory (correct me here if I am wrong).\nThanks again for the quick response!\n. @xibz if I understood properly, are new versions going to remove HostedZoneConfig from the CreateHostedZoneInput struct?\n. Sorry that I ask too many questions, but without that PrivateZone bool I don't see any other way of creating a private DNS inside my VPC.\n. Sorry again @xibz :)\nI've been checking boto documentation, and it seems that it's supported in the input payload: http://boto3.readthedocs.io/en/latest/reference/services/route53.html#Route53.Client.create_hosted_zone\nresponse = client.create_hosted_zone(\n    Name='string',\n    VPC={\n        'VPCRegion': 'us-east-1'|'us-west-1'|'us-west-2'|'eu-west-1'|'eu-central-1'|'ap-southeast-1'|'ap-southeast-2'|'ap-northeast-1'|'ap-northeast-2'|'sa-east-1'|'cn-north-1',\n        'VPCId': 'string'\n    },\n    CallerReference='string',\n    HostedZoneConfig={\n        'Comment': 'string',\n        'PrivateZone': True|False\n    },\n    DelegationSetId='string'\n)\nIf not, I am probably missing something, but as I said, I don't see any other way.\n. It would be really helpful if you can do it @xibz, but if you prefer I can open a supoort ticket in our Entreprise plan and see if they can manage. Just let me know and I will do whatever you prefer.\n. It works @xibz!\nBut it's actually strange because I was doing this as well:\n_, err = dns.route53.AssociateVPCWithHostedZone(&route53.AssociateVPCWithHostedZoneInput{\n        HostedZoneId: aws.String(id),\n        VPC: &route53.VPC{ // Required\n            VPCId:     aws.String(\"experimentalvpc\"),\n            VPCRegion: aws.String(\"eu-west-1\"),\n        },\n    })\nAfter the Hosted Zone was created and it wasn't setting it to private.\nI suppose that it's because setting it to private can just be done at creation time?\n. I forgot to say: thanks! :)\n. Thanks for the update @xibz!\n. ",
    "harlow": "looks like I was trying to call String() on one map that had nil for the value. \n. Any updates here? i'm seeing the same issue when using dynamostreams and trying to unmarshal JSON:\n{\"error\":\"parsing time \\\"1482529080\\\" as \\\"\\\"2006-01-02T15:04:05Z07:00\\\"\\\": cannot parse \\\"1482529080\\\" as \\\"\\\"\\\"\"}. ",
    "nicolai86": "any update on this front? would be great if the v4 signer were to be exposed\u2026\n. @xibz great, thanks. if he needs support I'd gladly support him to finish this.\n. Thanks! I'll jump in over the next few days!\n. @xibz @jasdel I've pushed a different approach here: https://github.com/nicolai86/aws-sdk-go/tree/feature/public_signer \nI've basically kept the private v4 signing, as a wrapper for the public v4 signing. This way I can keep compatibility, while not being forced to touch every file in the project which uses the internal signer.\nMaybe this is just a lazy approach - but tests are green.\nDo you think this approach is worth pursuing ?\n. @n054 thanks for pointing it out. I moved tests around and made prior accessible methods inaccessible. fixed!\n. I've adjusted the interfaces, but we also need to pass along the signTime for the above interface to work; Right now the Sign and Presign methods just set the attributes on the signer; I'm now starting to refactor the internals of the v4 signer to instead pass all required information along to the methods in question\u2026 will be done on Monday I guess. Would love for @jasdel to take another look by then.\n. @jasdel since v4.Sign requires the request.Body to be used, I've had to introduce a private {pre,}signWithBody method, to allow a different body to be used. I'm assuming there are situations where request.Body != request.HTTPRequest.Body ? Why is the body kept in multiple locations?\n. I think the current version is ready for a another review. \\cc @jasdel \n. To verify the public signer actually works I've build a tiny proxy for the elasticsearch domain service:\n```\npackage main\nimport (\n    \"flag\"\n    \"fmt\"\n    \"log\"\n    \"net/http\"\n    \"net/http/httputil\"\n    \"strings\"\n    \"time\"\n\"github.com/aws/aws-sdk-go/aws/credentials\"\n\"github.com/aws/aws-sdk-go/aws/signer/v4\"\n\"github.com/aws/aws-sdk-go/private/protocol/rest\"\n\n)\nfunc main() {\n    flag.Parse()\n    var esCluster = flag.Args()[0]\nfmt.Printf(`Connected to %s\n\nAWS ES cluster available at http://127.0.0.1:9200\nKibana available at http://127.0.0.1:9200/plugin/kibana/\n`, esCluster)\n    creds := credentials.NewEnvCredentials()\n    if , err := creds.Get(); err != nil {\n        log.Fatalf(\"Failed to load credentials: %v\", err)\n    }\n    signer := v4.NewSigner(creds)\ndirector := func(req *http.Request) {\n    req.URL.Scheme = \"https\"\n    req.Host = esCluster\n    req.URL.Host = esCluster\n    req.Header.Set(\"Connection\", \"close\")\n\n    if strings.Contains(req.URL.RawPath, \"%2C\") {\n        req.URL.RawPath = rest.EscapePath(req.URL.RawPath, false)\n    }\n\n    fmt.Printf(\"%s %s\\n\", req.Method, req.URL.Path)\n    t := time.Now()\n    req.Header.Set(\"Date\", t.Format(time.RFC3339))\n\n    if _, err := signer.Sign(req, \"es\", \"eu-west-1\", t); err != nil {\n        log.Printf(\"failed to sign: %v\", err)\n    }\n}\nproxy := &httputil.ReverseProxy{Director: director}\nhttp.ListenAndServe(\":9200\", proxy)\n\n}\n```\nworks without a problem!\n. @Jasdel I'll adjust the PR today. \n. @jasdel I've reverted the service re-generation. \n. Thanks alot for the feedback, @jasdel . I'll adjust where necessary and ask for more feedback once I'm done.\n. @jasdel I've adjusted all of your points. After taking another look at the signing code I now understand the need for an io.ReadSeeker, and also why we can not just use the http.Request.Body. \nSadly I didn't find a better abstraction than to just pass in an io.ReadSeeker into the v4.Sign and v4.Presign functions. If I come up with a better solution, I'll adjust the PR. Until then this is a pragmatic adjustment, which exposes the problem to the caller instead of hiding it.\nAlso note that I decided to stick to the separate Presign & Sign methods to hide the expireTime based toggle behaviour.\nLooking forward to another feedback from your side.\n. @jasdel please go ahead. Exposing a Sign without body func will force us to make the Sha256 helpers public, which probably will lead to more plumbing. Should I squash the commits first?\n. @jasdel I've squashed all the commits. Thanks for your continued feedback!\n. @sha1sum that's the exact same reason I started working on this. hoping it's not too much work left for @jasdel so we can make use of it  soon\n. I've renamed the property within the v4 signer to DisableHeaderHoisting\n. I've choosen the first approach here, because I think it's a clearer separation for now. Hoping to be able to extract the signingCtx later on.\n. I'm assuming this is purely about the documentation - I've adjusted it.\n. ",
    "insasho": "As your team has probably already done, I've experimented with various approaches to using async channels for paginating results. I'll share some findings here.\nThe Golang blog post on Pipelines and Cancellation proffers the minimum requirements for correct application of channels to the creation of a pipeline. Applying that model to the KMS's ListAliases call results in something like this:\n``` go\ntype AliasListEntryStream chan *kms.AliasListEntry\nfunc (c kms.KMS) ListAliasesStream(listAliasesInput kms.ListAliasesInput, done <-chan struct{}) (AliasListEntryStream, <-chan error) {\n    out := make(chan kms.AliasListEntry)\n    errc := make(chan error, 1)\n    go func() {\n        defer close(out)\n        errc <- c.ListAliasesPages(listAliasesInput, func(p kms.ListAliasesOutput, _ bool) bool {\n            for _, v := range p.Aliases {\n                v := v\n                select {\n                case out <- v:\n                case <-done:\n                    return false\n                }\n            }\n            return true\n        })\n    }()\n    return out, errc\n}\n```\nThis pattern is also compatible with the golang.org/x/net/context pattern as the done acts as a cancellation signal which would allow the generalized timeout behavior.\nHowever, this approach (whether using context or not), requires the user to write a few more lines of code and has higher cognitive burden than the existing pagination method which may not be worth the extra typing except in complex scenarios. Example:\ngo\ndone := make(chan struct{})  // user must create and may close this channel\ndefer close(done)  // Pages method will block until done is closed, which could hold server resources\nout, errc := client.ListAliasesStream(done)  // user must pass done channel to all pipeline methods\nfor entry := range out {\n  if *entry.AliasName == \"aws/aws/acm\" {\n    return true  // deferred method signals generated code to stop iterating\n  }\n}\nif err := <-errc; err != nil {  // complex error handling\n  panic(err)\n}\nreturn false\nThis pattern does allow for some cute uses of channels (such as filters and tees and methods to materialize the collection). However, KMS' ListAliases may be similar to most of the paginated methods in that the size of the result set is small enough where the streaming nature does not yield results commensurate to the maintenance burden of including these methods in the core SDK. cloudwatchlogs.GetLogEvents may be the counter-example due to it possibly returning millions of entries, but is atypical of AWS APIs in my experience.\nI experimented with implementing this pipeline-style method in the SDK code generator, which revealed a few more issues:\n- Some APIs (such as devicefarm and IAM) have multiple result_key fields. I'm unfamiliar with the iteration behavior of these APIs, but this could complicate the generated interface.\n- The result_key field from the paginators-1.json file is not deserialized by paginators.go, perhaps because of the reason above.\n- Dereferencing the type of the entry from the result key list and containing shape appears to require additional wiring through the code generator because the paginators-1.json file refers to the name and not to the shaperef.\n. More generally, it would be useful to have an API for fetching structured data from the Metadata Service (http://169.254.169.254/latest/meta-data).\n. Found it: https://github.com/aws/aws-sdk-go/wiki/sdk-utilities#amazon-ec2-metadata\nClosing ticket!\n. I'm excited to see this work! Can you also link to the crypto implementations in another SDK that you are trying to match? Is it https://github.com/aws/aws-sdk-java/blob/master/aws-java-sdk-s3/src/main/java/com/amazonaws/services/s3/internal/crypto/ContentCryptoScheme.java?\nThanks! \n. Until the customer is allowed to specify the cipher, consider defaulting to AES-GCM rather than AES-CBC. AES-GCM (an AEAD) is generally considered safer.\n. Tthe IV size is dependent on the algorithm being chosen: ECB doesn't use an IV, and the standard AES-GCM IV size is 12 bytes. If S3 or the other AWS SDKs use different conventions, those differences are worth noting in comments because they will come up when under crypto review.\n. Rather than attempting to adapt all the ciphers behind a stream-like API, what do you think about using the core golang crypto cipher.Block and cipher.Stream interfaces and using type assertions instead? As it stands, all of the algorithms currently implemented call ioutil.ReadAll immediately rather than use streaming crypto algorithms. Even the  CTR methods are not using cipher.NewCTR, which could simplify the implementation significantly.\n. ",
    "janeczku": "@jasdel Awesome  \ud83d\ude04 \ud83d\udc4d \n. Thanks for reviewing this so promptly @jasdel !\nThe current fixed value of 30 yields +100% random delay for non-throttling error retries (minTime=30) but just +3% of random delay for throttling error retries (minTime=1000). That's actually the crux of the problem.\nIf the API measures call rate over seconds (e.g. Route 53) then adding just 30, 60, 120, etc. milliseconds of random time to each retry delay won't do much in terms of lowering the global call rate as observed by the API.\n. ",
    "spaceweasel": "No problem @xibz - glad to have helped.\n. Yes I know, but this isn't calling any Json serialization package. The MarshalJSON and UnmarshalJSON methods belong to time. I was going to simply use Parse and Format with RFC3339, but to maintain compatibility with existing implementations (which use a slight variation) it seemed more sensible to make use of these methods. Take a look at the source for time.MarshalJSON; there is no reliance on encoding/json. They are simply methods which format and parse a time string.\n. I'll change these for Parse and Format.\n. ",
    "simonwistow": "@xibz ahah, cheers. That helps\n. @xibz actually that doesn't help in my specific case. I'm trying to use s3manager which doesn't have the Request methods as far I can tell.\n. Any update on this?\n. Any more news?\n. Bump on this?. ",
    "muratsplat": "My Delete method \n``` go\nfunc (s *Sqs) Delete(dels []AwsMessage) error {\nvar deletingMsg []*sqs.DeleteMessageBatchRequestEntry\n\nfor _, v := range dels {\n    // Creating UUID Version 4\n    u1 := uuid.NewV4()\n    ID := u1.String()\n    deletingMsg = append(\n        deletingMsg,\n        &sqs.DeleteMessageBatchRequestEntry{\n            Id:            &ID,\n            ReceiptHandle: &v.ReceiptHandle,\n        })\n}\n\ninput := &sqs.DeleteMessageBatchInput{\n    Entries:  deletingMsg,\n    QueueUrl: s.QueueUrl,\n}\n\nres, err := s.Srv.DeleteMessageBatch(input)\n\nif err != nil {\n    //caused by: Post https://sqs.eu-west-1.amazonaws.com/: dial tcp: lookup sqs.eu-west-1.amazonaws.com on 10.0.2.3:53: too many redirects\n    return err\n}\n\nsuccessCount := len(res.Successful)\nexpextedCount := len(deletingMsg)\nfailedCount := len(res.Failed)\n\nif expextedCount != successCount {\n    return errors.New(\"All messages is not deleted. nummber of message was could be deleted:\" + strconv.Itoa(failedCount))\n}\n\nreturn err // resp.Failed\n\n}\n```\nMy async delete method\n``` go\n// Delete Messages on the repo\nfunc (m *Manager) DeleteMessage(dels []AwsMessage) (result Result) {\ncollection := NewMessageBag(dels)\n\nresult.Failed = 0\nresult.Success = 0\nresult.Errors = nil\nresult.Timeout = false\n\nres := make(chan error)\n\ncountPacket := 0\nfor !collection.IsEmpty() {\n    bag := collection.Pop(DeleteMsgLimitForSqs)\n    countPacket++\n    go func() {\n        res <- m.Repo.Delete(bag) // this method shared on up by me \n    }()\n}\n\ntimeout := time.After(WaitForDeleteRequests)\n\nfor i := 0; i < countPacket; i++ {\n\n    select {\n    case isError := <-res:\n        if isError == nil {\n            result.Success++\n        } else {\n            result.Errors = append(result.Errors, isError)\n            result.Failed++\n        }\n    // Dont't wait slow response !\n    case <-timeout:\n        result.Timeout = true\n        return result\n    }\n}\n\nresult.Timeout = false\nreturn\n\n}\n```\nMy receiveMessage method\n``` go\n// Receive message from Aws Sqs service\nfunc (s Sqs) Receive(maxMessage int64) ([]AwsMessage, error) {\ninput := &sqs.ReceiveMessageInput{\n    QueueUrl:            s.QueueUrl, // my queue\n    WaitTimeSeconds:     s.TimeWait, // 1\n    MaxNumberOfMessages: maxMessage, // 10\n    VisibilityTimeout:   s.VisibilityTimeout,  // mostly 30 second\n}\n\nresp, err := s.Srv.ReceiveMessage(input)\nif err != nil {\n    //caused by: Post https://sqs.eu-west-1.amazonaws.com/: dial tcp: lookup sqs.eu-west-1.amazonaws.com on 10.0.2.3:53: too many redirects\n    return nil, err\n}\n\nbag := []AwsMessage{}\n\nfor _, v := range resp.Messages {\n    bag = append(bag, AwsMessage{\n        Body:          *v.Body,\n        MessageId:     *v.MessageId,\n        MD5OfBody:     *v.MD5OfBody,\n        ReceiptHandle: *v.ReceiptHandle,\n    })\n}\nreturn bag, nil\n\n}\n```\nAnd lastly my async receiveMessage method\n``` go\n// Get Messages from the repo\nfunc (m Manager) ReceiveMsgs() (ReceiveResult, error) {\n    num, err := m.Repo.NumberOfMessage()\n    result := NewReceiveResult()\nif err != nil {\n    return result, err\n}\n\nchanRes := make(chan []AwsMessage)\nchanError := make(chan error)\n\nif 0 == int64(num) {\n    return result, nil\n}\n\nvar abilityCount int64\nif num < m.MaxMessageForGetting {\n    abilityCount = num\n} else {\n    abilityCount = m.MaxMessageForGetting\n}\n\nvar counter int64 = 0\nvar requestCounter int64 = 0\nfor counter < abilityCount {\n    var max int64\n    if num <= ReceiveLimitForSqs {\n        max = num\n    } else {\n        max = ReceiveLimitForSqs\n    }\n\n    if (abilityCount - counter) < ReceiveLimitForSqs {\n        max = (abilityCount - counter)\n    }\n\n    counter += max\n    requestCounter++\n    go func() {\n        msgs, err := m.Repo.Receive(&max) // this method is shared on up!\n        chanRes <- msgs\n        chanError <- err\n    }()\n}\n\ntimeOut := time.After(WaitForReceiveRequests)\n\n// Catch Res\nfor i := 0; i < int(requestCounter); i++ {\n    select {\n    case res := <-chanRes:\n        result.Add(res)\n    case <-timeOut:\n        return result, errors.New(\"Some receive message requests is timeout !\")\n    }\n}\n\n// Catch Error\nfor i := 0; i < int(requestCounter); i++ {\n    select {\n    case errC := <-chanError:\n        result.AddError(errC)\n    case <-timeOut:\n        return result, errors.New(\"Some receive message requests is timeout !\")\n    }\n\n}\n\nreturn result, err\n\n}\n```\nmy go version is 1.5 and go-sdk lastest version\nI'am think  async requests cause this problem on Aws Sqs service\nNow I will be handle same message to avoid duplicate operation by looking MessageID\n. @xibz No Thank for these feedbacks \ud83d\udc4d \n. Hi @xibz \nI will able to test to detect my issue and I will share the code here..\n. Hi all,\nI have written a some test code to detect the issue.  It includes nothing async request to Sqs. When it is runing, all of it is expected. \nBut in my project I need to  send async request to get our accumulated message on Sqs. So It is getting same messages if MaxNumberOfMessages > 1 it cause same messages receiving..\nI think that Aws Sqs service on sending async requests does not work expected.  You should say or notice that point on official api documentation\n. If you talk about VisibilityTimeout, I'm setting 60 second every time..\n. Thank all of you for all feedback..\n. ",
    "GregorioDiStefano": "Thanks for the two comments. \nNo good reason to use a closure I suppose!\nFixing!\n. should I add the isASCII function to sign_url.go? Or do you have a better suggestion?\n. Sounds like a good idea :)\nI will fix this later today\n. I'll need some hints on how to test Cookies, thanks\n. Thanks @jasdel !\n. ",
    "guyc": "\ud83d\udc4d  I'm having the same problem, and would love to see an option to ensure empty maps and arrays are preserved.\n. Thanks @jasdel, we are using a custom marshaller and that's working fine.  One thought on using a struct tag: we are marshalling a complex object passed from a UI library as JSON, and in the API side we expect it to round-trip through dynamodb unaltered.  We don't really want to define structs for all of the deep elements in the structure to be able to tag them.  For this use case a new setting in MarshalOptions that indicates zero-length slices and maps should be preserved would be ideal.\n. ",
    "marcato15": "Checking in on the status to see if this is still just in the feature request but not implemented stage. . I got it working via a custom marshaler but might give a try to add the feature to the sdk. The only thing I'm running into is that I may want an empty list on a top level attribute but not necessarily on children attribute, as having an empty map inside an empty list might still initialize both an empty list and an empty map inside that list. \nMy current implementation is a generic function that works for any struct and you pass it a list of the fields you want to set as empty lists/maps if they are nil. While I don't like that approach I'm struggling to find a binary setting that'd allow the right types of lists/map to be set as empty without setting nested lists/maps. . I really like the idea of adding a struct tag. I may push on that for a bit and see what I can do. I think includeempty may be the best idea, even if it as you mention it does sound weird.. ",
    "austink-carb": "Another oddity is that when I worked around it by checking for if_not_exists in the UpdateExpression, I get my item added and the list gets populated in dynamo, but once I remove that item, it's an empty array! it doesn't become null again!. ",
    "Razz": "Sure. If you build InputLogEvent with the timestamp something like time.Now().Unix(), you get the response: RejectedLogEventsInfo: {\n    TooOldLogEventEndIndex: 1\n  }. If you change the timestamp to time.Now().UnixNano(), you get this: RejectedLogEventsInfo: {\n    TooNewLogEventStartIndex: 0\n  }.\nI found this API Doc that led me to belive that the timestamp might either be not granular enough, or too granular. Counting the places, it was obvious that Cloudwatch needed millisecond timestamps, not nano or second. Using the fix in the PR I was able to get it to work. I can try and make a working example of this in action if I've got time later today.\n. Awesome. I am still learning Go, so my PR was more a guess. Thank you for explaining the thought process. Would you rather me pull the PR or update?\n. Thanks for adding the fix! \n. Awesome\n. ",
    "jlafon": "Hi @xibz , this was built with Go 1.5.3 and SDK 1.1.4 (d17da62685915c25ab5b429b1d1e5b67faa98ac2).\n. Hi @xibz, I am not using any custom handlers. The messages vary in length. Any message that isn't below the 256K limit gets written to S3, with a 'pointer' to it going into SQS. For those messages an attribute with key of 's3' and value of '1' is set, and the body is set to the S3 object path. Messages have a few other attributes ('task_id', 'rank', 'seq') containing string representations of 64 bit numbers. The visibility timeout was not set on the individual messages, but was set on the queue at 200 seconds. The application is a molecular docking application with approximately 5000 SQS clients writing to the queue and 1 client reading results. \n. @xibz I haven't seen this exact error again. I have seen other similar errors and have reported them directly to AWS support. Unfortunately, at the time we weren't logging AWS request IDs.\n. ",
    "sergeyfd": "@xibz Thanks for the quick answer, that's what I thought. Are there any plans to add support for S3 V2?\n. ",
    "tmaiaroto": "Of course, no problem. Thank you.\nIn the interim I think I can do a second query (with DescribeTable) after seeing an error like that to check if the table is empty. I'd rather just use one query, but that should hold me over.\n. Actually, I think this may be my problem. I'm trying a few things here to reproduce but now can't.\n. More than certain it must have been me, sorry.\n. I thought it was from a read but may have been from a write.\nIt may have come from trying to set the secondary key along with the primary. Instead I just removed that and left the secondary key field defined in the normal attributes and it seems ok. Though I swear I wasn't at the point of writing records and that it was on a read...But again I think I just got confused. \n. I'm also getting the same error today. This was, very, recently working for me. Something changed in the last day or so. I just updated to the latest package here, but it didn't seem to fix the issue.. Yes, I am (for reference: https://github.com/tmaiaroto/aegis/blob/master/cmd/up.go#L510). I'm thinking service issue as well given how frequently it was just working. It has been a few hours, but I'll also check again tomorrow.. Also working for me now.. ",
    "Kedarnag13": "Thank you @jasdel.\n. ",
    "twoism": "I ran into this same issue. It seems strange to not have a native way to unmarshal the return types of both QueryOutput.Items and ScanOutput.Items. This was my solution for now but it would be nice to not have to iterate over the slice in both this code and UnmarshalList.\n``` Go\nfunc UnmarshalListOfMaps(l []map[string]dynamodb.AttributeValue, out interface{}) error {\n    attrs := make([]dynamodb.AttributeValue, len(l))\nfor i, m := range l {\n    attrs[i] = &dynamodb.AttributeValue{M: m}\n}\n\nreturn dynamodbattribute.UnmarshalList(attrs, out)\n\n}\n``\n. Not to harp on this but asking end users to incur N*2 marshaling time for a common return type doesn't seem like the answer. I did some digging and given that the above code is essentially just upcasting each item in[]map[string]dynamodb.AttributeValueinto anAttributeValue, couldn'tQueryOutput.Itemsjust return a[]dynamodb.AttributeValuewhere each item has the value type ofM? Unless I am wrong, these are essentially the same thing. If so, the marshaler would not need to be updated. It would break the API but I think using AWS types would make things easier. Just a thought and I could be missing something about why themap` is necessary, happy to help work on this if needed :)\n. This is awesome, thanks a ton!\n. ",
    "timogoosen": "I know this is an old issue but could someone explain the usage ? . ",
    "n054": "Just ran through Travis and this was result:\nThe command \"make unit-with-race-cover\" exited with 2.\n. @nicolai86 thank you. Will try to run continuous-integration.\n. Continuos-integtation has passed. Glad it's working.\n. ",
    "ci-iotsyst": "This is 100% supposed to be a part of the AWS IoT SDK.  You create keys as part of the tutorial with the intent you use these to communicate.  I'm ok with the fact this has not been coded yet, but this really shouldn't be closed.  The AWS SDK has some IoT capability but it's missing this part which is rather critical for AWS IoT development.\n. re: Go for IoT\nhttps://gobot.io/ \nIf you submit a feature request to remove all IoT stuff from SDK, then closing this issue would make sense otherwise I think you'd agree it's misleading of have IoT support that you aren't supposed to use\nhttp://stackoverflow.com/questions/34409792/http-post-to-aws-iot-using-golang-and-aws-sdk-iotdataplane.  \nIf code is generated from models, maybe just provide a hook in the generated code for people to implement their own auth.  Go's PKI infrastructure is complete  so I'm sure it's not a lot of work once we have a place to put it.  I can take a crack at it and we can work backwards at how to work that into the SDK.\n. BTW: You've mentioned IOT as if it's a group or project.  Is that a group internal to Amazon?\n. ",
    "bfosberry": "We do not have a request id, however the region is us-east-1.\nWhile a nil message id does seem like an sqs error, its very bad practice to allow for panics to occur since that affects the entire stack, not just your library. I would be much happier if there was some form of validation in place:\nfunc validateMsg(msg blah) error {\n  if msg.MessageId == nil {\n    return ErrNoMessageID\n  }\n}\nIf you can return an error rather than panic in this case it is something we can handle and then retry as a matter of course.\n. thanks!\nWe have only seen one instance of this btw\n. thank you!\n. Previous versions do not work either. I'll test that code and see what happens, we have been wondering if its a restriction on our bucket or account.. I can confirm this version works for me, as does a version passing an os.File reader directly in. Functionally there is no difference between this code and our production code which is storing as Standard, so Im at a loss as to why our objects are not getting stored correctly.\nAre there any client settings that might affect this, or other fields that might alter behaviour?. Thanks for the help with this, I was able to determine that the correct headers were being sent and that the issue was that I was checking the wrong bucket. We replicate to another non reduced-redundancy bucket. In the correct bucket files are being stored correctly.\nThis did uncover another issue in that the replication, which is just writing data to a signed upload url, also changed an object to a different storage class, but I suspect that is an issue on our end.. thanks!\n. ",
    "scohen28": "I replicated your steps above and it runs fine (1.7beta1)\n1) Then I removed go from my system (/usr/local/go and /etc/paths.d), remove src/github.com/aws - same issue.\n2) Did (1) above and then also removed the pkg from pkg/darwin_amd64/aws -- this time go get succeeded - so not sure why, but I'm guessing something in the pkg folder was the culprit.\n. ",
    "vgarg": "@jasdel thanks. Availability of #555 should definitely help.\n. ",
    "cvanderschuere": "@jasdel Thanks for the quick reply!\nUsing the newest go version and aws-sdk version (1.6.2 and 1.1.31). \nAs for headers, nothing was included that is not there by default. Also, by query I actually meant GetItemand PutItem. This particular profile did not have any Query usage but I would assume it to be the same issue. \nOur usage is essentially:\n``` go\n// We disable SSL and use a HTTP client with a large connection pool\nsvc := dynamodb.New(sess)\nfunc getHandler(w http.ResponseWriter, r *http.Request){\n...\n   output, err := svc.GetItem(input)\n...\n}\nfunc putHandler(w http.ResponseWriter, r *http.Request){\n...\n   output, err := svc.PutItem(input)\n...\n}\n```\nThe profile was run while subjecting these endpoints to high traffic (1k's/second).\nHere is the log output when I hit the local version of DynamoDB (profiles were actually hitting dynamo):\n2016/06/06 15:18:48 DEBUG: Request dynamodb/GetItem Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nPOST / HTTP/1.1\nHost: localhost:8000\nUser-Agent: aws-sdk-go/1.1.31 (go1.6.2; darwin; amd64)\nContent-Length: 87\nAccept-Encoding: identity\nAuthorization: AWS4-HMAC-SHA256 Credential=test/20160606/us-west-2/dynamodb/aws4_request, SignedHeaders=accept-encoding;content-length;content-type;host;x-amz-date;x-amz-security-token;x-amz-target, Signature=bcfcb3d790eff5e2e08b7d71d8d0ec5092c934b36e6d44483c29538e244e569e\nContent-Type: application/x-amz-json-1.0\nX-Amz-Content-Sha256: 5694cb0f71c563c0899410aada7e3d0884d7005ac22e3d98028636efa0a7e096\nX-Amz-Date: 20160606T221848Z\nX-Amz-Security-Token: test\nX-Amz-Target: DynamoDB_20120810.GetItem\n. ",
    "raphtheb": "So i've contacted our account rep internally to see if there was a way to indeed perform the validation. As it turns out, there is a way. Using https://docs.aws.amazon.com/cli/latest/reference/support/describe-trusted-advisor-check-result.html\nand the check-id \"eW7HH0l7J9\", plus a good deal of parsing, one can get the current limit. \nSo it is possible to do, i'll see if i have the need to, and time to, work on a PR that would add that functionality. \nThanks for answering, @xibz . We do have a need for more than 20 ELBs within one account here, and keep getting some very strange errors from many places once we break 20.\n. Oh we already had that limit increased to 50 a week ago. That part was the easiest!\n. Region is us-west-2 for everything. The requestId i don't have handy, but will provide as soon as i can.\n. Hey @xibz ! We actually found what was the issue internally. Pasting what a coworker sent me with regards to this issue:\nThe problem actually laid in the 'DescribeTags' call instead of the 'DescribeLoadBalancers' call, from first glance.\nTurns out that the API call: elb.DescribeTags(...) only allows for 20 loadbalancers worth of tags to be queried at once. This limitation is not described anywhere in the AWS SDK documentation, nor the source code. \nhttp://docs.aws.amazon.com/sdk-for-go/api/service/elb/#example_ELB_DescribeTags\nAfter being puzzled for the longest time,  found myself on the API doc itself for the ELB service and it was actually documented there instead: http://docs.aws.amazon.com/ElasticLoadBalancing/latest/APIReference/API_DescribeTags.html \nSo we've resolved the issue, it doesn't appear to be in any way, shape or form related to this very project! Thanks a lot for assisting though, it is greatly appreciated.\n. ",
    "rasky": "@xibz thanks. Is there a list of valid regions (per service) that I can use? Should I keep it in my application hardcoded and that's it? Shouldn't the SDK provide it?\n. Thanks, I think #108 sounds a good idea in fact. \n. ",
    "matthew-andrews": "Hi, I've just tried again and can definitely reproduce the issue.\nI've created a reduced test case here:-\nhttps://github.com/matthew-andrews/s3-reduced\nWhich I have also reproduced on CircleCI (click make test to see the output):-\n```\nWITH EU REGION\nexport AWS_REGION=eu-west-1; echo $AWS_REGION && go run main.go;\neu-west-1\nRESP:\n{\n  LocationConstraint: \"eu-west-1\"\n}\nERR:\n\n\nWITH NO REGION\nunset AWS_REGION; echo $AWS_REGION && go run main.go;\nRESP:\n{\n}\nERR:\nMissingRegion: could not find region configuration\n```\nThe user is a plain empty AWS IAM user with no groups, no policies, etc.  It is granted access to a bucket by a bucket policy attached to the bucket itself and that policy is this:-\n{\n    \"Version\": \"2008-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::862433486593:user/DeployUserFor_s3up\"\n            },\n            \"Action\": \"s3:*\",\n            \"Resource\": [\n                \"arn:aws:s3:::s3up-test/*\",\n                \"arn:aws:s3:::s3up-test\"\n            ]\n        }\n    ]\n}\nI believe the go version on both my local system and in CircleCI is 1.6 and I think Circle probably pulls in this sdk from master.  I just tested my local machine on v1.1.34.\nHope this helps!\n. Hm, actually that's a different error isn't it\u2026\n. OK, try here \u2014 https://circleci.com/gh/matthew-andrews/s3-reduced/4\nClick make test\n```\nEU REGION\nexport AWS_REGION=eu-west-1; echo $AWS_REGION && go run main.go;\neu-west-1\nRESP:\n{\n  LocationConstraint: \"eu-west-1\"\n}\nERR:\n\n\nWITH NO REGION\nexport AWS_REGION=us-east-1; echo $AWS_REGION && go run main.go;\nus-east-1\nRESP:\n{\n}\nERR:\nAccessDenied: Access Denied\n    status code: 403, request id: 740F596935D64CDE\n```\n. I think it's a bug\u2026?\n. Ah you think the IAM permissions are wrong?  I'll check that and get back to you. Apologies in advance if that's the case.\n. ",
    "tmichel": "@xibz I submitted a PR with the fix: #723. \n. ",
    "ChristianLohmann": "That's fine for me, thx!\n. ",
    "maxekman": "Thanks for your quick reply!\nI understand the implications of any changes here, and it was good hearing it from you. Your suggestion with a wrapper could work, but it adds a bit of complexity.\nThe custom type in this case is a UUID type, which is just a type alias for a string but with methods for parsing etc. I'm evaluating to skip the custom type, but then I'll miss out on the added type safety. The project where I'll use this is my CQRS/ES toolkit, https://github.com/looplab/eventhorizon.\nAnother way of solving this particular case would be to add support for marshaling alias types, at least with native types (in this case it would be a simple string). It does not seem to work at the moment, but I have not done any real testing of it. Any thoughts on this approach?\n. Thanks for marking it as a bug. Adding support for aliased types would be by far the best solution in my case. If you need any more info from me, please tell.\n. Thanks for that! Seems to work as advertised.\n. ",
    "vburenin": "This is definitely something that needs to be fixed.\n. How frequently this Copy is being called? Once per request?\nEdit: Yes, once per request. It takes around 3000ns on my laptop to make a copy.\n. I think it really should be implemented somehow differently. Let say if I initialize a client for dynamo db, everything should be initialized only and only once. I can't see there is anything needs to be copied in that already prepared context.\n. ",
    "AnvilStriker": "Great, thank you for investigating, and for working to get the additional text & examples back in.\nWill file additional issues as necessary.\n. ",
    "jafalas": "Thanks for the information and the quick replies! How much time/effort would it take to modify the SDK so it signs the header so it matches the S3 spec as described in your last comment? And is that something you are willing to do?\n. Hey @jasdel I see you tagged this issue as a feature request rather than a bug. Shouldn't this issue cause S3 to reject all v4 signed request to it, from aws-sdk-go? (since it's missing the signed header and violates the security described in the spec)\nIn practice, does S3 actually care that this particular header is not part of the signed headers? I'm trying to figure out if the spec is more strict than the actual S3 code. Thanks.\n. ",
    "andrewarrow": "here's the code I'm using for rename.  Maybe this can get put into s3manager some day:\n```\nimport \"github.com/aws/aws-sdk-go/aws\"\nimport \"github.com/aws/aws-sdk-go/service/s3\"\nimport \"fmt\"\nconst RenamePartSize int64 = 1024 * 1024 * 1024 * 1\ntype RenamerInput struct {\n    Size         int64\n    SourceBucket string\n    SourceKey    string\n    DestBucket   string\n    DestKey      string\n}\nfunc (a API) deleteAfterCopy(input RenamerInput) {\n    svc := a.getS3Session()\n    dparams := &s3.DeleteObjectInput{\n        Bucket: aws.String(input.SourceBucket),\n        Key:    aws.String(input.SourceKey),\n    }\n    _, err := svc.DeleteObject(dparams)\n    if err != nil {\n        msg := \"* S3 DID NOT DEL A FILE!\"\n        fmt.Println(err, msg, input.SourceBucket, input.SourceKey)\n    }\n}\nfunc (a API) rename(input RenamerInput) error {\n    svc := a.getS3Session()\nif input.Size < RenamePartSize {\n    copyInput := &s3.CopyObjectInput{\n        Bucket:     aws.String(input.DestBucket),\n        CopySource: aws.String(fmt.Sprintf(\"/%s/%s\", input.SourceBucket, input.SourceKey)),\n        Key:        aws.String(input.DestKey),\n    }\n\n    _, err := svc.CopyObject(copyInput)\n    if err != nil {\n        return err\n    }\n    a.deleteAfterCopy(input)\n    return nil\n}\n\nparams := &s3.CreateMultipartUploadInput{\n  Bucket: aws.String(input.DestBucket),\n  Key:    aws.String(input.DestKey),\n}\nresp, err := svc.CreateMultipartUpload(params)\nif err != nil {\n    fmt.Println(err)\n    return err\n}\nuid := string(*resp.UploadId)\ns := input.Size\npart := int64(1)\nparts := int(math.Ceil(float64(input.Size) / float64(RenamePartSize)))\nvar theParts []*s3.CompletedPart = make([]*s3.CompletedPart, parts)\nfor {\n    offset := RenamePartSize * (part - 1)\n    endbyte := offset + RenamePartSize - 1\n    if endbyte >= input.Size {\n        endbyte = offset + s - 1\n    }\n    source, err := svc.UploadPartCopy(&s3.UploadPartCopyInput{\n        Bucket:          aws.String(input.DestBucket),\n        Key:             aws.String(input.DestKey),\n        CopySource:      aws.String(input.SourceBucket + \"/\" + input.SourceKey),\n        CopySourceRange: aws.String(fmt.Sprintf(\"bytes=%d-%d\", offset, endbyte)),\n        PartNumber:      aws.Int64(part),\n        UploadId:        &uid,\n    })\n\n    if err != nil {\n        fmt.Println(err)\n        return err\n    }\n\n    etag := string(*source.CopyPartResult.ETag)\n    etagl := len(etag)\n    etag = etag[1 : etagl-1]\n\n    fooi := int64(part)\n    partn := &fooi\n    theParts[part-1] = &s3.CompletedPart{ETag: &etag, PartNumber: partn}\n\n    part++\n    s -= RenamePartSize\n    if s <= 0 {\n        break\n    }\n}\n\ncparams := &s3.CompleteMultipartUploadInput{\n    Bucket:   aws.String(input.DestBucket),\n    Key:      aws.String(input.DestKey),\n    UploadId: &uid,\n    MultipartUpload: &s3.CompletedMultipartUpload{\n        Parts: theParts,\n    },\n}\n_, err = svc.CompleteMultipartUpload(cparams)\nif err != nil {\n    fmt.Println(err)\n    return err\n}\na.deleteAfterCopy(input)\nreturn nil\n\n}\n```. thanks! posted https://forums.aws.amazon.com/thread.jspa?threadID=239573 you can close this issue or leave open.\n. sorry, I know tests are failing. Could use some help on how to make them respect the 10 tries.. thanks @jasdel ! Makes sense. Closing this PR.. hey @jasdel I think you'll have the answer for this one too right? :) thanks!. awesome, thanks!. ",
    "chris-redekop": "\ud83d\udc4d . ",
    "sveniu": "Being able to mix copied and new parts in a single upload would facilitate an append-like behaviour, so I'd suggest that an implementation would support it.\nUse cases: 1) Concatenating an existing object and new data to form a new object. 2) Concatenating an existing object and new data to overwrite the existing object. 3) Concatenating an existing smaller-than-5MB-object and new data, by doing GetObject on the existing object, and using its GetObjectOutput.Body together with the new data's Reader (via a MultiReader) as input to UploadInput.Body.. ",
    "tolidano": "Pinging on this - it's been over a year. I'd at least expect the blob issue complete and then the x-amz-server-side-encryption-context can be added by PR, I guess.. Follow-up note - you can work around this:\nhttps://docs.aws.amazon.com/sdk-for-go/api/service/kms/#KMS.GenerateDataKeyWithContext\nhttp://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html\n. ",
    "AHaymond": "432000\nWhich  leads me to believe that if my assumption that that value is the number of seconds til expiration of the token then 43200/60/60/24 = 5 days (or 120 hours) to be correct. Yet, it is not playing nicely and abiding by that for some reason. Thanks for looking into this.\n. ",
    "waeltken": "Hey @AHaymond, were you able to solve this in the end? I'm experiencing the same issue.\nI'm trying to create a pre-signed request to live for 7 days. The created request URL contains Expires=604800 as expected but the URL still returns ExpiredToken after just one day...\nurl, err := req.Presign(7 * 24 * time.Hour)\nUsing the aws-sdk-go 1.13.40...\nMaybe it's related to the role that is used to create the pre-signed request, but I can't seem to find anything that might explain a time of 24 hours for the token to become invalid.. ",
    "JeffersonSchuler": "I am seeing the same behavior.  The token is expiring too soon.\nX-Amz-Expires=604800 is part of the URL.\nI don't know exactly when the token expires, but is less than 24 hours.\nIs there anything else that could be pre-expiring the token?\n. > Hi @JeffersonSchuler , I happened to have the same problem few months ago and my case was because of the IAM role, which expires every 36 hours or so.\n\nhttps://stackoverflow.com/questions/42951040/s3-expiredtoken-error-for-s3-pre-signed-url-within-expiry-period/50398133\n\nThanks @t-oki for the link.  This is probably my issue.  I am making the generate_presigned_url call from a Lambda which uses an IAM role over an IAM user.  . ",
    "t-oki": "Hi @JeffersonSchuler ,  I happened to have the same problem few months ago and my case was because of the IAM role, which expires every 36 hours or so.\nhttps://stackoverflow.com/questions/42951040/s3-expiredtoken-error-for-s3-pre-signed-url-within-expiry-period/50398133. Yeah, but I didn't want to use IAM user instead of role so I'm creating a new URL every 30 minutes now. Hope it helps!. ",
    "mattpollard": "Thanks for the quick response, Jason!\n\nOlder ( < 1.5 ) versions of Go have difficulties sometimes with the pkg cache if packages are moved, or renamed. In this case I suggest clearing out your $GOPATH/pkg/.\n\nInteresting \u2013 that's something I could automate via Chef. Perhaps it's worth considering. I think I have some more fundamental misunderstandings about how all of this works, though, so bear with me...\n\nIn the OpsWorks environment does the GOPATH get reused between runs or is it supposed to be a fresh environment each time Chef is run?\n\nI'm not totally sure that I understand the question, sorry. These server instances are effectively your run-of-the-mill EC2 instances, with the added benefit (via OpsWorks and its agent) of us being able to create instances or groups of instances per a blueprint and automate their configuration with Chef.\n\nDoes you're OpsWorks environment use a base image with the build environment already setup?\n\nThe base image that exists on these server instances is an Amazon-provided Red Hat Enterprise Linux 7.2 image. I install Go from Red Hat's repositories via a Chef resource that fetches the golang package via yum.\n\nIf the environment is reused it might explain why you're seeing the issue only in OpsWorks.\n\nSo yes, I suppose the environment is being reused. My Chef cookbook creates the GOPATH, which is /usr/local/src/go; the cookbook then does a go get for the package I need while using GOPATH as the working directory (i.e. it's actually running the go get inside of /usr/local/src/go.\nAs I'm writing this, though, I've got a vague idea that it might be problematic... I know you're not offering support for Go, but perhaps you can point out if I'm doing something stupid!\n\nI don't think you'd be experiencing these issues if your Go version was updated to 1.5. I believe these issues were corrected in later version of go. If you're able to update 1.5 and 1.6 also have significant improvements to the Go GC that your application might benefit from.\n\nSounds like a good idea in general. Looks like Red Hat is distributing a pretty old version... it's a bit of a pain to have to install this outside of a package manager, but perhaps I'll just start including a later, stable version of Go with my Chef cookbooks.\n. I've just configured my cookbook to delete GOPATH/pkg/ and its contents before I go get the package that I'm using. Hopefully I'm not speaking too soon, but it seems as if that's solved the problem for now.\n. ",
    "grepory": "Oh fantastic. Thanks!\n. ",
    "thomshutt": "The problem was the endpoint - s3.amazonaws.com isn't as global as it looks\n. ",
    "oharlem": "@xibz \nHi,\nTo unmarshal: dynamodbattribute.UnmarshalMap()\nTo marshal this particular obj used ExpressionAttributeValues property of dynamodb.UpdateItemInput()\n. @xibz,\nThank you!\n. @xibz \nAlas, I can reproduce.\nMy code:\n``` Go\n    params := &dynamodb.UpdateItemInput{\n        TableName: aws.String(\"posts\"),\n        Key: map[string]dynamodb.AttributeValue{\n            \"postId\": {\n                S: aws.String(\"1dbc76e1-04c0-42f6-aea8-2b82254ecea4\"),\n            },\n        },\n        UpdateExpression: aws.String(\"SET #ATTR = #ATTR + :val\"),\n        ExpressionAttributeValues: map[string]dynamodb.AttributeValue{\n            \":val\": {\n                N: aws.String(\"1\"),\n            },\n        },\n        ExpressionAttributeNames: map[string]*string{\n            \"#ATTR\": aws.String(\"statistics.likeCount\"),\n        },\n        ReturnValues: aws.String(\"NONE\"),\n    }\n_, err := DB.UpdateItem(params)\nif err != nil {\n    return err\n}\n\n```\nError message: \"Error #01: ValidationException: The provided expression refers to an attribute that does not exist in the item status code: 400, request id: ...\"\nBtw., a little off-topic, why have you used AV as\nGo\n\":val\": &dynamodb.AttributeValue{                                                                                    \n        N: aws.String(\"1\"),                                                                                                \n      },\nand not just\nGo\n\":val\": {\n                N: aws.String(\"1\"),\n            },\n?\nThank you!\nD\nP.S.\nI'm using AWS SDK Go 1.2.6, Go 1.6.2\n. @xibz \nUpdate.\nJust realized, I think I know the issue: in your example you are creating an attribute where \"Test.Item\" is a string, serving as a key name.\nI.e. in Dynamo's console it looks like \nTest.Item Number: 1\nIn my case, as per the ticket title, I have a map (\"statistics\"), i.e.\nstatistics Map {2}\n   likeCount Number : 0\n... and I'm trying to update values of its keys.\nI.e., in my Post struct I have:\n`` Go\n...\nStatistics     PostStatisticsjson:\"statistics, omitempty\"`\n}\ntype PostStatistics struct {\n    LikeCount    uint64 json:\"likeCount\"\n        ...\n}\n```\nThen I marshal this struct using dynamodbattribute.MarshalMap() with PutItem().\nSo my intent is to increment counters in the \"statistics\" map.\n. Ooookay....\nLooks like i've found what's up: http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ExpressionPlaceholders.html\n\"... what if you decided to use an expression attribute name instead? For example, what would happen if you were to define #mmmk as a substitute for MyMap.MyKey? DynamoDB would return an empty result, instead of the expected string. This is because DynamoDB interprets a dot in an expression attribute value as a character within an attribute's name. When DynamoDB evaluates the expression attribute name #mmmk, it determines that MyMap.MyKey refers to a scalar attribute ...\nThe correct approach would be to define two expression attribute names, one for each element in the document path:\nmm \u2014 MyMap\nmk \u2014 MyKey \"\nSorry for taking your time with this. Really not intuitive...\n. Update\nAs per @natecode's comment in https://github.com/mathcamp/flywheel/issues/34 it looks like if you are paginating through an index, you have to pass table's PK as well (Amazon needs to add to documentation!).\nOK. Not throwing this error now, however there are no results returned as well, i.e. there's a result for the 1st call, but no results when you set ExclusiveStartKey.\nI suspect may be related to my case of DESC sorting with RFC3339 string used as a range key  multiplied by my lack of understanding if Limit is involved, so cannot confirm now... Any help is appreciated!\n. OK. Solved.\nWhat you need:\n1. When paginating against an index, you ALSO need to provide key(s) from the table.\n2. Because I need to transfer a last key to a client and back, to avoid serializing, etc. of the overblown page item being a LastEvaluatedKey, i'm just taking the DynamoDB primary key values, concatenate and send as a string. Then send back, parse, and use in ExclusiveStartKey.\n3. There was some confusion \"out there\": I do set Limit to the number of elements I need to return per page.\n4. I return \"return false\" in the end of \"func(page *dynamodb.QueryOutput, lastPage bool) bool, {\"\nas I need only result of the first page.\nHope this helps.\nStill, if you'll find anything wrong with my approach - I would appreciate your ping.\nP.S.\nAmazon needs to do some homework documenting DynamoDB.\n. ",
    "gonber": "My workaround is the following loop between Get and Put\nfor _, grant := range respGetObjectAcl.Grants {\n    if grant.Grantee.EmailAddress != nil {\n      grant.Grantee.Type = aws.String(s3.TypeAmazonCustomerByEmail)\n    } else if grant.Grantee.ID != nil {\n      grant.Grantee.Type = aws.String(s3.TypeCanonicalUser)\n    } else if grant.Grantee.URI != nil {\n      grant.Grantee.Type = aws.String(s3.TypeGroup)\n    }\n  }\n. Hi @xibz. Don't you think it would be worth fixing this in the SDK? I'm confused...\n. @xibz - This issue is not about the type of an object but the types of permissions. The GetObjectAcl method fills in the fields EmailAddress, ID or URI based on type. However it never sets the field Type. This field is nevertheless required for PutObjectAcl to succeed. Does that make sense?\n. ```\noptionsGetObjectAcl := &s3.GetObjectAclInput{\n      Bucket: aws.String(bucketSrc),\n      Key: aws.String(keySrc),\n  }\n  respGetObjectAcl, err := svc.GetObjectAcl(optionsGetObjectAcl)\n  if err = handleSdkError(err); err != nil {\n    return\n  }\nfor _, grant := range respGetObjectAcl.Grants {\n    if grant.Grantee.EmailAddress != nil {\n      grant.Grantee.Type = aws.String(s3.TypeAmazonCustomerByEmail)\n    } else if grant.Grantee.ID != nil {\n      grant.Grantee.Type = aws.String(s3.TypeCanonicalUser)\n    } else if grant.Grantee.URI != nil {\n      grant.Grantee.Type = aws.String(s3.TypeGroup)\n    }\n  }\noptionsPutObjectAcl := &s3.PutObjectAclInput{\n      Bucket: aws.String(bucketDst),\n      Key: aws.String(keyDst),\n      AccessControlPolicy: &s3.AccessControlPolicy {\n        Grants: respGetObjectAcl.Grants,\n        Owner: respGetObjectAcl.Owner,\n      },\n  }\n  _, err = svc.PutObjectAcl(optionsPutObjectAcl)\n  err = handleSdkError(err)\n```\nIn this case the permissions were set via the console. \nOk, I'll keep the loop in place then. Thanks for your support!\n. ",
    "Sjeanpierre": "Addressed PR comments from @xibz \n. The thought process behind this was that someone would want to filter all instances in account instead of from single region.\n. I've addressed other comments in this PR. not sure what this comment means. Do you want me to add instructions on how to run within the main function?\n. Done\n. ",
    "krish7919": "@xibz Thanks!\n. ",
    "fsenart": "@xibz we've implemented a comprehensive set of type definitions for AWS Lambda event sources https://github.com/eawsy/aws-lambda-go-event and continue to expand it over the time. The only purpose of the project is to provide type definitions with detailed documentation.\nIt may be of interest for AWS? We may transfer the ownership of the project to you in order to make it more \"official\", etc. Let me know what do you think.. @xoraes @xibz we use similar solutions for the ApproximateArrivalTimestamp of Kinesis records. Notice that the timestamp has millisecond precision in these cases with a particular format: \n// \"sec\".\"msec\"\n1479269477.816\nYou have to take care of it in the UnmarshalJSON method:\n``` go\nfunc (t *Timestamp) UnmarshalJSON(data []byte) error {\n    v, err := strconv.ParseFloat(string(data), 64)\n    if err != nil {\n        return err\n    }\nsec := int64(v)\nnsec := int64((v - float64(sec)) * float64(time.Second))\n\nt.Time = time.Unix(sec, nsec)\nreturn nil\n\n}\n``\n. @xibz I have taken the liberty of proposing this change because of:\n1. This is [already](https://github.com/aws/aws-sdk-go/blob/master/aws/awsutil/prettify.go#L32) the case with \"buffers\".\n2. In most cases, these bytes slices are the one used in Kinesis/Dynamodb streams. Thus, the \"pretty\" of thePrettifyfunction become very ugly when trying to print simply a Kinesis record, etc.\n3. I didn't see any use case in the sdk (beyond test fixtures), wherebyte` slices represent a few bytes. In most cases there are a lot of bytes. And, I repeat, this is a \"prettyfier\" not an inspector. \nI understand your point of view and I only defend mine by using the fact that this function should output something more human readable, isn't it?\n. Maybe we can make Prettify a variadic function (to conserve compatibility) with an optional second parameter to enforce compactness of the output?\n. I will make necessary changes and update the PR.\n. :heavy_check_mark:\n. ",
    "dexterous": "Don't the events defined in github.com/aws/aws-lambda-go/events essentially serve this purpose?. ",
    "d-smith": "Hi @dexterous - yes now with full support for golang in Lambda we are definitely good here - such a pleasure to close this. . ",
    "gracedo": "Thanks for the tip -- I've decided to use the CLI to launch the jobs instead of using the API, which is the faster option for me right now. I think it would still be good to support the other step types in the go sdk (I've used the Boto package to do the same work in Python https://github.com/boto/boto/blob/develop/boto/emr/step.py)\n. ",
    "elimisteve": ":+1:, FWIW\n. ",
    "jlory": "Thank you very much :)\n. ",
    "kayaklee": "This is the sample code, the struct incompleteReader is mock http response body, it will return an unexpected EOF while reading, to simulate the http connection break. But the s3 sdk does not handle the \"unexpected EOF\" correctly. \n``` go\npackage main\nimport (\n    \"fmt\"\n    \"github.com/aws/aws-sdk-go/aws\"\n    \"github.com/aws/aws-sdk-go/aws/credentials\"\n    \"github.com/aws/aws-sdk-go/service/s3\"\n    \"github.com/aws/aws-sdk-go/service/s3/s3manager\"\n    \"io\"\n    \"net/http\"\n)\nfunc newS3Client(endpoint string, region string, accessKey string, secretKey string) *s3.S3 {\n    creds := credentials.NewStaticCredentials(accessKey, secretKey, \"\")\n    config := &aws.Config{\n        Credentials:             creds,\n        Endpoint:                &endpoint,\n        Region:                  &region,\n        DisableSSL:              aws.Bool(true),\n        HTTPClient:              http.DefaultClient,\n        MaxRetries:              aws.Int(2),\n        LogLevel:                aws.LogLevel(aws.LogDebugWithSigning),\n        DisableParamValidation:  aws.Bool(true),\n        DisableComputeChecksums: aws.Bool(true),\n        S3ForcePathStyle:        aws.Bool(true),\n    }\n    return s3.New(config)\n}\nfunc upload(s3 *s3.S3, bucket string, key string, body io.Reader) {\n    opts := &s3manager.UploadOptions{\n        PartSize:          256 * 1024 * 1024,\n        Concurrency:       4,\n        LeavePartsOnError: false,\n        S3:                s3,\n    }\n    uploader := s3manager.NewUploader(opts)\n    if uploader != nil {\n        _, err := uploader.Upload(&s3manager.UploadInput{\n            Body:   body,\n            Bucket: aws.String(bucket),\n            Key:    aws.String(key),\n        })\n        if err == nil {\n            fmt.Printf(\"upload succ, bucket=[%s] key=[%s]\\n\", bucket, key)\n        } else {\n            fmt.Printf(\"upload fail, err=[%s] bucket=[%s] key=[%s]\\n\", err.Error(), bucket, key)\n        }\n    }\n}\ntype incompleteReader struct {\n    finish bool\n}\nfunc (self *incompleteReader) Read(p []byte) (n int, err error) {\n    if self.finish {\n        return 0, io.ErrUnexpectedEOF\n    } else {\n        self.finish = true\n        return copy(p, []byte(\"Hello world!\")), nil\n    }\n}\nfunc main() {\n    s3 := newS3Client(\"s3.amazonaws.com\", \"us-east-1\", \"\", \"\")\n    upload(s3, \"kayaklee-us\", \"s3_go_sdk\", &incompleteReader{})\n}\n```\n// jasdel added code block markdown\n. Thank you @jasdel \uff0c hope it will be fixed as soon as possible \n. ",
    "TarlochanKalsi": "@MathieuMailhos I am trying to implement the same logic in GO.  The example_test.go is not there anymore.  (https://github.com/aws/aws-sdk-go/blob/master/service/cognitoidentityprovider/examples_test.go) \nCan you please refer/point me to another example with usage in GO using cognitoidentityprovider.InitiateAuthInput to sign in as an user and get the access token?\nThanks in advance.. ",
    "revett": "For reference, I also had this issue:\n\nI was using a global secondary index.\nIt had a HASH and RANGE key.\nI was only supplying the HASH key.. \n",
    "ColourboxDevelopment": "There are two more if service is s3 checks in v4.go.. I don't know if those relate to glacier as well?\n. ",
    "MathieuMailhos": "Thank you xibz.\nSo I found weird that the object has both a Verifiy function and a SigningString() function with the Verify function asking for the output of SigningString() without loading it automatically. Anyway it is just a design matters.\nPublic cert values n and e are b64 encoded in jwks. I am trying to convert them to *big.Int and int as asked by the rsa.PublicKey interface. \nbig.Int.NewInt() is asking for a int64 parameter. \nThis is what I did so far, without success:\ndecoded, _ := base64.StdEncoding.DecodeString(n) (string to byte[])\nvalue _ := binary.Uvarint(decoded) (byte[] to uint64)\nbigint := big.NewInt(value)\nWhich leads to: e=1, n=4241445464621 and finally:\njwt.SigningMethodRS256.Verify(signing_string, token.Signature, rsa.PublicKey{N: big.NewInt(4241445464621), E: 1})\nBut I am running into this issue:\nkey is invalid\n. I tried to cast the signing string to []byte but received:\ncannot use ([]byte)(signing_string) (type []byte) as type string in argument to jwt.SigningMethodRS256.Verify \nYou are refering to the key but it has this structure, I think this part is fine. \ntype PublicKey struct {\n    N   *big.Int // modulus\n    E   int      // public exponent\n}\n. Right, dereferencing the structure did the trick:\njwt.SigningMethodRS256.Verify(signing_string, token.Signature, &rsa.PublicKey{N: big.NewInt(4241445464621), E: 1})\nWhich leads to, (with the log package):\n2016/08/03 17:31:33 crypto/rsa: verification error\nSo I assume that the Verify function worked but the values decoded from the base64 jwks are wrong (for example, e=1 is very doubtful). \nI updated the conversion process with the following:\nn := \"rdTmzrh7t0i_YN0MDLejnS0jXIFoSzRfFEbqf-bwGuRLnhLI4T3zGAk9HGZeAG6B5gg1D40Jsz1upo4E70VS0raGfSBPYPO7ZAJ2VCUUeblr9X_aWK4f294v4Cf3n8jZyFcGK9qhgcqy3DlHqqDANtjamWVtEhTRTFc-qoz1ScvHmPupsXlj1FsAEFEbVhP4705ez5gW3uQOoidrm38sPFwCN7g7xhA9CyzF04Zsjky55OfMCyWlIt7nljLx7ZRG3dVRD3vdEBI99qtxf43qMCWSPUk7Whn11Wf_u0xDrWhtGR9k599rKBBRWuqcujYYnFuOT0BeQIL25cePPK8lxw\"\ndecoded, _ := base64.StdEncoding.DecodeString(n)\nvar x big.Int\nbigintvalue := x.SetBytes(decoded)\nlog.Println(bigintvalue)\nOutput: \nn= 191129916979323\ne= 65537  (it looks much better!)\nFinally\njwt.SigningMethodRS256.Verify(signing_string, token.Signature, &rsa.PublicKey{N: x.SetInt64(191129916979323), E: 65537})\nWhich still leads to:\n2016/08/03 18:08:36 crypto/rsa: verification error\nEdit: Same issue with a fresh token.\nGoing to compare the b64 decoding with different languages such as Python (hopefully it does not come from bad SigningString). \nEdit: Looking closely, it looks like it is URL Base 64 encoding, not Std or Raw. Still not matching...\n. @xibz Yes the initial token is correct and contains 2 points to separate the header from the payload from the signature.\nHere is a simple piece of code including an non-altered access token and the public key related to the token: \nhttps://gist.github.com/MathieuMailhos/361f24316d2de29e8d41e808e0071b13\nOutput: \n2016/08/04 00:59:49 Tokenuse= access\n2016/08/04 00:59:49 TokenHeaderKid= jA6QojJtFB4NcHGPfq/9eh3Hr6buWXB4W91E7ymccJM=\n2016/08/04 00:59:49 Exp= 1470278311\n2016/08/04 00:59:49 TokenSignature= GYZQKv7o8_o9E4ktVKZngYD4BS5QluOMwE-MRcJB432CmNimQm6JbvT3H48ECThe4f3sZ1KyVbgDJbyUnlkaAwMEBjMnlV7AUaZb-ifveM7kHM30BS5LCV_SCiCk-PvmWjeIHu9bR3EwG8azJCceD5A7gDLmhAtPN94gRy-opXJPAnaCba00AwKBd_pN3UH7LYu4u4EQ29eIfn4k4RCLuR31jr7ad3dvvjhhy658dQSHzSuPZGcN1-CRVSlrd0nk0Ba2t8W33LtjxM6wzPThWgh0fpy2XEDosGU_9FiXdEjUKisE3VHxroygQ8ekVWKHssa2eujXCx8OthWzaGag0w1\n2016/08/04 00:59:49 Signing String= eyJhbGciOiJSUzI1NiIsImtpZCI6ImpBNlFvakp0RkI0TmNIR1BmcS85ZWgzSHI2YnVXWEI0VzkxRTd5bWNjSk09In0.eyJleHAiOjE0NzAyNzgzMTEsInRva2VuX3VzZSI6ImFjY2VzcyIsImlzcyI6Imh0dHBzOi8vY29nbml0by1pZHAudXMtZWFzdC0xLmFtYXpvbmF3cy5jb20vdXMtZWFzdC0xX0d1OFlhVGg3MiIsImNsaWVudF9pZCI6IjRwNzNuYjhra3NsbHJrbTlzMzdzYXZsNzEzIiwidXNlcm5hbWUiOiJtYXRlbyIsImtpZCI6IiIsImFsZyI6IiIsImp0aSI6IjhmNTBiZmU4LWVlNGUtNGFkZi04MDQxLWU5MGM4YWJkZDExZCIsImlhdCI6MTQ3MDI3NDcxMSwic3ViIjoiYjFjMDZhMTktYjE5Yy00NGMwLTgzZjctODY2NTZjYzRmMjMxIn0\n2016/08/04 00:59:49 Value of N = 21944212446918148307583266513211511961176501179660110972882270757464120247554839864039423096862533136364974693915136668416630333929475539217929135693935014796040215031370110392351836608149288005498214604075814317642291632460369313969964278103135047317799644939426174469533574133378199425168189176269507635563270873057483981163804984573367938044175828995131343800369166239708527999544583873649523245709447765091038652758632917341001616695912404965678061555823269517387737693508193881028187912513285002546706797506255288735805912213209305939523528571103281920754204216834697552752238593850648051945624431639572960454087\n2016/08/04 00:59:49 Value of E = -1\n2016/08/04 00:59:49 crypto/rsa: verification error\nI am still thinking that the problem is coming from the base64 raw data not properly converted into big.Int and int because all of the fields extracted from the Access Token looks correct. \n. Thanks a lot, it fixed the issue.. ",
    "ghdna": "Use cognito-express module - it does the decoding and verification of access key signature. . ",
    "jbergknoff-rival": "Can somebody explain why the default behavior is what it is, i.e. that you can't assume a role with AWS_PROFILE unless you also specify AWS_SDK_LOAD_CONFIG=1. Coming from Python and node, where the SDKs will successfully assume role without an equivalent of AWS_SDK_LOAD_CONFIG, it's surprising and confusing behavior.\nIs there some downside to behaving as the other languages' SDKs in this instance? I'm guessing there's a reason somebody would want AWS_SDK_LOAD_CONFIG to be off: what is that reason?. Thanks, I'll revisit this when I have a chance.. @jasdel thanks again for the suggestions! I've refactored the PR to do the credential lookup in the session initialization (not sure how I missed this spot the first time!). This also exposes a cleaner interface for testing the interaction with the EC2 metadata service, so I've added what seem like the appropriate tests.\nPlease take another look when you have a chance.. @jasdel please have another look when you have time.. @hoshsadiq no, it doesn't add support for Environment. I'll rename the PR to be more specific.. @xibz great news, thanks for running with this!. I think this would work equally well for EcsContainer, but I don't have a readily-available environment where I could test that.. ",
    "shwarzes89": "@jasdel Actually, that error is occurred on local development environment.\nThis is the set of what i did\nit succeed with wassup/50 request\nand i requested wassup/76,\nthis error message appeared\n**ERROR: context expired before API call urlfetch/Fetch completed\nrequest URL: /wassup/50\nWARNING  2016-08-09 17:52:34,273 urlfetch_stub.py:540] Stripped prohibited headers from URLFetch request: ['Content-Length']\nERROR: context expired before API call urlfetch/Fetch completed\nrequest URL: /wassup/50\nWARNING  2016-08-09 17:52:34,307 urlfetch_stub.py:540] Stripped prohibited headers from URLFetch request: ['Content-Length']\nERROR: context expired before API call urlfetch/Fetch completed\nrequest URL: /wassup/50\nWARNING  2016-08-09 17:52:34,379 urlfetch_stub.py:540] Stripped prohibited headers from URLFetch request: ['Content-Length']\nERROR: context expired before API call urlfetch/Fetch completed\nrequest URL: /wassup/50\n{\n}**\nHowever, i requested wassup/76 but wassup/50 is done with 200 response, error message show the previous request url. \n. ",
    "5k3105": "2016/08/09 15:09:35 Failed to list buckets InvalidArgument: Invalid Argument\n    status code: 400, request id: 0ace510d:1562d76b25d:1574:0\n. ```\n2016/08/09 15:13:17 DEBUG: Request s3/ListBuckets Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nGET http://x.x.x.x:xxxx/ HTTP/1.1\nHost: x.x.x.x:xxxx\nUser-Agent: aws-sdk-go/1.2.8 (go1.6.2; windows; amd64)\nAuthorization: AWS4-HMAC-SHA256 Credential=aop.admin/20160809/us-west-2/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=xxxx\nX-Amz-Content-Sha256: xxxx\nX-Amz-Date: 20160809T211317Z\nAccept-Encoding: gzip\n\n2016/08/09 15:13:17 DEBUG: Response s3/ListBuckets Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 400 Bad Request\nContent-Length: 128\nContent-Type: application/xml\nDate: Tue, 09 Aug 2016 21:13:18 GMT\nServer: ViPR/1.0\nX-Amz-Id-2: \nX-Amz-Request-Id: 0ace5112:1562d76d2c4:157c:0\nInvalidArgumentInvalid Argument0ace5112:1562d76d2c4:157c:0\n2016/08/09 15:13:17 Failed to list buckets InvalidArgument: Invalid Argument\n    status code: 400, request id: 0ace5112:1562d76d2c4:157c:0\n```\n. Yes. I will check. Thanks!\n. Thanks. You are right. The API only supports v2 requests right now. Is there a way I can use an older v2 version of this sdk? \n. Thank you for your response.\n. I believe this is my problem also. We use DELL EMC ECS S3 Compatible storage. The object is found through S3 Browser.\nI have code that still must be compiled with a 2 year old version of this (aws-sdk-go)  library. Maybe this is a V2 signing issue? \n```\n2018/08/20 15:06:02 DEBUG: Request s3/HeadObject Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nHEAD /2018041814_P2C1/L0/Ancillary/2018041814_P2C1_Extraction.png HTTP/1.1\nHost: test-s3.data.asd.org\nUser-Agent: aws-sdk-go/1.15.15 (go1.8.3; linux; amd64)\nAuthorization: AWS4-HMAC-SHA256 Credential=abc/20180820/us-east-1/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=4512...\nX-Amz-Content-Sha256: e3b0...\nX-Amz-Date: 20180820T210602Z\n\n2018/08/20 15:06:02 DEBUG: Response s3/HeadObject Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 404 Not Found\nContent-Length: 291\nCache-Control: must-revalidate,no-cache,no-store\nContent-Type: text/html;charset=iso-8859-1\nDate: Mon, 20 Aug 2018 21:06:02 GMT\nDate: Mon, 20 Aug 2018 21:06:02 GMT\nServer: ViPR/1.0\nStrict-Transport-Security: max-age=31536000; includeSubdomains; preload;\nX-Amz-Id-2:\nX-Amz-Request-Id: 0ace5b0e:164a40353aa:2fe8b:179\nX-Content-Type-Options: nosniff\nX-Frame-Options: DENY\n\n2018/08/20 15:06:02\n2018041814_P2C1/L0/Ancillary/2018041814_P2C1_Extraction.png getExist svc.HeadObject NotFound: Not Found\n        status code: 404, request id: 0ace5b0e:164a40353aa:2fe8b:179, host id:\nfound\n```\ncode is:\n``go\nfunc get_exist() {\n    op :=2018041814_P2C1/L0/Ancillary/2018041814_P2C1_Extraction.pngcreds := credentials.NewStaticCredentials(\"abc\",xyz, \"\")\n    nsession, err := session.NewSession(&aws.Config{\n        Region:      aws.String(region),\n        Endpoint:    aws.String(https://test-s3.data.asd.org:443`),\n        Credentials: creds,\n        LogLevel: aws.LogLevel(aws.LogDebugWithHTTPBody),\n    })\n    if err != nil {\n        println(op, \"new session\", \"getExist\", err.Error())\n    } \n    svc := s3.New(nsession)\nparams := &s3.HeadObjectInput{\n    Bucket:   aws.String(\"bucket1\"),\n    Key:    aws.String(op),\n}\n_, err = svc.HeadObject(params)\nif err != nil { \n    println(op, \"getExist\", \"svc.HeadObject\", err.Error())\n}\nprintln(\"found\")\n\n}\n```\n. Apparently the bucket name is not being appended to the path. This fixed the problem:\ngo\n    params := &s3.HeadObjectInput{\n        Bucket:   aws.String(bucketname),\n        Key:    aws.String(bucketname + \"/\" + op),\n    }\nI assume this is not expected behavior?\n. Hi, thanks for the feedback.\nThis fixed the previous code. It returned the headers with that change.\nUsing the same fix to upload, it uploads zero byte files.\n```\n2018/08/22 14:48:08 DEBUG: Request s3/PutObject Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nPUT /bucket1/testfile.txt HTTP/1.1\nHost: test-s3.data.asd.org\nUser-Agent: aws-sdk-go/1.15.15 (go1.8.3; linux; amd64)\nContent-Length: 0\nAuthorization: AWS4-HMAC-SHA256 Credential=aop-l0-writer/20180822/us-west-2/s3/aws4_request, SignedHeaders=content-md5;host;x-amz-acl;x-amz-content-sha256;x-amz-date, Signature=d9c9953c84cdc80099b46d075fc705d61b8115a49ccf9a6889c465ae887541e9\nContent-Md5: 1B2M2Y8AsgTpgAmY7PhCfg==\nX-Amz-Acl: authenticated-read\nX-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\nX-Amz-Date: 20180822T204808Z\nAccept-Encoding: gzip\n\n2018/08/22 14:48:08 DEBUG: Response s3/PutObject Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 200 OK\nContent-Length: 0\nDate: Wed, 22 Aug 2018 20:48:08 GMT\nEtag: \"d41d8cd98f00b204e9800998ecf8427e\"\nLast-Modified: Wed, 22 Aug 2018 20:48:08 GMT\nServer: ViPR/1.0\nStrict-Transport-Security: max-age=31536000; includeSubdomains; preload;\nX-Amz-Id-2: f11b72adf6598b44b044ca6c55a65bcc0cf69956790a37ffc0e7a4094796a1d6\nX-Amz-Request-Id: 0ace5b0e:164a40353aa:32937:43\nX-Amz-Version-Id: 1534970888641\nX-Content-Type-Options: nosniff\nX-Emc-Mtime: 1534970888641\nX-Emc-Previous-Object-Size: 0\nX-Frame-Options: DENY\n\n2018/08/22 14:48:08\nfalse a6101b4aa850e3e864e11502f8bfe5b1\n```\n```go\npackage main\nimport (\n    \"crypto/md5\"\n    \"fmt\"\n    \"io\"\n    \"os\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/credentials\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\n)\nvar (\n    drive       = /mnt/\n    slash       = /\n    maxw        = 6\n    port        = 443\n    prefix      = https\n    bucketname  = \"bucket1\"\n    bucketowner = bucket1-writer\n/// dev\nbucketpass = `asd`\nendpoint   = prefix + `://test-s3.data.asd.org:` + port\nendpointd  = `test-s3.data.asd.org`\n\n)\nfunc main() {\n    path := testfile.txt\n    file, err := os.Open(/mnt/disk1/ + path)\n    if err != nil {\n        println(path, \"broken\", \"upload:lstat\", err.Error())\n        return\n    }\n    defer file.Close()\n    b, str := uploadFileSmall(path, file)\n    println(b, str)\n}\nfunc uploadFileSmall(op string, f io.ReadSeeker) (bool, string) {\n    var fail bool\nvar result []byte\nhash := md5.New()\nif _, err := io.Copy(hash, f); err != nil {\n    println(op, \"broken\", \"chk5f:md5\", err.Error())\n    fail = true\n    return fail, \"\"\n}\n\nchk := fmt.Sprintf(\"%x\", hash.Sum(result))\n\nupinfo := &s3.PutObjectInput{\n    Body:   f,\n    Bucket: aws.String(bucketname),\n    Key:    aws.String(op),\n    ACL:    aws.String(\"authenticated-read\"),\n}\n\ncreds := credentials.NewStaticCredentials(bucketowner, bucketpass, \"\")\nnsession, err := session.NewSession(&aws.Config{\n    Region:           aws.String(\"us-west-2\"),\n    Endpoint:         aws.String(endpoint),\n    Credentials:      creds,\n    LogLevel:         aws.LogLevel(aws.LogDebugWithHTTPBody),\n    S3ForcePathStyle: aws.Bool(true),\n})\nif err != nil {\n    println(op, \"new session\", \"uploadFileSmall\", err.Error())\n}\nsvc := s3.New(nsession)\n\n_, err = svc.PutObject(upinfo)\nif err != nil {\n    println(op, \"broken\", \"uploadFileSmall:PutObject\", err.Error())\n}\n\nreturn fail, chk\n\n}\n```\n. I will test more but it looks like that was it...\nNot what I expected though. This same code works  when compiling on windows using GOOS=linux but not compiling on linux.\nThank you very much for your help!. ",
    "turtlemonvh": "An v2 spec implementation that works with HmacSHA1 signed requests can be found here:\nhttps://github.com/kahing/goofys/blob/v0.0.15/internal/v2signer.go\nThe HmacSHA256 version is here: \nhttps://github.com/aws/aws-sdk-go/blob/master/private/signer/v2/v2.go\nExample usage can be seen here:\nhttps://github.com/kahing/goofys/blob/v0.0.15/internal/goofys.go#L245. ",
    "xoraes": "Simply trying to unmarshal the dynamodbstream event in the Go json library. \ntype Event struct {\n    Records []Record json:\"Records\"\n}\ntype Record struct {\n    Dynamodb     dynamodbstreams.StreamRecord json:\"dynamodb\"\n}\n// this call is where the problem happens because StreamRecord has time.Time field \n// which has unix time and cannot be unmarshaled by go native json unmarshaller. \nif err := json.Unmarshal(jsonStr, &event); err != nil {\n        return nil, err\n// this is not relevant to the problem but its what i want to do with the event\nfor _, record := range event.Records {\n    if record.EventName == \"INSERT\" {\n    // Now Unmarshal the data for NewImage\n    var rec interface{}\n       dynamodbattribute.UnmarshalMap(record.Dynamodb.NewImage, &rec)\n}\n}\n. Comes dynamodb stream event. When our dynamo table is upserted, it sends a event back lambda in the following form. Thats what jsonStr represents.\njson\n{\n  \"Records\": [\n    {\n      \"eventID\": \"1\",\n      \"eventVersion\": \"1.0\",\n      \"dynamodb\": {\n       \"ApproximateCreationDateTime\": 1470788940,\n        \"Keys\": {\n          \"Id\": {\n            \"N\": \"101\"\n          }\n        },\n        \"NewImage\": {\n          \"Message\": {\n            \"S\": \"New item!\"\n          },\n          \"Id\": {\n            \"N\": \"101\"\n          }\n        },\n        \"StreamViewType\": \"NEW_AND_OLD_IMAGES\",\n        \"SequenceNumber\": \"111\",\n        \"SizeBytes\": 26\n      },\n      \"awsRegion\": \"us-west-2\",\n      \"eventName\": \"INSERT\",\n      \"eventSourceARN\": \"arn:aws:dynamodb:us-west-2:account-id:table/ExampleTableWithStream/stream/2015-06-27T00:48:05.899\",\n      \"eventSource\": \"aws:dynamodb\"\n    }\n  ]\n}\n. Here is the code you can add. I can do a pull req but it looks like you guys are using some sort of code generator? \n``` go\ntype Timestamp struct {\n    time.Time\n}\nfunc (t *Timestamp) MarshalJSON() ([]byte, error) {\n    ts := t.Time.Unix()\n    stamp := fmt.Sprint(ts)\n    return []byte(stamp), nil\n}\nfunc (t *Timestamp) UnmarshalJSON(b []byte) error {\n    ts, err := strconv.Atoi(string(b))\n    if err != nil {\n        return err\n    }\n    t.Time = time.Unix(int64(ts), 0)\n    return nil\n}\n```\n. ",
    "junlong-gao": "Thank you for the fast response, our team is using this go sdk to build our automatic deployment application, this would greatly simplify our app logic if we can specific the region when adding permission in lambda instead of creating a new client in different region every time.\n. ",
    "tleyden": "Ok thanks!  I think stscreds.AssumeRoleProvider is exactly what I was looking for. \n. ",
    "oneumyvakin": "Hmm. Can't reproduce on 1.6.3.\n. ",
    "voutasaurus": "I assume you mean \nsvc := s3.New(session.New(), &aws.Config{LogLevel: aws.LogLevel(aws.LogDebug | aws.LogDebugWithHTTPBody)})\nI will try with this now and then comment again.\nThe version I'm on as stated above is\n```\ncommit f80e7d0182a463dff0c0da6bbed57f21369d4346\nAuthor: xibz impactbchang@gmail.com\nDate:   Thu Aug 11 09:24:59 2016 -0700\nTag release v1.4.1\n\n```\n. Okay now I can't replicate the issue with v1.4.1 either. I'm going to double check version I have vendored in the project that was having the issue.\n. Right sorry the vendored version was v1.2.5. That's the version I was having the issue with.\nLooks like I need to revendor a later version.\n. Actually the first time I ran it it hung and then printed\n2016/12/06 16:06:18 RequestError: send request failed\ncaused by: Get https://route53.amazonaws.com/2013-04-01/hostedzone: EOF\nexit status 1. To be clear this all works fine with go 1.7.4.. This is with https://github.com/aws/aws-sdk-go/tree/v1.6.0 but I was also getting the issue on an earlier version.. I thought I was getting it with ChangeResourceRecordSets but I just checked and that appears to be working. So it's only affecting ListHostedZones as far as I can tell. I haven't tried any of the other functions in the route53 library.. Thanks jasdel. It sounds like Brad wants to fix it in the http package so I guess the pressure is off for a fix in the sdk.. I take that back, looks like he's saying something different.. thanks! :). ",
    "kfir-stratoscale": "Using version aws-sdk v1.3.0.\nIn aws-sdk-go/service/dynamodb/api.go:5451 we have the following definition of ScanOutput\n``\n// Represents the output of a Scan operation.\ntype ScanOutput struct {\n    _ struct{}type:\"structure\"`\n// The capacity units consumed by an operation. The data returned includes the\n// total provisioned throughput consumed, along with statistics for the table\n// and any indexes involved in the operation. ConsumedCapacity is only returned\n// if the request asked for it. For more information, see Provisioned Throughput\n// (http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughputIntro.html)\n// in the Amazon DynamoDB Developer Guide.\nConsumedCapacity *ConsumedCapacity `type:\"structure\"`\n\n// The number of items in the response.\n//\n// If you set ScanFilter in the request, then Count is the number of items\n// returned after the filter was applied, and ScannedCount is the number of\n// matching items before the filter was applied.\n//\n// If you did not use a filter in the request, then Count is the same as ScannedCount.\nCount *int64 `type:\"integer\"`\n\n// An array of item attributes that match the scan criteria. Each element in\n// this array consists of an attribute name and the value for that attribute.\nItems []map[string]*AttributeValue `type:\"list\"`\n\n// The primary key of the item where the operation stopped, inclusive of the\n// previous result set. Use this value to start a new operation, excluding this\n// value in the new request.\n//\n// If LastEvaluatedKey is empty, then the \"last page\" of results has been processed\n// and there is no more data to be retrieved.\n//\n// If LastEvaluatedKey is not empty, it does not necessarily mean that there\n// is more data in the result set. The only way to know when you have reached\n// the end of the result set is when LastEvaluatedKey is empty.\nLastEvaluatedKey map[string]*AttributeValue `type:\"map\"`\n\n// The number of items evaluated, before any ScanFilter is applied. A high ScannedCount\n// value with few, or no, Count results indicates an inefficient Scan operation.\n// For more information, see Count and ScannedCount (http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/QueryAndScan.html#Count)\n// in the Amazon DynamoDB Developer Guide.\n//\n// If you did not use a filter in the request, then ScannedCount is the same\n// as Count.\nScannedCount *int64 `type:\"integer\"`\n\n}\n```\nAs you can see the member Items is []map[string]*AttributeValue. That is a list of items like we can get from GetItem (Item map[string]*AttributeValue in GetItemOutput struct).\nThe following code is taken from a project that I'm working on to unmarshal items returned from GetItem\n```\n// DynamoObject is a generic type used to describe objects in dynamodb\ntype DynamoObject map[string]*dynamodb.AttributeValue\n// GetItem gets an item by its key from dynamo.\n// Note that the data will be placed in the given item instance.\nfunc (s *DynamoSvc) GetItem(item interface{}) error {\n    dynamoObj, err := marshalDynamoObject(item)\n    if err != nil {\n        return fmt.Errorf(\"failed to create & marshal dynamo object: %s\", err)\n    }\nlookupFields := make(DynamoObject, len(s.keys))\nfor _, key := range s.keys {\n    lookupFields[key] = dynamoObj[key]\n}\n\nparams := &dynamodb.GetItemInput{\n    Key:       lookupFields,\n    TableName: s.table,\n}\nresp, err := s.db.GetItem(params)\nif err != nil {\n    return fmt.Errorf(\"failed to get item from dynamo: %s\", err)\n}\n\nif err = unmarshalDynamoObject(resp.Item, item); err != nil {\n    return fmt.Errorf(\"failed to unmarshal the response from dynamo: %s\", err)\n}\n\nreturn nil\n\n}\n// MarshalDynamoObject will convert a given structure to a dynamo object\nfunc marshalDynamoObject(entry interface{}) (DynamoObject, error) {\n    av, err := dynamodbattribute.Marshal(entry)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to marshal dynamo object: %s\", err)\n    }\n    return av.M, err\n}\n// UnmarshalDynamoObject will convert a given dynamo object to a given structure.\nfunc unmarshalDynamoObject(obj DynamoObject, entry interface{}) error {\n    err := dynamodbattribute.Unmarshal(&dynamodb.AttributeValue{M: obj}, entry)\n    return err\n}\n```\nPretty easy to use and a simple wrapper for the GetItem from dynamo.\nFor scan I ended up with a far more complex implementation due to the lacking feature in the unmarshaling library.\n```\n// GetAll returns all the items in the table.\n// Note: if the table is big (over a few MB of data) only partial results will be returned with this call\nfunc (s *DynamoSvc) GetAll(refItem interface{}) (interface{}, error) {\n    params := &dynamodb.ScanInput{\n        TableName: s.table,\n    }\n    resp, err := s.db.Scan(params)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to get all items from dynamo: %s\", err)\n    }\nrefType := reflect.TypeOf(refItem)\nresults := reflect.MakeSlice(reflect.SliceOf(refType), 0, int(*resp.Count))\nfor _, item := range resp.Items {\n    newItem := reflect.Indirect(reflect.New(refType.Elem()))\n    if err = unmarshalDynamoObject(item, newItem.Addr().Interface()); err != nil {\n        return nil, fmt.Errorf(\"failed to unmarshal the response from dynamo: %s\", err)\n    }\n    results = reflect.Append(results, newItem.Addr())\n}\n\nreturn results.Interface(), nil\n\n}\n```\nThis implementation is not optimal but it explains my use case pretty well I think..\nI just want to call Scan and get a slice of objects that use. Similar to GetItem.\n. ",
    "djdenv": "Coincidentally, I just reported this issue here #813. FWIW, I'm the one that found this and is linked to AWS Support Center case #1833264031\n. Fantastic. Thank you!\nOn August 19, 2016 at 1:13:30 PM, xibz (notifications@github.com) wrote:\nHello @djdenv, I see you have already commented on the PR, and thank you for finding this. I will get this merged in as soon as I get it reviewed.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @xibz. I've verified the changes. Everything is working as I would expect now. Thank you!\n. ",
    "liamjbennett": "@xibz I am talking about registering a directory. I determined the api being made my using firebug to watch the requests from the console.\nI will post on the forums also and back-link to this issue.\n. Ref: https://forums.aws.amazon.com/thread.jspa?threadID=237801\n. ",
    "utilum": "The deprecation is documented there, a few lines down, it just looks like half of it was pasted in here.\n. ",
    "dhubler": "As a work around, If anyone is interested, I followed instructions in \n  http://docs.aws.amazon.com/iot/latest/developerguide/protocols.html#http\nported code to golang and using paho MQTT over websockets protocol from Golang SDK.   \n```\nfunc AwsIotWsUrl(accessKey string, secretKey string, region string, endpoint string) string {\n    host := fmt.Sprintf(\"%s.iot.%s.amazonaws.com\", endpoint, region)\n// according to docs, time must be within 5min of actual time (or at least according to AWS servers)\nnow := time.Now().UTC()\n\ndateLong := now.Format(\"20060102T150405Z\")\ndateShort := dateLong[:8]\nserviceName := \"iotdevicegateway\"\nscope := fmt.Sprintf(\"%s/%s/%s/aws4_request\", dateShort, region, serviceName)\nalg := \"AWS4-HMAC-SHA256\"\nq := [][2]string{\n    {\"X-Amz-Algorithm\", alg},\n    {\"X-Amz-Credential\", accessKey + \"/\" + scope},\n    {\"X-Amz-Date\", dateLong},\n    {\"X-Amz-SignedHeaders\", \"host\"},\n}\nquery := awsQueryParams(q)\n\nsignKey := awsSignKey(secretKey, dateShort, region, serviceName)\nstringToSign := awsSignString(accessKey, secretKey, query, host, dateLong, alg, scope)\nsignature := fmt.Sprintf(\"%x\", awsHmac(signKey, []byte(stringToSign)))\n\nreturn fmt.Sprintf(\"wss://%s/mqtt?%s&X-Amz-Signature=%s\", host, query, signature)\n\n}\nfunc awsQueryParams(q [][2]string) string {\n    var buff bytes.Buffer\n    var i int\n    for _, param := range q {\n        if i != 0 {\n            buff.WriteRune('&')\n        }\n        i++\n        buff.WriteString(param[0])\n        buff.WriteRune('=')\n        buff.WriteString(url.QueryEscape(param[1]))\n    }\n    return buff.String()\n}\nfunc awsSignString(accessKey string, secretKey string, query string, host string, dateLongStr string, alg string, scopeStr string) string {\n    emptyStringHash := \"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\"\n    req := strings.Join([]string{\n        \"GET\",\n        \"/mqtt\",\n        query,\n        \"host:\" + host,\n        \"\", // separator\n        \"host\",\n        emptyStringHash,\n    }, \"\\n\")\n    return strings.Join([]string{\n        alg,\n        dateLongStr,\n        scopeStr,\n        awsSha(req),\n    }, \"\\n\")\n}\nfunc awsHmac(key []byte, data []byte) []byte {\n    h := hmac.New(sha256.New, key)\n    h.Write(data)\n    return h.Sum(nil)\n}\nfunc awsSignKey(secretKey string, dateShort string, region string, serviceName string) []byte {\n    h := awsHmac([]byte(\"AWS4\"+secretKey), []byte(dateShort))\n    h = awsHmac(h, []byte(region))\n    h = awsHmac(h, []byte(serviceName))\n    h = awsHmac(h, []byte(\"aws4_request\"))\n    return h\n}\nfunc awsSha(in string) string {\n    h := sha256.New()\n    fmt.Fprintf(h, \"%s\", in)\n    return fmt.Sprintf(\"%x\", h.Sum(nil))\n}\n```\n. One thing to know even if you get this working, websocket connection will\nreset every 24 hours. Ultimately using straight MQTT and TLS was most\nreliable.\nOn Mon, Jul 2, 2018, 8:36 AM Scott Talbert notifications@github.com wrote:\n\nYes, but unfortunately, I'm not using it anymore and don't have access to\nthe code I was using.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-go/issues/820#issuecomment-401845527, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAgfip3PIZCg3P6uWC6cA8Jl1WVDpk7ks5uCj3ugaJpZM4Jwdvx\n.\n. \n",
    "jadekler": "+1 this, we're really hurting not being able to pubsub from iot topics. Thanks @jasdel , yes I am.. ",
    "aidansteele": "FWIW I made a library to fill the gap until the AWS SDK provides support for this: https://github.com/glassechidna/awsiot\ngo\nsess := session.Must(session.NewSessionWithOptions(sessOpts))\niot := awsiot.New(sess)\ntheUrl, _ := iot.WebsocketUrl(\"a1kxjqeyezkt7\") // can be used with the eclipse paho mqtt library. @swt2c I'll check that out. I had it working a couple of weeks ago, haven't tried since.. ",
    "swt2c": "Just a note, as it took me a while to figure this out.  It seems that @dhubler and @aidansteele solutions don't work (anymore?).  IOT seems to want the X-Amz-Security-Token parameter, but it cannot be part of the canonical query parameters.  It has to be added on after signing.. Yes, I did get it working at the time.  You included the security token outside of the signature?. Yes, but unfortunately, I'm not using it anymore and don't have access to the code I was using.. I'm not using any IOT services, I'm only using Presigned URLs to pass off to the paho MQTT client.  The MQTT client sees a 403 HTTP error when trying to connect.. @WIZARDISHUNGRY Unfortunately, I'm no longer using IOT and don't have ready access to the code I was using to look at it.  :-(. ",
    "noliva": "@swt2c @aidansteele Did you got this working? i've used @dhubler solution and appended the securityToken.. but i still get a 403 .. . @swt2c yes, did you use @dhubler example of the library by @aidansteele ?. ",
    "AMeng": "Yep, looking over my failed requests, they are all from Route53.\nLet me know what you think the best path forward is, and if I can help in any way.\n. I'm not terribly familiar with the Terraform internals. Are you suggesting that they are not correctly handling the request failures? This error is from the terraform CLI version 0.7.2. I am not doing anything custom there, just running the plan command. I guess Terraform could handle this failure from their end, but it feels more like an issue with the SDK.\nThe failure seems to come about because AWS is rate limiting my account (I have over 1000 Route53 records), so Terraform waits for the SDK to finish the request (through retries). The actual plan command does seem to take more than 10 minutes when it fails, which is why I assumed the issue was with re-signing. Do you think the race condition I mentioned earlier is a possible issue?\nOne other thing I considered was clock skew, where my machine is off from Amazon's API machine just enough to have the failure raise before the request gets re-signed. Although I was able to reproduce this on both EC2 machines, and my local machine.\n. Any updates on this? I wouldn't mind helping out here, given guidance on what the appropriate fix is.\n. ",
    "allspace": "Hi xibz and jasdel,\nThank you for your explanation. I now understand that you need io.ReaderSeeker because you need content length and calculating digest before sending the data to S3 service. As per AWS S3 specification, \"content digest\" is optional. But \"content length\" seems to be a must. So, this is a limitation of AWS S3 service.\nI don't know the final length of my data in advance (streaming case), so I cannot implement io.ReaderSeeker. The only workaround is to use multipart uploading. It can work, but is also not very good as it has limitations on \"minimum part size\" and \"maximum part numbers\".\nThanks\nMike\n. Hi rfielding,\nI was talking about multipart upload. AWS S3 has a requirement that a part's minimum size is 5MB (except last part). So, before I can call UploadPart function, I have to buffer 5MB data at least.\nThanks\nMike\n. I need try to simulate \"append write\" using S3. That's why I got this idea.\nWasn't method \"UploadPartCopyInput\" for multipart copy?\nIt seems work, but just does not work in the way as what I described.\nIf \"CopySource\" and \"Key\" have the same object name, it will produce error saying that source object is not mentioned.\nIf the two parameters have different name, it seems work. However, for a \"concatenate\" case, that means I need first copy data to a temp object name, and then copy from that object name back to my original name. This is obviously not efficient.\n. Thank you for the example.\nMy requirement is: concatenate B to A\nYour implementation is: combine A and B to C\nYou need further rename C to A. However, S3 does not support rename, so, you have to \"copy\" C to overwrite A.\nAnyway, I guess what I asked for is an enhancement for S3 service, instead of this SDK.\nThank you, and I am closing this issue.\n. I just realized I have no button to close this issue. So, Please close this issue.\n. Thanks for the quick response. I understand this function now.\nIt will be great if I can download some parts of an incomplete multipart upload. That can help me feed my client's \"read\" request without local buffer. I know that needs support on S3 service side.\n. ",
    "eikenb": "That is, if the object passed to s3manager.Upload() implements io.ReaderAt but not io.Seeker it should fall back to standard io.Reader behaviour. Just like it does if it doesn't implement io.ReaderAt.\n. Excellent. I was going to put together a pull request today, but you beat me to the punch. Thank you.\n. As the person who filed #824, I confirm that this fixes the issue and is exactly how I would have done it. \nThanks.\n. ",
    "cunnie": "(pairing with @ljfranklin) log with aws.LogDebugWithRequestErrors | aws.LogDebugWithHTTPBody | aws.LogDebugWithSigning is attached.\ndebug_with_signing_info.txt\n. @jasdel \n\nReviewing the code here it looks like the failure occurs because the downloadPart function exits while the download method is waiting to push into the queue. Is this what you were seeing?\n\nYes! That's exactly what we're seeing.\nMy co-worker @cdutra is interested in submitting a PR; I'm letting him know about contribution guidelines and your suggested fix.\nThanks for the prompt reply, by the way. It's appreciated.. Hi @jasdel Go ahead and do it \u2014 it would probably take less time for you to do it than to have us to do it & you shepherd us through the pull-merge process.\nThanks, you've been aweseome!. Hey @jasdel Thanks, it worked great! We bumped the SDK dependency on our executable, and within a few minutes (when we severed the network) it timed out \u2014 you made us quite happy.\n2017/05/05 18:05:51 performing operation get: RequestError: send request failed\ncaused by: Get https://s3.amazonaws.com/bosh-core-stemcells/vsphere/bosh-stemcell-3363.19-vsphere-esxi-ubuntu-trusty-go_agent.tgz: dial tcp 54.231.120.66:443: i/o timeout. ",
    "bigkraig": "It's already in the API response as ReaderEndpoint\n. ",
    "unclejack": "This works as intended with the latest SDK. I'm sorry for the noise.\n. ",
    "lbailly": "Hello,\nThis documentation http://docs.aws.amazon.com/AmazonS3/latest/\ndev/how-to-page-redirect.html suggests that the metadata x-amz-website-\nredirect-location should be set to allow a redirection.\nSo I made a first test using this code :\nparams := &s3.PutObjectInput{\n       Bucket:             aws.String(bucket),\n       Key:                aws.String(objectKey),\n       Metadata: map[string]*string{\"website-redirect-location\":\naws.String(redirectKey)}}\nand it wasn't OK, the console didn't show the \"Website Redirect Location\"\nbut x-amz-meta-website-redirect-location if I remember well.\nSo as written in the documentation I made a test by removing the \"meta-\"\nfrom the SDK source code and it works.\nI may have missed something.\nThanks,\nLudovic\nLe jeudi 15 septembre 2016, xibz notifications@github.com a \u00e9crit :\n\nHello @LBailly https://github.com/LBailly, thank you for reaching out\nto us. I believe the x-amz-meta- is correct. Why do you need the header\nto be x-amz-?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-go/issues/837#issuecomment-247460123, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AILJyg1z7LF90PsQbtH0X355TetwpHMZks5qqbgCgaJpZM4J-XYR\n.\n. Hi,\n\nI saw that there is a WebsiteRedirectLocation field inside PutObjectInput\nstruct.\nI tested and it worked !\n// If the bucket is configured as a website, redirects requests for this object\n// to another object in the same bucket or to an external URL. Amazon S3 stores\n// the value of this header in the object metadata.\nWebsiteRedirectLocation *string location:\"header\"\nlocationName:\"x-amz-website-redirect-location\" type:\"string\"\nThanks for you help,\nLudovic\nOn Fri, Sep 16, 2016 at 12:56 AM, xibz notifications@github.com wrote:\n\n@LBailly https://github.com/LBailly - It looks like this needs to\nmanually be passed in as a header.\nparams := &s3.PutObjectInput{\n       Bucket:             aws.String(bucket),\n       Key:                aws.String(objectKey),\n}req, out := svc.PutObjectRequest(params)\nreq.HTTPRequest.Header.Set(\"x-amz-website-redirect-location\", location)err := req.Send()\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-go/issues/837#issuecomment-247477791, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AILJyszklcqEHCRUVnZ84ZFosYiC8F02ks5qqc0igaJpZM4J-XYR\n.\n. \n",
    "zmalik": "Hi @jasdel and @pwaller \nI was trying to have a workaround for this, as I also need to use MFA. \nAnd what I'm doing is using the STS credential provider to assume the role. Not sure if it is the right approach. But atleast I avoid running aws-cli\nfmt.Print(\"Enter MFA code: \\n\")\nvar token string\nfmt.Scanln(&token)\ncreds := stscreds.NewCredentials(session, roleARN, func(o *stscreds.AssumeRoleProvider) {\n            o.Duration = time.Hour\n            o.ExpiryWindow = 5 * time.Minute\n            o.RoleSessionName = iamSession\n            o.TokenCode = aws.String(token)\n            o.SerialNumber = aws.String(serial)\n        })\nAnd later using this credentials for the session. Is this a right workaround for MFA?\nAnd its working as for now but not as elegant as I would like it to be.\nSo if I get it right this token could be provided by some TokenProvider like @jasdel mentioned above, so people can implement is as they require, stdin or other integrations\ntype TokenProvider interface {\n    Token() (string, error)\n}\n. ",
    "oli-g": "Hi @jasdel! Do you have any news to share on the topic here? This seems to be a much needed feature, that will ease configuration and usage of tools that builds on this SDK (check this Terraform pull request for example).. ",
    "bmurtagh": "Awesome! Thank you guys very much\n. ",
    "iscofield": "Why is the GO SDK handling this different than the other SDKs?  When I change the AWS_DEFAULT_PROFILE both boto3 and the CLI acknowledge that without any additional intervention or configuration.\n. Thanks @xibz and @jasdel this should resolve the issue if it falls back to AWS_DEFAULT_PROFILE if AWS_PROFILE isn't set.\n. ",
    "baberrehman": "@jasdel Thank you for the response.\nActually my point is on adding duplicate rules, the API shouldn't throw error. If the rule is already added, API should just skip that request instead of throwing the error.\n. @jasdel I am using the following parameters to create the security group:\ngo\n    securityGroupRequest := &ec2.CreateSecurityGroupInput{\n        Description: aws.String(\"Security Group For VPC \" + vpc),\n        GroupName:   aws.String(securityGroup.Name),\n        VpcId:       aws.String(vpc),\n    }\n    resp, err := m.Service.CreateSecurityGroup(securityGroupRequest)\nThis call by default opens all the outbound traffic. When I try to open all outbound traffic, then it throws the error that I described above. I am using the following call to modify rules:\n``` go\n    var outboundRules ec2.AuthorizeSecurityGroupEgressInput\n    outboundRules.GroupId = &securityGroupId\n//some code here\n\nif data.IpProtocol == \"All\" {\n\n    entry := new(ec2.IpPermission)\n    var rule bound = data\n    fromPort, _ := strconv.Atoi(rule.FromPort)\n    fp := int64(fromPort)\n\n    entry.FromPort = &fp\n    entry.ToPort = &fp\n\n    entry.IpRanges = []*ec2.IpRange{{CidrIp: aws.String(rule.IpRanges[0].CidrIp)}}\n\n    var allProtocol string = \"-1\"\n    entry.IpProtocol = &allProtocol\n    outboundRules.IpPermissions = append(outboundRules.IpPermissions, entry)\n\n    entry.IpRanges = []*ec2.IpRange{{CidrIp: aws.String(rule.IpRanges[0].CidrIp)}}\n    outboundRules.IpPermissions = append(outboundRules.IpPermissions, entry)\n}\n\n//some code here to handle other conditions\n\nrespE, errE := m.Service.AuthorizeSecurityGroupEgress(&outboundRules)\n\nif errE != nil {\n    utils.Info.Println(\"Could not add Outbound Rules to Security Group\")\n    utils.Info.Println(errE.Error())\n    deleteSecurityGroup(m, securityGroupId)\n    return \"error\"\n}\n\n```\n. Hi @jasdel thanks a lot for your response. I have resolved this issue by updating the SDK. Can you please give your feedback on the issue #1429 (https://github.com/aws/aws-sdk-go/issues/1429) ? I am still facing issue #1429 . ",
    "Ehekatl": "ok, I fix this problem by add tag dynamodbav:\"application_name\" instead of application_name, but question still, why unmarshal map and number works but not string ?\n. ",
    "akamensky": "I am seeing a similar issue with strings. It is either lack of documentation or some issue with unmarshalling.\nIn my case I have:\n\nTable with string key named \"email\" and string field named \"password\"\nCode below:\n\n```\nfunc login(w http.ResponseWriter, r *http.Request) {\n    if r.Method == \"POST\" {\n        r.ParseForm()\n        email := r.PostForm.Get(\"email\")\n        sess, err := session.NewSession(&aws.Config{Region: aws.String(\"ap-northeast-1\")})\n        if err != nil {\n            fmt.Println(\"failed to create session,\", err)\n        }\n    svc := dynamodb.New(sess)\n\n    params := &dynamodb.GetItemInput{\n        TableName: aws.String(\"redacted\"),\n        AttributesToGet: []*string{\n            aws.String(\"email\"),\n            aws.String(\"password\"),\n        },\n        Key: map[string]*dynamodb.AttributeValue{\n            \"email\": {\n                S: aws.String(email),\n            },\n        },\n    }\n\n    resp, err := svc.GetItem(params)\n\n    if err != nil {\n        // Print the error, cast err to awserr.Error to get the Code and\n        // Message from an error.\n        fmt.Println(err.Error())\n        return\n    }\n\n    u := user{}\n\n    err = dynamodbattribute.UnmarshalMap(resp.Item, &u)\n\n    if err != nil {\n        fmt.Println(err.Error())\n        return\n    }\n    fmt.Printf(\"%s\\n\", resp.Item)\n    fmt.Printf(\"%s\\n\", u)\n}\n\n}\ntype user struct {\n    email string email\n    password string password\n}\n```\nThe output for above code is as follows:\nmap[password:{\n  S: \"5583413443164b56500def9a533c7c70\"\n} email:{\n  S: \"test@test.com\"\n}]\n{ }\nHowever if I unmarshall individual fields it works great:\nerr = dynamodbattribute.Unmarshal(resp.Item[\"email\"], &u.email)\n        err = dynamodbattribute.Unmarshal(resp.Item[\"password\"], &u.password)\nwill produce:\nmap[password:{\n  S: \"5583413443164b56500def9a533c7c70\"\n} email:{\n  S: \"test@test.com\"\n}]\n{test@test.com 5583413443164b56500def9a533c7c70}. @xibz Please see my info above. I experience same problem, using same workaround as OP.. Okay, exporting the fields actually fixes my problem.\nImplicit exports are not the best design decision for programming language apparently. Always get bitten by it.. It is not SDK documentation, it is S3 documentation. If I manually (from console) set Metadata to desired headers I can successfully receive them when request objects from public URLs. Trying to do same from SDK results in malformed headers.\nNow to your comment about setting your pre-defined headers in SDKs struct -- so for EVERY header I or someone else might need to use we need to file a separate issue so that you guys can implement it? Because common HTTP headers are not limited to those you described above, and there is a wide variety of headers that are not present in 100% of HTTP requests, but nonetheless are common, being used and very valid as per HTTP specifications.\nSo the point of the issue is that SDK artificially limits users comparing to what service actually can do.. If my opinion here matters at all I think it is better to have API call fixed to match documentation. After all idempotency of such a call is an important feature and simplifies the code by order of magnitude (and since it\u2019s not idempotent now, my code has to do multiple separate API calls to emulate it from my side, which is not nice)\nSent from my iPhone\n\nOn 17 Oct 2018, at 09:04, Alex Diehl notifications@github.com wrote:\nThank you for bringing this to our attention @akamensky. I am able to reproduce this behavior using the AWS SDKs for Go and PHP, as with #2211 the issue here lies in the Service API rather than the AWS SDK for Go. API models and their associated documentation are provided by the service team and consumed by each of the SDKs.\nI have reached out to the CloudFront team internally about this to determine whether the documentation is correct and the API call is not operating as expected, or if the API call is working as expected and the documentation needs to be updated to reflect the current behavior.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \n",
    "misham": "Wow, that was fast.  Thank you!\nI'll test it out in the next couple of days.\n. ",
    "mbowBC": "anything I can do to help or look at, I'm guessing its a port of the pythonawscli/customizations/s3/syncstrategy\n. yes s3manager looks like the right way.\n. ",
    "colinmutter": "we're in need of this functionality too and are on the path to building this out with existing SDK functionality.  please advise if there's anything we can do to help out with this.\n. ",
    "tuxlife": "I think this is necessary for #849 \n. ",
    "MichaelLiZhou": "Here is a snippet of the debug:\nHTTP/1.1 403 Forbidden\nContent-Length: 114\nConnection: keep-alive\nContent-Type: application/json\nDate: Wed, 28 Sep 2016 17:52:23 GMT\nX-Amzn-Errortype: InvalidSignatureException\n{\"message\":\"Signature expired: 20160928T174419Z is now earlier than 20160928T174                 724Z (20160928T175224Z - 5 min.)\"}\nThanks for the response @xibz \n. Request:\nGET http://lambda.us-east-1.amazonaws.com/2015-03-31/functions/ HTTP/1.1\nHost: lambda.us-east-1.amazonaws.com\nUser-Agent: aws-sdk-go/1.4.8 (go1.6.3; linux; amd64)\nAuthorization: AWS4-HMAC-SHA256\nX-Amz-Date: 20160928T174419Z\nAccept-Encoding: gzip\nClient Creation: \nsess, err := session.NewSession(&aws.Config{\n    LogLevel: aws.LogLevel(aws.LogDebugWithHTTPBody | aws.LogDebugWithSigning),\n    Region:   aws.String(\"us-east-1\"),\n  })\n. @xibz there is only a few functions there <10. \nRequest ID:  22c9f641-8652-11e6-b0b6-0f6f764f14be\nTested the request a few times. --> will try another method\n. ",
    "dcu": "@jasdel see the linked ticket. It's this code: https://github.com/edoardo849/apex-aws-signer/blob/master/signer.go#L49\nIf I change it to:\n``` go\n    originalPath := req.URL.Path\n    req.URL.Path = req.URL.EscapedPath()\n    _, err := t.awsSigner.Sign(req, payload, t.awsServiceName, t.awsRegion, time.Now())\n    if err != nil {\n        log.WithError(err).Error(\"Couldn't sign the request\")\n        return nil, err\n    }\nreq.URL.Path = originalPath\n\n```\nit works fine\n. The url looks like this:\nhttps://search-my-elastic-cluster-slk6s42dsd1jdm2yju3w7kddswds.us-east-1.es.amazonaws.com/logs-*/_search?pretty=true\nand it's a POST\nI'm using the AWS ES service with https://godoc.org/gopkg.in/olivere/elastic.v3\n. good to know @jasdel \ndo you have the code in a branch or something?\n. thanks! are you planning to release this change soon?\n. ",
    "coopernurse": "I'm also getting this warning (with the lambda client, but the root cause is the same).\n. Fantastic, thank you for the quick work on this.\n. ",
    "kashook": "Thank you!\n. ",
    "MikeMangialardi": "Interesting. You may want to change the interface that you accept for the Body parameter in that case. Because you accept an io.ReadSeeker, which does not have a Close() method,I assumed that your library wouldn't close the file. Either way, thank you for for your prompt and thorough response!\n. @jasdel Thanks for the response! That makes sense, and I will work around until that PR is approved and merged. \n. ",
    "cam-stitt": "When I add this, it seems to make it work fine.\ngo\nfunc sendHandler(r *request.Request) {\n    r.HTTPRequest.URL.Path = fmt.Sprintf(\"%s/\", r.HTTPRequest.URL.Path)\n}\n. I think the key difference is the fact I'm inside a docker container on an ec2 instance. Perhaps there is something that causes docker to make 301's act like errors? I'm not certain.\nWe are currently using t2.micro.\n. No, I don't believe so.\n. This is how i used my sendhandler. \nsess := session.New()\nmetadata := ec2metadata.New(sess)\nmetadata.Handlers.Send.PushFront(sendHandler)\n. ",
    "calvn": "I am also encountering this issue with a service that's running inside Docker. curl -v \"http://169.254.169.254/latest/meta-data/iam/security-credentials\" returns fine on the host, but somehow the service that's using the library is not able to get metadata. \n``` go\n    sess, err := session.NewSession()\n    if err != nil {\n        log.Println(\"failed to create session,\", err)\n        return\n    }\nsess.Config.Credentials = ec2rolecreds.NewCredentialsWithClient(ec2metadata.New(sess, aws.NewConfig().WithLogLevel(aws.LogDebugWithHTTPBody)))\n\nsvc := sts.New(sess)\n\nresp, err := svc.GetCallerIdentity(nil)\nif err != nil {\n    log.Println(err.Error())\n    w.WriteHeader(http.StatusInternalServerError)\n    json.NewEncoder(w).Encode(&ErrorResponse{err.Error()})\n    return\n}\n\n``\n. @cam-stitt what did you do to make it work? Where did you addsendHandler()`?\n. @xibz, thanks. How can I get back the response?\n. No dice. This is the error I am getting:\n2016-10-26T21:27:13.958306992Z 2016/10/26 21:27:13 DEBUG: Request ec2metadata/GetMetadata Details:\n2016-10-26T21:27:13.958324725Z ---[ REQUEST POST-SIGN ]-----------------------------\n2016-10-26T21:27:13.958331011Z GET /latest/meta-data/iam/security-credentials HTTP/1.1\n2016-10-26T21:27:13.958336614Z Host: 169.254.169.254\n2016-10-26T21:27:13.958341881Z User-Agent: aws-sdk-go/1.4.22 (go1.7.1; linux; amd64)\n2016-10-26T21:27:13.958347079Z Accept-Encoding: gzip\n2016-10-26T21:27:13.958352432Z \n2016-10-26T21:27:13.958357393Z \n2016-10-26T21:27:13.958362282Z -----------------------------------------------------\n2016-10-26T21:27:18.958680560Z 2016/10/26 21:27:18 EC2RoleRequestError: no EC2 instance role found\n2016-10-26T21:27:18.958711350Z caused by: RequestError: send request failed\n2016-10-26T21:27:18.958718310Z caused by: Get http://169.254.169.254/latest/meta-data/iam/security-credentials/: net/http: request canceled (Client.Timeout exceeded while awaiting headers)\n2016-10-26T21:27:18.958728624Z 2016/10/26 21:27:18 DEBUG: Response ec2metadata/GetMetadata Details:\n2016-10-26T21:27:18.958734898Z ---[ RESPONSE ]--------------------------------------\n2016-10-26T21:27:18.958740257Z HTTP/0.0 000 status code 0\n2016-10-26T21:27:18.958745535Z Content-Length: 0\n2016-10-26T21:27:18.958750891Z \n2016-10-26T21:27:18.958755920Z \n2016-10-26T21:27:18.958760840Z -----------------------------------------------------\n. Also attempted to log the request path in sendHandler(), but it doesn't seem to be writing anything out.\ngo\nfunc sendHandler(r *request.Request) {\n    log.Printf(\"request path: %s\", r.HTTPRequest.URL.Path)\n    r.HTTPRequest.URL.Path = fmt.Sprintf(\"%s/\", r.HTTPRequest.URL.Path)\n}\n. Result with svc.Handlers.Build.PushFront(sendHandler):\n2016-10-26T23:01:29.924949549Z 2016/10/26 23:01:29 request path: /\nNo logs from handler with svc.Handlers.Send.PushFront(sendHandler).\nSame EC2RoleRequestError error on both occasions. \n. Sure, here is the gist of it:\n``` go\nfunc IdentityHandler(w http.ResponseWriter, r *http.Request) {\n    sess, err := session.NewSession()\n    if err != nil {\n        log.Println(\"failed to create session,\", err)\n        w.WriteHeader(http.StatusInternalServerError)\n        return\n    }\nsess.Config.Credentials = ec2rolecreds.NewCredentialsWithClient(ec2metadata.New(sess, aws.NewConfig().WithLogLevel(aws.LogDebugWithHTTPBody)))\nsvc := sts.New(sess)\nsvc.Handlers.Send.PushFront(sendHandler)\n\nresp, err := svc.GetCallerIdentity(nil)\nif err != nil {\n    log.Println(err.Error())\n    w.WriteHeader(http.StatusInternalServerError)\n    json.NewEncoder(w).Encode(&ErrorResponse{err.Error()})\n    return\n}\n\nresponse := IdentityResponse{\n    Identity: resp,\n}\njson.NewEncoder(w).Encode(response)\nreturn\n\n}\nfunc sendHandler(r *request.Request) {\n    log.Printf(\"request path: %s\", r.HTTPRequest.URL.Path)\n    r.HTTPRequest.URL.Path = fmt.Sprintf(\"%s/\", r.HTTPRequest.URL.Path)\n}\n```\n. Thanks for all your help!\nI see it's requesting from the correct path, but still getting the error...\n2016-10-27T00:25:06.423818802Z 2016/10/27 00:25:06 request path before: /latest/meta-data/iam/security-credentials\n2016-10-27T00:25:06.423854814Z 2016/10/27 00:25:06 request path after: /latest/meta-data/iam/security-credentials/\n2016-10-27T00:25:06.423873035Z 2016/10/27 00:25:06 DEBUG: Request ec2metadata/GetMetadata Details:\n2016-10-27T00:25:06.423878555Z ---[ REQUEST POST-SIGN ]-----------------------------\n2016-10-27T00:25:06.423883487Z GET /latest/meta-data/iam/security-credentials/ HTTP/1.1\n2016-10-27T00:25:06.423888470Z Host: 169.254.169.254\n2016-10-27T00:25:06.423893559Z User-Agent: aws-sdk-go/1.4.22 (go1.7.1; linux; amd64)\n2016-10-27T00:25:06.423898411Z Accept-Encoding: gzip\n2016-10-27T00:25:06.423908223Z \n2016-10-27T00:25:06.423912934Z \n2016-10-27T00:25:06.423917507Z -----------------------------------------------------\n2016-10-27T00:25:11.425828978Z 2016/10/27 00:25:11 EC2RoleRequestError: no EC2 instance role found\n2016-10-27T00:25:11.425868572Z caused by: RequestError: send request failed\n2016-10-27T00:25:11.425875278Z caused by: Get http://169.254.169.254/latest/meta-data/iam/security-credentials/: net/http: request canceled (Client.Timeout exceeded while awaiting headers)\n2016-10-27T00:25:11.425885176Z 2016/10/27 00:25:11 DEBUG: Response ec2metadata/GetMetadata Details:\n2016-10-27T00:25:11.425891187Z ---[ RESPONSE ]--------------------------------------\n2016-10-27T00:25:11.425896210Z HTTP/0.0 000 status code 0\n2016-10-27T00:25:11.425901073Z Content-Length: 0\n2016-10-27T00:25:11.425906169Z \n2016-10-27T00:25:11.425910894Z \n2016-10-27T00:25:11.425915553Z -----------------------------------------------------\n. Maybe it's worth mentioning that I am building with Go 1.7. Not sure if this error is due to a regression from Go 1.6 as mentioned in https://github.com/golang/go/issues/16094.\n. Yes I am running the service from within a Docker container, but I am using kubernetes as the orchestration platform. \nHowever, running a container without using k8s or ECS should produce the same result. I had a pod do perform curl on the same host instance and it worked fine, so I am pretty sure that it's not an IAM credential issue with the box.\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: metadata-test-2\n  labels:\n    name: metadata-test-2\n  annotations:\n    iam.amazonaws.com/role: proto-test\nspec:\n  containers:\n  - image: byrnedo/alpine-curl\n    command: [ \"curl\", \"http://169.254.169.254/latest/meta-data/\" ]\n    name: metadata-test-2\n```\nResult:\n2016-10-28T19:27:51.431392399Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n2016-10-28T19:27:51.431414884Z                                  Dload  Upload   Total   Spent    Left  Speed\n2016-10-28T19:27:51.471941907Z ami-id\n2016-10-28T19:27:51.471964277Z ami-launch-index\n2016-10-28T19:27:51.471969850Z ami-manifest-path\n2016-10-28T19:27:51.471974977Z block-device-mapping/\n2016-10-28T19:27:51.471980006Z hostname\n2016-10-28T19:27:51.471984886Z iam/\n2016-10-28T19:27:51.471989656Z instance-action\n2016-10-28T19:27:51.472004771Z instance-id\n2016-10-28T19:27:51.472011174Z instance-type\n2016-10-28T19:27:51.472015877Z local-hostname\n2016-10-28T19:27:51.472020496Z local-ipv4\n2016-10-28T19:27:51.472025176Z mac\n2016-10-28T19:27:51.472030101Z metrics/\n2016-10-28T19:27:51.472034948Z network/\n2016-10-28T19:27:51.472039815Z placement/\n2016-10-28T19:27:51.472044631Z profile\n2016-10-28T19:27:51.472049328Z public-keys/\n2016-10-28T19:27:51.472054109Z reservation-id\n2016-10-28T19:27:51.472058853Z security-groups\n2016-10-28T19:27:51.472073949Z \n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100   240  100   240    0     0   8071      0 --:--:-- --:--:-- --:--:--  8275\n2016-10-28T19:27:51.557636880Z services/\nSimilarly, I was able to run sts get-caller-identity with the aws-cli image on docker, so maybe that could be used as the base case to verify that IAM creds/instance profile is working.\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: aws-cli\n  labels:\n    name: aws-cli\n  annotations:\n    iam.amazonaws.com/role: proto-test\nspec:\n  containers:\n  - image: fstab/aws-cli\n    command:\n      - \"/home/aws/aws/env/bin/aws\"\n      - \"sts\"\n      - \"get-caller-identity\"\n    name: aws-cli\n```\nResult:\n```\n2016-10-28T19:32:40.960032615Z {\n2016-10-28T19:32:40.960090867Z     \"Account\": \"\", \n2016-10-28T19:32:40.960098637Z     \"UserId\": \"AROAJFTU3O5PYXX23BRY6:proto-test-2b001c93\", \n2016-10-28T19:32:40.960104446Z     \"Arn\": \"arn:aws:sts:::assumed-role/proto-test/proto-test-2b001c93\"\n2016-10-28T19:32:40.960109789Z }\n``\n. I was able to verify that the service runs fine and is able to query the metadata to get the instance profile if it's not running in docker.\n. @xibz I was finally able to get past that error. It was a mistake on my part. I am usingkube2iam` and provided the annotation for the role at the deployment metadata level, and not the template/pod level. Thanks so much for following up with me all along!\nI didn't have to implement sendHandler() at all. It was actually giving me 404's before I had to remove it.\n. ",
    "Quentin-M": "--> https://github.com/jtblin/kube2iam/pull/121. I was also very surprised to see the following error popping up.. I expect session.NewSession() to get all the required information by itself when running on EC2.\nMissingRegion: could not find region configuration\nCreating a temporary session simply to find the region through the metadata service, and then creating a real session is indeed counterintuitive. \nThanks.. @jasdel Just to make sure I read the code right, this PR adds end-to-end MD5 validation to all the following functions?\n\ns3manager.NewUploader().Upload()\ns3manager.NewDownloader().Download()\ns3.GetObject()\ns3.GetObject()\n\nThanks!. Thank you so much for your answer/work, much appreciated!. ",
    "OrZipori": "Thanks for the quick response. Indeed I'm using AWS SDK for Javascript.. Sorry for the hassle, I'll post it in the right place. Thanks again\n. ",
    "lionelmessi": "Updated it, thanks!\n. ",
    "nonexu": "@jasdel \nThanks for you reply.\nThere are two ways to solve this issue if I don't miss your suggestion.\n1. Update go version to 1.8( 1.7 used in my machine)\n2. Init sns as your suggestion.\n. @jasdel \nI init the code like this:\n``` go\n    sen := session.New()\n    ms.svc = sns.New(sen, &aws.Config{HTTPClient: &http.Client{\n        Transport: &http.Transport{\n            Proxy: http.ProxyFromEnvironment,\n            DialContext: (&net.Dialer{\n                Timeout:   30 * time.Second,\n                KeepAlive: 30 * time.Second,\n            }).DialContext,\n            MaxIdleConns:    100,\n            IdleConnTimeout: 90 * time.Second,\n        // Only difference between default transport and this example.\n        MaxIdleConnsPerHost: 100,\n\n        TLSHandshakeTimeout:   3 * time.Second,\n        ExpectContinueTimeout: 1 * time.Second,\n    },\n}})\n\n```\nBut I find it doesn't work. \nThere is still much connection to AWS.\ntcp        0      0 10.0.3.8:50844          54.240.255.44:443       TIME_WAIT   -\ndeveloper@pf-us-app-4:/data/log/apps/platform\n$> sudo netstat -apn | grep TIME | grep 54.240.255.44 -c\n3444\n. Hi @jasdel  Thanks for your reply. \nI changed my code as your suggestion. But the issue was not fixed.\nMy code logic:\n    Because there is about 10000message should be send to aws.\n    So, I Inited 8 goroutines, and inited 1 sns client for every goroutine. Kept 1 sns client for 1 goroutine.\n    But There was still so much timewait.\nThe TCP state after I modified the code:\n$> sudo netstat -apn | grep TIME | grep :443 -c\n5462\n$> sudo netstat -apn | grep TIME | grep 54.240.255.63:443  -c\n2397\n$> sudo netstat -apno  | grep keepalive | grep message | grep :443\ntcp        0      0 10.0.3.8:52511          54.240.255.50:443       ESTABLISHED 8076/message     keepalive (14.42/0/0)\ntcp        0      0 10.0.3.8:53677          54.240.255.50:443       ESTABLISHED 8076/message     keepalive (29.01/0/0)\ntcp        0      0 10.0.3.8:34229          54.240.254.146:443      ESTABLISHED 8076/message     keepalive (28.75/0/0)\ntcp        0      0 10.0.3.8:53072          54.240.255.50:443       ESTABLISHED 8076/message     keepalive (21.84/0/0)\ntcp        0      0 10.0.3.8:55117          216.58.193.74:443       ESTABLISHED 8076/message     keepalive (22.66/0/0)\ntcp        0      0 10.0.3.8:53668          54.240.255.50:443       ESTABLISHED 8076/message     keepalive (28.86/0/0)\ntcp        0      0 10.0.3.8:52512          54.240.255.50:443       ESTABLISHED 8076/message     keepalive (14.39/0/0)\n# $>#  sudo netstat -apno  | grep ES | grep message | grep :443\ntcp        0      0 10.0.3.8:53455          54.240.255.50:443       ESTABLISHED 8076/message     keepalive (26.58/0/0)\ntcp        0   1013 10.0.3.8:53915          54.240.255.50:443       ESTABLISHED 8076/message     on (0.20/0/0)\ntcp        0   1013 10.0.3.8:53890          54.240.255.50:443       ESTABLISHED 8076/message     on (0.20/0/0)\ntcp        0      0 10.0.3.8:46510          17.188.162.78:443       ESTABLISHED 8076/message     off (0.00/0/0)\ntcp        0      0 10.0.3.8:42254          17.188.162.16:443       ESTABLISHED 8076/message     off (0.00/0/0)\ntcp        0      0 10.0.3.8:34950          17.188.164.209:443      ESTABLISHED 8076/message     off (0.00/0/0)\ntcp        0      0 10.0.3.8:55117          216.58.193.74:443       ESTABLISHED 8076/message     keepalive (23.62/0/0)\ntcp        0   6666 10.0.3.8:60899          54.240.251.200:443      ESTABLISHED 8076/message     on (0.20/0/0)\ntcp        0      0 10.0.3.8:53913          54.240.255.50:443       ESTABLISHED 8076/message     keepalive (30.01/0/0)\ntcp        0      0 10.0.3.8:53274          54.240.255.50:443       ESTABLISHED 8076/message     keepalive (22.33/0/0)\ntcp        0      0 10.0.3.8:50751          17.188.163.201:443      ESTABLISHED 8076/message     off (0.00/0/0)\nSome TCP connection is in keepalive state, but the a large of timewait exist.\nDo you have some ideas for this issue?\nThank you inadvance!\n. Hi, @jasdel \n    I attach the test code below.\n    According to our idea, keepalive is activated. There should only a little TCP connection.\n    But there is so many connection, especially most of them is in TIME WAIT.\n    Could you help me check the code. \n    Thank you.\nmain.go.gz\naws_sns.go.gz\n. Thanks for your replay @jasdel \nFrom netstat log, some connection is keepalive, but most of them changed to timewait.\nTime wait connection returns to normal After disable the SSL.\nms.svc = sns.New(sen, &aws.Config{HTTPClient: httpClient, DisableSSL: aws.Bool(true)})\n$> sudo netstat -apno | grep TIME | grep 443 -c\n33\n$> sudo netstat -apn | grep TIME | grep :443 -c\n5462\n. Thanks for your reply @jasdel .\nIt happens with low probability, hard to reproduced in my develop env.\nit occurs in the product env where large records are sent.\nIn the loop where the code is retrying on error, do you know what error the code is receiving?\nI checked the logs, there is no error return, all records is sent successfully.  But some can't be get from kinesis in customer side.\nThere were about 10 records in one put method, and length of the records was about 200k.\nI checked aws console, it did not reach the limit of kinesis.\nThe average speed of put is about 40k in one minute.\nIs it possible that we put much message more than 1M in one second, it failed without error return?  There are about 100 servers as producer to send message via one kinesis stream.\n The average speed in one minute did not reach the limit of kinesis. But in one seconds, It might reach the limit, but did not display in aws console.\nRecords like this with regular format:\npartitionkey:\"ipo,1.0|1001|product_logs#2016-12-15_07#1001#316012.txt|wz-012-gs09.or.aws.tap4fun.com|timestamp_second|md5\"\ndata:\"{\"@timestamp\":\"2016-12-15T06:35:06.670Z\",\"beat\":{\"hostname\":\"wz-012-gs09.or.aws.tap4fun.com\",\"ip\":\"52.24.178.232\",\"name\":\"wz-012-gs09.or.aws.tap4fun.com\",\"version\":\"6.0.0-alpha1\"},\"file_size\":109429,\"input_type\":\"log\",\"length\":1962,\"message\":\"2016-12-15 06:34:58|1001|Invasion|IOS|com.tap4fun.reignofwar|318012|4A77602C-AC69-4202-B531-.........\",\"offset\":109026,\"source\":\"../ipo2/20161215/user_login_logs#2016-12-15_07#1001#318012.txt\",\"type\":\"log\"}\"\nExplicitHashKey:\"0\"  //default value for all record.\nThank you!\n. @jasdel \n    I reproduced the issue in product env with debug enabled.\n    It shows that the request is sent successfully, and reply is Ok from AWS server.\n    The logs and some description is attached.\nCould you help me check it?\nThank you very much!\n\nkinesis-record-missing.gz\n. Hi @xibz \nDescription and logs pasted. Thank you in advance!\nI implemented a small program to resend these five records, it was successfully, and could be got from the stream.  So I think the context of these records is valid.\n\n//There are five records send via one PutRecords method.\n//And the length of each record is: 20398  20300  20376  844  2321\n//From the log, these records were sent successfully, and response is Ok.\n//But I can't get from the kinesis stream in customer side.\n//It happens in low probability about 1/20000.\n//Product Env: There are more than 70 aws ec2 servers to send the record via the same kinesis stream.\n//Issue happend in different servers with low probability.\n2016-12-21T00:55:34Z INFO kinesis send data info filename:../ipo2/20161221/product_logs#2016-12-21_01#1001#315012.txt, filesize:17449999, offset:17446120, aws record length 20398\n2016-12-21T00:55:34Z INFO kinesis send data info filename:../ipo2/20161221/product_logs#2016-12-21_01#1001#314012.txt, filesize:8569720, offset:8567153, aws record length 20300\n2016-12-21T00:55:34Z INFO kinesis send data info filename:../ipo2/20161221/product_logs#2016-12-21_01#1001#318012.txt, filesize:7038292, offset:7037314, aws record length 20376\n2016-12-21T00:55:34Z INFO kinesis send data info filename:../ipo2/20161221/product_logs#2016-12-21_01#1001#323012.txt, filesize:8397, offset:8397, aws record length 844\n2016-12-21T00:55:34Z INFO kinesis send data info filename:../ipo2/20161221/user_login_logs#2016-12-21_01#1001#319012.txt, filesize:265214, offset:264817, aws record length 2321\n2016-12-21T00:55:34Z INFO aws sdk debug info:[stream:ipocore, records length 5]\n2016-12-21T00:55:34Z INFO aws sdk debug info:[aws sdk start send data]\n2016-12-21T00:55:34Z INFO aws sdk debug info:[DEBUG: Request kinesis/PutRecords Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nPOST / HTTP/1.1^M\nHost: kinesis.us-west-2.amazonaws.com^M\nUser-Agent: aws-sdk-go/1.5.5 (go1.7.1; linux; amd64)^M\nContent-Length: 86494^M\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAIQNJYNWAC6W5S2PA/20161221/us-west-2/kinesis/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-target, Signature=b8ab1fee9afdbc73b4cdf70e594181ab71c8bd78cd9678c4bb      5a93fbb69d4167^M\nContent-Type: application/x-amz-json-1.1^M\nX-Amz-Date: 20161221T005534Z^M\nX-Amz-Target: Kinesis_20131202.PutRecords^M\nAccept-Encoding: gzip^M\n^M\n-----------------------------------------------------]\n2016-12-21T00:55:34Z INFO aws sdk debug info:[DEBUG: Response kinesis/PutRecords Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 200 OK^M\nContent-Length: 410^M\nContent-Type: application/x-amz-json-1.1^M\nDate: Wed, 21 Dec 2016 00:55:33 GMT^M\nServer: Apache-Coyote/1.1^M\nX-Amz-Id-2: dKu5zACp31id2+Uu6bABOLuK7ZIgLnMcgUGM1MdMXm2A7/q4c5GrNgcl+Sc9uiPiZcJ8Po4EC/RKSqUlZ0cClhAlkYOFyjj2^M\nX-Amzn-Requestid: d458a8ef-d9ea-4e2c-848b-7da93e186ed8^M\n^M\n-----------------------------------------------------]\n2016-12-21T00:55:34Z INFO send data to kinesis successfully, data length: 64239, buffer length 900000\n. @jasdel  @xibz \nI have a question about PutRecords api\nMy code is below:\n  _, err := self.client.PutRecords(params)\nShould I check the response of PutRecords like this:\nresp, err :=  self.client.PutRecords(params)\nif err!= nil || *resp.FailedRecordCount != 0{\n    resend the data\n}\nShould I check *resp.FailedRecordCount, if no err return.\nIt may be failed, even through no error return?\nThank you very much!\n. @xibz  @jasdel  Thanks for your help, I will modify my code and verify it in product env.\nAnd I will close it, if no records missing.\nHave a nice day!. Problem solved.\n@jasdel  @xibz\nThanks for your help!\nMerry Christmas And Happy New Year!. @xibz \nThanks for your reply,\nThe answer for this question\nare you only sleeping if there is an error?\nYes, Only sleep 1 second when there is error.\n How often do you make a call to the service if there isnt an error?\n Call GetRecords continuous without sleep.\nIt looks like this issue. I will do some modification and test.\nI will give some feedback later.\nhttps://github.com/aws/aws-sdk-go/issues/301. Hello, @xibz \n   Sorry for delay in responding.\n   This issue is not solved, I have created a ticket for kinesis team.\n   I enable the sdk debug info.\n   We can see http request send at 2017-01-19 00:22:13.580\n   But the debug information from SDK, it get response at 2017-01-19 00:34:18.304.\n   The time in http header is : Date: Thu, 19 Jan 2017 00:22:12 GMT^M.\n   I want to know does sdk debug the header immediately when it receive the header or any delay after it receive the body?  The body is debug after the header.\n2017-01-19 00:22:13.580        main.go:93  [Info  ] aws sdk debug info:[DEBUG: Request kinesis/GetRecords Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nPOST / HTTP/1.1^M\nHost: kinesis.us-west-2.amazonaws.com^M\nUser-Agent: aws-sdk-go/1.6.10 (go1.7.1; linux; amd64)^M\nContent-Length: 257^M\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJYXRKCV4G2A7WBPQ/20170119/us-west-2/kinesis/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-target, Signature=fde4436512a27e006860b66f20a8dacb2f5af9206f0606e24f9cb7c8f653b8aa^M\nContent-Type: application/x-amz-json-1.1^M\nX-Amz-Date: 20170119T002213Z^M\nX-Amz-Target: Kinesis_20131202.GetRecords^M\nAccept-Encoding: gzip^M\n^M\n{\"Limit\":1000,\"ShardIterator\":\"AAAAAAAAAAHeniHBOBOBRr7BWaOpMUFrHaZ5AXhBTJWQk/PRn/baWpvHtGFFLVeG9Mdj66pR90J7ya1k/F3763i3uYPXZDGX7q2uioMsSmYpaJ5uAGNr0trEFafLjFNKuq2SUHFpAn4FpDVLssCHD05/oO1XeKVzvIr+15tI7Fhpgn91Yza7G0P7XRMGL6Dl4FvJ9BKsdQDYQG7NFUu5WeOajT2aIoUU\"}\n2017-01-19 00:34:18.304        main.go:93  [Info  ] aws sdk debug info:[DEBUG: Response kinesis/GetRecords Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 200 OK^M\nContent-Length: 2915437^M\nContent-Type: application/x-amz-json-1.1^M\nDate: Thu, 19 Jan 2017 00:22:12 GMT^M\nServer: Apache-Coyote/1.1^M\nX-Amz-Id-2: nBTcoj6/ZtcWzmfhFmjPT//k33UxvXEyp22mRMeq6m/+8HYA9bWMWPa1HTyabaD6CvrXymfTE9E2UVLW5qptyUlih0F9gN1n^M\nX-Amzn-Requestid: cb24b9b4-ca7d-5b31-9bde-b14185e803bd^M. @xibz\nSorry, I don't understand clearly about this: \"profiling using pprof to see where the bottleneck\"\nCould you give me a detail description about this and how to do it?\nOther question, sdk debug header and body immediately when it receive, why there is no time out \nfrom 2017-01-19 00:22:13.580 2017-01-19 00:34:18.304?\nIs there any data delivery at this time, how can I see the detail information?\nMy server locals on aws cloud, so I think this data can be delivered soon.\nI tried to set http timeout 3 minutes when init  sdk, but it did not fix this issue.\nThank you in advance!\n. @xibz \nThanks for your comment.\nI hope I understand your idea, I modified the function in aws sdk as below.\ngo\nfunc (c *Kinesis) GetRecords(input *GetRecordsInput) (*GetRecordsOutput, error) {\n    req, out := c.GetRecordsRequest(input)\n    req.Handlers.Send.PushBack(func(r *request.Request) {\n        fmt.Println(\"Time:\", time.Now().UTC())\n        })\n    err := req.Send()\n    return out, err\n}\nIssue was reproduced again, it took more than 1 hour this time.\nTime info is debugged as below after body debugged.\nTime: 2017-01-20 05:31:15.605637078 +0000 UTC\n```\n2017-01-20 04:13:00.312        main.go:460 [Info  ] start get records:shardId-000000000002, AAAAAAAAAAHDBZ56CD+KUTPh2oPGPe9vDxaGOp3iYA9I6QATwcI4NlRWuFmyJROzK3D22/s5jfJhYDJBEFNY4yhoWimXthuGiC64gCSwOT0xO+4PVePkmLEiD/Ug6QuW2WgQOA/H3bpQW1eXalttSrzvXUkSBaM0HByFARLtqYFC8v5vi48xlRJ4emKKwjyvsprP2WfiCpjsjT0Xi2+kc7dORjuCyASI, 1000\n2017-01-20 04:13:00.312        main.go:93  [Info  ] aws sdk debug info:[DEBUG: Request kinesis/GetRecords Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nPOST / HTTP/1.1^M\nHost: kinesis.us-west-2.amazonaws.com^M\nUser-Agent: aws-sdk-go/1.6.10 (go1.7.1; linux; amd64)^M\nContent-Length: 257^M\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJYXRKCV4G2A7WBPQ/20170120/us-west-2/kinesis/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-target, Signature=5a1dd0fae438cda816a5c166938777ce22eab2a1e85c92a2d4c7699656942922^M\nContent-Type: application/x-amz-json-1.1^M\nX-Amz-Date: 20170120T041300Z^M\nX-Amz-Target: Kinesis_20131202.GetRecords^M\nAccept-Encoding: gzip^M\n^M\n{\"Limit\":1000,\"ShardIterator\":\"AAAAAAAAAAHDBZ56CD+KUTPh2oPGPe9vDxaGOp3iYA9I6QATwcI4NlRWuFmyJROzK3D22/s5jfJhYDJBEFNY4yhoWimXthuGiC64gCSwOT0xO+4PVePkmLEiD/Ug6QuW2WgQOA/H3bpQW1eXalttSrzvXUkSBaM0HByFARLtqYFC8v5vi48xlRJ4emKKwjyvsprP2WfiCpjsjT0Xi2+kc7dORjuCyASI\"}\n-----------------------------------------------------]\n2017-01-20 05:31:15.598        main.go:93  [Info  ] aws sdk debug info:[DEBUG: Response kinesis/GetRecords Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 200 OK^M\nContent-Length: 2120174^M\nContent-Type: application/x-amz-json-1.1^M\nDate: Fri, 20 Jan 2017 04:13:00 GMT^M\nServer: Apache-Coyote/1.1^M\nX-Amz-Id-2: 2nUehIoVcc8MfC4HDK7EJT8vHz3QPWNzH00LMzwX26tdi03Vb+mIEN+mQLeLKP9IN+crako3GC4lRaVZwl9kgYuBiYvw2WWJ^M\nX-Amzn-Requestid: f4a299ca-c647-7e07-a459-09461b84cff5^M\n{\"MillisBehindLatest\":0,\"NextShardIterator\":\"AAAAAAAAAAFXMOfHraka9+Pw7oLD2AHQVmfwwSyml4fFfhCNG8IKTVA5NvlSE17fPu0JFNeBCRv7rSOaEOPkHaPAkOS0DcsQUxGqBsJ0qr+ZiVKnTbAokuvKIGZWlF+JUjzxgrOmwyysrkF2FEINmyqDEIgjxis/HZ6Mwc9GC4PiaXOHfmjZV+cl7bHo8LDPs8AZjdpKDyhFdIMVRHxkSnDsRIex1RwO\",\"Records\":[....body....]}\nTime: 2017-01-20 05:31:15.605637078 +0000 UTC\n2017-01-20 05:31:15.640        main.go:475 [Info  ] get records length 618:1000, shardId: shardId-000000000002\n```\nDo you need more information?\nSome additional information about this issue:\nThe issue always happened on this server, ip:35.165.XXX.XXX. \nIt is all ok on this server 52.27.XXX.XXX with same executable file and config file.\nBoth of them local aws cloud us-west-2c with different linux OS version\nThank you in advance!\n. Thank you, I only want to know the reason and solution for the issue.\nOtherwise, I can't ensure my application works well on different server, and can't update or change servers  without worries.. @xibz  Thanks for your help!\ninstance id: i-093416b7e6559c70b, which issue occurred on.\n                    i-0511a166cfada4299,  which is all correct with same config file. \nI once started new ec2 with same OS version as  i-093416b7e6559c70b,\nSame issue happened again.  So, I doubt is it related with OS env?. @xibz  Thanks for your idea!\nI once tried this solution. But it time out for 5 times continuously in my test, then Iterator expired.\nMaybe, I need to init new kinesis client, when iterator expired. But it is just a workaround.. @xibz  Sorry for delayed response, I took a leave in the past days.\nIt always takes about 20 minutes to return the result.\nIs there any news about this issue? . @xibz \nCan I do some help for this issue?\nSuch as, I reproduce it and service team checks the request from i-093416b7e6559c70b immediately.\nI will tell you, when it is reproduced. . @xibz  It is blocked now for 5 minutes on i-093416b7e6559c70b. \nCan service team check it now?. @xibz  detail information about this time.\nApi called at 2017-02-03 02:40:36\n      return at 2017-02-03 03:24:41. @xibz  Thanks for your feedback.\nWaiting for your good news.\nSame issue happened in other sever with same OS version.\n. @xibz  There is a sleep 1 second when it failed. \nIt is strange why issue happened on i-093416b7e6559c70b, same executed file is always successful on i-0511a166cfada4299,  If the root case is how it is being used.\nI will profile and check it.. @xibz \nThere is a daily work on 00:00 every day via crontab, so 100% usage at that moment.\nAnd pprof is empty and no data, when it is blocked.\nI update the test code, which just get records from kinesis. Same issue happened.\nCould you help me check whether the code is correct?\nThank you in advance!\nmain.go.gz\n. @xibz Thanks for your reply.\nSame issue happened after add a sleep in loop.\nAnd I checked CPU load is normal when I tested.. @xibz   It delayed for 10 minutes this time.\nSee cpu load via top command.\ntop - 01:14:47 up 38 days, 21:55,  1 user,  load average: 0.00, 0.01, 0.05\n\n. @xibz  Could you test this case on a server with same OS version and configuration as   i-093416b7e6559c70b.\nI think it may be reproduced.. ",
    "danehammer": "Here's an example: https://gist.github.com/danehammer/a2ba50c36158c8c1b7bb17fbddfe969b\nSteps I took:\n\nCreate new folder\nCreate main.go with simple use of code in question\nRun godep save\nRun go install\n\nI get this error: \nmain.go:3:8: no buildable Go source files in <GOPATH>/src/github.com/aws/codegen-example/vendor/github.com/aws/aws-sdk-go/private/model/api. While I understand they're not stable, I would argue it's a bug that I can't depend on them currently, at all, well this specific package anyway, without augmenting the tags I use at build time (which unfortunately is difficult for me to do in a cloud foundry / heroku build pack mode).. ",
    "btai35": "Each batch delete is performing \"successfully\" as seen by the response above, however the messages aren't actually being deleted in my sqs queue. I've went ahead and called the DeleteMessage request instead (looping over my messages) and it has given me the desired result.\n```\nvar entries []sqs.DeleteMessageBatchRequestEntry\nids := make(map[string]bool)\nfor _, msg := range resp.Messages {\n    if !ids[msg.MessageId] {\n        entry := &sqs.DeleteMessageBatchRequestEntry{Id: msg.MessageId, ReceiptHandle: msg.ReceiptHandle}\n        entries = append(entries, entry)\n    }\nids[*msg.MessageId] = true\n\n}\ndeleteParams := &sqs.DeleteMessageBatchInput{\n    Entries:  entries,\n    QueueUrl: aws.String(QUEUE_URL),\n}\ndeleteResp, err := svc.DeleteMessageBatch(deleteParams)\nif err != nil {\n    fmt.Println(err.Error())\n    return\n}\nfmt.Println(deleteResp)\n```\n. Seems like that was the case. Is that the desired result? If so, what is the reasoning behind that? I would assume the batch delete would have the same behavior as a single delete (but with multiple messages)\n. Ah yes, that is exactly what I ended up doing. Thanks!\n. Sounds great!. ",
    "krak3n": "Meh ignore me, ended up being a problem with my out put of the URL.\n. @joeldelvalle check there is no time drift on your machine, i was basically out of sync.. ",
    "keenan-v1": "Not to necro this 3 years later, but for those of us doing programming before our morning cups of coffee: don't Printf your URLs. :). ",
    "joeldelvalle": "Hi, i have the same error.   Can you resolve it?  how?\nregards. Sorry guys my mistake, the backet name was in null. ",
    "luopengift": "900 @jasdel ,I'm sorry to replay slowly.\nI Debug the codes that you have gaven me. It has something wrong with codes ,\"cannot use result.Reservations (type []ec2.Reservation) as type ec2.Reservation in append\".\nI understand it,It can also used function \"DescribeInstancesPages\" to fetch informations each page by page.So I fix it, then It works.But It do not returns what I want, it return 54 instances .\nCodes is:\n     svc := ec2.New(sess)\n//resv := []*ec2.Reservation{}\nparams := &ec2.DescribeInstancesInput{\n    DryRun:     aws.Bool(false),\n    MaxResults: aws.Int64(1000),\n}\npageNum := 0\nerr = svc.DescribeInstancesPages(params, func(result *ec2.DescribeInstancesOutput, lastPage bool) bool {\n    //resv = append(resv, result.Reservations)\n    fmt.Println(pageNum, len(result.Reservations))\n    pageNum++\n    return pageNum <= 10\n})\nThe result is :0 54\n. Yes,I think the function called only once no matter how many pages I have, or \nwhether the return's value of bool I set true or false. \nActually, I have 100 instances.In AWS web console,there are 50 instances per page,and two pages I have. \nThe code's result is Have 54 reservations.\nAll of instance are in one region at ap-southeast-1, I promise.\n. 2016/10/22 04:20:10 DEBUG: Request ec2/DescribeInstances Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nPOST / HTTP/1.1\nHost: ec2.ap-southeast-1.amazonaws.com\nUser-Agent: aws-sdk-go/1.4.17 (go1.7; linux; amd64) Paginator\nContent-Length: 72\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJ5ZSNA52HYQLGYUQ/20161021/ap-southeast-1/ec2/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date, Signature=4ef0622cba8ddeb35e9c483b955a4e6cdf08861f24b3ecdb0a2a4b2a4538a506\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\nX-Amz-Date: 20161021T202010Z\nAccept-Encoding: gzip\n\n2016/10/22 04:20:10 DEBUG: Response ec2/DescribeInstances Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 200 OK\nTransfer-Encoding: chunked\nContent-Type: text/xml;charset=UTF-8\nDate: Fri, 21 Oct 2016 20:20:09 GMT\nServer: AmazonEC2\nVary: Accept-Encoding\n\nNextToken:  \nHave 54 reservations\n. `2016/10/22 04:26:30 DEBUG: Request ec2/DescribeInstances Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nPOST / HTTP/1.1\nHost: ec2.ap-southeast-1.amazonaws.com\nUser-Agent: aws-sdk-go/1.4.17 (go1.7; linux; amd64) Paginator\nContent-Length: 72\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJ5ZSNA52HYQLGYUQ/20161021/ap-southeast-1/ec2/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date, Signature=b1addd5ed9fcc146be093b890c423a0c9b0f3b0d3ab99808fd423658e3705b55\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\nX-Amz-Date: 20161021T202630Z\nAccept-Encoding: gzip\nAction=DescribeInstances&DryRun=false&MaxResults=1000&Version=2016-09-15\n2016/10/22 04:26:30 DEBUG: Response ec2/DescribeInstances Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 200 OK\nTransfer-Encoding: chunked\nContent-Type: text/xml;charset=UTF-8\nDate: Fri, 21 Oct 2016 20:26:29 GMT\nServer: AmazonEC2\nVary: Accept-Encoding\n8000\nxml version=\"1.0\" encoding=\"UTF-8\"?\n\n3c8f153b-c4e9-48c6-992f-52a240261aaf\n\n      ....EC2's info...\n    \n\n0\n\n\nNextToken: \nHave 54 reservations\n. Thanks for your help. Before I used aws-sdk-go,I have already used boto3 that is a python-sdk for aws. The boto3 can get all informations correctly, but it can't work for go-sdk.\nAbout this issue, I have already created case for aws more than one times.But until now,it seems can't solve my trouble.\nI am very very very grateful that you can help me.\n. I used the same aws_access_key_id, same aws_secret_access_key, same path in  \"~/.aws/credentials\" and some config in \"~/.aws/config \". I will try to use another key for main account and re-config it...\nThe configure file in \"~/.aws/config\" is 3 lines total, contains:\n[default]\nregion = ap-southeast-1\noutput = json\nThe key file in \"~/.aws/credentials\" is :\n[default]\naws_access_key_id = xxxxx\naws_secret_access_key = xxxxx\n. ",
    "jferrer21": "Yes.\n. This is how I set up the env. variable.  export GOPATH=$HOME/work\n. I tried to install another package and it failed.  Are you running Go 1.7.3?\n. I re-installed GO, and problem solved.  However,  running the command:\ngo get -u github.com/aws/aws-sdk-go\nI get:\ncd /Users/serverOne/Documents/GO/src/github.com/aws/aws-sdk-go; git pull --ff-only\nfatal: could not read Password for 'https://jfe@bitbucket.org': terminal prompts disabled\npackage github.com/aws/aws-sdk-go: exit status 1\n. Hi,\nWhen I run,   go get -u github.com/aws/aws-sdk-go\nI get, package github.com/aws/aws-sdk-go: directory \"/home/ubuntu/comply/src/github.com/aws/aws-sdk-go\" is not using a known version control system\nIn which folder do I do git init.  Do I also need a remote repository?  \nMy settings:\nGOPATH=\"/home/ubuntu/comply\"\nMy folder structure\n~/comply/src/github.com/user/comply/comply.go\n~/comply/src/github.com/aws/aws-sdk-go\nThank you for your help,\nJulio\n\nOn Oct 25, 2016, at 10:48 AM, Jason Del Ponte notifications@github.com wrote:\n@jferrer21 https://github.com/jferrer21 I'm going to go ahead an close this issue since it seems to be related to Go or the workspace. Let us know if there is anything additional we can help out with.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub https://github.com/aws/aws-sdk-go/issues/903#issuecomment-256108580, or mute the thread https://github.com/notifications/unsubscribe-auth/ABPJCddvUTbmKVI5X3aAQszPrKhutiuaks5q3kDqgaJpZM4KdonQ.\n. Hi,\n\nI have no idea why or how, but now it works.\nThank you anyway,\nJulio\n\nOn Oct 25, 2016, at 5:21 PM, Julio Ferrer julioferrer@mac.com wrote:\nHi,\nWhen I run,   go get -u github.com/aws/aws-sdk-go http://github.com/aws/aws-sdk-go\nI get, package github.com/aws/aws-sdk-go: http://github.com/aws/aws-sdk-go: directory \"/home/ubuntu/comply/src/github.com/aws/aws-sdk-go http://github.com/aws/aws-sdk-go\" is not using a known version control system\nIn which folder do I do git init.  Do I also need a remote repository?  \nMy settings:\nGOPATH=\"/home/ubuntu/comply\"\nMy folder structure\n~/comply/src/github.com/user/comply/comply.go http://github.com/user/comply/comply.go\n~/comply/src/github.com/ http://github.com/aws/aws-sdk-go\nThank you for your help,\nJulio\n\nOn Oct 25, 2016, at 10:48 AM, Jason Del Ponte notifications@github.com> wrote:\n@jferrer21 https://github.com/jferrer21 I'm going to go ahead an close this issue since it seems to be related to Go or the workspace. Let us know if there is anything additional we can help out with.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub https://github.com/aws/aws-sdk-go/issues/903#issuecomment-256108580, or mute the thread https://github.com/notifications/unsubscribe-auth/ABPJCddvUTbmKVI5X3aAQszPrKhutiuaks5q3kDqgaJpZM4KdonQ.\n. Still having issues.\n\n\nI excuse:\ngo get -u github.com/aws/aws-sdk-go\nthen\u2026.\ngo install github.com/user/receive\nI got\u2026\ngithub.com/aws/aws-sdk-go/aws/ec2metadata\n../../aws/aws-sdk-go/aws/ec2metadata/service.go:57:4: error: unknown field \u2018Timeout\u2019 in \u2018http.Client\u2019\n    Timeout: 5 * time.Second,\n    ^\n../../aws/aws-sdk-go/aws/ec2metadata/service.go:87:87: error: reference to undefined field or method \u2018Timeout\u2019\n  return c == nil || (c.Transport == nil && c.CheckRedirect == nil && c.Jar == nil && c.Timeout == 0)\n                                                                                       ^\n\nOn Oct 26, 2016, at 10:19 AM, Jason Del Ponte notifications@github.com wrote:\n@jferrer21 https://github.com/jferrer21 glad you were able to get the go get to work. Let us know if you have any additional issues, or questions.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub https://github.com/aws/aws-sdk-go/issues/903#issuecomment-256417400, or mute the thread https://github.com/notifications/unsubscribe-auth/ABPJCXDMYle8q-ET2-CNXgqSarAIF7tWks5q34uRgaJpZM4KdonQ.\n. His xibz,\n\nSetup a new Go workspace and run go get -u github.com/aws/aws-sdk-go   No errors.\nThen run,  go install github.com/user/test\n../aws/aws-sdk-go/aws/credentials/shared_credentials_provider.go:8:2: cannot find package \"github.com/go-ini/ini\" in any of:\n    /usr/src/pkg/github.com/go-ini/ini (from $GOROOT)\n    /home/ubuntu/test/src/github.com/go-ini/ini (from $GOPATH)\n../aws/aws-sdk-go/aws/awsutil/path_value.go:9:2: cannot find package \"github.com/jmespath/go-jmespath\" in any of:\n    /usr/src/pkg/github.com/jmespath/go-jmespath (from $GOROOT)\n    /home/ubuntu/test/src/github.com/jmespath/go-jmespath (from $GOPATH)\nI looked and the packages go-ini and jmespath are not installed.  Aren't those packages automatically installed when running go get -u github.com/aws/aws-sdk-go\nThank you for your help.\n. Run \n\ngo get -u github.com/aws/aws-sdk-go/aws/...\npackage github.com/aws/aws-sdk-go/aws\n  imports github.com/aws/aws-sdk-go/aws/awserr\n  imports github.com/aws/aws-sdk-go/aws/credentials\n  imports github.com/go-ini/ini\n  imports github.com/go-ini/ini\n  imports github.com/go-ini/ini: no buildable Go source files in /home/ubuntu/test/src/github.com/go-ini/ini\npackage github.com/aws/aws-sdk-go/aws\n  imports github.com/aws/aws-sdk-go/aws/awserr\n  imports github.com/aws/aws-sdk-go/aws/credentials\n  imports github.com/go-ini/ini\n  imports github.com/aws/aws-sdk-go/aws/awsutil\n  imports github.com/jmespath/go-jmespath: directory \"/home/ubuntu/test/src/github.com/jmespath/go-jmespath\" is not using a known version control system\npackage github.com/aws/aws-sdk-go/aws\n  imports github.com/aws/aws-sdk-go/aws/awserr\n  imports github.com/aws/aws-sdk-go/aws/credentials\n  imports github.com/go-ini/ini\n  imports github.com/go-ini/ini\n  imports github.com/go-ini/ini: no buildable Go source files in /home/ubuntu/test/src/github.com/go-ini/ini\npackage github.com/aws/aws-sdk-go/aws\n  imports github.com/aws/aws-sdk-go/aws/awserr\n  imports github.com/aws/aws-sdk-go/aws/credentials\n  imports github.com/go-ini/ini\n  imports github.com/aws/aws-sdk-go/aws/awsutil\n  imports github.com/jmespath/go-jmespath: directory \"/home/ubuntu/test/src/github.com/jmespath/go-jmespath\" is not using a known version control system\n\nWhy do I need buildable Go source files in /home/ubuntu/test/src/github.com/go-ini/ini?  Also the reason you don't see \"/home/ubuntu/test/src/github.com/go-ini/ini\" is not using a known version control system.  It because I added a git repository in the  ini directory.  Do we need to add git repositories to all these packages?  \nThank you again.\n. Are you using GO 1.7.3 ?\n. Still same issue when execute \n\ngo get -u github.com/aws/aws-sdk-go/aws/...\n\ngithub.com/aws/aws-sdk-go/aws/ec2metadata\n../../aws/aws-sdk-go/aws/ec2metadata/service.go:57:4: error: unknown field \u2018Timeout\u2019 in \u2018http.Client\u2019\n    Timeout: 5 * time.Second,\n    ^\n../../aws/aws-sdk-go/aws/ec2metadata/service.go:87:87: error: reference to undefined field or method \u2018Timeout\u2019\n  return c == nil || (c.Transport == nil && c.CheckRedirect == nil && c.Jar == nil && c.Timeout == 0)\nThank you.\n. Everything is working.  \nI deleted the /usr/local/go folder, $GOPATH/pkg  and my GO environments.  Then, re-install GO 1.7.3 and install the aws-sdk-go with the following commands:\ngo get -u github.com/aws/aws-sdk-go/aws/...\ngo get -u github.com/aws/aws-sdk-go/service/...\nThanks to jasdel and xibs for your patience and your help.\n. ",
    "samt42": "thanks @jasdel , I will have a try.\n. ",
    "andrefsp": "I've done some research and didn't find any reason to keep numeric values to be decoded into strings instead of numeric values.  For that reason I've also updated the test with this behaviour. \n. Closed PR. Refer to  #913 \n. I've done some research and didn't find any reason to keep numeric values to be decoded into strings instead of numeric values. For that reason I've also updated the test with this behaviour.\nAnother approach, for backwards compability, could be to add an extra flag on the decoder to explicitly convert the dynamo Numeric attribute into an Int64. \ntype Decoder struct {\n    MarshalOptions\n    UseNumber bool\n    NumericAsInt64 bool\n}\n. ",
    "DavidJFelix": "So my use case right now is that I have a web service which manages file uploads and stores them on a user's behalf on s3. The http.Request object's Body is an io.Reader and I simply validate that the request is authenticated and then save it to s3 on behalf of the user. I was using ioutil.ReadAll and bytes.Reader to turn the request into a PutObjectInput. I think the 2x is caused by some saving I'm doing in the intermediate. The problem is that I have is these files could and likely will be large, so concurrent requests will spike RAM to an unpredictable and unacceptable level.\nI'll look into the io.Reader interfaces that you're mentioning and see if they make sense.\n. Uploader seems to be working closer to how I'd expect with regards to the buffer. It's still a much higher use of memory than goamz. Using uploader, the memory profile appears to go to about 130MB and flat line at 130MB. I'm not sure how it looks with concurrent requests. Here's the code I'm using:\n``` go\nfunc handler(w http.ResponseWriter, r *http.Request) {\n    basicPost(r.Body)\n}\nfunc basicPost(r io.Reader) {\nsess, err := session.NewSession(&aws.Config{Region: aws.String(\"us-east-1\")})\nif err != nil {\n    fmt.Println(err)\n}\nsvc := s3manager.NewUploader(sess)\ninput := &s3manager.UploadInput{\n    Bucket: aws.String(\"bucket\"),\n    Key: aws.String(\"whatever\"),\n    Body: r,\n}\n\n _, err = svc.Upload(input)\n if err != nil {\n      fmt.Println(err)\n}\n\n}\n```\nI know that declaring the session per call is possibly not efficient for  concurrent requests, but I wanted to ensure everything that could be cleaned up would be. All I'm doing right now is watching Activity Monitor in OSX and checking the memory (not virtual memory) field. goamz uses approx. 8MB.\n. @c4milo I'm also testing the use of s3gof3r, since goamz seems to be unmaintained for the past 2 years. Goamz worked, utilizing 30MB of RAM at peak and 8MB idle. s3gof3r seems to be solid interface and once I implement it I'll report back with anecdotal memory profile.\n. @jasdel I don't have a problem with it doing things concurrently, I have a problem with it not releasing 100MB that is gained at some point during transmission.\n. @jasdel are you able to reproduce what I'm seeing with the above code retaining about 100M of RAM even after the request is complete?\n. Just to clarify, it doesn't seem to be leaking memory per request, it seems to hit 100MB and then stay there, even on subsequent requests (which should create a new manager). The issue is that it never returns to the starting RAM of ~3MB.\n. ",
    "Redundancy": "I also have a use case where I want to stream content from one online source where I have an MD5 of the full content, and ensure that the content enters S3 with the same hash (end to end validation of the content copy).\nPutObject would solve this, but the use of ReadSeeker makes it difficult to use an http.Response.Body. The Uploader helps with the ReadSeeker issue, but the downside is that I lose the end-to-end hashes on anything with multipart uploads.\nI can solve that by putting a io.TeeReader into the uploader, and write to an MD5, to verify that the download MD5 was correct by the end, but I could still do with the UploadPart calls to automatically validate the MD5s of the chunks. \nGiven an overall MD5 for the download, chunk validation on upload, and modifying the content to have the right ETag after the upload, I can be pretty certain that the copy happened correctly.. See above why that wouldn't solve my use-case.. It's a bit of a longer workaround, but it's disappointing not to fix the issue of the loss of generality with the API. \nio.Reader is far more general than io.ReadSeeker, and the api could attempt to upgrade a Reader to a ReadSeeker internally to see if it can take a more efficient path (this sort of thing is sometimes done in the standard library).\nSee the zero copy io in io.Copy (copyBuffer) where it tests for ReaderFrom and WriterTo.. ",
    "joonas-fi": "I was thinking of TeeReader as well for calculating the MD5. Unfortunately that wouldn't help because at least the S3 REST docs (http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html) explain that the structure of the PUT request is:\n```\nPUT /.... HTTP/1.1\nAuth header (includes signed MD5 header)\nMD5 header\nBody as binary\n```\nNow, if the structure looked like this, streaming + MD5 could be supported with TeeReader:\n```\nPUT /.... HTTP/1.1\nContent-Type: multipart/form-data\n... multipart boundary ...\nbody here, base64\n... multipart boundary ...\nsigned auth value\n... multipart boundary ...\nmd5\n... multipart boundary ...\n```\nSo essentially, as long as S3 PutObject on-the-wire request requires the MD5 before the actual content, we can't support streaming, because we have to buffer the content somewhere. Streaming upload would probably be achieved with multipart insofar that after the content we can add additional stuff, but of course that's not pretty and it would increase bandwidth consumption because the content would have to be base64 encoded.\nSo, that's really a shame. The only solution for supporting streaming would require opting out of the MD5 mechanism (as a workaround s3gof3r stores additional file for just the MD5, goamz probably does not use MD5 at all). That's not cool either. Oh well.. :/. Hm, does S3 support trailing headers for chunked uploads? http://stackoverflow.com/a/11313254\nOkay, it appears it does not: https://forums.aws.amazon.com/message.jspa?messageID=561616\nThis chunked upload + trailing headers idea seems to have been discussed before here as well: #81. ",
    "ejcx": "Sure. I will post later tonight showing that type is nil.\n. Here's the code I have. I have only one grantee, and it is technically the user I am logged in as. It is always nil.\n```\n    acl, _ := svc.GetBucketAcl(&s3.GetBucketAclInput{Bucket: b.Name})\nfor _, grant := range acl.Grants {\n  fmt.Println(grant.Grantee.Type)\n}\n\n```\n. WOW! I just tried it and that worked. Ridiculous.\nAre there any docs that show this?\n. ",
    "xtudouh": "Hi @xibz , about this problem I traced it, and the correct request URL should be \"http://s3-us-west-2.amazonaws.com/mybucket//a/b/c.log\", look at the '/' between '.../mybucket/' and '/a/...', actually we send a request URL\"http://s3-us-west-2.amazonaws.com/mybucket/a/b/c.log\" as well, and I found this action be done in aws-sdk-go/private/protocol/rest/build.go, line 202, urlPath = path.Clean(urlPath), after this line the request URL changed and result in the 404 status code. But I don't think this is a bug, it just does what it should do, maybe other aws-sdks for other language should do a fix, as in the aws console we can not create the unnamed directory like which created in this case, this is a bug of aws-sdk-java, which might fix this bug, then all will be well.\n. @xibz - Yes, it is a valid request, but also strange, not common, we should not type a strange format URL, unless we make a mistake carelessly while typing. At the same time we can't do this in the console,     we shouldn't have the ability to do it by the SDK as usual, the AWS must provide a united user interface.  Normally http://s3-us-west-2.amazonaws.com/mybucket//a/b/c.log is same as  http://s3-us-west-2.amazonaws.com/mybucket/a/b/c.log while we exploring a website by the browser, the AWS should not be a special case. That's my opinion, thank you!\n. @xibz - OK, it will be a good solution, although it is not perfact\n. ",
    "aarthi184": "Will it be available in the SDK anytime soon?\n. ",
    "rajukumar4324": "Hi, Please let me know, if transaction support is added to dynamoDB for golang sdk. Thanks!!. ",
    "GeorgeHosuAdswizz": "Tank you very much for the help, I was very busy at the time and forgot to close the issue, sorry for that :s \n-Cheers. ",
    "iain17": "Whoops little mistake. It should've been dynamodbav\n. done.. moved it to the root README.md under the credentials header.. agreed. I only added it there for people to have a quick example reference of the tags.. ",
    "bobbydeveaux": "FWIW - the boto library in Python allows this:\ns3 = boto.connect_s3(\n  aws_access_key_id = \"%s/%s\" % (os.environ['MY_S3_ID'], os.environ['MY_S3_ID2']),\n  aws_secret_access_key = os.environ['MY_S3_SECRET'],\n  host = os.environ['MY_S3_HOST'],\n  port = 8443,\n  calling_format = boto.s3.connection.OrdinaryCallingFormat(),\n)\nIt's the 'host' parameter that I'm looking to pass using the S3 element of aws-sdk-go.\nCheers\n. Genius. Much appreciated, thanks :)\n. ",
    "charz": "Hi @jasdel,\nThanks for explaining that! I'm a newbie in Go, how can I switch to the common form of Request-URI (disable absolute URI)? just leave URL.Opaque as empty?\n. Thx, it works. . BTW, use newer version sdk will fix the problem (> 1.6.12).. Please abandon this pull request, it seems like the current version is working on my test env.\n. ",
    "tbarbugli": "I just extracted the relevant parts; the handle function is called in a loop and passed the body\ngo\nfunc handle(payload []byte) {\n    aws.String(\"us-west-1\")\n    bucketName := \"...\"\n    config := &aws.Config{\n        Region          :aws.String(\"us-east-1\"),\n    }\n    keyName := \"...\"\n    sess, _ := session.NewSession(config)\n    payloadBuf := bytes.NewBuffer(payload)\n    upParams := &s3manager.UploadInput{\n        Bucket: &bucketName,\n        Body:   payloadBuf,\n        Key:    &keyName,\n    }\n    uploader := s3manager.NewUploader(sess)\n    uploader.Upload(upParams)\n}\nI also tried a different approach and reused the Uploader but I could not see a clear improvement.\n(The payload is always just ~100 bytes)\n. thanks!\n. ",
    "ThomasAlxDmy": "hi @xibz \nYes, I have explored this option, but it looks like it only support one file (unique key per presign URL).  \nhere is a rough draft at what my code looks like ATM which is working but is not using all the nice utilities that the AWS/S3 package gives: https://gist.github.com/ThomasAlxDmy/d60372970884ae797fdaed227a2dafee \nStill defining the requirements for all the policy I need but basically the idea is that I want to generate a policy token per user that will allow to upload any file up to 50MB to a user path for 10 minutes (that's why I'm using start-with option).   . That's right, I am passing the AWS ACCESS KEY, but without the secret key it's useless so it's not a big deal :). \nTo use PutObject I need to have AWS ACCESS KEY, AWS SECRET KEY, bucket and key defined.\nBy creating a policy I only need AWS ACCESS KEY and bucket, and then my client can upload whatever key they are allowed to. . Humm potentially, Do you have some code that I can look at? \nAlso is there any limitation of number of temporary users? I would need 200k clients valid  at the same time . Also looking more into it: looks like using AssumeRole will force me to create a temporary account for all my clients (1 outbound call to STS/client) per hour, rather than creating a policy server side and handing off to my clients. Seems feasible but doesn't look extremely scalable.  . ",
    "sangameshb15": "Thanks, it worked after changing to \nsvc := s3.New(session.New(&aws.Config{\n        Region:   aws.String(awsRegion),\n        Credentials:creds,\n    }))\n. ",
    "xring": "Hi @xibz , thanks for help. Still little confused:\nI construct client by codes:\ngo\nclient := s3.New(session.New((&aws.Config{\n    Endpoint:         aws.String(endpoint),\n    Region:           aws.String(region),\n    S3ForcePathStyle: aws.Bool(true)}).\nWithLogLevel(aws.LogDebugWithRequestErrors | aws.LogDebugWithHTTPBody | aws.LogDebugWithSigning),\n))\nwhen I fill in an invalid region like abc, and use following codes:\ngo\nresult, err := client.CreateBucket(&s3.CreateBucketInput{\n    Bucket: aws.String(\"NEW_BUCKET_NAME\"),\n})\nto create a new bucket with name test, the response told me BucketAlreadyExists. This make sense because test bucket already exists. Seems it use the endpoint to find whether test bucket exists. And region is useless here.\nBut if I change bucket name from test to test123 or some name does not exist and still with region abc, it told me InvalidLocationConstraint. This also make sense because the region name is not valid.\nSo, what confused me is:\nSeems the SDK try to found whether a bucket exist using endpoint, but try to create a new bucket using region.\nCheck if bucket name exists should happen before create a new bucket, since it can use endpoint to look up whether bucket already exists, why it still rely on a region to create the bucket and not use the endpoint?. @xibz Thank you so much! I understand it now.\nAnd as a result, I change the code to:\ngo\nresult, err := client.CreateBucket(&s3.CreateBucketInput{\n    CreateBucketConfiguration: &s3.CreateBucketConfiguration{LocationConstraint: aws.String(\"\")},\n    Bucket: aws.String(\"NEW_BUCKET_NAME\"),\n})\njust change the SOME_ENDPOINT to an empty string make my code work. Thanks again!. All confusion are cleared and found a way to make my codes work. . ",
    "seanchann": "@xibz  thanks. ```go\nvar spec = \"spec\"\nvar userservice = \"userService\"\nvar nodes = \"nodes\"\ntype testUser struct {\n    Nodes map[string]interface{} json:\"nodes\"\n}\ntype testSpec struct {\n    User testUser json:\"userService\"\n}\ntype test struct {\n    Spec testSpec json:\"spec\"\n}\nfunc scanNotContains(db *dynamodb.DynamoDB) {\nfilter := \"not_contain(#spec.#userService.#nodes, :node1)\"\n\nexpressionAttrName := map[string]*string{\n    \"#spec\":        &spec,\n    \"#userservice\": &userservice,\n    \"#nodes\":       &nodes,\n}\nnodeVal := \"test\"\nexpressionAttrValue := map[string]*dynamodb.AttributeValue{\n    \":node1\": &dynamodb.AttributeValue{\n        S: &nodeVal,\n    },\n}\n\nscanParam := &dynamodb.ScanInput{\n    TableName:                 aws.String(table),\n    FilterExpression:          aws.String(filter),\n    ExpressionAttributeNames:  expressionAttrName,\n    ExpressionAttributeValues: expressionAttrValue,\n}\n\noutput, err := db.Scan(scanParam)\nif err != nil {\n    glog.Errorf(\" scan list  error %v\", err.Error())\n}\n\nglog.V(9).Infof(\"Get query output %+v\\r\\n\", *output.Count)\n\n}\nfunc main() {\n    sess, err := newDynamodbSession(\"\", \"\", \"\", \"us-west-2\")\n    if err != nil {\n        glog.Errorf(\"dynamodb create seesion error:%v\\r\\n\", err)\n    }\n    db := dynamodb.New(sess)\nscanNotContains(db)\n\n}\n. above program not work. error:go\n ValidationException: Invalid FilterExpression: Invalid function name; function: not_contain\n        status code: 400, request id: J3ITC142JMGSGDLBGNPPBRMAIFVV4KQNSO5AEMVJF66Q9ASUAAJG\n```\nnot_contain function not support? . ```\nvar spec = \"spec\"\nvar userservice = \"userService\"\nvar nodes = \"nodes\"\ntype testUser struct {\n    Nodes map[string]interface{} json:\"nodes\"\n}\ntype testSpec struct {\n    User testUser json:\"userService\"\n}\ntype test struct {\n    Spec testSpec json:\"spec\"\n}\nfunc scanContains(db *dynamodb.DynamoDB) {\nfilter := \"contains(#spec.#userService.#nodes,:1ae396ef1000)\"\n\nexpressionAttrName := map[string]*string{\n    \"#spec\":        &spec,\n    \"#userService\": &userservice,\n    \"#nodes\":       &nodes,\n}\nnodeVal := \"1ae396ef1000\"\nexpressionAttrValue := map[string]*dynamodb.AttributeValue{\n    \":1ae396ef1000\": &dynamodb.AttributeValue{\n        S: &nodeVal,\n    },\n}\n\nscanParam := &dynamodb.ScanInput{\n    TableName:                 aws.String(table),\n    FilterExpression:          aws.String(filter),\n    ExpressionAttributeNames:  expressionAttrName,\n    ExpressionAttributeValues: expressionAttrValue,\n}\n\noutput, err := db.Scan(scanParam)\nif err != nil {\n    glog.Errorf(\" scan list  error %v\", err.Error())\n    return\n}\n\nglog.Infof(\"Get query output %+v\\r\\n\", *output.Count)\n\n}\n``. above code ,  the scan filter the1ae396ef1000key in nodes map. this key exist in my table, but scan result is nil. @xibz sorry, I needattribute_existsnotcontains. . I useScanPagesfunc, already provided  filter expression in theScanInput, but scan the returned result set may be more, if the client needs a page, every page needs 6 entries, to 50th, thenFunc (p *ScanOutput, lastPage bool) (shouldContinue bool) function to skip 49 times, To get the 50th page of data, this is an unfair practice for this scenarios\nTherefore, provide a field for the result set in aScanInput` struct skips the specified entry. if client need a page,  use skip field will be work fine.. ",
    "olivoil": "@qhenkart having the same problem here. What policy change did you have to make for this to work?. ",
    "heynemann": "@qhenkart and @olivoil What you guys did? I'm going crazy!!!. ",
    "a-h": "@olivoil and @heynemann - I had the same problem, in my case when I attempted to use the presigned URL, I hadn't included the Content-MD5 HTTP header see https://github.com/aws/aws-sdk-go/issues/1808. ",
    "malekascha": "So, I was able to retrieve the credentials, but now I'm running into an issue with the standalone request signer. I'm getting a SignatureDoesNotMatch error for some reason. I verified that my access keys are correct, so I'm really unsure what's going on. Here is my code: \n```go\n  elasticsearchEP = endpoint\n  sess, err := session.NewSession()\n  if(err != nil){\n    return err\n  }\n  newSigner := v4.NewSigner(sess.Config.Credentials)\n  signer = newSigner\n  target := fmt.Sprintf(\"%s/goalbook/accounts/_search\", elasticsearchEP)\nreq, err := http.NewRequest(\"POST\", target, nil)\n  if(err != nil){\n    return []byte{}, err\n  }\npostBody, err := generateUserQuery(name, email)\n  if(err != nil){\n    return []byte{}, err\n  }\nsigner.Sign(req, bytes.NewReader(postBody), \"es\", \"us-west-2\", time.Now())\n  client := &http.Client{}\nresp, err := client.Do(req)\n  if(err != nil){\n    return []byte{}, err\n  }\n  defer resp.Body.Close()\nbody, err := ioutil.ReadAll(resp.Body)\n  if(err != nil){\n    return []byte{}, err\n  }\n  fmt.Println(fmt.Sprintf(\"%+v\", string(body)))\n  return body, nil\n```. Thanks for the response! It turns out that the issue was that I needed to pass the body into the call to http.NewRequest as well as the call to Signer.Sign. It works just fine now. Thanks!. This fixed it, thanks very much!. ",
    "tjchu": "So I was trying to implement the functionality of the put-object command with the upload function: aws s3api put-object --bucket md5testbucket --key fileupload.txt --body localfile.txt --metadata md5chksum=WZOTosUmxoARnYQVXZDx5Q== --content-md5 WZOTosUmxoARnYQVXZDx5Q==\nI'm assuming it's not possible to do something like with the APIs in upload.go? Basically, I was trying to perform a checksum verification when uploading a file to S3 with the upload APIs.. Thanks! I wanted to know if it's possible for me to use the Uploader APIs and carry out the Content-MD5 header setting like how you do it in PutObject. Now I know.. ",
    "tejasmanohar": "Oh I found err.OriginalError(). Ah, great. Thanks @jasdel . ",
    "bacoboy": "Feedback submitted with link to this issue.\nAsked them to add some go language usage examples to the documentation.\nThanks!. Yea, I think I was looking for higher level wrapper code, not the raw API calls (similar to KMS Encrypt or GenerateDataKey functions).  \nBut given the use of a local relay daemon sitting between apps and the actual AWS API you've implemented, it isn't clear if the go SDK should have this glue code for instrumenting apps or if it should be a different code base.... ",
    "gsg822": "To give a little more insight into what I noticed, is that it will update ACL with no errors if I set up PutObjectAclInput like\n```\nobjectAclInput := &s3.PutObjectAclInput{\n    ACL:    aws.String(s3.ObjectCannedACLPublicRead),\n    Bucket: aws.String(bucket.Bucket),\n    Key:    aws.String(key),\n}\n_, err = s3Client.PutObjectAcl(objectAclInput)\n```\nWhat I am trying to achieve is to add a public-read permission to an object in a bucket. The above works, but when I call GetObjectAcl on the updated object it returns a nil display name of nil. I'm wanting to ensure that Everyone has access to this object which is why I was trying it the way I was in the original comment. \nI shouldn't see an issue if I use the REST endpoint to update an object's permission right?. Looks good to me. Thanks for your help.. ",
    "bradfitz": "Where is the code in question in aws-sdk-go? I'd like to see exactly what it's doing to inform whether/how to make a fix on Go's side.\n. ",
    "pin": "We have run a bunch of S3-related tests on https://github.com/aws/aws-sdk-go/pull/991/commits/0a88972c3c60fbda1325db2009e0535cfa58baf9 with go1.8beta1 and it seems that it works correctly.\nIn current version we see issues you mentioned in https://github.com/aws/aws-sdk-go/issues/984#issuecomment-265854038: \"0\" symbol in the body where body is supposed to be empty (e.g. HEAD request) appears because net/http applies chunked encoding on []byte(\"\"): zero is just a length of the first chunk.. ",
    "harshitdx29": "```go\n    bucket := \"stg-ovo-bill-route\"\n    item := \"test.txt\"\n    file, err := os.Create(item)\n    if err != nil {\n        exitErrorf(\"Unable to open file %q, %v\", err)\n    }\ndefer file.Close()\n\n// Initialize a session in us-west-2 that the SDK will use to load\n// credentials from the shared credentials file ~/.aws/credentials.\nsess, _ := session.NewSession(&aws.Config{\n    Region: aws.String(endpoints.ApSoutheast1RegionID)},\n)\n\n// Create S3 service client\nsvc := s3.New(sess)\n\nresult, err := svc.ListBuckets(nil)\nif err != nil {\n    exitErrorf(\"Unable to list buckets, %v\", err)\n}\n\n```\nThis is my code. It is giving \"NoCredentialProviders: no valid providers in chain. Deprecated.\" I have ~/.aws/config as well as ~/.aws/credentials file.. ",
    "hajhatten": "If maps are used for this already, I have no issue keeping it that way.\nIf the order of regions is important, I would say that would be on the individual dev to sort it.\nAdding a Regions method to Service would be great.. ",
    "gfa": "wrong project, sorry. ",
    "aultokped": "Thanks @xibz for the response,\nOkay, I've turn on the logging and this is the result.\n```\n2016/12/15 09:22:49 DEBUG: Request Signature:\n---[ CANONICAL STRING  ]-----------------------------\nPOST\n/\naccept-encoding:identity\ncontent-length:188\ncontent-type:application/x-amz-json-1.0\nhost:localhost:4567\nx-amz-date:20161215T022249Z\nx-amz-target:DynamoDB_20120810.BatchWriteItem\naccept-encoding;content-length;content-type;host;x-amz-date;x-amz-target\nb10c829eddfd3abe2ca9e100539e3eac76368c21f45b50caad20e243cc7a7b4e\n---[ STRING TO SIGN ]--------------------------------\nAWS4-HMAC-SHA256\n20161215T022249Z\n20161215/us-west-2/dynamodb/aws4_request\n57e275ce07919f7c0a644e9d19f719a4d1f0dce7fdcccde0434984433546122e\n-----------------------------------------------------\n2016/12/15 09:22:49 DEBUG: Request dynamodb/BatchWriteItem Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nPOST / HTTP/1.1\nHost: localhost:4567\nUser-Agent: aws-sdk-go/1.6.2 (go1.6.4; linux; amd64)\nContent-Length: 188\nAccept-Encoding: identity\nAuthorization: AWS4-HMAC-SHA256 Credential={KEY_ID}/20161215/us-west-2/dynamodb/aws4_request, SignedHeaders=accept-encoding;content-length;content-type;host;x-amz-date;x-amz-target, Signature=a75699a7a618b31c885558c48eeb15a0de7767106d603b3329747c1\nContent-Type: application/x-amz-json-1.0\nX-Amz-Date: 20161215T022249Z\nX-Amz-Target: DynamoDB_20120810.BatchWriteItem\n{\"RequestItems\":{\"product_video\":[{\"DeleteRequest\":{\"Key\":{\"product_id\":{\"N\":\"29684991\"}}}},{\"DeleteRequest\":{\"Key\":{\"product_id\":{\"N\":\"4564564\"}}}}]},\"ReturnItemCollectionMetrics\":\"SIZE\"}\n2016/12/15 09:22:49 DEBUG: Response dynamodb/BatchWriteItem Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 200 OK\nContent-Length: 23\nConnection: keep-alive\nContent-Type: application/x-amz-json-1.0\nDate: Thu, 15 Dec 2016 02:22:49 GMT\nX-Amz-Crc32: 4185382651\nX-Amzn-Requestid: SXL6AK0Q8KFDE2LEO94IULUDSH6LI7FK0792X5OF1BMENENORYDB\n{\"UnprocessedItems\":{}}\n```\nhopefully you can find the solution from that...\nThanks.. @xibz thank you for the response, \nI don't think so. my table has no index,\nSo, if i do BatchWriteItem, I can't get the affected item from the request?. ",
    "alexbrand": "Now that I look more closely, it might actually be the GetChange call throwing this error. If you'd like more detail, you can look at our usage of the Route53 API here: https://github.com/apprenda/kismatic/blob/master/integration/aws/client.go#L299\nUsing go version go1.7.1 darwin/amd64. As far as I can tell, the user does have write access to the HostedZoneId.. ",
    "gotoxu": "The process of the go app only have these Environment Variables:\njavascript\nSUPERVISOR_GROUP_NAME=demo\nTERM=linux\nSUPERVISOR_SERVER_URL=unix:///var/run/supervisor.sock\nSUPERVISOR_PROCESS_NAME=demo\nRUNLEVEL=2\nUPSTART_EVENTS=runlevel\nPREVLEVEL=N\nPATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin\nUPSTART_INSTANCE=\nUPSTART_JOB=rc\nSUPERVISOR_ENABLED=1\nrunlevel=2\nPWD=/\nLS_HEAP_SIZE=5000m\nprevious=N\nI don't know why though, there is no HOME env?. The value of the command echo \"HOME=$HOME\" is:\nHOME=/home/ubuntu\nAnd yes, I'm using an EBS EC2 instance, the AMI ID is: ubuntu/images/hvm-ssd/ubuntu-trusty-14.04-amd64-server-20150325 (ami-0220b23b). In shell command, the $HOME environment variable is right, but when I host my go app by supervisor, it can't find the env. why?. thanks for your reply @jasdel , I also think this extra logic is not necessary. ",
    "MatTarantini": "Hey @xibz, thanks for the quick response.\nI have been unable to find a good way to lock the aws-sdk-go package to version 1.6.2 during the download on deployment. I believe Go has gained package version control in release 1.5 and later.\nI have attempted to use gopkg.in, but it is limited to major releases (0, 1, etc.) and so I'm unable to target 1.6.2.\nI hadn't considered setting the docker file to install Go 1.5+, but I wonder if I may be better off switching to a different server configuration since AWS now has Go without being Preconfigured Docker. Go without Docker is version 1.5, while the Docker configuration is limited to 1.4. This will be more of a permanent solution.\nI believe the best temporary solution will be to fork aws-dk-go v1.6.2 and import that package. After that I'll work on switching to a pure Go server without Docker. To me it seems odd that the aws-sdk-go package would drop support for Go 1.4 when there are AWS server configurations that are limited to Go 1.4. I suppose the best solution would be AWS providing an upgrade for Preconfigured Docker to go up to Go 1.5, but that may be a different request/issue.\nThanks again @xibz, let me know if you can think of any other solutions to the version issue that I haven't thought of.. ",
    "alasdairnicol": "It looks like amazon-ecr-credential-helper is using version 1.6.5 of the SDK. I'll try to come up with a simpler example to display the problem.. > You aren't removing your keys from the config file as well, correct?\nI have removed keys from the config file and the credentials file. I am using instance profiles, so I shouldn't need keys, just the name of the role that I want to assume. However, I don't know whether the SDK supports this.. @xibz I've come across that limitation in the Python API before, I wasn't sure whether it would be the same for the GO SDK. Thanks for your quick response.. ",
    "abferm": "I've run into this problem when trying to use ecs-cli across accounts. boto recently added this functionality in https://github.com/boto/botocore/pull/1313. It would be nice if that behavior was mirrored here.. ",
    "oshaughnessy": "+1 for the comment about boto/botocore#1313. This would enable Packer to use an instance profile as the source for session credentials when assuming a role to build images, where the process takes longer than an hour. Without this, I believe Packer is limited to doing all that only when it can read session credentials directly out of .aws/credentials.. ",
    "migrashgrutot": "+1. We have multiple accounts and would be nice not to have separate instances in each account just to run Go programs.. ",
    "eriksw": "Note: This actually works now with the recent addition of credential_source support. If you enable shared config when creating the session, call the session constructor with a nil Config.Credentials, and are using a profile like this, it'll work:\n[profile backups]\noutput = json\nregion = us-east-1\ncredential_source = Ec2InstanceMetadata\nrole_arn = arn:aws:iam::....\n\n. If this (and other things) are added as opt-in, it'd be good if there was a documentation update effort done to centralize and clearly communicate \"here's all the things you need to do to make aws-sdk-go behave the way everyone expects (from awscli)\".. ",
    "smugcloud": "Thanks @xibz.  I mistakenly left off the -u when trying to upgrade before I posted this :man_facepalming: \nAfter properly upgrading, it is working again.  . ",
    "rabbbit": "@xibz do you remember when was it fixed, what was the fix? Was it related to https://github.com/aws/aws-sdk-go/pull/874?\nI'm curious about the root cause, but we're also seeing this issue in [an ancient] version of Terraform (when talking to aws lambda)- it would be good to know how much we have to upgrade to get it fixed.. We are running the issue since ~2 weeks ago, on our calls to lambda - we\ndefinitely cannot create/read functions or triggers.\nNote that we're using Terraform 0.7, and that includes aws-sdk-go 1.14.15\nOn 30 November 2017 at 18:44, xibz notifications@github.com wrote:\n\n@rabbbit https://github.com/rabbbit - Are you still running into the\nissue? From what I remember this was on the service's side and was not\nrelated to #874 https://github.com/aws/aws-sdk-go/pull/874.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-go/issues/1022#issuecomment-348265177,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AARt0qoZT1A78F21KTVpwhm1GG2mIJh-ks5s7umTgaJpZM4LUf4K\n.\n. Woooahh, http2? I did see HTTP 1.1 in our logs, didn't expect 2 to make a difference.\n\nSo, without the debugging enabled, we were seeing a lot of:\n2017/11/27 10:01:53 [DEBUG] plugin: terraform: aws-provider (internal) 2017/11/27 10:01:53 [DEBUG] [aws-sdk-go] Request body type has been overwritten. May cause race conditions\nand\n2017/11/27 10:00:12 [DEBUG] plugin: terraform: aws-provider (internal) 2017/11/27 10:00:12 [DEBUG] [aws-sdk-go] DEBUG: Response lambda/GetFunction Details:\n2017/11/27 10:00:12 [DEBUG] plugin: terraform: ---[ RESPONSE ]--------------------------------------\n2017/11/27 10:00:12 [DEBUG] plugin: terraform: HTTP/0.0 0 status code 0\n2017/11/27 10:00:12 [DEBUG] plugin: terraform: Content-Length: 0\nWith debugging enabled, it's:\n```\n2017/11/30 19:22:16 [DEBUG] plugin: terraform: aws-provider (internal) 2017/11/30 19:22:16 http2: Transport failed to get client conn for iam.amazonaws.com:443: http2: no cached connection was available                                                                                                                                                  \n2017/11/30 19:22:16 [DEBUG] plugin: terraform: aws-provider (internal) 2017/11/30 19:22:16 http2: Transport creating client conn to 54.67.50.113:443                         \n2017/11/30 19:22:16 [DEBUG] plugin: terraform: aws-provider (internal) 2017/11/30 19:22:16 http2: Framer 0xc82015cdc0: wrote SETTINGS len=18, settings: ENABLE_PUSH=0, INITIAL_WINDOW_SIZE=4194304, MAX_HEADER_LIST_SIZE=10485760                                                                                                                             2017/11/30 19:22:16 [DEBUG] plugin: terraform: aws-provider (internal) 2017/11/30 19:22:16 http2: Framer 0xc82015cdc0: wrote WINDOW_UPDATE len=4 (conn) incr=1073741824      \n2017/11/30 19:22:16 [DEBUG] plugin: terraform: aws-provider (internal) 2017/11/30 19:22:16 http2: Framer 0xc82015cdc0: read SETTINGS len=18, settings: MAX_CONCURRENT_STREAMS=1\n28, INITIAL_WINDOW_SIZE=65536, MAX_FRAME_SIZE=16777215                                                                                                                       \n2017/11/30 19:22:16 [DEBUG] plugin: terraform: aws-provider (internal) 2017/11/30 19:22:16 http2: Framer 0xc82015cdc0: wrote SETTINGS flags=ACK len=0 \n2017/11/30 19:22:16 [DEBUG] plugin: terraform: aws-provider (internal) 2017/11/30 19:22:16 http2: Transport encoding header \":authority\" = \"lambda.us-west-1.amazonaws.com\"  \n2017/11/30 19:22:16 [DEBUG] plugin: terraform: aws-provider (internal) 2017/11/30 19:22:16 http2: Transport encoding header \":method\" = \"GET\"                                \n2017/11/30 19:22:16 [DEBUG] plugin: terraform: aws-provider (internal) 2017/11/30 19:22:16 http2: Transport encoding header \":path\" = \"https://lambda.us-west-1.amazonaws.com/2015-03-31/functions/test\"                                                                                                         \n2017/11/30 19:22:16 [DEBUG] plugin: terraform: aws-provider (internal) 2017/11/30 19:22:16 http2: Transport encoding header \":scheme\" = \"https\"                              \n2017/11/30 19:22:16 [DEBUG] plugin: terraform: aws-provider (internal) 2017/11/30 19:22:16 http2: Transport encoding header \"user-agent\" = \"terraform/0.7.0 aws-sdk-go/1.2.7 (go1.6.3; linux; amd64)\" \n2017/11/30 19:22:16 [DEBUG] plugin: terraform: aws-provider (internal) 2017/11/30 19:22:16 http2: Framer 0xc82015cdc0: read WINDOW_UPDATE len=4 (conn) incr=2147418112       \n2017/11/30 19:22:16 [DEBUG] plugin: terraform: aws-provider (internal) 2017/11/30 19:22:16 http2: Transport received WINDOW_UPDATE len=4 (conn) incr=2147418112              \n2017/11/30 19:22:17 [DEBUG] plugin: terraform: aws-provider (internal) 2017/11/30 19:22:17 http2: Framer 0xc82015cdc0: read SETTINGS flags=ACK len=0\n2017/11/30 19:22:17 [DEBUG] plugin: terraform: aws-provider (internal) 2017/11/30 19:22:17 http2: Transport received SETTINGS flags=ACK len=0\n2017/11/30 19:22:17 [DEBUG] plugin: terraform: aws-provider (internal) 2017/11/30 19:22:17 http2: Framer 0xc82015cdc0: read RST_STREAM stream=1 len=4 ErrCode=PROTOCOL_ERROR\n2017/11/30 19:22:17 [DEBUG] plugin: terraform: aws-provider (internal) 2017/11/30 19:22:17 http2: Transport received RST_STREAM stream=1 len=4 ErrCode=PROTOCOL_ERROR\n2017/11/30 19:22:17 [DEBUG] plugin: terraform: aws-provider (internal) 2017/11/30 19:22:17 RoundTrip failure: stream error: stream ID 1; PROTOCOL_ERROR\n2017/11/30 19:22:17 [DEBUG] plugin: terraform: aws-provider (internal) 2017/11/30 19:22:17 [DEBUG] [aws-sdk-go] DEBUG: Response lambda/GetFunction Details:\n2017/11/30 19:22:17 [DEBUG] plugin: terraform: ---[ RESPONSE ]--------------------------------------\n2017/11/30 19:22:17 [DEBUG] plugin: terraform: HTTP/0.0 0 status code 0\n2017/11/30 19:22:17 [DEBUG] plugin: terraform: Content-Length: 0\n2017/11/30 19:22:17 [DEBUG] plugin: terraform:\n2017/11/30 19:22:17 [DEBUG] plugin: terraform:\n```. Sure, although it might take a while before we get to upgrade terraform,\nit's not trivial.\nWhat did you mean by the original comment in this task though? You were\nsuggesting an issue that was fixed on your end, right?\nOn 30 Nov 2017 21:37, \"xibz\" notifications@github.com wrote:\n\n@rabbbit https://github.com/rabbbit - Looks like the RST_STREAM is\nbeing set which is canceling that stream. I am curious if this would be\nresolved by updating Golang. I know the language made several changes to\nthe HTTP2 library since then. Please let us know if this resolves the issue.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-go/issues/1022#issuecomment-348313449,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AARt0s_7toSLePOL7icZ08xUn_A_lzqQks5s7xIBgaJpZM4LUf4K\n.\n. \n",
    "mmacdermaid": "Is there a better way to investigate this?  This is not a 100% occurrence.  It seems to happen after server uptime hits around 3-4 days.  With the amount of data running through a single box those log flags produce over 50MB/s of logs.  It also looks like they contain auth/security keys which I would prefer not posting here into a github ticket.. I will see what I can do to try and get you the data, but it might take a few days given that the panic seems sporadic.  I wont be able to provide the post body since I also don't believe I can share this with you based on our company data restrictions.  I can tell you it's LZOP archive about ~25mb in size though.. >That panic is a result of a field not being populated.\nuploader        *s3manager.Uploader\nbody              io.Reader\n````\n    ui := &s3manager.UploadInput{\n        Bucket: &bucketName,\n        Key:    &key,\n        Body:   body,\n    }\n_, err := u.uploader.Upload(ui)\n\n````\nYou mean like Body: missing?\nI have no reason to believe body is empty, nil or len() == 0.  We have alerts and logs surrounding the callers of this section to make sure that doesn't happen, and if it does we know about it (I have no logs of this kind during the panic)\nThe body is non-specific zipped data, I wouldn't be able to confirm it's \"correct\" other than it being valid json, and that wouldn't be an easy task given the log size.\n. Is there a way I could log only the body of the return response, and not the outgoing body (LogDebugWithHTTPBody looked like it was logging both?). I will try when I get a chance.\nTo give you an update @xibz.  I wont get to this for a little bit.  Since the panic exists and is affecting current customers (and future deploys) I need to write some code to use the standard POST to HTTP aws, and stop using the golang uploader.\nAfter I get that code deployed I will stick a test version of the SDK back on a single box and help you debug this panic.\nThanks for your help.  Talk to you soon.. ",
    "disq": "We also encountered this today. Latest AWS SDK (1.7.6), Go 1.7, linux/amd64.\n```\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x4a8ba7]\ngoroutine 203 [running]:\npanic(0x83b700, 0xc4200100a0)\n    /usr/local/go/src/runtime/panic.go:500 +0x1a1\ngithub.com/peakgames/s5cmd/vendor/github.com/aws/aws-sdk-go/service/s3/s3manager.(multiuploader).upload(0xc4206158c0, 0xafa060, 0xc4205df470, 0x500000, 0x0, 0x0)\n    /home/user/workspace/src/github.com/peakgames/s5cmd/vendor/github.com/aws/aws-sdk-go/service/s3/s3manager/upload.go:514 +0x1d7\ngithub.com/peakgames/s5cmd/vendor/github.com/aws/aws-sdk-go/service/s3/s3manager.(uploader).upload(0xc4202b74a0, 0xc4202b74a0, 0xc420336000, 0x0)\n    /home/user/workspace/src/github.com/peakgames/s5cmd/vendor/github.com/aws/aws-sdk-go/service/s3/s3manager/upload.go:368 +0x528\ngithub.com/peakgames/s5cmd/vendor/github.com/aws/aws-sdk-go/service/s3/s3manager.Uploader.Upload(0x500000, 0x5, 0x0, 0x2710, 0xb09d80, 0xc4203ea028, 0xc42019a1a0, 0xc420c3ff50, 0x1, 0x1, ...)\n    /home/user/workspace/src/github.com/peakgames/s5cmd/vendor/github.com/aws/aws-sdk-go/service/s3/s3manager/upload.go:336 +0x129\ngithub.com/peakgames/s5cmd/core.(Job).Run.func11(0xc4202205a0, 0xc42038a000, 0xc4207740c0, 0xc420615860, 0x500000)\n    /home/user/workspace/src/github.com/peakgames/s5cmd/core/job.go:707 +0x2ce\ncreated by github.com/peakgames/s5cmd/core.(Job).Run\n    /home/user/workspace/src/github.com/peakgames/s5cmd/core/job.go:711\n```\nThis is how it was called:\nhttps://github.com/peakgames/s5cmd/blob/40b0fbe6115b5cfce73682870b0570e69784d473/core/job.go#L700-L707. It's not consistent / we were not able to reproduce it. I'll try enabling logging for now.. ",
    "donileo": "Hi @xibz,\nI believe you are correct, I checked my code and I do believe just as you say that my client was nil, hence my update above. But what about passing the output arg in the c.NewRequest() call? \nIf I get this issue shows up again I will let you guys know, Thanks.. Correct, feel free to close this issue, I will let you guys know if the problem occurs again. My other comments were mostly cleanliness. I do argue though that passing in output as an arg to c.NewRequest() makes it harder to read and almost implies that it shouldn't be nil. Otherwise nil should just be passed in. I would rewrite those lines of code as: \noutput = &ReceiveMessageOutput{}\nreq = c.newRequest(op, input, output)\nreturn. Great! \ud83d\udc4d . ",
    "AA33": "Here are the logs I got:\n```\n2016/12/27 15:54:53 DEBUG: Request Signature:\n---[ CANONICAL STRING  ]-----------------------------\nPOST\n/\ncontent-length:208\ncontent-type:application/x-www-form-urlencoded; charset=utf-8\nhost:sqs.us-west-2.amazonaws.com\nx-amz-date:20161227T235453Z\ncontent-length;content-type;host;x-amz-date\nb361be33a5c0d4c4cbcda8dcac42756ef631b6f77171f28f9113020db753ea18\n---[ STRING TO SIGN ]--------------------------------\nAWS4-HMAC-SHA256\n20161227T235453Z\n20161227/us-west-2/sqs/aws4_request\ned0c1c4d918908fcfb6540eca8562127be72afb9060acc95e54151efeceec94c\n-----------------------------------------------------\n2016/12/27 15:54:53 DEBUG: Request sqs/SendMessage Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nPOST / HTTP/1.1\nHost: sqs.us-west-2.amazonaws.com\nUser-Agent: aws-sdk-go/1.6.8 (go1.7.3; darwin; amd64)\nContent-Length: 208\nAuthorization: AWS4-HMAC-SHA256 Credential=####################/20161227/us-west-2/sqs/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date, Signature=################################################################\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\nX-Amz-Date: 20161227T235453Z\nAccept-Encoding: gzip\nAction=SendMessage&MessageBody=6d96ad1d-a064-4aee-a6ea-594d804c58f0&MessageGroupId=%01&QueueUrl=https%3A%2F%2Fsqs.us-west-2.amazonaws.com%2F077185562796%2Fdev-coupon-service-pool-usage.fifo&Version=2012-11-05\n2016/12/27 15:54:53 DEBUG: Response sqs/SendMessage Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 400 Bad Request\nContent-Length: 0\nConnection: keep-alive\nContent-Type: text/xml\nDate: Tue, 27 Dec 2016 23:54:53 GMT\nServer: Server\nX-Amzn-Requestid: 9a2693ca-582b-58e2-8acb-c31759a66503\n\n``. Thanks for your prompt response @xibz. Appreciate it.. I was able to fix the issue. I don't think the problem was with SQS. I was passing an empty string asMessageGroupId` and that causes the send to fail which is documented in the godocs. The error itself is a bit misleading though. . ",
    "shawnps": "@xibz no problem. You can find a bunch more here, although there are some false positives:\nhttps://goreportcard.com/report/github.com/aws/aws-sdk-go#misspell\nedit: I work on this tool just for full disclosure. @cristim I agree it's unfortunate to do separate commits/PRs but the reason I did that was b/c I'm using Go Report Card https://goreportcard.com/report/github.com/aws/aws-sdk-go which has direct links to the lines of code with the typos, then I use the GitHub web editor to make the change.. ",
    "morissette": "[mharris@mori aws-marketplace-service]$ curl -i -H \"Content-Type: application/json\" -d '{\"token\": \"test1234\"}' localhost:8000/v1/aws/1\nHTTP/1.1 500 Internal Server Error\nContent-Type: application/json; charset=utf-8\nX-Powered-By: go-json-rest\nDate: Tue, 10 Jan 2017 00:04:12 GMT\nContent-Length: 181\n{\n  \"Error\": \"InvalidSignatureException: Credential should be scoped to correct service: 'aws-marketplace'. \\n\\tstatus code: 400, request id: 517bcda2-d6c8-11e6-9a0a-4dc3bf507161\"\n}[. Some test code:\n``\ntype TokenPayload struct {\n    Token stringjson:\"token\"`\n}\nfunc resolveCustomer(token *TokenPayload) (string, string) {\n    envCreds := credentials.NewEnvCredentials()\n    config := &aws.Config{Region: aws.String(\"us-east-1\"), Credentials: envCreds}\n    var sess = session.Must(session.NewSession(config))\nsvc := marketplacemetering.New(sess)\nparams := &marketplacemetering.ResolveCustomerInput{\n    RegistrationToken: aws.String(token.Token),\n}\nresp, err := svc.ResolveCustomer(params)\nif err != nil {\n    return \"\", err.Error()\n}\n\nlog.Println(resp)\nreturn \"\", \"\"\n\n}\n```. And a Pull Request\nhttps://github.com/aws/aws-sdk-go/pull/1034. Note; I'm aware of the autogeneration, this has been autogenerated via the json :). ",
    "graham-ring": "Hi @xibz, added some unit tests. ",
    "palaiyacw": "I am not sure whether it is the issue with the key or not. Reason being when I try to fetch this image using http://bucketName.s3.amazonaws.com/n/bw/vs3poki_978.jpg url it works. Moreover, there are other images with the similar keyName and they doesn't create any issue. It just with few large images.\nThough, I am pasting the logs here.\n```\n Original Image request for : /n/bw/vs3poki_978.jpg\n2017/01/18 11:24:51 DEBUG: Request Signiture:\n---[ CANONICAL STRING  ]-----------------------------\nGET\n/n/bw/vs3poki_978.jpg\nhost:BucketName.s3-ap-southeast-1.amazonaws.com\nx-amz-date:20170118T055451Z\nhost;x-amz-date\ne3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n---[ STRING TO SIGN ]--------------------------------\nAWS4-HMAC-SHA256\n20170118T055451Z\n20170118/ap-southeast-1/s3/aws4_request\n24e70919bd52d4583ce5147b5cc392b6175f7d125907ba0a84d2d007b08b8b22\n-----------------------------------------------------\n2017/01/18 11:24:51 DEBUG: Request s3/GetObject Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nGET http://BucketName.s3-ap-southeast-1.amazonaws.com/n/bw/vs3poki_978.jpg HTTP/1.1\nHost: BucketName.s3-ap-southeast-1.amazonaws.com\nUser-Agent: aws-sdk-go/1.1.21 (go1.6.2; linux; amd64)\nAuthorization: AWS4-HMAC-SHA256 Credential=XXXXXXXXXXXXXXXXXX/20170118/ap-southeast-1/s3/aws4_request, SignedHeaders=host;x-amz-date, Signature=14497cd467e79756bc83cccd1905636c80666e98b0b6ceb529c9c96162102ecd\nX-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\nX-Amz-Date: 20170118T055451Z\nAccept-Encoding: gzip\n\n2017/01/18 11:24:52 DEBUG: Response s3/GetObject Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 404 Not Found\nTransfer-Encoding: chunked\nContent-Type: application/xml\nDate: Wed, 18 Jan 2017 05:54:49 GMT\nServer: AmazonS3\nX-Amz-Id-2: ZKtiSVNsYcDiYs8VakYCnXvwRYA3wIA5zibrGK0M3T8c2jjirAYjAnFHGBv6iLswTBywZy2Olas=\nX-Amz-Request-Id: 625708409D92C965\n123\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\nNoSuchKeyThe specified key does not exist.n/bw/vs3poki_978.jpg625708409D92C965ZKtiSVNsYcDiYs8VakYCnXvwRYA3wIA5zibrGK0M3T8c2jjirAYjAnFHGBv6iLswTBywZy2Olas=\n0\n\n2017/01/18 11:24:52 Inside serveHTTP Couldn't find the key : n/bw/vs3poki_978.jpgNoSuchKey: The specified key does not exist.\n    status code: 404, request id: 625708409D92C965\n``. No, Key doesn't have leading/. And I triedDisableRestProtocolURICleaning`. But didn't work. Did you get anything from the logs. @jasdel Will do that.  @xibz Let me know if you find anything. ",
    "jefferai": "Hi @xibz,\n@RichardKnop could hopefully provide more color around where he got those keys. As for the operations, if he can tell me what he was doing exactly, I can clue you in, but it's pretty basic IAM stuff -- just trying to use an existing credentials to create a new IAM credential.. Thanks @RichardKnop for getting back to this. @xibz closing, sorry for the noise!. ",
    "RichardKnop": "Hello, I have updated the issue @jefferai @xibz \nIt was a user mistake on my part: https://github.com/hashicorp/vault/issues/2277#issuecomment-273187996\nLong story short: I was using access key as secret and secret as access key which was causing the error (as AWS secrets can contain forward slashess so FOOBARFOOBARFOOBAR+g/HELLOWORLD is a valid secret while access keys only contain uppercase letters so FOOFOO/BAR is invalid access key).. ",
    "ror6ax": "I've contacted them as well for clarifications.\nI've modified AWS client file as follows:\n```\n--- a/builtin/logical/aws/client.go\n+++ b/builtin/logical/aws/client.go\n@@ -50,10 +50,10 @@ func getRootConfig(s logical.Storage) (*aws.Config, error) {\nfunc clientIAM(s logical.Storage) (*iam.IAM, error) {\n        awsConfig, _ := getRootConfig(s)\n-       return iam.New(session.New(awsConfig)), nil\n+       return iam.New(session.New(awsConfig.WithEndpoint(\"http:/myeucalyptus-cloud.net\n }\nfunc clientSTS(s logical.Storage) (*sts.STS, error) {\n        awsConfig, _ := getRootConfig(s)\n-       return sts.New(session.New(awsConfig)), nil\n+       return sts.New(session.New(awsConfig.WithEndpoint(\"http:/myeucalyptus-cloud.net\n }\n```\nOk, go's dependency system got the better of me :) \nI originally wrote there is no effect, but there is one now.\n```\nvault read aws/creds/readonly\nError reading aws/creds/readonly: Error making API request.\nURL: GET http://127.0.0.1:8200/v1/aws/creds/readonly\nCode: 400. Errors:\n\nError attaching user policy: SerializationError: failed to decode query XML error response\ncaused by: expected element type  but have \n```\n\nor with another example payload\n```\n vault read aws/creds/deploy                                       Error reading aws/creds/deploy: Error making API request.\nURL: GET http://127.0.0.1:8200/v1/aws/creds/deploy\nCode: 400. Errors:\n\nError putting user policy: MalformedPolicyDocument: Error in uploaded policy: net.sf.json.JSONException: Expecting net.sf.json.JSONArray but got net.sf.json.JSONObject\n        status code: 400, request id: 93c3faa8-35ba-4b20-9044-c452a9148991\n```\n\nApparently the replacement service sends not 1:1 response with AWS. Any tips on how do I debug these?. ",
    "codingzombie": "@jasdel - Could you help? I also tried adding role_arn to the aws_config. @xibz @jasdel : I want to use IAM roles. In order to use task role, the library needs to hit a metadata endpoint. I want to access the sqs queue without using credentials (and just with roles). I have all IAM roles - with permissions, resources setup. It works from python but doesn't work with aws-sdk-go? I have the aws/endpoint latest package in my Godeps.json. Is there anything else i'm missing? Not able to get it to work. I have picked the arn from the created queue. Do I need to add it in aws\nconfig or credentials?\nOn Tue, Jan 24, 2017 at 9:25 PM Jason Del Ponte notifications@github.com\nwrote:\n\n@codingzombie https://github.com/codingzombie is the ARN for your role\ncoming from the shared config file ~/.aws/config or the shared\ncredentials file ~/.aws/credentials? if the role ARN is coming from there\nyou may need to enable the SDK to read that value from the file by default\nit does not. You can use the env var AWS_SDK_LOAD_CONFIG=1 to enable this\nfeature or in code with, NewSessionWithOptions\nhttp://docs.aws.amazon.com/sdk-for-go/api/aws/session/#NewSessionWithOptions\n.\nCheck out the aws/session\nhttp://docs.aws.amazon.com/sdk-for-go/api/aws/session package for more\ndetails on what and how the SDK determines what configuration to load\nlet us know if this helps solve the issue you're running into.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-go/issues/1051#issuecomment-275022505,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGUj-xG8_qlQ-4jLnoeiH90jMoK2bBWeks5rVtyqgaJpZM4LsIou\n.\n. @xibz - Thanks for your reply.\n\n\nI have a IAM service role created with a policy to access SQS queue\nMy service runs inside an ec2 container - dockerized\nI added \"taskRoleArn\": \"$IAM_SERVICE_ROLE\" to my ecs task json\n\nNow I'm using aws-sdk-go with the latest version 1.6.14\n```\nfunc NewSQSConnection(queueURL, region string, MaxMessagesPerReq int64, PollingTime int64, SleepInterval time.Duration) *QueueConnection \n{\n    sess, _ := session.NewSession(&aws.Config{\n        Region: aws.String(region),\n    })\nreturn &QueueConnection{\n    QueueURL:          queueURL,\n    MaxMessagesPerReq: MaxMessagesPerReq,\n    PollingTime:       PollingTime,\n    SleepInterval:     SleepInterval * time.Second,\n    client:            sqs.New(sess),\n}\n\n}\n\nparams := &sqs.ReceiveMessageInput{\n        QueueUrl:              &init.QueueURL,\n        AttributeNames:        []string{aws.String(\"All\")},\n        MaxNumberOfMessages:   aws.Int64(init.MaxMessagesPerReq),\n        MessageAttributeNames: []string{aws.String(\"All\")},\n        WaitTimeSeconds:       aws.Int64(init.PollingTime),\n    }\n  for {\n        received, err := init.client.ReceiveMessage(params)\n        if err != nil {\n            if awsErr, ok := err.(awserr.Error); ok {\n                logger.Info(\"AWS Error reading queue: %s\", awsErr)\n                                //it fails here with 403 denied error\n            } else {\n                logger.Info(\"Error reading message from SQS queue: %s\", err.Error())\n         }\n\nThe app runs in a docker container inside ec2 ( ec2 container service). Am i missing something? \n. ",
    "Mchau2": "Thanks xibz.. ",
    "shaileshkm": "Consider a timeline. The timelines could happen across restarts.\nt1: I setup DLQ\nt2: I setup a normal queue and set its redrivePolicy with DLQ's name. I may repeat this step multiple times, each queue sharing the single DLQ.\nt3: I invoke send/receive operations on normal queue.\nt4. Occasionally I invoke receive from DLQ.\nThe only workaround I see here is to persist queue information and association with DLQ in persistent cache or db.\n. ",
    "KilleR": "I would suggest an error type specific to the InvalidSequenceTokenException response, which would have a nextSequenceToken field (being sure to use the same naming convention as the other SDKs to facilitate easy transferral of skills between people using it with different languages)\nAlternatively, have a JSON message or some other kind of text-based object structure, so that it can be reliably unmarshalled to give the requiered component fields.\nI feel, however, it is extremely important to preserve the structure of the original error string, otherwise it will break the code for anyone (like myself) who is using a regexp to find the sequenceToken.. ",
    "patsoffice": "What @KilleR is asking for, I believe, is a specific interface for an InvalidSequenceTokenException that has a NextSequenceToken field. PutLogEvents() call requires a a SequenceToken as part of PutLogEventsInput. You can either get this token via DescribeLogStreams() (which has a limit of 5 TPS) or you can manually parse the error output when the PutLogEvents() call fails.\nHere is example code:\n```go\n    svc := cloudwatchlogs.New(session.New(&aws.Config{\n        Region:      aws.String(\"us-west-2\"),\n        Credentials: credentials.NewStaticCredentials(\"foo\", \"bar\", \"\"),\n    }))\n    log.Println(\"Created a CWL session\")\nevent := cloudwatchlogs.InputLogEvent{\n    Timestamp: aws.Int64(time.Now().Unix()),\n    Message:   aws.String(\"a string\"),\n}\ni := cloudwatchlogs.PutLogEventsInput{\n    LogGroupName:  aws.String(\"test_log\"),\n    LogStreamName: aws.String(\"test_logstream\"),\n    LogEvents:     []*cloudwatchlogs.InputLogEvent{&event},\n}\n_, e := svc.PutLogEvents(&i)\nif e != nil {\n    if awsErr, ok := e.(awserr.Error); ok {\n        // Get error details\n        log.Println(\"Code:\", awsErr.Code())\n        log.Println(\"Message:\", awsErr.Message())\n        log.Println(\"Error:\", awsErr.Error())\n    } else {\n        fmt.Println(e.Error())\n    }\n}\n\n```\nthe output of the above is (something like):\n2017/02/01 16:40:40 Created a CWL session\n2017/02/01 16:40:41 Code: InvalidSequenceTokenException\n2017/02/01 16:40:41 Message: The given sequenceToken is invalid. The next expected sequenceToken is: foo\n2017/02/01 16:40:41 Error: InvalidSequenceTokenException: The given sequenceToken is invalid. The next expected sequenceToken is: foo\n    status code: 400, request id: bar. @xibz yeah, I wasn't suggesting that the SDK would do the parsing. Agreed, that seems fragile. Modeled exceptions seems the way to go. Thanks!. ",
    "dtan4": "I tried to assign dummy Operation, and finally the test passed.\nIt was just my overlooking...\ngo\n    s3mock.EXPECT().GetObjectRequest(&s3.GetObjectInput{\n        Bucket: aws.String(bucket),\n        Key:    aws.String(key),\n    }).Return(&request.Request{\n        HTTPRequest: &http.Request{\n            URL: u,\n        },\n        Operation: &request.Operation{},\n    }, &s3.GetObjectOutput{})\n    client := &Client{\n        api: s3mock,\n    }\nThank you for reviewing and telling me the correct way \ud83d\ude04 . ",
    "orlando": "@jasdel awesome, thanks for your reply. I'll check our code to see if it's better to have that CustomInput or just decode the base64 string before we send the request.\nThanks again. ",
    "kvadevack": "Any idea @jasdel when we'll see these paginators added?. ",
    "antoniomo": "Hi @xibz and thanks for the answer!\nI'm aware of those possibilities, but we feel that the most secure way to provide credentials to our EC2 instances is through instance IAM roles. If that's the case, we just want to rely on the EC2RoleProvider. As this provider queries the metadata service for credentials anyway, it would be cleaner in code if there was an elegant way to connect it to the region query. And yes, setting a config or environment variable achieves the same result, but makes provisioning a bit harder, and if we are using the metadata service anyway, we could achieve both cleaner code and cleaner deployments with this feature.\nMaybe this is more like a feature request than a question if the only way to do that is along the lines of the code sample I put above. Of course if this is a corner case, it's a totally discardable feature request :). Thanks for considering! I'm aware that it's not a perfect fit as it's a credential provider and in the other credential providers so region should be out of it. Perhaps there could be a separate config provider that can read from ec2metadata if running on an instance?\nIf you come up with a nice way to make it happen that would be great. Thanks for a great SDK!. To me both look very reasonable, depending if the user prefers to set it on code with session options or via provisioning tools with ENV vars.. ",
    "mikkeloscar": "+1 for this feature, we have several small services talking to the AWS API where they only care about the region in which they are deployed. If this was automatically discovered from the ec2 metadata we would not have to specify the AWS_REGION on all deployments (or implement the discovery ourselves outside of the SDK).\nI would expect the SDK to automatically find everything from the ec2metadata unless it's instructed otherwise by ENV vars or it's specifically hardcoded in the code. I believe this is how the python sdk (boto3) works by default, and it's very convenient.. @xibz awesome! If there's anything I can do to help out, just let me know!. @xibz Any news on this? I'll be happy to help in any way I can to get this going.. @jasdel Thanks for getting back to me on this.\nWhile I really appreciate you giving a reason for not wanting to go forward with this feature request, I'm sad that you will not consider it because of the reasons you give.\nI don't see what file locking issues there would be. In my mind the implementation should just be a simple read of the file, and if the credentials, at the time of reading, are up to date then fine, otherwise it would just fail to get credentials. It would be up to some external tool to keep the files up to date.\nAs an anecdote we implemented something similar for our IAM infrastructure in 2016 at Zalando and are currently using this in 85 Kubernetes clusters. Our experience is that this approach is much more stable than anything involving the network.\nI have ofc. considered what you propose, and it has also been discussed multiple times in the Kubernetes community. However I still think that the approach is so much more complicated to get right, that I really don't see the reason for not having the simple file read support in the SDKs. If you have something that calls an HTTP endpoint, you need to ensure that this endpoint is available at all times, this mean you need to run an HA setup in order to do updates of you component. Additionally you need to have retries in your clients to account for network issues, something you most likely won't need reading from a local file as you would have bigger problems if this started to fail.\nIf I have to ask users to set aws.Config.EC2MetadataDisableTimeoutOverride in the applications then I might as well ask them to use a custom SDK with file refresh support, so I don't think this is so helpful. Having an environment variable would be helpful, but I still consider this a hack rather than a robust solution.\nI wish you would reconsider this feature request, but if not, there is nothing I can do about it. Either way, thanks for looking at my request.. @jasdel thanks for providing an update on this!\n\nOur biggest concern with this file being refreshed is two factor. Usage outside of Kubernetes, and that the shared configuration and shared credential files are interchangeable for the most part. This means that there are non-credential related information that would be refreshed. The additional information in the files doesn't make sense to be refreshed during the lifetime of an application.\n\nIf this is such a big concern, then let me propose a new credentials provider for this use case. We could call it RefreshableFileCredentials or something like that and it could rely on a file with the same content as what you get from an assume role i.e.:\njson\n{\n    \"AccessKeyId\": \"ACCESS_KEY_ID\",\n    \"Expiration\": \"EXPIRATION_DATE\",\n    \"RoleArn\": \"TASK_ROLE_ARN\",\n    \"SecretAccessKey\": \"SECRET_ACCESS_KEY\",\n    \"Token\": \"SECURITY_TOKEN_STRING\"\n}\nIt could even be enabled ONLY when a certain environment variable is set, e.g.: AWS_REFRESHABLE_FILE_CREDENTIALS=/path/to/file. This way it would not affect users outside of Kubernetes (or other environments where this could be useful).\nI don't mind writing a proper proposal for how this would work, I just don't want to spend time on it if there's no chance it could be considered.\nBtw. today I looked into the code of the AWS SDK for Java and found out that it does support some form of file refreshing. It does so with a fixed refresh interval of 5 min. and reloads the file if it was changed since last check. While this is not the nicest solution, it actually solves the problem. I made a simple example application to verify that it does indeed work. This basically means the problem is solved for all JVM users which is nice.\nI don't suggest that it should be done the same way for the Go SDK, but considering that the Java SDK has support, would you be more willing to add support (in some form) to the Go SDK?\n\nA http endpoint for retrieving credentials per pod provides the best flexibility for applications of users.\n\nI don't agree. Users don't care how it's implemented. They care that it just works and they don't have to jump through a million hoops. To make an HTTP endpoint implementation \"just work\" is much more complex than what I propose. For reference this is the current HTTP endpoint based proposal from the community. It's very complex..\n\nI think further discussing EC2MetadataDisableTimeoutOverride as an environment variable makes absolute sense. Ideally this environment variable would be usable across all the SDKs.\n\nI would really prefer to spend effort on fixing the root cause instead of implementing hacks.\nFor me this is equivalent to putting sleeps in your code to work around race conditions.\nThat's why I don't like it because it moves the problem away from the SDKs to other components that becomes harder to implement in a good way because of that. When a change is needed to all SDKs anyway, why not make a change that can solve the problem at the lowest level, and solves the problem for good?. @xibz That sounds like great news!\nDo you have any rough estimation on the time frame for this? It's hurting us a lot not having a robust solution and we are able to help any way you need. We have 100 Kubernetes clusters on AWS we can use for testing this! I can also write a full proposal describing the idea described above if this would be helpful, just let me know!\nThanks for picking this up again!!. > In you're example you included a RoleArn member. Could you go into details more about the usage of this value?\nSorry, this I copied verbatim from your comment (comes from here: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html). I didn't actually have anything in mind for the RoleArn. The struct you suggest covers everything that would be needed and is actually what I had in mind.\nI did also not consider the Assume Role use case. I would expect whatever provides the credentials would already have performed the assuming of a role if needed. This I would simply consider out of scope.. This would also be a workaround for #1993 so I'm also in support for getting this reviewed/merged.. I just want to echo @lorengordon's concerns. If you only allow this provider via code it's pretty much useless. The reason it's useless is as @lorengordon says that it won't bring feature parity between the SDKs which will require every applicationn developer to do custom credentials initialization depending on the environment where the application is running. The whole point of the credentials chain is to avoid this and make it \"just work\" no matter the environment (or which language the SDK is written for).\nI have been trying to argue the same thing in #1993 for months.\nHaving this supported the same way as python, would \"solve\"/work around the problem stated in #1993 for the Go SDK. I have created a doc on the SDK support for solving #1933 https://github.com/mikkeloscar/kube-aws-iam-controller/blob/master/docs/sdk-configuration.md. @xibz great news!. > Excellent news! We look forward to being among the first to use it.\nI'll race you! :)\nAwesome work, thanks!. I also hit this issue now when testing with https://github.com/mikkeloscar/kube-aws-iam-controller. ",
    "BalmungSan": "Hello, any plans to implement this? - it would be great for us.\nWe are deploying many containers in ecs, all of them use ssm-parent to fetch some secrets from AWS SSM Parameter store in the same region the containers are executing. For now we have to set the AWS_REGION env var in the task definition of each container.\nPS: Curiously when running the services with FARGATE (instead of in a EC2 cluster) we don't need to specify the region - any ideas why?. > In Fargate, the AWS_REGION and AWS_DEFAULT_REGION environment variables are added automatically since the EC2 instance metadata service is not available.\nAh, thanks! - at least I know why it worked.. ",
    "samuelkarp": "\nCuriously when running the services with FARGATE (instead of in a EC2 cluster) we don't need to specify the region - any ideas why?\n\nIn Fargate, the AWS_REGION and AWS_DEFAULT_REGION environment variables are added automatically since the EC2 instance metadata service is not available.. @jasdel Done. ",
    "cjcjameson": "Beware getting a nil-pointer back from ec2metadata.New() in the workaround code!\nmetaSession, _ := session.NewSession()\nmetaClient := ec2metadata.New(metaSession)\nregion, _ := metaClient.Region(). ",
    "diamondap": "That's fine. I had concurrency set to 5, and then 2, and then 1 when this problem was occurring. Changing concurrency didn't seem to affect how much memory the uploader used. It was loading 128MB chunks until it used up all 4GB of system memory. . I'm using the Golang tar reader to read a number of large files directly from a tar archive. https://golang.org/pkg/archive/tar/#Reader\nWe're running Go 1.7.1 on Ubuntu 14.04.5 LTS (GNU/Linux 3.13.0-109-generic x86_64).\n. Thanks for looking into this. I will take another look at our own code when I have the time, and try to figure out if we're doing something to cause this.. ",
    "poopoothegorilla": "Thank you for taking the time @xibz. Yep this function has worked with other objects.\nGo: go1.8 darwin/amd64\nSDK: Release v1.7.1\nI was not quite sure what was or wasnt sensitive information here so I took out some sections. The code above uses the filename \"test.psv.gz\" but this code i ran with \"team_path_items.psv.gz\" it is the same code however.\n2017/03/01 14:36:53 DEBUG: Request s3/PutObject Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nPUT /dumps/team_path_items.psv.gz HTTP/1.1\nHost: [---]\nUser-Agent: aws-sdk-go/1.7.1 (go1.8; darwin; amd64)\nContent-Length: 17770\nAuthorization: AWS4-HMAC-SHA256 Credential=[---]/us-west-2/s3/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-content-sha256;x-amz-date, Signature=[---]\nContent-Type: text/plain; charset=utf-8\nX-Amz-Content-Sha256: [---]\nX-Amz-Date: 20170301T193653Z\nAccept-Encoding: gzip\n\n2017/03/01 14:36:54 DEBUG: Response s3/PutObject Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 403 Forbidden\nTransfer-Encoding: chunked\nContent-Type: application/xml\nDate: Wed, 01 Mar 2017 19:36:53 GMT\nServer: AmazonS3\nX-Amz-Id-2: [---]\nX-Amz-Request-Id: [---]\n-----------------------------------------------------. 2017/03/01 19:01:11 DEBUG: Request s3/PutObject Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nPUT /dumps/team_path_items.psv.gz HTTP/1.1\nHost:  [---]\nUser-Agent: aws-sdk-go/1.7.1 (go1.8; darwin; amd64)\nContent-Length: 101321\nAuthorization: AWS4-HMAC-SHA256 Credential= [---]/20170302/us-west-2/s3/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-content-sha256;x-amz-date, Signature= [---]\nContent-Type: text/plain; charset=utf-8\nX-Amz-Content-Sha256:  [---]\nX-Amz-Date:  [---]\nAccept-Encoding: gzip\n\n2017/03/01 19:01:13 DEBUG: Response s3/PutObject Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 200 OK\nContent-Length: 0\nDate: Thu, 02 Mar 2017 00:01:14 GMT\nEtag:  [---]\nServer: AmazonS3\nX-Amz-Id-2: [---]\nX-Amz-Request-Id:  [---]\n-----------------------------------------------------. @jasdel your suggestion of removing the ContentLength field worked! I did not know that it was optional. Thanks alot both of you guys @xibz @jasdel ... I really appreciate it \ud83d\ude04 . I see it now... \nhttp://docs.aws.amazon.com/sdk-for-go/api/service/s3/#PutObjectInput\n\nSize of the body in bytes. This parameter is useful when the size of the\n    body cannot be determined automatically.\n    ContentLength *int64 location:\"header\" locationName:\"Content-Length\" type:\"long\". \n",
    "bryevo": "I ran into the same issue.  I was using an IAM user with an old access key that didn't have S3FullAcess in the group permissions.  Once I added the correct permissions and created a new access key it worked!  Hopefully that fixes it!. ",
    "eric-luminal": "That would also be an acceptable solution. I have the newer version vendored but it is getting overridden by the nested dependency during build time.. Thank you very much for the quick turn around!. ",
    "crazed": "Thanks @jasdel, I thought I searched harder for this but apparently did not. Great news, looking forward to trying this out.. The new API looks great from a high level. I'll take a look at the diff\nwhen I get a chance.\nOn Mon, Mar 20, 2017, 7:47 PM Jason Del Ponte notifications@github.com\nwrote:\n\n:) Please let us know if you have any feedback on the PR. Any feedback now\nis much easier to address, than after the PR is merged.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-go/issues/1137#issuecomment-287933456,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAKm2gpJecFSUC8zqymn-f6VT2fCU3CEks5rnxAkgaJpZM4Mi_3p\n.\n. ping @jasdel. \n",
    "haonumen": "go version\ngo version xgcc (Ubuntu 4.9.3-0ubuntu4) 4.9.3 linux/amd64. Where can I get a new version of golang, I think I just downloaded from golang.org.  go1.8.linux-amd64.tar.gz that's it.. oh. let me try . thank you very much.. Just changed the env variable.\ngolang$ go version\ngo version go1.8 linux/amd64\nworked now. thank you.. ",
    "carrgilson": "This helped me out in getting the right version: https://github.com/tools/godep/issues/497. ",
    "twang-rs": "Here is a CloudWatch screenshot showing (green) IncomingRecords, and (blue & orange) OutgoingRecords for shard0 and shard1 of our stream.\n\n. This is the same stacktrace as above, but using debug/pprof/goroutine?debug=2\n```\ngoroutine 5535597 [running]:\nruntime/pprof.writeGoroutineStacks(0xf34060, 0xc4200f0460, 0x30, 0xc427b22bd0)\n        /usr/local/go/src/runtime/pprof/pprof.go:603 +0x79\nruntime/pprof.writeGoroutine(0xf34060, 0xc4200f0460, 0x2, 0xc420030a90, 0x411748)\n        /usr/local/go/src/runtime/pprof/pprof.go:592 +0x44\nruntime/pprof.(Profile).WriteTo(0xf649a0, 0xf34060, 0xc4200f0460, 0x2, 0xc4200f0460, 0xc420030cc0)\n        /usr/local/go/src/runtime/pprof/pprof.go:302 +0x3b5\nnet/http/pprof.handler.ServeHTTP(0xc427b22a91, 0x9, 0xf39ee0, 0xc4200f0460, 0xc421310000)\n        /usr/local/go/src/net/http/pprof/pprof.go:209 +0x1d1\nnet/http/pprof.Index(0xf39ee0, 0xc4200f0460, 0xc421310000)\n        /usr/local/go/src/net/http/pprof/pprof.go:221 +0x1e3\nnet/http.HandlerFunc.ServeHTTP(0xbb26e8, 0xf39ee0, 0xc4200f0460, 0xc421310000)\n        /usr/local/go/src/net/http/server.go:1942 +0x44\nnet/http.(ServeMux).ServeHTTP(0xf71cc0, 0xf39ee0, 0xc4200f0460, 0xc421310000)\n        /usr/local/go/src/net/http/server.go:2238 +0x130\nnet/http.serverHandler.ServeHTTP(0xc420157970, 0xf39ee0, 0xc4200f0460, 0xc421310000)\n        /usr/local/go/src/net/http/server.go:2568 +0x92\nnet/http.(conn).serve(0xc4204a2500, 0xf3a960, 0xc4253e9b00)\n        /usr/local/go/src/net/http/server.go:1825 +0x612\ncreated by net/http.(Server).Serve\n        /usr/local/go/src/net/http/server.go:2668 +0x2ce\ngoroutine 1 [select, 91 minutes]:\ngithub.com/rewardStyle/jester/vendor/github.com/rewardStyle/kinetic.(Listener).Listen(0xc4202e41b0, 0xc42049a0e0)\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/rewardStyle/kinetic/listener.go:177 +0x304\ngithub.com/rewardStyle/jester.(Jester).Start(0xc420074900, 0xc4200ec000, 0xf3abe0)\n        /go/src/github.com/rewardStyle/jester/jester.go:88 +0x4fd\nmain.processArgs(0xc420190140)\n        /go/src/github.com/rewardStyle/jester/jester_main/main.go:59 +0x89\nreflect.Value.call(0xa800a0, 0xbb22f8, 0x13, 0xb878e5, 0x4, 0xc4201b3d00, 0x1, 0x1, 0xc4201b3c90, 0xb76160, ...)\n        /usr/local/go/src/reflect/value.go:434 +0x91f\nreflect.Value.Call(0xa800a0, 0xbb22f8, 0x13, 0xc4201b3d00, 0x1, 0x1, 0x2, 0x2, 0xc420151640)\n        /usr/local/go/src/reflect/value.go:302 +0xa4\ngithub.com/rewardStyle/jester/vendor/github.com/urfave/cli.HandleAction(0xa800a0, 0xbb22f8, 0xc420190140, 0x0, 0x0)\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/urfave/cli/app.go:482 +0x18f\ngithub.com/rewardStyle/jester/vendor/github.com/urfave/cli.(*App).Run(0xc420158480, 0xc420010190, 0x1, 0x1, 0x0, 0x0)\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/urfave/cli/app.go:240 +0x560\nmain.main()\n        /go/src/github.com/rewardStyle/jester/jester_main/main.go:54 +0x1f7\ngoroutine 17 [syscall, 150 minutes, locked to thread]:\nruntime.goexit()\n        /usr/local/go/src/runtime/asm_amd64.s:2197 +0x1\ngoroutine 6 [syscall, 150 minutes]:\nos/signal.signal_recv(0x69d8ab)\n        /usr/local/go/src/runtime/sigqueue.go:116 +0x104\nos/signal.loop()\n        /usr/local/go/src/os/signal/signal_unix.go:22 +0x22\ncreated by os/signal.init.1\n        /usr/local/go/src/os/signal/signal_unix.go:28 +0x41\ngoroutine 7 [sleep]:\ntime.Sleep(0x3b9aca00)\n        /usr/local/go/src/runtime/time.go:59 +0xf9\ngithub.com/rewardStyle/jester/vendor/github.com/valyala/fasthttp.init.1.func1()\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/valyala/fasthttp/header.go:1372 +0x2a\ncreated by github.com/rewardStyle/jester/vendor/github.com/valyala/fasthttp.init.1\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/valyala/fasthttp/header.go:1375 +0x3a\ngoroutine 9 [IO wait]:\nnet.runtime_pollWait(0x7fe907ddcf58, 0x72, 0x0)\n        /usr/local/go/src/runtime/netpoll.go:164 +0x59\nnet.(pollDesc).wait(0xc4201839c8, 0x72, 0x0, 0xc4255589a0)\n        /usr/local/go/src/net/fd_poll_runtime.go:75 +0x38\nnet.(pollDesc).waitRead(0xc4201839c8, 0xffffffffffffffff, 0x0)\n        /usr/local/go/src/net/fd_poll_runtime.go:80 +0x34\nnet.(netFD).accept(0xc420183960, 0x0, 0xf34320, 0xc4255589a0)\n        /usr/local/go/src/net/fd_unix.go:430 +0x1e5\nnet.(TCPListener).accept(0xc42000e230, 0xc4204a2580, 0xab69c0, 0xffffffffffffffff)\n        /usr/local/go/src/net/tcpsock_posix.go:136 +0x2e\nnet.(TCPListener).AcceptTCP(0xc42000e230, 0xc420034db8, 0xc420034dc0, 0xc420034db0)\n        /usr/local/go/src/net/tcpsock.go:215 +0x49\nnet/http.tcpKeepAliveListener.Accept(0xc42000e230, 0xbb2508, 0xc4204a2500, 0xf3aa20, 0xc4201ec120)\n        /usr/local/go/src/net/http/server.go:3044 +0x2f\nnet/http.(Server).Serve(0xc420157970, 0xf3a360, 0xc42000e230, 0x0, 0x0)\n        /usr/local/go/src/net/http/server.go:2643 +0x228\nnet/http.(*Server).ListenAndServe(0xc420157970, 0xc420157970, 0x0)\n        /usr/local/go/src/net/http/server.go:2585 +0xb0\nnet/http.ListenAndServe(0xb90964, 0xe, 0x0, 0x0, 0x0, 0x0)\n        /usr/local/go/src/net/http/server.go:2787 +0x7f\nmain.main.func1()\n        /go/src/github.com/rewardStyle/jester/jester_main/main.go:35 +0x5a\ncreated by main.main\n        /go/src/github.com/rewardStyle/jester/jester_main/main.go:38 +0x8f\ngoroutine 29 [select, 150 minutes]:\ngithub.com/rewardStyle/jester/vendor/github.com/gocql/gocql.(*eventDeouncer).flusher(0xc420015270)\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/gocql/gocql/events.go:40 +0x136\ncreated by github.com/rewardStyle/jester/vendor/github.com/gocql/gocql.newEventDeouncer\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/gocql/gocql/events.go:28 +0x10e\ngoroutine 35 [IO wait]:\nnet.runtime_pollWait(0x7fe907ddd018, 0x72, 0x7)\n        /usr/local/go/src/runtime/netpoll.go:164 +0x59\nnet.(pollDesc).wait(0xc4200506f8, 0x72, 0xf360e0, 0xf30708)\n        /usr/local/go/src/net/fd_poll_runtime.go:75 +0x38\nnet.(pollDesc).waitRead(0xc4200506f8, 0xc420218000, 0x2000)\n        /usr/local/go/src/net/fd_poll_runtime.go:80 +0x34\nnet.(netFD).Read(0xc420050690, 0xc420218000, 0x2000, 0x2000, 0x0, 0xf360e0, 0xf30708)\n        /usr/local/go/src/net/fd_unix.go:250 +0x1b7\nnet.(conn).Read(0xc42000e0f8, 0xc420218000, 0x2000, 0x2000, 0x0, 0x0, 0x0)\n        /usr/local/go/src/net/net.go:181 +0x70\ncrypto/tls.(block).readFromUntil(0xc4201bb140, 0x7fe907ddd440, 0xc42000e0f8, 0x5, 0xc42000e0f8, 0xc420016050)\n        /usr/local/go/src/crypto/tls/conn.go:488 +0x98\ncrypto/tls.(Conn).readRecord(0xc4201c8a80, 0xbb2c17, 0xc4201c8ba0, 0x5)\n        /usr/local/go/src/crypto/tls/conn.go:590 +0xc4\ncrypto/tls.(Conn).Read(0xc4201c8a80, 0xc4201b0000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n        /usr/local/go/src/crypto/tls/conn.go:1134 +0x11d\nbufio.(Reader).Read(0xc4201f4300, 0xc4200f0108, 0x1, 0x9, 0xc42020fc48, 0x568b17, 0xc420050690)\n        /usr/local/go/src/bufio/bufio.go:213 +0x312\nio.ReadAtLeast(0xf320e0, 0xc4201f4300, 0xc4200f0108, 0x1, 0x9, 0x1, 0x30, 0x28, 0xb471a0)\n        /usr/local/go/src/io/io.go:307 +0xa9\nio.ReadFull(0xf320e0, 0xc4201f4300, 0xc4200f0108, 0x1, 0x9, 0xc42023a000, 0x7fe907e76000, 0x0)\n        /usr/local/go/src/io/io.go:325 +0x58\ngithub.com/rewardStyle/jester/vendor/github.com/gocql/gocql.readHeader(0xf320e0, 0xc4201f4300, 0xc4200f0108, 0x9, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/gocql/gocql/frame.go:360 +0xb6\ngithub.com/rewardStyle/jester/vendor/github.com/gocql/gocql.(Conn).recv(0xc4200f00e0, 0x0, 0x0)\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/gocql/gocql/conn.go:443 +0xba\ngithub.com/rewardStyle/jester/vendor/github.com/gocql/gocql.(Conn).serve(0xc4200f00e0)\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/gocql/gocql/conn.go:416 +0x2b\ncreated by github.com/rewardStyle/jester/vendor/github.com/gocql/gocql.Connect\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/gocql/gocql/conn.go:247 +0x7df\ngoroutine 30 [select, 150 minutes]:\ngithub.com/rewardStyle/jester/vendor/github.com/gocql/gocql.(*eventDeouncer).flusher(0xc4200152c0)\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/gocql/gocql/events.go:40 +0x136\ncreated by github.com/rewardStyle/jester/vendor/github.com/gocql/gocql.newEventDeouncer\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/gocql/gocql/events.go:28 +0x10e\ngoroutine 40 [IO wait, 91 minutes]:\nnet.runtime_pollWait(0x7fe907ddce98, 0x72, 0x8)\n        /usr/local/go/src/runtime/netpoll.go:164 +0x59\nnet.(pollDesc).wait(0xc420050a78, 0x72, 0xf360e0, 0xf30708)\n        /usr/local/go/src/net/fd_poll_runtime.go:75 +0x38\nnet.(pollDesc).waitRead(0xc420050a78, 0xc42018e400, 0x400)\n        /usr/local/go/src/net/fd_poll_runtime.go:80 +0x34\nnet.(netFD).Read(0xc420050a10, 0xc42018e400, 0x400, 0x400, 0x0, 0xf360e0, 0xf30708)\n        /usr/local/go/src/net/fd_unix.go:250 +0x1b7\nnet.(conn).Read(0xc42000e110, 0xc42018e400, 0x400, 0x400, 0x0, 0x0, 0x0)\n        /usr/local/go/src/net/net.go:181 +0x70\ncrypto/tls.(block).readFromUntil(0xc4201bbaa0, 0x7fe907ddd440, 0xc42000e110, 0x5, 0xc42000e110, 0xc420016050)\n        /usr/local/go/src/crypto/tls/conn.go:488 +0x98\ncrypto/tls.(Conn).readRecord(0xc42033c380, 0xbb2c17, 0xc42033c4a0, 0x7d)\n        /usr/local/go/src/crypto/tls/conn.go:590 +0xc4\ncrypto/tls.(Conn).Read(0xc42033c380, 0xc4201fe000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n        /usr/local/go/src/crypto/tls/conn.go:1134 +0x11d\nbufio.(Reader).Read(0xc4201f4ea0, 0xc4200f02c8, 0x1, 0x9, 0xc42004bc48, 0x568b17, 0xc420050a10)\n        /usr/local/go/src/bufio/bufio.go:213 +0x312\nio.ReadAtLeast(0xf320e0, 0xc4201f4ea0, 0xc4200f02c8, 0x1, 0x9, 0x1, 0x30, 0x28, 0xb471a0)\n        /usr/local/go/src/io/io.go:307 +0xa9\nio.ReadFull(0xf320e0, 0xc4201f4ea0, 0xc4200f02c8, 0x1, 0x9, 0xc420119800, 0x7fe907e76000, 0x0)\n        /usr/local/go/src/io/io.go:325 +0x58\ngithub.com/rewardStyle/jester/vendor/github.com/gocql/gocql.readHeader(0xf320e0, 0xc4201f4ea0, 0xc4200f02c8, 0x9, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/gocql/gocql/frame.go:360 +0xb6\ngithub.com/rewardStyle/jester/vendor/github.com/gocql/gocql.(Conn).recv(0xc4200f02a0, 0x0, 0x0)\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/gocql/gocql/conn.go:443 +0xba\ngithub.com/rewardStyle/jester/vendor/github.com/gocql/gocql.(Conn).serve(0xc4200f02a0)\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/gocql/gocql/conn.go:416 +0x2b\ncreated by github.com/rewardStyle/jester/vendor/github.com/gocql/gocql.Connect\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/gocql/gocql/conn.go:247 +0x7df\ngoroutine 59 [IO wait]:\nnet.runtime_pollWait(0x7fe907ddcdd8, 0x72, 0x3)\n        /usr/local/go/src/runtime/netpoll.go:164 +0x59\nnet.(pollDesc).wait(0xc421e351e8, 0x72, 0xf360e0, 0xf30708)\n        /usr/local/go/src/net/fd_poll_runtime.go:75 +0x38\nnet.(pollDesc).waitRead(0xc421e351e8, 0xc4287a4fde, 0x5022)\n        /usr/local/go/src/net/fd_poll_runtime.go:80 +0x34\nnet.(netFD).Read(0xc421e35180, 0xc4287a4fde, 0x5022, 0x5022, 0x0, 0xf360e0, 0xf30708)\n        /usr/local/go/src/net/fd_unix.go:250 +0x1b7\nnet.(conn).Read(0xc426c39498, 0xc4287a4fde, 0x5022, 0x5022, 0x0, 0x0, 0x0)\n        /usr/local/go/src/net/net.go:181 +0x70\ncrypto/tls.(block).readFromUntil(0xc4277518c0, 0x7fe907ddd440, 0xc426c39498, 0x4035, 0xc426c39498, 0x0)\n        /usr/local/go/src/crypto/tls/conn.go:488 +0x98\ncrypto/tls.(Conn).readRecord(0xc4201b2380, 0xbb2c17, 0xc4201b24a0, 0x0)\n        /usr/local/go/src/crypto/tls/conn.go:635 +0x1fa\ncrypto/tls.(Conn).Read(0xc4201b2380, 0xc42bb5cec4, 0x1a1b96, 0x216f3c, 0x0, 0x0, 0x0)\n        /usr/local/go/src/crypto/tls/conn.go:1134 +0x11d\nnet/http.(persistConn).Read(0xc425fe9560, 0xc42bb5cec4, 0x1a1b96, 0x216f3c, 0x400, 0xc4207e19a8, 0x42b4e6)\n        /usr/local/go/src/net/http/transport.go:1316 +0x14b\nbufio.(Reader).Read(0xc425b561e0, 0xc42bb5cec4, 0x1a1b96, 0x216f3c, 0x0, 0x0, 0x0)\n        /usr/local/go/src/bufio/bufio.go:199 +0x198\nio.(LimitedReader).Read(0xc426dc6300, 0xc42bb5cec4, 0x216f3c, 0x216f3c, 0xc42811da00, 0x0, 0x67bb7d)\n        /usr/local/go/src/io/io.go:436 +0x6c\nnet/http.(body).readLocked(0xc4285e2c40, 0xc42bb5cec4, 0x216f3c, 0x216f3c, 0xc420001a00, 0x0, 0xc4221a9900)\n        /usr/local/go/src/net/http/transfer.go:761 +0x61\nnet/http.(body).Read(0xc4285e2c40, 0xc42bb5cec4, 0x216f3c, 0x216f3c, 0x0, 0x0, 0x0)\n        /usr/local/go/src/net/http/transfer.go:753 +0xfd\nnet/http.(bodyEOFSignal).Read(0xc4285e2c80, 0xc42bb5cec4, 0x216f3c, 0x216f3c, 0x0, 0x0, 0x0)\n        /usr/local/go/src/net/http/transport.go:2035 +0xe9\nbytes.(Buffer).ReadFrom(0xc4207e1b58, 0xf33d20, 0xc4285e2c80, 0xc423dd6000, 0x0, 0x200)\n        /usr/local/go/src/bytes/buffer.go:179 +0x160\nio/ioutil.readAll(0xf33d20, 0xc4285e2c80, 0x200, 0x0, 0x0, 0x0, 0x0, 0x0)\n        /usr/local/go/src/io/ioutil/ioutil.go:33 +0x150\nio/ioutil.ReadAll(0xf33d20, 0xc4285e2c80, 0xc4207e1c70, 0x82ca65, 0xc4207e1c70, 0x40efc8, 0xad4ba0)\n        /usr/local/go/src/io/ioutil/ioutil.go:42 +0x3e\ngithub.com/rewardStyle/jester/vendor/github.com/aws/aws-sdk-go/private/protocol/json/jsonutil.UnmarshalJSON(0xafb9a0, 0xc427e1c0f0, 0xf33d20, 0xc4285e2c80, 0xc4285e2c80, 0x839e91)\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/aws/aws-sdk-go/private/protocol/json/jsonutil/unmarshal.go:17 +0x62\ngithub.com/rewardStyle/jester/vendor/github.com/aws/aws-sdk-go/private/protocol/jsonrpc.Unmarshal(0xc4201c8000)\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/aws/aws-sdk-go/private/protocol/jsonrpc/jsonrpc.go:65 +0x10f\ngithub.com/rewardStyle/jester/vendor/github.com/aws/aws-sdk-go/aws/request.(HandlerList).Run(0xc4201c81c8, 0xc4201c8000)\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/aws/aws-sdk-go/aws/request/handlers.go:136 +0x87\ngithub.com/rewardStyle/jester/vendor/github.com/aws/aws-sdk-go/aws/request.(Request).Send(0xc4201c8000, 0xc42a4c11e0, 0xc4201c8000)\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/aws/aws-sdk-go/aws/request/request.go:405 +0x435\ngithub.com/rewardStyle/jester/vendor/github.com/aws/aws-sdk-go/service/kinesis.(Kinesis).GetRecords(0xc42000e040, 0xc42a4c11e0, 0xc4201f7f98, 0x0, 0x0)\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/aws/aws-sdk-go/service/kinesis/api.go:861 +0x4d\ngithub.com/rewardStyle/jester/vendor/github.com/rewardStyle/kinetic.(Listener).consume(0xc4202e41b0)\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/rewardStyle/kinetic/listener.go:218 +0x1ec\ncreated by github.com/rewardStyle/jester/vendor/github.com/rewardStyle/kinetic.(*Listener).init\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/rewardStyle/kinetic/listener.go:86 +0x438\ngoroutine 42 [select]:\ngithub.com/rewardStyle/jester/vendor/github.com/gocql/gocql.(controlConn).heartBeat(0xc420122ac0)\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/gocql/gocql/control.go:61 +0x2bb\ncreated by github.com/rewardStyle/jester/vendor/github.com/gocql/gocql.(controlConn).connect\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/gocql/gocql/control.go:166 +0x268\ngoroutine 60 [chan receive]:\ngithub.com/rewardStyle/jester/vendor/github.com/rcrowley/go-metrics.(*meterArbiter).tick(0xf71ec0)\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/rcrowley/go-metrics/meter.go:221 +0x4c\ncreated by github.com/rewardStyle/jester/vendor/github.com/rcrowley/go-metrics.NewMeter\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/rcrowley/go-metrics/meter.go:40 +0x127\ngoroutine 58 [IO wait, 91 minutes]:\nnet.runtime_pollWait(0x7fe907ddcd18, 0x72, 0x9)\n        /usr/local/go/src/runtime/netpoll.go:164 +0x59\nnet.(pollDesc).wait(0xc420050d88, 0x72, 0xf360e0, 0xf30708)\n        /usr/local/go/src/net/fd_poll_runtime.go:75 +0x38\nnet.(pollDesc).waitRead(0xc420050d88, 0xc42018f000, 0x400)\n        /usr/local/go/src/net/fd_poll_runtime.go:80 +0x34\nnet.(netFD).Read(0xc420050d20, 0xc42018f000, 0x400, 0x400, 0x0, 0xf360e0, 0xf30708)\n        /usr/local/go/src/net/fd_unix.go:250 +0x1b7\nnet.(conn).Read(0xc42000e130, 0xc42018f000, 0x400, 0x400, 0x0, 0x0, 0x0)\n        /usr/local/go/src/net/net.go:181 +0x70\ncrypto/tls.(block).readFromUntil(0xc4201bbe30, 0x7fe907ddd440, 0xc42000e130, 0x5, 0xc42000e130, 0xc420016050)\n        /usr/local/go/src/crypto/tls/conn.go:488 +0x98\ncrypto/tls.(Conn).readRecord(0xc4201b2a80, 0xbb2c17, 0xc4201b2ba0, 0x7e)\n        /usr/local/go/src/crypto/tls/conn.go:590 +0xc4\ncrypto/tls.(Conn).Read(0xc4201b2a80, 0xc420208000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n        /usr/local/go/src/crypto/tls/conn.go:1134 +0x11d\nbufio.(Reader).Read(0xc4201f5a40, 0xc4200f03a8, 0x1, 0x9, 0xc420047c48, 0x568b17, 0xc420050d20)\n        /usr/local/go/src/bufio/bufio.go:213 +0x312\nio.ReadAtLeast(0xf320e0, 0xc4201f5a40, 0xc4200f03a8, 0x1, 0x9, 0x1, 0x30, 0x28, 0xb471a0)\n        /usr/local/go/src/io/io.go:307 +0xa9\nio.ReadFull(0xf320e0, 0xc4201f5a40, 0xc4200f03a8, 0x1, 0x9, 0xc420119800, 0x7fe907e76000, 0x0)\n        /usr/local/go/src/io/io.go:325 +0x58\ngithub.com/rewardStyle/jester/vendor/github.com/gocql/gocql.readHeader(0xf320e0, 0xc4201f5a40, 0xc4200f03a8, 0x9, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/gocql/gocql/frame.go:360 +0xb6\ngithub.com/rewardStyle/jester/vendor/github.com/gocql/gocql.(Conn).recv(0xc4200f0380, 0x0, 0x0)\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/gocql/gocql/conn.go:443 +0xba\ngithub.com/rewardStyle/jester/vendor/github.com/gocql/gocql.(Conn).serve(0xc4200f0380)\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/gocql/gocql/conn.go:416 +0x2b\ncreated by github.com/rewardStyle/jester/vendor/github.com/gocql/gocql.Connect\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/gocql/gocql/conn.go:247 +0x7df\ngoroutine 61 [IO wait, 150 minutes]:\nnet.runtime_pollWait(0x7fe907ddcb98, 0x72, 0x0)\n        /usr/local/go/src/runtime/netpoll.go:164 +0x59\nnet.(pollDesc).wait(0xc4200f33a8, 0x72, 0x0, 0xc42049c940)\n        /usr/local/go/src/net/fd_poll_runtime.go:75 +0x38\nnet.(pollDesc).waitRead(0xc4200f33a8, 0xffffffffffffffff, 0x0)\n        /usr/local/go/src/net/fd_poll_runtime.go:80 +0x34\nnet.(netFD).accept(0xc4200f3340, 0x0, 0xf34320, 0xc42049c940)\n        /usr/local/go/src/net/fd_unix.go:430 +0x1e5\nnet.(TCPListener).accept(0xc42000e1f0, 0xc4201bb7a0, 0xc4201fde78, 0x55256d)\n        /usr/local/go/src/net/tcpsock_posix.go:136 +0x2e\nnet.(TCPListener).AcceptTCP(0xc42000e1f0, 0x675293, 0xc4201fde90, 0xc4201fde88)\n        /usr/local/go/src/net/tcpsock.go:215 +0x49\nnet/http.tcpKeepAliveListener.Accept(0xc42000e1f0, 0xc4201bb770, 0xabd9a0, 0xf61330, 0xb1f7e0)\n        /usr/local/go/src/net/http/server.go:3044 +0x2f\nnet/http.(Server).Serve(0xc4202834a0, 0xf3a360, 0xc42000e1f0, 0x0, 0x0)\n        /usr/local/go/src/net/http/server.go:2643 +0x228\nnet/http.(Server).ListenAndServe(0xc4202834a0, 0xc4202834a0, 0x0)\n        /usr/local/go/src/net/http/server.go:2585 +0xb0\nnet/http.ListenAndServe(0xc42049a0c0, 0x5, 0xf33120, 0xc420494230, 0x5, 0xae02c0)\n        /usr/local/go/src/net/http/server.go:2787 +0x7f\ncreated by github.com/rewardStyle/jester.(Jester).Start\n        /go/src/github.com/rewardStyle/jester/jester.go:81 +0x48e\ngoroutine 48 [select]:\ngithub.com/rewardStyle/jester/vendor/github.com/gocql/gocql.(*Session).reconnectDownedHosts(0xc420100280, 0xdf8475800)\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/gocql/gocql/session.go:220 +0x259\ncreated by github.com/rewardStyle/jester/vendor/github.com/gocql/gocql.NewSession\n        /go/src/github.com/rewardStyle/jester/vendor/github.com/gocql/gocql/session.go:190 +0xb23\ngoroutine 49 [select, 150 minutes, locked to thread]:\nruntime.gopark(0xbb2b00, 0x0, 0xb89ecd, 0x6, 0x18, 0x2)\n        /usr/local/go/src/runtime/proc.go:271 +0x13a\nruntime.selectgoImpl(0xc4201fcf50, 0x0, 0x18)\n        /usr/local/go/src/runtime/select.go:423 +0x1364\nruntime.selectgo(0xc4201fcf50)\n        /usr/local/go/src/runtime/select.go:238 +0x1c\nruntime.ensureSigM.func1()\n        /usr/local/go/src/runtime/signal_unix.go:434 +0x2dd\nruntime.goexit()\n        /usr/local/go/src/runtime/asm_amd64.s:2197 +0x1\ngoroutine 5535598 [runnable]:\nnet/http.(connReader).backgroundRead(0xc4253e9b40)\n        /usr/local/go/src/net/http/server.go:655\ncreated by net/http.(connReader).startBackgroundRead\n        /usr/local/go/src/net/http/server.go:652 +0xdf\ngoroutine 5425964 [select, 92 minutes]:\nnet/http.(persistConn).readLoop(0xc425fe9560)\n        /usr/local/go/src/net/http/transport.go:1599 +0x9ec\ncreated by net/http.(Transport).dialConn\n        /usr/local/go/src/net/http/transport.go:1117 +0xa35\ngoroutine 5425965 [select, 92 minutes]:\nnet/http.(persistConn).writeLoop(0xc425fe9560)\n        /usr/local/go/src/net/http/transport.go:1704 +0x43a\ncreated by net/http.(Transport).dialConn\n        /usr/local/go/src/net/http/transport.go:1118 +0xa5a\n``. You can tell that ourListenfunction is waiting in aselectcall waiting for further messages to arrive in the channels.  Focusing in on the goroutine that contains theGetRecords` call:\n```\n1 @ 0x42e0aa 0x4291c7 0x428809 0x568678 0x5686e4 0x569e37 0x57d210 0x5c3498 0x5c3b3a 0x5c770d 0x684b8b 0x51e8a8 0x4635ac 0x67bca1 0x67bbed 0x688989 0x46e210 0x4d0d90 0x4d0e6e 0x8969c2 0x89954f 0x82b387 0x82e045 0x89dffd 0x8b4c6c 0x45afe1\n0x428808        net.runtime_pollWait+0x58                                                                                               /usr/local/go/src/runtime/netpoll.go:164\n0x568677        net.(*pollDesc).wait+0x37                                                                                               /usr/local/go/src/net/fd_poll_runtime.go:75\n0x5686e3        net.(*pollDesc).waitRead+0x33                                                                                           /usr/local/go/src/net/fd_poll_runtime.go:80\n0x569e36        net.(*netFD).Read+0x1b6                                                                                                 /usr/local/go/src/net/fd_unix.go:250\n0x57d20f        net.(*conn).Read+0x6f                                                                                                   /usr/local/go/src/net/net.go:181\n0x5c3497        crypto/tls.(*block).readFromUntil+0x97                                                                                  /usr/local/go/src/crypto/tls/conn.go:488\n0x5c3b39        crypto/tls.(*Conn).readRecord+0x1f9                                                                                     /usr/local/go/src/crypto/tls/conn.go:635\n0x5c770c        crypto/tls.(*Conn).Read+0x11c                                                                                           /usr/local/go/src/crypto/tls/conn.go:1134\n0x684b8a        net/http.(*persistConn).Read+0x14a                                                                                      /usr/local/go/src/net/http/transport.go:1316\n0x51e8a7        bufio.(*Reader).Read+0x197                                                                                              /usr/local/go/src/bufio/bufio.go:199\n0x4635ab        io.(*LimitedReader).Read+0x6b                                                                                           /usr/local/go/src/io/io.go:436\n0x67bca0        net/http.(*body).readLocked+0x60                                                                                        /usr/local/go/src/net/http/transfer.go:761\n0x67bbec        net/http.(*body).Read+0xfc                                                                                              /usr/local/go/src/net/http/transfer.go:753\n0x688988        net/http.(*bodyEOFSignal).Read+0xe8                                                                                     /usr/local/go/src/net/http/transport.go:2035\n0x46e20f        bytes.(*Buffer).ReadFrom+0x15f                                                                                          /usr/local/go/src/bytes/buffer.go:179\n0x4d0d8f        io/ioutil.readAll+0x14f                                                                                                 /usr/local/go/src/io/ioutil/ioutil.go:33\n0x4d0e6d        io/ioutil.ReadAll+0x3d                                                                                                  /usr/local/go/src/io/ioutil/ioutil.go:42\n0x8969c1        github.com/rewardStyle/jester/vendor/github.com/aws/aws-sdk-go/private/protocol/json/jsonutil.UnmarshalJSON+0x61        /go/src/github.com/rewardStyle/jester/vendor/github.com/aws/aws-sdk-go/private/protocol/json/jsonutil/unmarshal.go:17\n0x89954e        github.com/rewardStyle/jester/vendor/github.com/aws/aws-sdk-go/private/protocol/jsonrpc.Unmarshal+0x10e                 /go/src/github.com/rewardStyle/jester/vendor/github.com/aws/aws-sdk-go/private/protocol/jsonrpc/jsonrpc.go:65\n0x82b386        github.com/rewardStyle/jester/vendor/github.com/aws/aws-sdk-go/aws/request.(*HandlerList).Run+0x86                      /go/src/github.com/rewardStyle/jester/vendor/github.com/aws/aws-sdk-go/aws/request/handlers.go:136\n0x82e044        github.com/rewardStyle/jester/vendor/github.com/aws/aws-sdk-go/aws/request.(*Request).Send+0x434                        /go/src/github.com/rewardStyle/jester/vendor/github.com/aws/aws-sdk-go/aws/request/request.go:405\n0x89dffc        github.com/rewardStyle/jester/vendor/github.com/aws/aws-sdk-go/service/kinesis.(*Kinesis).GetRecords+0x4c               /go/src/github.com/rewardStyle/jester/vendor/github.com/aws/aws-sdk-go/service/kinesis/api.go:861\n0x8b4c6b        github.com/rewardStyle/jester/vendor/github.com/rewardStyle/kinetic.(*Listener).consume+0x1eb                           /go/src/github.com/rewardStyle/jester/vendor/github.com/rewardStyle/kinetic/listener.go:218\n```\nWe can see that the trace through the SDK code is:\nhttps://github.com/aws/aws-sdk-go/blob/5b99715ae2945a2434a2371f4e6c5542e839a32d/aws/request/request.go#L405\nhttps://github.com/aws/aws-sdk-go/blob/5b99715ae2945a2434a2371f4e6c5542e839a32d/aws/request/handlers.go#L136\nhttps://github.com/aws/aws-sdk-go/blob/5b99715ae2945a2434a2371f4e6c5542e839a32d/private/protocol/jsonrpc/jsonrpc.go#L65\nhttps://github.com/aws/aws-sdk-go/blob/5b99715ae2945a2434a2371f4e6c5542e839a32d/private/protocol/json/jsonutil/unmarshal.go#L17\nFrom the trace, we can see that we are blocked in IO Wait on an ioutil.ReadAll(stream) -- prior to the actual json.Unmarshal() call.\nNote that a netstat shows the connection to Kinesis as ESTABLISHED:\n/opt/jester # netstat -an\nActive Internet connections (servers and established)\nProto Recv-Q Send-Q Local Address           Foreign Address         State\ntcp        0      0 127.0.0.1:6060          0.0.0.0:*               LISTEN\ntcp        0      0 172.17.0.2:36490        172.16.10.124:9042      ESTABLISHED\ntcp        0      0 172.17.0.2:45670        54.240.251.200:443      ESTABLISHED\ntcp        0      0 172.17.0.2:36486        172.16.10.124:9042      ESTABLISHED\ntcp        0      0 172.17.0.2:36488        172.16.10.124:9042      ESTABLISHED\ntcp        0      0 :::8090                 :::*                    LISTEN\nActive UNIX domain sockets (servers and established)\nProto RefCnt Flags       Type       State         I-Node Path\nFurthermore, if I tcpkill this socket (from the ECS host, since I can't find tcpkill for alpine), the process immediately becomes unblocked.\n% sudo tcpkill -i eth0 host 54.240.251.200. Below is a sample payload that we're sending through Kinesis:\n{\n  \"Data\": {\n    \"data_type\": \"xxxxxxxxxxx\",\n    \"data\": {\n      \"user_agent\": \"Go-http-client/1.1\",\n      \"ts\": \"2017-03-21T20:34:13.57810677Z\",\n      \"ip\": \"xxxxxxxxxxxxxxxxxx\",\n      \"referer\": \"\",\n      \"fp\": \"stresstest\",\n      \"cookie_val\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n      \"ingestion_id\": \"bef4fab8-0e75-11e7-8061-db9afaebe7e0\",\n      \"extra\": {\n        \"height\": \"xxxx\",\n        \"title\": \"xxxxxxxx\",\n        \"ts\": \"2016-11-15T20:34:31.990481Z\",\n        \"width\": \"xxxx\",\n        \"account_id\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n        \"document_referrer\": \"https%3A%2F%2Fxxxxxxxxxxxxxx%2F\",\n        \"lang\": \"xxxxxx\",\n        \"orientation\": \"xxxxx\",\n        \"xxxxxxxxxx\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n      }\n    },\n    \"version\": \"0.0.1\",\n    \"ts\": \"2017-03-21T20:34:13.57810677Z\"\n  },\n  \"PartitionKey\": \"2be849cae2eaa6dbf949d615b65bf33a\",\n  \"ApproximateArrivalTimestamp\": 1490128453.699,\n  \"SequenceNumber\": \"49571551843860662170063845371697159861318589635654320130\"\n}\n% wc foo\n  31   55 1037 foo. We are in the process of instrumenting the code as recommended here: https://github.com/aws/aws-sdk-go/issues/301#issuecomment-231491380\nWill report more data when I have it.\nAs best as I can tell, we're experiencing a unexpected hang-up (or otherwise a lack of response) from the Kinesis service.  The implementation in the SDK (using ioutil.ReadAll()) as well as sendgridlabs/go-kinesis (using http.Client.Do()) do not provide any timeouts and it is unclear to me (especially now that that one locked reader has unwound) whether TCP KeepAlives are still going back & forth.\nRelated reading: https://medium.com/@nate510/don-t-use-go-s-default-http-client-4804cb19f779#.5y9m4d68c. Tried a tcpdump to capture 0 length packets.  Not sure how accurate this is:\n```\n% tcpdump -pni eth0 -v \"tcp port 443 and ( tcp[tcpflags] & tcp-ack != 0 and ( (ip[2:2] - ((ip[0]&0xf)<<2) ) - ((tcp[12]&0xf0)>>2) ) == 0 ) \"\ntcpdump: listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes\n10:29:35.243707 IP (tos 0x0, ttl 255, id 61938, offset 0, flags [DF], proto TCP (6), length 40)\n    172.17.0.2.45670 > 54.240.251.200.443: Flags [.], cksum 0xdee6 (incorrect -> 0xc333), ack 1567741584, win 8646, length 0\n10:29:35.244699 IP (tos 0x0, ttl 230, id 44126, offset 0, flags [DF], proto TCP (6), length 40)\n    54.240.251.200.443 > 172.17.0.2.45670: Flags [.], cksum 0x0c66 (correct), ack 1, win 432, length 0\n10:30:05.323713 IP (tos 0x0, ttl 255, id 61939, offset 0, flags [DF], proto TCP (6), length 40)\n    172.17.0.2.45670 > 54.240.251.200.443: Flags [.], cksum 0xdee6 (incorrect -> 0xc333), ack 1, win 8646, length 0\n10:30:05.324720 IP (tos 0x0, ttl 230, id 44127, offset 0, flags [DF], proto TCP (6), length 40)\n    54.240.251.200.443 > 172.17.0.2.45670: Flags [.], cksum 0x0c66 (correct), ack 1, win 432, length 0\n10:30:35.403718 IP (tos 0x0, ttl 255, id 61940, offset 0, flags [DF], proto TCP (6), length 40)\n    172.17.0.2.45670 > 54.240.251.200.443: Flags [.], cksum 0xdee6 (incorrect -> 0xc333), ack 1, win 8646, length 0\n10:30:35.404711 IP (tos 0x0, ttl 230, id 44128, offset 0, flags [DF], proto TCP (6), length 40)\n    54.240.251.200.443 > 172.17.0.2.45670: Flags [.], cksum 0x0c66 (correct), ack 1, win 432, length 0\n10:30:35.423818 IP (tos 0x0, ttl 255, id 61941, offset 0, flags [DF], proto TCP (6), length 40)\n    172.17.0.2.45670 > 54.240.251.200.443: Flags [.], cksum 0xdee6 (incorrect -> 0xbd67), ack 1461, win 8669, length 0\n10:30:35.424860 IP (tos 0x0, ttl 255, id 61942, offset 0, flags [DF], proto TCP (6), length 40)\n    172.17.0.2.45670 > 54.240.251.200.443: Flags [.], cksum 0xdee6 (incorrect -> 0xbd50), ack 1461, win 8692, length 0\n10:30:35.424989 IP (tos 0x0, ttl 255, id 61943, offset 0, flags [DF], proto TCP (6), length 40)\n    172.17.0.2.45670 > 54.240.251.200.443: Flags [.], cksum 0xdee6 (incorrect -> 0xbd39), ack 1461, win 8715, length 0\n10:31:05.483720 IP (tos 0x0, ttl 255, id 61944, offset 0, flags [DF], proto TCP (6), length 40)\n    172.17.0.2.45670 > 54.240.251.200.443: Flags [.], cksum 0xdee6 (incorrect -> 0xbd3a), ack 1461, win 8715, length 0\n10:31:05.484715 IP (tos 0x0, ttl 230, id 44132, offset 0, flags [DF], proto TCP (6), length 40)\n    54.240.251.200.443 > 172.17.0.2.45670: Flags [.], cksum 0x00fe (correct), ack 1, win 432, length 0\n10:31:35.563707 IP (tos 0x0, ttl 255, id 61945, offset 0, flags [DF], proto TCP (6), length 40)\n    172.17.0.2.45670 > 54.240.251.200.443: Flags [.], cksum 0xdee6 (incorrect -> 0xbd3a), ack 1461, win 8715, length 0\n10:31:35.564718 IP (tos 0x0, ttl 230, id 44133, offset 0, flags [DF], proto TCP (6), length 40)\n    54.240.251.200.443 > 172.17.0.2.45670: Flags [.], cksum 0x00fe (correct), ack 1, win 432, length 0\n10:32:05.643760 IP (tos 0x0, ttl 255, id 61946, offset 0, flags [DF], proto TCP (6), length 40)\n    172.17.0.2.45670 > 54.240.251.200.443: Flags [.], cksum 0xdee6 (incorrect -> 0xbd3a), ack 1461, win 8715, length 0\n10:32:05.644797 IP (tos 0x0, ttl 230, id 44134, offset 0, flags [DF], proto TCP (6), length 40)\n    54.240.251.200.443 > 172.17.0.2.45670: Flags [.], cksum 0x00fe (correct), ack 1, win 432, length 0\n10:32:35.723710 IP (tos 0x0, ttl 255, id 61947, offset 0, flags [DF], proto TCP (6), length 40)\n    172.17.0.2.45670 > 54.240.251.200.443: Flags [.], cksum 0xdee6 (incorrect -> 0xbd3a), ack 1461, win 8715, length 0\n10:32:35.724718 IP (tos 0x0, ttl 230, id 44135, offset 0, flags [DF], proto TCP (6), length 40)\n    54.240.251.200.443 > 172.17.0.2.45670: Flags [.], cksum 0x00fe (correct), ack 1, win 432, length 0\n10:32:35.743690 IP (tos 0x0, ttl 255, id 61948, offset 0, flags [DF], proto TCP (6), length 40)\n    172.17.0.2.45670 > 54.240.251.200.443: Flags [.], cksum 0xdee6 (incorrect -> 0xb76e), ack 2921, win 8738, length 0\n10:32:35.744676 IP (tos 0x0, ttl 255, id 61949, offset 0, flags [DF], proto TCP (6), length 40)\n    172.17.0.2.45670 > 54.240.251.200.443: Flags [.], cksum 0xdee6 (incorrect -> 0xb758), ack 2921, win 8760, length 0\n10:32:35.744765 IP (tos 0x0, ttl 255, id 61950, offset 0, flags [DF], proto TCP (6), length 40)\n    172.17.0.2.45670 > 54.240.251.200.443: Flags [.], cksum 0xdee6 (incorrect -> 0xb741), ack 2921, win 8783, length 0\n10:33:05.803713 IP (tos 0x0, ttl 255, id 61951, offset 0, flags [DF], proto TCP (6), length 40)\n    172.17.0.2.45670 > 54.240.251.200.443: Flags [.], cksum 0xdee6 (incorrect -> 0xb742), ack 2921, win 8783, length 0\n10:33:05.804711 IP (tos 0x0, ttl 230, id 44139, offset 0, flags [DF], proto TCP (6), length 40)\n    54.240.251.200.443 > 172.17.0.2.45670: Flags [.], cksum 0xf595 (correct), ack 1, win 432, length 0\n^C\n22 packets captured\n22 packets received by filter\n0 packets dropped by kernel\n```. Since it was suggested that high CPU utilization may have something to do with the hang (I'm not entirely sure how), here is the Service Utilization:\n\nThese are running on machines with 1024 CPU unit and each container has a 340 unit reservation.\nOne of the application is running on an ECS container instance by itself.  The other one shares with another (idle) application.\nWe see that the process goes up to about 250% (2.5 x 340 = 850 units).. Simply this has stabilized our client for about 2 days now:\nconfig := aws.NewConfig()\n    config = config.WithHTTPClient(&http.Client{\n        Timeout: l.httpClientTimeout,\n    })\nHere is a plot of our GetRecords:\n\nIn particular, notice initially that the two GetRecords graphs (orange & blue) fluctuated wildly and that the sum of the two values were significantly greater than the IncomingRecords (green).  At this point, we were most likely catching up from TRIM_HORIZON and presumably each GetRecords request was returning a full 10k records.  The download time for the payload was significant, causing the high fluctuation in the graph.\nAt some point, this settled down.  Since the records themselves did not change (same payload with a different timestamp), I presume that the number of records returned by the GetRecords call was less than 10k, which also implies that we have \"caught up\" to the LATEST on the stream.\nWhat is concerning is that at this point in the graph, the sum of the GetRecords for both shards is greater than the IncomingRecords.  One possible explanation of this is that our timeout (set to 1 minute) is cancelling a certain number (nearly half!) of otherwise healthy GetRecords requests that were in the process of downloading the payload.  This might cause CloudWatch to count the records returned by that particular request; however, since we cancelled that request, we would effectively retry that batch.  The second attempt to fetch the batch may be faster, perhaps due to variations on load on the Kinesis server or even perhaps caching.\nSimilarly concerning is the fact that at some point, the GetRecords drops to where the sum of the GetRecords of the two shards are again roughly equal to the IncomingRecords.  I have no explanation of how suddenly this would occur, other than variations on load (and therefore the response time of the Kinesis server).. Here is the same graph above, this time including GetRecords.IteratorAge.  This may provide more insight:\n\nIt is a little hard to interpret IteratorAge, as we have turn our message producer on and off at various points.  It does surprise me a bit that the IteratorAge is lower when our consumer starts initially, then jumps before making a slow decay to 0.\nThe retention on our stream is 96 hours (4 days) and I would have expected that the iterator age would be highest initially as the consumer starts from TRIM_HORIZON and reads our oldest records (those written prior to this graph).. In any case, we're currently working on a solution which does not simply set a global timeout for the entire client.  Instead, we are setting an incremental timeout in the ioutil.ReadAll() of the GetRecord's HTTPResponse.Body.\nThis solution builds on top of the debugging code that @jasdel provided in #301, where we use a DebugReadCloser to wrap the HTTPResponse.Body ReadCloser stream.\nWhile I don't have any code I can share just yet, these snippets should give you a general idea of how we're approaching this:\n```\nstruct DebugReadCloser {\n    io.ReadCloser\n    ReadFn func(io.ReadCloser, []byte) (int, error)\n    CloseFn func()\n}\nfunc (r *DebugReadCloser) Read(b []byte) (int, error) {\n    return r.ReadFn(r.ReadCloser, b)\n}\nfunc (r *DebugReadCloser) Close() error {\n    r.CloseFn()\n    return r.ReadCloser.Close()\n}\n// in the consumer loop\nreq.Handlers.Unmarshal.PushFront(func(r *request.Request) {\n    timer := time.NewTimer(2 * time.Second)\n    r.HTTPResponse.Body = &DebugReadCloser{\n        ReadCloser: r.HTTPResponse.Body,\n        ReadFn: func(stream io.ReadCloser, b []byte) (n int, err error) {\n            type Result struct {\n                n int,\n                err error\n            }\n            c := make(chan Result, 1)\n            go func() {\n                var result Result\n                result.n, result.err = stream.Read(b)\n                c <- result\n            }()\n            select {\n                case result := <-c:\n                    timer.Reset(2 * time.Second)\n                    n, err = result.n, result.err\n                case <-timer.C:\n                    err = errors.New(\"Timeout while reading GetRecords response body\")\n            }\n            return\n        },\n        CloseFn: func() { ... } // Unimportant\n    }\n}\n```\nAdmittedly, we were looking for a solution that does not modify the SDK code.  If were were willing to rewrite the UnmarshalJSON function to not use ioutil.ReadAll, a cleaner approach may be available.. One more screenshot showing (on top) sum metrics and (below) average metrics:\n\n. I believe using sum for GetRecords.IteratorAge provided a skewed graph.  An average of the IteratorAge makes far more sense.. Re: https://github.com/aws/aws-sdk-go/issues/1141#issuecomment-290262451\nA 2 minute timeout is more than enough (perhaps even too large), especially if only timing the duration of the first Read() on the HTTPResponse.Body.\nI do believe that exposing the duration (whether it be for reading the entire response or for each Read) should have sensible defaults as well as be exposed to the client application.. Awesome, this looks good to me.. Forgive me if this is what is intended, but wouldn't stopping the timer here essentially be putting a timeout on the first successful Read() call on the HTTPResponse.Body?\nThis could still lead to a situation where the Kinesis service stops sending data, improperly hangs up the connection, or if the TCP FIN was lost in the network. \nAs before, a global http.Client{Timeout:xxx} could catch this problem, but that is applied across all clients in use by a particular session.. Ah, I see it now.  If overhead is a concern, it may be best to create a timer inside the timeoutReadCloser and Reset it per read.. ",
    "docmerlin": "Thanks.\nOn Wed, Mar 22, 2017 at 17:32 xibz notifications@github.com wrote:\n\nDue to this being a duplicate I am going to close this and we can continue\ndiscussion in #1141 https://github.com/aws/aws-sdk-go/issues/1141\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-go/issues/1144#issuecomment-288560327,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABLfW4yTIxR-PYrP_aZISUkUbqLmTPY-ks5roaFdgaJpZM4Ml2P6\n.\n. \n",
    "patrickml": "@jasdel it might be worth including it here, it looks like the nodejs aws sdk has support for it. ",
    "bmess": "Thank you for linking to the documentation guide repo!  In the future I'll be glad to submit a PR. ",
    "bonifaido": "For me changing buildHashTree to BuildHashTree would be sufficient (or maybe with another name), do you think this is enough?. Great! Looks good to me.. Awesome, Thank you!. Thank you very much!. Thank you! Now it works great, I just checked it with master.. ",
    "linearregression": "Actually, I was referring to when user calling the AWS at this level:\nhttps://github.com/aws/aws-sdk-go/blob/master/service/ecs/api.go#L85\nhttps://github.com/aws/aws-sdk-go/blob/master/service/ecs/api.go#L99\nWe are no longer dealing with request directly. The action may not be retry at all,  but something else to signal to upstream. Suggestion?. ",
    "etsangsplk": "No I was looking at if after all your retry and I still get ThrottleException, it can help trigger event on our side but is user application specific. There is no point for application user to retry but to do something else. There are a= lot of code that will throttle, so wrapping every API like the one above make everything looks a lot more complicated . @jasdel It will be nice is SDK can expose this function. It is \"somewhat\" exposed but on retry.go and the receiver is Request. Since most people don't deal with Request directly, if there is  a similar one at a higher layer be appreciated.. more details there:\nhttps://github.com/aws/aws-sdk-go/issues/1436. @jasdel Can you take a look at this? I think what you suggested is what I have been doing.\nhttps://github.com/aws/aws-sdk-go/issues/1436. @jasdel.  Is to send back the new session object since it already got a new credentials from assume role. So I did not set that. Do I need to ???. @jasdel \nI did not  enable MFA . @jasdel \nI actually did not select MFA on either user, roles or profile ... I have this before \n```\nsess := getSessionByProfile(&validAwsRegion, \"admin\", true) <-- this is admin profile\n```\nDoes that matter? \nAttend to clear any default access creds but force to use config and credentials file at non default places. Then getSessionByProfile should use the corresponding creds to assume role and give me back the new sessions from creds obtained from assume role.\nIf somehow calls to s3 or ec2 still use the old creds read from config files, that may be a problem.\nWill there be a chance that the config files takes precedence somehow, even the sess returned should contains the new creds?\nos.Unsetenv(\"AWS_ACCESS_KEY_ID\")\n    os.Unsetenv(\"AWS_SECRET_ACCESS_KEY\")\n       os.Setenv(\"AWS_CONFIG_FILE\", testConfigFilename)\n        os.Setenv(\"AWS_SHARED_CREDENTIALS_FILE\", testCredentialFilename)\n        os.Setenv(\"AWS_SESSION_TOKEN\", \"token_not_exists\")\n    os.Setenv(\"AWS_SDK_LOAD_CONFIG\", \"1\"). @jasdel Hi I was not asking how to use the api but to add enhancement.\n I was referring to when  ListAttachedRolePolicies returns, can we add a type to tag what type is each attached policy, just like ListPolicy (which is just 1 policy). It will be very helpful.. I am doing this via Cloudformtion createstack api. \nfollowing is the portion that created LB:\nWeaveScopeElasticLoadBalancer:\n      Type: AWS::ElasticLoadBalancing::LoadBalancer\n      DependsOn: InternetGatewayAttachment\n      Properties:\n        Subnets:\n        - !Ref PublicSubnet1\n        - !Ref PublicSubnet2\n        SecurityGroups:\n        - !Ref LoadBalancerSecurityGroup\n        Scheme: 'internet-facing'\n        Listeners:\n        - LoadBalancerPort: '80'\n          InstancePort: '4040'\n          Protocol: TCP\n        Type: 'network'   <<<<<<<-----------------\nCloudformation EVents:\n| 11:51:36 UTC-0700 | CREATE_FAILED | AWS::ElasticLoadBalancing::LoadBalancer | WeaveScopeElasticLoadBalancer | Encountered unsupported property Type\n. It is my mistake it shodl be \ufeffAWS::ElasticLoadBalancingV2::. @jasdel \nhttp://docs.aws.amazon.com/efs/latest/ug/API_CreateFileSystem.html\nSince if create cluster with EFS support. There are a few ids returned. It is incontinent not be able to see those handles. \nLots of other efa API e.g. describe require filesystems or creation token etc to fetch result. . ",
    "jasonmoo": "Hey @jasdel thanks for the quick turn around but I think this is actually incorrect.  \ngolang\n+           var inCpy {{ .Operation.InputRef.GoTypeElem }}\n+           if input != nil  {\n+               inCpy = *input\n+           }\n+           req, _ := c.{{ .OperationName }}Request(&inCpy)\nYour solution will pass a pointer to an initialized type value rather than a nil pointer.  It seems like this may be significant if code expects to use a default where a nil input value is supplied, rather than using the zero-value of the field on the input struct.\ngolang\n            var inCpy {{ .InputRef.GoType }}\n            if input != nil {\n                tmp := *input\n                inCpy = &tmp\n            }\n            req, _ := c.{{ .ExportedName }}Request(inCpy)\nWhile slightly less clean, this will pass a nil pointer where a nil pointer is sent and copy the input value where the pointer value is an address.  \nOpen to a cleaner solution that evaluates to the same effect.. @jasdel  thanks again for the quick response.  LGTM.  \ud83d\udc4d . ",
    "t2y": "Hi @jasdel , Thank you for handling my PR, I'm glad to get alternative helper functions and escalate the models update. That's exactly what I wanted. I'm looking forward to merging #1165. \ud83d\ude04 \n. ",
    "vaijab": "@jasdel my apologies. As I was writing a small code bit to reproduce this issue, I discovered what I was doing wrong. Sorry about this. . ",
    "0x1eaf": "Thanks). ",
    "odeke-em": "In regards to the issues with thread local storage for Java vs context.Context in Go, I have prototyped how to trace all the auto-generated clients here https://github.com/orijtech/aws-sdk-go/tree/opencensus-tracing\nFor a quick introduction, I work on OpenCensus and coincidentally this evening I looked at how perhaps this could be implemented. OpenCensus allows you to trace and monitor then export to X-Ray and to a bunch of other backends likes Stackdriver Tracing and Monitoring, Prometheus, SignalFX etc It is a vendor agnostic distribution of libraries for distributed tracing and monitoring, no maintenance burdens or distributed systems expertise needed.\n1) OpenCensus-Go's tracing minimally involves starting and stopping a span\ngo\nctx, span := trace.StartSpan(parentCtx, nameOfSpan)\ndefer span.End()\n2) Most of the code is generated, hence in the API generation templates we can do\ndiff\n$ git diff private/model/api/operation.go \ndiff --git a/private/model/api/operation.go b/private/model/api/operation.go\nindex 59a716e..a20894d 100644\n--- a/private/model/api/operation.go\n+++ b/private/model/api/operation.go\n@@ -172,6 +172,9 @@ func (c *{{ .API.StructName }}) {{ .ExportedName }}(` +\n func (c *{{ .API.StructName }}) {{ .ExportedName }}WithContext(` +\n        `ctx aws.Context, input {{ .InputRef.GoType }}, opts ...request.Option) ` +\n        `({{ .OutputRef.GoType }}, error) {\n+        {{ if .API.EnableTracing }}ctx, span := trace.StartSpan(ctx,\"aws/{{ .API.PackageName }}.(*{{ .API.StructName }}).{{ .ExportedName }}\")\n+        defer  span.End()\n+        {{end}}\n        req, out := c.{{ .ExportedName }}Request(input)\n        req.SetContext(ctx)\n        req.ApplyOptions(opts...)\n3. To ensure proper context propagation as well as tracing when HTTP requests to the AWS APIs are being made, we also need to modify request/request.go a little to start some more spans and add some annotations for concrete visualization\n```diff\ndiff --git a/aws/corehandlers/handlers.go b/aws/corehandlers/handlers.go\nindex cfcddf3..8db9da3 100644\n--- a/aws/corehandlers/handlers.go\n+++ b/aws/corehandlers/handlers.go\n@@ -14,6 +14,8 @@ import (\n    \"github.com/aws/aws-sdk-go/aws/awserr\"\n    \"github.com/aws/aws-sdk-go/aws/credentials\"\n    \"github.com/aws/aws-sdk-go/aws/request\"\n+\n+   \"go.opencensus.io/plugin/ochttp\"\n )\n// Interface for matching types which also have a Len method.\n@@ -117,7 +119,15 @@ var SendHandler = request.NamedHandler{\n }\nfunc sendFollowRedirects(r request.Request) (http.Response, error) {\n-   return r.Config.HTTPClient.Do(r.HTTPRequest)\n+   hc := r.Config.HTTPClient\n+   if hc == nil {\n+       hc = &http.Client{}\n+   }\n+   var rt http.RoundTripper = hc.Transport\n+   if _, ok := hc.Transport.(*ochttp.Transport); !ok {\n+       rt = &ochttp.Transport{Base: hc.Transport}\n+   }\n+   return rt.RoundTrip(r.HTTPRequest)\n }\nfunc sendWithoutFollowRedirects(r request.Request) (http.Response, error) {\n@@ -126,7 +136,7 @@ func sendWithoutFollowRedirects(r request.Request) (http.Response, error) {\n        transport = http.DefaultTransport\n    }\n\nreturn transport.RoundTrip(r.HTTPRequest)\nreturn (&ochttp.Transport{Base: transport}).RoundTrip(r.HTTPRequest)\n }\n\nfunc handleSendError(r *request.Request, err error) {\n```\nand that I have done in two separate commits at my experimental fork at https://github.com/orijtech/aws-sdk-go\n1. Modified the template and the HTTP transports https://github.com/orijtech/aws-sdk-go/commit/711d79d3c8cfdec656caae086ee6d7b2ad1cfb82\n2. Ran make generate https://github.com/orijtech/aws-sdk-go/commit/de0c24fd2bfc1e637703184c4f5ad2c0b21321ff\nWith those steps, if we run the code below\nhttps://gist.github.com/odeke-em/844b33948506cba7de3323644b737688\nwe can get such results\nOn AWS X-Ray\n\nOn Stackdriver\n\n\n. ",
    "rasecoiac03": "@xibz, my bad. I forgot telling I was using a Uploader as well. The code was spread, putting together my code was like:\nMy bad was create a service passing the region config and the uploader not. Thanks a lot the fast response.\n```go\npackage main\nimport (\n    \"compress/gzip\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\"github.com/aws/aws-sdk-go/service/s3/s3manager\"\n\n)\nfunc main() {\n    sess := session.Must(session.NewSession(aws.NewConfig().\n        WithMaxRetries(3),\n    ))\n    service := s3.New(sess, aws.NewConfig().WithRegion(\"sa-east-1\"))\n    uploader := s3manager.NewUploader(sess)\nparams1 := &s3.HeadObjectInput{\n    Bucket: aws.String(\"vr.sitemap\"),\n    Key:    aws.String(\"dev/test.gz\"),\n}\n_, err := service.HeadObject(params1)\nif err != nil {\n    fmt.Println(\"file exists\")\n}\n\nurlMap := map[string]int64{}\nurlMap[\"test.com\"] = 1\n\njsonBinary, jsonErr := json.Marshal(urlMap)\nif jsonErr != nil {\n    fmt.Println(jsonErr)\n}\nreader, writer := io.Pipe()\n\ngo func() {\n    gw := gzip.NewWriter(writer)\n    gw.Write(jsonBinary)\n    gw.Close()\n    writer.Close()\n}()\n\ncontentType := \"application/x-gzip\"\nparams := &s3manager.UploadInput{\n    Body:        reader,\n    Bucket:      aws.String(\"vr.sitemap\"),\n    Key:         aws.String(\"dev/test.gz\"),\n    ContentType: aws.String(contentType),\n    ACL:         aws.String(s3.ObjectCannedACLPublicRead),\n}\n_, err = uploader.Upload(params)\nif err != nil {\n    fmt.Println(err)\n}\n\n}\n```. Thanks @xibz. ",
    "asac": "Thanks. Yes, I tend to agree that appengine could do better, but then the regression came from trying to update your code not theirs :).\nAnyway, I filed a google appengine issue on this too: https://issuetracker.google.com/u/1/issues/37184096\nThanks. @xibz I assume the diff in question would be:\n+func isSerializationErrorRetryable(err error) bool {\n+       if err == nil {\n+               return false\n+       }\n+\n+       if aerr, ok := err.(awserr.Error); ok {\n+               return isCodeRetryable(aerr.Code())\n+       }\n+\n+       if opErr, ok := err.(*net.OpError); ok {\n+               if sysErr, ok := opErr.Err.(*os.SyscallError); ok {\n+                       return sysErr.Err == syscall.ECONNRESET\n+               }\n+       }\n+\n+       return false\n+}\n+\nmaybe there is a way to duplicate those constant without need to import syscall?. FYI, the google appengine issue that tracks the syscall constant issues is at: https://github.com/golang/appengine/issues/67. ",
    "broady": "FWIW, the syscall package is platform dependent and not part of the Go 1 compatibility guarantee.. ",
    "mattsongb": "Are there any updates on this request? I've run into an issue lately where it would've been nice to be able to pull the description from the endpoint package's defaults. Seems like I'm not the only one that this would help, so if possible, could I submit a pull request for this feature? . ",
    "martonsereg": "This would help us either. We're working with the Pricing API and the only way to filter the get-products call with the location is to provide the region description instead of the ID. It'd be great if we could do it without managing a static list with the region IDs and descriptions.\n@jasdel any chance it could happen? I could submit a PR as well, at first sight it seems to be a one-line change in the code.. thanks @jasdel, I've sent a PR, it would be great if you could take a look at it: https://github.com/aws/aws-sdk-go/pull/1909.. ",
    "henriquechehad": "@jasdel yes solved using region option. thanks @jasdel, created there: http://stackoverflow.com/questions/43621542/sending-sns-push-notification-message-to-multiple-devices. ",
    "stefansundin": "I just undid the api.go changes. Don't forget to send them upstream. ;)\nAlso, don't forget to fix the markdown lists that I took screenshots of.. This is also an issue in v2: https://github.com/aws/aws-sdk-go-v2/blob/ac26486d921d336e20fec49e2afb348c801f0003/aws/ec2rolecreds/provider.go#L105. This path.join is removing the trailing slash :(\nhttps://github.com/aws/aws-sdk-go/blob/7ae709b086c9d83d556644336851ed6776b0b95e/aws/ec2metadata/api.go#L22\nLet me know how you want to resolve it.. Hi Jason. I added a second commit that simply removes the path.Join calls,\nand I think it might be good enough. Let me know what you think. Thanks!\nOn Thu, Jul 5, 2018, 13:30 Jason Del Ponte notifications@github.com wrote:\n\nThanks for creating this PR @stefansundin\nhttps://github.com/stefansundin. One solution to solving the path.Join\nissue would be to wrap the path.Join call detecting if last element has a\ntrailing / and re-append it to the string.\nAlternatively, we could investigate the usage of URL.ResolveReference.\nThis utility will preserve the trailing slash of resolved references.\nThough it wasn't added until Go 1.8, so a backported version would need to\nbe embedded in the SDK for older versions of Go\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-go/pull/2002#issuecomment-402843672, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AB5h7whZnl3OELofWWehsO4Jl2wX7KM3ks5uDndugaJpZM4Uv9mj\n.\n. @jasdel @xibz Just a reminder that this bug also affects v2. I'll let you two fix it there.. I can also confirm that v1.14.26 works as expected in my use case.\n\nBefore:\n10.40.11.67 - - [12/Jul/2018:14:59:35 PDT] \"GET /latest/meta-data/iam/security-credentials HTTP/1.1\" 301 0\n- -> /latest/meta-data/iam/security-credentials\n10.40.11.67 - - [12/Jul/2018:14:59:35 PDT] \"GET /latest/meta-data/iam/security-credentials/ HTTP/1.1\" 200 4\nhttp://169.254.169.254/latest/meta-data/iam/security-credentials -> /latest/meta-data/iam/security-credentials/\n10.40.11.67 - - [12/Jul/2018:14:59:35 PDT] \"GET /latest/meta-data/iam/security-credentials/role HTTP/1.1\" 200 611\n- -> /latest/meta-data/iam/security-credentials/role\nv1.14.26:\n10.40.11.67 - - [12/Jul/2018:14:59:47 PDT] \"GET /latest/meta-data/iam/security-credentials/ HTTP/1.1\" 200 4\n- -> /latest/meta-data/iam/security-credentials/\n10.40.11.67 - - [12/Jul/2018:14:59:47 PDT] \"GET /latest/meta-data/iam/security-credentials/role HTTP/1.1\" 200 611\n- -> /latest/meta-data/iam/security-credentials/role. I'd like to add one more thing.. Not all function calls have all the types linked. E.g. Credentials is missing links in NewSharedCredentials and NewStaticCredentials. This is probably a thing all over the place. See https://docs.aws.amazon.com/sdk-for-go/api/aws/credentials/#NewSharedCredentials. @jasdel let me know when the api.go fixes have been sent upstream, and then I will undo the changes from the PR.. ",
    "adhikarisandeep": "I'm running into the same issue. Was this fixed?. ",
    "rodriguise": "I am was the same issue with the apigateway, actually using govendor, so the GOPATH should not matter. I performed:\ngovendor remove github.com/aws/aws-sdk-go/...\ngovendor add +external\nThis solved the issue for me.\n. ",
    "abhay8nitt": "@jasdel Thanks for all your input. I have figured out the issue and it works fine. The environment variables(HTTP_PROXY etc) was not being set for a specific user in my case. In the standalone case the environment variables were available for the user.\nTo answer your questions -\nIn my case I am only looking at the aws service requests through proxy( however if there is a way to get the credentials and the aws service requests through proxy it would be awesome)\nYes the value of NO_PROXY=169.254.169.254\nHowever I feel this to be an issue now (even though I am not blocked by this). Correct my understanding\n```go\nhttpclient := &http.Client{\n        Transport: &http.Transport{\n            Proxy: func(h http.Request) (url.URL, error) {\n                return url.Parse(os.getEnv(HTTPS_PROXY)) \n            },\n        },\n    }\nsess_http_proxy := session.Must(session.NewSession(&aws.Config{\n        Region: aws.String(Region),\n                Credentials: credentials.NewStaticCredentials(id,secret,token), \n        CredentialsChainVerboseErrors: aws.Bool(true),\n        HTTPClient:                    httpclient,\n    },\n))\nValue_http_proxy, _ := sess_http_proxy.Config.Credentials.Get()\n```\nThe above code works, however if I remove\nCredentials: credentials.NewStaticCredentials(id,secret,token)\nit fails to get the credentials from the instance.\nWhy should the go sdk not provide the Credentials from the instance when just the httpclient and region is fed to the config? However when we create the session as below\ngo\nsess := session.Must(session.NewSession(&aws.Config{\n        Region: aws.String(Region),\n    },\n))\nValue, _ := sess.Config.Credentials.Get()\nThe above code works without any failure.\nIn both the cases we are creating the session by providing partial configuration. When we provide the http client it doesn't work but when we just give the region in the second case it works. Is it the correct behavior or the sdk should look for the credentials when the custom http client is fed to the session?\n. Hey Thanks for the detail answer.. One question. Can r.URL.Host be a dns name or it has to be an ip/ip:port ?. ",
    "srizzling": "I think I am affecting by this issue. \nTrying to run the following https://github.com/codesuki/ecs-gen which is dependant on this library. \nI am setting the following vars:\nHTTP_PROXY\nHTTPS_PROXY\nNO_PROXY\nContents of $NO_PROXY:\nbash\nroot@7a5a57d8f8a3:~# echo $NO_PROXY\n169.254.169.254,10.132.17.154\nRunning the ecs-gen binary, I get the following: \n2017/10/12 05:16:59 NoCredentialProviders: no valid providers in chain. Deprecated.\n    For verbose messaging see aws.Config.CredentialsChainVerboseErrors. ",
    "jamisonhyatt": "I ran into this in ECS when using task role + host network mode, behind a proxy.\n@srizzling \nI solved it by adding 169.254.170.2 to the no_proxy and NO_PROXY env vars inside the task definition.\n. ",
    "jgimenez": "I'm stupid, I don't know how to read. Thanks.\nI must say this is unintuitive, though, because the credentials file is looked up automatically. I guess you did this for backwards compatibility. If this is the reason, I think it would be good to default it to enabled in a future (API breaking) version.. ",
    "ByteFlinger": "Thank you.\nSo I spent a few hours looking into it and I really would like avoiding\ngoing over to the Java SDK if I can.\nI believe I have an idea of what is needed to implement my own wrapper and\nit looks simple enough but there are a few things I would like to know\nbefore doing that so I don't implement it differently than what might be\nsupported later by you guys.\nIt seems that the KMS solution will do something like this\nEncryption\n\nGenerate Symmetric AES 256 key and encrypt it with an asymmetric key\n(RSA??) hosted inside KMS\nUse Symmetric key to encrypt data and push encrypted data along with\nmetadata including the encrypted Symmetric key and a Wrapper ID so the\nclient can know how to decode it\n\nDecryption\n\nGet S3 object which includes the asymmetric encrypted key and the\nwrapper id\nSend encrypted key to Kms and retrieve decrypted Symmetric key back\nDecrypt data using Symmetric key\n\nIs this about correct?\nSo my question is how would the local asymmetric key flow look like when\nthe lib supports it?\nI could implement a simple wrapper that supplies my own custom ID and keeps\nthe whole Symmetric key part but rather than using Kms, it simply uses a\nlocal private/public key pair to perform the encryption/decryption of the\nsymetric key. Would that be what you would do or is that flow meant to not\nuse the hybrid model at all and just encrypt decrypt using the local\nprivate/public key pair? If the latter what do I store in the encryptedkey\npart of CipherData, ignore it?\nOn Tue, 2 May 2017, 22:17 xibz, notifications@github.com wrote:\n\nHello @ByteFlinger https://github.com/ByteFlinger, currently only kms\nis supported. We have this in our backlog, but I'll mark this as a feature\nrequest. You can also implement your own key wrap handler, if needed. I\nwill bring this up in our next planning meeting. Cheers!\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-go/issues/1241#issuecomment-298748570,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACn1S4DXLuisRgJ242YIGnZ5s-bewceUks5r149hgaJpZM4NOnef\n.\n. @xibz I have taken a look at the whole thing and am left wondering one thing. Do you guys have any specification on what you usually as the label when calling DecryptOAEP/EncryptOAEP?\n\nAlso, how does one upload encrypted data using the s3Manager.NewUploaderWithClient?\nIt seems that the Encryption and Decryption client do not embedding of the S3Client therefore they do not qualify as a client for the NewUploaderWithClient function. I was not able to find any label information on the Java SDK so I am uncertain whether this is a Go implementation or the algorithm itself. Can you confirm that when you mention RSA/ECB/OAEPWithSHA-256AndMGF1Padding, you mean the label that is later required to decrypt the key itself (because apparently according to Go documentation an encrypted value using a certain label can only be decrypted using the same label)?\nI have had some success implementing the whole thing and will be verifying it against the Java SDK. One thing which I am uncertain of if the MGF1 padding. While Go has sha256 which can be used with OAEP, I am not sure where MGF1 fits in the whole picture. I should add that while familiar with PKI I am by no means a crypto expert. I believe that MGF1 padding is already used by Go when using OAEP but uncertain so any guidelines on the matter are welcome (and I suppose any issues will arise when testing against the Java SDK).\nRegarding the s3manager support, I was able to basically wrap the Encryption and Decryption clients in a standard s3.S3 client (since they implement some of the APIs of the s3.S3API) and use that with the s3manager upload and download and it seemed to work although I have not performed any extensive verification. Should I refrain from doing that? Do you think it might be an issue?. Hi\nI spend some time going through everything and digging into the JDK SDK. The example I linked in the original post ends up encrypting using ENCRYPTION-ONLY which seems to use no wrapping algo (Thus RSA/ECB/OAEPWithSHA-256AndMGF1Padding is not used) which the Go SDK cannot decrypt because it complains about not supporting V1 headers.\nSetting encryption mode in the Java SDK to Authenticated solves the issue and it uses RSA/ECB/OAEPWithSHA-256AndMGF1Padding for encryption including V2 headers.\nHaving done all that I cannot seem to verify encryption/decrpytion using the JDK SDK.\nI get the following error when trying to decrypt a go encrypted value in Java\nException in thread \"main\" com.amazonaws.AmazonClientException: Unable to decrypt symmetric key from object metadata\n    at com.amazonaws.util.Throwables.failure(Throwables.java:74)\n    at com.amazonaws.services.s3.internal.crypto.ContentCryptoMaterial.cek(ContentCryptoMaterial.java:297)\n    at com.amazonaws.services.s3.internal.crypto.ContentCryptoMaterial.fromObjectMetadata0(ContentCryptoMaterial.java:422)\n    at com.amazonaws.services.s3.internal.crypto.ContentCryptoMaterial.fromObjectMetadata(ContentCryptoMaterial.java:345)\n    at com.amazonaws.services.s3.internal.crypto.S3CryptoModuleAE.decipherWithMetadata(S3CryptoModuleAE.java:249)\n    at com.amazonaws.services.s3.internal.crypto.S3CryptoModuleAE.decipher(S3CryptoModuleAE.java:150)\n    at com.amazonaws.services.s3.internal.crypto.S3CryptoModuleAE.getObjectSecurely(S3CryptoModuleAE.java:128)\n    at com.amazonaws.services.s3.internal.crypto.CryptoModuleDispatcher.getObjectSecurely(CryptoModuleDispatcher.java:116)\n    at com.amazonaws.services.s3.AmazonS3EncryptionClient.getObject(AmazonS3EncryptionClient.java:575)\n    at com.amazonaws.services.s3.AmazonS3Client.getObject(AmazonS3Client.java:1263)\n    at com.company.Application.main(Application.java:88)\nCaused by: java.security.InvalidKeyException: Unwrapping failed\n    at com.sun.crypto.provider.RSACipher.engineUnwrap(RSACipher.java:445)\n    at javax.crypto.Cipher.unwrap(Cipher.java:2550)\n    at com.amazonaws.services.s3.internal.crypto.ContentCryptoMaterial.cek(ContentCryptoMaterial.java:281)\n    ... 9 more\nCaused by: javax.crypto.BadPaddingException: Decryption error\n    at sun.security.rsa.RSAPadding.unpadOAEP(RSAPadding.java:499)\n    at sun.security.rsa.RSAPadding.unpad(RSAPadding.java:293)\n    at com.sun.crypto.provider.RSACipher.doFinal(RSACipher.java:363)\n    at com.sun.crypto.provider.RSACipher.engineUnwrap(RSACipher.java:440)\n    ... 11 more\nAnd doing the reverse (decrypting Java encrypted value in Go)\ncrypto/rsa: decryption error\nThe key handler implementation Go code is rather simple so I don't quite see where I could be maknig a mistake. Here are some snippets\n```\n// Generate random aesKey\naesKey := make([]byte, 32)\nrand.Read(aesKey)\n// Encrypt with RSA key\ncipherText, err := rsa.EncryptOAEP(sha256.New(), rand.Reader, publicKey, aesKey, nil)\n// Generate random IV.\niv := make([]byte, ivSize)\nrand.Read(iv)\n// s3crypto.CipherData\nCipherData{\n        Key:                 aesKey\n        IV:                  iv,\n        WrapAlgorithm:       \"RSA/ECB/OAEPWithSHA-256AndMGF1Padding\",\n        MaterialDescription: MaterialDescription{},\n        EncryptedKey:        cipherText,\n}\n// Decrypt Key\nrsa.DecryptOAEP(sha256.New(), rand.Reader, privateKey, key, nil)\n// Build encryption/decryption clients\ns3crypto.NewDecryptionClient(session, func(c *s3crypto.DecryptionClient) {\n            c.WrapRegistry = map[string]s3crypto.WrapEntry{\n                RSAWrap: (rsaKeyHandler{}).decryptHandler,\n            }\n}\ns3crypto.NewEncryptionClient(session, s3crypto.AESGCMContentCipherBuilder(rsaKeyGenerator))\n```\nI also tried setting the label to RSA/ECB/OAEPWithSHA-256AndMGF1Padding rather than nil as we talked above but the issues were the same.\nComparing the metadata headers in S3 indicates that they are pretty much the same (Go sets x-amz-meta-x-amz-unencrypted-content-length and x-amz-meta-x-amz-unencrypted-content-md5 which Java does not).\nThe java error seems to tell me that there might be some issue witht the Padding which I also noticed that it is not actually PKCS5Padding as per comments in the code.\nBefore I give this up, would there be any chance you can take a quick look at the above and see if you can spot any issue?\n. I have tested that go can encrypt and decrypt back and forth to/from S3 and I have also tested to\nmake sure it is actually encrypting by looking at the actual data in the S3\nbucket and making sure it is not in clear text in there.\nI do not have this code pushed anywhere and sadly it is not something I can\ndo at the moment but I think the code snippets I posted above covers pretty\nmuch the jist of it. Everything else is boiler plate such as implementing\nthe KeyHandler to fulfill the KeyGenerator interface in pretty much the\nsame way the KMS KeyHandler does but using local keys.\nI guess I'll spend some time writing some unit tests to make sure\neverything is working as intended. Discounting ant potential issues with boiler plate code (like a pointer being used instead of value or something of the sort) do you see any issue with the flow above and the methods being used?\nOn Wed, 17 May 2017, 23:06 xibz, notifications@github.com wrote:\n\n@ByteFlinger https://github.com/byteflinger - With encryption and\ndecryption it is going to be pretty difficult to see whether or not this is\nencrypting/decrypting properly. Do you have some tests in place to see if\nit is encrypting and decrypting properly? I would also run those test\nvectors in Java to ensure you are getting the appropriate values as\nexpected. Have you pushed this code up? I want to take a look at the\noverall implementation to see if I can see anything.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-go/issues/1241#issuecomment-302231641,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACn1S5AFjhWpKhuw38l00P86hH8ofq_Qks5r62FCgaJpZM4NOnef\n.\n. Unfortunately not.\n\nI did not have the time to spend on this a lot more and layed the issue aside for the moment.. @xibz An update on the matter is much appreciated and explains the issue.\nHere's hoping it does not take another year for this feature to be added in Go. ",
    "tfeng": "@ByteFlinger In my project, I experienced the exact same problem as yours. Thanks to your code skeleton, I was able to encrypt/decrypt files in S3 with RSA key pairs. But my Java program gave the exact same error. Did you figure out the reason?. ",
    "rogaha": "Any updates here? Is java the only official SDK available supporting  the Client-Side Master Key method? . ",
    "alwindoss": "Hello @xibz\nWhat I was referring to is that there is no easy way to respond to an auth challenge. Given the user pool details and the client ID client secret along with what is required I should be able to call respond to auth challenge.\nBut, today I have to calculate the key and the secret hash that needs to be returned and it becomes tedious.\nI am not a JavaScript developer, but I only heard from one that this process is simple in JavaScript since ask handles most of it.. @xbiz\nI need to calculate the following to RespondToAuthChallenge and I have no clue how to do it. This ideally should be provided by the SDK right?\n\n\nPASSWORD_CLAIM_SIGNATURE\n\n\nSECRET_HASH. @xibz in the meanwhile could you point me to a documentation that tells me how to calculate these things to respondtoauthchallenge\n\n\nThis would help me unblock myself.. Link: http://docs.aws.amazon.com/cognito/latest/developerguide/getting-started-with-cognito-user-pools.html\nUnder the section \"install the SDK\" it's clearly mentioned the act of sign in is made easy in higher level SDKs. 3 are listed:\n Android\n Javascript\n* iOS\nWhy can't we have this support in golang SDK as well.. @xibz does the service team track your request? Could you share with me the details about this request?. @xibz any updates on this feature from the services team?. Can't we have this part of as this repository itself, instead of waiting on the service team to provide this feature how about the community contribute this feature. I am ready to pick this up, if someone can help me with the direction i need to go.. AWS has released a developer preview for this feature in .Net\nhttps://aws.amazon.com/blogs/developer/cognitoauthentication-extension-library-developer-preview/\nI think it is time even Go developers have similar ease when working with Cognito.. @jasdel  The documentation for QueryPages says that I need to return a boolean flag from the callback depending on whether I want to continue. I would only know if I have to continue based on LastEvaluatedKey right? In this case how will QueryPages take care of this automatically?. @jasdel in that case we need to update the documentation to clearly state this and it would help if we add an example for this case.\nI will attempt to do this if you too think it should be done and I will raise a PR.. @jasdel In fact I had another question along the same lines. Let's say the QueryPages iterates multiple times. Would the QueryOutput object content be replaced or updated?. ",
    "dowlingw": "Full support for the \"Custom\" (read: default) Cognito authentication flow would be good too:\nhttp://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-authentication-flow.html#amazon-cognito-user-pools-admin-authentication-flow\nLet me know if that should be a separate issue, however this is another piece of functionality covered by the Android/JS/iOS SDKs that would be nice to see expanded out.\nopencoff/go-srp seems to be the only SRP implementation in go, but the API seems to contradict the flow as described in the RFC. Not sure on the policy for introducing dependencies into this library anyway, but figured it's worth mentioning for anyone looking at an implementation.. Example of fix in fork (see above), not lodging PR as autogenerated code, etc.\nThanks again :). ",
    "mrichman": "Does anyone know where I can find a Cognito example for Go? I'm looking to implement signup and authentication for a web app I'm writing in Go.. Thanks @jasdel . Does anyone know where I can find a Cognito example for Go? I'm looking to implement signup and authentication for a web app I'm writing in Go.. Stellar idea. I'd use it this week for sure.. This is a dupe of the one I opened a few days ago: https://github.com/aws/aws-sdk-go/issues/1611. ",
    "george-e-shaw-iv": "This might be a service-level request in terms of cogntio, but I really think there needs to be some way (or maybe there is and I just don't know about it/can't find it) to implement social identity providers into server side authentication using AWS.\nWhether that be using the tokens that they return from oauth2 or through some other means, it would definitely be nice to have.. ",
    "bfallik": "@xibz OK, cool. hopefully code-generated means that it can happy relatively quickly. =). ",
    "jhwang09": "I have tried the original solution as proposed by the doc but it wasn't working that's why I start trying different variation.\nthe original doc said that the dbEndpoint should include [scheme]://[host][:port], and the same variable is used for both rdsutils.BuildAuthToken and for the DSN:\ndnsStr = fmt.Sprintf(\"%s:%s@tcp(%s)/%s?tls=true\",\n   dbUser, authToken, dbEndpoint, dbName,\n)\nHowever, I am not sure if the one in DSN actually requires scheme in there\nthe other thing I find odd is the password used, in this case it's the authToken coming back from rdsutils\nand it is of this format \nhost.xxxx.us-east-1.rds.amazonaws.com:3306?Action=connect&DBUser=appuser&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAJHJ4MX7HB6STZ5QQ%2F20170504%2Fus-east-1%2Frds-db%2Faws4_request&X-Amz-Date=20170504T164655Z&X-Amz-Expires=900&X-Amz-Security-Token=FQoDYXdzENr%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDB3PYlunbHE2bh4ylCLWAevRs7cztGGATW3iJm0tpL1J2G%2FsqJjilAlhI2uj6VW%2BWH0txpt2bZ7DgQeZ0lutoJj3rffcznu7o0VIG%2F7L8MXC11BjXOIOEXFwx%2BhEIAqM%2F3v9vpa9Jp1L2xqPBs%2FLuOYmHFxufykYE3D9%2BdoPRp3srEj3AqGbv9Nanw6zRXbsRkAj96VzsAnFTzyTAyOknBUDkWpBzjR%2Fo1Gqdd9gwu6HdJRcp6H%2B9oI0FLrDuQqUfSZez5BUspe4EYDWctSEuNoNREQzzUkkoU2yXB69m4KxA4TyIy4ogLatyAU%3D&X-Amz-SignedHeaders=host&X-Amz-Signature=2b249517f6dc4ad145232cdab3ea59e9b8b20dedad6666b07bbe686e33b6859e\ndoes this look correct? this looks more like a host with bunch of query params, not sure if this is supposed to be the authToken/password used in DSN. see below is the panic\n```\npanic: Error | StdError: dial tcp: address https://testprod.xxx.us-east-1.rds.amazonaws.com:3306: too many colons in address | goroutine 1 [running, locked to thread]:\nruntime/debug.Stack(0x1086de0, 0xc4201ba3c0, 0x0)\n        /var/lib/jenkins/tools/org.jenkinsci.plugins.golang.GolangInstallation/go_1.8/src/runtime/debug/stack.go:24 +0x79\ngithub.com/jhwang09/elmo/errs.NewStdError(0x1086de0, 0xc4201ba3c0, 0xc4201ba3c0, 0x347)\n        /var/lib/jenkins/workspace/go/src/github.com/jhwang09/elmo/errs/errs.go:28 +0x26\ntest/lib/mysql.Config.ConnectViaIAM(0x0, 0x0, 0x0, 0x0, 0xc420013dc0, 0x5, 0xc420013dc5, 0x7, 0xc4200a9520, 0x12, ...)\n        /var/lib/jenkins/workspace/go/src/test/lib/mysql/mysql.go:67 +0x29f\ntest/lib/hdb.init.1()\n        /var/lib/jenkins/workspace/go/src/test/lib/hdb/hdb.go:35 +0x37d\ntest/lib/hdb.init()\n        /var/lib/jenkins/workspace/go/src/test/lib/hdb/hdb.go:64 +0x7b\ntest/model/games.init()\n        /var/lib/jenkins/workspace/go/src/test/model/games/game.go:233 +0x5a\ntest/lib/hutil.init()\n        /var/lib/jenkins/workspace/go/src/test/lib/hutil/url.go:124 +0x70\ntest/middleware.init()\n        /var/lib/jenkins/workspace/go/src/test/middleware/userLoginFilter.go:22 +0x6e\ntest/boot.init()\n        /var/lib/jenkins/workspace/go/src/test/boot/boot.go:136 +0x7a\nmain.init()\n        /var/lib/jenkins/workspace/go/src/test/main.go:32 +0x49\n | info:[map[] Time: 2017-05-04 21:55:56.074731368 +0000 UTC\n.\ndbEndpoint := fmt.Sprintf(\"https://%v:%v\", c.MySQL.Hostname, c.MySQL.Port)\nauthToken, err := BuildAuthToken(dbEndpoint, awsRegion, dbUser, awsCreds)\ndnsStr = fmt.Sprintf(\"%s:%s@tcp(%s)/%s?tls=true\",\n  \"someDBUser\", authToken, dbEndpoint, \"dbName\",\n)\ndb, err := sql.Open(\"mysql\", dnsStr)\n```\nThis is what I currently have, following the doc code snippet. what will be helpful is if you can show me your final dsn, it will be a huge help for me to debug\nthanks. Could you please let me know what the scheme should be? for when we doing the BuildAuthToken? is it https? http? something else?\nI tried to use http/https when build authToken, and in the actual TCP dial to db, I remove the scheme, and this is what I got:\npanic: Error | StdError: x509: certificate signed by unknown authority | goroutine 1 [running, locked to thread]:\nruntime/debug.Stack(0x1087f20, 0xc4200a9520, 0x0)\n        /var/lib/jenkins/tools/org.jenkinsci.plugins.golang.GolangInstallation/go_1.8/src/runtime/debug/stack.go:24 +0x79\ngithub.com/jhwang09/elmo/errs.NewStdError(0x1087f20, 0xc4200a9520, 0xc4200a9520, 0x359)\n        /var/lib/jenkins/workspace/go/src/github.com/jhwang09/elmo/errs/errs.go:28 +0x26\nhoopr/lib/mysql.Config.ConnectViaIAM(0x0, 0x0, 0x0, 0x0, 0xc420013dd0, 0x5, 0xc420013dd5, 0x7, 0xc4200a9540, 0x12, ...)\n        /var/lib/jenkins/workspace/go/src/test/lib/mysql/mysql.go:66 +0x35d\ntest/lib/hdb.init.1()\n        /var/lib/jenkins/workspace/go/src/test/lib/hdb/hdb.go:35 +0x380\ntest/lib/hdb.init()\n        /var/lib/jenkins/workspace/go/src/test/lib/hdb/hdb.go:64 +0x7b\ntest/model/games.init()\n        /var/lib/jenkins/workspace/go/src/test/model/games/publishGame.go:233 +0x5a\ntest/lib/hutil.init()\n        /var/lib/jenkins/workspace/go/src/test/lib/hutil/url.go:124 +0x70\ntest/middleware.init()\n        /var/lib/jenkins/workspace/go/src/test/middleware/userLoginFilter.go:22 +0x6e\ntest/boot.init()\n        /var/lib/jenkins/workspace/go/src/test/boot/boot.go:136 +0x7a\nmain.init()\n        /var/lib/jenkins/workspace/go/src/test/main.go:32 +0x49\n | info:[map[] Time: 2017-05-05 12:23:13.715718397 +0000 UTC\nthis is what the actual DSN looks like:\ndoes this looks correct?\nappuser:testdb.xxx.us-east-1.rds.amazonaws.com:3306?Action=connect&DBUser=appuser&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAI3KBUZ6HRLRTS5MQ%2F20170505%2Fus-east-1%2Frds-db%2Faws4_request&X-Amz-Date=20170505T122313Z&X-Amz-Expires=900&X-Amz-Security-Token=FQoDYXdzEO7%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDJ54WPOcw6PJ4atE6CLWARY%2FB9qZvZ9NI9ZgjJUDbhh%2FwKuvRVLWN5%2F2TgVZ%2F%2F1Fo7Vf3rprSbBeSkR8DIx0YpbkK%2BNrpANfmre6jA0yM13JGvwAJkmD7%2Byt%2Fpjf7wjipBlu%2Fl64rjnqTxcL0%2F0fdic2cjBEk1fJp02%2BOKfRMlJB151A1ckPd9Z1cpJVXf3SraYKoBLu9zQBedwpsUFH%2BpjXiZqUE3H7NbJXvkA%2FjOA9I%2FQWQjkUgLwb5Cf%2F1micbtrIM%2BeoA5zBYkFaT2a1KfiDMQI7dq8IfmOUXwRU%2FsYLtyLZKIUosd2xyAU%3D&X-Amz-SignedHeaders=host&X-Amz-Signature=023c61b185c4c5bb318d277c0a3857e0808344630149d0041cb1ed34727fc88a@tcp(testdb.xxx.us-east-1.rds.amazonaws.com:3306)/dbName?tls=true. to double confirm, my db endpoint is correct, it is consist of [hostname]:[port]\nI am suspecting if I should open my RDS up a bit in its security group?\nit is only allowing \nMYSQL/Aurora\nTCP\n3306\nsg-xxxx (website-sg). on the console, it has two places it says endpoint, one is with port one is without port... which one is correct?. Please refer to my previous post, If I do http or https, below is the panic:\npanic: Error | StdError: dial tcp: address https://testprod.xxx.us-east-1.rds.amazonaws.com:3306: too many colons in address | goroutine 1 [running, locked to thread]:\nruntime/debug.Stack(0x1086de0, 0xc4201ba3c0, 0x0)\n        /var/lib/jenkins/tools/org.jenkinsci.plugins.golang.GolangInstallation/go_1.8/src/runtime/debug/stack.go:24 +0x79\ngithub.com/jhwang09/elmo/errs.NewStdError(0x1086de0, 0xc4201ba3c0, 0xc4201ba3c0, 0x347)\n        /var/lib/jenkins/workspace/go/src/github.com/jhwang09/elmo/errs/errs.go:28 +0x26\ntest/lib/mysql.Config.ConnectViaIAM(0x0, 0x0, 0x0, 0x0, 0xc420013dc0, 0x5, 0xc420013dc5, 0x7, 0xc4200a9520, 0x12, ...)\n        /var/lib/jenkins/workspace/go/src/test/lib/mysql/mysql.go:67 +0x29f\ntest/lib/hdb.init.1()\n        /var/lib/jenkins/workspace/go/src/test/lib/hdb/hdb.go:35 +0x37d\ntest/lib/hdb.init()\n        /var/lib/jenkins/workspace/go/src/test/lib/hdb/hdb.go:64 +0x7b\ntest/model/games.init()\n        /var/lib/jenkins/workspace/go/src/test/model/games/game.go:233 +0x5a\ntest/lib/hutil.init()\n        /var/lib/jenkins/workspace/go/src/test/lib/hutil/url.go:124 +0x70\ntest/middleware.init()\n        /var/lib/jenkins/workspace/go/src/test/middleware/userLoginFilter.go:22 +0x6e\ntest/boot.init()\n        /var/lib/jenkins/workspace/go/src/test/boot/boot.go:136 +0x7a\nmain.init()\n        /var/lib/jenkins/workspace/go/src/test/main.go:32 +0x49\n | info:[map[] Time: 2017-05-04 21:55:56.074731368 +0000 UTC\nI tried the static credentials and it works fine. I am almost to a point to give up using the IAMAuth because it's too much pain.... How do you test STS credentials w/o being on an EC2 host? I tried to assign the role to an user, but it isn't allowing user to assume role for some reason.\nThis is failing on when we do db.Ping(). \nMy application basically can't start because of unable to do ping on startup. looks like someone else already does it https://github.com/aws/aws-sdk-go/pull/2131\nclosing. ",
    "fosini": "Same problem. The documentation truly is a mess.\n  . ",
    "sujunzhu": "Same :(. ",
    "sheeley": "Hi,\nI believe I've managed to solve this - it does appear to me that the docs are missing a couple of things.\n1. When passing in endpoint, you must include the port\n2. When creating the db connection, you must register the RDS x509 cert & pass in a specific TLS config\n1:\ngo\npasswd, err := rdsutils.BuildAuthToken(fmt.Sprintf(\"%s:%d\", host, 3306), region, cfg.User, creds)\n2:\n```go\nfunc RegisterRDSMysqlCerts(c *http.Client) error {\n    resp, err := c.Get(\"https://s3.amazonaws.com/rds-downloads/rds-combined-ca-bundle.pem\")\n    if err != nil {\n        return errs.Wrap(err)\n    }\ndefer fileutil.CloseLoggingAnyError(resp.Body)\npem, err := ioutil.ReadAll(resp.Body)\nif err != nil {\n    return errs.Wrap(err)\n}\n\nrootCertPool := x509.NewCertPool()\nif ok := rootCertPool.AppendCertsFromPEM(pem); !ok {\n    return errs.New(\"couldn't append certs from pem\")\n}\n\nerr = mysql.RegisterTLSConfig(\"rds\", &tls.Config{RootCAs: rootCertPool, InsecureSkipVerify: true})\nif err != nil {\n    return errs.Wrap(err)\n}\nreturn nil\n\n}\n```\nA full working example (you'll need to fill in the appropriate information for your user/credentials/etc):\n```go\npackage main\nimport (\n    \"crypto/tls\"\n    \"crypto/x509\"\n    \"database/sql\"\n    \"fmt\"\n    \"io/ioutil\"\n    \"net/http\"\n\"github.com/aws/aws-sdk-go/aws/credentials\"\n\"github.com/aws/aws-sdk-go/service/rds/rdsutils\"\n\"github.com/go-sql-driver/mysql\"\n\"github.com/richardwilkes/errs\"\n\"jaxf-github.fanatics.corp/forge/furnace/fileutil\"\n\n)\nfunc RegisterRDSMysqlCerts(c *http.Client) error {\n    resp, err := c.Get(\"https://s3.amazonaws.com/rds-downloads/rds-combined-ca-bundle.pem\")\n    if err != nil {\n        return errs.Wrap(err)\n    }\ndefer fileutil.CloseLoggingAnyError(resp.Body)\npem, err := ioutil.ReadAll(resp.Body)\nif err != nil {\n    return errs.Wrap(err)\n}\n\nrootCertPool := x509.NewCertPool()\nif ok := rootCertPool.AppendCertsFromPEM(pem); !ok {\n    return errs.New(\"couldn't append certs from pem\")\n}\n\nerr = mysql.RegisterTLSConfig(\"rds\", &tls.Config{RootCAs: rootCertPool, InsecureSkipVerify: true})\nif err != nil {\n    return errs.Wrap(err)\n}\nreturn nil\n\n}\nfunc main() {\n    // FILL THESE OUT:\n    host := \"\"\n    user := \"\"\n    dbName := \"\"\n    creds := credentials.NewSharedCredentials(\"\", \"\")\nregion := \"us-east-1\"\n\nhost = fmt.Sprintf(\"%s:%d\", host, 3306)\ncfg := &mysql.Config{\n    User: user,\n    Addr: host,\n    Net:  \"tcp\",\n    Params: map[string]string{\n        \"tls\": \"rds\",\n    },\n    DBName: dbName,\n}\ncfg.AllowCleartextPasswords = true\n\nvar err error\ncfg.Passwd, err = rdsutils.BuildAuthToken(host, region, cfg.User, creds)\n\nif err != nil {\n    panic(err)\n}\n\nerr = RegisterRDSMysqlCerts(http.DefaultClient)\nif err != nil {\n    panic(err)\n}\n\ndb, err := sql.Open(\"mysql\", cfg.FormatDSN())\nif err != nil {\n    panic(err)\n}\n\nerr = db.Ping()\nif err != nil {\n    panic(err)\n}\nfmt.Println(\"ok\")\n\n}\n```. One additional tidbit - if you fail to provide a port in the connection string, you'll encounter this: https://github.com/go-sql-driver/mysql/issues/717. ",
    "nextdimension": "Oh, thank godness, theres an answer! I was going out of my mind :D Thanks.. ",
    "hongchaodeng": "Hi @jasdel Thanks for replying. I have looked into all three methods you mentioned. That still does not work. Here's why:\n\nSession constructor takes an option where you can specify the shared credentials file profile to use.\n\nThe multi-tenancy model above is per profile, but not per account.\n\nAlternatively you can create a SharedCredentials credential provider and assign that to the Config when creating the service client. \n\nActually we are doing this as a workaround. The problem with this is that it couldn't get the \"config\" file, the file referred to by env AWS_CONFIG_FILE. Currently we need to pass in parameters like \"region\" to work around.\n\ndefine your own credentials provider with the Provider interface\n\nThis has the same issue as using NewSharedCredentials().\nFeel free to ping me if you need ask more info.\n. > Could you clarify this? The profile is just a name that is used to group AWS credentials in the shared credentials file. Each shared credentials file can contain multiple profiles. In addition the credentials files can contain profiles for multiple accounts. Profiles do not need to have any association with one another.\nOur model looks like:\n\nEach user will pass in his AWS credentials when registering. This is dynamic and user accounts could not be known ahead.\nEven though this shared credentials file could contain multiple accounts, does that mean we need to generate all accounts, profiles ahead?\nWhat's more, it's scary for different users to share one file.\n\nAh this makes sense, the SDK does not read the shared configuration file by default, only the credentials file. The Sessions from Shared Config section of the session package docs should be helpful here. The AWS_SDK_LOAD_CONFIG=1 environment variable needs to be set, or NewSessionWithOption used when creating a Session with SharedConfigState set to session.SharedConfigEnable.\n\nThe AWS_SDK_LOAD_CONFIG=1 needs load shared file or use shared env, which could not be achieved if we create multiple clients in one program.\n\nCould you go into more details about this? Is this setting the Region parameter of the SDK's Config type? If the SDK's shared configuration support mentioned above ...\n\nYou are right but this is not achievable for shared environment. Ideally, we want to read individual config file not from ENV, but from a param in config:\n```go\ncfg := aws.Config{\n  CredentialsFile: ...\n  ConfigFile: ...\n}\nsession.NewSession(cfg)\n... or ...\nsession.NewSession(credsFile, cfgFile)\n``\nWhile we can load credentials file usingNewSharedCredentials(), we can't load the \"config\" file like this. If we use env AWS_CONFIG_FILE, this could not allow creating multiple clients.. Isn't it valid to create client from credentials and config without sharing ENV? This SDK has providedNewSharedCredentials()` to parse credentials file. Isn't it OK to also provide methods to parse config file?. Sure. Thanks for the information.\nFor now, we are trying to modify this line of code:\nhttps://github.com/aws/aws-sdk-go/blob/301ea4de9fcd6f424fd15d5b01e5516d14acec35/aws/session/session.go#L309\nto change envCfg.SharedConfigFile to some param and export it to outside.\nIt would be great if this official SDK could provide it too :). > If your application deals with multiple shared configuration files throughout the lifetime of the application another solution would be needed.\nYeah. This is the context I have been talking in.. Hi @jasdel .\nCurrently we are trying to work around this by locking on shared env, e.g. AWS_CONFIG_FILE.\nBasically, the code looks like:\n```Go\n// Assume that this method do not have side effect\nfunc createS3Client() ... {\n  mutex.Lock()\n  defer mutex.Unlock()\nos.Setenv(\"AWS_SHARED_CREDENTIALS_FILE\", \"...\")\n  defer os.Unsetenv(\"AWS_SHARED_CREDENTIALS_FILE\")\nos.Setenv(\"AWS_CONFIG_FILE\", \"...\")\n  defer os.Unsetenv(\"AWS_CONFIG_FILE\")\nsession.NewSession()\n  ...\n}\n```\nIs it true that the credentials and config file will only be used once and won't be reloaded?\nNote that this is just a work-around. After this, we would like the official SDK to export functionality to pass in config file name. Are you going to work on this? Or are you fine to assign me to take care of this issue?\nThanks in advance!. > I think the best way to support this is to update the session.Option struct to have an additional optional member which is a list of files the user wants to load the credentials from.\n\nsess := session.Must(session.NewSessionWithOptions(session.Options{\n    // Ordered list of files the SDK will load configuration from\n    SharedConfigFiles: []string{file1, file2, file3}, \n}))\n\nWill do that.\n\nIn doing so the we'd probably also want to export the sharedCredentialsFilename and sharedConfigFilename functions.\n\nI'm not catching it. Can you clarify?. @jasdel \nAny ETA for coming release?. Test added. PTAL. ",
    "kr": "\nOut of curiosity is your code attempting to put to the bucket as soon as the CreateBucket API call returns?\n\nNo, I created the bucket manually on the AWS console, then a few minutes later attempted to put an object into it. As I mentioned, putting an object on the console or with the CLI worked fine, but subsequent attempts to put an object using the AWS Go SDK still failed over the course of many minutes.\nI tried the same PUT call on the same bucket with the Go SDK again the next day (today) and it worked.. Awesome, thank you so much!\n(It looks like you don't need the RequestID, but in case you ever do for whatever reason, there's one in my original message above. And yep, I was/am using Go 1.8.). ",
    "grubernaut": "Hey @jasdel, thanks for the responses! \nJust to note, we're also seeing this issue with Kinesis Firehose Delivery Streams, as noted here: https://github.com/hashicorp/terraform/issues/11143. \nIf there's any other information or debug output needed, please let me know. Thanks for the help on this one :) . Works perfectly, thanks for the quick turnaround on this @jasdel. :tada: . That would make sense, however, I don't believe that's the case. We're initializing the client with the correct region as specified when we parse a users configuration. \nhttps://github.com/hashicorp/terraform/blob/master/builtin/providers/aws/config.go#L242\nAnd in the test example that I'm using, I'm specifying the region during the provider initialization to be eu-west-1, which is what we initialize the aws config object with. \n```\nprovider \"aws\" {\n  region = \"eu-west-1\"\n}\nresource \"aws_lightsail_instance\" \"foo\" {\n  name = \"foo-bar\"\n  availability_zone = \"eu-west-1a\"\n  blueprint_id = \"joomla_3_6_5\"\n  bundle_id = \"nano_1_0\"\n}\n```\nEven the addition of a debug log confirms that we're creating the client with the region of eu-west-1: \n[DEBUG] Setting AWS Region to: eu-west-1\nI believe this is still an issue. Thanks\n. :man_facepalming: \nI wrote a similar test, and also couldn't reproduce the issue. So I dove back into our configuration code, and found the issue. \nhttps://github.com/hashicorp/terraform/blob/master/builtin/providers/aws/config.go#L371\nIs hard-setting the region of the session to us-east-1 regardless of what the initial configuration was: \nhttps://github.com/hashicorp/terraform/blob/master/builtin/providers/aws/config.go#L277\nSorry to waste your time again with this one, and thanks for the help here. . hey @diehlaws, thanks for the reply. Oddly enough, I can get a single ASG to modify, and check via code above with no problem.However, when I attempt to modify over 900 ASGs concurrently, the ASGs with no instances fail with no response. :man_shrugging: Running the request serially works just fine, unfortunately can't debug it further as there are no more ASGs to modify now. Requests were definitely issued, mutex was working correctly to the best of my knowledge. Was probably user error, but just out of curiosity, is the SuspendProcesses() request rate limited in any way? \nThanks for your help on this as well, going to close as it's no longer an issue. :100: . Nevermind, found the issue with my go-routines. :man_facepalming: Thanks again for the help . ",
    "thomas91310": "@jasdel are you suggesting to have the sdk execute the S3.HeadObject operation on every downloaded object?\nI was trying to implement the callback on the Downloader object but there is nowhere to get the input *s3.GetObjectInput since this input is being passed as a parameter of the Downloader.Download method (my idea was that I can then use that input to construct a *s3.HeadObjectInput and then make a call to d.S3.HeadObject).\nOn the other hand, I have an implementation with a new Download method.\n```golang\n//DownloadWithMetadata with metadata from downloaded object\nfunc (d Downloader) DownloadWithMetadata(w io.WriterAt, input s3.GetObjectInput, options ...func(Downloader)) (n int64, err error, downloadedObject s3.HeadObjectOutput) {\n    downloadedObject, err := d.S3.HeadObject(&s3.HeadObjectInput{\n        Bucket: input.Bucket,\n        Key:    input.Key,\n    })\n        //not handling the error right now\n    if err != nil {\n        downloadedObject = nil\n    }\nn, err = d.Download(w, input, options...)\n\nreturn n, err, downloadedObject\n\n}\n```\nand this is an example of using it:\n```golang\nfunc main() {\n    sess := session.Must(session.NewSession(&aws.Config{\n        Region: aws.String(\"us-east-1\"),\n    }))\ntmpfile, err := ioutil.TempFile(\"/tmp\", \"thomas_test\")\nif err != nil {\n    panic(err)\n}\ndefer os.Remove(tmpfile.Name())\ndefer tmpfile.Close()\n\ndl := s3manager.NewDownloader(sess)\ndl.Concurrency = 5\n\n    n, err, downloadedOutput := dl.DownloadWithMetadata(tmpfile, &s3.GetObjectInput{\n    Bucket: aws.String(\"hello\"),\n    Key:    aws.String(\"hello.csv\"),\n})\nif err != nil {\n    //do something\n}\n\nfmt.Println(\"dl bytes: \", n)\nfmt.Println(\"object: \", downloadedOutput)\n\n}\n```\nI just want to get some feedback on this to see if I'm going in the right direction or not.\nIf I'm going in the right direction:\n- Is there anything else that needs to be passed from the *s3.GetObjectInput to the *s3.HeadObjectInput besides Bucket and Key?\n- Should a new type be returned from DownloadWithMetadata instead of a *s3.HeadObjectOutput?\n- What do I do if my HeadObject call returns an error, just return it back to the client?. ",
    "shaunc": "How about an interface such as https://docs.aws.amazon.com/sdk-for-go/api/aws/request/#WithGetResponseHeader ? ... the existing options ...func(*Downloader) in the signature could be overloaded to accept metadata reader, either retrieving specific keys or all of it?\nUPDATE -- Wait -- don't we already have WithDownloaderRequestOptions?. I'm still having problems with this (using modules):\n```\n$ go version\ngo version go1.11.3 darwin/amd64\n$ go get github.com/jmespath/go-jmespath@latest\ngo: finding github.com/jmespath/go-jmespath latest\ngithub.com/jmespath/go-jmespath\n/Users/shauncutts/pkg/mod/github.com/jmespath/go-jmespath@v0.0.0-20180206201540-c2b33e8439af/api.go:8:7: undefined: ASTNode\n/Users/shauncutts/pkg/mod/github.com/jmespath/go-jmespath@v0.0.0-20180206201540-c2b33e8439af/api.go:9:8: undefined: treeInterpreter\n$ go get github.com/jmespath/go-jmespath@v0.0.0-20160202185014-0b12d6b521d8\n$\n```\nand the latest one seems to be loaded by default by both v1 and v2 of the sdk.. @jasdel -- In fact, that worked!\nSorry -- I'm a go newbie... if this were npm I would have thought of the analogous on my own. :)\nThank you!. ",
    "FrenchBen": "bleh I searched for Unmarshal and missed the issue you linked to - Closing. \nThanks for the prompt reply. ",
    "zmarois": "Wow, that is an awesome testing feature. Thanks for suggesting it. \nI think it should be simple enough to write and use travis to actually run it, so pardon me if I kick off a couple builds just trying to get it working. If I fail after a few commits, I'll actually get a local build working.. Nice catch. Thank you for consolidating and fixing the problem deeper for me. . ",
    "sandoracs": "Thank you!. Hi! Thank you very much! \ud83d\udc4d . Hello! Thank you for help. The object is 303MB, but I can test with different sizes as well.. I see. Thank you!. ",
    "timk10": "ok, thanks!. ",
    "myshen": "Sorry for the noise, this ended up looking more like a problem with downstream using an SDK version older than 1.8.0.. ",
    "anupavanm": "Thank you. It's Working.. After i removed / at the end of url. ",
    "fxaguessy": "Many thanks @jasdel for this great answer. This confirms what I thought, but your mock will be a good start for the implementation of the cache of credentials in awless.\nI'm not sure to have fully understood this:\n\nWrapping the Provider instead of the Credentials adds a significant benefit as it removes the synchronous locking overhead the Credentials type uses to ensure it is safe to use service clients across multiple goroutines.\n\nBut I will start the implementation and if I find problems, I will come back and discuss it here.. Thanks @jasdel for the suggestion. This is indeed a useful feature, and we might integrate that into awless, in addition to STS credentials caching. The main limitation of Go 1.8's plugins for now is that it only works on Linux, but this might interest some users, for sure.\nI will let you know if I have some feedback on this feature.. Hi @jasdel, I started a POC implementation of the FileCacheProvider, beginning with the code you provided above (thanks again !). \nIt is a good start, as it successfully caches MFA credentials during multiple awless commands. However, I don't know how to check if/when the credential is expired (cf. https://github.com/wallix/awless/commit/4048c5ec72deb925c280f8ea870bc410e8ed2284#diff-fbe390e32c27fafd0343c9684ffd6686R161). As I get credentials from f.Creds.Get(), stored in a credentials.Value, I can not get back to the original provider with the expiration property.\nDid I miss something or should I do a query to AWS, catching the RequestExpired: Request has expired error ? As a hack, I could also store time.Now().Add(stscreds.DefaultDuration) as expiration date, but it doesn't seems to be a good solution either.. Yes exactly, the problem is for credentials that have been cached. . Yes, indeed, such an Expirer interface would allow to know the expiration date, before caching the credential. That would be a great help for my use case.. Hello,\nI just had the same problem a few days ago. You also need to specify the cluster in \ngo\n&ecs.DescribeTasksInput{\n    Tasks: result.TaskArns,\n        Cluster: aws.String(cluster),\n}\nC.f. the documentation, if you do not specify the cluster in DescribeTasks, it will take the default cluster.. ",
    "posilva": "\nI wrote this small library that uses some sort of in memory \"cache\" of MFA:  https://github.com/posilva/go-mfa/\nI will extended it to cache locally the credentials to be easy to reuse without having to enter the MFA while the session token is not expired.\n\nPR, comments and issues are welcome \n. ",
    "llamahunter": "The aws-iam-authenticator seems like it could use an implementation of a disk based credentials cache.  I'm unclear why they can't use ~/.aws/credentials, tho.  Or why this isn't built in to the aws-sdk-go package.. See PR #2375 for an implementation of the above discussed Expirer interface on Providers.  I've implemented a persistent credential cache in the aws-iam-authenticator project built on aws-sdk-go using this functionality.. See https://github.com/kubernetes-sigs/aws-iam-authenticator/pull/193 for an example of writing an application level credential cache using aws-sdk-go.. @jasdel could you take a look at this PR?  Needless to say, it would be great to get it accepted and released ASAP so that I can submit a pull to the aws-iam-authenticator project that depends on this.  Would rather not hand vendor this directly in their project.. It would be great to get this accepted and released soon.  It's blocking my ability to create a pull request for aws-iam-authenticator fixing the lack of SSO credential caching there.. Let me know if you want me to squash these changes together.. Any chance this pull could be accepted soon?. But.... no one calls Credentials.Expire() or sets forceRefresh to true after Credentials.Get() is called. It seems unused in any non testing/example part of the SDK.  Unimplemented feature?. Or is this API for applications written on top of the SDK to force credential expiration?  Seems odd to push that up out of the SDK, tho.. apologies... I somehow missed that one when searching. So, if I'm caching credentials at a higher level in the calling program, and the credential I'm using is expired, the AfterRetryHandler will call Expire on the Credential, and then... it looks like... automatically retry?  I don't think I will need to do anything special so long as I support the forceRetry flag on ExpiresAt(). It will be caching credentials.Value to disk, and only calling Credentials.Get() if the on disk cache of the credentials.Value has an expiration time that is in the past.  This is per your suggestions in issue #1329. . Also, FWIW, it seems to work fine.  Already have implemented it for aws-iam-authenticator.  Just waiting for this pull request to go in before submitting a pull to that project.. HNY! What's the schedule look like for pulling this and publishing a new release including it?. Hmm... rather than using reflection might be unnecessary here, as c.creds.ProviderName exists.. It wasn't clear to me that forceRefresh (i.e. Credentials.Expire()) is actually used outside of test programs or example programs.  But, can check forceRefresh, and return a zero time.Time{} in that case.. ",
    "davyzhang": "Thanks @xibz for your quick reply. Here's the full code to reproduce the problem.\n```golang\npackage main\nimport (\n    \"fmt\"\n    \"os\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/awserr\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/ecs\"\n\n)\nfunc main() {\n    cluster := os.Args[1]\nsvc := ecs.New(session.New())\n\ninput := &ecs.ListTasksInput{\n    Cluster: aws.String(cluster),\n}\n\nresult, err := svc.ListTasks(input)\nif err != nil {\n    if aerr, ok := err.(awserr.Error); ok {\n        switch aerr.Code() {\n        case ecs.ErrCodeServerException:\n            fmt.Println(ecs.ErrCodeServerException, aerr.Error())\n        case ecs.ErrCodeClientException:\n            fmt.Println(ecs.ErrCodeClientException, aerr.Error())\n        case ecs.ErrCodeInvalidParameterException:\n            fmt.Println(ecs.ErrCodeInvalidParameterException, aerr.Error())\n        case ecs.ErrCodeClusterNotFoundException:\n            fmt.Println(ecs.ErrCodeClusterNotFoundException, aerr.Error())\n        case ecs.ErrCodeServiceNotFoundException:\n            fmt.Println(ecs.ErrCodeServiceNotFoundException, aerr.Error())\n        default:\n            fmt.Println(aerr.Error())\n        }\n    } else {\n        // Print the error, cast err to awserr.Error to get the Code and\n        // Message from an error.\n        fmt.Println(err.Error())\n    }\n    return\n}\n\nfmt.Println(result)\ninput2 := &ecs.DescribeTasksInput{\n    Tasks: result.TaskArns,\n}\n\nresult2, err := svc.DescribeTasks(input2)\nif err != nil {\n    if aerr, ok := err.(awserr.Error); ok {\n        switch aerr.Code() {\n        case ecs.ErrCodeServerException:\n            fmt.Println(ecs.ErrCodeServerException, aerr.Error())\n        case ecs.ErrCodeClientException:\n            fmt.Println(ecs.ErrCodeClientException, aerr.Error())\n        case ecs.ErrCodeInvalidParameterException:\n            fmt.Println(ecs.ErrCodeInvalidParameterException, aerr.Error())\n        case ecs.ErrCodeClusterNotFoundException:\n            fmt.Println(ecs.ErrCodeClusterNotFoundException, aerr.Error())\n        default:\n            fmt.Println(aerr.Error())\n        }\n    } else {\n        // Print the error, cast err to awserr.Error to get the Code and\n        // Message from an error.\n        fmt.Println(err.Error())\n    }\n    return\n}\n\nfmt.Println(result2)\n\n}\n```\nThe task arns is directly pulled out from listTask, so there should be valid, and the listTasks returns no error.\n. ",
    "xanderdunn": "@xibz Thanks for your help.  \nI just checked to make sure: The S3 bucket and objects, as well as the KMS key are in us-west-2.  . @xibz Ooohhhh.  I didn't realize the difference.  That is indeed the problem!  Thanks!. ",
    "jamesdbowman": "Thanks!. ",
    "nanjekyejoannah-zz": "Dont you think it would be good if we provided some sort of validation for especially obvious cases where some one uses a negative value for max values because a max value can logically be 0 or more than zero. You can let me know anything I may be missing here. I can take it like a normal way the SDK works and not test for this.\nI have seen a similar issue was solved somewhere with this solution in a different SDK.. @xibz Then in this case does  it mean that this is a validation that I that is using the SDK should do in my logic some how?. Thanks @xibz for the insight..forgive my many questions. I just started using this SDK recently. Now AWS has an error message BucketAlreadyexists among other messages like NoSuchBucket . The later is returned in cases where you are accessing a bucket that does not exist. I have written tests where the AWS go SDK has validated and returned this (NoSuchBucket). In this case I tried deleting a bucket that does not exist. Which ideal situation will make the AWS GO SDK return me the BucketAlreadyexists error because I thought there is validation the SDK does to return this error.. ",
    "zach5410": "\nThis is what I am getting. ",
    "TalLevAmi": "Thanks for the quick response!\nHowever, I believe the issue is not fully resolved, see https://play.golang.org/p/BqtevOQfM8. ",
    "sethcleveland": "I'll work on adding that debug logging. It'll take some time and effort to repro the issue. Until I can grab those logs, because the signing is abstracted from the user, are there expectations on time or environment with signing? In other words, is signing idempotent?. With regards to the idempotent question, I'm trying to identify some leads to follow until this is root caused. I've been using the SDK for a while and haven't seen this error before. So, I'm somewhat lost trying to identify the problem. Adding in the logging.... \ud83d\udc4d I turned on the detailed logging. I'll share it when I capture the problem again. Is there a better way to share that info to you'll out side of a public forum?. I tracked down some examples with the SignatureDoesNotMatch error. I'm assuming you'll only need the error xml. I'm not clear what's all in there, So, at the moment, I'm most comfortable sending that through AWS's private channel. How do I accomplish that?\nThe SDK version is 1.8.38 and with regards to the code, it's more or less the following...\ngo\n    in := &s3.PutObjectInput{\n        Bucket:             aws.String(bucket),\n        Key:                aws.String(key),\n        Body:               bytes.NewReader(body),\n        ContentType:        contentType,\n        ContentDisposition: contentDisposition,\n        ContentEncoding:    contentEncoding,\n    }\n    _, err := svc.PutObjectWithContext(ctx, in). Putting some closure on this. The SignatureDoesNotMatch error hasn't showed up for the past week. I did upgrade the client to 1.10.8 too. Thanks for the assistance.. ",
    "amongil": "Thanks very much. I should've known better ;). ",
    "bboreham": "To illustrate what happens in one particular case, the container has a /etc/resolv.conf as follows:\nnameserver 10.0.0.10\nsearch cortex.svc.cluster.local svc.cluster.local cluster.local\noptions ndots:5\nand when it looks for dynamodb.us-east-1.amazonaws.com the following 10 DNS requests are issued:\n```\ntcpdump -n -c 100 -i ens3 port 53 | grep '.53:.*dynamodb'\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on ens3, link-type EN10MB (Ethernet), capture size 262144 bytes\n16:48:54.516454 IP 10.244.229.62.36333 > 10.244.204.93.53: 37258+ A? dynamodb.us-east-1.amazonaws.com.cortex.svc.cluster.local. (75)\n16:48:54.516455 IP 10.244.229.62.59004 > 10.244.254.213.53: 18060+ AAAA? dynamodb.us-east-1.amazonaws.com.cortex.svc.cluster.local. (75)\n16:48:54.517638 IP 10.244.229.62.52850 > 10.244.204.93.53: 50231+ AAAA? dynamodb.us-east-1.amazonaws.com.svc.cluster.local. (68)\n16:48:54.517685 IP 10.244.229.62.46981 > 10.244.254.213.53: 24523+ A? dynamodb.us-east-1.amazonaws.com.svc.cluster.local. (68)\n16:48:54.518878 IP 10.244.229.62.39936 > 10.244.204.93.53: 8874+ AAAA? dynamodb.us-east-1.amazonaws.com.cluster.local. (64)\n16:48:54.518887 IP 10.244.229.62.41833 > 10.244.254.213.53: 42850+ A? dynamodb.us-east-1.amazonaws.com.cluster.local. (64)\n16:48:54.520119 IP 10.244.229.62.49066 > 10.244.254.213.53: 52171+ AAAA? dynamodb.us-east-1.amazonaws.com.ec2.internal. (63)\n16:48:54.520244 IP 10.244.229.62.54248 > 10.244.254.213.53: 22530+ A? dynamodb.us-east-1.amazonaws.com.ec2.internal. (63)\n16:48:54.522510 IP 10.244.229.62.53087 > 10.244.254.213.53: 23036+ AAAA? dynamodb.us-east-1.amazonaws.com. (50)\n16:48:54.522573 IP 10.244.229.62.37583 > 10.244.254.213.53: 27640+ A? dynamodb.us-east-1.amazonaws.com. (50)\n```\nif you add the trailing . to signify that the address is absolute, the host lookup will only do the last two (A is ipv4 and AAAA is ipv6).\n. @jasdel thanks for very useful info, and I apologise I have not been back to this; I was on vacation lately.\nIt seems pretty clear to me that DNS lookups are not being cached, and I am under the impression that we are not getting connections kept alive either.  But we do run a lot of DynamoDB operations in parallel (like, hundreds).\nWhen I get some time I will investigate the behaviour in more detail.. ",
    "mtestrot": "Ok. Thanks for your quick reply. I didn't find the existing issue, so sorry for posting it again.. I store and pass around the location as a reference to retrieve the file later on. So everything the using application has to know is the reference in order to Http GET the object. For the using application It's unnecessary to know the construction rule for the url and it's unnecessary to know that the object is stored in S3.. Yes. The URL works as soon as we do a string replace before passing it around:\nreference = strings.Replace(output.Location,\"%2F\",\"/\",-1)\nOf course there will be a Problem if the URL contains a part which has to be URL encoded. For example if a path-segment from the S3 Object Key is build from a URL. But this is currently no use case for us. So the workaround works.\n. Hi,\nI gathered some code for an isolated test case:\n```\npackage awsissue\nimport (\n    \"os\"\n    \"time\"\n    \"fmt\"\n    \"testing\"\n    \"io/ioutil\"\n    \"log\"\n    \"math/rand\"\n    \"github.com/aws/aws-sdk-go/aws\"\n    \"bufio\"\n    \"github.com/aws/aws-sdk-go/service/s3/s3manager\"\n    \"github.com/aws/aws-sdk-go/aws/session\"\n    \"github.com/aws/aws-sdk-go/aws/signer/v4\"\n    \"net/http\"\n)\nvar (\n    sess     = session.Must(session.NewSession())\n    uploader = s3manager.NewUploader(sess)\n)\nfunc NewTempFile(size int, parentDir string, blockSize int) string {\n    tmpfile, err := ioutil.TempFile(parentDir, \"test\")\n    if err != nil {\n        log.Fatal(err)\n    }\nb := make([]byte, blockSize)\nfor i := 0; i < size; i++ {\n    for x := 0; x < blockSize; x++ {\n        b[x] = byte(rand.Int())\n    }\n\n    if _, err := tmpfile.Write(b); err != nil {\n        log.Fatal(err)\n    }\n}\nif err := tmpfile.Close(); err != nil {\n    log.Fatal(err)\n}\n\nreturn tmpfile.Name()\n\n}\nfunc store(filepath string) (string, error) {\n    file, err := os.Open(filepath)\n    defer file.Close()\n    if err != nil {\n        return \"\", err\n    }\noutput, err := uploader.Upload(&s3manager.UploadInput{\n    Bucket: aws.String(\"bucket\"),\n    Key:    aws.String(fmt.Sprintf(\"a/b/%d\", time.Now().UnixNano())),\n    Body:   bufio.NewReader(file),\n})\n\nif err != nil {\n    return \"\", err\n}\n\nreference := output.Location\n// Uncomment the following line to fix the issue\n//reference = strings.Replace(reference, \"%2F\", \"/\", -1) // Workaround for https://github.com/aws/aws-sdk-go/issues/1385\nreturn reference, nil\n\n}\nfunc TestLocation(t testing.T) {\n    // at least 5MB to reproduce the issue\n    filepath := NewTempFile(5, \"\", 10241024)\n    defer os.Remove(filepath)\nlocation, err := store(filepath)\nif err != nil {\n    t.Fatal(err)\n}\n\nreq, err := http.NewRequest(\"GET\", location, nil)\nif err != nil {\n    t.Fatal(err)\n}\nsigner := v4.NewSigner(sess.Config.Credentials)\n_, err = signer.Sign(req, nil, \"s3\", *sess.Config.Region, time.Now())\nif err != nil {\n    t.Fatal(err)\n}\nclient := &http.Client{}\nresp, err := client.Do(req)\nif err != nil {\n    t.Fatal(err)\n}\nif resp.StatusCode != 200 {\n    t.Fatalf(\"Status was %d\", resp.StatusCode)\n}\n\n}\n```\n. Hi,\nsorry for the late response. I have been out of office.\nThe reason I'm building the request by myself is the reduced dependency for the GET part of the TestLocation function. In production the program which stores the S3 object is different from the program which retrieves/gets the object. So the GET part  - apart from the signer - has no dependency on the sdk.. ",
    "philip-firstorder": "I also got this problem, see here:\nOriginal key I use on the uploader is this:\n5bacdea9f893e03936044e3a/2018-09-27 - archive.zip\nOn s3 it gets saved to this:\n5bacdea9f893e03936044e3a/2018-09-27+-+archive.zip\nSmaller files return this (spaces are encoded):\n5bacdea9f893e03936044e3a/2018-09-27%20-%20archive.zip\nLarge files return this (slash is encoded):\n5bacdea9f893e03936044e3a%2F2018-09-27+-+archive.zip\n. ",
    "ktravelet": "Just bumped into this as well.  We are saving backups to S3.  I'm not sure what the threshold is but very small backups (< 1MB) don't encode while our medium (~14 MB) and larger (>100MB) size backups encode.. ",
    "the1337beauty": "@xibz thank you, I had a call yesterday with some AWS folks to discuss my experience with Cognito and I brought this up in that call.. Is there any update on this? Will it be implemented in Go SDK v2? @jasdel ?. I had found another way to paginate but thank you for reaching out @BastienM . ",
    "BastienM": "Hi there,\nA little bump on the subject.\nI am using the snippet found here to \"iterate\" through the pages returned by cognitoidentityprovider.ListUsers(), sadly it only browse one page before failing silently ...\nThe pool has over 600+ users and query 50 items per page.\nGo version\ngo version go1.10 darwin/amd64\nSDK\nversion: ~1.13.3\nCode extract\n```go\nfunc getUsersFromPool(pool cognitoidentityprovider.UserPoolDescriptionType) (cognitoidentityprovider.ListUsersOutput, error) {\n    var users []*cognitoidentityprovider.UserType\nparams := cognitoidentityprovider.ListUsersInput{\n    UserPoolId: &*pool.Id,\n    Limit:      aws.Int64(50),\n}\nctx := context.Background()\n\np := request.Pagination{\n    NewRequest: func() (*request.Request, error) {\n        req, _ := CognitoClient.ListUsersRequest(&params)\n        req.SetContext(ctx)\n        return req, nil\n    },\n}\n\nfor p.Next() {\n    page := p.Page().(*cognitoidentityprovider.ListUsersOutput)\n    for _, obj := range page.Users {\n        users = append(users, obj)\n    }\n}\n\noutput := &cognitoidentityprovider.ListUsersOutput{}\noutput.SetUsers(users)\n\nreturn output, p.Err()\n\n}\n```\nIs something missing ?\n@the1337beauty I think this method is what you were looking for, no ?. Hi @jasdel,\nThank you for the code snippet, very much appreciated.\nI can now iterate over all the users without further errors.\nI also notified the person in charge of the ticket I opened to AWS Support of your answer in order to speed things up.. ",
    "silasdavis": "Great, thanks for speedy response :). ",
    "senthilmk": "the goamz package seems to work effortlessly. \nbut the aws's go-sdk seems to have multiple issues.... My bad. I had to change the var name for the aws/s3 dependency and that seems have worked. \nThanks. ",
    "artong0416": "3ks\uff0c i will try it again.. it works\uff0c 3ks. @jasdel . ",
    "amerine": "@jasdel This happens rarely enough that I haven't isolated a failure case yet that I can reproduce. I'm working on a aws-sdk-go bump right in this system in hopes to address. It's nice to get confirmation that you suspect this is fixed. Thank you.\nI'm going to close this, I can reopen at a later date if the stuff continues after the version bump.. ",
    "bennycao": "@jasdel possibly a simplified version to the stripping of double spaces be as simple as strings.Join(strings.Fields(trimmed), \" \") ?\ni realise this may have a performance hit. @jasdel ok thanks for fixing so quickly. ",
    "cdelguercio": "The goal is to have the client not include a ContentLength header at all in both their request for the presigned URL and the actual upload to S3. Yes, I've tried commenting out the \"ContentLength: aws.Int64(contentLen),\" line completely to no effect.. It works fine when I put nil data in http.NewRequest(\"PUT\", u, nil). But when I try and upload a file, it gives me:\n<Code>NotImplemented</Code><Message>A header you provided implies functionality that is not implemented</Message>\nHere's my file reading code:\ndata, err := os.Open(localFilePath)\nif err != nil {\n    return err\n}\ndefer data.Close()\nreq, err := http.NewRequest(\"PUT\", url, data)\nWhen all I do is change data to nil, it works fine.. Haha, yup, just figured that out. As a reference for others, here's some of my solution Go code:\n```\nlocalFile, err := os.Open(localFilePath)\nif err != nil {\n    return err\n}\ndefer localFile.Close()\nreq, err := http.NewRequest(\"PUT\", url, localFile)\nif err != nil {\n    return err\n}\nfor k, vs := range header {\n    for _, v := range vs {\n        fmt.Printf(\"Header name: %v, Header value: %v\\n\", k, v)\n        req.Header.Add(k, v)\n    }\n}\ninfo, _ := localFile.Stat()\nreq.ContentLength = info.Size()\nclient := &http.Client{}\nres, err := client.Do(req)\nif err != nil {\n    return err\n}\n```\nThis corresponds with asking for the presigned URL WITHOUT the content length like this:\nsdkReq, _ := svc.PutObjectRequest(&s3.PutObjectInput{\n    Bucket: aws.String(bucket),\n    Key:    aws.String(key),\n})\nurl, header, err := sdkReq.PresignRequest(s.presignedURLExpirationTime)\nThanks so much for all your help!. ",
    "NeelamAggarwal": "@jasdel I am doing the same with one difference, my bucket is encrypted and \nI added ServerSideEncryption: aws.String(\"aws:kms\"),\nI get error\n&{403 Forbidden 403 HTTP/1.1 1 1 map[X-Amz-Request-Id:[F7A7303D7D2545D6] X-Amz-Id-2:[5LZEcPAO7qIYwyWGTPU5vTgJhisDpdGn4bIwR8CIziOndVtPqup2M6BB0nWxkXbrjcyXFyUppAs=] Content-Type:[application/xml] Date:[Fri, 24 Aug 2018 19:37:24 GMT] Server:[AmazonS3]] 0xc420378040 -1 [chunked] true false map[] 0xc4201d0200 0xc4202c2210} <nil>\n. ",
    "colinfaulkingham": "PutObject also has the same bug. If you comment out the DeleteObject and review your newly created object on the S3 console it will not show it in /bucket//test.txt  it shows it as /bucket/test.txt\n. Ok, I made a simple mistake in my test. This did not solve the issue.\nUsing  configuration option DisableRestProtocolURICleaning with your code will not create the object.. I have figured out what was causing the confusion. When using the DisableRestProtocolURICleaning. Having a single / as the start of the object path indicates an empty folder. Having  2 would indicate 2 empty folders. . ",
    "meirf": "Of the objects that weren't being copied, many had + so I thought that was the issue, but as you pointed out, that's not the issue. I am closing this and will open a separate issue that is more focused on the problem.. Here are example keys being rejected:\n\na.b.c/foo/bar/baz/foo/bar/baz\ufffd\ufffd\ufffd\ufffd.txt\na.b.c/foo/bar/baz/foo/bar/baz\ufffd\ufffd\ud7d1\ufffd.txt\na.b.c/foo/bar/baz/foo/bar/baz/foo+bar\na.b.c/foo#bar%5Bbaz%5D/0\na.b.c/foo/bays/foo-%23foo.html\na.b.c/\n\nTha path.Join issue you mentioned definitely explains a.b.c/. Okay. I now know both categories of errors I was seeing: trailing / being stripped off by path.Join and not url encoding CopySource. I am only url-encoding CopySource - not the Key field and things seem to work. I am closing this with that assumption (should not encode Key).\nThank you.. Oh I see the problem - needed to specify TTL.. ",
    "TylerBrock": "Let me know if #1433 gets this done, thanks!. Yeah, let me know what you think of the code. This is a huge help when dealing with CloudWatch apis in the SDK.. Thanks for the help @xibz. Why is len(out.LogStreams) > 0  the right condition for this indicating we are finished? Wouldn't lastPage indicate that we are done as well?\nI'm fairly certain the code works without providing specific stream names so I just wanted to make sure that my understanding of what this feature is supposed to do is correct. Namely, if I provide a slice of pointers to stream names it filters output so that it comes only from those streams. If so I think there may be something wrong with the code.. Interesting... I have so many streams in our production log group (from containers being created destroyed) that it just takes forever to paginate!\nSide note: We both have a bug in that we need to return !lastPage because the iteration stops when the returned value is false (and therefore would only stop after a single page for both of us because lastPage is false). When I do that it just takes forever to paginate and then eventually tells me there is a limit of 100 streams that I can filter. :-). I'm in kind of a pickle here because what would be ideal is sorting by lastEventTime for a given prefix (which explicitly is not allowed).\nFor example, I'd like to see all of our production log group streams that begin with \"worker/\" and if there are more than 100 (which over time almost all ECS applications will have over 100) ideally I'd like to limit it to the 100 that are the most recently active. There isn't any way to do that.. I guess I'll just retrieve them all and then filter them in my program but its going to get slower and slower as time goes on as I create thousands of new streams a month. Thanks for all the help @xibz!. That was helpful, thanks!. Another approach could be to extract the highest time resolution possible (millis) and keep the function name Int64TimeValue. Rounding the output to seconds feels like something that the SDK should not be directly responsible for. Thoughts? I'm open to either approach as I'm not sure which follows convention more closely here.. Understood and agreed. Let me get that up for us.. ",
    "zbintliff": "~Also, from what I can tell the java sdk supports it as well.~ Think I'm wrong but it then caches the correct region and provides a semi meaningful error message.. Pretty embarrassed now. I was looking at docs, looking at BucketHead requests. Somehow missed this specific method.  For clarification, that call will work even if the s3 client is configured with a different region than the bucket?. Thanks for both of your help. I ended up doing it this way:\nlocationInput := &s3.GetBucketLocationInput{\n    Bucket: aws.String(\"test-bucket\"),\n  }\n  req, locationOutput := client.S3.GetBucketLocationRequest(locationInput)\n  req.Handlers.Unmarshal.PushBackNamed(s3.NormalizeBucketLocationHandler)\n  err := req.Send()\n  if err != nil {\n    return nil, fmt.Errorf(\"unable to get bucket location: %s\", err.Error())\n  }\n  region := locationOutput.LocationConstraint\nProbably not the cleanest but I didn't want to do s3manager and write a new mock client.  This is basically what the s3manager does anyway.. @Bankq I am now having the same issue when I go Cross Account AND Cross Region using the GetBucketLocation request.  When I go cross account, but within the same region I can get a response. When I'm in the same account, but across region I get a response. When I'm in a different account and a different region I get a 403. \nI might have to switch over to s3manager.GetBucketRegion which is bummer because I don't quite follow how a unsigned request will provide more valuable information than a signed, allowed request.. ",
    "Bankq": "@xibz \"You are correct, this will still work even if the client is configured with a different region.\" \nI'm seeing different behavior. If my session configured in 1 region and bucket locates in a different region, I get Access Denied error. Any ideas?. @xibz thanks for the quick response!\nThat's the part puzzles me. The bucket is in us-east-2, if I configure my S3 client with us-east-2 then GetBucketRegion works as expected. However when configured with us-east-1 it gets access denied.\nJust wondering you have a feeling that I might overlook something.. @jasdel \nYes I don't own the bucket. Bucket owner grants me s3:GetBucketLocation in the policy, but not s3:HeadBucket. \nOddly s3manager.GetBucketRegion works, but not S3.GetBucketLocation.  My understanding is s3manager uses anonymous requests but S3 using my session?\nSo my questions is what's the difference between GetBucketLocation and HeadBucket and what's the correct use case for S3.GetBucketLocation ?. ",
    "lostick": "@jasdel sure thing!. ",
    "piusranjan": "@xibz  Thanks for the reply. Yes  I am using  right credentials and region. I will check in stackoverflow or gitter.. @xibz  now  I am able to get the result . But not sure about the reason. \nThanks again.. Need to check the naming convention. @xibz . Thanks for all your valuable review . I will change all according to your sugesstion. @xibz  , I have made changes as per your suggestion. Please have a look. . Not sure why the build is failing  in github.com/aws/aws-sdk-go/service/s3/s3manager for go 1.6.  How my code is related to that module?. Closing because I  will create a new PR to make a fresh build . @jasdel  thanks.. @xibz . Thank. I will change this.. Thanks , I will change this. @xibz , what mock value you mean to say? Are you talking about any unit test with some mock value?. Yes I will change it. It should .. Agreed. I am new to go. I will take care.. ",
    "dfuentes77": "Awesome, Thanks!. ",
    "alexbilbie": "I've taken a look at another implementation we have written using the JavaScript SDK and that too embeds everything in the URL and doesn't require the client to send headers. @jasdel unfortunately Presign doesn't work earlier.\nHere's the URL provided by PresignRequest:\nhttps://my-bucket-name.s3-eu-west-1.amazonaws.com/some-object-key?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJVIY4ICMYSHWGGSQ%2F20170814%2Feu-west-1%2Fs3%2Faws4_request&X-Amz-Date=20170814T200914Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host%3Bx-amz-content-sha256%3Bx-amz-meta-user-id&X-Amz-Signature=8ab49e7bbc45f944cf5bc58898d813dd3e1aa81ccfd6be2f6c117dac600e061f\nand the accompanying required headers:\nx-amz-meta-user-id: 12345\nx-amz-content-sha256: UNSIGNED-PAYLOAD\nHere's the URL provided by Presign:\nhttps://my-bucket-name.s3-eu-west-1.amazonaws.com/some-object-key?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJVIY4ICMYSHWGGSQ%2F20170814%2Feu-west-1%2Fs3%2Faws4_request&X-Amz-Date=20170814T201106Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host%3Bx-amz-meta-user-id&X-Amz-Signature=d85a5cdc560b690fc88dae6e030ba4578f7ec7a1ccdbb3dc0d4dd8c96bc636d3\nIn this case you still need to send the headers for the request to validate.. Sending data to the Presign URL then it does produce a signature error unless the x-amz-meta-user-id header is set. Thanks for fixing the content sha issue.\nI don't suppose you could please explain what the PHP and JS SDKs are doing differently and why you consider this an S3 service issue?\nThank you!. Thank you for the update @jasdel . I've just noticed the BulkEmailDestination used in SendBulkTemplatedEmailInput.Destinations is also affected by this with it's ReplacementTemplateData property.. ",
    "s-maj": "Looking at the https://github.com/aws/aws-sdk-go/blob/feaea4b9265bf620c56303e5ef7e4c388da7f093/service/applicationautoscaling/errors.go#L43 I can see a quite suitable error for this case but it's not listed in API documentation (http://docs.aws.amazon.com/ApplicationAutoScaling/latest/APIReference/API_DescribeScalingPolicies.html).. Cheers.. ",
    "kylegato": "@xibz I have NGINX sitting inside my cluster, accepting internal traffic so it acts like a transparent caching box/proxy. I was tricked by my cache, I've confirmed by adding an /etc/hosts entry for \"s3.amazonaws.com\" that the Host header is indeed what is invalidating the sig, because it works when I use that hostname/header but not when I use my custom internal path of \"http://nginx-s3-proxy\". @xibz What you're suggesting sounds like using a forward proxy, I've setup a reverse proxy. ",
    "matthewmueller": "Thanks for the feedback guys. I guess this is more of a feature request then. I'll definitely try gitter/stackoverflow next time something comes up.\n@jasdel I want to create a signing proxy so I can do something like this:\nsh\ncurl https://AK123:S123@proxy.com/s3-us-west.amazonaws.com/my-bucket/123.jpg\nor more generally:\nsh\ncurl https://${accessId}:${secretKey}@proxy.com/${service}/${path}\nAs far as I can tell, being able to parse the path is required to sign the request:\ngo\nsigner.Sign(...). > I strongly recommend not passing the AWS access key and secret in the URL. These credentials could easily be logged by the proxy or intermediate host unexpectedly leaking the credentials.\nThat's a really good point. This was something internal, but you're right, it's flawed from the outset.\n\nSince parsing the endpoint URL is not supported by the SDK, and there isn't a definitive format for the URL hostname I don't think it would be a good idea for the SDK to bake in this functionality which may break based on service hostname patterns.\n\nActually, this is the reason I think the SDK should handle it instead of 3rd party libraries. You guys can stay up-to-date with any hostname changes that may come our way.. Ahh okay, now I have a better idea of the scope of this issue now and why something like this wouldn't be in the SDK. Thanks for explaining that in more detail.\nI was sort of hoping to just have this signing function sitting on API gateway backed by a lambda function that signs requests for all AWS services. My use-case for this was a serverless RPM repository and so in this case I needed to parse the request on the proxy server itself and then fill in the appropriate fields (id, secret, region, service) to the SDK and return a signature along with the URL. I can't remember exactly now, but I think the discrepancies in the api url formats made it not really possible to do this in a generic way.\nFor inspiration, this was the package I was looking for: https://github.com/mhart/aws4. It looks like the author manually maintains the different URL formats. I was thinking there could be some help from the AWS\u00a0SDK(s) in that respect.\nFor now I ended up scaling down my ambitions and just baked in S3 signing. I'd love to make this a bit more generic so I can call it from the browser or cURL easily. I think you're right that I should bake it into the lambda itself, but even internally, you'd need something like that node repo if you want to do this generically. Anyway, thanks for your thoughtful responses!. Ah okay great, that's what I ended up doing \u2013 thanks!. ",
    "sleavitt": "I'm not sure if I'm understanding what the ultimate goal is, but if the use case here is that the SDK should calculate the signature correctly regardless, it appears that the Elasticsearch Kibana plugin breaks if you do not sort the field values prior to calculating the signature.\nThe examples below vary only the search_fields fields in the query string as to the order in which they are presented.\n\nExample where SDK calculates signature incorrectly:\nGET /_plugin/kibana/api/saved_objects/?type=dashboard&per_page=1000&page=1&search_fields=title&search_fields=description HTTP/1.1\nHost: .es.amazonaws.com\nHTTP/1.1 403 Forbidden\nAccess-Control-Allow-Origin: *\nContent-Length: 192\nContent-Type: application/json\nX-Amzn-Requestid: 57a3c8e1-8ac2-11e8-b305-f151ed41ab7c\nDate: Wed, 18 Jul 2018 19:39:53 GMT\n{\"message\":\"The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\"}\n\nExample where SDK calculates signature correctly:\nGET /_plugin/kibana/api/saved_objects/?type=dashboard&per_page=1000&page=1&search_fields=description&search_fields=title HTTP/1.1\nHost: .es.amazonaws.com\nHTTP/1.1 200 OK\nCache-Control: no-cache\nContent-Type: application/json; charset=utf-8\nDate: Wed, 18 Jul 2018 19:40:28 GMT\nKbn-Name: kibana\nKbn-Version: 6.2.3\nVary: accept-encoding\nX-Amzn-Requestid: 6c38cb6e-8ac2-11e8-b305-f151ed41ab7c\nTransfer-Encoding: chunked\n... (chunked encoding and JSON object of search results)\n\nI am of the opinion that the SDK should handle this automatically, that I should not have to recreate the URL with the fields sorted by key and value in the correct order so that the SDK can sign the request correctly. I believe that this is the same issue, so I am adding this as a comment here. Please advise if I need to open a new issue or if this is sufficient.. Thank you for making me look at my glide.{yaml,lock} files. You may have just saved me a fair amount of work. It looks like the version it's using is version ddfd17ec06eee10c24c5c474633273fd034afdda (tag v1.4.10), which is obviously pretty old. I will update to v1.14.29 and try again. If I continue to have issues, I will open a new ticket. Sorry for what may be nothing more than a false alarm at this point.. Feel free to disregard my previous comment. Didn't work on v1.4.10, works fine on v1.14.29. Sorry for the false alarm.. ",
    "mhart": "I can certainly confirm that a number of AWS services expect, in the canonical string, query values for the same key to be sorted. Or at the very least, their error messages strongly imply this.\nHere's an example (assumes AWS_ACCESS_KEY_ID declared and uses node for the date formatting)\ncurl 'https://lambda.us-east-1.amazonaws.com/2015-03-31/functions/?b=1&a=2&a=1' \\\n  -H \"X-Amz-Date: $(node -p 'new Date().toISOString().slice(0, 19).replace(/[-:]/g, \"\")')Z\" \\\n  -H \"Authorization: AWS4-HMAC-SHA256 Credential=${AWS_ACCESS_KEY_ID}/$(node -p 'new Date().toISOString().slice(0, 10).replace(/-/g, \"\")')/us-east-1/lambda/aws4_request, SignedHeaders=host;x-amz-date, Signature=1283cc55db9b511d289cf9518fd97c71a958adc6a7151d50fd4cd42d2a0dcfc6\"\nOr with python:\ncurl 'https://lambda.us-east-1.amazonaws.com/2015-03-31/functions/?b=1&a=2&a=1' \\\n  -H \"X-Amz-Date: $(python -c'import time;print(time.strftime(\"%Y%m%dT%H%M%SZ\", time.gmtime()))')\" \\\n  -H \"Authorization: AWS4-HMAC-SHA256 Credential=${AWS_ACCESS_KEY_ID}/$(python -c'import time;print(time.strftime(\"%Y%m%d\", time.gmtime()))')/us-east-1/lambda/aws4_request, SignedHeaders=host;x-amz-date, Signature=1283cc55db9b511d289cf9518fd97c71a958adc6a7151d50fd4cd42d2a0dcfc6\"\nThis deliberately uses a bogus Signature and result in something like:\n{\"message\":\"The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\\n\\nThe Canonical String for this request should have been\\n'GET\\n/2015-03-31/functions/\\na=1&a=2&b=1\\nhost:lambda.us-east-1.amazonaws.com\\nx-amz-date:20170830T205130Z\\n\\nhost;x-amz-date\\ne3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'\\n\\nThe String-to-Sign should have been\\n'AWS4-HMAC-SHA256\\n20170830T205130Z\\n20170830/us-east-1/lambda/aws4_request\\n9e3851521dca49b1ba0bad9261e650e5eba22fa1c39f684b62e752a6c166e2da'\\n\"}\nThe relevant portion of which is:\n```\nThe Canonical String for this request should have been\n'GET\n/2015-03-31/functions/\na=1&a=2&b=1\nhost:lambda.us-east-1.amazonaws.com\nx-amz-date:20170830T205130Z\nhost;x-amz-date\ne3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'\n```\nNote that the expected canonical string has sorted the query params by value for the same key (after sorting the keys) \u2013 b=1&a=2&a=1 has become a=1&a=2&b=1. Of course, S3 being S3, it has completely different expectations and eliminates duplicate query param values altogether:\ncurl 'https://s3.us-east-1.amazonaws.com/?b=1&a=2&a=1' \\\n  -H \"X-Amz-Date: $(node -p 'new Date().toISOString().slice(0, 19).replace(/[-:]/g, \"\")')Z\" \\\n  -H \"X-Amz-Content-Sha256: UNSIGNED-PAYLOAD\" \\\n  -H \"Authorization: AWS4-HMAC-SHA256 Credential=${AWS_ACCESS_KEY_ID}/$(node -p 'new Date().toISOString().slice(0, 10).replace(/-/g, \"\")')/us-east-1/s3/aws4_request, SignedHeaders=host;x-amz-date, Signature=1283cc55db9b511d289cf9518fd97c71a958adc6a7151d50fd4cd42d2a0dcfc6\"\nYields:\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\nSignatureDoesNotMatch\nThe request signature we calculated does not match the signature you provided. Check your key and signing method.\n...\nGET\n/\na=2&b=1\nhost:s3.us-east-1.amazonaws.com\nx-amz-date:20170830T212017Z\nhost;x-amz-date\nUNSIGNED-PAYLOAD\n...\n\n```\nThere are a bunch more issues with S3 which you can read about here (I notice some have since been addressed in the documentation, eg \"you do not normalize URI paths for requests to Amazon S3\"). ",
    "Ninir": "Oh ok, good to hear about that :)\nThanks!. ",
    "ramvasanth": "when i start the worker , i don't see this IDLE TCP connections , after a week , it keeps growing .. Hi @jasdel  , i dont have small sample snip-it, i will create it and let you know and also i will create a worker without DB connection and test it as well.\nI am using the following for http.Client\ntimeout := time.Duration(60 * time.Second)\nawsCredential := credentials.NewStaticCredentials(awsEnv.Key, awsEnv.Secret, \"\")\nawsConfig := aws.NewConfig();\nawsConfig = awsConfig.WithHTTPClient(&http.Client{\n        Timeout: timeout,\n    }). Hi @jasdel  Thanks, it looked like MySQL driver causing the issue. we ran the SQS worker without mySQL we did not have any issues. Thanks for your help. . ",
    "atsushi-ishibashi": "@jasdel Yes, you're right. It exited after an expired time.. In my case, a single client were checking the state.. @jasdel I can share the code with you.\nhttps://github.com/atsushi-ishibashi/influencer/blob/master/cmd/sync_deploy.go#L125\n. @jasdel Thank you for your advice. I replaced the code and the below is the log.\nDeploy oneshot task:\n    cluster: hoge\n    task definition: hoge-db-migrate\n        - hoge-db-migrate:21\n        - 111111111.dkr.ecr.ap-northeast-1.amazonaws.com/hoge:latest\n        + hoge-db-migrate:22\n        + 111111111.dkr.ecr.ap-northeast-1.amazonaws.com/hoge:latest\n    container imager: hoge:latest\n    Executing...\n    Registered task definition: hoge-db-migrate:22...\n    Running task of hoge-db-migrate:22 on cluster hoge...\n    Waiting until hoge-db-migrate finish...\nResourceNotReady: exceeded wait attempts\nIt exported expiration error again but no other log...\nSorry for bothering you.. @jasdel Sorry for replying late.\nI don't set another logger to the SDK's client ec. @jasdel Thanks for replying. Actually, each ID has some attributes, such as \"Price\", \"Volume\" and so on..\nAnd each attribute updates independently, so I was trying to express condition with Map value.. @jasdel Thanks for teaching. I'm going to check out the package and update the working status :bow:. Sorry for replying late.\nIt seems to be impossible to use map's content for ExpressionAttributeNames, such as \nExpressionAttributeNames: map[string]*string{\n    \"#FT\": aws.String(\"Price[Timestamp]\"),\n},. Additionaly, it'd be great if you could add a description that CreateElasticsearchDomainInput.DomainStatus.Endpoint is nil when using vpc options instead of that Endpoints you can refer.. @diehlaws Thanks for replying. I'm going to implement in my repository but it's better to implement in aws-sdk-go when it's a reasonable feature. Cloud I take charge of it? \nI'll follow the below.\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-custom-validation.html. @diehlaws Thanks! I'll try it!\nBut I have two questions.\n1. validate-logs requires s3client so I'm wondering how to implement it. Do you have any examples already implemented?\n2. Should the input interface be the same with awscli interface?. Will the below way be allowed?\ntype CloudTrail struct {\n    *client.Client\n        s3client s3iface.S3API\n}. This may be compatible with the below in the previous code.\nif len(b) == 0 {\n    return nil\n}\nIf there isn't if err == io.EOF {, the new UnmarshalJSON returns error with 0 bytes stream.\nBut I cloudn't confirm where the scope is the same:thinking:. > I really don't like relying on the io.EOF as it requires internal knowledge on how Decode works.\nCompletely agree\ud83d\udc4dI'll modify the way of error handling following with your suggestion.\n\nIt may be a good idea of not creating a new decoder every time either.\n\nI don't agree with passing decoder as an argument because it will change interface. Currently UnmarshalJSON is package's public fuction so one possible choice is that creating decoder as package variable in init fuction or variable initialization. But we have to consider that there is a way to reassign io.Reader to Docoder. As far as I examined in a few minutes, it doesn't seem to exist:thinking: \nWhen I misunderstand your suggestion, please correct me!. ",
    "marekt77": "I am also having the same behavior.  Has this been addressed?. ",
    "vedhavyas": "My Bad. Issue was on my end.. ",
    "razvanm": "Thanks for the response. I miss the fact that // +build go1.8 means go 1.8 and above. :-)\nReference: https://golang.org/pkg/go/build/#hdr-Build_Constraints. ",
    "harshavardhana": "\nThanks for reaching out to us @harshavardhana I think the best way to get answer to you question is via the Amazon S3 forums. There others who use S3 and may be able to help you with your question.\n\nOkay I thought this is the right place, thanks for clarifying will ask in Amazon S3 forums. . > The S3 Upload Manager does not use ListParts because the uploader has full knowledge of the parts as they are uploaded.\nSo lets say uploadManager is not used then ListParts would be considered a valid way to do CompleteMultipartUpload? . > I would not recommend using ListParts when performing a CompleteMultipartUpload. While today it may work it is documented that the part numbers returned should not be used for completing a multipart.\n\nThe recommended way is for the application to maintain the list of parts that were uploaded and use that list for completing the multi-part upload.\n\n@jasdel do you know what is the reason behind this ? is it due to the List() not being consistent with an in progress UploadPart() ? . ",
    "sateeshyandapalli": "I got a additional keys response from dynamodb, when i used getItem or query function to fetch the records from the dynamodb, i am getting all attribute keys. could please help me. ",
    "trung": "@xibz thanks for getting back. It is not about the response of decode-authorization-message. It's about the 403 response which contains the encoded message. Thought it would be user-friendly to decode it and return when possible. . ",
    "ilovezfs": "This breaks with glide install too, when attempting to build terragrunt, which depends on aws-sdk-go. See https://github.com/gruntwork-io/terragrunt/issues/296.. Just hit this in Homebrew too in https://github.com/Homebrew/homebrew-core/pull/18363.. ",
    "lokesh-shekar": "1539 fix /models/apis/Greengrass dep case sensitivity .",
    "andrewmeissner": "Is this in anyway related to the latest release?  This exact code was working perfectly last week / yesterday. ```\n2017/09/26 15:23:36 DEBUG: Request route53/ChangeResourceRecordSets Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nPOST /2013-04-01/hostedzone/myHostedZoneID/rrset/ HTTP/1.1\nHost: route53.amazonaws.com\nUser-Agent: aws-sdk-go/1.11.0 (go1.9; darwin; amd64)\nContent-Length: 315\nAuthorization: AWS4-HMAC-SHA256 Credential=/20170926/us-east-1/route53/aws4_request, SignedHeaders=content-length;host;x-amz-date, Signature=6a697e39c1c8901892239a612d4cebe5f17b6fa2e8d1eac430f8caea4fd31a21\nX-Amz-Date: 20170926T212336Z\nAccept-Encoding: gzip\nUPSERTmytest.mydomain.commyIPAddress300AJust a little test\n2017/09/26 15:23:37 DEBUG: Response route53/ChangeResourceRecordSets Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 400 Bad Request\nConnection: close\nContent-Length: 312\nContent-Type: text/xml\nDate: Tue, 26 Sep 2017 21:23:37 GMT\nX-Amzn-Requestid: f588cc2d-a300-11e7-8b4d-07e5ef427368\n\n2017/09/26 15:23:37 <?xml version=\"1.0\"?>\nSenderMalformedInputChangeBatch is not valid, expected ChangeResourceRecordSetsRequestf588cc2d-a300-11e7-8b4d-07e5ef427368\npanic: MalformedInput: ChangeBatch is not valid, expected ChangeResourceRecordSetsRequest\n    status code: 400, request id: f588cc2d-a300-11e7-8b4d-07e5ef427368\ngoroutine 1 [running]:\nmain.main()\n    /Users/ameissner/test.go:45 +0x9f6\nexit status 2\n```. @jasdel thanks so much!. Thanks so much!  Hope this bug isn't too problematic in bringing back the marshalers!. @jasdel Thanks so much for getting me up and running again!. Thanks, @jasdel.  I'll post there.  Sorry for the noise.. ",
    "gurleen-gks": "Hi,\nThan you for the response.\nBelow is the code for file upload. It does not give me any error but the file is not getting uploaded to s3.. Can you please point out what is it that I am doing wrong.\npackage utils\nimport (\n    \"os\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/credentials/stscreds\"\n\"github.com/aws/aws-sdk-go/aws/endpoints\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\n\"log\"\n\n\"time\"\n\"fmt\"\n\n\"net/http\"\n\"crypto/md5\"\n\"encoding/base64\"\n\"bufio\"\n\n)\n// Upload files to the S3 bucket.\nfunc UploadFiletoS3(filename, bucket string) error {\n    // All clients require a Session. The Session provides the client with\n    // shared configuration such as region, endpoint, and credentials.\nsess := session.Must(session.NewSession(&aws.Config{Region: aws.String(endpoints.UsWest2RegionID)}))\nawsCreds := stscreds.NewCredentials(sess, \"my aaws:iamXXXXX\")\ncreds, err := awsCreds.Get()\nlog.Printf(\"Successfully got temporary credentials with id : %v \",creds.AccessKeyID)\n\nsvc := s3.New(sess, &aws.Config{Credentials: awsCreds})\n\nfile, err := os.Open(filename)\n\nif err != nil {\n    return err\n}\n\ndefer file.Close()\n\n//The file name on s3 bucket\n    s3ObjectKey:= Gets3ObjectKey(filename)\n// Upload the file's body to S3 bucket as an object with the key \n    input:=&s3.PutObjectInput{\n    Bucket: aws.String(bucket),\n    Key: aws.String(s3ObjectKey),\n    }\n\nreq,_:=svc.PutObjectRequest(input)\ncontent:=bufio.NewReader(file)\nh := md5.New()\ncontent.WriteTo(h)\nsum := h.Sum(nil)\ncontentMd5 := make([]byte, base64.StdEncoding.EncodedLen(len(sum)))\nbase64.StdEncoding.Encode(contentMd5, sum)\nreq.HTTPRequest.Header.Set(\"Content-Md5\", string(contentMd5))\n\nurl, err := req.Presign(15 * time.Minute)\nif err!=nil{\n    return err\n}\nput, err := http.NewRequest(\"PUT\", url, content)\nif err != nil {\n    fmt.Println(\"error creating request\", url)\n    return err\n}\nput.Header.Set(\"Content-Md5\", string(contentMd5))\n_, err = http.DefaultClient.Do(put)\nif err != nil {\n    fmt.Println(\"failed making request\")\n    return err\n}\n\nlog.Printf(\"Successfully uploaded %v to %v\\n\", filename, bucket)\nreturn nil\n\n}\n. ",
    "danielnelson": "I still have a few questions about the behavior, when I run the gist now I see this (after a few calls to establish the connection pool):\n```\nGet connection monitoring.us-west-1.amazonaws.com:80                       \nGot connection info {Conn:0xc4203f6a20 Reused:false WasIdle:false IdleTime:0s}\n2017/10/17 11:10:56 DEBUG: Request monitoring/GetMetricStatistics Details: \n---[ REQUEST POST-SIGN ]-----------------------------                      \nPOST / HTTP/1.1                                                            \nHost: monitoring.us-west-1.amazonaws.com                                   \nUser-Agent: aws-sdk-go/1.8.39 (go1.9.1; linux; amd64)                      \nContent-Length: 286                                                        \nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJ3NHEU5JNLHNFLRQ/20171017/us-west-1/monitoring/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date, Signature=39c2b6b91da7f15c917b9c7fb7fd213cec7f69433a83cf39c6d6f917259d1e35\nContent-Type: application/x-www-form-urlencoded; charset=utf-8             \nX-Amz-Date: 20171017T181056Z                                               \nAccept-Encoding: gzip                                                        \n\nGet connection monitoring.us-west-1.amazonaws.com:443                      \nGot connection info {Conn:0xc4201ca000 Reused:true WasIdle:true IdleTime:102.682284ms}\nConnection put in idle pool  \n2017/10/17 11:10:56 DEBUG: Response monitoring/GetMetricStatistics Details:\n---[ RESPONSE ]--------------------------------------                      \nHTTP/1.1 200 OK                                                            \nContent-Length: 515                                                        \nContent-Type: text/xml                                                     \nDate: Tue, 17 Oct 2017 18:11:19 GMT                                        \nX-Amzn-Requestid: 935b4e26-b366-11e7-a801-dd5e4e7dd120\n```\nMy observation is that the initial connection goes to port 80 but a connection is then also retrieved to port 443.  I think this means we are being redirected, perhaps we can avoid this extra step?\nAlso, the connection to port 80 is never WasIdle:true.  Again, maybe we can solve this by not redirecting but I'm not sure why this connection cannot be reused as well.  This means we are still placing many sockets in TIME_WAIT.  Based on my reading, I believe sockets are only placed into this state when the client is initiating the close, and if the server was closing the connections then they do not go into TIME_WAIT.. What about the switch from http to https and the redirects on connection?  These seem to be the source of newly created connections and we are always forwarded to a https endpoint.\n. ",
    "adriantodorov": "Hey @radeksimko , thank you for this. Our Jenkins pipeline is hanging because of this bug. is there a timeline for a new release for the plugin for provider \"aws\"?. ",
    "mariusae": "Hi @xibz, \nThat's true, I suppose other things outside of aws-sdk-go can implement awserr.Error.\nHaving helper functions to inspect errors would be very useful, as long as the actual error implementations follow some standard interface (perhaps Status() int, Timeout() bool, Temporary() bool, NotExist() bool) so that existing code in that attempts to generically interpret errors would still work.. ",
    "bboughton": "@jasdel, I updated my changes based on your feedback.. ",
    "theherk": "I am, not just to validate, but to modify it. So if it can be modified without the knowledge of the caller. For example, if a helper method that writes to DynamoDB on behalf of the callers sees that the input is missing an \"updated\" timestamp, it can be added to the input before making the API request.\nI am currently working on a PR to reverse the build process. This would add methods that take an input as an argument and give you back the builders. This would allow the addition of operations to the builder, but not the inspection of those that are already in the operationList. To overcome that, I considered adding getters for the list of operations. This would still not have the desired outcome, though it would get closer. Ideally, the workflow would be:\n\nPass dynamo input to request method (a helper that would make the API request).\nRequest method would then call  builder := expression.FromUpdateInput(input) (or similar) with the input argument being the dynamodb.UpdateItemInput.\nGet the update builder from the builder, up := builder.UpdateBuilder().\nThen the helper could check if \"updated\" is in the update builder, up.Get(\"updated\"), and if not found call up.Set(expression.Name(\"updated\"), expression.Value(int(time.Now().Unix()))), before updating the update item input and making the request.\n\nI hope that makes sense. It is fairly complex, so I don't mind working on it, but I'm going to hold off until you either tell me to hold off permanently since you aren't interested in its inclusion in the library, or how you would like to see it implemented. The change is large enough that it would change the shape of the package a bit. I don't want to diminish the clarity of the library; it is very good, but it would be neat to be able to go both directions.. I bit off the harder part first, because it was the most interesting. I have a branch in progress, feature/builders-from-input. I really like how things are looking, but it is not in ship shape yet. Once I have tests written for the NewUpdateBuilder().WithInput(input) workflow, I'll open the PR for review before I write the other lexers.\nIt has been a bit complex to work in the package, since it was not designed with reversal in mind (well, at least it isn't apparent if so). Also, it seems not inefficient but a bit counter-intuitive to copy objects all over the place rather than using pointers. However, my assumption is that the purpose is to get the desired composability for the user, and it does very well at that. This is a clever package.. Yeah, that would have been awesome, but as long as it isn't that way this implementation will do the trick. If it does modify to AST in the future, modifying the WithInput methods would be simple, I believe.. @xibz. I planning on getting back to this, but I thought I'd ask early on, do you expect implementing AST here is likely to happen? If so, are we talking weeks, years, or something between?. ",
    "Phlamethrower": "\nHaving this use json.Number by default would be a breaking change.\n\nAt the moment the only affect this PR has on default behaviour is that encoding a json.Number will now produce a DynamoDB Number instead of a String. I can easily switch that off by default if necessary.\n\nAnother potential solution would be to use the json and dynamodbattribute marshaling interfaces.\n\nYes, I probably would have gone with that approach if I was in direct control over the types that are being used, but in this case it's another library which is handling converting the JSON to map[string]interface{}. Adding an extra step which scans through the structure to replace things seems a bit wasteful, ideally it should either be handled in the JSON deserialiser or the DynamoDB interface.. @xibz I'm working on a REST-based server. go-swagger is being used to generate the server frontend from an OpenAPI spec (including code for converting the incoming HTTP/JSON data to map[string]interface{}). For the backend I then have to convert the data to/from the appropriate DynamoDB types/structure. This left me with the following options:\n\nUse dynamodbattribute with an extra loop to deal with json.Number (annoying to implement, waste of CPU time)\nAllow dynamodbattribute to support json.Number (this PR)\nFork dynamodbattribute and make whatever modifications I desire (which is essentially what I'm currently doing, unless/until this PR is accepted)\nWrite my own code to replace dynamodbattribute (waste of dev time to start from scratch, since the AWS SDK license is permissive enough to allow forking)\nFork/improve go-swagger in order to make it more flexible with how it handles marshalling/unmarshalling data (unknown dev time cost)\n\nIn terms of execution time, code which contains an extra loop to convert from json.Number to/from dynamodbattribute.Number would almost certainly take longer to execute than code which doesn't contain an extra loop. Adding an extra loop is also likely to result in higher memory costs, especially if the entire data structure needs copying (although I doubt the memory costs would be very significant in my use case).\nIt doesn't really bother me whether this PR is accepted or not - since I have option 3 to fall back on. But I figured it would be worth opening a PR for it to see if the AWS SDK team or anyone else were interested in this kind of functionality, or whether it could generate some useful discussion on how to best handle encoding/decoding of \"foreign\" types (where the user isn't able to add MarshalDynamoDBAttributeValue/UnmarshalDynamoDBAttributeValue to the type, or where such addition would be insufficient - e.g. UnmarshalDynamoDBAttributeValue won't help you if you need to unmarshal into an interface{}). E.g. one option could be to add a function/interface pointer to Encoder & Decoder which will then be invoked for each element of the data structure being converted, allowing the user to override the behaviour for some types without needing to duplicate the entire library.. ",
    "damienstanton": "Got it, this does describe the scenario we found and explains the unfortunate behavior of the CLI tool. \nI\u2019ll close this issue as a change would have to come from the boto/cli side and not the Go SDK. Thanks \ud83d\ude4f . ",
    "brodul": "Figured that there is S3ForcePathStyle *bool.. ",
    "crazytan": "Understood. So I'll leave the PR as it is then.. I changed the signature because I found out that testing the function with signature verification is very difficult. If there's a good way to test it, then we don't have to change it.\nThe identity document is contained in the PKCS7 structure. But we can always add an additional verification: request the doc and see if it agrees with the signed doc in PKCS7 data.. Yes that's true. Since currently AWS doesn't provide an API for requesting its public cert, perhaps we can add SDK user to supply the cert somewhere if they want to check the signature.. the dependency is kind of necessary to parse the pkcs7 structure; there's no built-in package to do this right now.. ",
    "SCKelemen": "Thanks, I'll test it tonight. Is it possible to hard code it, and pass it in the configuration? . ",
    "itskingori": "@xibz @jasdel I'm having this issue too. Why do I have to specify region if I have region set in my credentials file?\nconsole\n$ cat ~/.aws/credentials\n[default]\naws_access_key_id=some-key\naws_secret_access_key=some-secret\nregion=us-east-1\nIt's counter-intuitive to have everything else loaded i.e. the key and secret and not region. \ud83e\udd14 . @jasdel thanks. I've re-read the documentation and seen what you've just mentioned. It's kinda confusing and you have to read it more than once. \ud83d\ude05 \n. ",
    "Arnold1": "@xibz thanks for reply. are there any golang code samples you can point me to?. ",
    "ppetko": "@xibz - I'm sorry for the delayed response, but I want it to provide you with some examples. \nYes I would like to manipulate the output. If the output from the SDK is valid JSON format I could easily do json.Unmarshal it into struct. Please see the  Example. Note: I manually added double quotes around the key elements in the output. \nIf I take the same example and use the original output from the SDK, I get error Example with default output\nPerhaps I'm missing something, but so far I have no luck of parsing the output. If I could parse the it, then I easily could inset it to nosql database. \nThanks! . @xibz - Sure this is the code that I run and I get the output that I used in the previous examples. \nWhen I try to Unmarshal the original output I get  this error. I believe we get this error, because the output of the SDK is not a valid json and Unmarshal requires json format. This is just my hunch. \nThanks!\n. Gents @jasdel  @xibz - That's what I was trying to figure out. Well this feature will be nice to have. \nI was able to resolve the issue and  parse the output once I used the json.Marshaler, which turned the output into json and then I Unmarshal it into struct. \nThank you very much!   . ",
    "s12v": "@jasdel, makes sense, thanks for looking into it.\nHost header includes port, so SDK has to modify header? Or maybe just calculate correct signature. I'm wondering how does it work in the java sdk.... @jasdel, found it: https://github.com/aws/aws-sdk-java/blob/1.11.221/aws-java-sdk-core/src/main/java/com/amazonaws/auth/AWS4Signer.java#L533-L545. I changed the code quite a bit and moved tests to functional_test.go, please have a look. @jasdel, changed.\nI'm thinking about one more thing: current implementation modifies ctx.request.Host when needed. It's possible to achieve the same result in func (ctx *signingCtx) buildCanonicalHeaders(..), without touching the original request. Which is probably better... what do you think?\nPlease check the next commit to see what I mean.\n. > I was thinking about this as well. I think the original request must be modified because if the HTTP request sent doesn't use the exact Host header value the signature will fail\nI think it will not fail (at least it was working for me with AWS ES). Probably, on the receiving side default port is removed before signing. However, it's not explicitly documented anywhere, and maybe it even works differently for different services... no idea.\nBut I agree, that on the other hand, it's better to have signature and host header consistent.\nIf you'd like, I can revert the last commit.\n\nMaybe a better place for this logic would be when the request is created instead of the signer. \n\nNot sure, the signer accepts http.Request.\n. @jasdel, sounds good, moved to request. @jasdel, it seems the issue with default ports exists in aws-sdk-go-v2, do you have any plans to move the code there?. @jasdel, I assume it's always set, but I don't know :) Removing the check.\nWhat do you think about ctx.Request.URL? Should be also always set?. ",
    "fanminshi": "@xibz I see. thanks!. . ",
    "Luizm": "I have the same problem, sometimes it works, sometimes not.\nI'm using the version 1.10.23. > Hmm while investigating it suddenly stopped happening, so more like this was an infrastructure issue in the zone?\nCan be, and if this is an AZ problem or something like that, it can happen with other SDKs?\nBecause, I'm using the java SDK and there is a similar error:\ncom.amazonaws.SdkClientException: Unable to execute HTTP request: javax.xml.stream.XMLStreamException: java.util.zip.ZipException: invalid code lengths set\n. @jasdel I'm doing test and so far everything works on region us-east-1\n. Problem solved . ",
    "danielfm": "This is happening consistently here in Terraform v0.10.7 with aws provider v1.2.0 (that apparently uses aws-sdk-go v1.12.19) while trying to manage resources in the sa-east-1 region.\nEdit: Some logging:\n2017-11-03T18:39:24.029-0200 [DEBUG] plugin.terraform-provider-aws_v1.2.0_x4: 2017/11/03 18:39:24 [DEBUG] [aws-sdk-go] DEBUG: Response ec2/DescribeSecurityGroups Details:\n2017-11-03T18:39:24.029-0200 [DEBUG] plugin.terraform-provider-aws_v1.2.0_x4: ---[ RESPONSE ]--------------------------------------\n2017-11-03T18:39:24.029-0200 [DEBUG] plugin.terraform-provider-aws_v1.2.0_x4: HTTP/1.1 200 OK\n2017-11-03T18:39:24.029-0200 [DEBUG] plugin.terraform-provider-aws_v1.2.0_x4: Connection: close\n2017-11-03T18:39:24.029-0200 [DEBUG] plugin.terraform-provider-aws_v1.2.0_x4: Transfer-Encoding: chunked\n2017-11-03T18:39:24.029-0200 [DEBUG] plugin.terraform-provider-aws_v1.2.0_x4: Content-Type: text/xml;charset=UTF-8\n2017-11-03T18:39:24.029-0200 [DEBUG] plugin.terraform-provider-aws_v1.2.0_x4: Date: Fri, 03 Nov 2017 20:39:23 GMT\n2017-11-03T18:39:24.029-0200 [DEBUG] plugin.terraform-provider-aws_v1.2.0_x4: Server: AmazonEC2\n2017-11-03T18:39:24.029-0200 [DEBUG] plugin.terraform-provider-aws_v1.2.0_x4: Vary: Accept-Encoding\n2017-11-03T18:39:24.029-0200 [DEBUG] plugin.terraform-provider-aws_v1.2.0_x4:\n2017-11-03T18:39:24.029-0200 [DEBUG] plugin.terraform-provider-aws_v1.2.0_x4:\n2017-11-03T18:39:24.029-0200 [DEBUG] plugin.terraform-provider-aws_v1.2.0_x4: -----------------------------------------------------\n2017-11-03T18:39:24.029-0200 [DEBUG] plugin.terraform-provider-aws_v1.2.0_x4: 2017/11/03 18:39:24 [DEBUG] [aws-sdk-go]\n2017-11-03T18:39:24.029-0200 [DEBUG] plugin.terraform-provider-aws_v1.2.0_x4: 2017/11/03 18:39:24 [DEBUG] [aws-sdk-go] DEBUG: Unmarshal Response ec2/DescribeSecurityGroups failed, not retrying, error SerializationError: failed decoding EC2 Query response\n2017-11-03T18:39:24.029-0200 [DEBUG] plugin.terraform-provider-aws_v1.2.0_x4: caused by: flate: corrupt input before offset 4\n2017-11-03T18:39:24.029-0200 [DEBUG] plugin.terraform-provider-aws_v1.2.0_x4: 2017/11/03 18:39:24 Error on SGStateRefresh: SerializationError: failed decoding EC2 Query response\n2017-11-03T18:39:24.029-0200 [DEBUG] plugin.terraform-provider-aws_v1.2.0_x4: caused by: flate: corrupt input before offset 4. @xibz Thank you. I can no longer see any failed requests on my end now, it was probably some transient issue in AWS infrastructure.. ",
    "patthehuman": "Thanks for the reply @xibz \nCan you point me in the right direction to some commands to verify which is out dated? . Thx @xibz\nLooks like my versions of both SDK's are 1.12.11. Any Idea?\nI appreciate you assistance.. Thanks @jasdel @xibz \nI was able to go in and update some of my vendor projects. I don't know exactly what the issue was, but it seemed to work after I deleted and re-added.\nThanks for your help.. ",
    "vaibhavkewl": "Hi again, \nThis feature is specifically for DynamoDb service. There is a constraint that table name should be unique in every AWS region in the same AWS account. \nWe want to have our staging and production environments set up in the same region. And one of the possible solution to enable this is that dynamodb tables that we create should be prefixed with environment name. for eg production_user or stage_user. \nI see .net and java aws sdk have support for specifying the prefix for the table names in the sdk. This is exactly what we can also incorporate in go sdk. This is a nice to have feature.. @xibz - I dont think this is really a custom feature. There is a constraint in DynamoDb service and it should be possible for SDK to provide a simple wrapper or workaround to handle this constraint. \nAs I mentioned since .net and java sdk already have something like this, then I dont really see the reason why we can not have it in go sdk as well.\nEg http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/datamodeling/DynamoDBMapperConfig.TableNameOverride.html. @xibz  - Ok. Do you think this can be handled at the configuration level, rather than having to use this KeyBuilder everytime I am trying to do some operation on the tables.\nfor eg if we can address it like here by providing an additional configuration, this will make it generic:\nfunc New(p client.ConfigProvider, cfgs ...*aws.Config) *DynamoDB\nYour thoughts?. ",
    "simar7": "Hey @xibz I was told on gitter chat that the ETag  doesn't represent the md5 if server side encryption is used. I haven't played with it just yet to confirm but if that's true, I'll need some way to get the md5 of the object.. ",
    "justone": "Awesome, @jasdel, thanks for fixing it up!. ",
    "fly1028": "Thank you @xibz ! . ",
    "atcrawford": "Thanks, @jasdel! That link was helpful for more reasons than originally intended because now I know that the true name of what I'm looking for is a \"data plane client\" (https://forums.aws.amazon.com/thread.jspa?messageID=792000&#792000). \nThanks again!. ",
    "lorengordon": "@jasdel Ok, thank you. Hopefully the service team will also update the documentation (and the model) with the parameter that manages whether you create a Standard vs Enterprise MicrosoftAD directory. I had just assumed from the API docs that would be the Size parameter.. For anyone tracking this issue, this option is exposed through the Edition parameter. Still missing from the DS API docs (which still reference Size), but you can see it in the model. Available in aws-sdk-go v1.12.58.. Ok, the API doc has been updated to include the Edition parameter and remove the Size parameter. Thanks!. This would be amazing, very much looking forward to when this is implemented!. @micahhausler @jasdel Any updates on this feature?. @micahhausler Are you still working in this one? Would love to see this implemented!. >>> we have looked at this and we have decided to only allow the credential process provider to be enabled via code and not the shared config\nUgh. As a user, that's a major bummer. . <rant>\n@xibz Because applications are written around the AWS SDK, and lean strongly on the AWS SDK to load and handle credentials for AWS. This is especially an issue for Go, since applications are generally compiled as static binaries.\nThe AWS Shared Config file is a common way for users of such applications to manage credentials for multiple accounts and multiple roles, in a way that, from a user's perspective, seems ought to work the same way across all applications, regardless of the backing AWS SDK. It would be great for that to work in a way that didn't require extra boilerplate in every application, and result in a lot of confusing issues for every application about why a particular credential method worked for some app \"foo\" but not some app \"bar\".\nRight now, it sucks that Golang apps handle AWS credentials differently than Javascript apps, differently than Ruby apps, differently than Python apps, etc, etc. I certainly get that there may be some delay in support for new methods, with Python seeming to lead the way here, but simply choosing not to support the same methods in common across all SDKs... well, that's just a terrible user experience.\nAmazon in the past had seemed to indicate we could expect this kind of parity across SDKs as the new way forward:\n\nhttps://aws.amazon.com/blogs/security/a-new-and-standardized-way-to-manage-credentials-in-the-aws-sdks/\n\nWe were just tackling one credential provider here, because we have a specific need for this provider in our environment, but the larger criticism is still valid. Without including support for loading the credential_process from the shared config, this contribution will likely end up being a waste of our time. That's a risk we take with all contributions to open source projects, but still, it stings. And I'm not looking forward to the maintenance tail of maintaining a fork just to build this in to open source apps we use.\n</rant>. > Found here. This is a major reason why we are potentially deciding this to only be allowed via code.\nYes. And yet, support for the option in the shared config is already baked in to the python and ruby SDKs. They nicely leave the option to the user, rather than trying to decide centrally for every user and use case.\nEdit: This is what I'm talking about with the inconsistent user experience. I shouldn't need to know that the AWS CLI is written in Python and is using botocore to load the Shared Config. Nor should I need to know that Terraform is written in Go, and using the AWS Go SDK to load the Shared Config. Do both SDKs support the same options? If not, is it a subset of options? Is it a totally different set of options? Is there a master list of the possible options somewhere? I shouldn't need to care about this. They should both just work.. @xibz I'm failing to see how handling this only in code addresses the linked issues, at all. But I'm open to the idea that is a failing on my part and not a failing of what you're currently proposing.\nHere's a workflow I would consider a success:\n\nAWS Go SDK merges support for credential_process and releases a new version.\nApplications update the version of the AWS Go SDK they use, make no other changes to their code*, and release a new version.\nUsers of such applications are now able to use credential_process via the AWS Shared Config file to make credentials available to that application.\n\nAny workflow that requires applications in Step 2 to make further changes to their code, I would consider to be failing to address the linked issues. I'd even wonder why bother merging, since no one seems to be asking for the feature being considered. And really, if that ends up being the case, the linked issues should be unlinked so they are not closed.\n*There is probably an assumption here that the application is otherwise already creating the session correctly.. @jasdel Alright, let's grant that such services may exist, and that they don't actually deserve to be compromised (debatable). \ud83d\ude02\nIsn't loading the shared config already gated by code?\n\nSessions from Shared Config\nSessions can be created using the method above that will only load the additional config if the AWS_SDK_LOAD_CONFIG environment variable is set. Alternatively you can explicitly create a Session with shared config enabled. To do this you can use NewSessionWithOptions to configure how the Session will be created. Using the NewSessionWithOptions with SharedConfigState set to SharedConfigEnable will create the session as if the AWS_SDK_LOAD_CONFIG environment variable was set.\n\nSince an env was considered sufficient, would you consider an approach here that further gated this feature in the SDK behind an env? Basically, require the user to do something like export AWS_SDK_LOAD_CREDENTIAL_PROCESS=true to enable the credential_process provider? That would satisfy our use case.\n. Understood. We'll figure it out and start trying to patch applications we use.\nI think there is a difference in who you consider a user and who we consider a user...\nThis targets application developers, who are not users to me:\n\nThis allows for users to enable and opt into the feature that has potential security holes rather than having this be a security risk to all users who may not be aware of the feature.\n\nUsers to me are people who use the applications developed by those application developers... Those users cannot enable this feature and opt in without first patching each and every application they use that depends on the AWS Go SDK to support this feature.... ",
    "ericofusco": "@lorengordon,\nThat's correct. I also have a ticket with AWS to update the API documentation.\nThey updated the documentation for the command-line (https://docs.aws.amazon.com/cli/latest/reference/ds/create-microsoft-ad.html) and added the --edition parameter, all SDKs updated theirs models with the new parameter but they haven't updated the API documentation yet.. ",
    "bwhaley": "Looks like this:\n$ curl -s http://169.254.170.2$AWS_CONTAINER_CREDENTIALS_RELATIVE_URI\n{\n  \"RoleArn\":\"arn:aws:iam::...:role/GrafanaIamRole-\",\n  \"AccessKeyId\":\"...\",\n  \"SecretAccessKey\":\"...\",\n  \"Token\":\"...\", \n  \"Expiration\": \"2017-12-05T05:50:27Z\"\n}. @xibz - I did see @hflamboauto1's response there, but simply updating aws-sdk-go did not actually resolve the issue.\nFortunately, after some further debugging I think I've found the source. Ignore the comment about the date above - that was a red herring. The actual issue is that the response body is consumed before the endpointcreds provider can read it. \nSee https://github.com/aws/aws-sdk-go/blob/master/aws/ec2metadata/service.go#L97 \nThe same response body is passed to be unmarshaled in endpointcreds/provider.go but the stream has been read and closed and is thus empty, hence the serialization error.\n. Looks like this is a bug in how Grafana is using the session. Apologies, I will submit a patch to Grafana.. ",
    "krobertson": "I was just about to submit an issue on this when I saw it was fixed. \ud83d\udc4d . ",
    "beard1ess": "I've worked around the issue, and will try out what @xibz mentioned when I get time (the project is minor).\nI don't have more information to provide though, the example in the docs still produces the same result - null if 'us-east-1'. I pulled the latest SDK just to double check. If that's expected I suppose it's fine, though it is quite confusing at first pass. . This is the test I used verbatim: \n```\npackage main\nimport (\n    \"github.com/aws/aws-sdk-go/aws/session\"\n    \"github.com/aws/aws-sdk-go/aws\"\n    \"github.com/aws/aws-sdk-go/service/s3\"\n    \"github.com/aws/aws-sdk-go/aws/awserr\"\n    \"fmt\"\n)\nfunc main() {\nsess, err := session.NewSession(&aws.Config{\n    Region: aws.String(\"us-east-1\"),\n})\n\nsvc := s3.New(sess)\ninput := &s3.GetBucketLocationInput{\n    Bucket: aws.String(\"bucket-name-here\"),\n}\n\n\n\nresult, err := svc.GetBucketLocation(input)\nif err != nil {\n    if aerr, ok := err.(awserr.Error); ok {\n        switch aerr.Code() {\n        default:\n            fmt.Println(aerr.Error())\n        }\n    } else {\n        // Print the error, cast err to awserr.Error to get the Code and\n        // Message from an error.\n        fmt.Println(err.Error())\n    }\n    return\n}\n\nfmt.Println(result)\n\n}\n```\nproduces output of:\n```\n{\n}\n```. ",
    "Atanas-Kanchev": "Same here. ",
    "sudhirj": "No so much case sensitivity, as models/apis/sagemaker/2017-07-24/waiters-2.json - the dep git operations keep getting stuck on this file for some reason - it either shows up as modified when it shouldn't be, or has whitespace changes that it should not have. . Is there some sort of hook that runs whenever a branch is checked out? This is causing modifications to the files when checking out v1.12.42, while v1.12.41 checks out cleanly.. ",
    "brikis98": "Awesome, thanks!\nMight be worth looking into a linting tool that catches these sorts of errors. Would probably be useful for all Go projects.. ",
    "crwgregory": "It looks like this may be more of a Docker Issue/Networking Issue/Linux Config Issue? \nI installed the cli in my docker container and my results are limited like they where in my API. But when running the same command in my windows command prompt through the aws command, the results aren't limited.. Opened a question on Stackoverflow. ",
    "hwh33": "Thanks for the quick response @xibz !\nYeah, perhaps Set was not the best function to use for identifying this issue.  I think that it is clear, as you said, that Set is not a setter, but a reflection of Set in dynamodb.\nI think the confusion comes from this line:\n\nGenerally the builder pattern doesn't modify the underlying thing you are building and returns a new object every time.\n\nIt seems that we have different expectations for the builder patten.  lf you look at the wikipedia article for the builder pattern, the user makes a series of calls which do modify the builder itself.  Of course, whoever wrote that Wikipedia article may have the same biases that I do!\nIs your description of the builder pattern generally the AWS interpretation?. Ah I see.  That explains why a value receiver makes sense then.  Cool!\nThank you to both of you, @xibz and @jasdel, for helping to clear up my understanding.  I do really like the expression package by the way.  It cleans up a substantial chunk of DynamoDB interaction pretty neatly.  Keep up the good work!. ",
    "Doug-AWS": "What happens if you make the call without a CacheClusterId property? According to the docs (http://docs.aws.amazon.com/sdk-for-go/api/service/elasticache/#ElastiCache.DescribeCacheClusters):\nReturns information about all provisioned clusters if no cluster identifier is specified. The closest example we have in the Go developer guide is an example of firing a CloudWatch event to log the changes in state for an Amazon EC2 instance:\nhttps://docs.aws.amazon.com/sdk-for-go/v1/developer-guide/cw-example-sending-events.html. ",
    "nsagnett": "Ok I see, when I call without CacheClusterId property, I see my clusterIds are wrong.\nThanks !. ",
    "Amitgb14": "S3ForcePathStyle parameter was missed to set true.. @xibz, I try with both way, empty endpointURL and without empty endpointURL, Output still same;\noutput:\n{\n} . Thanks, @xibz .. @jasdel, my S3 server access logging was disabled, so that why it return empty and When I make enable it shows,\nLoggingEnabled: {\n    TargetBucket: \"\",\n    TargetPrefix: \"abc\"\n  }\nWith enable logging\nX-Amz-Id-2: 6P2rSkPSchnYFmJnmhyU/As0fQjtQ2Jxz8pOV4MYeJLw3+zdzheNL0UnxvLtiu8Yh3XEp4FeSS4=\nX-Amz-Request-Id: 9EA6A03D5E019BA7 \nWith disable logging\nX-Amz-Id-2: VC1YEk3Vl1fOU1bPvHo6HbiOlRqZ5kpv3Q5uL/3zgS1T9/h/Y2h8oQR6NtNEbI1kUuNX61V9oNs=\nX-Amz-Request-Id: 60685C564A124573. @jasdel But I think it should be like below, when access logging is disabled.\n```\nLoggingEnabled: {\n}\ncurrent is,\n{\n}\n. Hi @xibz , Thanks for response. I usedFilestruct to read file before start goroutines and added intoClientstruct,\ntype File struct {\n    snapFileBytes *bytes.Reader\n    snapSize      int64\n}\ntype Client struct {\n    client s3api.Client\n    F      File\n}\nfunc (f *File) readSnapFile(fname string) {\n    // open and read file\n    sf, err := os.Open(fname)\n    if err != nil {\n        log.Error(err)\n    }\n    defer sf.Close()\n    fileInfo, _ := sf.Stat()\n    f.snapSize = fileInfo.Size()\n    buffer := make([]byte, f.snapSize)\n    sf.Read(buffer)\n    f.snapFileBytes = bytes.NewReader(buffer)\n}\ninside main():\nfiles := File{}\nfiles.readSnapFile(sfname)\nclient := Client{}\nclient.F = files\n```\nBelow block called by goroutines;\n```\nWriteFile(snapshotKey, client.F.snapFileBytes)\nfunc (c Client) WriteFile(key string, fileReader bytes.Reader) {\n    input := &s3.PutObjectInput{\n        Body:   fileReader,\n        Bucket: aws.String(bucketName),\n        Key:    aws.String(objectName),\n    }\nresult, err := c.svc.PutObject(input)\n\n}\n```\nI try with other way, each goroutines handle file individually then it works fine.. Hi @jasdel , You are correct, I try to use same resource (fileReader) with 10 goroutines.  Thanks for your explanation. I could not to be use CopyObject because my aim is different. Thanks for your help.. I'm going to close this issue now.. ",
    "forensicsguy20012004": "Has there been any work done on this issue? I am having the same issue and really need this functionality. @jasdel \n I can confirm the same results as @Amitgb14. \n I get a lot of logging data but the result set is still {}\n Not sure if you want to see all the logging information...which I can post but it looks pretty standard to me. @jasdel\n My goal of this whole thing is to try and figure out how to systematically get information that pertains to what the screenshot is showing\n\n. @jasdel\n So you are saying not matter what, I should get a result set back? Not blank?\n And the return set will be 'GetBucketLoggingOutput'\n  * and within the return set I should see a data object called 'LoggingEnabled' ?\n     * if it is enabled I will get something back and if not it will be nil?\n\nHowever, the basic understanding I am reading, correct me if I am wrong, I should be getting a response back that is not blank {}?. I agree with @Amitgb14 ... I would like to see a result set like this:\nLoggingEnabled: {\n}\ninstead of just a {}...I have never been a fan of blank or empty result sets.. \n",
    "yujinis": "@xibz - Thanks for your response! I was not aware of your point and it worked with the following code modified according to your comment. Kindly be noted that I am writing this also for my own and others' future reference.\nsess, err := session.NewSession(&aws.Config{Region: aws.String(\"ap-northeast-1\")})\nif err != nil {\n    panic(err)\n}\n\nvar t *http.Transport\nswitch v := sess.Config.HTTPClient.Transport.(type) {\ncase *http.Transport:\n    t = v\ndefault:\n    // May need error handling\n}\n\n/* we can use cast instead of type switch\nif t, ok := sess.Config.HTTPClient.Transport.(*http.Transport); ok {\n}\n*/\n\nif t == nil {\n    t = &http.Transport{}\n}\n\nif t.TLSClientConfig == nil {\n    t.TLSClientConfig = &tls.Config{}\n}\n\nt.TLSClientConfig.MinVersion = tls.VersionTLS12\n\nYou can close this pull request after you read this because environment variable is not that meaningful here.\nThanks, again.\n. ",
    "tnclong": "Hi @jasdel \nI found my problem. \nI create a session in us-east-2 region but my topicArn is us-west-2.\nSorry for my noise and thanks for your help.\n. ",
    "philm": "Actually that extra field was a red herring. The actual cause is resource's launchTime - when set to null, it serializes fine, but when it is present (notice it is not a string type), the serializer barfs.\n\"resource\": {\n  \"instanceDetails\": {\n    \"launchTime\": 1484253279000,\nHopefully that's enough info for you. Otherwise, I can provide more details after the new year. Happy New Year!. Thanks @xibz - similar issue:\nOrganization ASN is serialized as a string per https://github.com/aws/aws-sdk-go/blob/v1.12.54/service/guardduty/api.go#L6816 but the GuardDuty API is returning an integer:\n\"organization\": {\n  \"asn\": 9929,\n  \"asnOrg\": \"CHINA UNICOM Industrial Internet Backbone\",. @xibz - so far, I'm seeing asn come back as an integer consistently in the API but the SDK expects it as a string.. ",
    "varroba": "@xibz : has there been any progress on this issue? I am seeing this issue too.. Thanks for looking into this @jasdel - this was reported by a member of my team. I am seeking more information and will try the log level you suggested. Wait out and I will get back to you.. Hi @jasdel,\nWe have managed to reproduce the issue, but not consistently. My colleague is seeing errors in cases where actionType is PORT_PROBE. The case in question has a portProbeDetails array with two elements under it. The first element has the asn as a quoted string, while the second has the asn as a bare integer.\nThe inconsistency here is that my colleague's code encounters a deserialisation error (failed decoding JSON RPC response), while my minimal test example fails silently, and simply does not include a portProbeAction block under the action block.\nSo there might be a problem in how the JSON is being created on the GuardDuty side rather than this being a problem with the Golang SDK?. Here is the offending block (from HTTP wire logging):\n\"portProbeAction\" : {\n                  \"blocked\" : false,\n                  \"portProbeDetails\" : [\n                     {\n                        \"remoteIpDetails\" : {\n                           \"organization\" : {\n                              \"asnOrg\" : \"SingleHop LLC\",\n                              \"isp\" : \"SingleHop LLC\",\n                              \"org\" : \"SingleHop LLC\",\n                              \"asn\" : \"32475\"\n                           },\n                           \"city\" : {\n                              \"cityName\" : \"Chicago\"\n                           },\n                           \"country\" : {\n                              \"countryName\" : \"United States\"\n                           },\n                           \"ipAddressV4\" : \"198.20.69.98\",\n                           \"geoLocation\" : {\n                              \"lon\" : -87.6441,\n                              \"lat\" : 41.8825\n                           }\n                        },\n                        \"localPortDetails\" : {\n                           \"portName\" : \"HTTP\",\n                           \"port\" : 80\n                        }\n                     },\n                     {\n                        \"localPortDetails\" : {\n                           \"portName\" : \"HTTPS\",\n                           \"port\" : 443\n                        },\n                        \"remoteIpDetails\" : {\n                           \"country\" : {\n                              \"countryName\" : \"United States\"\n                           },\n                           \"organization\" : {\n                              \"isp\" : \"SingleHop LLC\",\n                              \"asnOrg\" : \"SingleHop LLC\",\n                              \"org\" : \"SingleHop LLC\",\n                              \"asn\" : 32475\n                           },\n                           \"city\" : {\n                              \"cityName\" : \"Chicago\"\n                           },\n                           \"ipAddressV4\" : \"198.20.69.98\",\n                           \"geoLocation\" : {\n                              \"lat\" : 41.8825,\n                              \"lon\" : -87.6441\n                           }\n                        }\n                     }\n                  ]\n               }\nNote the inconsistent representation of the second asn.. Hi @jasdel,\nThe request with the inconsistent ASN representations was X-Amzn-Requestid: 45c5fb29-48ea-11e8-b3c6-ab68554d80a0.\nToday we are seeing different behaviour. Querying the same finding returns a completely different remote IP address, therefore a different ASN, which does not exhibit the integer/string inconsistency, and only appears once. The request is X-Amzn-Requestid: 5b5ec42b-49b7-11e8-8522-a704037df735. ",
    "jammerful": "Contacted the service team, and they said this is expected behavior. Closing the issue.\n@jasdel Thanks for the response.. ",
    "earlonrails": "Thnx for quick merge!. ",
    "bkatrenko": "@xibz thank you very much! \ud83d\udc4d  \ud83e\udd47 . ",
    "rawipfel": "I'm having a possibly related problem is reproducible with both Minio and Ceph radosgw. For both S3 object servers, SSE-C works if using awscli like this:\n```\necho \"pii data\" > ./secret-pii-data.txt\naws s3api create-bucket --bucket SSE-C \\\n--no-verify-ssl \\\n--endpoint-url https://192.168.1.105:8000\naws s3api put-object \\\n--no-verify-ssl \\\n--endpoint-url https://192.168.1.105:8000 \\\n--bucket SSE-C \\\n--key secret-pii-data \\\n--sse-customer-algorithm AES256 \\\n--sse-customer-key MzJieXRlc2xvbmdzZWNyZXRrZXltdXN0cHJvdmlkZWQ= \\\n--sse-customer-key-md5 7PpPLAK26ONlVUGOWlusfg== \\\nbut for both of the same S3 object servers, this AWS SDK go code fails with a S3 server error:\nfunc uploadFile(svc *s3.S3, bucket string, fileName string) error {\n// Open the file to read\nfile, err := os.Open(fileName)\nif err != nil {\n    return err\n}\ndefer file.Close()\n\n// Get file size and read the file's data into a buffer\nfileInfo, _ := file.Stat()\nvar size int64 = fileInfo.Size()\nbuffer := make([]byte, size)\nfile.Read(buffer)\n\n// Put the file's data into a bucket object, named fileName\n_, err = svc.PutObject(&s3.PutObjectInput{\n    Bucket:               aws.String(bucket),\n    Key:                  aws.String(fileName),\n    Body:                 bytes.NewReader(buffer),\n    ContentLength:        aws.Int64(size),\n    ContentType:          aws.String(http.DetectContentType(buffer)),\n    ContentDisposition:   aws.String(\"attachment\"),\n    SSECustomerAlgorithm: aws.String(\"AES256\"),\n    //\"32byteslongsecretkeymustprovided\"\n    SSECustomerKey:       aws.String(\"MzJieXRlc2xvbmdzZWNyZXRrZXltdXN0cHJvdmlkZWQ=\"),\n    //SSECustomerKey:       aws.String(base64Encode(\"32byteslongsecretkeymustprovided\")),\n})\nreturn err\n\n}\nMinio:\n$ go run main.go test11 filename.txt\nWaiting for bucket \"test11\" to be created...\nError uploading \"filename.txt\": InvalidArgument: The secret key was invalid for the specified algorithm.\n    status code: 400, request id: 1532DF41C9B506C9, host id\nCeph/S3:\n$ go run main.go test11 filename.txt\nWaiting for bucket \"test11\" to be created...\nError uploading \"filename.txt\": InvalidArgument: Requests specifying Server Side Encryption with Customer provided keys must provide an appropriate secret key.\n    status code: 400, request id: tx00000000000000000003a-005b0c3d36-2f635-default, host id:\nexit status 1\nFrom the Ceph radosgw debug log isERROR: invalid encryption key size:\n2018-05-28 17:32:38.777781 7fcaf1353700  2 req 58:0.000207:s3:PUT /test11/filename.txt:put_obj:verifying op params\n2018-05-28 17:32:38.777782 7fcaf1353700  2 req 58:0.000208:s3:PUT /test11/filename.txt:put_obj:pre-executing\n2018-05-28 17:32:38.777783 7fcaf1353700  2 req 58:0.000209:s3:PUT /test11/filename.txt:put_obj:executing\n2018-05-28 17:32:38.778424 7fcaf1353700  5 ERROR: invalid encryption key size\n2018-05-28 17:32:38.778451 7fcaf1353700  2 req 58:0.000877:s3:PUT /test11/filename.txt:put_obj:completing\n2018-05-28 17:32:38.778526 7fcaf1353700  2 req 58:0.000952:s3:PUT /test11/filename.txt:put_obj:op status=-22\n2018-05-28 17:32:38.778542 7fcaf1353700  2 req 58:0.000968:s3:PUT /test11/filename.txt:put_obj:http status=400\n2018-05-28 17:32:38.778547 7fcaf1353700  1 ====== req done req=0x7fcaf134d110 op status=-22 http_status=400 ======\n2018-05-28 17:32:38.778632 7fcaf1353700  1 civetweb: 0x557da4e3a000: 172.17.0.1 - - [28/May/2018:17:32:38 +0000] \"PUT /test11/filename.txt HTTP/1.1\" 400 0 - aws-sdk-go/1.13.47 (go1.10.2; darwin; amd64)\n```\nfrom the code here (is checking key size): https://github.com/ceph/ceph/blob/f4e95ca473d0e64aafb03e14a61f88a97a5c5cc1/src/rgw/rgw_crypt.cc#L958\nAfaict, SSECustomerKey is not being passed to S3 correctly (it's supposed to be a base64 encoded 32 byte string for AES256). I'm still debugging this, comments are welcome.. Please ignore the prior comment, was my mistake, SSECustomerKey must be set to a 32 byte string, and the AWS SDK will base64 encode it. . This worked for me, noting that there is a discrepancy between the AWS Java versus go SDKs regarding the customer key argument. In Go, the customer key argument is a 32-char string. In Java it's a base64 encoding of the 32-char string. 32 chars * 8 bits per char = 256 bits needed for AES256.\nGo:\n// Put the file's data into a bucket object, named fileName\n    _, err = svc.PutObject(&s3.PutObjectInput{\n        Bucket:               aws.String(bucket),\n        Key:                  aws.String(fileName),\n        Body:                 bytes.NewReader(buffer),\n        ContentLength:        aws.Int64(size),\n        ContentType:          aws.String(http.DetectContentType(buffer)),\n        ContentDisposition:   aws.String(\"attachment\"),\n        SSECustomerAlgorithm: aws.String(\"AES256\"),\n        SSECustomerKey:       aws.String(\"32_byte_string_needed_for_AES256\"),\n    })\nJava (from https://github.com/confluentinc/kafka-connect-storage-cloud/pull/173):\npublic void uploadPart(ByteArrayInputStream inputStream, int partSize) {\n      int currentPartNumber = partETags.size() + 1;\n      UploadPartRequest request = new UploadPartRequest()\n                                            .withBucketName(bucket)\n                                            .withKey(key)\n                                            .withUploadId(uploadId)\n                                            .withSSECustomerKey(sseCustomerKey)\n                                            .withInputStream(inputStream)\n                                            .withPartNumber(currentPartNumber)\n                                            .withPartSize(partSize)\n                                            .withGeneralProgressListener(progressListener);\n      log.debug(\"Uploading part {} for id '{}'\", currentPartNumber, uploadId);\n      partETags.add(s3.uploadPart(request).getPartETag());\n    }\nwhere sseCustomerKey =\nfinal String sseCustomerKeyConfig = conf.getSseCustomerKey();\n    this.sseCustomerKey = (SSEAlgorithm.AES256.toString().equalsIgnoreCase(ssea)\n        && StringUtils.isNotBlank(sseCustomerKeyConfig))\n      ? new SSECustomerKey(sseCustomerKeyConfig) : null;\nwhere:\n```\nUncomment the following two s3.sse* lines to enable SSE-C\ns3.ssea.name=AES256\ns3.sse.customer.key=MzJfYnl0ZV9zdHJpbmdfbmVlZGVkX2Zvcl9BRVMyNTY=\nNB, key=base64encode(\"32_byte_string_needed_for_AES256\")\n```. ",
    "Puneeth-n": "@xibz We use Terraform to provision our Infrastructure. We are seeing it in every other deployment. My guess is MaxRetries isn't working because InvalidParameterValueException is not a throttling exception. @jasdel Thanks for getting back to us. We added a retry logic in Terraform. Will post it in the AWS forums!. ",
    "diehlaws": "@btai24 The alarms you're describing are part of CloudWatch rather than EC2 Auto-Scaling. You'll need to issue a cloudwatch.PutMetricAlarm request with the criteria you're using for your scaling policy in order to create a CloudWatch alarm.. @luoxiaoxun Did you still need assistance uploading objects to S3 using the AWS SDK for Go? If so, can you please give us more details on the trouble you're running into with this task?. I have tried reproducing this using an S3 Bucket in us-east-1 on different platforms (local workstation, t2.micro in us-west-2, t2.micro in ap-southeast-2, third party VPS in NY) with different versions of the AWS SDK for Go (v1.12.67 and v1.15.51), and have not been able to elicit a connection reset from S3 under any of these circumstances. This does not appear to be an issue with the AWS SDK for Go, you can confirm this by running your code on an EC2 instance, or attempting the same actions using the AWS SDK for another language, and checking for the same connection resets from S3.\nI suggest further troubleshooting this behavior from the service end with AWS Support by opening a new support case under the Simple Storage Service (S3) service if you haven't already, as they will be able to better assist with behaviors relating to the service rather than the SDK.\nAs an aside, in my experience issuing GET or PUT requests to S3 at a concurrency above 20 generally results in diminishing returns in terms of bandwidth - specially if you are performing single-part uploads/downloads as shown in your code rather than multi-part upload/downloads (for example using s3manager.Downloader as @xibz suggested).. @Broham S3 Select is supported in the AWS SDK for Go as of June of this year. Here is an AWS Blog post that includes examples to query an object, select records, perform concurrent stream processing and dealing with error handling.. @fkerlach Unfortunately I'm not able to reproduce this using the latest version of the SDK (v1.15.51), do you still see this behavior when using a version newer than v1.13.33?. Object-level logging is actually a feature of CloudTrail that has been integrated into the S3 Console. In order to determine object-level logging status of an S3 Bucket via API you'd have to issue a GetEventSelectors call for your account's trails in CloudTrail.. Hey @dlsniper are you still running into these errors when importing github.com/aws/aws-sdk-go/service/s3 instead of github.com/awslabs/aws-sdk-go/service/s3?. Thanks for reaching out about this @vitaly-zdanevich, and apologies for the delay in our response. I tried reproducing this behavior but including StartTime in cloudwatchlogs.FilterLogEventsInput is working as expected on my end.\n```go\nfunc main() {\n        sess := session.Must(session.NewSession())\n        svc := cloudwatchlogs.New(sess, aws.NewConfig())\n        logGroupName := \"/aws/lambda/EncryptBucket\"\n        startTime := int64(1528230870000)\n        input := cloudwatchlogs.FilterLogEventsInput{\n                LogGroupName: &logGroupName,\n                StartTime:    &startTime,\n        }\n    resp, _ := svc.FilterLogEvents(&input)\n    println(len(resp.Events))\n\n}\n```\n$ go run filterLogs.go\n1063\nCommenting out the StartTime field in cloudwatchlogs.FilterLogEventsInput and the startTime variable returns the same number for the same log group.\n$ go run filterLogs-noStartTime.go\n1063\nAs a sanity check, looking up the same log group via the AWS CLI shows the same number of Event IDs in the output.\n$ aws logs filter-log-events --log-group-name \"/aws/lambda/EncryptBucket\" | grep eventId | wc -l\n1063\nAre you still seeing this behavior using the latest version of the AWS SDK for Go (1.15.12)?. Just to rule out your Lambda function's log group and stream as the potential cause for this, could you run the following to create a new log group and stream, and add events to see if it exhibits the same behavior as your code?\n```go\n// LogCheck.go creates a log group and a log stream inside the log group,\n// then adds events to the log stream and tries to read them back.\npackage main\nimport (\n    \"fmt\"\n    \"time\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/cloudwatchlogs\"\n\n)\nvar (\n    // Initializing CloudWatch logs client\n    logsClient = cloudwatchlogs.New(session.Must(\n        session.NewSession(&aws.Config{\n            Region: aws.String(\"us-east-1\"),\n        }),\n    ))\n)\nfunc main() {\n    // Unix timestamp at runtime gets appended to log group/stream names\n    // to avoid running into AlreadyExists exceptions\n    timestamp := time.Now().UnixNano() / int64(time.Millisecond)\n    logGroup := fmt.Sprintf(\"testGroup%d\", timestamp)\n    logStream := fmt.Sprintf(\"testStream%d\", timestamp)\n    eventCount := 10\nif _, err := createGroup(logGroup); err != nil {\n    panic(err)\n}\n\nif _, err := createStream(logGroup, logStream); err != nil {\n    panic(err)\n}\n\nif _, err := addEvents(logGroup, logStream, eventCount); err != nil {\n    panic(err)\n}\n\n// Attempting to filter events immediately after adding them can result\n// in the newly added events not being returned\nfmt.Println(\"Waiting a second for logs to populate\")\ntime.Sleep(1 * time.Second)\n\nif _, err := filterEvents(logGroup, timestamp); err != nil {\n    panic(err)\n}\n\n}\nfunc createGroup(logGroup string) (*cloudwatchlogs.CreateLogGroupOutput, error) {\n    // Preparing input for CreateLogGroup call\n    params := cloudwatchlogs.CreateLogGroupInput{\n        LogGroupName: &logGroup,\n    }\nfmt.Println(\"Creating Log Group\", *params.LogGroupName)\n\nreturn logsClient.CreateLogGroup(&params)\n\n}\nfunc createStream(logGroup string, logStream string) (*cloudwatchlogs.CreateLogStreamOutput, error) {\n    // Preparing input for CreateLogStream call\n    params := cloudwatchlogs.CreateLogStreamInput{\n        LogGroupName:  &logGroup,\n        LogStreamName: &logStream,\n    }\nfmt.Println(\"Creating Log Stream\", *params.LogStreamName, \"in log group\", *params.LogGroupName)\n\nreturn logsClient.CreateLogStream(&params)\n\n}\nfunc addEvents(logGroup string, logStream string, eventCount int) (cloudwatchlogs.PutLogEventsOutput, error) {\n    params := cloudwatchlogs.PutLogEventsInput{\n        LogEvents:     []cloudwatchlogs.InputLogEvent{},\n        LogGroupName:  &logGroup,\n        LogStreamName: &logStream,\n    }\n// Adding events to params before calling PutLogEvents\nfor i := 0; i < eventCount; i++ {\n    message := fmt.Sprintf(\"test%d\", i)\n    timestamp := time.Now().UnixNano() / int64(time.Millisecond)\n    logEvent := cloudwatchlogs.InputLogEvent{\n        Message:   &message,\n        Timestamp: &timestamp,\n    }\n    params.LogEvents = append(params.LogEvents, &logEvent)\n}\n\nfmt.Println(\"Adding \", len(params.LogEvents), \"events to stream\", logStream, \"in log group\", logGroup)\n\nreturn logsClient.PutLogEvents(&params)\n\n}\nfunc filterEvents(logGroup string, timestamp int64) (*cloudwatchlogs.FilterLogEventsOutput, error) {\n    params := cloudwatchlogs.FilterLogEventsInput{\n        LogGroupName: &logGroup,\n        StartTime:    &timestamp,\n    }\n    fmt.Println(\"Filtering logs in group\", logGroup)\n    logs, err := logsClient.FilterLogEvents(&params)\n    fmt.Println(\"Found\", len(logs.Events), \"logs in group\")\nreturn logs, err\n\n}\n``. Thanks for the update @vitaly-zdanevich. Your test of the above code shows that theFilterLogEventscall works as expected when including theStartTimeparameter inFilterLogEventsInput. Unfortunately, this doesn't explain the behavior you've described on the log group(s) for the Lambda function(s) running on your account. If you're still seeing that your CloudWatch log groups aren't returning the expected events, you may want to seek further guidance with the service by opening a [Premium Support case](https://console.aws.amazon.com/support/v1?#/case/create) under the CloudWatch service, or posting in the [AWS Forums](https://forums.aws.amazon.com/forum.jspa?forumID=138).. Hey @jney, the [API reference](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Operations_Amazon_DynamoDB_Accelerator.html) fordax` does not include a ListTables call, this feature request is better geared for the DynamoDB AWS Forums as @jasdel mentioned.\nPlease let us know if you need further assistance or have any other questions for us about the AWS SDK for Go.. Hey @discordianfish, apologies for the delay on this. I was able to reproduce the behavior you describe, however it definitely seems like a bug in the service's API since I'm not quite able to bring my test queue's in-flight message count up to 120,000 using various methods (AWS SDK for Go, AWS SDK for PHP, and AWSCLI). After several attempts, the highest in-flight message count I saw for my test queue was 119,915, after which sending additional ReceiveMessage calls via the AWSCLI and the AWS SDKs for Go and PHP would simply return an empty response (as if the queue had no messages at all).\nAs @jasdel mentioned, the best way to reach out to the SQS service about this is via the AWS Forums. That being said, I will continue testing this behavior with more thorough documentation of my steps and will reach out to the SQS service team internally to raise awareness about this.. Hey @EduardShaid were you looking for AWS Step Functions as @jasdel mentioned? Or are you looking for something different to be implemented in the AWS SDK for Go?. Typo has been fixed, closing issue.. Thanks for reaching out to us about this @dav009. Unfortunately I'm not able to reproduce the behavior you're describing, however I'm using newer versions of Go (1.10.3) and the AWS SDK for Go (1.14.22) to test your code. \nAre you still seeing this behavior after updating Go and the AWS SDK for Go to the latest version? If so, it sounds like this may be specific to the document(s) to which you're trying to add permissions, or possibly your WorkDocs site configuration. I suggest reaching out to our Premium Support team by opening a support case under the WorkDocs service, or posting in the AWS Forums for further guidance with this.. Thanks for reaching out to us @HarishAtGitHub. Since the AWS SDK for Go does not define a struct for SNS or SQS notifications, you'll need to define your own with the keys that you want to use from the JSON message from SQS. For example, if you only wanted the three fields you mentioned (resourceType, resourceId, awsRegion) your struct would look like the following:\ngo\ntype SQSMessage struct {\n    ResourceType    string\n    ResourceId      string\n    AwsRegion       string\n}\nIf you wanted a key-pair value nested inside an object in the response (e.g. changeType, snapshotId) in addition to the key-value pairs you mentioned, your struct will become a little more complex:\ngo\ntype SQSMessage struct {\n    ConfigurationItemDiff struct {\n        ChangeType  string\n    }\n    ConfigurationItem struct {\n        Configuration struct {\n            CreateTime  time.Time\n            SnapshotID  string\n        }\n    }\n    ResourceType    string\n    ResourceId  string\n    AwsRegion   string\n}\nOnce the struct is defined you can call json.Unmarshal, passing it a []byte of JSON data (we'll call it response in this example) and a pointer to a defined variable of the same type as the struct (we'll call it message in this example). You can then call the key-pair values as needed:\ngo\nvar message SQSMessage\nerr := json.Unmarshal(response, &message)\nfmt.Println(message.ResourceType)\nfmt.Println(message.ConfigurationItem.Configuration.SnapshotID). @alexbilbie thanks for reaching out. This seems like a reasonable feature request so I've labeled the issue as such.. Thank you for your patience in this matter @alexbilbie. After putting some thought into this feature request, we've decided not to implement it as changing the type for these properties would be a breaking change. Additionally, these properties are modeled by the service rather than the AWS SDK for Go, so any customization done to these properties would be automatically undone the next time the SDK is updated.\nWhile we appreciate that this change would enhance usability of the AWS SDK for Go when interacting with SES, it is more likely to be accepted if it is raised as a feature request to the SES service in the AWS Forums.. Hey @danielvaughan, thanks for reaching out. SNS initialization is closer to STS than S3 in the AWS SDK for Go. Following is an example function that initializes SNS and lists subscriptions in the us-west-2 region.\ngo\nfunc main() {\n    svc := sns.New(session.New(), aws.NewConfig().WithRegion(\"us-west-2\"))\n    subs, err := svc.ListSubscriptions(&sns.ListSubscriptionsInput{})\n    if err != nil {\n        fmt.Println(err.Error())\n        return\n    }\n    fmt.Println(subs)\n}. Hey @HarishAtGitHub, thanks for reaching out to us. The output you're providing is a standard API response for AWS Config rather than output specific to the AWS SDK for Go. That being said, I can help clarify some of your questions.\n1) When issuing a GetComplianceSummaryByConfigRule call, \"CompliantResourceCount\" refers to the number of AWS Config rules that are in compliance, and \"NonCompliantResourceCount\" refers to the number of AWS Config rules that are not in compliance. By contrast, when issuing a GetComplianceSummaryByResourceType call, \"CompliantResourceCount\" refers to the number of AWS resources in your account that are in compliance, and \"NonCompliantResourceCount\" refers to the number of AWS resources in your account that are not in compliance.\n2) Unfortunately the AWS Config documentation does not make it very clear what the \"CapExceeded\" parameter indicates beyond what the name implies. This would be a good question to ask in the AWS Config Forums, alternately you could submit feedback on the bottom of the documentation page for the ComplianceContributorCount data type to request clarification on this parameter's meaning and usage.\n3) The suffix \"ByConfigRule\" aims to differentiate the GetComplianceSummaryByConfigRule call from GetComplianceSummaryByResourceType as it returns the number of rules that are compliant and noncompliant rather than returning the number of resources that are compliant and noncompliant. If you're looking for compliance details for a specific config rule you'll want to use GetComplianceDetailsByConfigRule.. Thanks for reaching out to us about this @chenko47. \nThe AWS CLI command you've referenced is for the AWS Organizations service rather than for IAM. The function equivalent to this command within the AWS SDK for Go would be DescribePolicy. \nWhile IAM does not have an API call to return the body of a specified IAM policy, you can use the GetAccountAuthorizationDetails function to retrieve all IAM users, groups, roles, and policies and then list the one(s) you need. You can see a code example that uses this function to locate IAM users that have Administrator privileges here.. Hey @rubensf, thanks for reaching out to us about this. The different error responses you're seeing from S3 between HeadObject and GetObject are defined by the service rather than the AWS SDK for Go. You can see the same difference in responses from S3 when using curl to HEAD and GET a nonexistent object in a bucket.\n```bash\n$ curl -I https://s3.amazonaws.com//asdf\nHTTP/1.1 404 Not Found\nx-amz-request-id: RID\nx-amz-id-2: HostID\nContent-Type: application/xml\nTransfer-Encoding: chunked\nDate: Fri, 10 Aug 2018 22:55:50 GMT\nServer: AmazonS3\n$ curl https://s3.amazonaws.com//asdf\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\nNoSuchKeyThe specified key does not exist.asdfRIDHostID\n```\nSince this behavior relates to the S3 service and is not specific to the AWS SDK for Go, I'd suggest posting about this in the AWS Forums for S3. Thanks for bringing this to our attention @SarangMane. I've labeled this issue as a Bug as I'm able to reproduce this behavior from my end.\nWe will look into what's causing this behavior, and will work toward resolving it soon.. This behavior is caused by the AWS SDK for Go automatically cleaning up URIs in requests to a service, which is a feature typically seen only when interacting with S3. This functionality can be disabled by setting DisableRestProtocolURICleaning to true in the Config struct for NewConfig().. @atsushi-ishibashi Thanks for reaching out about this. This seems like a reasonable feature to implement so I've labeled the issue accordingly. We will keep the thread updated as more information is made available for this feature.. @atsushi-ishibashi apologies for the delay in my response. You most certainly can! We'd love to review a pull request for this.. @atsushi-ishibashi thank you for your patience on this. I have spoken to the rest of the team about implementing this feature. We have come to the conclusion that, since the SDK is a low-level wrapper for the API, higher level functions like this would better serve as an external package that uses the SDK rather than a function that is built in to the SDK. . Hi @JonDuffy, thanks for reaching out to us about this. I'm able to reproduce the behavior you describe, however this looks to be an issue with the service API since I can reproduce this behavior using AWSCLI v1.16.4 and the AWS SDK for PHP v3.67.2 in addition to the AWS SDK for Go v1.15.24.\nSince this behavior is not specific to the AWS SDK for Go, I suggest reaching out to our Premium Support team by opening a support case under the Glue service, or posting in the AWS Forums for further guidance with this.. Hi @karlgrz, thanks for reaching out to us. After some testing with the AWSCLI and the AWS SDK for Go, it looks like the behavior for the ListObjects delimiter is the same between the two. I noticed in your AWSCLI output that the response includes common prefixes but does not return any keys, however in your code sample you return only the keys included in the service's response and not the common prefixes. I mirrored your prefix structure in a test bucket and used your code to list objects in these prefixes, replacing \nreturn keys, nil with fmt.Println(response), and I see the same CommonPrefixes output as your AWSCLI command along with some additional information not shown by the AWSCLI.\nTest bucket structure:\n$ aws s3 ls s3://TESTBUCKET/go-2130/ --recursive\n2018-09-04 23:59:58          0 go-2130/\n2018-09-05 00:00:28          3 go-2130/en-GB/123.json\n2018-09-05 00:00:28          3 go-2130/en-US/123.json\n2018-09-05 00:00:28          8 go-2130/unknown/123.json\nGo code output:\n```\n$ go run .\\listobjects.go\n{\n  CommonPrefixes: [{\n      Prefix: \"go-2130/en-GB/\"\n    },{\n      Prefix: \"go-2130/en-US/\"\n    },{\n      Prefix: \"go-2130/unknown/\"\n    }],\n  Contents: [{\n      ETag: \"\\\"d41d8cd98f00b204e9800998ecf8427e\\\"\",\n      Key: \"go-2130/\",\n      LastModified: 2018-09-04 23:59:58 +0000 UTC,\n      Owner: {\n        DisplayName: \"\",\n        ID: \"\"\n      },\n      Size: 0,\n      StorageClass: \"STANDARD\"\n    }],\nDelimiter: \"/\",\nEncodingType: \"url\",\nIsTruncated: false,\nMarker: \"\",\nMaxKeys: 1000,\nName: \"TESTBUCKET\",\nPrefix: \"go-2130/\"\n}\n```\nFurther testing with the AWSCLI shows that, in order for list-objects to return the expected keys, the character(s) specified in the --delimiter argument need to be included in the --prefix argument, otherwise the --delimiter flag acts as a NOT IN operator as you describe:\n$ aws s3api list-objects --bucket TESTBUCKET --prefix \"go-2130/\" --delimiter \"/\"\n{\n    \"CommonPrefixes\": [\n        {\n            \"Prefix\": \"go-2130/en-GB/\"\n        },\n        {\n            \"Prefix\": \"go-2130/en-US/\"\n        },\n        {\n            \"Prefix\": \"go-2130/unknown/\"\n        }\n    ],\n    \"Contents\": [\n        {\n            \"LastModified\": \"2018-09-04T23:59:58.000Z\",\n            \"ETag\": \"\\\"d41d8cd98f00b204e9800998ecf8427e\\\"\",\n            \"StorageClass\": \"STANDARD\",\n            \"Key\": \"go-2130/\",\n            \"Owner\": {\n                \"DisplayName\": \"<redacted>\",\n                \"ID\": \"<redacted>\"\n            },\n            \"Size\": 0\n        }\n    ]\n}\n$ aws s3api list-objects --bucket TESTBUCKET --prefix \"go-2130\" --delimiter \"/\"\n{\n    \"CommonPrefixes\": [\n        {\n            \"Prefix\": \"go-2130/\"\n        }\n    ]\n}\n$ aws s3api list-objects --bucket TESTBUCKET --prefix \"go-2130/\" --delimiter \"en\"\n{\n    \"CommonPrefixes\": [\n        {\n            \"Prefix\": \"go-2130/en\"\n        }\n    ],\n    \"Contents\": [\n        {\n            \"LastModified\": \"2018-09-04T23:59:58.000Z\",\n            \"ETag\": \"\\\"d41d8cd98f00b204e9800998ecf8427e\\\"\",\n            \"StorageClass\": \"STANDARD\",\n            \"Key\": \"go-2130/\",\n            \"Owner\": {\n                \"DisplayName\": \"<redacted>\",\n                \"ID\": \"<redacted>\"\n            },\n            \"Size\": 0\n        },\n        {\n            \"LastModified\": \"2018-09-05T00:00:28.000Z\",\n            \"ETag\": \"\\\"d9f7497c382d9ee2709f9d1b560aecaf\\\"\",\n            \"StorageClass\": \"STANDARD\",\n            \"Key\": \"go-2130/unknown/123.json\",\n            \"Owner\": {\n                \"DisplayName\": \"<redacted>\",\n                \"ID\": \"<redacted>\"\n            },\n            \"Size\": 8\n        }\n    ]\n}\n$ aws s3api list-objects --bucket TESTBUCKET --prefix \"go-2130/en-US\" --delimiter \"US\"\n{\n    \"Contents\": [\n        {\n            \"LastModified\": \"2018-09-05T00:00:28.000Z\",\n            \"ETag\": \"\\\"d10acba4acaca68b887d94b595e71ec6\\\"\",\n            \"StorageClass\": \"STANDARD\",\n            \"Key\": \"go-2130/en-US/123.json\",\n            \"Owner\": {\n                \"DisplayName\": \"<redacted>\",\n                \"ID\": \"<redacted>\"\n            },\n            \"Size\": 3\n        }\n    ]\n}\n$ aws s3api list-objects --bucket TESTBUCKET --prefix \"go-2130/en-GB\" --delimiter \"/\"\n{\n    \"CommonPrefixes\": [\n        {\n            \"Prefix\": \"go-2130/en-GB/\"\n        }\n    ]\n}\n$ aws s3api list-objects --bucket TESTBUCKET --prefix \"go-2130/en-GB/\" --delimiter \"/\"\n{\n    \"Contents\": [\n        {\n            \"LastModified\": \"2018-09-05T00:00:28.000Z\",\n            \"ETag\": \"\\\"fad7aa65f9b2a35af671791f4d883839\\\"\",\n            \"StorageClass\": \"STANDARD\",\n            \"Key\": \"go-2130/en-GB/123.json\",\n            \"Owner\": {\n                \"DisplayName\": \"<redacted>\",\n                \"ID\": \"<redacted>\"\n            },\n            \"Size\": 3\n        }\n    ]\n}\n$ aws s3api list-objects --bucket TESTBUCKET --prefix \"go-2130/en-GB/\" --delimiter \"en\"\n{\n    \"Contents\": [\n        {\n            \"LastModified\": \"2018-09-05T00:00:28.000Z\",\n            \"ETag\": \"\\\"fad7aa65f9b2a35af671791f4d883839\\\"\",\n            \"StorageClass\": \"STANDARD\",\n            \"Key\": \"go-2130/en-GB/123.json\",\n            \"Owner\": {\n                \"DisplayName\": \"<redacted>\",\n                \"ID\": \"<redacted>\"\n            },\n            \"Size\": 3\n        }\n    ]\n}\nWhile I realize this may not help much as far as the correct usage of the ListObjects delimiter for your use case, these tests safely rule out this behavior as being specific to the AWS SDK for Go - updating the documentation for this parameter in the S3 API reference, AWSCLI, and AWS SDK for Go (and probably other SDKs) seems like the best approach to address this. I will reach out to the appropriate team internally to request better wording for this property and include an example of its proper usage, but if you submit feedback via the Feedback button on the GET Bucket (List Objects) API documentation page this would help raise awareness and likely result in the documentation getting updated faster.. @grubernaut Thanks for reaching out to us about this. Unfortunately I'm not able to reproduce this behavior, or perhaps I'm not fully understanding the issue you're experiencing. The output for a successful SuspendProcesses call is expected to be empty, however it sounds like you're getting a successful (empty) response despite this process not being successfully suspended for your empty auto-scaling group. Please do let me know if my understanding of this situation is incorrect.\nWhen running the provided code sample on a test ASG with no instances, the process I'm specifying to suspend for my empty ASG (AZRebalance in this example) does appear to be suspended when I check that ASG in the AWS console. If I'm already looking at the ASG in the console before running the code, I need to refresh the page to see the change in Suspended Processes.\nWhat is the result when you check SuspendedProcesses in the output of DescribeAutoScalingGroups for your empty auto-scaling group after suspending a process on that group? You can list your auto-scaling group's suspended processes using the function below.\n```go\nfunc readSusp(name string) {\n    params := &autoscaling.DescribeAutoScalingGroupsInput{\n        AutoScalingGroupNames: []*string{\n            aws.String(name), // replace name with your ASG name if not using a variable for this\n        },\n    }\nresp, err := asg.DescribeAutoScalingGroups(params)\nif err != nil {\n    fmt.Println(err.Error())\n    return\n}\nfor _, group := range resp.AutoScalingGroups {\n    if len(group.SuspendedProcesses) > 0 {\n        fmt.Printf(\"ASG %s has suspended process(es):\\n\",\n            *group.AutoScalingGroupName)\n        for i, process := range group.SuspendedProcesses {\n            fmt.Println(i+1, \" - \", *process.ProcessName)\n        }\n    } else {\n        fmt.Printf(\"ASG %s has no suspended processes\",\n            *group.AutoScalingGroupName)\n    }\n}\n\n}\n``. @zhejiananzhuren Are you still experiencing this issue? If so can you please provide the full error message and debug output as @jasdel requested, along with the versions of Go and the AWS SDK for Go you're using when encountering this behavior?. Thanks for reaching out to us about this @mennis. While the EC2 API documentation showsrequestId` as a response element for a number of EC2 API calls, this element is not included in the response by most of the AWS SDKs. We are investigating the best way to expose this field within the SDK, however in the meantime you can retrieve request IDs for these API calls by enabling debug logging with HTTP body in the service client as shown below.\ngo\nlog := aws.LogDebugWithHTTPBody\nsvc := ec2.New(session.New(&aws.Config{Region: aws.String(\"us-west-2\"), LogLevel: &log})). Thank you for your feedback on this @polothy. The behavior you're seeing is expected, other ways to specify the region for the AWS SDK for Go are documented here.\nReading the region from the shared configuration file is disabled by default for backwards compatibility. Our current stance with version 1 of the SDK is that enabling this behavior by default would be an unexpected change from the user's perspective, so we have left if disabled with the option to enable it with the AWS_SDK_LOAD_CONFIG environment variable as you have mentioned.. Hi @bflad, thanks for bringing this to our attention. We have reached out to the EC2 team internally about this, they are actively working on a fix. I've associated the internal request with your Support case so you should receive future updates on this matter from that end.. Hi @beiriannydd, thanks or your feedback on this topic. In order to maintain organized discussions on issues we encourage commenting on existing issues rather than re-initiating the discussion on a new issue. I am closing this issue as a duplicate, however your feedback has been included in the internal discussion for the implementation of this feature.. Thanks for reaching out about this @dovreshef. I was able to reproduce this behavior both on the AWS SDKs for Go and PHP with presigned URLs for EC2, IAM, STS, and Route 53. The only service I observed that invalidated a presigned URL after the time specified in the \"x-amz-expires\" header (instead of the default 15 minutes) was S3.\nThis consistent behavior across multiple SDKs and services indicates an issue with the service API rather than the AWS SDK for Go. As such, the best approach to raise awareness of this behavior would be to notify Premium Support via a new case under the IAM service, this way the service team receives the request to address this unexpected behavior via the expected channels.. The Signature Version 4 documentation includes X-Amz-Expires in a sample query string that contains all request parameters; the code example that @dovreshef mentioned for Using GET with Authentication Information in the Query String (Python)\n includes the X-Amz-Expires header when generating a signed URL for the IAM CreateUser API call. The combination of these two documentation pages could easily lead users to believe that this header should be supported by services other than S3 (at least IAM).\nI haven't been able to find any documentation that authoritatively states whether this header is supported by services other than S3. If S3 is the only service that supports this header I agree that the SDK's documentation should be updated to reflect that - including a note in the description for this header in S3's SigV4 documentation stating that this header is exclusive to presigned URLs for this service would also be helpful.. Thanks for bringing this to our attention @ysugimoto. The API models and their documentation are provided by the Elastic Beanstalk team to the SDK teams, I'll reach out to them internally to let them know about this.. @ysugimoto the Elastic Beanstalk team has fixed this typo from their end, the change should be reflected on our end soon. . Thanks for reaching out about this @imabilal. Your code in the first snippet is not changing the number of times the service client should retry each request, which is 3 by default (for a total of 4 requests sent each time). As a result, it makes sense that your requests time out after 20 seconds when your timeout duration is 5 seconds.\nIncluding MaxRetries: aws.Int(0) inside your config struct will result in your requests timing out after the specified duration as expected.. Thanks for reaching out to us @chaitanya11. The CreateRole API call for IAM only allows one IAM policy (PermissionsBoundary) to be set at the time of the role's creation. If you need to attach multiple IAM policies to your new role they must be attached with the AttachRolePolicy function after the role has been created. \nBelow is an example to create a role with EC2's default trust relationship policy and the AWS Managed Policy AmazonSQSReadOnlyAccess. If you want to attach a Customer Managed Policy to your new role when it is being created, you will need to create the policy before creating the role and use the policy's ARN for the PermissionsBoundary value in the CreateRoleInput struct.\n```go\nparams := &iam.CreateRoleInput{\n  AssumeRolePolicyDocument: aws.String(\"{\\\"Version\\\": \\\"2012-10-17\\\",\\\"Statement\\\": [{\\\"Effect\\\": \\\"Allow\\\",\\\"Principal\\\": {\\\"Service\\\": \\\"ec2.amazonaws.com\\\"},\\\"Action\\\": \\\"sts:AssumeRole\\\"}]}\"),\n  Description:              aws.String(\"Role description\"),\n  PermissionsBoundary:      aws.String(\"arn:aws:iam::aws:policy/AmazonSQSReadOnlyAccess\"),\n  RoleName:                 aws.String(\"rolename\"),\n}\nresp, err := svc.CreateRole(params)\nif err != nil {\n  fmt.Println(err.Error())\n}\nfmt.Println(resp)\n``. Unfortunately theCreateRoleAPI call does not have the capability to create and attach an inline policy, you will need to issue aPutRolePolicycall to do this after the role has been created. ThePolicyDocumentelement of thePutRolePolicyInputstruct can be formatted in a similar way to theAssumeRolePolicyDocumentshown above, escaping double quotes within the string and keeping the policy on a single line.. Thanks for reaching out to us @saravanan30erd. When creating a new Lambda function via the web console, the request body for theCreateFunctionAPI call includes some data as theZipFileparameter's value that equates to the template that is populated in the code section of the function once it is created. While the SDKs do not explicitly mirror this particular behavior, the requirements for the API call are the same in the SDKs - theCodeparameter of the [CreateFunction`](https://docs.aws.amazon.com/lambda/latest/dg/API_CreateFunction.html) API call is required, and inside this parameter you must specify either the S3 Bucket and Key where your code resides (and version if using a versioned bucket), or the local ZipFile to upload as the function's code.\nUnfortunately this point is not made explicitly clear in the documentation for the API call itself, or in the code example for our SDK's function mapping to this API call. I will reach out to the Lambda documentation team internally about this so that this point can be clarified both in the API reference and the code example.. Thanks! As @jasdel mentioned in the PR, API models and code examples are provided by the service team for the various AWS SDKs to use. As such, any changes made to this from our end would be undone next time we receive an updated API model from the Lambda team. That being said, I've attached a link to your PR in my internal request for this as a suggestion to update the code example.. The event for Secrets Manager described in the Service Health Dashboard newsfeed exclusively impacted our us-west-2 region between 02:00 and 07:50 PDT. If this is the region in which your Lambda function retrieves secrets from Secrets Manager and you are no longer seeing intermittent errors as described above, this event is the most likely cause of the errors that you were seeing.\nIf you continued to see these errors after 08:00 PDT there's a chance you were receiving these errors for another reason, in that case reviewing Request IDs associated with these error responses would give us more insight on what's happening here.. @vkrishs Are you still seeing intermittent errors when interacting with Secrets Manager using the AWS SDK for Go? if so, can you try updating the SDK to v1.15.47 or newer and provide the full error received?. @imabilal Thanks for reaching out about this. ValidateResponse is not the best handler to use when retrieving error messages since that step in the processing of the API call occurs too early to retrieve the response from the service.\nIf you are wanting to capture errors before the API call is retried, the Retry handler would be the correct one to use - by contrast, if you want to capture the error returned after the SDK evaluates whether any retries are needed (and executed if the error is retryable) you'd use the AfterRetry handler. If you want to capture details for the API call after it terminates (e.g. if you want to capture details for successful responses as well as errors) you'd want to use the Complete handler instead.\nYour first code snippet does not return the ListObjects response body in its output because HTTPResponse.Body reads the response body directly from the network, so once the response body has been read (in this case by your function requestErrorHandler) it is no longer available for re-use elsewhere. Further discussion of this behavior in Go can be found here.. @btai24 Thanks for reaching out to us. The API call you're looking for falls under the AWS Resource Groups API rather than the AWS Systems Manager API. The documentation for the CreateGroup function in the AWS SDK for Go can be found here.. Hi @marekt77, thanks for reaching out about this and apologies for the delay in our response. Unfortunately I'm not able to reproduce the behavior described here and in #1506, I've tested tasks with both EC2 and FARGATE launch types and the waiter returns the expected result within 5-15 seconds of the task's status changing to STOPPED in the console.\nCan you enable verbose logging in your session to review the responses for the requests sent out by the waiter? It's possible that the waiter is getting 4xx responses from the service (e.g. HTTP 403 due to ecs:DescribeTasks not being allowed in the IAM Policy applied to the IAM user/role being used to run your code). You can enable verbose logging by adding LogLevel: aws.LogLevel(aws.LogDebugWithHTTPBody) to your session as shown below.\ngo\n    sess := session.New(&aws.Config{\n        Region:   aws.String(\"us-west-2\"),\n        LogLevel: aws.LogLevel(aws.LogDebugWithHTTPBody),\n    })\n    svc := ecs.New(sess). Thanks for reaching out to us @akamensky, I'm sorry to hear about your experience uploading content for an S3 Static Website. Please link the documentation page that shows standard headers needing to be entered as user-defined metadata so it can be updated as this is certainly not the case.\nWhen uploading objects to S3 using the AWS SDK for Go you can set standard headers such as Cache-Control, Content-Disposition, Content-Encoding, and Content-Type directly in the input struct rather than under Metadata like so.\ngo\ninput := &s3.PutObjectInput{\n  Body:               aws.ReadSeekCloser(strings.NewReader(\"path/to/file.html\")),\n  Bucket:             aws.String(\"bucketname\"),\n  Key:                aws.String(\"keyname\"),\n  CacheControl:       aws.String(\"max-age=2500\"),\n  ContentDisposition: aws.String(\"inline\"),\n  ContentType:        aws.String(\"text/html\"),\n  ContentLanguage:    aws.String(\"en-US\"),\n  ContentEncoding:    aws.String(\"gzip\"),\n}\nYou can also modify the metadata for your existing objects by copying the objects to themselves using the CopyObject function, specifying the appropriate headers as shown above and including MetadataDirective: aws.String(\"REPLACE\") in the input struct to replace existing metadata with the desired content.. Thanks for your response. I was requesting a link for the S3 documentation you were referring to that points to using metadata to setup Headers for S3 hosted static website, so that I may reach out to the appropriate team internally to have this documentation corrected.\nPlease keep in mind that there are several ways in which the S3 Console behaves differently compared to interacting with the service via one of the SDKs or by issuing API calls directly. As an example in addition to the behavior discussed here, Object-Level Logging for an S3 Bucket operates via the CloudTrail service and must be configured using API calls for that service instead of S3 API calls when it is enabled programmatically, however the S3 Console offers a panel in the bucket's Properties tab to enable this feature without having to open the CloudTrail console.\nThe pre-defined headers in the SDK's input struct mirror the headers available in the S3 Console, the example I provided above is not a complete list of headers available to apply to objects via the AWS SDK for Go. If you look at the PutObjectInput struct I linked in my previous resonse, you will find the following headers available to specify for your objects, which is the same list available in the S3 Console's Metadata drop-down:\n\nCache-Control\nContent-Disposition\nContent-Encoding\nContent-Language\nContent-Type\nExpires\nWebsite-Redirect-Location\n\nAs with the S3 Console, if you want to specify headers outside of the list above they must be prepended with x-amz-meta. If you need to specify common HTTP headers for your objects outside of the list above without the x-amz-meta prefix, S3 Static Website hosting may not be the best solution for your use case as you won't be able to apply these headers on S3 objects via the SDKs, S3 Console, or via direct API calls.. Thanks for reaching out to us @akamensky. While this behavior is specific to the service API rather than the AWS SDK for Go, it does not appear to be documented in the GetDistribution API call which can be confusing as the logical expected response would be a NoSuchDistribution error.\nI've reached out to the CloudFront team internally about this to determine whether it is expected behavior (and in need of documentation) or if the input for this API call is not being properly validated by the service.. Thank you for your patience in this matter @akamensky. The CloudFront team has responded to my internal request stating that the current implementation is the expected behavior from the service's API, and they have updated the documentation for this call clarifying its behavior.. Hi @DamonYellow, thanks for reaching out to us. This is similar to issue #795 since you're trying to sign S3 requests using Signature Version 2. As @jasdel mentions in that issue, the SDK only supports Signature Version 4 signing for S3 requests. \nUnfortunately there are no previous versions of the SDK that supported signing S3 requests with Signature Version 2, so you'll want to use Signature Version 4 when signing S3 requests with the AWS SDK for Go.. @DamonYellow thanks for your response. We have no plans to implement a SigV2 signer for S3 within the AWS SDK for Go, since S3 will stop accepting requests signed using SigV2 in all regions on June 24, 2019 per the AWS Forums announcement here. The Signature Version 2 signer you see in aws-sdk-go/private/signer/v2/v2.go is intended for SimpleDB, and uses a different signing process than what S3 uses for Signature Version 2. As a result, this signer will not work for S3 but it may work for services other than SimpleDB listed on this page.\nCould we get some more details on your use case to better understand the need for using Signature Version 2 instead of Version 4? Looking at the values for the Host header in the request, and the Server and X-Amz-Request-Id headers in the response included in your initial comment, this request does not appear to be sent to S3. Are you using the AWS SDK for Go to interact with an object storage service similar to S3 that uses Signature Version 2 but does not support Signature Version 4?. Thanks for the additional information. Boto has continued to support SigV2 signing to prevent breaking changes since SigV2 was the only way to sign streaming requests to S3 at the time that V2 signing was implemented in Boto. Since SigV4 had changed to support this feature for S3 by the time the AWS SDK for Go was released, we did not feel the need to implement a SigV2 signer for S3.\nWith that in mind, it is possible to specify a custom signer in the AWS SDK for Go to sign requests to your object storage service with Signature Version 2 as @jasdel mentions here.. Thank you for bringing this to our attention @akamensky. I am able to reproduce this behavior using the AWS SDKs for Go and PHP, as with #2211 the issue here lies in the Service API rather than the AWS SDK for Go. API models and their associated documentation are provided by the service team and consumed by each of the SDKs. \nI have reached out to the CloudFront team internally about this to determine whether the documentation is correct and the API call is not operating as expected, or if the API call is working as expected and the documentation needs to be updated to reflect the current behavior.. The CloudFront team has updated my internal request for this. Unfortunately it does not appear that they intended for CreateDistribution to be idempotent, instead the expected behavior of this call with a previously used CallerReference is for the service to return a DistributionAlreadyExists error. The documentation for CallerReference has been updated in the API Reference for this call to reflect this behavior.. Thanks for reaching out to us @buroa. We are only including endpoints for public regions in our endpoints.json file, so a pull request to include these isolated endpoints in our endpoints.json file would not be accepted. \nIn order to use the AWS SDK for Go with non-public regions you'll need to configure a custom resolver using the endpoints package that points to the appropriate URL to resolve the endpoints for these regions.. Hi @denismakogon, thanks for reaching out to us. Unfortunately S3's API does not natively provide an option to list objects by creation date, however I have written a code example that lists objects in a given bucket, checks the LastModified date for each one to see if it is younger than the specified date, and outputs the key and LastModified date for objects that match the criteria. Feel free to use this example as-is, or modify it to better suit your needs:\n```go\npackage main\nimport (\n    \"fmt\"\n    \"os\"\n    \"time\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\n)\n// listObjectsByDate.go lists objects last modified after the specified date.\n//\n// Usage:\n// go run listObjectsByDate.go  \nfunc main() {\n    if len(os.Args) < 3 {\n        fmt.Println(\"Invalid parameters, correct usage is 'go run listObjectsByDate.go  '\")\n        os.Exit(1)\n    }\n    bucket := os.Args[1]\n    date := os.Args[2]\n// Creating new AWS session in us-west-2 region\nsess := session.New(&aws.Config{\n    Region: aws.String(\"us-west-2\"),\n})\n\nsvc := s3.New(sess)\n\n// Specifying short-form date format to parse date entered as time.Time\nconst shortForm = \"2006-01-02\"\nt, tErr := time.Parse(shortForm, date)\nif tErr != nil {\n    fmt.Printf(\"%v\\nPlease enter the date as YYYY-MM-DD (e.g. 2016-09-25)\", tErr)\n    os.Exit(1)\n}\n\n// Empty s3.Object array to add objects that match the search criteria\nvar objectList []*s3.Object\n\n// Borrowing paginated ListObjects from example/service/s3/listObjects/listObjects.go\nerr := svc.ListObjectsPages(&s3.ListObjectsInput{\n    Bucket: &bucket,\n}, func(p *s3.ListObjectsOutput, last bool) (shouldContinue bool) {\n    for _, object := range p.Contents {\n        // Compare LastModified date of each object listed in ListObjects,\n        // If entered date is earlier then add object to list\n        if object.LastModified.After(t) {\n            objectList = append(objectList, object)\n        }\n    }\n    return true\n})\n// Basic error handling\nif err != nil {\n    fmt.Printf(\"Error listing objects:\\n%v\\n\", err)\n}\n// Print out key and LastModified date for objects in list\nfor _, object := range objectList {\n    fmt.Printf(\"%s\\t%v\\n\", *object.Key, *object.LastModified)\n}\n\n}\n``. @denismakogon It's certainly possible that this could be implemented at the service API level, specially if the same feature is being requested by multiple customers. The best way to submit service-level feature requests currently is to create a [new support case](https://console.aws.amazon.com/support/v1#/case/create) under the S3 service. You can post your feature request in the [AWS S3 Forum](https://forums.aws.amazon.com/forum.jspa?forumID=24) as well for other customers to chime in with the same need and demonstrate a broader impact in the implementation of this feature.. Hi @mourya92, thanks for reaching out to us about this. I'm able to reproduce this behavior, adding theX-Amz-Taggingheader torequiredSignedHeadersas suggested does result in this header being added to the presigned URL, which in turn allows the uploader to include the specified tag(s) for the object in the headers for the PUT request as expected. We will work toward resolving this soon.. Hi @lizilong007, thanks for reaching out to us. What you are describing is expected behavior of DynamoDB'sGetItem` API call as it is currently implemented. From the API reference page for this call:\n\nIf there is no matching item, GetItem does not return any data and there will be no Item element in the response.\n\nIf you look at the Errors section of that page near the bottom, you can see that ResourceNotFoundException is only returned when attempting to access a nonexistent table or index, and makes no mention of nonexistent items. If you would like to see the service return an error when attempting to retrieve a nonexistent item in an existing table, I suggest raising a new service feature request by creating a new support case under the DynamoDB service, or posting in the AWS Forums for DynamoDB.. Thanks for the clarification, and I'm glad to hear the service is working as expected on your end at this time. I'll be closing this issue now, but please feel free to open a new issue if you encounter unexpected behavior with the SDK.. Hi @iselind, thanks for reaching out to us about this. Unfortunately s3manager is not very light when it comes to memory usage at this time, there have been discussions about this in issues #142, #232, and #272 among others. At this time, we have an open feature request (#2036) to re-use buffer pools across requests issued by s3manager.Uploader which should reduce memory usage when using s3manager. You are welcome to +1 that feature request and include additional details for your use case, or suggestions for the implementation of this enhancement, in a comment on that issue.\nUntil this is implemented, s3manager unfortunately won't work for your use case without increasing available memory to your Kubernetes containers,so you'll have to perform your own multi-part uploads without s3manager. You could create a custom uploader that, for example, creates an io.ReadSeeker from []byte data streamed from the file you want to upload as the Body for the input of an UploadPart API call.\nIssue #232 includes a link to a custom uploader written by @cespare that may work for your use case, or that you could use as a starting point to implement your own custom uploader.. Thanks for reaching out to us about this @dagger. Validating the schema for Glue requests within the SDKs would result in a better user experience with the SDK when interacting with this service, I'll mark this as a feature request and bring it up during our next sprint.. Apologies for the delay on this. While the rest of the team agreed that this would be a good feature to have, unfortunately we currently do not have the capacity to make its implementation a priority. That being said, we are always glad to review PRs for features like this; our contribution guidelines can be found here.. Hi @challarao, thanks for reaching out to us about this. The SDK should automatically retry network errors including \"no such host\" errors, you can verify this behavior by setting error and retry logging for request errors and retries in your session as shown below.\ngo\n    sess := session.Must(session.NewSession(&aws.Config{\n        Region:   aws.String(\"us-west-2\"),\n        LogLevel: aws.LogLevel(aws.LogDebugWithRequestErrors | aws.LogDebugWithRequestRetries),\n    }))\nIf you do not see that the SDK is retrying these errors automatically, please provide the following information so we can further investigate this unexpected behavior.\n\nVersion of the AWS SDK for Go that you're using\nVersion of Go (go version)\nOutput returned with error and retry logging enabled. Thanks for your response @challarao! I'm glad to hear you were able to rule out the AWS SDK for Go as the culprit for this behavior on your end, and I hope you can identify and resolve its cause. I'll close out this issue for now, but if you continue running into this problem please feel free to leave another comment here so we can re-open it.. Hi @smilemakc, thanks for reaching out to us. Can you set the logging level in your config to LogDebugWithHTTPBody and provide its output? This will show details about what is wrong with the request signature.. Thanks for the additional information @smilemakc! I was able to reproduce this behavior on my DigitalOcean account - when setting the SDK's region to any AWS region other than us-east-1, CreateBucket failed with a SignatureDoesNotMatch error, whereas ListBuckets and PutObject worked as expected regardless of the region specified. \n\nLooking at DigitalOcean's CreateBucket API documentation, I see that it supports including LocationConstraint in the request body with a value that aligns with their regions. This value is not required since the region in which the space is being created is included in the hostname used for the request. When setting the SDK's region to us-east-1, a LocationConstraint value is not included in the CreateBucket request unless specified in CreateBucketConfiguration within the input struct. By contrast, using any other region in the client config without specifying a LocationConstraint value in the input struct will result in the request including a LocationConstraint value that matches that region to ensure S3 creates the bucket in the specified region. \nComparing the XML body for requests between S3 and DigitalOcean Spaces reveals that S3's CreateBucketConfiguration element includes a namespace that specifies the API version for S3 (e.g. <CreateBucketConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"><LocationConstraint>us-east-2</LocationConstraint></CreateBucketConfiguration>), whereas DigitalOcean is not expecting a namespace associated with this element (e.g. <CreateBucketConfiguration><LocationConstraint>ams3</LocationConstraint></CreateBucketConfiguration>). This means that including LocationConstraint in a CreateBucket request to DigitalOcean's Spaces with the SDK will result in the request payload not matching what DigitalOcean's API expects regardless of the LocationConstraint value, so you'll still get a SignatureDoesNotMatch error. \nThe best approach to resolve this would be to use us-east-1 as the region for your client's config when issuing CreateBucket requests to DigitalOcean Spaces so the XML body is omitted from the request altogether.. Thanks for your response! I'm glad to hear my suggestion worked as expected on your end. I'll go ahead and close out the issue, but if you run into similar problems please feel free to leave another comment here so we can re-open it.. Hi @MNibble, thanks for bringing this to our attention. API models for services and their associated documentation are provided by the service teams and consumed by each of the SDKs, which is why you see the AWS CLI usage note in our StartSession docs. I have reached out to the SSM team so the documentation can be updated from their end, once this is done the changes should be reflected in our documentation as well.. Thanks for reaching out to us @eacp. Unfortunately I wasn't able to reproduce this from my end testing on my local machine, a t2.micro instance in us-west-2, and a third party VPS. Running this command with time prepended on each machine resulted in the command completing within 10 to 19 seconds.\nAre you able to reproduce this behavior consistently across different machines connected to different networks?\nDo you see the same behavior with go get for other packages? I recommend trying packages from GitHub as well as other sources to rule out connection problems between your machine(s) and GitHub specifically.\n. Hi @darshan-ghumare, thanks for reaching out to us. All clients in the AWS SDK for Go use a fixed API version with which they were generated to ensure it matches the service's API model.\nSince the AWS SDK for Go did not exist prior to the release of the latest DynamoDB API version (2012-08-10), there is no version of the SDK that supports version 2011-12-05 of DynamoDB's API.. Thanks for reaching out to us @WIZARDISHUNGRY. While most of the AWS SDKs don't validate the expiry time when presigning a request/URL, this would be a sensible enhancement to avoid generating invalid presigned URLs so I'll mark the issue as a feature request and bring it up in our next sprint. If you would like to contribute to a PR we'd be glad to review it, you can find our contribution guidelines here.. Thanks for bringing this to our attention @frigus02. This behavior is a bug and I've and marked this issue as such. We will work toward fixing this quickly.. Thanks for reaching out to us @cpramodroy. Unfortunately S3 errors surrounding XML are generally a bit vague, however these are passed directly from the service so they cannot be improved from the SDK's end. If you'd like to see more detail on errors coming from S3, you may submit a feature request by opening a new support case under the S3 service or posting in the S3 Forums.\nThe issue you're running into with the code provided is that the Permission value must be in all caps (FULL_CONTROL | WRITE | WRITE_ACP | READ | READ_ACP). The DisplayName value in the Owner struct is indeed optional. \nIf you're just granting the S3 Log Delivery Group write permissions to your bucket, a simpler way to implement this would be to use GrantWrite:       aws.String(\"uri=http://acs.amazonaws.com/groups/s3/LogDelivery\") (as shown in the example from the SDK's PutBucketAcl documentation page) instead of including the full AccessControlPolicy struct. This would reduce the input struct to 2 lines (Bucket and GrantWrite) and would avoid having to issue an additional ListBuckets call to determine the owner ID as well as avoiding mistakes such as casing in the Permission value.. Hi @cpramodroy, thanks for reaching out to us. What you are experiencing is expected behavior, the AWS SDK for Go requires the region specified in the S3 client's session config to match the region the target bucket is in since the SDK does not forward requests to other regions.. @bflad These updates are now live in our docs, thanks again for bringing this to our attention.. Hi @ouroboros8, thanks for reaching out to us. This seems like a reasonable feature to implement so I've labeled the issue accordingly and will bring this up in our next sprint. If you would like to submit a PR for this we would be happy to review it, you can find our contribution guidelines here.. @ouroboros8 I've dug further into this, and the current implementation that requires explicitly providing stscreds.StdinTokenProvider to the session is by design. \nSince StdinTokenProvider makes no attempt to make atomic prompts from stdin across multiple goroutines, does not synchronize the token provider across multiple AssumeRoleProviders or Credentials, and does not time out waiting for input from stdin, making this token provider the default for an AssumeRoleProvider would make it trivial for users to create an application that would lock up unexpectedly in a way that is difficult to trace when using a role that does not require an MFA token to be assumed. . Hi @roy-michael, thanks for reaching out to us about this. Looking over the sample code you've provided, it looks like your time window starts at the time the code is executed and ends 10 seconds into the future. This likely won't return anything unless a delay is introduced between the time the StartTime parameter is initialized in your StartQueryInput struct and the time the StartQuery API call is issued to allow CloudWatch Logs to populate log entries within that timeframe to the given log group.\nAnother thing to note, if the start time and end time in your code sample do not accurately reflect your real-world use of the service, is that per our docs CloudWatch Logs Insights will only search log data that was sent to CloudWatch Logs on November 5, 2018 or later. As a result, if you are trying to query logs that were sent before this date you will not see any results.\nI was able to use your code to retrieve populated query results on a log stream with logs from Nov 8 by modifying your StartQueryInput parameters to the following.\ngo\nsqi := &cloudwatchlogs.StartQueryInput{\n  StartTime: aws.Int64(time.Now().Add(-(time.Hour * 24 * 22)).Unix()), // 22 days ago (Nov 7)\n  EndTime: aws.Int64(time.Now().Unix()),\n  LogGroupName: aws.String(\"my-log-group\"),\n  QueryString: aws.String(\"fields @timestamp, @message | stats count() by bin(5m)\"),\n}\nThis returned the 3 log entries that I expected to see:\n```\n$ go run cwQuery.go\nINFO[0002] {\n  Results: [[{\n        Field: \"bin(5m)\",\n        Value: \"2018-11-29 21:35:00.000\"\n      },{\n        Field: \"count()\",\n        Value: \"1\"\n      }],[{\n        Field: \"bin(5m)\",\n        Value: \"2018-11-29 21:30:00.000\"\n      },{\n        Field: \"count()\",\n        Value: \"1\"\n      }],[{\n    Field: \"bin(5m)\",\n    Value: \"2018-11-09 00:45:00.000\"\n  },{\n    Field: \"count()\",\n    Value: \"1\"\n  }]],\n\nStatistics: {\n    BytesScanned: 1064,\n    RecordsMatched: 3,\n    RecordsScanned: 3\n  },\n  Status: \"Complete\"\n}\n``. Thank you for reaching out about this @guregu. While theCancellationReasonstructure is present in the API model for DynamoDB, the AWS SDK for Java is the only SDK that currently supports unmarshalling this structure. I've marked this issue as a Feature Request, and will bring it up in our next sprint.. Hi @yyl719133368, thanks for reaching out to us. TheListObjects` command for S3 in the AWS SDK for Go is recursive by default, as such there is no need to specify an option to enable this behavior. I have included an example below with corresponding output to demonstrate.\n```go\n$ cat listObjects.go \npackage main\nimport (\n    \"fmt\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\n)\nfunc main() {\n    sess := session.Must(session.NewSession(&aws.Config{\n        Region: aws.String(\"us-west-2\"),\n    }))\n    svc := s3.New(sess)\n    res, err := svc.ListObjects(&s3.ListObjectsInput{\n        Bucket: aws.String(\"go-2321\"),\n    })\n    if err != nil {\n        fmt.Printf(\"Error listing bucket:\\n%v\\n\", err)\n    }\n    for _, object := range res.Contents {\n        fmt.Println(*object.Key)\n    }\n}\n$ go run listObjects.go \nfile1\nfile2\nfile3\nprefix1/\nprefix1/file1\nprefix1/file2\nprefix1/file3\nprefix1/prefix2/\nprefix1/prefix2/file1\nprefix1/prefix2/file2\nprefix1/prefix2/file3\nprefix1/prefix2/prefix3/\nprefix1/prefix2/prefix3/file1\nprefix1/prefix2/prefix3/file2\nprefix1/prefix2/prefix3/file3\n``\n. Apologies for the delay in response on this @yyl719133368. In order to list objects in a given prefix non-recursively you'll need to specifyPrefixandDelimitervalues in your [ListObjectsInput](https://docs.aws.amazon.com/sdk-for-go/api/service/s3/#ListObjectsInput) struct. ThePrefix` value must be the full path to the prefix you want to list, using the example above if I wanted to only list the contents of prefix2/ excluding prefix3/ and its contents I would need the following in my input struct:\ngo\n&s3.ListObjectsInput{\n  Bucket:    aws.String(\"go-2321\"),\n  Prefix:    aws.String(\"prefix1/prefix2/\"),\n  Delimiter: aws.String(\"/\"),\n}\nNote the trailing slash in the Prefix value. If I set Prefix to prefix1/prefix2 without the trailing slash and kept Delimiter as is, I would get no results since there is no object in prefix1/ named prefix2 and S3 would end the object listing at the first object that includes the Delimiter character (i.e. prefix2/). . Hi @Kiura, thanks for reaching out to us! It doesn't look like there's anything that needs to be changed in the AWS SDK for Go for this to work as expected, so I'll be closing out the issue at this time.. Thanks for reaching out to us @bmd. Unfortunately I wasn't able to reproduce the described behavior; running the provided code sample on a database migration task on my end worked as expected, and adding a line below the getTask call to print *taskStatus.ReplicationTaskStats produced the expected output:\n```\n$ go run describeReplTask.go\naws.SDKVersion = 1.15.89\nReplication task stats:\n{\n  ElapsedTimeMillis: 1608,\n  FullLoadProgressPercent: 100,\n  TablesErrored: 0,\n  TablesLoaded: 12,\n  TablesLoading: 0,\n  TablesQueued: 0\n}\n2018/12/07 12:04:29 Table loading is 100 percent complete\n```\nEnabling debug logging for this code by setting LogLevel: aws.LogLevel(aws.LogDebugWithHTTPBody) in your aws.Config struct and providing the resultant output can help us further troubleshoot the behavior you're seeing.. @bmd You're right, unfortunately I don't have a large database handy to migrate between RDS instances so by the time I ran this on a migration task on my end to check its status it was already finished.\nIf you are not already using the WaitUntilReplicationTaskRunning waiter on your migration task before the for loop starts, using this waiter (and maybe sleeping for a few seconds after it's done waiting) should help prevent running into the issue you're seeing. There is also a WaitUntilReplicationTaskStopped waiter you can use in place of your for loop, however using this waiter would not allow you to review the migration task's progress before it completes.\nIf using WaitUntilReplicationTaskStopped does not fit your use case, another thing that could help prevent this error is to check *taskStatus.Status instead of *taskStatus.ReplicationTaskStats.FullLoadProgressPercent in your if statement to determine whether complete should be changed to true.. Thanks for reaching out to us @rfielding. Unfortunately the S3Object data type in the Rekognition API does not have a member that would allow you to pass a customer-managed encryption key, so it does not look like S3 objects encrypted using customer-managed keys are supported in Rekognition API calls. That being said, I was able to issue SearchFacesByImage calls using S3 objects encrypted with S3-managed keys as well as KMS-managed keys. If your use case requires you to use S3 objects encrypted with customer-managed keys you'll need to download these objects locally then use Bytes instead of S3Object in your Image struct to upload the object to Rekognition to use with your API call(s).\nIf you'd like to see Rekognition support the use of S3 objects encrypted with customer-managed keys (SSE-C) I recommend creating a new service feature request either by opening a new support case under the Rekognition service or posting in the Rekognition section of the AWS forums.. Thanks for reaching out to us @sunilkumarmohanty. I'm able to get the expected response with your input struct when swapping out the TargetGroupArn and Id values for a valid ELB target group on my account and an aliased Lambda function associated with that target group. \nThat being said, when I swap my account ID # with xxxxxxxxxxxx in either TargetGroupArn or Id I do see a ValidationError stating that the value with xxxxxxxxxxxx is not valid. I recommend double checking your Id string to ensure its format is valid - you can copy the function's ARN from the Targets tab of the appropriate Target Group to ensure this value is correct.. Hi @nicowernli, thanks for reaching out to us about this. I was able to locate this forum post suggesting this option was introduced for older AWS accounts that were reliant on legacy API behavior, however for newer AWS accounts the API key value will always be distinct from its ID. \nI've reached out to the service team to clarify the intended usage for this parameter so that they can update this parameter's documentation in the service's API reference, once this documentation is updated the change will be reflected in the documentation for the AWS SDK for Go as well as our SDKs for other languages.. Apologies for the delay in response on this @nicowernli. The response for a CreateApiKey call should include the key's Id and Value which you can store to reference the key(s) in future actions. If you are creating your API keys through a different method than you're using to generate them, or otherwise cannot use the output of the CreateApiKey call for actions that require these values then you'll need to use GetApiKeys to retrieve the key(s) as you mentioned.. Thanks for reaching out to us about this @joaoferrao. Where are you expecting the AWS SDK for Go to load your credentials from? \nIf you are running this in EC2 and loading Instance Profile credentials from the EC2 metadata service it is possible that your application is getting rate limited when it queries the metadata service for credentials as described in the Throttling section of this documentation page.. @joaoferrao Thanks for the additional information! If your application is creating a new Kinesis client each time your PutKinesisRecord function is called, the application is most likely getting throttled by the instance metadata service once your request rate ramps up since ECS Task Role credentials are retrieved via the underlying instance's metadata service. If that is the case, I suggest re-using your session and service client as much as possible to reduce the frequency and concurrency of requests to the metadata service. If you need to make Kinesis calls to different AWS regions within the same instance of the application, you can initialize a new Kinesis client using an existing session with a second argument that specifies the new AWS region to which you want to issue calls (e.g. kinsesis.New(s, aws.NewConfig().WithRegion(\"us-west-2\")))\nIf my suggestion doesn't help eliminate these credential provider errors, seeing a more complete code sample would help us get a better idea of how the provided functions are interacting with each other.. Hi @rnzsgh, thanks for reaching out to us about this. Can you give us more details on how you're importing the AWS SDK for Go before building, as well as more detailed steps to reproduce this from our end? Also, are you able to reproduce this in a clean environment?. Thanks for reaching out to us @dcu! The AWS SDKs typically do not support services still in developer preview. \nIf you have been accepted into the developer preview for this service I suggest reaching out to the Textract team about implementing this service in the AWS SDK for Go as any customization that would be required for this service to work on a given SDK would have to be provided by the service team.. Hi @itchyny, thanks for bringing this to our attention. It looks like this typo originated in the CloudWatch API reference for this API call, so I've reached out to the CloudWatch team to have this corrected on their end. Once the change is made it shouldn't take long to be reflected in our documentation.. Thank you for your patience in this matter @itchyny. The CloudWatch team has updated the API reference for this call to fix these typos, this correction should propagate to our documentation soon.. Thanks for reaching out to us about this @jugalde-r7. I'm able to reproduce this on my end with the provided code, however looking at the resultant debug output it looks like this behavior is originating from the service rather than the AWS SDK for Go. I've reached out to the SQS team internally about this so it can be further investigated from the service end, and will update the issue once we have additional information.. Thank you for your patience in this matter @jugalde-r7. The service team has responded suggesting that the behavior you're seeing is caused by messages being added to the queues immediately after they're getting purged, and have suggested adding a delay of at least 1 minute between purging these queues and beginning to add more messages.\nPer the PurgeQueue documentation page, this API call takes up to 60 seconds to complete - messages sent to the queue before you call PurgeQueue might be received but are deleted within the next minute, and messages sent to the queue after you call PurgeQueue might be deleted while the queue is being purged.\nI hope this helps eliminate the behavior you're seeing between your SQS queues, however if it does not please do let us know so we can further investigate this behavior.. No problem! I'll close out the issue since it sounds like this solves the behavior you're seeing. Please don't hesitate to reach back out if you continue to see this behavior and we'll re-open the issue to further investigate this.. Hi @qcoldiron, thanks for reaching out to us. Looking over the provided code, I see you are initializing the variable cd as a new CodeDeploy client. When you're trying to set other variables such as rev and params as types that exist in the CodeDeploy package (RevisionLocation and CreateDeploymentInput respectively), you need to specify the package name codedeploy before the type's name instead of the service client variable cd since the CodeDeploy client struct does not contain the types to which you want to set these variables. This addresses the first two errors you're seeing for lines 19 and 28.\n```go\ncd := codedeploy.New(sess)\nrev := &codedeploy.RevisionLocation{\n  GitHubLocation: &codedeploy.GitHubLocation{\n    // RevisionLocation parameters\n}\nparams := &codedeploy.CreateDeploymentInput{\n  // CreateDeploymentInput parameters\n}\nres, err := cd.CreateDeployment(params)\n```\nAnother valid approach would be to alias the codedeploy package to cd in the import statement as shown below, then rename your service client variable to something like svc or client when initializing the client and when issuing API calls so that the package and client for the service are clearly differentiated. \n```go\nimport (\n    \"fmt\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\ncd \"github.com/aws/aws-sdk-go/service/codedeploy\"\n\n)\nfunc main() {\n    // Load session from shared config\n    sess := session.Must(\n        // session parameters\n    )\n// Create new AWS API clients\nsvc := cd.New(sess)\n\nrev := &cd.RevisionLocation{\n    GitHubLocation: &cd.GitHubLocation{\n        // RevisionLocation parameters\n}\n\nparams := &cd.CreateDeploymentInput{\n    // CreateDeploymentInput parameters\n}\n\nres, err := svc.CreateDeployment(params)\n\n```\nThe third error that involves line 39 is returned because there is no CreateCodeDeploy operation for the CodeDeploy client - based on the type being assigned to params it looks like your intent is to run CreateDeployment so I've accounted for this in the code examples above.\nAnother thing I noticed in your params struct is that you're assigning the parameter RevisionLocation to a function named RevisionLocation() that doesn't exist in the provided code sample or in the codedeploy package of the AWS SDK for Go. If this function is delcared elsewhere in your code, that should work as expected as long as that function returns a RevisionLocation struct as defined in the codedeploy package, but it seems more likely to me that it should be assigned to the rev variable you defined above the input struct (e.g. Revision:  rev,)\n. No problem! Since there is nothing further to address on this issue I'll be closing it out now.. Hi @sambengtson, thanks for reaching out to us. I'm guessing that by EB you are referring to Elastic Beanstalk, however please do correct me if I'm wrong. Have you had a chance to review the logs for your Elastic Beanstalk environment, or the OS/application logs on the instance(s) where you are seeing this behavior for additional information on this?\nThe trace does appear to be consistent with a network timeout, however it is not clear from the trace what is causing the timeout. Enabling debug logging on the SDK and providing its output here would allow us to see more details on what's happening from the SDK's end, however providing a code sample would be ideal so that we can reproduce this scenario on our end as closely as possible.. Thanks for the additional info! Debug logging can be enabled by adding LogLevel: aws.LogLevel(aws.LogDebug) to your config struct. You can also chain multiple log level types as needed in the same statement with a pipe between each log level (e.g. LogLevel: aws.LogLevel(aws.LogDebug | aws.LogDebugWithRequestErrors | aws.LogDebugWithRequestRetries)). Please keep in mind that debug output will contain sensitive information such as the S3 Bucket + Key involved in the request(s) as well as the AWS Access Key ID being used (but not the Secret Access Key corresponding to this key ID), so you'll want to review the output and scrub sensitive information before pasting it here.\nI will test the provided code on my end shortly and will update the issue with my findings.. Unfortunately I haven't been able to reproduce this behavior on my end - running this on my end for a 70MB object and a 120MB object results in the requested object being downloaded successfully. Your output looks like the SDK attempted to reach out to S3 twice to download separate byte ranges for the object, but never received a response from the service.\nThis is a good indicator that the SDK is working as expected here, so the behavior you're seeing is likely caused by something in your Elastic Beanstalk environment. I suggest opening a new support case for Premium Support in the Elastic Beanstalk service to further troubleshoot this from EB's end as their support engineers will have more in-depth knowledge of that service.. Hi @onetwopunch, thanks for reaching out to us. Moving the Tag and TagSet structs to a top-level package such as aws would certainly make for a more uniform experience when working with multiple AWS services using the AWS SDK for Go, however this would be a breaking change for the SDK. \nThese structs are part of the API models from each AWS service that supports resource tagging, which are consumed by all of the AWS SDKs. Changing the location of these structs would alter the current members of the API models, which would require each of these services to release updated API models that account for this change. \nIn addition to the above, moving all Tag and TagSet shapes from the service API models to a common package would mean that these structs would have to change for all services if a single service decides to expand the member fields of their API's Tag and TagSet shapes. This could result in common structs that contain members that are valid for one service but invalid for another, requiring end users to be mindful of which services support each member of these structs.. Hi @stephencoe, thanks for reaching out to us. The behavior you are seeing stems from the service's API - when executing your code with debug logging enabled, we can see that the service returns an HTTP 400 (Bad Request) response with the error message you mention. \nThe Secrets parameter in the ContainerDefinition struct is not a required field, so if no secrets need to be passed to the container being defined in this struct this parameter should not be present. That being said, if the Secrets parameter is present and empty you can pass any role's ARN as the executionRoleArn for the call to succeed regardless of the contents of the role's IAM Policy and Trust Relationship Policy - you can even specify a nonexistent role's ARN as long as it passes parameter validation (e.g. arn:aws:iam::<account-id>:role/fakeRoleName).. Hi @gazoakley, thanks for reaching out about this. I'm able to reproduce the described behavior in both the AWS SDKs for Go and PHP.\nIn both cases, when printing the response for a CreateMembers request I can see the AccountId value within UnprocessedAccounts but no Result or ProcessingResult, however when enabling debug logging with HTTP body I can see that the response includes the Result value along with the AccountId. I've reached out to the Security Hub team internally to have the API model corrected, and will update the issue once this is fixed.. Hi @stefansundin, thanks for taking the time to put this together and apologies for the delay in our response. Most of these issues stem from the way in which our docs are generated from our source code, which is something we're aware of and will work toward fixing in the near future.. Hi @praka07, thanks for reaching out to us. It does not sound like this issue is not related to the AWS SDK for Go, but related instead to the AWS SDK for Java. I suggest re-opening your issue in that repository so you can receive support that is more appropriate for that language.\nI will be closing this issue since it is not related to this repository. If I am mistaken and this is related to the AWS SDK for Go, please comment on the issue with more context on how this relates to the AWS SDK for Go so that we can re-open the issue to work on this further.. Hi @adeshrd, thanks for reaching out to us. Unfortunately I'm not able to reproduce this on my end using Go v1.11.2 and AWS SDK for Go v1.16.11.\nWhich version of the AWS SDK for Go are you using when encountering this error?\nCan you provide a code sample that shows how your session and ECS client are being initialized?. Thanks for the additional info @adeshrd. I was able to use the provided struct and function to initialize a new ECS client and describe a given Task Definition without errors using the code I've included below - I tested both service clients just to ensure each one worked as expected.\nThe behavior you're seeing is likely caused by either a corrupt Go cache, or a vendored dependency in your application that's conflicting with the AWS SDK for Go. \nYou can clean your Go cache by running go clean -cache in a terminal window, if that does not resolve the issue you'll want to review your vendor directory for a package that implements the type session.Session outside of the AWS SDK for Go.\n```go\n$ cat ecs-2379.go \npackage main\nimport (\n    \"fmt\"\n    \"log\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/autoscaling\"\n\"github.com/aws/aws-sdk-go/service/autoscaling/autoscalingiface\"\n\"github.com/aws/aws-sdk-go/service/ecs\"\n\"github.com/aws/aws-sdk-go/service/ecs/ecsiface\"\n\n)\n//SpotTermination is used to detach an instance, used when a spot instance is due for termination\ntype SpotTermination struct {\n    asSvc  autoscalingiface.AutoScalingAPI\n    ecsSvc ecsiface.ECSAPI\n}\n//NewSpotTermination is a constructor for creating an instance of spotTermination to call DetachInstance\nfunc NewSpotTermination(region string) SpotTermination {\nlog.Println(\"Connection to region \", region)\n\nsession := session.Must(\n    session.NewSession(&aws.Config{Region: aws.String(region)}))\n\nreturn SpotTermination{\n\n    asSvc:  autoscaling.New(session),\n    ecsSvc: ecs.New(session),\n}\n\n}\nfunc main() {\n    spot := NewSpotTermination(\"us-west-2\")\nfmt.Printf(\"Service clients: \\n%#v\\n\\n\", spot)\n\necsInput := &ecs.DescribeTaskDefinitionInput{\n    TaskDefinition: aws.String(\"sleep360\"),\n}\n\necsResp, err := spot.ecsSvc.DescribeTaskDefinition(ecsInput)\nif err != nil {\n    fmt.Println(\"Error describing task definition: \", err)\n}\nfmt.Printf(\"ECS call response:\\n%v\\n\\n\", ecsResp)\n\nasInput := &autoscaling.DescribeAccountLimitsInput{}\n\nasResp, err := spot.asSvc.DescribeAccountLimits(asInput)\nif err != nil {\n    fmt.Println(\"Error describing tags: \", err)\n}\nfmt.Printf(\"AS call response:\\n%v\\n\", asResp)\n\n}\n$ go run ecs-2379.go \n2019/01/03 17:25:00 Connection to region  us-west-2\nService clients: \nmain.SpotTermination{asSvc:(autoscaling.AutoScaling)(0xc00000e080), ecsSvc:(ecs.ECS)(0xc00000e090)}\nECS call response:\n{\n  TaskDefinition: {\n    Compatibilities: [\"EC2\"],\n    ContainerDefinitions: [{\n        Command: [\"sleep\",\"360\"],\n        Cpu: 10,\n        Environment: [],\n        Essential: true,\n        Image: \"busybox\",\n        Memory: 10,\n        MountPoints: [],\n        Name: \"sleep\",\n        PortMappings: [],\n        Secrets: [],\n        VolumesFrom: []\n      }],\n    ExecutionRoleArn: \"arn:aws:iam:::role/testRole\",\n    Family: \"sleep360\",\n    PlacementConstraints: [],\n    Revision: 8,\n    Status: \"ACTIVE\",\n    TaskDefinitionArn: \"arn:aws:ecs:us-west-2::task-definition/sleep360:8\",\n    Volumes: []\n  }\n}\nAS call response:\n{\n  MaxNumberOfAutoScalingGroups: 200,\n  MaxNumberOfLaunchConfigurations: 200,\n  NumberOfAutoScalingGroups: 1,\n  NumberOfLaunchConfigurations: 4\n}\n``. No problem! I'm glad to hear my suggestions helped. Since there is nothing further to address on this issue I'll be closing it out now.. Hi @alsmola, thanks for reaching out to us about this. I'm able to reproduce the behavior you're describing on my end, and can confirm that modifying thecredSourceEnvironment` case in session.go as described fixes this behavior. I will bring this up to be added in our next sprint so we can correct this behavior, and will update the issue once this is fixed.. Hi @andreipetrescu95, thanks for reaching out to us. Since the described behavior does not occur consistently this sounds like an issue with the service's API moreso than the AWS SDK for Go. \nI've reached out to the Autoscaling team pre-emptively about this to see if they are aware of any issues that would cause this behavior, however providing Request IDs for DescribeAutoScalingGroups calls that do not return the expected DesiredCapacity value would help them dive deeper into why the service did not behave as expected. You can retrieve these request IDs by enabling debug logging in your session config as shown here.\nUnfortunately I'm not able to test this to completion since it is not clear exactly what the function asgReadinessCheck does. Providing the code behind this function would allow us to ensure we are not missing anything in testing this so we can determine whether this is a bug with the SDK, and would allow the Autoscaling team to reproduce this more accurately from their end.. Thank you for the additional information! Unfortunately I haven't been able to reproduce the described behavior with your code, however I have forwarded the code sample along with the Request IDs to the Autoscaling team for further review on their end. Once I hear back from them I will update the issue accordingly.. Hi @wbhuberIBM, thanks for reaching out to us. Are you seeing this behavior when using the AWS SDK for Go to interact with Amazon S3, or is the SDK being used to interact with another object storage service similar to S3?\nUnfortunately I wasn't able to reproduce this behavior with S3 in any AWS region, issuing a PutObject call to a non-existent S3 Bucket consistently results in an HTTP 404 NoSuchBucket response as expected. We may be able to identify unexpected behavior from the SDK by looking at debug output from requests that exhibit the described behavior, you can enable debug logging in your session config as shown here.. Hello @jmirc, thanks for reaching out to us. Unfortunately I wasn't able to elicit the same error you received from EC2 when reproducing this with the provided input parameters - including SourceSecurityGroupName with an SG ID as its value in my input resulted in a parameter validation error. Reviewing the debug output for the equivalent AWS CLI command you mention made the expected input for this command more clear.\n2019-01-15 10:36:50,337 - MainThread - botocore.endpoint - DEBUG - Making request for OperationModel(name=AuthorizeSecurityGroupIngress) with params: \n{\n'url_path': '/',\n 'query_string': '',\n 'method': 'POST',\n 'headers': \n {\n  'Content-Type': 'application/x-www-form-urlencoded; charset=utf-8',\n  'User-Agent': 'aws-cli/1.16.21 Python/3.6.5 Darwin/16.7.0 botocore/1.12.30'\n },\n 'body': \n {\n  'Action': 'AuthorizeSecurityGroupIngress',\n  'Version': '2016-11-15',\n  'GroupId': 'sg-08be3bcf21f796782',\n  'IpPermissions.1.IpProtocol': '-1',\n  'IpPermissions.1.Groups.1.GroupId': 'sg-7811be08'\n },\n 'url': 'https://ec2.us-west-2.amazonaws.com/',\n 'context': \n {\n  'client_region': 'us-west-2',\n  'client_config': <botocore.config.Config object at 0x10e09fef0>,\n  'has_streaming_input': False,\n  'auth_type': None\n }\n}\nSince you're authorizing traffic originating from an existing security group, you shouldn't have to specify the VPC ID associated with the source security group in question. Since IpProtocol is already being specified in the IpPermission struct it doesn't need to be included again outside of that struct. In addition to this, if you are allowing traffic for all ports the FromPort and ToPort parameters are not required. As a result, the following input format should work for this use case.\ngo\nsvc.AuthorizeSecurityGroupIngress(&ec2.AuthorizeSecurityGroupIngressInput{\n  GroupId: aws.String(\"sg-08be3bcf21f796782\"),\n  IpPermissions: []*ec2.IpPermission{\n    {\n      IpProtocol: aws.String(\"-1\"),\n      UserIdGroupPairs: []*ec2.UserIdGroupPair{\n        {GroupId: aws.String(\"sg-7811be08\")},\n      },\n    },\n  },\n})\nPlease don't hesitate to reach back out if this input format does not work as expected on your end.. Hi @jniesen, thanks for reaching out to us about this. The behavior you're describing can happen when the response from the service comes in a form other than REST/JSON - most commonly, when the HTTP response code is 502, the response body can come in as HTML. This behavior should not occur with HTTP 400 response codes.\nI've reached out to the service team about this so they can review the Request ID provided, however if you can consistently reproduce this behavior providing additional Request IDs along with their associated timestamps would be useful for the service team to better troubleshoot why the service is not returning this error response in the expected format.\nIn the meantime, you should be able to review these error responses in full to troubleshoot the reason you're receiving the errors by enabling debug logging with HTTP body as shown here. . @jniesen I'm glad you were able to get your Lambda function to work as expected. The behavior you're describing is certainly unexpected; if your Lambda function was unable to reach the public DynamoDB endpoint due to the function running in a private subnet, the SDK should not have received any response from the service on the GetItem request.\nThis was my experience when reproducing this issue - when setting up a Lambda function in a private subnet, my GetItem request timed out due to the SDK not being able to reach the service's endpoint. After creating a VPC Endpoint for DynamoDB on this subnet's VPC and updating the subnet's route table accordingly, the Lambda function received OK 200 responses from DynamoDB as expected. \nUnfortunately I have not yet received a response from the service team regarding the Request ID you've provided, once I do I will update the issue accordingly.. Hi @ijt, thanks for reaching out to us and I do apologize for the long delay in response from our end. There are a couple of things going on here that are preventing you from seeing the behavior you're expecting.\n\nThe code provided looks like it uses the same credential set to issue requests relating to the SQS queue and the SNS topic.\n\nWhen the SNS topic and SQS queue exist in the same AWS account, subscribing the SQS queue to the SNS topic does not require confirmation so a subscription confirmation message won't be sent to the SQS queue. Likewise, as mentioned here, if the SQS queue owner creates the subscription for an SNS topic that exists in another AWS account (provided they have the appropriate IAM permissions) the subscription does not result in a confirmation message being sent to the SQS queue.\n\nIf the SQS queue and the SNS topic belong to different AWS accounts and the SNS topic owner is issuing the subscription request you need to grant appropriate permissions to the SQS queue to receive the subscription confirmation message.\n\nWhen creating the SQS queue you need to include a policy similar to what's shown in this section of the SQS docs as an attribute. Expanding on the CreateQueue command included in your code:\n``go\n    subName := \"test-subscription\"\n    queuePolicy :={\n  \"Version\": \"2012-10-17\",\n  \"Id\": \"AllowAll\",\n  \"Statement\": [\n    {\n      \"Sid\": \"Sid1550017136052\",\n      \"Effect\": \"Allow\",\n      \"Principal\": \"\",\n      \"Action\": \"SQS:\",\n      \"Resource\": \"arn:aws:sqs:us-east-2:123456789012:+ subName +\"\n    }\n  ]\n}`\n    out2, err := sqsClient.CreateQueue(&sqs.CreateQueueInput{\n        QueueName: aws.String(subName),\n        Attributes: map[string]*string{\n            \"Policy\": &queuePolicy,\n        },\n    })\n```\n\nThe subscription token does not exist in the format you're expecting from out4.\n\nOnce the SQS queue has been created with the appropriate policy and the subscription request results in a confirmation message being successfully received by the SQS queue, the confirmation message body contains more information than just the subscription token. The quickest way I found to address this is to unmarshal the message body before the ConfirmSubscription call to retrieve the subscription token like so:\n```go\n    var dat map[string]string\n    err = json.Unmarshal([]byte(*token), &dat)\n    if err != nil {\n        return fmt.Errorf(\"unmarshalling confirmation message %v\", err)\n    }\n    subToken := dat[\"Token\"]\nlog.Printf(\"Using the token to finish subscribing.\")\n_, err = snsClient.ConfirmSubscription(&sns.ConfirmSubscriptionInput{\n    TopicArn: out.TopicArn,\n    Token:    &subToken,\n})\n\n```\nI hope this information helps you programmatically subscribe SQS queues to SNS topics across accounts! Please be sure to let us know if you continue running into trouble with this after implementing my suggestions.. Thanks for the additional information. Unfortunately point #2 in my previous response was partially incorrect - when creating an SQS queue without specifying a Queue Access policy, only the owner of the queue has permission to send messages to this queue. I do apologize for the oversight on this point.\nWhen a message is published to an SNS topic that has an SQS queue subscribed to it, SNS uses a different account than the one that owns the SQS queue and the SNS topic to send the message to the queue. This means that, even if the SQS queue and the SNS topic are on the same account, you need to specify an SQS Queue Access policy that allows messages to be sent to this queue as long as the source ARN matches the ARN of the SNS topic to which the SQS queue is subscribed. The final part of such a policy was not reflected in the sample policy I provided previously, following is an updated sample policy that represents this more accurately.\n{\n  \"Version\": \"2012-10-17\",\n  \"Id\": \"AllowSNS\",\n  \"Statement\": [\n    {\n      \"Sid\": \"Sid1234567890123\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"*\"\n      },\n      \"Action\": \"SQS:SendMessage\",\n      \"Resource\": \"arn:aws:sqs:us-east-2:123456789012:my-queue\",\n      \"Condition\": {\n        \"ArnEquals\": {\n          \"aws:SourceArn\": \"arn:aws:sns:us-east-2:123456789012:my-topic\"\n        }\n      }\n    }\n  ]\n}\nThis policy can be set at the time the queue is being created as I showed in my previous response, or in a SetQueueAttributes call after GetQueueAttributes in your code sample. The latter would retrieve the ARNs for the SQS queue and SNS topic from the corresponding responses for the CreateTopic and GetQueueAttributes calls rather than having to hard-code the AWS account ID in the policy as my previous example showed:\ngo\n    queuePolicy := `{\n\"Version\": \"2012-10-17\",\n\"Id\": \"AllowQueue\",\n\"Statement\": [\n{\n\"Sid\": \"MySQSPolicy001\",\n\"Effect\": \"Allow\",\n\"Principal\": {\n\"AWS\": \"*\"\n},\n\"Action\": \"sqs:SendMessage\",\n\"Resource\": \"` + *qARN + `\",\n\"Condition\": {\n\"ArnEquals\": {\n\"aws:SourceArn\": \"` + *out.TopicArn + `\"\n}\n}\n}\n]\n}`\n    log.Printf(\"Adding queue access policy to queue.\")\n    _, err = sqsClient.SetQueueAttributes(&sqs.SetQueueAttributesInput{\n        Attributes: map[string]*string{\n            \"Policy\": &queuePolicy,\n        },\n        QueueUrl: out2.QueueUrl,\n    })\n    if err != nil {\n        return fmt.Errorf(\"setting policy: %v\", err)\n    }\nAs far as the validation error shown in your last screenshot is concerned, testing on both the web console and the AWS SDK for Go on my end did not show this behavior. Given that the subscription ID shows a subscription ARN rather than 'PendingConfirmation' the SQS queue should be successfully subscribed to the SNS topic (unless you selected a different subscription to open the 'Edit subscription attributes' window than the one included at the bottom of the screenshot). If you continue seeing this error when subscribing SQS queues to SNS topics after setting the Queue Access Policy I suggest reaching out to our Premium Support team by opening a new support case under the SNS service as they will have more in-depth knowledge on the service as well as access to internal tools that can give them more information about your SNS topic and the corresponding subscription to better troubleshoot this behavior.. @tj Have you had a chance to use the AWS SDK for Go with WASM to identify any potential pain points?. Hi @takeyourhatoff, thanks for reaching out to us. Implementing a waiter for DB Cluster snapshots seems like a reasonable feature request, once we have more information on this I'll update the issue accordingly.. Hi @challarao, thanks for reaching out to us. Unfortunately there is no way to set the SDK's logging level to return the HTTP response body without including the HTTP request body. That being said there are ways to retrieve information from the HTTP response without setting a debug level in the session config, such as adding a  handler to the session or issuing your requests with context (e.g. PutObjectWithContext if you're using the s3 package or UploadWithContext if you're using s3manager to manage your uploads for you). \nIf you give us additional details on what exactly you are looking to retrieve from the HTTP response we can provide more direct suggestions to fit your use case.. Thanks for the additional information! For your use case the best approach would be to keep your log level at LogDebugWithHTTPBody and replacing the named handler LogHTTPRequestHandler to omit the request body but not the response body as shown below.\ngo\n    svc := s3.New(sess)\n    svc.Handlers.Send.Remove(client.LogHTTPRequestHandler)\n    svc.Handlers.Send.PushFront(func(r *request.Request) {\n        origLogLevel := r.Config.LogLevel\n        defer func() {\n            r.Config.LogLevel = origLogLevel\n        }()\n        r.Config.LogLevel = aws.LogLevel(aws.LogDebug)\n        client.LogHTTPRequestHandler.Fn(r)\n    })\n. Hi @daviskoh, thanks for reaching out to us. The behavior you're describing doesn't look to be an issue with the IAM permissions associated with the credentials you're using for this call, as that would return an AccessDeniedException rather than a ResourceNotFoundException. \nIt sounds like the Cognito User Pool ID you're specifying in your params either doesn't exist or was created in a different AWS account than the account associated with the environment credentials you're using. I suggest double checking the Pool ID for the User Pool you want to list users from to ensure it matches the ID you're specifying in your parameters, and that this User Pool exists in the same AWS account as the IAM user associated with the API key you're setting in your environment variables.. Thanks for the update @daviskoh, I'm glad to hear this is now working as expected for you.\nIt is possible to hard-code static credentials, however we don't recommend doing so since it makes it very easy to accidentally expose your API keys to a larger audience than intended (e.g. committing to a repository visible to people that shouldn't have access to your API keys).. Hi @radeksimko, thanks for reaching out to us. While the SDK should currently work as expected when importing it via go mod, the go.mod file currently only includes runtime dependencies. Unfortunately we aren't quite prepared to drop support for godep at this time, as this would be a breaking change for the SDK. \nThat being said, this change is something we will consider in the future, once Go modules are supported in a more official capacity.. Hi @kannanvr, thanks for reaching out to us. If you're using the s3 package to upload objects to S3, just calling PutObject will perform a single-part upload. If you are instead using s3manager to manage your S3 uploads you can set the maximum upload parts for the uploader to 1 in order to avoid multi-part uploads as shown below.\ngo\n    uploader := s3manager.NewUploader(session, func(u *s3manager.Uploader) {\n        u.MaxUploadParts = 1\n    }). Hi @arminc, thanks for reaching out to us. Looking over IAM's API reference, the Tags member of IAM's Role data type is not marked as required, as such this member is not guaranteed to be present in all responses from IAM that contain the Role data type. \nUnfortunately IAM does not include a role's tags in the response for a ListRoles API call, which is why you're seeing this behavior both in the AWS SDK for Go and the AWS CLI. You may use ListRoleTags to list the tags for a given role, or iterate over the output of the ListRoles call to list tags for the listed roles as shown below.\ngo\n    resp, err := svc.ListRoles(&iam.ListRolesInput{})\n    if err != nil {\n        fmt.Println(\"error listing roles:\", err)\n    }\n    for _, role := range resp.Roles {\n        tags, err := svc.ListRoleTags(&iam.ListRoleTagsInput{\n            RoleName: role.RoleName,\n        })\n        if err != nil {\n            fmt.Println(\"error listing tags:\", err)\n        }\n        if len(tags.Tags) < 1 {\n            fmt.Printf(\"No tags for role %s\\n\", *role.RoleName)\n        } else {\n            fmt.Printf(\"Tags for role %s:\\n%v\\n\", *role.RoleName, tags.Tags)\n        }\n    }\nIf you'd like to see IAM's ListRoles API call include role tags in its response you may open a new feature request for IAM including your use case either by creating a new technical support case under the 'Identity and Access Management (IAM)' service or by creating a new post in the IAM section of the AWS Forums.. @arminc Thanks for the additional information, and I do apologize for the delay in our response. The Tags field in the Role struct is used in the output for various IAM functions including GetRole and CreateRole.\nGiven your use case I agree that it would be useful to include a role's tags in the ListRoles output. I've reached out to the IAM team about this internally, however submitting a feature request from your end as mentioned above would help focus attention on this request from the IAM team. Once I hear back from them I'll update the issue accordingly.\nIn the meantime, iterating through the ListRoles output to call ListRoleTags seems like the most functional approach to retrieve the roles you're looking for based on their tags. Can you expand on your concern regarding the volume of API calls to IAM to find the roles with the tags you're looking for?. Hi @kendavis2, thanks for reaching out to us. The documentation for DescribeParameters mentions that SSM will return the requested results on a best-effort basis, so receiving the number of results specified in MaxResults is not guaranteed.\n\nRequest results are returned on a best-effort basis. If you specify MaxResults in the request, the response includes information up to the limit specified. The number of items returned, however, can be between zero and the value of MaxResults. If the service reaches an internal limit while processing the results, it stops the operation and returns the matching values up to that point and a NextToken. You can specify the NextToken in a subsequent call to get the next set of results.\n\nI'm able to reproduce this behavior, but not to the extent you're describing. I've created 50 SSM parameters with the same prefix to reproduce this, issuing DescribeParameter calls for parameters that begin with this prefix using MaxResults values ranging betwen 5 and 50 will return different amounts of results, sometimes matching the MaxResults value and sometimes less. Unfortunately I'm not sure why you're only receiving 1 or 2 results from SSM on each DescribeResults call - if this exact behavior is occurring consistently regardless of changes in your MaxResults value, the prefix you're using for this call, and when the call is being issued (to account for requests received by the service in the region where you're issuing your requests) you should create a new SSM case with our Premium Support team to further troubleshoot this from the service end.\nIf this behavior is not consistent and you sometimes see fewer parameters returned than what you're expecting it sounds like the service is responding to your DescribeParameters requests as expected. In that case, you can account for this behavior by retrieving the NextToken value in the response that did not contain the complete set of parameters requested to use in another DescribeParameters request as shown below.\n```go\n    filters := []*ssm.ParameterStringFilter{\n        &ssm.ParameterStringFilter{\n            Key:    aws.String(ssm.ParametersFilterKeyName),\n            Option: aws.String(\"BeginsWith\"),\n            Values: aws.StringSlice([]string{prefix}),\n        },\n    }\ninput := &ssm.DescribeParametersInput{\n    ParameterFilters: filters,\n    MaxResults:       aws.Int64(int64(count)),\n}\n\n// Initialize variable to store and track parameters returned by service.\nvar ssmParams []ssm.ParameterMetadata\n\n// Loop until the amount of parameters stored in ssmParams\n// matches count provided for MaxResults.\nfor len(ssmParams) < count {\n    output, err := ssmClient.DescribeParameters(input)\n    if err != nil {\n        fmt.Println(\"error describing parameters:\", err)\n    }\n\n    // Add parameters returned by SSM to ssmParams\n    for _, p := range output.Parameters {\n        ssmParams = append(ssmParams, *p)\n    }\n    // Reduce next call's MaxResults to remaining desired results\n    // to prevent ssmParams from containing more parameters than desired.\n    input.SetMaxResults(int64(count - len(ssmParams)))\n\n    // End loop if no NextToken in response to prevent nil pointer dereference.\n    if output.NextToken == nil {\n        break\n    }\n    input.SetNextToken(*output.NextToken)\n}\n\nfor _, p := range ssmParams {\n    fmt.Println(*p.Name)\n}\n\n``. Hi @lockwobr, thanks for bringing this to our attention. I'm able to reproduce this consistently on boths3manager.Uploadands3.PutObject. I will bring this up to be added in our next sprint so that we can investigate and correct this behavior.. Hi @loohill, thanks for reaching out to us. The behavior you're seeing stems from the difference in the way theDeleteObjectandDeleteObjectsAPI calls handle theKeyelement in their input structs. WhileDeleteObjectwill ignore the leading slash for the object key (e.g. a request to delete/abc/aaa.sthwill result in S3 deletingabc/aaa.sthin the specified bucket),DeleteObjectswill include that leading slash in the key name. \nTrimming the leading slash from the key name should allowDeleteObjectsto work as expected outside of the loop that populates the keys in theobjectsvariable.. @cbarraford Thanks for the additional info. The only way I'm able to reproduce your error inside a Docker container created with the given Dockerfile is by creating a broken.aws/credentials` file - without this file present, the AWS SDK for Go uses the credentials specified in environment variables without issues.\n```\n$ docker run -v $PWD:/root -it go-2455 /bin/sh\n/go # go get -u github.com/aws/aws-sdk-go\n/go # cd /root/\n~ # export AWS_ACCESS_KEY_ID=\n~ # export AWS_SECRET_ACCESS_KEY=\n~ # go run s3creds.go         // Code runs without errors here\n~ # mkdir .aws\n~ # echo '[text' > .aws/credentials\n~ # go run s3creds.go \nerror uploading file: SharedConfigLoadError: failed to load config file, /root/.aws/credentials\ncaused by: INIParseError: invalid state with ASTKind {section_stmt {1 STRING 0 [116 101 120 116]} true []} and TokenType {0 NONE 0 []}\n~ # \n```\nDuring my testing I did notice that replacing session.NewSession() with session.New() in the UploadS3File function resulted in the SDK using the environment variable credentials without errors even with the broken .aws/credentials file present. I've marked this issue as a bug since session.NewSession() shouldn't take the .aws/credentials file into account if credentials are set via environment variables.. Hi @buroa, thanks for reaching out to us about this. Can you share more details on how this behavior is affecting you? Are you unable to connect to these services via AWS PrivateLink as a result of the way in which these endpoints are modeled?. Hi @b-dean, thanks for reaching out to us about this. Unfortunately I haven't been able to reproduce this on my end, more information on how you're running into this would help. \n\nAre there any common elements on instances where you receive this error? For example, does this only happen with a specific instance type / AMI / AWS region / credential set / VPC?\nCan you share sanitized debug logs that show this behavior for further review on our end? You can enable debug logging by including a LogLevel value in your aws.Config struct like so:\n\ngo\n    sess, err := session.NewSession(&aws.Config{\n        Region:   aws.String(region),\n        LogLevel: aws.LogLevel(aws.LogDebug),\n    }). Thanks for reaching out to us @shawnhankim. Unfortunately I'm not entirely sure what you're requesting here. Are you trying to change the product code present in the instance metadata for an instance that was launched using a paid AMI? Or do you want to use the AWS SDK for Go to mock out the product code that the EC2 metadata client returns to the SDK?. Hi @minamijoyo, thanks for reaching out to us about this and for the extensive level of detail provided regarding this behavior. I'm looking into this and will update again once I have more information. Please feel free to reply with any additional information you come across on this in the meantime.. Thank you for your patience in this matter @minamijoyo. As you've mentioned, this behavior stems from go-jmespath attempting to evaluate pointer values against literal values; I've opened https://github.com/jmespath/go-jmespath/issues/40 to address this.. Hi @kurtostfeld, thanks for reaching out to us. Unfortunately I haven't been able to reproduce the described behavior with the provided code. If you're hard-coding your credentials as shown in the doIt function this could be due to a typo or missing/extra character in your secret access key, can you double check this to ensure it is correct?. Hey @nqbao, thanks for reaching out to us. The SigV4 signing process for IoT Websocket URLs when using temporary session credentials requires appending the session token (X-Amz-Security-Token) to the end of the URL after the canonical request is signed. By contrast, the standard SigV4 signing process implemented in the non-IoT AWS SDK's signers requires including the session token in the canonical request before the request is signed, so these signers won't work for Websocket URLs when using temporary session credentials. Instead you'll need to perform the SigV4 signing manually as shown in your second snippet or use one of the AWS IoT Device SDKs to pre-sign Websocket URLs using credentials that require a session token.. Thanks for reaching out to us about this @AlexGascon. Unfortunately we do not yet support Amazon Transcribe's streaming feature on the AWS SDK for Go, however this is something we do plan on implementing in the near future. Once we have more information on this we will update the issue accordingly.. Thanks for reaching out to us @aisensiy. Unfortunately we do not have a code example for the AWS SDK for Go at this time that iterates through a given directory to upload multiple files concurrently. We do have the example sync.go that creates an iterator to batch upload files in the specified folder, however the concurrency in this example falls within the multi-part uploads for each object in the specified folder rather than concurrently uploading all files in the folder and its sub-folders. This example could be used as a reference to write something that, for example, calls s3manager's Upload function within a goroutine for each file within the specified folder rather than using UploadWithIterator to batch these uploads.. Hi @klauern, thanks for reaching out to us. The ConfigurationItem data type for AWS Config is part of the API model provided to us by the service team, as such any changes made from our end would be undone the next time the API model is updated by the service team. In addition to this, altering the type for ConfigurationItem.Configuration would be a breaking change.\nUnfortunately the AWS SDKs don't support dynamic JSON documents; given the wide variety of resource types with different sets of properties that can be represented in ConfigurationItem.Configuration, this member's format is quite dynamic in nature. As a result, the AWS Config team decided to make this member a string instead of a combination of shapes to accommodate every resource type currently supported by the service - this implementation also has the advantage of preventing breaking changes for this shape should new resource types be added to the list of resources that can be managed by AWS Config.. Hi @roger-king, thanks for reaching out to us. Unfortunately I haven't been able to reproduce the behavior you describe when saving the change set ID to a file in one function calling CreateChangeSet and then reading the same file in separate functions calling WaitUntilChangeSetCreateComplete and ExecuteChangeSetRequest. \nCan you provide a code sample so we can better understand how you're interacting with the file containing this change set ID and how you're using this file in relation to these API calls to CloudFormation?. Thanks for reaching out to us @shwetahealthians. Unfortunately I'm not able to reproduce the described behavior, however based on the error returned it sounds like SQS is prematurely terminating the connection to your Lambda function. Given that the behavior is not consistent this seems like a service issue moreso than a problem with the AWS SDK for Go, as such I recommend reaching out to our Premium Support team by opening a new support case under the SQS service for further assistance on this as they will have more in-depth knowledge of the service and how other services interact with it.. ",
    "dahankzter": "Thx for the prompt response!\nWe are using it very vanilla style as in the examples but we are creating a single instance of both Session and S3 service. We are presigning GET download urls, is that what you mean by which S3 api we are using?\nA typical payload can have a hundred urls plus extra metadata describing these urls to our client apps.\nThese are served in a pretty standard HTTP API manner with json format. Normally we see the cpu being dominated by json serialization since we often cache our basic structures. In this cases however we see the json share being completely overshadowed by the presigning.\nCaching the presigned urls for some time (less than the S3 timeout) gives it the usual profile where json serialisation dominates and the request throughput is increased upwards of 20 times.\nYour profile seems to have similar shape as mine but my numbers are much bigger after hammering the API with wrk. The system seems to be maxing out on CPU during these tests but I haven't sorted all the stats available since it was so clear from the profile what was taking time.\nI don't know what to expect actually. It is natural that signatures are cpu intensive but as you say if it is possible to reduce the effect of the code surrounding hmac computations etc that would be good.\n. ",
    "cwedgwood": "Replying to myself...\nLook at this API it's quite different to the rest, auto-generation probably isn't relevant here though it would be possible to have suitable wrapped for auth and endpoints.. ",
    "jeffb4": "That change looks amazing. I'm not certain but I'd suspect the Ruby sdk would choke with a \"0\". Its error handling around that isn't the greatest.. ",
    "hori-ryota": "build failed but I didn't touch s3manager \ud83d\ude07 \n--- FAIL: TestUploadUnexpectedEOF (0.04s)\n    upload_test.go:824: Expected \"UploadPart\", but received \"AbortMultipartUpload\"\npanic: interface conversion: interface is nil, not io.Reader [recovered]\n    panic: interface conversion: interface is nil, not io.Reader\ngoroutine 252 [running]:\ntesting.tRunner.func1(0xc420096a50)\n    /home/travis/.gimme/versions/go1.9.2.linux.amd64/src/testing/testing.go:711 +0x5d9\npanic(0xb91040, 0xc420170680)\n    /home/travis/.gimme/versions/go1.9.2.linux.amd64/src/runtime/panic.go:491 +0x2a2\ngithub.com/aws/aws-sdk-go/service/s3/s3manager_test.buflen(0x0, 0x0, 0xc27c09)\n    /home/travis/gopath/src/github.com/aws/aws-sdk-go/service/s3/s3manager/upload_test.go:96 +0x4f\ngithub.com/aws/aws-sdk-go/service/s3/s3manager_test.TestUploadUnexpectedEOF(0xc420096a50)\n    /home/travis/gopath/src/github.com/aws/aws-sdk-go/service/s3/s3manager/upload_test.go:832 +0xa92\ntesting.tRunner(0xc420096a50, 0xc459b8)\n    /home/travis/.gimme/versions/go1.9.2.linux.amd64/src/testing/testing.go:746 +0x16d\ncreated by testing.(*T).Run\n    /home/travis/.gimme/versions/go1.9.2.linux.amd64/src/testing/testing.go:789 +0x569\nFAIL    github.com/aws/aws-sdk-go/service/s3/s3manager  12.485s. ",
    "sdboyer": "This is the second time this happened in four months or so - is this something that could be checked as part of a CI process?. @xibz awesome. thanks!. ",
    "ronnylt": "Working as expected now!\nThanks!\nEl El vie, 26 ene 2018 a las 1:34 p. m., sam boyer notifications@github.com\nescribi\u00f3:\n\nThis is the second time this happened in four months or so - is this\nsomething that could be checked as part of a CI process?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-go/issues/1755#issuecomment-360773469,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AABf4jNBL9qWn1eeiwF3EAKLfn6kxuytks5tOcZQgaJpZM4Rt8Yi\n.\n. \n",
    "yizha": "@xibz it is not a question/issue about whether the request will be retried or not, it is about the abnormal slow download speed when there are a lot of concurrent downloaders. We have already switched to use s3.GetObjectWithContext(...) with a timeout context to break the slow downloader, so it won't occupy the go routine for too long, I would assume switching to s3manager.Downloader won't help with the slow downloading in this case. . btw I can reproduce the issue with concurrency like 70 or even 60, not like 100 which guarantees to reproduce it on my box, I have to run it several times with 70 or 60 to reproduce it, but I never be able to reproduce it with concurrency 50 or lower, and it looks like an contention issue/bug in the sdk.. @xibz - The s3 object I test with is about 6MB so for 100 concurrent downloads the memory for storing them is 600MB+. My test box has 8GB memory so I don't think memory has anything to do with my test. Anyway I ran the test again and monitored the go processes with top and checked system memory with free and the result seems reasonable,\nTest script is run with\nShell\n$ C_CNT=100 go run test.go\nOutput from 'top -p' while the test is running\nShell\n  PID USER      PR  NI    VIRT    RES  %CPU %MEM     TIME+ S COMMAND                                              \n10455 yizha     20   0  670.4m  12.3m   0.0  0.2   0:00.15 S go                                                   \n10479 yizha     20   0 1712.0m 1.091g   0.0 14.1   0:15.84 S  `- test\nOutput from 'free -m'\nShell\n[yizha@yizha-hp-z420 ~]$ free -m\n              total        used        free      shared  buff/cache   available\nMem:           7916        2385        1108          12        4422        5223\nSwap:          4095           0        4095\nBut to make sure the issue is not related with memory usage I replaced 'ioutil.ReadAll(...)' with a for-loop which calls Read(...) with a []byte buffer of 8192  so now for each download it only needs 8KB memory. I ran it again, this time it only used ~28MB memory but the result was same that some downloads were extremely slow and ended with the \"connection reset by peer\" error.  \nSource code which uses for-loop to read whole response body with a 8KB []byte buffer\n```Go\npackage main\nimport (\n    \"fmt\"\n    \"io\"\n    \"os\"\n    \"strconv\"\n    \"sync\"\n    \"time\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\n)\nfunc main() {\nconfig := &aws.Config{\n    MaxRetries: aws.Int(0),\n    Region:     aws.String(\"us-east-1\"),\n}\n\nsess := session.Must(session.NewSession(config))\ns3 := s3.New(sess)\n\nconcurrency, err := strconv.Atoi(os.Getenv(\"C_CNT\"))\nif err != nil {\n    panic(err.Error())\n}\nfmt.Printf(\"concurrency: %d\\n\", concurrency)\n\nfmt.Println()\n\nbucket := \"tr-content-archive-us-qa\"\nkey := \"0DE3/tag:reuters.com,2018:binary_RC1B9F653ED0-BASEIMAGE:191352371\"\nwg := &sync.WaitGroup{}\nwg.Add(concurrency)\nfor i := 0; i < concurrency; i++ {\n    go download(i, s3, bucket, key, wg)\n}\nwg.Wait()\n\n}\nfunc readBody(r io.Reader) (int, error) {\n    bytesRead := 0\n    buf := make([]byte, 8192)\n    for {\n        n, err := r.Read(buf)\n        if n > 0 {\n            bytesRead += n\n        }\n        if err != nil {\n            if err == io.EOF {\n                return bytesRead, nil\n            } else {\n                return bytesRead, err\n            }\n        }\n    }\n}\nfunc download(id int, client s3.S3, bucket, key string, wg sync.WaitGroup) {\n    fmt.Printf(\"[%d] start downloading ...\\n\", id)\ndefer wg.Done()\n\nt1 := time.Now()\n\nresp, err := client.GetObject(&s3.GetObjectInput{\n    Bucket: aws.String(bucket),\n    Key:    aws.String(key),\n})\nbytesRead := 0\nif err != nil {\n    fmt.Printf(\"[%d] failed to get object, error: %v\\n\", id, err)\n} else {\n    defer resp.Body.Close()\n    bytesRead, err = readBody(resp.Body)\n    if err != nil {\n        fmt.Printf(\"[%d] failed to read body, error: %v\\n\", id, err)\n    }\n}\nfmt.Printf(\"[%d] finished download job (read %d bytes) in %v\\n\", id, bytesRead, time.Now().Sub(t1))\n\n}\n```\nOutput from 'top -p'\nShell\n  PID USER      PR  NI    VIRT    RES  %CPU %MEM     TIME+ S COMMAND                                              \n10626 yizha     20   0  598.2m  12.1m   0.0  0.2   0:00.18 S go                                                   \n10651 yizha     20   0  550.0m  28.2m   0.0  0.4   0:14.46 S  `- test\nOutput from 'free -m'\nShell\n[yizha@yizha-hp-z420 tmp]$ free -m\n              total        used        free      shared  buff/cache   available\nMem:           7916        1293        2522          12        4100        6315\nSwap:          4095           0        4095. @xibz - I enabled debug log with \nGo\nconfig.WithLogLevel(aws.LogDebug)\nthen reran test.go and attached the full output as a file test.log. \nThis time the result is really bad, the longest one took 18 minutes to finish and failed at last. And I don't think it is throttled because there is no retry (I set MaxTries to 0) and the responses were 200, and even for the failed ones they did read some bytes from response but didn't finish reading all. You can tell from the log which prints out how many bytes it reads from response body no matter it is a success download or not.\n[71] failed to read body, error: read tcp 10.90.7.237:36854->52.216.98.35:443: read: connection reset by peer\n[71] finished download job (read 1688073 bytes) in 18m0.034458929s. ",
    "huangxc2013": "Do we have a solution for this now? Been experiencing the same problem.. ",
    "suanziliu": "Hi, I also met this issue in my program(Each go routine will download a .gz file then read its content). \nWhen we download 70*20M .gz files from s3(ap-northeast-1) by using s3manager.Downloader(file, &s3.GetObjectInput{}), it will stuck for 5 minutes for after dealing with 66 goroutines. \nI got following stack trace by using runtime/pprof when it is stucked, details are attached. \ngoroutine_profile.txt\nI know that aws ec2 will throttle requests if exceeds certain limits, is s3 still has such limites?. ",
    "nickrobinson": "You don't have to create a thing with a name having a leading '/'. All you need to do is publish to a topic with a leading '/'. I will add a working example to this issue tomorrow morning.. ### Command Used To Execute\nAWS_REGION=us-east-1 go run example.go\nexample.go\n```\npackage main\nimport (\n    \"encoding/json\"\n    \"fmt\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/iot\"\n\"github.com/aws/aws-sdk-go/service/iotdataplane\"\n\n)\ntype Record struct {\n    Foo string\n}\nfunc main() {\n//sess := session.New(aws.NewConfig().WithLogLevel(aws.LogDebug))\nsess := session.New(&aws.Config{})\n\niotSvc := iot.New(sess)\nres, err := iotSvc.DescribeEndpoint(&iot.DescribeEndpointInput{})\nif err != nil {\n    fmt.Println(\"describe endpoint err\", err)\n    return\n}\nfmt.Println(\"Endpoint:\", res)\n\nsvc := iotdataplane.New(sess, &aws.Config{Endpoint: res.EndpointAddress})\nr := Record{\n    Foo: \"bar\",\n}\n\npayload, _ := json.Marshal(r)\nfmt.Println(string(payload))\n\nresult, err := svc.Publish(&iotdataplane.PublishInput{\n    Topic:   aws.String(\"/test\"), // Required\n    Payload: payload,\n    Qos:     aws.Int64(0),\n})\n\nfmt.Println(\"publish\", result, err)\n\n}\n```. ",
    "sapnatodwal": "Figured this out so closing the issue.. ",
    "bhmorse": "so for clarity, I should be able to set up a PostContent call with a buffer, that I add to after and it should work? For example:\n```go\nbytesBuf := []bytes{}\nbuf := bytes.Buffer(bytesBuf)\n// CALL GO ROUTINE HERE THAT CALLS buf.Write() WITH NEW DATA WHEN I GET IT\nresp, err := svc.PostContent(&lexruntimeservice.PostContentInput{\n    BotAlias:    aws.String(\"botAlias\"),\n    BotName:     aws.String(\"botName\"),\n    ContentType: aws.String(\"audio/l16; rate=16000; channels=1\"),\n    UserId:      aws.String(\"userID\"),\n    InputStream: aws.ReadSeekCloser(buf),\n})\n```\nI am using this pattern and getting truncated results.. Thanks, the pipe did the trick, everything is running as expected now. I am going to try and add endpointing so I can stream arbitrary audio now.. ",
    "hmcgonig": "Everything should be fixed up after my last rebase from yesterday, I had just forgot to call Seek() in nextReader() in the readerAtSeeker case. The latest travis build is marked as passing.. The big problem is that in the previous iteration nextReader() is allocating a buffer here https://github.com/aws/aws-sdk-go/blob/master/service/s3/s3manager/upload.go#L535. Then, nextReader() is called in a loop here https://github.com/aws/aws-sdk-go/blob/master/service/s3/s3manager/upload.go#L644 which means that we're creating as many buffers as it takes to read in the entire upload. Eventually this memory will be reclaimed via gc, but in the short term there is significant memory bloat. \nMy implementation gets around this by processing the data all with a set amount of worker goroutines. Each goroutine allocates its own buffer only once when it is spawned (this is the line https://github.com/aws/aws-sdk-go/pull/1784/files#diff-cb8f38ef6559870d3538aceb52550e2bR664), then it reuses that same buffer over and over.\nI was perf testing this change in another application that's backing up many database files and went from having my application OOM killed at 1G ram, to now only using 60% ram at peak upload time (it'd be even lower, but the app is doing other things as well). \nI'm going to whip up a simple test project that just does an upload of a larger file and I'll update here with the results between old/ new implementations.. I wrote an app that uses an io.Pipe and passes the reader into the upload input body. When I write a 1gb file to that writer using the old implementation, the memory increase looks like this:\ncopying file to writer\nuploading file\nAlloc: 105186\nTotal Alloc: 1087585\nHeap Alloc: 105186\ndone\nthen using my implementation:\ncopying file to writer\nuploading file\nAlloc: 27580\nTotal Alloc: 47345\nHeap Alloc: 27580\ndone. I reworked everything so that it overall stays the same, but uses sync.Pool to generate a new []byte as it needs one. The byte arrays are put back into the pool once the data is sent.. This benchmark is still much better than the original:\nAlloc: 95210\nTotal Alloc: 108848\nHeap Alloc: 95210\nSo about 10% of the original's total allocation.. @xibz What else is left on this one? I think it's ready to go for another review when you get a moment. Thanks!. No problem \ud83d\udc4d . Oh wow, yeah you're totally right. The only reason this worked at all was because this method is only ever called when u.readerPos is 0. I'll change this to explicitly be 0.. I like it. This would work and probably end up being much less code change. I'll rework this and update asap.. ",
    "bbeaudreault": "How does the benchmark look with the new approach?. ",
    "tpetr": "Hey @xibz! :wave: Any chance we can get your eyes on this PR soon? We're excited to get it rolled out so that we can solve our OOM kill issues in Vitess once and for all. Thanks!!. ",
    "sjeandeaux": "@xibz Thanks for your answer. \nI want just to validate it. I want to be sure GOMAXPROCS is equal NumCPU.\nI have no problem, we have the number of CPU equals to maximum number of CPUs .\ngolang\nruntime.NumCPU()//==2\nruntime.GOMAXPROCS(-1)//==2. ",
    "prats226": "@xibz Thanks for the response. Just making sure I am being clear. I am referring to Godoc generated by golang code [https://godoc.org/github.com/aws/aws-sdk-go/service/batch#Batch.ListJobs] and not documentation on aws documentation. If it is generated by forums guys, then will file this issue there.. ",
    "outcoldman": "@jcoyne have you tried to use omitempty (see from docs)\ntype Resource struct {\n    ID        string     `json:\"id\"`\n    SourceID  string     `json:\"source_id,omitempty\"`\n}. ",
    "jcoyne": "Yes, omitempty does fix if for an initial create. @seongju I don't recall, but I suspect it I was looking at documentation for a different version of the library from the code I was using.\nI'm doing this now on  v1.13.16 and it works: \nsess := session.NewSession()\ndynamoConfig := &aws.Config{Endpoint: aws.String(dynamodbEndpoint)}\ndynamodb.New(session, dynamoConfig). ",
    "edwardbrowncross": "The Node.js SDK returns a URL that looks like this:\nhttps://<bucketname>.s3.eu-west-2.amazonaws.com/test2.jpeg?Content-MD5=xikaAsi2auHyFu3BqXHo8Q%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAI**********%2F20180226%2Feu-west-2%2Fs3%2Faws4_request&X-Amz-Date=20180226T153657Z&X-Amz-Expires=600000&X-Amz-Signature=59ab3f8419fb393890aea61e3a2563d8a0c073bfc450a340532f4b2232ee84f1&X-Amz-SignedHeaders=content-md5%3Bhost\nYou can see that it contains the Content-MD5 querystring parameter. ",
    "bharath-srinivas": "I have fixed this issue in my fork and tested it. If you want I can create a PR for the same.. Any update on this?. @xibz, Thanks for the reply and for the mock example. This cleared my doubts on how to write mock test for batch download. I'm closing this issue now.. ",
    "elegos": "Hello @jasdel \nAfter investigating, it seems it's a batch size issue. Running the lambda locally in fact results in an error informing the batch size exceedes the maximum limit of 256 kB.\nI can only suppose that in the Lambda ecosystem inside the AWS environment errors are sent back to the function as HTML documents instead of XML ones.. Unfortunately I've already fixed the problem by my side (avoiding sending batch requests), but you can easily reproduce the error by invoking a lambda sending more than 256 kB (invocation type request). In my case it was around 3MB of data. I hope it can help. If you still can't reproduce the error I'll try spending some time creating an example.. ",
    "seongju": "@jcoyne, just wondering but why did you close this issue? I'm having a similar problem using a session for a s3manager.NewUploader.. Yikes, I really thought I tried that... That worked, thank you.. ",
    "adubkov": "I had this error when pass session.Session as an argument. In my case solution was passing it by reference.. ",
    "otmezger": "I was having the same problem, but upgrading to the newest version of the SDK solved the problem. (1.18.2). ",
    "doertedev": "Works great. Thanks a lot!. ",
    "btsteve": "For some reason, it took a while for me to find that, but it is what i used. . ",
    "bflad": "Hi, @xibz! \ud83d\udc4b I'm a maintainer of aforementioned Terraform AWS provider above. We can certainly implement that code and in fact we think we have a decent implementation ready to go. \ud83d\ude04\ngo\n// client.s3conn being a *s3.S3\n// isAWSErr being a simple wrapper for working with awserr.Error\nclient.s3conn.Handlers.Retry.PushBack(func(r *request.Request) {\n    if isAWSErr(r.Error, \"NotImplemented\", \"\") {\n        r.Retryable = aws.Bool(false)\n    }\n})\nOut of curiosity, is there a good reason why the SDK should be retrying on this error code at all? Does AWS have actual use cases for NotImplemented API responses that would flip their response within a normal retry period (say up to 5 minutes)? Just seems like an edge case (API service/action being turned on in a region) unless this error code is used for other purposes.. Looks like this was added in v1.13.27 -- closing!. Hi @btai24 \ud83d\udc4b  Sorry, we do not currently. See also (and \ud83d\udc4d  upvote): https://github.com/terraform-providers/terraform-provider-aws/issues/2972. If I had to venture a guess, it may have to deal with eventual consistency in the service. ELBv2 historically has displayed it can take a few seconds to propagate various changes.. Would you folks accept a pull request similar to https://github.com/aws/aws-sdk-go/pull/2080?. Submitted a pull request just in case. \ud83d\ude04 #2395. Ah, sorry I missed that @jasdel. \ud83d\ude41 Thanks for pointing that out.\nLuckily we don't depend heavily on the default endpoint resolvers in this manner, however it would be great if removals like these were bumped at least in a minor version to signify their potential for breaking behavior.. ",
    "timduhenchanter": "Thanks @jasdel \nreference if anyone is interested:\nhttps://forums.aws.amazon.com/thread.jspa?threadID=275626&tstart=0. ",
    "YakDriver": "@cboss24 If you are still interested in this, I've submitted PR #2217 to provide the feature.. Please see PR #2217 where I have addressed each of @jasdel 's review points on top of this original PR.. ~~Currently working on getting make ci-test to pass~~. @xibz @jasdel Any idea why some of the travis-ci builds are failing? They are unrelated to my changes.. Updated to work with #2210 (internal/ini: add support for ini parser). For what it's worth, locally make ci-test is working:\n...\n?       github.com/aws/aws-sdk-go/service/xray  [no test files]\n?       github.com/aws/aws-sdk-go/service/xray/xrayiface    [no test files]\nCI test validate no generated code changes. @jasdel @xibz x/tools are only supported for the latest two versions of Go: 1.10 and 1.11. Your travis-ci test mixes 1.7 and 1.8 with x/tools. Specifically, x/tools tries to call obj.IsAlias(), which didn't exist until 1.9. Is there anything I can do to avoid the 1.7 and 1.8 tests?\n . @jasdel it looks like all the Linux/Darwin 1.9, 1.10, 1.11 tests for this PR are working fine. I've run the the PR tests on an EC2 windows instance also and they ran fine. Looking forward to a review when you get a chance. \ud83d\ude03 . - Botocore, shared_config implementation of credential_process, November 20, 2017, by @JordonPhillips \n- Ruby, shared_config implementation of credential_process, July 3, 2018, by community contributor (@YashoSharma). I don't dispute that there are risks. \ud83c\udf83 \ud83d\ude28 All features must face a risk/benefit determination. We just ask that a consistent risk/benefit determination be made across AWS CLI, Ruby, Python, and go.\nThis PR follows a similar mitigation pattern to the other implementations. It will only use credential_process if other, pre-existing credential mechanisms don't exist. At the very least, that makes it very noticeable that you are using credential_process. If it's setup to run an arbitrary command, you won't get credentials and CLI, Ruby, Python, go won't connect anymore. An unauthorized change to credential_process can't be slipped in without breaking existing apps.\nIn fact, CLI/Ruby/Python/go sharing the same shared_config and working the same way provides a great security benefit over one or the other working independently. If the config is compromised, you'll notice it across apps and platforms.\nFor example, here, a slipped in credential_process would be entirely ignored, and never executed, similar to the botocore and Ruby implementations.\n[default]\naws_access_key_id = <YOUR_DEFAULT_ACCESS_KEY_ID>\naws_secret_access_key = <YOUR_DEFAULT_SECRET_ACCESS_KEY>\ncredential_process = <YOUR_PROCESS_COMMAND>\nThe very precise configuration is akin to purposely enabling the feature.\nThis PR's go implementation further mitigates beyond the botocore and Ruby implementations by providing a process timeout and a very limited stdout/stderr buffer.. @kyleknap @jamesls @JordonPhillips @awood45 probably discussed all this when considering credential_process for botocore and Ruby. Maybe they have some useful input?. Terraform sits on AWS SDK for Go. If you use Terraform to manage your AWS infrastructure, you have no way of using the same credential_process config file you use with the CLI.. In our organization and the organizations we support, we have many, many \"users\" who use, consume, and work with AWS. None of them are aws-sdk-go users. . @xibz Absolutely! This is excellent news.. @xibz I'll look back at the boto/CLI code to make sure this is handled as exactly the same as possible. I'll post back after with some tests to verify parity.. The processes look exactly the same, as far as the credential_process provider is concerned:\nThe CLI/botocore credential chain is:\nenv -> assume role -> shared -> process -> config -> ec2 -> boto -> container -> instance metadata\nThe proposed go credential chain is:\nenv -> assume role -> shared -> process -> remote (local http -> ECS -> ec2)\nWith the new addition here. The defaults.RemoteCredProvider is called here and the remote chain is here.\nI will post results of tests comparing the CLI with this PR shortly.. @xibz @jasdel This is ready for another look. I've made the changes requested.\nok      github.com/aws/aws-sdk-go/aws/credentials/processcreds  2.055s  coverage: 93.1% of statements\nSuccess: Tests passed.\nIn addition, I made some changes that make MFA useable/possible:\n\nDisplay stderr on the console (i.e., the os's stderr). stderr is used by MFA processes for prompts (e.g., Please choose from the following authentication choices:).\nConnect the subprocess's stdin to console input (os stdin). By default the subprocess does not have any access to the console input. If the subprocess's stdin pipe is not connected to anything (the default), the subprocess doesn't wait for input. (For example, if the subprocess is a bash script with a read -r myvar, the read will return immediately and the script will continue.) \n\nCLI allows MFA but doesn't display stderr (would be fixed in boto/botocore#1349) making it difficult to use unless you know what to enter without seeing the prompts.\nLet me know if you have additional feedback.. Here are results from a comparison between when the CLI and aws-sdk-go executing the credential_process. \nBottom Line\nFor every test where CLI and go differed, the go implementation is correct (IMO).\nProcedure: I provided valid credentials either through environment variables (\"env\"), the shared config file (\"shconf\"), the shared credential file (\"shcred\"), and/or through the credential_process (\"cp\"). In the \"cli?\" and \"go?\" columns, \"yes\" means that the credential_process was executed. The \"diff?\" column indicates whether the CLI and go were the same (\"\u2205\") or different (\"\u03b4\"). Below is a description of each of the differences.\nDetails of each of the tests that were different are available below.\nResults\n| # | env | shconf | shcred | cli? | go? | diff? |\n| ---| --- | --- | --- | --- | --- | --- | \n| 1 | no | no | cp | yes | yes  | \u2205 |\n| 2 | no | no | yes/cp | no | no | \u2205 |\n| 3 | no | yes | cp | yes | no | \u03b4 |\n| 4 | no | yes | yes/cp | no | no | \u2205 |\n| 5 | yes | no | cp | yes | no | \u03b4 |\n| 6 | yes | no | yes/cp | no | no | \u2205 |\n| 7 | yes | yes | cp | yes | no | \u03b4 |\n| 8 | yes | yes | yes/cp | no | no | \u2205 |\n| 9 | no | cp | no | yes | yes | \u2205 |\n| 10 | no | cp | yes | no | no | \u2205 |\n| 11 | no | yes/cp | no | yes | no | \u03b4 |\n| 12 | no | yes/cp | yes | no | no | \u2205 |\n| 13 | yes | cp | no | yes | no | \u03b4 |\n| 14 | yes | cp | yes | no | no | \u2205 |\n| 15 | yes | yes/cp | no | yes | no | \u03b4 |\n| 16 | yes | yes/cp | yes | no | no | \u2205 |\n| 17 | yes | yes/cp | yes/cp | no | no | \u2205 |\nTest 3\n\nCLI executes the credential_process\naws-sdk-go does not execute the credential_process\n\nConfiguration\nENV\n(No environment credentials)\nShared credentials file\n[default]\ncredential_process = ./process.sh\nShared config file\n[default]\naws_access_key_id = <VALID>\naws_secret_access_key = <VALID>\nTest 5\n\nCLI executes the credential_process\naws-sdk-go does not execute the credential_process\n\nConfiguration\nENV\nAWS_ACCESS_KEY=<VALID>\nAWS_SECRET_ACCESS_KEY=<VALID>\nShared credentials file\n[default]\ncredential_process = ./process.sh\nShared config file\n(None)\nTest 7\n\nCLI executes the credential_process\naws-sdk-go does not execute the credential_process\n\nConfiguration\nENV\nAWS_ACCESS_KEY=<VALID>\nAWS_SECRET_ACCESS_KEY=<VALID>\nShared credentials file\n[default]\ncredential_process = ./process.sh\nShared config file\n[default]\naws_access_key_id = <VALID>\naws_secret_access_key = <VALID>\nTest 11\n\nCLI executes the credential_process\naws-sdk-go does not execute the credential_process\n\nConfiguration\nENV\n(No environment credentials)\nShared credentials file\n(None)\nShared config file\n[default]\naws_access_key_id = <VALID>\naws_secret_access_key = <VALID>\ncredential_process = ./process.sh\nTest 13\n\nCLI executes the credential_process\naws-sdk-go does not execute the credential_process\n\nConfiguration\nENV\nAWS_ACCESS_KEY=<VALID>\nAWS_SECRET_ACCESS_KEY=<VALID>\nShared credentials file\n(None)\nShared config file\n[default]\ncredential_process = ./process.sh\nTest 15\n\nCLI executes the credential_process\naws-sdk-go does not execute the credential_process\n\nConfiguration\nENV\nAWS_ACCESS_KEY=<VALID>\nAWS_SECRET_ACCESS_KEY=<VALID>\nShared credentials file\n(None)\nShared config file\naws_access_key_id = <VALID>\naws_secret_access_key = <VALID>\ncredential_process = ./process.sh\n. @jasdel I updated the folder name from testjson to testdata.. @xibz @jasdel I cleaned out several things that I didn't like about my implementation. The new changes have reduced the number of lines of code and increased test coverage (now at 95.1%): \n\nNo longer magically add hardcoded system path to path. If the user doesn't have a functional environment, there's no need to try to save them.\nRemove use of the kludgey AlreadyPreparedCommandKey environment variable, which saved very little and unnecessarily involved the environment.\nTreat windows/nix/mac the same when it comes to subshell launch. Before nix went to /bin/sh, while Windows just went to cmd. Now, both rely on the user's PATH.. @xibz @jasdel I made the requested changes. I added new commits to make review easier but happy to squash if you prefer.\n\nAre either of you at re:Invent?. Excellent news! We look forward to being among the first to use it.. @jasdel 512 is ultra conservative \ud83d\ude1e . In my opinion, even 2048 would be safe and accommodate most situations.\n@bodhi DefaultBufSize is a const, as it should be because it's a default. However, MaxBufSize is exported and can be set programmatically for an individual ProcessProvider. (This doesn't help much if, like us, there's a layer or two between you and the aws-sdk-go and you need it to work out of the box.)\ngo\n    creds := processcreds.NewCredentials(\n        \"/path/to/command\",\n        func(opt *ProcessProvider) {\n            opt.Timeout = time.Duration(2) * time.Minute\n            opt.Duration = time.Duration(60) * time.Minute\n            opt.MaxBufSize = 2048\n        }). @xibz I copied the same warning from CLI here. Let me know if it's needed elsewhere too.. CLI/botocore requires the credential process to return a json in a specific format including \"Version\": 1,.. Absolutely, good call.. Sure, I cleaned up the goroutine logic.. Instead of creating this custom LimitedBuffer, I simplified by adding an io.LimitReader() to the other side of the pipe.. Yes, definitely.. Removed lazy load and added 3 constructors for what I think will be common scenarios.. The AlreadyPreparedCommandKey was a bit hackish and I've removed it. . This avoids the splitting problem. exec.Command() requires the program and args to be separate. Simplistic splitting on spaces (i.e., strings.Split(cmd, \" \") or strings.Fields(cmd)) will cause errors if quoted arguments have spaces (e.g., echo \"My name is Bob\").  Using a subshell allows us to pass all that complication to the OS to determine the correct argument splitting.\n\nUnexpected behavior of exec.Command argument parsing\nSplitting up quoted/escaped command line arguments\nCore code of Terraform's local exec provisioner using similar solution. I only add the system32 dir to the path if it's not already in the path. 99.9% of the time, it should already be on the path. The main time it's not on the path is during testing when the env gets clobbered.. I have fixed this (preserved system32 env when popping env).. not using this as often as before so I simply removed. Agreed.. Yes. This necessitated a new constructor (and test to cover the constructor).. Shared config has been tested already so no need to add another profile.. Shared config has been tested already so no need to add another profile.. Instead of using a profile and shared config file, this can be tested more directly:\n\ngo\ncreds := processcreds.NewCredentials(\n        fmt.Sprintf(\n            \"%s %s\",\n            getOSCat(),\n            strings.Join(\n                []string{\"testdata\", \"longsessiontoken.json\"},\n                string(os.PathSeparator))))\n    _, err := creds.Get()\n    if err != nil {\n        t.Errorf(\"expected %v, got %v\", \"no error\", err)\n    }\nThis way you can get rid of the two changes to the shared config files.. ",
    "zachahuy": "@cboss24 can you merge the PR #2217 so we can use process_credentials?. ",
    "jdlehman": "That works perfectly, thanks for getting this done so quickly!. ",
    "patrobinson": "```\n2018/03/15 15:17:22 DEBUG: Request s3/GetObject Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nGET /cloudtrails/AWSLogs/012345678910/CloudTrail/ap-southeast-2/2018/03/14/012345678910_CloudTrail_ap-southeast-2_20180314T2300Z_5QuI3IyY2ZsMQroI.json.gz HTTP/1.1\nHost: envato-logs-012345678910.s3.amazonaws.com\nUser-Agent: aws-sdk-go/1.13.14 (go1.10; darwin; amd64)\nAuthorization: AWS4-HMAC-SHA256 Credential=ASIAEXAMPLEEXAMPL/20180315/us-east-1/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-te, Signature=c36c457ff3fe52e541dd88fe5d54c7c51a2af457251ae792f7390b93810a7706\nX-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\nX-Amz-Date: 20180315T041722Z\nX-Amz-Security-Token: [REDACTED]\nX-Amz-Te: append-md5\nAccept-Encoding: gzip\n\n2018/03/15 15:17:24 DEBUG: Response s3/GetObject Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 200 OK\nAccept-Ranges: bytes\nContent-Type: application/json\nDate: Thu, 15 Mar 2018 04:17:24 GMT\nEtag: \"92bdb06a1179ddbd6dad37a9b1f0a646\"\nLast-Modified: Wed, 14 Mar 2018 23:01:39 GMT\nServer: AmazonS3\nX-Amz-Id-2: DD4PP0WGkr/Lar2NVFjpDZg7FG3Rt54HlFHeUhq/GefsVbpC2hQx6Gt+j+dG6QnnMjrtj5aDnhA=\nX-Amz-Request-Id: AEA8AC6055F5B667\nX-Amz-Server-Side-Encryption: AES256\nX-Amz-Transfer-Encoding: append-md5\n\n2018/03/15 15:17:24\nChecksumValidationError: invalid Content-Length 0 for append-md5 X-Amz-Transfer-Encoding\n```. @jasdel thanks for that. We received this error for all our cloudtrail log files. ",
    "lekki": "@jasdel is it a bug or an expected behaviour? Limit param seems to be ignored on paging.. ",
    "WIZARDISHUNGRY": "@swt2c I'm attempting your workaround and not having that much luck \u2013 are you setting SessionToken to the empty string in your credentials.Value, signing, and then concating \"&X-Amz-Security-Token=whatever\" onto your url? A code example would be helpful.. This works, but isn't especially clean:\n```go\nfunc signedMqttURL() string {\n    sessionToken := os.Getenv(\"AWS_SESSION_TOKEN\")\n    os.Setenv(\"AWS_SESSION_TOKEN\", \"\")\n    mqttURL = \"wss://\" + envConfig.AwsDataPlaneEndpoint + \"/mqtt\"\n    creds := credentials.NewChainCredentials(\n        []credentials.Provider{\n            &credentials.EnvProvider{},\n            &credentials.SharedCredentialsProvider{},\n        })\nsigner = v4.NewSigner(creds)\nreq, err := http.NewRequest(\"GET\", mqttURL, nil)\nif err != nil {\n    log.Error(\"Error while setting up request signing for\", mqttURL, err)\n    return \"\"\n}\n\n// FIXME sources say Duration must be less than a week\n// https://github.com/minio/minio/issues/5162\n_, err = signer.Presign(req, nil, \"iotdata\", envConfig.AwsRegion, util.JwtConfig.Duration, time.Now())\n\nif err != nil {\n    log.Error(\"Error while request signing for\", mqttURL, err)\n    return \"\"\n}\n\nurl := req.URL.String()\n\n// Work around for https://github.com/aws/aws-sdk-go/issues/1842\nif sessionToken != \"\" {\n    v := net_url.Values{}\n    v.Set(\"X-Amz-Security-Token\", sessionToken)\n    url = url + \"&\" + v.Encode()\n\n}\n\nreturn url\n\n}. I've got a work around here https://github.com/aws/aws-sdk-go/issues/1842#issuecomment-439100622. Thank you, would be nice to generate a warning or some other such thing.. ",
    "arthurpaimarnold": "Hi @jasdel,\nMy colleague is able to connect to this SwiftStack S3 interface using S3 Browser, for example. Doesn't that mean that the store itself implements the S3 protocol correctly? Is there anything peculiar about the Go SDK in that it doesn't understand Swift's response and S3 Browser does?. ",
    "tomvachon": "I have also seen issues with the new autoscaling services and mobile @jasdel . https://github.com/rebuy-de/aws-nuke/issues/114\nInstead of typing it out twice, does this work @jasdel ?. @jasdel I have this one handy, not sure I added it\nThe service is \"mobile\"\nInvalidSignatureException: Credential should be scoped to correct service: 'AWSMobileHubService'.\n. Ok, thank you!. ",
    "svenwltr": "Thank you @xibz, this indeed works. I hope changing this does not cause any other side effects.\nI guess it is hard to fix it in general for DeleteObjectInput, but it might be helpful to add a note to the docs, since \"empty directories\" are possible in S3.. ",
    "MrGossett": "ThatAwkwardMomentWhen an SDK released a feature before its announced. CodeBuild branch filtering. As of last night, I saw an announcement for CodePipeline S3 push events, but no announcements yet for CodeBuild branch filtering. Also the API docs for CodeBuild hadn\u2019t been updated yet.\n\nOn Mar 22, 2018, at 6:14 PM, awstools notifications@github.com wrote:\nMerged #1858.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \n",
    "bensont1": "@jasdel That would be strange though, the instance itself has been running for a while now, and even then I'm able to curl the metadata from 169.254.169.254 from ssh into the instance. I've tried restarting my application multiple times, and same issue each time, my application (using the code above) has never been able to perform interactions with AWS resources because it never was able to get the proper EC2 iam role.. @jasdel I did a test with the V2 of the Golang AWS SDK, and using cfg, err := external.LoadDefaultAWSConfig() seems to load EC2 IAM roles perfectly fine. This is with the same EC2 instance that described the previous issue, have not restarted the instance or anything either.. ",
    "brianmoran": "I was having the same issue and found that my aws credential var names were lowercase. Coming from the python sdk this is fine. The go sdk on the otherhand is case-sensitive. Once I changed the aws var names to uppercase it worked fine.. ",
    "TheGUNNER13": "@jasdel both v1 and v2 fails for me. aws cli is able to access the bucket but code is not. below i am pasting both v1 and v2 implementations.\n`func main() {\n     fmt.Println(\"v2\")\nerr := func() error {\n    cfg, err := externalv2.LoadDefaultAWSConfig(&awsv2.Config{\n        LogLevel: awsv2.LogDebugWithHTTPBody,\n    })\n    if err != nil {\n        return err\n    }\n    cfg.Region = endpointsv2.ApSoutheast1RegionID\n\n    svc := s3v2.New(cfg)\n    input := &s3v2.ListObjectsInput{\n        Bucket: awsv2.String(\"some-bucket\"),\n    }\n\n    result, err := svc.ListObjectsRequest(input).Send()\n    if err != nil {\n        return err\n    }\n    fmt.Println(result.Contents)\n    return nil\n}()\nif err != nil {\n    fmt.Println(\"err : \", err.Error())\n}\nfmt.Println(\"end v2\\n\\n\")\n\nfmt.Println(\"v1\")\nerr = func() error {\n    sess := session.New(&aws.Config{\n        LogLevel:                      aws.LogLevel(aws.LogDebugWithHTTPBody),\n        CredentialsChainVerboseErrors: aws.Bool(true),\n        Region:                        aws.String(\"ap-southeast-1\"),\n    })\n    _, err := sess.Config.Credentials.Get()\n    if err != nil {\n        return err\n    }\n    svc := s3.New(sess)\n    input := &s3.ListObjectsInput{\n        Bucket: aws.String(\"bucket-name\"),\n    }\n\n    result, err := svc.ListObjects(input)\n    if err != nil {\n        if aerr, ok := err.(awserr.Error); ok {\n            switch aerr.Code() {\n            case s3.ErrCodeNoSuchBucket:\n                fmt.Println(s3.ErrCodeNoSuchBucket, aerr.Error())\n            default:\n                fmt.Println(aerr.Error())\n            }\n        } else {\n            // Print the error, cast err to awserr.Error to get the Code and\n            // Message from an error.\n            fmt.Println(err.Error())\n        }\n        return err\n    }\n    fmt.Println(result)\n    return nil\n}()\nif err != nil {\n    fmt.Println(\"err : \", err.Error())\n}\nfmt.Println(\"end v1\")\n\n}`\nOUTPUT - \nv1\n`\n2019/03/11 11:11:26 DEBUG: Request ec2metadata/GetMetadata Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nGET /latest/meta-data/iam/security-credentials/ HTTP/1.1\nHost: 169.254.169.254\nUser-Agent: aws-sdk-go/1.17.14 (go1.10.3 gccgo (Ubuntu 8.2.0-1ubuntu2~18.04) 8.2.0; linux; amd64)\nAccept-Encoding: gzip\n\n2019/03/11 11:11:26 DEBUG: Response ec2metadata/GetMetadata Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.0 200 OK\nContent-Length: 11\nAccept-Ranges: bytes\nConnection: close\nContent-Type: text/plain\nDate: Mon, 11 Mar 2019 11:11:26 GMT\nEtag: \"2818113054\"\nLast-Modified: Mon, 11 Mar 2019 10:53:28 GMT\nServer: EC2ws\n\n2019/03/11 11:11:26 social-prod\nerr :  NoCredentialProviders: no valid providers in chain\ncaused by: EnvAccessKeyNotFound: AWS_ACCESS_KEY_ID or AWS_ACCESS_KEY not found in environment\nSharedCredsLoad: failed to load shared credentials file\ncaused by: FailedRead: unable to open file\ncaused by: open /home/ubuntu/.aws/credentials: no such file or directory\nEmptyEC2RoleList: empty EC2 Role list\n`\nv2\nv2\nerr :  EmptyEC2RoleList: empty EC2 Role list\nend v2. Do we need specific IAM policy attached to get EC2RoleList ?. fix raised at https://github.com/aws/aws-sdk-go/pull/2504. I am using an EC2 attached to a IAM role. \nNote that all aws cli commands (s3) run on that EC2 works fine, so that tells me the problem is not with EC2. Also with the above code changes, it surprisingly works for me.\nIf you see the httpDebugLogs, you will see that the role is being fetched from the EC2 metadata, but its not being used. \nWill need to debug more, but I dont feel that there is any problem with how the aws roles are setup for the EC2.. > Potentially the a different instance was used for testing?\nNo, @jasdel, the same machine was being used.. @jasdel Yes I agree, this change doesn't make any difference functionally. I forgot to update here once I noticed that. \nWhen I was debugging, I quickly raised the PR because this change fixed the issue I highlighted in #1861, thinking that it might be a problem with ordering of go return statement execution.\nI will debug more as to why the master code doesn't work for me, however with this change it works.. ",
    "ibrt": "@xinst FYI the example is broken... What happens is that the uploader reads the whole thing twice, haven't dug into why but I think it has to do with determining the number of parts. The first read is very fast as the contents are discarded. The second actually seems to be piped to the upload. \nThe sample reports progress by dividing the number of bytes read by two, so what I'm seeing is that it jumps straight to 50%, and then proceeds to 100% at half the actual \"speed\". I \"fixed\" it by initializing read to -size, starting to report when read passes 0.\n. ",
    "lzbpythoner": "thk,i had fix this problem\u3002\n. ",
    "Green-Beret": "@lzbpythoner do you want to share how did you fixed this issue? Thanks.. ",
    "tahoward": "Would also like to see this implemented. We have a use case with aws-iam-authenticator with K8 and Okta.. Hello @xibz we assume roles in AWS with SAML in order to access EKS clusters as we would like to keep access control bound to our domain. ATM we run an okta client CLI command and export AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN before executing kubectl with a config utilizing aws-iam-authenticator. While this works OK saving okta client commands as native AWS credential profiles is lost. This also requires us to run the okta client command independently of our kubectl commands to ensure the session is still current. Preferably, aws-sdk-go would interpret an AWS credential profile utilizing credential_process in the same manner as botocore.\nIt would also be great to see this implemented just for the sake of consistency when it comes to interpreting profile configurations in ~/.aws/credentials.. ",
    "benjafire": "@xibz \uff0c Got it. Thanks for replying.  :-) . ",
    "kkesley": "after digging the code I found this\nsess := session.Must(session.NewSession(&aws.Config{\n    Region: aws.String(\"ap-southeast-2\"),\n        S3ForcePathStyle: &[]bool{true}[0],\n})). Hmm I thought it's resolved the issue. But I still get a timeout error :(. gee.. my bad. I didn't read about accessing resources in VPC. you need VPC endpoint basically. \nhttps://aws.amazon.com/blogs/aws/new-access-resources-in-a-vpc-from-your-lambda-functions/. ",
    "kmcampott": "Thanks!\n. ",
    "wolfeidau": "@jasdel Thanks for the super quick response, if there is anything I can do to help or test please ping me. \nCheers. I wrote my own rather simplistic implementation of this standard by porting over some of the c implementation, however yours looks a lot more complete. \nThe main thing I noticed interacting with the stream is that an event can return a partial JSON document so you need to use a LINE reader or something to detect \\n for the end of these documents. \nI will pull down your PR and compare how they perform and provide feedback there.. I think this should be msec*1e6 shouldn't it?\nAs a nanosecond is 10\u22129 seconds and this is milliseconds.. ",
    "AkhterAli": "Hi @jasdel Thanks for pushing this to the service team. @jasdel Can we keep this ticket open until it becomes available?. ",
    "rsm10": "Ok, so I think this is just an issue of me misunderstanding interface types. Did some more tests and passing \"nil\" explicitly hits this code path while passing a default constructed pointer does not. . UploadInput.Body is being supplied the body from a get response (GetObjectOutput.Body). \nHere is the pprof output (-alloc_space) after running one of my tests (200,000 30 KB objects):\nShowing top 10 nodes out of 14\n      flat  flat%   sum%        cum   cum%\n  475073MB 98.28% 98.28%   475073MB 98.28%  github.com/aws/aws-sdk-go/service/s3/s3manager.(uploader).init.func1\n 2668.20MB  0.55% 98.83%  2782.74MB  0.58%  main\n    2.50MB 0.00052% 98.83% 475242.16MB 98.31%  uploader\n    1.50MB 0.00031% 98.83% 475239.66MB 98.31%  github.com/aws/aws-sdk-go/service/s3/s3manager.Uploader.UploadWithContext\n    1.50MB 0.00031% 98.83% 475611.50MB 98.39%  replicator\n         0     0% 98.83% 475077.02MB 98.28%  github.com/aws/aws-sdk-go/service/s3/s3manager.(uploader).nextReader\n         0     0% 98.83% 475238.16MB 98.31%  github.com/aws/aws-sdk-go/service/s3/s3manager.(*uploader).upload\n         0     0% 98.83% 475239.66MB 98.31%  github.com/aws/aws-sdk-go/service/s3/s3manager.Uploader.Upload\n         0     0% 98.83%   475613MB 98.39%  main\n         0     0% 98.83%   475613MB 98.39%  main.run.func5\nand without alloc_space:\n(pprof) top1\nShowing top 1 nodes out of 54\n      flat  flat%   sum%        cum   cum%\n     640MB 83.16% 83.16%      640MB 83.16%  github.com/aws/aws-sdk-go/service/s3/s3manager.(*uploader).init.func1\n. ",
    "tosh001": "@xibz I have been trying to debug the issue and it definitely seems to be an SSL issue.\nIf I set DisableSSL to true then it works fine.\nWould you have any ideas as to where I should be looking at next? Otherwise I can just assume its a localised problem specific to my environment.. @jasdel It turned out being my local setup. I was having issues with TLS so once I reconfigured my local setup the issue was resoled. So it wasn't anything to do with this package in the end.\nThanks for the replies.. ",
    "ColinHebert": "FYI this is the boto version of the changes required here:\nhttps://github.com/boto/botocore/pull/1313. ",
    "hoshsadiq": "https://github.com/aws/aws-sdk-go/pull/2201 got merged which adds support for credential_source. @jasdel any updates on this?\n. By the way, @jbergknoff-rival does this support credential_source = Environment?. ",
    "CLBray": "Hey @joestump. CodePipeline team here. This API isn't generally available yet. We'll update you when the SDK has gone live.. Good news @joestump. We launched the CodePipeline webhooks service this week. The API endpoint for PutWebhook is now supported. See #1922 for the model updates.. ",
    "fkerlach": "I see this error consistently . ",
    "lookstar": "Thank you @jasdel. Glad to know that you will work on this.. ",
    "johanneswuerbach": "I just started to use this API and noticed the never ending pagination with any kind of provided input.\nThe current ruby sdk also still has the last page condition changes added in 2015, so I would say those changes are still valid https://github.com/aws/aws-sdk-ruby/blob/4edc8bc84b8994b9db497ecd5ad229348252bb37/gems/aws-sdk-core/lib/aws-sdk-core/pager.rb#L42-L44. Any additional details I can provide @jasdel?\nAn example requiring a workaround can now be seen here https://github.com/virtual-kubelet/virtual-kubelet/blob/b4cb8099681118f8049234f12328f7a73c4b1a50/providers/aws/fargate/cluster.go#L330-L344\n//cc @ofiliz. ",
    "artyom": "@xibz Thank you for looking into it!\nAs a workaround I'm retrieving url-encoded keys with s3.ListObjectsV2Input.EncodingType attribute set to aws.String(s3.EncodingTypeUrl), but it will still be nice to have this properly resolved on the service end.. @xibz I can see reasoning behind this, but still consider this as at least an UX/documentation issue, which is worth resolving by better documentation/examples in SDK.\nInitially I was not aware of EncodingType at all and was quite surprised to receive broken XML from the API endpoint, to debug this I even had to dive into encoding/xml internals.\nI know this is somewhat \"works as expected\" considering that SDK is a 1:1 mapping of API calls, but one may expect SDK to abstract and hide such nuances at least when it can be done without any side effects (i.e. request by default encoded values and decode them upon presenting result). Look at it this way: other languages SDKs have the very same bug repored:\n\naws/aws-sdk-core-ruby#95\naws/aws-sdk-java#333\naws/aws-sdk-php#104\n\nSo this is clearly a stumbling point for many \u2014\u00a0i.e. this 11 year old report. And they're all about API returning broken (violating spec) XML and official SDKs failing on parsing such invalid reply.\nWell, at least we now have the same bug for Go. \ud83d\ude1e\nInterestingly, boto seems to handle this in a transparent manner.. ",
    "alinadonisa": "I connect to the api.pricing.us-east-1.amazonaws.com  by hard coding the region and filter by location attribute:\ngo\nsess := session.Must(session.NewSessionWithOptions(session.Options{\n        Config: aws.Config{Region: aws.String(\"us-east-1\")},\n    }))\nsvc := pricing.New(sess)\ninput := &pricing.GetProductsInput{\n        Filters: []*pricing.Filter{\n            {\n                Field: aws.String(\"location\"),\n                Type:  aws.String(\"TERM_MATCH\"),\n                Value: aws.String(\"EU (Ireland)\"),\n            },\n        },\n        FormatVersion: aws.String(\"aws_v1\"),\n        MaxResults:    aws.Int64(10),\n        ServiceCode:   aws.String(serviceCode),\n    }\n    result, err := svc.GetProducts(input)\n. ",
    "zero-master": "Solved by defining:\n```\nfunc (r UserStatus) MarshalDynamoDBAttributeValue(av *dynamodb.AttributeValue) error {\n    s, ok := _UserStatusValueToName[r]\n    if !ok {\n        return fmt.Errorf(\"invalid UserStatus: %d\", r)\n    }\n    av.S = &s\n    return nil\n}\nfunc (r UserStatus) UnmarshalDynamoDBAttributeValue(av dynamodb.AttributeValue) error {\n    s := av.S\n    v, ok := _UserStatusNameToValue[s]\n    if !ok {\n        return fmt.Errorf(\"invalid UserStatus %q\", s)\n    }\n    r = v\n    return nil\n}\n```. ",
    "akskap": "@xibz Perfect, thanks !\nI tried with the additional filter:\n&ec2.Filter{\n          Name: aws.String(\"owner-id\"),\n          Values:[]*string{aws.String(\"XXXXYYYYYZZZZ\")},\n        },\nand now I see results only for that region. This also adds to my knowledge that there are public snapshots also, which are available for anyone to create a volume based off them.\nThanks again !\n. ",
    "srock": "This seems to be fixed with the 0.17.0 aws provider for terraform: https://github.com/terraform-providers/terraform-provider-aws/blob/0cd78bfffa0bdae226f1311362977566af6fb21e/CHANGELOG.md. ",
    "segersniels": "Nevermind, I figured it out. Re-executing the ListImages with the provided NextToken seems to provide the results I was looking for.. ",
    "alexd765": "thanks, that is great. ",
    "tbroadley": "Thank you! Happy to help.. ",
    "jazzyarchitects": "Hi @xibz , Here is what shows up when logging is enabled:\n```\nHandle /aws/log-to-firehose map[delivery_stream:[api-logs-test-staging]]\n2018/05/23 09:23:28 Read Body  \"{\\\"requestId\\\":\\\"1527047608024-4968a5b6-256e-49a2-94a3-ac1f3824cb0d\\\",\\\"host\\\":\\\"localhost:3000\\\",\\\"env\\\":\\\"development\\\",\\\"method\\\":\\\"GET\\\",\\\"url\\\":\\\"/api/Locations\\\",\\\"status\\\":\\\"200\\\",\\\"responseLength\\\":\\\"2815\\\",\\\"responseTime\\\":\\\"53.236\\\",\\\"timestamp\\\":\\\"2018-05-23T03:53:28.083Z\\\",\\\"headers\\\":{\\\"user-agent\\\":\\\"curl/7.54.0\\\",\\\"accept\\\":\\\"%2A/%2A\\\"}}\"\n2018/05/23 09:23:28 DEBUG: Request firehose/PutRecord Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nPOST / HTTP/1.1\nHost: firehose.ap-northeast-1.amazonaws.com\nUser-Agent: aws-sdk-go/1.13.16 (go1.10.1; darwin; amd64)\nContent-Length: 542\nAuthorization: AWS4-HMAC-SHA256 Credential=XXXXXXXXXXXXXXXX/20180523/ap-northeast-1/firehose/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-target, Signature=21e13e6ec5c4f15005b0a9ba74acb0a01f9f0b0713398957c5dec82a5a998ac0\nContent-Type: application/x-amz-json-1.1\nX-Amz-Date: 20180523T035328Z\nX-Amz-Target: Firehose_20150804.PutRecord\nAccept-Encoding: gzip\n{\"DeliveryStreamName\":\"test-delivery-Stream\",\"Record\":{\"Data\":\"IntcInJlcXVlc3RJZFwiOlwiMTUyNzA0NzYwODAyNC00OTY4YTViNi0yNTZlLTQ5YTItOTRhMy1hYzFmMzgyNGNiMGRcIixcImhvc3RcIjpcImxvY2FsaG9zdDozMDAwXCIsXCJlbnZcIjpcImRldmVsb3BtZW50XCIsXCJtZXRob2RcIjpcIkdFVFwiLFwidXJsXCI6XCIvYXBpL0xvY2F0aW9uc1wiLFwic3RhdHVzXCI6XCIyMDBcIixcInJlc3BvbnNlTGVuZ3RoXCI6XCIyODE1XCIsXCJyZXNwb25zZVRpbWVcIjpcIjUzLjIzNlwiLFwidGltZXN0YW1wXCI6XCIyMDE4LTA1LTIzVDAzOjUzOjI4LjA4M1pcIixcImhlYWRlcnNcIjp7XCJ1c2VyLWFnZW50XCI6XCJjdXJsLzcuNTQuMFwiLFwiYWNjZXB0XCI6XCIlMkEvJTJBXCJ9fSI=\"}}\n2018/05/23 09:23:28 DEBUG: Response firehose/PutRecord Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 200 OK\nContent-Length: 239\nContent-Type: application/x-amz-json-1.1\nDate: Wed, 23 May 2018 03:53:28 GMT\nX-Amz-Id-2: c0aPDdIdgmy3Q1OqqPo9xEnH6TsTzetKXSycJd3+mQx+c0OYR2MqIyx/qRUBqJqYy0hbv/vA2KLUn05VimNIfBprd5dy+MLE\nX-Amzn-Requestid: 492401e0-ed5e-410e-bcbe-ef3a65e1d3c2\n\n2018/05/23 09:23:28 {\"RecordId\":\"0AKLdT40kXnGL1/FCBgMjikx8ai8jQFsCwhJ5XJ3o/R2SglFvUx76HEBTYM+paXiox99Tw3iI513s/rTbYu+koGLV16h1MgP7UXnApgkhvVVN2G0wgeIMcHkddDy/+cfmOZ7R1b7xjDbMNITkYco+zkE6+j28f0JBxidgUvFTd3mQXm2QByE7ZTwjPoe6q8wZQioqh/M0dQxj8tL+1eHssR6WaQuX/zS\"}\n2018/05/23 09:23:28 Sent to firehose {\n  RecordId: \"0AKLdT40kXnGL1/FCBgMjikx8ai8jQFsCwhJ5XJ3o/R2SglFvUx76HEBTYM+paXiox99Tw3iI513s/rTbYu+koGLV16h1MgP7UXnApgkhvVVN2G0wgeIMcHkddDy/+cfmOZ7R1b7xjDbMNITkYco+zkE6+j28f0JBxidgUvFTd3mQXm2QByE7ZTwjPoe6q8wZQioqh/M0dQxj8tL+1eHssR6WaQuX/zS\"\n}\n```\nSo my stream basically sends data to elastic search. It does not use any transformers. Attaching screenshots of the stream config.\n\n\n\nI have removed the account id, AWS secrets and ES domain names in the image.\nP.S. Our NodeJS service also uses the same stream and it works. So I figured it might be something with this SDK. \nI read somewhere that it might be because of existing indexes. So I changed this stream destination index to a new one. So Elastic search does not have this index still. \n. Hey @xibz, Is this something from the SDK or on my codebase? \n. The request sent by the SDK always returns a 200 status code from AWS. But I guess elastic search is not able to process the payload being sent by the SDK due to some compression issue. \nP.S. The above request was successful but the entry was not created in Elastic Search due to the reason shown \n. Hi @xibz \nYour code snippet works for my firehose stream.\nI am using the following code:\n```go\nfunc sendLogToFirehose(firehoseSession *firehose.Firehose, body []byte, deliveryStream string) {\nputRecordInput := &firehose.PutRecordInput{}\nrecord := &firehose.Record{}\n\nb, _ := json.Marshal(string(body))\n\n// base64String := base64.StdEncoding.EncodeToString([]byte(b))\n\nlog.Println(\"Read Body \", string(b))\n\nrecord.SetData([]byte(b))\nvalidateError := record.Validate()\n\nif validateError != nil {\n    log.Panicln(\"Record Validation error \", validateError)\n}\n\nputRecordInput.SetDeliveryStreamName(\"test-delivery-Stream\")\nputRecordInput.SetRecord(record)\n\nres, firerr := firehoseSession.PutRecord(putRecordInput)\n\nif firerr != nil {\n    log.Println(\"Error sending to firehose\")\n    log.Panicln(firerr)\n}\n\n// if false {\nlog.Println(\"Sent to firehose\", res)\n// }\n\n}\n```\nWhere body is body, err := ioutil.ReadAll(r.Body)\nIs it some issue with b, _ := json.Marshal(string(body)) ? . ",
    "Vidhuran": "I've had the same issue as well. Double encoding the json is causing this error with elasticsearch. \nhttps://github.com/elastic/elasticsearch-rails/issues/606#issuecomment-259675889. ",
    "peterabarry": "```go\nfile, header, err := r.FormFile(\"file\")\n    .......\nvalidMimes := map[string]bool{\n    \"image/png\":                                                               true,\n    \"image/jpeg\":                                                              true,\n    \"image/pjpeg\":                                                             true,\n    \"application/pdf\":                                                         true,\n    \"application/msword\":                                                      true,\n    \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\": true,\n}\n\nimageMimes := map[string]bool{\n    \"image/png\":   true,\n    \"image/jpeg\":  true,\n    \"image/pjpeg\": true,\n}\n\ndefer file.Close()\n\nfileMime := header.Header.Get(\"Content-Type\")\ndocumentIsValid := false\n\nif validMimes[fileMime] {\n    documentIsValid = true\n\n    //Check that the image meets minimum resolution requirement\n    if imageMimes[fileMime] {\n\n        resolution, err := Helpers.GetImageResolution(file, fileMime)\n\n........................\n    if documentIsValid {\n    settings := Helpers.GetSettings()\n\n    sess := session.New(&aws.Config{})\n    uploader := s3manager.NewUploader(sess)\n    _, err = uploader.Upload(&s3manager.UploadInput{\n        ACL:                  aws.String(\"private\"),\n        ServerSideEncryption: aws.String(\"AES256\"),\n        Bucket:               aws.String(settings[\"s3Bucket\"].(string)),\n        Key:                  aws.String(vars[\"DocumentId\"]),\n        ContentType:          aws.String(mimetype),\n        Body:                 file,\n    })\n\n```\nYes MIME type likely to be correct and works on that old commit hash.. Also tested it with this borrowed code prior to posting: \n```go\nfunc Sample(w http.ResponseWriter, r *http.Request) (string, error) {\n    if err := r.ParseMultipartForm(5 * MB); err != nil {\n        return \"\", err\n    }\n// Limit upload size\nr.Body = http.MaxBytesReader(w, r.Body, 100*MB) // 5 Mb\n\n//\nfile, multipartFileHeader, err := r.FormFile(\"file\")\n\n// Create a buffer to store the header of the file in\nfileHeader := make([]byte, 512)\n\n// Copy the headers into the FileHeader buffer\nif _, err := file.Read(fileHeader); err != nil {\n    return \"\", err\n}\n\n// set position back to start.\nif _, err := file.Seek(0, 0); err != nil {\n    return \"\", err\n}\nmimetype := http.DetectContentType(fileHeader)\nlog.Printf(\"Name: %#v\\n\", multipartFileHeader.Filename)\nlog.Printf(\"Size: %#v\\n\", file.(Sizer).Size())\nlog.Printf(\"MIME: %#v\\n\", mimetype)\nreturn mimetype, err\n\n}\n```. I have moved on from the issue as the roll-back fixed our immediate problem. I do apologise but I am just too busy to try to create a clean isolated repro given the fact I had a workaround. I do feel bad but genuinely cannot spare the time at the moment. Probably we should keep the ticket open for a while and assume that if it is genuinely an issue you will see repeat occurrences? If no on one reports in a few months then just close it? \nReviewing your code it looks similar except you do not reuse a file from a form, and you do not specify the ACL, and also do not defer close the file not that I see any reason for these influencing the behaviour ( my go experience is only basic - few months).. ",
    "kks32": "Hi @jasdel I had a similar issue, when the uploader was uploading files of 0Bytes to S3 using the latest version of the SDK. However, when I used this 15a0ee4 version of SDK it was able to upload the files properly. \nI noticed that there was an unused file.Read(buffer) line in the code, which was never removed, which was causing this 0Byte upload issue using bc3f534 SDK version. I can confirm using file.Seek(0,0), to seek to the beginning of the file fixed the uploader issue and upload files of the right size. file.Seek(0,0) was not needed in 15a0ee4. I just wanted to confirm that the issue was because of incorrect reuse of a file, which works in the old version 15a0ee4 (this should have been a bug in that version, which is now fixed). Hope this helps!\n. Hi @jasdel Thanks. It was an unexpected error that was occurring. Here is a code snippet. Some lines have been removed for brevity.\nfile, err := os.Open(filename)\n    buffer := make([]byte, size)\n    fileType := http.DetectContentType(buffer)\n        file.Read(buffer) // This line was the problem. \n        // Upload `file` to S3 using S3 manager\nIn v1.12.77 it uploaded the file of the right size, in the latest version it resulted in 0 Bytes. I don't need file.Read(buffer) , so I have since removed that line. However to test if it is indeed the seek that was causing the  issue of 0 byes on the latest version, I added file.Seek(0,0) after file.Read(buffer) and then it uploaded the file correctly with the right size and contents. Thanks for fixing the change log.. ",
    "vitaly-zdanevich": "go get -u all just now updated AWS SDK.\npip3 install awscli --upgrade just now updated awscli\nAfter that - the same result - zero from Go, logs from Python.\nMy full code:\n```\npackage main\nimport (\n    \"github.com/aws/aws-sdk-go/aws/session\"\n    \"github.com/aws/aws-sdk-go/service/cloudwatchlogs\"\n    \"time\"\n)\nvar (\n    sess       = session.Must(session.NewSession())\n    logsClient = cloudwatchlogs.New(sess)\nlogGroupName = \"/aws/lambda/IntelligentSpeaker--podcasts_build\"\nyesterday    = time.Now().AddDate(0, 0, -1).Unix() * 1000\ninput        = cloudwatchlogs.FilterLogEventsInput{\n    LogGroupName: &logGroupName,\n    StartTime:    &yesterday,\n}\n\n)\nfunc main() {\n    println(\"yesterday:\", yesterday)\nresp, err := logsClient.FilterLogEvents(&input)\nif err != nil {\n    panic(err)\n}\nprintln(len(resp.Events))\n\n}\n```\nAgainst data from:\naws logs filter-log-events --log-group-name \"/aws/lambda/IntelligentSpeaker--podcasts_build\" --start-time 1534321945000\nstart-time is copied from Golang println();. Executed locally, result:\nCreating Log Group testGroup1535114966477\nCreating Log Stream testStream1535114966477 in log group testGroup1535114966477\nAdding  10 events to stream testStream1535114966477 in log group testGroup1535114966477\nWaiting a second for logs to populate\nFiltering logs in group testGroup1535114966477\nFound 10 logs in group. ",
    "discordianfish": "@jasdel This isn't a support request. The docs say it should return ErrCodeOverLimit but it doesn't:\nFrom https://godoc.org/github.com/aws/aws-sdk-go/service/sqs#SQS.ReceiveMessage:\n\nReturned Error Codes:\n* ErrCodeOverLimit \"OverLimit\"\nThe action that you requested would violate a limit. For example, ReceiveMessage\nreturns this error if the maximum number of inflight messages is reached.\nAddPermission returns this error if the maximum number of permissions for\nthe queue is reached.\n\nIt's either a bug with the docs or the code and I can't say.. I'm not using this anymore, so going to close this before you send me once again to some forum where nobody responds anyway ;). ",
    "songpengwei": "And the example from https://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-query-string-auth.html has not X-Amz-Content-Sha256 in query, why must aws-sdk-go add it when generate presigned url?. Thank you for fixing this!  Cause we are waiting for using it, and could I ask about when it will be merged? Thanks!. @jasdel  Thanks for fixing it so quickly!. ",
    "yyl719133368": "I can't found file in ams-qa/IN/autotest/segment_based_data/ClientB_2345678/data/test1/mapping.csv. My mistake,The key doesn't need bucket.\ncopyInput := &s3.CopyObjectInput{\n        Bucket:     aws.String(\"ams-qa\"),\n        CopySource: aws.String(\"ams-qa/IN/100010/segment_based_data/ClientB_2345678/data/test1/mapping.csv\"),\n        Key:        aws.String(\"IN/autotest/segment_based_data/ClientB_2345678/data/test1/mapping.csv\"),\n    }. @diehlaws \nThank you for this.\nActually, If i do not want recursive,Where i can set option?\nThere are so many keys in our platform, it will cause performance issue.. ",
    "kalafut": "I can definitely add some comments to doc.go. I think the change is narrowly enough scoped that it shouldn't cause new problems, and will allow much easier upgrades like we're facing. The key warning will be that data written using Marshal is not going to necessarily be readable by the deprecated functions. One still needs to think through the broader upgrade path, but at least this backwards compatibility should ease the implementation burden somewhat.. Set Err to the base64 decode error? I gave some thought to that but thought it might be confusing. This change really supports the happy path where everything lines up to support a successful upgrade. If anything isn't right, return the same error that would have been shown before if misc string data was trying to be marshalled into []byte.\nI'm fine changing this as suggested, but that was my rationale behind the current design.. Using UnmarshalError (vs. UnmarshalTypeError) works well. This output looks good:\n--- FAIL: TestUnmarshalConvertToData (0.00s)\n    shared_test.go:385: case 1, expect no error, got UnmarshalError: cannot unmarshal \"string\" into []uint8.\n        caused by: illegal base64 data at input byte 4. ",
    "davidkretch": "Whoops. Sorry about that. I've removed the AddHostExecEnvUserAgentHander change.. Cool, thanks!. ",
    "dianeo": "Hi, Thanks for letting us know! I'm the doc writer for API Gateway. I've pushed a fix, which should appear online around the end of this week.. ",
    "kundankumarjha": "I am using ElasticFileSystem, and I am using postman to get Tags.\nI have 5 tag defined on my filesystem, 1st time I get 3 tags and got nextMarker. In 2nd call I provided Marker which I got nextMarker of 1st call.\nIt everytime giving same result. Thanks alot, I understood my problem, it should call in same session. When I expect this will fix?. I saw fixes are ready, when this will be available as released version?. ",
    "mitchlloyd": "Thanks for the follow up @xibz. I took a shot at using SET rather than ADD this morning. I suppose first I need to figure out what the DynamoDB syntax would be if I use SET rather than ADD to add an item to a StringSet.\nHere is what I tried:\n\nUsing list_append as shown in the Appending Elements to a List documentation where I add an SS type:\n\nSET #n = list_append(#n, :v)\n// Error: Incorrect operand type for operator of function; operator or function; list_append, operand type: SS\n\nUsing list_append with an L type containing an S type:\n\nSET #n = list_append(#n, :v)\n// Error: The provided expression refers to an attribute that does not exist in the item.\nThe issue here seems to be that #n does not exist yet. So perhaps we can add to an existing StringSet but not create a new one with this operation. In contrast, ADD handles this case:\nADD #n :v\nI'm not too confident that SET would work here even if I used 2 operations since the documentation in your second link seems to stress that list_append is only for List types (not Sets).\n\nListAppend() only supports DynamoDB List types\n\nThe behavior I want is what I see documented in Adding Elements To A Set\n\nAssume that the Color attribute does not exist. The following AWS CLI example sets Color to a string set with two elements...\nNow that Color exists, we can add more elements to it...\n\nUsing ADD seems to be working well for me, but I can't find a way to use it with the expression package.. Implementing the Marshaler interface is a nice solution \ud83d\udc4d . ",
    "choonkeat": "getting errorRequestTimeTooSkewed sporadically when performing s3 list / s3 put. @jasdel what do we need?. ",
    "ggaaooppeenngg": "@jasdel Can I set a range like 2MB-50MB, instead of ensuring a specific size?. ",
    "Godley": "Hmm, actually I guess I can use a timeslice and just sort it at the end since I need the delete markers as well. Nevermind!. ",
    "leonsim": "I haven't figured out a use case that could possibly break after this change, not sure why you say its a breaking change. Could you give an example or code that could break after this change?\nI'm aware of the s3iface test example, however, that's not the recommended way of golang (golang recommends consumer side interface) Plus, that kind of mock in the test example cannot be generated by mockery automatically.. Thanks for your use case. Totally understood.. ",
    "ngsw": ":-). \ud83e\udd14.oO(but maybe no one is in trouble\u2026\u2026) \n. @xibz thank you :-) . ",
    "usrenmae": "\nI think the behavior is good, and we should integrate this as the default behavior for v2 if the response is an empty list is received from an API request.\n\nGreat, @jasdel . I have included a set of new Unmarshal*WithEmpties methods to preserve BC. Hope this helps.\n\nI think this PR is also related to #2105.\n\nIndeed this PR is a powerful enough, but it relies on tagging your structures, which is not always possible. For example for the cases when you just unmarshal an unknown structure, and maybe even marshal it back. For such cases tagging wouldn't work.\n. ",
    "paulnivin": "AWS doesn't publish documentation on how long i3.metal instances take to stop. Assuming a waiter option is added, should we be waiting up to 15 minutes or 30 minutes or multiple hours? I think it's reasonable to document what the value should be such that a waiter can support all AWS instance types -- instead devs are left to guess through trial and error.\n@mwhooker, do you have thoughts on this issue?. i3.metal is the only platform where I've seen this issue. @mwhooker I assume has a larger dataset of if other packer users have hit this limitation.. ",
    "bashtoni": "Thanks for this suggestion, which I've implemented and have working.\nHowever, I would argue that having 'empty' variables of every other type as empty except for time.Time, violates the principle of least surprise. The IsZero method exists specifically to discover this scenario, and I believe the SDK should be using this when marshalling in this way.. ",
    "nwalke": "Let me know if I can provide any more info.  It'd be really nice to have this work the same as Boto does since that seems to be the golden child sdk.. Note this works as expected if I set AWS_SDK_LOAD_CONFIG=true in my environment.. This is the tool's fault :)\nThanks for the rubber duck!. ",
    "Dean-Coakley": "Issue was with godep. ",
    "agorman": "That was it, thank you. If anyone else is wondering how to do this here is a code example.\nroleMappingProvider := \"cognito-idp.<region>.amazonaws.com/<user_pool_id>:<client_id>\"\n\nci.SetIdentityPoolRoles(&cognitoidentity.SetIdentityPoolRolesInput{\n    IdentityPoolId: identityPool.IdentityPoolId,\n    Roles:          map[string]*string{},\n    RoleMappings: map[string]*cognitoidentity.RoleMapping{\n        roleMappingProvider: &cognitoidentity.RoleMapping{\n            AmbiguousRoleResolution: aws.String(\"Deny\"),\n            Type: aws.String(\"Token\"),\n        },\n    },\n}).\n",
    "mikaelrandy": "@jasdel thank you for answering.\nIs this new feature development already planned ?. ",
    "HarishAtGitHub": "Yes, but if I am going to create struct it becomes unreliable. But if I have a struct defined by sdk, i can trust and use it. Thats why I raised this question. \nIs there a possibility of aws sdk creating this struct in near future ?. Sure. I will raise it there... Thanks... ",
    "mathankumart": "Disabling using ModifyInstanceAttributeInput worked!\nsvc := ec2.New(sess)\ninput := &ec2.ModifyInstanceAttributeInput{\n    DisableApiTermination: &ec2.AttributeBooleanValue{\n        Value: aws.Bool(true),\n    },\n    InstanceId: aws.String(\"i-037c192da3b9053f7\"),\n}\nresult, err := svc.ModifyInstanceAttribute(input)\nif err != nil {\n    if aerr, ok := err.(awserr.Error); ok {\n        switch aerr.Code() {\n        default:\n            //fmt.Println(\"error\")\n            fmt.Println(aerr.Error())\n        }\n    } else {\n        // Print the error, cast err to awserr.Error to get the Code and\n        // Message from an error.\n        fmt.Println(err.Error())\n    }\n. ",
    "danielvaughan": "Thanks, I tried that as well. I still get the same error:\n`\nCannot use 'session.New()' (type *Session) as type client.ConfigProvider Type does not implement 'client.ConfigProvider' \nneed method: ClientConfig(serviceName string, cfgs ...*aws.Config) Config \nhave method: ClientConfig(serviceName string, cfgs ...*aws.Config) client.Config\n. Thank you @jasdel, that sounds likely. I will take a look. great, yes that is fixed it. ",
    "rubensf": "Hi @diehlaws - thanks! Really appreciate the help.\nI'll check on the forum.. ",
    "tomelliff": "Digging around a bit more I've found that Lambda functions have their LastModified values return as an ISO8601 formatted string (with +0000 instead of Z) but the SDK seems to handle this by setting it as a string and not a timestamp so it doesn't attempt to parse it. Should that be the case here too?. The following patch seems to solve things for me locally:\n```diff\ndiff --git a/vendor/github.com/aws/aws-sdk-go/service/dlm/api.go b/vendor/github.com/aws/aws-sdk-go/service/dlm/api.go\nindex 0cb19f0..fc3ec75 100644\n--- a/vendor/github.com/aws/aws-sdk-go/service/dlm/api.go\n+++ b/vendor/github.com/aws/aws-sdk-go/service/dlm/api.go\n@@ -4,7 +4,6 @@ package dlm\nimport (\n        \"fmt\"\n-       \"time\"\n    \"github.com/aws/aws-sdk-go/aws\"\n    \"github.com/aws/aws-sdk-go/aws/awsutil\"\n\n@@ -847,10 +846,10 @@ type LifecyclePolicy struct {\n        _ struct{} type:\"structure\"\n    // The local date and time when the lifecycle policy was created.\n\n\nDateCreated *time.Time type:\"timestamp\"\nDateCreated *string type:\"timestamp\"// The local date and time when the lifecycle policy was last modified.\n\n\nDateModified *time.Time type:\"timestamp\"\n\nDateModified *string type:\"timestamp\"\n// The description of the lifecycle policy.\nDescription *string type:\"string\"\n@@ -880,13 +879,13 @@ func (s LifecyclePolicy) GoString() string {\n }\n\n\n\n\n// SetDateCreated sets the DateCreated field's value.\n-func (s LifecyclePolicy) SetDateCreated(v time.Time) LifecyclePolicy {\n+func (s LifecyclePolicy) SetDateCreated(v string) LifecyclePolicy {\n        s.DateCreated = &v\n        return s\n }\n// SetDateModified sets the DateModified field's value.\n-func (s LifecyclePolicy) SetDateModified(v time.Time) LifecyclePolicy {\n+func (s LifecyclePolicy) SetDateModified(v string) LifecyclePolicy {\n        s.DateModified = &v\n        return s\n }\n```\nI'm not overly sure how to make this change in the SDK though as I gather most of it is generated from the REST API and I also don't know how you guys add tests for things like this?. @xibz thanks, figured that was the case but wasn't sure if the solution was going to be fixing that the service outputs or fixing the SDK so it doesn't attempt to parse it as a time like with Lambda functions.\nI assume if the service changes what they output then we won't need to change anything in the SDK to support this properly? Do you know if the teams involved here know when this might be fixed?. @xibz thanks for that. I've rebased my PR to Terraform so hopefully DLM support will land in Terraform soon.\nOn a side note (and as mentioned in the above linked PR) the validation for both description and policyDetails.schedule.name are inconsistent across the API docs and the CLI/SDK docs with the API referencing the length limit alone and the CLI/SDK docs referencing the allowed characters/regex alone. Quickly testing that seems to be that both are applied to both fields so I can add that to Terraform's plan time validation but it would be great if the docs for the API and CLI/SDK matched the behaviour of the API and showed what is actually allowed.\nDid you want me to raise a separate issue for the above validation stuff?. ",
    "JGalego": "Any news?. ",
    "ryan-dyer-sp": "@xibz Any update on the fix?. ",
    "delitescere": "Thanks @xibz! Yes I definitely know the annoyance that the names of these configuration variables causes - I have several variations of the same values depending on what the client expects.\nAlthough, I don't know that my suggestion makes it a breaking change. Falling back to AWS_REGION if AWS_DEFAULT_REGION is blank seems like nice backwards compatibility and pushing towards eventual uniformity?. ",
    "snithish": "@jasdel agreed working on adding support for marshalling nil slices to empty too. . @jasdel This PR now handles the following cases:\nMarshaling:\n- [x] Marshal nil map/slice to empty when nilasempty  tag is set\n- [x] Marshal nil map/slice to nil when nilasempty  tag is not set\n- [x] Marshal empty map/slice to empty when nilasempty  tag is set\n- [x] Marshal empty map/slice to nil when nilasempty  tag is not set\nUnMarshaling:\n- [x] Unmarshal nil map/slice to empty when nilasempty  tag is set\n- [x] Unmarshal nil map/slice to nil when nilasempty  tag is not set\n- [x] Unmarshal nil map/slice to empty when nilasempty  tag is set\n- [x] Unmarshal nil map/slice to nil when nilasempty  tag is not set. @jasdel any updates on this?. ",
    "eerenwong": "any updates on this ?. ",
    "daniellockard": "Wll do :) Honestly didn't know filepath.Ext was a thing, haha.. @jasdel this good to merge now?. ",
    "JonDuffy": "Ok thank you.\nNot seeing the issue on ~ aws-cli/1.15 versions.\nThank you for testing. Will follow up with a support case. Will update here in the future if necessary.\n. Appears to be a documentation issue/ mis understanding, catalogueID needs to be the accountID. ",
    "clocklear": "Hey guys, can we get an update on this functionality?  I recently wanted to do enhanced fanout things within my Go codebase and was very disappointed to learn that SubscribeToShard was missing.  It's been 6 months since this issue was raised; has any progress been made on this front?. ",
    "tnine": "@clocklear I agree with your sentiment. We've been using this, https://github.com/vmware/vmware-go-kcl. ",
    "karlgrz": "Total PEBKAC, thank you for the very detailed response @diehlaws! Very much appreciated, we can close this. Thanks!. ",
    "xiaosongdhls": "oh thanks for your instructions,I understand now. I do not have any more! thanks for you instruction.. ",
    "mennis": "Thanks for the tip.  Would really like to find this in a future release.. ",
    "polothy": "Thanks for the reply.  It makes sense not to change it, I just interpreted the doc comments in the code to mean that the file should be fully read.. ",
    "wking": "\nJMESPath is a client only utility.\n\nIs that how aws-cli works to?  I had the impression that it was passing the JMESPath up to the server for server-side filtering.. ",
    "beiriannydd": "Ah #445 . How would you feel about implementing an aws.Canon(s string) function to be used when generating Metadata in order to generate it in the same format as would be returned by the methods?  That way the programmer can use the names they're familiar with and leave it to the library to ensure that the metadata is comparable?  Basically it would return textproto.CanonicalMIMEHeaderKey(s).  The important piece would be using this function everywhere metadata keys are used in examples and documentation.. ",
    "rogchap": "Tracked this down to the json.Marshall of the policy:\nhttps://github.com/aws/aws-sdk-go/blob/master/service/cloudfront/sign/policy.go#L172\nThe Resource gets marshalled to json which turns & into \\u0026 and therefore the signature will not match the requested URL. Seems the Go json marshaller is the root cause: https://golang.org/src/encoding/json/encode.go?s=6528:6546#L157. @jasdel This should be good to go... the build failed on a go lint error for go1.8... I think if we re-run the build it should pass.. ",
    "dovreshef": "@diehlaws Thanks. I've opened a case with AWS support. Will update here once I get an answer.. @diehlaws It will be helpful to support this for STS, at least. Since that is the mechanism aws-iam-authenticator uses to check the users. Currently you have to renew a log in to an EKS cluster every 15 minutes because of this.. ",
    "nckturner": "@diehlaws @dovreshef Is it an issue with the services?  Where is it documented that x-amz-expires is a header shared by all AWS services?  From what I have found, it is only used by S3.  \nI think to start, it should be documented that this header currently only applies to S3, at least in the Request.Presign() and Request.PresignRequest() docs.. @dovreshef @diehlaws FWIW I spoke to some folks from AWS Auth and the only service they know of using the header is S3 (interesting that you found a code sample using IAM).  They suggested that the 15 minute expiration for STS presigned URLs would not be changing.  . ",
    "warmchang": "/hold please\nThere are some other spelling errors, I will fix and commit later.. Finished, please check it, thanks.\n$ git diff origin/master..origin/successfully --stat\n service/autoscaling/api.go | 108 ++++-----\n service/ec2/api.go         | 548 ++++++++++++++++++++++-----------------------\n service/ecr/api.go         |  44 ++--\n service/elb/api.go         |  58 ++---\n service/elbv2/api.go       |  68 +++---\n service/kms/api.go         |  70 +++---\n service/sts/api.go         |  14 +-\n 7 files changed, 455 insertions(+), 455 deletions(-)\n. /hold cancel. /lgtm. ",
    "svett": "I fixed the issue by cleaning the cache with the following command:\n```go\ngo clean -modcache\n````. ",
    "turtleDev": "I've run into this same issue with aws-sdk-go v1.16.7. Is this a bug with vgo?. @jasdel I'm using go version 1.11.2. ",
    "imabilal": "Hi Alex, \nThank you for responding. Maxretires was indeed what I was missing! Thanks for your help. ",
    "sectorsize512": "@jasdel thanks for the reply, makes sense. Do you accept pull requests? If yes, then I maybe could work on this and submit a patch\nTo clarify why I need this (maybe you'll have a suggestion for a workaround). With 32KB read from the socket, only 32KB are written to the local file in a single pwrite() syscall. The file system I use does not like small 32KB I/O. Buffering 1MB pieces in memory seems to be non-trivial as writes are positioned (WriteAt) and can go to different intermingled offsets 'cause of concurrent download.. ",
    "chaitanya11": "Thanks for writing back @diehlaws . Seems like a work around !\nSo, there is no way to add inline policies while creating role from aws-sdk-go ?. ",
    "saravanan30erd": "Thanks.. @diehlaws Submitted PR #2188 to update working code example.. ",
    "vkrishs": "@jasdel Thanks for the quick response. I will update our code to point to v1.15.47 and update you back with RequestId and HTTP status code. However since this is intermittent and cannot be reproduced, I prefer keeping this issue open, so that I can comment back with more info to investigate further. . Also noticed in newsfeed that there was elevated latencies and API error rates this morning around 2AM PDT in AWS Secrets Manager. And we faced issue around same time. Are these related? . @diehlaws We still didn't get a chance to update the SDK to v1.15.47. However we didn't face this issue again with older version. We will try to change it soon and near future. If needed we will open a new issue. Thanks. . ",
    "tixxdz": "Hello @jasdel @diehlaws \nCan you please reopen the issue and help us investigate this ? thank you.\nFor us it happens from time to time, on Cognito.\n\n\nWhere: when we do the GetUser() API Call https://docs.aws.amazon.com/sdk-for-go/api/service/cognitoidentityprovider/#CognitoIdentityProvider.GetUser\n\n\nRegion: ohio\n\n\naws sdk go version:  output of command strings on the package archive aws.a\nSDKName\nSDKVersion\n1.13.38 \n<autogenerated>\n\n\nI will try to update asap, to newer versions.\n\nInformation logs Time 2018-12-27  Time (UTC +00:00) from cloudwatch logs (cut logs):\n19:53:58\nSTART RequestId: 264c0a79-0a11-11e9-a070-01d558c5e86e Version: $LATEST \n19:53:58\nCognito API: Error: Cognito Get User failed: SerializationError: failed decoding JSON RPC error response\n19:53:58\ncaused by: invalid character '<' looking for beginning of value\n19:53:58\nCognito API: Error: getUser(): 403: Forbidden\n19:53:58\n2018/12/27 19:53:58 io.opendevices.api.proxy.go:229: Cognito failed to resolve User: Forbidden\n19:53:58\nEND RequestId: 264c0a79-0a11-11e9-a070-01d558c5e86e\n19:53:58\nREPORT RequestId: 264c0a79-0a11-11e9-a070-01d558c5e86e Duration: 71.16 ms Billed Duration: 100 ms Memory Size: 128 MB Max Memory Used: 42 MB\n\nThe getUser() call is from our library that wraps up the Cognito call\ncc @ubugnu. Addition\n@jasdel @diehlaws the 403 Forbidden is our code from our internal getUser().\nThis error happened once for me when requests was from Germany, but happens a lot more (can't give a unit but more) when requests come from some of our developers routed from Algeria DZ, CLOUDFRONT-VIEWER-COUNTRY:DZ.\nIf more headers or IDs are required I can provide them, that request dumped all the request header. just pass me an email.\nThank you. ",
    "jniesen": "I am getting this issue when trying to read from DynamoDB with the following code:\nresult, err := svc.GetItem(&dynamodb.GetItemInput{                                                                                                                                                                                                                                                                          \n    Key: map[string]*dynamodb.AttributeValue{                                                                                                                                                                                                                                                                                 \n      \"KnownAddress\": {                                                                                                                                                                                                                                                                                                       \n        S: aws.String(k),                                                                                                                                                                                                                                                                                                     \n      },                                                                                                                                                                                                                                                                                                                      \n    },                                                                                                                                                                                                                                                                                                                        \n    ProjectionExpression: aws.String(\"PrimaryAddress\"),                                                                                                                                                                                                                                                                       \n    TableName:            table,                                                                                                                                                                                                                                                                                              \n  })                                                                                                                                                                                                                                                                                                                          \n  if err != nil {                                                                                                                                                                                                                                                                                                             \n    return \"\", err                                                                                                                                                                                                                                                                                                            \n  }\nThe error:\nCould not set users AD Mail, KnownAddress was 'xxxxxxxxxx', SerializationError: failed decoding JSON RPC error response\nstatus code: 400, request id: 1AA3F803935A8355\ncaused by: invalid character '<' looking for beginning of value\nI am running this code in two different way. From my command line and from a Lambda running in a Private VPC using a Private DynamoDB Endpoint. When running from my command line, I do not get the error. When running in the Lambda I do. This probably doesn't mean that the decoding issue is only happening in the Lambda, it probably just means that an underlying error is only happening in the Lambda which exposes the decoding issue.\nAWS SDK Version v1.16.20\nGo Version 1.11. This issue was opened because a similar issue that I commented on is closed. That was issue #2184 . Thank you for that link @diehlaws. That will definitely come in handy.\nI got it working and I know why the error was occuring, but I'm not sure how to interpret it along with your explanation about the service error responses.\nThe error was happening because I was deploying the Lambda into a private subnet of a VPC. The VPC did not have a VPC Endpoint configured for DynamoDB so I believe that my Lambda couldn't actually talk to the DynamoDB service. Once I put the VPC Endpoint in place, the error stopped occurring.\nSo, if the error was occurring because no connection could be made to the DynamoDB service, where did the non-JSON error response come from?\n. ",
    "Sundin": "I'm still seeing this issue occasionally when calling GetSecretValue using AWS SDK v1.16.26. Works most of the times though!\nSerializationError: failed decoding JSON RPC error response status code: 400, request id: caused by: invalid character '<' looking for beginning of value. ",
    "Botono": "Seeing this today on aws-go-sdk v1.18.0. We have Lambdas in 4 regions that run once a minute. I'm seeing about 7 errors an hour, all on the same GetSecretValue call. The secret in question is an RSA certificate.. A bit more context: I enabled debug logging on the client and got this message:\nDEBUG: Validate Response secretsmanager/GetSecretValue failed, not retrying, error SerializationError: failed decoding JSON RPC error response\nRetry is bypassed somewhere.. ",
    "ijt": "I found something that seems like it would work: https://github.com/go-stomp/stomp. Closing this.. I tried this out. From what you said above, I gathered that I didn't need to confirm. So I wrote the following and still didn't see the messages from the SNS topic showing up in the SNS queue.\n```\npackage main\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"math/rand\"\n    \"net/http\"\n    \"time\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/sns\"\n\"github.com/aws/aws-sdk-go/service/sqs\"\n\n)\nfunc main() {\n    if err := makeTopicAndQueue(); err != nil {\n        log.Fatalf(\"aws-sns-sqs: %v\", err)\n    }\n}\nfunc makeTopicAndQueue() error {\n    sess, err := session.NewSession(&aws.Config{\n        HTTPClient:  &http.Client{},\n        Region:      aws.String(\"us-east-2\"),\n        Credentials: nil,\n        MaxRetries:  aws.Int(0),\n    })\nlog.Printf(\"Creating an SNS topic.\")\nsnsClient := sns.New(sess, &aws.Config{})\ntopicName := \"test-topic\"\nout, err := snsClient.CreateTopic(&sns.CreateTopicInput{Name: aws.String(topicName)})\nif err != nil {\n    return fmt.Errorf(`creating topic \"%s\": %v`, topicName, err)\n}\ndefer snsClient.DeleteTopic(&sns.DeleteTopicInput{TopicArn: out.TopicArn})\n\nlog.Printf(\"Creating an SQS queue.\")\nsqsClient := sqs.New(sess, &aws.Config{})\nsubName := \"test-subscription\"\nout2, err := sqsClient.CreateQueue(&sqs.CreateQueueInput{QueueName: aws.String(subName)})\nif err != nil {\n    return fmt.Errorf(`creating subscription queue \"%s\": %v`, subName, err)\n}\n\nlog.Printf(\"Getting queue ARN.\")\nout3, err := sqsClient.GetQueueAttributes(&sqs.GetQueueAttributesInput{\n    QueueUrl:       out2.QueueUrl,\n    AttributeNames: []*string{aws.String(\"QueueArn\")},\n})\nif err != nil {\n    return fmt.Errorf(\"getting queue ARN for %s: %v\", *out2.QueueUrl, err)\n}\nqARN := out3.Attributes[\"QueueArn\"]\n\nlog.Printf(\"Subscribing the queue to the topic.\")\n_, err = snsClient.Subscribe(&sns.SubscribeInput{\n    TopicArn: out.TopicArn,\n    Endpoint: qARN,\n    Protocol: aws.String(\"sqs\"),\n})\nif err != nil {\n    return fmt.Errorf(\"subscribing: %v\", err)\n}\n\nlog.Printf(\"Sending a message to the topic.\")\nmsg := fmt.Sprintf(\"sns/sqs test. random number: %d\", rand.Int())\nctx := context.Background()\n_, err = snsClient.PublishWithContext(ctx, &sns.PublishInput{\n    Message:  aws.String(msg),\n    TopicArn: out.TopicArn,\n})\nif err != nil {\n    return fmt.Errorf(\"publishing: %v\", err)\n}\n\nlog.Printf(\"Getting the queue URL\")\nreq, resp := sqsClient.GetQueueUrlRequest(&sqs.GetQueueUrlInput{QueueName: aws.String(subName)})\nerr = req.Send()\nif err != nil {\n    return fmt.Errorf(\"getting queue URL: %v\", err)\n}\nqURL := *resp.QueueUrl\n\nfor _ = range time.Tick(time.Second) {\n    log.Printf(\"Receiving the message from the queue.\")\n    max := int64(10)\n    output, err := sqsClient.ReceiveMessageWithContext(ctx, &sqs.ReceiveMessageInput{\n        QueueUrl:            aws.String(qURL),\n        MaxNumberOfMessages: aws.Int64(max),\n    })\n    if err != nil {\n        return fmt.Errorf(\"receiving message: %v\", err)\n    }\n    for _, m := range output.Messages {\n        log.Printf(\"Got message %s.\", *m.Body)\n        log.Printf(\"Stopping.\")\n        return nil\n    }\n}\n\nreturn nil\n\n}\n```\nHere's the result:\n[ ~/src/aws-sqs-issue ] go run main.go\n2019/03/04 18:54:08 Creating an SNS topic.\n2019/03/04 18:54:09 Creating an SQS queue.\n2019/03/04 18:54:10 Getting queue ARN.\n2019/03/04 18:54:10 Subscribing the queue to the topic.\n2019/03/04 18:54:10 Sending a message to the topic.\n2019/03/04 18:54:10 Getting the queue URL\n2019/03/04 18:54:12 Receiving the message from the queue.\n2019/03/04 18:54:13 Receiving the message from the queue.\n2019/03/04 18:54:14 Receiving the message from the queue.\n2019/03/04 18:54:15 Receiving the message from the queue.\n2019/03/04 18:54:16 Receiving the message from the queue.\n2019/03/04 18:54:17 Receiving the message from the queue.\n2019/03/04 18:54:18 Receiving the message from the queue.\n2019/03/04 18:54:19 Receiving the message from the queue.\n2019/03/04 18:54:20 Receiving the message from the queue.\n2019/03/04 18:54:21 Receiving the message from the queue.\n2019/03/04 18:54:22 Receiving the message from the queue.\n^Csignal: interrupt\nIs there a bug somewhere or am I doing something wrong?. The response from the subscribe call gives a subscription ARN (arn:aws:sns:us-east-2:462380225722:test-topic:839a49af-fbf8-4ca9-b575-5fb81d59a07c) implying that it doesn't need confirmation.\nHowever, when I try to edit the subscription attributes in the console (SNS dashboard | Topics | Other subscription options | Edit subscription attributes), it shows something wrong. \n\n. Okay, this works. Thank you so much for your help. I'm posting the final working example here for future reference in case anyone else runs into this:\n```go\npackage main\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"math/rand\"\n    \"net/http\"\n    \"time\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/sns\"\n\"github.com/aws/aws-sdk-go/service/sqs\"\n\n)\nfunc main() {\n    if err := makeTopicAndQueue(); err != nil {\n        log.Fatalf(\"aws-sns-sqs: %v\", err)\n    }\n}\nfunc makeTopicAndQueue() error {\n    sess, err := session.NewSession(&aws.Config{\n        HTTPClient:  &http.Client{},\n        Region:      aws.String(\"us-east-2\"),\n        Credentials: nil,\n        MaxRetries:  aws.Int(0),\n    })\nlog.Printf(\"Creating an SNS topic\")\nsnsClient := sns.New(sess, &aws.Config{})\ntopicName := \"test-topic\"\nout, err := snsClient.CreateTopic(&sns.CreateTopicInput{Name: aws.String(topicName)})\nif err != nil {\n    return fmt.Errorf(`creating topic \"%s\": %v`, topicName, err)\n}\ndefer snsClient.DeleteTopic(&sns.DeleteTopicInput{TopicArn: out.TopicArn})\n\nlog.Printf(\"Creating an SQS queue\")\nsqsClient := sqs.New(sess, &aws.Config{})\nsubName := \"test-subscription\"\nout2, err := sqsClient.CreateQueue(&sqs.CreateQueueInput{QueueName: aws.String(subName)})\nif err != nil {\n    return fmt.Errorf(`creating subscription queue \"%s\": %v`, subName, err)\n}\n\nlog.Printf(\"Getting queue ARN\")\nout3, err := sqsClient.GetQueueAttributes(&sqs.GetQueueAttributesInput{\n    QueueUrl:       out2.QueueUrl,\n    AttributeNames: []*string{aws.String(\"QueueArn\")},\n})\nif err != nil {\n    return fmt.Errorf(\"getting queue ARN for %s: %v\", *out2.QueueUrl, err)\n}\nqARN := out3.Attributes[\"QueueArn\"]\n\nlog.Printf(\"Adding queue access policy to queue.\")\nqueuePolicy := `{\n\n\"Version\": \"2012-10-17\",\n\"Id\": \"AllowQueue\",\n\"Statement\": [\n{\n\"Sid\": \"MySQSPolicy001\",\n\"Effect\": \"Allow\",\n\"Principal\": {\n\"AWS\": \"\"\n},\n\"Action\": \"sqs:SendMessage\",\n\"Resource\": \"+ *qARN +\",\n\"Condition\": {\n\"ArnEquals\": {\n\"aws:SourceArn\": \"+ *out.TopicArn +\"\n}\n}\n}\n]\n}`\n    _, err = sqsClient.SetQueueAttributes(&sqs.SetQueueAttributesInput{\n        Attributes: map[string]string{\n            \"Policy\": &queuePolicy,\n        },\n        QueueUrl: out2.QueueUrl,\n    })\n    if err != nil {\n        return fmt.Errorf(\"setting policy: %v\", err)\n    }\nlog.Printf(\"Subscribing the queue to the topic\")\nsubResp, err := snsClient.Subscribe(&sns.SubscribeInput{\n    Attributes: map[string]*string{\"RawMessageDelivery\": aws.String(\"true\")},\n    TopicArn:   out.TopicArn,\n    Endpoint:   qARN,\n    Protocol:   aws.String(\"sqs\"),\n})\nif err != nil {\n    return fmt.Errorf(\"subscribing: %v\", err)\n}\nlog.Printf(\"Subscription arn: '%s'\", *subResp.SubscriptionArn)\n\nlog.Printf(\"Sending a message to the topic\")\nmsg := fmt.Sprintf(\"sns/sqs test. random number: %d\", rand.Int())\nctx := context.Background()\n_, err = snsClient.PublishWithContext(ctx, &sns.PublishInput{\n    Message:  aws.String(msg),\n    TopicArn: out.TopicArn,\n})\nif err != nil {\n    return fmt.Errorf(\"publishing: %v\", err)\n}\n\nlog.Printf(\"Getting the queue URL\")\nreq, resp := sqsClient.GetQueueUrlRequest(&sqs.GetQueueUrlInput{QueueName: aws.String(subName)})\nerr = req.Send()\nif err != nil {\n    return fmt.Errorf(\"getting queue URL: %v\", err)\n}\nlog.Printf(\"Queue URL: %s\", *resp.QueueUrl)\n\nfor _ = range time.Tick(time.Second) {\n    log.Printf(\"Receiving the message from the queue\")\n    max := int64(10)\n    output, err := sqsClient.ReceiveMessageWithContext(ctx, &sqs.ReceiveMessageInput{\n        QueueUrl:            resp.QueueUrl,\n        MaxNumberOfMessages: aws.Int64(max),\n    })\n    if err != nil {\n        return fmt.Errorf(\"receiving message: %v\", err)\n    }\n    for _, m := range output.Messages {\n        log.Printf(\"Got message %s\", *m.Body)\n        log.Printf(\"Stopping\")\n        return nil\n    }\n}\n\nreturn nil\n\n}\n```. ",
    "bradleyjames": "Make sure your clock is in sync and is returning the appropriate time.  For whatever reason my machine's clock gets skewed and I run into this issue as well.  But it's a problem on my side and isn't specific to any one AWS service.. ",
    "llicety": "\nMake sure your clock is in sync and is returning the appropriate time. For whatever reason my machine's clock gets skewed and I run into this issue as well. But it's a problem on my side and isn't specific to any one AWS service.\n\nThanks for your reply bradleyjames. \nI am sure the server clock is in sync with NTP, but not sure it  return the appropriate time at that time. Now it is all right and the project works well. The exception can't be reproduced with the working environment. While if I bring the server time forward more than 5 min, the same exception \"InvalidSignatureException\" occurs.  Thanks a lot!!\n. ",
    "btai24": "@diehlaws thanks didn't realize that. Possibly the wrong person to ask, but by any chance do you know whether terraform has a resource for AWS Resource Groups? I can't seem to find it in the documentation.\nhttps://www.terraform.io/docs/providers/aws/. Thanks @bflad, I've upvoted.. ",
    "emmanuel": "Great perf. But the behavior changed without warning: https://github.com/aws/aws-sdk-go/issues/2281\n(To be clear, perhaps more than one space between LHS tokens and equals sign was never intended to be supported. Whatever the intent; it used to work, and now it doesn't.). ",
    "buroa": "I receive errors like this [using some iptables redirect magic], since the endpoints.json is built wrong:\nmonitoring.us-iso-east-1.amazonaws.com/: x509: certificate is valid for monitoring.us-iso-east-1.c2s.ic.gov, not monitoring.us-iso-east-1.amazonaws.com.\nI have tried setting both the AWS_CA_BUNDLE environmental variable and the CLOUDWATCH_ENDPOINT, like it says in the guide also, but still not working.. Looked more into this. The file that would require commits is https://github.com/aws/aws-sdk-go/blob/master/aws/endpoints/defaults.go.\nI have the file updated with us-iso-east-1 and us-isob-east-1 on my end, but if I submit a merge request, will it be accepted?. @diehlaws Thanks! I'll look into the custom resolver option. :). > Thanks for taking the time to create this PR @buroa. The endpoint's default.go file is code generated from the contents in the ./models/endpoints/endpoints.json file. The contents of this file are provided by the AWS service teams.\n\nI can forward this request upstream to the Amazon SQS team to see their plans on updating this definition. I'm not positive if the SDKs can be updated though as the regional endpoints may have different behavior from the \"global\" endpoint.\n\nYes, that would be fantastic! Please keep me updated -- this will help out a lot of our projects (and probably others).. Sure can. Give me a few.. go: #2457 \nbotocore: https://github.com/boto/botocore/issues/1681\nI'm sure this affects more SDKs ... but haven't tested all of them or use them daily.. ",
    "denismakogon": "@diehlaws is there any chance that AWS will consider this type of an API change as \"something that people been asking for\" or \"nice to have\"?\n. ",
    "sumanthreddym": "+1. ",
    "mourya92": "Thanks diehlas \ud83d\udc4d\ud83c\udffb. ",
    "lizilong007": "Thanks Re @diehlaws The problem I encountered before was to access a non-existing index and did not return a ResourceNotFoundException error. Now that the interface is normal, your server may have fixed this problem recently. ^_^. ",
    "iselind": "Thanks a lot for looking into my issue, much appreciated!\nI'll take a look at the other issues you mention and get appropriately involved in those.\nYour suggestion for my own multipart uploader seem strange to me. In order for an io.Seeker to work, you need the data in a seekable storage, i.e. the data needs to be in RAM. The requirement for io.Seeker is, as i understand it, to a big extent the reason why the s3manager requires to much memory. Perhaps i misunderstood your suggestion?. Firstly; how can you in any way or respect be confident that an io.Reader is in fact a file? In our case we get our data from a socket.\nSecondly; even if the io.Reader is a file, that's still not page based until the file has been either memory mapped or read into RAM. Either way you end up in RAM again.\nPerhaps i'm missing something vital here?. Thanks @jasdel I'll look into those options and get back to you. I think the presigned URL solution looks the most promising.\nWould you mind keeping this issue open until i've tested those options? It will probably not take more than a week for me to test this properly.... It seems we hit a snag, perhaps you can see what we're missing.\nWe're able to presign the request and make the request. We get \nxml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n    <Error>\n    <Code>NotImplemented</Code>\n    <Message>A header you provided implies functionality that is not implemented</Message>\n    <Header>Transfer-Encoding</Header>\n    <RequestId>4FC2B7F1A122D0C6</RequestId>\n    <HostId>fKG9On8k+o7mGCpu/FlslvghnsTx9deoMCaFTs9nIDNmUucvX3esKq7Ih53649+M99qK/PnehQ8=</HostId>\n</Error>\nback as a response.\nThe code looks like this now:\n```go\n    svc := getSession()\nfname := fmt.Sprintf(\"%s-%s-%s-%d.core\", meta[\"SerialNumber\"], meta[\"ProdNbr\"], meta[\"Version\"], time.Now().Unix())\n\nputObj, _ := svc.PutObjectRequest(&s3.PutObjectInput{\n    Bucket:        aws.String(coreFileBucket),\n    Key:           aws.String(fname),\n    ContentLength: aws.Int64(0), // We cannot know the length in advance\n})\n\nurl, err := putObj.Presign(15 * time.Minute)\nif err != nil {\n    return \"\", err\n}\n\nreq, err := http.NewRequest(\"PUT\", url, body)\nif err != nil {\n    return \"\", err\n}\nresp, err := http.DefaultClient.Do(req)\n\n```\nI think we've followed the linked example code. I cannot see what we're doing wrong.\nWe get the same issue if we exclude the line setting content-length to 0 in the PutObjectRequest.. @jasdel Thanks a lot for your insights. We set the content length to 0 because we cannot possibly know the length of the body before receiving it all. As we cannot store it temporarily on disk we are left with RAM and we haven't won anything by using Presign(). I assume PresignRequest() has the same limitation, right?\nI found this issue which seem very similar to my issue. They seem to have solved the content length issue. Can the same be done in the GoLang AWS SDK? I cannot interpret this in any other way than there is a way to do chunked transfers to S3. I know that issue is for the javascript version of the AWS SDK but the language can hardly be the culprit here, right?\nBased on the previous discussion it seems to me that this issue is in essence based on the limitations in the AWS REST API and not really in the SDK. Based on this i'd say changing language to something like C, Python, or Java wouldn't change a thing in this regard. So changing the concurrency to 1 in the uploader seems to be our only vay forward then in trying to reduce the memory consumption of the uploader. Would you agree?. @diehlaws @jasdel Any updates on this?. ",
    "challarao": "Hi @diehlaws,\nSorry for delay in replying to this post. We have been able to test that this works correctly. The assumption was due to a red-herring on our part.\nThank you.. @diehlaws Sometimes we receive html instead of xml (I think due to our proxy). The library tries to parse it and says something along the lines of \"expected xml but received html\". In those case I would like to see what is the exact response sent by the proxy.\nIn general I want to always log all the requests without body. Or at least just the response bodys for debugging purposes.\nThe problem withe current option is that if I upload a 50MB file my log file will be 50MB and so on and it's hard to go through the logs like that.\nThis logging is important for us to help our customers in case of any problems.\nThanks. Thanks @diehlaws for the prompt response. This technique is good enough for me. . ",
    "smilemakc": "Hi @diehlaws, I use aws-sdk-go for DigitalOcean space. with S3 CreateBucket worked.\nBelow output with logging levels LogDebugWithSigning LogDebugWithRequestErrors LogDebugWithHTTPBody\n```code\n2018/10/30 18:44:21 DEBUG: Request Signature:\n---[ CANONICAL STRING  ]-----------------------------\nPUT\n/\ncontent-length:153\nhost:cloud-identix-one-connector3.ams3.digitaloceanspaces.com\nx-amz-content-sha256:70cae86320841ea73b0bdc759f99920c7caa405e61af2742575750c6586272c9\nx-amz-date:20181030T154421Z\ncontent-length;host;x-amz-content-sha256;x-amz-date\n70cae86320841ea73b0bdc759f99920c7caa405e61af2742575750c6586272c9\n---[ STRING TO SIGN ]--------------------------------\nAWS4-HMAC-SHA256\n20181030T154421Z\n20181030/us-east-2/s3/aws4_request\n5d141c4c351530185faa4078b5c1e8291013f87891a4dcd751d0f846f1d8b3f7\n-----------------------------------------------------\n2018/10/30 18:44:21 DEBUG: Request s3/CreateBucket Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nPUT / HTTP/1.1\nHost: cloud-identix-one-connector3.ams3.digitaloceanspaces.com\nUser-Agent: aws-sdk-go/1.15.61 (go1.9.2; darwin; amd64)\nContent-Length: 153\nAuthorization: AWS4-HMAC-SHA256 Credential=7KMNJPFUIQF6CQ6JJLXY/20181030/us-east-2/s3/aws4_request, SignedHeaders=content-length;host;x-amz-content-sha256;x-amz-date, Signature=5f6f635e23b01e015811da7efd20ca3231400fe72717db218865d12259dc5754\nX-Amz-Content-Sha256: 70cae86320841ea73b0bdc759f99920c7caa405e61af2742575750c6586272c9\nX-Amz-Date: 20181030T154421Z\nAccept-Encoding: gzip\nus-east-2\n2018/10/30 18:44:23 DEBUG: Response s3/CreateBucket Details:\n---[ RESPONSE ]--------------------------------------\nHTTP/1.1 403 Forbidden\nContent-Length: 191\nContent-Type: text/xml; charset=utf-8\nDate: Tue, 30 Oct 2018 15:44:23 GMT\n\n2018/10/30 18:44:23 <?xml version=\"1.0\" encoding=\"UTF-8\"?>SignatureDoesNotMatchtx0000000000000149948a6-005bd87c57-b8ca18-ams3ab8ca18-ams3a-ams3\n2018/10/30 18:44:23 DEBUG: Validate Response s3/CreateBucket failed, not retrying, error SignatureDoesNotMatch: \n    status code: 403, request id: , host id: \n``. @diehlaws That's just great. You saved me fromDuct Tape Programmer`. Thank you very much for the work done and detailed answer to my question. ",
    "ktravis": "Awesome, thank you for the quick response!. ",
    "jritsema": "Hi. Just a heads up that this is still impacting us. We use https://github.com/turnerlabs/samlkeygen which populates our credentials file with temporary keys.  For example:\n[account1:admin]\naws_access_key_id = ASIAYJ4K2VEOKFT3RABC\naws_secret_access_key = hF1P78htExgQKgbB21O5DFsTCVg54ww+3l93uabc\naws_session_token = FQoGZXIvYXdzENf//////////wEaDM1saxMRcsPJ47GosCKoAlFzBAKLYfddIBpcwq6Xw3OIHZR8ZzSsCEGhweQn/W3AW/g+ksEoBq8wFj8zA8ls7bKrDvLosXvCWXdye118EqP+4YLc9M3YndWcWRIWOZ67fcRHvCiIJeq9C9qqSm3pmEg3UgI/PVzPevVEx1kfthqVdeYTTADkTp6wupfQd1QTYvvcy4HZtTYVqu+CXkEBskIcBoxDMyZxE/tDS96YhzrVIHfCALIp73K/cxklL7G/bS9dSKYKWrxfoDwJcUPAQfCb3Ep9qhHxr4/blLx3JRoHZ9MsF+yU4KbvEomQj0GRR2TB/ErQrNJT9I/HK6c/SbuCusKXTnGrCpVCjKc5PZc5sC8UC/ph9jVvfrS7/6Qz+qKNMXey7DBmNVZ1I1Pn0dLAJuUBt546KLCr8asdf\naws_security_token = FQoGZXIvYXdzENf//////////wEaDM1saxMRcsPJ47GosCKoAlFzBAKLYfddIBpcwq6Xw3OIHZR8ZzSsCEGhweQn/W3AW/g+ksEoBq8wFj8zA8ls7bKrDvLosXvCWXdye118EqP+4YLc9M3YndWcWRIWOZ67fcRHvCiIJeq9C9qqSm3pmEg3UgI/PVzPevVEx1kfthqVdeYTTADkTp6wupfQd1QTYvvcy4HZtTYVqu+CXkEBskIcBoxDMyZxE/tDS96YhzrVIHfCALIp73K/cxklL7G/bS9dSKYKWrxfoDwJcUPAQfCb3Ep9qhHxr4/blLx3JRoHZ9MsF+yU4KbvEomQj0GRR2TB/ErQrNJT9I/HK6c/SbuCusKXTnGrCpVCjKc5PZc5sC8UC/ph9jVvfrS7/6Qz+qKNMXey7DBmNVZ1I1Pn0dLAJuUBt546KLC8asdf\nlast_updated = 2018-11-02T13:48:32Z\nexpiration = 2018-11-02T22:48:32Z. I verified that HEAD looks like it fixes it.  Thanks @xibz @jasdel!. ",
    "bostrt": "nvm... ",
    "emirb": "@jasdel According to Go's release policy, each major Go release is supported until there are two newer major releases. Also, considering Go's backwards compatibility promise, does it still make sense to keep Travis test matrix with ancient versions like 1.6.x and so on?. ",
    "alienth": "Interesting... I can reproduce reliably until I remove all hyphenated profile names. Let me try and break it down further.. Ah, my mistake - I was testing by deleting all of my hyphenated profiles and leaving my non-hyphenated profiles. What was actually happening is one of the hyphenated profiles (one I wasn't using) had a syntax error. Apparently this was ignored under the previous parser, but under the new parser causes all profiles to fail.\nThe syntax error:\n```\n[profile]\naccess_key_id = foo\nSyntax error below: no underscore between secret and key\nsecret key = bar\n```\nSo, I guess the behaviour change here is that a syntax error in an unrelated profile causes all profile parsing to fail. Previously that broken profile was just ignored.\nSorry for the misunderstanding! Welcome to close this ticket. . Sounds good to me! I'll update the original report to clarify.. ",
    "Hendra-Huang": "Hi @jasdel , no strong reason for it. i just chose the version when the .gitignore issue is fixed.. okay. thanks. ",
    "cullenmcdermott": "That was so fast! Thanks!. ",
    "cpramodroy": "Great! That fixed it. Thanks a lot for your help!. @diehlaws Thanks for clearing that up \ud83d\udc4d. ",
    "a4abhishek": "Hey @jasdel , What is it which is keeping us from merging this PR?. Sure @jasdel , thanks.. ",
    "azr": "Gotcha, thanks!\nAha ! That's good to know; will try it next time I have to play with go 1.5 \ud83d\ude42 . Howdy @jasdel, happy new year ! \ud83d\ude42 Hoping this can be merged soon ! Is there something I can do to ease the process ? \nEdit: I rebased the branch after a conflict.. Ah, tests are failing on go 1.11 after https://github.com/golang/go/commit/7f6105f138b3836e9ad85b8da26d44c742bf217b ( ECONNRESET is retryable only during the accept phase ) will try to find a good solution and tests everything.. Okay, current state of code is good to me \ud83d\ude42 tell me what you think. Hello there \ud83d\ude42, so, is there something I can do to make this PR acceptable ? Thanks !. No problemo \ud83d\ude42, fixing this right now !\nEdit: done !. Nice !!! Thanks for doing the change and bearing with me here \ud83d\ude42\nSorry I couldn't reply earlier, I was moving to a new place.. Hey @jasdel, just a heads up, but this is not solved for windows yet apparently. When no credentials are set, it retries forever ( up to max retry ). I will try to come up with a fix soon \ud83d\ude42.. Just when no credentials are available.\nDo you mean on windows or on any ec2 instance in general ?  Because if it is in general then this pr could have introduced some harm as it is betting for every not-retriable error to really be not-retriable and stopping tries. ( But it was already the case I think ). Okay, good, I'll try follow and adapt then \ud83d\ude42 . net.Error and *url.Error both implement temporary here; so the test can be simplified to a Temporary()? check\n. Ah, I didn't think of this ! Thanks for pointing it.\nurl.Error has a Temporary func since go 1 but *net.Error added this in go 1.6.\nSo, I added a case using the Err field in *net.Error that should only happen before go 1.6\n. Is it okay now ?. Ah sorry I thought the ci was broken for some wrong reason; it was me.\nWill try to check if the error returned in the test is the error returned by such a case. I have a hunch it could be another type and then the type assertion should work.\nNow I need to see how to play with an old version of Go.. Okay, this took me longer than I expected but adding realistic tests helped way more; Also sorry about all that force push spam. I tried installing go 1.5 from binary and sources but my Mojave doesn't like it apparently. Travis-ci helped me a bit here.\nI added two tests Test_shouldRetryCancel_timeout and Test_shouldRetryCancel_cancelled that should cover the possible cases of a request canceled. oh, right, I added this at different moments ! Good catch.. done. Gotcha, done. This underscore happened when I generated a unit test using vscode.. Yes, well during unit tests it was. A nil error was wrapped in an error. And because of the recursive nature of shouldRetryCancel, This caused a panic. So I thought since it's the old behaviour and we still don't know what exactly the error is : keep on retrying.\nThis also feels safer.. This is the test case that made me leave the nil case: \nhttps://github.com/aws/aws-sdk-go/blob/f40bf444ed4ceebb2a3c2e49a41cb4fbb33d5034/aws/request/request_test.go#L82-L111\nIn those cases ( 5xx errors ), somehow a nil error is wrapped.\nShould I may be set r.Error line 96  instead ? Will that always be the set in case of http error ?. Okay, adding this now.. @jasdel, while trying to remove the nil error handling case; I stumbled upon TestRequest4xxUnretryable. This test really counts on the fact that a awserr.RequestFailure is returned. But this happens only if the error is nil at some point.\nif I remove case nil:, the following line panics:\nhttps://github.com/aws/aws-sdk-go/blob/8cd8452b3b916419156906097ad4704e7a119090/aws/request/request.go#L587-L592\nBut if I set the error so that this doesn't panic; this line in the test panics:\nhttps://github.com/aws/aws-sdk-go/blob/8cd8452b3b916419156906097ad4704e7a119090/aws/request/request_test.go#L162-L165\nI still think there has to be a nil error handling case, and the default behaviour of retrying is correct here and like it was before. But in case you disagree, what path would you recommend I take here ?\nI thought of setting the Error with an awserr.RequestFailure error in the handler but this felt like cheating \ud83d\ude42.. Gotcha, adding a doc \ud83d\ude42 . ",
    "ouroboros8": "@diehlaws thanks for the info, would it be useful if I raised a feature request to address the various issues you meantioned with StdinTokenProvider? To be honest I feel there's generally a lot of room for improvement around credential handling; I've come from using boto3, where everything kind of just works, so things like having to use AWS_SDK_LOAD_CONFIG=true and MFA not working out of the box are quite jarring.. ",
    "roy-michael": "I will update the aws-sdk dependency to the version you suggested.\nThanks. . ",
    "markphelps": "Any update on why these 500 errors are being returned by Kinesis? We are experiencing the same issue and have updated out client to v1.15.90 and are still experiencing the error above.. Here's the error message we are getting:\nerror=\"SerializationError: failed decoding JSON RPC error response\\n\\tstatus code: 500, request id: fe8f2cd3-c917-f536-aa0c-764568af694c\\ncaused by: invalid character '<' looking for beginning of value\"\nSo Im assuming its because of HTML being returned?. And heres from our Gopkg.lock to confirm that we have updated to the latest version:\n[[projects]]\n  digest = \"1:474f91b5fd01b3cac32b641eda7e3e64a9c23a0cd27243f93abb1ccf9eba526a\"\n  name = \"github.com/aws/aws-sdk-go\"\n  packages = [\n    \"aws\",\n    \"aws/awserr\",\n    \"aws/awsutil\",\n    \"aws/client\",\n    \"aws/client/metadata\",\n    \"aws/corehandlers\",\n    \"aws/credentials\",\n    \"aws/credentials/ec2rolecreds\",\n    \"aws/credentials/endpointcreds\",\n    \"aws/credentials/stscreds\",\n    \"aws/csm\",\n    \"aws/defaults\",\n    \"aws/ec2metadata\",\n    \"aws/endpoints\",\n    \"aws/request\",\n    \"aws/session\",\n    \"aws/signer/v4\",\n    \"internal/ini\",\n    \"internal/s3err\",\n    \"internal/sdkio\",\n    \"internal/sdkrand\",\n    \"internal/sdkuri\",\n    \"internal/shareddefaults\",\n    \"private/protocol\",\n    \"private/protocol/eventstream\",\n    \"private/protocol/eventstream/eventstreamapi\",\n    \"private/protocol/json/jsonutil\",\n    \"private/protocol/jsonrpc\",\n    \"private/protocol/query\",\n    \"private/protocol/query/queryutil\",\n    \"private/protocol/rest\",\n    \"private/protocol/restxml\",\n    \"private/protocol/xml/xmlutil\",\n    \"service/kinesis\",\n    \"service/s3\",\n    \"service/s3/s3iface\",\n    \"service/s3/s3manager\",\n    \"service/secretsmanager\",\n    \"service/sts\",\n  ]\n  pruneopts = \"NUT\"\n  revision = \"ddc06f9fad886ea5daa5f828f3ca094084f8c2a7\"\n  version = \"v1.15.90\". ",
    "jleeh": "Thanks @diehlaws, changed the StartTime and EndTime to what you mentioned and got a response back \ud83c\udf89 \nWhat was confusing me is the API spec found here. As it mentions \"The time to start the query\" under startTime. I thought that it was just for scheduling when the query should be ran and its timeout rather than the log timeframe.. ",
    "Kiura": "The solution is very simple:\nAdd RUN apk update && apk add ca-certificates && rm -rf /var/cache/apk/* to dockerfile\n. ",
    "bmd": "Hi @diehlaws - thanks for taking a look here. Based on your response, I was able to figure out what was happening. Basically, it appears there there is a small window of time between when the task enters the \"running\" state, and when the *taskStatus.ReplicationTaskStats is actually returned (it looks like this corresponds to the \"Table Statistics\" tab in the DMS UI populating).\nFrom your example, it looks like you ran against a task that had already completed successfully, so you didn't run into the window before ReplicationTaskStats had populated. I'm not sure if there's a TODO on your end here, but feel free to close if everything is working as expected.\nEDIT: I think probably just documenting this behavior would be enough to save some cycles for anyone who runs across this in the future.. Closing this out - thanks for all your help here. ",
    "bodhi": "@jasdel thanks!\n\nI note that DefaultBufSize is exported, so it could be set larger in client code\n\nOne of my team-mates just pointed out to me that DefaultBufSize is const, not var \ud83d\ude40 .\n. @YakDriver I just updated the tests to not use the profile configuration. It's done as a separate commit, but I'm fine to squash them if that's preferred.. ",
    "gazoakley": "@jasdel: Thanks for looking at this. I think I've spotted another issue with their API - the behaviour of DisassociateMembers seems inconsistent:\n\nIf a member has accepted an invite then DisassociateMembers withdraws the invite (as expected)\nIf a member has not accepted an invite then DisassociateMembers doesn't do anything\n\nAlso, calling InviteMembers fails if a member has previously accepted an invitation and has then had their membership removed using DisassociateMembers (the console shows the same error when you try to resend an invite).. This might be a service model/API issue. RAM is supposed to use the restjson protocol (which uses the attribute code), but their error JSON is modelled after that of jsonrpc (which uses the attribute __type). ",
    "sunilkumarmohanty": "Now it looks stupid of me. It seems to be working fine now. I do not know what was the issue. I was obviously using the correct Account ID. I just did not paste them here.\nApologies for the trouble and Thanks . ",
    "nicowernli": "Hi @diehlaws thanks for you answer!\nSo I guess there is no realiable way to access to and API Key by its value. I have to use GetApiKeys filtering by name... is that right?\nIf thats the case I have the feeling that something is missing, that should be an easy way to access to an ApiKey by its value, because the value is the only information you get in the requests.\nAnyway, thanks for your help.. ",
    "joaoferrao": "@diehlaws thanks for the quick response and I understand now what the problem is.\nI'm using a IAM Role associated with an ECS Task that has permissions to make these API calls. What would then be the most correct way of doing this?. @diehlaws thank you for all the help. Still an issue arrises (maybe unrelated).\nI did as instructed and I ended up storing the session in a package level var and run the aws.sessions.Session.Config.Credentials.IsExpired() method every time I need to use a Kinesis client (for every Put record).\nI don't have the error from before BUT I tried making a stress test to the whole tool, I notice there is a huge delay in calling the .PutRecord() method (it's called by the KinesisPutRecord func below).\nMoreover, there is a huge difference between what the AWS CloudWatch metrics show for the put record time and the actual average this method takes to run.\nLooking at the CloudWatch metrics, during the stress test MAX PutRecord latency is 30ms but calling this method is showing it to be well above 500ms average (and >12000ms if I make 1000 requests / sec during 10 secs).\nThe Stream is encrypted but I have made the same test after decrypting it and still have the same general results.\nAny ideas of how to troubleshoot/improve this?\n```go\nvar awsSession *session.Session\nfunc GetOrRefreshAWSSession() *session.Session {\n    switch {\n    case awsSession == nil:\n        Logger.Debug(\"Creds cache was empty, creating one\")\n        awsSession = NewAWSSession()\n        return awsSession\n    case awsSession.Config.Credentials.IsExpired():\n        Logger.Debug(\"Creds expired, renewing\")\n        awsSession = NewAWSSession()\n        return awsSession\n    default:\n        Logger.Debug(\"Session didn't need updating\")\n        return awsSession\n    }\n}\nfunc NewAWSSession() *session.Session {\n    cfg := aws.Config{\n        Region: aws.String(AWSClientSetup.Region),\n    }\nif AWSClientSetup.Name == \"\" {\n    return session.Must(session.NewSessionWithOptions(session.Options{\n        SharedConfigState: session.SharedConfigEnable,\n        Config:            cfg,\n    }))\n}\nreturn session.Must(session.NewSessionWithOptions(session.Options{\n    SharedConfigState: session.SharedConfigEnable,\n    Config:            cfg,\n    Profile:           AWSClientSetup.Name,\n}))\n\n}\nfunc kinesisNewClient(s session.Session) kinesis.Kinesis {\n    return kinesis.New(GetOrRefreshAWSSession())\n}\n// KinesisPutRecord writes []bytes into a Kinesis Stream specified\n// command line options.\nfunc KinesisPutRecord(data []byte) (*string, error) {\n    c := kinesisNewClient(awsSession)\n    randomPartition := fmt.Sprint(uuid.New())\n    start := time.Now().UnixNano()\n    res, err := c.PutRecord(&kinesis.PutRecordInput{\n        Data:         data,\n        StreamName:   &AWSClientSetup.StreamName,\n        PartitionKey: &randomPartition,\n    })\n    Logger.Warn(\"Time to put msg in kinesis: \", (time.Now().UnixNano()-start)/int64(time.Millisecond))\nif err != nil {\n    return nil, err\n}\nreturn res.SequenceNumber, nil\n\n}\n```. ",
    "rnzsgh": "This looks like it was fixed in v1.16.4 and v1.16.5. I can pin to those versions and they compile properly Oddly enough, if I now try to pin to v1.16.3 it updates my go.mod file to v.1.16.5 and pulls latest.\nI just set this in my go.mod\n```\nmodule github.com/rnzsgh/test\nrequire github.com/aws/aws-sdk-go v1.16.3\n```\nThen I ran:\ngo clean\ngo build\nThen my go.mod file looks like:\n```\nmodule github.com/rnzsgh/test\nrequire github.com/aws/aws-sdk-go v1.16.5\n```\nAnd the go.sum is:\ngithub.com/aws/aws-sdk-go v1.16.3 h1:esEQzoR8SVXtwg42nRoR/YLftI4ktsZg6Qwr7jnDXy8=\ngithub.com/aws/aws-sdk-go v1.16.3/go.mod h1:KmX6BPdI08NWTb3/sm4ZGu5ShLoqVDhKgpiN924inxo=\ngithub.com/aws/aws-sdk-go v1.16.5 h1:NVxzZXIuwX828VcJrpNxxWjur1tlOBISdMdDdHIKHcc=\ngithub.com/aws/aws-sdk-go v1.16.5/go.mod h1:KmX6BPdI08NWTb3/sm4ZGu5ShLoqVDhKgpiN924inxo=\ngithub.com/jmespath/go-jmespath v0.0.0-20180206201540-c2b33e8439af h1:pmfjZENx5imkbgOkpRUYLnmbU7UEFbjtDA2hxJ1ichM=\ngithub.com/jmespath/go-jmespath v0.0.0-20180206201540-c2b33e8439af/go.mod h1:Nht3zPeWKUH0NzdCt2Blrr5ys8VGpn0CEB0cQHVjt7k=\nIt looks like you can close this.\n. ",
    "itchyny": "Thank you for the correction.. ",
    "jugalde-r7": "@diehlaws thanks for following up. This explains what we were seeing, good to know.. ",
    "qcoldiron": "Perfect, and thanks for taking the time to help a Golang rookie.  Compiles withour error now.. ",
    "leonardorifeli": "To use AutoTerminate set false in KeepJobFlowAliveWhenNoSteps.. ",
    "sambengtson": "@diehlaws The trace that I provided is actually straight from my Elastic Beanstalk logs.  Here is a code example straight from our app.  I can certainly enable debug logging as well.  How do I go about that?\n```\n        key := os.Getenv(\"S3Key\")\n        pass := os.Getenv(\"S3Pass\")\n        creds := credentials.NewStaticCredentials(key, pass, \"\")\n    config := &aws.Config{\n        Region:      aws.String(\"us-west-2\"),\n        Credentials: creds}\n\n    input := s3.GetObjectInput{\n        Bucket: aws.String(\"<our bucket>\"),\n        Key:    \"<file.zip>\",\n    }\n\n    buff := &aws.WriteAtBuffer{}\n    downloader := s3manager.NewDownloader(session.New(config))\n    _, err := downloader.Download(buff, &input)\n\n    if err != nil {\n        utilities.Logger.Info(err.Error())\n        return c.JSON(utilities.PackageResponse(http.StatusInternalServerError, \"\"))\n    }\n\n/ Process buff.Bytes() below /\n```. @diehlaws Appreciate the help so far.  Here are the logs that I was able to obtain.  I scrubbed Host, Authorization and SHA from the debug info.\n\n/var/log/eb-docker/containers/eb-current-app/0fed160a3308-stdouterr.log\nbufio.(Reader).Peek(0xc421d9f800, 0x1, 0x0, 0x0, 0x0, 0xc421f8e180, 0x0)\n    C:/Go/src/bufio/bufio.go:132 +0x3a\nnet/http.(persistConn).readLoop(0xc4214846c0)\n    C:/Go/src/net/http/transport.go:1601 +0x185\ncreated by net/http.(*Transport).dialConn\n    C:/Go/src/net/http/transport.go:1237 +0x95a\ngoroutine 174 [runnable]:\ninternal/poll.runtime_pollWait(0x7fa2cbd9b6e0, 0x72, 0xc421421760)\n    C:/Go/src/runtime/netpoll.go:173 +0x57\ninternal/poll.(pollDesc).wait(0xc421838098, 0x72, 0xffffffffffffff00, 0xe36aa0, 0x1256698)\n    C:/Go/src/internal/poll/fd_poll_runtime.go:85 +0x9b\ninternal/poll.(pollDesc).waitRead(0xc421838098, 0xc4214e8000, 0x8000, 0x8000)\n    C:/Go/src/internal/poll/fd_poll_runtime.go:90 +0x3d\ninternal/poll.(FD).Read(0xc421838080, 0xc4214e8000, 0x8000, 0x8000, 0x0, 0x0, 0x0)\n    C:/Go/src/internal/poll/fd_unix.go:157 +0x17d\nnet.(netFD).Read(0xc421838080, 0xc4214e8000, 0x8000, 0x8000, 0xc421421898, 0xc4214218a0, 0x40f886)\n    C:/Go/src/net/fd_unix.go:202 +0x4f\nnet.(conn).Read(0xc42000c338, 0xc4214e8000, 0x8000, 0x8000, 0x0, 0x0, 0x0)\n    C:/Go/src/net/net.go:176 +0x6a\ncrypto/tls.(block).readFromUntil(0xc4214c2240, 0x7fa2cbdc0c38, 0xc42000c338, 0x5, 0xc42000c338, 0xc4217de900)\n    C:/Go/src/crypto/tls/conn.go:493 +0x96\ncrypto/tls.(Conn).readRecord(0xc420206000, 0xd8fb17, 0xc420206120, 0x451e10)\n    C:/Go/src/crypto/tls/conn.go:595 +0xe0\ncrypto/tls.(Conn).Read(0xc420206000, 0xc42162d000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    C:/Go/src/crypto/tls/conn.go:1156 +0x100\nnet/http.(persistConn).Read(0xc4214847e0, 0xc42162d000, 0x1000, 0x1000, 0xc421421b98, 0x403355, 0xc421f8e6c0)\n    C:/Go/src/net/http/transport.go:1453 +0x136\nbufio.(Reader).fill(0xc421d9fce0)\n    C:/Go/src/bufio/bufio.go:100 +0x11e\nbufio.(Reader).Peek(0xc421d9fce0, 0x1, 0x0, 0x0, 0x0, 0xc421f8e600, 0x0)\n    C:/Go/src/bufio/bufio.go:132 +0x3a\nnet/http.(persistConn).readLoop(0xc4214847e0)\n    C:/Go/src/net/http/transport.go:1601 +0x185\ncreated by net/http.(*Transport).dialConn\n    C:/Go/src/net/http/transport.go:1237 +0x95a\ngoroutine 200 [select]:\nnet/http.(persistConn).writeLoop(0xc421484480)\n    C:/Go/src/net/http/transport.go:1822 +0x14b\ncreated by net/http.(Transport).dialConn\n    C:/Go/src/net/http/transport.go:1238 +0x97f\ngoroutine 175 [select]:\nnet/http.(persistConn).writeLoop(0xc4214847e0)\n    C:/Go/src/net/http/transport.go:1822 +0x14b\ncreated by net/http.(Transport).dialConn\n    C:/Go/src/net/http/transport.go:1238 +0x97f\ngoroutine 173 [select]:\nnet/http.(persistConn).writeLoop(0xc421484a20)\n    C:/Go/src/net/http/transport.go:1822 +0x14b\ncreated by net/http.(Transport).dialConn\n    C:/Go/src/net/http/transport.go:1238 +0x97f\ngoroutine 177 [select]:\nnet/http.(persistConn).writeLoop(0xc4214845a0)\n    C:/Go/src/net/http/transport.go:1822 +0x14b\ncreated by net/http.(Transport).dialConn\n    C:/Go/src/net/http/transport.go:1238 +0x97f\ngoroutine 171 [select]:\nnet/http.(persistConn).writeLoop(0xc4214846c0)\n    C:/Go/src/net/http/transport.go:1822 +0x14b\ncreated by net/http.(Transport).dialConn\n    C:/Go/src/net/http/transport.go:1238 +0x97f\n---[ REQUEST POST-SIGN ]-----------------------------\nGET  HTTP/1.1\nHost:\nUser-Agent: aws-sdk-go/1.16.8 (go1.10.3; linux; amd64) S3Manager\nAuthorization: \nRange: bytes=47185920-52428799\nX-Amz-Content-Sha256: \nX-Amz-Date: 20181220T143148Z\n\n2018/12/20 14:31:48 DEBUG: Request s3/GetObject Details:\n---[ REQUEST POST-SIGN ]-----------------------------\nGET  HTTP/1.1\nHost: \nUser-Agent: aws-sdk-go/1.16.8 (go1.10.3; linux; amd64) S3Manager\nAuthorization: \nRange: bytes=52428800-57671679\nX-Amz-Content-Sha256: \nX-Amz-Date: 20181220T143147Z. @diehlaws Appreciate the help.  . ",
    "jamie-digital": "Hi @jasdel. I'm afraid I don't have the resources to submit a PR, but while I was triaging the issue I found that the extra environment variable data is added here, so it might just be a case of copying that functionality into the test for the expected value.. ",
    "mikecook": "I rebased the generated updates into one commit. Let me know if you'd like me to switch the generated changes to their own commit.. ",
    "adeshrd": "Hi,\nI am using the same version i.e v1.16.11\nHere is the part of the code for session initialization:\n```\n//SpotTermination is used to detach an instance, used when a spot instance is due for termination\ntype SpotTermination struct {\n    asSvc autoscalingiface.AutoScalingAPI\n    ecsSvc ecsiface.ECSAPI\n}\n//NewSpotTermination is a constructor for creating an instance of spotTermination to call DetachInstance\nfunc NewSpotTermination(region string) SpotTermination {\nlog.Println(\"Connection to region \", region)\n\nsession := session.Must(\n    session.NewSession(&aws.Config{Region: aws.String(region)}))\n\nreturn SpotTermination{\n\n    asSvc: autoscaling.New(session),\n    ecsSvc: ecs.New(session),\n}\n\n}\n```\nIn above, strangely I don't get an error for autoscaling.New(session) but I do get the error for ecs.New(session)\nThanks for your help.. Hi,\nThanks a lot for giving me the idea about the vendor directory.\nThis project wasn't written by me so I didn't actually check the vendor directory.\nI found that when I copied the service/ecs folder into the vendor directory it started working.\nLike you said, it must be some conflicts. Thanks again!. ",
    "nestorsokil": "Additional investigation has shown that this may actually be an issue with Echo framework. I still think that AWS SDK should provide a way to do such validation, but this is not a really problematic issue, so I will close the ticket for now.\nSorry for bothering, cheers.. ",
    "andreipetrescu95": "Hey, the function checks if all the instances are In Service when upscaling, and checks that the number of instances is right when downscaling.\nHere is the code:\n```\nfunc asgReadinessCheck(instances []*autoscaling.Instance, newASGsize int64) bool {\n    var statusOK bool\n    statusOK = true\nif len(instances) != int(newASGsize) {\n    statusOK = false\n    return statusOK\n}\n\nfor _, instance := range instances {\n    if *instance.LifecycleState != \"InService\" {\n        statusOK = false\n        break\n    }\n}\nreturn statusOK\n\n}\n```. The request IDs are as follows:\nFor upscaling: d37d60dc-1427-11e9-acd5-6d243622e0d0\nFor downscaling: 85db6c66-1436-11e9-8fda-37080c1c4fa0\nOnce again, it upscaled from 2 to 3 nodes, but did not downscale to 2 again.. ",
    "aurelijusbanelis": "Similar error handling is missing for WaitUntilChangeSetCreateComplete as well.\nRequest times out even stack status was in CREATE_COMPLETE long time ago.\nTo reproduce: remove cloudformation:DescribeChangeSet permissions from the user \u2013 error would be silently ignored:\nAccessDenied: User: ... is not authorized to perform: cloudformation:DescribeChangeSet\n. ",
    "dmolesUC3": "As a workaround, I came up with the following reflection-based solution, but I don't like it:\ngo\nfunc validateCredentials(sess *session.Session) (*session.Session, error) {\n  providerVal := reflect.ValueOf(*sess.Config.Credentials).FieldByName(\"provider\").Elem()\n  if providerVal.Type() == reflect.TypeOf((*credentials.ChainProvider)(nil)) {\n    chainProvider := (*credentials.ChainProvider)(unsafe.Pointer(providerVal.Pointer()))\n    providers := chainProvider.Providers\n    if len(providers) > 0 {\n      err := reflect.ValueOf(providers[0]).Elem().FieldByName(\"Err\")\n      if err.IsValid() {\n        if e2, ok := err.Interface().(error); ok {\n          return nil, e2\n        }\n      }\n    }\n  }\n  return sess, nil\n}. \u2026and the above solution breaks down if you are in EC2 and want to use the IAM role credentials.. Hi @jasdel \u2014 that works, but not for the purpose of quickly informing a user that they've forgotten to provide credentials. (I can't set the environment variable programmatically, since this tool I'm writing should run both in EC2 with IAM role credentials, and outside EC2 without them; and if the user has forgotten to provide credentials, they're not likely to remember to set AWS_EC2_METADATA_DISABLED.)\nI temporarily disabled the reflection-based credentials method above and was able to get IAM role credentials to work when running in EC2. Oddly enough, the next thing I was looking at was the EC2Metadata API, to see if I could use it to determine programmatically whether the tool is running in EC2. It seems doable, although I was thrown for a while by the fact that I couldn't use the same session (/ session options) I was using for S3.. Unfortunately, it looks like just calling EC2Metadata.Available() makes the same slow credentials check.. My current hack is to keep the validateCredentials() method above, but only run it if uname -a does not include the string \"amzn\". This seems less than ideal!. @smaftoul Interesting!. @tnarg A good point, and I imagine that's true for the uname -a solution as well. For the tool I'm working on, it would be easy enough to add a flag to skip the check, but I think a general solution would have to be at the API level.. ",
    "smaftoul": "@dmolesUC3 I think I have a better way to identify AWS machines:\n$ cat /sys/devices/virtual/dmi/id/sys_vendor\nAmazon EC2\nWDYT ? Maybe it doesn't work for bare metal machines, but at least, work better then uname -a.. ",
    "tnarg": "I just happened upon this thread. If for some reason you decide to leverage /sys/devices/virtual/dmi/id/sys_vendor, please make sure there is a way to opt in to ec2 metadata discovery. My company relies on ec2 metadata in our on premise kubernetes clusters using kube2iam, and /sys/devices/virtual/dmi/id/sys_vendor does not contain \"Amazon EC2\". I imagine we aren't the only ones.. ",
    "wbhuberIBM": "Some user error on my test write up, closing this issue. . ",
    "timmersuk": "Is there an ETA for when this PR will be able to be merged into mainline? I currently have production code which is dependant on getting enhanced fan-out support in the go-sdk, and whilst its been great to be able to test it by pulling the feature-fork and compiling against that, the code can't go into main  like this.. ",
    "Zebradil": "It'd be great. Is there any logic behind these values? They look like the values for YAML's boolean type.. ",
    "rayjlinden": "Thank you!  (Might be useful to include in the docs though...). I'll work to get a more cleaned up version.\nHowever, I know my version of kinesislite is using SSL.  I was kind of forced to.  I'm using some software called \"Maxwell's daemon\" which reads from mysql biunlog and writes db changes to kinesis.  It in turn uses the Java KCL library.  That library requires the use of SSL.  No way to configure it not to use SSL.  (It does have a way, however, to not verify a self signed cert.)\nSo our kinesislite instance had to use SSL (which uses a self signed cert).  I have a separate bug request asking how to specify the equivalent of the was-cli --no-verify flag.  I wouldn't think the errors I'm getting in this bug report are related to that - but maybe it is?\n. Yea - I reduced my code to be closer to yours:\n```\npackage main\nimport (\n        \"os\"\n        \"log\"\n        \"fmt\"\n        \"github.com/aws/aws-sdk-go/aws\"\n        \"github.com/aws/aws-sdk-go/aws/session\"\n        \"github.com/aws/aws-sdk-go/service/kinesis\"\n)\nvar ( \n        stream = \"maxwell\"\n        region = \"us-west-1\"\n        endpoint = \"http://kinesislite:4568\"\n }\nfunc main() {\n        log.Println(\"Starting...\")\n    s, err := session.NewSession(&aws.Config{\n            Endpoint: aws.String(endpoint),\n            LogLevel: aws.LogLevel(aws.LogDebug),\n            Logger: aws.NewDefaultLogger(),\n    })\n    if err != nil {\n            panic(err)\n    }\n    kc := kinesis.New(s)\n\n    kc.Config.Logger.Log(\"List streams\")\n    resp, err := kc.ListStreams(&kinesis.ListStreamsInput{})\n    fmt.Println(resp, err)\n\n}\n```\nWhich gives me the following output (and error):\n```\n2019/01/14 22:43:57 Starting...\n2019/01/14 22:43:57 List streams\n{\n} InvalidEndpointURL: invalid endpoint uri\ncaused by: parse \"http://kinesislite:4568\"/: first path segment in URL cannot contain colon\n```\n(Before adding the Logging it was failing silently...). Sure - using go modules by the way (from go.mod) this is only dependency:\nrequire github.com/aws/aws-sdk-go v1.16.18\nI am running in docker - and launching in docker compose.  It is actually a very big compose file.  However, here are the relevant parts:\nkinesislite: \n  image: docker.io/vsouza/kinesis-local:latest\n  ports:\n    - \"4568:4568\"\n  environment:\n    - AWS_CBOR_DISABLE=true\n  command: --port 4568 --ssl\nkinesis-reader:\n  image: kinesis-lambda:dev\n  environment:\n    - AWS_ACCESS_KEY_ID=foo\n    - AWS_SECRET_ACCESS_KEY=foopass\n    - AWS_REGION=us-west-1\n    - KINESIS_ENDPOINT=\"http://kinesislite:4568\"\n    - STREAM_NAME=maxwell\n  links:\n    - kinesislite\n  volumes:\n    - /var/tmp/rayj/gocode/src/github.com/lindenlab/ecom-kinesis-lambda/service:/app\nmysql:\n  image: registry.docker/extraction/dev-all-databases:{{schema}}\n  environment:\n    - MYSQL_ROOT_PASSWORD=password\n  volumes:\n    - /tmp/makeworld/configs/mysql/:/etc/mysql/conf.d/:ro\nmaxwell:\n  build: dockerfiles/.\n  dockerfile: dockerfile-run-maxwell\n  environment:\n    - AWS_ACCESS_KEY_ID=foo\n    - AWS_SECRET_ACCESS_KEY=foopass\n  links:\n    - mysql\n    - kinesislite\n  volumes:\n    - /tmp/makeworld/configs/maxwell/config.properties:/app/config.properties:ro\n    - /tmp/makeworld/configs/maxwell/kinesis-producer-library.properties:/app/kinesis-producer-library.properties:ro\nIn fact, only the kinesislite and kinesis-reader section are relevant.  It is using a link since this is a compose 1.x file.  However, networking wise this is a very simple set up.. If I change the config to point to a non-existent host - I get what you get:\n```\n{\n} RequestError: send request failed\ncaused by: Post http://nokinesislite:4568/: dial tcp: lookup nokinesislite on 10.132.0.2:53: no such host\n```\nSo clearly it can resolve the host - and is choking later in the process.. I think this is happening in request.go in the New function when it calls url.Parse.  However, http://kinesislite:4568 is a valid url that should pass url.Parse.  I suspect the URL is getting changed somehow before this is checked?  . Ugg!!!!  The bug was in my compose file.  The url basically had another set of quotes around it.  So url.Parse was literally trying to parse \"http://kinesislite:4568\" instead of http://kinesislite:4568.  Ug!\nIt can now at least read the streams.  (Though I think my other question around how to set InsecureSkipVerify: true also needed to happen for it to work.)\nThanks!. ",
    "rafaelkperes": "Hi @jasdel, thanks for reaching back so fast.\nWeirdly enough, my use case does require the user to set the endpoint, region and bucket (and none may be empty). But the endpoint may point to another S3 service besides AWS.\nI figure one option would be to check if endpoint points to amazonaws.com, I could simply leave it empty and let the SDK take care of.\nAnyway, what bothers me most is the error message, as it points the correct region as \"incorrect\" (the one set in the region parameter, while the actual incorrect one from the endpoint is not raised).. > Thanks for the update. I agree the error message is confusing. To make sure I understand this correctly. The bucket is in the us-west-2 region, correct?\nCorrect.\n\nThe error is valid because the bucket doesn't actually exist in the us-east-1 region\n\nAgreed.\n\nit looks like S3 API is using the wrong value in the error message\n\n\ud83d\udc4d \n\nI can forward this onto the S3 service team to let them know about the error, that the error message should refer to the region the API request was made to, not the region the request as signed for.\n\nPlease do so. Maybe there's a reason it is done that way, but I cannot see it as of now.\nThanks!\n. Hi @jasdel, thanks for the fix and the heads up!. ",
    "takeyourhatoff": "Despite the warning in the request package, I have rolled my own waiter in the meantime. I cannot find documentation that lists all the possible values Status can take. I assume that it can take the same values as a DescribeDBSnapshot request:\n```go\nfunc waitUntilDBClusterSnapshotAvailableWithContext(ctx aws.Context, svc rds.RDS, input rds.DescribeDBClusterSnapshotsInput, opts ...request.WaiterOption) error {\n    w := request.Waiter{\n        Name:        \"WaitUntilDBClusterSnapshotAvailable\",\n        MaxAttempts: 60,\n        Delay:       request.ConstantWaiterDelay(30 * time.Second),\n        Acceptors: []request.WaiterAcceptor{\n            {\n                State:   request.SuccessWaiterState,\n                Matcher: request.PathAllWaiterMatch, Argument: \"DBClusterSnapshots[].Status\",\n                Expected: \"available\",\n            },\n            {\n                State:   request.FailureWaiterState,\n                Matcher: request.PathAnyWaiterMatch, Argument: \"DBClusterSnapshots[].Status\",\n                Expected: \"deleted\",\n            },\n            {\n                State:   request.FailureWaiterState,\n                Matcher: request.PathAnyWaiterMatch, Argument: \"DBClusterSnapshots[].Status\",\n                Expected: \"deleting\",\n            },\n            {\n                State:   request.FailureWaiterState,\n                Matcher: request.PathAnyWaiterMatch, Argument: \"DBClusterSnapshots[].Status\",\n                Expected: \"failed\",\n            },\n            {\n                State:   request.FailureWaiterState,\n                Matcher: request.PathAnyWaiterMatch, Argument: \"DBClusterSnapshots[].Status\",\n                Expected: \"incompatible-restore\",\n            },\n            {\n                State:   request.FailureWaiterState,\n                Matcher: request.PathAnyWaiterMatch, Argument: \"DBClusterSnapshots[].Status\",\n                Expected: \"incompatible-parameters\",\n            },\n        },\n        Logger: svc.Config.Logger,\n        NewRequest: func(opts []request.Option) (request.Request, error) {\n            var inCpy rds.DescribeDBClusterSnapshotsInput\n            if input != nil {\n                tmp := *input\n                inCpy = &tmp\n            }\n            req, _ := svc.DescribeDBClusterSnapshotsRequest(inCpy)\n            req.SetContext(ctx)\n            req.ApplyOptions(opts...)\n            return req, nil\n        },\n    }\n    w.ApplyOptions(opts...)\nreturn w.WaitWithContext(ctx)\n\n}\n```. ",
    "daviskoh": "I can confirm that the User Pool exists in the correct account because when I deploy using the same credentials from sess.Config.Credentials.Get(), the Lambda is in the under that same account. \nDouble checked the UserPoolId by copy pasting the value in the CMD+f in my browser in the AWS Console and it indeed is the right value in the right account.. This seems to be working now for some odd reason. I will investigate, but going to close this for now. Is there a way to just pass credentials in code? I already have the env vars set in prod, but would like to reference those values manually when setting up the aws config in code.. ",
    "kannanvr": "@diehlaws, Thanks for your response. . ",
    "arminc": "Hi @diehlaws, thanks for answering. I understand the Tags are optional but I find it a bit weird that Roles struct does have a Tags field which is never filled, even when tags are available on a Role, or is there somewhere else some code that uses this struct and does populate that field? If not then it feels like dead code, or it is at least misleading. \nMy goal is to find roles which we are using with EKS:\nOption 1) Use ListRoles and look at the Tags, like I tried. I understand I can fetch Tags (like you present) but that means I need to do that for 70+ roles which sounds a bit overkill. I would need to do 70+ requests to AWS to find out which roles have a certain Tag.\nOption 2) Use Tag API, this does not work for IAM roles.\nOption 3) Use the 'path' separator in Roles, the problem here is that EKS can not handle that.\nI feel like I am stuck :)\n. I had a chat with our contacts from AWS and they told me they have provided the internal teams our case as well.. ",
    "ohwell123": "Um ton\nOn Wed, Feb 13, 2019, 12:10 PM Kenneth Davis <notifications@github.com\nwrote:\n\nVersion of AWS SDK for Go?\nv1.16.33\nVersion of Go (go version)?\nv1.11.1 darwin/amd64\nWhat issue did you see?\nWhen using a ParameterStringFilter on Name with option BeginsWith, SSM\nDescribeParameters request appears to only respect the MaxResults field\nwhen set to 1 or 2, but anything greater appears to default to 2. This\nalso might be causing strange pagination behavior. It is hard to tell until\nthe max results is working as expected.\nSteps to reproduce\nfilters := []*ssm.ParameterStringFilter{\n  &ssm.ParameterStringFilter{\n      Key:    aws.String(ssm.ParametersFilterKeyName),\n      Option: aws.String(\"BeginsWith\"),\n      Values: aws.StringSlice([]string{\"START_PARAM_PATH\"}),\n  },\n}\ninput := &ssm.DescribeParametersInput{\n  ParameterFilters: filters,\n  MaxResults:       aws.Int64(10),\n}\noutput, err := svc.DescribeParameters(input)\nif err != nil {\n  return nil, err\n}```\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-go/issues/2446, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AdZUQmhBry19-YfziNUgcjeq7wpeYnwPks5vNFUegaJpZM4a5-aw\n.\n. \n",
    "kendavis2": "Thank you, @diehlaws, for the quick reply.\nI understand that the number of results returned are not guaranteed. That makes sense. After running dozens of tests and seeing two results returned on the first call without exception regardless the value of MaxResults had me confused. I expected the number of returned results to vary. That seemed odd and inefficient to make two calls to get three results every time. So, is would seem MaxResults is more of a cap for the client on the results returned since AWS does internal capping as needed.\nYour explanation was much appreciated. This issue can be closed.. ",
    "cbarraford": "@jasdel no, there is no file there, it is running within a docker container. I have a file at /Users/<name>/.aws/credentials on my user, but nothing within the docker container. if its helpful, here is the Dockerfile of the docker image im running aws-sdk-go within.\n```\nFROM golang:alpine\nRUN \\\n    apk add --update \\\n        python \\\n        python-dev \\\n        py-pip \\\n        build-base \\\n    && \\\n    pip install dumb-init \\\n    && \\\n    rm -rf /var/cache/apk/* && \\\n    :\nRUN apk update && \\\n    apk add curl make git postgresql-client && \\\n    apk del curl && \\\n    rm -rf /var/cache/apk/*\nENTRYPOINT [\"dumb-init\"]\nCMD [\"/bin/sh\"]\n``. I've switched tosession.NewSession()` for now to unblock me, but apparently its deprecated according to my linter.\nBut, i don't think it matters whether or not the ~/.aws/credentials file is there or not, because the envConfigLoad() function just assumes it is there and adds it to the configuration.\nhttps://github.com/aws/aws-sdk-go/blob/master/aws/session/env_config.go#L218\nThen, latter on, we assume the creds file is legit and look for it, (which causes my error).\nhttps://github.com/aws/aws-sdk-go/blob/master/aws/session/session.go#L359-L372\nI don't see in the code path where we check for the existence of credentials file.. ",
    "dwrz": "Apologies -- it looks like the messages are being deleted, but somehow new messages with same bodies are being added to the queue again.\nUpdate: this was an issue with a worker using the wrong queue URL. My apologies again.. ",
    "b-dean": "region: us-east-1\nami: nothing in particular, usually something RHEL 7 based but nothing special\nvpc: yes (not default vpc or ec2 classic or whatever it's called)\nI'm not sure how easily I can get the logs, I can't reliably reproduce this bug. It happens with some tools we run as part of our automation and I think I've only seen it once or twice out of dozens of times. I'll see what I can do about cranking up the logging but I have no idea when it might show up.. ",
    "universonic": "I finally found this that works for my local Ceph RGW environment:\n```go\n    sess, _ := session.NewSession(&aws.Config{\n        Credentials:      credentials.NewStaticCredentials(\"QU5B2DUZRYBGESO460N9\", \"sYd8DURD7a4MfDVtk7Vr0yz9DDkyOTV9EBObNCCN\", \"\"),\n        Endpoint:         aws.String(\"ceph.local:7480\"),\n        Region:           aws.String(\"default\"), // The region must be provided\n        DisableSSL:       aws.Bool(true),\n        Logger:           aws.NewDefaultLogger(),\n        LogLevel:         aws.LogLevel(aws.LogDebugWithSigning),\n        S3ForcePathStyle: aws.Bool(true), // disable endpoint update\n    })\nsvc := s3.New(sess)\ncbo, err := svc.CreateBucket(&s3.CreateBucketInput{\n    // Remove the LocationConstraint specification\n    CreateBucketConfiguration: &s3.CreateBucketConfiguration{LocationConstraint: aws.String(\"\")},\n    Bucket:                    aws.String(\"helloworld\"),\n})\nif err != nil {\n    if aerr, ok := err.(awserr.Error); ok {\n        switch aerr.Code() {\n        case s3.ErrCodeBucketAlreadyExists:\n            fmt.Println(s3.ErrCodeBucketAlreadyExists, aerr.Error())\n        case s3.ErrCodeBucketAlreadyOwnedByYou:\n            fmt.Println(s3.ErrCodeBucketAlreadyOwnedByYou, aerr.Error())\n        default:\n            fmt.Println(aerr.Error())\n        }\n    } else {\n        // Print the error, cast err to awserr.Error to get the Code and\n        // Message from an error.\n        fmt.Println(err.Error())\n    }\n    return\n}\nfmt.Println(cbo)\n\n```. ",
    "shawnhankim": "@diehlaws : Thanks for kindly replying me regarding the request.\nSummary\n\nI want to use the AWS SDK for Golang to mock out the product code that the EC2 metadata client returns to the SDK.\nI want to add mock product code in EC2 instance or in an AMI using AWS SDK for Golang.\nI want to also delete mock product code in EC2 instance or in an AMI using AWS SDK for Golang prior to completing the entire process of AWS Marketplace from scanning to approval from AWS.\nI am not trying to change the product code in the instance metadata for an EC2 instance which was launched using a paid AMI.\n\nBackground\n\nI have to implement and test my projects on the different types of licensing model such as AWS Marketplace and BYOL everyday.\nI am building lots of AMIs based on CI/CD pipeline everyday.\nWhenever the CI/CD pipeline build AMIs with different feature, it is difficult to proceed the AWS Marketplace process from scanning to approval and it needs more automation.\nI would like to make sure that the ongoing implementation is always good for AWS Marketplace and BYOL whenever I build new feature every day.\nIf one of my microservices(called \"A\") identifies that there is product code in EC2 instance when the \"A\" is started, I could easily skip BYOL in the code and it is also able to be confirmed that the AWS Marketplace model works well without scanning the AMIs which is being updated everyday.\nThen, we could start and complete the entire AWS Marketplace process once I have a confidence to launch the product on AWS Marketplace.\nTherefore, I am requesting that any engineer could add/delete the mock product code in an EC2 instance or in an AMI using AWS SDK for Golang.\n\nPlease feel free to let me know if this description is not enough or if it is not sure what I am requesting.. ",
    "minamijoyo": "@diehlaws Thank you for working on this and opening the issue on upstream.\nI'm not familiar with go-jmespath and reflect metaprogramming and I'm not sure why the past attempt was reverted. So, I will wait for a while using WaitUntilUpstreamMaintainersActive() \ud83d\ude04 . ",
    "sanketplus": "Regardless of shared config enabled or not, the PR drops files that has name *credentials if both key id and secret keys are present in env vars. since credentials from env vars has higher precedence, it is assumed safe to ignore those files. \nAm I missing something obvious here? Also for test case, I see one to be test for providing conf from both env vars and a file and assert that one from env takes precedence. Looking forward to your comments @jasdel . ",
    "nqbao": "If I use this snippet, it works\n```\npackage main\nimport (\n    \"bytes\"\n    \"crypto/hmac\"\n    \"crypto/sha256\"\n    \"fmt\"\n    \"net/url\"\n    \"strings\"\n    \"time\"\n)\nfunc AwsIotWsUrl(accessKey string, secretKey string, sessionToken string, region string, endpoint string) string {\n    host := fmt.Sprintf(\"%s.iot.%s.amazonaws.com\", endpoint, region)\n// according to docs, time must be within 5min of actual time (or at least according to AWS servers)\nnow := time.Now().UTC()\n\ndateLong := now.Format(\"20060102T150405Z\")\ndateShort := dateLong[:8]\nserviceName := \"iotdevicegateway\"\nscope := fmt.Sprintf(\"%s/%s/%s/aws4_request\", dateShort, region, serviceName)\nalg := \"AWS4-HMAC-SHA256\"\nq := [][2]string{\n    {\"X-Amz-Algorithm\", alg},\n    {\"X-Amz-Credential\", accessKey + \"/\" + scope},\n    {\"X-Amz-Date\", dateLong},\n    {\"X-Amz-SignedHeaders\", \"host\"},\n}\n\nquery := awsQueryParams(q)\n\nsignKey := awsSignKey(secretKey, dateShort, region, serviceName)\nstringToSign := awsSignString(accessKey, secretKey, query, host, dateLong, alg, scope)\nsignature := fmt.Sprintf(\"%x\", awsHmac(signKey, []byte(stringToSign)))\n\nwsurl := fmt.Sprintf(\"wss://%s/mqtt?%s&X-Amz-Signature=%s\", host, query, signature)\n\nif sessionToken != \"\" {\n    wsurl = fmt.Sprintf(\"%s&X-Amz-Security-Token=%s\", wsurl, url.QueryEscape(sessionToken))\n}\n\nreturn wsurl\n\n}\nfunc awsQueryParams(q [][2]string) string {\n    var buff bytes.Buffer\n    var i int\n    for _, param := range q {\n        if i != 0 {\n            buff.WriteRune('&')\n        }\n        i++\n        buff.WriteString(param[0])\n        buff.WriteRune('=')\n        buff.WriteString(url.QueryEscape(param[1]))\n    }\n    return buff.String()\n}\nfunc awsSignString(accessKey string, secretKey string, query string, host string, dateLongStr string, alg string, scopeStr string) string {\n    emptyStringHash := \"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\"\n    req := strings.Join([]string{\n        \"GET\",\n        \"/mqtt\",\n        query,\n        \"host:\" + host,\n        \"\", // separator\n        \"host\",\n        emptyStringHash,\n    }, \"\\n\")\n    return strings.Join([]string{\n        alg,\n        dateLongStr,\n        scopeStr,\n        awsSha(req),\n    }, \"\\n\")\n}\nfunc awsHmac(key []byte, data []byte) []byte {\n    h := hmac.New(sha256.New, key)\n    h.Write(data)\n    return h.Sum(nil)\n}\nfunc awsSignKey(secretKey string, dateShort string, region string, serviceName string) []byte {\n    h := awsHmac([]byte(\"AWS4\"+secretKey), []byte(dateShort))\n    h = awsHmac(h, []byte(region))\n    h = awsHmac(h, []byte(serviceName))\n    h = awsHmac(h, []byte(\"aws4_request\"))\n    return h\n}\nfunc awsSha(in string) string {\n    h := sha256.New()\n    fmt.Fprintf(h, \"%s\", in)\n    return fmt.Sprintf(\"%x\", h.Sum(nil))\n}\n```. Alright, thank you. I will use the custom sign method.. ",
    "AlexGascon": "@diehlaws Good to know, thanks for the quick reply!. ",
    "ks2211": "Hey @jasdel thanks for the response. \nThe g.Wait() is returning InvalidParameterValue as the error from the failed request. \nHowever, the error inside of the CreateSubnet request, it shows the OTHER request return RequestCancelled but the subnet still gets created inside of aws/the response is empty--is there a way to prevent the subnet from creating in aws if its a request cancelled error or will I just have to clean up after the fact on error?\nI guess my issue is the 1a call is returning the InvalidParameterValue as expected. The 1b call goes through succesfully but returns a nil response and the RequestCancelled error. I can technically check for this error and do a DescribeSubnets and DeleteSubnets to clean up but it adds a bit of extra code. @jasdel Thanks for the explanation, that makes perfect sense. I was expecting 1b while still using errgroups (e.g response unmarshalling/retuning instead of the RequestCancelled error) but doesn't seem like it's possible.\nGetting rid of the errgroup.WithContext and using just a regular errgroup.Group works but its a bit slower (since no cancel is called, the other goroutines finish before I can run my clean up funcs)/less code I have to refactor at the moment so I'll stick with this for now.\nThanks for digging into this!\n. ",
    "klauern": "I don't think configservice is usable with Go with this unmarshal error, unless we all fork the repo to provide our own implementation.  Since this appears to be a codegen issue, I don't think this can be handled within Go, either.. Hi @diehlaws.  Thank you for the detailed response.  What I'm struggling with is what I'm supposed to do in my case as a user of the API.  Right now, I can't take the ConfigurationItem type and unmarshal it, as it panics on that *string type.  Outside of forking the repo and providing my own implementation (perhaps changing *string to json.RawMessage), how is a user expected to parse AWS Config objects from an S3 bucket in Go?. ",
    "jnst": "Oh, perhaps it is a service client?. ",
    "cracraaf": "Help with my page it\u2019s been hacked. ",
    "pmatseykanets": "Created by mistake. ",
    "dcoker": "In my experience a common use case is that there are objects in S3 with a variety of encryption settings and an organizational policy to write new objects using a single crypto standard. This happens because teams are sloppy with crypto as they learn about it and then become more rigorous over time as team members become educated or due to security reviews. Thus it feels appropriate to configure a client-level default policy (which would be used for Put operations) but also allow the Get operations to gracefully handle the encryption configuration associated with whatever objects they encounter.\nAlso, note that the AWS Java SDK uses a CryptoMode type to determine a combination of encrypt and decrypt behavior. I find this confusing because there are three settings and they each configure a different subset of encryption algorithm, decryption algorithm, and decryption policy, and also the EncryptionOnly option is questionable because encryption w/o authentication is a dangerous and usually insecure choice. The only feature that the CryptoMode design allows which might be awkward for yours is the ability to reject non-authenticated encryption on Get*, though if you wanted to support that it then perhaps a simple bool would be a more elegant design.\n. ",
    "Fugiman": "WithDelayedWaiter?. ",
    "enisoc": "Shouldn't the second arg be one of the whence constants?. ",
    "micahhausler": "This should never be nil, an assumed role credential will always have an expiration and a token. https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithWebIdentity.html. The role ARN is a required field, I'd say it should be required to use this provider. As stated above, a SessionToken is a required field and should be added. This is similar to what the CLI does. Would the overhead of adding the hostname (which would be fairly unique for Kubernetes pods) be too much? Could that be done once initially?. Ah ok that makes sense. I like an error being reported if only one is present. Should you add the Value.ProviderName to the credentials?. ",
    "bretambrose": "In chime you mentioned\n\"But... in the case of error the information needed isn't available until Retry handlers finishes\"\nIt looks like we're invoking the new Complete handler before the Retry handlers.  Will all the required error information be present at that time with this new control flow?. ",
    "zhangzhx": "Curiously, are these indents intended or should they be aligned? . "
}
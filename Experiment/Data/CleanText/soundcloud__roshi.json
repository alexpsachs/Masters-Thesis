{
    "peterbourgon": "They were just races in some mock implementations in tests. Fixed in 6ca6703cfb2087f2c48c02385ceabd046de42855 and e46559a4bd9f337e58bcb51eeeb17af8216f662d.\n. Ah, I think we have some copyright issues here :)\nPlease make a sketch and resubmit :)\n. I'm uncomfortable with how complex the read strategy is getting. There's too much state there, too many threshold/rate checks and anonymous goroutines. Also, as previously mentioned, I indeed think the reportReadsToWalker link between the walker and the read strategies increases the coupling within the farm too severely.\nCan you think of any nicer ways to decouple all the moving parts in the farm? I'll try to come up with something, and maybe we can compare notes in a day or so...\n. As ratepolice is only imported by farm, can you move it to that package?\n. Hmm. So far, the existence of a separate package implies either deliberate division of responsibility (shard, cluster, farm, roshi-server) or a shared component that requires a separate package (instrumentation, common). Those \"rules\" help one understand the interacting pieces within Roshi; having a top-level component that's only used in one place confuses that understanding. (In my opinion.)\n. :dango:\n. Very dirty history :frowning: I would squash it, but I'm not sure if that will do bad things to the new subtree merges...\n. Closed by #14 \n. A good idea.\n. Described primary use-case and linked to blog post in README. Cheers!\n. :dango:\n. Thanks for the comment. There's more to cluster configuration changes (i.e. \"elasticity\") than simply discovery. First and foremost is re-sharding of data, which costs time, and adds an extra dimension of consideration to all cluster operations. We spent a long time weighing the pros and cons of dynamic cluster reconfig, and settled very strongly on disallowing it: the complexity it brings isn't outweighed by the value it provides. \nBecause pools don't need to be the same size, standard operating procedure is to provision a new pool, bring it online, and run a walker process (or your own backfill job) to populate it. I also plan to introduce a \"write-only\" flag to a pool descriptor, to make reads more predictable when one pool is in this intermediate mode.\n. Thanks for the comment! We started by trying to use twemproxy, but it performed very poorly and lacked features we needed.\n. It's designed for a specific type of workload, effectively a write-through cache. Roshi's interactions are structurally different.\n. Also, it runs as a separate service, introducing an extra network hop for all operations. That's nontrivial, especially for the throughput that Roshi's designed for (millions of reads per second) and given that one of our major bottlenecks is already the network.\n. > I expected that delete operations on elements that not actually removed will result {deleted: 0} in response.\nNot exactly. The return on a write operation signals whether the client needs to resubmit their request: 1 means everything is fine, even if the submitted event was a no-op due to score; 0 means there was likely some communication or quorum problem. CRDT semantics ensure the eventual consistency of the data model; there's no need to signal any information to the client about whether the write op actually effected a readable mutation.\nI'll clarify that in the README.\n. > .delete(\"foo\",2,\"bar\")\n\n.delete(\"foo\",3,\"bar\")\nboth will give {deleted:1}, as a surprise, because no item was actually removed.\n\nAs I hope is clearer now, these operations are actually incrementing the score of the bar element in the foo key's delete set. Effectively, they're saying that the most recent state of foo/bar is a delete, at t=3. At that point, you'd need to insert foo/bar with a score of at least 4 to have it appear in a read of foo.\n. Ack. Pending.\n. @u-c-l, would you mind giving me a sanity-check here?\n. @rsltrifork\u2014thanks for the comment! I'll dig into the paper again shortly...\n@freenerd\u2014just FYI?\n. If you delete (a, 99) from a set that has no element a in the add set, the element (a, 99) is put into the remove set. Selecting a will return the same result before and after the delete operation, but after the delete, only write operations that have a score higher than 99 will be able to successfully mutate the set.\nDoes it make sense?\n. > doomstones\nHehe. I like it.\n\nwhat is the outcome if two sets merge where . . .\n\n~~This initial condition could never exist in Roshi. Our write operations ensure that a given element a will only exist in the Add set or the Remove set for a given logical set\u2014never both.~~ Oops, I see what you mean.\nIf one cluster has A(a,1) R() and another has A() R(a,1), the merge would result in the add winning. But the reasoning is a little bit convoluted. The API between the cluster (a single logical CRDT node) and the farm (multiple nodes) is biased towards adds, i.e. it returns only the add sets. (That decision was taken deliberately, for reasons of performance.) \nThe farm will see cluster A returning (a,1) and cluster B returning nothing, and return (a,1) to the user, but would also detect that there's not perfect agreement and issue a read repair. So far so good. But since the scores are identical, the obvious path of the read repair is stymied. Looking at the code we don't explicitly deal with the case where multiple clusters return the same score but different \"presence\" indicators\u2014so I guess we have underdefined behavior there. But we do have a bias on the write operation: an element must have a purely higher score to make it into the Remove set, but merely a greater-than-or-equal-to score to make it into the Add set, i.e. we're biased towards adds at that layer. \nSo I suppose I should update the read repair to exhibit the same bias, i.e. if an element has the same score but different presence indicators from multiple clusters, prefer to add it. If it's OK with you, I'll edit the title of this issue to reflect that concrete bit of work, and make a PR shortly.\nNote this all assumes the SendAllReadAll read strategy. Other read strategies could shortcut parts of what I've described. And very interesting corner case! Thanks for bringing it up.\n. Closed with #33 \n. @tsenart\u2014does that make sense?\n. Yep! Those extra deps came in with the new version of the prometheus client lib, and neither I nor the committer caught them at the time. Thanks!\n. Thanks for doing that! There needs to be one small change in the g2s PR, then we can merge and merge.\n. :100:\n. Thank you for the notification, but I guess it's not really an issue :)\n. :1234:\n. Roshi actually supports cursor-based pagination, it's just not heavily advertised :)\n. I'm happy to fix this issue via PR, but I can't commit to providing ongoing maintenance. I would also abstain from any vote about moving the repo to some other organization.\n. Oh, wow, that's some old Travis config :) Let me update...\n. @beorn7 Over to you for the merge. I suspect 8c835b0 is explained by some random regression on tip.\n. It's technically possible, and you've identified the right place for the changes, but it's not currently supported. I can't see any reason why it couldn't be, but it's not a use-case SoundCloud currently has, so I suspect the best path forward would be to create a PR, which I'd be happy to review.\n. > 1) At soundcloud, are you then using another db or redis for multiget the key responses you get back from roshi so you can create the entire feed server-side, i.e from who/tracks/metadata/likes etc or do you just supply the member server side and let client side fetch all the details on this member and the build the feed client side?\nIf the question is \"how do you materialize the IDs that are returned by Roshi\" the answer is \"it depends\". As I recall, SoundCloud does both. In some situations, there is another service in front of Roshi, which creates object representations based on the Roshi IDs. In other situations, that work is deferred to the browser, with the client JS making other requests to specific API endpoints to populate additional information.\n\nShouldnt running select.go yield the same order every time ? What am i missing ?\n\nYou're creating 50k independent keys. But timestamp order is preserved only within a key. To get the behavior you want, use the same key for each insert.\n. If you select multiple keys, you will get multiple independent lists of values, in score order. If you want to merge them together into a single list, you will have to walk each list and combine them. If you use roshi-server, there is a coalesce parameter which will do this for you. Does that answer the question?\n. Can you write a small comment on what this block accomplishes?\n. Is this a data race with L95, below?\n. Is this a data race with L75, above?\n. ~~math.MaxInt32 or whatever's called for~~\nedit: read, then comment.\n. We do have an explicit notion of quorum: https://github.com/soundcloud/roshi/blob/master/farm/farm.go#L112\nIn fact, Roshi implements CRDT semantics iff writes return as successful when they've been acknowledged by a quorum of Redis instances, defined precisely as (N/2)+1.\n. Yes! Thanks.\n. Excellent idea.\n. Duplication from merge?\n. Can everything except the chans be parameters to the ratePolice loop function?\n. Ah, but did you instrument the added GC pressure? ;)\n(Just kidding, in case it's not obvious.)\n. It might be interesting to emit a \"throttled\" metric (counter) via instrumentation here.\n. Good catch! Thanks!\n. It's declarative, there's no runtime cost AFAIK. I'll see if I can't find evidence of that...\n. It's a property of the implementation rather than the spec, but the Go compiler resolves all type declarations at compile-time. (It can even inline them, if the container function is small enough.)\nCompare do bodies, here: https://gist.github.com/peterbourgon/9251445\n\n14:42 <+Aram> types are nothing but size and offset information the compiler uses to lay out code.\n14:42 <+Aram> so using only logic you can deduce that where you define them doesn't matter at runtime.\n. :dancer: \n. Agree. Thanks for taking a look!\n. \n",
    "xla": ":+1: :shipit: :octocat: :octopus: \n. With the latest adjustment in peterbourgon/g2s#7 the only thing we have to do is to update the vendored dependency.\n. ",
    "beorn7": "@peterbourgon Please meet your new friend, the rate police... :)\n. I have actually moved ratepolice out of the farm package after I had done the basic implementation, when I had realized that it is pretty generic and might very well be moved out of roshi as a whole (which, should it happen) is easier if it is already an own package...)\n. Having ratepolice in a separate package conveys the message that ratepolice has no dependency or knowledge of any farm internals. Also, it allows nice idioms like a New() function.\nBut I'm not going to fight about it, so moved it back into farm.\n. Merging now so that I can put it properly on Bazooka...\n. Very nice. And I don't mind dirty histories...\nLGTM\n. It's not a fix to prefer the old operation:\nNode1: A() R() + insert(foo,123) = A(foo,123) R() + delete(foo,123) = A(foo, 123) R()\nNode2: A() R() + delete(foo,123) = A() R(foo,123) + insert(foo,123) = A() R(foo, 123)\nMy suggestion is to prioritize delete over insert (or vice versa):\nNode1: A() R() + insert(foo,123) = A(foo,123) R() + delete(foo,123) = A() R(foo, 123)\nNode2: A() R() + delete(foo,123) = A() R(foo,123) + insert(foo,123) = A() R(foo, 123)\n. client_golang is merged now. I'd like to merge this ASAP.\n. :+1: \n. @pxue Thanks for reporting.\n@peterbourgon Can you still read this? Are you planning to maintain Roshi and deal with issues like this one? Which would raise the question about merge rights. The repo is public, but I guess only SC employees can merge PRs. I could proxy you for the time being so that we don't need to move the repo. What do you think?\n. Alright. Then @peterbourgon will fix this one, I'll merge his PR, and if anything comes up in the future, we'll take a raincheck. The repo stays where it is for as long as things work out.\n. Thanks.\n. How about periods at the end of those lines?\n. Perhaps drop the (N/2 + 1). It adds little value but could be misunderstood. (Integer division is (only) implied. One might read it as defining \"half\" instead of \"the minimum value that is more than half\".)\nA different consideration: Many readers will probably jump to conclusions when they head \"more than half\". It will trigger the idea that Roshi has a notion of quorum. But it hasn't. Perhaps that should be mentioned here. The \"more than half\" is pretty arbitrary as we don't require quorum. In a 4x replicated cluster (or even more), it might still be very reasonable to declare success when a write makes it to two out of four (or more) replicas... (which is something we still need to implement, I guess...)\n. Here we have \"quorum\" mentioned explicitly. I think we should implement a configuration options for write success. (\"more than half\" might be one of them, but there should be other options like \"at least n replicas\" or \"x% of replicas\"). But even without that, I'd avoid the magic \"quorum\" word because it has so many connotations with readers experienced in distributed computing.\n. See my other remarks about \"quorum\". Perhaps it's fine to use it if we clearly define somewhere what it means and that there is no notion of a quorum for data correctness in Roshi.\n. You probable mean member B instead of C in the last example.\n. Any incompletely applied delete will have the chance of be present in the reads. There is no notion of a quorum in the game. A single replica that didn't receive the delete will be able to mix-in its not-yet-deleted element.\n(I see many readers wondering here why we implemented it that way. Perhaps a short justification is in order... along the lines: \"In that way, we avoid reading from the remove set for every read, effectively halving the cost of the read queries.\")\n. We probably should mention that even with this strategies, you might get inconsistencies if a delete didn't make it to all replicas (see above). If a delete made it to a majority of replicas, but not to all, ironically  SendOneReadOne has a higher probability of returning the correct result.\nBUT of course SendAllReadAll will find all the inconsistencies and issue read-repairs, so next time (if the repair made it through) everything is well...\n. Perhaps add a note about the practical merits of running Roshi instances on the same machines as the Redis instances (orthogonal resource requirements: Redis much RAM, little CPU, Roshi vice versa).\n. Done.\n. It's not, see below. (But this code is gone now, anyway.)\n. It's not a race because this here happens during construction while L75 is only called after construction. (But this code is anyway gone.)\n. Oops... fixed.\n. Good idea. Done.\n. I actually wrote a go benchmark test (which in my understanding includes GC overhead).\n. Done.\n. I think this still doesn't nail it.\nPrior to this change, the modification coming in later will win.\nNow the modification coming in earlier will win.\nI think to solve the issue, you have to prioritize delete over insert (or vice versa):\nif insertTs and tonumber(ARGV[1]) <= tonumber(insertTs) then\n        return -1\n    elseif deleteTs and tonumber(ARGV[1]) < tonumber(deleteTs) then\n        return -1\nNow deletes with the same timestamp will always win, and CRDT semantics is restored.\n. ",
    "tsenart": "So this is how you're doing dependencies versioning... I like it.\n. :+1:\n. Agreed. I think this is true for pagination in any database as long as the chosen cursor is well indexed. As an example, MySQL OFFSET, LIMIT pagination needs to perform full table scans while BETWEEN with PK as a cursor is very efficient.\n. As per documentation, unlike client requests, closing the body of requests is not necessary on servers.\n. Unless I'm missing something, yes, it is redundant.\n. You should put the tokens back in the bucket if you're not \"using\" them. This way you're leaking tokens.\ngo\nfunc (p tokenBucketPermitter) canHas(n int64) bool {\n    if got := p.Bucket.Take(n); got != n {\n        p.Bucket.Put(got)\n        return false\n    }\n    return true\n}\n. I have been always reticent to create types inside non-test functions. Do you know how these work at a lower level? I am interested to perhaps bust my myth.\n. Runtime cost is what the little voice inside my head was poking on. Perhaps a golang-nuts thread on this matter would enlighten us all.\n. Running these benchmarks hints that there is no runtime cost to it. http://play.golang.org/p/Zei6N7TGes\n. Great! Thanks for the research. :)\n. Since you're now defining a properly bounded range, the following could make this signature and downstream related code more obvious.\ngo\n// Selecter defines the methods to retrieve elements from a sorted set.\ntype Selecter interface {\n    // ...\n    SelectCursor(keys []string, start, stop common.Cursor, limit int) <-chan Element\n}\n. In fact, the method would be better named SelectRange in this case.\n. You're very welcome :-)\n. ",
    "thedrow": "Very poorly? That's surprising. Isn't it on @twitter's production servers?\n. ",
    "russelldb": "Yes, so you can remove elements that aren't present, and doing so with a very high time stamp will drop adds with a lower time stamp, what I think the cassandra community call \"doomstones.\"\nGot it, many thanks for answering the question. I ask as I am making an LWW-Element-Set for Riak and thought I'd use your semantic.\nMany thanks\n. I have a further question, please?\nIn State based CRDTs there is a merge function to get a single value from divergent CRDTs. In a Roshi LWW Element Set, given what you said above, what is the outcome if two sets merge where\nSet A = Add(a,0), Rem()\nSet B = Add(), Rem(a, 0)\nFrom the table in your README it would seem that the Add wins?\nA(a,1) R() remove(a,1) A(a,1) R()\nBut also remove wins?\nA() R(a,1) add(a,1)    A() R(a,1)\nIn the case of this two original states merging, what is the value of the Set? Say Line one is replica X and line two is replica Y, what happens when X LUB Y? Is it A(a, 1)R() or A(), R(a, 1) or something else?\n. Hey, you updated your reply as I was writing to you to describe the divergent behaviour you then described! Excellent.\nI went for the same semantic (favour adds when add and remove timestamp are the same and both occur when merging divergent views of the data.)\nThis is has been a very valuable conversation for me, many thanks for taking the time!\n. ",
    "bernerdschaefer": ":+1: \nDid you try pulling fmt.Sprint(start.Score) out of pipelineRangeByScore's inner loop, too?\n. ",
    "juliusv": "Looking good, pending @peterbourgon's :+1: \n. ",
    "stuartnelson3": "so https://github.com/soundcloud/roshi/blob/close-request-bodies/roshi-server/main.go#L210 is unnecessary?\n. ",
    "pkieltyka": "awesome! thanks\n. ",
    "grobie": "Should we just remove support for go1.2 and rebase? Happy to do that.\n. :+1:\n. ",
    "lundin": "Hi Peter, thanks for the answers. Ok i see that makes sense indeed.\nAnd for question 2 i think i follow you and now i can see that the entries get sorted within the key correctly but i still have a question. \n\nYou're creating 50k independent keys. But timestamp order is preserved only within a key. To get  the behavior you want, use the same key for each insert.\n\nSo assume multiple sources/users (lets call them the keyID) creates activities which then gets inserted into the db. If i run my insert program twice and only change the member we have the same key twice, eg. item4 that has an activity with different timestamp and member.\nIf i then run select.go which picks 20 keys twice i get different sorted lists (but each key has correct ordered timestamp within themselves)..but to construct the overall feed i have to sort them according to some logic so i can show the very latest even in-between the keys, right ?\nitem11\nitem12\nitem15\nitem16\nitem19\nitem18\nitem5\nitem6\nitem8\nitem9\nitem10\nitem13\nitem2\nitem7\nitem0\nitem1\nitem14\nitem17\nitem3\nitem4\nand\nitem15\nitem5\nitem18\nitem2\nitem6\nitem7\nitem1\nitem13\nitem19\nitem3\nitem9\nitem4\nitem0\nitem10\nitem11\nitem12\nitem14\nitem16\nitem17\nitem8\n. Thanks! That made it, cool. Again thank for your time Peter! \n. "
}
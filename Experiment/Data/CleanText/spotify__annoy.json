{
    "a1k0n": "man this is annoying, i ran into this again today. i should fix this.\n. i am!  one moment!\n. That's very strange; could you share the code you're using to generate and to use the indexes? When does it run out of memory -- when creating the index or after saving it and using it for ANN lookups?\nHow much memory does it use in your small test? It shouldn't be very much if it's only 500 items, even if they're 128 dimensional items.\nAnnoy definitely works under Linux (Debian or Ubuntu is what we use) and I thought it worked fine on OS X as well.\n. the actual package dependency needs to be libboost-python which is different for different debian/ubuntu distros so I'm going with libboost-python-dev for now, but that isn't quite right either.\n. Ok, this should work now.  PTAL.\n```\nPython 2.7.5 (default, Mar  9 2014, 22:15:05)\n[GCC 4.2.1 Compatible Apple LLVM 5.0 (clang-500.0.68)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport annoy\nannoy.AnnoyIndex(20).load(\"lkdjsfkljf\")\nTraceback (most recent call last):\n  File \"\", line 1, in \nIOError: [Errno 2] No such file or directory\nannoy.AnnoyIndex(20).save(\"/idiot.tree\")\nTraceback (most recent call last):\n  File \"\", line 1, in \nIOError: [Errno 13] Permission denied\n```\n. this is also just good style, thank you\n. Hmm, and actually the asymmetric LSH stuff doesn't even have to affect the query algorithm at all, only the tree building phase, and so it actually only affects the way the tree nodes are split; the extra two dimensions don't need to be stored.\n. Well, you have to keep in mind that the magnitudes are all scaled to be < 0.75 first, so the last two components are < 0.5 and > -0.06 or so.\n\n\n\nWait, I think I see what you're saying, but I don't see why you wouldn't be able to find a split between two points. The last two \"fake\" coordinates give you a lever to split points which differ by magnitude only.\n. ...but then those buckets would be useless on the query side, where those coordinates are 0. Hmm.\n. If AI and EI were subclasses with virtual member functions, you could save a whole lot of dispatch code here.\n...but I can see why you didn't do that, because of the template headaches involved. Hmm.\n. :+1:\n. I also just realized I was running .save() on a tree that I had previously .load()ed, not the one I was trying to save. So, different problem.\n. I'm just relaying a request from your annoy-users@ mailing list. Are you on it?\n. Yeah, that makes sense. More compact in memory too. Cool, thanks Erik.\n. Or just store an indirect pointer alongside the item vectors, and have it point to a separate variable-sized chunk of memory after the vectors but before the tree nodes.\n. Oh, I see, Chris is asking about general keys. Yeah, then it needs to be pretty much like a sparkey hash.\n. Yeah, that makes a lot of sense.\nI mean, you could also take a random direction and then compute the dot product on all points in the leaf and find the median, and do that over and over, but for angular distance you'd have to introduce a bias coefficient to make that work and I don't know if that destroys the cosine distance guarantees you get with signed random projections. It would be better for the euclidean one though.\nThis is a really neat idea though.\n. That method still picks a uniformly random direction; the points x and y are only used to find the split plane along the direction, which annoy doesn't really support -- the split plane is always at the point where the dot product is 0.\n. @erikbern of course. Yes, this might make sense for the Euclidean method. I don't follow the argument in the paper yet but it's very interesting. Thanks @stefansavev \n. That is super awesome, though.\n. try git rebase origin/master?\n. @zhdeath can you post a snippet of code of how you're generating your annoy index, and how you're loading and using it?\nthe return value of load() isn't really of much consequence is it? I think it got changed to return 0 instead of true inadvertently to be more similar to POSIX return values, but it should still work.\n. hah! that explains a lot.\n. :-1: needs Kool-Aid man bursting through main graphic\n. I think you need to use with open('README.rst', encoding='utf-8') as fobj: -- the default encoding for open is ascii. Don't know why Python 3.x blows up and 2.x doesn't...\n. @erikbern: to fix your PR in a python 2.x-compatible way you have to import codecs. codecs.open('README.rst', encoding='utf-8') should work then.\n. Awesome.\n. For the angular split planes, certainly. I've always been advocating for this :)\n. Well, if you start to quantize the input elements, you can no longer compute exact distances from your query to the points in the dataset; for an ANN library, I guess that's OK.\nRescaling everything from -128..127 in the angular domain is all fine with me, but I was concerned about what happens in Euclidean space. Maybe it's fine, though. A pathological dataset with a relatively huge diameter but with relatively small clusters might be badly served by that, but on second thought it's probably fine for any reasonable dataset.\n. Yeah, exactly... since leaf size must be == vector size (which is sort of an odd tradeoff) your leaves are 1/4th the size. Can those be made independent? You don't really need them to be equal-sized in memory, but the indexing would be a bit more involved.\n. Huh, that's true. A tree is a Voronoi diagram then, effectively. So you could use Hamming distance too.... which I actually have a use for, right now. Neat!\n. And actually, then you don't have to store the split planes at all, only pointers to the left/right tree nodes. Which again breaks your fixed tree node size constraint unless leaves are tiny, but if you had dynamic leaf node sizes then the whole tree would be a lot smaller. \nIf you do that then it would also make sense to reorder the indices so that the roots and interior tree nodes are first, etc, for memory locality purposes.\nAnother idea I had was to just store the points in the interior nodes and not in the leaves, and push them onto the priority queue while traversing the tree. But that doesn't really help when you have multiple trees and the same points which are interior in one tree end up in a leaf of another tree.\n. Oh yeah, I hadn't considered hamming as a special case of euclidean. Still, it would be much more compactly represented as bits rather than 0/0.5/1 floats :)\nI did see that, yeah. That could enable all kinds of neat stuff.\n. Huh. Shouldn't make much difference, but OK!\n. Ah, of course. LGTM.\n. Oh, there are mixed tabs and spaces, which explains it. Heh.\nAnyways, that is awesome. And sort of obvious in retrospect.\nWith 5 attempts and 5 iterations each, are the various trees in the forest still sufficiently different? I don't know how to quantify that other than maybe seeing an accuracy curve of using 1 up to N trees when querying...\n. I dunno, the small world graph type algos have pretty convincing results...\n. You still aren't updating best_d_sum on line 134 (apparently github mobile doesn't let me comment on lines directly)\n. LGTM now. this is rad.\n. Nice, that makes a lot of sense. Looks good.\n. @erikbern Erik, are you going to merge this? It seems like a good idea and it seems to work better..?\n. :rocket: \n. This might have been the fopen mode \"wb\" fix that went in a little while\nback.. when did you last update pypi @erikbern?\nOn Fri, Jul 1, 2016, 9:53 AM Erik Bernhardsson notifications@github.com\nwrote:\n\nnice!\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/spotify/annoy/issues/130#issuecomment-229967621, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe/AAC0WtXI4K3-MzQ4OJ-DOJovNqpWFOosks5qRSnFgaJpZM4HIeIS\n.\n. Erik I think you just need to upload the latest version to pypi. Or does\nthat happen automatically somehow?\n\nOn Fri, Jul 1, 2016, 10:27 AM Erik Bernhardsson notifications@github.com\nwrote:\n\nfigure out what the issue is and suggest a solution :)\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/spotify/annoy/issues/130#issuecomment-229976128, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe/AAC0WoAdjoHSbsU-OQkZWdmZpG8nPbFeks5qRTHLgaJpZM4HIeIS\n.\n. Whoa. Is one of the roots being added twice, or is the scan just seeing it twice somehow?\n. Hm. I don't understand how... but that code isn't the clearest...\n. now it's on the stack, not the heap. it was in bss. Either way if it was segfaulting then something isn't right...\n\nHave you tried valgrind?\nedit: actually either way the actual data would go on the heap. the vector struct is all that changed; plus it gets reinitialized at every call now. Hmm.\n. @jfemiani did you ever get a chance to try it?\n. All right, I'll make a PR\nOn Wed, Apr 20, 2016, 5:10 PM jfemiani notifications@github.com wrote:\n\nYes it worked.\nOn Apr 18, 2016 2:36 PM, \"Andy Sloane\" notifications@github.com wrote:\n\n@jfemiani https://github.com/jfemiani did you ever get a chance to try\nit?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/issues/144#issuecomment-211593100\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/issues/144#issuecomment-212660123\n. Isn't that paper pretty much exactly what annoy is already doing? It even\nuses a heap as a priority queue. The construction method is the same as\nwell.\n\nOn Tue, May 17, 2016 at 11:58 AM Erik Bernhardsson notifications@github.com\nwrote:\n\ninteresting \u2013 might be worth experimenting with\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/issues/148#issuecomment-219818963\n. Hmm, the go build is busted:\n\nE: Some index files failed to download. They have been ignored, or old ones used instead.\nERROR: InvocationError: '/usr/bin/sudo apt-get update -qq'\n___________________________________ summary ____________________________________\nERROR:   go: commands failed\n. still, might be good to catch the error and raise an exception or something instead of crashing...\n. Yeah, with no header on the format, I guess it's doomed to be error-prone...\nI can envision a backward-compatible way to introduce one, though, since it loads indexes from the end rather than the beginning of the file..\n. if p is equal to the length of a vector, then vec[p] is out of bounds.\nedit: and m is always out of bounds, here. so yeah, this makes sense to me even though it's pretty weird code.\nthe could would be a lot less weird if it used nns_dist.begin() and nns_dist.end() and so forth instead. . If you have an STL that does bounds checking (Microsoft compiler maybe?)\nthen vector::operator[] will throw an exception on that line.\nOn Sun, Feb 26, 2017, 11:34 AM Erik Bernhardsson notifications@github.com\nwrote:\n\nI still don't understand what bug this solves. Can you send a reproducible\ncase?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/spotify/annoy/pull/186#issuecomment-282580637, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAC0WkeXqNrj_7gBVrMGJKRrvZ6lsxzpks5rgdPEgaJpZM4MMErg\n.\n. I was suggesting keeping the API to set the seed, and just pass it to the RNG... but I see now that it's part of the constructor, so that might be tricky. @erikbern thoughts?. Yeah, it would be a nice property if two speculatively executed tasks ended up with the same result.. Or use the RNG someone already contributed? Or boost::random since it's\nalready using boost...\n\nOn Mon, Mar 6, 2017, 7:33 PM Erik Bernhardsson notifications@github.com\nwrote:\n\ni'm not sure if it's ideal to rely on rand/srand \u2013 it's possible that\nnumpy uses it somewhere under the hood as well. set_seed uses a global\nstate which is pretty bad. we should probably replace it with a local state\nRNG like using ? @yonromai https://github.com/yonromai is it\nsomething you could take a look at?\nhttp://www.cplusplus.com/reference/random/\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/spotify/annoy/issues/188#issuecomment-284611924, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAC0WsSx9ZBmtj1TyME8BNmW3_NXBwZzks5rjNARgaJpZM4MS0Yq\n.\n. Yes, it's (size_t) -10, which is going to be a large positive number. Effective you're doing brute force search of the entire thing, which is why you get better results.. That's weird. Are the \"hole\" nodes all zero vectors in both versions if you query them directly?. What I meant by my comment is that the holes might actually be uninitialized memory, and not actually zero vectors. You could be getting junk data in the new version but zeros in the old, or something. I'm suggesting you actually check the index files to see.\n. Actually yeah, nevermind. add_item calls _allocate_size which does a memset, but even if it didn't, there's no API to just add a \"hole\"... you have to add all the vectors in order, so unused items will have whatever vectors you give them... @yonromai what is your _holes_* code doing?. I'm guessing the underlying cause of this, however, is finding the split vector based on a random pair, and if both are zero vectors, the split is the NaN vector. So maybe the splitter could also check that the distance between random vectors is > eps?. This might also explain the observations in #223 -- that newer annoy is less robust to zero vectors, because instead of random splits, it splits based on random input vector pairs, and that could be an invalid split in the presence of \"bad\" inputs.. Would be good to try it on the dot product (for cosine distance, and for every single plane test when traversing the tree) also, if it's not already auto-vectorized...\n\n. :+1: amazing! Nice work!. Effectively you have to ask for a bunch of extra items, and then filter them down using your external metadata. AFAIK this is still the same Annoy used internally at Spotify -- in that case it's usually an item blacklist (e.g. stuff the user is already quite familiar with).\nIf you already have a full subset of feasible items wouldn't it make sense to just exhaustively rank them and use a top-k heap instead of Annoy?. Ah, I see. Yeah, that sort of thing would have to be engineered in concert with the ANN indexing scheme and vastly complicate the file format.\nOne feature which might improve this situation is if Annoy had an iterator API; instead of asking for a certain number of items, you would just iterate the stream of potentially all items, ranked. Then you'd just walk the iterator, checking your metadata for each match -- if this were implemented internally in annoy, that's what it would be doing anyway. This is slightly problematic though because it's not clear how search_k would fit into this.\nOr, if you have a small number of tags, make separate indexes for each tag? Doesn't help if you want exactly K items matching a union or intersection query though; you'll have to ask for extra items then too.. The major issue with that is that the iterator won't be guaranteed to return results in order of best match (since it's an approximate nearest neighbor search), and it will require O(k log k) memory for k results as it needs to de-duplicate them. Overall that's not too bad, I guess, but it won't be as clean to use as the existing API -- if you want to get results as good as the normal API call, you have to iterate quite a bit further than the number of results you actually want and discard the worst matches.\n. The size of the index is directly proportional to the dimensionality, with basically no padding IIRC. So 10 separate 10-dimensional indexes should be pretty close to exactly the same size as one 100-dimensional index... so what do you gain by combining them?\n. I guess I misread -- you still want to search along multiple axes simultaneously, so if the indexes were separate you'd have to take a union or something. I see. Hmm.\n. I'm guessing this is because the leaf node is defined to be the same size in bytes as a split vector, and there isn't room to actually store any leaves in the 1d case?\nLuckily, 1-dimensional spaces are very easy to search without Annoy :)\n. Oh, right, never mind!\nI misremembered how the structure is laid out in memory -- i thought you had multiple vectors stored in leaves, but I can see it's only one where instead of the split plane it's the leaf node's vector. And yeah, there's a different node size for euclidean vs. angular because of the extra split offset.\n. I think he means this? https://github.com/spotify/annoy/blob/master/examples/precision_test.cpp\n. If the underlying file is deleted (unlinked), then the contents of the file are actually preserved on the filesystem until all handles to the file are closed. So it's completely safe to do that, and any existing references to the file will continue on as if nothing happened in the background.\nIf the underlying file is overwritten (recreated with O_TRUNC), then the contents of the file are replaced and the length of the file becomes 0, then however big it's rewritten to. The mmap region becomes invalid, hence the SIGBUS -- references within the old file point outside the virtual address space of the new file. So when you overwrite with a smaller index, references to data outside the length of the index cause a SIGBUS; even if you overwrite with a larger index, you will end up with invalid data on the existing process.\nSo the upshot is if you want to safely replace an index, write to a tmp location, unlink the old one, then rename the new one into place. Existing references to the old one will be none the wiser until they reload.. The warning\nwarning: include path for stdlibc++ headers not found\nhints at the root of the problem; vector is a basic C++ standard library header; for some reason, whatever compiler pip install is using doesn't have a C++ compiler properly set up. Hard to say; is there homebrew on that system? which python/pip is it using? which compiler? is gcc an alias for clang or is gcc actually installed somehow?\nAnyway it's definitely not something annoy can fix.. not (sizeof(T) * f + sizeof(S) * 2) / sizeof(S) ?\n. It should be safe to use PyErr_SetFromErrno(PyExc_IOError); here, as errno should be preserved from the load_py call. I think.\n. would you mind expanding the tabs to spaces above here?\n. PyErr_SetFromErrno(PyExc_IOError); here too, would be more informative.\n. fwiw drand48() is a better random number generator and should be widely compatible but... this is fine.\n. what if i == j?\n. what if i == j?\nyou'll have to sample nodes.size()-1 for j, and if(j >= i) j++;\n. Oh, right, that's true.\nYou could also enforce slightly more rigorous splits, i.e. at least 80%/20%, yielding better accuracy at the cost of taking longer to build the tree.\n. Chris: that makes sense for the euclidean case but it's less clear how it works in the angular case. But you're right, the las vegas algorithm would be much, much more costly unless you subsample first. Hmm.\n. I thought of that too but how do you get the principal component? You either need to store a d x d matrix or do power iterations I guess?\n. you'd start with an initial guess, maybe ones(d), and repeatedly multiply it by the covariance matrix from summing up the outer products of all the vectors, which is the same as adding up the dot product of the guess with the outer product of each vector with itself... here: http://www.texpaste.com/n/7ao61x20\n. But if you subsample different points each time you either converge to the optimum split plane, which means your different random trees wouldn't be so random anymore, or you don't converge at all... I think the benefit from the random forest comes from sampling a specific number of points, and as Chris says, you might as well just keep a small sample, and maybe that sample being 2 points is good enough anyway.\n. you have a weird 1-space indent here and above\n. and here...\n. Also, you don't set best_d_sum here... so how does this work?\n. ah i see, you sample 100 random nodes here, so there could be a bit of variance here...\n. another tab indent here\ni recommend :se et, :retab\n. do you get better results with this in? :)\n. do you need to include the parameterized types in <>s here for it to compile? two_means(nodes, f, ...) doesn't work?\n. oh, you do because it can't infer S. why isn't S just size_t?\n. yeah you aren't even using S at all here, since k is size_t...\n. ah, it can't infer Distance either, and that's a tougher nut to crack, unless you make two_means a static member of the class instead of having it as a global function.\n. and that doesn't really help either. so I think you should just get rid of S from two_means, and leave the other arguments in the call.\n. Wow, I've never seen template used in this construct. What does it mean here?\n. sounds good to me.\n. argh, was logged into the wrong one on my phone, haha. ignore that!\n. You could use boost::thread, maybe, since it's already using boost python?. this needs to be protected by a mutex.\n. Oh, that's right... I forgot. Hm.\n. well, then stick with c++11 then, since std::thread was standardized in c++11?. Yeah, if the various components of x and y are already loaded in registers, seems like you might want to roll them up while you have them.\nRun a benchmark and see!. ah, so you have space for a norm where the children pointers go in the case of a leaf node?\n. ",
    "erikbern": "Stop complaining and send a PR!!\n. Hm, no idea what's going on. Seems like the compilation issue could be fixed by adding \"this->\" in a couple of places. I will fix that and push a new version.\nBut it's still weird that the unit tests fail. Maybe it's an alignment problem on a specific processor? I think @a1k0n pointed out at some point there could be issues if the offset of a node isn't on a multiple of 4 or 8 on some architectures. I wonder if removing the \"packed\" attribute would help.\n. Pushed a fix for the compilation issues, and put the packed structs behind a macro. Would be super helpful if you can try to compile using -DNO_PACKED_STRUCTS and see if the unit tests are still failing.\n. Yeah I'm pretty sure those two issues were unrelated.\nI ran it on a micro instance on EC2 and the unit tests succeed so I'm not sure what's going wrong. I might try to set up an beefy EC2 instance and try later. If it's broken I'll try with -DNO_PACKED_STRUCTS to see if that resolves the issue\n. I assume you didn't use windows right? Looks like something changed in 4.7: http://www.bttr-software.de/forum/mix_entry.php?id=11767\n. See #24 \n. Compilation issues were fixed a long time ago \u2013\u00a0the unit test issues were fixed in #24 \n. Fixed in #80 \n. Cool. The problem is a lot of it is done in compile-time but it should be possible to refactor.\nFeel free to add support for other languages if you know and we can figure out how to add it\n. Yeah, you also need to include the entire thing, so we should probably rename it annoylib.h instead\n. Not planning to push anything, but feel free to submit a pull request :)\n. We should probably couple Annoy with a hashtable so that you can put in arbitrary stuff. But yeah, until then, you will see those issues :(\n. I'm not sure if there is a fix to this \u2013\u00a0what do you suggest?\n. There's a lot of file-based fast hashtables, including Spotify's own: https://github.com/spotify/sparkey \u2013\u00a0other ones I'm aware of are bsddb and Tokyo Cabinet.\n. Closing this issue since it's not a \"bug\" per se \u2013\u00a0maybe the documentation should be more clear\n. oops, i'll fix. thanks for reporting this!\n. Try now!\n. Great!\n. I'll take a look at it.\n. I set up travis integration hoping it would catch this issue but it doesn't.\nWill try a EC2 machine as suggested in #3 \n. Was finally able to repro the issue on a EC2 instance (m3.medium, Ubuntu 14.04) which is great \u2013\u00a0will try to figure out what's going on\n. Great! I'm digging through it. It's some really obscure thing going on \u2013\u00a0as soon as I added debug printf statements, the unit tests suddenly pass :)\n. It's failing on line 386 somehow:\npython\n    if (indices.size() <= (size_t)_K) {\n      printf(\"make tree has only %d indices\\n\", indices.size());\n      for (size_t i = 0; i < indices.size(); i++) {\n        // printf(\"index: %d\\n\", indices[i]);                                                                                                                                                                                                 \n        m->children[i] = indices[i];\n      }\n      return item;\n    }\nI think there's something strange with how indices are passed by reference but my C++ is a bit rusty so I'm not sure what's going on. With the second debug printf statement activate, the tests are passing.\nLet me know if you have any thoughts.\n. See #24 \n. Yeah, what I think happened was that since m->children wasn't being used, it optimized out the whole loop copying indices to m->children. But it's weird because m is a pointer to an object that could be used elsewhere, so I'm not sure why GCC thinks writing to m->children has no side effects. In other words I think this is actually some crazy compiler bug we ran into.\n. That's loco. I'll clone your R package and try. Maybe the compiler is even smarter than I thought. If std::copy is an inlined function I could see how the compiler still removes that statement. I can  switch to memcpy instead and try\n. I'm guessing the flags you used to compile it for the R package included even harder optimization. Will try to repro building the R package\n. I'm running the R code now but there's actually something else going on. It seems like the same bug is happening again in another place. _get_all_nns: https://github.com/spotify/annoy/blob/master/src/annoylib.h#L507\nJust adding a REprintf statement resolves the issue. So I'm going to rewrite it to avoid the explicit loop\n. It's crazy that the compiler behaves this way. But I'm abusing C++ so I guess I was asking for it\n. What do you mean with the indices set to be overwritten? There's some abuse of how C happily lets you access arrays out of bounds etc. Maybe I could have avoided some of the more crazy stuff but it seemed to work until now :(\n. Merged #26 now which resolves the issue I saw when compiling the R package (when I did the same change in your repo). Hope this finally solves it!\n. ... although I still feel like my changes are pure voodoo magic. Would be much nicer to understand what's going on.\n. Np. Was planning to have some Sunday hacking anyway. Btw see #27 \n. nice!\n. Nice!\nI haven't been able to repro the issue (#13) unfortunately. It seems like it happens on certain architecture or something like that.\n. See #25 if it makes sense\n. Sure, or you could also just put all the output within an #ifdef and simply turn it off. It probably makes more sense that the default behavior is no output\n. Sounds good to me!\n. Unfortunately I can't get this working with Python 3. I get an issue: undefined symbol: PyClass_Type (see https://travis-ci.org/spotify/annoy/jobs/40478879 for more info). It seems like linking against boost_python with Python 2.7, but I haven't been able to figure out how to get boost_python4 to be installed\n. There's some build issue that's probably was introduced in #20, let me resolve it separately\n. Btw saw some flakiness in Travis, should fix it at some point\n. I think it's possible to fix it by assigning it a length of 1 and then subtracting sizeof(T) when computing _s. Or something like that. A little ugly though. Should be easy to try now that I added a unit test for binary compatibility\n. Something like that should work. Note that T v[1] will end up taking memory so you need to account for that when doing sizeof(Distance::node).\nI tried a couple of changes but for some reason they broke the unit tests. Not sure why. \n. I realized I forgot to change it in both distance functions, that's why.  See #29\n. Running python setup.py nosetests generate a ton of warnings about implicit conversions between int and size_t, but I'll fix that later\n. I just submitted #30 this morning which might also do the trick.  Also agree there should be a verbosity member variable. I'll merge your PR but will put it behind a flag that defaults to false. Will abandon my PR.\nI have a minor preference towards all-uppercase macros, and there's a minor spelling mistake in the comment, \"onw\" vs \"own\". Overall LGTM though\n. LGTM = looks good to me. I've just seen that within code review systems.\n. Let's merge it as it is. I'll see if I have time to add the verbosity flag and the int32_t stuff at some point next few days\n. This commit (a) generalizes index variables to be any arbitrary type that you can pass as a template argument (b) sets the type to int32_t for the Python bindings rather than int (since the latter could potentially be a 64-bit integer on certain platforms). This should make indexes binary compatible between 32 and 64 bit machines. We still have an issue with endianness that I'm not intending to address though\n. Btw of course this is issue number 32 by coincidence :)\n. Yeah, I will update the precision test to run multiple times and average, or just relax the criterion a bit. Also noticed the unit test fails about 25% of the time\n. Thanks!\n. Yeah, not a big thing. I think ideally it would be a boost::lambda (or python lambda) passed in run-time so we don't have to mess around with preprocessor macros. But not top prio to fix\n. Ok, would be useful for Windows users\nFor int32_t did you see #32?\n. See #36 \n. I don't get any warnings \u2013 can you send me your compiler flags?\n. I don't have GCC 4.8 installed, but I'll just merge this for now. Don't want to be too picky with it\n. I'm a bit cautious of unsigned integers, since I've had issues in the past. It's usually good to have negative numbers signifying some kind of error\n. Awesome!!!\n. thanks!\n. I never used Euclidean distance much, so the code isn't very optimized. There's some hacky stuff in create_split that's really slow for Euclidean, and it's probably not super hard to optimize if you have some time to spend on it: https://github.com/spotify/annoy/blob/master/src/annoylib.h#L193\n. See #65 \u2013\u00a0this makes Euclidean indexes substantially faster\n. Let me take a look at this in the next few days\n. On #1: I think the key feature of Annoy is that it uses mmap rather than allocates memory from the heap. This will not change. So please if you can figure out another way around it. One way is to keep the original chunk in mmap and use heap memory for any new nodes.\n. On #2: You don't have to move the root nodes at the end of the index. It's nice if you do. But they only take up a very small amount of memory.\n. On #3: This breaks backwards compatibility (that's probably why the unit test fails) so can you figure out a way around adding the \"parent\" field? I don't see it being used in many places so I'm not sure how it works \n. This looks pretty cool overall. I really want to keep mmap and avoid adding the \"parent\" field.\nThere's also a lot of commits in this branch. Seems like only the first one is relevant for the PR. Can you remove the other commits for now? We can deal with them in a subsequent PR.\nI hope we can merge this \u2013 would be a great addition.\nI'm very curious about what you're using this for. Are you using it at Houzz?\n. Ok, sounds great. Maybe I'll play around with your code. I might not understand all the complexity yet.\nYou should be able to run the unit tests using nosetests\n. I want to pick this up. I might put together a separate PR and borrow a bit from this code. Incremental adds would be a great new feature.\n. I have been thinking about that case too. I think it's not too hard to fix\nactually. Unfortunately mmap is not possible to resize, but you could\nimagine splitting the memory in two slots where the first one is mmap and\nsecond is dynamic. Then spill the second one to disk when needed. Or\nsomething like that.\nI never heard about LMDB but if it solves the issue then absolutely.\nEither way my first step is probably to implement incremental add_item\nafter build() but before save(). This way I can avoid the memory management\nissues for now.\nOn Saturday, August 15, 2015, Longbin Chen notifications@github.com wrote:\n\none problem with Annoy we encountered in our system is that when we tried\nto build a super large index, the system crashed because building indexing\nuses the physical memory, not memory-map file. Using LMDB could solve the\nproblem. Of course, we might work with memory array however it would be\nmuch harder to work and the codes would be harder to maintain.\nOn Sat, Aug 15, 2015 at 3:55 PM, Lbchen Chen lbchen@gmail.com\n<javascript:_e(%7B%7D,'cvml','lbchen@gmail.com');> wrote:\n\ni have been looking into using LMDB as the backend storage for the index,\ninstead of using a simply memory-map array. There are some pros/cons for\nusing LMDB. Downside is that there would be some memory overheads.\nHowever\ngood side is that LMDB is a well-tested well-maintained memory-map\nkey-value store, with transaction support. I am afraid if we spend a lot\nof\ntime working on the Annoy's own memory map file operations, it is\nactually\njust re-inventing the wheels of LMDB.\nThoughts?\nOn Sat, Aug 15, 2015 at 3:47 PM, Erik Bernhardsson \nnotifications@github.com\n<javascript:_e(%7B%7D,'cvml','notifications@github.com');> wrote:\n\nI want to pick this up. I might put together a separate PR and borrow a\nbit from this code. Incremental adds would be a great new feature.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/pull/42#issuecomment-131461536.\n\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/pull/42#issuecomment-131464829.\n. Read some more about LMDB. It seems interesting. I wonder what the overhead would be for int keys. But using non-integer keys has been a requested feature, so maybe it's fine incurring a small speed penalty\n. no plans\n. Can you paste the full log of python setup.py install?\n. I don't know what's going on. I think if you are in the directory of the repository, you can get issues trying to import annoylib because annoylib.so doesn't get installed in annoy/ \u2013 in that case just go to some other directory and try to import annoy, OR create a symlink from build/... to annoy/\n\nSame issue for a pip install?\n. Sounds like the first issue was just the problem with the directory\nNow you're having a problem with boost dependencies. That's really annoying \u2013\u00a0not sure what to do\n. I was able to run it a fresh macbook by running brew install boost-python and then sudo pip install annoy. Maybe you have some local installations of something that causes issues?\n. Boost no longer needed, will close this. Try again, if it's still broken I will reopen\n. @B4yesC4t what problem? boost has been removed a long time ago. Yes and it sounds pretty easy to construct a hyperplane if you sample two point from the distribution. If you sample p and q then the boundary becomes dot(p, x) = dot(q, x) which is equivalent to dot(p-q, x)=0 which means you can just compute the sign of p-q and x for any vector. The problem with this idea though is that if |p| >> |q| then almost all vectors will be on the same side of the split. I don't know if that's an issue in practice\nEDIT: now that I think about it, it's probably a pretty big deal. First you will construct splits by random angles, but then when you have a bunch of points with the same angle left in the bucket, it will be impossible to split them further\n. I meant without adding extra coordinates, just doing the split in the original space\n. nope. I looked at the code but I'm not sure if it's a bug... the heuristic n * roots.size() is very arbitrary though.\nIn your case I'm not surprised most of the trees aren't visited because you have so many trees per node. Annoy keeps a priority queue of the most promising nodes (initialized to the roots)\nIf n is much smaller than K I see why it doesn't visit all the trees though.\nEither way the solution is probably to change the n * roots.size() to something that makes more sense. maybe max(_K, n) instead of n?\n. Also you really shouldn't need 500 trees for 2000 nodes. It should be enough to have 5-10 trees\n. See #80 \u2013\u00a0can now specify at query time now many nodes to inspect!\n. That's annoying. I should enable Python 3 builds on Travis and fix issues\n. Doesn't seem like boost for Python 3 is available in apt-get\nHm I should really just drop the dependency on Boost since it's causing so many issues\n. Ok let me give it a shot when I have time (could take a few weeks/months, would love it if someone else could give it a shot)\n. Nice. I think dropping boost isn't more than a few hours though. Whole\nannoy is a few hundred lines and the boost part is just a handful lines. I\njust need a few spare hours (hard right now)\nOn Wednesday, April 8, 2015, Radim \u0158eh\u016f\u0159ek notifications@github.com wrote:\n\nI bet you would :)\nWe started an \"incubator\" thing where we assign open source improvements &\nalgos to students (a year project or thesis or whatever). Mostly gensim,\nbut annoy is a good fit too.\nThis task may be a bit too non-academic, but if someone suitably hardcore\nturns up, I'll direct them here. Would be great to have Annoy more\naccessible and noob-proof.\nCC piskvorky/gensim#51 https://github.com/piskvorky/gensim/issues/51\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/issues/46#issuecomment-90957664.\n. Getting closer \u2013 Boost dependency is now dropped, next step is Python 3 support.\n. Try now! Will re-open if broken\n. not sure if i want to add support for that to annoy \u2013\u00a0strings probably aren't enough, and then you need more complicated data structures etc\n. maybe use http://www.h5py.org/ ?\n. Did you install Boost?\n\nI'm planning to drop the dependency on Boost fairly soon. It's been causing a lot of issues for people.\nhttp://www.pyimagesearch.com/2015/04/27/installing-boost-and-boost-python-on-osx-with-homebrew/\n. If you come back in a few days the Boost dependency should be dropped.\nYes you can do that. Note however for TF-IDF if the number of terms is high the nearest neighbor quality might not be perfect. You should probably do a dimensionality reduction (using matrix factorization etc) to a representation with 10-100 dimensions at most.\n. Can you try to install from PyPI now? The newest version (1.1.0) should not depend on Boost (thanks to @dvenum)\n. Looks good.\nThe sort on line 537 sorts by the first element of each pair (the distance) which is smaller for element 1 than for element 0.\nElement 1 should have squared distance 2 and element 0 should have squared distance 3.\n. Look at https://github.com/spotify/annoy/blob/master/test/annoy_test.py#L105\nnn_dist should contain\n1 1\n2 0\n(sorry I incorrectly said 2 and 3 in my previous comment, that was not right)\n. As @a1k0n pointed out making AI and EI subclasses with virtual functions seems like the easiest way to avoid the switching and static casting.\nAnother way is to move the whole switching to compile time and compile two different python extensions. But that's a bit more complex and the performance gains are pretty irrelevant\n. I don't understand why it solves the problem but great that it works now!\n. oh ok, makes sense! of course. thanks for the explanation!\n. Ok, that's fine, I can take a look at it later.\nCan you remove the \"-std=c++11\" flag though? Travis-CI doesn't like it for some reason\n. Which lib exists only for c++11? I think #include <random> should work for older versions too\n. Ok maybe we just have to include something like this: http://phoxis.org/2013/05/04/generating-random-numbers-from-normal-distribution-in-c/\n. I can fix. Let's merge this now!\n. It was pretty easy: https://github.com/spotify/annoy/pull/52\n. nice! i will add python 3 to travis as well\n. Looks good although minor complaint is I would have preferred if you had squashed your three commits into one\n. What's the impact of the fix? If it solves a real issue I can upload a new version to pypi\n. looks like it solves a compilation issue if the rng is overridden?\n. there you go\n. 1.2.2 is out!\n. actually forgot to push setup.py to github, done now\n. yes this would be great to add as a new method. can you do it :)\n. Just realized I haven't been seeing annoy-user emails.\nI was registered using my spotify email and then when I signed up again I accidentally hit the \"don't send me updates\" option. Fixed now so I'll keep an eye on the email list going forward!\n. See #93 \u2013\u00a0should resolve this issue\n. Although instead of returning a list of tuples, I return a tuple with two lists. Made the code marginally simpler\n. No hesitations but it's probably not trivial. You would have to store a mapping that works well with mmap meaning it has to be packed in contiguous memory. Strings can have different length etc so I'm not sure how to do that easily.\nYou could probably prepend the index with something like <id1><string1>NULL<id2><string2>NULL... and binary search in that index... I don't know\n. the other solution is just put it in a separate file and use some existing hashmap format like bdb, TC, \n. yes but that only gives you mapping in one direction, what about the other direction? you need some kind of binary search or hashing to make that work\n. I guess instead of using a contiguous sequence of nodes you can treat the whole annoy index as a hashmap and use something like cuckoo hashing? that way you just need to store the mapping from nodes to keys, not the other way around.\n. I don't think Sparkey is available in PyPI so not sure if we want to depend on it\n. Sure. If you rely on Sparkey you could actually ignore the C++ part and implement this as a pure Python solution\n. I don't know either maybe a git submodule?\nIt sounds like a lot of work to get this working compared to the benefit tbh... it's not super hard to just store a hash table separately. I think this is also how the other ANN libraries work (flann, sklearn, panns, nearpy)\n. I'll think of a way to test how well it works.\n. See #65 \u2013\u00a0results are very good\n. Thanks a lot \u2013\u00a0looks pretty interesting! Here's the relevant snippet (I think)\n\nI'll implement this at some point and benchmark!\n. @a1k0n \u2013\u00a0that's only for cosine. For Euclidean, you can pick any split plane\n. This also came out from ICML \u2013\u00a0looks pretty interesting: https://izbicki.me/blog/fast-nearest-neighbor-queries-in-haskell.html (probably not related to the above discussion though)\n. I'm actually not trying separate direction for the Euclidean, but it's something I should do.\nLet me know if orthogonality helps, sounds like it could be useful!\nSeparately from the discussion \u2013 I really think the best method wouldn't be to use smart heuristics, it would be to define some loss function and optimize the whole tree for that. What you're really trying to do is you have some high dimensional distribution and you want to represent it as a tree in a way that items that are close in the first space are also close in the tree, and items that are far in the first space are far in the tree. I think it's possible to define a loss function so that you look at discordant pairs of nearest neighbors and try to optimize the tree splits after that.\n. I want to run this on a real dataset to see if it helps.\nI'm also a bit concern what happens in degenerate cases such as i==j but I think it will work out fine\n. Before \nRan 21 tests in 115.291s\nAfter\nRan 21 tests in 23.230s\n. See accuracy test that I just added: https://github.com/spotify/annoy/commit/f5c1f58df11cc156aad8e6d73f809c8bccdae7c7\nThe improvements are pretty spectacular:\nBefore\n```\n             angular   25 accuracy: 34.64%\n             angular   50 accuracy: 20.44%\n             angular  100 accuracy: 12.90%\n             angular  200 accuracy:  7.99%\n       euclidean   25 accuracy: 33.71%\n       euclidean   50 accuracy: 21.04%\n       euclidean  100 accuracy: 13.37%\n       euclidean  200 accuracy:  9.31%\n\n```\nAfter\n```\n             angular   25 accuracy: 46.80%\n             angular   50 accuracy: 31.00%\n             angular  100 accuracy: 24.50%\n             angular  200 accuracy: 15.18%\n       euclidean   25 accuracy: 47.34%\n       euclidean   50 accuracy: 33.04%\n       euclidean  100 accuracy: 25.10%\n       euclidean  200 accuracy: 17.99%\n\n```\nThe runtime of the tests also dropped from about 6000s to 1500s meaning a speedup of 4x.\nPretty cool for a code review that's mostly red!\n. But something is weird. I can fix it in a separate commit\n. Actually let me try to merge and if it causes something bad I'll revert\n. looks like it worked\n. the RAND_MAX thing was weird. I don't understand why it didn't cause more issues :) I think the overflow ended up just mapping it to a different permutation of 0...RAND_MAX-1\n. It wouldn't be too hard \u2013 you would need a mutex in a couple of places and also watch out for reallocations, but other than that it should be easy\n. not afaik :(. roughly, but  it's a bit more complicated._make_tree also modifies the datastructure. You also need a mutex around line 643-644 and line 702-703. Maybe that mutex could just be folded into _allocate_size and we could rename the function to something like _add_item ... probably makes more sense\nthen you obviously need to create pthreads and collect them afterwards... honestly I haven't done that in like 10 years, but iirc it's not too hard. sure, feel free to submit a PR, just make sure to put \"WIP\" in the subject or something\nyou definitely need a mutex around those lines, that's probably the issue :). Yes seems like this\nhttps://github.com/spotify/annoy/blob/master/src/annoylib.h#L368\nis not respected here:\nhttps://github.com/spotify/annoy/blob/master/src/annoymodule.cc#L108\n. hm get_item_vector shouldn't return 0... that's weird\n. That's strange. Let me add a unit test and try to repro\n. I won't have time today unfortunately\n. Not able to reproduce...see #72 \n. Hm it seems like the issue is some kind of integer casting.\nThe item vector looks like this for me:\n[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, -1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, -1, 0, 0, -1, 0]\n. I pushed a breaking test to #72 \nWill now fix it\n. Can anyone at Spotify give me admin rights to this repo? I removed myself from the organization but would be great if I can merge pull requests for Annoy\n. ping @a1k0n @MrChrisJohnson @nevillelyh \n. This was fixed in #72 however I didn't publish a new version to pypi yet. Will do right away.\nThe problem was just returning data, it was still properly inserted etc\n. I uploaded 1.3.2 to PyPI \u2013\u00a0can you verify that it works if you install the latest version from PyPI?\nAlso see #74 \u2013 silly PR but I can't push directly and/or merge my own stuff any more :(\n. Should be easy to fix the add_item return value so feel free to send a PR if you have time!\n. Thanks!\n2015-06-08 12:37 GMT-04:00 Noa Resare notifications@github.com:\n\n@erikbern https://github.com/erikbern you should be invited now.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/pull/74#issuecomment-110068146.\n. Awesome PR. We should add Kool-Aid man in a subsequent PR\n. Actually running ann-benchmarks shows pretty substantial improvements: +50-100% in most cases\n\n\nInterestingly it's actually on par or faster than kgraph. The only faster algorithm is SW-graph from nmslib\n\nNote that some of the increase in performance might be due to #82 as well\n. Seems like this introduced a regression from 0.98 to 0.97 for the test_precision tests... it's probably fine\n. This doesn't look good in benchmarks \u2013\u00a0I will trouble shoot it\n\n. I haven't tried it, but in theory Annoy works with out of core data. If you put it on an SSD drive then it probably could perform OK. I think you would experience a slowdown of say 10-100x.\nOne issue that comes to mind is that Annoy builds up the index in RAM and then writes it to disk \u2013\u00a0it would be more efficient to build it up directly to disk. That should actually be pretty easy to support.\nSpinning disk would be ridiculously slow \u2013\u00a0probably 1000x slower at least.\nMore harder stuff you can do to optimize for out of core:\n- Partition the index (like @piskvorky said)\n- Don't store the vectors in the index, just the search structure\n- Support axis-aligned splits instead of arbitrary vectors \u2013 this way the splits will only take a few bytes rather than 4kB per split (with (1000 dim))\nI also encourage you to do some sort of dimensionality reduction before putting the data into Annoy \u2013 even a simple SVD down to 100D would probably help tremendously.\n. See https://github.com/spotify/annoy/blob/master/src/annoylib.h#L399 for how memory is used while adding items. It isn't until you call save that the index is written to disk. Then later if you call load it will perform an mmap. It would make more sense to support mmap during insertion too. Problem is afaik mmap doesn't support resizing, but you could probably just write to the file pointer, flush, then munmap/mmap again (not sure how fast it would be). This way you would always use the file system for persistance, and the kernel will use the page cache to fit as much as possible in RAM. With an SSD this would be pretty reasonable\n. Not clear why, how are you measuring it? Measuring memory consumption of a process is notoriously unreliable. That's super cool \u2013 so some sort of k-NN classifier on satellite images?\n. Curious to see how k-NN compares to deep learning (tbh I would suspect the latter is better, given enough data)\n. Oops thanks. I can fix \u2013 feel free to submit a PR next time :)\n. Fixed per 84b98a1be2848fa7bc298f7d782f5750e50bafa6\n. i can add some tests for other dtypes!\n. but i don't think it should matter. if you cast it to a list, it will probably cast all the underlying elements too right? there's no unboxed float32 in Python i think\n. Using various numpy types in the test now\n. That's annoying. Seems like it's not an issue here though\n. Interesting \u2013\u00a0I think probably just wrapping the file reading in a try/except should resolve the issue \u2013\u00a0I can do later today\n. Travis is building it on Python 3 so I don't get it. Are you on Win or something?\n. weird will try to repro\n. Can't repro on Ubuntu 14.04. Anything else that could be special about your setup?\nI could fix the bug though, just nicer to be able to repro it\n$ python3.4 setup.py build\nrunning build\nrunning build_py\nrunning build_ext\nbuilding 'annoy.annoylib' extension\nx86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -g -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.4m -c src/annoymodule.cc -o build/temp.linux-x86_64-3.4/src/annoymodule.o -O3 -march=native -ffast-math\ncc1plus: warning: command line option \u2018-Wstrict-prototypes\u2019 is valid for C/ObjC but not for C++ [enabled by default]\nx86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.4/src/annoymodule.o -o build/lib.linux-x86_64-3.4/annoy/annoylib.cpython-34m.so\n. My PR didn't work either. Can you figure out a way to fix it and let me know? Possibly send a PR if you want to\n. I fixed it using codecs now in #91\nWill push new version to PyPI\n. Isn't the problem that your file is annoy.py so when you do from annoy import AnnoyIndex you end up trying to import it from the same file not from the package? Try renaming the script to something else and run again\n. I meant for everything. It's a bit nontrivial to change just the split planes and not the item vectors and I also think the benefit is the same for both.\nWhy did you specifically mention the angular split planes? Should work for Euclidean too right?\n. Yeah but it should be fine. I ran some tests on GloVe (same as the unit tests) and accuracy was hardly affected by quantizing the vectors. Might be a good option that doesn't have to be enabled by default\nThe bottleneck is really memory latency so to get the big benefit you need both the split planes and the vectors to be stored compactly \u2013\u00a0especially the latter since more vectors are fetched compared to split planes\nMight put together something quick and benchmark\n. Doesn't seem to work super well. For GloVe (f=25) the index size goes down from 289M -> 209M (angular) and 332M -> 259M (euclidean).\nInterestingly, accuracy goes up slightly. I think it's because _K gets smaller so each leaf node stores more items\nUnfortunately search time goes up as well, from 60s to 100s (for angular + euclidean).\nI think what happens is this\n- Conversion to/from 8bit is negligible and quantization has no impact on quality\n- The RAM access time is mostly dominated by latency not throughput. So every random access has the same penalty no matter how big the request is\n- Leaf size has some impact and most likely if you push leaf size down, you need to search more internal nodes -> higher total number of RAM requests -> slower (but marginally higher quality)\n. Yeah not sure what the tradeoff is actually. Smaller leaf sizes seem better.\nAnother thing worth trying at some point is more than 2 child nodes per split \u2013\u00a0should improve performance too\n. It's kind of convenient that they have the same size though. I've been thinking about adding support for incremental adds at some point and in that case it would be really useful \u2013\u00a0you can mix vectors with tree nodes all over the index.\n. Btw will try the 8-bit thing with larger vector sizes too \u2013\u00a0might have to do something with the size of a cache line\n. looks good but can you put the random number generator in a separate header file?\n. also would be good to understand why this is useful\n. btw separately i want to templatize in terms of S as well but i can do that some other time\n. ok yeah that makes sense. yeah maybe the templatizing is the main thing you need at this point. but if S is 64 bit it would be useful to have a better RNG.\nActually even if S is smaller than 2^32 it probably makes sense to always use the 64 bit RNG since when you are doing modulo it will create a distribution that's not uniform (except if number of items is a power of two)\n. can you squash the first three commits?\n. Can you fix compiler errors (check Travis) and rebase your commits? Thanks!\n. LGTM!\n. interestingly the accuracy went up marginally in the unit test. probably just a fluke\n. do you think we should default to the 64 bit RNG? if you have say 1B nodes then you will get a really skewed distribution with a 32 bit RNG\n. Nice! Let me know if you need any help!\nOn Sunday, September 6, 2015, Longbin Chen notifications@github.com wrote:\n\nbest way to celebrate the labor day is to work. I will work on this for\nthis weekend and hopefully come out something.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/issues/96#issuecomment-138040974.\n. Did you end up doing anything with this? I've been thinking more and more about this and might spend some time on it\n. Yeah I do. I think with LMDB it might be worth considering moving this somewhere else though \u2013\u00a0it's a big non-compatible change anyway\n. not afaik. It seems like lseek is defined in unistd.h.\n\nSee if you can do something like this and make it compile: http://stackoverflow.com/questions/341817/is-there-a-replacement-for-unistd-h-for-windows-visual-c\nAnnoy has been successfully used on Windows but support was added by @thirdwing and I don't know what environment was used.\n. mman.h is available in the same directory so i'm not sure what's going on here :(\n. I realized mman.h wasn't added to the manifest \u2013\u00a0fixed now\n. maybe try this? http://www.lfd.uci.edu/~gohlke/pythonlibs/#libpython\n. also seems like this can be fixed: http://www.gamedev.net/topic/167094-undefined-reference-to-_imp___py_nonestruct/\ndo you want to try and if it works send a pull request?\n. weird. i just googled your error and found someone mentioning libpython :)\nsure great if you want to try the other option. i don't have access to any windows machines\n. not sure\nhttp://stackoverflow.com/questions/31524867/compile-c-to-python-using-swig maybe?\n. At this point it's probably only a problem with the path. Make sure annoylib.dll (or whatever it's called, on os x it's .so usually) is in the same directory as __init__.py\n. what about the .pyd file it's building on the last file? that's probably the one you need\npython setup.py doesn't put the artifacts in the right directory for some reason\n. or just run python setup.py install!\n. I'm sorry, no idea what's going on. Maybe someone else with Windows can help?\n. nice!\n. think this is unrelated to annoy\nhttps://stackoverflow.com/questions/24683305/python-cant-install-packages-typeerror-unorderable-types-nonetype-str\nhttp://bugs.python.org/issue2698. Sorry no\nOn Tuesday, August 25, 2015, Dat Le notifications@github.com wrote:\n\nI have to load in, says 1 millions (700 dims) records from a csv file or\npandas dataframe.\nIs there any way to do a bulk load for AnnoyIndex instead of loading each\nrecord one by one with add_item(i, v)?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/issues/98.\n. yeah i think it should be pretty fast in practice\n. n_items will return max(id)+1\n\nThey don't have to be sequential at all. You can add item 5 and then item 3 and then item 1. n_items will be 6 in that case.\n. You can think of it as how a matrix works in numpy, it's just that it resizes dynamically\n. Not saying this is perfect behavior, but it was by far the easiest thing to implement and it supports most use cases with minor extra work (sometimes a dict mapping to unique id's)\n. ok that's not so good. there's some internal state that's not validated. i could add some simple checks\n. This looks great!\nDid you consider using a plain old struct instead of protobuf? It's probably faster.\nIt might be worth just starting over with a new repository and calling it annoy2 or annul or something :)\n. Sure SGTM although I think in practice people might be fine just using the Python bindings and build a simple server around it\n. If you want to move this to houzz/xyz, I'm happy to help. Was planning to do something under erikbern/ but I've been postponing it so long that it's better if someone else takes charge and I'm happy to contribute to it\n. SGTM\nThe LMDB backend will have a different interface and indexes will not be binary compatible so I think it's the right way to go. \n. But Hamming distance is pretty much the same as Euclidean unless you are\nsaying some leaves could have many items?\nBtw did you see https://github.com/houzz/annoy2 ? Will be interesting to\nsee if the LMDB backend works out - then you don't have to worry about the\nfixed size stuff\nOn Friday, September 11, 2015, Andy Sloane notifications@github.com wrote:\n\nAnd actually, then you don't have to store the split planes at all, only\npointers to the left/right tree nodes. Which again breaks your fixed tree\nnode size constraint unless leaves are tiny, but if you had dynamic leaf\nnode sizes then the whole tree would be a lot smaller.\nIf you do that then it would also make sense to reorder the indices so\nthat the roots and interior tree nodes are first, etc, for memory locality\npurposes.\nAnother idea I had was to just store the points in the interior nodes and\nnot in the leaves, and push them onto the priority queue while traversing\nthe tree. But that doesn't really help when you have multiple trees and the\nsame points which are interior in one tree end up in a leaf of another tree.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/issues/102#issuecomment-139447487.\n. You could implement hamming distance by just having T=int64 and add up the bit distance btw \n\nANyway, closing this...\n. It will affect the priority queue during searching. Nodes will now be\nexplored in order by distance to split.\nOn Tuesday, September 22, 2015, Andy Sloane notifications@github.com\nwrote:\n\nHuh. Shouldn't make much difference, but OK!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/pull/104#issuecomment-142481979.\n. i have a couple of smaller commits coming up, so don't merge yet.\n\nyeah this is quite obvious and i noticed last time i tried it i had a couple of bugs in my code. doing the math i think this is roughly a 2x speedup (i.e. hopefully now it should beat all other ANN libraries)\n. SW-graph (nmslib) and kgraph are less than 2x faster than Annoy so I hope it's doable :)\n. Thanks Andy \u2013\u00a0that's really crazy, not sure how that ended up outside the PR\n. Some benchmarks below.\nThe problem is I'm getting segfaults when trying to use 400 trees. Fixing this would give even better results since now I'm maxing out at 200 trees. The k-means algorithm uses a bunch more dynamic memory allocation so I will try to rewrite the tree building algorithm to use as little as possible.\nTree building is also 10x slower which is a bit unfortunate but Annoy still is much faster than other algos\n\n. Turns out since I'm subsampling it's better to do it using SGD rather than in batches. Also seems like the number of attempts doesn't matter at that point \u2013\u00a0better to increase the number of samples.\nResult is substantially faster index building, slightly shorter code, and marginally higher quality. Will do some more analyses later today though.\n. i'm planning to merge this but i want to spend a bit more time first \u2013\u00a0the reason is that I don't fully understand why it works so well, so I'm a bit concerned it will introduce regressions for certain cases.\n. rebased this, will probably merge this after a bit of cleanup and a few sanity tests to make sure rebasing didn't mess up anything\n. Sorry, have no idea why :(\n. Saw in http://stackoverflow.com/questions/33193970/error-suffix-or-operands-invalid-for-vbroadcastss\nDoes it work if you remove the -march flag altogether? Want to see if that is more generic\n. What's pip2?\n. Seems like something broken with your Python environment.\nFor example: http://stackoverflow.com/questions/32152166/python-2-7-suddenly-not-working-symbol-not-found-pyerr-replaceexception-wh\nDid you try to install it in a virtual environment? Might work better\n. Works on my mac:\n```\n$ virtualenv venv\n$ venv/bin/pip2 install annoy\n$ venv/bin/python2\n\n\n\nimport annoy\n```\n\n\n\n(a bunch of output removed)\n. pretty sure _PyModule_Create2 is defined in /usr/lib/libpython\nI think your problem is that libpython isn't found in runtime\n. I actually noticed I have this problem on my work computer so will try to repro & fix\n. @edonyM what happens if you clone the repository and run python setup.py install? Does it also give the same error? If so \u2013\u00a0can you send the command line options for the compiler? Mine look like this:\ncc -fno-strict-aliasing -fno-common -dynamic -arch x86_64 -arch i386 -g -Os -pipe -fno-common -fno-strict-aliasing -fwrapv -DENABLE_DTRACE -DMACOSX -DNDEBUG -Wall -Wstrict-prototypes -Wshorten-64-to-32 -DNDEBUG -g -fwrapv -Os -Wall -Wstrict-prototypes -DENABLE_DTRACE -arch x86_64 -arch i386 -pipe -I/System/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c src/annoymodule.cc -o build/temp.macosx-10.10-intel-2.7/src/annoymodule.o -O3 -march=native -ffast-math\nSee the -dynamic flag here \u2013\u00a0that one is key\n. Yes that seems correct, glancing at the code, the fd file is never closed in the unload function.\nDo you want to put together a pull request for this?\n. would be awesome if you can include that code sample as a unit test\n. It should take me a few minutes, so I can do it.. give me 5 min\n. something is weird with my python installation \u2013\u00a0i'll take a look later\n. Seems like tests are failing because GloVe vectors moved, will resolve separately\n. will fix the glove stuff separately\n. Cosine distance can be between 0 and 2. The distance between x and -x should be exactly 2\nhttp://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.spatial.distance.cosine.html\nI will add a unit tests though\n. Did you try the latest code? I think I actually fixed this yesterday\nhttps://github.com/spotify/annoy/commit/ae74624fcdc89db6ab947149b7db7545212bb062\n. The underlying python C extension doesn't handle kw arguments so well but I solved it by a dumb wrapper on the Python side\n. When I meant latest version I meant bleeding edge github master.\nI updated master but didn't push it to pypi. I can bump the version and\npush. I keep forgetting to do that so in glad you reminded me :)\nOn Friday, November 13, 2015, Horace89 notifications@github.com wrote:\n\nYes, I confirm that I use the latest version of Annoy (1.5.2), and the\ndescribed bug still occurs. I'm on Python 3.4.3.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/issues/114#issuecomment-156356511.\n. I bumped the version to 1.6.0 and pushed it to PyPI\n. let me take a look at it\n. I had forgotten to include src/kissrandom.h in setup.py.\n\nI added it and pushed 1.6.1.\nSorry about the inconvenience\n. hm still not working \u2013\u00a0not sure what's going on. let me try again\n. try again \u2013\u00a0had to bump to 1.6.2 :(\n. It might work, but what probably works much better is to do dimensionality\nreduction before you put the data in Annoy.\nFor instance Spotify typically runs some latent factor model to get he\nnumber of dimensions down to 40-100 and then uses Annoy to find similar\nitems.\nOn Tuesday, November 17, 2015, hiyijian notifications@github.com wrote:\n\nfor high dimension, i mean size of vector larger than 3000\nfor sparse, i mean vector with none zero less than 1%\nis annoy a good choice for this kind of knn task? if answer is no, what\ncan we do?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/issues/115.\n. thanks!\n. I don't understand... why don't you just compute the distance between A1 and all A2...An? You don't need Annoy to do this...\n. That's O(n) so it's hard to optimize...\n. wow this is excellent. ignore the appveyor tests\n\nlet's set up go tests separately in travis (i might be able to do that)\n. it would be great if you can make the tests run in travis\nlet me see if i can set up the basics of it\n. See #123!\n. @rosmo can you take a look at this? I can't get the build working in Travis \u2013\u00a0I don't think it's super hard\n. There's some issue with swig: https://travis-ci.org/spotify/annoy/jobs/98008653\n. is it possible to add trusty-backports as a apt source? i tried to figure out how to do it but wasn't able to\n. Hm not sure if that's possible. Is it easy to install Swig 3 manually?\n. yeah seems to work \u2013 let me amend and try some more\n. hm actually still same issue\n$ swig3.0 -go -intgosize 64 -cgo -c++ src/annoygomodule.i\nswig error : Unrecognized option -cgo\n. It works if I remove the -cgo option but then when I run \"go build\" I get tons of errors like this\n```\n_/home/ubuntu/gopath/src/annoyindex\n./annoygomodule_wrap.cxx: In function \u2018void _wrap_AnnoyIndex_getNnsByItem__SWIG_0(void*)\u2019:\n./annoygomodule_wrap.cxx:411:19: error: \u2018class std::vector\u2019 has no member named \u2018len\u2019\n     swig_a->arg5->len = arg5->size();\n```\n. great\u00a0\u2013 thanks.\n. btw is it possible to use swig for the python module too? might simplify the code a bit\n. i'm not getting this to work on some other machine (could be some apt issue) but will try on travis!\n. thanks a lot!\n. ok this seems to work now except the \"cd\" command fails because it's not a real shell command.\nis there a way to run \"go build\" without the cd?\n. trying something...\n. Not sure what to do about the latest issue: https://travis-ci.org/spotify/annoy/jobs/98461064\n. lol $HOME in the commit msg got expanded to my local value of $HOME :)\n. looks good except \"go test\" doesn't run anything\nhttps://travis-ci.org/spotify/annoy/jobs/99135828\n@rosmo any idea why?\n. i'm fine merging this as-is, but let's follow up and make sure tests are run etc\n. hm not sure if i can pipe data using tox but will try\n. similarly \"cd\" won't work\n. yeah maybe. there must be some nicer way to do this though?\n. does it eventually run out of memory and crash? i'm guessing it's just on the python side that the interpreter allocates memory. maybe it doesn't collect it until needed\n. I think I've been able to repro is using this:\npython\nclass MemoryLeakTest(TestCase):\n    def test_get_item_vector(self):\n        f = 10\n        i = AnnoyIndex(f, 'euclidean')\n        i.add_item(0, [random.gauss(0, 1) for x in xrange(f)])\n        for j in xrange(100 * 1000 * 1000):\n            i.get_item_vector(0)\n. great if you can take a look again\n. np (just published 1.7.0 to pypi with this fix in it)\n. Should resolve #124 \u2013\u00a0maybe also running a bit faster\n. The Py_RETURN_NONE is a separate thing but seems like it should have been fixed a long time ago \u2013 not sure why it hasn't caused any issues\n. Great! I can probably write a macro that pulls out the docstrings and writes it into the README (or maybe just use readthedocs)\n. no plans but would love it if you want to show me how to do (or even better \u2013\u00a0send a pull request!)\n. Doesn't seem like eigen has built-in functions for Euclidean distance and cosine distance though?\nI guess I could compute three dot products using Eigen at\nhttps://github.com/spotify/annoy/blob/master/src/annoylib.h#L119 and\nhttps://github.com/spotify/annoy/blob/master/src/annoylib.h#L180\nIt seems like a bit of overkill to use Eigen only for dot products though?\n. cool \u2013\u00a0seems very hard to read though.\ni'm using -march=native for gcc which should enable it in the compiler right? i'm not sure if i need to tell the compiler to align right though\n. Saw this from kgraph btw: https://github.com/aaalgo/kgraph/blob/master/metric.cpp\n. Thanks a lot \u2013\u00a0i'll give it a shot!\nWhat happens if the vector is not aligned?\n. I also wonder if just changing the definition of the Node class so that it's aligned against 128 bits (8 bytes) would be enough.\nhttp://locklessinc.com/articles/vectorize/\nThe definition of Node is currently pretty dumb \u2013\u00a0it's 12 bytes and then the float* array\n. See https://github.com/spotify/annoy/pull/135\nI ran some basic tests on my laptop and it actually seemed marginally slower (2.2s instead of 2.0s). Was only with f=100.\n. closing this as i'm pretty sure that gcc enables SIMD by default\n. thanks!\n. sorry not sure what's going on. i don't have access to a windows machine. maybe you can help me out by trouble shooting this and see if you can find out what the issue is?\n. nice!\n. if you can help me fix the pypi version i'd be grateful!\n. figure out what the issue is and suggest a solution :)\n. i can upload latest version but don't know if anything window specific changed. but will do\n. i pushed a new version (1.8.1) to pypi\nnot sure what went into it :)\n. > @bnbwn\n\nIf you're still interested in this, you might want to try my fork that is compiling correctly on Windows (at least in python 3.5/3.6).\nhttps://github.com/tjrileywisc/annoy\n\n@tjrileywisc do you mind sharing a pull request back into annoy?. interesting \u2013\u00a0didn't know about windows subsystem. maybe it's possible for to hook it up with CI. I don't think that's true \u2013 there's a unit test here: https://github.com/spotify/annoy/blob/master/test/annoy_test.py#L172\nand in the following test here, I check that the distance is always less than or equal to 2: https://github.com/spotify/annoy/blob/master/test/annoy_test.py#L186\nIf the distance had been Euclidean this had not been the case\nIf you still don't trust it, can you try to produce a minimal breaking example?\n. since you are dividing by the norm of the vector, the euclidean and angular distance will be identical\nannoy's \"angular\" distance is really just the euclidean distance of normalized vectors i.e. (u / |u| - v / |v|)^2\n. np :)\n. glove euclidean 100 goes from 2.0s (before) to 2.2s (after) so 10% slowdown\nwith bumped search_k to 10000 it goes from 15s to 20s so 25% slowdown\n. I ran nosetests test/accuracy_test.py:AccuracyTest.test_euclidean_100 multiple times.\nThe first time it will build an index etc so will be very slow. Subsequent runs will be much faster.\nI wonder if either (a) GCC is already using SIMD instructions (b) Eigen didn't use SIMD instructions\n. I just noticed this: http://eigen.tuxfamily.org/index.php?title=FAQ#How_can_I_enable_vectorization.3F\nWill try to re-run with different flags to see if it helps\n. The other thing I'm starting to think is that time might not be dominated by CPU but by RAM access time. Annoy does a lot of random access when searching the tree. Not sure how to benchmark this\n. thanks a lot for looking into this!\n. I was confused by your message for a bit but I assume with L2 you mean vector norms not L2 memory? :)\nBy now, I'm pretty sure GCC vectorizes the vector norms so I think if anything I should focus on memory access time (L2/L3 latency). \n. the latter\n. it would blow up the search time by a factor of (trees + 1) / trees so not that much\n. i copy the roots r_1...r_n to the end of the index. but since the last item of the index was already a root r_n then you end up having n+1 roots at the end of the index. so like r_n, r_1, r_2, ..., r_n\nthe way annoy finds the roots is it just takes the last few nodes that has the biggest number of descendants. so that would include the last root r_n twice (and all the other roots once)\n. you are right that my description was wrong\ni think the problem was that static objects are only initialized once. so if f would increase, then the second time it wouldn't be resized properly\ni'll probably remove all dynamic memory allocations in a separate PR\n. i tried valgrind but it went nuts because annoy does all this magic stuff with pointer offsets. so wasn't super helpful.\n. Sure wanna send a PR changing the name to _k instead?\n. sure, go ahead\n. I'm open minded. What's the benefit of this?\n. I don't really see the benefit of using angle brackets though? It's clearly in the same directory and double quotes enforces that right?\n. Ok since you haven't outlined any benefits, I'm closing this\n. Yes this seems incorrect. There is a missing normalization on line 184\nI fixed something similar before and it led to a marginal improvement: https://github.com/spotify/annoy/pull/104\n. Thanks for noticing! Let me fix it this weekend. Hopefully it improves perf a bit\n. Curious how come you noticed this?\n. cool! let me know if you need help\n. sorry i don't have access to any windows computers, so i'm not sure what the issue is\nwould love it if you can help me figure out!\n. would be great if someone can troubleshoot and come up with a solution!\n. thanks!\n. Haven't thought about this use case. There is no such functionality atm, but shouldn't be too hard to implement. Basically during the search (where you use the priority queue) you would have to keep track of the distances and then use some heuristic stopping rule\n. I don't know any way to give theoretical guarantees, but generally don't think that's very important. I believe in empirical results :) \n. There is no such functionality, sorry\nBefore you build the index, you can overwrite data using add_item\n. I wish I had more time, but it's very unlikely at this point\n. But would love to see a pull request for it\n. I would guess you're just hitting disk because of the page cache. Does the total size of all indexes fit in RAM? If you do a few thousand queries does the query time start increasing? \n. Most likely the kernel swaps out the indexes to disk. As you do more and more queries, the kernel will put the indexes back into RAM and you should start seeing substantial speedup. It's just how the page cache works\n. Another thing you can do to warm up the page cache is to just read them from disk a few time. If you cat all the files to /dev/null a few times you will probably see that the first time it takes a long time, but subsequent times it will be substantially faster\n. Annoy doesn't guarantee to find the nearest items \u2013\u00a0that's suspected to be an NP complete problem. It will return most of them though.\nTry to increase the parameter search_k to something larger. That will cause Annoy to search more nodes.\nWith only 13000 items it's possible you might be better off just doing an exhaustive search.\nThe other option I would recommend is to pre-process the data set by doing dimensionality reduction. Annoy was built for roughly <100 dimensions, although I've seen it work well up to 1000 dimensions. Not sure what distribution your data set has, but something like a truncated SVD might work well: http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n. @piskvorky sorry i'm drunk. I didn't mean NP-complete \u2013\u00a0meant that it's suspected that it's impossible to do in less than linear time (but this has not been proven)\ndidn't know gensim has truncated SVD \u2013\u00a0good to know!\n. interesting \u2013\u00a0might be worth experimenting with\n. i'm thinking this could work\n1. use fewer trees\n2. when you get to the leaf nodes, pick some neighbors and search by those neighbors\n3. since we're using fewer trees, we get better cache locality\n4. fewer trees also means we can avoid searching nodes we already searched, meaning we don't waste time exploring similar nodes\n. in fact you could actually extend the current priority queue to do this pretty easily. you just need to keep track of (a) what vector are you searching by (b) what's the distance from the initial query point (so that you can add it to the distance)\n. i would reduce the number of trees to say 100. This will decrease accuracy, but 100 is more reasonable. Increase search_k during search instead!. Also 2048 is a LOT of dimensions. I would use some kind of dimensionality reduction to have fewer dimensions.\nI haven't tried the other methods but HNSW/KGraph is a bit faster and more accurate (though they don't have the ability to save indexes to disk). sure \u2013\u00a0do you want to edit the documentation and send a pull request? thanks!\n. Either one of them is fine. Thanks!\n. If you use angular distance then there's no need to prenormalize\n. thanks!\n. i can try to fix/remove go separately\n. thanks!\n. That's expected. I recently added the search_k parameter to be able to vary this separately.\nTry get_nns_by_vector(v1, 10, search_k=1000)\n. cool!\n. very exciting with lua support. what are you using it for?\n. cool\n. cool \u2013\u00a0what's the main benefit(s)?\n. that's great \u2013\u00a0thanks for fixing!\n. were you able to fix this? since you closed the issue\n. can you share code?\noff the top of my head maybe you are changing the metric after you already built the index? that would probably lead to a crash\n. not sure how to catch the error \u2013\u00a0there's no marker in the file that says what distance function is used\ni guess you could have some sanity checks when you load the index to make sure it's not corrupt (eg. there's no pointers to offsets outside the file) but it won't be a 100% guarantee\n. the data is stored in results/ so it should be easy to plot indexing time.\nif you do it feel free to submit a PR!\n. yeah also please check out https://github.com/erikbern/ann-benchmarks\n. thanks!\n. Sorry about this. I don't have Windows but would love it if someone can debug this.\n. sorry \u2013\u00a0there's no easy way. and generally the annoy index takes more space than the vectors\n. runtime of the new test went from 15s to 6s after the fix\nthe speedup should be a lot more though since there's some serial overhead (Amdahl's law)\n. nice! will see if there's an easy way to run this as a part of unit tests\n. Were you able to resolve it?\n. ok might be worth removing it in the repository\ni'll see if other people have the same issue\n. are you on any nonstandard platform?. i can't reproduce the issue so i have no idea unfortunately\nwhat happens if you remove the -march=native flag?. Sorry, no, Annoy doesn't use a fixed set of projections, it builds up a tree of random projections.\nCurious what's your use case?\nClosing this for now\n. Sorry not possible, for the reason I just mentioned. It's a tree not a fixed set of projections\n. I have no idea, but doesn't seem related to Annoy, so I'm closing this...\n. interestingly this breaks the Lua tests \u2013\u00a0will update them\n. I'm not sure how useful unbuild() would be in general but I think it's pretty easy to implement\n- clear _roots\n- set _n_nodes to _n_items\nfeel free to send a PR :)\n. I'm actually not sure how it would work. Right now I think you would get a page fault or something if you tried to add an item after doing unbuild() on a load()'ed file. However if you mmap with PROT_WRITE then it might be possible to write back data. The problem is what happens once you overflow the size of the mmapped region \u2013\u00a0I'm not sure how _allocate_size should work in that case.\n. googling it, it seems like it's possible to resize an mmapped region, but i don't have any more details than that :)\n. If I had unlimited amount of time (I don't) I would rewrite Annoy to use LMDB. That's probably the best strategy to make it support updates. I doubt I'll ever have the time, but feel free to give it a shot :) I think it could be done in a week of full time work \u2013\u00a0happy to outline the approach\n. After you call build, you can't add more items. Or at least you're not supposed to \u2013\u00a0I think it may lead to undefined behavior. With the PR to do unbuild() it would work (assuming you never map it to disk)\n. you would have to call unbuild(), add items, then call build()\n. there is no way atm. cool!!!\n. nice!\ncan you add a unit test for this? ideally something that builds and unbuilds a bunch of times\n. waiting for a few unit tests for this and i'm happy to merge\n. looks good for now! thanks!\n. Hamming would be possible to do fairly easily (probably day's work or something)\nRight now it's probably easier to do a dimensionality reduction into a continuous space (for instance train an autoencoder to embed everything in R^40) and you might get better results doing so anyway\n. This is already implemented!. sure yeah would be a very minor optimization (save like 1ms at most).\nclosing this, feel free to send a PR if you want to. i'm not really sure what this has to do with annoy?. seem fine. not sure why the tests wasn't working for python, can ignore that. @leonardidouglas what the fuck?. How did you install it?. If you used pip, can you paste the full output of the installation?. and which OS is this?. is it still broken? closing until you can verify. I don't understand how this can lead to an out of bounds assertion since p is always less than or equal to the length of the array?. I still don't understand what bug this solves. Can you share a reproducible case?. Hm weird... &v[p] doesn't actually access v[p] though, it's just pointer arithmetic. It shouldn't invoke operator[] afaik.. cool i guess that makes sense. what's the point of this?. i guess, but then what are reproducible builds useful for? :). i mean that if you have flakiness in your tests then you should fix that... don't rely on exact comparisons between the results arrays for instance, allow some percentage of missing values\nsee the unit tests for annoy \u2013\u00a0https://github.com/spotify/annoy/blob/master/test/annoy_test.py#L138\nI don't have a problem with this PR just pointing out I think you're solving it in the wrong place. no i think seeding is generally good to have... for some things (like train/test split) it's crucial.. i'm not sure if it's ideal to rely on rand/srand \u2013\u00a0it's possible that numpy uses it somewhere under the hood as well. set_seed uses a global state which is pretty bad. we should probably replace it with a local state RNG like using <random>? @yonromai is it something you could take a look at? http://www.cplusplus.com/reference/random/. > Or use the RNG someone already contributed?\nyes even better. I'm not able to reproduce this. See https://github.com/spotify/annoy/blob/master/test/annoy_test.py#L528\nI increased this to 100k vectors, 300D, 30 trees\nRan it five times, passes every time. is it possible you were using an older version of the c++ code? just speculating. sorry there's no way atm... we could probably support this by having a special \"tombstone\" vector (like all zeros or something) so lmk if you want to add support for this and i can give you the rough outline. No\nI also don't see a huge use case for this. If it's before building the index, why don't you just prevent adding it in the first place. If it's after building the index, the dataset is immutable anyway, so it won't work.. It's possible you can run t.build multiple times, but that's not a \"supported\" feature and I would discourage you from relying on it. Every time you call t.build, it will allocate a lot of new memory for the tree structure.. yes, annoy is not suitable for streaming updates\nany time t.build is invoked, it will allocate a lot of new memory. sorry no \u2013\u00a0if I had infinite time available, I would implement it, but alas I don't :). Yes this is \"working as intended\" \u2013\u00a0let me know if the documentation is unclear and we can fix it. this sounds like a bug. let me add a test case for it and see if i can reproduce. thanks!. I'm not able to reproduce this \u2013\u00a0see #196\ndid you use the latest version?. ok awesome :). sorry not sure what's going on \u2013\u00a0did you compile annoy on the same machine as where you're running it?. not sure why this one was sitting here so long :). yeah might make sense \u2013\u00a0i'll try to remember to add a tag next time :). tagged the latest version. i've been trying to set tags in git yeah. get_distance actually changed at some point \u2013\u00a0it looks like if you square all the distances in v.1.8.3 then you get roughly the distances in v1.5.2 (actually marginally better). https://github.com/spotify/annoy/pull/171. closing this until further notice :). it doesn't support L1 distance, but it wouldn't be very hard to add... let me know if you need any pointers. L1 distance isn't pushed to pypi yet \u2013 you have to clone the github repo and build it from scratch\ni can bump the latest version on pypi today. i've bumped it to 1.9.0 and uploaded to pypi \u2013\u00a0can you try again?. sorry won't be enough \u2013\u00a0annoy uses float32 internally\ni tried to compile it with half precision floats and it wasn't necessarily faster. although saves memory so maybe it's worth exposing that option to the end user. I just compiled it with some random half-precision float library I found online. Forgot which. But it ended up being slightly slower because of the overhead (extra arithmetic)\nIs there native float16 support in gcc? Only thing I can find is __fp16 but it's only ARM: https://gcc.gnu.org/onlinedocs/gcc-4.5.3/gcc/Half_002dPrecision.html. there is a number of half precision floating point libraries online \u2013\u00a0since annoy uses templates it's quite easy to use any of them as a drop in replacement \u2013\u00a0you can try and lmk what happens! i tried it a long time ago and it definitely works, but was slightly slower than single precision float. closing this for now. sure go ahead and try!\ngenerally i would recommend some kind of unsupervised dimensionality reduction before putting the items in annoy. yeah it will probably improve either/both of speed and accuracy. cool. @rshest hey sorry forgot about your PR. I just rebased it on latest master. does it look OK?. very cool! i think this makes a lot of sense. merging it for now since i'm going to forget to do the more thorough review in any reasonable time, but i'm hoping to get back to it soon with some comments!. sorry, but seems this is something in cygwin/anaconda rather than annoy?. would be great if someone could figure out how to build it consistently\nhttps://www.appveyor.com/ would be nice too. very nice @tjrileywisc !. Ok, wanna send a PR instead?. that's weird. i made some changes to the metric argument recently. let me check if i broke something accidentally. actually there are already tests for this, see eg https://github.com/spotify/annoy/blob/master/test/annoy_test.py#L264\nso it's likely you're doing something strange.\ncan you share source code that reproduces the behavior?. that sounds problematic. can you share relevant code?. i'm able to reproduce this.\n```\n\n\n\nimport annoy\na = annoy.AnnoyIndex(3, metric='euclidean')\na.add_item(0, [1, 0, 0])\na.add_item(1, [2, 0, 0])\na.get_distance(0, 1)\n0.0\n```\n\n\n\nproblem seems to be when metric is passed as a kwarg. this works:\n```\n\n\n\na = annoy.AnnoyIndex(3, 'euclidean')\na.add_item(0, [1, 0, 0])\na.add_item(1, [2, 0, 0])\na.get_distance(0, 1)\n0.9999999403953552\n``. i think the problem is the initialization in c++ is using thetp_newmember rather than thetp_initmember, and so the Python layer's code to handle the kwarg is ignored. the solution is either to changetp_newintotp_init`, or remove the subclass constructor and handle the kwarg in the c++ module\n\n\n\ni'm somewhat biased towards the later approach since imo the whole python glue code is a bit ugly. the workaround for now is just don't use the metric=xyz as a kwarg, just pass it as a second argument. will push to pypi shortly. that would be interesting, but unfortunately there's no way right now\nthe binary format of annoy is very easy though. you could probably just try to open the annoy index raw in Python and try to parse it. There's some documentation in annoylib.h\nclosing this for now.. The reason why you're getting inconsistent results is that Annoy does an approximate search where the size of the search space depends on n if you don't provide a value for search_k.\nTry increasing the value of search_k. In particular if you keep it constant, you should get consistent neighbors:\nprint t.get_nns_by_vector(query, n=1, search_k=1000, include_distances=True)\nprint t.get_nns_by_vector(query, n=2, search_k=1000, include_distances=True)\nprint t.get_nns_by_vector(query, n=4, search_k=1000, include_distances=True)\nprint t.get_nns_by_vector(query, n=10, search_k=1000, include_distances=True)\n. haha that's funny\nclosing this though. that's interesting. i think gcc sometimes goes overboard with optimizations and since annoy uses some C \"hacks\" it could cause issues.\ncould also be an issue with memory alignment... not sure. out of curiosity what environment was it? can you share output from uname -a?\nwhat dimensionality for the vectors?. should be fine. can you share code?. is it possible you're passing it some id that doesn't exist in the index?. flask forks the process right? i wonder if there's some issue with that\nyou should make sure to save the index to disk and then make sure each process loads the index separately\ni don't know how annoy handles a process fork with an open index \u2013\u00a0it's possible there's some permission problem with mmap. when it crashes, what does it say?. The problem is that you're loading an index but the distance metric defaults to angular which has a different memory layout. Annoy doesn't store the distance function in the index.\nDoing loaded_index = AnnoyIndex(40, 'euclidean') should be enough to solve the problem. memory consumption is hard to measure \u2013\u00a0did you keep the program running for a while? if there truly is a memory leak then eventually malloc will fail and the program will crash. if it's not then the memory usage will converge to some high amount\nyour code doesn't even use mmap so i doubt that's the issue. it's entirely possible that it's something out side of Annoy, like just Python GC (although I assume you kept the python version same when you tested 1.8.0 vs 1.9.3)\ncould use valgrind i guess to see if it helps\nnothing has changed related to memory allocation so I'm surprised you're seeing different result. . are you saying 1.9.2 doesn't leak? that's very helpful \u2013\u00a0i'll try to understand what happened between 1.9.2 and 1.9.3. OK \u2013\u00a0thanks. It's possible I messed something up when I removed the Python glue.\nOff the top of my head it's possible that I need to decrement the refcount to lists or numpy arrays that are passed into the C layer. actually i wonder if the problem is here: https://github.com/spotify/annoy/blob/master/src/annoymodule.cc#L202\nprobably need a Py_DECREF(pf)\nlet me put together a a pull request and see what happens. https://docs.python.org/2.4/api/refcountDetails.html\nTherefore, the generic functions that return object references, like PyObject_GetItem() and PySequence_GetItem(), always return a new reference (the caller becomes the owner of the reference).. @ulges can you run your script with latest master and see if it works now?\ni'll push a 1.9.4 to pypi once you have confirmed it. awesome!. uploaded to pypi now. thanks for all the help identifying the bug and nailing down the problematic version!. it's certainly possible, but it will be slow \u2013\u00a0annoy is built for random access. also during the build phase it doesn't flush the index to disk, so it might not work super well. best strategy is probably to shard the data and build a bunch of separate indexes. to answer the issue title: yes, absolutely\ni don't have the energy to understand your code so please explain :)\nbtw probably makes sense to store the index to a file and load that file independently in each process since mmap is read only so guaranteed to be shared across processes. ok \u2013\u00a0for best performance, try to save the index to disk, and then in each worker process, load it from disk. since mmap will share the memory in a read-only way, it will only consume the memory once. oh sorry, i didn't read the code. yeah you can't create an index across processes. has to be a single process. you can't even use multiple threads afaik. i never really designed annoy to care about holes \u2013\u00a0it's been working somewhat unintentionally.\nshould be possible to filter them out on index building pretty easily. look at https://github.com/spotify/annoy/blob/master/src/annoylib.h#L526 \u2013\u00a0all you need to do there is to ignore any vector that's all zeros.. let me know if you aren't able to fix it \u2013\u00a0i could probably put together a quick fix this weekend. @a1k0n should be easy to memzero or bzero or whatever the function is called...i thought i already did that but could be wrong. Ok so I don't see any bugs with the code at least. This works:\npython\nclass HolesTest(TestCase):\n    def test_holes(self):\n    f = 10\n        index = AnnoyIndex(f)\n        valid_indices = set()\n    for i in range(10000):\n            i2 = int(i*2**-0.5) # leave holes every few items                                                                                                                                                                              \n        valid_indices.add(i2)\n            v = numpy.random.normal(size=(f,))\n            index.add_item(i2, v)\n    index.build(10)\n        for i in valid_indices:\n            js = index.get_nns_by_item(i, 100)\n            for j in js:\n                self.assertTrue(j in valid_indices)\n        for i in range(1000):\n        v = numpy.random.normal(size=(f,))\n        js = index.get_nns_by_vector(v, 100)\n            for j in js:\n                self.assertTrue(j in valid_indices)\nI'll take a look and see if I can improve the index building though. actually nvm i can reproduce this\npython\nclass HolesTest(TestCase):\n    def test_holes(self):\n    f = 10\n        index = AnnoyIndex(f)\n        index.add_item(1000, numpy.random.normal(size=(f,)))\n    index.build(10)\n    js = index.get_nns_by_vector(numpy.random.normal(size=(f,)), 100)\n    self.assertEquals(js, [1000]). something is broken on travis with python 3.5, going to ignore for now. still not working... not sure what's up. i hate tox, might remove it. no it should converge to 100% at some point. you can verify that by setting search_k to some enormous number so that it degenerates into exhaustive search.\nbut hard to say without knowing more about the data. what's the dimensionality?. nice, thanks. i'm not following exactly what's going on, but are you loading and saving the same index multiple times? that shouldn't be allowed. I think the problem is in your code. Look at this function\npython\n    def generateIndexNumber(self):\n        return self.Xindex.get_n_items() + 1\nThis means you will add an item at index 0, 2, 4, .... This is excellent \u2013\u00a0really appreciate the addition. I will review in the next few days. Should add a few unit tests in Python as well. Random thought but I guess it would be very easy to support Jaccard similarity as well based on the binary vector? Just an idea. Hoping to take a look at this during the weekend. true that's a good point. how does the split algo work for hamming distance?. only one single random position? think you can probably improve on that algorithm. ok, interesting. i would have assumed random projection would be better.. Is there a reason you chose to preserve the meaning of f as the dimensionality rather than the number of vector \"slots\" \u2013\u00a0I think in the latter case if you let f basically represent the \"chunk count\" then you don't have to introduce the extra chunksize and chunkcount helpers. I think that would be marginally simpler to deal with in the code.\nMy biggest question design wise is whether the Python layer should accept 64 bit integers as vectors, or binary vectors. The former is far more efficient and leads to simpler code, but the latter might be a bit more logical from a user point of view. I might start by just doing the 64 bit thing since that's simpler. actually after looking into it a bit more, i think the problem is how to deal with the python layer \u2013\u00a0atm the python object contains an AnnoyIndexInterface<int32_t, float>* ptr pointer, so dealing with any other types is problematic. not sure how to solve it.\none idea would be to create a wrapper object that implements AnnoyIndexInterface<int32_t, float> but contains a pointer to a AnnoyIndex<int32_t, int64_t, Hamming, Kiss64Random> and does the translation to/from binary vectors. probably the easiest approach.. could take a while to bhild the index with so many dimensions. Have you tried to enable verbose mode?. one hour seems normal with so many dimensions and 100 trees. you can make it faster by building with fewer trees \u2013\u00a0100 is usually not needed. try 10 or 20. this is excellent! going to merge this for now. i don't have access to any windows computer but will trust that this works\nhttps://www.appveyor.com/ would be nice to set up at some point btw. just pass a number to the build method, eg. build(40) and it will build 40 trees. https://github.com/spotify/annoy/blob/master/src/annoylib.h#L523. very nice \u2013\u00a0see comment i left!. happy to merge this if you address my comment about std::copy. we could probably actually replace it with memcpy and it should work on both platforms... let me see. Take a look at #236 \u2013 it might work and remove the need for MSVC-specific workarounds. closing this in favor of #238 . merging this \u2013\u00a0@tjrileywisc can you see if this works?. hm that's annoying. sorry i really thought my fix would resolve the issue\ndo you see any compiler warnings? anything else that could indicate what the issue is?. Finding some evidence that MSVC adds memory checks to memcpy: http://www.informit.com/articles/article.aspx?p=2036582&seqNum=6\nmaybe add #pragma runtime_checks(\"s\", off) at the top of the file?. @tjrileywisc let me know if this works!. munmap is in unload which is run by the destructor. But it's only run if _fd is nonzero.\nThe only reason I can see that _fd would be nonzero would be if we try to load a file then fail. In that case it ends up being -1. Pushing a fix as a long shot \u2013\u00a0let's see if it helps. It would be useful if you could run nosetests on each test file separately to see which one triggers the failure. Eg python nosetests test/index_test.py (I actually suspect that's the test that breaks). merging this for now... should be harmless at worst. think this actually seems to work!. This is great \u2013\u00a0how do we configure Github to run Appveyor tests? How do we display a badge etc :). This is great \u2013\u00a0I actually have an Appveyor account set up with Annoy, just never got it working last time I tried (about 2 years ago). I'll merge this for now and will try to make it build on Appveyor\nthanks a lot @tjrileywisc !. i think the fact that it runs well on python 3.6 is pretty awesome though!\nshould make it a bit easier to debug since it indicates it's something with python 2.7 (as opposed to the core c module). let's at least start running appveyor on python 3 \u2013\u00a0great improvement. @maumueller let me know if this is good to be merged!. Btw looking at the splitting code again. You're picking a random index to split on right? Seems like maybe a slightly better solution would be to sample say sqrt(f) items and pick the \"best\" split (whichever is most even). I think that's how random forests are generally built.\nI might play around a bit with the algorithm to see which ones perform better.. > That's a great idea. Looking at sqrt(f) coordinates should still allow for fast build times while leaving enough randomness in there. Taking a too large sample would yield too redundant trees, I guess. I also wanted to look at more \"data dependent\" methods of splitting the data in the nodes.\nExactly \u2013\u00a0that's the tradeoff. https://en.wikipedia.org/wiki/Random_subspace_method. Merging this for now \u2013\u00a0will follow up at some point later and play around with other splitting methods. not a typo\nhttp://man7.org/linux/man-pages/man2/mmap.2.html. nice to see more activity on this one \u2013\u00a0let me know if there's any way for me to help!. Any update on this?. Ok, I\u2019ll take a look to see what can be done. looks like @tjrileywisc gave up sadly, but feel free to give it a shot!. what was broken with the previous code? it would add vectors at every [k * sqrt(2)] position, so basically leave random gaps\n```\n\n\n\n[int(i2*0.5) for i in range(40)]\n[0, 1, 2, 4, 5, 7, 8, 9, 11, 12, 14, 15, 16, 18, 19, 21, 22, 24, 25, 26, 28, 29, 31, 32, 33, 35, 36, 38, 39, 41, 42, 43, 45, 46, 48, 49, 50, 52, 53, 55]\n```\n\n\n\n. maybe more instructive would be to do something like random.choice(range(1000), 500). haha ok that's not ideal. LGTM but would have a minor preference for a more random hole distribution. yeah i think n_items() will just return max(items) + 1 so shouldn't be an issue. thanks @yonromai \nare you on the latest version of annoy yet? would love to hear if it leads to any improvements!. This makes sense. I consider this arguably a \"user error\" since the distance to the zero vector is undefined (or nan) but I agree the error could be more useful to humans.\nAnnoy support \"holes\" now (see #223) so we could probably make something similar. Maybe if the vector has length <= eps, then just don't insert it.\nI'd appreciate a pull request (including a unit test). Let me know if you want to put that together an if you need any pointers.. @a1k0n if that's correct then another broken case would be if you create an index with a lot of (nonzero) vectors that are identical. That should be easy to reproduce. I'll put together a unit test for this to make sure it doesn't degrade. This should be resolved by https://github.com/spotify/annoy/pull/276. probably some 32 bit limitation in windows. annoy doesn't have a plan, it's an open source project i'm working on for fun :)\nfeel free to try to solve the issue and submit a pull request if you have a solution!. I wouldn't be surprised if the limit on Windows is 1GB or 2GB dues to 32 bit issues. cool!. ok want to create a pull request instead?. lol that's pretty lazy but whatever guess i'll do it.... yeah the id is so large that annoy will try to allocate a ton of memory and it probably fails doing so. might just be flakiness, let's try to restart the build. weird that both appveyor and travis flaked out. thanks @eddelbuettel . source code?. wonder if the issue is this line: https://github.com/spotify/annoy/blob/master/src/annoylib.h#L589\nbut i'm not sure what's going on. oh yeah, that's a classic one. should probably try to catch it. sorry, there's no way. there's no way to do it, but you can just filter the results?. The A in Annoy stands for \u201capproximate\u201d. Try to increase the value of search_k in get_nns_by_vector and see if the problem goes away. If not, please reopen this issue!. No \u2013\u00a0the c++ lib is not thread safe. The Python lib, however, is\nActually as a part of #246 we might get thread safety. Weird. Can you give me a small reproducible example?. also if you only have 25 items then i don't really see the value of using annoy, brute force search is faster. I just saw something similar locally actually. This is for a large index (1e6 elms, 200 dimensions). Trying to repro.. I figured out what happened in my case. Turns out my disk ran out of memory while saving the index. We don't check whether fwrite succeeds. Should probably enable that. Could even write to a temp file and move it after closing it just to make sure.\n@cggaurav @yogesh-kamble is it possible you had some similar problem?. I'm also going to add some checks to make sure that loading a garbage index fails. It's also possible you're trying to load an index created using euclidean distance using a different distance maybe?. ok i would say generally when this problem happens, it's because annoy fails to parse the index properly. could be either\n\nthe index was corrupted during saving (out of disk space probably)\nyou're loading it with a different distance function than it was saved with\nyou're loading it with a different number of dimensions than it was saved with\n\ni should probably add more error handling to prevent all those cases\nlet me know if you are sure none of the things above happened. If you want to patch save in the C++ layer so that it catches any exceptions during saving, that would be nice. Wonder if it's the same as #279 . that's pretty cool! I think the small differences in the distances is totally acceptable (after all, the A in Annoy stands for Approximate)\nI might run some tests and merge this if I can reproduce the speedup!. Good point @a1k0n \u2013\u00a0should add that too and test!. excellent \u2013\u00a0will take a look at this in the next few days. This is great \u2013\u00a0let me take a look soon!. Thanks @ReneHollander \u2013\u00a0this is great!\nI think the code is a bit repetitive so let me refactor and share a PR shortly!. @ReneHollander did you see #265 ? I factored out two helpers\n\ndot(p, q) for dot product\ndot3(p, q) for computing the quantities pq, pp, and qq. Sorry \u2013\u00a0none of these things are easy to support using Annoy. I'm going to close this issue as a \"won't fix\" \u2013\u00a0feel free to continue the discussion, but I don't think it's something Annoy will ever support. It's just not within the small scope of Annoy to support it.. It might be possible to make the results iterable in Annoy, but it seems pretty hard unfortunately. yes correct @a1k0n \u2013\u00a0it's also nontrivial to implement since it has to permeate through the C bindings into the C++ module. See README.md\n\n\"Annoy uses Euclidean distance of normalized vectors for its angular distance, which for two vectors u,v is equal to sqrt(2(1-cos(u,v)))\"\nso you can figure out the cosine similarity through that. Ok I added a new primitive dot3 to compute the products xx, yy, and xy at the same time now. Think it should do the trick!. Unscientific benchmark:\nAfter this PR:\nBuilding + searching: 246.822s\nSearching (best of 3): 8.584s\nCurrent master:\nBuilding + searching: 232.608s\nSearching (best of 3): 8.243s\nMaster from before the AVX instructions\nBuilding + searching: 230.247s\nSearching (best of 3): 8.505s\nThis is really weird. No difference? this is on my macbook air from 2014 so pretty old. It does compile with AVX though. Maybe it doesn't  give any speedup on my CPU (?)\n. seems like my processor might just be too slow: https://stackoverflow.com/questions/43245787/vectorization-speed-up-expected-for-sse-avx-and-avx2\ni'll try on a different CPU tomorrow. rebased on master (#266 in particular). Ok great \u2013\u00a0I'm glad my rusty C++ intuition was correct and template specialization is the way to go here.\nI'll probably merge this for now. Will run some benchmarks on a cloud instance maybe tonight to confirm the speedup. Awesome! Was actually just running some benchmarks myself, this time on a c4.2xlarge:\ncurrent master:\nglove-25 build+search: 136.447s\nglove-25 search (best of 3): 5.697s\nfashion-mnist build+search: 19.126s\nfashion-mnist search (best of 3): 8.798s\npre-avx:\nglove-25 build+search: 152.749s\nglove-25 search (best of 3): 6.351s\nfashion-mnist build+search: 33.285s\nfashion-mnist search (best of 3): 18.096s\nthis branch:\nglove-25 build+search: 146.448s\nglove-25 search (best of 3): 5.754s\nfashion-mnist build+search: 20.259s\nfashion-mnist search (best of 3): 9.163s\nlooks like master and this branch is basically same (great, my refactoring didn't ruin anything) and much faster than pre-avx, by up to a 2x factor. very nice @ReneHollander !\n. great \u2013 will rebase my change #265 on this. Hi @mazorigal \u2013\u00a0I'm confused what you mean. Do you still want to use consecutive integer ids? in that case, Annoy should be fine.\nIs it that you want to allocate extra empty memory at the end of the index?. Got it. No, you have to maintain that mapping yourself, unfortunately. Same thing applies if you're using strings as identifiers or anything else. Going to close this, Annoy has no plans to offer support for this. Sorry, there are no plans to support this atm. If I did it, it would probably be as a completely new library, but unfortunately I have way too much going on for that to be realistic :). I think it's a great idea though! Think you can probably build something on top of eg http://www.lmdb.tech/doc/\nJust nothing I personally have time for right now :(. It's definitely possible, and if you're interested, I would definitely encourage you to implement it!\nLooking forward to a pull request!. The split looks good, but how do you actually implement updates/deletions etc? I don't want to just split it up in two classes unless there's a use for it. If you have an idea of how the algorithm would work, let me know!. nice!. sounds like you don't have stdio.h on your computer so something is probably messed up with your compiler\ni haven't used conda/anaconda but probably makes sense to upload a binary from appveyor at some point (given that i don't have access to a windows system). Cool. Found this which seems like it could be useful: https://github.com/rmcgibbo/python-appveyor-conda-example. yeah if anyone can help me with conda, would appreciate it!. @AsmaZbt I don't have access to a windows computer, but feel free to help me troubleshoot. ok, i'll see if i can automate the process somehow!. sorry not sure if this makes sense to add to annoy \u2013\u00a0think it would make it hard to work with how annoy builds the tree structure in advance. Very unscientific experiment shows that the time it takes to compute all the distances for search_k = 10000 goes down from 2.5ms to 1.8ms and the whole search query goes down from about 3.6ms to 2.6ms. So that's roughly a 40% speedup. I might throw together a PR, should actually be fairly easy.. > This means indices created with the previous version of annoy can't be read by this version (it will silently assume norms of 0 or whatever's in that spot in the file and return a bunch of negative distances, right?)\nGood observation! But I thought about that (and there's also a unit test to load an \"old\" index). Here's the fix: https://github.com/spotify/annoy/blob/c940823688e7d86387c4ecc04196faa144e0938d/src/annoylib.h#L232\nActually means that loading and using old indexes will be a bit slower since I removed the dot3 function (not by a lot though). Didn't seem worth keeping it.. Added back Manhattan distance using AVX, figured since we already have the code. I factored it out to use template specialization though.. @a1k0n lmk ifyou have any other concerns, planning to merge this today!. ran some more extensive benchmarks using glove-100-angular\nunfortunately the speedups aren't that big, think 100 is just too small for it to make a big difference\n\n. Noticed bizarre thing \u2013\u00a0when I decreased the iteration_steps parameter of the two_means function from 200 to 50, the accuracy went up, from 19% to 23%. When I increased it to 1000, accuracy dropped from 19% to almost 0%.\nI have a theory that too many iteration steps means the \"random\" splits converge to something that is almost identical for all trees. Can't think of anything else!. Think I figured out what's going on. nytimes-256 has \"holes\" in the index, which occasionally causes  the two_means algorithm to end up with a bunch of nans.\nAfter fixing the bug, the accuracy went up a lot. The accuracy went up even further after increasing the number of iteration_steps inside two_means.. cool!. does this really help conserve memory? you still need to map the whole file into primary memory at some point or else querying will be prohibitively slow (on the order of 100-1000x slower than RAM). Annoy essentially does random access, so the I think you'll see a very severe slowdown going to SSD from RAM. According to this link, SSD is about 1500x slower than RAM in terms of latency.\nSo if you can tolerate a slowdown of about 1000x, then I guess this feature is useful.\nI think for Annoy to truly be useful with SSD, you would have to think more about memory locality and have much bigger fan out for the tree so that the number of random accesses is far lower.. I think this PR will make it a lot more stable when building an index that's within [x/2, x] where x is the total RAM on your machine. So that seems like a big benefit. I honestly don't think Annoy will be useful for indexes larger than the RAM of the machine \u2013\u00a0it will be painfully slow given (since the algorithm is built assumiong random access). Hey what's the status of this?. Ok, cool, let me take a look at this and get back to you!. nice, let me know if i can help!. To be honest it's probably not a game changer, more a nice to have. Hope that's helpful.. This looks good \u2013 let me review a bit more thoroughly in the next week or so. My 2nd child was just born last week so I'm a bit behind!. forgot about this but will review shortly!. @ReneHollander this looks good! Some thoughts\n\nI'd like a few more unit tests. Will add myself.\nI think (like you said) the memory management code is a bit messy so I'd appreciate post-merge cleanup\nI think the API is a bit confusing. Thinking about it more I wonder if it really should just be a part of the constructor of AnnoyIndex where either it takes a filename (in which case it builds on disk) or not (in which case it builds in-memory). I think in fact at that point you could even deprecate the save method and remove it in a future version. What do you think? I don't see a reason why you'd want to build it in memory and then save it to disk rather than just build it onto disk from scratch.. thanks for the contribution @ReneHollander !. @ReneHollander i don't think it necessarily has to be that problematic if there's enough of a deprecation period of the save method (like, six months maybe?)\n\ni don't have super strong feelings though. let me know what you think.. (and yeah, 2.0 would be one way to justify backwards incompatibility). annoy probably needs a bit better handling of allocation errors and file operation errors. it's possible the index got corrupted during writing. @Trollgeir can you share the code? Does it only happen for large datasets?. yeah sounds reasonable that it would fail on windows for an index that's around 2GB since that's the limit of an unsigned integer... maybe mmap doesn't work beyond that. yeah that's correct, but definitely feel free to try to fix it if you have any idea!\nwindows does some magic to mmap here, not sure how it works: https://github.com/spotify/annoy/blob/master/src/mman.h. Will push a new version to pypi tonight. What's Chamming distance? Can you send a link to an explanation?. Can you get this one to pass the tests?. This seems like a bug, no idea why it behaves like that.\nLet me try to reproduce. i can reproduce this. figured out what's going on \u2013\u00a0when load()ing, annoy is confusing the number of nodes with the maximum number of nodes. i'm actually not quite sure how to fix this without doing something weird like scanning through the whole index. seems a bit unfortunate. let me think about it.. There's a simple/dumb solution \u2013\u00a0just do binary search to find the largest id with a vector associated with it. I'll hack something together tonight.\nThis is an unfortunate consequence of my attempt to always keep Annoy indexes indefinitely backwards compatible. At some point I'm tempted to drop the compatibility and redefine the binary format.. Yeah the only issue is that the number of items in the index will be underestimated. Everything else should work. hm interesting. as a workaround maybe you can insert \"dummy\" vectors (with all zeros) or something similar for now. Yes \u2013\u00a0it's correct that when Annoy fails to split, it puts them in a random subtree.\nIt's hard to say why it fails to split but typically it means that the point set is degenerate in some way.  For instance many duplicates. It's quite rare actually \u2013\u00a0I haven't seen it for any real world applications for a very long time.. > What I am not sure to understand is the numbers in the error message. It's the number of vectors it fails to put somewhere?\nYes, exactly. Look at the make_tree function in annoylib.h\n\nAre the quantities shown in error message expected for 4 millions vectors and 200 trees?\n\nNo. I would assume you have some kind of weird data distribution with lots of duplicates or something similar. But it's hard for me to know without knowing your data. Can you tell me a bit about your input data?\n. yeah might make sense... i'll think of something. thanks for verifying that it was due to duplicates!. nice!. Btw added a note here \u2013\u00a0we should resurrect the Hamming tests: https://github.com/erikbern/ann-benchmarks/issues/68. Forgot to merge this, sorry. Yeah, this will lead to all kinds of undefined behavior unfortunately.\nIn retrospect I wish I had defined the binary format in a different way... there's no place for \"metadata\" right now. So unfortunately this bug is pretty much \"unsolvable\" until I redefine the format. I think however it's possible to catch many common errors (eg if the index size is not an multiple of the vector size). yeah probably worth closing for now. not sure I follow you @a1k0n \u2013\u00a0euclidean takes a bit more memory since you need to store the split offset. Anyway there's a few checks you can do that have no false positives but sometimes false negatives. \nFor instance on this line we're computing the number of nodes: https://github.com/spotify/annoy/blob/master/src/annoylib.h#L690\n_n_nodes = (S)(size / _s);\nThis should always be a multiple, or else something is really weird. you can't add items after loading an index unfortunately\ngoing to close this since it's a missing feature and kind of hard to implement :(. Sorry, there's no way to add items to an index once it's built. Spotify builds the index from scratch every time :). Maybe you can split it up in several indexes so you don't have to rebuild all of it\nAlso 50M probably won't fit in RAM? So will be very slow. weird, i can take a look. honestly the support for missing vectors is a bit flaky. I just tried running your code and it seems to work well (does not segfault)\ndid you use the latest version? i've fixed a number of issues related to index \"holes\". The latest master also doesn't return 0 as one of the neighbors although I don't think that's on pypi yet.\nCan you please check if latest version of pypi or latest master still exhibits the issue you saw?. sweet, thanks for confirming. i might push the 0 fix to pypi and bump the version to 1.11.6. weird, no idea.\ni might publish wheels for windows soon, stay tuned :). pretty sure it will work!. for a fixed search_k: more trees is always better, up until the point where you run out of memory on your computer\nfor a fixed number of trees: a higher search_k means higher accuracy but at the cost of slower search time\nbasically pick as large of a n_trees as you can afford, and then pick search_k to be a good tradeoff between accuracy and speed. > Thank you for the insight! If I understand correctly:\n\nsearch_k corresponds to some sort of split/thresholding on the k branches of the n_trees trees that were constructed, and can take values between 1 to n * n_trees, where n is the number of nearest neighbors to fetch\nsearch_k = -1 corresponds to search_k = n * n_trees (i.e. maximizing accuracy)\nAm I correct?\n\nNo \u2013\u00a0search_k is just the number of total nodes that will be searched. If you have say 1ms to spend searching, set search_k to eg 10k-100k.  If you set search_k large enough, you end up with basically exhaustive search.. @rbares do you have some alternative code for __popcount64? i guess we can provide an alternative implementation for visual studio using a preprocessor directive. pull request is very welcome @rbares !. Can you share code example?\nClosing this until then!. Ok got it. I'll see if I can repro. Should probably run those as a part of the CI build.. that's weird. is it consistently reproducible i.e. does it happen every time you run the script?. Which version are you using? Did you install it from pypi or are you using the version in git?\nThere are some minor bugs fixed in the git repo that I haven't yet pushed to pypi, that have to do with \"missing\" items.\nBtw @threehundred  it's not recommended to use arbitrary integers for ids \u2013\u00a0the index will still allocate memory assuming all ids are 0 ... max(id)-1. If you insert an item with id 1,000,000 then it will allocate space for 1M vectors. This looks broken still in latest master. Will look at it later.. Closing this, will continue the conversation on #298 . what's your operating system?\nthis is another reason i want to build wheels at some point. can you share full compiler output?. I made some recent changes to fix compilation errors, it's possible it resolved it. I like the addition of const methods.\nDoes the other code actually solve some OS X compilation issue?. Happy to merge this if you can make the unit tests pass.\nWould have a mild preference for splitting this up into separate PRs but not blocking.. do you mind rebasing on current master?. hm this PR is getting pretty complicated. very interesting features though. but can you break it up into a few separate pull requests?. @ViperCraft there's no way i'm going to be able to merge this at this point \u2013\u00a0can you please break it up into a series of smaller PRs if you want this to make it into the repo?. @ViperCraft I appreciate this work, but there's no way this can be merged at this point. Will have to be broken up into several smaller PRs. Going to close this for now.. Sorry, no, I\u2019m an industry practitioner :)\nThink you can just include an URL?. I wrote this library back in 2012, so unfortunately I don't remember, but pretty sure I saw something about it somewhere.. makes sense \u2013\u00a0i can put this in the README.md. does that make sense @eddelbuettel @bartolsthoorn @cai-lw ?. Weird... I could have sworn I had some retry logic for split creation where it would run create_split multiple times\ni'd rather keep the randomization as a last option but try to run create_split a few times in a loop if that makes sense. sorry forgot about this one, but looking at the code i'm confused what's going on. going to close it for now but feel free to let me know if you want me to reopen. interesting \u2013 let me take a look later this week. Hey I completely forgot about this one. Just rebased it. Let me see if it works and if it impacts performance.. sorry forgot about this one, will take a look. how many trees did you use when you built the index?. 10 trees is probably enough! There\u2019s no golden rule for number of trees, more is always better for accuracy so it really just depends on how much disk space you can afford!. yes both disk space and RAM \u2013\u00a0the index will take up the same amount of memory in RAM once you mmap it. This looks nice! One major issue is that the dot product is not a distance function \u2013\u00a0I'm concerned the splits will be really bad for instance. Let me follow up with some more thoughts though.. @psobot take a look at https://github.com/spotify/annoy/issues/44 \u2013\u00a0it contains some older notes that may be useful. \nMaybe also https://www.benfrederickson.com/approximate-nearest-neighbours-for-recommender-systems/ although it looks like it refers to the same xbox paper. Very cool!\nI don't think you need the internal_dimensions thing actually. If you look at the different distance metrics, note that they redefine the Node struct. For instance the Euclidean distance metric uses that to store an extra element that denotes the offset of the plane for each split. So you could just add an extra element to the Node definition for DotProduct and use that.\nSeparately I suspect the preprocessing can be avoided and that you can move that logic into the create_split but that's more speculative and I have to think a bit more about it.. > That's one option, although it saves a lot of custom code to just tack on an extra dimension and defer to Angular for create_split and other functions. (Adding an extra element to the Node definition would require us to override those functions to consider the extra element whenever we do math on an entire vector.) If there's a way to do that without adding too much extra complexity though, I'd be interested.\nI see that point but I think that's possible to do by just overriding D::margin and D::distance or something similar right? The internal_dimensions is a bit hacky to me \u2013\u00a0very unlikely to generalize to any other distance metric. I'd rather keep it contained in the DotProduct class rather than making it a high level concept. > I thought about doing so, but the preprocessing requires a first pass over all of the nodes to compute a global max_norm for the universe. create_split gets called on successively smaller lists of nodes, meaning we'd have to pass along this global max_norm every time we call create_split (and two_means) and if we ever see the same node twice in create_split, we'd end up re-computing the additional dimension multiple times.\nLet me think some more about that. I think it's possible to make the max norm local to each split rather than global but not sure yet.. nice, starting to look good. This is approved except for my comments. sorry forgot about this one. will go through shortly \u2013\u00a0i promise!. haha i forget that this repo is under spotify and you can merge whatever you want :)\ni'll try to find some time tonight!. This looks good. I'm curious what would happen if you update the margin method as per my suggestion \u2013\u00a0would be interesting to see if it has an impact on recall!\nSide note but Annoy could really use a redesign from scratch at some point.... Also this is just tangentially related but what's the purpose of this PR? I did a fair amount of benchmarking and even though dot product logically makes sense for music recommendation, cosine always performed vastly better. It was always a bit of a mystery to me. @psobot so should i merge this?. > Interestingly enough, recall went down by about 2% (at 10 items) in my tests after including dot_factor in the margin calculation. I'm not sure if that means we should leave it out entirely, but if we're trying to match the original Xbox paper, we should include dot_factor for correctness.\nthat's kind of random btw, hopefully within the level of noise. \ud83d\udca5 . I'm not sure what your question is. Annoy doesn't traverse all nodes since the whole point is to avoid exhaustive search. If you need exact answers, some bruteforce method is better, like just relying on numpy.. Yes, that\u2019s by design. You can always set search_k to a larger number if you want higher recall.. sure \u2013\u00a0will do that later tonight. thanks for reminding me. this is now done!. cool, thanks @orf . no, not planned atm :). i think it only works for int64 right now but not sure. I think it has to be int64 since we're using int64 bit count primitives right?\nI suspect if you're using some other size then you might not get the correct bit counts returned correctly\nFYI hamming distance is somewhat experimental and the algorithm probably isn't as good as it could be \u2013\u00a0for instance we're just doing axis-aligned splits so far. I want to do arbitrary splits at some point. Thanks!. my guess is we just need to cast pointer types explicitly. actually looking at it again i'm confused. seems like make_pair is acting up\nmaybe we need to be explicit about what template to use? i think you could do something like make_pair<x, y>(...) right? my c++ is getting more and more rusty. nice!. this seems fine if it's enough to fix the compiler errors. thanks @eddelbuettel \u2013\u00a0sorry, forgot about this. what's ODR?\nnot sure what's going on since there's definitely an #ifndef guard at the top. sure if inlining resolves the issue, feel free to submit a pull request!. thanks!. this sounds too large to be attributable to rounding errors. i think there's some arithmetic issue going on.. Ok, cool. I'll see if I can reproduce this from the Python layer and if your suggestion ((pp - pq) + (qq - pq) rather than pp + qq - 2*pq) fixes the problem then I'll make that change. Unfortunately a \"too smart\" compiler might rewrite that line though.\nBtw if you truly need precision for Annoy, consider using double rather than float. See fix in https://github.com/spotify/annoy/pull/315\nIIRC the performance benefit was actually slightly small (like maybe 5%) for using the latest distance calculation \u2013 the real benefit was I didn't have to have multiple different AVX functions to do different things, but could have a single one (dot) that I can use from different places. > Ah, I see the dot<float> specialization now... would you be willing to consider a Euclidean::distance<, float> specialization, written with AVX and using the more-precise algorithm? Similar to manhattan_distance<float>? I'm happy to try to write this.\nSeems like a bit too much work tbh. Let's merge #315?. interesting. can you check if #315 is enough for what you need though? this PR roughly doubles the amount of arithmetic operations so unless it's truly needed i'd prefer to avoid it. hm ok will think about this some more. -50% is a pretty severe performance hit though.. will take a look shortly. busy life so i don't always have time to jump on pull requests quickly!. Yeah I'm starting to lean towards just reverting my earlier optimization and going back to the (slightly slower) old method. Might as well revert for now so that things are working.\nHave been very busy, hopefully if I have half an hour this weekend I'll take a look.. merging this for now. i'm going to manually revert #272 as well. @eddelbuettel take a look at https://github.com/spotify/annoy/pull/325. Thanks! . nice \ud83d\udc4d \ni'm tempted to change the default to False actualy. thanks @rbares . Thanks \u2013\u00a0appreciate the contribution. something looks weird with the diff:\n\nany idea what's going on?\nanyway thanks for this change!!. haha that's funny. i thought maybe it was something with CR+LF since this was a windows related fix. wasn't too far off.. thanks!. let me see if enabling python 2.7 in appveyor works. nice!. if you have highly discontinuous ids, you will have to maintain a mapping yourself. same thing if your data types are non-ints. you can convert it to 3d-coordinates and use those instead\nsee https://github.com/erikbern/ping/blob/master/plot.py#L11 for instance. Interesting but I'm somewhat skeptical of this since I suspect it would be incredibly slow to do this. Have you tried?. thanks @eddelbuettel !. you will have to map them to integers yourself!. yes just assign integers to each word. yeah should be fine \u2013\u00a0it's a const method iirc so there's no shared state. Annoy isn't really built for out-of-memory nearest neighbor search. Despite the capabilities to do mmap, it still needs fairly quick random access to the underlying vectors. The mmap functionality is good for (a) sharing indexes across multiple process (since every process that opens the index uses the same memory) (b) fast loading of indexes.\nSo I think what you're asking for is a bit out of scope for what Annoy does right now.\nI do agree that at some point it would be really cool if some \"real\" database would offer index support for nearest neighbors, and I wouldn't be surprised if we see that in the (near?) future since nearest neighbor search is becoming increasingly important.\nFor your particular use case maybe one option is to do clustering as a preprocessing technique to get the number of items down from 1B+ to say 10M. Then put the cluster centroids into Annoy. Just a random idea.\nClosing this now since it seems unlikely Annoy will ever implement this but feel free to continue the conversation.. let me know if you have any ideas on how to fix it. actually looks like the header isn't used, i removed it and it's still compiling\nwill submit a PR. Might release a new version in a few days since there\u2019s been a few minor changes. Interesting. No idea what's going on.\n\nThe other becomes the \"fetch\" process which monitors the file system for changes to index files and reloads any changed index.\n\nThis seems fishy. I'm not sure what happens if you mmap a file and then the file changes. I wouldn't be surprised if it leads to complications. Might make more sense to create a new file, but never modify existing files.\n\nOnce the fetch process finishes reloading an index\n\nHow do you reload the index? Again, a bit nervous about reloading. You should probably just create a new index and load a new file from scratch.. > Will investigate further.\nnice, let me know what you find!\n\nWe initialize a new AnnoyIndex object and call .load() to read the file from disk.\n\nOk, this should be fine.. https://stackoverflow.com/questions/12383900/does-mmap-really-copy-data-to-the-memory\nYou might definitely get issues if you modify existing files. However the way the Linux kernel works (IIRC) is if you delete the file and create a new one with the same filename, that's considered a different file.. > We actually write the new index to a temporary file and upon completion move it to replace the old one. Not sure if that is problematic.\nThat should be fine actually. That would be considered a different file by the kernel so there should be no conflict.\nHowever maybe the issue is that the previous underlying file is deleted? not sure what the behavior is for mmap in that case. According to https://stackoverflow.com/questions/51259706/mmap-and-file-deletion-in-macos it should be fine (at least on Mac)\nSeeing comments online saying it migh tbe a problem though. I'm highly confused why this would fail. Just writing to the same filename shouldn't do anything with the existing file that's open \u2013 and the existing test.ann file shouldn't be deleted until all references are closed. Doesn't make any sense to me.\nUnfortunately I can't think of an easy way to catch this error. What OS are you on? You're on Linux right?. I can reproduce the on my Mac laptop and it doesn't crash if I change test.ann to test2.ann on L24 so at least that's a good starting point.\nWill see if I can figure out what's going on. This is highly confusing.\n@a1k0n feels like you might have any good ideas about how mmap would behave if the underlying file is deleted?. > f the underlying file is overwritten (recreated with O_TRUNC), then the contents of the file are replaced and the length of the file becomes 0, then however big it's rewritten to\nOh wow, this is super interesting. I thought overwriting generally is the same thing as creating a new file.\nIt seems like fopen on https://github.com/spotify/annoy/blob/master/src/annoylib.h#L835 really runs an open under the hood with O_TRUNC set. I wonder if I can replace the fopen call with an open call without O_TRUNC\n(this seems like kind of marginal value from a user value, but thought it was interesting so why not fix it). actually probably what i want to do is to check if the file exists and delete it prior to the fopen call. See #339 \u2013\u00a0turns out the fix is a one liner. Yes sounds like the use case it\u2019s built for!. see the README for documentation. See https://github.com/spotify/annoy/blob/master/src/annoylib.h#L788\nIt's building the index in such a way that the index will take roughly twice as much space as just the vectors. There's no way to get the number of trees (should be easy to add a method for it though). > Sure, that works. You could just issue the unlink without checking stat and not caring if the unlink fails, as if it does and the file exists then the create will probably also fail, but perhaps it's better to be safe.\nI think you're right. I did that first in aab358fb4446ca07abef085cb2bcf1ee5aae9d7e but I ran into some issue with Windows that caused me to rewrite that code. In retrospect the first code should work though.. Don't think this warrants a new release so I'll wait a bit until I upload a new version to PyPI. sounds a lot more complicated: https://github.com/travis-ci/travis-ci/issues/2312. nice! can you also add a quick test for this? put something in text/index_test.py.\nthanks!. Thanks @ArielSSchwartz !. sorry, that's not possible. You'd have to use some method to project the sentences to vectors, but then you could def use Annoy to look up similar sentences. No :(. Wouldn't be a bad idea to build a nearest neighbor library on top of Redis though. But quite different scope.. that's wacky. not sure what's going on. well... i could fix it by providing a precompiled wheel i guess. maybe worth doing at some point. it's possible i can add something to setup.py to fix this. let me see. @KatyaRuth thanks for reminding me, let me see if i can fix. Something is broken with installing Mojave on my laptop, so I can't test it myself, but I'll try to just fix it and maybe one of you could check that it's working. @thebarbershop @KatyaRuth can you take a look at #362 to see if it fixes the issue?\nI'll probably just merge it later today, seems highly likely that it will resolve the issue. This has been fixed in 1.15.1 and published to pypi: https://pypi.org/project/annoy/. that should be totally fine out of the box!. this error seems unrelated to annoy!. it's probably related to the compilation of annoy. 4000000 * 128 * 4 = 2G which is the max that Windows can handle afaik. yeah linux should handle >2GB\nthe on disk build bug is separate, i'll look at it at some point. @ReneHollander actually do you know what's going on?. Awesome @renehollander!. That sounds right. with a fixed search_k and varying the number of trees, it should take roughly the same time.\nMore trees will always yield better accuracy (but use more memory). Sorry, there's no way a priori to estimate it. Depends on the distribution of the underlying data. Can you provide more details?. great thanks @kyowill . Not sure what you mean by percentages?. ok, sorry, not clear what that means. going to close this for now. Can you share a reproducible error?. interesting \u2013 should probably define __popcount64 if it's not defined. looks like we should maybe have a fallback version or just implement it ourselves\ni'm happy to rely on this: https://github.com/bengras/minix/blob/master/common/lib/libc/string/popcount64.c. Actually looks like this is the same as #293 . not really, but can't that be accomplished by transforming the data so that the euclidean distance can be used?. shouldn't be super hard to change the hamming distance to add support for jaccard, i think\nnote that hamming distance is somewhat experimental and might be far from optimal\nclosing this for now!. Thanks @Calinou . Will merge this in a couple of days. Will upload new version to pypi today. 1.41 is the square root of 2 \u2013 see https://github.com/spotify/annoy/blob/master/src/annoylib.h#L959. the euclidean distance of the normalized vectors is sqrt(2). See the readme at https://github.com/spotify/annoy\na.get_distance(i, j) returns the distance between items i and j. NOTE: this used to return the squared distance, but has been changed as of Aug 2016.\nAnnoy uses Euclidean distance of normalized vectors for its angular distance, which for two vectors u,v is equal to sqrt(2(1-cos(u,v))). 49152 dimensions is extremely high. Annoy is built for a few hundred. Try doing some dinensionality reduction of your data?\nNot sure why it hangs though . 49152 * 48000 * 4 is about 9GB\nMaybe what's happening is your computer runs out of RAM if you build with many trees and it has to resort to swap space. That will typically be 100-1000 times slower (depending on the disk)\nClosing this for now since this seems a bit like an extreme case. Annoy doesn't support pickling. Why are you trying to pickle? Just save the index to disk.. there's no foolproof way to catch that the metric isn't right (it's not stored inside the index)\nthat being said there's a few tricks you can use so maybe we should add those. Can you send the full code? There's something I'm missing here. Are you trying to save the same index multiple times? It's not possible (once you save an index, it will become immutable)\nI should probably improve the error message, but not sure if this is a bug per se. Are you trying to save the same index multiple times?. Or are you trying to save an index that hasn\u2019t been built?. > One important thing here is that the indexes in memory should be saved in files, and then stay in memory the be used in training.\nI don't really see the point of saving in that case. Why are you saving?\nI feel like this isn't really a bug as much as just confusing exception message. Let me see if I can come up with a simpler case.. Looked at the code and yeah, you're trying to unbuild a saved index. That's not possible with Annoy.\nWhen an index is saved, it's immutable from that point on.. sorry, there's no such capability in annoy!. Can you share code and any error messages?. sorry, there's no way to get that data!. you are right. let me fix it\n. it's just for random splits anyway, should be fine\nactually the reason i use normal distribution is just that it's a pretty easy way to generate random unit vectors. maybe there's some even simpler way\n. There is an outer loop that will repeat the sampling up to 10 times anyway, so it shouldn't be a problem.\nBut sure maybe I should fix it\n. I fixed it later.\nYes you can do something like that too. I wonder if there are some cheap tricks in practice where you can compute statistics on subsamples, eg. sample 10 random points and pick the pair of points that gives the most even split. Sounds a bit more messy to implement though!\n. Yes it makes sense. I have no idea what's the implications on runtime of any of these ideas though \u2013\u00a0would have to implement it and try. The beauty of new method is how trivial it is (only a few lines).\nOne pitfall if you try to be \"too smart\" is also that the splits end up being too nonrandom. You still need to have a fair amount of randomness to decorrelate the trees. There might be some smart way to achieve the same thing but I have no idea what it would be\n. It's not always an even split \u2013 you could end up having 1 point on one side and n-1 points on the other side. Or what did you mean?\nYes you need to be careful about not consistently separating the same points that are close to each other. That's why you need to build an index with multiple trees. Any given tree might separate two close points, but it's unlikely they are separated in all trees\n. Btw I think an easy generalization from 2 to k would be: sample k (where k is even) points, compute the principal component, split on the median.\n. was thinking power iterations. i think you can do it in a few lines but i don't remember the math.\n. that makes sense. you could even subsample the indexes within each power iteration. so every iteration you just multiply with a bunch of randomly sampled vectors.\ni can try this later this week, should be a few lines of code\n. @MrChrisJohnson not sure if I understand the O(k) algo you're describing\n@a1k0n yes as i mentioned the reason you use different trees is to make sure you don't consistently separate the same two close points\nbut so far annoy has erred too much on the side of uncorrelated trees and too little on using the  distribution of the points. i think you could probably go a little further leveraging the distribution of the points\ni'll prob just keep the 2-point solution for now. should be enough\n. Ok I get what you are saying. Makes sense.\nI think you still want to pick the first vector not uniformly randomly but somehow skewed to the distribution of the data set. If you have a very high dimensional dataset that is basically some low dimensional hyperplane H plus a bunch of noise then the new 2-point sampling algorithm will generate splits approximately perpendicular to H which seems beneficial\n. I think this could cause nasty things \u2013\u00a0you have a function that should return a PyObject* but without a \"return\" statement it will just read some random memory off the stack and treat as a PyObject*\nIn Python returning None is the same as no return statement so I think you can do return Py_None; here and everything should work\n. yeah you got a bunch of segfaults... but try what I said and it should work\n. i'm not sure about this refactoring \u2013\u00a0can you explain why you moved the sampling to here instead?\n. why did you remove this method?\n. i think that's a leaky abstraction \u2013\u00a0what if at some point later another distance function has a very different way to create a split plane?\n. cool will fix\n. it's a speedup trick. there's some magic numbers here though, would be nice to avoid\n. not sure, i can try. when i work with templates i really just add/remove \"typename\" and add/remove template arguments randomly until it compiles :)\n. i think i had it there all the time when i was testing, must have pushed the wrong code for some reason.\n. will do. this code would probably benefit from some refactoring but i don't really have the energy/competence\n. no \u2013\u00a0doesn't change anything except the values returned by get_distance\nstill on the fence about merging this since it might break existing users, but I think i'll merge it into master and wait a few weeks to publish it to pypi\n. haha what's up with two github accounts?\n. probably not useful to allow threads here since the operation should take a few microseconds, but also doesn't harm\n. cool yeah i don't think you need to go crazy with it, just run build and unbuild a bunch of times and make sure things don't crash :)\n. oops should have removed this line\nwhatever. IIRC there's actually two places that uses std::copy in the code.\nmight be worth making this a macro btw so that we don't clutter the code with too much compiler specific stuff. this should be f / chunksize() rounded up right? so i think it should be\n(f + chunksize() - 1) / chunksize() instead?. replace \"i\" with \"i|i\" and set int n_threads=1 as the default value. that way users don't have to provide a value (it will default to 1). see comment higher up on default argument \u2013\u00a0should be no reason to force the user to provide this argument by default. should use two spaces here, not tabs. also this function shares a lot of code with build, would be nice to merge back. what's the reason we need to add support for c++11?. should pass mutable datastructures as pointers, not references, i.e. vector<S>* roots. annoy doesn't use boost any more since it caused a lot of problems installing annoy, so i'm not too excited about reintroducing it. could maybe just have some #ifdef magic and only support threading on posix systems?. seems easier if each thread just returns a vector of roots and the main thread pushes them into the roots array. would avoid anything that require any linking with libraries just because it gets more complex to install. ok seems like that's the logical conclusion :). no reason to make these global. strange! i can fix later. do you mind replacing this with random.sample(range(2000), 1000) to make it a bit uneven?. think this could be just (T)child_nr but not blocking. i'm planning to change the splitting code for Hamming distance anyway, so not important. actually this function doesn't seem to be used?. why is this functino actually needed? Shouldn't the distance function already use AVX when possible?. I wonder if rewriting it this way will be slower because of cache locality?. actually realized the AVX version I just removed had some speedups that were unique to it -\u00a0when it updates iv and jv it actually won't benefit from AVX in the current version. I'm tempted to make that sacrifice since tree building isn't usually a big issue (Annoy is already very fast) and I found the AVX version to be much harder to read (and creates a bit of code redundancy). also possibly not quite as fast because of cache locality? feels like @a1k0n would know. hm yeah actually now that I'm looking at the AVX code, it definitely benefits from doing all of those at the same time (can load into a vector only twice, not six times)\nlet me add another specialized function. already have a fix coming up, 1 sec. yeah, exactly. children isn't used for leaf nodes. could probably preallocate search_k items (not super important). where is p defined? i'm not sure if this code is a simplification. is there a reason we need to rely on c++11? i'd rather drop that if possible. what's this needed for?. what's this needed for?. probably shouldn't mix calloc and malloc :). please use malloc here to make it consistent. why would different distance functions have different implementations? I would rather make it consistent. not that it matters but why did you change this?. nice!. also nice. this looks like jaccard similarity to me? anyway why is that needed? in all the other tests we use recall. shouldn't this just be the euclidean distance between the vectors including the extra element?. doesn't entirely make sense to me that this returns the negative dot product. maybe should just return the positive dot product? either way it's not a \"distance\" in the correct meaning of the word\nhttps://en.wikipedia.org/wiki/Metric_(mathematics)#Definition. don't think this memset is needed\ni guess the for loop could just be replaced by a memcpy though. why did you change this?\nif anything should probably change off_t size to size_t size but i don't remember the difference between off_t and size_t. might just be some win specific thing. ok i'm very confused now, let me take a deeper look later today. i think we use the terminology \"distance\" for multiple separate things.. @yonromai the reason i originally brought up euclidean distance was that i thought the distance method was used internally somewhere for the splits.\nif it's only for external consumption then i don't have any strong feelings, i guess positive dot product is fine. just need to document it since \"distance\" is a bit of a misnomer. this is implementation inheritance, which i'm not a super big fan of. but not blocking. shouldn't this include the dot_factor?. isn't this a reimplementation of the superclass method?. this is nice. however i think the second line has to be Node<S, T>* q = (Node<S, T>*)(nodes_buffer + s)\n. is there an easy way for you to figure out why it's crashing? it doesn't crash on linux and in appveyor. suggestion\n  static inline int std_popcount(T v) {\n    // Note: this is only used with Visual Studio. might want to add a comment like ^. maybe just doing a if(!indices.empty()) gate would resolve it on all platforms\nweird though. oh i bet indices[0] will cause a memory access violation if indices has zero size. that's funny.\neasy fix though. i can do it myself.. ",
    "piskvorky": "Compilation OK now, thanks!\nBut tests still fail, in the same way. I'm thinking these two were unrelated issues.\nNot sure where to look for problems. Seems like a big in-your-face failure => easier to debug, but I don't have time for that and Amazon is billing me per hour :(\nI can still try some quick debugging if you have any suggestions where to look.\n. Background: I decided to run all the shootout tests from scratch, on a \"reproducible\" machine = EC2.\nAnnoy was returning rubbish accuracies so I dug down, found out it's nothing to do with the corpus, as the same thing happens with arbitrary data and even with the annoy's unit tests.\n. I forgot to say I tried -DNO_PACKED_STRUCTS as well :)\n. Downgraded gcc and g++ to version 4.6 => all good, tests pass now. No special compile defines needed.\nI still don't know what exactly the problem was, but feel free to close this issue if you wish. Annoy works for me again.\n. Yeah, I assumed it must be something compiler-specific, since Annoy has always worked for me with \"older\" machines (=older package versions). That's why I tried downgrading!\n(OS info is in the first post; haven't tried on Windows)\n. @eddelbuettel these issues with ubuntu & gcc 4.8 were reported in #3 . Maybe the workaround there could work for you too?\n. Correct.\n. Brilliant.\nIf adding a printf helped, it must have been some over-eager optimization (not done by the older compiler...).\nThanks Erik, I see poking you periodically has great effect. Note to self: try more of that :)\n. +1 on verbose. The prints look distinctly out of place in apps that have more complex logging set up, log to files/http instead of stderr etc.\nIdeal would be plugging into Python's standard logging module somehow, but I realize it's a non-trivial call to get that in cleanly.\n. Good one Erik!\nLooking forward to issue #64.\n. +1 on dropping Boost.\n. I bet you would :)\nWe started an \"incubator\" thing where we assign open source improvements & algos to students (a year project or thesis or whatever). Mostly gensim, but annoy is a good fit too.\nThis task may be a bit too non-academic, but if someone suitably hardcore turns up, I'll direct them here. Would be great to have Annoy more accessible and noob-proof.\nCC piskvorky/gensim#51\n. I for one wouldn't like Annoy to move toward doubling as a datastore. I really like Annoy's current clear, targeted focus.\n@malthe , there are many lightweight ways to supplement the raw ids with additional metadata: built-in dict, built-in shelve/anydbm,  sqlitedict, numpy & mmap...\n. I don't think so; and any workaround would likely be too slow (swapping during indexing/search). Annoy targets the scenario where the whole index fits in main memory.\nA scalable solution here would be distributing the index across multiple nodes, in a cluster. But that's not on the roadmap for Annoy AFAIK. Interesting and not that difficult algorithmically, but it would bring a lot of devops complexity.\n. Nice. Calling list(numpy array) doesn't change the dtype of its elements though. It's still a list of numpy.float64 or float32 or whatever. \nI forgot how Annoy handled such cases, but isn't there a potential for segfault here as well?\nIIRC I had to always manually re-cast Annoy inputs to float, in my benchmarks.\n. No, it won't. It is a common grievance, because you end up with numpy types inside plain Python structures.\nSometimes you don't notice until you try to serialize them and things blow up, such as when sending the objects through an HTTP interface. For example, json cannot serialize numpy types by default, and will throw an exception that will leave you scratching you head, unless you've seen it before :)\n``` python\n\n\n\na = numpy.arange(10).astype(numpy.float32)\nl = list(a)\njson.dumps(l)\nTypeError: 0.0 is not JSON serializable\n```\n. Yeah, I saw you added unit tests for that, great idea.\n\n\n\nThere must be some NumPy / Python C API conversion magic that takes care of this implicitly... though I don't recall where exactly :)\n. Just wondering... what does the move entail?\nTo be honest erikbern/annoy sounds a little safer to me, but I may not understand all the implications here.\n. That's what I call rapid development! :)\nBtw PyPI treats rc1, rc2 tags as proper releases, which pip will ignore when users ask to install (a good option for release checking).\n. Also somewhat related: this saga of improving Annoy baseline benchmark using BLAS (vectorization + memory caches + parallelization).\nSuch optimizations had greater effect there than here (as brute force has to do more low-level work; memory access patterns matter more) but it will be exciting to see this stuff in Annoy! Erik is pushing Annoy to ever more greatness :)\n. (if you're on linux, both ps and top can display the number of page faults, which can be useful for debugging such issues)\n. @eldor4do The \"A\" stands for \"Approximate\". SVD tip: if you have a large dataset, you'll run out of memory quickly with sklearn. For fast and memory efficient truncated SVD, you can use gensim SVD\u00a0instead.\n@erikbern NP-complete... hmm. In what var? A brute force linear scan will find the nearest items 100%, and is linear (in the input data size).\n. mmap. Another one of Erik's weekend coding projects? :). @mazorigal have a look at https://github.com/spotify/annoy/issues/96 for a previous discussion.. +1 on single precision.\nIt should make a significant difference in practice (near ~2), both for memory & speed. @erikbern maybe the compiler wasn't generating SIMD/SSE for your CPU properly? Still, I'd expect a speedup, even if just due to more effective caching.. Hah, I misread completely. My apologies.\nI thought it was weird -- vaguely remembered Annoy being single precision :). Seems @jklapuch wants to construct an Annoy index in parallel, using several workers. How does a read-only mmap'ed index help here?. Memory mapping is one of the main features of Annoy. I'm not aware of a DB (besides the file system) that would allow you to mmap data to virtual memory.. \"Rapidly changing data\" doesn't seem a good fit for Annoy. There's no dynamic update/delete, and the beauty of Annoy comes from its indexes statically residing on disk (mmap), so even add is not straightforward.. What clustering?. ",
    "jcspencer": "I don't have much experience with C++ at all, but it looks as if all that needs to be done is to move Line 504 in annoylib.cc and below into a new, python specific file. I tried, but with my little C++ experience, I couldn't find the correct namespaces. Is there any chance someone else knows how to do this?\n. Sounds good\n. Will these changes be pushed soon?\n. ",
    "herrbuerger": "While stripping down the code to get a minimal working example, I found the actual cause of the problem.\nI was generating quite long integers that I have used as identifiers (see https://gist.github.com/herrbuerger/5f22ee159d0df800c844). Once I had changed them back to smaller numbers, everything went smoothly.\nHere is a minimal working script to reproduce the issue: https://gist.github.com/herrbuerger/e4452d55ffb03a53d423\n. I have found another issue regarding the identifiers, not sure this is related. If not, a separate issue would probably make sense:\nhttps://gist.github.com/herrbuerger/6cbeada92901a282c957\nIf you check the output of the script, you will see that annoy shows identifiers which have never been used. Took me a while to catch this :)\n. A hashtable would be great. This would definitely solve my problems.\nStill, I have a working example with the integers now as well and the results look pretty good. However, I will have to create a lookup table, which will probably be a challenge itself depending on the number of items.\n. ",
    "rogueleaderr": "Works! Appreciate the prompt fix. :)\n. ",
    "dina-iofe": "I used gcc 4.6 for building annoy as it was suggested in issue #3 , but it didn't help me.\n. ",
    "ogrisel": "I reproduce similar failures under Python 2 and Python 3 on Debian Jessie (in a docker container) with libboost-python-dev 1.55.0.2 and gcc 4:4.9.1-1.\n. Here are the exact failures on my host (current master):\n```\nFAIL: test_get_nns_by_item (_annoy_test.AngularIndexTest)\nTraceback (most recent call last):\n  File \"/code/annoy/test/_annoy_test.py\", line 48, in test_get_nns_by_item\n    self.assertEqual(i.get_nns_by_item(0, 3), [0,1,2])\nAssertionError: Lists differ: [0, 1] != [0, 1, 2]\nSecond list contains 1 additional elements.\nFirst extra element 2:\n2\n\n[0, 1]\n[0, 1, 2]\n?      +++\n\n======================================================================\nFAIL: test_get_nns_by_vector (_annoy_test.AngularIndexTest)\n\nTraceback (most recent call last):\n  File \"/code/annoy/test/_annoy_test.py\", line 38, in test_get_nns_by_vector\n    self.assertEqual(i.get_nns_by_vector([3,2,1], 3), [0,1,2])\nAssertionError: Lists differ: [0, 1] != [0, 1, 2]\nSecond list contains 1 additional elements.\nFirst extra element 2:\n2\n\n[0, 1]\n[0, 1, 2]\n?      +++\n\n======================================================================\nFAIL: test_large_index (_annoy_test.AngularIndexTest)\n\nTraceback (most recent call last):\n  File \"/code/annoy/test/_annoy_test.py\", line 100, in test_large_index\n    self.assertEqual(i.get_nns_by_item(j, 2), [j, j+1])\nAssertionError: Lists differ: [38, 39] != [130, 131]\nFirst differing element 0:\n38\n130\n\n[38, 39]\n[130, 131]\n\n======================================================================\nFAIL: test_large_index (_annoy_test.EuclideanIndexTest)\n\nTraceback (most recent call last):\n  File \"/code/annoy/test/_annoy_test.py\", line 136, in test_large_index\n    self.assertEqual(i.get_nns_by_item(j, 2), [j, j+1])\nAssertionError: Lists differ: [32, 33] != [394, 395]\nFirst differing element 0:\n32\n394\n\n[32, 33]\n[394, 395]\n\n======================================================================\nFAIL: test_precision_10 (_annoy_test.EuclideanIndexTest)\n\nTraceback (most recent call last):\n  File \"/code/annoy/test/_annoy_test.py\", line 161, in test_precision_10\n    self.assertEqual(self.precision(10), 1.0)\nAssertionError: 0.2 != 1.0\n======================================================================\nFAIL: test_precision_100 (_annoy_test.EuclideanIndexTest)\n\nTraceback (most recent call last):\n  File \"/code/annoy/test/_annoy_test.py\", line 164, in test_precision_100\n    self.assertGreaterEqual(self.precision(100), 0.99)\nAssertionError: 0.89 not greater than or equal to 0.99\n======================================================================\nFAIL: test_precision_1000 (_annoy_test.EuclideanIndexTest)\n\nTraceback (most recent call last):\n  File \"/code/annoy/test/_annoy_test.py\", line 167, in test_precision_1000\n    self.assertGreaterEqual(self.precision(1000), 0.99)\nAssertionError: 0.899 not greater than or equal to 0.99\n\nRan 15 tests in 3.366s\n```\n. Note @dina-iofe you can use triple backticks block to escape the spurious markdown formatting of the Python traceback in your github reports. You can edit the description of this issue to fix it.\n. The test failures are not related to Python 3. I can reproduce the similar failures with Python 2.7 on the same host.\n+1 for merging this PR and dealing with the failures in another PR.\n. I did not mean to change that line. Let me amend my commit.\n. ",
    "eddelbuettel": "For what it is worth I am getting the same result -- ie  a two-element vector where three are expected -- under Ubuntu 14.10 using Python 2.7, g++ 4.8, Boost 1.54.\n. If you are referring to downgrading to g++-4.6, that is not a fix but a crutch.\n. To provide a bit more context, I amused myself in writing a complete little R package for it, and if I ever were to upload this to CRAN it would have to behave under current compilers.  But I presume @erikbern wants the behaviour to be correct in any event.  I glanced at this, but sadly did not grok where / how the change occurs.\n. Super.I'd also be happy to test -- I do have g++ 4.6 (and both older and newer), plus some clang versions and should be able to get most other things via Docker reasonably quickly.   But I have no idea where this may have gone off in the code.\n. Awesome.  As I mentioned in passing, I set your nice header-only library up for access from R yesterday (in a new repo rcppannoy in my account).   \nWe can compare notes in a few days -- I would expect the mmap files to be portable as well (hell, it is just a bunch of atomistic types written by the C/C++ library layer). \nI didn't tout that yet as I'd love for it to behave as the Python tests suggests so I really appreciate that you are digging through this.\n. It must be. If the older compiler let it through, and the newer (\"generally better\") ones do not, some silly corner condition is probably raising its pimpled head.   Anyway, I'm off for a run now :)\n. Sweet, I'm back from the long run, bug has been squashed and I have been tweeted :)  \nThanks for the fix.\n. Bearer of bad news over here.  My R build, using a 100 per cent plain copy of your annoylib.h, still returns [0, 1] only.   Crap.\n``` r\n!/usr/bin/r\nlibrary(RcppAnnoy)\ntestGetNNsByVector <- function() {\n    f <- 3\n    a <- new(AnnoyAngular, f)\na$addItem(0, c(1,0,0))\na$addItem(1, c(0,1,0))\na$addItem(2, c(0,0,1))\n\na$build(10)\n\nprint(a$getNNsByVector( c(3,2,1), 3 ))  # c(0,1,2)\n\n}\ntestGetNNsByVector()\n```\nand I get\nbash\nedd@max:~/git/rcppannoy(master)$ ./tests/testGetNNsByVector.R \npass 0...\npass 1...\npass 2...\npass 3...\npass 4...\npass 5...\npass 6...\npass 7...\npass 8...\npass 9...\nhas 23 nodes\n[1] 0 1\nedd@max:~/git/rcppannoy(master)$\nIdeas?\nHere is how I built:\nbash\nedd@max:~/git/rcppannoy(master)$ R CMD INSTALL .\n* installing to library \u2018/usr/local/lib/R/site-library\u2019\n* installing *source* package \u2018RcppAnnoy\u2019 ...\n** libs\nccache g++-4.8 -std=c++11 -I/usr/share/R/include -DNDEBUG -I../inst/include/  -I\"/usr/local/lib/R/site-library/Rcpp/include\" -I\"/usr/local/lib/R/site-library/BH/include\"   -fpic  -O3 -Wall -pipe -Wno-unused -pedantic  -c annoy.cpp -o annoy.o\nccache g++-4.8 -std=c++11 -shared -L/usr/lib/R/lib -Wl,-Bsymbolic-functions -Wl,-z,relro -o RcppAnnoy.so annoy.o -L/usr/lib/R/lib -lR\ninstalling to /usr/local/lib/R/site-library/RcppAnnoy/libs\n** R\n** demo\n** inst\n** preparing package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** testing if installed package can be loaded\n* DONE (RcppAnnoy)\nedd@max:~/git/rcppannoy(master)$\nso  g++-4.8 with some optimization.  Plain Ubuntu 14.04 otherwise.\nTurning off C++11 has no bearing (but gets me warnings on long long types).\n. And for completeness with g++-4.6 it passes:\nbash\nedd@max:~/git/rcppannoy(master)$ R CMD INSTALL .\n* installing to library \u2018/usr/local/lib/R/site-library\u2019\n* installing *source* package \u2018RcppAnnoy\u2019 ...\n** libs\nccache g++-4.6 -I/usr/share/R/include -DNDEBUG -I../inst/include/  -I\"/usr/local/lib/R/site-library/Rcpp/include\" -I\"/usr/local/lib/R/site-library/BH/include\"   -fpic  -g -O2 -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security -D_FORTIFY_SOURCE=2 -g  -O3 -Wall -pipe -Wno-unused -pedantic  -c annoy.cpp -o annoy.o\nIn file included from /usr/local/lib/R/site-library/BH/include/boost/random/additive_combine.hpp:23:0,\n                 from /usr/local/lib/R/site-library/BH/include/boost/random.hpp:36,\n                 from ../inst/include/annoylib.h:33,\n                 from annoy.cpp:5:\n/usr/local/lib/R/site-library/BH/include/boost/cstdint.hpp:300:35: warning: use of C++0x long long integer constant [-Wlong-long]\n/usr/local/lib/R/site-library/BH/include/boost/cstdint.hpp:300:35: warning: use of C++0x long long integer constant [-Wlong-long]\n/usr/local/lib/R/site-library/BH/include/boost/cstdint.hpp:300:49: warning: use of C++0x long long integer constant [-Wlong-long]\n/usr/local/lib/R/site-library/BH/include/boost/cstdint.hpp:300:105: warning: use of C++0x long long integer constant [-Wlong-long]\n/usr/local/lib/R/site-library/BH/include/boost/cstdint.hpp:300:105: warning: use of C++0x long long integer constant [-Wlong-long]\n/usr/local/lib/R/site-library/BH/include/boost/cstdint.hpp:300:105: warning: use of C++0x long long integer constant [-Wlong-long]\n/usr/local/lib/R/site-library/BH/include/boost/cstdint.hpp:300:123: warning: use of C++0x long long integer constant [-Wlong-long]\n/usr/local/lib/R/site-library/BH/include/boost/cstdint.hpp:300:195: warning: use of C++0x long long integer constant [-Wlong-long]\nIn file included from annoy.cpp:5:0:\n../inst/include/annoylib.h: In member function \u2018void AnnoyIndex<T, Distance>::build(int) [with T = float, Distance = Angular<float>]\u2019:\nannoy.cpp:18:29:   instantiated from \u2018void Annoy<T, Distance>::callBuild(int) [with T = float, Distance = Angular<float>]\u2019\nannoy.cpp:62:58:   instantiated from here\n../inst/include/annoylib.h:268:7: warning: ISO C++ does not support the \u2018z\u2019 gnu_printf length modifier [-Wformat]\n../inst/include/annoylib.h: In member function \u2018void AnnoyIndex<T, Distance>::build(int) [with T = float, Distance = Euclidean<float>]\u2019:\nannoy.cpp:18:29:   instantiated from \u2018void Annoy<T, Distance>::callBuild(int) [with T = float, Distance = Euclidean<float>]\u2019\nannoy.cpp:81:60:   instantiated from here\n../inst/include/annoylib.h:268:7: warning: ISO C++ does not support the \u2018z\u2019 gnu_printf length modifier [-Wformat]\ng++-4.6 -shared -L/usr/lib/R/lib -Wl,-Bsymbolic-functions -Wl,-z,relro -o RcppAnnoy.so annoy.o -L/usr/lib/R/lib -lR\ninstalling to /usr/local/lib/R/site-library/RcppAnnoy/libs\n** R\n** demo\n** inst\n** preparing package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** testing if installed package can be loaded\n* DONE (RcppAnnoy)\nedd@max:~/git/rcppannoy(master)$ ./tests/testGetNNsByVector.R                                                                                                                                                        \npass 0...\npass 1...\npass 2...\npass 3...\npass 4...\npass 5...\npass 6...\npass 7...\npass 8...\npass 9...\nhas 23 nodes\n[1] 0 1 2\nedd@max:~/git/rcppannoy(master)$\n. @erikbern Quick ping just to reiterate the last two comments:\n- passes with g++-4.6\n- fails with g++-4.9 \nso it seems that the change to std::copy() help for the Python case but not in general.\n. I'd be happy to hold your hand on that if you need help. I was also thinking that maybe just creating a quick standalong C++ version of the 1st test would help.\nAs for the optimization and flags: Some come from R itself, some come from how the Debian package calls configure when R is built (Disclaimer: I'm the maintainer for that package), some come from default Debian flags.\n. @erikbern I just pushed to GH what I had here.  I omitted some microchanges such as adding the loop instead of copy() and adding printf() -- which did not help on my side.\n. I will add that I haven't quite grok'ed how you grow and walk the tree -- the indices, set to be sometimes overwritten, sometimes makes me a wee bit dizzy.  Sorry that I can't be of help.\nAnd glad you found the Rcpp and BH packages etc. \n. Re the C++ abuse: I am a very happy user of STL types and don't really do the old-school C vector manip any more.  But you're in control of where your performance gains come from :)\n. I meant that you seem to use an idiom of   T v[N];  and liberally go beyond N.\n. Looks great so far.  Will commit on my end.  \nThis may have fixed it.  I will make sure I add proper unit tests on the R shadowing all of yours, will enable Travis for R (as I do with all my other projects) and then we should be ready to pounce. The changes will likely have to wait for tomorrow though.\nThanks a metric ton for the fixes.\n. I just carried this over. I don't see old Boost anymore so not sure what kind of man-to-man combat is needed there.  \nBut I like the #define. I have something similar I need to run by you:  R won't let me use straight fprintf(stderr, ...), instead we use REprintf(...) to be within R's buffered IO.  Ok if I create a #define PrintError(or justPRINT) which does yourfprintf(stderr.by default else myREprintf(?\n. Yep. I was also thinking about new member variableverboseto toggle that.\n. Ack.  Any idea how to turn the compiler warning off?  Maybe justint pinstead ofint p[0]?\n. Maybe useT v[1]; T pv = &v[0];and usepvwhere you usedvbefore? \n. Maybe replace a nakedsizeof()with aninlinehelper that does the, err, \"correction\" you outlined earlier?\n. I get those too under theg++warnings I usually enable, ie with-pedantic`.\n. Cool. I understand the preference for UPPERCASE. Makes it crystal-clear where it came from -- an old evil macro (and the C++ fanboy in me always cries when I resort to SCREAMING MACROS).  Will update.\nWill check your #30 for comparison.\nWas about to create another PR for the verbosity flag. Lovely that I don't have to. :)\nAnd dict and jargon just failed me: what is LGTM?\n. Ahh. Thanks. \nDo you want me to expand the PR by sticking a bool member variable (or static bool in the file, tickled by a member function of AnnoyIndex()) in, or do you want to take it as is -- small and simple?  Tests fine from R as well but I am waiting for you here...\n. Here is a simple addition of a verbosity toggle and setter.  \nWhen I just had this run through Travis, Python 2.6 passed but 2.7 failed for one of the 'precision' tests.  That seems ... random.  Do we need to to set the RNG there?\nFWIW it passed nosetests for me with 2.7 before I committed and as you can see I did not touch any actual index calculations...\n. Ok, assuming it was spurious I restarted that half of the build and it passed on the second try:\nhttps://travis-ci.org/eddelbuettel/annoy/builds/40780314\n. And it passed for you too:\nhttps://travis-ci.org/spotify/annoy/builds/40780876\n. Oh, another micro-aspect:   we could of course make showUpdate() a member of AnnoyIndex. But as it is a sort-of configuration choice it may be cleaner to leave it at the top.  Doesn't really matter either way, does it?\n. Had not seen #32 -- I wrote #33 on the train in and committed but noticed that my previous one was #31 so I wondered :)\n. We can close this now, of course, in lieu of #37 by @thirdwing.\n. From a cursory glance this does not address the issue my PR addressed.\n. This usually does it:\nCXXFLAGS +=           -O3 -Wall -pipe -Wno-unused -pedantic\nIt was a vanilla signed vs unsigned comparison.\nCome to think about it, maybe you even want to template as uint32_t rather than the current int32_t (which is better for me as R uses just (signed) int and double).\nOh, and on my main worker box it is g++-4.8 from Ubuntu 14.04.\n. Howdy -- thanks for the merge.  I'll submit with an error log motivating a change next time.\nAgreed re the unsigned/signed. It is good to -1 etc in reserve.\nAnd lastly, if I may: discovered Docker yet?  Becoming a huge fan (and doing some work here and in other repos) as it makes eg running a different compiler super easy...\n. Thanks.  Do you think you'll call this 1.2.2?\n. I don't understand how I didn't catch that yesterday as I kept the annoylib.h in sync -- but yes, when the #define I use is set, we end up with two } and hence in tears.\nIt would be cleaner to roll up a new tarball.  And heck, that is what the third digit is for :)\n. :+1: \nWill roll up RcppAnnoy 0.0.6 once I see 1.2.2 -- so far I only see a change to the README.rst\n. See, I was watching setup.py and it had not moved :)\nNow it's all good.  I'll get to work too.\n. No, that is not it:\nedd@max:~/git/annoy(master)$ git rebase origin/master \nCurrent branch master is up to date.\nedd@max:~/git/annoy(master)$\n. yes, all good now -- looks like we're in sync\nand sorry by the way for the RAND_MAX egg I put up there....\n. Hm, can't speak to AppVeyor blowing up.  \nTravis passed for me on all subtests, including Go.  Any idea why swig all of a sudden goes funky on you?. Yay for random flakyness. Now we're 2 for 2.  . I used the following which produces something closer to what the R Journal,  JSS, ... use for CRAN packages:\n@Manual{Github:annoy,\n  author =       {Erik Bernhardsson},\n  title =        {Annoy: Approximate Nearest Neighbors in C++/Python},\n  year =         2018,\n  note =     {Python package version 1.13.0},\n  url =          {https://pypi.org/project/annoy/},\n}\nThe publication really is the PyPi upload, so versioning and dates make some sense.  To me at least :grinning:. I can probably test something.  My Docker container with this is still up.\n. Good call on the cast.  A simple   static_cast<S>(...) was all it took: https://github.com/eddelbuettel/annoy/commit/dbe5bfec22b07f14085c0d3b8760d45ac621cdf6\nI send a PR if it passes Travis.  . Poking @erikbern :  anything else we need to do before merging a two-liner?. No worries, \"been there, done that\" too.. Hi @erikbern if you get a chance to glance at this -- we are basically holding an update to RcppAnnoy back waiting for your view on this.. And you followed up with #322.  Should we be good now to syncing to RcppAnnoy and releasing there?\n. I see one wart: your current load() and save() version add a new parameter without a default, invalidating all existing signatures.  Could that bool be set to false?. Perfect, thanks @erikbern. I had approximated this locally.. That's a good one.. Tested, and now committed, here. . ",
    "javier-artiles": "Sorry, just found I was missing libboost-all-dev\nI't compiles without problems after installing that package.\n. ",
    "LongbinChen": "hi, Eric,\n   thanks for the comments.\n   I am building an image search engine whose core ranking function is knn.\nI hope to build something like Solr that has the functionality of\n add/delete/update/filtering/sort, and probably with replication if i have\nto shard the index.\nI like the mmap which can reduce the memory requirement when serving a\nlarge index. The function \"add_item_to_index\" would require the serving\nmachine have big memories which might be a bummer. i will try to make it\nwork so that even when building index, we don't need a large memory.\nI didn't run the unit test because I don't know how to. can you let me\nknow how to run the unit test? thanks.\nOn Sat, Feb 28, 2015 at 9:24 AM, Erik Bernhardsson <notifications@github.com\n\nwrote:\nThis looks pretty cool overall. I really want to keep mmap and avoid\nadding the \"parent\" field.\nThere's also a lot of commits in this branch. Seems like only the first\none is relevant for the PR. Can you remove the other commits for now? We\ncan deal with them in a subsequent PR.\nI hope we can merge this \u2013 would be a great addition.\nI'm very curious about what you're using this for. Are you using it at\nHouzz?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/pull/42#issuecomment-76535589.\n. i have been looking into using LMDB as the backend storage for the index,\ninstead of using a simply memory-map array. There are some pros/cons for\nusing LMDB. Downside is that there would be some memory overheads. However\ngood side is that LMDB is a well-tested well-maintained memory-map\nkey-value store, with transaction support. I am afraid if we spend a lot of\ntime working on the Annoy's own memory map file operations, it is actually\njust re-inventing the wheels of LMDB.\n\nThoughts?\nOn Sat, Aug 15, 2015 at 3:47 PM, Erik Bernhardsson <notifications@github.com\n\nwrote:\nI want to pick this up. I might put together a separate PR and borrow a\nbit from this code. Incremental adds would be a great new feature.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/pull/42#issuecomment-131461536.\n. one problem with Annoy we encountered in our system is that when we tried\nto build a super large index, the system crashed because building indexing\nuses the physical memory, not memory-map file.  Using LMDB could solve the\nproblem. Of course, we might work with memory array however it would be\nmuch harder to work and the codes would be harder to maintain.\n\nOn Sat, Aug 15, 2015 at 3:55 PM, Lbchen Chen lbchen@gmail.com wrote:\n\ni have been looking into using LMDB as the backend storage for the index,\ninstead of using a simply memory-map array. There are some pros/cons for\nusing LMDB. Downside is that there would be some memory overheads. However\ngood side is that LMDB is a well-tested well-maintained memory-map\nkey-value store, with transaction support. I am afraid if we spend a lot of\ntime working on the Annoy's own memory map file operations, it is actually\njust re-inventing the wheels of LMDB.\nThoughts?\nOn Sat, Aug 15, 2015 at 3:47 PM, Erik Bernhardsson \nnotifications@github.com wrote:\n\nI want to pick this up. I might put together a separate PR and borrow a\nbit from this code. Incremental adds would be a great new feature.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/pull/42#issuecomment-131461536.\n. best way to celebrate the labor day is to work. I will work on this for this weekend and hopefully come out something. \n. I added something that are not working yet. I can make a pull request and\nso you can see what I have done.\n\n\nand feel free to take from there.\nOn Wed, Sep 9, 2015 at 12:38 PM, Erik Bernhardsson <notifications@github.com\n\nwrote:\nDid you end up doing anything with this? I've been thinking more and more\nabout this and might spend some time on it\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/issues/96#issuecomment-139024092.\n. pull request made.\n\nOn Wed, Sep 9, 2015 at 1:41 PM, Lbchen Chen lbchen@gmail.com wrote:\n\nI added something that are not working yet. I can make a pull request and\nso you can see what I have done.\nand feel free to take from there.\nOn Wed, Sep 9, 2015 at 12:38 PM, Erik Bernhardsson \nnotifications@github.com wrote:\n\nDid you end up doing anything with this? I've been thinking more and more\nabout this and might spend some time on it\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/issues/96#issuecomment-139024092.\n. do you still have the access to spotify/annoy?\n\n\nOn Wed, Sep 9, 2015 at 2:06 PM, Lbchen Chen lbchen@gmail.com wrote:\n\npull request made.\nOn Wed, Sep 9, 2015 at 1:41 PM, Lbchen Chen lbchen@gmail.com wrote:\n\nI added something that are not working yet. I can make a pull request and\nso you can see what I have done.\nand feel free to take from there.\nOn Wed, Sep 9, 2015 at 12:38 PM, Erik Bernhardsson \nnotifications@github.com wrote:\n\nDid you end up doing anything with this? I've been thinking more and\nmore about this and might spend some time on it\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/issues/96#issuecomment-139024092.\n. and houzz would be happy to host the repo if you like.\n\n\n\nOn Wed, Sep 9, 2015 at 5:19 PM, Erik Bernhardsson notifications@github.com\nwrote:\n\nYeah I do. I think with LMDB it might be worth considering moving this\nsomewhere else though \u2013 it's a big non-compatible change anyway\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/issues/96#issuecomment-139078583.\n. yes. i debated a lot.\n\nprotobuf would have very nice interface for whoever want to add new\nindexing field into the system later; also it saves the work for\nseriazation and deserialzation.\nthe downside is of course the overhead. however i think protobuf is highly\noptimized so it is probably fine.\nagree we can start another repo.\nOn Wed, Sep 9, 2015 at 5:25 PM, Erik Bernhardsson notifications@github.com\nwrote:\n\nThis looks great!\nDid you consider using a plain old struct instead of protobuf? It's\nprobably faster.\nIt might be worth just starting over with a new repository and calling it\nannoy2 or annul or something :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/pull/101#issuecomment-139079352.\n. maybe we can throw in a flask server to make it a full blown search engine\nbased on distance.\n\nOn Wed, Sep 9, 2015 at 5:29 PM, Lbchen Chen lbchen@gmail.com wrote:\n\nyes. i debated a lot.\nprotobuf would have very nice interface for whoever want to add new\nindexing field into the system later; also it saves the work for\nseriazation and deserialzation.\nthe downside is of course the overhead. however i think protobuf is highly\noptimized so it is probably fine.\nagree we can start another repo.\nOn Wed, Sep 9, 2015 at 5:25 PM, Erik Bernhardsson \nnotifications@github.com wrote:\n\nThis looks great!\nDid you consider using a plain old struct instead of protobuf? It's\nprobably faster.\nIt might be worth just starting over with a new repository and calling it\nannoy2 or annul or something :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/pull/101#issuecomment-139079352.\n. great. will do it soon.\n\n\nOn Wed, Sep 9, 2015 at 6:09 PM, Erik Bernhardsson notifications@github.com\nwrote:\n\nIf you want to move this to houzz/xyz, I'm happy to help. Was planning to\ndo something under erikbern/ but I've been postponing it so long that it's\nbetter if someone else takes charge and I'm happy to contribute to it\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/pull/101#issuecomment-139084289.\n. up now.\n\nhttps://github.com/Houzz/annoy2\nOn Wed, Sep 9, 2015 at 6:09 PM, Erik Bernhardsson notifications@github.com\nwrote:\n\nIf you want to move this to houzz/xyz, I'm happy to help. Was planning to\ndo something under erikbern/ but I've been postponing it so long that it's\nbetter if someone else takes charge and I'm happy to contribute to it\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/pull/101#issuecomment-139084289.\n. it is not quite about being safer or more danger. The codes are open so\nthere is not really much difference.  The houzz/annoy2 would be the place\nfor developing lmdb backend. if lmdb backend works, we can either merge it\nback or to transfer to somewhere else.\n\nOn Wed, Sep 9, 2015 at 9:55 PM, Radim \u0158eh\u016f\u0159ek notifications@github.com\nwrote:\n\nJust wondering... what does the move entail?\nTo be honest erikbern/annoy sounds a little safer to me, but I may not\nunderstand all the implications here.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/pull/101#issuecomment-139116158.\n. \n",
    "ozankabak": "Hello Erik. Do you plan to integrate the proposed incremental update support?\n. ",
    "AKSHAYUBHAT": "(.pyenv)dhcp-ccc-1913:annoy-master aub3$ python setup.py install\nrunning install\nrunning bdist_egg\nrunning egg_info\nwriting annoy.egg-info/PKG-INFO\nwriting top-level names to annoy.egg-info/top_level.txt\nwriting dependency_links to annoy.egg-info/dependency_links.txt\nreading manifest file 'annoy.egg-info/SOURCES.txt'\nreading manifest template 'MANIFEST.in'\nwriting manifest file 'annoy.egg-info/SOURCES.txt'\ninstalling library code to build/bdist.macosx-10.6-intel/egg\nrunning install_lib\nrunning build_py\nrunning build_ext\ncreating build/bdist.macosx-10.6-intel/egg\ncreating build/bdist.macosx-10.6-intel/egg/annoy\ncopying build/lib.macosx-10.6-intel-2.7/annoy/init.py -> build/bdist.macosx-10.6-intel/egg/annoy\ncopying build/lib.macosx-10.6-intel-2.7/annoy/annoylib.so -> build/bdist.macosx-10.6-intel/egg/annoy\ncopying build/lib.macosx-10.6-intel-2.7/annoylib.so -> build/bdist.macosx-10.6-intel/egg\nbyte-compiling build/bdist.macosx-10.6-intel/egg/annoy/init.py to init.pyc\ncreating stub loader for annoylib.so\nbyte-compiling build/bdist.macosx-10.6-intel/egg/annoylib.py to annoylib.pyc\ncreating build/bdist.macosx-10.6-intel/egg/EGG-INFO\ncopying annoy.egg-info/PKG-INFO -> build/bdist.macosx-10.6-intel/egg/EGG-INFO\ncopying annoy.egg-info/SOURCES.txt -> build/bdist.macosx-10.6-intel/egg/EGG-INFO\ncopying annoy.egg-info/dependency_links.txt -> build/bdist.macosx-10.6-intel/egg/EGG-INFO\ncopying annoy.egg-info/top_level.txt -> build/bdist.macosx-10.6-intel/egg/EGG-INFO\nwriting build/bdist.macosx-10.6-intel/egg/EGG-INFO/native_libs.txt\nzip_safe flag not set; analyzing archive contents...\ncreating 'dist/annoy-1.0.5-py2.7-macosx-10.6-intel.egg' and adding 'build/bdist.macosx-10.6-intel/egg' to it\nremoving 'build/bdist.macosx-10.6-intel/egg' (and everything under it)\nProcessing annoy-1.0.5-py2.7-macosx-10.6-intel.egg\nRemoving /Users/aub3/Dropbox/.pyenv/lib/python2.7/site-packages/annoy-1.0.5-py2.7-macosx-10.6-intel.egg\nCopying annoy-1.0.5-py2.7-macosx-10.6-intel.egg to /Users/aub3/Dropbox/.pyenv/lib/python2.7/site-packages\nannoy 1.0.5 is already the active version in easy-install.pth\nInstalled /Users/aub3/Dropbox/.pyenv/lib/python2.7/site-packages/annoy-1.0.5-py2.7-macosx-10.6-intel.egg\nProcessing dependencies for annoy==1.0.5\nFinished processing dependencies for annoy==1.0.5\n(.pyenv)dhcp-ccc-1913:annoy-master aub3$ \nSure, I also inspected the egg in the site-packages \nArchive:  annoy-1.0.5-py2.7-macosx-10.6-intel.egg\n  inflating: annoylib.py\n  inflating: annoylib.pyc\n  inflating: annoylib.so\n  inflating: annoy/init.py\n  inflating: annoy/init.pyc\n  inflating: annoy/annoylib.so\n  inflating: EGG-INFO/dependency_links.txt\n  inflating: EGG-INFO/native_libs.txt\n  inflating: EGG-INFO/PKG-INFO\n  inflating: EGG-INFO/SOURCES.txt\n  inflating: EGG-INFO/top_level.txt\n  inflating: EGG-INFO/zip-safe     \n. Yes, I even deactivated the virtualenv. Even with or without the virtualenv or using pip it still generates the same error, also on ubuntu. \n(.pyenv)dhcp-ccc-1913:annoy-master aub3$ pip uninstall annoy\nUninstalling annoy:\n  /Users/aub3/Dropbox/.pyenv/lib/python2.7/site-packages/annoy-1.0.5-py2.7-macosx-10.6-intel.egg\nProceed (y/n)? y\n  Successfully uninstalled annoy\n(.pyenv)dhcp-ccc-1913:annoy-master aub3$ pip install annoy\nDownloading/unpacking annoy\n  Downloading annoy-1.0.5.tar.gz (622kB): 622kB downloaded\n  Running setup.py egg_info for package annoy\nInstalling collected packages: annoy\n  Running setup.py install for annoy\n    building 'annoy.annoylib' extension\n    /usr/bin/clang -fno-strict-aliasing -fno-common -dynamic -arch i386 -arch x86_64 -g -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c src/annoymodule.cc -o build/temp.macosx-10.6-intel-2.7/src/annoymodule.o\n    src/annoymodule.cc:19:20: warning: extra tokens at end of #include directive [-Wextra-tokens]\n    #include :\n                       ^\n                       //\n    1 warning generated.\n    src/annoymodule.cc:19:20: warning: extra tokens at end of #include directive [-Wextra-tokens]\n    #include :\n                       ^\n                       //\n    1 warning generated.\n    /usr/bin/clang++ -bundle -undefined dynamic_lookup -arch i386 -arch x86_64 -g build/temp.macosx-10.6-intel-2.7/src/annoymodule.o -lboost_python -o build/lib.macosx-10.6-intel-2.7/annoy/annoylib.so\n    ld: warning: ignoring file /usr/local/lib/libboost_python.dylib, file was built for x86_64 which is not the architecture being linked (i386): /usr/local/lib/libboost_python.dylib\nSuccessfully installed annoy\nCleaning up...\n(.pyenv)dhcp-ccc-1913:annoy-master aub3$ python\nPython 2.7.8 (v2.7.8:ee879c0ffa11, Jun 29 2014, 21:07:35) \n[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport annoy\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"annoy/init.py\", line 15, in \n    from .annoylib import *\nImportError: No module named annoylib\n. Also tried from another directory different error, also I get a fatal exception. \n\n\n\nImportError: No module named annoy\n\n\n\nexit()\n(.pyenv)dhcp-ccc-1913:~ aub3$ pip install annoy\nDownloading/unpacking annoy\n  Downloading annoy-1.0.5.tar.gz (622kB): 622kB downloaded\n  Running setup.py egg_info for package annoy\n\n\n\nInstalling collected packages: annoy\n  Running setup.py install for annoy\n    building 'annoy.annoylib' extension\n    /usr/bin/clang -fno-strict-aliasing -fno-common -dynamic -arch i386 -arch x86_64 -g -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c src/annoymodule.cc -o build/temp.macosx-10.6-intel-2.7/src/annoymodule.o\n    src/annoymodule.cc:19:20: warning: extra tokens at end of #include directive [-Wextra-tokens]\n    #include :\n                       ^\n                       //\n    1 warning generated.\n    src/annoymodule.cc:19:20: warning: extra tokens at end of #include directive [-Wextra-tokens]\n    #include :\n                       ^\n                       //\n    1 warning generated.\n    /usr/bin/clang++ -bundle -undefined dynamic_lookup -arch i386 -arch x86_64 -g build/temp.macosx-10.6-intel-2.7/src/annoymodule.o -lboost_python -o build/lib.macosx-10.6-intel-2.7/annoy/annoylib.so\nld: warning: ignoring file /usr/local/lib/libboost_python.dylib, file was built for x86_64 which is not the architecture being linked (i386): /usr/local/lib/libboost_python.dylib\nSuccessfully installed annoy\nCleaning up...\n(.pyenv)dhcp-ccc-1913:~ aub3$ \n(.pyenv)dhcp-ccc-1913:~ aub3$ python\nPython 2.7.8 (v2.7.8:ee879c0ffa11, Jun 29 2014, 21:07:35) \n[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport annoy\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/Users/aub3/Dropbox/.pyenv/lib/python2.7/site-packages/annoy/init.py\", line 15, in \n    from .annoylib import *\nImportError: dlopen(/Users/aub3/Dropbox/.pyenv/lib/python2.7/site-packages/annoy/annoylib.so, 2): Symbol not found: __ZN5boost6python7objects15function_objectERKNS1_11py_functionERKSt4pairIPKNS0_6detail7keywordES9_E\n  Referenced from: /Users/aub3/Dropbox/.pyenv/lib/python2.7/site-packages/annoy/annoylib.so\n  Expected in: flat namespace\n in /Users/aub3/Dropbox/.pyenv/lib/python2.7/site-packages/annoy/annoylib.so\nexit()\n(.pyenv)dhcp-ccc-1913:~ aub3$ deactivate\ndhcp-ccc-1913:~ aub3$ python\nPython 2.7.8 (default, Jul 13 2014, 17:11:32) \n[GCC 4.2.1 Compatible Apple LLVM 5.1 (clang-503.0.40)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nimport annoy\nFatal Python error: PyThreadState_Get: no current thread\nAbort trap: 6\ndhcp-ccc-1913:~ aub3$ \n. Thanks I will try. \n. \n\n\n",
    "bobonovski": "Today I tried to install this using pip, after that I had the same problem. How you guys solved this?. Collecting annoy\n/usr/local/lib/python2.7/dist-packages/pip/vendor/requests/packages/urllib3/util/ssl.py:318: SNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#snimissingwarning.\n  SNIMissingWarning\n/usr/local/lib/python2.7/dist-packages/pip/vendor/requests/packages/urllib3/util/ssl.py:122: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#insecureplatformwarning.\n  InsecurePlatformWarning\n  Downloading annoy-1.8.0.tar.gz (628kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 634kB 21kB/s\nInstalling collected packages: annoy\n  Running setup.py install for annoy ... done\nSuccessfully installed annoy-1.8.0. ubuntu 14.04. ",
    "B4yesC4t": "@bobonovski I had same problem after i install with pip. Had you solved it? . ",
    "nemo83": "HI, was this ever implemented?. ",
    "lesterlitch": "For anyone chasing this dream, check out: https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/XboxInnerProduct.pdf \nSeems to work out reasonably using Annoy.. ",
    "aimeida": "Thanks erikbern! I agree with your quick suggestions by now ^_^\nps: 500 trees is just an example, i should calculate the probability before choosing the parameters. \n. ",
    "malthe": "byte strings are enough.\n. Definitely. Right now I just do it like this:\npython\nwith open(\"index.map\", \"w\") as f:\n    for i, j, vector in items:\n        u.add_item(i, vector)\n        f.write(pack('<L', j))\nThis way I can mmap, or simply seek directly to the vector id \u2013 in this example j \u2013 from a vector id.\nBut perhaps Annoy could grow an API to load data from a stream. This way I can wrap the tree data in my own format and only have a single file on disk.\n. ",
    "asharma567": "1 error generated.\nerror: command 'gcc' failed with exit status 1\n. I remember liking boost a lot when I was a quant on a trading desk! I'll\ninstall it.\nI'm trying to use Annoy instead of a BallTree or KDtree which takes too\nlong to construct and query: 2.5 million documents TFIDF vectorized.\nThe goal would be to query 100,000 against 2.5 in a reasonable amount of\ntime.\nWould you have any insight?\n- Ajay\n  \u1427\nOn Tue, Apr 28, 2015 at 3:59 PM, Erik Bernhardsson <notifications@github.com\n\nwrote:\nDid you install Boost?\nI'm planning to drop the dependency on Boost fairly soon. It's been\ncausing a lot of issues for people.\nhttp://www.pyimagesearch.com/2015/04/27/installing-boost-and-boost-python-on-osx-with-homebrew/\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/issues/49#issuecomment-97253536.\n\n\nGithub https://github.com/asharma567/ | LinkedIn\nhttp://www.linkedin.com/in/mrasharma/\n. Interesting, I'll try that approach -- Thanks!\n\u1427\nOn Wed, Apr 29, 2015 at 11:14 AM, Erik Bernhardsson \nnotifications@github.com wrote:\n\nIf you come back in a few days the Boost dependency should be dropped.\nYes you can do that. Note however for TF-IDF if the number of terms is\nhigh the nearest neighbor quality might not be perfect. You should probably\ndo a dimensionality reduction (using matrix factorization etc) to a\nrepresentation with 10-100 dimensions at most.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/issues/49#issuecomment-97528513.\n\n\nGithub https://github.com/asharma567/ | LinkedIn\nhttp://www.linkedin.com/in/mrasharma/\n. works pypi works : OSX\nOn Wed, Apr 29, 2015 at 6:03 PM, Erik Bernhardsson <notifications@github.com\n\nwrote:\nClosed #49 https://github.com/spotify/annoy/issues/49.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/issues/49#event-293587378.\n\n\nGithub https://github.com/asharma567/ | LinkedIn\nhttp://www.linkedin.com/in/mrasharma/\n. ",
    "dvenum": "It's work for me. Debian 8.\n. It seems to my bad. nns_dist contain:\n8 0\n13 1\nBut for angular test: \n2 0\n2 1\n2 2\n. As @erikbern pointed me, it will similar to:\nc\n// annoy python object\ntemplate<typename S, typename T, typename Distance>\ntypedef struct {\n    PyObject_HEAD\n    int f;\n    char ch_metric;\n    AnnoyIndexPython<S, T, Distance >* ptr;\n} py_annoy;\nI was a little confusing with templates exposing. It was first working code with rewriting purpose.\n. Critical bugfix in c02bb89. Input vector was always zero.\ntest_get_nns_by_vector passed.\n. push_back was used me for vector object when it converting from input python list. New items was added to end, but it using by index.\nc\n    vector<T> w(this->_f); <<-- start length no zero\n    for (int z = 0; z < PyList_Size(v) && z < this->_f; z++) {   <<-- check border also\n        PyObject *pf = PyList_GetItem(v,z);\n        w[z] = PyFloat_AsDouble(pf);             <<-- correct way\n    }\n. I have same problem with templates.\nAs below, object py_annoy should be allocated when I don't know, what is object required. No metrics received yet. ptr should be a pointer to an universal base class.\n``` c\n// annoy python object\ntemplate\nstruct py_annoy_ { \n  PyObject_HEAD\n  int f;\n  char ch_metric;\n  void ptr;\n  AnnoyIndexPython ptr;\n};\nstatic PyObject \npy_an_new(PyTypeObject type, PyObject args, PyObject kwds) {\n  py_annoy *self;\nself = (py_annoy *)type->tp_alloc(type, 0);\n  if (self != NULL) {\n    self->f = 0;\n    self->ch_metric = 0;\n    self->ptr = NULL;\n  }\nreturn (PyObject *)self;\n}\n```\nBoost determine class template, which may wrap C++ class and expose him. It's a bit complex way for this task.\nI think about some tricks in C-style. It'll ok when we will have few wrapper functions if no code will duplicated.\n.  lib exists only for c++11. I can include another library as part of sources if nothing to more will be.\nMay you try something, similar to it in .travis.yml?\n``` bash\ninstall:\n  - sudo apt-get install -qq gcc-4.8 g++-4.8 \nWe want to compile with g++ 4.8 when rather than the default g++\n\nsudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-4.8 90\n```\n. As you can see on the head, only C++11 marker exists: \nhttp://www.cplusplus.com/reference/random/\n\nOld C++ have only rand() function from stdlib.\n. If using rand() call is correct, I can rewrite it, no problem. Or I can include a bit more complex library.\nTravis-CI uses gcc-4.6, that very old.\n. ;). Nice. \nrand() no good generator. I glad that it's enough for task.\nLeave \"depends=['src/annoylib.h'],\" please. It's good, when you can rebuild after change in header.\n. It's done, sorry.\n. ",
    "MrChrisJohnson": "Given that Sparkey already supports this with mmapping how would you feel about solving it with an annoy dependence on Sparkey?\n. So maybe we can hold off on implementing for python annoy at the moment, but should be fine for the java version. In the meantime I can also try and get Sparkey submitted to PyPI. If we get Sparkey in PyPI then does this seem like a clean enough solution?\n. Just realized that there is a C library for Sparkey so should be possible to do this outside of Python altogether. Would be nice to manage the dependencies through Travis but not sure how that would work with the travis.yml being language: python. Any ideas?\n. Why 2_(1-cos)? Distance should just be 1-cos which is equivalent to 1 - u_v / ||u|| * ||v||. So, distance would range from 0 (angle of 0) to 2 (angle of 180). See the link that @erikbern links to earlier.\n. Not sure the probability of choosing a poor hyperplane by sampling only 2 points, but if you sample 10 then it's pretty easy to find a hyperplane that splits the 10 evenly. You sample a random hyperplane, project all 10 points to the randomly chosen hyperplane, then choose a hyperplane perpendicular to this one that passes through the original splitting the 10 projected points evenly. Make sense? Easier to draw on paper then to describe here.\nI'd be careful using a fully randomized Las Vegas algorithm (reject if < 80/20 split) since it costs O(n) to verify (where n is the number of points left to split in the subtree). If you end up in a loop of rejects then this can be quite costly in the early splits, but sampling more points on the initial hyperplane choice is much less costly, and if they are chosen uniformly then more or less likely to represent the full distribution. \n. Hmm, maybe I am misunderstanding, but shouldn't it always end up with an even split since it's a hyperplane and the similarity metric won't define what side of the hyperplane a point lies? I think what you are worried about is that it could unfairly split similar points because they could potentially seem like they are far from each when you project onto the randomly chosen plane. ie: if the angle between 2 points was small but the magnitude was large. In that case, as long as you normalize ahead of time it should be a more fair split since then all points will lie on the unit sphere. Might need to think about this some more. \nGood point about the simplicity and avoiding any determinism in the tree splits. Not sure on the theoretical implications of smarter splits, but seems like there will still be plenty of randomness from the initial hyperplane choice as well as the samples.\n. I just meant for the proposed sampling method above it will always split the 10 sampled points evenly regardless of similarity metric (obviously not the full set of points if you are only sampling 10). \nYep, completely agree. Not fully sure on the theoretical implications of the sampling method but seems like a nice balance of being smarter about the hyperplane split (splitting k points instead of just 2) but also maintaining nondeterminism so as to avoid trees replicating splits.\n. Seems doable, you could potentially even calculate several splits at once (take the top 3 principle components). One thing to keep in mind with PCA though is that it's susceptible to outliers so you aren't really guaranteed an even split, especially if your sample isn't very gaussian-like. In that case, to maximize the variance it may find a split that overemphasizes 1 or 2 outliers in your sampled set.\n. Still, I'm unsure why you'd prefer PCA to the simple O(k) method above. Less costly, simple to implement, and will always guarantee an even split.\n. @erikbern The O(k) method is to sample k points as well as a random hyperplane h. Then, project each of the k points to the random hyperplane (in 2 dimensions this is just a bunch of points on a line). It should now be trivial to find a second hyperplane h' perpendicular to h that splits the projected points and will in turn split the points in the original space. \nI tried to draw a picture of this. Not sure if it came through.\n\n. ",
    "stefansavev": "Hi Erik,\nFirst congratulations on the excellent results with the new method. Just wanted to point out a paper by Sanjoy Dasgupta where he considers a related rule. May be the paper will useful.\nhttp://cseweb.ucsd.edu/~dasgupta/papers/rptree-stoc.pdf (Random projection trees and low dimensional manifolds)\nYou can look for the text: \"choose a random unit direction v \u2208 R; pick any x \u2208 S; let y \u2208 S be the farthest point from it\"\nNo idea how it relates performance-wise to your rule.\nCheers,\nStefan\n. @a1k0n ,@erikbern\nThanks guys for your comments. I just brought up this paper because it is using a sample of two points to estimate a global property of the dataset (in their case the diameter of a dataset). You are also using two points towards a similar goal but differently.  I saw as Andy pointed out that Dasgupta use x and y to figure out the split point only but not the direction, while you use x and y for the direction. I would think that your rule makes more sense.\nI am not sure if Dasgupa's rule will bring something, but just to clarify his argument.\nHis setup is that there are two clusters (this is his figure 2), Bi and Bj which are with small diameter but somewhat further apart [a bit stronger assumption]. He wants to separate them cleanly. He is saying that by adding a random offset to the median we can improve the chance of a good split. Bi and Bj themselves live in a top level clusters and he uses x and y to find the diameter of the top level cluster.\nUnder your rule if you have two clusters, you will have a good split if you pick x and y to be from different clusters. I think this rule works better when you have a larger diameter (more dimensions in the dataset, more data points, more diverse data points). So it's interesting to see if the effectiveness of the rule diminishes in lower nodes of the tree.\nI know that when you do euclidean distance you try a couple of directions and choose the best (the one with the widest projection). In conjunction with the new rule, this will mean that you also want x and y to be further apart.  Together both rules will imply that you will select the split points to be from different \"clusters\". \nOne thing that I saw that people do in a similar context (PCA-trees) is when they choose a direction d (in your case x - y) they subtract it from the data. This gives them the property that\nsubsequent splits are orthogonal. I'm keen to try this myself. I'm planning to subtract the previous direction from the new direction (not from the data) which is obtained by sampling two new points. I think here is also safe to require that ||x - y|| is not too small because the orthogonality will give rise to a different split than at the parent level. When ||x - y|| is small the rule approaches the random hyperplane rule, so it might not be too bad (as long as x - y does not have too many zeros).\nI'm pretty excited about your methods because it has quite nice properties: 1) cheap; 2)it has randomness; 3) it is informed by the data.\nCheers,\nStefan\n. Sounds like a very promising idea (the idea of a loss function). May be this discussion deserves a separate page. I'd be happy to contribute comments on it. One cheap way to try it is to take a dataset, compute top 10 NN, and then run it through boosted decision trees for learning to rank (there is an implementation here which is of high quality: https://github.com/yasserg/jforests). Learning to rank considers discordant pairs in its objective.\nI'll write separately for the other ideas.\n. ",
    "ravimody": "Cool thanks, I'll look into it (although my C++ is a little rusty :)) \n. No work on it from my end.. ",
    "thomas4g": "Has there been any further work on this?. Bummer. I'd love to help out, but I don't think I'm familiar enough with annoy yet (or frankly competent enough with C++) to spearhead anything. If anyone starts working on this and wants a hand, I'm happy to help out. . Actually, I take it back. It looks like a pretty simple change. If I understand it properly, it's line 496 of annoylib.h:\n_roots.push_back(_make_tree(indices));\n\nThat can get parallelized, but with a mutex around _roots to avoid simultaneous push_back calls, right? . @tjrileywisc what a coincidence! I also started working on this over the weekend. Mine also doesn't quite work...  but I've pushed a copy to my fork: https://github.com/thomas4g/annoy/tree/parallelize_build\nI hope to keep working on it tonight, but if you're further along let me know if you'd like any help! Feel free to ping me here or shoot me an email me@thomasshields.net . @tjrileywisc ahh, I wanted to use that but got thrown off by the build not supporting my #include <mutex> by default. I didn't want to fiddle with build settings. . This definitely looks a bit nicer than my WIP! One thing I did do differently that you might consider is instead of assigning each thread a particular number of threads, I just let each thread loop with the same terminating condition - That way if one thread is a bit faster it can go ahead and start on another tree instead of just ending.. @tjrileywisc Yeah - if you look at my branch, I used a separate counter to track _roots.size() and used the gcc atomic builtins to atomically fetch/increment. . This is why I used pthreads in my branch, but if that doesn't work on Windows... is that a lose-lose then? What's the de-facto cross-platform threading? . ",
    "tjrileywisc": "@erikbern Actually I was working on a branch to add this just this weekend. It doesn't quite work yet though (I can only get it to run the precision_test.cpp example, only a debug binary works and it crashes sometimes even then); should I submit a PR?\nI didn't touch anything towards the bottom of annoylib.h (near those lines that you mentioned), that might be the problem.... @thomas4g \nJust took a peak at your fork - I'm actually using std::thread instead of pthread . I think Windows doesn't have support for pthread built in. std::thread should be available for gcc and MSVC.. Just submitted a PR for my build_trees_threaded branch. \nhttps://github.com/spotify/annoy/pull/246. Doesn't work for me unfortunately (on Windows 7).  I just cloned the repo and tried to build with gcc included in TDM-GCC-64 (gcc is version 5.1). I get this when trying to compile (below are arguments sent to gcc):\nundefined reference to '__imp_Py_InitModule4'\n. @birolkuyumcu Awesome, I was able to get gcc to build a working annoy and the .egg worked as well.\nTo get this to build on Windows, I added [build] compiler=mingw32 to the config.cfg file in the repo and '-D=MS_WIN64' in extra_compile_args.\n. Though now (for me at least) python has a tendency to crash at the end of a session where annoy has been used. \n. Maybe have a look at this Largevis repo, they take annoy a step further and\ndo dimensionally reduction.\nThey made a few changes to the annoy code to allow compiling on Windows.\nI'm not sure if it makes an importable module for annoy in Python though.\nhttps://github.com/lferry007/LargeVis\nOn May 5, 2017 4:27 AM, \"bnbwn\" notifications@github.com wrote:\n\nanother windows user trying to get this to work... I think I am so close.\nI have read all the previous issues related to getting annoy compiled\nand properly loaded on windows.\npython 2.7 x64 using miniconda on Win10\ndownloaded the gcc / c++ complier using mingw-w64-install.exe\nextract to and save in C:/mingw-w64\nadd path to bin in PATH\ndownloaded annoy from github\nextract and save into C:/annoy\nC:\\annoy>python setup.py build --compiler mingw32\nC:\\annoy>python setup.py install\nafter many hours... victory. I brute forced through every permutation of\navailable compilers and build flags...\nat least for a moment....\n[image: so close]\nhttps://cloud.githubusercontent.com/assets/3788107/25723070/2d0d4314-30dc-11e7-86c4-c746dce3ebd8.jpeg\nat this point I am unsure as it seems to be built correctly, and loaded\nproperly, yet still isn't found. the .pyd is the newly created .dll.\npython, then help(), then modules does show annoy as an option.\nif @cadobe https://github.com/cadobe, @sergeyf\nhttps://github.com/sergeyf, @thirdwing https://github.com/thirdwing,\n@birolkuyumcu https://github.com/birolkuyumcu @tjrileywisc\nhttps://github.com/tjrileywisc have any ideas, I'm all ears... and\nthanks again @erikbern https://github.com/erikbern\n[image: image]\nhttps://cloud.githubusercontent.com/assets/3788107/25723559/20f4384c-30de-11e7-875c-8fa2c434a830.png\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/spotify/annoy/issues/130#issuecomment-299299524, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AFkT8FnTOku9WOcGU2sZ2Ae4CiHS5g6Vks5r2jTKgaJpZM4HIeIS\n.\n. @bnbwn \n\nIf you're still interested in this, you might want to try my fork that is compiling correctly on Windows (at least in python 3.5/3.6).\nhttps://github.com/tjrileywisc/annoy. @erikbern I will submit a PR, just having an issue getting the tests to work after I rearranged the includes to get Linux builds working again (I only have access to Windows Subsystem for Linux so I'm not 100% sure my fix will be correct). . @erikbern PR here for Windows support (only Python 3 for now). I think Linux is working based on my tests in Windows Subsystem for Linux.\nhttps://github.com/spotify/annoy/pull/233\n. Added Python 2.7 support in this PR https://github.com/spotify/annoy/pull/235\n. @MonkeyChap \nI have a fork that compiles on Windows (comfirmed to compile for Win7&10/ python3.5 & 3.6 with VS 2015 compiler) at https://github.com/tjrileywisc/annoy if you want to try that. I confirmed that all examples run. \nI'm trying to confirm that it still compiles in gcc before sending a PR (I only have access to bash subsystem for Windows so I'm not 100% sure it is the same though). Most changes were in the annoylib.h header but I had to fiddle with compiler arguments in setup.py as well.\n. @MBetters\nIt should compile if you just run setup.py install.. @MBetters \nThe stdint.h header file is missing (see the second to last line in your output). This file isn't included with VS 2008 or the VS C++ compiler for Python 2.7 installer.\nYou can get it from here, and since you're using the VS C++ compiler for Python 2.7 compiler, you'll want to put it into \n'C:\\Users\\\\AppData\\Local\\Programs\\Common\\Microsoft\\Visual C++ for Python\\9.0\\VC\\include'.\nYou should be able to compile it after that.\n. For anyone still monitoring this thread, Windows support from my fork was just merged (https://github.com/spotify/annoy/pull/233)\nOnly python 3 is working (3.6 is working, anything with a MSVC 2015 compiler should be okay). 2.7 not working yet.. I've been trying to figure out a solution- my C++ knowledge is pretty weak so it will probably take me a while.\nI'm actually trying to figure out why the crash is happening in the first place instead of using the loop. I think it might be because it's indexing out of bounds. \nMaybe some reallocation needs to be done? m->children has length of 2 but at the time of the crash there are more than 2.. @erikbern I'm afraid that didn't work. I'm seeing python crashes in 3.6 and 2.7 now, trying to figure out why.\nI'm able to compile the precision_test.cpp example though in MSVC and it runs fine, so I think the problem is in the python module specifically.. Still didn't work I'm afraid. Looks like it's crashing python in the same place. It's crashing when running the nosetests. This is the last output just before the crash:\nfinding nbs for 161\nfinding nbs for 22055\nfinding nbs for 49154\nfinding nbs for 54985\nfinding nbs for 36080\nfinding nbs for 25961\nfinding nbs for 47264\nlimit: 10        precision:  13.56% avg time: 0.000068s\nlimit: 100       precision:  21.06% avg time: 0.000095s\nlimit: 1000      precision:  56.51% avg time: 0.000311s\nlimit: 10000     precision:  97.12% avg time: 0.002118s\n.......... Okay, might have a signal here. Finally got to a point where I could use MSVC debug symbols to find out where exactly it's crashing:\nAnnoyIndex<int,float,Angular,Kiss64Random>::unload() Line 440\n... which I think sounds about right, from the output above. I think all of the tests run and then it crashes when finishing up.\nIt's crashing at this function:\nmunmap(_nodes, size);\nsize appears to be zero, because _n_nodes is zero.\nAs an aside (from your comment in an earlier PR)- I think this is what we need to get appveyor to compile and test this on windows:\nhttps://github.com/ogrisel/python-appveyor-demo/blob/master/appveyor.yml. I think Python 3 is working again but 2.7 isn't according to the last time I ran nosetests, didn't yet get a chance to run individual tests for that version. Did they complete for you according to appveyor?. I think you'll have to create an appveyor account and add this repo. It's all set on mine- check here if you want to see what it looks like (I added custom messages for pending, success and failing to make it obvious that it's the windows version):\nhttps://github.com/tjrileywisc/annoy\n(instructions here)\nHaven't yet figured out how to do separate python 2.7 and python 3.x badges. As far as I can tell you need to have separate branches.\nIf you're happy with it I'll edit the badge to watch this repo instead of mine.. I'm trying to get visual studio 2008 up and running to build the module there so I can get a symbol file and actually debug it when it crashes. Will update when I have more.. thanks for the readme shoutout!. Not sure what this EEE thing is about in the appveyor output. I saw this when I was building locally too:\nhttps://ci.appveyor.com/project/erikbern/annoy/build/1.0.41#L267\n. Seems like it's failing on tests and compiles for Lua and Go.  It tox.ini needs a c++11 argument somewhere.\n@thomas4g I think I understand your suggestion- so would you have the loop check some increment for number of trees built so far then? This would probably be safer than my hacky fix which won't work for non-even divisions of trees and threads!. Okay, I think I've got all of the changes above integrated but there's some cleanup to do.\nGood news- seems to compile everywhere and is not crashing. I did see a speedup when building the precision_test.cpp example and generating 10,000 vectors and building the index.. Tests seem to be failing without clear error messsages.\nI know why the Lua/Go tests would fail (I don't know how to tell them to compile with the -std=c++11 option) but not sure why the python tests are failing.. I also added a compiler option for MSVC for floating point in setup.py ('-ffast-math' equivalent I think). It's incidental to the CI errors though (probably).. Yeah, I took a break on this one. It's flaky at the moment, usually crashing Python on Windows, or running and failing tests (not consistently). The precision_test.cpp example works fine in linux, but then I can't get the python nosetests to run there. It's almost as if nose isn't finding the tests at all (I tried nosetests -v without any difference in output).. It looks as if python is crashing when running the accuracy tests (it failed right away on the fashion mnist dataset), right at this line:\njs_fast = annoy.get_nns_by_vector(v, 10, 1000)\nI added something similar to the precision_test.cpp example (albeit not with this dataset):\n```\ndouble vec2 = (double )malloc(f * sizeof(double));\nfor (int z = 0; z().swap(closest);\n``\n... which didn't crash, so I guess that means the python module code for this function has an issue with my threading changes.. I'm having trouble figuring out why accuracy tests are failing... I'm going to close this PR for now since I think I need to learn a lot more C++ before I can get this.. Looks like we need apip install h5pyin the appveyor script.. @hanabi1224 Does '10g' mean 10 gigabytes or something else?. @hanabi1224 I saw a Python crash trying to build an index with 100 trees of 10 million 100-d vectors, so I didn't even quite get to the loading part. Did you create your index elsewhere?. Though I think I am probably just coming up against the memory limits of my laptop in that case (I think I used up all 16 GB).. @wergeld \nIt appears that the VS 2012 compiler is selected when you runsetup.py install` (you almost certainly should be using the VS 2015 compiler for python 3.6)\nNot sure where setuptools selects the compiler, but I'm fairly sure your problem is there.. This should work for python 3.x, on any OS (I've used it on Win 7 and Win 10 without issues). \nI was never able to get this to build on Python 2.7 unfortunately when I worked on adding Windows support.\nIt appears that one would have to figure out how to implement __popcount64 for VS 2008 (MSDN has the earliest implementation in VS 2010):\nhttps://msdn.microsoft.com/en-us/library/bb385231.aspx\n. I have if (!PyArg_ParseTupleAndKeywords(args, kwargs, \"i|i\", (char**)kwlist, &q, &n_threads)) now is there another change needed for that line?\nThere should be a default for n_threads in annoylib.h, does that work?\nvoid build(int q, int n_threads = 1). Should be fixed now. Fixed now (I think). I got a lot of 'thread' is not a member of std and similar warnings for other <thread> functions. Would compiling for c++ 11 cause issues for *nix users?. It shouldn't be using boost python (I think an earlier version of annoy did though).. Is OpenMP an option?. I was trying to put them in the class with everything else, but I kept getting 'deleted function' errors when compiling.. The exact warning (when compiling the precision_test.cpp example in Linux with the s_compile_cpp.sh script):\nerror: use of deleted function \u2018std::mutex::mutex(const std::mutex&)\n(similar warnings with MSVC).. ",
    "zhdeath": "Thanks for your reply.\nMy own computer which is also ubuntu 14.04 works fun.\nAny idea what I should do with Amazon EC2?\n\n\n\ni.get_item_vector(..) always returns [0,0..]\n. it indeed is!\ni.get_item_vector(18)\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n\n\neven i.get_nns_by_item(18, 10) works\nBut I need those vectors in my app\n. @a1k0n Worked on these trivial code with 1.2.2 or 1.3.1\n``` python\nfrom annoy import AnnoyIndex\nimport random\nf = 30\na = AnnoyIndex(f) # length of item vector\nn = 1000  # number of vector\nfor i in xrange(n):\n    v = []\n    for z in xrange(f):\n        v.append(random.gauss(0, 1))\n    a.add_item(i, v)\na.build(-1)\na.save('test.tree')\nb = AnnoyIndex(f)\nb.load('test.tree')\nprint(i.get_item_vector(10))\nprint(b.get_nns_by_item(10, 10))\n```\nload() returns false, get_item_vector returns [0,0...]; but get_nss_by_item(10,10) works fun. Python is 2.7.6\nI think it's version related.\nWith 1.0.5, load() returns true and get_item_vector() also works\n. ",
    "atqamar": "Great! get_item_vector() method works fine. And I was mistaken about the get_distance() method not working.\nIs there a way to prevent 0 being returned by add_item()? It feels unnecessarily verbose:\n```\n\n\n\nfrom annoy import AnnoyIndex\nimport random\nf=2\nt = AnnoyIndex(f)\nt.add_item(0, [0, 2.5])\n0\nt.add_item(1, [3.5, 0])\n0\nt.build(10)\nTrue\nt.get_item_vector(0)\n[0.0, 2.5]\nt.get_item_vector(1)\n[3.5, 0.0]\n```\n. Ahh yes, it's not a void typed function. Thanks.\n\n\n\nIt's working now.\n. ",
    "nresare": "@erikbern  you should be invited now.\n. ",
    "jonbakerfish": "@piskvorky  @erikbern  Thanks for your advice. I'm going to try partition the index first, and I also add two functions for get_nns_by_item and get_nns_by_vector to support returning both ids and distances, which are get_nnsd_by_item and get_nnsd_by_vector respectively.\n. Didn't try it. But I think it is better to save that flag. My problem is a rare case, where the CentOS's loader is too old to support the new Haswell CPU.\n. ",
    "saustar": "@erikbern Hi, I am using annoy tree to get most accurate match of product names. My question is regarding size of tree. When I saved the tree on disk its size ig 2.3GB. But while creating the tree I can see it is using 3 times more memory. Is this okay? . Umm.. True that but I am running it on pyspark and I can see memory error\nOn 27 Dec 2016 10:22 p.m., \"Erik Bernhardsson\" notifications@github.com\nwrote:\n\nNot clear why, how are you measuring it? Measuring memory consumption of a\nprocess is notoriously unreliable\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/spotify/annoy/issues/85#issuecomment-269351496, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AQniXmFoG24ke7XQeUi9r26CSIdecrCwks5rMUJEgaJpZM4FSWQI\n.\n. \n",
    "zachmayer": "Nothing yet, but link to your slides here when you're done =D\n. ",
    "JohnNay": "Using it for a baseline model to compare to deep learning and gradient boosting machines, we're building a system to that trains on hundreds of millions of examples of remotely sensed values of vegetation health and lagged values of other remotely sensed (NASA satellite data) variables at high spatial resolution for Sri Lanka for the past ten years to forecast future values of vegetation health in drought prone farming areas. I'm using annoy to look up pixel-time observations closest in space and time in the trianing data and use their values of vegetation health as the predictions for the holdout data to compare to the more complex models to see if they will be worth deploying.\n. Yeah k-NN regression on a vegetation health index measured from satellite images (http://modis.gsfc.nasa.gov/data/). Thanks for this software by the way!\n. I'll share the results with you once I have them if you would like\n. ",
    "duhaime": "This is an old thread, but I wanted to drop a quick line to say thanks for putting together this awesome library! I'm using it to study patterns in text reuse within literary history (imitations, plagiarisms, etc). I have a simple repo that couples Annoy with D3 for some interactive visualizations in case you're curious. Thanks again for sharing this great work!\n. ",
    "shawnjhenry": "I would like to say thanks for building this, too!  I'm indexing an archive of chat messages with distributed document representations and Annoy for semantic similarity search.\n. I have 250GB of RAM, so that's not the issue.   I'll try upping the number of queries.\n. ",
    "Horace89": "No problem. I just wanted to inform you.\n. I'm on Ubuntu 14.04.\n. Really really weird... \nI just tried at home on my local install of Ubuntu 14.04 and I'm not able to reproduce the bug.\nI found the previous issue when using the default (Ubuntu 14.04) VM on koding.com. I guess it comes from their config, but I don't know what is special to it. \n. First I must learn how to do pull requests :wink:\n. ok. On my koding.com VM, I edited setup.py. I just changed\nwith open('README.rst') as fobj:\nto\nwith open('README.rst', encoding='utf-8') as fobj:\nand now the the install finishes flawlessly. Thanks to @a1k0n for his comment!\n. ok, I'll test it again tomorrow. Thank you.\n. Yes, I confirm that I use the latest version of Annoy (1.5.2), and the described bug still occurs. I'm on Python 3.4.3. \n. It's working great now! Thanks!\n. ",
    "npes87184": "Thanks!\nI think I am sleeping that time.  LOL\n. ",
    "Jochen0x90h": "Yes, having the extra random number generators in an extra header is a good idea. I personally don't trust random functions from standard libraries. But it could be possible to remove Kiss altogether again and keep only the default rand() based implementation. Then I can add Kiss myself. Templating for S may not be necessary if you pick a random number generator matching for S (e.g. Kiss32 for S = int32_t). But for convenience there could be a KissRandom that selects 32 or 64 bit according to S using partial specialization.\n. If the number of data points is much smaller than 2^32 then 32 bit RNG is ok too. By the way the standard guarantees only that RAND_MAX is at least 32767 (15 bit RNG). I now moved the RNGs into a separate file and reverted create_split, but implemented the picking of the two points directly without the original index2 function. I hope that's ok.\n. Now I hope it can be merged\n. For me it also marginally became better but no surprise as I don't believe in system RNGs ;)\n. Yes, maybe it's better to use 64 bit. Shouldn't hurt on today's 64 bit machines if it looks good in the unit test\n. Now it's ready I think\n. It makes it easier to implement a random number generator. The sampling of two points seems to be done only at this place so why not have the code here?\n. ",
    "rikenmehta03": "@LongbinChen Are above mentioned support implemented or someone working on that..? . ",
    "sergeyf": "I tried another approach - clone the repo, and build with:\npython setup.py build --compiler=mingw32\npython setup.py install\nThis compiled and installed without errors, but an import call causes:\n```\nimport annoy\nTraceback (most recent call last):\nFile \"\", line 1, in \n    import annoy\nFile \"build\\bdist.win-amd64\\egg\\annoy__init__.py\", line 15, in \nFile \"build\\bdist.win-amd64\\egg\\annoy\\annoylib.py\", line 7, in \nFile \"build\\bdist.win-amd64\\egg\\annoy\\annoylib.py\", line 6, in bootstrap\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\n```\n. It's throwing other errors about stdint.h not being found. It looks like \"Visual C++ for Python\" is just no good. I wonder why the MINGW version compiles but has that DLL load failure. \nOh well, Windows is just trouble!\n. So python setup.py build --compiler=mingw32 should work? I wonder what I'm doing wrong...\nThanks for your help!\n. Just as a warning to future searchers, it's still not working for me as of the latest commits. \nAfter running python setup.py build --compiler=mingw32, here is the error (skipping a slew of warnings):\nC:\\Users\\S\\Documents\\GitHub\\annoy [master]> python setup.py build --compiler=mingw32\nrunning build\nrunning build_py\nrunning build_ext\nbuilding 'annoy.annoylib' extension\nC:\\TDM-GCC-64\\bin\\gcc.exe -DMS_WIN64 -mdll -O -Wall -ID:\\Anaconda2\\include -ID:\\Anaconda2\\PC -c src/annoymodule.cc -o build\\temp.win-amd64-2.7\\Release\\src\\annoymodule.o -O3 -march=native -ffast-math\nwriting build\\temp.win-amd64-2.7\\Release\\src\\annoylib.def\nC:\\TDM-GCC-64\\bin\\g++.exe -DMS_WIN64 -shared -s build\\temp.win-amd64-2.7\\Release\\src\\annoymodule.o build\\temp.win-amd64-2.7\\Release\\src\\annoylib.def -LD:\\Anaconda2\\libs -LD:\\Anaconda2\\PCbuild\\amd64 -lpython27 -lmsvcr90 -o build\\lib.win-amd64-2.7\\annoy\\annoylib.pyd\nbuild\\temp.win-amd64-2.7\\Release\\src\\annoymodule.o:annoymodule.cc:(.text+0x76): undefined reference to `__imp__Py_TrueStruct'\nbuild\\temp.win-amd64-2.7\\Release\\src\\annoymodule.o:annoymodule.cc:(.text+0x86): undefined reference to `__imp__Py_NoneStruct'\nbuild\\temp.win-amd64-2.7\\Release\\src\\annoymodule.o:annoymodule.cc:(.text+0xb6): undefined reference to `__imp__Py_NoneStruct'\nbuild\\temp.win-amd64-2.7\\Release\\src\\annoymodule.o:annoymodule.cc:(.text+0xee): undefined reference to `__imp__Py_NoneStruct'\nbuild\\temp.win-amd64-2.7\\Release\\src\\annoymodule.o:annoymodule.cc:(.text+0x11a): undefined reference to `__imp__Py_TrueStruct'\nbuild\\temp.win-amd64-2.7\\Release\\src\\annoymodule.o:annoymodule.cc:(.text+0x163): undefined reference to `__imp__Py_NoneStruct'\nbuild\\temp.win-amd64-2.7\\Release\\src\\annoymodule.o:annoymodule.cc:(.text+0x1ce): undefined reference to `__imp__Py_NoneStruct'\nbuild\\temp.win-amd64-2.7\\Release\\src\\annoymodule.o:annoymodule.cc:(.text+0x1f1): undefined reference to `__imp__Py_TrueStruct'\nbuild\\temp.win-amd64-2.7\\Release\\src\\annoymodule.o:annoymodule.cc:(.text+0x235): undefined reference to `__imp__Py_NoneStruct'\nbuild\\temp.win-amd64-2.7\\Release\\src\\annoymodule.o:annoymodule.cc:(.text+0x4de): undefined reference to `__imp__Py_NoneStruct'\nbuild\\temp.win-amd64-2.7\\Release\\src\\annoymodule.o:annoymodule.cc:(.text+0x506): undefined reference to `__imp__Py_TrueStruct'\nbuild\\temp.win-amd64-2.7\\Release\\src\\annoymodule.o:annoymodule.cc:(.text+0x517): undefined reference to `__imp_PyExc_IOError'\nbuild\\temp.win-amd64-2.7\\Release\\src\\annoymodule.o:annoymodule.cc:(.text+0x55e): undefined reference to `__imp__Py_NoneStruct'\nbuild\\temp.win-amd64-2.7\\Release\\src\\annoymodule.o:annoymodule.cc:(.text+0x586): undefined reference to `__imp__Py_TrueStruct'\nbuild\\temp.win-amd64-2.7\\Release\\src\\annoymodule.o:annoymodule.cc:(.text+0x597): undefined reference to `__imp_PyExc_IOError'\nbuild\\temp.win-amd64-2.7\\Release\\src\\annoymodule.o:annoymodule.cc:(.text+0xb13): undefined reference to `__imp__Py_NoneStruct'\nbuild\\temp.win-amd64-2.7\\Release\\src\\annoymodule.o:annoymodule.cc:(.text+0xbc3): undefined reference to `__imp__Py_NoneStruct'\nbuild\\temp.win-amd64-2.7\\Release\\src\\annoymodule.o:annoymodule.cc:(.text+0xe91): undefined reference to `__imp__Py_NoneStruct'\nbuild\\temp.win-amd64-2.7\\Release\\src\\annoymodule.o:annoymodule.cc:(.text+0xf6c): undefined reference to `__imp__Py_NoneStruct'\nbuild\\temp.win-amd64-2.7\\Release\\src\\annoymodule.o:annoymodule.cc:(.text+0xfa7): undefined reference to `__imp__Py_NoneStruct'\ncollect2.exe: error: ld returned 1 exit status\nerror: command 'C:\\\\TDM-GCC-64\\\\bin\\\\g++.exe' failed with exit status 1\n. Thanks @erikbern I tried the libpython install and it worked! Can you explain what problem that package solves?\nWould you like me to uninstall libpython to try the solution in your second link?\n. Oh this is weird - it imported fine the first time, but after reopening Python, I get this error.\n```\nIn [1]: import annoy\n\nImportError                               Traceback (most recent call last)\n in ()\n----> 1 import annoy\nC:\\Users\\S\\Documents\\GitHub\\annoy\\annoy__init__.py in ()\n     13 # the License.\n     14\n---> 15 from .annoylib import *\n     16\n     17 class AnnoyIndex(Annoy):\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\n```\nIt seems to import OK at random and produce this error at random. This is extremely mysterious.\n. I've replaced Py_None by Py_BuildValue(\"\") as per https://docs.python.org/2/faq/windows.html#id7\nNext steps: \nPyExc_IOError - what should this be replaced by?\nPy_RETURN_TRUE - what should these be replaced by?\nThere's documentation on Py_BuildValue here: https://docs.python.org/2/c-api/arg.html but I can't make heads or tails of how to get it to do the IOError or the RETURN_TRUE.\n. For posterity: I updated my TDM-GCC-64 as it turns out there was a bug:\n\"An updated 64-bit GDB package has been released, as the Python distribution bundled with the first package was mistakenly the 32-bit version instead of the 64-bit version.\"\nand things compile now, but import annoy yields \n```\nImportError                               Traceback (most recent call last)\n in ()\n----> 1 import annoy\nC:\\Users\\S\\Documents\\GitHub\\annoy\\annoy__init__.py in ()\n     13 # the License.\n     14\n---> 15 from .annoylib import *\n     16\n     17 class AnnoyIndex(Annoy):\nImportError: No module named annoylib\n```\nHere is the full trace from the compilation:\n```\nC:\\Users\\S\\Documents\\GitHub\\annoy [master]> python setup.py build --compiler=mingw32\nrunning build\nrunning build_py\ncreating build\ncreating build\\lib.win-amd64-2.7\ncreating build\\lib.win-amd64-2.7\\annoy\ncopying annoy__init__.py -> build\\lib.win-amd64-2.7\\annoy\nrunning build_ext\nbuilding 'annoy.annoylib' extension\ncreating build\\temp.win-amd64-2.7\ncreating build\\temp.win-amd64-2.7\\Release\ncreating build\\temp.win-amd64-2.7\\Release\\src\nC:\\TDM-GCC-64\\bin\\gcc.exe -DMS_WIN64 -mdll -O -Wall -ID:\\Anaconda2\\include -ID:\\Anaconda2\\PC -c src/annoymodule.cc -o build\\temp.win-amd64-2.7\\Release\n\\src\\annoymodule.o -O3 -march=native -ffast-math\nIn file included from src/annoylib.h:30:0,\n                 from src/annoymodule.cc:15:\nsrc/mman.h: In function 'void mmap(void, size_t, int, int, int, off_t)':\nsrc/mman.h:102:48: warning: right shift count >= width of type [-Wshift-count-overflow]\n                     (DWORD)0 : (DWORD)((off >> 32) & 0xFFFFFFFFL);\n                                                ^\nsrc/mman.h:111:52: warning: right shift count >= width of type [-Wshift-count-overflow]\n                     (DWORD)0 : (DWORD)((maxSize >> 32) & 0xFFFFFFFFL);\n                                                    ^\nIn file included from D:\\Anaconda2\\include/Python.h:80:0,\n                 from src/annoymodule.cc:17:\nsrc/annoymodule.cc: In function 'PyObject py_an_load(py_annoy, PyObject)':\nD:\\Anaconda2\\include/object.h:769:22: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\n     ((PyObject)(op))->ob_refcnt++)\n                      ^\nD:\\Anaconda2\\include/boolobject.h:27:31: note: in expansion of macro 'Py_INCREF'\n #define Py_RETURN_TRUE return Py_INCREF(Py_True), Py_True\n                               ^\nsrc/annoymodule.cc:109:3: note: in expansion of macro 'Py_RETURN_TRUE'\n   Py_RETURN_TRUE;\n   ^\nsrc/annoymodule.cc: In function 'PyObject py_an_save(py_annoy, PyObject)':\nD:\\Anaconda2\\include/object.h:769:22: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\n     ((PyObject)(op))->ob_refcnt++)\n                      ^\nD:\\Anaconda2\\include/boolobject.h:27:31: note: in expansion of macro 'Py_INCREF'\n #define Py_RETURN_TRUE return Py_INCREF(Py_True), Py_True\n                               ^\nsrc/annoymodule.cc:128:3: note: in expansion of macro 'Py_RETURN_TRUE'\n   Py_RETURN_TRUE;\n   ^\nsrc/annoymodule.cc: In function 'PyObject py_an_build(py_annoy, PyObject)':\nD:\\Anaconda2\\include/object.h:769:22: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\n     ((PyObject)(op))->ob_refcnt++)\n                      ^\nD:\\Anaconda2\\include/boolobject.h:27:31: note: in expansion of macro 'Py_INCREF'\n #define Py_RETURN_TRUE return Py_INCREF(Py_True), Py_True\n                               ^\nsrc/annoymodule.cc:240:3: note: in expansion of macro 'Py_RETURN_TRUE'\n   Py_RETURN_TRUE;\n   ^\nsrc/annoymodule.cc: In function 'PyObject py_an_unload(py_annoy, PyObject)':\nD:\\Anaconda2\\include/object.h:769:22: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\n     ((PyObject)(op))->ob_refcnt++)\n                      ^\nD:\\Anaconda2\\include/boolobject.h:27:31: note: in expansion of macro 'Py_INCREF'\n #define Py_RETURN_TRUE return Py_INCREF(Py_True), Py_True\n                               ^\nsrc/annoymodule.cc:251:3: note: in expansion of macro 'Py_RETURN_TRUE'\n   Py_RETURN_TRUE;\n   ^\nsrc/annoymodule.cc: In function 'PyObject py_an_verbose(py_annoy, PyObject)':\nD:\\Anaconda2\\include/object.h:769:22: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\n     ((PyObject)(op))->ob_refcnt++)\n                      ^\nD:\\Anaconda2\\include/boolobject.h:27:31: note: in expansion of macro 'Py_INCREF'\n #define Py_RETURN_TRUE return Py_INCREF(Py_True), Py_True\n                               ^\nsrc/annoymodule.cc:295:3: note: in expansion of macro 'Py_RETURN_TRUE'\n   Py_RETURN_TRUE;\n   ^\nsrc/annoymodule.cc: In function 'PyObject create_module()':\nD:\\Anaconda2\\include/object.h:769:22: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\n     ((PyObject)(op))->ob_refcnt++)\n                      ^\nsrc/annoymodule.cc:389:3: note: in expansion of macro 'Py_INCREF'\n   Py_INCREF(&PyAnnoyType);\n   ^\nIn file included from src/annoymodule.cc:15:0:\nsrc/annoylib.h: In instantiation of 'void AnnoyIndex::build(int) [with S = int; T = float; Distance = Euclidean; Random = Kiss\n64Random]':\nsrc/annoymodule.cc:401:3:   required from here\nsrc/annoylib.h:46:36: warning: unknown conversion type character 'z' in format [-Wformat=]\n   #define showUpdate(...) { fprintf(stderr, VA_ARGS ); }\n                                    ^\nsrc/annoylib.h:292:21: note: in expansion of macro 'showUpdate'\n       if (_verbose) showUpdate(\"pass %zd...\\n\", _roots.size());\n                     ^\nsrc/annoylib.h:46:36: warning: too many arguments for format [-Wformat-extra-args]\n   #define showUpdate(...) { fprintf(stderr, VA_ARGS ); }\n                                    ^\nsrc/annoylib.h:292:21: note: in expansion of macro 'showUpdate'\n       if (_verbose) showUpdate(\"pass %zd...\\n\", _roots.size());\n                     ^\nsrc/annoylib.h: In instantiation of 'bool AnnoyIndex::load(const char*) [with S = int; T = float; Distance = Euclidean; Random\n = Kiss64Random]':\nsrc/annoymodule.cc:401:3:   required from here\nsrc/annoylib.h:46:36: warning: format '%lu' expects argument of type 'long unsigned int', but argument 3 has type 'std::vector<int, std::allocator<int\n\n\n::size_type {aka long long unsigned int}' [-Wformat=]\n   #define showUpdate(...) { fprintf(stderr, VA_ARGS ); }\n                                    ^\nsrc/annoylib.h:374:19: note: in expansion of macro 'showUpdate'\n     if (_verbose) showUpdate(\"found %lu roots with degree %d\\n\", _roots.size(), m);\n                   ^\nsrc/annoylib.h: In instantiation of 'void AnnoyIndex::build(int) [with S = int; T = float; Distance = Angular; Random = Kiss64\nRandom]':\nsrc/annoymodule.cc:401:3:   required from here\nsrc/annoylib.h:46:36: warning: unknown conversion type character 'z' in format [-Wformat=]\n   #define showUpdate(...) { fprintf(stderr, VA_ARGS ); }\n                                    ^\nsrc/annoylib.h:292:21: note: in expansion of macro 'showUpdate'\n       if (_verbose) showUpdate(\"pass %zd...\\n\", _roots.size());\n                     ^\nsrc/annoylib.h:46:36: warning: too many arguments for format [-Wformat-extra-args]\n   #define showUpdate(...) { fprintf(stderr, VA_ARGS ); }\n                                    ^\nsrc/annoylib.h:292:21: note: in expansion of macro 'showUpdate'\n       if (_verbose) showUpdate(\"pass %zd...\\n\", _roots.size());\n                     ^\nsrc/annoylib.h: In instantiation of 'bool AnnoyIndex::load(const char*) [with S = int; T = float; Distance = Angular; Random =\n Kiss64Random]':\nsrc/annoymodule.cc:401:3:   required from here\nsrc/annoylib.h:46:36: warning: format '%lu' expects argument of type 'long unsigned int', but argument 3 has type 'std::vector::_make_tree(const std::vector<_Tp>&) [with S = int; T = float; Distance = Eu\nclidean; Random = Kiss64Random]':\nsrc/annoylib.h:298:34:   required from 'void AnnoyIndex::build(int) [with S = int; T = float; Distance = Euclidean; Random = K\niss64Random]'\nsrc/annoymodule.cc:401:3:   required from here\nsrc/annoylib.h:46:36: warning: format '%lu' expects argument of type 'long unsigned int', but argument 3 has type 'std::vector::_make_tree(const std::vector<_Tp>&) [with S = int; T = float; Distance = An\ngular; Random = Kiss64Random]':\nsrc/annoylib.h:298:34:   required from 'void AnnoyIndex::build(int) [with S = int; T = float; Distance = Angular; Random = Kis\ns64Random]'\nsrc/annoymodule.cc:401:3:   required from here\nsrc/annoylib.h:46:36: warning: format '%lu' expects argument of type 'long unsigned int', but argument 3 has type 'std::vector<int, std::allocator<int\n::size_type {aka long long unsigned int}' [-Wformat=]\n   #define showUpdate(...) { fprintf(stderr, VA_ARGS ); }\n                                    ^\nsrc/annoylib.h:482:9: note: in expansion of macro 'showUpdate'\n         showUpdate(\"Failed splitting %lu items\\n\", indices.size());\n         ^\nwriting build\\temp.win-amd64-2.7\\Release\\src\\annoylib.def\nC:\\TDM-GCC-64\\bin\\g++.exe -DMS_WIN64 -shared -s build\\temp.win-amd64-2.7\\Release\\src\\annoymodule.o build\\temp.win-amd64-2.7\\Release\\src\\annoylib.def -\nLD:\\Anaconda2\\libs -LD:\\Anaconda2\\PCbuild\\amd64 -lpython27 -lmsvcr90 -o build\\lib.win-amd64-2.7\\annoy\\annoylib.pyd\n```\n. There is no .dll file anywhere in the github directory nor inside the egg when I unzip it. Probably why Python can't find it.\n. Ok so, here is what I'm doing:\n\n\n(1) Get rid of all traces of annoy.\n(2) Clone a fresh copy.\n(3) I figured out how to use pip for this: pip install --global-option build_ext --global-option --compiler=mingw32 annoy\nIt installs the ,pyd file and everything in the correct place.\nSo now import annoy causes this error:\n```\nIn [4]: import annoy\n\nImportError                               Traceback (most recent call last)\n in ()\n----> 1 import annoy\nD:\\Anaconda2\\lib\\site-packages\\annoy__init__.py in ()\n     13 # the License.\n     14\n---> 15 from .annoylib import *\n     16\n     17 class AnnoyIndex(Annoy):\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\n```\nI should probably just give up =)\n. No problem - thanks for your effort. I will keep subscribed to this thread and hope against hope.\n. Fixed with the latest commit!\n. I can confirm. I installed successfully as above, and then tried the Python example from the README. \nThe crash occurs on the last line:\nu.get_nns_by_item(0, 1000)\nwhile getting the nearest neighbors from the object that was not loaded from memory works fine:\nIn [10]: np.array(t.get_nns_by_item(0, 1000))\nOut[10]:\narray([  0, 477, 547, 746, 949, 580, 877,  29, 748, 543, 175, 341, 634,\n       447, 796, 294, 489, 750, 549, 760, 770, 878, 872, 718, 822, 559,\n       125, 991, 649, 134, 404, 430, 603, 733, 861, 923, 179, 223,  51...\n. ",
    "thirdwing": "Yes, MinGW is used to support Windows.\nI used to compile and load it. I think I need to write a Wiki page for Windows.\nGive me some time.\n. ",
    "Turkeynator": "Hi, I tried to use python setup.py build --compiler=mingw32 the compilation was terminated with the following message:\nIn file included from src/annoymodule.cc:15:0:\nsrc/annoylib.h:28:18: fatal error: mman.h: No such file or directory\n #include \"mman.h\"\n                  ^\ncompilation terminated.\nerror: command 'C:\\MinGW\\bin\\gcc.exe' failed with exit status 1\nHave you written the wiki page yet on how to compile annoy for windows, or do you know a way to solve this error?\nI'm relatively new to coding, so any help would be greatly appreciated!\nThank you\n. ",
    "cmpute": "still meet the problem, missing unistd.h.\nTry both installing from pip and source code, the problem remains\nwhen trying building from source using mingw, an error occured\n\n. ",
    "lenguyenthedat": "Thanks for clarifying!\nadd_item(i,v) turned out to be fast enough for me anyway.\n. ",
    "danielvarga": "Please do! Thanks a lot! Amusingly, this weird zombie process somehow interferes with the working of live processes, for example makes Preview dead slow, reproducibly. (Takes two minutes to open a document.) Only a full system restart helps. I'm wondering how can that even happen.\n. ",
    "edonyM": "pip2 is python package manager pip. I rename pip as pip2 for reminding of this is python2.x packages manager.\n. I tried as following.\n- step 1, pip install virutalenv venv\n- step 2, follow your recommendation\n_It did not work._ \nMaybe I should reinstall Python and try it again. Personally, I am curious about how to locate the _PyModule_Create2. There is not such symbol in file annoymodule.cc and annoylib.so\nsh\nreadelf -a annoylib.so | grep PyModule\n00000020e028  000500000007 R_X86_64_JUMP_SLO 0000000000000000 PyModule_AddObject + 0\n     5: 0000000000000000     0 FUNC    GLOBAL DEFAULT  UND PyModule_AddObject\n    83: 0000000000000000     0 FUNC    GLOBAL DEFAULT  UND PyModule_AddObject\n. ",
    "amcgregor": "Interestingly, I have begun to see the same linker error on an unrelated package, and this was the only Google hit for the exact exception string.\n```\ntox -e py27\nGLOB sdist-make: /Users/amcgregor/Projects/marrow/src/libsass-python/setup.py\npy27 inst-nodeps: /Users/amcgregor/Projects/marrow/src/libsass-python/.tox/dist/libsass-0.9.2.zip\npy27 installed: flake8==2.5.0,libsass==0.9.2,mccabe==0.3.1,pep8==1.5.7,pyflakes==1.0.0,six==1.10.0,Werkzeug==0.11.2,wheel==0.24.0\npy27 runtests: PYTHONHASHSEED='3228397935'\npy27 runtests: commands[0] | python -c from shutil import *; rmtree(\"build\", True)\npy27 runtests: commands[1] | python -m unittest sasstests\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"main\", fname, loader, pkg_name)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/main.py\", line 12, in \n    main(module=None)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/main.py\", line 94, in init\n    self.parseArgs(argv)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/main.py\", line 149, in parseArgs\n    self.createTests()\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/main.py\", line 158, in createTests\n    self.module)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py\", line 130, in loadTestsFromNames\n    suites = [self.loadTestsFromName(name, module) for name in names]\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py\", line 91, in loadTestsFromName\n    module = import('.'.join(parts_copy))\n  File \"sasstests.py\", line 23, in \n    import sass\n  File \"sass.py\", line 26, in \n    from _sass import OUTPUT_STYLES, compile_filename, compile_string\nImportError: dlopen(./_sass.so, 2): Symbol not found: _PyModule_Create2\n  Referenced from: ./_sass.so\n  Expected in: flat namespace\n in ./_sass.so\nERROR: InvocationError: '/Users/amcgregor/Projects/marrow/src/libsass-python/.tox/py27/bin/python -m unittest sass tests'\n```\nRunning using a brew-installed pypy or pypy3 does not exhibit this problem, but the both CPython 2 runs do.  The py27 compiled module similarly does not contain a reference to that symbol:\n```\nsymbols .tox/py27/lib/python2.7/site-modules/_sass.so | grep PyModule\n            0x000fad92 (     0x6) DYLD-STUB$$PyModule_AddObject [DYLD-STUB, LENGTH, NameNList, MangledNameNList, NList]\n\n```\nWeird.\n. ",
    "singlakdeepak": "@edonyM were you able to find out a solution to the error that you talked about,\n>>> import annoy\nTraceback (most recent call last):\n  File \"<input>\", line 1, in <module>\n  File \"/usr/local/lib/python2.7/site-packages/annoy/__init__.py\", line 15, in <module>\n    from .annoylib import *\nImportError: dlopen(/usr/local/lib/python2.7/site-packages/annoy/annoylib.so, 2): Symbol not found: _PyModule_Create2\n  Referenced from: /usr/local/lib/python2.7/site-packages/annoy/annoylib.so\n  Expected in: flat namespace\n in /usr/local/lib/python2.7/site-packages/annoy/annoylib.so\nI am getting the same error while I am working on Python 2.7 on HPC.. ",
    "brc591": "I haven't used GitHub before aside from browsing through code and downloading projects, so please bear with me.\nWhat is it that you're asking/wanting me to do?\n. Sorry for the late reply, but just wanted to say thank you very much for the fix! Just tested it out on my end and it's running without qualms here. So thank you again!\n. ",
    "anhldbk": "@erikbern Thank you Erik\n. ",
    "harsh157": "Shouldn't the distance range be 0 to 4? as it is computing 2*(1-cos). That is if we have vectors in opposing directions also.\n. ",
    "dudevil": "@erikbern \nI'm having trouble installing annoy through pip after this update:\nInstalling collected packages: annoy\n  Found existing installation: annoy 1.5.2\n    Uninstalling annoy:\n      Successfully uninstalled annoy\n  Running setup.py install for annoy\n    building 'annoy.annoylib' extension\n    x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -I/usr/include/python2.7 -c src/annoymodule.cc -o build/temp.linux-x86_64-2.7/src/annoymodule.o -O3 -march=native -ffast-math\n    cc1plus: warning: command line option \u2018-Wstrict-prototypes\u2019 is valid for C/ObjC but not for C++\n    src/annoymodule.cc:16:24: fatal error: kissrandom.h: No such file or directory\n     #include \"kissrandom.h\"\n                            ^\n    compilation terminated.\n    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\nClonning and running python setup.py install works fine.\n. Works great now. Thanks for the quick fix!\n. ",
    "kentkawai": "Ah, maybe I misunderstood. I thought ANN is the fast algorithm to get NN for one vector.\nThank you for your suggestion.\n. ",
    "rosmo": "Thanks. All kinds of suggestions, criticism and such are appreciated (first time using Go or SWIG, last time I touched C++ was years ago) :)\n. I can help if needed. The test is run by copying (I guess?) the annoy-test.go file under $GOPATH and running \"go test\".\n. I guess it might need swig 3.0. Unfortunately looks like swig 3.0 is only from Ubuntu trusty onwards (in trusty-backports). \n. I think trusty-backports is only for Ubuntu trusty. Can you upgrade to a newer release of Ubuntu on the CI test VM? \n. Can you try adding this PPA https://launchpad.net/~teward/+archive/ubuntu/swig3.0?\ndeb http://ppa.launchpad.net/teward/swig3.0/ubuntu precise main \ndeb-src http://ppa.launchpad.net/teward/swig3.0/ubuntu precise main\n. Let me set up an Ubuntu box this evening, and I'll try to compile it there.\n. I wouldn't use SWIG for anything unless I had to. I'm working on the Ubuntu compilation... it is quite more difficult than I thought. \n. Allright, so you'll need to these two PPA repositories:\n- https://launchpad.net/~evarlast/+archive/ubuntu/golang1.5\n- https://launchpad.net/~rosmo/+archive/ubuntu/swig3.0.7\n(eg. add-apt-repository ppa:evarlast/golang1.5 && add-apt-repository ppa:rosmo/swig3.0.7)\nThis should give you a newer Go than 1.0 (which is ancient) and also Swig 3.0.7 (-cgo was added in Swig 3.0.6). I managed to successfully build the example with these two.\n. Oh, and the swig command is: swig3.0 -go -intgosize 64 -cgo -c++ src/annoygomodule.i\nAnd make sure \"go version\" returns \"go version go1.5 linux/amd64\" (you will probably need to upgrade/dist-upgrade after adding the PPAs, and of course \"apt-get install swig3.0\").\n. That seems really strange. Maybe you can chain the shell commands into one command like /bin/sh -c \"cd $GOPATH/src/annoyindex ; go build\"\n. It looks like it's using some built-in version of Go (/home/travis/.gimme/versions/go1.4.2.linux.amd64/bin/go). Can you override it using /usr/bin/go (or whatever the path is)?\n. This should work:\nmkdir -p $GOPATH/src/annoy_test\ncp test/annoy_test.go  $GOPATH/src/annoy_test\ncd  $GOPATH/src/annoy_test\ngo list -f '{{range .TestImports}}{{.}} > {{end}}' annoy_test | xargs go get\ngo test\nIt's probably a suboptimal way of doing things, but it seemed to work :)\n. Maybe you can wrap that through /bin/sh? Like /bin/sh -c \"go list...\"\n. @doykle, thanks for the fix!\n@erikbern, Looks like a weird bot or crazy person :D. ",
    "Jeffrey04": "@erikbern I did not really wait until it crashed in my actual application, as I was running it in a shared server. But the memory usage was maxed out and the server started swapping heavily.\n. I will check again next week, thanks for the quick response\nand HAPPY NEW YEAR #offtopic\n. oh, it is fixed, thanks (and sorry for taking this long to confirm)\n. ",
    "ummae": "@erikbern \nhttp://eigen.tuxfamily.org \nEigen is one of promise linear algebra library. There is no dependencies and well tuned(SIMD, AVX, LAPACK..)\n. Yes, we can get cosine similarity using Eigen's dot products and math library, it would be much faster. \nI think container for feature vector could be changed by Eigen's DenceVector something similar.. \nhttps://github.com/spotify/annoy/blob/master/src/annoylib.h#L111 \nbut, I agreed it seems like little a bit over-spec ;)\n. In Eigen, just put \"-lgomp -fopenmp -march=native -fPIC\" will be enough for most cases, and actually, it's not that easy to optimize SIMD codes and make works for various cpu architectures and gccs. (Eigen does)\nYou don't need to digging SIMD instructions, Eigen is very easy to use. At the same time it's very fast.\n- Cheat sheet: http://eigen.tuxfamily.org/dox/AsciiQuickReference.txt\n- Benchmark: http://eigen.tuxfamily.org/index.php?title=Benchmark\nIf you want minimize refactoring cost, considering Map. It can map raw array data into Eigen container(Matrix, Array, Vectors..), so you can plugged-in Eigen codes into the place where you want without changing whole structures. \nfloat raw_array[128];\nMap<Matrix<float,16,8> > mm(raw_array);\n- http://eigen.tuxfamily.org/dox/group__TutorialMapClass.html\n. I'm not sure but maybe.. it will not works, by the way, is there not aligned vector in annoy? If so, you will face to the problems.. \nThis is documentation for alignment issues \n- http://eigen.tuxfamily.org/dox/group__DenseMatrixManipulation__Alignement.html\nIt seems you need to replace allocator for aligned vector... and there is a way turn off checking alignment, though in that case, you don't take advantage of vectorization, it's big deal for performance.\n- http://eigen.tuxfamily.org/index.php?title=FAQ#Vectorization\nAnyway, I look around annoylib.h a little bit, and now I'm sure Eigen would be helpful. Many functions written through for-loop, that means there are plenty of room for improvement through vectorization :smile: for example, distance function(https://github.com/spotify/annoy/blob/master/src/annoylib.h#L114) could be re-written one or two lines of code with Eigen like comment you written: a^2 / a^2 + b^2 / b^2 - 2ab/|a||b|\n. @erikbern would you let me know how did you do the benchmark? I will try\n. | branch | AccuracyTest.test_euclidean_100 |\n| --- | --- |\n| master | 1.64(+-0.18) |\n| erikbern/eigen | 1.67(+-0.07) |\n- (+-) stdev\n- seconds\nI was expected as 20~50% performance gain from Eigen (I didn't looks compiler options yet)\nPerhaps yes, cpu operations might not be main bottleneck point.  \nI'll take a look at these in a bit more detail..\n- Environ\n  - Intel(R) Xeon(R) CPU E5-2620 v3 @ 2.40GHz\n  - gcc 4.8.3\n. ",
    "cysin": "Eigen or other SIMD libs might be suitable for spotify but I had never used any before. I only code with specific intrinsics. Here is a code snippet to calculate l2 norm distance with avx2, hope this helps\n```\n// note that the input vectors are quantized and with fixed size\nint l2_norm_avx(unsigned char * f1, unsigned char * f2) {\n    __m256i t = _mm256_setzero_si256();\n    int i;\n    for(i = 0; i < 128;i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i ) (f1 + i));\n        __m256i b = _mm256_loadu_si256((__m256i ) (f2 + i));\n        __m256i sublo = _mm256_sub_epi16(\n            _mm256_unpacklo_epi8(a, _mm256_setzero_si256()),\n            _mm256_unpacklo_epi8(b, _mm256_setzero_si256())\n        );\n        t = _mm256_add_epi32(\n            t,\n            _mm256_madd_epi16(sublo, sublo)\n        );\n        __m256i subhi = _mm256_sub_epi16(\n            _mm256_unpackhi_epi8(a, _mm256_setzero_si256()),\n            _mm256_unpackhi_epi8(b, _mm256_setzero_si256())\n        );\n        t = _mm256_add_epi32(\n            t,\n            _mm256_madd_epi16(subhi, subhi)\n        );\n    }\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, t);\nreturn temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n}\n```\n. Compiler can optimize but it is hard to tell if it does the job right. You can try to write code like:\nfor(int i = 0; i < LEN; i += 4) {\n    data[i] = ...\n    data[i + 1] = ...\n    data[i + 2] = ...\n    data[i + 3] = ...\n}\nIn my mind this might help the compiler to vectorize data(process 4 or 8 items in a loop), but I am not sure. And for checking it out, this might help: http://superuser.com/questions/726395/how-to-check-if-a-binary-requires-sse4-or-avx-on-linux \nStill, a simd library would be a better choice:)\n. ",
    "pommedeterresautee": "this may help https://github.com/spotify/annoy/issues/97\n. for what it worth, no way to install Annoy on Python 3.5 (mingw is not yet ready and VS 2015 is missing a header)\n. Python version works also for me (tried with 10 millions 10 times).\nI close the issue for now.\nWill reopen if I find something related to the C++ part.. Nope, using the very last version (yesterday), I am using the set_seed function which have been included very recently.\nThe bug is quite strange... and this kind of issue difficult to fix (usually I blame the concurrency but here there is none).\nI am using Annoy in a prod grade system, having small variations in the results is very... annoying.\nI plan to try JavaCpp to call C++ code, may be it will help.\nIf you have any idea/tip... my mind is very open.. What I am not sure to understand is the numbers in the error message. It's the number of vectors it fails to put somewhere?\nAre the quantities shown in error message expected for 4 millions vectors and 200 trees?\nFor what it worth I am using rcppannoy.. Thanks for the pointer.\nIt was due to duplicates. I was not aware it may cause this issue.\nMay be you want to add in the error message that a cause of this issue may be related to the existence of duplicated vectors in the dataset? It may help others to debug... \nI let you clause the issue (in case you want to keep it open until the error message is updated). ",
    "birolkuyumcu": "for windows 10 64 bit\npython 2.7.x 64 bit - Anaconda distribution -\nTDM-GCC 64\ndownload source code\n'python setup.py install'\n\nworks\n. last pypi not working \nhttps://github.com/spotify/annoy/issues/160\nVC++ not compile because of missing header\nMingW compilers not compatable \ni solved by rebuilding for my system - look above-\n. how can help you ?\n. for windows version you must be  use a mingw compiler \nbut between version of compiler Binary incompatibility exist\nhttp://mingw.5.n7.nabble.com/MinGW-GCC-4-7-0-released-td21925.html\n\nBinary incompatibility notice!\nThe C and C++ ABI have changed, which means in general you can't link\ntogether binaries compiled with this version of the compiler and any\nprevious version.\n\nso simply re-build it \n. http://stackoverflow.com/questions/2842469/python-undefined-reference-to-imp-py-initmodule4\n. try this egg file \nannoy-1.8.0-py2.7-win-amd64.zip\n. problem solved\ndownload \nrebuild annoy\ni think binary incompatability of compiled libs caused this problem\nnow code works as expected\n. ",
    "bnbwn": "another windows user trying to get this to work... I think I am so close.\nI have read all the previous issues related to getting annoy compiled and properly loaded on windows.\npython 2.7 x64 using miniconda on Win10\ndownloaded the gcc / c++ complier      using    mingw-w64-install.exe\nextract to and save in C:/mingw-w64\nadd path to bin in PATH\ndownloaded annoy from github\nextract and save into C:/annoy\nC:\\annoy>python setup.py build --compiler mingw32\nC:\\annoy>python setup.py install\nafter many hours... victory.    I brute forced through every permutation of available compilers and build flags...\nat least for a moment....\n\nat this point I am unsure as it seems to be built correctly, and loaded properly, yet still isn't found. the .pyd is the newly created .dll.   python, then help(), then modules does show annoy as an option.\nif  @cadobe, @sergeyf, @thirdwing, @birolkuyumcu @tjrileywisc have any ideas, I'm all ears... and thanks again @erikbern \n\n. Yes, Thank you!\n-ben\nOn Thu, Jul 20, 2017 at 8:18 AM, tjrileywisc notifications@github.com\nwrote:\n\n@bnbwn https://github.com/bnbwn\nIf you're still interested in this, you might want to try my fork that is\ncompiling correctly on Windows (at least in python 3.5/3.6).\nhttps://github.com/tjrileywisc/annoy\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/spotify/annoy/issues/130#issuecomment-316700037, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ADnNSwzhWJs5KI_FBAx6X7GCALeaZziuks5sP1OJgaJpZM4HIeIS\n.\n. \n",
    "quole": "I thought I should mention I wrote a C#/.NET port of the Java (read-only) Annoy as part of my fork of Word2vec.Tools. It appears to work correctly but much too slowly, so it might not be useful in its current state. \nI haven't created detailed performance tests, but it appears that running gensim within a Docker image is much faster, with or without using an index, meaning an indexed (less accurate) lookup is generally far slower than even an unindexed (100% accurate) lookup in gensim. Partly this may be because I don't use a Memory Mapped File, but there are many other optimizations lacking (e.g. a lot of objects being created unnecessarily elsewhere in the supporting code).\nIt can only read an annoy index, not create one (just like the Java version). However, unlike the Java port, it supports index files larger than 4 GB.\nI haven't had time to work on it for a while so I thought I'd mention it in case anyone wanted to pick up the project or is looking for a starting point for a more proper C# port.\nhttps://github.com/quole/Word2vec.Tools/blob/master/Word2vec.Tools/AnnoyIndex.cs. ",
    "MarkWuNLP": "It works fine on my Windows Server 2012 with Anaconda 3. The only thing I did is to change the compiler=msvc that in the file \"C:\\Program Files\\Anaconda3\\Lib\\distutils\\distutils.cfg\".. ",
    "kamalkraj": "@MarkWuNLP  Thanks.It worked. ",
    "bittremieux": "Ok, see the attached file for an approximation of what I'm doing:\neuclidean.txt\nIf I run this, the returned distances are equal to the computed Euclidean distance (I also added the dot product for comparison, the vectors are normalized).\nFor example, I get the following output:\nannoy_dist = 0.8814489841461182    euclidean = 0.8814489556668296  dot = 0.6115238692769277\nannoy_dist = 0.8938978314399719 euclidean = 0.8938978219006966  dot = 0.6004733420005951\nannoy_dist = 0.9004353880882263 euclidean = 0.9004354783899279  dot = 0.5946079746283507\n. Ok, that was silly. Thanks for clearing up the confusion.\n. ",
    "searchivarius": "Annoy relies on random memory accesses. I bet, it's several dozens if not hundreds of CPU cycles on top of the actual computation. From my experience with optimizing memory layouts, for 100d data this typically means that L2s are twice as slow compared to the case when data is accessed sequentially. So, making L2s 10% faster won't make a big difference.\nHowever, if you make your L2s (or cosine's) 4times as slower, you will probably see the difference. This would happen if the compiler fails to vectorize the loops. Modern GCC compilers are good at vectorizing L2s and the cosine. However, some older versions and Clang aren't.\n. Sorry, I mean the Euclidean distance. Yep, optimizing memory access would be important. \n. ",
    "elfring": "How do you think about to rename also the identifier \"__ERROR_PRINTER_OVERRIDE__\"?\n. I suggest to reconsider the consequences of the following wording from the section \"16.2 Source file inclusion\" in the standard specification for the programming language \"C++\".\n```\n\u2026\nThe named source file is searched for in an implementation-defined manner. If this search is not supported, or if the search fails, the directive is reprocessed as if it read\ninclude  new-line\n\u2026\n```\n- How do you think about to avoid \"a duplicated file search\" here?\n- Would you like to restrict the searched directories for header files of your software?\n. There are different opinions about the handling of the involved implementation-defined behaviour.\n- Will header files be also searched outside the specified include directories if double quotes are used for the discussed preprocessor statement?\n- Is there a speed difference measurable if a file is not found there and the search will be retried with \"the angle brackets inclusion method\"?\n. No. - Are you quicker to integrate such a tiny adjustment?. ",
    "nicolamontecchio": "i was porting the euclidean part to java and found out that the angular one in c++ was suspicious \n. :+1: \n. ",
    "andy256": "One potential problem is the call to fopen() with \"w\" instead of \"wb\" in annoylib.h -- does changing that help?\n. It's not just pointer arithmetic, because nns_dist is a vector and it overloads []. That's why you can fix it by referencing [0] and THEN doing pointer arithmetic. You'll probably want to extend the RandRandom strict with this, and maybe implement it for the other (boost?) RANG that's available.... I hate autocorrect. Struct, RNG. Reproducible builds?. float32 is single precision\u200b. I don't know if there's any native support for half precision.... Hah, I never noticed that before but now that I think about it, this makes sense.\nThis doesn't actually affect any accuracy benchmarks does it?\n. ",
    "jfemiani": "I'll make sure to look into that first thing tomorrow and let you know if\nchanging that helps\nOn Mar 9, 2016 7:44 PM, \"Andy Sloane\" notifications@github.com wrote:\n\nOne potential problem is the call to fopen() with \"w\" instead of \"wb\" in\nannoylib.h -- does changing that help?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/issues/144#issuecomment-194632379.\n. Yes it worked.\nOn Apr 18, 2016 2:36 PM, \"Andy Sloane\" notifications@github.com wrote:\n@jfemiani https://github.com/jfemiani did you ever get a chance to try\nit?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/annoy/issues/144#issuecomment-211593100\n. \n",
    "dselivanov": "@erikbern thanks for your answer. But I suppose in case of usage some heuristic there is no theoretical guarantee for probability of finding candidates within some similarity threshold.\n. @erikbern this make sense :+1: \n. ",
    "wong2": "will you consider adding that?\n. ",
    "thomasjungblut": "FYI a neat trick has been proposed in this paper which builds multiple projection trees and then iteratively refines the nearest neighbours.\nThey report quite big gains in accuracy, go and check it out ;)\n. ",
    "Santimto": "@erikbern I am doing tests with bases of almost 1million and my interest is to improve the precision of the results.\nI am currently using:\ndims = 2048\nn_nearest_neighbors = 30\ntrees = 1000\nTo improve accuracy would it indicate reducing the number of trees? And in the part: t = AnnoyIndex (dims), the higher the better?\nCongratulations on the project!. @erikbern I'm not currently passing search_k. As I checked that search_k is n * n_trees, so am I analyzing 30k items coming up for similarity?\nMy intention is not time nor processing, I only want accuracy. I'm going to do the test assigning this value to 200k (in 1mi base). Maybe more... im try now 5mi in search_k value.\nAnalyzing your articles I really liked the: https://erikbern.com/2018/02/15/new-benchmarks-for-approximate-nearest-neighbors.html. For the task of finding the closest images for a new image on really big bases do you recommend any of them (HNSW, KGraph or other)?\nThanks for the response in such a short time!. ",
    "benbo": "Sure. Should I add a tiny section to \"How does it work\" or rather to \"more info\"?\n. ",
    "MonkeyChap": "Hi,\nJust waking up an old thread. Can you clarify if Annoy is normalising all the vectors (e.g. to a 0.0->1.0 range) or if the data must be supplied to it pre-normalised ?  The reason I ask is that I'm getting quite different results with FLANN and ANNOY and I think FLANN normalises before indexing. \nThanks\nIan\n. Thanks for the quick response Erik !\n. Hi Erik,\nAfter a bit of pain, I set up a new VM (I'm a Mac user) with no Anaconda, and just a vanilla mingw32 install. Still doesn't work, and the errors also seem to be in the disutils library. Is it possible that the problem is a python 3.6 compatibility thing ?\nC:\\annoy>python setup.py build --compiler=mingw32\nrunning build\nrunning build_py\nrunning build_ext\nTraceback (most recent call last):\n  File \"setup.py\", line 69, in \n    setup_requires=['nose>=1.0']\n  File \"C:\\Users\\Ian\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\distutils\\cor\ne.py\", line 148, in setup\n    dist.run_commands()\n  File \"C:\\Users\\Ian\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\distutils\\dis\nt.py\", line 955, in run_commands\n    self.run_command(cmd)\n  File \"C:\\Users\\Ian\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\distutils\\dis\nt.py\", line 974, in run_command\n    cmd_obj.run()\n  File \"C:\\Users\\Ian\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\distutils\\com\nmand\\build.py\", line 135, in run\n    self.run_command(cmd_name)\n  File \"C:\\Users\\Ian\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\distutils\\cmd\n.py\", line 313, in run_command\n    self.distribution.run_command(command)\n  File \"C:\\Users\\Ian\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\distutils\\dis\nt.py\", line 974, in run_command\n    cmd_obj.run()\n  File \"C:\\Users\\Ian\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\n\\setuptools\\command\\build_ext.py\", line 75, in run\n    _build_ext.run(self)\n  File \"C:\\Users\\Ian\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\distutils\\com\nmand\\build_ext.py\", line 308, in run\n    force=self.force)\n  File \"C:\\Users\\Ian\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\distutils\\cco\nmpiler.py\", line 1031, in new_compiler\n    return klass(None, dry_run, force)\n  File \"C:\\Users\\Ian\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\distutils\\cyg\nwinccompiler.py\", line 282, in init\n    CygwinCCompiler.init (self, verbose, dry_run, force)\n  File \"C:\\Users\\Ian\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\distutils\\cyg\nwinccompiler.py\", line 157, in init\n    self.dll_libraries = get_msvcr()\n  File \"C:\\Users\\Ian\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\distutils\\cyg\nwinccompiler.py\", line 86, in get_msvcr\n    raise ValueError(\"Unknown MS Compiler version %s \" % msc_ver)\nValueError: Unknown MS Compiler version 1900\nPS - there's no cygwin on this VM, so the cygwin errors look a bit odd.. OK, so some progress, but still doesn't work. For anyone else hitting this problem, you need to patch the distutils section in your Python install (ugly) as follows:\nhttps://stackoverflow.com/questions/34135280/valueerror-unknown-ms-compiler-version-1900\nThat said, it's still not compiling...\nwriting build\\temp.win-amd64-3.6\\Release\\src\\annoylib.cp36-win_amd64.def\nC:\\MinGW\\bin\\g++.exe -shared -s build\\temp.win-amd64-3.6\\Release\\src\\annoymodule\n.o build\\temp.win-amd64-3.6\\Release\\src\\annoylib.cp36-win_amd64.def -LC:\\Users\\I\nan\\AppData\\Local\\Programs\\Python\\Python36\\libs -LC:\\Users\\Ian\\AppData\\Local\\Prog\nrams\\Python\\Python36\\PCbuild\\amd64 -lpython36 -lvcruntime140 -o build\\lib.win-am\nd64-3.6\\annoy\\annoylib.cp36-win_amd64.pyd\nc:/mingw/bin/../lib/gcc/mingw32/5.3.0/../../../../mingw32/bin/ld.exe: cannot fin\nd -lvcruntime140\ncollect2.exe: error: ld returned 1 exit status\nerror: command 'C:\\MinGW\\bin\\g++.exe' failed with exit status 1. Hmmm...given what a PITA this is, is it worth putting a Windows binary up on the repo ?. Agreed - Annoy works fantastically well on my Mac, but after hours and hours of trying everything I can think of, I still can't build it on Windows (7 or 10).. Thanks - I'll give it a try !  Would be really great to just have something that installs with pip though :)\n. I can confirm it compiles (first time !) on windows with no messing about required.\nI did find another another problem though which is totally unrelated to the Windows stuff. I'll dig into it some more and post another thread.. Thanks - I couldn't see anything obvious other than the catch-all for unknown metrics...but couldn't see any issues in that. If you need example data let me know and I'll write some scripts to generate it.. Hi\nOur code didn't change. I can reproduce the problem by reinstalling (pip) Annoy at either 1.8.3 or 1.9.1. With 1.8.3 we get great results that closely match Euclidean brute KNN. At 1.9.1, with no changes to our code, we get output that looks more like what you'd expect for cosine distance.\nIan. Here goes...\nFirst stage - create the index:\nmyIndex = AnnoyIndex(myData, metric=\"euclidean\")\nthen loop through the data, adding each vector:\nfor i,vector in enumerate(myData) :  \n    myIndex.add_item(i,vector)\nmyIndex.build(10)\nmyIndex.save('testdata.annoyIndex')\nThen I'm calling get_nns_by_item to get the six near neighbours for each of the items the user selects:\nresult = myIndex.get_nns_by_item(selectedItem,6)\nPretty simple really, but I get different neighbours in some cases using the two different versions of Annoy. It's not always the case, but it's often enough to stand out when I'm using test datasets I know well. The results look a bit like angular distance comparisons when I run Euclidean on 1.9.1. On 1.8.3 they're pretty close to brute-force KNN - so close in fact that I rarely see a wrong neighbour. In both cases, it's really fast - awesome NN algorithm !. Ah - I'm lagging here - sounds like you've found the problem !\n. ",
    "starius": "All lua build targets work. The build fails because of unrelated issue with go.\n. I am going to apply it in bioinformatics to predict gene starts using DNA sequence.\n. > cool \u2013 what's the main benefit(s)?\nThe main (and single) benefit is that new method works and fixes tests.\n. ",
    "seanlaw": "It's not entirely clear what happened but restarting my terminal fixed the problem. I tried it on a Mac Mini which worked fine as well and so it could've been a problem with my environment. \n. ",
    "tangfucius": "sorry it was a stupid mistake on my part. I didn't put in the metric argument when I loaded the index.\n. ",
    "csanhuezalobos": "Maybe this is a starting point?\nhttps://erikbern.com/2014/01/12/benchmarking-nearest-neighbor-libraries-in-python/\n. Really small contribution, but it was part of my learning process.\n. ",
    "SkyTodInfi": "Yes,  just remove -march=native from setup.py,  everything is ok.\nThank you.\n. ",
    "arjunktr": "I'm facing the same issue, is there any workaround?. Thanks for the reply, I'm trying to install on AWS EC2 which has  python\n2.6 version\nOn Sat, Mar 24, 2018, 6:34 PM Erik Bernhardsson notifications@github.com\nwrote:\n\nare you on any nonstandard platform?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/spotify/annoy/issues/167#issuecomment-375938544, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AB2GqF7KYeTo2jBBNeaoFB0joGuqwJqBks5thvQ0gaJpZM4JW7kR\n.\n. Hi Erik,\nIs there any workaround for this?\n\nThanks!. ",
    "fpergola": "@erikbern remove the -march=native flag allows install and I've had no issues with using the package.. ",
    "ajkl": "Even with random projections, once the index is built, you can generate a binary hash based on the forest of trees built right? I was wondering if i get binary hash of all the items in the index, i can pack them and save on memory footprint and just perform a hamming distance at runtime. Thoughts ?\n. ",
    "codeAshu": "realized there is no need of it. It's pretty fast!\n. ",
    "kylemcdonald": "this was literally my first question too, not surprised to find you here @Quasimondo :)\nfor me, the change was adding to AnnoyIndex:\n``` c++\n  void unbuild() {\n    if (_loaded) {\n      showUpdate(\"You can't unbuild a loaded index\\n\");\n      return;\n    }\n_roots.clear();\n_n_nodes = _n_items;\n\n}\n```\nand to AnnoyIndexInterface:\nc++\n  virtual void unbuild() = 0;\nbut the more i play with it, the less certain i am that annoy is the right tool for situations with online/realtime updates. however, if you have two sets of things: one big set you can precalculate, and one smaller set of recent additions, it can make sense to keep two annoy instances and do lookups on both with some final filtering/combining step. then when you add recent additions you only need to do a small rebuild instead of a big one.\n. ",
    "Quasimondo": "I am already successfully using Annoy for dynamically adding new elements and at least so far the delay through having to copying all old elements to a new instance, then adding the new elements, building the tree is bearable - though my use case is not realtime and so far I am not yet in the million items areae, so the 2 seconds that this takes I can tolerate. Still it feels like it's a bit ineffective that way.\n. Thanks @kylemcdonald for the detailed code example. Since I'm using the Python wrapper there were a few more changes in addition to the ones you mentioned above necessary to make it work:\nIn annoymodule.cc I had to add the following lines:\n```\nstatic PyObject \npy_an_unbuild(py_annoy self, PyObject *args) {\n  if (!self->ptr) \n    Py_RETURN_NONE;\nPy_BEGIN_ALLOW_THREADS;\n  self->ptr->unbuild();\n  Py_END_ALLOW_THREADS;\nPy_RETURN_TRUE;\n}\n```\nas well as augment the AnnoyMethods[]:\nstatic PyMethodDef AnnoyMethods[] = {\n  {\"load\",  (PyCFunction)py_an_load, METH_VARARGS, \"\"},\n  {\"save\",  (PyCFunction)py_an_save, METH_VARARGS, \"\"},\n  {\"get_nns_by_item\",(PyCFunction)py_an_get_nns_by_item, METH_VARARGS, \"\"},\n  {\"get_nns_by_vector\",(PyCFunction)py_an_get_nns_by_vector, METH_VARARGS, \"\"},\n  {\"get_item_vector\",(PyCFunction)py_an_get_item_vector, METH_VARARGS, \"\"},\n  {\"add_item\",(PyCFunction)py_an_add_item, METH_VARARGS, \"\"},\n  {\"build\",(PyCFunction)py_an_build, METH_VARARGS, \"\"},\n  {\"unbuild\",(PyCFunction)py_an_unbuild, METH_VARARGS, \"\"},\n  {\"unload\",(PyCFunction)py_an_unload, METH_VARARGS, \"\"},\n  {\"get_distance\",(PyCFunction)py_an_get_distance, METH_VARARGS, \"\"},\n  {\"get_n_items\",(PyCFunction)py_an_get_n_items, METH_VARARGS, \"\"},\n  {\"verbose\",(PyCFunction)py_an_verbose, METH_VARARGS, \"\"},\n  {NULL, NULL, 0, NULL}      /* Sentinel */\n};\nIn annoy/init.py these lines have to be added:\ndef unbuild(self):\n        \"\"\"\n        Allows to add new items after a tree has been build\n        \"\"\"\n        return super(AnnoyIndex, self).unbuild()`\nAfter increasing the version number, using pip install . from inside the main directory it works in my Python as well now.\n. I just realized that there is one snag with unbuild() for the typical use case: right now this does not work with loaded indices. So what I think this would need to work is to first unlink the index from the shared image but keep a copy, then unload() it, then call unbuild(), add the new items, call build() again. Of course then comes the problem that it cannot save the new index to the same file in case other processes are still using it, but at least in my case there is only one instance using it.\nIs that a feasible approach?\n. I have added two tests in annoy_test.py but I am not sure if I did it correctly:\nhttps://github.com/Quasimondo/annoy/commit/20bda288d06ecf9156199f1df800bb468f26c22b\n. Ah yes - admittedly all I did here was to mimic what the build() method was doing without really understanding what is going on :-) \n. Regarding the unit test I'm kind of super-unexperienced in that area, but I will look at how the other tests look like and try to cook something up.\n. ",
    "robclouth": "@kylemcdonald fancy seeing you here too! yeah i just came across this. i need to be able to dynamically add new vectors. it seems like the sklearn BallTree implementation has this same restriction. unbuild is a good start though.\n. Flann for example allows you to add new points after the initial build, but will trigger a new build of the index after a certain number of additions to re-balance it. I can't find any implementations that don't need re-balancing at some point...\n. ",
    "vadimkantorov": "I'm also looking to dynamically update an index, but don't actually need to save it on disk: training a ConvNet, and need to update the index with freshly computed embeddings. Should calling add_item(index_to_update, new_vector) and periodically calling build() work?\n. But once unbuild() is called, the index cannot answer NN queries, correct?\n. ",
    "stgzr": "So is there any solutions for adding new items after index file loaded?\nI tried unbuild() after calling load(), but it just showed \"You can't unbuild a loaded index\"...\n. ",
    "beniz": "@erikbern hi, regarding your comment https://github.com/spotify/annoy/issues/174#issuecomment-255596110 on using LMDB thus allowing annoy to support updates, I'd be interested in hearing more, as you suggested you may outline the approach. Thanks!. ",
    "lakshayg": "I would like to work on this issue. Can you tell me where to get started?. ",
    "Yorko": "Oops.. sorry. . ",
    "kevindoyle": "@leonardidouglas is there a reason you say that? I cannot use the package without this modification.\n@erikbern thanks for taking a look! It doesn't look like @rosmo has been active recently, I'm not sure when he'll get to review this. It would be nice to have this PR come in soon if no changes are requested.. ",
    "lisitsyn": "@erikbern I was hunting down exactly what @a1k0n said.\nUsual begin and end would be better, indeed. ~~Let me know if you prefer these.~~. ",
    "yonromai": "@andy256: perhaps a simple seeding would do?. @andy256: Is this what you mean? It seems like the other RNGs (KissXYRandom) are already seeded . @andy256: is this more like what you had in mind?\n(don't worry I'll squash if we want to merge...). lol, CI/testing are totally overrated ;)\n@erikbern do you think this change would impact perf of annoy?. actually AFAIC it's not for testing, but allowing to seed the RNG would be nice to make our pipelines idempotent. \nBut I don't have superstrong feelings about this, so feel free to close the PR if you disagree. . Maybe the Python/C++ binding code contains some static state that interferes when another tree is created within the same file. But the fact that it depends on the numpy version is kinda scary. \nIn any case, when I run this code many times, I always get the same result:\n```python\nimport numpy as np\nfrom annoy import AnnoyIndex\nnp.random.seed(42)\nX = np.random.rand(100000, 60)\nY = np.random.rand(500, 60)\nannoy1 = AnnoyIndex(60)\nannoy1.set_seed(100)\nfor i in range(X.shape[0]):\n    annoy1.add_item(i, X[i, :])\nannoy1.build(10)\nfor k in range(Y.shape[0]):\n    print \"annoy1\", annoy1.get_nns_by_vector(Y[k, :], 3)\nAlso, when I change the annoy seed, I get different neighbors, which means that the change is propagated. . Yes, that's a good point. So perhaps the drop I'm seeing is due to a bunch of all zero vecs polluting the results? or perhaps just the increase in tree size?. Thanks for coming back to me. Next week's crazy so I won't have much time to look into it but don't worry too much about it; we've been using a stale version of annoy for 2 years, we can wait a bit more xD. Your code was `i*2**-0.5`:python\nt = set([int(i2*-0.5) for i in range(1000)])\nmax(t) # 706\nlen(t) # 707\n```. I bumped into this because I think the code with holes doesn't work here:\nhttps://github.com/spotify/annoy/blob/master/src/annoymodule.cc#L233\nIt crashes when you have holes because n_items != max(items). maybe not, looks like the tests are still passing. The problem is probably in my code :). yes, looking into it right now :). sure thing. (apologies for randomly popping up here and writing an essay)\n@erikbern, I don't understand the rational behind using the euclidean distance here?\n\nTo try to clarify this from an API standpoint:\n\n\nThe distance function is exposed to the users as follows:\npy\nt = AnnoyIndex(2, 'dot')\nt.add_item(0, [0, 1])\nt.add_item(1, [1, 1])\nd = t.get_distance(0, 1)\nwhere get_distance is defined as follows:\ncpp\nT get_distance(S i, S j) {\n    return D::normalized_distance(D::distance(_get(i), _get(j), _f));\n  }\n\n\nThe doc does not specify anything special about the distance: \"Returns the distance between items i and j.\"\n\n\nTherefore, I believe the user would expect that t.get_distance(v_1, v_2) would use the distance specified in AnnoyIndex(f, {DIST}).\nSo in this case it would be dot(vectors[v_1], vectors[v_2]). \n\n\n\nOne issue is that dot(x, y) is more a similarity than a distance. I assume -dot(...) is more convenient than C - dot(...) since the dot is not normalized.\nI guess in this case we could return either -dot(original_vec_i, original_vec_j) or 1 - cos(augmented_vec_i, augmented_vec_j).\n(where original_vec_i is of dimension N and augmented_vec_i is of dimension N+1).\nGiven the API example above, returning the dot value seems to make more sense to me than returning the cos value.\n\n=> @erikbern, @psobot: WDYT?. ",
    "ijklr": "Hi Gan, I ran your python, it produced the same results for me. Where do\nyou see the difference?\nOn Fri, Mar 3, 2017 at 5:26 PM, Gan Gao notifications@github.com wrote:\n\nHi Team,\nI hope you're all doing well. Thank you for all hard works you put to\ndevelop Annoy.\nI've noticed that the most recent commit adds the functionality to set\nseed, and this is really good for making results reproducible. However,\nwhen I run the program with the follow code, it does not produce the same\nresults. Would anyone of you can help me to look at it?\nThank you very much. I appreciate it.\nBest,\nGan\nimport numpy as np\nfrom annoy import AnnoyIndex\nX = np.random.rand(100000, 60)\nY = np.random.rand(500, 60)\nannoy1 = AnnoyIndex(60)\nannoy1.set_seed(100)\nfor i in range(X.shape[0]):\n    annoy1.add_item(i, X[i, :])\nannoy1.build(10)\nannoy2 = AnnoyIndex(60)\nannoy2.set_seed(100)\nfor j in range(X.shape[0]):\n    annoy2.add_item(j, X[j, :])\nannoy2.build(10)\nresult1 = []\nresult2 = []\nfor k in range(Y.shape[0]):\n    print \"annoy1\", annoy1.get_nns_by_vector(Y[k, :], 3)\n    result1 += annoy1.get_nns_by_vector(Y[k, :], 3)\n    print \"annoy2\", annoy2.get_nns_by_vector(Y[k, :], 3)\n    result2 += annoy2.get_nns_by_vector(Y[k, :], 3)\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/spotify/annoy/issues/188, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AMX5gFJEN5EylLekcSwxJR9Pc5xgCW6_ks5riJOzgaJpZM4MS0Yq\n.\n. @gg2572  so I was able to reproduce it finally. I think it's just python trying to do some optimization by use-using objects. Apparently if they are generated in different scope, then you'll be fine, like this https://github.com/ijklr/annoy/blob/python/seed_test.py\n\nbtw The python API uses Kiss64 RNG by default and it has the same seed in the constructor, so even if you don't set the seed, the random numbers should be same every time.. ",
    "gg2572": "Hi Yi Cheng,\n@ijklr \nThank you for your reply!\nYes, here's what I get.\nannoy1 [23559, 25499, 49827]\nannoy2 [23559, 25499, 49827]\nannoy1 [97178, 58504, 30602]\nannoy2 [97178, 58504, 30602]\nannoy1 [16834, 15731, 15958]\nannoy2 [16834, 15731, 15958]\nannoy1 [13066, 48258, 14626]\nannoy2 [40847, 9026, 28475]\nannoy1 [82735, 37999, 56475]\nannoy2 [49271, 50974, 45406]\nannoy1 [8258, 99461, 3842]\nannoy2 [74761, 17914, 42611]\nSometimes I get the same but sometimes they're different.. So I tried with the same code with a different numpy version (1.11.3) and now they're the same. Trying to figure out why the latest numpy (1.12.0) is causing the problem.. @yonromai Hi Romain Thank you for your reply. I did notice that Numpy 1.2.0 changed something with numpy.random, but not quite sure if that matters. . Actually it may not be the problem with numpy. I ran a few times with python2.7 and numpy1.11, it gives me different results. However when I use python3.4, I'm getting consistent results.. @erikbern @yonromai @ijklr Thank you guys for all the effors! This works for me.. ",
    "AakashKumarNain": "Sure.. ",
    "mazorigal": "Hi, \nis there any update regarding that issue ?\nthanks,. according to the docs:\n```\nfrom annoy import AnnoyIndex\nimport random\nf = 40\nt = AnnoyIndex(f)  # Length of item vector that will be indexed\nfor i in xrange(1000):\n    v = [random.gauss(0, 1) for z in xrange(f)]\n    t.add_item(i, v)\nt.build(10) # 10 trees\nt.save('test.ann')\n```\nI tried and saw that when I have new items which I would like to include in the index, or to override existing items in the index, I can just use \nt.add(i,v) for the new item, or item to be overridden, without adding all other items again.\nthen just using  t.build(10) to build the new tree.\nThe tree build step is quite fast, hence can fit for streaming use cases, in which I would like to update and rebuild the index in real-time for new streaming items.\nSometimes there are items which are deleted, and I dont want to recommend them, and here I see a big advantage to have the ability removing those items from the index.. could you please explain a bit more the meaning \"allocate a lot of new memory for the tree structure\" ? does the memory allocation is for completely new tree which is re-builded each time t.build() is executed ? does the memory of the \"old\" tree would be released once t.build() is triggered ?\nCan I conclude from your answer that Annoy is not suitable for streaming updates style but rather batch training, let say each 1 hour ?. Ok, thanks.\nIs there any option at all to implement streaming ann ? Any ideas ?\n. in my case the ids are integers, but those are items ids and for example an item id could be 256020837, which according to the logic of max(id), annoy would allocate memory for 256020838 items, whereas in reality I have only 2M items.\nAs a result I get memory errors.\nSo I cannot use consecutive ids, and the question is whether only mapping between my real item ids and consecutive ids is the only solution ?\nOr its possible to add to annoy the functionality to work with non consecutive ids, but in such case the user would have to provide the num of items to be indexed ?. ",
    "geert56": "It looks like the angular distance still has a normalize_distance function that is a left-over\nfrom when it was wrongly implemented as Euclidean. There is no need for a sqrt() computation\nto obtain the final distances of the nearest neighbors (although quite often these distances are\ndiscarded).. ",
    "tpeng": "hmm, apologies, i use 1.8.0.\nchecking the commit history, looks like https://github.com/spotify/annoy/commit/c33b84909aa96b75a055fe78b909ef53a9f1ba5c fixed it. closing it.. ",
    "loisaidasam": "I'd love to see some more tags if possible.\nI was just trying to compare a few (older) versions and found it very difficult. In the end I used pypi's history (https://pypi.org/project/annoy/#history) and then looked through the project's commit history (https://github.com/spotify/annoy/commits/master) to find related commits around those dates.\nThis is a bit of a tedious task, but might be helpful to others down the road.\nApplying older tags should be as simple as:\n```\ngit checkout 079a6786135e9a229d2ab7ba73c9679bc229a5b5\ngit tag v1.8.3\n...\ngit checkout f547058b5d9d7f230c9bf2684d1879665e45ea8e\ngit tag v1.9.3\n... etc\ngit push --tags\n```. ",
    "natanlaverde": "I implemented the L1 distance modifying the L2 distance and also the tests for python.\nI made a pull request #206 containing that.\n. ",
    "eugenepalovcak": "Hi, I just installed annoy (on Ubuntu 16.04, with 'pip install annoy', today) but haven't gotten the L1 distance to work. If I run:\nfrom annoy import AnnoyIndex\nf = 2\ni = AnnoyIndex(f, 'manhattan')\ni.add_item(0, [2,2])\ni.add_item(1, [3,2])\ni.add_item(2, [3,3])\ni.build(10)\nprint i.get_nns_by_vector([2,2],3)\nthe output is 'None'. The 'angular' distance instead works as expected. Is something wrong?. ",
    "bartolsthoorn": "I am not so concerned about speed, just about RAM, but I wasn't aware that using float16 in C is actually quite a hassle.. For citing the library, we used the following bibtex entry (and it ended up published with this):\ntex\n@misc{annoy,\n  title = {{ANNOY} library},\n  howpublished = {\\url{https://github.com/spotify/annoy}},\n  note = {Accessed: 2017-08-01}\n}\n(update the accessed date with your own). ",
    "masakinakada": "Thanks @erikbern !\nWhy is it better to do dimensionally reduction? for the speed or for the\naccuracy?\nSorry for the additional question.\nOn Wed, Apr 19, 2017 at 8:54 AM, Erik Bernhardsson <notifications@github.com\n\nwrote:\nClosed #201 https://github.com/spotify/annoy/issues/201.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/spotify/annoy/issues/201#event-1048853930, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ADU4RNrDUN4nOLUvHQutHENVZeRFnjDyks5rxi4xgaJpZM4NB4lX\n.\n. Thank you very much! I will try that then! \nMasaki\nOn Apr 19, 29 Heisei, at 9:34 AM, Erik Bernhardsson notifications@github.com wrote:\nyeah it will probably improve either/both of speed and accuracy\n\u2015\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \n",
    "rshest": "@erikbern Looks good, thanks a lot!. ",
    "MBetters": "Hello, I'm trying to build on windows and also had difficulty. I cloned @tjrileywisc's branch. @MonkeyChap, how did you compile it? I appreciate any help!. @tjrileywisc \nI changed into the directory and ran python setup.py install, which gave me some errors...\nrunning install\nrunning bdist_egg\nrunning egg_info\nwriting annoy.egg-info\\PKG-INFO\nwriting top-level names to annoy.egg-info\\top_level.txt\nwriting dependency_links to annoy.egg-info\\dependency_links.txt\nreading manifest file 'annoy.egg-info\\SOURCES.txt'\nreading manifest template 'MANIFEST.in'\nwriting manifest file 'annoy.egg-info\\SOURCES.txt'\ninstalling library code to build\\bdist.win32\\egg\nrunning install_lib\nrunning build_py\nrunning build_ext\nbuilding 'annoy.annoylib' extension\nC:\\Users\\591961\\AppData\\Local\\Programs\\Common\\Microsoft\\Visual C++ for Python\\9.0\\VC\\Bin\\cl.exe /c /nologo /Ox /MD /W3 /GS- /DNDEBUG -IC:\\Python27\\include -IC:\\Users\\591961\\Desktop\\fusion\\system\\fef-tuning\\ml_model\\bin\\venv\\PC /Tpsrc/annoymodule.cc /Fobuild\\temp.win32-2.7\\Release\\src/annoymodule.obj\nannoymodule.cc\nC:\\Users\\591961\\AppData\\Local\\Programs\\Common\\Microsoft\\Visual C++ for Python\\9.0\\VC\\Include\\xlocale(342) : warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc\nc:\\users\\591961\\desktop\\fusion\\system\\fef-tuning\\ml_model\\bin\\annoy\\src\\annoylib.h(31) : fatal error C1083: Cannot open include file: 'stdint.h': No such file or directory\nerror: command 'C:\\\\Users\\\\591961\\\\AppData\\\\Local\\\\Programs\\\\Common\\\\Microsoft\\\\Visual C++ for Python\\\\9.0\\\\VC\\\\Bin\\\\cl.exe' failed with exit status 2. @tjrileywisc \nSo I added the header file. Now, Im able to run python setup.py install no problem. However, when I go to run the following code, the python interpreter crashes without a stack trace. It just says that python.exe has stopped working.\n```\nfrom annoy import AnnoyIndex\nimport random\nf = 40\nt = AnnoyIndex(f)  # Length of item vector that will be indexed\nfor i in xrange(1000):\n    v = [random.gauss(0, 1) for z in xrange(f)]\n    t.add_item(i, v)\nt.build(10) # 10 trees\nt.save('test.ann')\n...\nu = AnnoyIndex(f)\nu.load('test.ann') # super fast, will just mmap the file\nprint(u.get_nns_by_item(0, 1000)) # will find the 1000 nearest neighbors\n```. Nevermind, I just updated my project to python 3.6 and installing and using annoy worked. Although, it would be nice to have this work with windows 10/python 2.7 in the future.. ",
    "seansaito": "Another example, this time including the distances:\nprint t.get_nns_by_vector(query, n=1, include_distances=True)\nprint t.get_nns_by_vector(query, n=10, include_distances=True)\nResults:\n```\n\n\n\n([9982], [0.5744565725326538])\n([5587, 4605, 5092, 1910, 9714, 428, 9982, 7339, 1149, 6366], \n[0.5291593074798584, 0.5517486929893494, \n0.5546104907989502, 0.5563807487487793, \n0.5574793815612793, 0.5640119314193726, \n0.5744565725326538, 0.5751141905784607, \n0.5753101706504822, 0.5798651576042175])\n```. Also running into the same problem for other values of n:\n\n\n\nprint t.get_nns_by_vector(query, n=1, include_distances=True)\nprint t.get_nns_by_vector(query, n=2, include_distances=True)\nprint t.get_nns_by_vector(query, n=4, include_distances=True)\nprint t.get_nns_by_vector(query, n=10, include_distances=True)\nResults:\n```\n\n\n\n([9982], [0.574456512928009])\n([9982, 2316], [0.574456512928009, 0.5811503529548645])\n([9982, 2316, 8023, 4576], \n[0.574456512928009, 0.5811503529548645, \n0.5942330956459045, 0.594707727432251])\n([5587, 4605, 5092, 1910, 9714, 428, 9982, 7339, 1149, 6366], \n[0.5291593074798584, 0.5517486929893494, \n0.5546104907989502, 0.5563807487487793, \n0.5574793815612793, 0.5640119314193726, \n0.5744565725326538, 0.5751141905784607, \n0.5753101706504822, 0.5798651576042175])\n```. \n\n\n",
    "pmezard": "Build node cpuinfo:\nprocessor       : 0\nvendor_id       : GenuineIntel\ncpu family      : 6\nmodel           : 63\nmodel name      : Intel(R) Xeon(R) CPU E5-2666 v3 @ 2.90GHz\nstepping        : 2\nmicrocode       : 0x2a\ncpu MHz         : 2900.040\ncache size      : 25600 KB\nphysical id     : 0\nsiblings        : 2\ncore id         : 0\ncpu cores       : 1\napicid          : 0\ninitial apicid  : 0\nfpu             : yes\nfpu_exception   : yes\ncpuid level     : 13\nwp              : yes\nflags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm fsgsbase bmi1 avx2 smep bmi2 erms invpcid xsaveopt\nbugs            :\nbogomips        : 5800.08\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 46 bits physical, 48 bits virtual\npower management:\nRuntime node cpuinfo:\nprocessor       : 0                             \nvendor_id       : GenuineIntel                  \ncpu family      : 6                             \nmodel           : 62                            \nmodel name      : Intel(R) Xeon(R) CPU @ 2.50GHz\nstepping        : 4                             \nmicrocode       : 0x1                           \ncpu MHz         : 2500.000                      \ncache size      : 30720 KB                      \nphysical id     : 0                             \nsiblings        : 8                             \ncore id         : 0                             \ncpu cores       : 4                             \napicid          : 0                             \ninitial apicid  : 0                             \nfpu             : yes                           \nfpu_exception   : yes                                                                                                                                                                           cpuid level     : 13                                                                                                                                                                            \nwp              : yes                                                                                                                                                                           \nflags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nons\ntop_tsc eagerfpu pni pclmulqdq ssse3 cx16 sse4_1 sse4_2 x2apic popcnt aes xsave avx f16c rdrand hypervisor lahf_lm fsgsbase tsc_adjust smep erms xsaveopt\nbugs            :                               \nbogomips        : 5000.00                                                                       \nclflush size    : 64                               \ncache_alignment : 64                               \naddress sizes   : 46 bits physical, 48 bits virtual\npower management:\nDiffing flags gives:\n@@ -1,10 +1,6 @@\n-abm\n aes\n apic\n avx\n-avx2\n-bmi1\n-bmi2\n clflush\n cmov\n constant_tsc\n@@ -14,26 +10,23 @@\n eagerfpu\n erms\n f16c\n-fma\n fpu\n fsgsbase\n fxsr\n ht\n hypervisor\n-invpcid\n lahf_lm\n lm\n mca\n mce\n mmx\n-movbe\n msr\n mtrr\n+nons\n nopl\n nx\n pae\n pat\n-pcid\n pclmulqdq\n pdpe1gb\n pge\n@@ -46,14 +39,16 @@\n rep_good\n sep\n smep\n+ss\n sse\n sse2\n sse4_1\n sse4_2\n ssse3\n syscall\n+top_tsc\n tsc\n-tsc_deadline_timer\n+tsc_adjust\n vme\n x2apic\n xsave\nI cannot tell you exactly all the invalid instructions which were generated but disassembling annoy .so and sampling the instructions returned FMA(3?) ones.\nI think vectors size was 150 (I did not generate the annoy dataset).. ",
    "pquentin": "I work in the same team as @pmezard. For what it's worth, the cloud CI service we currently use is called \"Shippable\" and is hosted on AWS. Our deployment environment is GCP with n1-standard-4 machines.. ",
    "agauniyal": "You can try using sanitizer tools (pass -fsanitize=address or memory, undefined etc) and maybe get some more information about the problem. They are available in both gcc and clang but clang has more.. ",
    "petrovlesha": "I'll try to compress it.\nInit and declaration of class to load index:\n```\nfile annoy_manager.py\nclass AnnoyManager(object):\n    def init(self):\n        self._model = None\ndef load_model(self):\n    self._model = AnnoyModel.load(get_annoy_path())\n\n@property\ndef model(self):\n    if not self._model:\n        self.load_model()\n    return self._model\n\nannoy_manager = AnnoyManager()\nannoy_manager.load_model()\n```\nAnnoyModel is a wrapper to handle {real_id: ordinal_number} mapping and metainfo saving/loading.\nThis module is imported in file which runs Flask app:\n```\nfrom similar_api.config.application import app\nfrom similar_api.utils import annoy_manager\nimport similar_api.handlers\nif name == \"main\":\n    app.run(\n        host=HTTP_HOST,\n        port=HTTP_PORT,\n        debug=DEBUG\n    )\nelse:\n    application = app   # for uwsgi\n```\nThen I have method handler:\n```\nimport logging\nfrom webargs import fields\nfrom webargs.flaskparser import use_args\nfrom similar_api.handlers.base import BaseHandler\nfrom similar_api.utils import stats, annoy_manager\nfrom utils import webargs_utils\nlogger = logging.getLogger('handler')\nclass SimilarByIdHandler(BaseHandler):\n@stats.timer('metrics.api.byid.v1.get.response')\n@use_args({\n        'item_ids': webargs_utils.NonEmptyList(webargs_utils.UInteger(), required=True),\n        'item_type': fields.String(required=True),\n        'result_type': fields.String(required=False),\n        'limit': webargsutil.UInteger(required=False, missing=20)\n    }, locations=('json',))\ndef get(self, params):\n    item_ids = params.get('item_ids')\n    annoy = annoy_manager.model # AnnoyIndex wrapper\n    limit = params.get('limit')\n\n    result = annoy.get_nns_by_id(item_ids[0], limit) # call corresponding method of AnnoyIndex\n    stats.incr('metrics.api.byid.v1.get.success')\n    return self.success(result)\n\n```\nMethod of wrapper:\n```\n    def get_nns_by_id(self, id_, n, search_k=-1, include_distances=False):\n        \"\"\"\n        Return N nearest neighbours by ID of item\n        Args:\n            id_: id of an item\n            n: number of nearest neighbours\n            search_k: depth of search, -1 search in all trees\n            include_distances: whether return distances\n    Returns: list of ids or tuple of list of ids and list of distances\n\n    \"\"\"\n    result = self.model.get_nns_by_item(self.ids[id_], n, search_k, include_distances)\n\n    if include_distances:\n        return [(self.ids.keys()[r]) for r in result[0]], result[1]\n    return [self.ids.keys()[r] for r in result]\n\n```\nWhole index wrapper class:\n```\nimport hashlib\nimport json\nimport os\nimport time\nfrom collections import OrderedDict, Iterable\nfrom annoy import AnnoyIndex\nimport settings\nfrom utils.misc import get_state\nclass AnnoyModel(object):\n    \"\"\"Wrapper-class for AnnoyIndex for convenient loading/saving and encapsulation\n    of {real_item_id: ordinal_number} mapping \"\"\"\nNUM_TREES = settings.ANNOY_NUM_TREES\nMETRIC = settings.ANNOY_METRIC\n\ndef __init__(self, num_trees=NUM_TREES, metric=METRIC, model=None, ids=None, dim=None, fitted_at=None):\n    self._model = model\n    self._ids = ids\n    self.dim = dim\n\n    self.num_trees = num_trees\n    self.metric = metric\n    self.fitted_at = fitted_at\n\n@property\ndef model(self):\n    \"\"\"AnnoyIndex\"\"\"\n    if self._model is None:\n        raise Exception(\"Model had not been built\")\n    return self._model\n\n@property\ndef ids(self):\n    \"\"\"OrderedDict like {real_id: ordinal_id}\"\"\"\n    if self._ids is None:\n        raise Exception(\"No id-ordinal mapping\")\n    return self._ids\n\ndef build(self, features):\n    \"\"\"\n    Builds AnnoyIndex and mappings\n    Args:\n        features: list of (id, feature_vector) tuples\n        num_trees: number of trees in AnnoyIndex\n        metric: type of metric to use in Annoy\n\n    Returns:\n\n    \"\"\"\n    if not isinstance(features, Iterable):\n        raise TypeError(\"Wrong type of features parameter, expected Iterable, got {}\".format(type(features)))\n\n    dim = len(features[0][1])\n    ids = OrderedDict()\n    model = AnnoyIndex(dim, self.metric)\n\n    for i, (id_, vector) in enumerate(features):\n        if isinstance(vector, (list, tuple)):\n            raise TypeError(\"Bad type of feature vector\")\n\n        if len(vector) != dim:\n            raise ValueError(\"Inconsistent sizes of feature vectors\")\n\n        model.add_item(len(ids), vector)\n        ids[id_] = i\n\n    was_built = model.build(self.num_trees)\n\n    if not was_built:\n        raise Exception(\"Cannot build AnnoyIndex\")\n\n    self._model = model\n    self._ids = ids\n    self.dim = dim\n    self.fitted_at = int(time.time())\n\ndef get_nns_by_id(self, id_, n, search_k=-1, include_distances=False):\n    \"\"\"\n    Return N nearest neighbours by ID of item\n    Args:\n        id_: id of an item\n        n: number of nearest neighbours\n        search_k: depth of search, -1 search in all trees\n        include_distances: whether return distances\n\n    Returns: list of ids or tuple of list of ids and list of distances\n\n    \"\"\"\n    result = self.model.get_nns_by_item(self.ids[id_], n, search_k, include_distances)\n\n    if include_distances:\n        return [(self.ids.keys()[r]) for r in result[0]], result[1]\n    return [self.ids.keys()[r] for r in result]\n\ndef get_nns_by_vector(self, vector, n, search_k=-1, include_distances=False):\n    \"\"\"\n    Return N nearest neighbours by feature vector\n    Args:\n        vector: list of float\n        n: number of neighbours\n        search_k: depth of search, -1 search in all trees\n        include_distances: whether return distances\n\n    Returns: list of ids or tuple of list of ids and list of distances\n\n    \"\"\"\n    result = self.model.get_nns_by_vector(vector, n, search_k, include_distances)\n\n    if include_distances:\n        return [(self.ids.keys()[r]) for r in result[0]], result[1]\n    return [self.ids.keys()[r] for r in result]\n\ndef get_item_vector(self, id_):\n    \"\"\"\n    Features of item\n    Args:\n        id_: int id of item\n\n    Returns: list of floats\n\n    \"\"\"\n    return self.model.get_item_vector(self.ids[id_])\n\ndef get_distance(self, i, j):\n    \"\"\"\n    Return the distance between items\n    :param i: id of first item\n    :param j: id of second item\n    \"\"\"\n    return self.model.get_distance(self.ids[i], self.ids[j])\n\ndef add_item(self, id_, vector):\n    \"\"\"\n    Adds item to index\n    Args:\n        id_: id of item\n        vector: feature vector\n\n    Returns:\n\n    \"\"\"\n    self.model.add_item(len(self.ids), vector)\n    self.ids[id_] = len(self.ids)\n\n@property\ndef state(self):\n    \"\"\" Current serializable state of the model\n    \"\"\"\n    return get_state(\n        self,\n        invalid_types=(AnnoyIndex,)\n    )\n\n@property\ndef name(self):\n    \"\"\" Model save path without ts\n    \"\"\"\n    state = self.state\n    return \"{cls}_{params}_{fitted_at}\".format(\n        cls=state[\"class\"],\n        params=hashlib.md5(json.dumps([\n            state[\"dim\"], state[\"num_trees\"], state[\"metric\"], state[\"_ids\"]\n        ], sort_keys=True)).hexdigest(),\n        fitted_at=state[\"fitted_at\"]\n    )\n\ndef save(self, base_path):\n    \"\"\"\n    Saves model with meta on disk\n    Args:\n        base_path: base path for models\n\n    Returns:\n\n    \"\"\"\n    path = os.path.join(base_path, self.name)\n    if not os.path.exists(path):\n        os.makedirs(path)\n    if not self.model.save(os.path.join(path, \"index\")):\n        raise Exception(\"Cannot save AnnoyIndex\")\n\n    with open(os.path.join(path, \"meta\"), \"w\") as f:\n        f.write(json.dumps(self.state))\n\n@classmethod\ndef load(cls, path):\n    \"\"\"\n    Loads model with meta from disk\n    Args:\n        path: absolute path of model\n\n    Returns: AnnoyModel\n\n    \"\"\"\n    with open(os.path.join(path, \"meta\")) as f:\n        state = json.loads(f.read())\n\n    ids = [(int(k), int(v)) for k, v in state[\"_ids\"].iteritems()]\n    ids = OrderedDict(sorted(ids, key=lambda x: x[1]))\n    dim = state[\"dim\"]\n    model = AnnoyIndex(dim)\n\n    if not model.load(os.path.join(path, \"index\")):\n        raise Exception(\"Cannot load AnnoyIndex\")\n\n    return cls(ids=ids, dim=dim, model=model, fitted_at=state[\"fitted_at\"], num_trees=state[\"num_trees\"],\n               metric=state[\"metric\"])\n\n``\n. I tried to debug it \u2013 it crushes in methodget_nns_by_idof AnnoyIndex.. @erikbern I tried it withget_nns_by_vector` with vectors that are in dataset, and it also crushes.. I run flask locally without uwsgi, so it doesn't fork process.. Unfortunately, it crushes immediately without any exceptions or errors. Sorry, it is not Flask problem.\nThe issue is the next: when I build AnnoyIndex with Euclidean metric and save it then it loads incorrectly. It works OK only with angular metric.\n\n. ",
    "ulges": "Hi there,\nI did some more testing on different platforms:\n- Debian 9 non-virtual -> leakage :-(\n- Ubuntu 16.04 virtual -> leakage :-(\n- Ubuntu 16.04 non-virtual (different machine) -> no leakage :-)\nAny thoughts are welcome ;-)\nCheers,\nAdrian. Did some more testing... it turns out to work better for older annoy versions, but there are cross-effects with platform and virtualization:\n|virtual?   |     Platform      |  Annoy 1.8.0   |      Annoy 1.9.3|\n|------------|---------------------|-----------------|---------------------|\n| yes    |        Debian 9   |      LEAK               |     LEAK |\n| yes  |       Ubuntu 16.04     |  OK          |             LEAK |\n| no    |         Debian 9         |   OK           |           LEAK |\n| no     |      Ubuntu 16.04   |    OK            |           LEAK |\nCheers,\nAdrian\n. My bad! There was a bug in my measurements earlier (I had placed annoy 1.9.3 in my home, where pip looked it up, i.e. one of my tests accidentally used annoy 1.9.3 instead of 1.8.0). Here's the correct data:\n|virtual?   |     Platform      |  Annoy 1.8.0   |      Annoy 1.9.3|\n|------------|---------------------|-----------------|---------------------|\n| yes    |        Debian 9   |     OK               |     LEAK |\n| yes  |       Ubuntu 16.04     |  OK          |             LEAK |\n| no    |         Debian 9         |   OK           |           LEAK |\n| no     |      Ubuntu 16.04   |    OK            |           LEAK |\nIt does look like a library issue to me now (same for all platforms). The leak seems to appear when switching from 1.9.2 to 1.9.3.\nI'll work with 1.9.0 for now. Let me know if you're interested in further input.\n. Correct. I'm adding my repro script (also see above), should be easy to run (just run test_annoy.py)\nrepro.zip\nCheers,\nAdrian\n. ACK. Cloned the master, ran the script. It works fine now, the leak is gone.. Great - thanks for the quick help!. ",
    "lexwraith": "LGTM. ",
    "jklapuch": "Ok, sorry. I tried to create a simple example. In my code I starts 4 processes as workers with shared queue, then the master process fills this queue with the vectors and the workers take them from the queue and put them in the annoy index. When creating a worker, I pass an annoy index instance as a parameter. I checked that each worker shows the index with the same memory address. But when I list the number of vectors in the index, each process only shows the number that it added itself, so the annoy index is not shared. I do not know how to create this shared object. Using shared memory from multiprocessing package works only for dict and list objects...\nWhen I load the built index individually in each process, then I get a lot of memory usage.. Perfect, thanks. A didn't know that about mmap.. ",
    "ksindi": ":shipit: on tests. ",
    "jethrokuan": "I think I figured it out. The exact search I was using was using a slightly different metric as compared to the one I used with annoy. Your comment somehow led me down this path of thought. Thanks!!. ",
    "Thommas2316": "Wow, I didn't realized that. \nThank you for your quick response, you saved me a lot of time.\nHave a wonderful rest of the day,\nTomas. ",
    "maumueller": "Conceptually that wouldn't be a problem. However, we need to use something like MinHash to do the split in a node.. It looks at a random position in the bit string, elements with a 0 go to left, elements with a 1 go to the right. . Well, it worked much better than a projection (say, take the inner product with a random {-1,1} vector and split by <0 and >= 0). It\u2019s also what the standard LSH hash family hamming distance would do. You could look at multiple positions at once, but then I didn\u2019t know what a good split criterion is.. >Is there a reason you chose to preserve the meaning of f as the dimensionality rather than the number of vector \"slots\" \u2013 I think in the latter case if you let f basically represent the \"chunk count\" then you don't have to introduce the extra chunksize and chunkcount helpers. I think that would be marginally simpler to deal with in the code.\nNo reason except from me being unable to predict whether f in its current meaning might be useful at another point. I don't like the chunksize and chunkcount helpers either, but wanted to change as few of the original code logic as possible.\n\none idea would be to create a wrapper object that implements AnnoyIndexInterface but contains a pointer to a AnnoyIndex and does the translation to/from binary vectors. probably the easiest approach.\n\nI have no experience how to write these Python wrappers, but I guess you want to have a switch metric = \"hamming\" to AnnoyIndex, and not a different type of index, say AnnoyIndexBitvector. The latter would allow for a cleaner separation, but the former is easier to use from a user perspective. So, hiding it inside the wrapper may be ok. (Although it feels a bit strange conceptually, because the float variant and the bit variant are on the same level.). > @maumueller let me know if this is good to be merged!\nRemoving the chunk part makes the code surprisingly clean. Looks good and thanks for your effort in including this PR!\n\nBtw looking at the splitting code again. You're picking a random index to split on right? Seems like maybe a slightly better solution would be to sample say sqrt(f) items and pick the \"best\" split (whichever is most even). I think that's how random forests are generally built.\n\nThat's a great idea. Looking at sqrt(f) coordinates should still allow for fast build times while leaving enough randomness in there. Taking a too large sample would yield too redundant  trees, I guess. I also wanted to look at more \"data dependent\" methods of splitting the data in the nodes. \n. In the long run, I also want to play around with jaccard similarity. Maybe we will find some nice abstractions from there. . Do you by any chance mean to guarantee recall 1? Approximate techniques will have a very hard time achieving this, and it is also the question if your application really needs \"exact nearest neighbors\" all the time.\nFinding parameters that achieve a certain recall (say, 0.9) is usually achieved by building the data structures for smaller samples of your data set, for which you can easily build many data structures and query them. From this you build a set of candidate parameters that you might need to refine for the larger dataset, but only small changes should be necessary.  You can also fix n_trees (basically fixing the space you want to use) and then increase search_k until you achieve the recall you wish for.\n. Yes, you are right! Fixing this.. ",
    "byeong0": "Thanks. I'll try again on the Linux.. I tried on Linux as the same data.\nis succeeded, but very slow.\n```\nGoogle cloud platform\nvCPU 16 skylake, 14.4GB RAM \ndata file read time is  912.2113721370697s\nannoy build run time is  3096.4241728782654s\ncreate annoy file total time is  4081.326126098633s\nand Google cloud platform vCPU 4, 14.4GB RAM is don't stop\nand Microsoft Azure vCpu 8, 64G RAM is\nMicrosoft Azure vCpu 8, 64G RAM\ndata fille read time is  1914.9890041351318s\nannoy build run time is  136693.05296897888s\ncreate annoy file total time is  141938.09434103966s\n```. ",
    "leafjungle": "@erikbern , I mean, how the trees are created inside, not the API.. ",
    "ghost": "Thank you @erikbern and @maumueller for this feature!\nLooking at the performance screenshots in #231 I see that there is a tradeoff between the number of queries per second and the recall, my question is : what n_trees  and search_k values should I have in order to ensure a maximum recall of 1?. I see, I did not mean exactly 1, but close to 1 as you had for the SIFT dataset, so I wondered how to achieve that the same way you had in the graphs. In my case the nearest neighbors are used for a supervised classification algorithm so I thought maximizing the recall would lead better accuracy. \nThanks a lot for your explanation!. ",
    "zygm0nt": "@erikbern how is this effort going? will it arrive in any near future? :). ",
    "hanabi1224": "@erikbern thanks for your response, is there plan to fix this in next release?. @erikbern  FYI, I tried to create a rust-lang(whose tool chain is more windows friendly) port of index serving part of annoy and it works fine on windows with those large indices, I plan to add some compatible c apis for FFI as well\nhttps://github.com/hanabi1224/RuAnnoy. ",
    "SeanU": "FWIW, Windows does limit 32-bit processes to 2GB.  64-bit processes should be allowed all they can eat. (Though I've only ever worked in .NET and Java on Windows, so there may be some extra hoops processes have to jump through that the VMs are smoothing over.) \nAnaconda and Miniconda have both 32-bit and 64-bit distributions on Windows, so I'd think the first thing to check, if it hasn't been already, is which of those is installed.. ",
    "calz1": "I think I figured it out:\n\nit assumes your items are numbered 0 \u2026 n-1. If you need other id's, you will have to keep track of a map yourself.. \n",
    "lidaweironaldo": "```python\nfrom annoy import AnnoyIndex\nf=64\ni = AnnoyIndex(f, 'hamming')\nimct = 0\nfor img in imgList:                #imgList is a list of 1000 images\n        fet = getFeature(img)  #getFeature returns 64-dim vector of binary values\n        i.add_item(imct, fet)\n        imct+=1\ni.build(10)\ni.save('img_index.ann')\nprint i.get_n_items()\n```. I made a mistake. When loading, I declared u = AnnoyIndex(f) which should be u = AnnoyIndex(f, 'hamming'). ",
    "mkoske": "And well, my usecase is that I try to implement algorithm which requires traversing through the dataset by going from a point to it's nearest neighbor and recording it's class label. I thought to experiment with a-nn instead of exact nn.. ",
    "yogesh-kamble": "while iterating in for loop to add_item if we skip some of the index and then build tree then tree will also skip those index and build tree using sequential index ?\nfor file_index, i in enumerate(infiles):  # length of infiles is 27\n    if file_index not in (1, 2):\n        print(str(file_index) + ' ' + i)\n        file_vector = np.loadtxt(i)\n        print(\"Adding index %s\" % file_index)\n        t.add_item(file_index, file_vector)\nt.build(trees)\nin above case I am iterating through list of 27 items and skipping the 1, 2 index. and now If I search for 26 index it gives\nItem index larger than the largest item index\nbut add_item add 26 index and it's just skipping 1,2 index.\n  . Yes we are loading index using euclidean distance.. ",
    "cggaurav": "Hey @yogesh-kamble is there a reproducible example from you? We have a similar problem that we are debugging.. @erikbern We use just euclidean and this seems to be the index being corrupted during saving and will be adding exception handling for our .save() calls for further info.. ",
    "ReneHollander": "Sure! I will try to vectorize some more functions (first with AVX) and later with SSE. Sadly I am not able to look into AVX512, because my CPU doesn't support it.. After some testing I found an issue in the way features with a dimension that is not a multiple of 8 are handled. After fixing those calculations, all tests pass now.\nI also optimized the dot product in the margin function and the angular distance. Here are the results: https://gist.github.com/ReneHollander/4def83f7755c6d63e1efb120774f2306\nThe speedups for index building are quite dramatic, but also the query time got reduced quite a bit.\nI did some profiling with valgrind and also found a hotspot with the inner loops in two_means. I don't think optimizing it will improve the speed as much as my other changes did, but it could help. Some goes with the vector norm.. I also forgot to add, that those optimizations are enabled if AVX support is found (building with -march=native, as it is currently done in setup.py, will pick it up). One can override the hand vectorization with defining NO_MANUAL_VECTORIZATION before including annoylib.h.\nI also don't know if it is of use adding support for only SSE CPUs. Every modern CPU since 2011 has support for AVX.. get_norm, normalize and two_means are now also optimized with AVX intrinsics. This only speeds up trees using angular distance (Benchmark, baseline is with hand optimized distance and margin), euclidian distance is completly unaffected, as expected.\nThere are also intrinsics for popcount, so it should be possible to optimize hamming distance by hand as well. Also manhattan distance is missing the optimizations for distance (I have to look into the absolute value calculation using AVX first).. I decided against vectorizing hamming distance calculation. Because the data is packed into 64bit integers, it would only make sense when you have features with over 256 dimensions.\nOther than that, everything is finished and it just needs to be reviewed \ud83d\ude04. You are right, the code is pretty repetitive. Maybe it is possible to move out the dot product calculation. I also fixed some potential windows incompatibility (and the unused code) in another pull request: #266. I checked the assembly and without declaring get_norm and normalize like you did with dot3 and dot, g++ won't use it. The float implementation of two_means forced the usage of the float variants and thus created the speedup. My optimized two_means is mostly unreadable and does not affect performance at all.\n```diff\nIndex: src/annoylib.h\nIDEA additional info:\nSubsystem: com.intellij.openapi.diff.impl.patch.CharsetEP\n<+>UTF-8\n===================================================================\n--- src/annoylib.h  (revision 35be9a1b8ff8c0f7ccfece330b1386691993c0ae)\n+++ src/annoylib.h  (date 1516371870000)\n@@ -237,7 +237,8 @@\n   }\n }\n-static inline float get_norm(float v, int f) {\n+template<>\n+inline float get_norm(float v, int f) {\n   float sq_norm = 0;\n   int i = f;\n   if (f > 7) {\n@@ -258,7 +259,8 @@\n   return sqrt(sq_norm);\n }\n-static inline void normalize(float v, int f) {\n+template<>\n+inline void normalize(float v, int f) {\n   float norm = get_norm(v, f);\n   __m256 v_norm = _mm256_set1_ps(norm);\n```\nI also ran my simple benchmarks, and I still see the same results I posted in my other PR. Index build time gets halfed, Query time reduced by 25% or something like that.. I did some more benchmarking (all synthetic data):\nThere is a noticable improvement on my i7 4770K in WSL on Windows 10. At no point is the AVX optimized code slower, and it performs the best if the number if dimensions is a multiple of 8 (as expected). I think I set the k for the search test a bit too low, but I did not want to rerun the test (I could not use my PC for 6 hours...). I hope you see similar improvements in your tests!\nEuclidian\n\n\n(Search time in us)\nAngular\n\n. I did not do any speed comparisons yet, but my little test program runs only a bit slower. I think the OS is doing a good job caching the pages. (I used an SSD to store the index). I did not get around to test it on native linux yet (my arch install broke some time ago).\nBut if you are still interested in including it, I will put some more time into it and make sure it is tested on all platforms :)\nAnd if you have an idea on how to handle memory allocation errors I would also make sure that every allocation made throws an error, or at least prints something.. I am still interested in finishing this PR and doing proper performance analysis. I still think it could be useful, especially with the recent developments off SSD technology (eg Intel Optane).\nI think I could work on it some more next week.\n. The only question is if you would want something like this to be included in annoy. If it is something you are not so sure about, I would rather not invest the time. I don't really need support for this usecase in my project at the time of writing this, but I would be happy working on it.. I also get your concerns regarding memory locality, but you should at least be able to build an index where each tree fits into memory in a reasonable amount of time. As only one tree is built at a time, the OS should keep the needed pages in memory, while keeping trees already built on disk. When building 20 trees, it could dramatically increase the amount of items the index can hold if you are bound by RAM.. The build seems to still be passing after merging with master. Maybe @erikbern can take a look at the API and if it should be done differently. After that I will do some more cleanup of the code related to memory management.. Thanks for merging! I will go over the memory management first (maybe I can get rid of some unnecessary mallocs).\nIf we change the API like that, I guess it would cause problems for people using the library. It is a pretty big change. Are there any other things with the API that you think need to be looked at? Then we could do it in a bigger release (maybe 2.0 if we consider semver?).. You are probably running out of memory during build. You can try pull request #274, which allows you to build the index on disk directly instead of in RAM. See the added test case on how to use it.. I tried to separate the distance type from the type of the vectors, but the resulting reduction of memory usage was not that impressive. It only reduced the amount of memory needed by like 10% (maybe 20, but I am not so sure what it was...) with data that was stored as uint8_t instead of float. I think casting the values was having a performance impact as well, because the distance is calculated so often, that a cast from int to float got noticeable. There are also handcrafted AVX and SSE optimizations that rely on the data being floats (which I think is most likely due to Python only relying on an index which uses floats) which further help with performance.\n. Can confirm the mentioned bug under WSL, was not able to check Windows yet. I tried the latest release under native Linux und can confirm, it does not happen there.\nI guess the issue has something todo with the ftruncate call to cut the unneeded memory after the build finished. This is known bug under WSL and was also mentioned in the PR #274. I am pretty sure I tested it under Windows and had it working correctly.\n@Slikus, did you try it with native Windows, or did you use the WSL?. The bug does not happen on Linux and it should be able to support well over 2GB large memory maps.\nMaybe check how much storage space you will need: Fully build an index with 1000000 items, then multiply the size by 100 to get to 100 million (I think you should be able to get a good approximation). Make sure to have maybe twice as much space on your harddisk (preferably SSD, I think it will take too long without), otherwise the build will fail at some point.. @erikbern I will fix those issues and improve the things mentioned in #274 when I get to the next stage of my own project that is using annoy :). It is currently not needed.. The distance function will use the optimized code, but I also did some optimizations in the inner for loops. Without them I was not able to measure any improvements when normalize and get_norm where also optimized.. ",
    "FlorianWilhelm": "Thanks for the fast reply. \n@a1k0n: Exactly, but if the subset is still quite large, exhaustively ranking them might be too expensive. Let's say I want to recommend a new song to the user from the rock genre since that's what the user selected. \nFirst using Annoy to retrieve candidates and filter them down afterwards could also result in having an empty result set and does not feel right in a way, but maybe that's the best workaround for now.. @a1k0n that really sounds like a good idea. An iterator that just provides me with as many neighbours as I want would solve my problem and seems not be out of scope for Annoy. . ",
    "mskimm": "I think it is not impossible. A simple idea is using Storage abstraction as like:\n```\ntemplate\n  class AnnoyIndex : public AnnoyIndexInterface {\n  ...\n  Storage::Storage _storage;\n  ... \nvoid add_item(S item, const T* w) {\n    _storage.add_item(item, w, _f);\n  }\n}\nstruct FooStorage {\n  template\n  class Storage  {\n  public:\n    void add_item(S item, const T *v, size_t f) {\n    ...\n    }\n    ....\n  }\n}\n```\nAnnoyIndex, as-is, has both storage and indexing operations together. My suggestion is splitting them using the abstraction. If the storage does not support one of CRUD, It can be handled using flags. For example, MMapStorage supports the only R, so the boolean variable R is only true (others false). . Please check my proposal.. updates/deletions on Memory/MMapStorage are hard to implement I think. So I will try it on top of RocksDB using this abstraction.. I\u2019m not doing this anymore. The purpose of CRUD is to prevent rebuilding the index due to slightly changed data. I was originally solving this problem to implement CRUD, but I just changed it to rebuild the index quickly.\nAnyway, please check out https://github.com/mskimm/ann4s.. ",
    "wergeld": "Seems odd that I would not have stdio.h from Visual Studio as I have VS 2015 (and 2013) installed and do other projects from within it. Will do more digging on that but a binary would be really helpful if/when you have time.. I checked - I have many stdio.h files. Some from R builds, some from mingw, some from conda, and, of course from Visual Studio. I have:\nC:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\cl.exe in my Path (it is used in build as seen in log).\nC:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\include - many header files including stdio.h.\nSo, not really sure how to resolve currently.. @tjrileywisc This was it! My VS 2015 install did not have the C++ development environment installed (standard for our shop as it is all .NET). Installed and changed my path to point to cl.exe and built without issue.. Closing this issue. Would still be good to have this package in conda repo, though.. ",
    "AsmaZbt": "did you find a solution for that please? i can't install annoy in windows????????? . pip install annoy\nCollecting annoy\nException:\nTraceback (most recent call last):\n  File \"C:\\Users\\twins\\Anaconda3\\lib\\site-packages\\pip\\basecommand.py\", line 215, in main\n    status = self.run(options, args)\n  File \"C:\\Users\\twins\\Anaconda3\\lib\\site-packages\\pip\\commands\\install.py\", line 335, in run\n    wb.build(autobuilding=True)\n  File \"C:\\Users\\twins\\Anaconda3\\lib\\site-packages\\pip\\wheel.py\", line 749, in build\n    self.requirement_set.prepare_files(self.finder)\n  File \"C:\\Users\\twins\\Anaconda3\\lib\\site-packages\\pip\\req\\req_set.py\", line 380, in prepare_files\n    ignore_dependencies=self.ignore_dependencies))\n  File \"C:\\Users\\twins\\Anaconda3\\lib\\site-packages\\pip\\req\\req_set.py\", line 554, in _prepare_file\n    require_hashes\n  File \"C:\\Users\\twins\\Anaconda3\\lib\\site-packages\\pip\\req\\req_install.py\", line 278, in populate_link\n    self.link = finder.find_requirement(self, upgrade)\n  File \"C:\\Users\\twins\\Anaconda3\\lib\\site-packages\\pip\\index.py\", line 465, in find_requirement\n    all_candidates = self.find_all_candidates(req.name)\n  File \"C:\\Users\\twins\\Anaconda3\\lib\\site-packages\\pip\\index.py\", line 423, in find_all_candidates\n    for page in self._get_pages(url_locations, project_name):\n  File \"C:\\Users\\twins\\Anaconda3\\lib\\site-packages\\pip\\index.py\", line 568, in _get_pages\n    page = self._get_page(location)\n  File \"C:\\Users\\twins\\Anaconda3\\lib\\site-packages\\pip\\index.py\", line 683, in _get_page\n    return HTMLPage.get_page(link, session=self.session)\n  File \"C:\\Users\\twins\\Anaconda3\\lib\\site-packages\\pip\\index.py\", line 811, in get_page\n    inst = cls(resp.content, resp.url, resp.headers)\n  File \"C:\\Users\\twins\\Anaconda3\\lib\\site-packages\\pip\\index.py\", line 731, in init\n    namespaceHTMLElements=True,\nTypeError: parse() got an unexpected keyword argument 'transport_encoding'\ni've used cygwin64. in the file index.py this is the function : \nclass HTMLPage(object):\n    \"\"\"Represents one page, along with its URL\"\"\"\ndef __init__(self, content, url, headers=None):\n    # Determine if we have any encoding information in our headers\n    encoding = None\n    if headers and \"Content-Type\" in headers:\n        content_type, params = cgi.parse_header(headers[\"Content-Type\"])\n\n        if \"charset\" in params:\n            encoding = params['charset']\n\n    self.content = content\n    self.parsed = html5lib.parse(\n        self.content,\n        transport_encoding=encoding,\n        namespaceHTMLElements=False,\n    )\n    self.url = url\n    self.headers = headers\n\ndef __str__(self):\n    return self.url. i changed transport_encoding to encoding it's ok ,\n\nbut this the problem that i get even with git , cmd , visualC++command prompt : \n```\nGenerating code\n    Finished generating code\n    LINK : fatal error LNK1158: cannot run 'rc.exe'\n    error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\link.exe' failed with exit status 1158\n----------------------------------------\n\nCommand \"C:\\Users\\twins\\Anaconda3\\python.exe -u -c \"import setuptools, tokenize;file='C:\\cygwin64\\tmp\\pip-build-7j698hpp\\annoy\\setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" install --record C:\\cygwin64\\tmp\\pip-tmxbv_uu-record\\install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in C:\\cygwin64\\tmp\\pip-build-7j698hpp\\annoy\\\n```\n. oh finally it's work so happy :)\nthank you so much for the library \nI've applied the process described in your site: https://ci.appveyor.com/project/erikbern/annoy \ni've a problem : LINK : fatal error LNK1158: cannot run 'rc.exe' error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\link.exe' failed with exit status 1158\nand the solution here : https://stackoverflow.com/questions/14372706/visual-studio-cant-build-due-to-rc-exe\nthank you \n . ",
    "JThinkable": "After installing Build Tools for Visual Studio 2017 from https://visualstudio.microsoft.com/downloads/#build-tools-for-visual-studio-2017 I got a similar error:\n\nannoylib.h(19): fatal error C1083: Cannot open include file: 'stdio.h': No such file or directory\n\nInitially I did not include the Windows 10 SDK.  After including just that component, the compile (from pip install annoy) went without a hitch.. @SirJAKfromSpace \n\n\nI downloaded vs_buildtools_*.exe from the link https://visualstudio.microsoft.com/downloads/#build-tools-for-visual-studio-2017.\n\n\n\nRun the executable, eventually you'll get to the \"Visual Studio Installer\" which should have a section \"Visual Studio Build Tools 2017\".  \"Install\" or \"Modify\" that.\n\n\n\n\nOn the right side of the dialog, one of the optional components is \"Windows 10 SDK (10.0.17134.0)\"  I doubt the minor version numbers matter much.  Make sure you install it.\n\n\n\n. ",
    "NikilRagav": "@erikbern I'm looking at the appveyor page trying to build this. It seems like you have a script but I can't figure out where to download the script.\nI'm using Anaconda on Windows 10.  It looks like my system environment vars don't currently include \"PYTHON=C:\\Python36-x64, PYTHON_VERSION=3.6.5, PYTHON_ARCH=64\"\nAm I supposed to add that?\nI had read somewhere in Anaconda docs that you're not supposed to use pip and conda together - so I am confused about how to build this and link it to Anaconda. ",
    "SirJAKfromSpace": "@JThinkable \nHi I've also been having trouble with the exact same error. Can you please a a bit more specific as to which components you installed? A screenshot perhaps would be very helpful. Thanks. @JThinkable thank you for the info, but its STILL telling me the same thing. Driving me crazy.\nI checked the environment variables too if thats what youre thinking. Anywho it built ok on my laptop rather than my desktop so thanks anyway.. ",
    "AjayP13": "@ReneHollander this looks exactly like what I was looking for. Any updates on finishing this PR up? I would like to use this feature. Would be happy to help if needed, I think I understand most of these changes. . I think it would allow creating a large index on a machine bounded by low\nRAM (development machine) for use on a larger machine with sufficient RAM\n(prod machine). Currently, this is not possible. That's my use case at\nleast.\nOn Mon, Nov 19, 2018, 12:18 PM Rene Hollander notifications@github.com\nwrote:\n\nI also get your concerns regarding memory locality, but you should at\nleast be able to build an index where each tree fits into memory in a\nreasonable amount of time. As only one tree is built at a time, the OS\nshould keep the needed pages in memory, while keeping trees already built\non disk. When building 20 trees, it could dramatically increase the amount\nof items the index can hold if you are bound by RAM.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/spotify/annoy/pull/274#issuecomment-440028188, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AFJ2EQ337dkxBnHM3LdNaVrNkoS9XIiVks5uwxIfgaJpZM4R2bgh\n.\n. Yeah, I have done it on the Magnitude SQLite file already (SQLite has a Virtual File System implementation built in) and it worked surprisingly well if you add in a small predictor for sequential reads and a local cache for repeated reads in the same region. It basically let you query large SQLite queries over static HTTP using the range headers. It really depends on its read patterns and how much data is read from the file\n\nI'll give it a shot on annoy and submit a PR in that case, and you can decide whether its worth adding.. ",
    "szkocot": "This PR helped me a lot in my recent project, where I would need 100-200GB or more of RAM to index whole data with annoy. With SSD I was able to index whole data with very little performance hit. SSDs are getting cheap (130-150 USD/TB) and it would be a great feature to have in annoy officially.\nThe only drawback is It didn't work (with very much of rows) under Windows 10. I was getting a length of indexed annoy model == -1 (like others reported), but this isn't something very important as Linux is better for such applications anyway.. ",
    "ArielSSchwartz": "This is a very useful feature. +1 for merging into the official release.. Done.. ",
    "zhuang-li": "@ReneHollander Thank you. I tried your code but still failed. When loading, the memory didn't change at all. I guess there must be something wrong with the load. But I haven't figured out why.. @ReneHollander @erikbern Pretty weird. It involves with the decimal precision. If I use build_on_disk, even 5 data points of only 1 dimension can cause the size of saved index to become -1. The build in memory will be fine with only 10 data points but the result would be still wrong with the data points increasing. I am using 'float' and running the program on windows. I don't know whether the same question will happen on linux.. ",
    "Trollgeir": "I'm having the same problem for large datasets. \nIt doesn't seem to be a memory error during build, as get_n_items() returns the correct n of items right after it's been built (while it's still in memory), and all the correct functions work. However if i dump the memory with save, I notice that a silent crash occurs in python. The mmap file seems to be the correct size though, but when loaded the index is -1. . I experimented a bit, and it seems like I found a threshold of a certain number of items for my dim that caused the problem.\nI'm running python 3.6 on Windows 10 64bit with 16gb ram. I've also tried saving on two different disks.\nIn the failed attempt, the last thing I see from verbose mode before python exits is 'unloaded' after it's attempting to save.\n```\nfrom annoy import AnnoyIndex\nimport numpy as np\nnum_items = 1_030_000 # works (  2 087 769 KB )\nnum_items = 1_040_000 # fails (  2 108 127 KB )\nann = AnnoyIndex(512, metric='euclidean')\nann.verbose(True)\nprint(\"adding items..\")\nmu, sigma = 0, 0.3\nfor i in range(num_items):\n    ann.add_item(i, np.random.normal(mu, sigma, 512))\nprint(\"building index..\")\nann.build(1)\nprint(\"items:\", ann.get_n_items())\nprint(\"attemting to save..\")\nann.save('tree')\nprint(\"attempting to load index\")\nann.load('tree')\nprint(\"items:\", ann.get_n_items())\n```. Confirmed that this is a windows related issue, as I could easily run the code on linux. So in other words, annoy for large indexes on windows is unsupported due to OS constraints?\n. ",
    "Eldar7": "ouch, it worked fine with https://github.com/spotify/annoy/commit/f29296cfd30c80abf98a21781f128d3e4cd5d7a2 commit.. It is regular hamming distance https://en.m.wikipedia.org/wiki/Hamming_distance that can be applied to any vectors, not only to binary vectors.\nDue to wiki:\nThe Hamming distance between:\n\"karolin\" and \"kathrin\" is 3.. @erikbern , ok, I'll try to make this one to pass the tests.. ",
    "lbertolotti": "```\n-- coding: utf-8 --\n\"\"\"\nSpyder Editor\n\"\"\"\nimport annoy\nimport inspect\nu = annoy.AnnoyIndex(5,metric='hamming')\nu.add_item(0,'12346')\nu.add_item(1,'12345')\nu.build(1)\nu.save('tree')\nu.get_n_items()\nIn:u.get_distance(0,1)\nOut: 0.0\n```. ",
    "puregame": "Is it safe to say that searching affected indexes by vector will be unaffected?\nBased on some small tests it seems as though searching these indexes by a vector returns the correct id's.. ",
    "peterldowns": "@erikbern just wanted to thank you for the clear writeup and responses to this issue -- just ran into it myself. I can confirm that when querying by vector, the returned results still have the correct item numbers, it's just that I can't use them in a search.\npython\nindex.get_n_items() # 1200\ndoc.id # 1798\nindex.get_item_vector(doc.id) # IndexError: Item index larger than the largest item index\nindex.get_nns_by_vector(doc.vector, 1, search_k=1000) # [1798]\nIn my system, not every document has a vector, and I'm serving queries using workers that call index.load() to refer to a pre-computed index. So for now I'm working around the problem by only allowing querying by vector rather than document ID, which is obviously slightly more of a pain but still workable. I would greatly appreciate if this would be fixed. If I can figure out the code and fix it myself I will send a PR but I'm no C++ expert, please don't count on it.. Not a bad idea, but querying by vector isn't that painful -- everywhere I want to find related documents I have both the document ID and the document vector available. And then I'd have to filter out any 0 vectors after the fact, which would mean that the workers would have to know which vectors were bad, which means another synchronization problem... going to stick with this for now :). ^ Thank you!. ",
    "gregsadetsky": "Thank you for your answer! So then ok to close this?. woah, that's absolutely awesome! just tested on the latest master and it does not, indeed, cause either segfault nor the incorrect index value return. apologies for not testing it on the latest code..!\nthank you x 1000000!\nok to close this I guess?. err, even the latest version on pypi does not have these issues. I should have definitely upgraded before filing this...!\nthanks again. definitely closing. :-)\n\nfor the record, I was running annoy==1.11.4, which I just re-tested and does segfault/return 0. . ",
    "zpmmehrdad": "So, what do you do for Spotify?\n. I have 50M items.means, I have to build from scratch every time?\nHow long does it take?. ",
    "chinmayrane": "@erikbern That sounds great! Thanks a ton! Look forward to the wheel files.. It might be an issue with Python3.4\nI upgraded to 3.5/3.6 and annoy could be built/installed.\nThanks for the Library @erikbern !. ",
    "Aashit-Sharma": "@erikbern Thanks! \nbtw your ann_benchmarks are super helpful. Thanks for that too :+1: . ",
    "chaitjo": "Thank you for the insight! If I understand correctly:\n- search_k corresponds to some sort of split/thresholding on the k branches of the n_trees trees that were constructed, and can take values between 1 to n * n_trees, where n is the number of nearest neighbors to fetch\n- search_k = -1 corresponds to search_k = n * n_trees (i.e. maximizing accuracy)\nAm I correct?. ",
    "eric-kimbrel": "same for windows 10. ",
    "simon19891101": "Anyone knows how to solve it? Cheers.. ",
    "rbares": "After testing with a popcnt64 implementation from Sean Anderson's \"Bit Twiddling Hacks\", handling some further missing identifiers and adding a couple of static casts I finally got this building. Unfortunately my build currently crashes during nosetests but will let you know if I get any further.. Yes indeed that is the approach in my current implementation, although I have also forced its use in linux for comparison.\nI was implementing __popcount64 with the generalized approach suggested by Eric Cole, i.e:\nc\nv = v - ((v >> 1) & (T)~(T)0/3);\nv = (v & (T)~(T)0/15*3) + ((v >> 2) & (T)~(T)0/15*3);\nv = (v + (v >> 4)) & (T)~(T)0/255*15;\nc = (T)(v * ((T)~(T)0/255)) >> (sizeof(T) - 1) * CHAR_BIT;\nbut moved to:\ncpp\nstd::bitset<numeric_limits<T>::digits>(v).count()\nTwo problems:\n- Crash in angular_index_test test_no_items. Temporarily disabled for now.\n- Test failures in hamming_index_test for any case of assertAlmostEqual(numpy.dot(...)). These do not appear to be related to popcount, as when using std::bitset on linux these tests pass fine.\nBoth will benefit from reproducible test cases and stepping through the intermediate calculations. Happy to push up a WIP if that will help. Unlikely to get another chance to look at it until the weekend. My pleasure, thanks for the review. > something looks weird with the diff:\n\n\nany idea what's going on?\nanyway thanks for this change!!\n\nOh dear. I allowed github to commit the comment you proposed using the web interface ( https://github.com/spotify/annoy/pull/319/commits/7650312212c5c193dc485dc346fee12a48c9f829 ). It looks like that has changed line endings throughout the file. My apologies. Will sort that out while debugging the remaining tests.. Ready for review. LGTM, performance is as expected. Will be trying the VS mixed-mode debugger this weekend. From additional logging the crash occurs in https://github.com/spotify/annoy/blob/a7b351222cc0ab25ac1ecd6282c1d26836d3a003/src/annoylib.h#L932 when indices.size() == 0. I can't imagine why as that sounds equivalent to a nop, but conditionally executing the memcpy fixes the test.. Looks good. I'm unclear on whether this will use intrinsics or not so this make have a performance impact.. Yes that's what I used. The &index[0] should be fine as we only ever get the address, so I suspect a bug in this old memcpy implementation may be causing an unnecessary attempt to read. ",
    "nevergiveuphe": "gdb detail\n\n. ",
    "goodloop": "in my test , I added about 15 items into index.   build(n_trees)  n_trees set to 10.\nthe first version only return 2 results, the second version can return more results with proper order.\ndoes this give any hints?. I am using pypi version. I'll try git repo later.. that's fair, 10 times retry then randomize as last option is pretty fine for me .. @erikbern will you find the code or should I submit the code according to your description?. @erikbern any thoughts about this?. sounds great! Pls remind me if any result you get.. @erikbern any problem about this pr?. ",
    "threehundred": "I'm getting the exact same problem as @goodloop.\nOnly two results returned when using database keys (i.e. large ints no starting at zero) . Everything is fine when using your example code though. . ",
    "bermeitinger-b": "It is Fedora 28 in 64bit.\nI tried using clang instead of gcc but either I failed in doing so, or it's the same problem.. At least for me, it is working again. Unfortunately, I do not know what changed. I'm not using Fedora anymore.. ",
    "jhljx": "It couldn't be installed correctly on Windows 10 operating system with Python 3.5.\nThe error msg says it couldn't open 'unistd.h' on Windows, no such file or directory, for this file is a linux .h file.\n. ",
    "ViperCraft": "no, generally this is just realloc overhead remove for get_NN, const for get() methods and some C++11 style for little more performance. hmm, sorry, from another experiment, I'll remove this. not compiled with clang on MacOS:\n```c++\ncompiling precision example...\nprecision_test.cpp:22:29: error: use of undeclared identifier 'generator'; did you mean 'operator'?\n        std::default_random_engine generator;\n                                   ^~~~~~~~~\n                                   operator\nprecision_test.cpp:22:7: error: no type named 'default_random_engine' in namespace 'std'\n        std::default_random_engine generator;\n        ~~~~~^\nprecision_test.cpp:22:38: error: expected a type\n        std::default_random_engine generator;\n                                            ^\nprecision_test.cpp:23:7: error: no member named 'normal_distribution' in namespace 'std'\n        std::normal_distribution distribution(0.0, 1.0);\n        ~~~~~^\nprecision_test.cpp:23:33: error: expected '(' for function-style cast or type construction\n        std::normal_distribution distribution(0.0, 1.0);\n                                 ~~~~~~^\nprecision_test.cpp:23:35: error: use of undeclared identifier 'distribution'\n        std::normal_distribution distribution(0.0, 1.0);\n                                         ^\nprecision_test.cpp:38:27: error: use of undeclared identifier 'generator'\n                        vec[z] = (distribution(generator));\n. fixed. ",
    "cai-lw": "@erikbern I just want to know where the random division tree idea comes from. If you learned about it from a course, a book, or even a blog post, I would like to cite them in my work. If you come up with the idea yourself, then it's fine.. ",
    "brianhie": "@cai-lw here's a STOC paper describing random projection trees: http://cseweb.ucsd.edu/~dasgupta/papers/rptree-stoc.pdf  This could be what you're looking for?. ",
    "funnydevnull": "n_trees=100.  I see that switching to 10 reduced the file size to ~120 mb.  Would you mind explaining how this works and what a reasonable number of trees would be?. You mention disk space but does this also translate into memory usage (disk space is obviously much cheaper than memory).  IIRC annoy mmaps some data structure but is that the entire index file or only part of it?  Thanks for the prompt answers and help!\n. ",
    "psobot": "Thanks @erikbern! Good point - I didn't change the way that we do splits, and just assumed that the existing logic would be good enough. The tests for precision still pass, but with real-world data we might end up with pretty poor accuracy. I'll try to get some realistic data to test on and add an understandable test for splits.. So, if not made obvious from the radio silence on my end - this has been notoriously difficult to optimize Annoy for. As you expected, the splits are really bad and the overall efficiency is very low, to the point where callers need to crank up search_k to get any reasonable results.\nI'm not sure where to go from here, unfortunately. I've tried overriding DotProduct::create_split to skew the splits toward nodes with higher magnitude, but this doesn't seem to have significantly affected the lookup performance on larger random datasets. Is there a way you know of to change the tree construction to ensure it's more likely that we scan through higher-magnitude nodes first during lookup?. Thanks for that link @erikbern! After reading through that Xbox paper and trying a bunch of tests on random and real-world data, I've updated this PR with a solution that's a bit more involved, but is now much more efficient than just straight dot product.\nThe new method adds:\n - an optional number of internal_dimensions added to the index by Annoy when using the dot distance measure (it adds exactly one internal dimension, but this is configurable in case other metrics need to use this in the future)\n - a preprocess step, called from build, that populates this extra internal dimension before any queries are made\n - logic to all of the get_nns_* methods to pad out input vectors with zeros in the internal dimensions, as per the paper\nMy tests show that on random gaussian-distributed data, this method seems to actually outperform regular cosine distance for accuracy when fetching 10 neighbours (using kendall-tau rank correlation as a metric):\n```\nAverage kendall-tau accuracy:\nat search_k=10:     cosine      7.01%\nat search_k=10:     Xbox trick  12.28%\nat search_k=10:     Annoy Dot   12.28%\nat search_k=50:     cosine      9.28%\nat search_k=50:     Xbox trick  18.72%\nat search_k=50:     Annoy Dot   18.72%\nat search_k=100:    cosine      13.63%\nat search_k=100:    Xbox trick  24.68%\nat search_k=100:    Annoy Dot   24.68%\nat search_k=200:    cosine      23.74%\nat search_k=200:    Xbox trick  35.03%\nat search_k=200:    Annoy Dot   35.03%\nat search_k=500:    cosine      54.44%\nat search_k=500:    Xbox trick  60.78%\nat search_k=500:    Annoy Dot   60.78%\nat search_k=1000:   cosine      85.14%\nat search_k=1000:   Xbox trick  81.28%\nat search_k=1000:   Annoy Dot   81.28%\n```. > I don't think you need the internal_dimensions thing actually. If you look at the different distance metrics, note that they redefine the Node struct. For instance the Euclidean distance metric uses that to store an extra element that denotes the offset of the plane for each split. So you could just add an extra element to the Node definition for DotProduct and use that.\nThat's one option, although it saves a lot of custom code to just tack on an extra dimension and defer to Angular for create_split and other functions. (Adding an extra element to the Node definition would require us to override those functions to consider the extra element whenever we do math on an entire vector.) If there's a way to do that without adding too much extra complexity though, I'd be interested.\n\nSeparately I suspect the preprocessing can be avoided and that you can move that logic into the create_split but that's more speculative and I have to think a bit more about it.\n\nI thought about doing so, but the preprocessing requires a first pass over all of the nodes to compute a global max_norm for the universe. create_split gets called on successively smaller lists of nodes, meaning we'd have to pass along this global max_norm every time we call create_split (and two_means) and if we ever see the same node twice in create_split, we'd end up re-computing the additional dimension multiple times.. Thanks for the review, @erikbern! And my apologies - I pushed this code at the end of the day on Friday without being completely sure it was ready for review. I'll address each comment individually.. Hey @erikbern - friendly ping on this. We've got some internal customers interested in using this new feature, so I'm planning to merge this by Friday unless you have any final objections. (Also happy to hand-deliver some cookies to your office if that'd incentivize you to push this PR over the line. \ud83d\ude42) . > I'm curious what would happen if you update the margin method as per my suggestion \u2013 would be interesting to see if it has an impact on recall!\nInterestingly enough, recall went down by about 2% (at 10 items) in my tests after including dot_factor in the margin calculation. I'm not sure if that means we should leave it out entirely, but if we're trying to match the original Xbox paper, we should include dot_factor for correctness.. @erikbern Merge away, I'm ready if you are. \ud83d\udc4d Thanks for all the review!. Fixed!\nThe reason for changing this to calloc was to zero-out the entire Node struct first, but you're right - it's better to be consistent and explicit. I've added a zero_value method to Base to allow us to set sane defaults for any metrics that require them - in this case, after malloc'ing a new node, we need to set the dot_factor of the node to zero.. \ud83d\udc4d Removed. This was to make the test_precision tests easier, as dot is a bit less precise than other metrics.. Ah - this was a holdover from the previous iteration where we used extra_dimensions. This is no longer needed. \ud83d\udc4d . Switched this to use recall like the other tests.. I'm not entirely sure, tbh. Both perform fairly well, but if we return dot as the actual distance measure, then callers will get back distances that are actually dot products, which might be what callers expect.\nI tried switching this to plain cosine distance, and results are pretty similar regardless of which distance metric we use:\n``\n\"dot\" with cosine asdistance` (incl extra element): \nRecall at 10 is 70.21%\nRecall at 100 is 96.50%\nRecall at 1000 is 100.00%\n\"dot\" with dot as distance: \nRecall at 10 is 69.55%\nRecall at 100 is 96.46%\nRecall at 1000 is 100.00%\n```\n(The reason changing this one method doesn't break everything is that DotProduct::margin still takes the dot_factor into account.). Good point - if we leave this as returning the dot product distance directly, we can flip the sign to return the positive dot product.. Whoops, this was another holdover from using extra_dimensions. I've changed this to a straight memcpy.. Changed this to try and eliminate a bunch of clang warnings, but I can undo this.\nsrc/annoylib.h:846:12: warning: implicit conversion loses integer precision: 'off_t' (aka 'long long') to 'size_t' (aka 'unsigned long') [-Wshorten-64-to-32]. It is, but I'm not sure we can omit it, as Angular::Node and DotProduct::Node are incompatible types so calling the superclass method directly won't work.. Good catch - yes, it should! This is fixed.. ",
    "madan-ram": "What I mean is given n_trees =50 and n=10, this code is going to stop after nn.size()>n_trees *n=500. Hence usually it is not going to search for the approx nearest neighbor from all the trees.. ",
    "kkelk": "Cool, thanks for the quick response. ",
    "orf": "https://github.com/travis-ci/travis-ci/issues/9815\nsigh. ",
    "kchro": "bummer. thank you for the quick response!. ",
    "CookingCookie": "Ok I solved this problem. Just had to use int (any int works). I tried that before but I was passing the vector data to the index the wrong way. Now I am able to build an index.\nBut what I don't quite understand is the distance result when calling get_nns_by_vector. \nWhy does it have to be of the same type as my vector data? \nShouldn't it just be an int value of how many bits are different?. I tested with uint16_t for a small dataset (100k points) and the results were fine. Just as good as with int64_t. However when I test with larger datasets (int64_t index of 2 million points) the results are pretty bad when compared to FLANN. \n\nFYI hamming distance is somewhat experimental and the algorithm probably isn't as good as it could be \u2013 for instance we're just doing axis-aligned splits so far. I want to do arbitrary splits at some point\n\nI don't know much about ANN so this does not say anything to me^^\nIs that the reason my results are bad?. When given a fixed number of trees, a fixed search_k and the index size, is there a way to roughly calculate the accuracy?\nIn my case for example: Even with an index size of 1M the accuracy was still good. But I would like to be able to make an assumption at what size it will drop significantly.. ",
    "LTLA": "Ah, sorry - One Definition Rule. The header guard only protects against multiple includes in a single translation unit (i.e., each *.o, in my case) but it doesn't protect against multiple definitions upon linking different units into a single library, which is the problem here. I'm no expert on this, but inlineing the relevant functions seems to help, presumably by forcing the functions to be be defined locally in each unit that includes the header file.. The individual pp, qq and pq values look fine when I printf them out:\n```\nprintf(\"%.10f %.10f %.10f\\n\", pp, qq, pq);\n0.5077888370 0.5073949099 0.5075918436\n```\nBut if you were to do the same with pp + qq and 2 * pq, you'd see identical values. I assume that precision is just getting lost when pp + qq forms a large-ish intermediate. One could mitigate this with:\ncpp\nreturn (pp - pq) + (qq - pq);\n... which returns a distance of 0.0002441406... still not great, but at least it's not zero.\nP.S. Perhaps this loss of precision is not surprising. floats give us about 7 significant figures, and the squared distance here is around 1e-7, so we're definitely in the risk zone for catastrophic cancellation.. If I may: is there a compelling reason to keep the current algorithm for computing the Euclidean distance? From what I can see, Euclidean::distance still needs at least one dot() call, and that dot() call still needs to loop over the dimensions to compute the dot product - there doesn't seem to be any magic that would make it happen super-fast. So, we will have one multiplication and one addition per iteration of the loop, not considering cases where we need multiple dot() calls. By comparison, the more numerically stable calculation in my original post would just have an extra subtraction (plus a stack variable) per iteration. From my perspective, this seems like the time cost - if any - is well worth the extra accuracy, especially if it avoids getting a distance that's very wrong (depending on the whims of the compiler).\nAnd yes, I agree that using double would be one solution for increasing precision in this situation, but this has its own issues with respect to speed/memory usage. I felt that the previous distance calculation struck a more practical balance between efficiency and accuracy. At least, any previous loss of accuracy was firmly in the \"negligible\" basket, whereas now it's apparent with even a simple mock data set.  . Ah, I see the dot<float> specialization now... would you be willing to consider a Euclidean::distance<, float> specialization, written with AVX and using the more-precise algorithm? Similar to manhattan_distance<float>? I'm happy to try to write this.. I guess I saw this too late, I went ahead and did it before lunch. Well, it's in #316 if it's ever needed.. Hmm. I don't want to whale on this, but from my perspective, #315 doesn't seem to be sufficient. \nTo demonstrate, here's a little elaboration of the program used in #314:\n```cpp\ninclude \ninclude \"annoylib.h\"\ninclude \"kissrandom.h\"\ntypedef float ANNOYTYPE;\ntypedef AnnoyIndex MyAnnoyIndex;\nint main() {\n    MyAnnoyIndex obj(1);\nANNOYTYPE first=1.0001, second=1.0002, third=1.00015;\nobj.add_item(0, &first);\nobj.add_item(1, &second);\nobj.add_item(2, &third);\n\nobj.build(10);\n\nstd::vector<int> stuff;\nstd::vector<ANNOYTYPE> outdist;\nobj.get_nns_by_item(2, 2, -1, &stuff, &outdist);\n\nprintf(\"%i %.10f\\n\", stuff[0], outdist[0]);\nprintf(\"%i %.10f\\n\", stuff[1], outdist[1]);\nreturn 0;\n\n}\n```\nOn my computer, this gives me:\n0 0.0000000000\n1 0.0000000000\n.... so not only is the distance wrong, item 3 is not even in its own nearest neighbor set! This seems particularly egregious when the differences between input items are well within float precision. I could also imagine a lot of breaking code that assumed the first element of stuff would be the item itself.\nTo be clear; the precision of the output distances themselves is not the problem, as I can just recompute them. The question is how much this loss of precision reduces the accuracy of the NN search, in a manner that is not easily adjustable by increasing ntrees or search_k. Certainly, I could switch to double in the example above (rather unhappily, because of the cost of time and memory), but Python (and R) users won't be able to do so at all, and I guess that's probably a substantial proportion of Annoy's user base.\nFinally - unless I'm reading my code incorrectly - the proposed modification here should only add one subtraction operation plus an extra intermediate variable. Some testing suggests that it's about 50% slower. I'll admit that's not great, but if speed is a priority and accuracy is not, then a more natural way to make that sacrifice would be to reduce search_k directly (or indirectly, via a lower ntrees).\n. Cool. I should mention that, in the context of the rest of the Annoy operations, the extra time required for these distance calculations seems negligible. I've tried with a 50000-by-20 random normal matrix, looking for the 20 nearest neighbors with 50 trees, and the performance hit from the switch is 1%.. Nudging this PR. Just to clarify; the 50% slowdown refers to the head-to-head comparison between dot() and euclidean_distance(). This effect is considerably diluted when considering the Annoy algorithm in its entirety. In my applications, I see a 1-5% slowdown for the search, with the more severe performance drops occurring at higher search_k (as one might expect, given the context in which distance() is called). Nonetheless, this seems like a small price to pay for obtaining a correct final ranking among the collected search_k set when the differences between observations are somewhat small. Certainly, differences at the 5th significant figure are not uncommon (at least in scientific data sets),  and a user of the Annoy API would expect that to be reasonably well-handled by floats.. Is char node_buffer[s*2]; legal C++? It doesn't seem that s is a compile-time constant, and variable length arrays don't seem to be standard (though they are supported by GCC), see here and here.. ",
    "Jarlonyan": "thank you!. ",
    "zxyza": "thanks a lot,my txt content just like : dog  0.233345 0.434343 0.123854 0.987456 ...\ni want to load it as model,what do you mean \u201cmap them to integers\u201d\uff0cForgive me for being a newbie\n. oh\uff0cdo you mean i should  give an integer(just like 1) to represent the \"dog\"?. thank you very much\uff01. ",
    "perone": "Thanks @erikbern !. ",
    "yaxu75": "I'm facing the same problem.\nThe good news is I have a temporary solution, simply downgrade your xcode from 10 to 9 will allow you pip install successfully.\nBut just as you point out we need some update to solve the compatibility problem with xcode 10.. ",
    "aysark": "Same issue.... Thanks @erikbern, i just used it in a docker container... but that was only temporary.\nAny ETA on next pip release?. ",
    "mxgr7": "Thank you!\n\nI'm not sure what happens if you mmap a file and then the file changes.\n\nWe had not considered that, but you raise a valid point! If this actually causes problems it could explain the observed failures. Will investigate further.\n\nHow do you reload the index?\n\nWe initialize a new AnnoyIndex object and call .load() to read the file from disk. . We actually write the new index to a temporary file and upon completion move it to replace the old one. Not sure if that is problematic.. We were able to trigger the error:\n1. Load an index file\n2. Write a new index containing fewer items to the same file path\n3. Call get_nns_by_vector() on the original loaded index\n4. Get a Bus Error\n```\nfrom annoy import AnnoyIndex\nimport random\nf = 40\nBuild the initial index\nt = AnnoyIndex(f)\nfor i in range(1000):\n    v = [random.gauss(0, 1) for z in range(f)]\n    t.add_item(i, v)\nt.build(10)\nt.save('test.ann')\nLoad index file\nt2 = AnnoyIndex(f)\nt2.load('test.ann')\nOverwrite index file\nt3 = AnnoyIndex(f)\nfor i in range(500):\n    v = [random.gauss(0, 1) for z in range(f)]\n    t3.add_item(i, v)\nt3.build(10)\nt3.save('test.ann')\nGet nearest neighbors\nv = [random.gauss(0, 1) for z in range(f)]\nnns = t2.get_nns_by_vector(v, 1000)\n```\nHowever, we were not able to reproduce this behavior when writing the new index to a different file path, then using os.rename() to replace the old one. It worked fine even for large index files. This is what we do in production, so we are still not 100% sure what is causing the error there. It does seem likely that it is related to the error above though. We decided to change our logic a bit so that we never touch nor replace existing index files and hope that this will solve the issue.\nBy the way, is there a way to catch this error in Python so that the process doesn't completely crash? We would have had a much easier time locating the error.\n. Yes, Linux. Tried on different distros and inside a docker container.. ",
    "changzeng": "Build a nearest neighbor library on top of Redis will allow the package to handle a wider scale of data.. ",
    "thebarbershop": "I couldn't reproduce the situation on another mac (becauase I have only one), but #334 seems like a similar issue. It was resolved by removing the include in problem, but is that going to work here?. After some hours of research and experiments, I finally got it. (thanks to @a1k0n 's hint that my pip wouldn't be using gcc proper, but apple's clang.)\nI think it is an issue related to the deprecation of libstdc++ by Apple (see Deprecation Notes), and will affect all future mac users.\nenv CXX=/usr/local/Cellar/gcc/8.2.0/bin/g++-8 CC=/usr/local/Cellar/gcc/8.2.0/bin/gcc-8 pip install annoy\ndoes the job to force pip to use the brew-installed compiler for command line instructions g++ and gcc.\nMaybe you don't have to re-write or pre-compile things but just let mac users (after XCode 10 and their clang deprecation of libstdc++) know from README.md that they should do this to install annoy.\n```\n\nbrew install gcc\nbrew info gcc\n(and get the path to the brew-installed gcc bin)\nenv CXX=/your/brew-installed/gcc/version-number/bin/g++-8\\ \nCC=/your/brew-installed/gcc/version-number/bin/gcc-8\\\n pip install annoy\n```\n\nNow I finally got it, thanks guys, I'll enjoy annoy.. Here's what I did to check this. Never done this before, so I want to make sure if I did it right.\n\nAll these being done in a new conda environment, \nBrew-uninstalled gcc, checked both commands gcc and g++ run clang.\ngit clone -b fix-349 git@......annoy.git, then pip install ./annoy\nIt installs in no time, no errors or warnings.\nRan some python scripts of mine that depends on Annoy.\nWorks great.. \n",
    "KatyaRuth": "Still having this issue. Have tried to set env variables, use conda install, etc., no luck. Having in setup.py would be much appreciated!. ",
    "gagan144": "\n\"Rapidly changing data\" doesn't seem a good fit for Annoy. There's no dynamic update/delete, and the beauty of Annoy comes from its indexes statically residing on disk (mmap), so even add is not straightforward.\n\nI understand, once the annoy is build, it cannot be altered. Any guidelines on how to deploy annoy with django web framework, assuming the data is constant?. ",
    "allieonpoppyfield": "But i can install any another package\n. ",
    "Slikus": "and 1 more problem:\nif i add item with id more than ~4000000 and 128 points i got python crashing. Why? How to add 10-20-30-100M rows? Is it possible?. > 4000000 * 128 * 4 = 2G which is the max that Windows can handle afaik\nwhat about linux? And on_disk_build bug?. @ReneHollander native windows 7 x64. Also how biggest tree i can create on linux? Can i 100M x 128D for example?  On Win i can did only 4M x 128D ;c. ",
    "kyowill": "Annoy is right.I added some null items to the model, which affected the sorted list. When I delete the null items, result is same. . ",
    "OElesin": "Thanks for the prompt response, @erikbern .\nSee example below:\n```python\n\n\n\n\nt = Annoy()\nt.get_nns_by_vector(features[144], 10, search_k=1, include_distances=True)\n\n\n\n\n([144, 0, 89, 8, 85, 112, 44, 37, 3, 97],\n [824.9801635742188, 798.65625, 752.6791381835938, 741.093505859375, 715.7233276367188, 705.79248046875, 698.050537109375, 697.43310546875, 694.0565185546875, 666.2744750976562])\n```\nGiven the distances, I would like to convert the distance to percentage.. ",
    "moshest": "I had also a problem with the dot metric.\nThe problem was saving the index with \"dot\" metric and load it with the default metric.. ",
    "abhi1489": "I have also tried to do a python setup.py install after downloading the zip file and faced the same error.I see that the identifier __popcnt64 is defined within the annoylib.h file\nifndef _MSC_VER\ndefine popcount __builtin_popcountll\nelif _MSC_VER == 1500\ndefine isnan(x) _isnan(x)\ndefine popcount cole_popcount\nelse\ndefine popcount __popcnt64\nendif.",
    "PattF": "I experienced the same issue, however if I specified version 1.15 in pip install, that worked for me. Just my 2 cents :). ",
    "ConnorBarnhill": "Did some reading, you're right. Thanks!. ",
    "braingineer": "hi @erikbern \nI feel this may be prematurely closed.  The angular distance distance between 2 orthogonal vectors should not be the square root of 2. The value of cosine for 90 degrees is 0. . ",
    "Paperone80": "@erikbern, thanks for your quick reply. Yes, I might be pushing boundaries of annoy a little bit. Here is what I observed:\nMy tests were with 31,000 samples out of 120,000 and n_tree 1,500. Annoy loads and builds in about 24min. Retrieval of 1 index including all other indices with distances about 1min. \n48,000 builds in 1h 27min retrieval of 1 index with all other indices and distances about 5min.\n48,000+1 samples and annoy.build() does not finish within 24hours. Maybe it's Macbook or Mac OS X.\nReason why I am using dims=49152 is because the vectors are images of 128x128x3 and I am retrieving all the nearest indices and respective distances to actually find the least-nearest images to understand and visualise diversity or 'opposite' images in the image set. \nMaybe annoy is not build for that but I got the 32K example working within a day and it runs superfast. \nNow, I was eager to see how the results would look like on the entire dataset... \n. ",
    "Axel-CH": "Thank you for your help @erikbern I will do that.. I managed to make It work by doing what I said above, had some isues while testing. Do you think we \ncan modify the save function by adding an option to \"save_only\".\nIt could be something like that:\nannoylib.h\n```\n  bool save(const char* filename, bool prefault=false, bool saveonly=false) {\n    if (_on_disk) {\n      return true;\n    } else {\n      // Delete file if it already exists (See issue #335)\n      unlink(filename);\n  FILE *f = fopen(filename, \"wb\");\n  if (f == NULL)\n    return false;\n\n  fwrite(_nodes, _s, _n_nodes, f);\n  fclose(f);\n\n  if(saveonly){\n    return true;\n  } else {\n    unload();\n    return load(filename, prefault=false);\n  }\n\n}\n\n}\n```. You will find the Agent code here the saving operation is at line 206. It's the pickle that i'm trying to remove.\nAnd the used memory code here. It's this par that interact direcly with the AnnoyIndex. Below a chema of my understanding of the training process.\n\n. At each training episode, the index is rebuilded. (It's not my code I'm figuring out what is happening by debugging now).\nThe \"handle_episode_ended\" (nec_agent.py line 194) call the \"add\" function (differentiable_neural_dictionary.py  line 98 and 100) that rebuild the index.\n. I think what I said was Inaccurate as I saw that at each episode an index is rebuild.Thank you for your help I will find a workaround. ",
    "YuxiangLu": "Code:\nfrom annoy import AnnoyIndex\nf = 3\ni = AnnoyIndex(f, 'dot')\ni.add_item(0, [0, 0, 2])\ni.add_item(1, [0, 1, 1])\ni.add_item(2, [1, 0, 0])\ni.build(10)\nl, d = i.get_nns_by_vector([2, 2, 2], 3, -1, True)\nprint(l)\nprint(d)\nTraceback (most recent call last):\n  File \"a.py\", line 8, in \n    l, d = i.get_nns_by_vector([2, 2, 2], 3, -1, True)\nTypeError: 'NoneType' object is not iterable. Solved, solution as follow:\nDownload original code from GitHub, and reinstall by :  python setup.py install. ",
    "dcdillon": "It's somewhat like when you have to use typename on nested types.  It just tells the compiler that D::Node is a template (and not a variable, standard type, etc.)\n. "
}
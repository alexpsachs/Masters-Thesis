{
    "gtcampbell": "Looks good. :+1: \n. I think those circleci failures are legit. Code looks good to me, but I'll wait for the official star-ing until the relevant tests are updated.\n. this feels like something that could use a test.\n. In a random survey of open-source projects, it seems more common to have a LICENSE file at the root of the repo. Thoughts?\n. One thing to decide when taking the composition approach: should a BaseRouter object have a Protocol, or vice versa? One salient question might be whether we ever need to type-specialize Servers based on the Protocol.\n. @adleong Thanks, that seems pretty reasonable and straightforward. If there are no objections, I think I'll use that approach and work next on demonstrating how we would replace our existing configuration-parsing code with this.\n. TODOs on this branch, in some semblance of order:\n- [ ] Investigate whether separating the \"configuration as parsed\" and \"validated configuration with defaults applied\" types would make things more readable. (I suspect it will).\n- [ ] Add support for Namer configuration.\n- [ ] Test more error cases.\n- [ ] Update validated configuration to generate Stack.Params suitable for configuring instances.\n- [ ] Replace existing Parsing-based system with this one.\n. Resolving this one in favor of #59.\n. I may be able to address this as part of my configuration work - I've been doing some thinking on how we want to handle defaults there.\n. Are we happy with the current state of this now that server and client are explicitly separated and there is no longer a thriftFramed setting on the router?\n. I've changed the title to reflect @adleong's suggestion.\n. Looks good, and I agree that this organization makes more sense. :+1: \n. Looks good to me other than the minor grammar notes I added. :star: \n. It does seem reasonable to skip the intermediate step, but it depends what kind of package structure we want. I was envisioning linkerd-config as a standalone module that produces validated config objects suitable for constructing a linker, but we could instead make it more of a factory.\n. @adleong: I used Jackson since we depend on it already for our other JSON parsing needs - we could certainly take a look at other libraries if we think they will provide easier/better config parsing.\n. on the JSON parsing note: if we did decide to switch off Jackson and persist in using cats for validation, Circe would be an obvious candidate for a new JSON library since it depends on cats as well.\n. @adleong My rationale for the .Defaults and .Validated types is to make it clear (via the type system) which values are truly optional and which values should be assumed to have been set either by the defaulting step or to be guaranteed to exist via the validation step. For instance: a label is optional in the base config (which represents the configuration as parsed), so its type there is Option[String], but once defaults are applied there will always be some label, so at that point the type is just String.\nThe reason I don't set the defaults in the base traits is that in some cases they can be taken from the parent object (i.e. some Router defaults can come from Linker configs, some Server defaults from Router configs, etc., which isn't something I can specify to Jackson at parse time.\n. Similarly, dtab attributes go from Option[String] initially to Dtab by the time validation is complete, which is useful because it means that validation includes verifying that the Dtab can be parsed correctly.\n. closing in favor of #113.\n. I was also going to ask about the usage of $ as well. However, I'm assuming we're limited to the special characters currently defined in com.twitter.finagle.Path, namely \"_:.#$%-\".toSeq, right?\n. :star: :name_badge: (although it might be nice to fix the typo first)\n. Thanks for doing this; I just noticed that one of my branches failed on this timeout. :+1: :ship: \n. :star: :deciduous_tree: \n. :+1: looks good!\n. Could you fix the commit message on 7df1b420190cd675073028217f38dd6dede4cf55? Right now it shows up weird in Github because the summary/first line is too long; it looks like Github breaks it off at 70 characters.\n. :star: thanks for @adleong and @klingerf's hard work on this!\n\n. @esbie points out that this is likely a browser-compatibility issue, as Array.prototype.find is supported in newish Firefox/Chrome/Safari but not universal across browsers. We should switch to using the lodash-provided _.find for this function, and possibly consider using a runtime polyfill to help avoid this kind of issue in the future.\n. Thanks, Kevin - I'll see if I can capture that problem in a test.\n. Okay: I added a bunch of tests that I hope will verify we have configured LoadService correctly.\n. Rebased - would appreciate another look to verify I handled all of the conflicts correctly.\n. I updated our CircleCI config to compile integration tests to prevent that class of failure in the future.\n. Discussion in Finagle Gitter indicates there may be a Finagle patch release soon to fix this.\n. Resolving this in favor of/as a duplicate of #152.\n. I created #132 for the requested test.\n. As discussed offline, we decided to call this next release 0.2.0, since it contains breaking changes to the configuration file format.\n. we will revisit the default port stuff for the next release.\n. :star: looks good; needs a rebase of course.\n\n. I added some additional tests, etc. Looks like the branch has merge conflicts, so I'll deal with those now.\n. fixed merge conflicts - there were a bunch of changes conflicting  with @adleong's recent configuration branch, so I'd especially like him to look at this to ensure I resolved them in reasonable fashion.\n. :star: I like the way this turned out!\n\n. :star: This looks like a useful thing to be able to configure.\n\n. :star:\n\n. :+1: LGTM.\n\n. :star: This looks fine. If you like, you could move the default val into the companion object as I mentioned.\nMore general question: can/should we configure Jackson to ignore private properties?\n\n. :+1:\n\n. :star: I agree, the tests are :100: \n\n. LGTM other than my two cleanup comments\n\n. Currently, k8s storage can not handle slashes.\n. :star: \n\n. :+1: this looks fine to me, although it would probably be good to switch to the finagle-http constants as opposed to the hard-coded strings here\n\n. :star: this looks good to me; my comments are optional and/or questions.\n\n. :star:\n\n. :+1: this looks good to me, and should be easy to merge into my DtabStore work on #219.\n\n. :star:\n\n. :star:\n\n. re 100 char cutoff - I had 120 as the setting in my IDE; I'll adjust that.\n. (rebased and resolved conflicts, including adding a delete() method)\n. rebased/resolved conflicts again.\n. LGTM pending @adleong's note about the type alias duplication between io.buoyant.namerd.DtabStore.Namespace and io.buoyant.namerd.Ns.\n\n. :+1: \n\n. :star: with the caveat that I'm not that familiar with the Zookeeper client. That said, the additions and subtractions from Finagle's ZKSession make sense to me.\n\n. :star: \n\n. :+1: \n\n. :+1: This looks fine to me, thanks for the discussion.\n\n. On the ConcurrentHashMap-related comments: I see we use a similar pattern in a number of other places, so if you'd prefer, we can address those all at once outside the context of this branch.\n. \u2b50  My comments are all minor/non-blocking.\n\n. :star: I believe we already require JDK8, although others can correct me if I'm mistaken.\n\n. :+1: :raised_hands: \n\n. :star: Looking back at my comments, they're all non-blocking style stuff.\n\n. \u2b50  (although I am still not a true master of Activity and Var, this all seems reasonable to me).\n\n. \u2b50 \n. are we talking about polling for changes since a particular version, or just polling for the current state of the world?\n. \u2b50  I had the same comment as @olix0r: better to just remove the reference, which you did. I guess this is a (minor) downside to our current project structure.\n\n. :star: looks good, my comments are all minor suggestions\n\n. \u2b50  looks good overall, my comments are non-blocking\n\n. resolving in favor of #362.\n. Resolving in favor of #362.\n. :star: this looks fine other than my question about the repeated toInt calls\n\n. \u2b50  again, thanks for the minor fixup\n\n. :ship: looks nice, and I agree that the tests are very helpful.\n\n. :star: This looks good, although I don't have direct experience with the Finagle ZK announcer.\n\n. :star: looks good\n\n. :star: looks good, although I wonder if any of this would be amenable to unit testing\n\n. :star: looks good.\n\n. Do we just want to disable coverage in this case? Or is there some other way to configure CI to make this work?\n. :ship:\n\n. :star:\n\n. :star: \ud83c\udf86 \n\n. re CI: maybe! I'll look into it. I think CircleCI will do some automatic caching of NPM dependencies for you, but possibly only if they're in your root package.json.\n. Update: looks like CI works.\n. :ship:\n\n. :star: note for a potential follow-up that there is an apiMappings feature in sbt that might allow things like this to work in the future, but I haven't verified it.\nEDIT: autoApiMappings might actually do what we want in some cases?\n. :star: LGTM\n. How did we settle on these numbers (besides their being 10x the old ones)?\n. :star: seems like a good idea\n. :star: my comments are largely cosmetic. I appreciate the attention to detail in the documentation!\n. :star: thanks for updates and answers!\n. :ship:\n. :star: I like this approach!\n. LGTM\n. \ud83d\udea2 \n. :star: my comments are non-blocking since they're documentation-only; it might be good to add some information to this PR or the code indicating how these changes impacted performance. (I have some decent guesses given our discussions of the last few days, but not hard numbers, obviously).\n. :star: thanks!\n. :star: thanks for the explanation! Was/would it be possible to reproduce the problem in a test?\n. :star: is it possible to write an automated test for this?\n. \ud83d\udc4d \n. :star: code looks good; what would it take to add a test for this?\n. :star: Other than my questions, this looks great! I like the tables and the asides a lot.\n. I think we follow Twitter's convention where imports are strictly alphabetized\n. I think we try to use end-of-file newlines everywhere\n. Is dentry used anywhere? I don't see it used elsewhere in this file, but I may be missing something.\n. TIOLI: this could be slightly less code if you had annotate() return msg, i.e.:\n``` scala\nprivate[this] def annotate(msg: Message): Message = {\n  msg.headerMap(Bound) = boundShow\n  for (p <- pathShow) {\n    msg.headerMap(Residual) = p\n  }\n  msg\n}\ndef apply(req: Request, Service: Service[Request, Response]) =\n  service(annotate(req)) map annotate\n```\n. (...that said, it's probably better style not to do so when you're going to mutate the parameter, since having Unit as the return type makes it more clear that it's side-effecting)\n. agreed (as my follow-up noted)\n. > Do these values have to be kept in sync with the default values elsewhere? Or is this the source of truth for the default values?\nThere are a few ways we could handle default values. My assumption was that we'd want these config objects to just represent the JSON we parsed (and thus would be straightforward to re-serialize), and there would be another layer somewhere that provided the defaults. I can write a quick demonstration of what that layer could look like, if it would be helpful.\n\nRelated question: what do we want to emit when a value is absent from the config? Nothing? Or the default value?\n\nI had been assuming we'd want to emit nothing, because that more accurately represents the configuration passed to linkerd. That would obviously change if it's more important to represent the state as configured, which would obviously include defaults - maybe both would be useful?\n. Yeah, my assumption is that the next step is for the config objects to provide params() methods that return Stack.Params with defaults included.\n. Hmm, I usually lean toward putting this kind of dev-environment-specific thing (IDEA files, Vim .swp files, etc.) in a global gitignore rather than the one in the repo, reserving the repo's .gitignore for things that are repo-specific. Happy to hear others' thoughts on this matter though.\n. @klingerf actually removed build.sbt in 84531bc3, so I think you're going to need to update to a newer master and change LinkerdBuild.scala instead.\n. looking at a non-scientific random sampling of major OSS projects on Github, you definitely see both styles, so I think we can just make a call on what we prefer. Happy to withdraw this comment.\n. TIOLI or for later cleanup - if we want to be RFC-compliant for MethodNotAllowed:\n\nThe response MUST include an Allow header containing a list of valid methods for the requested resource.\n. This may have been a YAGNI violation, but one cannot override vars in subclasses, and I thought that might be a useful thing to do. That said, I haven't needed that capability yet.\n. two small grammar fixes:\n\"its easy\" :arrow_right: \"it's easy\"\n\"it is also convenient starting point\" :arrow_right: \"it is also a convenient starting point\"\n. typo: \"specificatoin\"\n. non-blocking question: are we using HTML5 elements in our admin UI? If so, this could be a <dialog> instead of a <div>. \n. typo: \"clietn\"\n. it might be good to add a comment indicating why we need to do this typecasting (i.e. erasure)\n. We should switch these to make use of LoadService so custom types can be defined in plugins.\n. we should probably address this TODO. :)\n. nit: since we're only doing this for the side-effects, I'd prefer a foreach to collect here.\n. I also agree that this is confusing. Is there a reason we can't leave a way to access the namer via the linker? (we could even pass the linker to the router rather than the namer itself)\n. Q: does this work with ConfigDeserializer[_] rather than ConfigDeserializer[Any]?\n. We should probably add an explicit throw as the default case rather than relying on a MatchError here\n. Yeah, I think this was a remnant of a refactor where I had to add a few more lines. Braces removed.\n. tioli: I'm not a huge fan of case _ =>, although the only alternative I can think of is to use collectFirst instead of foreach. something like: \n\nscala\nrouters\n  .groupBy(_.label)\n  .collectFirst { case (label, rts) if rts.size > 1 => ConflictingLabels(label) }\n  .foreach(throw _)\n. it might be good to note here what default load balancer is used if this option is unspecified (unless I missed that?)\n. this is clever, but I think it would be more readable to add another method that takes a JsonFactory or YamlFactory and have both this and objectMapper call into that.\n. Is this needed? I don't see it used in the file.\n. I would prefer splitting the *Serializer classes out into their own .scala files, to match the Scala Style Guide's recommendation on file organization. Thoughts?\n. Mostly, I think it's a little surprising that a file called FooDeserializer.scala would contain both FooDeserializer and FooSerializer. Like the rest of you, I don't feel that strongly about this either way. :)\n. tioli: another option would be to move this into the companion object, right?\n. tioli: If you like, you can shorten this to\nscala\ncase (_: Exception | _: Empty | _: Fail | _ : Neg) => orig\n. I don't think this is correct - \"/foo/bar\".split(\".\").lastOption is Some(\"/foo/bar\"). Can you switch this to use req.fileExtension?\n. lines 20-22 should be indented one more level, I think\n. minor: finagle-http appears to have \"ETag\" as a constant in Fields; might as well use that\n. can use the constant in finagle-http here as well\n. tioli: get and head have a bunch of shared logic (look up dtab, 404 if not found, add etag to response) that could be refactored into a shared method\n. tioli: I might cut some of the duplication by defining a baseQuery() function, i.e.\njs\nfunction baseQuery(routerName, clientName) {\n  return Query.clientQuery().withRouter(routerName).withClient(clientName);\n}\nand then you could do baseQuery().withMetric(\"requests\").build() here and below. Actually, I would have expected to be able to make baseQuery a var since often I see methods named withFoo make copies rather than mutating. In a follow-up branch, I might suggest either changing those methods to be called setFoo or to have them copy the original query, but that's certainly not worth doing here. \n. Are we trying to follow the \"use triple-equals everywhere\" convention in this codebase? If so, this should be a triple-equals.\n. True, I've seen the jQuery style too in other contexts - it was just the with that tripped me up here. Maybe I'm used to the usage in Finagle?\n. idle question only tangentially related to this review: would it be useful to have the various CRUD operations return Future[VersionedDtab] rather than Future[Unit]? That way, the caller knows the state/version of the object as of its creation/update/deletion, rather than having to call a get() later.\n. same, sorry I didn't catch that in my review.\n. Got it, thanks for the link. FWIW, k8s also gives us that functionality for free.\n. This was moved from elsewhere; I think I'd actually like to change it to:\nscala\nwatch.foldLeft(initial) {\n  (version: Option[String], watch: W) => watch.resourceVersion orElse version\n}\n. Sadly, you can't use context bounds in traits :(. I can put a comment to that effect.\n. I tried to keep the original API from Endpoints intact when I generalized it, and that used namespace, but I think I would prefer either withNamespace or namespaced; happy to change it.\n. I was following the convention from http://docs.scala-lang.org/style/files.html#multiunit_files, but maybe we should have a conversation about our preferred file naming convention for multi-unit files.\n. Happy to revert, although I'd actually like to make sure I understand your concern a little better too.\n. I waffled on this, and would be happy to discuss further. I tried to name these based on the Kubernetes documentation: https://github.com/kubernetes/kubernetes/blob/master/docs/devel/api-conventions.md#types-kinds, which refers to Object and List, and usually our code style is fine with reproducing simple names in multiple packages (i.e. we have a bunch of Api classes). I can call these K8sObject and K8sList, but then you end up with the moderately-repetitive io.buoyant.k8s.K8sObject as the fully-qualified class name.\n. I agree - I was going with the precedent from the other two storage kinds  here, which also use class paths.\n. yes, whoops! :( I'll try to capture that in a test.\n. My understanding (although I am far from an expert in Scala's type inference):\n- named(ns).put() returns Future[Dtab]\n- rescue's signature is rescue[B >: A](pf: PartialFunction[Throwable, Future[B]): Future[B]\n- because nothing in the rescue partialFunction here returns anything that alters the type of the Future, the result is also Future[Dtab]\n. Not at the moment - they're just case classes, which can be handled by Jackson without any special-casing. The reason to make this pluggable is that the ObjectMapper is constructed in the k8s project, but only namerd-storage-k8s needs it to be able to parse Path and NameTree[Path] instances (for dentries).\n. Hmm, there's already a DtabCodec declared in HttpControlService that does something different. Are you okay with the name duplication?\n. Yes, I was wondering what to do with the Closable as well - DtabStore is not Closable at the moment.\n. This ended up being changed to a one-liner as suggested by @adleong elsewhere.\n. Sure, I went with KubeObject. (and KubeList).\n. (ultimately, this should probably be a sub-item of a tls object, but for now I made your suggested change)\n. I made it a trait because it seems like something one might want to mix in to classes that have other superclasses, but you're right that in the current implementation it could be an abstract class.\n. Would you prefer KO now that it's a KubeObject?\n. I'm having a hard time coming up with a good name - suggestions?\n. never mind, after our in-person discussion we decided it's correct just to take the last item of the stream.\n. In this case, I don't think we want to use that because the block below might throw an exception, which we want to handle in the handle clause; Future.apply does the right exception handling, but I don't believe Future.value does.\n. I believe it does so only if the reader has already been discarded (which is fine in this case). That said, @olix0r wrote this code originally (I just relocated it) and can correct me if I'm wrong.\n. Thanks, that seems reasonable.\n. looks like we're about half/half for infix vs. dots on andThen, and finagle uses infix most of the time.\n. I went with KubeList and KubeObject.\n. Sure.\n. Sorry, I was looking at the other rescue clause when making that comment. This one works not because Unit >: Dtab, but, I believe, because of value discarding (\"If e has some value type and the expected type is Unit, e is converted to the expected type by embedding it in the term { e; () }.\"), which I agree is a little weird. I'll switch to .unit.rescue.\n. so I'm trying to figure out the right name scheme. Right now, the Kubernetes API for apis returns an ApiGroupList, with items like:\n{\n      \"name\": \"buoyant.io\",\n      \"versions\": [\n        {\n          \"groupVersion\": \"buoyant.io/v1\",\n          \"version\": \"v1\"\n        }\n      ],\n    },\nand \"/api\" returns an ApiVersionList, of which the only element is \"v1\".\nSo: it sounds like the closest nomenclature to what we want is actually \"GroupVersion\"? Or we could introduce another layer in our API, so you'd have something like Group(\"buoyant\").version(\"v1\").namespace(\"ns\")...?\n. It might be good to add some comments noting which portions of the code are fully copied from the Finagle implementation and which are specific to this one.\n. In @olix0r's review of #247, he mentioned that he prefers .unit.rescue over .rescue.unit.\n. It would be nice to add a comment indicating why this needs to be in the com.twitter.finagle package (I assume because we make use of some package-private code?)\n. link to relevant issue here?\n. ThriftProtocolDeserializer got moved to linkerd-protocol-thrift  as part of this PR.\n. nit: this allows strings with duplicates that I assume are invalid, like \"ccccc\".\n. We're currently logging in the Combined Log Format, which extends the Common Log Format by adding the referer and user-agent headers at the end. I think it would be better to add the host header at the end of the log string rather than at the beginning, as that would allow many tools that interact with the Common or Combined formats to continue working (assuming they ignore any additional fields at the end of the string, etc.). Thoughts?\n. tioli: I might avoid the early return Nones and  write this instead as:\nscala\nif (path.startsWith(prefix)) {\n  path.stripPrefix(prefix).split(\"/\").toSeq match {\n    case head +: tail if tail.nonEmpty => Some(head, Path.Utf8(tail: _*))\n    case _ => None\n  }\n} else {\n  None\n}\n. some extraneous printlns here\n. Not your change, but it would be good to correct the spelling of \"interpreters\" here.\n. I find this code a little difficult to understand, so maybe some comments would be in order. Are we guaranteed that closable will be non-null by the time this line of code is called? Why is writeFuture needed?\n. I think this (and similar code elsewhere) could be simplified using a java.util.concurrent.ConcurrentHashMap (or alternately one of the Guava cache implementations, but I think ConcurrentHashMap gives us everything we want). The Java8 version of CHM includes a computeIfAbsent method that implements exactly the logic we have here, without the need for any extra synchronized blocks.\n. This could also make use of ConcurrentHashMap:\nscala\nobject StreamingResponse {\n  private[this] val set = ConcurrentHashMap.newKeySet[StreamingResponse]\n  def register(sr: StreamingResponse): Unit = set.add(sr)\n  def unregister(sr: StreamingResponse): Unit = set.remove(sr)\n}\n. To wit: I think you could do something like (assuming we can take on a dependency on scala-java8-compat; otherwise it would be slightly more verbose)\n``` scala\nprivate[this] val bindingCache =\n  new ConcurrentHashMap[(String, Path), Activity[NameTree[Name.Bound]]]\nprivate[this] def getBind(ns: String, path: Path): Activity[NameTree[Name.Bound]] = {\n  val fn: ((String, Path)) => Activity[NameTree[Name.Bound]] =\n      (ns: String, path: Path) => namers(ns).bind(Dtab.empty, path)\n  bindingCache.computeIfAbsent((ns, path), fn.asJava)\n}\n``\n. That seems reasonable (although of course the Host header is not a precise match for Apache vhosts). Concern withdrawn. :)\n. good question: I was trying to have things match the URL structure. The third-party namespaces are outside the core of the v1 API (i.e. they start withapis/owner/owner-specified-versionrather thanapi/v1), so I'm not sure how they map into the version of the API as a whole. How would we want to differentiate between the core v1 API and things likeapis/extensions/v1beta1, etc.? \n. Good catch, will fix these.\n. hmm - we currently also use the namespace as the name for the object we store in Kubernetes in my thirdparty branch, and that's restricted to disallow most special characters other than-. Should that be amended to use something else as well?\n. I had it that way earlier in this branch's lifecycle, but I found it easier to read this way since the single test was a bit buried among all the setup classes otherwise.\n. q: Why is this a val rather than a def?\n. q: Could this beClosable.nopto avoid the need for the null-check below?\n. tioli, tiny: might be good to put\"/api/1\"as a base API URL inNsPathUriand construct this and friends via string interpolation, rather than have it repeated for each prefix. \n. parens unnecessary here\n. tioli: naming-wise, it's a little odd that you call thisnamersin this constructor butdelegateabove inHttpControlServiceConfig, whereasnamersis used forMap[Path, Namer]there. Not a huge deal, of course.\n. FWIW, I tried to make this same change in #247 and @olix0r had [some objections](https://github.com/buoyantio/linkerd/pull/247#discussion_r58918627).\n. This appears to be a broken link.\n. tioli: link to the [API documentation for compareAndDelete](https://coreos.com/etcd/docs/latest/api.html#atomic-compare-and-delete) here?\n. I'm not sure I understand what this means.\n. tioli: is there something more specific thanExceptionwe can extend here?\n. I will echo @adleong's comment on an earlier revision of this branch: it strikes me as weird to be using a FinaglePathto represent a URI, both because it's surprising semantically and because they have different formats (although in this case of course they map to the same string).\n. tioli:Requestexposes aaccept_=method\n. tioli: don't need parens here\n. (or here)\n. tioli: I prefer to avoid shadowing, so maybe it would be good to use a name other than \"ttl\" here?\n. could also usereq.accept =here\n. tioli: this line is repeated a bunch throughout this file and could be extracted to a private helper method. not a huge deal since it's a one-liner.\n. should we default to something sensible likeBackoff.exponentialJittered?\n. I appreciate the comprehensiveness of this test!\n. tioli, code-golf:contains(\"true\")is slightly fewer characters (and also slightly more efficient, not that matters here in a test)\n. true, but not if you're using the non-type-safe==inside it :)\n. I was thinking of having a parent trait or abstract class for etcd exceptions, but maybe that's not especially meaningful.\n. Got it - I think I was thrown by the word \"instrument\".\n. This should probably be a forEach since we're not doing anything with the result? except that we should probably actually just assign routerClients to the result of _.map rather than having initializeClient push onto routerClients below.\n. Why do we need thesetoInt`s here following the one on line 411?\n. right, I get why it's needed on 416-7 - thanks.\n. tioli: it would be possible to slightly reduce this boilerplate by creating a mkMain method, but might not be worth the effort:\nscala\ndef testMain(body: Service[Request, Response] => Unit): Unit = {\n  val _ = new BuoyantMainTest {\n    override def main(): Unit = {\n      body(client)\n    }\n  }\n}\nand then the tests would look like:\nscala\ntest(\"serves buoyant admin at /\") {\n  testMain { client =>\n    val rsp = await(client(Request(\"/\")))\n    assert(rsp.status == Status.Ok)\n  }\n}\n...not a huge deal either way, though.\n. agreed; could be a follow-up if that's easier\n. might be worth adding a comment here explaining that we provide this because intercept triggers value-discarded warnings?\n. is the extra space between the comment and the annotation intentional?\n. foldLeft on Option is a little weird. maybe use fold instead, i.e.\nscala\n_responseClassifier.fold(baseResponseClassifier)(_.mk)\n?\n. typo: \"repsonse\"\n. tioli, tiny: I might extract the method sets into separate values and construct them as follows, to make it clear that Idempotent is a superset of ReadOnly:\n``` scala\nprivate[this] val ReadOnlyMethods = Set(Method.Get, Method.Head, Method.Options, Method.Trace)\nprivate[this] val IdempotentMethods = ReadOnlyMethods ++ Set(Method.Put, Method.Delete)\nval ReadOnly = ByMethod(ReadOnlyMethods)\nval Idempotent = ByMethod(IdempotentMethods)\n``\n. finagle-http provides very similar functionality inStatus.ServerError.unapply, I think.\n. I see that now! maybeflatMap/getOrElse?\n. tiny: preferflatMaptoflatten.map`\n. tioli: I like defining defaults as constants in a companion class vs. having them hard-coded in the code, but obviously it's not a huge deal in this case where it's only used once.\n. Rather than requiring this explanatory comment, could we consider using a more structured data type for this and other similar APIs in this class? something like:\n``` scala\nsealed trait KeyType\nobject Directory extends KeyType\nobject Empty extends KeyType\ncase class Named(value: Buf) extends KeyType\n``\n. sounds reasonable\n. should we add a warning or error in the event that this is larger than the TTL? (ignore this if there already is one, but I couldn't spot it)\n. tioli: I usually prefer named parameters when we have this many parameters\n. curious: why the need for the orNull + Option wrapping here?\n. Sure.\n. TIOLI: should we provide guidance here on whether these headers should be stripped at the edge? (i.e. is there any risk they leak sensitive information?)\n. tioli: should this return an Option[FDeadline] to match the signature ofunmarshalDeadlinefromfinagle-http?\n. typo: \"linked\" -> \"linkerd\"\n. I assume theviewcall here is to save on allocating the intermediate collection? If so, IIRC usingbreakOut` would be even cheaper, but I haven't done the microbenchmark recently:\nscala\nelse Try { FDtab(headers.getAll(key).flatMap(FDtab.read)(breakOut)) }\n. typo: I assume this should read \"to propagate/control\"\n. it might be good to add comments indicating which settings differ from the default values from ZipkinTracer\n. (and also add a comment noting where this was copied from)\n. for my education: I had assumed we would handle this by making v1 a package rather than a package object, thus avoiding the extra level of object nesting for everything inside it. Would that also have worked around this bug, or am I missing something?\n. println\n. typo: \"coonfiguration\" (and I think the \"applies\" may be extraneous, unless I misunderstand its intent?)\n. I think Finagle should be capitalized throughout\n. How do we feel about target=\"_blank\"? IIRC there are some accessibility concerns, but I am not an a11y expert.\n. CSS question: in my browser, `io.l5d.nonRetryable5XX is currently line-wrapped as:\nio.l5d.n\nonRetrya\nble5XX\nWould it cause problems to change the word-wrap or word-break CSS rules to avoid this?\n. That sounds reasonable to me as well.\n. tioli: In cases where it's straightforward to do so (it isn't always, but appears to be here), I prefer lining up the table in the Markdown, so it's more readable without styling, something like:\nmd\nnamespace | path | The dtab namespace to retrieve.\nwatch     | uri  | If true, updates are returned as a streaming response.\n. is the dtab format for application/dtab documented elsewhere in this document? I couldn't find it.\n. ",
    "esbie": "thank you for updating the delegator! :star2: \n. :+1: \n. :+1: really happy with how this turned out\n. really happy with how this turned out :+1: \n. WIthout a pre-compiler, I think we should back out of the es6 changes :(\nUsing those features excludes all of IE and most older versions of browsers https://kangax.github.io/compat-table/es6/\n. I hear you :( I really don't have any way of knowing what browsers our customers are going to use, or what browsers open source folks will need support for. ES5.1 sounds reasonable to me, I've also pushed for adding the lodash library to make things like reduce easier (there's a ticket somewhere for it)\n. really happy with how the js turned out :star: :night_with_stars: \n. :star: :ice_cream: \n. @olix0r makes sense, I'm not wedded to this approach; just looking to unblock multi-router admin work\n. hi all: plz review incremental changes before I rebase\n. :star: rebased\n. had to change from router.name to router.label after rebase. sorry folks\n. :star: \n. I like the clean feeling of this config approach as compared to the anyHost/anyMethod namers. My one concern with arguments like dropVersion & dropMethod etc is that our customers have too little context on identifiers to make good decisions on what to drop.\nOne option to address this would be to invert the config arguments to be includeVersion & includeMethod, so that it feels more like a builder pattern to the user (even though subtraction is what's happening under the hood). Then the only remaining identifier ambiguity would be ordering.\nAnother option--and this one is my favorite!--would be to add an optional identifier argument to routers that specifies the prefix eg identifier: VERSION/METHOD/HOST. I like this approach because it puts the identifier right next to the thing that operates on it: the dtab. Of course people would still need to read documentation to know how to create a valid identifier...\nAnyways give it some thought, I'm fine moving forward with the approach as-is also, looks good!\n. annoying request to update docs/config.md with the config change, otherwise it looks ready to go :+1: \n. thank you for reviewing!\n. :star: rebased (again)\n. :+1: lgtm\n. :star: \n. @adleong consul does not do streaming updates like k8s does, it instead uses blocking calls for its watch api https://www.consul.io/docs/agent/watches.html\n. :star: rebased\n. ship this already? :+1: \n. :star: rebased\n. lgtm :+1: \n. :star: thanks for updating!\n. lgtm :+1: \n. @olix0r sorry I jumped the gun on this review before updating tests. I fixed all the tests up and the imports too. thanks! ready for review!\n. :star: \n. Thanks for adding examples to all these config explanations. It's definitely something that was missing. generally I'd like the namer setup example and baseDtab example to be part of the same code block. Makes it easier to c/p and gives a better sense of context. Also agree w/ @olix0r that the dtab examples are too complicated to be useful. Other than that looks good!\n. :star: this address everything I want except dtab examples to be followed up with later\n. please just merge this already :star: \n. lgtm! (spoilers I don't know much about sbt :dog: :helicopter:)\n. :star: rebased off origin/master\n. Took me a long time to reason about this feature. My thoughts are it's very complicated and very powerful.\nProbably not necessary for most tls configs? Since we control the http identifier ordering, users could specify HOST, VERSION, or METHOD as part of the name:\nrouters:\n- kind: http\n  baseDtab: /http/1.1/GET => /io.l5d.k8s/default/http\n  tls:\n    kind: boundPathMatcher\n    name: ${HOST}.buoyant.io\n. Ah ok I was confused in thinking you were interested in using io.l5d.rewrite for tls... when in fact that wouldn't be possible because rewrite never returns a Name.Bound anyways...\nI have more questions about how tls will work, but I'll take them offline. PathMatcher lgtm. Agree that a general purpose rewrite namer is not a great fit.\n. The capture step could be made optional if we specify names for path segments in our NameInitializers. We know for example that io.l5d.k8s will always bind svc-names 3rd in the prefix. So the config can be shortened to just name: ${io.l5d.k8s.svc-name}.buoyant.io\n. Oh! I was not assuming that prefixes and Name.Bound ids needed to be one and the same. If it's important to disambiguate namers of the same kind, we could have alphanumeric labels that users can set per namer. Then we'd have a consistent /namer-class/namer-label/segment-to-extract style as the bound id. \n. ah yes I noticed this earlier, thank you for fixing! :+1: :surfer: \n. I'll move the nav bar back to 580px width\n. @siggy yeah that's fair, I'll add \"all\" to the dropdown. It's a bit tricky since all isn't valid on the dtab page. I'll investigate a way around that\n. yes it reverts. I'd like to change that behavior, but it requires us to do one of the following, and I'm not sure which is best/worst\n1) use js to append ?router=foo to the end of all hrefs (ugly/hacky solution)\n2) move to using hashes instead of question marks & move admin to being a single page app (lots of work)\n3) stop using router.json endpoint and render ?router=foo as part of the page on the server (sort of a step backwards if our goal is to move logic onto the client)\n. :star: rebased\n. :star: rebased off master\n. :star: amended with the correct comment\n. lgtm :star: :ski: \n. as per in-person discussion:\nLooks great!! Let's start with app id only implementation, then follow up with more flexible label-based routing :star: \n. is this ready for re-review?\n. documentation looks great, thanks for fixing those examples! :put_litter_in_its_place: \nI would explain what uriPrefix is and how to use it in the docs. tioli I would make empty string the default, since that's a possible configuration. other than that it looks :star: \n. great! :+1: \n. review-ninja gods demand :star: \n. sweet :+1: :battery: \n. otherwise very straightforward :star: \n. :star: :100: :mag: \n. :+1: \n. This is super useful stuff! There's also some docker documentation in the README.md\nWhat I might do is put all the docker image publishing information in README.md, and then all the docker image usage information in docker.md for use on linkerd.io\n. looks great! tioli: it could also be potentially useful to show some sample tls configurations in the docs, as this is one of the trickier config options :star: \n. woop woop :+1: \n. :ship: yay! :fireworks: \n\n. @olix0r thanks! better commit message and nits addressed. \nfyi I set up pull approvals to remain around after rebase (so there's no need to re-star now)\n. :star: I don't see anything here that can't be addressed in a followup\n\n. I'm fine adding tests, just a note thought that #119 could be a browser-compat issue, which would not necessarily be caught by unit tests\n. sbt plugin option: https://github.com/sbt/sbt-mocha\n. :star: makes sense to me, my comments are optional\n\n. ptal\n. updated commit msg\n. :+1: looks good, sorry for the tracer conflict!\n\n. rebase looks good\n. :+1: thanks for adding!\n\n. looks good, but I don't actually see the interpreter config section tested anywhere?\nAlso I see the default interpreterinitializer defined, but that's it. Is the plan to not open source the others until later?\n. looking good! Can you add a unit test to verify it has the kind of json output we expect? Also, this probably has a fair amount of overlap with routers.json and could potentially replace it\n. lgtm thanks for the test!\n\n. @adleong yeah I think I understand what you're proposing there, there's just an added wrinkle where I want a type-safe way to make sure the identifier can be used in the http router. Users shouldn't be able to use io.l5d.identifier.basicHttp in the thrift router etc.\n...maybe a moot point since only http accepts identifiers right now? But I think oliver had a similar request in #120 \"These plugins must be protocol-specific\"\n. ok cool, I'll try it out, thanks :)\n. squashed and rebased after latest master conflict\n. rebased and added to changelog\n. :star: lgtm\n\n. lgtm!\n\n. ok thanks I chose some brewer colors\n\n. there are greens in that color set too, they just weren't randomly chosen for that screenshot\n. alright, BigBoard now uses routers.onAddedClients() to listen for clients that are added after initialization \n. sorry @klingerf @siggy @rmars for that last-minute refactor I did in the last commit that broke things.\n@olix0r it sounds like you may not have pulled the 2nd commit\n. lgtm pending tracing. simplifications are also a great win!\n\n. :star: this is great and the tests are great #great\n\n. :star: lgtm! don't think it'll work if we ever want server colors, but we can cross that bridge later if we need to\n\n. :star: yay! was able to run this locally no problem\n\n. :star: :no_pedestrians: :oncoming_bus: \n\n. looks good so far!\n. :+1: awesome thank you!\n\n. :star: let the code re-use begin!\n\n. This looks solid. Since clients don't start sending metrics until they're used, I'm pretty sure we'll need to allow registering/deregistering for metrics updates after initialization. Something like\nfunction registerListener(listenerFn, desiredMetricsArr)\nfunction deregisterListener(listenerFn)\n. :star: cool thanks\n\n. @adleong the v1/v2 banner distinction is in io.buoyant.linkerd.admin, so it's not an easy move into io.buoyant.admin.  I don't think it's a big issue though, DtabHandler.scala is a short file to cp to namerd\n. :star: lgtm\n\n. realizing I made a fatal mistake by not naming this module TotalRequestsLive\n. This looks great, I learned a lot from reading it! couple suggestions above\n. :star: looks very nice, good work! :cherry_blossom: :deciduous_tree: \n\n. :+1: :bullettrain_side: \n\n. rebased against master\n. lgtm\n\n. :star: makes sense to me\n\n. :star: looks great, thanks for starting shared.css\n\n. looks good, just that one routers.update thing. There's a lot of duplication here, but I think it's fine to stick with rule of threes and save a possible refactor for later\n. :star: :balloon: :balloon: \n\n. yeah! let's make a ticket\n. :star: :rabbit2: thank you!\n\n. lgtm! router_clients.js is getting kind of big/messy, so consider modularizing it further in upcoming PRs\n\n. :star: looks nice, although git's diff really makes this a difficult read :disappointed: . I'd feel more confident if there was a regression test, but don't worry if it's a pita\n\n. :star: :leaves: :fallen_leaf: \n\n. coming along really nicely :)\n. :star: :stars: thank you for updating! \n\n. :star: :100: \n\n. please hold while I move all our assets to a shared location...\n. ok ready for re-review :+1: \n. :star: \n\n. Usually when I see this happen it means a query has failed to filter out unneeded metrics\n. :star: yay thank you :)\n\n. :star: good catch\n\n. :+1: lgtm thanks for splitting into separate commits!\n\n. :star: :wind_chime: :dash: \n\n. lgtm just 1q\n\n. :star: good catch\n\n. \ud83d\udc4d  looks great!\n\n. \u2b50  \ud83d\ude0d \nis it possible to do the same with SvcCache ports?\n\n. \ud83d\udc4d  \ud83d\udc2b \n\n. :star: we could also read the body of the GET. It doesn't follow the http spec exactly, but the intent might be more clear than a POST\n\n. :star: \ud83d\udd21 \n\n. lgtm\n\n. \ud83d\udc4d \ud83d\udc79 \n\n. \u2b50  \ud83c\udf1f \n\n. \u2b50  yay! \u2b50 \n\n. \u2b50 \ud83d\udc2b \n\n. Cool! I'd really like to see a partial re-implementation of Namer.global with this prefix. As the review currently stands, our service discovery namers are /#/namerId but inet is still /$/inet/host/port and our http helpers are /$/io.buoyant.http.domainToPathPfx/marathonId\n. Hm ok. I still think a single symbol representing terminality is more useful and clearer than two symbols representing terminality and/or configurability.\n. \u2b50 I don't want to stand in the way of progress, but I'm interested in what others think here\n\n. I like this and think it's a good call. I think the more tedious work will be reworking the pipeline to get it into https://linkerd.io/\nAny thoughts there?\n. \u2b50  Will likely also require linkerd site navigation update. I'd test that the inter-file links work in linkerd-site before going through with the merge\n\n. looks good, any way to add it to ci?\n. \u2b50 \ud83c\udf64 \n\n. \ud83d\udc4d \ud83c\udfa9 \ud83d\udc30 \n\n. \u2b50 \ntioli: a wordier intro, eg \"This documentation section describes the configuration options for the loadbalancer key\"\n\n. \u2b50 \ud83c\udf14 \n\n. \ud83d\udc4d  non-blocking comments above\n\n. \ud83d\udc4d  js readability looks good\n\n. \u2b50  looks good code-wise. I'd be interested to understand why this happens\n\n. my preference would be to hide the blue summary banner when on an individual router page\n. \ud83d\udc4d looks good, thanks!\n\n. \u2b50  yay thank you!\n\n. \u2b50 \ud83c\udfaf \n\n. lgtm :)\n\n. \ud83d\udc4d  good cleanup\n\n. \u2b50 \ud83d\ude92  sweet\n\n. \u2b50 looking good\n\n. \u2b50  Thank you!\n\n. \u2b50  I really like this solution, can definitely be shipped as is\n\n. lgtm\n. OK great once I've got linkerd in a working state, I'll do namerd next\n. This example is extremely useful! It feels more guide-like than reference-like, so I think we should add it as part of the existing k8s guide https://linkerd.io/doc/0.7.4/k8s/\n. \ud83d\udc4d  thanks @moleksyuk\n. Thanks for the info!\n. @adleong I like using a curl command. I agree that response formats are not always that easy to document. Internally we did some format documentation here: https://github.com/BuoyantIO/helium/pull/1225/files#diff-0f426376b964e609c5c4618eb8bed3b2R41\nBut I think it's also fine to punt on it for now.\n. @satybald are you still interested in moving this PR forward?. closing due to inactivity, let us know if  buoyantio/linkerd:zipkin doesn't solve your use case!. I think this is already done?. @olix0r are we still interested in moving this PR forward?. Alright, this is ready to go on my side. It's changed a fair bit from the outset, so a quick re-review would be appreciated.. I haven't yet figured out a clever place on the admin site to put the payload, so I'll add something in a follow up branch. Thanks for helping me find the \"root\" of that issue, @rmars! It works a lot better now.. extremely feasible/moderate amount of work (like a couple days worth). @olix0r I suggested the same thing in this thread for a related issue https://github.com/linkerd/linkerd/pull/1025#issuecomment-276824758. I'm with @wmorgan on serving the font locally. My general thought is a separate repo fits all the requirements we've put in place around dev productivity and modularity. I will provide a more in-depth analysis in #1035. @klingerf can we close this issue now?. Let's leave it as is, we can restrict it later if it's a problem. I support making sure relative urls work, but this breaks nav highlighting as mentioned in https://github.com/linkerd/linkerd/pull/970. not reproducible on buoyantio/linkerd:0.9.1. Yay, thanks for fixing this! Can you update the usage telemeter also?. My 2c on /s: I don't find it approachable. I think it's one of the reasons why the finagle documentation is dense and hard to parse. My vote is a token that provides more context, like a word eg /service /router /id /request /go /start. I don't recommend splitting the resources across multiple plugins. It sounds like a nightmare to maintain. If we're squeamish about this admin project we should move the resources into the linkerd/namerd projects themselves or create a separate project solely for assets et al. I'll throw in nuclear option 5: move js development to a different repo. Yeah it's going to be really painful to do that going forwards once we introduce bundling, minifying, uglifying, transpiling, testing, etc.\nWe'd have to duplicate these systems in every plugin or house them at the root level of the project. thinking about this more... does this mean we can remove lib/handlebars-v4.0.5 from the repo entirely?. Canonical js modularity comes from npm modules, with dependencies defined in package.json. To combine the modules for use in the browser, usually a second build tool is used like browserify, webpack, or jspm. It's possible to use requirejs in this capacity, but probably lacks features of the newer build tools.\nGenerally this approach seems at odds with the linkerd plugin system being configured at runtime.\nThis approach also adds a build step into js development that wasn't there previously.\nAnd as discussed previously I want to avoid duplicate build configs living in various places in the repo.. Oh interesting! The ?m= endpoint has a delta field for all stat types, but the field behaves differently based on stat type. \nThe client doesn't receive any stat type information, so it'd be difficult to imitate this behavior (and also, the behavior is kind of crazy IMO so I don't want to recreate it). I think instead I'll update the RouterSummary and RouterClient components to be gauge-aware for the gauges they use. Then as part of #1055 we can send stat type in the api payload.. Hm ok, show me your setup when you get a chance; connections look fine in my tests.\n\n. yay thank you for checking!. Hm yeah I agree with you, seeing an empty graph when you first get to the admin page is pretty confusing. \nI'd prefer to show all requests in the combined graph if all clients are closed. Then if the user ever bothers to expand the client we can go back to ignoring collapsed clients. . Previous slowness from large numbers of clients was more likely due to # of DOM nodes rendered on the page & # of metrics parsed than to # of smoothie lines generated. We can always cap at the chart at 500 if it becomes an issue.. If flicker is the issue, I'd rather put this logic on the server. Each admin handler knows whether or not it needs a dropdown.. sort of related: it'd be nice to know which counter goes with which router. Hm, ok we'll have to do further investigation. My initial hypothesis of no traffic being the issue could be incorrect. We continue to see usage data with this symptom in loggly. Anecdotally I would say 25% of messages have this issue.. This looks like a grpc interpretation issue on the usage data collector side of things. Here's what I see:\ncounters from admin/metrics/usage:\njson\n\"counters\":[{\"name\":\"srv_requests\",\"value\":0},{\"name\":\"srv_requests\",\"value\":0}]\ncounters from usage data collector:\njson\n\"counters\":[{\"name\":\"srv_requests\"},{\"name\":\"srv_requests\"}]\nGoing to close this and open a ticket against the collector instead.\n. thanks @adleong! I really went back and forth on whether to make namespace optional or required. On some level linkerd needs to be able to watch all namespaces, as this is what other implementations do by default. This is probably not useful, but here's an extremely long ticket where folks are confused that ingress resources are namespaced, but ingress controllers aren't: https://github.com/kubernetes/kubernetes/issues/17088#issuecomment-221442374\nThere could definitely be an argument for required namespaces, where instead of a single IngressCache, the identifier could use a cache per namespace. That'd add a bit of complexity in order to add/remove namespaces. It'd also keep a watch open per namespace when k8s has a perfectly reasonable all-namespaces api. And finally, the namespace is not known when receiving a request. So having a map of caches keyed off of namespaces isn't particularly useful... we have to look through every single ingress rule regardless.\nPlz help me come up with something even better that still satisfies an all-namespaces use case :D. I confess I don't understand the suggestion. This PR already defines an api that extends Version. Then that api is used here to make a non namespaced call.. Thanks for the style review! Looks a lot better now. ok honestly I think the best approach is a combination of the two. we find the longest line and then set that same length for the rest of the pfx&dsts. wdyt. good idea, when it's done we can update the grpc example too https://github.com/BuoyantIO/linkerd-examples/blob/master/gob/k8s/namerd/default.dtab. Not currently possible, but sounds like an easy rewriting namer to implement https://linkerd.io/config/0.9.1/linkerd/index.html#rewriting-namers. lol I'm hilarious. Closing in favor of #1145 . Hi @21stio, in your example yes /foo/yo is routed to service s1 on port 80. The issue that's being referenced here is this: forwarded requests will still have a path of /foo/yo. This is surprising to a number of clients who expect it to be /yo.. Great @pawelprazak ! I'm happy to help. Currently the ingress annotations are read here.\nThe linkerd ingress identifier matches paths based on regex, not path prefix (we took this idea from k8s type definitions, even though most other ingress controllers don't seem to behave this way). So without the concept of a prefix, we don't know how much of the path to strip and how much of the path to keep. \nOne solution is to keep the regex and specify number of segments to consume, like the path identifier does. Another is to change the path matching behavior to be prefix-based and then use the definition you've described. \nI'm not thrilled with either one of these options honestly \ud83d\ude05 , but either one should do the job. Folks should chime in if they have strong opinions about it.\n. Good feedback! \nI like the use-capturing-groups option. Since it's a backwards-compatible change, we could release it as early as v1.0.3 instead of waiting until v1.1.0. And it's not as limiting as consume segments.. Cool! Convened w/ @adleong and it seems like the h2 requests's :path header should be changed since there's no uri. This issue is blocking potential features like https://github.com/jetstack/kube-lego integration for LetsEncrypt . Also, Istio support should include tls via ingress resources (not necessarily SNI, but at least dynamic cert reloading). Tell me more about your use case! Our hope is that dynamic configuration happens by using namerd, in a manner similar to what's described here: https://blog.buoyant.io/2016/11/04/a-service-mesh-for-kubernetes-part-iv-continuous-deployment-via-traffic-shifting/. Re config format: love the tls changes. Very surprised we haven't named anything io.l5d.default until now! This is the only place I know of where the kind: field is optional. Since this is going to be a breaking change anyway, I propose we rename io.l5d.default to io.l5d.global and make the kind: field required.. @olix0r right, it's possible, just hasn't happened up until this point (I already grepped for it). Either way, the name io.l5d.global gives better context. wfm, rename the ingress identifiers however you see fit. arrgh yeah it is... I failed to find #1108 in my search. I haven't seen this in a while.... please reopen if this is still an issue. Not familiar with influxdb, but I looked over the telemeter and it lgtm.. This looks great! I like the mirroring of the client section. I have some thoughts, but none are actionable. \n\nI wish the words \"client\" and \"server\" were at the same level in the config hierarchy.\nI think there's a potential source of confusion that prefix means logical name under service.configs but then means concrete name under client.configs.\nRetries are still really hard for me to grok. Not really a fault of the config though.\n\nNice jorb \ud83d\udc4d . Oh! yeah my mistake. I thought servers were tucked under configs. . A catch-all global default service could end up being a pretty common use case. If multiple folks are confused by this, we should re-evaluate how the identifier evaluates.. oh, shit. this was likely introduced as a result of #1212 . I'm able to reproduce this behavior using the consul example from linkerd-examples \nlinkerd version 1.0.0\nconsul version 0.8.0\nrepro steps:\n```bash\ndocker-compose build && docker-compose up -d\ncurl localhost:4140/helloworld\ndocker-compose stop consul consul-registrator\ndocker-compose up -d consul consul-registrator\ndocker-compose stop service\nmove the helloworld service from port 7777 to some other port\ndocker-compose up -d service\nconfirm service is running on different port\ncurl http://localhost:8500/v1/catalog/service/helloworld\ntry to call service\ncurl localhost:4140/helloworld\n```\nexpected output:\nHello!\nactual output:\norg.jboss.netty.channel.ConnectTimeoutException: connection timed out: /172.27.0.3:7777. Remote Info: Upstream Address: /172.27.0.1:50422, Upstream Client Id: Not Available, Downstream Address: /172.27.0.3:7777, Downstream Client Id: #/io.l5d.consul/dc1/helloworld, Trace Id: 2ce882a90ff921cc.2ce882a90ff921cc<:2ce882a90ff921cc\n. Now reproduction with linkerd-examples is inconsistent but still possible. It takes 2 to 4 rounds of restarting consul before it shows up.. @bashofmann metrics from your instance would be helpful in debugging, thanks!. For the record, v1.0.2 recovers 5 minutes after consul restart (whereas previously, recovery never occurred). We don't have a consul e2e test that I know of. I've been manually testing using the steps laid out here: https://github.com/linkerd/linkerd/issues/1230#issuecomment-297831954.. forgot to add, thanks :). Thanks for adding this issue @Ashald!. I had a dream someone submitted this same CL . I agree that the dispatcher map entry is the problem. Very curious, since our current fixture data has dispatcher as a child of each client (so like under $/inet/127.1/8282 in this example). I wonder if something has changed recently leaving the dispatcher unscoped.\nI'll fix the UI to not error on unexpected client ids and also investigate what's up with the stats scoping.\nOne thing I noticed is the metrics.txt linked has \"outgoing\" and \"incoming\" router stats, but there aren't any routers labeled that in the gob example app. @klingerf is that expected or...?\n. Update: verified this dispatcher metric appears when the first client is created. Occurs in both netty3 and netty4.. I tested a couple, but I can run through the rest. ok, given that half of our telemeters are tracing-only, I added a warning at startup.. Thanks for the writeup @arbarlow, we'll take a look! . Yes I noticed that too! I'm a little embarrassed to admit we don't have tests for this component yet. It is certainly possible to do, would you be alright adding tests in a followup PR?. Totally! These prefixes used to be (maybe still are?) clickable. They highlight associated resolution steps. We should take that functionality out if/when we unbutton-ify things.. Agreed, this is a better-but-not-perfect solution.. Good point @Ashald! When linkerd fails to talk to consul, linkerd issues the retry immediately regardless of the wait time param. (lmk if I've misunderstood)\nI considered making the consul namer's wait time configurable, but wasn't sure if it was a feature folks would actually use.. I agree, the apiserver error doesn't seem related. It is a bit worrying though that the k8s namer isn't handling the \"too old resource version\" warning more gracefully... might warrant a separate investigation if this is something other folks are seeing.\nThe documentation for these identifiers are here & here\n(which are nearly identical copies of each other)\nWe should add the global useCapturingGroups key to the identifier configuration table, and probably update the example ingress resource to use the linkerd.io/use-capturing-groups annotation.\n. Regex pattern & replacement seems fine to me (not exactly sure where we'd put the replacement statement, but yeah). On the other hand I don't have a good feel for what kinds of features folks are needing. Like, is rearranging path segments a common thing to do in prod? @pawelprazak do you have thoughts here?. I believe this is resolved by https://buoyant.io/2017/07/24/using-linkerd-kubernetes-rbac/. Added README.md, reverted import statements, and moved to nested directory structure. Thank you!. okay I'm going to merge this into the istio branch as is and address comments in a followup, thank you!. depends on #1278. depends on #1278. Seems like SNI not yet needed. From https://istio.io/docs/tasks/ingress.html:\n\nNote: Envoy currently only allows a single TLS secret in the ingress since SNI is not yet supported.. I mentioned this last week, but the istio identifier needs to know source id and source tags too (for MatchConditions). So whichever solution we decide on here will be used by the identifier as well.. See https://github.com/linkerd/linkerd-inject for follow up issues. First pass implemented and further features documented in separate issues.. https://linkerd.io/getting-started/istio/. thanks for the style pointers!. \ud83d\ude0d  I think I like the second one best! first one is good too. More investigation on this today:\n\nThe services show up in RDS with routes rewritten/redirected to egress cluster. From there I think the current the egress cluster routes to the FQDN, which kubedns has a CNAME for that points outside the cluster.\nThere's no way to get externalName from the istio pilot apis, so the only options I see are (1) Talk to the k8s api directly to get this information or (2) blindly rely on kubedns CNAME w/o knowing if the route is external or not. Or (3), whatever alex comes up with :)\n. No, they don't... so currently the identifier returns /svc/dest/externalbin.default.svc.cluster.local/::/http  but /#/io.l5d.k8s.istio/reviews.default.svc.cluster.local/::/http fails to resolve and returns \"No hosts\" . Deprioritizing this feature until after mvp. Istio users can already reach external apis w/o external name services. Fixed as part of #1440 . Discussed bullet point 1 with @adleong. Decided to attempt a combination ingress-istio identifier & send  traffic directly to the service in lieu of forwarding traffic to the daemonset.\n. OK this is ready for review, thank you!. cool find! we also may need to jump some hoops so that the linkerd dtab playground works with the new interpreter (currently you can't change the dtab for namerd/istio interpreters in the ui). Since more than one person has asked for this, we should consider what kind of docs update would make this more clear.\nWe suggest deploying at least 3 instances of namerd in its own cluster. A basic namerd config is in https://blog.buoyant.io/2016/11/04/a-service-mesh-for-kubernetes-part-iv-continuous-deployment-via-traffic-shifting/ to get you started.\n. looks good! but I feel pretty strongly that this needs to be in its own repo (similar to namerctl) (or possibly in linkerd-examples). please note that as per https://discourse.linkerd.io/t/flavors-of-kubernetes, minikube does not support hello-world.yml please use hello-world-legacy.yml. I've written up most everything I know at https://discourse.linkerd.io/t/flavors-of-kubernetes: spec.nodeName is not available in minikube or in k8s < v1.4 and so cannot be used in the http_proxy address. In absence of this, hello-world-legacy queries the k8s api (via kubectl sidecar): https://github.com/linkerd/linkerd-examples/blob/master/docker/helloworld/hostIP.sh\nI don't know a ton about CNI, so not sure if this is what's impacting your setup or if it's something else.\nrelated: https://github.com/linkerd/linkerd-examples/issues/130. Sorry none of these examples are working out of the box :(\nHopefully we can learn from this and improve them as a result.. Oh! great to hear that it is working. I'm going to close this issue, please contact us in http://slack.linkerd.io/ or http://discourse.linkerd.io for additional support!. fwiw, we need something similar to implement istio http redirects as part of the identifier. not sure how to fix this branch after rebase, so closing and opening a new PR. possibly related #841. Confirmed that this works with vanilla istio 0.1.6 in k8s 1.6, but does not work with istio-linkerd.. I ran ./sbt e2e:test on master on my laptop a number of times and UsageDataTelemeterEndToEndTest passed every time (although other* tests intermittently failed)\n\nMuxEndToEndTest, HttpEndToEndTest, TlsEndToEndTest, RouterEndToEndTest. duplicate of #1218 . yeah I was noticing that also. I think @mejran might be right on this one. From https://kubernetes.io/docs/api-reference/v1.7/#ingressrule-v1beta1-extensions:\nHost is the fully qualified domain name of a network host, as defined by RFC 3986. Note the following deviations from the \"host\" part of the URI as defined in the RFC: 1. IPs are not allowed. Currently an IngressRuleValue can only apply to the IP in the Spec of the parent Ingress. 2. The : delimiter is not respected because ports are not allowed. Currently the port of an Ingress is implicitly :80 for http and :443 for https. Both these may change in the future.. Agreed that most implementations favor prefix match. We are attempting to adhere to the ingress spec https://kubernetes.io/docs/api-reference/v1.7/#ingressrule-v1beta1-extensions which states: \n\n\n\n\nPath is an extended POSIX regex as defined by IEEE Std 1003.1, (i.e this follows the egrep/unix syntax, not the perl syntax) matched against the path of an incoming request. Currently it can contain characters disallowed from the conventional \"path\" part of a URL as defined by RFC 3986. Paths must begin with a '/'. If unspecified, the path defaults to a catch all sending traffic to the backend.\n\nThere's a pretty big back and forth at https://github.com/kubernetes/ingress/issues/555 on what the behavior should be.. Hi @perspectivus1!\nI don't totally understand your failure case yet. Is the problem that there's no address when kubectl get ingress -o wide? Syncing the ip is a neat feature that the k8s folks have implemented, but I'm not sure the other controllers have. Either way it's not required in order for the ingress controller to work. For that blog post we recommend using the ip of kubectl get svc l5d.\nLet me know your thoughts\n(related to #1491). Also if this is how most folks want to use ingress controllers, we should add address syncing to linkerd. Good question. Yes, l5d looks over all ingresses in all namespaces (unless you specify a specific namespace in the l5d config). For conflicting rules, whichever one l5d arbitrarily finds first is the one it uses, order not guaranteed.. Update: added HaltClosePropagationFactory to the stack which swallows calls to close unless they come from DynParamsFactory explicitly. Tested manually for correctness and now running load through it. Will attempt to write unit tests. Some previous feedback still needs addressing. . putting this on hold until the finagle 7 upgrade. Haven't seen that before, will try to repro when I get a chance. The code blocks in the description look identical, c/p error?. OK I see. So the expected behavior should be that l5d first evaluates all possible ingress definitions for matching rules. Then if none are found, it uses a default backend (if there is one defined) (if there are multiple defaults, it's nondeterministic which one l5d will choose). Lmk if I've misunderstood.. If you're interested in adding tests for the config, I'd recommend writing a ConfigTest. We have a bunch of examples of this, eg https://github.com/linkerd/linkerd/blob/a9c1627d389fab206faac44d1f674695207ff5cd/namerd/storage/consul/src/test/scala/io/buoyant/namerd/storage/consul/ConsulConfigTest.scala . oh snap! I was about to do this also ha. I really like k8s's https://raw.githubusercontent.com/kubernetes/kubernetes/e123311d8a45e4c1abc2cbc9a407f75dec25341f/.github/ISSUE_TEMPLATE.md\n1) makes it clear this is not the place to ask for help\n2) directs the filer on how to write better bug reports/feature requests. Hi @sokoow! Thank you for your suggestion. The article you linked to instructs users to deploy linkerd-namerd.yml, not linkerd.yml. If you have additional issues or feature requests for linkerd-examples (or the blog posts that reference them), please file them in the linkerd-examples repo https://github.com/linkerd/linkerd-examples. Confirmed via https://discourse.linkerd.io/t/io-l5d-mesh-issue/318/3 that the issue does not occur with the io.l5d.namerd.http interpreter. related to https://github.com/linkerd/linkerd-examples/issues/183. Thanks @zsojma @dlaidlaw! We'll be updating our documentation soon to reflect these changes.. Thanks @mirosval, we'll take a look. Alright, you're basically going to hate me after this review, but here we go:\nThis block is really hard to read, especially now that I've forced you to not use fat arrows. Also the servers var is shadowed which doesn't help. In these cases I'd stick to a for ... in loop:\nvar servers = [];\nfor (var router in routerList) {\n  servers = servers.concat(router.servers);\n}\nAnd honestly, I'd prefer for ... in just about everywhere you're currently using reduce().\n. Same sort of situation here, the 2x reduce is making it difficult for me to understand the intent. This would be a super great task for something like lodash max http://devdocs.io/lodash/index#max\n. Given that each of these modules does its own MVC work, it's counterintuitive that at the very end it hands its update cycle over to an external party. Let's just make this\nfetchUpdate();\nscheduleUpdates(fetchUpdates);\n. I prefer screaming snake case as the convention for constants\nUPDATE_INTERVAL\n. +1\n. Generally when I've seen the functional class pattern used, init() is the function returned by the singleton/module instead of a value on a return object. So in this case it'd look like:\n```\nvar init = function(server, template) {\n  ...\n}\ninit.findMostActiveServer = function(routers) {\n  ...\n}\nreturn init;\n```\nAnd you call init like BigBoard(server, template)\n. :hankey: \n. really surprising to hear, since this is standard fair in the prev repo\n. Definitely agree that updates should be configured in a central place. I think there are better encapsulated ways of doing it than returning an updateMe function to the caller. \nFor example we can pass in polling length to init. Or we can get very fancy, and have some intelligent timer module that emits events when we want to increase or decrease polling. Then each module could listen for those events and update themselves as needed. What we definitely wouldn't want is a timer module that needs to keep track of all the modules that rely on it.\n. I was asked to change these already, sorry!~\n. we don't technically need to replace any parts of the pathname for this site\n. Very happy with this, it's so much cleaner!\nnit:\nSome js conventions require curly braces for if / else statements. Personally I'm fine having small if / elses with no braces. In this particular case though, bc the else is multiline with braces, I think the if should also have braces ... so that it's easier to tell they are a pair\n. lol, I asked for the opposite earlier in this review ^_^\n. putting it all in the same place is fine for now, but I still don't agree with this architectural change in the future for the reasons I outlined earlier\n. yes!\n. ok sure!\n. no problemo\n. fair enough\n. my preference is tests-as-documentation, since they can never be out of date :P \nbut if you find it useful, I will add them !\n. good point, removeClass is clearer, I will add a TODO\n. not directly related to this change, but it'd be nice to use the linkerd logo instead of the crystal ball (although I like the crystal ball, it's pretty cute)\n. Yeah consul is missing too. Plus we need to write out separate sections for ZK/Consul below. I can do that in a follow up\n. Ah, yeah it's part of https://linkerd.io/doc/latest/userguide/#basic-concepts\nI suppose we could use the full path on that link?\n. interesting thought. I'll let you battle it out/update the docs to better reflect intention once this is merged (the goal for this review is consistency across markdown files, not necessarily documentation correctness) thanks!\n. happy to update format while we're here\n. \"basic concepts\" isn't a section in this doc, currently\n. So to be specific about what I'm looking for in simpler dtabs:\nHere's an example for routing all http GET requests based on their host header using the filesystem namer\nbaseDtab: /http/1.1/GET => /$/io.l5d.fs;\n. This bit was a confusing read for me, as I thought maybe test was for use in the test environment only. I'd probably rename to add(excluded: Boolean, str: String)\n. yep you're right! It's\nhost \u2013 the Consul host. (default: localhost)\nport \u2013 the Consul port. (default: 8500)\n8500 is the default for Consul\n. ok sure\n. agreed\n. agreed, it's not immediately useful where it is, I'll move it\n. could you give me an example of when you'd intentionally not match them?\n. ok given that we're interested in changing this config setup anyhow, I won't bother including an example. I'll adjust the wording to match oliver's suggestion.\n. cool thanks, I had trouble finding that so I put the finagle default\n. oops! thanks\n. can't say I'm super familiar with html5 tags, but from what I can tell no browsers support dialog except for chrome and opera? https://developer.mozilla.org/en-US/docs/Web/HTML/Element/dialog\n. looks like thriftMethodInDst isn't actually set in the server, could you update the docs to reflect this?\n. even though these captures are the first & second match, they're the also the 2nd and 4th segments of the path.... so the test's intention is not super clear.\n. I don't understand why the api is prefixed with /marathon, or what I would do to change it away from the default of /marathon. Adding docs to config.md might be good to do as part of this change\n. tioli it's more clear to call these AppIds\n. the other namers don't package underneath the api version (eg v2)... I think the intent there is that AppIdNamer could work under any version of the api that's released\n. shrug alright, whatever the bossman says then\n. tioli feels more like a getMonitoredAddr\n. sure sure, I had the opposite confusion though, where I was concerned that a new--possibly duplicate--watch request was sent on every lookup\n. At this point the bullet indentations are making documentation hard to read. (eg, here we are documenting commonNamePattern which is a required option for a list of names, which is a required option for a tls plugin, whose class name is specified as a required option for tls.) Let's make separate headered sections for each tls plugin\n. Is this right? Is it ok to resolve the dtab of one router with the name interpreter of another? Are all router nameinterpreters the same?\n. I'd also be down for adding a more straightforward way to access the namers\n. I noticed that if we want to match the naming pattern, FancyConfig should be Fancy, Fancy should probably be FancyProtocol. Same with Plain\n. oof yeah I forgot about wanting to trace namer requests. On the other hand I really want to pass the name interpreter into the tracer so I can use it to resolve the tracer's client's path :|\n. oh! vestigial \n. huh, transform only operates on the stack, so withParams isn't an option here. gonna go with a .copy() instead, I think\n. I waffled on this a bit, as I assume most every http identifier will have this option. Ultimately I think you're right, it's specific enough to identifiers that I think it makes sense\n. maybe mention whether the object is optional or required\n. looks like this import is still needed\n. I'm not an activity expert, but transform expects matches on Activity.State that map to Activity :|\n. yah, I wanted to use .handle{}, but it expects you to like... actually handle the error, not turn it into another better error\n. cool, turns out I was unnecessarily worried about type erasure\n. Found it in Intellij:\nSettings\u2192Editor\u2192General\u2192Other\u2192Ensure line feed at file end on save.\n. I like default more, so I'll move to that\n. What about additional identifiers that aren't the default? I was thinking of writing some identifiers to replace rewriting namers. Would they also go in protocol.http or should they stay here?\n. huh, Intellij's refactoring was not so hot\n. no reason, I thought I needed more flexibility than I did\n. yeah I see your point. I'll implement client adds. It's going to be a bigger change though, since this will be the first client-side component that keeps state (the others don't really keep state, they just purge and re-render on every single update)\n. do we still want to trace found/notfound?\n. is it intentional that pending is never used here except to raise closed?\n. I have such a love/hate relationship with this test :joy: \n. nice catch!\n. I'd move this inside mkColor\nno reason for the rest of the routers code to access it\n. ok sure\n. weird. maybe it was scalariform. I'll revert it\n. Could you move these stats to be rendered on the client using handlebars instead of the server? We want to head towards client-side html rendering for most parts of the admin page\n. nit: rename to dashboardHtml?\n. nit: this file could use a return at the end\n. O_O :ghost: \n. \n. I don't think this needs to be done now, but it'd be nice if in the future this used process_info.template\n. I tried that in my previous review, but the yield block never executes if commonName == None\n. I think these modules should go in their own files\n. this would be easier to read as a handlebars template eg\n{{#each router}}\n <div class=\"router router-{{router}}\">\n    <div class=\"summary\"></div>\n    <div class=\"server\"></div>\n</div>\n{{/each}}\n. fyi $(\"<div class='foo'>\") is totally valid jquery\n. yup yup very similar to my branch\n. I think this method could be a lot simpler if RouterSummary was responsible for rendering a single summary instead of a list of summaries. In a single summary scenario, the RouterController parent could pass in a $summaryEl to render the template in & there would be no need for the summaryEls mapping\n. this is my understanding as well\n. I like this convention, so I vote we keep it\n. that was my original thinking, although looking at this a second time, it's probably fine to alter stat (and maybe even preferable for less object garbage)\n. well they would be WRONG!!!\nj/k\nI guess I can change it, it's not gonna be as pretty though\n. omg thank you for reminding me!\n. code golf! you could alternatively use finagle paths (tioli)\nval components = Path.read(path)\nif (components.size() > 2) {\n  ensurePath(components.take(components.size()-1).show)\n}\n. they're there for library completeness/symmetry, I expect they'll be used at some point\n. good point, I'll migrate to hex\n. sorry about that! should be fixed once you rebase/merge master\n. I'd add this div directly to router_container.template\n. I'm a little confused by this. If I'm understanding correctly, $container is a <div class=\"router-server\"/> but you then stick another <div class=\"router-server\"/> into it .... thereby recursively nesting this div on every update?\n. tioli, this method would probably work just as well inside RouterServer where it's actually used\n. Instead of routers being updated by one of many possible modules, could you pass metricsCollector to routers and have it register itself for updates? It's annoying, but in the end cleaner\n. I don't think clearfix is necessary anymore (may need the grid branch to land first? not sure)\n. Disagree that withFoo implies copying over mutating, there's plenty of examples the other way, especially when it comes to the builder pattern:\nhttps://www.thekua.com/atwork/2013/04/a-builder-pattern-implementation-in-javascript/\nhttp://brainbaking.com/enhancing-the-builder-pattern-with-closures/\nfwiw jQuery is the most well-known builder pattern lib in use for js, and it mutates using .foo(bar)\n. Yeah, I understand your thought process. Sounds like jQuery style could be a good compromise\n. hm, col-md-* classes really should be inside a row.\nto avoid all the extra row/col classes, I'd make .client-metrics a row, then have 3 .col-md-2 divs: one with requests/successes, one with connections/failures, and the last one with latencies\n. These stats barely fit the definition of what I might consider tabular data, so I'll let it slide. Know that generally going forward I'd like to avoid using tables (I honestly don't care for data lists much either).  It makes separation of content & design much easier if they're not used\n. please add these as part of a template\n. I'd prefer clientMapping to be by client.label not by prefix\n. I prefer that we don't try parsing metrics in multiple places. Consider something like:\nstrokeStyle: chartLegend[name.match(Query.clientQuery().build())[2]]\n. I prefer that we don't try parsing metrics in multiple places. Consider something like:\nstrokeStyle: clientColors[name.match(Query.clientQuery().build())[1]].color\n. simpler as:\nvar color = _.property(colorName)(color);\nwould have to add .neutrals to the baseColorOrder\n. hm ok fair enough, let's revisit later\n. These repeated metric-containers are getting pretty verbose, I think at this point it'd be worth our time to move them into partials http://handlebarsjs.com/partials.html\neg this would become\n{{> serverMetric context=metrics.success containerClass=\"success-metric-container\" metricClass=\"success-metric\"}}\n{{> serverMetric context=metrics.failures containerClass=\"failure-metric-container\" metricClass=\"failure-metric\"}}\netc\n. any reason to have this file outside of acceptance-test.yaml?\nif it's specific to a local kube setup, maybe document that\n. nothing inherently wrong with that\nIt's not tested though (acceptance-test.yaml is), so it could come back to bite us as these examples fall out of date\n. we should probably move this file from admin to iface if we're going to use it in HttpControlService\n. It's helpful, would be even helpfuller with comments\n. since \"metric\" doesn't say a lot, drop the if/else\n. lol we should not have so many charts all with id of \"canvas\" (id is supposed to guarantee uniqueness)\n. hehe this is our fav number \ud83c\udfa9 \ud83d\udc30 \n. nit:\n} else {\n. Is there a reason this maxValue, 100.000001 is different from the yRange max of 100.001? Might help to pull these into constants\n. tioli: more succinct as _.find(data, \"metricSuffix\", \"success\")\n. nit, I wouldn't bother listing _i here\n. these cases can be combined: case \"/\" | \"/routers\"\n. my preference for these borders is 1px solid dimgray or 1px solid slategray\n. Can we send this html down as a string inside a script tag? Like:\n<script id=\"linkerd-navbar\" type=\"application/json\">{\"navbarToInject\": \"<nav class=...\"}</script>\nThis way ideally we can reuse the nav bar from other pages without duplication.\n. if dashboard-shared is going to be shared across all pages, should we add it to AdminHandler instead?\n. I had the same question as @adleong. Given that this page is 100% static, no template variable needed, I think it should be hardcoded here. (I know it's a pain to develop that way, but the help page won't need to change too often)\n. I actually agree with siggy here. This would be clearer as if(shouldExpandInitially) showClient() else hideClient()\nDeregistering something that's not registered is a no-op. What's the worry here?\n. If you're worried about the list of listeners' performance, I bet an implementation based off of  addEventListener/dispatchEvent would be more performant\n. s/serivces/services\n. This list has only one element. I would remove this line altogether\n. this is apparently how slate deals with header name collisions\n. sure, works for me\n. If there are, this is my first time hearing about it. target=\"_blank\" is standard 2016 website practice when linkifying external urls.\n. It's not the slate default, but we can def do that. It squishes the descriptions but I think that's not so bad.\n. sgtm\n. ah yep! oversight on my part, thank you\n. I was noticing that too. Sometimes the descriptions are full sentences and sometimes they are only subjects/noun phrases. We could probably pretend they're all sentences though\n. Oh thank you! I was a bit confused on the units here.\n. hmmmmmmmmmmmmm ok I'll put N/A\n. oh, yeah the quotes shouldn't be there\n. haha github linkified your suggestion against your will\n. oops yeah I noticed that too. I'll hardcode it for now. I could hardcode it to head if we think that's a better alternative\n. yup this is a c/p issue\n. this formatting is nice here but breaks the markdown table\n. oops yeah this isn't right... it's not the ids that are domains, it's marathon's dns interface. Regardless I can take it out\n. oooooh yes good idea\n. ah, right. I can back out of that change\n. It just so happens that every place calling this method has done id.take(3) before passing the id in, but I can move the take to be part of this method instead\n. For these and other template imports, they should be part of the define function if at all possible. I believe requireJS can do this out of the box, see http://requirejs.org/docs/api.html#text. newlines at end of file, please. newline on eof. newline on eof. Instead of using title, how about using a data attribute to specify admin type, similar to what we do for build version. Related to this, linkerd and namerd don't need to share the same main.js now that requirejs tracks dependencies for us. why does this have to be a subdirectory? and if it has to be, maybe name it test instead of base?. yeah fine as is thanks. ur my hero ^^. These vars can now be hoisted out of the returned function... although this is only actually useful if we're instantiating these component more than once. (are we?)\nTemplate compilation on the client is usually a very costly operation because of the string manipulation required.. Awesome \ud83d\udc4d . could also be\nexpect(client.color).not.toBe(\"\")\nwhich I think is a tiny bit more clear. We're doing a lot of length checks here and other places... not a requirement but it'd be nice to provide a custom toHaveLength matcher. oh good idea, I'll try that out. OK no problem, I will hunt them down. This confuses me also  \u00af\\_(\u30c4)_/\u00af. oh good catch. I like this idea! I'm worried that someone might infer dryRun to mean \"representative of any performance degradation while using the telemeter\", which wouldn't be the case if we don't actually make the calls. What do you think, am I being paranoid here?. No, only trying to be as transparent as possible about what's being sent. Currently in order to use /admin/metrics/usage to see what's being sent, the telemeter needs to be loaded. But the telemeter also sends an initial payload on load. If we think docs + protobuf file sets enough context then this may not be an issue.. @siggy's load test agrees that performance impact is negligible, so let's stick with dryRun. Interesting, I hadn't really considered that option. \nI agree that changing AdminHandler to a class was kind of painful. Currently I don't want to pass namerdNav into the html method because it means each handler using the function would need to know information about namerd and I'd rather keep handlers self contained. \nKeeping mkResponse in an object seems reasonable enough.. hrm, I tried putting mkResponse in an object, but any handler using mkResponse also at some point uses html \ud83d\ude4d . this test is like 3 tests in one.... let's break it out. I'd like to see these barchart-related methods in their own function or object named lbBarChart to help separate them from the rest of the router client. Currently it's hard to tell whether or not these methods rely on the router client's scope. I'd start by defining them outside of RouterClient.\nSame situation for RouterSummary and retriesBarChart . these 3 vars don't look used?. ah \ud83d\udc4d . OK this is great. I think we can make it even greater by tweaking how the chart is instantiated. Let's make RetriesBarChart more of a wrapper that contains the scope and less of a chart library. Here's the interface I'm thinking:\nvar retriesBarChart = new RetriesBarChart($retriesBarChart)\nfunction RetryBarChart($container) {\n  function getColor(...){...}\n  function getPercent(...) {...}\n  ...\n  var chart = new BarChart($container, getColor);\n  return {update: chart.update};\n}. tests are very clean now, good work!. It's my preference to separate js and css as much as possible. Instead of directly setting the border-bottom here, I'd like to add/remove a class instead. That way css for the class can remain in dashboard.css. ahhh. I mean generally yes but that would be too big of a refactor. via meat bag conversation: it could be interesting to create a main-usage.js file that would be loaded in place of main-linkerd.js. Doesn't really solve the \"where to serve static assets\" problem, but would make this routing less of an eyesore. not that it matters in any way whatsoever, but could be a little less verbose as \n$usageContainer.html(template(view)). weird formatting?. huh, wonder if we should move away from .partial notation. this instruction should just be npm install since handlebars is listed in devdeps, and it doesn't hurt to run npm install even if you're uptodate. if you like it, let's keep it. Hmm, this is a confusing change to how html works w/o navbar or navHighlight specified. html used to display the navbar if none was specified. In this version html displays no navbar if navbar & navHighlight are unspecified.\nI suggest we change the method signature to make this logic easier to follow, something like:\ncustomNavbar: Option[String] = None,\nnavHighlight: Option[String] = None\n): String = {\n...\nval navbarWithHighlight = (customNavbar, navHighlight) match {\n  case (Some(custom), _) => custom\n  case (None, None) => navBar()\n  case (None, Some(highlight)) => navBar(highlight)\n}\nor just get rid of the navbar arg entirely, as it's not used anywhere that I can see. Good idea. Thinking about it more, I'd rather send uptime. I'll update accordingly . good call. Geez this is tough since we're hitting the limits of bootstrap. My thought is to ditch the css changes and instead set the active class on the panel heading and list group items.  We can stop using bootstrap for the delegator in a different CL. \n\n. First of all, love this function, v useful. Adding more enum strings makes me think we should define type constants and use those instead, so the js runtime will complain if you make a typo.\nThere's pretty wide coverage for const: http://caniuse.com/#feat=const so I advocate adding it to our repo's lexicon (I should also list what browser versions we're realistically targeting).\nI'd suggest something like\nconst neg = \"neg\"\nconst fail = \"fail\"\nconst success = \"success\"\n.... tioli this was already here, but for the record I prefer break to be on its own line. nit: my style preference is always use braces for if/else. \nTernary could be cool though\nreturn !ignoredClients[routerName][client.label] ? client : null;. Given the new API, I think we should take out the .metrics() function that listeners define and instead calculate deltas for every counter in the payload. I don't think it'd be a performance hit and it'd certainly be easier to maintain. I'd like to get rid of routers.js because I don't think it adds a lot of value on top of the fully hydrated metrics tree. I guess the adding clients logic is nice, but could just as easily be implemented in metrics_collector.js. Oops, this was for testing only! Nice catch. Traefik has a StripPrefix option that I think we should emulate later.. Thanks for looking at this. This was my attempt to treat prefixes the same way that the path identifier does. There are two controllers implemented by the kubernetes team (GCE & nginx) and they themselves disagree on the behaviour.\nSee https://github.com/kubernetes/contrib/issues/885\n.     >_>\nYeah I think you're right\n<_<. Oh! Here I was thinking how nice I am to convert all the uris to fpaths. Seemed like a cleaner way to compare prefixes, but I'm also cool w/ string manip. tioli add parens around the inner ternary for readability. multiplier is 0.8 here but 0.85 in `_resize`, intentional?. I'm a little confused here since triggering `resize` invokes `render`, but `render` is already invoked in the previous line.... Oh cool, I hadn't seen this comment. I'll switch to regex. Ah ok, I found `state()` more confusing since I'm not too familiar with `Updatable`. I'll switch them out.. good catch!. for my own edification, why is this needed? I figured a neg was a neg transformer or no. Yay! Could we do something like info log \"no addrs found were on this subnet\" if selected is empty but `addrs` is not?. Right, it could be extremely useful or extremely annoying. I'm fine leaving as is. I wish the docs better defined what \"NameInterpreter functionality\" is. huh, surprising that the namerd http interpreter supports tls, but this grpc/h2 one doesn't. ah, duh, yes thank you!. good point, let me test it out. I'm not 100% sure what's going on here, but when `ConnectionFailedException` happens (or connection refused or connection time out), `BaseApi` doesn't seem to call the backoff retry policy at all. I am sort of assuming the exception is happening in `client` and then the retry filter and api filter are skipped.\n\nSo this is a long way of saying I couldn't get it to work by adding something like this to the retry policy\ncase (req, Throw(Failure(Some(err: ConnectionFailedException)))) => true\nbut I'm open to suggestions. I guess I could dig around in finagle to better understand how Http.client works.... yeah you called it, the failures are flagged as both rejected and nonretryable. Looking around some in finagle I don't see where those are set or how to configure them. RetryFilter/RequeueFilter seem to be the primary users of the nonretryable flag. nit: eof newline. Sure I can add a comment, I find this role pretty confusing too. infiniteRetryFilter defined in BaseApi still applies.. that's a great idea, thanks!. oh right good point. if you're not using the passed in event object, just leave it out? function() {. Given that the collector api is pretty small, I'm wondering if we should just use a stub here instead of loading up the metricsJson fixtures. Something like\n{\n        start: function() {},\n        registerListener: function() {},\n        deregisterListener: function() {},\n        onAddedClients: function() {},\n}\n Wdyt?. Argh this diff is nigh impossible. That's right, nothing changed besides the method params stuff.. Sounds like a pretty small edge case to me, I'd go for removing all references.. I'm not opposed to moving to a map, but I don't understand what additional lookups you're anticipating making? This diff has the same number of lookups.. Why an if here? Might be more performant in registerListener.. protocol is h2 here. I'd rather not do a DOM find for every added service if we don't need to (and I don't think we do for initializeServices). For addServices, you really only need to check size at the end, maybe something like _.flatten(svcsByRouter).length > 0. this argument style is kinda wonky, either $ should get its own line or all args on one line.. this... is a jquery object too right? might as well put a $ before it like the others. (applies to other tests too). I bet we could do that, makes sense if we need to import a bunch more. However I also edited the google protobuf files to change their package option java_package = \"com.google.local\"; otherwise they conflict with java's protoc lib. If you figure out a better solution, I'm all ears.. gc, forwarding would be a lot more user friendly. sgtm we can put logging in the client or identifier etc. heads up that the route-rules from the api server don't include a namespace field, so we need to derive that from the fully qualified domain name or something. So is the plan to not make the istio namer directly configurable? Also, do we need to pass this in here?. might consider naming these mixerHost mixerPort to be explicit (the other istio components need different host/port combos so I could see this getting confusing). Default should be istio-mixer.default.svc.cluster.local imo. ah whoops yeah I was using 8080 for kubectl proxy. maybe remove pollIntervalMs from here since it's not being used. still needed?. rename this to RouteCache for consistency (or renamed ClusterCache to ClusterManager). I agree that's how istio currently operates. I think it's sort of a point of contention though, I've seen a few people in the istio/k8s slacks challenging the design choice of only allowing external traffic via services. Maybe we should make it optional. (I don't have a strong opinion, but wanted to bring it up for discussion). In person discussion:\nGoing to leave this as is until we find a good solution for https://github.com/linkerd/linkerd/issues/1430. good point, will fix. The 0.1.6 I downloaded has it as istio-pilot.\nI had the exact opposite fear: that someone would deploy istio+linkerd in a namespace other than default and be confused why it doesn't work.. Fear Driven Development. For sure, this is not very clear as is. ok np thanks for verifying. Actually, I don't think this is needed anymore. Originally I thought I'd need to separate out \"/my/path\" from \"foo.bar.com/my/path\".. Ok I can make that more explicit. This is a bit overkill imo since we wouldn't be doing a replace without all matchConditions being met in the first place. Of course that ordering isn't obvious here... thanks!\n. I could use some feedback here. Without this change, the ResponseException never makes it to the ErrorResponder/ErrorResetter. But with this change, the exception also gets propagated to the default monitor:\nException propagated to the default monitor (upstream address: /172.17.0.1:50971, downstream address: n/a, label: 0.0.0.0/80).\nio.buoyant.linkerd.protocol.http.ErrorResponder$HttpResponseException\nMakes me doubt that ErrorResponder is the right place in the stack to be handling these kinds of responses. Thoughts?. Neat that totally worked!. Ohhh good point I hadn't considered that. I'm glad you're thinking about UX here. I don't see this as internal vs. user complexity. More like accuracy vs. accessibility. I share your concerns though; I don't really see a path towards integrating this with the admin ui.\nServing from the existing admin port would preclude being able to implement this for h2 identifiers. Multiple ports allow for multiple protocols.\nLet's talk through it more though, there could be something I'm not thinking of.. I looked into it briefly, but let me squint at it some more. Bleh, yeah thanks for looking at this, it's a different Stream. Will update to h2.Stream. HttpLoggerConfig/H2LoggerConfig: new Stack.Module itself already does good job of reducing to the essentials.\nIstioLoggerConfig: I could pull out host, port, client, etc into it's own trait (IstioLoggerConfig extends HttpConfig with IstioLoggerConfigBase), but having overrides in all three places was unnecessarily confusing. \nIn general I only focus on DRYing things up when there are 3 copies of something, and didn't find a lot of value in sharing code between configs.. shouldn't this content string be a json object?. I would like to suggest {\"error\": \"No such subtree: $q\"}. Cool I will try out the idea. Originally I thought io.l5d.fs could have config defined in BOTH a file and also statically, but dropped that idea due to its complexity. So I bet separating them is no longer needed.. Oh weird I thought I did this, but I didn't. Must have been a dream. . Wouldn't we want to keep the previous service factory open even if we like, fail to read the updated service config file? So that we can continue to serve requests?. I don't know why, but moving the dedup up to the dynamicParams causes the stack to be rebuilt on every request. I'm a little confused why this path isn't matched, since I don't see a \"kubernetes.io/ingress.class\" specified for nginx-ingress, and we should still use the ingress if no ingress.class is specified.. I'm a bit confused about these test names. It seems like this is the test that tests a single configured ingress, whereas the test above is the one with \"more than one [ingress class] configured\".. I don't see anything in the ingress resource spec that dictates controller annotation behavior. In lieu of a good spec, I try to follow other ingress controllers' behavior. So for traefik https://docs.traefik.io/user-guide/kubernetes/#excluding-an-ingress-from-trfik\n\nYou can control which ingress Tr\u00e6fik cares about by using the kubernetes.io/ingress.class annotation. By default if the annotation is not set at all Tr\u00e6fik will include the ingress. If the annotation is set to anything other than traefik or a blank string Tr\u00e6fik will ignore it.\n\nand nginx https://github.com/kubernetes/ingress/tree/master/controllers/nginx\n\nSetting the annotation kubernetes.io/ingress.class to any value other than \"nginx\" or the empty string, will force the NGINX Ingress controller to ignore your Ingress. Do this if you wish to use one of the other Ingress controllers at the same time as the NGINX controller.\n\n. looking at this line for another 10 seconds, I see there's a typo in nginx. This does not describe the behavior I expect to see. Linkerd IngressCache will add as many ingress resources as have correct annotations. Whether linkerd chooses to match to echo1 or echo2 is undefined in this case. This looks great, thanks for all the updates!. nit: spaces. thanks, this has been bothering me. (I think you'll need to recompile the templates in order for this change to take effect). Sure! I will add a test. I agree with you, not sure why I'm tiptoeing around toIterator like a sacred cow. Oh figured it out, we need to return the path that matches, not the whole matching spec. We could do it in 2 phases, but we'd like to be as efficient as possible on the request path.. I think we want \"as a path with port followed by host\". I tried doing this but it only seemed to increase the number of updates. Now they happen on every request instead of every file change, so I am going to revert it. Not sure what's going wrong there. Given the only differences between the TPRs and CRDs in k8s/src/main/scala/io/buoyant/k8s/resources.scala is the name of the classes/objects, I'd argue that the upgraded version is backwards compatible. (lmk if I'm missing something). ",
    "olix0r": "looks good! :star: \n. :star: \n. I've added a basic sanity-check test.  In doing so, I realized that we can make authTokenFile optional altogether (which I believe helps when using kubectl proxy).\nFurthermore, io.buoyant.k8s.AuthFilter is now shared between both k8s and linkerd-namer-k8s, which satisfies the original problem.\nOnce I get an ack on this, I'll rebase with an overhauled commit message (and I'll probably drop the Problem/Solution format).\n. no code change; just a history cleanup\n. :+1: :lock:\n. tioli: consider grouping with objects.  e.g.\n``` scala\nobject LinkerdProject {\n  ... // setup\nobject Router {\n    val core = standardProject(\"router-core\")...\n    val http = standadrProject(\"router-http\")\n      .dependsOn(core)...\n    ...\n    val all = standardProject(\"router\")...\n  }\nobject Linkerd {\n    val core = standardProject(\"linkerd-core\")\n      .dependsOn(Router.core)...\nobject Namer {\n  val k8s = ...\n  ...\n  val all = standardProject(\"router\")...\n}\n\nobject Protocol {\n  ...\n}\n\n}\n}\n```\nThis allows editors to fold sections, and is just sort of aesthetically satisfying.\nAlso, I'm fairly sure that we can avoid using lazy vals now (we couldn't in build.sbt).  lazy vals can be chained in unsafe ways that vals cannot be:\n```\nscala> object Foo { lazy val bar: String = bah ; lazy val bah: String = bar }\ndefined object Foo\nscala> Foo.bar\njava.lang.StackOverflowError\n```\nvs\nscala> object Foo { val bar: String = bah ; val bah: String = bar }\n<console>:10: warning: Reference to uninitialized value bah\n       object Foo { val bar: String = bah ; val bah: String = bar }\n                                      ^\nerror: No warnings can be incurred under -Xfatal-warnings.\n. :+1: \n. :+1: \n. Looks good! Before merging, please rebase the formatting fixes into the original. For example, by git rebase -i master and then changing the command to 'fixup' on the formatting commit.\n. :+1: looking good! thanks alex\n. In fixing e2e testing, I found a small issue with tracer and stats initialization.  It doesn't have much bearing on actual configuration (yet), but I'd like to fix this in a separate branch first before pursuing this branch further.\n. @adleong if you go to files changed and click on the \"16 files changed\" link, you'll see a per-file breakdown.  I don't think anything you see in the diff is a rename, besides Headers (which changes the prefix from Buoy to l5d).\n. Headers was a rename, but since the contents changed (header prefix changed), github lost the detail\n. rebased against master (to pick up other changes)\n. :+1: \n. Actually, would you mind expanding the commit message a bit with a brief (1 paragraph) description of the feature?  Sorry to catch this late.\n. :star: :star: :star: \n. :star: myself\n. discarding in favor of #18\n. you're right, that should be fixed to only produce one artifact per config.\nre: these specific configs, this is mostly a demonstration of being able to support >1 build configuration.  I don't know if it actually makes sense to distribute multiple configs (or even to keep them for now), but it's a step in that direction.  Ideally, I'd like to be able to do a build like ./sbt 'set bundles in linkerdProtocolThrift := false' bundle or something (to be able to have even finer control over these things).\n. :star: \n. @siggy I'd like to move forward with this independently of #16, since we need stats receivers to become sane to complete #6.  I don't think that there are actually any dependencies we care about between these issues, so they should be able to happen in more-or-less parallel.\n. :star: \n. This looks like a good start!\nYou'll have to figure out to generate a self-signed cert with the openssl commandline (i've done this before but not frequently enough to recall how to do it); and you should be able to point curl at the same certificate to get it to validate.\nIt would be great if we could get an end-to-end test to verify this behavior (especially after https://github.com/BuoyantIO/linkerd/pull/17 lands, when the http e2e stuff moves into linkerd-protocol-http).  In the meantime, I'm happy with a spot-test. Furthermore, it would be a great exercise to try to get this running with something like let's encrypt.\n. I believe that we could move this logic into Router?  I don't think there's anything HTTP-specific about what you've posted, is there?\n. great, @adleong! would you mind sharing the commands you ran so it's easy to reproduce? i can take a look at validation.\n. Ooookay.  i've finally confirmed that this works in https://github.com/BuoyantIO/linkerd/compare/ver/tls-e2e?expand=1\nI think this branch is good to go after a rebase and fixing one nit\n. The world is a vampire.\n. btw, part of me thinks it's Just Fine to target modern auto-updating browsers.  If someone want to live in the stone age (or somehow? use IE) there are endpoints for all of the raw data.  I'll change all of this stuff to use ugly old javascript, but I don't think we should feel toooo beholden to luddites. \n. Can we at least rely on ES5.1 (which does have support from all major browsers)?  Specifically for things like Array.prototype.reduce.\n. (still need to fix slashes in client ids)\n. Blegh, this change has gotten bigger than I hoped.  Feel free to push back to break this into smaller changes.  I like having routers.js in its own file, but if I've made summary.js less clear, i'm happy to revert summary.js and re-integrate the routers.js changes in a more targeted way.  That said, I do prefer having all of the scheduling (setInterval) logic controlled in one place rather than having dom initialization tied to updates.\nDisclaimer: I'm still very much learning javascript, so I'm keen on getting feedback on why one approach is preferred/not.  Reorganizing things is a big part of how I learn, so take this only as that.\n. rebase :star: (sorry, caught something incorrect in the commit message)\n. :star: awesome!\n. I'll take a stab at unit tests, but they may be more trouble than they're worth.\nWe'll definitely need e2e tests, but we'll want to push off on that until this has gotten a little further (i.e. server support #17, and subsequent config integration).\n. I'd like to defer tests to a followup branch, so that we can leverage io.buoyant.linkerd.protocol.HttpsIntegrationTest\n. :star: rebase\n. thanks for catching that, @adleong \n:star: rebase fix commit message.\n. tests for config parsing?\n. We should add docs, but I'm okay doing this in a followup.  I'll wanted to give all the docs a once-over while prepping for the release, anyway.\n. thanks for humoring me!  Give this a rebase, and a brief overview of the feature in the final commit message, and it'll be good to go.\n. :star: \n. :+1: \n. :star: \n. This doesn't seem unreasonable.\nHowever, I really want to move (soon) to a model where we have something like val admin: Service[Request, Response] = AdminWeb(linker); and so it Should simpler to just pass linkers around where we need to.  (Although I agree it's too cumbersome in the existing setup).  This isn't a blocker or anything, just trying to provide fuller context.\n. :star: \n. :star: \n. @esbie yeah that's all good feedback. I had thought a bit about supporting VERSION/METHOD/HOST but what happens if it's an http/1.0 request and there's no host?  I think you're onto something better, but there are some details to iron out...\n. We may want to abandon this in favor of io.l5d.rewrite provided in #65\n. instead of solving this as a Namer, we should accomplish this with https://github.com/twitter/finagle/pull/472 and/or https://github.com/BuoyantIO/linkerd/issues/120\n. I think all of the initialization (startServer) logic should be outside of service itself.  AdminService should only have to know about serving http requests and have no idea about where that service is configured to run.\nAs described in #16, I think there should be an admin block and not a single flag.  Eventually, we'll want to allow others to extend the admin service via plugins (i.e. to provide company-specific features); and so at some point we'll want to support more than one kind of admin server.  I wouldn't try to add any of the flexibility now since we don't understand how it will integrate, exactly; but we should carve out space for future extension.\nTo these ends, I'd prefer to see an AdminInitializer that is distinct from the AdminService so that configuration & initialization is decoupled from HTTP serving logic.\n. Okay! This is starting to look really great.\nI think we can simplify this still, but I don't mind if that happens in a followup.  So this can ship :+1:, but I think it should ultimately be a bit different:\nIt seems that we need the split between AdminInitializer and Admin because linkerd-core doesn't (and shouldn't) depend on linkerd-admin.  So linkerd-admin needs all of the startup logic, but the configuration primitives have to be in linkerd-core.\nI'd like to move to a regime where linkerd-core has an abstract AdminInitializer plugin trait, and we can support arbitrary admin interfaces.  This could even allow us to split admin-api from admin-ui -- or allow admin-ui to operate in a mode where it includes admin-api as a library.\nWe shouldn't carve any of that up yet, but I think it is worthwhile to establish a proper Admin api disjoint from its protocol implementation (in router-core, that supports router management, etc).  This is how we will (later) support dynamic configuration).  I'll elaborate in another ticket.\n. :star: \n. :star: \n. This is done! #368 \n. thanks for taking this on!\n. :star: :+1: \n. :star: \n. I didn't think it was really appropriate for CONTRIBUTING to include information about sbt or the details of writing tests.  I think that file should just be a high-level overview of what to expect to get patches accepted.  I think it's preferable to have a separate developer guide so that we can go into much more detail on these sorts of details.\n. :star: :surfer: \n. How much time does this add to our CI job?\n. :+1: lgtm\n. this looks like a good approach!  tests need updating...\n. We want a way to introduce new and unproven features for evaluation.  We'd like to require some form of acknowledgement from the user to enable these features so that it's easy to distinguish these modules. Furthermore, this should be accomplished in a way so that users are not required to alter their configurations when the features are no longer considered experimental.\nCurrently, modules such as the consul namer have experimental in their names, such as io.l5d.experimental.consul and a default prefix like io.l5d.consul.  The module name includes experimental as a form of acknowledgement, but the prefix was intended to be stable so that it would not have to change once the module is no longer experimental.  This is confusing, and isn't as future-proof as we'd like\nThis should be fixed as follows:\n- module names are no longer placed in an 'experimental' namespace.  io.l5d.experimental.consul becomes io.l5d.consul\n- modules support an optionally-specified experimental field.  A module may fail to initialize if this field is not specified.\nWhile we're here, we should consider experimental features within a module.  We have a few options in how to structure this:\nyaml\nkind: io.l5d.consul\nexperimental: true\nyaml\nkind: io.l5d.consul\nexperimentalSomeFeature: true\nOr, we could allow experimental to be either boolean or a feature map\nyaml\nkind: io.l5d.consul\nexperimental: true\nyaml\nkind: io.l5d.consul\nexperimental:\n  someFeature: true\n. my issue with the existing dtabs is that there's no explanation of what they accomplish.  okay to push it to another review if that actually happens\n. This sets up docker support: https://github.com/BuoyantIO/linkerd/compare/ver/assemble-exec...ver/docker?expand=1\n. Waiting for my readme change to merge so i have somewhere to document it... ;)\n. :star: rebased history\n. It is known & intended that this will break all prior linkerd-specific tracing configurations\n. I took a bunch of notes while I read through it.  I still don't have 100% of the context, and I'm curious if parts of this could be simplified further but I don't have enough context to make suggestions.\n- I think linkerd-config should just be merged into linkerd-core.  I don't think it makes any sense to ever depend on one and not the other?\n- I don't think Parser belongs in core/config; and jackson-yaml should not be a dependency.  None of the core code should care about data formats, and we should be able to add data formats without adding dependencies to all plugins. \n- My initial impression of cats is that it's pretty opaque.  I'm curious if it would be more readable (at the expense of being somewhat more verbose) without.  That said, I'm curious how you feel having played with it for a few days now.\n  - As a module author, how much would I have to interact with cats?  It looks like it's at least hidden... which brings me to:\n- It would be helpful to mark everything that' doesn't have to be used elsewhere as private[io.buoyant.linkerd.config]. It seems like there are a lot of intermediary types that don't need to be exposed, and restricting their visibility would help me.\n- For things like baseDtab, it seems like we could just read them as Dtab by exposing a Jackson module that supports custom de/serializers, eliminating the need to do validation on dtabs.\nAlso, several (non-comprehensive) style nits:\n- Break lines @ 100 chars\n- if an anonymous function uses braces, its params should be named (and not _).  E.g. filter(_.nonEmpty) or filter { t => t.nonEmpty } and not filter { _.nonEmpty }.  (especially relevant when functions are multiline)\n- generally prefer all imports at the top of the file\nFinally, there's one part of this that I'm still stuck on -- default values.  Linkerd configs have very few values that actually must be specified.  The vast majority of these values are defaulted on the stack (e.g. DefaultPool, tcp options, timeouts, etc etc).  I don't particularly like the model where we have to build a validated config that specifies all possible values defaulted -- I'd prefer that we treat most of these configuration-specified values as overrides that we have to put on the stack and that we do not try to specify them on the stack otherwise.   Basically, we should assume that The Stack may just decide on some of these values and we can't know about them at config-time unless they've been specified in the config.\nPer Alex's suggestion, I think that if we want to materialize runtime configuration for the user, we should do this from the stack (i.e. registration), as stack modules may alter configuration at runtime.  But we should definitely maintain a way of round-tripping a configuration without materializing its defaults.\n. Summarizing an offline discussion:\nThe split between Config, Defaulted, and Validated types is hard to understand in the context of the code, and it feels like there's duplication as a result of this. This split is necessary primarily because of having several types of \"defaults\" -- default configuration parameters that belong to the stack, as well as default servers, etc.  Furthermore, some of the post-parsing validation logic could be pushed down into a jackson deserializer to alleviate the need for validated types.\n@adleong suggested that removing these types of defaulting could drastically simplify things, while also making the configuration more explicit (i.e. less surprising -- esp wrt default servers).\n@gtcampbell is going to continue with this branch and attempt simplify this further.\nI'm excited about it ;)\n. :star: rebase history\n. :star: rebase history again\n. :star: rebased\n. whoa i feel like i understand what everything does here!  totally juice :tangerine: :star: \n. Should we omit the \"NAMER CLIENTS\" section when there are no namer clients? it's odd when it's empty.\n. I've split out per-service validation into #64 so that establishing base functionality (no validation + static name validation) can be done independently of figuring out all of the matcher stuff.\n. It occurs to me that we don't need a special static tls client configuration, as this is just a special case of per-service validation:\nyaml\n- kind: dstRewrite\n  matches:\n  - prefix: /\n    dst: linkerd.io\n(i still haven't found a great name for this module)\nSee #65 for more details, but this should be effectively the same as a static configuration.  I'm not sure if it makes sense to provide 2 modules for this or not...\n. kind on the protocol was just an oversight.  It should be protocol.  I don't think we should change it.  I think the TLS field should be called kind or module or something\n. todo: docs\n. Yeah, I agree.  I think * is ambiguous. what does it refer to all numbers greater than the greatest positional argument?  if i do \"/$3/$2/*\" things are .. odd.\nBut you're right that we need a way to do ranges or wildcards.\nPerhaps ${1..3} and ${4*}?\nMaybe this doesn't really make sense as a Namer...\n. Not necessarily.  The intent was to make this feel vaguely awkish (where positional params are referred to as $1 $2 $3 etc).  I think introducing any other syntax for this would feel weird.\n. But as this is, it wouldn't support rewrites to names like /$/inet -- so this buggy at the least\n. My takeway from this is that PathMatcher is (i think?) exactly what we want for #64, but probably not a great fit for a general purpose namer.\n. @esbie for tls, we care about bound ids (Name.Bound.id) and not unbound paths (Dst.Path).  Because TLS establishes authenticity on the connection and not the request, we want to rely on the cluster/service-discovery naming and not on the logical rpc level (which is where we have a predictable-ish http-specific naming scheme).\n. also, doing this in a generic name-focused way means that (i think) we get mux+tls support for free\n. @adleong i agree, that's a much easier way to think about it.\nPerhaps the following:\n- * is used as wildcard to match any single path segment ($_ is a valid path segment ;/)\n- we use some sort of other other non-valid-path character to denote capturing, e.g.:\n  - /http/1.1/*/{host}\n  - /http/1.1/*/(host)\n  - /http/1.1/*/host- `/http/1.1/*/<host>`\n- destination expressions probably just use the same syntax as the matcher, like:\n  - `{host}.buoyant.io`\n  - `(host).buoyant.io`\n  -host.buoyant.io\n  - <host>.buoyant.io\n. that's an interesting idea, there are some details to work out:\n- io.l5d.k8s is a namer type.  Its default prefix is also io.l5d.k8s but we shouldn't have a scheme that only works with defaults.\n- A prefix may match multiple namers (of different namer types).\n- We need to match on the prefix of a specific namer.\nSo let's say we have a config like:\n``` yaml\nnamers:\n- kind: io.l5d.k8s\n  prefix: /local\n- kind: io.l5d.k8s\n  master: ...\n  prefix: /dc2\nrouters:\n- kind: http\n  baseDtab: |\n    /zone => /local | /dc2;\n    /env => /zone/prod\n    /web => /env/http;\n    /http/1.1/GET => /web;\n  tls:\n    kind: boundMatch\n    dst: ${...service...}\n```\nWhen we're processing a name like /local/prod/http/users or /dc2/prod/http/users and we want to extract users, how would we determine that?  Furthermore, we can't (or shouldn't) rely on a 1:1 mapping between prefixes and namer kinds -- there could be more than one namer with the prefix /local.\nI suppose we could impose such a restriction--that all namers of a given type must share a prefix.  I'm not sure what the implications of that are.  I'll think about this more.\n. Actually, I don't think there's any way to support namer-aware rewriting and also support arbitrary user-provided prefixes.  So the question is how do we support:\n- multiple instances of a kind of namer\n- that are disambiguated for the purposes of routing\n- that are uniform enough to be parsed predictably\nI suppose if (1) namers are prefixed reliably with the namer kind and (2) multiple namers of a kind do not share overlapping prefixes, then we would be able to disambiguate paths.  This is getting complicated, though.\n. :star: rebase with typo fix\n. :star: rebased to pick up #67 \n. No pressing need for this.\n. :star: \n. lgtm :+1: \n. Excellent work, @adleong. This looks great.  A minor quibble abut naming i'd like to resolve, but modulo that: :star: \n. rebase, write a nice description, and good to go ;)\n. my name is oliver and i endorse this branch :+1: \n. i'd prefer client because in my mind plural \"clients\" means it's a list\n. looks like that one comment is wrong but otherwise\n\n. we have an acceptance test script \n. agree completely. started to sketch this out in https://github.com/BuoyantIO/linkerd/pull/26, but it needs some more thought/work\n. Right, but this means our dtabs are no longer dtabs, can't be used in finagle, etc.  So we'd be effectively forking the dtab syntax.  It's not off the table, but not my first option.  If we were going to go this approach, which I do think is much nicer, I'd prefer to try to alter the dtab syntax to support this upstream.\n. If we are going to support it, we'd want to be support this for dtab (local) propagation as well (otherwise there are multiple syntaxes the user must know about in different contexts).  Currently, we rely on finagle to do all of this dtab context management for us (i.e. de/serializing Dtab-local headers and setting them on the context).  We'd have to replace that logic with something that doesn't use Dtab-local headers, otherwise finagle services won't work with these dtabs.\nAll this said, I think we should try to submit this as a change to the Dtab syntax.  If that doesn't work, we can evaluate other ways of doing things -- but it seems like a great change to Dtabs.\n. A first attempt of this posted at https://github.com/twitter/finagle/pull/472\n. @jimmidyson if you run linkerd with the -log.level=DEBUG and -com.twitter.finagle.tracing.debugTrace=true flags, your logs should be a bit more verbose.\nIf you can grab a snapshot of the stats in on :9990/admin/metrics.json when the process is in this state, that would be helpful, too.\nAlso, we'll be better able to help if you share your linkerd config (especially your dtab) -- this is a common source of confusion.\nFinally, would you mind explaining how this state arises?  What version of kubernetes you're running, etc?\nThanks!\n. :heart_eyes_cat: :star: \n. When rebasing, please explain in the commit message that this breaks config compatibility. :)\n. I don't think this is really tractable as-is.\n. this is looking good! rebase and it's :+1: :100: \n. very nice! thanks for taking time to dig into this. :star: \n. :star: \n. doubleplusawesome! :star: \n. :star: \n. :star: nice find!\nI'm curious if it would be possible to test (I.e. on a test of DstBindingFactory).  Not critical.\n. Oh, also, squash these commits (i.e. with git rebase -i master before merging).\nIt would be nice if the commit message wasn't \"Investigate incorrect hosts showing on error: caused by caching\".  The change doesn't investigate anything, it fixes it ;)\n. :star: once rebased :dango: \n. :star: :heart: \n. :star: :star: :star: :crying_cat_face: \n. :star: :expletives:\n. Specifically, the /admin endpoint is useless.\n. We could extend the telemeter API to support allow metrics.\n. Metrics browser UI is dead.\n. I don't know if this warrants its own file, but I wanted to write this stuff down somewhere.\n. pull master, rebase, and :shipit: \n. :star: rebase and good to go\n. WONTFIX\n. I think I'd prefer that we wrap CHANGES.md at 80 or 100 chars...  not a real blocker :+1: \n. :star: i like it! just style nits\ntioli: it'd be nice to have a slightly more descriptive commit message (a sentence or two), but not a hard requirement\n\n. Fixed by https://github.com/BuoyantIO/linkerd/pull/168 (right?)\n. note: typo in commit messages \"Explose\" which is a cool word but probably not what you meant ;)\n. :star: this looks good!  my main beef is that there are a bunch of super-long lines that should be wrapped at 100 chars.  in no way a blocker\n\n. this looks great, @liamstewart. thanks for sharing.  once you've updated config.md, would you mind rebasing this into one commit with a commit message that describes the motivations?  Nothing fancy, just a sentence or two saying that you're adding support for thrift's compat protocol.\n\u00a1muchos gracias!\n. :+1: thanks so much, @liamstewart. merging this now.\n. :star: \n\n. Thanks for reporting this!  That's a neat fix, but I think we're inclined to just remove the wrapper script in favor of platform-specific rc/init controllers.  I've opened https://github.com/BuoyantIO/linkerd/issues/118 to track that.\n. In the 0.2.0 release, we removed the linkerd wrapper script in favor of an executable that simply runs in the foreground.\n. We added eslint to CI, but we still probably want better unit testing\n. :star: :+1: \n\n. this looks good! needs a rebase with changes to namer initialization\n. :star: thanks for cleaning this up! looking good\n\n. looking good! i think we have to fix the withParams thing, though, or we'll break the kubernetes namer\n. :star: myself (trivial)\n\n. Very nice.  Is it possible to verify this in the end-to-end test?  If it seems like a bunch of work, feel free to push back (in favor of getting this in today's release).\n. :star: tioli: commit message: Update documentation to describe marathon group naming or something\n\n. :star: after rebase\n\n. Would you mind updating config.md as well?  We mention default ports several times in that doc.\n. :star: after rebase\n\n. rebased\n. :star: \n\n. :star: \n\n. Going to hold off on documenting this until there is a concrete implementation.  config.md should be a reference, and there's nothing to reference at this point\n. :star: nice relaxing vacation work!\n\n. other implementations don't really exist yet (i've got a branch cobbled together for demo purposes, but nothing ready to share). i expect that these implementations will land in this repo in the next ~month.\n. status: i've rebased this down to 2 primary commits. it still needs some docs improvements and better testing.  If someone feels like picking this up and taking it over the line... by all means, please do.  Otherwise I'll get to this as time permits.\n. fwiw, i don't think it makes sense to be able to shorten \"io.l5d.k8s\" to k8s -- this is just some configured prefix name in a namer.  however, it would be very nice to be able to at least split apart outgoing from io.l5d.k8s/prod/incoming/foo/requests\n. note: i would prefer if users don't have to specify fully-qualified paths.  I'd love it if each protocol could have a default identifier, and others could be provided.  Not a hard requirement but io.l5d.identifier.basicHttp is a bit verbose when we're already in the identifier field in the http protocol.\n. This is looking good! just a few comments around naming/clarity\n. looks good! just some small style nits.\n. :star: i think we should ship this, but I'd like to understand one thing before doing so (to determine if there needs to be followup work): are watches cached/shared across namer lookups?  Specifically, if we resolve two names:\n1. /foo/bar/bah/baz => Bound(/foo/bar, /bah/baz)\n2. /foo/bar/bah => Bound(/foo/bar, /bah)\nwhat zk requests are required to satisfy this?  Ultimately, if we have an active watch on /foo/bar, the second lookup should be ~free.\nAs I said, I'm fine merging this without this optimization, but we should understand what's going on...\n\n. :star: \n\n. :star: \n\n. I'd be inclined to use D3's color scales; although they also reference ColorBrewer (which I hadn't seen before, but looks like a nice library).\n. Do we care that a hard refresh is required to show stats for new clients?  I think the client box appears, but no traffic in the top graph.  May be a bikeshed for another day...\n. Also I tested this in Safari and it all seemed to work\n. @esbie ah, yup. sorry about that.  looks good! even in safari.\n. :star: \n\n. couple nits but I'm generally happy with those tests.  Once you're happy with it rebase the test commits down (so that this branch is just one commit from each of us) and we should be good to merge\n. :star: \n\n. There's no difference to the end user -- but really this is something that should be a default in finagle.  If someone were to use the Router type directly as a library -- which is a reasonable expectation -- they should get this default behavior.\n. To test the UI changes, I ran:\n:; ./sbt examples/marathon:run\nand then went to localhost:9990/delegator?router=http#%2Fhttp%2F1.1%2FGET%2Fdog\n. :star: \n\n. It appears that netty/netty#4503 has made it into Netty-4.1.0.CR5. Finagle's next release will support 4.1.0.CR7. Furthermore, the finagle-http2 package is introduced with the beginnings of http/2 support.\n. There's experimental support for h2c (cleartext http/2) on the ver/netty4 branch.\n```\n:; /usr/local/bin/curl -svH 'Host: default' -o/dev/null --http2 localhost:4142     \n Rebuilt URL to: localhost:4142/\n   Trying 127.0.0.1...\n* Connected to localhost (127.0.0.1) port 4142 (#0)\n\nGET / HTTP/1.1\nHost: default\nUser-Agent: curl/7.48.0\nAccept: /\nConnection: Upgrade, HTTP2-Settings\nUpgrade: h2c\nHTTP2-Settings: AAMAAABkAAQAAP__\n< HTTP/1.1 101 Switching Protocols\n< connection: upgrade\n< upgrade: h2c\n< content-length: 0\n Received 101\n Using HTTP2, server supports multi-use\n Connection state changed (HTTP/2 confirmed)\n TCP_NODELAY set\n Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0\n Connection state changed (MAX_CONCURRENT_STREAMS updated)!\n< HTTP/2.0 200\n< content-type:text/html;charset=UTF-8\n< content-length:2877\n< l5d-dst-bound:/#/io.l5d.fs/default\n< l5d-dst-path:/http/1.1/GET/default\n< \n{ [2877 bytes data]\n* Connection #0 to host localhost left intact\n```\n. @stevej and I have been slowly making progress on linkerd's HTTP/2 support.\n\nCurrently, we are observing the following issues with http2 servers:\nHTTP/2 responses frequently fail:\n```\n   Trying ::1...\n connect to ::1 port 4142 failed: Connection refused\n   Trying 127.0.0.1...\n Connected to localhost (127.0.0.1) port 4142 (#0)\n\nGET /admin/ping HTTP/1.1\nHost: localhost:4142\nUser-Agent: curl/7.49.0\nAccept: /\nConnection: Upgrade, HTTP2-Settings\nUpgrade: h2c\nHTTP2-Settings: AAMAAABkAAQAAP__\n< HTTP/1.1 101 Switching Protocols\n< connection: upgrade\n< upgrade: h2c\n< content-length: 0\n Received 101\n Using HTTP2, server supports multi-use\n Connection state changed (HTTP/2 confirmed)\n TCP_NODELAY set\n Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0\n Connection state changed (MAX_CONCURRENT_STREAMS updated)!\n HTTP/2 stream 1 was not closed cleanly: CANCEL (err 8)\n Connection #0 to host localhost left intact\n```\n\nIn linkerd, we see logs like:\nI 0622 23:37:32.339 THREAD71 io.netty.handler.codec.http2.Http2FrameLogger.log: \n----------------OUTBOUND--------------------\n[id: 0x4d96fd14, L:/127.0.0.1:4142 - R:/127.0.0.1:57569] RST_STREAM: streamId=1, errorCode=8\n------------------------------------\nE 0622 23:37:32.341 THREAD71: channelReadComplete: ChannelHandlerContext(finagleChannelTransport, [id: 0x02c2682d, L:/127.0.0.1:4142 - R:/127.0.0.1:57569])\nW 0622 23:37:32.345 THREAD71 io.netty.channel.AbstractChannelHandlerContext.notifyOutboundHandlerException: Failed to fail the promise because it's done already: DefaultChannelPromise@59fab9dd(failure: io.netty.util.IllegalReferenceCountException: refCnt: 0)\nio.netty.util.IllegalReferenceCountException: refCnt: 0, decrement: 1\n    at io.netty.buffer.AbstractReferenceCountedByteBuf.release(AbstractReferenceCountedByteBuf.java:111)\n    at io.netty.buffer.AbstractDerivedByteBuf.release(AbstractDerivedByteBuf.java:65)\n    at io.netty.handler.codec.http2.DefaultHttp2DataFrame.release(DefaultHttp2DataFrame.java:129)\n    at io.netty.util.ReferenceCountUtil.release(ReferenceCountUtil.java:84)\n    at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:794)\n    at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1271)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:724)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:716)\n    at io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:802)\n    at io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:709)\n    at io.netty.channel.ChannelDuplexHandler.write(ChannelDuplexHandler.java:106)\n    at com.twitter.finagle.netty4.channel.ChannelStatsHandler.write(ChannelStatsHandler.scala:79)\nFurthermore \"pure\" (no prior knowledge) HTTP/2 requests are simply not processed by the server:\n```\n:; /usr/local/bin/curl --http2-prior-knowledge -vs localhost:4142/admin/ping\n   Trying ::1...\n connect to ::1 port 4142 failed: Connection refused\n   Trying 127.0.0.1...\n Connected to localhost (127.0.0.1) port 4142 (#0)\n Using HTTP2, server supports multi-use\n Connection state changed (HTTP/2 confirmed)\n TCP_NODELAY set\n Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0\n* Using Stream ID: 1 (easy handle 0x7fae22004c00)\n\nGET /admin/ping HTTP/1.1\nHost: localhost:4142\nUser-Agent: curl/7.49.0\nAccept: /\n```\n\n(There is no logging emitted in linkerd, no stats are incremented, and it appears that these messages do not make it out of netty, even).\n. hey there, @samuel! that is indeed the plan. I think that once we get basic http2 support, grpc should basically Just Work through linkerd with minimal additional work.  We will probably end up building some additional grpc-specific utilities/configuration, as well.  But in the short term, we hope to nail down http2 support enough to allow grpc to be routed through linkerd.\n. I've spent the past few days investigating finagle-http2. While it's known that finagle-http2 is not yet production ready, netty's codec-http2 library appears to have most of the necessary APIs to support HTTP/2\nInitially, we want to implement an HTTP2 router in linkerd that is capable of routing\ngRPC messages. In order to provide the most basic level of gRPC transport, we need at least:\n- Cleartext, prior-knowledge HTTP/2 server (that does not require an HTTP/1 message to initiate an HTTP/2 connection)\n- Cleartext, prior-knowledge HTTP/2 client (that does not attempt to perform HTTP1-to-HTTP2 upgrading)\nIn order to satisfy these goals, we defer several requirements necessary for production-quality HTTP/2 support, including:\n- HTTP1-to-HTTP2 client and server upgrading\n- ALPN/TLS bootstrapping\n- PUSH_PROMISE support -- we do not need to accept push requests from servers.\n- Multiplexed clients -- it is sufficient to use a serial dispatcher (with multiple connections) as done with HTTP/1.\n- Streaming -- while gRPC does require support for streaming, we can defer this for now.\n- Trailers -- while gRPC does require support for trailers, we can defer this for now\nChanges\nThe ver/http2/mess finagle branch includes an altered finagle-http2 implementation.  The changes on this branch include:\n- Http2ServerInitializer now only installs Http2MultiplexCodec.\n  This changes the server to only accept pure HTTP2 requests and not\n  HTTP1 upgrade requests. Before, only HTTP1 upgrade requests were\n  supported.\n- Http2Transporter now does not attempt to send HTTP1 upgrade\n  requests. Before, only HTTP1 upgrade requests were supported, but\n  gRPC (at least, the go implementation) does not support HTTP1\n  upgrade requests.\n- Almost everything that happens on an http2 client or server is\n  logged now. Multiple times, even. Lots and lots of logs.\nlinkerd now accepts grpc requests and handily routes them using the io.l5d.path identifier:\nyaml\n- protocol: http\n  label: http2\n  identifier:\n    kind: io.l5d.path\n  baseDtab: |\n    /srv => /#/io.l5d.fs;\n    /http => /srv;\n  servers:\n  - port: 4142\n    ip: 0.0.0.0\n    engine:\n      kind: netty4\n      http2: true\n  client:\n    engine:\n      kind: netty4\n      http2: true\nHowever, the client side of finagle-http2 does not yet behave properly.  It appears that it does not properly initiate client connections:\n- The connection preface isn't sent by the finagle-http2 client, causing the grpc server to error\n  with\n2016/06/29 20:25:10 transport: http2Server.HandleStreams received bogus greeting from client: \"\\x00\\x00\\xb4\\x01%\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x0fD\\x8bb\\xbb\\x0e\\x93\\xaf\\xc8\\xf6I\"\nIt should be sending PRI * HTTP/2.0\\r\\n\\r\\nSM\\r\\n\\r\\n (or, 0x505249202a20485454502f322e300d0a0d0a534d0d0a0d0a)\nin netty-4.1 there are two APIs for building HTTP2 clients, the Http2Connection API, which is currently used by finagle:\n``` scala\n        val connection = new DefaultHttp2Connection(false /server/)\n    val inboundH2ToH1 = new InboundHttp2ToHttpAdapterBuilder(connection)\n      .maxContentLength(maxResponseSize)\n      .propagateSettings(true)\n      .build()\n\n    new HttpToHttp2ConnectionHandlerBuilder()\n      .connection(connection)\n      .initialSettings(new Http2Settings().pushEnabled(false))\n      .frameListener(inboundH2ToH1)\n      .frameLogger(new Http2FrameLogger(LogLevel.INFO))\n      .build()\n\n```\nAs well as a newer, preferred but less mature Http2MultiplexCodec.  Unfortunately, there doesn't appear to be a good way to perform HttpObject<->Http2Frame object translation with this API, so it is difficult to just drop this into finagle-http2, since finagle-http2 still requires that a Transport[HttpObject, HttpObject] is exposed to the client stack.\nNext steps:\nFirst and foremost, we need to get a proper cleartext, prior-knowledge HTTP/2 client configured properly.\nFollowing that, we need to\n- Streaming -- requires that DATA streams are written to a Message's Reader\n- Multiplexing -- Requires replacing HttpClientDispatcher (a serial dispatcher) with a stream-aware dispatcher (as is done in Mux).\n- Trailers -- requires extending Message types to include something like a Future[Trailers] that is satisfied when a message is complete.\n- Servers should accept both Prior-Knowledge requests and HTTP1-upgrade requests.\n- Clients should initiate either HTTP1-upgrade or Prior-Knowledge requests (as controlled by a Param)\n- ALPN+TLS\nIt seems that it may be substantially easier to build out this functionality--especially multiplexing--by exposing an Transport[Http2Frame, Http2Frame] to an Http2-specific dispatcher.  It should be possible to map HTTP2 message types to com.twitter.finagle.http.Message types, but I'm not sure that this translation should happen on the netty channel.  It seems like this could be much easier to accomplish by translating types closer to the application code.\n. I'm happy to report that we have a variant of finagle-http2 that is able to transmit gRPC messages!\nI've modified an example application to use gRPC so that it works as follows:\n- a web service serves http/1 requests with chunked responses\n- a word service serves a simple gRPC service with a single endpoint.\n- a gen service serves a gRPC service with an endpoint that streams response data.\n- web is configured to connect its word and gen clients to a finagle server that routes requests to the appropriate backend.\n```\n:; curl -vs -o/dev/null 'localhost:8080/gob?limit=1000000'              \n   Trying ::1...\n Connected to localhost (::1) port 8080 (#0)\n\nGET /gob?limit=1000000 HTTP/1.1\nHost: localhost:8080\nUser-Agent: curl/7.49.0\nAccept: /\n< HTTP/1.1 200 OK\n< Content-Type: text/plain\n< Date: Thu, 14 Jul 2016 18:59:01 GMT\n< Transfer-Encoding: chunked\n< \n{ [3986 bytes data]\n* Connection #0 to host localhost left intact\n```\n\n(... and it works, streaming large responses even! \ud83c\udf89 )\nWhile this work is based on finagle-http2, it diverged substantially. We have moved away from HTTP1 types entirely. No attempts have been made to preserve compatibility with finagle-http.  This approach required very little work on the server-side, as netty's Http2Codec handles most of the complexity of framing and demultiplexing -- we simply provide a ServerStreamTransport (analogous to finagle-http's StreamTransport) that maps H2 stream frames into our Request types and our Response types into H2 stream frames.\nThe client side of this is a bit more complex, since, as @mosesn mentioned, netty's Http2MultiplexCodec doesn't yet satisfy any of these concerns...  So, we introduced a ClientDispatcher to compensate for this.  The client dispatcher is responsible for initializing and managing outbound client streams and mapping our Request types into H2 stream frames (and mapping received H2 stream frames into an existing stream's response.  Furthermore, ClientDispatcher manages flow control by pushing window updates into netty's Http2FrameCodec (which handles buffering and delivery of window updates).\nIt was necessary to patch netty with a fix for window updates; but once I moved finagle onto a snapshot version of netty, everything worked as hoped (so this is all predicated on finagle using Netty 4.1.3).\nThis an exciting start. There are almost certainly severe bugs lurking in my integration, but it's nearly at a point where we can start iterating on specific parts of this. In the coming days, I'd like to prep the ver/http2 branch so that I can start to share it more broadly for review.  Additionally, I'd like to get linkerd into a state where it's possible to configure an http2 router based on this codec.\nWe'll continue to update this issue as we progress.\n. In the past two weeks, we've made substantial progress on our H2 implementation including:\n- Simultaneous server support for H2C and HTTP1-to-HTTP2-upgrade\n- Initial burn-in/perf baseline testing/profiling\nIn order for this code to hit linkerd's master branch, we're waiting for the next finagle release, which includes Netty-4.1.3.Final.  Furthermore, I'd like to support non-prior-knowledge (i.e. H1-to-H2-upgrade) client support so that linkerd can communicate with non-grpc, non-linkerd servers.  Following that, we'll be focused on ALPN/TLS and general feature parity (the H2 codec doesn't yet support tracing/context propagation/etc).\nSummary: h2 works. we're close to merging it in linkerd. After that, we'll work with @mosesn and the finagle team to figure out the best path forward for graduating/merging our codec with that in finagle.\n. @mosesn I haven't worked on H1 downgrading yet.  I've moved away from the finagle request types, but I believe that we should be able to be bridge the difference from our (very abstract) request types to an implementation of c.t.f.h.Message.  We started on something that used Readers, but it turns out that Readers don't really model H2 streams as well as I'd like, so I've settled on a DataStream type that can be backed by a reader/writer, etc. (specifically, we want the ability to model a data frame with the EOS bit set -- something that Reader makes cumbersome).\nI've tested streaming support in my branch at length and feel fairly happy with the implementation I've arrived on.\nI'll be traveling next week, but when I get back it would be good to do a bit of a walkthrough and figure out how/if there's a path to merge codecs.\n. Please excuse the radio silence.  This feature is still under active development/testing.\nWe've primarily been waiting for linkerd to be able to use a new finagle release.  We found a show-stopper bug in 6.37, which we fixed, but not before 6.38.0 was released.  We're now waiting on a 6.39.0 release, but I don't have a timeline for that.\n@mosesn @kevinoliver @vkostyukov Can share any information about the 6.39.0 release timeline?\n. As we enter the first week of October, a brief update on the linkerd side of things:\n- Prior-knowledge h2c works well enough to start testing against gRPC (without TLS).  I've published a build against Finagle HEAD that can be used for testing as we prepare this for a merge into master (following the next finagle release).\nPlease check out this pre-release and give us early feedback. I expect (hope, even?) to find bugs here.\n- I've started to work on h2 (with TLS) support. Serverside TLS is working on that branch, with a few caveats. In working on this, I found a problem (h/t @moderation) with the way we configure netty4 server TLS.  I will fix this on master independently of the h2 work.\n- I figured out how to squelch annoying log4j warnings. This will be fixed on master as well.\nI think we're on track to merge nascent (explicitly experimental) h2 support as soon as finagle 6.39 is released (barring any quality issues in the release, but initial testing is promising).  There are a number of features that we'll want to follow up with after this release -- I'll enumerate these as we get closer to knowing what's going to make it in the next linkerd release.\n. @mosesn for what it's worth, i tried to use your most recent finagle-http2 with h2c support, but gRPC expects that trailers are supported (and afaict, trailers are not supported by finagle-http), and so gRPC responses just never complete.  But I definitely borrowed heavily from your work, so thanks again for pushing forward with it.\nIn addition to trailers, I think we really need to support on end-to-end flow control, which finagle is not currently positioned to provide.  We should definitely continue to talk, though, about how we can unify implementations over time.\nLet us know if there's anything we can do to help expedite a finagle release ;)  As I've noted before, we've been unable to get a stable upgrade since early July.\n. The ver/h2+h1up branch includes support for:\n- h2 server (via tls config)\n- h2 client (via tls config)\n- h2c server (both prior knowledge and http/1 upgrade, simultaneously without config)\n- h2c client with prior knowledge\nCleartext, http/1 upgrading clients are not yet supported, but this has no bearing on gRPC so this can land later.\nAdditionally, many of finagle-http's features---error handling, tracing, retries, per-request dtab overrides, etc---are not yet supported for linkerd's h2 protocol.  I'll continue work on feature parity.\nIn the meantime, I'll publish another pre-release with TLS support once I have a chance to write a README explaining the relevant configuration options.\n. I've published another pre-release off of this newest work: https://github.com/BuoyantIO/linkerd/releases/tag/h2-alpha-2\nPlease don't hesitate to get in touch with questions/bugs/etc\n. I've just published the h2-alpha-3 pre-release.  This includes support for tls, linkerd headers (like l5d-dtab), proper proxy support (so that http_proxy=... curl --http2 http://service/ works), and support for pluggable identifiers (the default, now, is to simply use the :authority header -- the io.l5d.h2.path identifier is useful for routing grpc requests).\nI think the big remaining tasks for linkerd's h2 implementation are to get parity for stats and tracing.  If we get a finagle release before that happens, we'll release h2 as-is and improve things in the next release.  (Though, it seems possible that this may be beat finagle 6.39 out the door).\n. Whoops, sorry for dropping the thread on this issue.\nlinkerd's h2 support is fully released in linkerd-0.8.4, the protocol is still marked as experimental.  \nThere are a number of enhancements planned for the future, but the basic features necessary for gRPC transport--both TLS and plaintext--work with no known bugs.\nFurthermore, I've begun work on a gRPC frontend over our h2 implementation so that it's easy to access/expose grpc services from linkerd and namerd.  The first use case for this will be to build a gRPC interface for namerd.  Work on this is progressing well.\nIn order to promote h2 to stable, we need your help battle-testing it.  If you run a linkerd with an h2 router, let us know!  If you can't run linkerd with an h2 router due to some bug or lacking feature, definitely let us know.. :star: just nits\n\n. :star: \n\n. :star: \n\n. :star: \n\n. hi @zhao141!\nI'm sorry you hit this error -- I think our example is flawed and we need to fix it.\nsh\nwhile true; do echo -e 'HTTP/1.1 200 OK\\n\\nIt works!' | nc -l 9999; done\ndoes not actually implement a valid http server -- it can send the response before the client sends a request.  Unfortunately, this causes an error in the http codec that we're not handling gracefully.\nIf you run a proper HTTP server like twistd -n web -p 9999, I expect that this error will go away.\nLet us know if this fixes it, and in the meantime we'll update the docs.\n. We updated the example at https://linkerd.io/doc/getting-started/#making-sure-it-works.  Please reopen this if you have any further issues.  Thanks!\n. :star: thanks, @esbie!\n\n. :star: small q, but lgtm\n\n. :star: yesssssssssssssssssssssssssssssssss!\n\n. Is this perhaps just as simple as specifying /dev/std{out,err} in the destination file?\n. thanks @endzyme.  closing this for now, but willing to reopen if it turns out that this doesn't work generically.\n. @adleong zookeeper itself gives us this for free, but we currently have some poor abstractions that hide it from us: The setData api returns a Stat that contains the node's version.\nWe'll likely have to move away from the finagle-serverset abstractions...\n. :star: \n\n. :-1: Unfortunately, I think we are going to have a lot of problems running this in CI.  It isn't really designed to be a good barometer for correctness -- it's prone to plenty of timing/raciness issues that will make this a poor indicator of regressions, especially in a CI environment with variable performance.\n. Also, I think we should replace the validator.sh script with sbt logic that ensures that the artifacts are built, etc...  We should only have scripts that call out to sbt if we really have to.\n. :star: :sparkles: \n\n. :star: \n\n. :star: lgtm, one tioli\n\n. :+1: looks good!\n\n. hi @pchojnacki, thanks for the feedback!  While I have operational experience with other service discovery backends like zookeeper and kubernetes, I have only used consul in our test environment.  So this sort of feedback is very helpful!\n\ndc - should be optional - as by default consul agent resolves queries to local \n\nWe include dc in the naming structure so that we can support multi-dc service routing.  For example, in the following dtab, we send 90% of traffic to dc1 and 10% of traffic to dc2:\n/srv => 9 * /io.l5d.consul/dc1 & 1 * /io.l5d.consul/dc2;\n/host => /srv;\n/method => /$/io.buoyant.http.anyMethodPfx/host;\n/http/1.1 =>  /method;\nAnd so in order to accomplish this, we want dc names to be part of the name structure.  If they are truly optional, it's difficult for us to distinguish service names and dc names.\nOne option would be to require a consul configuration block for every dc, e.g.:\nnamers:\n- kind: io.l5d.experimental.consul\n  host: 127.0.0.1\n  port: 2181\n  prefix /io.l5d.consul.local\n- kind: io.l5d.experimental.consul\n  host: 127.0.0.1\n  port: 2181\n  dc: dc2\n  prefix /io.l5d.consul.dc2\nAlternatively, perhaps we could give special treatment to local dc, so that dc does not need to be named explicitly in a Dtab like:\n/srv => /io.l5d.consul/local;\n/host => /srv;\n/method => /$/io.buoyant.http.anyMethodPfx/host;\n/http/1.1 =>  /method;\n\nyou should be able to query services by tags,\n\nThis is one place where I feel that consul's model does not mesh well with Dtabs.  I'm interested for input here.\nIn linkerd, names reflect some hierarchical address -- not a query.  Let's say that a service, birdsvc, has the tags: master, agent, green, & blue.  Since tags are not key-value -- we don't have role=[master|agent] and color=[green|blue] -- it's not clear how to encode these names into a path.  There is no obvious hierarchy.  \nIn our view--coming from other service discovery systems--all instances of a service should be able to accept traffic equally.  Instead of having a birdsvc with a master tag, we would opt to have a birdsvc-master service.  Basically, a service is a load balancing unit.\nWould you mind elaborating on how you lay out your services with tags?  It would be helpful to have some concrete cases in mind so that we can figure out a solution for this.\n\nI also have mixed feeling about linkerd using v1/catalog consul api for service discovery instead of '/health/service'\n\nThat's fair.  linkerd should be able to operate well without service health checks, since its load balancer is able to maintain its own view of what is healthy via latency measurements and circuit breaking. I think it's reasonable, though, to honor consul health-checking as well.\n\nAs I'm new to both scala and finagle (including dtabs) I might have missed something so excuse my ignorance.\n\nNo worries ;)  You shouldn't have to be familiar with Scala or Finagle to use linkerd.  And Dtabs take some getting used to...\nAgain, thanks for the feedback!\n. \ud83d\udc4d  lgtm (legal disclaimer: i don't have an immense amount of context here. but i can follow what this is doing)\n. The log line io.l5d.k8s/prod/incoming/books: name resolution is negative is logged from finagle's LoadBalancerFactory and indicates that the name is bound and the address is in the state Addr.Neg.\n. Update: I believe that this is not a bug in namerd but a bug in the kubernetes Namer: It appears as if namerd's resolution returns an Addr.Neg and that this negative resolution is provided to linkerd via namerd.\nMy working hypothesis is that the following occurs:\n1. service is deleted: its Var[Addr] is updated to Addr.Neg\n2. service is re-created: a new Var[Addr] is created with a Bound address.\nMy guess is that Service and Port creation logic needs to be changed to check for existing services.\n. So it seems that the k8s namer is actually doing the proper thing -- it sets its Addr vars to Neg and removes references to them when a service is deleted.  If it maintained references to deleted services, it would become complicated to determine whether a service exists or not (which is important for fallback, etc).\nThe problem is that namerd's AddrObserver cache keeps Addr observations open forever and has no mechanism to close it.\nMy proposed fix is the following:\n- stop caching addrs in the bind call.  This creates a purer separation at the cost of some extra work on some addr calls. (As we discovered earlier, we need to support addr on a name that has not been processed by bind previously).\n- When an addr lookup is not cached, bind its id first.  Use the NameTree state--when it's not a Leaf--to indicate when the addr observation should be torn down\nBranch to follow \n. @rmars is this still worth pursuing?\n. \u2b50 \n. \u2b50 \n. It occurs to me that the namerd annotations should be thrift-specific since there are other namerd servers.  Let's do that separately.\n. @adleong https://github.com/BuoyantIO/linkerd/pull/246/commits/bc3061625cb243e1601dda04196b819e61276125 alters addr resolution to skip delegation entirely so that we have a safeguard against user-provided dtabs interfering with addr binding.  This approach isn't set in stone -- let's weigh our options.\n. @esbie good point.  the thrift tests are slightly unwieldy but I'll at least give it a try.  maybe i can make them less unwieldy, even\n. simplified tests ThriftNamerInterface addr tests and added a test that properly reproduces this issue (i.e. fails as expected against master and passes on this branch).\n. style: let's cut line lengths somewhere around 100 chars (guideline, not a rule).  There are some quite long lines, particularly comments, that would be nice to shorten.\n. To recap IRL conversation: this is looking good. But we should give this a bit of a burn-in in a test cluster.\n. lgtm modulo those nits!\n. :star:\n. :star: :w00t:\n\n. :star: for what my review is worth, this looks very nice\n\n. will merge once 0.3.1 is tagged\n. :star: i'm a fan!\n\n. For what it's worth -- and we don't have to do this in this branch -- I'd prefer that the ZkDtabStore itself exist in the io.buoyant.namerd package.  We should use the com.twitter.finagle.serverset2.buoyant package to expose the raw utilities that we need to implement a DtabStore.  This is more aspirational than a hard requirement...\n. :star: looks like a good step in the right direction! no blockers -- i think any of my comments could be addressed in a followup branch\n\n. @adleong let's say we have two time windows: t0 and t1:\n- In t0, the process sends 1000 request and receives 500 responses.\n- In t1, the process sends 100 requests, and receives 600 responses.\n(Yes, this is a contrived example, but there's no reason this can't happen).\n. moved interpreter back out of linkerd, changed dependency to Namer.core instead of Linkerd.core\n. @siggy, I've fixed it so that validator/run just builds the needed assembly targets.  However, I'm pretty sure that your suggested change doesn't work.  If I have something like:\n.settings(\n       assembly <<= assembly in linkerd,\n      assembly <<= assembly in namerd\n)\nonly the last one is honored -- the namerd assembly target overrides the prior definition.  I'm inclined to just remove these commands from the root project.\n. \u2b50 \n. We'll probably have to add our own admin endpoint that accepts POST requests...  Otherwise we'll probably have to split this into multiple requests?\n. I'm betting that we'll hit the limit pretty quickly once we have several clients and long enough client names...  We'll need a more robust approach before this is ready for prime time.  Right?  The key names seem to chew up a lot of URI-real-estate\n. \u2b50 this fine, just some language nits\n. \u2b50 \n. :star: -- can/should this test run in CI?\n. \u2b50 \n. :star:, minor comments\n\n. \u2b50 \n. :star:\n\n. I think that this is good safeguard, but doesn't solve the underlying problem.  If a namer's backend isn't available, it shouldn't hang indefinitely -- it should return a NameTree.Neg so that another namer may be tried.\n. :star:\n\n. lgtm\n\n. I think we still need you to sign our Contributor License Agreement (as descrived in CONTRIBUTING) before we can merge this.  #lawyers\n. @benley so you have, i was looking at the wrong spreadsheet.  Thanks!\n. \u2b50  very nice! i definitely think we should add non-streaming variants of these apis, but i'm fine if that happens in another branch\n\n. I've added some logging on a branch and it's pretty clear that linkerd does not stop observing the original Var[Addr], which explains this behavior.  I suspect the difference between linkerd-only and namerd is the following:\n- When linkerd observes a namer (io.l5d.fs in this example), it originally gets a NameTree.Leaf(Name.Bound(addr0, /io.l5d.fs/foo)), which later turns into NameTree.Neg, and then NameTree.Leaf(Name.Bound(addr1, /io.l5d.fs/foo)).  addr0 is not the same object as addr1.  we start addr1 and go on our merry way (i have no idea what happens to addr0, maybe it stays around indefinitely?! this warrants further investigation).\n- When linkerd binds through namerd, linkerd originally gets a NameTree.Leaf(Name.Bound(addr0, /io.l5d.fs/foo)), which later turns into NameTree.Neg, and then NameTree.Leaf(Name.Bound(addr0, /io.l5d.fs/foo)).  This is because /io.l5d.fs/foo is already cached in the client.  This Var never receives an update from the server after the original Addr.Bound, presumably because the server has torn down the serverside observation and has stopped updating it.\nThat addr0 stays observed is fundamental here: if linkerd/finagle released the observation, this would recover properly.  Otherwise, we'd need to update the addr protocol to support the same observation-teardown logic that exists on the serverside.\n. The Addr observation is bound in finagle's load balancer factory (actually, the TrafficDistributor).  It is released when the factory is closed, which means that it won't get released until it's evicted by the ServiceFactoryCache (in DstBindingFactory).  It sounds like we'll need to extend the addr API to propagate \"disappeared\" info.\nAnother approach might be to generate special bound IDs that allow clients to differentiate states -- but this seems harder to get right.\n. Furthermore, the reason we had not observed this behavior in our tests (i.e. on kubernetes) is because when we restored the service, it was against the original set of addresses.  When this is the case, the cached Var[Addr] is okay because it provides the stale state -- but no further updates will be received by the client.\nWriting a test to reproduce this...\n. https://github.com/BuoyantIO/linkerd/commit/007e8a06c69ed9c0f89f31aacef4fdfe4f8aaf91 adds a test that reproduces this behavior\n. In kubernetes, this isn't a problem because we can rely on LoadBalancer Services to give us a single L3-loadbalancer so clients only maintain a single connection.  In other environments, we can use DNS RR, but this is still error prone and doesn't provide for resiliency.\nThe desired behavior is to accept a list of namerd endpoints but to only operate on a single endpoint at a time (i.e. as long as it's healthy).  This could be accomplished by:\n- using the newClient api to obtain a ServiceFactory instead of a Service. This would require the namerd client to recover connections after failures.\n- or, writing a LoadBalancer implementation that only chooses a single endpoint at a time (basically, pushing this logic down into the load balancer.\n- or, as I said earlier, moving to a streaming response implementation.  Either gRPC, a mux streaming implementation, maybe plain-old HTTP/1.1 streaming.\n@mosesn do you have any thoughts on how best to address this?  We don't want request-level load balancing nearly--we want to ensure that we have exactly one healthy connection to a logical service, similar to how a zookeeper client manages its connection to a zk cluster.\n. @mosesn aperture currently only supports a minSize. I suppose it could be modified to support a maxSize as well\n. @mosesn ah, thanks! i'll give that a shot.\n. will reopen if there's a real need to do this.\n. @adleong has a better approach to this in the works that doesn't rely on sending these sorts of updates over the wire... \n. @adleong \n1. an integration test with multiple namerd ports over a network -- i can try to put that together\n2. this should be okay.  when a connection is lost, the node should be marked down, and the client should retry on another endpoint.  at this point, the stamps likely won't match, and but once it receives the current stamps from the remote, it will resume watching properly.\n. @adleong i think that's right. if we wanted to be rigorous, we could move retries up out of the client and wipe the stamp on error before retrying... i think this is all justifying moving to a streaming api sooner than later.\n. https://github.com/BuoyantIO/linkerd/pull/362\n. We have the ability to do this (we have an etcd client), but we don't have any authoritative service registry scheme for etcd. We should open tickets for each of these schemes, if they exist.\n. This code is actually some of Buoyant's oldest code, so please do give this a thorough review (esp for style).\n. lgtm -- how do we test this? (seems like we'll want to vet this against the new EndToEndTest...)\n\n. :star:\n\n. Oh, the use case i had in mind was just simple non-streaming read -- not necessarily stamped polling (which sounds a bit more complex)\n. Yeah, I think guava caches are promising.  Although, I think our desired caching semantics are not quite \"LRU.\"  LRU will punish entries that are not updated frequently.  We want something closer to reference counted + LRU.\nLRU could work if we, for instance, force an update on pending requests when the entry is being evicted so that clients have an opportunity to re-establish the watch...\n. :star: good catch\n\n. The list of namespaces isn't updated by a watch. It's a simple cache of namespace observations.  Once we're observing a namespace, the list of services and ports can change dynamically, and so (i think) we need to be able to reflect those updates...  (I'm not saying it can't be simplified, just the motivation is that this is something which can be updated by the watch).\n. :star:\n\n. what changes on /api/1/delegate?  The dtab itself can't change...  I suppose we should reflect updates to service discovery?  (It does sound useful, but I'm good with deferring it)\n. There are a ton of examples in https://linkerd.io/doc/latest/config, fwiw.  Some  of them could use some clarification... maybe it's poor organization that they're buried in the individual config sections? Also there's a linkerd/examples directory with several example configurations.\n. :star:\n\n. What's the change in behavior? (This is probably where the Problem/Solution style commit message would be helpful ;)\n. I'm pretty sure that you'll have to update the cache immediately.  It may be workable to store an ADT in the cache instead -- something like:\nscala\nsealed trait NsState\nobject NsState {\n  case class Pending(future: Future[Ns]) extends NsState\n  case class Cached(ns: Ns) extends NsState\n  case class Failed(exception: Throwable, retry: Future[Ns]) extends NsState\n}\n. I'll need to give this another read through to get my head around it... it looks promising but I don't quite understand it yet.\n. :star:\n\n. Can you provide a brief description of what behavior is changing (i.e. in the commit message)?\n. Oh.. what is a \"value discarded warning\" is this some intellij thing? (turn that off?)\n. :star: After discussion, I'm good with this -- this will help catch subtle errors.  We'll get used to reading val _ =\n\n. :star:\n\n. :star:\n\n. I think this is basically resolved, as we have separate packages for admin, linkerd-admin, and namerd-admin.\n. So I understands, this changes the k8s namer so that any update to a namespace triggers updates to all observations on that namespace?  So if i'm observing default/http/web and there's and the default/http/users service is created, my original observation will receive an update?\n. :star:\n\n. \u2b50 \n\n. If we do this, we need to stop CI'ing jobs submitted from people outside of buoyant (otherwise, we expose our docker credentials to them...)\n. I don't think we need a separate script to do docker pushing.  I think we can simply use something like:\n./sbt linkerd/dockerBuildAndPush or ./sbt linkerd/dockerPush\n. \u2b50  looks good!\n\n. The l5d-ctx-deadline header is now used.\n. I'd prefer to cover docs for this in the context of https://github.com/BuoyantIO/linkerd/pull/364.  Since we can't actually talk about retries yet, all of these classifiers are functionally equivalent (and so there's not really anything to configure...)\n. :Star:\n. At the least, I think we could expose a counter that tracks the number of newly-minted trace ids.\n. :star: I do think we should use a base class for tests, but not a blocker\n\n. :star:\n\n. :star:\n\n. \u2b50 \ud83d\ude0d \n\n. duplicate of #198 \n. :star:\n. :star: i really like this! all of my comments are about clarity and not about the general approach\n\n. :star: thanks!\n\n. :shipit: thanks @topiaruss!\n\n. good idea!\nlooks like marathon does twitter commons' leader election (1 2) under /marathon/leader by default.\n. That's a neat idea.  We could simply implement a leader namer (there's nothing marathon-specific about it), and use that as a way to configure the marathon namer:\nyaml\nnamers:\n- kind:      io.l5d.experimental.marathon\n  dst: /$/io.l5d.zk.leader/zkhost:2181/marathon/leader\n...\n. I have a branch to add this functionality to Finagle: https://github.com/twitter/finagle/pull/504\nI'll temporarily duplicate this work in linkerd so we don't have to wait a full finagle release cycle.\n. lgtm, thanks!\n\n. We'll revisit this as-needed/requested.\n. It occurs to me that, for the purposes of linkerd, we probably want to move the requeue filter up the stack: Right now requeueing occurs on the same client. Given a situation like /host/foo => 1 * /srv/foo & 9 * /srv/bar where /srv/foo fails some portion of requests, we should be able to retry onto /srv/bar. Furthermore, in an ideal world, we'd perform load balancing, failure accrual, etc in the same fashion...\n. reminder to add configuration docs for https://github.com/BuoyantIO/linkerd/pull/339\n. @klingerf good call.  writing better tests for this showed that both min and max have to be greater than 0, so they're both required.\n. re: naming, i think we may want to call these namers (and announcers) io.l5d.zk.serverset -- serverset describes the scheme (esp the format registered in the nodes) -- there can be other schemes on zk.  (for instance, we will probably want to add io.l5d.zk.leader discovery for communicating with marathon).\n. The relationship between announcers and announce names seems likely to be a problem. Announcers may encode or require attributes specific to a registration scheme.  I don't think linkerd should attempt to provide a single abstraction over all possible registration schemes, and so we need a way to provide tighter coupling between announced names and the backend.\nI think, ultimately, this requires supporting typed announce names that are bound to types supported by configured announcers...\n. :star: I'm okay merging this as-is if we can communicate that this will almost certainly change soon...\n\n. That's more what I had in mind, yeah; though this requires that each announcer has its own client (or we try to maintain a cache of clients by... something (lik, their (type, dst)?)\nBut, yeah, that addresses my base concern around abstractions.\n. :star: this turned out great\n. Your understanding matches mine.  Currently load balancing does not factor into unions: name binding is distributed randomly/weighted (see c.t.f.factory.NameTreeFactory).\nWe can't really write a deterministic test but we can probably write a test with unions... We probably have two options:\n1. write a test that issues up to N-large requests until a retry hits the alternate\n2. write a test where we do the following:\n   1. Use a dtab like /req => /srv/a | /srv/b\n   2. issue request, which gets routed to a. don't respond\n   3. cause /srv/a to go neg\n   4. respond from a with a retryable failure\n   5. ensure that retry goes to b\nI think (1) is easier, though it's a lilttle \"sloppier\".\n. replaced by https://github.com/BuoyantIO/linkerd/pull/403\n. :star:\n\n. thanks, tony! would you mind signing the CLA (if you haven't)?\n. :star:\n\n. :star:\n\n. I'd be in favor of just removing the link.  Not sure how useful it is to get dumped to a huge inscrutable json blob\n. :star: you may want to add an Also-by: Vincent Chu <...@.....> note to the bottom of your commit message so he gets credit ;)\n\n. Unfortunately, I think this is a dangerous feature.  We have no control over what sort of data is in response bodies -- it could potentially contain passwords, credit card numbers, health information, etc.  Instead, I think we should use headers to communicate errors.  linkerd, for instance, sets an l5d-err header with linkerd-generated errors.  I think we should use something like this rather than arbitrary response bodies.\n. :star: we'll to update the examples repo as well (we can do this before the next release)\n\n. \u2b50 \n\n. lgtm, modulo module name change\n\n. WONTFIX.  Doc update tracked in #661\n. lgtm. i agree that it makes some sense to differentiate class-namers from configured-namers.\n\n. let's hold off on merging backwards-incompatible changes for the moment. there appear to be some issues in 0.5.0 that we should diagnose first.\n. Does it perhaps make more sense just to change the default? While it's nice for this to be configurable, if prometheus has a default location, perhaps we should just use that?\n. Well, that's not confusing ;p I was hoping it was /prometheus or something...\n. @lsjostro thanks for the fix! Would you mind signing our Contributor License Agreement?\n. :star: ignore the CI failure -- we need to change to coverage publishing script to exit gracefully for pull requests...\n\n. i'd suggest that we just do ./ci/coverage-publish.sh || true :)\n. while where here... should we choose another name besides default?  I.e. we might change the default at some point...  Also there may be identifiers for other plugins (is it confusing to have io.l5d.default be different across protocols)?  Perhaps io.l5d.http?\n. let's hold off on merging backwards-incompatible changes for the moment. there appear to be some issues in 0.5.0 that we should diagnose first.\n. hmmm... so sorry to :bike::house: naming, but technically this is versionAndMethodAndMaybeHostAndMaybeURI...  other ideas: httpMeta, httpEnvolope...?\n. I guess my main quibble is that host isn't included for http/1.0 requests\n. :star:\n\n. cc @stevej \n. Would like to stagger this behind https://github.com/BuoyantIO/linkerd/pull/413, which moves some of the router tracing around.\n. After input from kevino, I think requeueing without backoffs is sane default behavior.  We may want to to introduce requeue backoffs later, but I don't think it's necessary to do this immediately (and it certainly complicates configuration).  There are other mechanisms---failure accrual, failfast, budgets themselves---that help guard against requeue storms.  In the majority of cases, requeues occur when a downstream service has not received or process the request at all, unlike application-level retries.  In the case of Nacks, the finagle server has rejected the request before it has been admitted into application code.  Requeues are generally safe to retry immediately.\nAt this point, we'll be closer to finagle's default stack in terms of requeues; and we introduce an application-level retry filter within the path stack.  It shares a retry budget because retry budgets should be coordinated (otherwise when the requeue filter exhausts its budget, the classified retry filter would kick in continue to retry --- we can think of retry budgets as limiting client work as much as limiting server work, and so we we want to limit the max amount of extra work this client does.  This explains why retry budgets are intended to be shared across clients (even in finagle).  We care about being able do backoff application requests because these requests generally touch application code, and so we want a mechanism to give the application time to recover. This does not apply the same way for requeues.\n. :shipit: myself\n\n. We believe this was resolved by https://github.com/BuoyantIO/linkerd/commit/fd65ae3a5e5d573766d306dfa27b60564f26fece.  Please reopen if this is still an issue.\n. :star:\n\n. wait where's the package?  it would be nice to keep the name lowercased to match the other finagle namers.  perhaps ...zk.leader?\n. Should the name in the config initializer change as well to keep it consistent? Can we write a simple sanity test to ensure the config loads?\n. :star:\n\n. Does this configuration works properly when configured without a responseClassifier?\nI expect that this issue isn't related to response classifiers.  From the router's perspective, retrying shouldn't be any different from issuing two requests.  It may be helpful to get a PCAP file (i.e. from tcpdump) to give us some more insight into what's going on between linkerd and your server.\n. Okay, I've been able to reproduce this in a test... So we've got that going for us\n. retries don't occur at the client level, so it would be difficult to display the serverside successrate in the client graph.\n. :star:\n\n. \u2b50 \n\n. :star:\n\n. \u2b50 \n\n. :star:\n\n. :star:\n\n. \ud83d\udc4d \n\n. :star:\n\n. :shipit: trivial\n\n. @stevej au contraire, mon frere\n. :star:\n\n. :star: nice!\n\n. :star:\n\n. yeah, this broke in the last release -- we probably need a better k8s integration test\n. Alternatively, consider having a section (as in finagle)\n```\n0.6.0\nNew Features\n\n...\n\nBreaking Changes\n\n...\n...\n```\n. :star:\n\n\n. There's a new Telemeter plugin API (yet to be fully documented) that supports pluggable stats receivers.  The default one exports metrics on /metrics.json; but we'd also accept a PR that adds a statsd-push telemeter.\n. @andersschuller good question. It's definitely preferable to do this in finagle so we get the same level of visibility and control over the client.  However, if there's an existing library that works reasonably well, we can stick it in a FuturePool and see how that works out for us\nI might suggest that this be developed as a Telemeter plugin outside of the linkerd repo and when it's generally useful and appealing and we're happy with how it works, we can promote it as an official component.  We're of course happy to help through this -- I'm not trying to blow this off, but I want to make sure that the features that land in the linkerd repo are in a place that we're comfortable maintaining them. The plugin architecture allows us to be a little more incremental about this sort of thing.\nDoes this make sense?\n. @obeattie Thanks for the patch! Would you please fill out the Contributor License Agreement so we can accept this ;)\n. :star:\n\n. @obeattie thanks! fyi, we've published an updated kubectl to buoyantio/kubectl:1.2.3 and it sounds like the kubernetes folks are going to  start publishing kubectl regularly (https://github.com/kubernetes/kubernetes/issues/25900).\n. lgtm\n\n. no objections from me, i defer shipits to those more familiar with the admin javascript\n. lgtm\n\n. needs tests\n. :star:\n\n. question: typically, successrate will hover hover somewhere between 95% and 100%, or even between 99% and %100...  How do the graphs behave when success rate is at ~99.9%?  It's probably not a blocker, but ideally we should be able to show not-quite 100% success clearly.\n. lgtm! thanks, @rmars \n\n. \ud83d\udc4d \n\n. My attempt to address this is here:\n- https://github.com/BuoyantIO/linkerd/tree/ver/forless-routing-factory\n- or based on ver/netty4: https://github.com/buoyantio/linkerd/tree/ver/netty4-valed-routingfactory\nI'll defer to @stevej to report on how much this helps\n. \u00af_(\u30c4)_/\u00af \n. \u2b50 lgtm\n\n. Thanks for pointing this out, @Ashald!\n. \"See 0.0.0.0:9991\" -- you can't actually visit that ;) it's going to be tricky to provide a routable address in a link? Furthermore, this link isn't particularly helpful when namerd is in play...  I'm not against the change but I'm skeptical how helpful it's actually going to be in practice.\n. @wmorgan re: driving delegator UI from namerd: I agree in principal, but this is quite a bit trickier than it sounds.\n. also, we should consider that linkerd's admin UI may not be externally routable/discoverable (in typical k8s & mesos deploys).  We rely on namerd being served at a well-known place, but outside of development, I think it's atypical for users to have easy access to linkerd's admin UI.\n. lgtm.\nI think it would be even clearer to include a full snippet of how each of these is used in the context of a configuration file.  E.g. for tls:\nyaml\nrouters:\n- ...\n  client:\n    tls:\n      kind: io.l5d.static\n      commonName: foo.example.com\n\n. :star:\n\n. \ud83d\udc4d thanks @stevej \n\n. > In ThriftNamerClient, we cache the bind Activity and addr Var. However, each request passing through linkerd will observe an Activity/Var and then close the observation.\nThis is surprising to me.  My understanding is that DynBoundFactory (as created from DstBindingFactory) should be responsible for holding the observation open.  Similarly, the Var[Addr] is held open by the client's LoadBalancingFactory.\n. :star: though, we should follow up with easier to understand documentation about these parameters for users\n\n. This looks good.  I still suspect the DstBindingFactory capacities are slightly on the low side -- certainly conceivable that modest-sized deployments could hit these limits.  Also, I'm curious how the namerd sizes relate to linkerd capacities -- right now the linkerd capacity is 1/10th the size of the namerd capacity. What's the motivation for limiting namerd caches at 100/10 -- was there any notable difference from 500/50?\n. @adleong thanks, that makes sense.\n. Currently, it should not be assumed that identifiers can mutate requests, the contract is effectively Request => Id.  We need to be careful as to where request modification occurs, as it gets tricky in the face of retries, etc.  I'll take some time to write this up...\n. :star:\n\n. LGTM. Though, I worry about how this will work with IPV6 addresses (where :: is perfectly legal). For better or worse, our current method doesn't work well with IPV6 anyway, so it's probably fine to proceed with this.  Would it be a better fix to simply admit commas in paths?\n\n. :shipit: self-ship trivial\n\n. \ud83d\udc4d \n\n. \ud83d\udc4d \n\n. at first blush, this looks like really nice.  it's a big change, so I'd like to spend a little more time understanding it (and check it out etc).\ni noticed there's are typos in the commit history (e.g. \"Refator\") that you should probably rebase out of.\nAlso, I wonder if we can write some better tests for the tree-map bijections--I know I didn't do a great job of that with the original code, but it seems complex enough to warrant some dedicated unit tests.\n. Also, (bikeshedding), I'm curious if we should expose a long-polling-style API from delegator.json so that the page can receive updates as namerd change.  Fine to do in another/later branch, just thinking aloud.\n. lgtm!\n\n. My understanding is that we should only have to drop the first data point (since it is absolute and not a delta to the prior). Is this not the case?\n. :star: very odd... i'd like to understand why that happens but this seems like a good workaround\n\n. \u2b50  thanks for doing this, @obeattie!\n\n. Sounds like a good target for the telemeter plugin system.\n. :star: wow. this must have been a fun one!\n\n. thanks @cacoco! really appreciate the heads-up :D\n. \u2b50 \n\n. \u2b50  lgtm. thanks for adding tests!\n\n. \u2b50  self-ship #trivial\n\n. Thanks for the extra data, @jacob-koren. This helps us understand the failure in more detail.  From what I gather:\n- The sre-testing service has only a single node\n- There are up to ~3 connections to this node at a given point in time. (By default, linkerd provisions connections as needed to be able to satisfy all requests).\n- The service occasionally emits a 400 response (No flow is deployed for ...).\n  - When it returns a 400, the server signals Connection: close on the response and immediately closes the connection: \n  - The next on this service sometimes (not always) fails with a connection error.\nThis may be a timing issue in linkerd where the connection is not marked as closed before new requests are dispatched onto the connection.  I think I have enough data to build a good reproduction we can test against.  Thanks!\n. I've been able to reproduce this locally:\n- Run a small web server that closes connections\n  - go run shutter.go\n- run ./sbt linkerd-examples/http:run\n  - echo \"127.1 8080\" >linkerd/examples/io.l5d.fs/ugly\n- in two terminals run: while true ; do curl -H 'Host: ugly' localhost:4140 ; done\nAfter a few seconds, the following error is encountered in linkerd's logs:\nom.twitter.finagle.ChannelClosedException: null at remote address: /127.0.0.1:8080 from service: 0.0.0.0/4140. Remote Info: Upstream Address: /127.0.0.1:62989, Upstream Client Id: Not Available, Downstream Address: /127.0.0.1:8080, Downstream Client Id: #/io.l5d.fs/ugly, Trace Id: 525ae9bf20f23725.e45f5414a27b8f9b<:525ae9bf20f23725\n    at com.twitter.finagle.NoStacktrace(Unknown Source)\nCaused by: java.nio.channels.ClosedChannelException\nThis should give us enough information to debug this in linkerd.  Thanks again for all your help documenting the problem, @jacob-koren.\n. As described in https://github.com/twitter/finagle/issues/517, I've isolated this to a problem--probably a race condition--in finagle's Netty3 integration.  I have a branch that enables Netty4 which seems to fix this issue:\nBinary here: https://slack-files.com/T0JV2DX9R-F1J07MA0M-267f77a77f\nTo enable netty4, linkerd needs a router config like:\nyaml\n- protocol: http\n  ...\n  servers:\n  - port: 4140\n    ip: 0.0.0.0\n    engine:\n      kind: netty4\n  client:\n    engine:\n      kind: netty4\nThis feature is expected to be released in linkerd-0.7.0.  Let me know if it resolves the issue for you--if it does, I may just make netty4 the default engine in 0.7.0...\n. Sorry, please disregard that. I've reproduced this problem with netty4 as well.  I'm working with the finagle team to fix this...\n. Alright, I have a build against a patched version of finagle that has fixes for a related issue. The binary is available at https://slack-files.com/T0JV2DX9R-F1JL2EN78-3a17e49419\nWe're still working through some of the details of the fix, but I expect that the provided binary does not exhibit the exceptional behavior.  When you get a moment, @jacob-koren, I'd be curious to know if this resolves your issues. Thanks!\n. sorry @jacob-koren -- yeah the build is against a development version of finagle using these two branches: https://github.com/BuoyantIO/linkerd/tree/ver%2Fnetty4 https://github.com/twitter/finagle/pull/518 -- it's somewhat tricky to get all of the unreleased dependencies in order, so I've pushed an image to buoyantio/linkerd:gh482.  Let me know if this doesn't work as expected for any reason.\n. excellent news! I expect that this fix will make it into 0.7.0.  We'll publish a release once all of these fixes have made it into a stable finagle release.\n. lgtm! we should look at moving our guava cache stuff to caffeine, since this is where the rest of the finagle codebase is going. not a blocker\n\n. :star: This is okay for the time being if it fixes a problem, but I think we really do want to include namer tracing when it blocks the request path.\n\n. We've recently started publishing linkerd artifacts. http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22io.buoyant%22 !\n. Won't there be a whole bunch of other problems writing plugins in Java?\n. lgtm\n\n. IIRC there was follow-up on this offline.  Please reopen if this is still an issue.\n. lgtm! thanks, @klingerf \n\n. :star:\n\n. :star:\n\n. Closing this for now.  Please feel free to reopen this if you think it's worth pursuing.. lgtm\n\n. lgtm\n\n. lgtm\n\n. Should we disable decompression on the serverside as well so that bodies sent to linkerd are not decompressed needlessly? (Note: i don't think that Decompression controls whether outbound bodies are compressed --only whether incoming bodies are decompressed.  CompressionLevel controls outbound bodies, aiui).\nHave we validated that this doesn't end up double-encoding bodies?  Perhaps an end-to-end test?\n. awesome! as i said, i believe that we want to disable decompression on the serverside as well, as--as I understand it-- should disable decompresing posted bodies.\n. :star: thanks!\nWould you mind updating CHANGES while you're here?\n\n. \"This is really annoying to do because of the finagle code is structured.\"\n. lgtm. is this testable?\n\n. :star:\n\n. lgtm\n\n. thanks for your contribution, @Ashald!  Could you please sign our Contributor License Agreement so that we're able to accept this?\n. lgtm!   another approach might me to define something like protected def copy1(path: Path, dentry: Dentry): DelegateTree[T] on DelegateTree -- then have each implementor implement copy1 with copy (analogous to ClientStack/ServerStack).  tioli, though.\nIs it possible to improve test coverage while we're here?\n\n. \u2b50  nice!  There are some additional changes i'd like to get in before we do another release, but this is a great first step. thanks!\n\n. \u2b50 \n\n. It would be nice to test this stuff better but it seems a little complicated.\n. \u2b50  thanks!\nas always, i'm curious what we can do to test these things better (but not a blocker)\n\n. \ud83d\udc4d \n\n. :star:\n\n. lgtm\n\n. lgtm\n\n. Fixed: we now bundle a log4j.properties file in the build. :star: thanks, alex!\n\n. needs tests\n. We've introduced plugin interface to support announcement in https://github.com/BuoyantIO/linkerd/pull/368.  If there are specific service registry schemes you'd like to support, please open an issue for each with references to relevant docs.  Thanks!\n. :star:\n\n. Please reopen if you have any more questions!\n. @Ashald thanks for your patience waiting for us to get to this.\nMy initial understanding of this feature was that the Host value you'd like to set was actually stored in Consul.  I now see that this is provided to the namer configuration statically in the namerd/linkerd configuration.   So, effectively, all names resolved through this namer must receive the same domain value.  If that's the case, why does need any integration at all with the consul namer?  Couldn't we just add a module to linkerd that sets the host value?  It seems odd to me that this is set globally on the namer rather than on a per-service basis.  If it is indeed universal, it seems like it shouldn't be tied to the namer at all?\n. @Ashald thanks for the background (and sorry for missing your other explanation in the docs ;)\nI agree that it would be preferable to fetch this value from consul directly, rather than have to synchronize configuration across consul and linkerd.\nSo:\n- the consul namer propagates a domain suffix\n- linkerd has some sort of stripDomain option that causes this value to be used to modify downstream requests\n. Thanks again for all the work on this.\nRight now I think the main blocker is that I'd like to change this to move away from an arbitrary map of attributes to something typed.  Since the read side of this is a part of the core linkerd http functionality, I think it makes sense to introduce the metadata as a first-class primitive.\n. modulo nits and whatever alex has, this is looking about ready to me\n. \ud83d\udc4d  looks great!\n\n. All in all this looks good! A few small comments...\n. Sorry, just a few more nits.  Once we fix that test, I think this is good to go.\n. lgtm!\n\n. Thanks for submitting this, @Ashald.\nThis is a neat approach, but I'd like to find a more general purpose solution to these problems that is exposed explicitly via the API.  The proposed approach strikes me as a bit complex.  I don't think that subtleties like trailing slashes or lexical sorting should have substantial semantic meaning in the API.\nLet's take a step back and enumerate the problems we are trying to solve:\nDtab Syntax\nIt sounds like the root problem is that you don't want to have to write dtabs as plaintext and you'd like some object notation to describe dtabs and name trees (so that, for instance, it is easier to mutate them programmatically). This is a great idea. To address this, I think it would be preferable to add additional encodings to namerd's HTTP API to accept & emit JSON nametrees.  (Similarly to what is done by the delegator API -- in fact, I think the API already supports JSON encodings but doesn't sufficiently encode nametrees?). This would impose no constraints on the storage layer; and makes it easier to build generic tools on namerd.\nDtab Composition\nI think there needs to be a way to have an application use multiple dtabs so that it's easy to change one part of routing policy without creating unintended consequences; but I feel strongly that this behavior is made explicit by the API. @adleong has done some design exploration on this front which seems really promising.  We're being careful about introducing this, since we're hoping that we can begin to stabilize the namerd API over the next few months, and so we have been reticent to introduce a public API only to have to break it shortly thereafter.  We'll keep you in the loop on this as it progresses.\n. lgtm\n\n. please update CHANGES\n. lgtm! this definitely deserves an entry in CHANGES. maybe even with a '_BUGFIX_' ;)\n\n. \ud83d\udea2 \n\n. :star:\n\n. whoops, thanks for those catches (that's what i get for pushing a fix from the bus ;p)\nas for tests -- it's certainly possible to test but about 3x as much work as the fix.  I'll ry to get tests together...\n. \u2b50  self-review\n\n. What are these annotations supposed to indicate?\n. We definitely set them:\n0808 18:08:11.235 43762e354d0d8731.43762e354d0d8731<:43762e354d0d8731] Message(namer.success)\n0808 18:08:11.348 43762e354d0d8731.fc5318d3b71edaec<:43762e354d0d8731] ServerAddr(/127.0.0.1:9990)\n0808 18:08:11.351 43762e354d0d8731.fc5318d3b71edaec<:43762e354d0d8731] ClientAddr(/127.0.0.1:55143)\n. Complete transaction:\n0808 18:34:29.811 b3b19e8b2d89e9e8.b3b19e8b2d89e9e8<:b3b19e8b2d89e9e8] ServiceName(0.0.0.0/4140)\n0808 18:34:29.818 b3b19e8b2d89e9e8.b3b19e8b2d89e9e8<:b3b19e8b2d89e9e8] BinaryAnnotation(srv/finagle.version,6.36.0)\n0808 18:34:29.823 b3b19e8b2d89e9e8.b3b19e8b2d89e9e8<:b3b19e8b2d89e9e8] ServerRecv()\n0808 18:34:29.825 b3b19e8b2d89e9e8.b3b19e8b2d89e9e8<:b3b19e8b2d89e9e8] LocalAddr(/127.0.0.1:4140)\n0808 18:34:29.826 b3b19e8b2d89e9e8.b3b19e8b2d89e9e8<:b3b19e8b2d89e9e8] ServerAddr(/127.0.0.1:4140)\n0808 18:34:29.827 b3b19e8b2d89e9e8.b3b19e8b2d89e9e8<:b3b19e8b2d89e9e8] ClientAddr(/127.0.0.1:56379)\n0808 18:34:29.828 b3b19e8b2d89e9e8.b3b19e8b2d89e9e8<:b3b19e8b2d89e9e8] Rpc(netty3)\n0808 18:34:29.829 b3b19e8b2d89e9e8.b3b19e8b2d89e9e8<:b3b19e8b2d89e9e8] BinaryAnnotation(router.label,netty3)\n0808 18:34:29.831 b3b19e8b2d89e9e8.b3b19e8b2d89e9e8<:b3b19e8b2d89e9e8] BinaryAnnotation(namer.dtab.base,/srv=>/#/io.l5d.fs;/host=>/srv;/http/1.1/*=>/host)\n0808 18:34:29.831 b3b19e8b2d89e9e8.b3b19e8b2d89e9e8<:b3b19e8b2d89e9e8] BinaryAnnotation(namer.dtab.local,)\n0808 18:34:29.831 b3b19e8b2d89e9e8.b3b19e8b2d89e9e8<:b3b19e8b2d89e9e8] BinaryAnnotation(namer.path,/http/1.1/GET/default)\n0808 18:34:29.832 b3b19e8b2d89e9e8.b3b19e8b2d89e9e8<:b3b19e8b2d89e9e8] Message(namer.success)\n0808 18:34:29.832 b3b19e8b2d89e9e8.b3b19e8b2d89e9e8<:b3b19e8b2d89e9e8] BinaryAnnotation(dst.id,/#/io.l5d.fs/default)\n0808 18:34:29.832 b3b19e8b2d89e9e8.b3b19e8b2d89e9e8<:b3b19e8b2d89e9e8] BinaryAnnotation(dst.path,/)\n0808 18:34:29.833 b3b19e8b2d89e9e8.9b6ff667838fd02c<:b3b19e8b2d89e9e8] ServiceName(#/io.l5d.fs/default)\n0808 18:34:29.834 b3b19e8b2d89e9e8.9b6ff667838fd02c<:b3b19e8b2d89e9e8] BinaryAnnotation(clnt/finagle.version,6.36.0)\n0808 18:34:29.834 b3b19e8b2d89e9e8.9b6ff667838fd02c<:b3b19e8b2d89e9e8] ClientSend()\n0808 18:34:29.834 b3b19e8b2d89e9e8.9b6ff667838fd02c<:b3b19e8b2d89e9e8] Rpc(GET)\n0808 18:34:29.834 b3b19e8b2d89e9e8.9b6ff667838fd02c<:b3b19e8b2d89e9e8] BinaryAnnotation(http.uri,/admin/metrics.json)\n0808 18:34:29.834 b3b19e8b2d89e9e8.9b6ff667838fd02c<:b3b19e8b2d89e9e8] BinaryAnnotation(http.req.method,GET)\n0808 18:34:29.835 b3b19e8b2d89e9e8.9b6ff667838fd02c<:b3b19e8b2d89e9e8] BinaryAnnotation(http.req.host,default)\n0808 18:34:29.835 b3b19e8b2d89e9e8.9b6ff667838fd02c<:b3b19e8b2d89e9e8] BinaryAnnotation(http.req.version,HTTP/1.1)\n0808 18:34:29.835 b3b19e8b2d89e9e8.9b6ff667838fd02c<:b3b19e8b2d89e9e8] WireSend\n0808 18:34:29.835 b3b19e8b2d89e9e8.9b6ff667838fd02c<:b3b19e8b2d89e9e8] ServerAddr(/127.0.0.1:9990)\n0808 18:34:29.836 b3b19e8b2d89e9e8.9b6ff667838fd02c<:b3b19e8b2d89e9e8] ClientAddr(/127.0.0.1:56321)\n0808 18:34:29.861 b3b19e8b2d89e9e8.9b6ff667838fd02c<:b3b19e8b2d89e9e8] WireRecv\n0808 18:34:29.862 b3b19e8b2d89e9e8.9b6ff667838fd02c<:b3b19e8b2d89e9e8] BinaryAnnotation(http.rsp.status,200)\n0808 18:34:29.863 b3b19e8b2d89e9e8.9b6ff667838fd02c<:b3b19e8b2d89e9e8] BinaryAnnotation(http.rsp.version,HTTP/1.1)\n0808 18:34:29.863 b3b19e8b2d89e9e8.9b6ff667838fd02c<:b3b19e8b2d89e9e8] BinaryAnnotation(http.rsp.content-length,51211)\n0808 18:34:29.863 b3b19e8b2d89e9e8.9b6ff667838fd02c<:b3b19e8b2d89e9e8] BinaryAnnotation(http.rsp.content-type,application/json)\n0808 18:34:29.864 b3b19e8b2d89e9e8.9b6ff667838fd02c<:b3b19e8b2d89e9e8] BinaryAnnotation(l5d.success,1.0)\n0808 18:34:29.864 b3b19e8b2d89e9e8.9b6ff667838fd02c<:b3b19e8b2d89e9e8] ClientRecv()\n0808 18:34:29.905 b3b19e8b2d89e9e8.b3b19e8b2d89e9e8<:b3b19e8b2d89e9e8] ServerSend()\n. Closing this until we have a clearer problem description.\n. Thanks, @Ashald. \nIsn't the delegate API sufficient for this? It exposes a parseable object that includes all of the information needed to determine what the bound addr(s) apply (we do this is in the dtab playground UI, for instance)\nHow do you expect to use this API endpoint?  It doesn't seem like the output is all that parseable...\n. Thanks for providing more context---this really helps to have all of this captured in the PR so that it's easy to reference.  I have a better understanding now.\nI wasn't aware that the addr API worked like that! That's unfortunate.  We should fix it... I think we'll need to take a pass at cleaning up this API in general, but let's leave that for another issue.\nI agree that the API endpoint makes sense.  Since we're returning a non-trivial data structure, though, I'd prefer that we use a proper serialization format.  Hopefully some of the DelegateApi structures can be reused/copied to satisfy this.\n. Hypothesis: there are assumptions in the binding or load balancer factories about Activities not going Pending once they are OK.\n. #567 provides a workaround--potentially a correct fix.  We still need to get to the bottom of this behavior and document the contract between namers and linkerd.\n. lgtm -- as noted in #566, there's still some investigation to do; but this seems like a reasonable fix in the interim.\n\n. :star:\n\n. LGTM!  Would you mind adding something to the acceptance configuration for this?\n\n. lgtm\n\n. lgtm!  thanks, alex\n\n. This looks good to me, thanks!\nOne practical question before merging:  If this is a secret token, would it make more sense for this to be read from another file rather than encoded in linkerd configuration? I'm envisioning linkerd's config as non-sensitive (so you can share them easily, for instance), and secrets should be provided separately.  In this case, we'd change token to i.e. tokenFile.\nI don't feel strongly--whatever seems most practical.\n. Yeah, I think it's fine to start with token and introduce tokenFile later. :star::shipit:\n. lgtm!\n\n. Thanks for writing this up, @asheshambasta \nThe Identifier plugin interface is probably a great place to get started.\nIdentifiers essentially expose Request => Future[(Dst, Request)] -- meaning that, given a request, they asynchronously find the destination (service) and a potentially modified request.  This would likely be a natural place to extract credentials from the request, call another service, modify the request with a resulting cookie, and choose a destination based on the results of authentication.\nHere's an example identifier plugin written in Java: https://github.com/BuoyantIO/linkerd-examples/tree/master/plugins/header-classifier\nI expect that identifiers and plugins are somewhat under-documented; so don't hesitate to ask questions if you get stuck.\n. Note! We added support for using namers from identifiers in #616.\nI'm going to close this issue for now, but please reopen if you have any more questions.\n. lgtm!\n\n. \ud83d\udc4d \n\n. lgtm\n\n. lgtm, but a question:\nWhat if the name doesn't become pending for a very long time?  Does it make sense to accept a wait=N query param that specifies how long to wait for a pending message?  if it times out, we just return the pending response.  This allows the consumer to specify their tolerance explicitly.  I get that this can be implemented in timeouts, but doesn't seem onerous to be a part of the API, either.\nTIOLI.\n\n. I believe this was merged in #595.  Please feel free to reopen if there are remaining issues.\n. :star: will merge after CI passes\n\n. It looks like this uncovers an actual test error.  Would you mind fixing the test (missing T)?\ncom.fasterxml.jackson.databind.JsonMappingException: Could not resolve type id 'io.l5d.testelemeter' into a subtype of [simple type, class io.buoyant.telemetry.TelemeterConfig]: known type ids = [TelemeterConfig, io.l5d.testTelemeter]\n. thanks!\n. :star: this looks really good. Very happy to merge this ;)\n\n. We need to be careful about introducing dependencies between plugin systems. For example, namers should get stats receivers and tracers, so telemeters can't rely on namers.  We'll just need to be careful/explicit about how these features are composed.\n. :star: should we just bump the default in Awaits.scala?\n\n. \ud83d\ude11  ...very clever...\n. :star:\n\n. Thanks for reporting this. This was definitely an oversight -- I think we'll need to change the retry policies to not consider any request as retryable that has a chunked body.  With chunked bodies, we explicitly do not want to buffer the entire body; but retrying these requests would require that we are able to re-stream the body, which isn't possible without buffering.\nI'll submit a change to the retry classifiers to correct this.\n.  @obeattie agreed completely. As I said, this was an unintended consequence, and we simply hadn't tested retries with chunked request bodies.  I'm updating classifiers to consider such requests non-retryable. I'll update the documentation as well.\n. needs user documentation\n. lgtm\n\n. Do you get the same behavior if you replace ! with /$/fail?\n. :star: the approach looks good to me! just some comments on documentation\n\n. This looks good, though I'd prefer that we add some test cases to illustrate that it works properly.\n. lgtm\n\n. lgtm\n\n. We've been tracking this in #397. We should extend the admin configuration to support a configurable resource root.\n. router-http/e2e:test fails with:\n```\nAug 29, 2016 10:32:03 PM com.twitter.util.RootMonitor$ handle\nSEVERE: Exception propagated to the root monitor!\nFailure(request cancelled. Remote Info: Not Available, flags=0x02) with Service -> \nCaused by: com.twitter.finagle.CancelledRequestException: request cancelled. Remote Info: Not Available\nAug 29, 2016 10:32:04 PM com.twitter.util.RootMonitor$ handle\nSEVERE: Exception propagated to the root monitor!\nFailure(request cancelled. Remote Info: Not Available, flags=0x02) with Service -> \nCaused by: com.twitter.finagle.CancelledRequestException: request cancelled. Remote Info: Not Available\n[info] HttpEndToEndTest:\n[info] - end-to-end routing\n[info] - strips connection header  FAILED \n[info]   com.twitter.finagle.ChannelClosedException: ChannelException at remote address: /0.0.0.0:49830 from service: upstream. Remote Info: Upstream Address: Not Available, Upstream Client Id: Not Available, Downstream Address: /0.0.0.0:49830, Downstream Client Id: upstream, Trace Id: fe6cb6315d6db6a8.fe6cb6315d6db6a8<:fe6cb6315d6db6a8\n[info]   ...\n[info]   + serving on /0.0.0.0:49829 \n[info]   + routing on /0.0.0.0:49830 \n[info] - http/1.1: server closes connection after response\n[info] - http/1.0: server closes connection after response  FAILED \n[info]   com.twitter.finagle.ChannelClosedException: ChannelException at remote address: /0.0.0.0:49838 from service: upstream. Remote Info: Upstream Address: Not Available, Upstream Client Id: Not Available, Downstream Address: /0.0.0.0:49838, Downstream Client Id: upstream, Trace Id: 83170beb9fb2e73c.83170beb9fb2e73c<:83170beb9fb2e73c\n[info]   ...\n[info] Run completed in 2 seconds, 869 milliseconds.\n[info] Total number of tests run: 4\n[info] Suites: completed 1, aborted 0\n[info] Tests: succeeded 2, failed 2, canceled 0, ignored 0, pending 0\n[info]  2 TESTS FAILED \n```\nThe only potentially related change I see in 6.37.0 is from this shady character: https://github.com/twitter/finagle/pull/530\nI believe that this is an error encountered on the first client (i.e. to the router server) and that the request is sent (confirmed via wireshark).  The second request (from router to app) is not issued.\n. Simplest reproduction is just:\nscala\n  test(\"what is happen\") {\n    val server = FinagleHttp.serve(new InetSocketAddress(0), ServiceFactory.const(\n      Service.mk[Request, Response] { req =>\n        Future.value(req.response)\n      }\n    ))\n    val addr = server.boundAddress.asInstanceOf[InetSocketAddress]\n    val client = FinagleHttp.newService(s\"/$$/inet/127.1/${addr.getPort}\")\n    try {\n      val req = Request()\n      req.headerMap.set(\"Connection\", \"close\")\n      assert(await(client(req)).status == Status.Ok)\n    } finally await(client.close().join(server.close()).unit)\n  }\nFails with:\nAug 29, 2016 11:11:39 PM com.twitter.util.RootMonitor$ handle\nSEVERE: Exception propagated to the root monitor!\nFailure(request cancelled. Remote Info: Not Available, flags=0x02) with Service -> \nCaused by: com.twitter.finagle.CancelledRequestException: request cancelled. Remote Info: Not Available\n...\n[info] - what is happen *** FAILED ***\n[info]   com.twitter.finagle.ChannelClosedException: ChannelException at remote address: /127.0.0.1:56028 from service: /$/inet/127.1/56028. Remote Info: Upstream Address: Not Available, Upstream Client Id: Not Available, Downstream Address: /127.0.0.1:56028, Downstream Client Id: /$/inet/127.1/56028, Trace Id: fc1c7c922cd044f8.fc1c7c922cd044f8<:fc1c7c922cd044f8\n. Per test failures, I don't think we'll be able to adopt 6.37.0.  We'll need to wait for a release that includes https://github.com/twitter/finagle/pull/550---hopefully we can get a patch release.\n. lgtm\n\n. lgtm!\n\n. this looks fine, though I'm having some trouble understanding exactly what this means...\nI think this means that namers could potentially be configured to resolve through themselves?  What happens with this sort of configuration? Does it fail gracefully or does the process melt down? (I'd expect the latter).\nWe have an interpreter plugin interface -- should these interpreters be accessible from modules?\nIf we do this in linkerd's main, we may also want to do this for namerd's main...\n\n. I think it's slightly trickier than that--since telemeters are passed to namers, telemeters could be configured using namers in ways that cause things to blow up.  So, a person writing the config has to have some pretty intimate knowledge of how things are wired under the hood and what is allowable and what isn't.\nI like the intent of the change, but I'm not sure how to provide this functionality without making the configuration more fragile.\n. Can we guard against self-referential naming?  Since we don't actually allow you to set Dtab.base on the process (right? i hope not...), all names in a config are necessarily concrete -- so we can detect /#/$prefix and prevent this?\n. lgtm! probably warrants some docs (or at least a note in CHANGES)\n\n. :star:\u26a1  Huge thanks for fixing this, @obeattie! Not a blocker for this change, but we should try to get a test case that covers this as well.\n\n. This change breaks some tests:\norg.scalatest.exceptions.TestFailedException: Expected exception com.twitter.io.Reader$ReaderDiscarded to be thrown, but org.scalatest.exceptions.TestFailedException was thrown.\nIt sounds like only some exceptions should be ignored..\n. Routers use response classifiers in several ways:\n- Clients use response classifiers to determine success/failure counts.\n- Routers use response classifiers to determine success/failure counts, and to determine whether a request is retryable.\n- Servers use response classifiers to determine success/failure counts.\nWe measure response classifications at all three places because they each have different values.  Specifically:\n- A client is connected to a single concrete cluster.\n- A router sends requests to logical names.  Retries occur in the router (and not the client) so that retries may occur across clusters (i.e. when weighted destinations are used).\n- Since a router may expose multiple servers, we track request counts (and success/failure) for each server.  Also, because retries happen in the router, server counts represent the original inbound request rate.\nSo, I think it's correct for classifiers to be configured on the router level so that requests are classified uniformly.\n. lgtm! though, i'd prefer that we use the least permissive visibility for intermediate types.\n\n. thanks, @Ashald \n. It's definitely my preference that we not add this behavior to the existing identifiers -- it makes it harder to reason about them, as it allows for the production of highly irregular names, it's unclear what consume means, etc...\nI'd prefer that we be able to express composed identifiers like:\nidentifier:\n  kind: list\n  identifiers:\n  - kind: header\n    header: my-special-header # override default\n    prefix: /h\n  - kind: path\n    prefix: /p\nThis allows the identifiers to stay to doing one thing well.\n. The running flags are necessary.  Future cancellation is advisory.  If the following state arises, this would spin forever:\n1. Open var observation, starting a request\n2. Receive response\n3. Close observation while in handler\nFuture cancellation doesn't actively stop a thread from executing. It simply notifies the underlying Promise that the future does not need to be satisfied.  The flags are necessary to ensure that work eventually stops.\n. Nope.  Consider what happens if the future is already satisfied when the cancellation happens.  We're executing code in a thread with the satisfied value.  That thread is not killed.  There's no way to force an exception to be raised in that case.\nIt's idiomatic (i.e. in finagle) to use guards like this.\n. \ud83d\udc4d this seems like a huge improvement. I leave grammar/style nits up to the group, but i heartily endorse the spirit of the change.\n\n. Related to #605\n. Thanks for the patch, @erdody!\nWe had tried to do something like this in twitter-server, but it turns out that it's extremely unreliable. VM shutdown hooks run in a dedicated thread that is run as/after all other threads are killed.  Because finagle relies on netty thread pools, shutdown hooks don't allow for any sort of reliable cleanup action on clients or servers.  The docs even seem to indicate that accessing other threads may lead to deadlocks.\nThe /admin/shutdown admin endpoint is unfortunately the only way to ensure graceful termination.\nI'm not vehemently opposed to this, but I don't think it's going to work as you hope.\n. Here's the commit where we took addShutdownHook out of com.twitter.app.App: https://github.com/twitter/util/commit/f71efa3f5a699c1c42d2a8bb33ee36caddc0dec8\n(This is a few years ago, so I'm trying to shake the cobwebs off and rationalize why we did this).\nAs I recall, the root of the issue is that we wanted to simply log some information at shutdown, but even logging facilities may not be available from the context of a shutdown thread (presumably because some other shutdown hook is racing to tear down that resource).\nI don't want to stand on FUD to block a feature that might improve things, though.  We should test your branch a bit more to see where it encounters problems, if it all.  If it doesn't cause problems, this may even be something worth feeding back to twitter/util.\n. I've mocked out an App:\n``` scala\nobject ShutterDowner extends App {\n  private[this] val log = Logger.get(\"ShutterDowner\")\ndef main() {\n    val client = Http.newService(\"/$/inet/linkerd.io/80\")\n    val server = Http.serve(\"127.1:8123\", Service.mk[http.Request, http.Response] { req =>\n      client(req).map { rsp =>\n        rsp.headerMap(\"awesome\") = \"yep\"\n        rsp\n      }\n    })\n    closeOnExit(server)\n    closeOnExit(client)\nsys.addShutdownHook {\n  log.info(\"closing\")\n  Await.result(close(defaultCloseGracePeriod))\n  log.info(\"closed\")\n}\n\nlog.info(\"running\")\nAwait.result(server)\nlog.info(\"completed\")\n\n}\n}\n```\nThen I put some load on the service:\n:; while curl -vso /dev/null http://localhost:8123 ; do ; done\nThe first time I run it, we don't see any of the shutdown messages:\n:; java -cp test.jar io.buoyant.ShutterDowner\nSNAPSHOT.jar io.buoyant.ShutterDowner        \nSep 08, 2016 11:54:49 PM com.twitter.finagle.Init$$anonfun$1 apply$mcV$sp\nINFO: Finagle version 6.36.0 (rev=7efeb4cc2babd7c99731090fa76ad960627fce14) built at 20160708-090553\nSep 08, 2016 11:54:50 PM com.twitter.finagle.BaseResolver$$anonfun$resolvers$1 apply\nINFO: Resolver[inet] = com.twitter.finagle.InetResolver(com.twitter.finagle.InetResolver@5443d039)\nSep 08, 2016 11:54:50 PM com.twitter.finagle.BaseResolver$$anonfun$resolvers$1 apply\nINFO: Resolver[fixedinet] = com.twitter.finagle.FixedInetResolver(com.twitter.finagle.FixedInetResolver@3d1848cc)\nSep 08, 2016 11:54:50 PM com.twitter.finagle.BaseResolver$$anonfun$resolvers$1 apply\nINFO: Resolver[neg] = com.twitter.finagle.NegResolver$(com.twitter.finagle.NegResolver$@7dda48d9)\nSep 08, 2016 11:54:50 PM com.twitter.finagle.BaseResolver$$anonfun$resolvers$1 apply\nINFO: Resolver[nil] = com.twitter.finagle.NilResolver$(com.twitter.finagle.NilResolver$@6e4566f1)\nSep 08, 2016 11:54:50 PM com.twitter.finagle.BaseResolver$$anonfun$resolvers$1 apply\nINFO: Resolver[fail] = com.twitter.finagle.FailResolver$(com.twitter.finagle.FailResolver$@4b6e2263)\nSep 08, 2016 11:54:50 PM com.twitter.finagle.BaseResolver$$anonfun$resolvers$1 apply\nINFO: Resolver[flag] = com.twitter.server.FlagResolver(com.twitter.server.FlagResolver@344f4dea)\nSep 08, 2016 11:54:50 PM io.buoyant.ShutterDowner$ main\nINFO: running\n^C%\n:;\nThe second time, we see only the closing message:\n```\n:; java -cp test.jar io.buoyant.ShutterDowner\nSep 08, 2016 11:55:09 PM com.twitter.finagle.Init$$anonfun$1 apply$mcV$sp\nINFO: Finagle version 6.36.0 (rev=7efeb4cc2babd7c99731090fa76ad960627fce14) built at 20160708-090553\nSep 08, 2016 11:55:10 PM com.twitter.finagle.BaseResolver$$anonfun$resolvers$1 apply\nINFO: Resolver[inet] = com.twitter.finagle.InetResolver(com.twitter.finagle.InetResolver@5443d039)\nSep 08, 2016 11:55:10 PM com.twitter.finagle.BaseResolver$$anonfun$resolvers$1 apply\nINFO: Resolver[fixedinet] = com.twitter.finagle.FixedInetResolver(com.twitter.finagle.FixedInetResolver@3d1848cc)\nSep 08, 2016 11:55:10 PM com.twitter.finagle.BaseResolver$$anonfun$resolvers$1 apply\nINFO: Resolver[neg] = com.twitter.finagle.NegResolver$(com.twitter.finagle.NegResolver$@7dda48d9)\nSep 08, 2016 11:55:10 PM com.twitter.finagle.BaseResolver$$anonfun$resolvers$1 apply\nINFO: Resolver[nil] = com.twitter.finagle.NilResolver$(com.twitter.finagle.NilResolver$@6e4566f1)\nSep 08, 2016 11:55:10 PM com.twitter.finagle.BaseResolver$$anonfun$resolvers$1 apply\nINFO: Resolver[fail] = com.twitter.finagle.FailResolver$(com.twitter.finagle.FailResolver$@4b6e2263)\nSep 08, 2016 11:55:10 PM com.twitter.finagle.BaseResolver$$anonfun$resolvers$1 apply\nINFO: Resolver[flag] = com.twitter.server.FlagResolver(com.twitter.server.FlagResolver@344f4dea)\nSep 08, 2016 11:55:11 PM io.buoyant.ShutterDowner$ main\nINFO: running\n^CSep 08, 2016 11:55:31 PM io.buoyant.ShutterDowner$$anonfun$main$1 apply$mcV$sp\nINFO: closing\n```\nI'm unsure this is just because logging is being lost or because the code isn't even running.\nSo, shutdown hooks definitely introduce some level of unpredictability...\n. I like the signal handler approach! Let's do it.\nI think I'd prefer that the grace value come from the admin configuration block than from a flag -- we're trying to make the linkerd configs as self-contained as possible and not rely on a mix of flags and config options.  If this seems inconvenient for your use case, it's not a hard requirement.\n. This is starting to look really good!  Left a few comments.\n. @erdody Can you please sign our CLA? Thanks!\n. :star: fantastic! I'll merge this once it passes CI.\n\n. cc @Ashald \n. @Ashald hah! i already converted SvcCache to SvcAddr in a followup branch: https://github.com/BuoyantIO/linkerd/tree/ver%2Fconsul-lazy-services.\nI think it's possible that we're currently leaking request loops; and we're certainly eagerly starting requests for services we possibly don't care about.\n. My test plan was \"politely ask @Ashald to try this branch overnight...\" Any other ideas?\n. Since it seems that this is basically uncontroversial, I'm going to address error handling in the follow-up  #632 \n. Merged in https://github.com/BuoyantIO/linkerd/commit/944a2f421f8710fcb9f835b6b46e8bbbbb32b295\n. attn @Ashald \n. @Ashald tested this branch in his environment and did not detect any regressions.\n. cc @obeattie \n. The tests are broken, but the code appears to be correct.  Working on a fix.\n. Yeah, I have a fix prepared already.  No worries at all (these tests are way too complex).\n. cc @obeattie \n. A question worth considering while we're here:\nexceptions tend to be much costlier code paths than other graceful returns (i.e. stack tracing, etc).\nWe're admitting now that an unidentifiable request isn't so exceptional: we expect that identifiers won't always identify requests.\nWhat if we made Identifiers return an ADT like:\nscala\nsealed trait RequestIdentification[Req]\ncase class IdentifiedRequest[Req](dst: Dst, request: Req) extends RequestIdentification[Req]\nobject UnidentifiedRequest extends RequestIdentification[Nothing]\nThis makes the interface much clearer, though it's an API breakage (worth it imo)\n. It's my preference that we push forward with the API break and communicate the change.  I think this puts the Identifier API in a much better place moving forward.\nI have a slight preference for an ADT over an Option:\n- Option[Tuple] screams for want of a proper type\n- We could conceivably support additional states (idk what, just thinking)\n- We could conceivably extend IdentifiedRequest with additional information (on that note, we probably should avoid case classes here -- case classes cause all field additions to be API breakages (on unapply)).\n. :star: my name is oliver and i endorse this message.\n\n. lgtm!  Do you think we should document l5d-name in the http headers documentation?\n\n. lgtm\n\n. It's unfortunate that consul doesn't provide error codes that distinguish failure types.\nSince this is more of a nuisance than an active problem, I don't think there's any urgency of having a bulletproof solution.\nAnd, I think approach (3) is a more robust solution that will apply beyond the consul namer: I think it's fine for retries to occur as long as the name is being observed.  If we can solve observation lifecycle---where we stop observing irrelevant names---the observation teardown should stop the retry behavior.\n. Yes, that's absolutely right, @adleong. If we can introduce a streaming (gRPC?) api, this would be much easier to reason about.\n. lgtm\n\n. related to #630 \n. based on #651.\n. based on #651.\n. - Endpoint list at startup moved from INFO to DEBUG\n- the admin server Monitor still prints stack traces but uses the logger instead of our own version of stacktrace printing.\n- AdminInitializer is just a function instead of a mutable/stateful class\n- Namerd shutdown ordering is fixed.\n. :star: skipping js-readability, no ui changes.\n\n. The default tracer is \"whatever was loaded on the classpath\".  At twitter, this is finagle-zipkin, configured to write to localhost:SCRIBEPORT.  I don't think linkerd should maintain this behavior.\nYou're right with the one exception that the new behavior is:\n- Some telemeters but none that provide stats => null stats receiver\n. lgtm! thanks\n\n. attn @Ashald \n. Dupe of #606 \n. :star: agreed!  would you mind updating CHANGES.md as well?\n\n. Fixed by #666 \n. I think it's fine to move forward with DtabStore support consistency.   I think operationally speaking, if I were running consul, I'd want to ensure that writes are consistent but reads do not have to be.\n. +1 I would support changing the default read consistency\n. no js changes, rolling forward with this\n. On reread, I think the only blockers on this are:\n- put k8s in the module names\n- do we want to gate these with an experimental flag? don't feel strongly, but the k8s namer is still experimental...\n. Well, the process has to buffer chunked messages.  A 2G unchunked payload could easily cause the process to OOM if you haven't given it at least 3 or 4 G of heap space.  Or a 200M payload would cause the process to OOM if there are multiple concurrent requests... etc.\nThe limits are there to protect the process against unconstrained memory pressure; and if you want to raise these limits, you need to tune the processes's memory allowance explicitly (there are environment variables that can be used to change the process's default memory settings.\n. Can we find a more descriptive name than \"utils\"?\n. Thanks for pointing out this post, @fantayeneh.  Very clear & helpful.\n. Thanks for contributing to linkerd!\n. We already strip Proxy-Authenticate, Proxy-Authorization and Proxy-Connection headers in ProxyRewriteFilter.  But i fully endorse stripping the rest.\nPerhaps it's best to rename StripConnectionHeader to StripHopByHopHeader, and put all of the header-removal logic (including Proxy-* headers), into a single filter.\n. :star:\n\n. I suspect that finagle does most of this for us, but we should verify in e2e tests\n. It looks like master changed under you and there are some conflicts that need to be resolved ;/ Alllmost there.\n. Thanks for your help, @fantayeneh!\n. hey @OleksandrBerezianskyi, have you had a chance to investigate this further.  We think it is likely this class of error was fixed in https://github.com/BuoyantIO/linkerd/commit/793e82c21a7eaa3a5ff636c3f06f20eaa16ab164.\n. It would be Nice (not required) to get #683 into this as well if the timing works out\n. doh, looks like you're out of sync with master again ;/\n. Merged https://github.com/BuoyantIO/linkerd/commit/d2dc194e584d06cca03fcc5822e774a041eced60\n. Alternatively, it may make sense to have an inet namer like /$/port/80/foo. I don't have a strong sense of what's preferred.  Appending gets really weird with residuals...\n. This is cool! It seems useful, but I want to be careful about adding something like this. It makes dtabs even more complex, so we should be sure it's worth introducing such a generic utility.\n. closing for now, reopen as needed\n. Big thanks for this, @halve. We're really happy about how this branch turned out.  I'll pull this into master, and we should be releasing 0.8.3 in the next couple of days.\n. @halve can you please sign our contributor agreement when you get a moment? https://buoyant.io/cla/individual/  (basically stating you won't claim we violated your copyright)\nThanks!\n. Client-side TLS already works (as validated by integration tests)\n. com.twitter.util.events.sinkEnabled controls whether finagle emits diagnostic events through this API.  In our testing, we've found it negatively impacts performance (and I happened to discover this on the same branch that I had made logging changes, so just lumped it in).\n-log.level still works.  This log4j config is really there to control netty's logging.  AFAICT it has no impact on finagle's logging.\n. Thanks for the patch, @AbhiMedallia!  This is really cool.\nI don't think we should add Medallia-specific logic to linkerd; but I understand the need to support an alternate header scheme.  This seems like a good target for a new plugin subsystem so that you can maintain this outside of the linkerd repo.  If we could provide such a plugin interface, would that work for you?\n. No worries. If you let us know (via Issues is best) of the places where you have found the need to modify linkerd, I think we'd be pretty interested in getting plugin interfaces in place so that you can maintain separate packages instead of a substantial fork.  Looks like you're making good progress, though ;)\n. @AbhiMedallia no problem. If you've found the need to change this sort of thing, I expect other users will as well. Ideally, you should be able to run vanilla-released linkerd with a set of medallia specific plugins (provided at runtime) to get all of the features you need.  This should substantially reduce your maintenance overhead in the long run.\n. thanks for opening this, @jduan.  Let me clarify a few points related to this:\nUser-provided plugins are already supported.  when the L5D_HOME environment variable is set, linkerd will automatically add $L5D_HOME/plugins/*.jar to its classpath.  linkerd's plugin subsystem will automatically load these plugins before processing configuration, so that plugins may be configured via the config file.\nWe currently do not plan to introduce a \"generic filter plugin\" interface.  Instead, we'd prefer to carve out specific interfaces for each functional thing that plugins do (for instance, rate limiting).  If linkerd doesn't currently expose an interface for this, we should add a plugin system (and we'd probably want to start with enumerating each subsystem in its own Issue).\nWe may consider loosening these constraints if we encounter a number of filter types that really are unique to one deployment, but I'd rather be forced into that decision than to start with an overly generic interface.\n. Why is it necessary to track this twice?\n. What's the exact server-side data we want? The request as it enters the server? Or after identification? or?\n. Specifically, I think it's more \"correct\" for linkerd to trace the exact request it received before all rewriting/processing. But I suspect that this won't actually help you with your task.\n. We talked about this offline: what we really want is to ensure that client spans are emitted even when the client fails to bind properly.  I think we want to do this by moving the client tracing filter up into the bottom of the path stack.\n. You're right that it's not as eager as I thought:\nscala\n      identifiers.foldLeft(HttpConfig.NilIdentification) { (identification, next) =>\n        identification.flatMap {\n          // the request has already been identified, just use that\n          case identified: IdentifiedRequest[Request] => Future.value(identified)\n          // the request has not yet been identified, try the next identifier\n          case unidentified: UnidentifiedRequest[Request] => next(req)\n        }\n      }\nThis does eagerly create the future chain (i.e. we call f.flatMap for all identifiers), though identifiers aren't actually invoked until the future is satisfied and unidentified.\nI find the combination of Seq.foldLeft and Future to be a bit mind-bendy.  This implementation definitely isn't bad, though I do find the recursive approach to be a it clearer (or, more idiomatic for future looping/iteration)\n. related to #722 \n. After discussion, I don't think that this particular change makes that much sense.... I have an end-to-end test that verifies that netty's Http2Codec doesn't enforce flow control properly.  Specifically, the write() promise is satisfied before the data is actually written (i.e. when flow control is enforced).  This causes linkerd to call Frame.Data.release(), opening inbound flow control prematurely.\nWhen I started digging into the write-completion logic, I found the following in netty's AbstractHttp2StreamChannel:\n\n// TODO: this is pretty broken; futures should only be completed after they are processed on\n   // the parent channel. However, it isn't currently possible due to ChannelOutboundBuffer's\n   // behavior which requires completing the current future before getting the next message. It\n   // should become easier once we have outbound flow control support.\n   // https://github.com/netty/netty/issues/4941\n\nWe could potentially fix this by using the legacy APIs.  Though, it looks like https://github.com/netty/netty/pull/5914 should just fix this in Netty.  But, we'll have to get this upgrade through Finagle, which, as we know, currently can take a bit of time.\n. Looking forward to it, @buchgr!\nAnother question: It seems that Http2Codec (I haven't checked where exactly) automatically updates the window when receiving request streams? Is this known behavior? For example:\nINF [20161017-22:57:05.179] http2: ----------------INBOUND--------------------\nINF [20161017-22:57:05.179] http2: [id: 0xae5561fd, L:/192.168.1.187:55378 - R:/192.168.1.187:55379] DATA: streamId=3, padding=0, endStream=false, length=16384, bytes=01010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101...\nINF [20161017-22:57:05.179] http2: ------------------------------------\nINF [20161017-22:57:05.188] http2: \nINF [20161017-22:57:05.188] http2: ----------------INBOUND--------------------\nINF [20161017-22:57:05.188] http2: [id: 0xae5561fd, L:/192.168.1.187:55378 - R:/192.168.1.187:55379] DATA: streamId=3, padding=0, endStream=false, length=16384, bytes=01010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101...\nINF [20161017-22:57:05.188] http2: ------------------------------------\nINF [20161017-22:57:05.189] http2: \nINF [20161017-22:57:05.189] http2: ----------------OUTBOUND--------------------\nINF [20161017-22:57:05.189] http2: [id: 0xae5561fd, L:/192.168.1.187:55378 - R:/192.168.1.187:55379] WINDOW_UPDATE: streamId=0, windowSizeIncrement=32768\nINF [20161017-22:57:05.189] http2: ------------------------------------\nAs far as I can tell, Netty is issuing this WINDOW_UPDATE on its own. We need to be able to defer this update until the data has been forwarded.\n. Thanks for the clarification, @buchgr & @ejona86!\n. stream stats added in https://github.com/BuoyantIO/linkerd/pull/784\n. Fixed by #746\n. no longer true. What does this have to do with http?\n. Is it really common practice to provide secrets via environment variables?  As far as I understand, this isn't really safe since users on the parent OS can read these without appropriate access control (i.e. via ps -E).\n. I'm curious if, for instance, we can prepare an analogous change in Finagle.  If Twitter can exercise this in production, it would be a pretty strong signal.\n. Welcome to the project, @bobymicroby.\nWhile linkerd doesn't explicitly provide features for this sort of stateful routing, it is possible to achieve this sort of configuration with linkerd.\nSpecifically, the Identifier plugin system may be used to determine the destination for a given request (i.e. by client IP, cookie, etc).  linkerd's load balancers are not aware of any such affinity, and so each backend would have to represented as its own target cluster.\n. @OleksandrBerezianskyi It's probably helpful to get a little more detailed about this use case. I don't think it should be necessary to write a custom load balancer to accomplish this sort of thing... For instance, a combination of an Identifier and Namer may be paired to route requests the the appropriate backend.  The important thing to consider in this case is that we would expect service discovery to be organized in terms of logical shards --- linkerd treats service discovery strictly as replica sets.  In truly stateful services, we might be inclined to expose each instance as its own replica set with size=1 -- distribution of load to these replicas is moved up to the identifier/namer logic.   I don't have any examples of this offhand, but I think it's something we can support in linkerd.\n@alinvasile I don't have an examples ready offhand.  I believe that com.twitter.finagle.http.Request objects expose the client address, so this could be accessed and used to build the destination name.\n. Merged https://github.com/BuoyantIO/linkerd/commit/b0f422286c431d2d00731ebe97ade3a617c9bc53\n. Merged https://github.com/BuoyantIO/linkerd/commit/afc056a52b6cefce1379f8f6e862183445763bb1\n. I took a stab for my own understanding.  Here's what I came up with: https://gist.github.com/olix0r/2585d60de53533249dc2baff4504720c\nThe main goal was to make the state stuff (what the activity does in this review) more explicit:\nThe authenticator has 3 states, Init, Authenticating(token: Future[String]), and Closed.  The authenticator is in the Init state before it has processed any requests.  As soon as it tries to process the request, it tries to update the state to Authenticating with a future that will be satisfied with the token once it's authenticated.  If another thread races this and starts getting a token before we update the state, we start over again (probably off of the other thread's authentication result).\nI think this accomplishes roughly the same thing as your review but is easier (for me) to think about. This has the added benefit of removing the conditional \"should i or shouldn't I auth\" check from the runtime and up into the configuration -- we either provide the Namer with a straight Api.Client or with an Api.Authenticated(Api.Client) and the namer can remain blissfully ignorant about authentication.\nHappy to chat more about this.\n. needs updates to CHANGES, and probably to marathon docs?\n. @klingerf were you running this in Docker? I want to ensure that we're both in the same config, since I can't reproduce the exceptions that you saw. I do still see some sort of error, though:\nINFO 0110 23:53:05.263 finagle/netty4-2: \n----------------OUTBOUND--------------------\n[id: 0x31a8f10c, L:/192.168.1.187:64330 - R:http2.golang.org/130.211.116.44:443] SETTINGS: ack=false, settings={}\n------------------------------------\nDEBUG 0110 23:53:05.325 finagle/netty4-2: Accepting a server certificate: CN=http2.golang.org\nD 0110 23:53:05.423 THREAD1153757 TraceId:72714965c2ffaa63: [C L:/192.168.1.187:64330 R:http2.golang.org/130.211.116.44:443 S:3] initialized stream\nINFO 0110 23:53:05.433 finagle/netty4-2: \n----------------OUTBOUND--------------------\n[id: 0x31a8f10c, L:/192.168.1.187:64330 - R:http2.golang.org/130.211.116.44:443] HEADERS: streamId=3, headers=DefaultHttp2Headers[:method: GET, :path: /, :scheme: http, :authority: localhost:4142, user-agent: curl/7.52.1, accept: */*, l5d-dst-logical: /h2/localhost:4142, via: h2 linkerd, l5d-dst-concrete: /$/inet/http2.golang.org/443], streamDependency=0, weight=16, exclusive=false, padding=0, endStream=true\n------------------------------------\nDEBUG 0110 23:53:05.460 finagle/netty4-2: [id: 0x31a8f10c, L:/192.168.1.187:64330 - R:http2.golang.org/130.211.116.44:443] HANDSHAKEN: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\nINFO 0110 23:53:05.525 finagle/netty4-2: \n----------------OUTBOUND--------------------\n[id: 0x31a8f10c, L:/192.168.1.187:64330 - R:http2.golang.org/130.211.116.44:443] GO_AWAY: lastStreamId=0, errorCode=1, length=77, bytes=4669727374207265636569766564206672616d6520776173206e6f742053455454494e47532e204865782064756d7020666f7220666972737420352062797465...\n------------------------------------\nDEBUG 0110 23:53:05.531 finagle/netty4-2: [id: 0x31a8f10c, L:/192.168.1.187:64330 - R:http2.golang.org/130.211.116.44:443] Sending GOAWAY failed: lastStreamId '0', errorCode '1', debugData 'First received frame was not SETTINGS. Hex dump for first 5 bytes: 485454502f'. Forcing shutdown of the connection.\njavax.net.ssl.SSLException: SSLEngine closed already\n    at io.netty.handler.ssl.SslHandler.wrap(...)(Unknown Source)\nE 0110 23:53:05.560 THREAD1153757: [C L:/192.168.1.187:64330 R:http2.golang.org/130.211.116.44:443] dispatcher failed\ncom.twitter.finagle.ChannelClosedException: ChannelException at remote address: http2.golang.org/130.211.116.44:443. Remote Info: Not Available\nEspecially: First received frame was not SETTINGS. Hex dump for first 5 bytes: 485454502f.\nIt looks like the client immediately tears down the connection after (it thinks) the TLS handshake is completed.  However, the mystery deepens: 485454502f is the hex representation of HTTP/, as if we receive a plain-old HTTP response.\nPerhaps we're not correctly negotiating HTTP/2 via ALPN?\n. I put together a simple debug server serving over TLS.\nWhat we see is that a request from curl to the debug server looks like:\nGET / HTTP/2.0\nUser-Agent: curl/7.52.1\nAccept: */*\nRequests routed through linkerd hit the debug server with:\nPRI * HTTP/2.0\nAFAIU, we shouldn't be sending the magic preamble after TLS, so I think this is the linkerd bug.. aside: Wireshark can't decrypt streams when diffie helman key exchange is performed, so it's not all that easy to debug the communication from outside the processes...  Perhaps we should change linkerd to support configurable ciphers (at some point).. Nope, that won't matter. HTTP/2 requires ciphers that wireshark can't decode:\nError: http2: TLSConfig.CipherSuites is missing HTTP/2-required TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256. Try --help for help.\nexit status 255\nSo, looks like we're going to have to debug this from inside the app.\nI think I understand what's going on now, though. Shoutout to nghttpd for being a super helpful server to test against:\n:; nghttpd -v 4443 debug_priv.pem debug_pub.pem \nIPv4: listen 0.0.0.0:4443\nIPv6: listen :::4443\n[ALPN] client offers:\n * h2\n * http/1.1\nSSL/TLS handshake completed\nThe negotiated protocol: h2\n[id=1] [  5.126] send SETTINGS frame <length=6, flags=0x00, stream_id=0>\n          (niv=1)\n          [SETTINGS_MAX_CONCURRENT_STREAMS(0x03):100]\n[id=1] [  5.146] recv SETTINGS frame <length=12, flags=0x00, stream_id=0>\n          (niv=2)\n          [SETTINGS_MAX_CONCURRENT_STREAMS(0x03):100]\n          [SETTINGS_INITIAL_WINDOW_SIZE(0x04):1073741824]\n[id=1] [  5.146] recv WINDOW_UPDATE frame <length=4, flags=0x00, stream_id=0>\n          (window_size_increment=1073676289)\n[id=1] [  5.146] recv (stream_id=1) :method: GET\n[id=1] [  5.146] recv (stream_id=1) :path: /\n[id=1] [  5.146] recv (stream_id=1) :scheme: https\n[id=1] [  5.146] recv (stream_id=1) :authority: localhost:4443\n[id=1] [  5.146] recv (stream_id=1) user-agent: curl/7.52.1\n[id=1] [  5.146] recv (stream_id=1) accept: */*\n[id=1] [  5.146] recv HEADERS frame <length=30, flags=0x05, stream_id=1>\n          ; END_STREAM | END_HEADERS\n          (padlen=0)\n          ; Open new stream\n[id=1] [  5.146] recv SETTINGS frame <length=0, flags=0x01, stream_id=0>\n          ; ACK\n          (niv=0)\n[id=1] [  5.146] send SETTINGS frame <length=0, flags=0x01, stream_id=0>\n          ; ACK\n          (niv=0)\n[id=1] [  5.146] send HEADERS frame <length=69, flags=0x04, stream_id=1>\n          ; END_HEADERS\n          (padlen=0)\n          ; First response header\n          :status: 404\n          server: nghttpd nghttp2/1.17.0\n          date: Wed, 11 Jan 2017 05:31:06 GMT\n          content-type: text/html; charset=UTF-8\n          content-length: 147\n[id=1] [  5.146] send DATA frame <length=147, flags=0x01, stream_id=1>\n          ; END_STREAM\n[id=1] [  5.146] stream_id=1 closed\n[id=1] [  5.147] closed\nSSL/TLS handshake completed\nClient did not advertise HTTP/2 protocol. (nghttp2 expects h2)\n[id=2] [ 27.900] closed\nIt seems likely that linkerd's protocol negotiation isn't configured properly. I'll look into that.. It turns out that Finagle doesn't actually support client-side ALPN today.\nBut fear not! I've done the netty version of monkeypatching to get client ALPN working.  Review forthcoming; and then we can figure out how to upstream a proper fix.\n(Aside: I guess that this means that the grpc-go server doesn't actually care about client alpn, which is odd).\nIt turns out there are several tls configuration options not exposed as params on the finagle stack.  @vkostyukov @mosesn @ryanoneill We'd definitely like the ability to configure TLS protocols, ciphers, etc via the stack. Any guidance on the best approach?  Additional TlsConfig types?. merged https://github.com/BuoyantIO/linkerd/commit/0ff4fc2e5fbeaf1bd27b5aa241ad6c1966a1a0ad\n. Is there a problem with the existing setup or is it just a little duplicated code?  It's such a tiny bit of boiler plate that it doesn't seem worth having a json library that's just like 10 lines...\n. merged https://github.com/BuoyantIO/linkerd/commit/59df8f2fe7a975eedf3dd10d88d43b89614091b9\n. Do we know what the linkerd issue is?\n. So the issue is that the output of hostname is not resolvable on the host?  That sounds like a misconfigured host.   What can linkerd do to fix this?\n. Oh right I'm recalling now, we could potentially enumerate all of the local interfaces and try to match against that, but this requires a taking a pretty different approach to this transformer.\nIt sounds fairly broken to me that dig +short $(hostname) should ever fail...\n. And after discussing this a bit with one of the DCOS people, I'm reminded that DNS is just crazy and we should really really do everything we can to avoid relying on it for this sort of discovery.\nI'm told that marathon exports a HOST environment variable containing the IP, but this isn't a very robust solution for linkerd.  I think we should pursue the interface-driven approach.\n. Welcome to the project, @mmolimar!\nI'm not all that familiar with Kafka's API, but from a glance at the docs it seems like it may be possible (though further investigation is needed to figure out exactly how this integrates).\nCan you share any of linkerd's features that you're especially interested in for kafka?\n. merged https://github.com/BuoyantIO/linkerd/commit/cc2290055cfd838eecb661a4654502b3bebc6c85\n. this was fixed by #826 and released in 0.8.4. not yet.  i'll try to throw something together\n. 615d52d\n. It's really tricky to test this without supporting upgrade on the clientside (which we don't).  I've done a manual test for now.\n. per discussion: we're a little concerned that a single pull of the entire dataset could be much heavier resource-wise.  We'd want to be ensure that this system works well in non-trivial deployments.\nIdeally, of course, marathon would provide a non-SSE streaming or watch API.\n. @moderation yeah, this is another approach that could work well especially for namerd.  I'm reluctant to rely on callbacks to hit all linkerd instances in a cluster...\n. Ah hah! It appears we already have this.  From the http protocol config docs.  Setting compressionLevel: 0 on a router should disable compression explicitly.\n. namerd change ticketed: https://github.com/BuoyantIO/linkerd/issues/805\n. I'm thrilled to announce a new project, linkerd-tcp.  This project is still very new, but we think this is the best solution for integrating raw TCP support in the linkerd ecosystem.\nPlease test it, file bugs, feature requests, and pull requests!   And, of course, ask questions!\nFinally, thanks for encouraging us to do this.  I'm really excited about it.\nhttps://github.com/linkerd/linkerd-tcp. I think fail is working as designed -- it causes binding to fail since a NameTree.Fail cannot be bound.  I think you want to use /$/empty instead, which should bind to an empty Name.  IIRC, this also did not work correctly.  But I think /$/fail is working as expected -- it causes name binding to fail.. We should test the behavior without namerd.. Oops, it's /$/nil, not /$/empty.  Finagle a little less-than-consistent here.. Testing wise -- at the least I think we need an EndToEnd test for this.. Hm...\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\n[warn]  ::          UNRESOLVED DEPENDENCIES         ::\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\n[warn]  :: com.twitter#finagle-core_2.11;6.40.0: not found\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\nHas finagle 6.40 been released yet?. @dschobel hah really appreciate your help on this. we sort of Have To do unsupported things, because parts of finagle simply can't be used without touching privates.  I'm going to punt this from this week's release and fold this into the followup.  I can try to figure out what you all broke ;). thanks for the note, @zackangelo. that error is benign but i'll fix logging. Does linkerd log anything?. I'm pretty staunchly against having a totally open-ended plugin interface, since it makes it much harder to reason about what is going on in a request's lifecycle and how a filter behaves in the context of the rest of the stack (routing, retries, timeouts, etc). I really don't want to have to try to explain to users what is going on with arbitrary plugins in the mix.\nI would much prefer to have an authentication plugin interface and a rate-limiting plugin interface (and etc) so we can be clear about the contract, use common interfaces, etc.\nThe need for something much more general than that isn't clear to me.. https://github.com/BuoyantIO/linkerd/commit/8fefc98184f20a659004ca9c95ddb120eaf62736. https://github.com/BuoyantIO/linkerd/commit/9fe3c4d8791d1f7f9779486578ad050bd3c61158. Thanks for the detailed, reproducible report.  It made it much easier to track this down.\nThe problem is caused by linkerd's stripping of the TE header.  the gRPC protocol uses a header value of TE: trailers to inform endpoints that they must support trailers.  linkerd stripped out this header, as it was considered to be a connection-specific header. From RFC 7540 \u00a78.1.2.2:\n\nHTTP/2 does not use the Connection header field to indicate connection-specific header fields; in this protocol, connection-specific metadata is conveyed by other means. An endpoint MUST NOT generate an HTTP/2 message containing connection-specific header fields; any message containing connection-specific header fields MUST be treated as malformed (Section 8.1.2.6).\nThe only exception to this is the TE header field, which MAY be present in an HTTP/2 request; when it is, it MUST NOT contain any value other than \"trailers\".\nThis means that an intermediary transforming an HTTP/1.x message to HTTP/2 will need to remove any header fields nominated by the Connection header field, along with the Connection header field itself. Such intermediaries SHOULD also remove other connection-specific header fields, such as Keep-Alive, Proxy-Connection, Transfer-Encoding, and Upgrade, even if they are not nominated by the Connection header field.\n\nSo, linkerd almost does the right thing.  But it needs to preserve the the TE: trailers header for C++ gRPC servers.. Fix tracked here: https://github.com/BuoyantIO/linkerd/issues/833. Please open a new issue if any additional problems are encountered. Thanks!. @kamilchm Yes. At the moment, we're focused on stabilizing the HTTP/2 codec for gRPC.  Once the underling codec is ready for production, we'll pursue this and other enhancements.. A working end-to-end gRPC Interpreter Service & Client is up on the ver/namerd-grpc branch.  Additionally, I've generated and tested a Go client for this interface.  It all works.\nThis branch needs more automated tests. This branch needs https://github.com/BuoyantIO/linkerd/pull/932 to ship. This will eventually need to be extended to support the explain api.  We need to expose a Stream.reset method that allows the consumer to cancel a stream explicitly.\nOn the API itself:\n- I've tried to strip out things that don't belong in the API: specifically stamps and client ids from thrift.  client ids can manifest as request metadata and don't need to be explicit in the API.\n- Still would like to think through the Controller API a bit more before solidifying Interpreter api.. note: we'd want to explicitly label these namers like:\nyaml\nkind: io.l5d.rewrite\nlabel: hostMethod\npattern: ...\nname: ...\nSo that we accept names like /#/hostMethod/....\nPoint is that it's not useful to see io.l5d.rewrite in a dtab.. In h2 we have two separate identifiers.  I have a mild preference for that approach, but ultimately prefer that it's consistent between protocols, either way.. sorry about that -- circleci seems to have changed some defaults.  non-buoyant PRs should now run through CI. this looks great, @dmexe!  Can you please sign our CLA so that we can accept this? Thanks!. Also, feel free to add an entry to CHANGES.md (otherwise, we'll try to remember to add a note there before release). The logs show zk state changes as follows:\n17:38:30.561  connected\n18:14:06.466  disconnected\n18:14:07.141  connected\n18:14:09.760  disconnected_duration\nWhile there is some seemingly conflicting information in the logs, it appears that the zookeeper client never really reconnects after 18:14.  It's at this point when we start to see 503s from aurora, indicating that we are not talking to the proper leader.. It's very interesting to see this in the logs from 18:14 on:\nDec 08 18:14:47 tfec01fu2 linkerd[2004]: WARN 1208 18:14:47.993 UnboundedFuturePool-2-SendThread(mesos-masterc03fu2:2181): Session 0x358df7884e9000d for server mesos-masterc02fu2/172.31.10.11:2181, unexpected error, closing socket connection and attempting reconnect\nDec 08 18:14:51 tfec01fu2 linkerd[2004]: WARN 1208 18:14:51.256 UnboundedFuturePool-2-SendThread(mesos-masterc01fu2:2181): Session 0x358df7884e9000d for server mesos-masterc02fu2/172.31.10.11:2181, unexpected error, closing socket connection and attempting reconnect\nDec 08 18:14:55 tfec01fu2 linkerd[2004]: WARN 1208 18:14:55.864 UnboundedFuturePool-2-SendThread(mesos-masterc02fu2:2181): Session 0x358df7884e9000d for server mesos-masterc02fu2/172.31.10.11:2181, unexpected error, closing socket connection and attempting reconnect\nAt first, it seems that it's failing to connect to all 3 instances (since we see a SendThread for each zk host), however it seems that all 3 threads are failing to connect to the same host: masterc02fu2/172.31.10.11:2181.\nThe zookeeper client is absolutely in a bad state.. Steps to reproduce should be (unconfirmed):\n\nlinkerd configured with serverset namer.\nexplicitly configured to connect to multiple zookeeper nodes\n\n\na service registered with a serverset\nroute at least one request through linkerd to the service\nshutdown zookeeper nodes\nrestart zookeeper nodes\ncapture linkerd state\nrequests to service through linkerd should fail. dup of #842 . https://github.com/BuoyantIO/linkerd/commit/b4a1ae3807d855b792e0680c133d3de7723f9a99. I'm pretty sure I know what's going on here:\n\n:; docker run -it --rm --entrypoint=bash buoyantio/linkerd:0.8.4 \nroot@3306c8d614d7:/io.buoyant/linkerd/0.8.4# env\nHOSTNAME=3306c8d614d7\nTERM=xterm\nL5D_EXEC=/io.buoyant/linkerd/0.8.4/bundle-exec\nCA_CERTIFICATES_JAVA_VERSION=20140324\nL5D_HOME=/io.buoyant/linkerd/0.8.4\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nPWD=/io.buoyant/linkerd/0.8.4\nJAVA_HOME=/usr/lib/jvm/java-8-openjdk-i386\nLANG=C.UTF-8\nJAVA_VERSION=8u91\nSHLVL=1\nHOME=/root\nJAVA_DEBIAN_VERSION=8u91-b14-1~bpo8+1\nno_proxy=*.local, 169.254/16\n_=/usr/bin/env\nWe're trying to use the 32b JVM image (which uses less memory):\n\nJAVA_HOME=/usr/lib/jvm/java-8-openjdk-i386\n\nHowever, the boringssl fatjar shipped with linkerd is surely only compiled against the 64b jvm.\nWe probably need to use a 64b jvm by default and allow users to move to 32b jvms as needed.. Running with JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 fixes the issue for me, which confirms the above.. In the latest release, we changed the default docker image to depend on library/java, which resolves this issue. (32b images are published separately). Transformers should just get the stats scope from whatever their parent is, probably.  When under the interpreter, the interpreter's stats scope; when under the namer, the namers stats scope.. oh and there are a bunch of zipkin client stats:\n\"clnt/zipkin-tracer/...\n:; grep -c clnt/zipkin-tracer debug.json\n54. We can probably achieve this by adding some system property settings in linkerd/namerd's init script. Caused by: java.io.IOException: No data available in passed DER encoded value.\n        at sun.security.x509.GeneralNames.<init>(GeneralNames.java:61)\n        at sun.security.x509.SubjectAlternativeNameExtension.<init>(SubjectAlternativeNameExtension.java:141)\n        at sun.security.x509.X509CertImpl.getSubjectAlternativeNames(X509CertImpl.java:1670)\n        ... 34 more\nThis looks like a problem with your certs.. linkerd already sets the Forwarded header, which supersedes use of XFF. We can still consider how to log this, but it was a conscious decision to use the newer standard.. I should note that that this is disabled by default, per RFC7239:\n\nDue to the sensitive nature of the data passed in this header field (see Sections 8.2 and 8.3), this header field should be turned off by default.\n\nAnd it appears that this isn't exposed via configuration -- we'll add an option.. Forwarded header configuration support: https://github.com/BuoyantIO/linkerd/pull/905\nWe should probably augment this to rewrite XFF as Forwarded when it can.  Note that, as the RFC points out, requests with both X-Forwarded-By and X-Forwarded-For headers cannot be upgraded, but upgrade; but requests with only, for instance, XFF, should be upgradeable.  We should add that as an additional stack filter as a followup to that review.. Thanks, @fantayeneh. This looks good to me. I'm going to wait a little on pulling this, as I want to think about #904 at the same time.  But this will make the next release.. @adrian yeah, these new namers will be present in 0.8.6 (being released now).  Let us know if you have any further questions!. I have a feeling the documentation may need some work still?. Hi @akreiling! Thanks for filing the issue.\nI think this can be accomplished without code change by layering the /$/io.buoyant.http.subdomainOfPfx namer, for instance:\n/consul => /#/io.l5d.consul/dc1 ;\n/host => /$/io.buoyant.http.subdomainOfPfx/service.local/consul ;\n/http/1.1/* => /host ;\nI think this should Just Work for the situation you describe.. RFC: now that I've done this, i'm considering moving these to be configured namers so that we don't have to do the weird prefix-suffix dance (as done by all Pfx namers). This change is insufficient. linkerd client TLS configurations do not us the netty-compatible config types (due to ca cert path configuration).  TlsConfig is sort of a mess.... This is going to be trickier than it seemed at first:\nThe initial change works only with the io.l5d.noValidation.  This is because the io.l5d.noValidation happily uses a netty ssl context under the hood.\nThe other client validators, as constructed by TlsClientPrep, use a java SSLContext built with a trust chain from a file. ALPN relies on netty's SslContext, but the rest of linkerd depends on java's SSLContext.\nIn order to change TlsClientPrep to not use java's SSLContext, we need to replace the client tls initialization for all of linkerd (and not just h2).\nOur options are basically:\n1. Overhaul all client TLS in linkerd to support an alternative means of configuring trust managers for TLS.  We're going to have to do this eventually when @ryanoneill moves Finagle onto new APIs.\n2. Don't use the default TlsClientPrep module system for h2 client configurations.  We could introduce a new config type that configures h2 differently, but this will require a bit of a refactor in linkerd-core to change TlsClientPrep to stop exposing java's SSLEngine, etc.. After some discussion, I think the short-term plan is the following:\n\nModify this branch to fail to load h2 configs when unsupported TLS configurations are configured\nh2 will only support io.l5d.noValidation for now\nwe'll wait to see what's going to land in the next finagle release before we figure out whether we can move onto the new finagle types or if we have to solve this ourselves (to remove experimental from h2).. As soon as I left the office yesterday, I realized there's a simpler solution (which I've implemented and will push momentarily):\nMove TlsClientPrep to a new finagle/core project, usable from finagle/h2.\nTlsClientPrep now exposes two stack roles:\nTlsClientPrep.role is responsible for configuring the (new) TlsClientPrep.TlsEnabled and  TlsClientPrep.Trust params.\nTlsClientPrep.role.finagle is responsible for consuming these params and configuring finagle params on the underlying stack (i.e. Transport.Tls, etc).\nAn alternative implementation of the Tls.ClientPrep.role.finagle module can be used (or it can be elided entirely) to support ALPN.\nWhen finagle changes the TLS config api, we should be able to update this module only without changing TlsClientPrep module implementations.\n\n\nModify TlsClientInitializer to expose tlsClientPrep[Req, Rsp]: Stackable[ServiceFactory[Req, Rsp]], which is installed into the TlsClientPrep.role stack slot. I'm not too clear on the details of how ThriftMux does protocol upgrade/downgrade.\n\nIf I were going to start working on this, I'd probably create a new protocol plugin, thriftmux, and figure out how to implement a Router that does the right thing.  I think the wrinkle is that when you typically instantiate a ThriftMux service in finagle, you bind the service to a specific Thrift service.  In linkerd, we can't do this, so we need to figure out how to create a Thrift service with a ThriftOrMux transport.  It might be possible to just add this into the thrift router directly, but it'll probably be easier to sort out the details in its own module.. thanks @apakulov . Or we can just backtick-escape these definitions like:\nscala\n`type`: Option[grpc.testing.PayloadType.Value]. Fixed https://github.com/linkerd/linkerd/commit/16cc88be67f42b37424d532feaee49ad5669ba1b. Fixed https://github.com/linkerd/linkerd/commit/caeae411af761b981563d356838aa1753ec6e99d. I'm not sure how this test is supposed to work:\nfrom test_util.go:\ngolang\n    largeReqSize        = 271828\nbut with linkerd advertising a window size of 64K, we can never fit this message in the window.\nWe'll need to be able to configure linkerd with a large initial window size for this test to work.. that's wild. fixed in #1024 #1032 . fixed https://github.com/linkerd/linkerd/commit/7cb015954d1f9d90ec16da5daf15fb3fa5d7b2de. It may be difficult to measure this exactly as described, though I think we can come up with some good-enough heuristics and tools.  Let me explain:\nWhen running with namerd, routing information isn't distributed to linkerd instances (i.e. linkerd never \"downloads\" a Dtab).  Instead, linkerd resolves individual names through namerd, and maintains watches to receive updates. (In the thrift interface, this is accomplished through long-polling, though the replacement for this--currently in testing--will use streaming responses). This allows the details of service discovery lookup to be centralized/cached in namerd.\nIt's fairly simple to expose a view of the namerd client's health in terms of success rate and throughput, which indicates whether the client has received some updates but can't really answer the question of whether the instance has received all updates for the following reason:\nNot all linkerd instances observe the same names at the same time, and so updates are propagated unevenly. Consider a situation where appA runs with linkerdA, appB runs with linkerdB, and appA talks to appB through linkerdA.  linkerdA will have a watch for a name like /s/appB, but linkerdB will have no such watch.\nIn the near term, we can recommend namerd client metrics that indicate whether the client is generally healthy. We can probably augment this with additional diagnostic tools (i.e. that use linkerd's admin API) to allow you to ask deeper diagnostic questions of an individual running linkerd instances.. Repo moved! We'll consider moving other repos separately (i.e. we'll do it, but not this second ;). Even if the gRPC spec dictates this (which I would be surprised if it did), intermediaries (like linkerd) are free to re-frame messages as they see fit.  Any assumption that couples transport framing with message delimiting is going to be fragile.  I think I have a pretty straightforward approach we can take for the time being -- sketching that out presently.. Fixed https://github.com/linkerd/linkerd/commit/33424702b6808400af7a3ad1961c56351107df41. still need to add some explicit tests for the conditions we're concerned about. only unit-tested. should manually test before merging. @klingerf thanks for the help verifying this!. fyi, @fantayeneh we plan to change the default identifier is 0.9.0 ;). https://github.com/linkerd/linkerd/wiki/Linkerd-code-of-conduct. @wmorgan ping. merged in #1024 . This sounds reasonable to me. nitpick: are we instructing linkerd to use the node address? Or are we instructing it not to use the service address?  It might be clearer to describe this as ignoreServiceAddress or something along those lines.. google claims that browsers should cache the fonts https://developers.google.com/fonts/faq#will_web_fonts_slow_down_my_page\nDo we see this in practice?\nre: build tool; As it is now, we only really depend on java/mvn, and then you run ./sbt linkerd/assembly, and all of the dependency management just happens. I'm very squeamish about adding other build-time dependencies that can't be managed by sbt.  I just voliated this with protoc, in fact, and it's caused problems.  We can work around this by fetching/caching artifacts for the build (like we do with sbt) -- perhaps this could work for grung/gulp/etc, but I worry we biting off a bunch of dependencies.\nHow crazy of an idea is it to split the admin UI into a separate repo that produces a bundle of js,css,html assets? This way, linkerd can depend on a released tarball of the UI and  the UI buildchain can be separate. I think we could still make this work without making UI development much harder (but i'm guessing)--we'd have to start linkerd with a config that reads its admin json from e.g. ../linkerd-ui/assets, but a build in the linkerd-ui repo would produce artifacts that could be served instantly from the running linkerd.  Is this crazy? Would it help?. Hi @haggag.  Good question.  At the moment, linkerd doesn't expose any good primitives for this.  I believe that some users have implemented some form of rate limiting by implementing an Identifier plugin that fails requests in some conditions, but this isn't a great solution.\nI have some concrete ideas on how I'd like to solve this in the near future, and I've opened https://github.com/linkerd/linkerd/issues/1006 to track this.\nI'm going to close this issue out for now, but feel free to reopen if you have further questions!. We could potentially dockerize the interop clients so they can be invoked via docker run.... merged in #1024 . cool, I know how/where to fix this (once we have GrpcError types): https://github.com/linkerd/linkerd/blob/master/grpc/runtime/src/main/scala/io/buoyant/grpc/runtime/ServerDispatcher.scala#L164. Fixed here https://github.com/linkerd/linkerd/commit/caeae411af761b981563d356838aa1753ec6e99d. I think this is a bug in the test server and not grpc interop: https://github.com/linkerd/linkerd/pull/978#pullrequestreview-18535493. Ok, so I improved the interop server implementation, but there's still a problem here: I think, due to flow control, linkerd isn't capable of buffering some of the larger body sizes.  Basically, linkerd is trying to buffer a ~45KB-sized message, it's read ~32KB, and the client stops sending additional frames... it seems likely that this is because the server hasn't opened the window further (because we use flow control to limit memory pressure).\nAt the very least, I think we'll need to expose better window size configuration so we can accommodate larger streams. (May be tricky to wire into netty, we'll see).\nNote that this should not be any problem for grpc messages transiting h2 routers. This is purely for scala clients/servers generated with the grpc tools in this dir.. Fixed in #1024. i think is was a bug in the test server and not in our grpc interop: https://github.com/linkerd/linkerd/pull/978#pullrequestreview-18535493. Fixed\n:; go-grpc-interop-client -use_tls=false -server_port=60001 -test_case=empty_stream        \n2017/01/28 00:29:31 Emptystream done. Fixed in #1024 . This test completes on https://github.com/linkerd/linkerd/commit/0cacf46ed51ba51408e7c6a7d5cdc2bf751d0f9b:\n```\n; ~/b/l5d  (ver/grpc-interop-server \u2718)\n:; go-grpc-interop-client -use_tls=false -server_port=60001 -test_case=server_streaming\n2017/01/27 23:44:50 ServerStreaming done\n``. works as of https://github.com/linkerd/linkerd/commit/2e61fb272f940cec6391562f82af3ed8414becf8. We didn't support status messages before. Now that we do, the test server needs to be changed to support reading the response_status field on requests. https://github.com/linkerd/linkerd/commit/2e61fb272f940cec6391562f82af3ed8414becf8. fixed in #1024. our grpc implementation currently ignores all metadata.  We should probably jam this into the context or expose it in interfaces likedef call(req: Req, md: Metadata): Future[Rsp]? Or something. This is a reasonable request.  If it's not done before then, we should consider this as a part of 1.0 API/config stabilization. That's an interesting workaround, kevin. I think if we were going to do something like that, we should generalize it asl5d-local-*` and mandate that services not forward these headers. Thanks for the well-written issue. We should fix this before making netty4 default (cc @adleong). @adleong that sounds like a reasonable assumption. We can probably ameliorate this when processing the config (i.e. if streaming is set and request size is set and chunk size is not, set chunk size... or something). Mostly i meant to flag this as something we should do before making netty4 default.. Can you describe this problem a little more (i.e. in the commit message)?. dropping this in favor of https://github.com/linkerd/linkerd/pull/1065. let's stay away from /go -- way overloaded.\nmy preferred are /svc, /srv, /call, /dst, /to.\nI think it's somewhat important to have a scheme that is both succinct and allows for extension (i.e. for source-prefixed schemes). https://github.com/linkerd/linkerd/commit/bc545e1d57fd53d124a1499a034c3ad5b671ca47. @apakulov Please excuse the delay. We've been prioritizing issues which are blocking the 0.9.0 release. We'll try to get to this soon, though. Thanks for your patience.. I think I'd prefer that we:\n- Leave GET as it is\n- Add a POST, that takes a json object as the body\nAlso note that in POST requests, query parameters are always form-encoded into the body (and not in the HTTP resource). \n. It turns out that ratios have to be on (0.0, 1.0) (exclusive).. \u2b50\ufe0f myself. Combining this with #1034 . needs docs.... ```\nThe following restriction on distributions, which is part of OFL, has been widely accepted by open source projects when it is applied to fonts:\n1) Neither the Font Software nor any of its individual components, in Original or Modified Versions, may be sold by itself.\n```\nThis sounds like it's not onerous.  We'll probably need to bundle the license with the distribution... I'm not a lawyer.. From http://theleagueof.github.io/licenses/ofl-faq.html\n\nYes! Fonts licensed under the OFL can be freely included alongside other software under FLOSS (Free/Libre and Open Source Software) licenses. Since fonts are typically aggregated with, not merged into, existing software, there is little need to be concerned about incompatibility with existing software licenses. You may also repackage the fonts and the accompanying components in a .rpm or .deb package and include them in distribution CD/DVDs and online repositories. (Also see section 5.9 about rebuilding from source.). https://github.com/linkerd/linkerd/commit/98210aeb9dcf924f85155a365db93e2042dcf773. I'm skeptical that linkerd should be responsible for monitoring the underlying system.  Surely there's a way to configure this sort of monitoring directly with prometheus, etc?. I'm removing this from 0.9.0. I'd rather spend some more time working through this branch than merge it in a state where we recommend against its use.. Manually tested with retries.  I think we can pull this in if we want.. it's actually the plan to remove this feature, so we should fix the underlying issues instead.. Ok, digging into this a bit... linkerd is actually doing the RFC-defined behavior with regard to removing content-length when chunking a message.\nThe Content-Length header field must not be sent if these two lengths are different (i.e., if a Transfer-Encoding header field is present).\n\nSo the real problem is that linkerd automatically chunks these messages. I'm digging into the underlying APIs to figure out what the best course of action is.. Thanks for looking further @fantayeneh.\nI believe that in netty4, the maxChunkKB configuration must be raised to satisfy the large request (it treats the entire message as a chunk).\nI think the real problem is that when linkerd receives a large message, it automatically chunks it (because the client and server are configured with streaming).  Ideally, we wouldn't change the chunking for any messages.  However, we really really want to run our endpoints with chunking, since this allows for linkerd to process message streams without buffering. I did a bit of investigation on this last week and discovered that, as best I can tell, Netty EITHER buffers an entire message up to content-length, or it sends chunk-encoded frames. there's currently no way to read/write a partial message into/out of Netty without chunked encoding.\n. Hi @teodor-pripoae. There's no reason that this can't work outside this repo, but there's some work that needs to be done to make it easier.\n\nOur repo has special SBT configuration that builds the protoc-gen-io.buoyant.grpc protoc plugin.  You'll need to build this manually form our repo with ./sbt grpc-gen/assembly.  This will produce a binary that should be placed on your $PATH.\n\nYou'll need to add the sbt-protobuf plugin and override it to use additional settings including:\n```scala\n  val grpcGenSettings =\n    protobufSettings ++ inConfig(protobufConfig)(Seq(\n      javaSource <<= (javaSource in Compile),\n      scalaSource <<= (sourceManaged in Compile) { _ / \"compiled_protobuf\" },\n      generatedTargets <<= scalaSource { d => Seq(d -> \".pb.scala\") },\n      protocOptions <<= scalaSource { ss =>\n        Seq(s\"--io.buoyant.grpc_out=plugins=grpc:${ss.getCanonicalPath}\")\n      }\n    )) ++ Seq(\n      // Sbt has trouble if scalariform rewrites the generated code\n      excludeFilter in ScalariformKeys.format := \".pb.scala\",\n      coverageExcludedFiles := \"\"\".*.pb.scala\"\"\",\n// We don't need protobuf-java.\n  libraryDependencies <<= libraryDependencies(.filterNot(.name == \"protobuf-java\"))\n)\n```\n\n\nThis should really all be packaged as an SBT plugin, but I haven't had the time to do that.\nFurthermore, the APIs are not very well documented yet, so please don't hesitate to ask questions here or in the #finagle room on slack.linkerd.io!. @teodor-pripoae we'll upgrade to the newest finagle after the 0.9.0 this release.. We'd really like to make it easier for others to work on our finagle-h2 and finagle-grpc implementations.  I'd like to propose the following:\n\nWe'll create a new repo with copies of linkerd's finagle and grpc subprojects.  The finagle project houses our h2 implementation and the grpc project has the protoc plugin, runtime library (that depends on the h2 impl, etc).\nWe'll probably need help putting together an SBT plugin.\nOnce we can publish all of this from the new repo, we'll update the linkerd repo to depend on these libraries.\n\nPractically, we're not likely to make much progress on this until after linkerd-1.2.0 is released (unless we find some free time).  This is about 2 weeks out.  I've opened https://github.com/linkerd/linkerd/issues/1588 to track this.. We could potentially use packagecloud.io. I don't think it should be a section. We can call it out on individual items when appropriate.. @Ashald We'd probably take a pull request that did something like that! ;). @huggsboson i think it's reasonably straightforward to fix this, and I think we should definitely pursue more flexible TLS configurations.  We've been waiting on finagle 6.42, which adds support for clientside tls configuration.  Once this merges (today or tomorrow), I think it's simply a matter of exposing the proper configuration options from linkerd. The coding work is probably fairly minor for this, though the devil is always in the details.. We're waiting on 6.44 because 6.42 had a show-stopping bug and the fix didn't make it in 6.43.  Once we're on 6.44--if there are no new show-stoppers--we'll be able to dig more deeply into this feature.  So, we can hopefully start work on this soon.  We'll be able to give a better idea of when we'll be able to ship this feature once we've gotten a bit further into the investigation.\nWe definitely want to do it soon.  There's also a chance this feature may land in linkerd-tcp before it lands in linkerd.. Ah, this relates https://github.com/linkerd/linkerd/issues/1033.  For the time being, you'll need to install protoc 3.0+ either via a package manager like homebrew, or by downloading a binary from the protobuf release page: https://github.com/google/protobuf/releases.. thanks @djKooks!. @ashald pull requests accepted \ud83d\ude09\nOn Thu, Mar 2, 2017 at 08:52 Borys Pierov notifications@github.com wrote:\n\n@Ashald commented on this pull request.\nIn linkerd/docs/namer.md\nhttps://github.com/linkerd/linkerd/pull/1117#discussion_r103972647:\n\n@@ -55,7 +55,7 @@ apps    users   web\n $ cat config/web\n 192.0.2.220 8080\n 192.0.2.105 8080\n-192.0.2.210 8080\n+192.0.2.210 8080 * 2.0\n\nWas just passing buy and thought that maybe using YAML here as you do for\nall the configs would've been nice... :)\n/me goes away\n\u2014\nYou are receiving this because you were assigned.\nReply to this email directly, view it on GitHub\nhttps://github.com/linkerd/linkerd/pull/1117#pullrequestreview-24782783,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAOsYv5Hs17iYUrlu4kDT6qbndk6awYsks5rhvOwgaJpZM4MQkVi\n.\n. It looks like Finagle 6.43 is out.  Are we waiting for 6.44?. Also note what the spec says:\nIntermediaries that process HTTP messages (i.e., all intermediaries other than those acting as a tunnel) MUST send their own HTTP-Version in forwarded messages.  In other words, they MUST NOT blindly forward the first line of an HTTP message without ensuring that the protocol version matches what the intermediary understands, and is at least conditionally compliant to, for both the receiving and sending of messages.  Forwarding an HTTP message without rewriting the HTTP-Version might result in communication errors when downstream recipients use the message sender's version to determine what features are safe to use for later communication with that sender.. Can you share linkerd's metrics? (:9990/admin/metrics.json?pretty=1). to add some color here: lifecycle management, especially with regard to tearing things down, is quite difficult to get right. This isn't a \"we'll never do this,\" though; probably just a \"too complicated for the nearterm future.\". Thank you for the excellent, detailed bug report. I think this should give us enough to reproduce and investigate further.. @esbie plugin names are not in a global namespace, so it's possible that we have things called io.l5d.default in other interfaces (i'm not sure).\n\n@siggy there will be more kinds, possibly with configs sections. Whoops! sorry for dropping the ball on this.  If you resolve the merge conflicts we should be able to pull this.. Hi @thekalinga. Thanks for the question.\nFirst of all, we started working on linkerd well before kubernetes 1.0.  linkerd is not and will never be a kubernetes-specific project.  linkerd should work well with kubernetes, but it should also work well in other environments (and should be especially useful in bridging orchestrators).\nThe primary reason that linkerd was written in scala is because we have extensive production experience with finagle, which is a networking library that we helped develop while operating production services at Twitter.\nWe chose finagle (and therefore scala), because there is a rich, battle hardened, mega-scale feature set available to us.\nThat said, we're continually and actively making investments to improve resource utilization.  Watch this space!. that's an odd error. i hit rebuild, which i expect to fix it. @macalinao it's definitely not your branch. seems like something's flakey in the ci running scripts. i've bounced it again, but in the worst case we'll test locally before merging.  (I'd expect early this coming week.). It think it would be best to tie this behavior to the clearContext setting, so it can be controlled per-server (and not on the entire process).  I think the semantics are: if the l5d-err header is set, drop the message body.. We certainly could have a timeout filter above the retry filter in the path stack to achieve post-retry client timeouts.\nAll-in-all I think this is okay, but for 1.0, we should write a doc on timeouts so that our thinking is clear.  We may want/need to fill in some gaps/rename some things for clarity before cutting f'real 1.0.. This is a good idea.  Though, I'd argue we don't need an onStart -- this should be a part of the plugin's Initializer -- and we should make Identifier implement Closable, like most other plugin interfaces do.. I think we should rename the http identifers as well, then, to include .http ?. thanks @fantayeneh! this is helpful. @adleong I like this proposal, assuming the classification stuff actually works.  minor quibble that i'd call it l5d-rsp-class or something like that -- i don't think this is really context in the same way other things are on requests.  For instance, when a single response transits multiple linkerds, the clients and services of the upward linkers shouldn't honor this header blindly.. For more context on why we use Forwarded and not X-Forwarded-For see RFC 7239.  Basically, X-Forwarded-* is ambiguous in many scenarios, and this RFC lays out a standard that avoids these ambiguities.. @zircote would you mind elaborating on your motivations for this?\nMy (hopefully incorrect) understanding is that zookeeper's password authentication provides no real security benefit, since credentials are transmitted in plaintext. I just want to make sure this feature is going to be actually useful and not snake oil ;). Ok, that's the answer I wanted to hear ;)  This sounds reasonably easy if it's acceptable to have these credentials stored in linkerd configs.. Thanks @DukeyToo, glad you resolved it.  We think there may be a memory leak in the mesh API but haven't had time to do proper profiling yet.  Hopefully we'll be able to take a deeper look at this in the coming days.. good catch, @amedeedaboville! thanks and welcome to the project ;D. i think the mesh api listens publicly by default.  @DukeyToo mentioned in slack that this behavior is seen with all interfaces.  my first guess is that it has something to do with storage:\nkind: io.l5d.zk\n  experimental: true\n  zkAddrs:\n  - host: 127.0.0.1\n    port: 2181\n  pathPrefix: /dtabs\n  sessionTimeoutMs: 10000\nIs there a zookeeper on localhost with a directory /dtabs?. can you grab a stats dump from both linkerd and namerd when it's in a bad state?  linkerd:9990/admin/metrics.json?pretty=1 and namerd:9991/admin/metrics.json?pretty=1. @DukeyToo yeah, that could create a large number of watches... but still, that error behavior is really not good.\nWe've intentionally avoided making it easy to express this sort of complex identification logic because we really expect service names to be uniform... I'm curious how your dtab handles this level of irregular naming?\nDepending on the logic you're trying to achieve, this may be a good case for a custom Identifier plugin.. It looks like we're relying on library/java:openjdk-8-jre, so these problems may exist in the default public dockerhub java image!. good news: it looks like there are now alpine-based jre images \ud83d\ude04 \nthe bad news: literally all of the available jre images appear to have vulnerabilities \ud83d\ude31 \nhttps://hub.docker.com/r/library/java/tags/ -- we may still want to switch to the 8-jre-alpine tag for size reasons. @maneeshvittolia Currently, we rely on the public JRE dockerhub images. As noted, all of these images are currently subject to CVEs.  I think we should definitely consider moving to the alpine-based image, but that won't resolve the open security issues.  It looks like there's already an issue queue for CVEs on these images.  We will have to wait until these issues are fixed upstream.\nIn the meantime, it should certainly be possible to roll your own base image -- the linkerd-specific additions are quite small, essentially just setting the L5D_HOME environment variable and adding our release binary.\nI'm going to close this for now, since we have #1211 to track moving to alpine, but there's no action that the linkerd project can reasonably take to resolve this at this point ;/\nThanks for reporting this, though!. What are the http1 identifiers named?. This isn't strictly a unit test, because it's testing that the file system watcher works.  I assume this is a timing issue, where we the fs watcher doesn't observe any updates in time for the test.\nI took off the bug label and created a new flakey label.. It's technically bounded, just with a very high bound... something like 2^31-1 / 2 ;). Good timing! Alex just posted a review which introduces the l5d-success-class response header.. The alpine image does appear to improve linkerd's image size by roughly 30%: 133MB vs ~200MB.. Are slashes forbidden by the RFC? Is this just  a bug that we're not\nproperly encoding headers?\n. @adleong I think it would make sense for what you describe to be rolled into the clearContext feature.... No, slashes are obviously allowed in http headers (e.g. text/html).  loggy is silly.\nThis does seem like a reasonable client feature.. I don't see anything obvious in https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md#v160. We'll probably need to compare the raw responses for the Endpoints API between 1.5.x and 1.6.x. Thanks for the detailed report, @bashofmann.\nAre you able to share linkerd's metrics when (1) linkerd is routing requests to my-service and (2) when consul is restarted and linkerd is still attempting to route requests to the downed instance?  I'm particularly curious about my-service's loadbalancer stats (which show additions/removals/size/available) as well as metrics for the consul client itself.\nAlso, does linkerd stay in this state indefinitely (say, more than 5 minutes?)\n. thanks! @Ashald you haven't seen anything like this, have you?. The issue is that namerd's TlsServerConfig builds Netty3-specific TLS types, which require openssl to be installed on the system.  In #1214, we moved to a smaller base image that does not include openssl, and so listening fails.  The most expedient solution is to set namerd's dockerBaseImage := \"library/java:openjdk-8-jre\".  We should follow up to move namerd to netty4 so that we can use the more minimal image.. That's a potential solution... but it will require changing TLS key formats...  So not without going to 1.1.0. Thanks for the detailed report, @rclayton-the-terrible. the io.l5d.namerd.http interpreter relies on streaming responses (via the Chunked transfer encoding).  Can you check that the ALB is actually sending responses? I suspect that the ALB is re-chunking the responses such that linkerd never receives the update.\nHere's an example streaming response (from localhost):\n```\n:; curl -v 'localhost:4180/api/1/bind/default?path=/svc/default&watch=1'\n   Trying ::1...\n TCP_NODELAY set\n Connection failed\n connect to ::1 port 4180 failed: Connection refused\n   Trying 127.0.0.1...\n TCP_NODELAY set\n* Connected to localhost (127.0.0.1) port 4180 (#0)\n\nGET /api/1/bind/default?path=/svc/default&watch=1 HTTP/1.1\nHost: localhost:4180\nUser-Agent: curl/7.51.0\nAccept: /\n< HTTP/1.1 200 OK\n< Transfer-Encoding: chunked\n< \n{\"type\":\"leaf\",\"bound\":{\"addr\":{\"type\":\"bound\",\"addrs\":[{\"ip\":\"127.0.0.1\",\"port\":9990,\"meta\":{}},{\"ip\":\"127.0.0.2\",\"port\":9990,\"meta\":{}}],\"meta\":{}},\"id\":\"/#/io.l5d.fs/default\",\"path\":\"/\"}}\n``. Hmm that _looks_ correct -- you're actually getting a response message (the full response stay open so that future updates can just be sent on that stream).  There should be metrics on linkerd's:9990/admin/metrics.json?pretty=1that reflect the client's state; and namerd has metrics on:9991/admin/metrics.json?pretty=1that reflect the server's state. . @rclayton-the-terrible excuse the delay, i've been mostly-offline for the last week. Those metrics may help debug this.. The port name could either be encoded in the name itself (as is done in kubernetes) or this could be made an explicit configuration option on the namer.. @leozc i expect that we'll do a release by early next week, depending on progress of some other pending issues.. I have a feeling the usage data reporter doesn't need any stats that would be reaped, but we should confirm. Even if it does, i think I'd prefer to try to get the usage reporter to compensate for this in some other way (like polling stats more aggressively than the idle timeout and saving them to be reported later).. Does this remove support for pkcs#8, or simply augment the existing support?. @adleong the current netty3 behavior requires openssl to shell-out. We were hoping to move to an openssl-less image after the netty4 upgrade...  I think the ideal behavior would be to only invoke openssl when the key is in the legacy format.. This looks to me like an undesirable interaction between thehttp_proxysetting and grpc... The grpc client is trying to tunnel http2 over http1, rather than speak http2 directly. linkerd doesn't currently supportCONNECT`-based tunneling (and we have linkerd-tcp for raw tcp routing). Even if we did support http/1 CONNECT tunneling, linkerd would be of limited value, as it would be unaware of individual http2/grpc requests.\n\nThe best way to use linkerd with grpc is to simply use the linkerd h2 port as the address for the grpc client. linkerd can route grpc requests based on well-formed service URIs.\nI'm unsure if there's an http2_proxy environment setting supported by any of these frameworks.  I think we'd need something like that.... Will we need to do this for all netty4 servers?\nHave we tested the clientside?. thanks for the great report, @jamessharp!. reopening this because @aguilbau continues to see some sort of memory leak.  The current theory is that this arises when namerd watches the kube-system namespace which, in some configurations, updates every few seconds.  I suspect that we can reproduce this by automating service changes in k8s.. it would be helpful to have stats snapshots from namerd when its near OOM state.  i'm not convinced that the consul and k8s issues are related, though it's certainly possible that this issue is in the namerd interface and not the namer itself.  metrics will help narrow down what's happening. i'd get stats directly from namerd's admin port if possible: :9991/admin/metrics.json?pretty=1. We were looking at reproducing this, but I'm not really clear on how consul is configured to expose kubernetes' service discovery information. Can you share any resources to help us better understand your configuration?. I'm mostly trying to nail down some use case we can reproduce.  Just to confirm... you see the issue with either the io.l5d.k8s or io.l5d.consul namers, and only when namerd has resolved names in the kube-system namespace (which updates ~every 2s)?  Is that all correct?. You're correct.  Our sbt setup currently runs some git commands to figure out if it's at a stable release tag.  This should probably be changed to handle non-git builds (or our archive should include some sort of .git).. @zackangelo I'm pretty sure that's indicating that your client is timing out before the response completes. \ud83c\udf89 thanks for doing so much work to get to the bottom of this @adleong @klingerf!\nFor those following along:\nIf a linkerd h2 endpoint (either client or server) received a RST_STREAM message as it was trying to send a frame on that stream, netty could throw an exception because it thought a write was being attempted on a non-existent stream. It would then try to create the stream (even though it already been closed), but would fail when trying to allocate a client stream on the server (or vice versa).\nThe fix @adleong pulled in prevents the h2 encoder from trying to create a stream that has already been used.. What configuration are you changing frequently? Our goal is to push all of the things that change frequently outside of the main configuration.. @DukeyToo we have some plans to make the service/client configurations dynamic -- as a first step, as separate configuration files that are watched for changes, and later even integrated into the namerd API.  I definitely agree it's a valuable feature.. the way i've done this sort of thing in the past is passing in the config on stdin like:\n./genconfig.sh |./linkerd -\nwhere genconfig.sh has something like:\n```sh\n!/bin/sh\nset -eu\ncat <<EOF\ninterpreter:\n  kind: default\n    transformers:\n    \u2013 kind: io.l5d.specificHost\n      host: $LOCAL_IP\nEOF\n```. It's fine to add a linkerd-specific solution for the time being.\nYou'll likely want to write client and server stack modules that install the appropriate filtering logic to fail requests with multiple content-lengths on the server and fail responses with invalid content-lengths on the client.\nThe tricky part here is figuring out where to install these filters so that we measure success/failures properly.. thanks for reporting this.  i suspect that this is a new feature in Finagle (the library linkerd uses) that isn't working so well...\nIf I had to guess: I suspect that a linked http1 server is receiving and http2 frames and trying to do the \"right thing\" by upgrading, however when linkerd tries to forward this to an http1 client, the underyling http2 request can't be downgraded automatically...\nWe should investigate this and, if my guess is right, prevent http1 servers from trying to handle http2 requests.. I can't even venture a guess at why this would be the case.. There's an error responder on the server stack only for convenience (so that errors on the server are handled).  It's conceptually correct for the the error responder to be at the top of the router stack -- exceptions communicating with the client are turned into responses, which the server just transmits.  This way we get a clear indication of where exceptions are being thrown (in the server or in the client). We should probably just replace the default inet resolver throughout linkerd.  i don't think that's ever good behavior for linkerd?. Note: it's fine for linkerd's config to continue to reference response classifiers.  i think in the code, though, we should be clear that these are separate APIs. I'm not too worried about regressions in this case. if it works, ship it. Hey @vadimi.  This looks to me like you're using a token identifier instead of a path identifier?  Can you share your linkerd config?. Yeah, I think using the path identifier should resolve the issue you see for grpc -- basically it's trying to read the /service/method as a single path element instead of as a path. Sorry for the radio silence on our end.  We've been focused on a number of issues that touch finagle-h2, and moving this subproject out of this repo would add significant friction to our debug cycle...\nI am not against this change in general, though I think it is ill-advised to do this before these features are considered stable in linkerd.. @cponomaryov thanks for submitting this!  At a first glance, this looks great.  I think we'll want to do a bit of manual testing on this branch before merging it, but I hope to be able to include this in the next linkerd release (if not this one, certainly the following one).. sorry, assume a dtab rule like /svc/book => /$/inet/10.0.0.2/8080/api/published. @ccmtaylor The Var/Activity apis are a bit nuanced and are difficult at times for all of us ;)  If you poke around some other namers you may find an idiom like the following:\n```scala\nimplicit private[this] val timer = DefaultTimer.twitter\n// ...\n  Activity(Var.asyncActivity.State[NameTree[Name]] { state =>\n @volatile var stopped: Boolean = false\n\n  def loop(): Future[Unit] = {\n    if (stopped) Future.Unit\n    else pool(resolve(name)).transform { result =>\n        result match {\n          Ok(addrs) =>\n            state() = Activity.Ok(NameTree.Leaf(Addr.Bound(addrs)))\n\n          Throw(e) =>\n            log.error(e, \"resolution error: %s\", name)\n            // it may be appropriate to fail the resolution (i.e. Activity.Failed) if there was a pending state before\n        }\n\n        // Wait a TTL before resolving again.\n        Future.sleep(ttl).before(loop())\n    }\n  }\n\n  val pending = loop()\n  Closable.make { _ =>\n    stopped = true\n\n    // Cancel the timer notification or pending requests\n    pending.raise(...)\n\n    Future.Unit\n  }\n\n})\n```\n... or something roughly like that.. To the best of my knowledge, thrift does not encode the service name in the message structure, and so the router has no means to route by service.  This is a deficiency of the thrift protocol.\nMore modern RPC protocols like Mux or gRPC (which can use thrift encoding for payloads), have a metadata layer that encodes this information, and so routers are easily able to multiplex services based on the headers.. I'm pretty sure that this is benign and that we just need to alter the logging to only log at the warning level when the failure isn't an interrupt (otherwise we should probably log at the debug level).  As always, patches welcome ;). @adleong i assumed that since consul exposes things via DNS, that there is some DNS restriction somewhere. I don't actually know if that's appropriate here.... @edio Can you share some more context around the use of delegator.json? What exactly are clients trying to do? Is this just to avoid the two calls to bind & addr?. @b-hoyt is there anything special about the filesystem under which /private/tmp/output/disco is mounted? Is this a docker volume mount point? tmpfs? something else exotic?. linkerd honors the standard unix environment variable, TZ.  For example:\n```\n:; TZ=UTC date\nThu Feb  1 14:48:56 UTC 2018\n:; TZ=UTC+8 date\nThu Feb  1 06:48:59 UTC 2018\n:; TZ=PST8PDT date\nThu Feb  1 06:50:48 PST 2018\n```\nif you run linkerd with TZ set on the environment, log lines should be printed in the proper timezone.  For example:\n:; TZ=UTC ./linkerd\n-XX:+AggressiveOpts -XX:+CMSClassUnloadingEnabled -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled -XX:+CMSScavengeBeforeRemark -XX:InitialHeapSize=33554432 -XX:MaxHeapSize=1073741824 -XX:MaxNewSize=348966912 -XX:MaxTenuringThreshold=6 -XX:OldPLABSize=16 -XX:+PrintCommandLineFlags -XX:+ScavengeBeforeFullGC -XX:-TieredCompilation -XX:+UseCMSInitiatingOccupancyOnly -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+UseStringDeduplication \nFeb 01, 2018 2:54:51 PM com.twitter.finagle.http.HttpMuxer$ $anonfun$new$1\nINFO: HttpMuxer[/admin/metrics.json] = com.twitter.finagle.stats.MetricsExporter(<function1>)\nFeb 01, 2018 2:54:51 PM com.twitter.finagle.http.HttpMuxer$ $anonfun$new$1\nINFO: HttpMuxer[/admin/per_host_metrics.json] = com.twitter.finagle.stats.HostMetricsExporter(<function1>)\nusage: linkerd path/to/config\n:; TZ=America/Los_Angeles ./linkerd\n-XX:+AggressiveOpts -XX:+CMSClassUnloadingEnabled -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled -XX:+CMSScavengeBeforeRemark -XX:InitialHeapSize=33554432 -XX:MaxHeapSize=1073741824 -XX:MaxNewSize=348966912 -XX:MaxTenuringThreshold=6 -XX:OldPLABSize=16 -XX:+PrintCommandLineFlags -XX:+ScavengeBeforeFullGC -XX:-TieredCompilation -XX:+UseCMSInitiatingOccupancyOnly -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+UseStringDeduplication \nFeb 01, 2018 6:54:39 AM com.twitter.finagle.http.HttpMuxer$ $anonfun$new$1\nINFO: HttpMuxer[/admin/metrics.json] = com.twitter.finagle.stats.MetricsExporter(<function1>)\nFeb 01, 2018 6:54:39 AM com.twitter.finagle.http.HttpMuxer$ $anonfun$new$1\nINFO: HttpMuxer[/admin/per_host_metrics.json] = com.twitter.finagle.stats.HostMetricsExporter(<function1>)\nusage: linkerd path/to/config\nPlease reopen this issue if you can't get this to work for some reason! Thanks :D. @xiaobin sorry, i missed your reply.  We should probably change this logger to honor the TZ environment variable.. \ud83c\udf88 . @mrezaei00 Thanks for the report.\nWe build Linkerd on the published OpenJDK images.  If upgrading the JRE image will resolve these issues, we'll happily upgrade; but the Linkerd project is not interested in maintaining its own JRE images. . I should also note that, due to licensing, we can only distribute docker images with OpenJDK. From what I've seen, many production users build their own linkerd image on Oracle JRE base images (usually maintained within that organization). . The circleci config is not relevant, but changing the value of dockerJavaImage at build-time will use a different base image (and this should be configurable without editing LinkerdBuild.scala, though I can't quite figure out how to do that at the moment).\nI agree that we should update BUILD.md with instructions on how to build on different docker images.. Thanks so much, @chrisgoffinet. Have these settings been tested in a prod(-ish) environment yet?\nAlso, in order to satisfy the DCO, can you please run:\nshell\n$ git commit --amend -s -a --no-edit\nAnd the force push the branch. This will add a signature to the commit message.. @zackangelo You may want to test this branch if you get a chance.. @adleong I suspect that it's going to be extremely hard to implement this feature, as described here, in a way that doesn't violate the spec. I'm wary of reaching for a silver bullet to like cache control -- there are going to be a slew of other complications.  For instance, would this feature cause bodies to be returned for HEAD requests?\nDo we actually need to alter the response body?  Can this information be encoded in headers? Alternatively, can we buffer this information somewhere such that the user can retrieve this information after the request has completed? . Another approach, along the lines of TRACE, would be to introduce a linkerd-specific verb for this.. @dadjeibaah Would you mind sharing the output of curl -v for the example configuration you provide in the description?. Last (I think) comment:\n< HTTP/1.1 200 OK\n< Via: 1.1 linkerd, 1.1 linkerd\n< l5d-success-class: 1.0\n< Content-Length: 738\n<\nThe response should include a content-type header. Technically, according to the RFC, the response must include a Date header.\nAre there any other HTTP-correctness things we should do?. CI failure:\n[info] ExamplesTest:\n[info] io.buoyant.namerd.examples.ExamplesTest *** ABORTED ***\n[info]   java.lang.NullPointerException:\n[info]   at scala.collection.mutable.ArrayOps$ofRef$.length$extension(ArrayOps.scala:199)\n[info]   at scala.collection.mutable.ArrayOps$ofRef.length(ArrayOps.scala:199)\n[info]   at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:31)\n[info]   at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:29)\n[info]   at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:193)\n[info]   at io.buoyant.namerd.examples.ExamplesTest.<init>(ExamplesTest.scala:15)\n[info]   at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n[info]   at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n[info]   at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n[info]   at java.lang.reflect.Constructor.newInstance(Constructor.java:423). @adleong would you mind including the full response (i.e. curl -v) in the example?. > We have number of things that rely on the fact that Linkerd returns 502 when it cannot route a request so this would be a breaking change for us. It'd be great if it was configurable instead.\nI have a strong preference to avoid configuration for this sort of thing.\n@adleong Why is it appropriate to return a 400?. I'd prefer that we limit this change to only address output formatting. If we want to dig into error/response codes as a followup, I think that would also be fine.. The conduit repo has been moved to linkerd/linkerd2. The linkerd2-proxy repo has been split out, containing the rust proxy; and the linkerd2-proxy-api repo has been split out containing only the gRPC bindings (and related utilities) needed by the proxy and core projects.\nOnce basic docs (README, etc) have been restored and the proper access groups have been established, the repos will be unlocked. The linkerd2 repo still needs a fair amount of work before Linkerd 2.0-RC1 can be released.. FWIW, this should (in theory) be as easy as setting the dockerJavaImage setting at build-time. i think this can be simplified\nscala\n`linkerd-core` % \"compile->compile,test->test\"\n. let's remove this log message.  the admin server already logs on startup; and this hardcodes the port, so it may just be flat-out wrong.\n. scala\nimport io.buoyant.linkerd.{Build, Linker}\n. While we're here, can we rename this file AdminHandler.scala and move LinkerdAdmin into its own LinkerdAdmin.scala?  It's just mildly confusing when I'm looking for LinkerdAdmin and I find a bunch of handler code.\n. I'd prefer that all of the HttpMuxer.addHandler calls were in this function rather than in each file.  Something like:\nHttpMuxer.addHandler(\"/delegator\", DelegateHandler.ui(linker))\nHttpMuxer.addHandler(\"/delegator.json\", DelegateHandler.api)\n...\nThe goal is to concentrate all the singletonny stuff in one place instead of propagating it.\n. nit:\nscala\nnew DelegateHandler(dtab)\nsince dtab is already () => Dtab\n. omit needless parens:\nscala\nnew WebDelegator\n. Can we just leave out /flags?  We don't want to expose flags to users, anyway.\n. omit needless braces:\nscala\noverride def apply(req: Request): Future[Response] =\n  AdminHandler.mkResponse(dtabString)\n. yeah, i think i'm okay with removing it (I think the gist stuff should be removed in a followup anyway).  If we want to keep it and have access to details about twitter-server; then i'd prefer this all stay as a trait so it can just access the flags directly.\nAs it is, this is only marginally useful, since it doesn't even include java's property definitions.\n. I find this clearer with parens, since it's not clear if => is an arrow or a gte\n. It's not really a base anymore.  Perhaps Project? or LinkerdProject?\n. +1\n. this project should perhaps be called \"root\" or \"all\" or something, since this is the default project you'll get if you just do ./sbt.  I think it's fine for the docs settings to be bound to the root project.\n. i prefer that annotate returns unit so that it's explicit that it mutates the object.  The other form could lead a reader to believe that the message is immutable.\n. we can probably remove all of the assembly stuff from this file?\n. remove\n. this is still a WIP (good catch though).  I may roll this into a larger change for https://github.com/BuoyantIO/linkerd/issues/14\n. tioli: i'd probably break this up so only the future ends up in the await (and I'd use awaits so the timeouts get applied here -- i've got bad memories of having to use jstack to  to figure out where a test is hanging.  Something like:\nscala\nval iface = ThriftServiceIface(...)\nawaits(iface(args).handle { case RequestCaptureException(req) => req })\n(Also i believe you can just use handle here instead of transform)\n. I noticed this and have mixed feelings on how/whether to fix it.  I suppose it's okay to ensure that the label is always a path and to drop the double-slash\n. no, i agree that this is probably better, especially without arrow-functions.\nfwiw, i like this sort of shadowing here.  it makes it impossible to accidentally refer to the wrong servers -- i don't have to keep track which is the inner variable and which is the outer.\n. I agree with this.  We should rely on Stack.Params to provide the proper default values to the stack.  We want to be able to contribute to the stack's configuration from a config file.  I think the config objects should reflect the config source's state as closely as possible (and so I prefer Options where approriate).\n. Combining router and label names is a presentation/templating detail.   Furthermore, if you just get a reference to a dst or server, it's extremely helpful (i.e. when debugging) to have this information stored on the object so you know its full context. Generally, I think we should decouple the \"get routers\" code from the \"render a summary\" code -- i think at this point we could support a router-centric UI without having to change metricsToRouters.\n. I added this so it's easy to add the role to the stack with an empty implementation by default:\nscala\n      orig.replace(TraceInitializer.role, TraceInitializer.client[Req, Rsp]) ++\n         (TlsPrep.none[Req, Rsp] +: nilStack)\nThis is done support something in linkerd just doing\nscala\nStackRouter.Client.clientStack\n  .replace(TlsPrep.role, LinkerdTlsPrepModule)\nThis makes it so that placement in the stack is controlled in StackRouter; and policy gets plugged in by a user.  Otherwise, the user has to be explicit about inserting this module in the proper place.\nI could go either way on this, but it seems like a useful default.\n. the intent was the have all router stuff refer to dst ids and to have the Interfaces code talk about clients.  I'll make sure that's consistent.\n. +1\n. s5s is hilarious ;D\ni'd say 'serversets' are probably best.\n. I think I'd prefer that we take on hosts in the configuration so that they don't need to be managed dtab.  Finagle does this stuff because they are trying to avoid configuration; but we have a perfectly viable configuration channel.\n. I think I usually put a Copyright notice in there.  Mostly important that there is some attribution.\n. cool that makes sense\n. The intent here was to centralize all scheduling of updates.  This way, the invidvidual modules just have logic for fetching & rendering, but updates may be controlled externally.  Anticipating being able to control/disable update interval.  (1sec is going to be wayyy aggressive for real uses, but is nice to demo during devel)\n. doh, missed that. thanks.\n. they're not used yet.  but router-core is pure library code (and can be used outside of linkerd).  As noted in the description, these two modules are there as an example so that a user could install them (e.g. via Http.router.withStack(Http.router.stack.replace(PrepTls.role, PrepTls.unsafe)).  More work (another branch) is required to wire tlsprep into linkerd.\nSuggestions for refactor?  I don't think this can really be reduced any further.\n. i'm anticipating more dst stat scopes. (dst/path, for instance)\n. This file extension is used so that sbt has a sane way to autoload.  In theory you should be able to drop a maximal.l5d config file in that directory and ./sbt examples/miximal:run should Just Work.  (there's a slight wrinkle in this that I should fix before merging this).\nWe could use yml as the extension, but this would mean we can't have json configs or other non-linkerd-config yaml files in that dir.\nAlternatively we could move to a scheme like examples/<name>/config and drop the extension enitrely.\nOf these options, I think the .l5d extension is the simplest.\n. i'd prefer not to change this and just to let #11 fix the need for this.  (I don't have to call my checkout directory 'linkerd'...)\n. can we instead expose ip and port?  (currently name is just ip/port).  I'd like to get away from the idea that servers are named.  Their names are just //\n. this is changing: https://github.com/BuoyantIO/linkerd/pull/18/files#diff-2daf7c656f0ffc27fec022c422993789R42.\nWould you mind just calling this 'label' now?\n. style nit (for the future): prefer to only use _ with parens like linker.routers.map(_.name).  if brackets, param gets a name.\n. The reason those commands do the same thing is that examples/minimal:run is literally an alias for linkerd/minimal:run exmaples/minimal.l5d.  I was going to punt on supporting examples/*:run, but I was sure someone was going to ask to provide default configurations so config paths don't have to be specified manually.\nThis would probably be clearer if i renamed minimal.l5d to http.l5d (which I'll do now that there's a thrift.l5d).  That would make these commands examples/http:run and \"linkerd/minimal:run examples/http.l5d\".\nTo put a finer point on it: there are two types of sbt configurations at play here: example configurations and runtime configurations.  runtime configurations determine what set of dependencies are compiled and example configurations determine what configuration file is used.  example configurations have a dependency on a runtime configuration (i.e. the thrift example configuration can't use the minimal runtime configuration because it doesn't have thrift support).\n. should be server.ip.getHostAddress to avoid weird dns name additions\n. host shouldn't apply if version is 1.0\n. FWIW, i disagree with this somewhat.  These configs should be examples of very minimal configs, otherwise we'd explicitly provide ALL of our default config variables here.  I think there's room for another config that does this, but I wouldn't change the existing ones.\n. I'd prefer that this were a list, preferably:\nyaml\nzkHosts:\n- host: host.name\n- host: host.name\n  port: 2181\n. Actually, on this point, perhaps it's a good idea to at least disambiguate the example server admin ports.  E.g. http gets admin port 9991 and thrift gets 9992\n. it's idiomatic (i.e. in finagle) to write this as:\nscala\nval AdminPort(port) = admin.params[AdminPort]\nI think this extractor pattern reads nicely.\n. if you're anticipating having multiple admin protocols (which may be a good idea), perhaps admin.http or admin/http?\n. preferred:\nscala\nval dtabs = linker.router.map { router =>\n  val RoutingFactory.BaseDtab(dtab) = router.param[RoutingFactory.BaseDtab]\n  router.label -> dtab\n}.toMap\n. 2 nits:\n- I'd prefer certPath to certificatePath (after spending a few days with a bunch of ssl  tools, cert is used widely and is idiomatic).\n- I think we need to do an ensureTok(VALUE_STRING)  in each of these cases.  I think getText will happily try to coerce arbitrary values to a string\n. Yeah, I think it's clearer as it is.  Pattern matching is preferred in 99% of cases like this, as is idiomatic in finagle.\n. since IntegrationTest extends Test, both configs have to be present.  I could push this into Base~\nEDIT: oops, wrong branch. that's wrong. ignore me\n. I believe that Parsing.ensureTok is still required here.  I think, for instance, getIntValue will just return 0 if it's not actually an int value.\n. can this simply be ZkAddrs(addresses: Set[SocketAddress])?\nThe param type shouldn't have to know about partial configs.\nI.e I should able to do params + ZkAddrs(Set(new InetSocketAddress('zk.foo\", 2181))) and not have to worry about validation.\nValidation does belong within the parsing logic, and so I'd probably have something like:\nscala\nprivate case class PartialZkAddr(host: Option[String], port: Int = 2181) {\n  def toAddr: SocketAddress = host match {\n    case None => throw ... // hostname must be specified\n    case Some(host) => new InetSocketAddres(host, port)\n  }\n}\nand then you use\nscala\nval addrs = Parsing.foldObject(json, Seq.empty[PartialZkAddr]) { ... }\nZkHosts(addrs.map(_.toAddr))\n. I think it's reasonable to use a default port of 2181\n. These aren't \"hosts\" (they include ports).  Zookeeper's underlying abstraction is a \"connect string\", so it seems natural to expose that here:\n``` scala\n/*\n * A zookeeper 'connect string' that is used to build the underlying client.\n /\ncase class ZkConnectString(connectString: String)\nimplicit object ZkConnectString extends Stack.Param[ZkConnectString] {\n  val default = ZkConnectString(\"\")\n}\n```\n(It's common throughout finagle to see param strings defaulted to \"\")\n. and then this config param should be called zkAddrs -- host should be reserved for actual hostnames.\nscala\nval addrs = Parsing.foldArray(json, Seq.empty[PartialZkAddr]) { ... }\nparams + ZkConnectString(addrs.map(_.toString).mkString(\",\"))\n. Also, if we're being true to zk, we should support the optional chroot field too:\n\nconnectString - comma separated host:port pairs, each corresponding to a zk server. e.g. \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\" If the optional chroot suffix is used the example would look like: \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002/app/a\" where the client would be rooted at \"/app/a\" and all paths would be relative to this root - ie getting/setting/etc... \"/foo/bar\" would result in operations being run on \"/app/a/foo/bar\" (from the server perspective).\n. unrelated changes?  if just fixing formatting, prefer:\n- foreach over map for side-effecty things\n- using an explicit variable name in the index-seting block.  guideline: If a block gets curly braces, variables get named.\n\nscala\nindexedSvcs.index.map { i =>\n  index = i\n}\nOR\njust do\nscala\nvar index = ...\ndef setIndex(i: Int) = {\n  index = i\n}\n...\nindexedSvcs.index.foreach(setIndex)\n. oh i see, retry = true set here.\n. ah yeah, but if i uncomment that line it prints all of the commands being run.  I can remove debug, though, since everything goes through run (that wasn't always the case)\n. agreed.  it may be something we want to put in test-util, but let's DRY that in a followup after the http-housecleaning branch lands.\n. This is true.  However it's probably not a great idea to have a single thrift port handle more than one service...  I don't know how to phrase that in a way that makes sense.\n. there's nothing wrong with that if things are actually divided like that, but if there are just methods Add and Get, we are SOL and lose any way to identify services.\nSplitting a service-per-port allows us to provide additional naming hints.  Sharing a port is an optimization, not a general solution.\n. .... sentence?\n. tioli: I would omit DNS from this list.  We have some support for DNS.  It could be better.\n. tioli, another way i like to do this is:\n``` scala\npackage io.buoyant.linkerd\npackage admin.names\n...\n```\nThis is effectively the same as doing import io.buoyant.linkerd._\nFurthermore, I imagine this is necessary because one of our (jackson?) imports causes io to be imported. This can be avoided by:\nscala\nimport com.fasterxml.jackson.core.{io => _, _}\nwhich causes io not to be imported.\n. would prefer the explicit target like ./sbt linkerd/bundle:assembly.  I don't think it's valuable to run assembly across all projects.\n. also, it will save time to put this in the same sbt invocation as e2e:test:\n./sbt e2e:test linkerd/bundle:assembly\nsbt startup can take like 10+ seconds\n. Fwiw, this section is wrong/misleading\n. where did this number come from?\n. wrong link\n. i think these example dtabs should be simpler.  /ext/http has nothing to do with anything here.  It should have some documentation to explain what the intent is\n. I'd omit this\n. dtab should be simpler, documented\n. dtab sould be simpler, documented\n. it's a port-name not port number\n. HTTP/1.0 logical names are in the form:\ndstPrefix / \"1.0\" / method [/ uri* ]\nand HTTP/1.1 logical names are in the form:\ndstPrefix / \"1.1\" / method / host [/ uri* ]\nuri is only present if httpUriInDst is true.  dstPrefix may be specified in the configuration or is \"/http\" by default.\n. oh, i forgot i made assembly default to linkerd/bundle:assembly, so the target doesn't have to change, but i would group sbt commands if possible\n. it's platform-dependent http://docs.oracle.com/javase/7/docs/api/java/nio/file/WatchService.html\n. We have a plugin system, so we must not expect all configurations to work with all builds. If our errors aren't clear, that should be fixed.  I think it's important to maintain multiple build configurations for exactly this reason.\nAs for other touchups/wordsmithing, let's push that to another review, please\n. yeaaaaap\n. configuration here refers to the sbt build configuration (i.e. bundle or minimal)\nThis produces a file /io.buoyant/linkerd/0.0.10/bundle-exec that is used as the entrypoint for the docker image.  Then, the actual config file can just be passed in as the only argument, like:\ndocker run -it --rm -v $PWD/examples/http.l5d:/http.l5d:ro io.buoyant/linkerd:bundle-0.0.10-SNAPSHOT /http.l5d\n. what's your concern?  it's actually really nice and helpful to open the router port to and see the admin page -- the benefit being that it generates traffic, which can be seen on the admin page.  it's not something i'd do for external users, but as far as examples useful for development, it's pretty nice.\n. unrelated, i assume this defaults to localhost or something and that is relatively sane?\n. Fwiw, I don't think it's particularly worth documenting this, because this is just how Stack modules are used...  This isn't the place to do that sort of introduction to Stacks.\n. good call\n. ... words\n. Yeah, it's weird to \"recommend\" this.  It is probably something more like \"if this is set, it is typically the case that this should be set on the server as well\" or something\n. When you want to change the thrift encoding -- i.e. as an adapter from framed to buffered or vice versa.  Something like:\nyaml\n- kind: thrift\n  thriftFramed: true # downstream service is framed\n  servers:\n  - port: 9991\n    thriftFramed: false # buffered endpoint\n  - port: 9992\n    thriftFramed: true # framed endpoint\nI don't know how practical this is but there's no reason this configuration wouldn't make sense.\n. It's important to distinguish client tls from server tls...\nI don't think the config should refer to clientTls since it's already set on the router and not the server...\nI'm not saying this shouldn't be renamed, but I think we should be sensitive to distinguish client and server functionality in the naming\n. whoa that seems like a pretty big oversight. nice find!\n. How would you feel about calling these something like io.l5d.clientTls.static or io.l5d.tls.client.static or something?\nRight now, only namers are in io.l5d -- which wasn't really an intentional decision -- some sort of namespacey solution would be nice, and i'd prefer names that look more like reverse-dns than like classes.\n. oh, it's just a convenience for high-leveled containers so that, if they want to overload the entrypoint, they have convenient way to know where the linkerd executable is located\n. copypasta?\n. nit: omit needless braces (the def doesn't need its own braces since everything is in the sampler match\n. tioli: this block may warrant a small comment about the ordering of things.  e.g.\n// 1. Set the trace id from the header, if one was provided\n// 2. Get a new trace Id for this request.\n// 3. Use the sample header to determine if tracing should be forced.\n. tioli: if _extract doesn't actually use path directly, i'd probably pull this out of the function scope and make it private[this] just to show that explicitly.  i think it's just fine as it is, though.\n. nit: omit needless braces ;)  since the function is a single match statement, braces aren't necessary on the function\n. doh, no, that's cruft.\n. it should all probably be scoped under v2, as (1) there may be more than one marathon namer and (2) they are probably all version-specific (if the next version of marathon doesn't expose the same app id concept, for instance).\n. It being implicit means that it can be provided to i.e. Future.sleep\nscala\n  def sleep(howlong: Duration)(implicit timer: Timer): Future[Unit]\n. That's probably fine -- once it disappears from the list of all apps, the NameTree will go Neg.\nSo, it's subtle -- but we do stop polling for an app when it is no longer being observed (not when it actually goes away), which I think is the correct behavior.\nVar.async doesn't actively start polling until the var is observed, and it doesn't stop polling until all observers have stopped observing it.\n. this recursive behavior is fairly idiomatic.  With, async APIs especially, it will probably be more trouble to try to fit them into a synchronous model than to do something like this.\nIf this is problematic, we'd have to change transform to respond, and provide a var to keep track of the currently-pending future.\n. This should probably be removed.  I put this there when I was trying to do this a different way.\n. something like the following should work:\n``` scala\n    def toAddresses(): Set[SocketAddress] = app match {\n      case Some(AppNode(, Some(tasks)) =>\n        tasks.collect {\n          case TaskNode(, Some(host), Some(Seq(port, _*)) =>\n            new InetSocketAddress(host, port))\n        }.toSet[SocketAddress]\n  case _ => Set.empty[SocketAddress]\n}\n\n``\n. I think we still want this parser, we just want to use the finagle-provided param in place of our Framed param.\n. preferapp.startsWith(\"/\")` which can't blow up on index out of bounds\n. style nit, prefer:\nscala\nval appId =\n  if (app startsWith \"/\") app\n  else s\"/$app\"\n. We have a ticket elsewhere for this, but it's worth preempting it here: Let's use Trace.record, which will save us a lot of time in the longterm.  E.g.\nscala\nTrace.recordBinary(\"marathon.appId\", appId)\n...\nTrace.record(\"marathon.found\", true)\n...\nTrace.record(\"marathon.found\", false)\nIt's probably not useful to log addr here (since it changes on its own, and this will just be some memory address here)\n. tioli, mild pref for:\nscala\n val NamerInitializer.Prefix(path) = params[NamerInitializer.Prefix]\n...\n   .configured(Label(\"namer\" + path.show))\n. for blocks:\nscala\nTry {\n  ...\n}\nBut does this really need to be a Try at all?\n. Prefer Set.empty[String]\n. Similarly, what exceptions are we expecting here that this needs to be a Try?\n. thought was: appid is a feature of the v2 api.  it may not exist in v3?\n. Minor, but:\nEhh, I think your paths are funky here -- shouldn't have a slash within a path element otherwise the whole thing will end up hex-encoded:\nscala\ncom.twitter.finagle.Path.Utf8(\"/servicename\").show ==\n  \"\"\"/\\x2f\\x73\\x65\\x72\\x76\\x69\\x63\\x65\\x6e\\x61\\x6d\\x65\"\"\"\nI'd probably do Path.read(\"/servicename/residual\") or Path.Utf8(\"servicename\", \"residual\").  Also I'd probably pull the paths out into temp vals so the assertions are a little more readable.\n. tiolo: [String] isn't necessary here\n. tioli: I'd put a newline above case None to give it some visual spacing\n. needless braces?\nnot a blocker, just noting style issues as i got through this\n. tioli:\nscala\ncase Some(Nil) => NullTracer\ncase Some(Seq(tracer)) => tracer\ncase Some(tracers) => BroadcastTracer(tracers)\ncase None => DefaultTracer\n. I'm skeptical about this.\nDo we want to be able to trace naming-related requests? Or do we want to be able to use namers in tracers.  I don't think both makes sense; and I think it's probably more important to be able to trace naming-related requests...\nNeed to think about this a little.\n. what does config do here?  omit it?\n. Per offline discussion: we want to invert this so that the tracer is passed to the namers rather than passing namers to tracers.\n. so, i think this will entirely replace the client's params (and i.e. wipe out the tls config, and any http-specific param settings).\nThis probably needs to be:\nscala\n.transformed { stk => stk.withParams(stk.params ++ params) }\n. probably has to be\nscala\n.transformed { stk => stk.withParams(stk.params ++ params) }\n. nit: this is more accurately namerParams -- the params for namers.\n. +1 preferred\n. tioli: i'd be inclined to capitalize this as FsInitializer (etc, elsewhere)\n. and in fact MarathonInitializer is capitalized... so let's be consistent.  and i'd prefer that we go camelcase with K8sInitializer, FsInitializer, etc.\n. ah, yep, correct. i'll clarify that\n. Good question.\nThere's no reason that 2 routers couldn't share a dtab (so that they can be controlled centrally), but have distinct entry points...\nReally, I think that we should drop dstPrefix in favor of identifier modules (#120).  Individual Identifiers could chose to expose a prefix configuration, or not.\n. also, per @stevej, i think we'd like to change the default for routers to be EWMA...\n. per my understanding, p2c and ewma (and apperature) aren't mutually exclusive. I believe that EWMA is a costing strategy to feed into P2C, and apperature is a windowing strategy taht still uses P2C.  I think that \"p2c\" should actually be \"queuedepth\".\nThere are multiple selection strategies: P2C and Heap\ncosting strategies: EWMA, queue-depth, ...\nand windowing strategies: apperature, ???\nI believe that all 3 of these may be selected independently...\nThat said, we probably want to support several \"bundled\" configurations (and not arbitrary selections).  So perhaps this could just be simplified with naming like:\n- p2c.queuedepth\n- p2c.ewma\n- aperture\n- heap.queuedepth (we may not even want to bother exposing this)\n. these are so small, and closely related. i think it's fine for them to share a home.\n. If you want to adhere to the letter of the style guide, I might do object DtabCodec { class Serializer ... class Deserializer } ; but I don't feel strongly about that.  I do think it's a lot easier for these things to share a home...\n. Can do:\nscala\ncase state => Activity(Var.value(state))\n. prefer:\nscala\ncase Activity.Failed(e: ClassNotFoundException) =>\n. tioli: omit id = -- unless you think it adds clarity\n. now that we're allowing multiple Identifier types -- this should probably become BasicIdentifier or DefaultIdentifier or something that distinguishes it from other kinds.  Probably Basic, since that seems to be what is used elsewhere?\n. tioli: I know before I suggested that this should be \"default\", but since we refer to this everywhere as basic, maybe it should just be that.  My main objection was to having a fully qualified classname.\n. nbd but files should have newlines at the end.  maybe update your ide/editor to do that by default.\n. tioli: it might be nicer to just keep this in the protocol project since it's a default implementation.  especially since the actual identifier implementation is part of the router package and isn't actually modular.  but generally for default implementations i think it's fine to keep it part of the main protocol package.\n. it's redundant to trace both appId and found.  should probably restore notfound.\n. yeah, it's there only so that if the Var is closed we can tear down a pending request.\nI should probabably set pending = Future.sleep... as well though\n. To recap IRL discussion: we should try to replace these sleeps with use of a MockTimer so that these tests are faster and less prone to flakiness\n. more realistic to use Addr.Pending as the initial value\n. Addr.Pending?\n. i'd probably just assert(addrs == Set(addr)) -- scalatest is generally good at turning that into a reasonable error.\n. assert(addrs == Set(addr))\n. assert(addrs == Set(addr))\n. you may not want to share this across tests (i don't know how stateful it is).  probably better as def newTimer\n. tioli: this won't close on test failure.  would need to try ... finally ...\n. nit: all of the other fields have aligned values\n. 250 is probably not actually a good default.  Can we default at like.. 5000?\nAlso, we probably want to add some sort of jitter so that all instances don't hammer simultaneously... (can be done in a followup branch)\n. I'd just omit this.  We can always run it with bash -x if we want details.\n. slightly better to send these to stderr (>&2)\n. +1\n. yep, good catch\n. I'd be inclined to break this up -- \nval f = .. {\n}\nFuture.value(f)\n. @adleong we'll never get to 52/53 if this returns None...  We may want to figure out a better way to do this moving forward (so that we're not just throwing an exception during stack construction) but for now I'm happy just to fix this as-is.\n. would namer.lookup(pathWithoutPrefix).sample() work?\n. orite\n. tioli: i don't think it adds much value to have def getFoo given that these just get called once, immediately.  Could even inline them into ZipkinTrace.mk...\nnbd one way or the other\n. better:\nscala\n...unit.handle { case KeeperException.NodeExists(_) => }\ni'm a little surprised this compiles given that create returns one type and rescue returns another...\n. not a blocker, this is obviously not production-ready.  Creating files with global-writable is dangerous for any real usage.  We'll have to revisit permissions and privileges before we tell anyone to use this outside of experimental use.  Maybe worth putting a note in the docs to this effect.\n. i know you didn't write this initially, but I just realized that we're supposed to return a 406 Not Acceptable if we are given an Accept header we can't satisfy.  I think in general, the accept stuff I did is not particularly RFC compliant, so maybe best to revisit in a followup\n. noob question: what does the $ in $clientEl indicate?\n. nit: should this be serversEl (plural) now?  we have $clientsEl -- would be nice to keep them consistent\n. oh, that makes sense! thanks\n. preemptive reminder to use Await.result(f, timeout) in tests\n. tioli: it might be nice to emit both so that it's explicit?  This isn't really the RPC name it's just convenient for zipkin's UI, so this is sort of a hack. it would be nice to reliably be able to look at router.label, or even l5d.router?\n. tioli: not a huge deal but we could probably do this with fewer allocations by having recordReq and recordRsp methods that use val'd strings (aiui this will have to do new string allocations on each message...)\n. yeah, the more i think about it, the more in favor i am of avoiding having to do string interpolation on every trace...  seems wasteful\n. Should we actually remove it from the state cache after updating it?  Won't it continue to be returned from list() otherwise?\n. i'd test that store.list() doesn't return \"test\"\n. This raises an interesting point: are there situations when Activity.Ok(tree) can be satisfied where NameTree is neither Leaf nor Neg? I think it's uncommon but certainly possible given the way this currently works.  And I don't think we can expect users to know to avoid these situations.\nFor example, given a dtab like:\n/http/1.1/* => /io.l5d.k8s/prod/http ;\na bound name might be /io.l5d.k8s/prod/http/web.\nWhat happens if we update the dtab so it looks like:\n/http/1.1/* => /io.l5d.k8s/prod/http ;\n/io.l5d.k8s/prod => /io.l5d.k8s/pre-prod ;\nNow /io.l5d.k8s/prod/http/web binds to an Alt: /io.l5d.k8s/pre-prod/http/web | /io.l5d.k8s/prod/http/web? (Is that right? Or does that go infinitely recursive?)\nOr more simply, we may find something like:\n/http/1.1/* => /io.l5d.k8s/prod/http ;\n/io.l5d.k8s/prod/http/web => /$/empty ;\nPerhaps we can avoid this by doing addr lookups directly against a namer instead of through via a dtab?\nOr perhaps we can simply forbid dtabs that contain namer prefixes on the LHS? This seems like a generally good policy in any regard.\n. This won't work well with /$ names like /$/inet.  Easy fix incoming\n. yeah, this was pretty jarring when I first looked through this.  perhaps KubeObject or something?\n. I think this is harder to reason about, since we have no guarantees that this closure is called within the context of the synchronized block\n. I think this would be clearer in a ClientConfig companion object.\nI.e.\nscala\nobject ClientConfig {\n  val DefaultHost = ...\n}\n. The signature for Map[K,V].getOrElse is effectively\nscala\ndef getOrElse(k: K, f: => V): V\nIn this case we happen to know that f is going to be called immediately, but generally speaking, this is not a good assumption that if i pass a closure to something that it will be called immediately (in the same thread, etc).  when I see a synchronized and then passing a closure to function, i need to stop and make sure that everything's safe (\"ok, this isn't a Future, or a view, or something like that\").  When there's no closure, it's easy for me to reason about without thinking about the specific types at play.\n. generally, i have mixed feelings on using class paths for kinds.  I know we've done this for other ones, but it encodes a bunch of redundant information that isn't particularly user-friendly.  We can revisit this outside of this review, though.\n. If this ends up encoded in resrouces, i think i'd prefer \"l5d.io\"?\n. nit: omit needless braces\nalso I think .unit.rescue is slightly cleaner than .rescue { ... }.unit but not a big deal either way\n. This should almost certainly be .unit.rescue?  (Slightly surprised this compiles as i'd expect there to be confusion over the return type of rescue?  Or does named(ns).put() return a Unit? If so is .unit necessary at all?\n. nit: would be slightly easier to read if the whole for { } loop was indented:\ne.g\nscala\ncase Activity.Pending =>\n  for {\n    ..\n  } yield ..\ncase ...\nsomewhat visually disorienting that the yield is at the same indent level as the next case\n. This block could probably use a brief comment about what it does (in addition to some of the details of its behavior).  As it is, it's hard to contextualize the comment.\n. IIUC we have no means to tear down the stream or report an error?\nI think I'd be inclined to write this block slightly differently:\n``` scala\nprivate[this] val states = Var.async[Activity.State[NsMap]] { state =>\n  val closeRef = new AtomicReferenceClosable\n  val pending = api.dtabs.get(...).respond {\n    case Throw(e) =>\n      state = Activity.Failed(e)\n    case Return(dtabList) =>\n      state() = Activity.Ok(dtabList)\n      val (stream, close) = watchApi.dtabs.watch(None, None, dtabList.apiVersion)\n      closeRef.set(close)\n      stream ... {\n        state() = Activity.Ok(...)\n      }\n  }\n  Closable.make { t =>\n    pending.raise(...)\n    Closable.ref(closeRef).close(t)\n  }\n}\nprivate[this] val act = Activity(states)\n```\nWe may need to hold this activity open, but we should be able to gracefully tear it down. (If DtabStore isn't Closable, that's another thing we should do in a followup).\n. perhaps listResource would be clearer? it was slightly odd to see dtabs = list[...]\n. nit: i'd break this up into multiple lines e.g.\nscala\nprivate[k8s] class NsVersion[O <: Object](\n  val client: Client,\n  group: String,\n  val version: String,\n  ns: String\n) extends Resource {\n  ...\n}\n. should group be documented?\n. I agree that it's a bit jarring seeing List in this context.  ListResource?  KubeList?\n. What are the benefits of this being pluggable?  Don't the concrete ThirdParty types require specific serialization support?\n. i think i agree withNamespace is good/better\n. I agree that generally Future.value is preferable, but here Future.apply is right so that exceptions are raised into Future.exception.\nIt seems like this block could simply be the following, though:\nscala\ntry rsp.reader.discard() catch { case _: Reader.ReaderDiscarded }\nFuture.Unit\n. oh? i thought that because create() returns Future[Unit], .rescue { ... => create() } would be a problem?  Is it the case that Unit >: Dtab?\n. tioli:\nI generally prefer to avoid nesting -- easier to understand:\nval v = ..\nFuture.value(v)\nmight write this as:\nscala\ndef list(): Future[Set[String]] = {\n  val keys = dtabStatesMu.synchronized(dtabStates).collect {\n    case (k, v) if v.sample.isDefined => k\n  }\n  Future.value(keys.toSet)\n}\n. redundant\n. generally, we shouldn't actually define a version until we're doing the release (so it should just say e.g. ## 0.X.X -- it's clearer that it isn't released yet so that when someone else goes here they know not to create e.g. 0.3.2.  In this case it's fine, since we're prepping for the release (though you may want to include a placeholder marker for the next set of changes above this.\n. tioli: \"properly\" feels awkward in this sentence\n. Prefer the present tense \"Add ..\" to \"added\"\n. Why don't we just go the other way and pull dcosBootstrap into the Bundle configuration (so it'll have access to all of the bundled plugins)?\nAIUI this should only need a dependency on core and Storage.zk...\n. ... otherwise the process may never exit\n. oh, i'm saying that -XX:+CMSClassUnloadingEnabled is specified twice ;)\nI opted to declare flags separately for namerd and linkerd because they probably should diverge...\nAlso, we may not want to set -Xms/mx here -- or if we do we  should support easy override?\n. Not a blocker, but this isn't great for something like namerd.  It means that once a node is observed, it will be observed forever (even if there are no observers).  If for instance, many short-lived dtabs are created and deleted, this will manifest as a memory leak.\n. not a blocker, but we probably need to think more about validation.  Can zkPrefix end in a slash?  Can ns start with a slash?\n. style: i assume this is copied, but prefer safeRetry(go).respond\n. It's not clear to me why this returns Future[Unit] and not just Unit.  Perhaps just to satisfy retryWithDelay's signature?\n. what does this mean, practically? Should this be configurable?  Should this be an exponentialJitter backoff?\n. minor: i'd be inclined to separate this from infiniteRetryFilter and have i.e. val logFailureFilter = ... and then layer it in below as retry and then logFailureFilter andThen client\nThis way, errors are still be logged and counted when retries are disabled.\n. How relevant is this for the readme? Maybe tmi?\n. Similarly... Maybe it's just enough to describe that there's another configuration published under the tag?\n. Nit: omit needless braces.\n. Omit needless braces\n. Here we may want to split this into e.g. execPreamble, the which execScript and dcosExwcSceipt append?\n. should we special case the max timeout so we don't always set timeouts?\ni.e.\nscala\nval f = pathCache(path, conn)\nbindingTimeout match {\n  case BindingTimeout(Duration.Top) => f\n  case BindingTimeout(timeout) => f.raiseWithin(timeout, exception)\n}\nI think we could also val this exception -- it doesn't need to be instantiated on each request but can just be a private on the class.  Exceptions can be fairly costly to instantiate (especially if they have stack traces).  Still, fewer allocations is better.\n. should this default to timeoutMs?  It doesn't make sense for this to be higher than the request timeout.\n. Yeah, my concern is that we're creating a new throwable on every lookup regardless of whether a timeout is set or the exception is actually thrown.  Perhaps use RequestTimeoutException which is mixes in TimeoutException with NoStacktrace?\n. style (tioli)\ni prefer\nscala\n  case path: Dst.Path =>\n    val exc = new RequestTimeout..\n    pathCache(path, conn).raiseWithin(bindingTimeout.timeout, exc)\n. this is doing a read and then a write on current, which I believe needs to be synchronized.  Two threads could read current, and evaluate to false, and then both of them would create a new Observation, and an arbitrary one would win, causing setValue to be called twice, which throws an exception.\n. Hmm.. maybe i'm misreading this.  Still the, read/write strikes me as racey.  I think we need to at least mark current as @volatile if it's being accessed outside of synchronized.  We should think about how this handles with concurrent updates, though...\n. yeah, i think the real issue is that i think it would have to be volatile if outside of the synchronized block to ensure visibility? i'm not a jmm lawyer...\n. i think that was intended to be an assert on clientAddr1 and not clientAddr0...\n. I think at the very least we should check that closable is not null.  Furthermore, because this is going to cross thread boundaries, i think closable has to be a @volatile....\n. can we add tests?\n. would be nice to support polling requests as an alternative to streaming responses.  perhaps require ?stream=true?\nBasically, it seems like any/all of the read apis could be either streaming or not.  (even the dtab api).  would be nice to have a uniform approach to that, even if not in this branch.\n. although, it's probably okay to assert that clientAddr1 is Neg... sup to you.\n. thoughts about scoping this under a v1 package?  Since the API is versioned, and all of this is (i think?) specific to the v1 api, should we try to reflect that in the packaging?\n. i think this log message is no longer correct? this may not be an endpoints request\n. endpoints-specific log message\n. probably should be able to configure the port that etcd listens on to avoid conflicts...\n. Hm... this is probably more trouble than it's worth (and if there really is a v2 api we can just create a k8s-v2 package...)\n. ahh, yeah good catch.  should be in k8s/ or a comment on k8s/src/main/scala/io/buoyant/k8s/package.scala\n. i'd prefer that this just be part of the HttpControlServiceTest so that all of the endpoints are enumerated in once place, and then test names should be in the same form...\n. I suspect that it's a fragile assumption that namespaces can't have slashes...  It would probably be more robust to have the paths be read from a  query param?\n. To put a finer point on it: I expect that we're moving to a model where namespaces are paths.  We can reduce some api churn by anticipating that\n. @adleong i think we should consider namespaces as resource trees.  we can create a dtab now (with some backends, at least).  e.g\n:; namerctl dtab create other/ns - <<EOF\n/srv       => /io.l5d.k8s/default/http ;\n/host      => /srv ;\n/method    => /$/io.buoyant.http.anyMethodPfx/host ;\n/http/1.1  => /method ;\nEOF\nCreated other/ns\n:; namerctl dtab list\ndefault\nother/ns\nThe problem is when we're mixing Dtab Namespaces and Name.Path -- I'd opt to treat the Name.Path as an argument to the Dtab. e.g. /api/1/addr/other/ns?p=/foo/bar\n@gtcampbell we'll probably need a way of encoding slashes in the k8s store... (maybe just b64 the name and call it a day)\n. this will blow up if the param is not a valid path -- does that cause a 400? (test please?)\n. This is similar to how we designed the util-zk api at twitter.  It allows some nice separation of concerns (key selection vs actions).\nFor instance, if we want to \"chroot\" a module (i.e. lock into a prefix), we don't have to rely on the module to manage this itself (and vigilantly apply path prefixes).  We can just do:\nscala\nval root = etcd.key(\"/some/root\")\nand then if we path the module root, all of its operations will occur under /some/root.\nThis could all use some better documentation/explanation, but I like this pattern.\n. The library deals in finagle primitives (services, futures, etc), so I'm not sure how severe of a coupling this is.  I suppose this puts some additional requirements on permitted characters.  This all came from a Namer implementation, so it was expected it would receive paths.  I think we'd want to maintain support for passing paths to it, though it may be better to support Seq[String] as the underlying primitive.\n. tioli: this could just be val services = Activity(state). activity isn't used otherwise.\n. After going back on forth on this for a while, I think it would be needlessly restrictive to drop support for events---it exposes etcd's API with minimal interference.  I suspect that it makes sense to provide helpers over this API to model updating states of a single node, but I've removed the watch helper for now until we have a concrete use case for it (to determine whether it makes sense -- I suspect we care more about def dataWatch: Activity[Option[Buf]] and def dirWatch(recursive: Boolean): Activity[Option[Dir]])\n. I've documented this, but I think this is correct given the semantics of the API\n. In this case, i think it's probably more readable than the alternative?\n. This is racey:\nT0: caches.get(name) => None; mkNs(name)\nT1: caches.get(name) => None; mkNs(name)\nT0: onSuccess.. update\nT1: onSuccess.. update\nFurthermore, updates to the cache are done from another thread and are not synchronized.\n. just occurred to me that we should possibly attempt to close the stream if the future is cancelled?  I.e. a user hits ctrl-c on the request, we can close the stream preemptively (i.e. before trying to write on it).  This is an optimization so not really a blocker\n. maybe clearer as isStreaming\n. should we report the error on the stream?\n. yeah, you're right. i was confused about how interrupt handlers worked.\n. This is okay for now, but we're going to need real API docs...\n. Is this helpful? If this is for the uninitiated, there's no explanation of what's going on.  If this is for the initiated... is this helpful?\n. pretty sure we're going to have to move this to a POST api in the near future...  When dtabs have comments, etc, this is going to be fragile.\n. how come this is null if we've specified a dtab?\n. at some point we had separate delegator/{core,http} projects?  seems like linkerd-admin and namerd-iface-http-controller should pull in delegator-http?\n. nevermind, it's on the child...\n. fwiw, i expect that this is going to break in an upcoming change.  I expect it will work as follows:\n- / is a valid (root) namespace.\n- all delegation requests will accept a dtab (override) that is appended to the namespace's dtab.\n- a special namespace, e.g. /.empty, will be supported to support requests that don't use an underlying dtab.\n. perhaps warrants comments like\n$ open http://localhost:9990  # open linkerd's admin interface in a browser\n. i think it's fine to be here, i just think it would be better with a little explanation if it's displayed so prominently in the docs.  (I worry that the README is getting to be a bit overwhelming)\n. style: prefer Var[Activity.State[T]](Activity.Pending) (this is done throughout the codebase already)\n. Ah, ok.  We should discuss introducing new stylistic checks.  Throughout our codebase (and certainly finagle), discarded values are used idiomatically.  I'm not opposed to it, but i'm not sure how much value this actually adds if we are diligent about annotating return types\n. nit: states.respond\n. might be slightly clearer as retryToActivity { watch(name, ns) } to make it obvious that this is a closure\n. i think this is going to be problematic -- we never update the var, we just return a new name tree every time the address changes.  this is going to be at-best costly and at-worst broken.\n. 1. Name.Bound is not required to be a path (it may be a reference, for instance).\n2. Multiple NameTrees may return the same Name.Bound -- \"same\" being determined by id equality.\n3. clients are cached on this.   Load balancers observe updates to the Var[Addr] and update their list of endpoints accordingly.\nIt's probably helpful to review finagle's client stack (ignoring linkerd for the time being)--specifically, BindingFactory and how it's layered onto LoadBalancerFactory.\n. i would just remove the link -- linking to the package won't be helpful here.\n. Should we make render implement Try[T] => Buf?  For json streams, in particular, it seems sketchy to return a raw string in a sequence of json objects.  It would be nice if it could be encoded as json...\n. \ud83d\udc4d \n. tioli: The prior AtomicReference approach before made it possible to cancel the request if its closed before the request is satsified. Practically, it probably doesn't matter, but it seems more correct to do so....\n. I don't think we should update latest to snapshot tags...  It means that a user casually doing docker pull buoyantio/linkerd will get a non-release version\n. I think the sbt-docker plugin will do this for us.\n. dirty secret: Buf.Utf8 always matches\n. TIL, yes\n. i have some PTSD about this since chained futures once had a subtle reference leak... i'll try to convince myself it's safe to do this.\n. yeah, we should fix this outside of this review\n. yeah, this always matches, just sometimes with funny characters.  turns out it's really hard to test whether a byte array contains a utf8 string\n. doh! good catch\n. incomplete sentence in \"An Event that reflects\"...\n. per conversation, Event already has a dedup :)\n. Consider providing i.e. io.buoyant.test.FunSuite that mixes in scalatest's FunSuite with all of the helpers that we normally use (Awaits, for instance)...  Then we can simply use io.buoyant.test.FunSuite throughout our code.\n. tioli: may be nicer to do this sort of thing as:\nscala\nval ctx = new AddrCtx\nimport ctx._\n...\n. I think it's preferable to return unit here, since these methods are actually side-effecty.  returning values makes them appear as if they are not side-effecty.\n. looks like you changed the other set of set/clear functions, but these should still be Unit...\n. Should this be removed now? Or make it a link to the \"Old dashboard\"\n. ... or is this served from the old dashboard.  regardless, i think this link needs to change...\n. raising is advisory and isn't guaranteed to be delivered.\n. thought was that the consumer of this can log... (we don't have a logger in this module as it is)\n. aiui /dashboard is no longer a valid link.  Also probably shouldn't call it \"beta\" anymore ;)\n. clients can be constructed with names that have residual paths.  I.e. i can create a client with /$/inet/127.1/4001 or /$/inet/127.1/4001/some/prefix.  These both bind clients on 127.1:4001.  The latter is bound with a residual path, /some/prefix.  It would be cool to use this as a prefix for all etcd operations.  I don't have a concrete use case for this right now, though.\n. yeah, i tend to prefer explicit match statements, especially in synchronized blocks.\n. ideas?\n. oops, lost in a rebase.\n. i think it's likely we should change this, but i think we can do so in a follow-up.  (It would be nice to continue to accept Paths but it shouldn't be the only way to refer to keys...)\n. ???\n. whoa! i never would have thought of this. crafty!\n. :; ./sbt linkerd-admin/test\n...\n[info] LinkerdAdminTest:\n[info] - serves buoyant admin at /\n[info] - serves buoyant static files at /files\n[info] - serves 404 for a non-existent route\n[info] - serves twitter-server admin at /admin\n[info] - serves twitter-server static files at /admin/files\n[info] - serves /admin/metrics.json\n...\n. oh, yeah, i see.  nothing actually calls main\n. prefer (_.map { t => ... })\n. I can drop it here, but otherwise:\n[error] /Users/ver/b/l5d/etcd/src/test/scala/io/buoyant/etcd/KeyTest.scala:416: type mismatch;\n[error]  found   : Long\n[error]  required: Int\n[error]         requested(idx).setDone()\n[error]                   ^\n[error] /Users/ver/b/l5d/etcd/src/test/scala/io/buoyant/etcd/KeyTest.scala:417: type mismatch;\n[error]  found   : Long\n[error]  required: Int\n[error]         responses(idx)\n[error]                   ^\n[error] two errors found\n[error] (etcd/test:compileIncremental) Compilation failed\n[error] Total time: 10 s, completed May 2, 2016 7:49:22 PM\n. should this be root or something?  should it have a default?\n. fwiw, the states() = Activity... form is fairly idiomatic throughout the rest of our codebase, as well as finagle's...  i don't feel strongly, though\n. alphasort?\n. nit: let's use /namerd/dtabs so we have a place to stick other non-dtab things later.\n. neither of these are functionally equivalent ;) but I can change this since it seems to be confusing.\n. yeah, it's intended to basically be a section heading for the next few blocks. trying to provide visual cues about the sections\n. this this side-effecty? if not, i'd omit ()\n. I'd argue that the class shouldn't have defaults (because i don't think it can really know what a sane size is without considering how its being used.  Defaults should be applied higher up\n. Also, i'd probably avoid the double-function constructor syntax here -- I don't think it makes anything easier to read elsewhere, especially given that you explicitly provide attribute names.  I'd just make a single constructor...\n. tioli I'd probably write this as:\nscala\n{ k =>\n  val (ns, dtab, path) = k\n  BindingObserver(interpreters(ns).bind(dtab, path), stamper)\n}\nsince this makes it clearer that this isn't a partial function...\nAlso, I'd be inclined to split this out into a separate function so it's clear:\n``` scala\nprivate[this] case class BindingKey(ns: Ns, dtab: Dtab, name: path)\ndef mkBindingObserver(k: BindingKey) =\n  BindingObserver(...)\nprivate[this] val bindingCache = new ObserverCacheBindingKey, NameTree[Name.Bound]\n``\n. similarly, i think this would be a bit clearer as a separate function rather than inlined.\n. nit: prefernotification.wasEvictedwithout parens, unless this is side-effecty\n. nit: needless braces\n. i'd probably add a note about CHM is used to make reads lockless, but all updates are explicitly synchronized\n. It uses the stats receiver for the path stack which is scoped byrt//dst/path/`.  The HttpEndToEndTest illustrates this.\n. They're not. jackson will happily default them to zero.\n. i've added a test, though.\n. jackson doesn't enforce any sort of requirements, it will just leave null-values for missing fields. for Ints, this is 0 (which is good for minMs).  for options, it's None.\n. ah yeah, that's an oversight. good catch.\n. Maybe eventually.  This is what finagle does fwiw.\n. i'd take a registry in the constructor so this can be tested.  Something like:\nscala\nclass PrometheusStatsHandler(registry: MetricsRegistry) {\n  ...\n}\n. does this really need to be sorted? that's potentially a bunch of work...\n. @adleong any thoughts on mkString versus StringBuilder, etc?\n. Generally, It would be preferable to not have to iterate over this thing many times.\ncan we just build the response string in the formatting loop?\n. use string interpolation over .format. s\"\"\"$label{stat=\"$stat\"}\"\"\"\n. I think we could just do this in a single regex.  But, unfortunately this is a lossy transformation and will make it difficult to correlate with other systems.\n. is this really a name? it seems in many cases this will be a path like /role/env/job -- we should probably be graceful (or at least error explicitly) about leading slashes\n. It would be helpful to have some documentation somewhere on how announcers work - it's sort of spread around the implementation right now and, unless I'm missing it, I don't really see an overview of how this works.  From what I gather, each server must be announced separately, and there is no mechanism for i.e. registering multiple servers in a single node?\nConcrete example:\nyaml\n- protocol: http\n  servers:\n  - label: https\n    ...\n  - label: http\n    ...\n  baseDtab: |\n    /host/foo => /$/inet/127.1/8080 ;\n    /host/bar => /$/inet/127.1/8081 ;\n    /http/1.1/* => /host ;\nI assume if I want to register both 'foo' and 'bar' i might have an announcers block like:\nyaml\n- kind: io.l5d.zk.serversets\n    hosts:\n    - ...\n    path: /role/env/foo\n- kind: io.l5d.zk.serversets\n    hosts:\n    - ...\n    path: /role/env/bar\nBut i'd end up with paths /role/env/bar/http, /role/env/bar/https, /role/env/foo/http, , /role/env/foo/https?  This might be okay, but it diverges a bit from, for instance, the announcer scheme used by aurora.\n. i'll comment on your proposal, but I don't think we should have \"announcer\" in this -- we're in an announcers block, what would it mean to specify anything else?  Also, i'd prefer that we avoid capitalization.\n. I'd prefer just io.l5d.zk.serversets -- the fact that it's an announcer is self-evident\n. I think it's preferable to expose raw sums of counters. this way the exporter doesn't have to know anything about its polling interval. deltas should be handled by the stats system (i.e. at read time)\n. what does !0 mean?\n. In this example I have two servers that may route over two local services...  my point is there's not necessarily a 1:1 relationship between servers and services\n. I think we should default to true.  If I have an announcers block, I want things to be announced.\n. these stats receivers should be scoped to differentiate the caches, right?\n. We have multiple observer caches, bindingCache and addrCache, right? Are these differentiated in stats scope?\n. hm, not related to your change, but i'm very dubious about this feature in general. what if response is chunked? What if response contains sensitive data? I'd be more in favor of checking specific headers...\n. also 64 * 1000 isn't 64kb... anyway, i don't think this line should have been touched by this change ;)\n. not a strong objection, but i'm slightly dubious about putting newlines in exception messages... i've worked on things in the past that try to parse exceptions from logs and this sort of things make it super painful.\n. resultant stats will be e.g. \"binding-cache/obsever-cache/active-size\" -- can we tighten this up a bit?  Do we even need the observer-cache scope anymore?  Also, I would omit \"-size\" -- key verbosity comes with a cost at scale.\nCan this just be bindingcache/active? (seems more idiomatic with rest of finagle stats)\n. unrelated\n. It's unclear to me if this is the proper ordering. it's consistent with finagle's client metrics, but I'm concerned that it will be difficult to compute a proper success rate with stats measured below retries.  I think for now we should move the StatsFilter above retries.  If we need stats below, we can add another module later.\n. We don't have to eliminate them entirely, but as in finagle, we should avoid using anonymous functions in the request path. We know it creates more objects, and we should strive to minimize allocation.  Don't fear imperative code ;)  They're fine to use in other places (i.e. configuration code, admin interfaces, etc), but we need to be disciplined about allocation.\n. this signatures of finagle's things are a bit funky.. i'll see if i can reuse it, though\n. retry policies take a Try[Nothing] and we are handling a Try[Any] (from ReqRep), so I'm not sure they can be reused here:\n[error] /Users/ver/b/l5d/linkerd/protocol/http/src/main/scala/io/buoyant/linkerd/protocol/http/ResponseClassifiers.scala:77: type mismatch;\n[error]  found   : PartialFunction[com.twitter.util.Try[Nothing],Boolean]\n[error]  required: PartialFunction[com.twitter.util.Try[Any],Boolean]\n[error]       TimeoutAndWriteExceptionsOnly.orElse(ChannelClosedExceptionsOnly)\n[error]\n. incomplete sentence\n. Yeah, the goal here to save on needless object construction.\nFor debug trace, I think we should make the debug tracer a full fledged tracer like:\nyaml\ntracers:\n- kind: io.l5d.log\n  sampleRate: 1.0\nThis solves this problem nicely so that isActivelyTracing will be true.\nNote that isActivelyTracing returns true when:\n1. at least one tracer is configured\n2. and sampling is not false.\n. this might be clearer as experimentalRequired?\n. Also, it would be helpful to have doc strings for all 3 of these fields\n. experimentalRequired? docstrings...\n. you're right good catch. it should be a val. and it can be a method but kl asked me to change it.  I think it makes sense as a method, so I'll change it.\n. should this expose an Activity (like lookup)? Polling for a set is a special case of watching, and I imagine it makes sense to watch this for many applications\n. tioli: i'd be inclined to write this as\nActivity.collect(namers.map(_.getAllNames)).values.toFuture.flatMap(Future.const).map { names =>\n(You don't really want an Activity of responses ever -- you want to get a list of names and then build a response). Not a huge deal.\nAlso nit: we use rsp throughout linkerd.  Prefer rsp over resp (since this isn't go ;p)\n. should this be on the admin or as a part of the HTTP controller API?  I'd expect it to be on the controller api.\n. yep\n. not sure we need to do this, cleared so that it's not passed-through automatically and the client-side is responsible for encoding any downstream deadlines.\n. i want all of the context headers to be l5d-ctx-*, and also i'd like that these be assumed to be linkerd-generated (so that, for instance, all all l5d-ctx can be stripped from external traffic).  l5d-dtab is intended to be user-facing.  Not 100% sure the distinction is necessary, but i wanted the user-facing header to be terse.\n. Yeah i understand.  I think this is simple because we can clarify the context of this header more clearly:\nl5d-ctx headers are ONLY to be generated by linkerd. They should always be stripped at the edge.  l5d-dtab and l5d-sample are ONLY to be set by users, and only admitted from trusted sources.\nIn the same way that l5d-sample influences l5d-ctx-trace, l5d-dtab influences l5d-ctx-dtab.  l5d-ctx headers should be totally opaque to users and services.\n. Does this need to extend NameInterpreter? I think this trait could simply express delegation features, i.e. Delegator.  The NameInterpreter-ness of this type isn't really related to Delegator-ness.  ConfiguredNamersInterpreter would extend NameInterpreter with Delegator, and DelegateApiHandler can take a String => Option[Delegator]... tioli.\n. Is it easy to test what happens when we request for a non-delegating interpreter?\n. I generally prefer to break up complex blocks into a few clear statements:\ne.g.\nscala\ndef apply(req: Request): Future[Response] = {\n  val dtabs = baseDtabs.map {\n    case (ns, dtab) => getDtab(ns).map(ns -> _)\n  }\n  Future.collect(dtabs.toSeq)\n    .map { ds => render(ds.toMap, baseDtabs) }\n    .flatMap(view.mkResponse(_))\n}\nI sometimes find it hard to understand chains of collect/map/flatmap, and I find it helpful to see them split out.\n. bikeshed:  It seems like a metrics section might have some other things (like, configuration that controls how metrics are recorded in addition to exported.  Does it make sense to structure this as:\nyaml\nmetrics:\n  exporters:\n  - ...\n. I'd be inclined to separate configuration types from the type that actually does the work. I'd prefer to see something like def mk(timer: Timer, metrics: MetricsProvider): MetricsExporter here.\n. I'd be inclined to push this logic into each exporter.  There's nothing that says stats export has to be on a fixed interval that's known up front.  Let the exporter make that decision.  MetricsExporter could simply expose Closable.  Then, all exporters can be joined with Closable.all(exporters:_*)\n. That said, it definitely seems like a good idea to provide PeriodicMetricsExporter and PeriodicMetricsExporterConfig types to make it easy to write these.\n. I think DelegateApiHandler can now be expressed in terms of Delegators.  delegatorByNs: String => Option[Delegator]...\n. I generally prefer to use wrapper types only where they are necessary.  I.e. calling read should fail with an exception if it can't be read.  In get, we call Try(read(v)).toOption to wrap it appropriately.  Much in the way that Path.read and Dtab.read throw exceptions on illegal input.\n. Yeah, agreed.\n. Yep. For instance, when an identifier cannot name a request, errors are thrown on the server stack.\n. leave ## x.x.x up top?\n. we should just use FinagleHttp.param.Decompression like we do for Streaming... makes it slightly clearer\n. I think this can be a (lazy?) val.  Since vars/activities are populated lazily, it's safe to build statically.  Furthermore, this ensures that multiple consumers share observations, potentially reducing redundant work.\n. Similarly, is this necessary to be a def?  I'd expect it be to be fine/good for this to be static (potentially a lazy val?).\n. oh, i see. thanks\n. Wait -- doesn't this mean that that this activity can't actually behave properly if new namespaces are added?\n. can this be private?\n. We may want to note (in a comment) that caches must be updated with synchronized -- otherwise this could be potentially unsafe.\n. TIOLI: there are probably ways of doing this with fewer copies (e.g. req.uri.drop(captured.show.length)) -- probably not worth optimizing yet\n. Oh, we get a ton of warnings from sbt always because, for instance jackson, pulls in alternate scala versions.  I put this in at some point to pin scala versions to ours\n. That is intentional, since response size computation is totally broken for chunked responses, which is really misleading. We should track request and response body sizes and durations, but I'm happy to drop them for the time being.  Should ticket the fix, though.\n. this will throw an exception if there aren't 4 elements returned.  we should be more tolerant of unexpected input:\nreq.uri.split(\"/\", 4) match {\n  case Array(...) =>\n  case _ => // doesn't match ...\n}\n. If at all possible i'd prefer to use a real url parser.  URLs come in some unexpected shapes and sizes.  Does java.net have something that does this job?\n. We should probably drop Proxy-Connection regardless of whether it's detected to be a proxy request or not.  Connection headers should never be forwarded by http intermediaries.\n. i know we've done this elsewhere, but now i have a slight preference for making this a class, e.g.\n``` scala\nclass DiscardProxySideEffects extends SimpleFilter... {\n}\nobject DiscardProxySideEffects {\n  ...\n}\n```\nSo that the filter can easily be extended.\n. Finally, I think that this needs to be installed on the serverside and not the client-side.\nWe want this rewriting to happen before we apply an Identifier to the request (which happens above the client).\n. super minor: can yiou please sort imports without breaking out sections.\nFrom Effective Scala\n\nSort import lines alphabetically\nThis makes it easy to examine visually, and is simple to automate.\n\nIntelliJ, etc have settings to configure this.\n. sort without breaking...\n. This test should have a different host value so that the test validates that the URI's host value is used.\n. please try to wrap lines around 100 chars.  In this case I'd just move agentApi... onto the following line.\n. similarly, I'd break this up like:\nscala\ncatalogApi.serviceNodes(\n  key.name,\n  datacenter = ...,\n  tag = ...,\n  blockingIndex = index,\n  retry = true\n)\n. I'd prefer to enumerate the set of supported metadata fields until we have a demonstrable need for an arbitrary map.\nLet's have an optional authority field (to match HTTP/2 conventions) that captures what we're doing here.  Please include some brief (inline) documentation about the role of the field.\n. Where does versionString come from?\n. Omit override?  I don't think these actually override anything...\n. please lowercase field names (domain, config).\nplease include a ~1-sentence summary comment of each of these types so that a maintainer can get their bearings without having to read consul docs\n. I'd prefer if we had one val'd version of this string so that typos were compilation errors.\nAlso minor style preference for Addr.Metadata(\"authority\" -> s\"...\").  Neither are blockers, just sharing...\n. tioli: it would be fine to write this as mkRequest(None).flatMap(update(_, meta)).handle(...)\n. Ideally, it would be great to have the known metadata keys somewhere (as we do for headers, for instance) -- we can be incremental to getting it somewhere global, though.\n. This feels a bit awkward to me. I think this should be a Path -- there's more information that needs to be encoded in some registry schemes (e.g. zone, environment, role, etc).  I don't think we should try to force this into a 2-tuple, and instead should provide a mechanism for these schemes to encode announcer-specific coordinates.\n. Is it correct that this announces all instances with shard id=0?  Should we support a Path scheme that encodes shard ids? e.g. /some/path/to/thing/0\n. At the very least, we should be careful to document this, since it may lead to surprising behavior in some environments\n. Thinking aloud here -- perhaps it makes sense to pass announcers to routers/servers rather than have servers expose their paths to main.\nMy thought here is that it's an implementation detail that the list of endpoints is statically provided.  We could envision a model where routers can register additional services at runtime.  If routers have access to the announcers -- or an \"AnnouncerDispatcher\" that deals with prefix mapping/removal -- then the static-ness of announcement is an implementation detail rather than a fundamental assumption.\nPerhaps YAGNI.  I'm fine with this as-is, just thinking about some possible future use cases.\n. You're right, but it's even crazier -- there are some initializers in FooConfig.scala.  The issue arises because the style is match TypeName with TypeName.scala, but it doesn't make sense to split these things out into separate files because they're so tightly coupled -- and we can't use classes namespaced in companion objects because it messes with the reflection/object loading needed to do plugins & json loading.\nThen there's a newer part of the scala style guide that says files with multiple classes should be given lowercase names... Which is what I am (implicitly) proposing here.\nGood catch, though.  We should standardize.  I've found the *Initializer.scala naming to be confusing when I'm looking for the non-initializer class.  I'll move these onto that convention for the time being though\n. Yeah, you're probably right that this shouldn't look at id.sampled at all...\n. eh, revert this change\n. How do you feel about using the descriptive paths like /$/fail instead?  I find these symbols to be really hard to keep track of mentally.\n. might be a little robust with something like:\nscala\ndtab match {\n  case null | \"\" => Return(Dtab.empty)\n  case dtab => Try(Dtab.read(dtab))\n}\n. have a slight preference for \"/$/neg\" over ~, etc...\ndefinitely not a blocker\n. It seems like a bug for FutureCancelledException to be thrown ever... (Future.cancel is deprecated).  Any ideas where that's coming from?\n. We should probably return a Failure with Interrupted set -- I think Failure is the new idiomatic way to do this\n. I think the most idiomatic thing to do is that we do somethign like future.raise(Failure(\"consul observation released\", Failure.Interrupted)) instead of future.cancel()\nBasically, I'd prefer to use failures to signal this information instead of a list of exception types.\n. I think it's fine to fix the exception site and the retry handling together in this PR.  Thanks!\n. This is the main meat of the fix, fwiw\n. herein lies the bug\n. Does anyone have suggestions wrt how we should deal with tags like this in our docs? We don't have a great way to keep these up to date...\n. Would you mind moving this into newNamer so it's clearer how this is used (it's not really part of the config).\n. same\n. can these be private, or even private[this]?\n. private?\n. This seems a little surprising to me. If I specify a prefix on the identifier, I'd expect this prefix to apply to all names provided by the identifier.\n. but this isn't actually concrete, is it? It's going to be applied through a dtab.\n. Also, we should clear this header if we read it.\n. Yeah, I think that's the best we can do at the moment.\n. open question\n. I think this code is only reached when the loop has completed and there's nothing here that resumes looping, so this probably doesn't behave as expected. I've changed this in a followup branch.\n. This is common and intentional throughout Finagle (and while we don't use Finagle's style as dogma, I find it's convenient for the projects to stay aligned).\nWhen we have box types like Option, it's preferred to simply use a name like domain rather than domainOpt or maybeDomain etc.  Because we're just unboxing the value, it's fine to shadow the name because it signals the intent of \"just unboxing this thing\".  I suppose it'd also be fine to use a temp val like d here. \n. right, so if mkRequest() throws a ChannelClosedException, handleUnexpected will log a message but no further requests will be issued.\nI think this is a moot point, though, since retries are handled elsewhere and this exception should never be encountered here.\n. We'd just wrap that future in an Activity below to compose it in watchDc, so I don't think that it really makes a difference\n. As far as I can tell, this is exceptional.  AFAIU, the consul api should always return an index.  This is more of a \"we're not really talking to consul are we\" situation.\nI think we can't retry here because we'll just busy loop just issuing the same request over and over again.  Unless this is dictated as part of the consul API, think we should bail here.\n. it handles exceptions constructing Address (i.e. in case the IP or port is invalid)\n. this is done below in pending -- but this is different from the other file, so perhaps better to just make this a big transform block like in DcServices\n. As discussed in #625, it's necessary to use guards to ensure that work stops eventually.\n. Alright, though this means that the activity will never be satisfied for invalid datacenters...  I'll remove this error handling.  If we want to have a special-case handler for invalid dc, we should raise some dedicated exception type and handle it here.\n. @Ashald What would you expect the namer to do if it gets a 403?\n. As discussed, we're going to fail the activity for now when any non-automatically-retryable exception is encountered.  We can revisit this (in which case, the ConsulApi should be modified to provide clearer exception types).\n. Added a comment to this effect\n. doh, yeah.\n. intent was that it may not actually be used if there are no endpoints.\n. (1) withParams comes first so that withMonitor overrides any default monitors in the params.\n(2) this should probably be prefix.show.stripPrefix(\"/\")\n. good point.  i think this is actually what I was trying to measure, but i should clarify with scoping.  Maybe lookups/neg and lookups/leaf\n. Prefer to start conservative with number of metrics to start.  What we have here should be enough to do relative debugging (\"huge spike in service changes\", etc).  If this ends up being too opaque, we can always add more stats.  My gut says this is Good Enough.\n. I'm wary of having Unidentified include a request -- this seems to indicate that failed identification can/should be able to modify requests which seems like something we don't want? (yes yes i know about our awful http types).\n. what's the motivation for keeping this exception around?\n. nevermind, i see now.  Maybe a comment to indicate how it's used?\n. this ADT probably deserves some /** commentary */\n. tioli: you can provide unapply helpers for these (and we'll just never update them... or have to break API again). This is, for instance, how the Name ADT works: the Name types are classes that have companion objects with unapply\n. tioli: if you have unapply, you can sugar this with IdentifiedRequest(dst, req) <- getDst(req0).transform { ...\n. i'd guess the type annotation isn't needed here.\nALso I'd be inclined to have a val path = mkPath(...) so that you can fit this all one line with a bit more clarity\n. tioli nit: similarly, i'd be inclined to capture val dst separately\n. omit empty lines, and sort\n. I'd prefer to omit this. Doesn't add much information\n. I'd prefer that this said something like: Received SIGNALNAME. Shutting down.\n. sorting is weird here\n. might consider just referring to http.Request in the code, since we refer to http.Version (or import Version, etc)\n. [Request] is probably inferrable\n. If we don't actually do any string formatting here, we can just make this exception static\n. i'd leave a little breathing room between the trait and the next comment\n. Surprised this works... I'd expect to have to do Some((a, b)) or Some(a -> b)... but if it compiles...\n. sorting is weird\n. Another thought:\nsince this ADT is under RoutingFactory, it will be cumbersome to write Java identifiers (they'll have to use $s or, we'll have to provide RequestIdentifications compat builders\n. we may want to break this into a few functions so it's easier to read\n. fwiw JavaConverters is the recommended replacement for JavaConversions.  Supports n.asScala. not at all a blocker\n. I think if we just had a def composeIdentifiers(id0, id1), this might read a lot clearer\n. can we make this package private to indicate it's for internal use?\n. or even just private to the file\n. yeah, the intent is that you can't use the unfiltered one in the block. you have to use the right one. that said, i'll rename the first to announcers0\n. My thought was, since it's called Announcer, it's intent is pretty clear. When I have a class called Verber, I tend to prefer Verber.apply to Verber.verb so that:\nscala\ndef thing(verb: Verber) = verb(noun)\nvs\nscala\ndef thing(verber: Verber) = verber.verb(noun)\n. Since the scope is so narrow (just this function), I don't find it any clearer to call it pending, but I can if you find it confusing.\n. I think we want to signal cancellation in case the announcer is out to lunch / etc.  I added a var that signals to pending-handling whether cancellation has already occurred.   It's not 1000% safe, but I think it's basically as good as we can do.\n. ah yeah, i thought i fixed this (probably in another branch)\n. correct\n. I don't feel strongly one way or the other. will revert\n. I find this naming to be unclear. Elsewhere, we describe pre-delegation names (i.e. Dst.Path) as logical and post-delegation names (i.e. Dst.Id) as concrete.  In this context, the value isn't really a concrete name.  We should try to reconcile this.\n- l5d-dst-name?\n- l5d-dst-path?\n- l5d-dst-logical?\n- l5d-dst-target?\n. alpha sort, por favor\n. add a test with the default header?\n. Hmmmm....  I suspect that this fallback logic will yield to some unexpected behavior.\nscala> com.twitter.finagle.Path.Utf8(\"how/r/u\").show\nres0: String = /\\x68\\x6f\\x77\\x2f\\x72\\x2f\\x75\nIt may be best to fail if it's not a parseable path.\n. Yeah, I understand that there are configurations where it makes sense to use prior concrete name (or the prior residual) to influence next-hop routing.  It seems like a confusing default, though.\n. Who would ever come up with such an awful idea?\nWe could support this IFF the value contains only showable path chars...  Not sure it's worth it? Up to you.\n. prefer sorted\n. It's probably worth documenting explicitly how \"local\" is determined.  From the docs:\n\nReturns the address of the local host. This is achieved by retrieving the name of the host from the system, then resolving that name into an InetAddress.\n\nIt's unclear what this line does:\n- when the host has no IP\n- when the host's name isn't resolvable\n- when the host has multiple IPs\nFurthermore, in some cases (kubernetes), we may need to do additional work to determine locally-routable IPs.\nPerhaps this class should take an isLocal: InetAddress => Boolean argument so that this behavior may be easily swapped?\n. That's not bad.  Now that I look at the header docs, is it confusing to reuse the l5d-dst prefix?\n\nThe informational headers linkerd emits on outgoing requests.\n...\nApplications are not required to forward these headers on downstream requests.\n\nPerhaps l5d-name or l5d-target?\nNames are hard.\n. Just a note to answer your question -- AdminConfig.port is probably wrong (throws NPEs) as it is here. It should probably be an Option[Port].  Don't worry about fixing it here, I have follow branches that clean up admin initialization.\n. tioli: per discussion, maybe worth elaboring on the intended use of this class in the docstring.\n. I think we should try to explain the localhost-determination logic here.  (Resolve the local hostname in dns, non updating.)\n. docstring?  how is this intended to be used\n. a few /* comments / would probably help here\n. I don't love that consul is picking up extra deps, but its probably the least worst option for the time being.\n. we tend to just use object over case object -- it doesn't add anything valuable\n. after reading up, it seems like the only real value of case object is that it generates a \"pretty\" toString.  not a big deal one way or the other.\n. not required, but it signals (to me) that there's a function allocation here.  Got this convention from finagle.\n. just so you have a way to list all of the admin endpoints without guessing or reading the code, or worse yet logs (a problem i personally have a lot)\n. TBD.  we'll have to plumb this through into the router, presumably\n. perhaps io.l5d.k8s.daemonset?  I think k8s should be in here somewhere.\n. good use of \"smoosh\" \ud83d\udc4d \n. similarly, if this is k8s-oriented, would like to have k8s somewhere in the plugin name.\n. I'd probably wrap these lines, they feel a bit long\n. Hmm, I'm pretty sure that we shouldn't just drop metadata here?\n. this comment is a little unclear -- there are multiple name trees.  What is daemonSet and how is it used?\n. Is it okay to drop daemon metadata? It's hard for me to answer that question in the context of this class alone...\n. i prefer parens for mk() since it reads imperative and not something like toTransformer  (also, from the interface point of view, it may be side-effecty).\n. I guess more generally, i might be inclined to do this comparison manually/procedurally just because it seems unfortunate that we have to allocate to do an IP comparison.\n. e.g.\n``` scala\nval b1 = addr1.getAddress.getAddress\nval b2 = addr2.getAddress.getAddress\nvar matches = true\nvar i = 0\nwhile (matches && i != 3) {\n  matches = b1[i] == b2[i]\n  i += 1\n}\nmatches\n```\n. do we need to ensure that these are ip4 addresses? (perhaps match against java.net.net4Address?)\n. i realize this is nowhere near uniform throughout the codebase. we can sort this out later\n. I forget... are we required to build a new client for each namespace?\nI remember there being a bug that made us do this, but I thought we fixed it.  Perhaps not.\nI would expect this to be \nscala\n    val client = mkClient(Stack.Params.empty)\n      .configured(Label(\"daemonsetTransformer\"))\n      .newService(dst)\n    def mkNs(ns: String) = Api(client).withNamespace(ns)\n. ok. now that i've wrapped my mind around this, i think this is right.\n. It occurs to me that this class is in no way kubernetes-specific (only the /24-ness of the subnet match is really kubernetes specific).  If we write a generic shareSubnet(resolved: Address, routable: Address, subnetMask: Int), this is a generic way to route to a single linkerd per subnet!\nThis is very cool and probably useful in a variety of network configurations.\n. similarly, this just a stone's throw from being useful for arbitrary subnet masks\n. I think case object is fine.\n. disregard this client change for this branch\n. Here's a sketched up change that introduces an arbitrary subnet gateway transformer, usable from the k8s transformers: https://github.com/BuoyantIO/linkerd/pull/671, but also probably useful on its own.  We can consider this in a followup change.\n. understood now, dropping daemon metadata is ok\n. prefer Nil over Seq() -- Seq incurs an allocation and Nil does not.\n. Since this is going to run on all requests that traverse http routers, we want to be as conservative as possible about allocation.  And, unfortunately, things like list.foreach { _ => ... } incur an allocation for the anonymous function!\nWe can prevent this allocation by \"val-ing up\" the anonymous function: \n``` scala\ndef scrub(msg: Message): Unit =\n  Headers.foreach(scrubHeader)\nprivate[this] val scrubHeader: String => Unit = {\n  case h@Connection => ...\n  case h => msg.headerMap.remove(h)\n}\n```\nThis allows us to only allocate scrubHeader once and not on each invocation of scrub.\n. nit: prefer dotted svc(req).map {\n. oh, we probably want to val-up the response function as well.\n``` scala\nsvc(req).map(scrubResponse)\nprivate[this] val scrubResponse: Response => Response = { resp =>\n  HopByHopHeaders.scrub(resp)\n  resp\n}\n```\n. D'oh, of course you're right.  I'd be inclined to remove the foreach loop and just enumerate the headers in the function.\n. please alphasort with no spaces:\nscala\nimport java.io.File\nimport scala.io.Source\nimport sun.misc....\n. tioli: const-ify this default somewhere so this is\nscala\nshutdownGraceMs.map(Duration.fromMilliseconds(_)).getOrElse(DefaultShutdownGrace)\n. string interpolation is a bit cheaper than + (again, allocations):\nscala\n  case Some(via) => s\"${via}, ${viaLinkerd(msg)}\"\n. no need for the link in the description (though it's great to have somewhere in the docs in this file).  If I'm not mistaken, the description is used for displaying diagnostic information about client/server configuration.\n. Remove the [braces]?\n. I don't think it breaks anything. It's an improvement. SIGINT and SIGTERM would shut down the process before, just not before draining its queues.  In other words, I don't think we would have bumped minor version for this change alone.\n. D'oh. Of course you were. My eyes were drawn to the green line.\n. We have a HostAndPort type.  Perhaps it's preferable to use that?\n. We're breaking backwards compatibility by changing the defaults, though.  Either we shouldn't change the default, or I think we should go ahead and break compat.\nI think an admin section makes sense, regardless, as I think we want to be able to add additional configuration to the admin subystem.  For instance, we may want to make the behavior of the shutdown and gc endpoints configurable...\n. can we just have def mk(defaultAddr: InetSocketAddress)?\n. Is there a reason that this logic shouldn't just be in Api.parse?\n. Prefer that this start with a verb. Something like:\n\nAdd an ip option to admin configuration so that access to the admin server may be constrained\n. tioli: Api. is now redundant since this is within Api\n. Call out/link to namerctl?\n. Should we have distinct Config and API sections?\n. supported content types?\n. supported content types?\n. don't we support an Accept header for this?\n. it's parameterized on [Req]\n. yeah probably better to call this fold\n. i like it\n. not with future.flatMap -- it's still \"safe\" in that this won't blow the stack, but tailrec and futures are not friends.\n. nit: would be nice to annotate the type since this is basically a public interface now\n. does this need to be public?\n. does it even need to exist?\n. does this need to be public?\n\nalso, prefer type annotation like:\nscala\nval serviceNs: Ns[Service, ServiceWatch, ServiceList, ServiceCache] = new Ns(backoff, timer {\nI care less about this if it's not public.\n. i imagine this logging (and elsewhere) would be more useful if it included the namespace?\n. tioli: i know this naming comes from the prior api, but might it be more appropriate to s/Watch/Event/ ?\n. It seems like we actually have a pretty well-defined for interface for Cache.  Does it make more sense to just have NsCache and ServiceCache implement a common interface that handles initialize, update, etc?\n. tioli: there are a few stateless private helpers in this file.  You could move them into a companion object to more clearly differentiate static-helper and stateful-updatey logic\n. this is the first/only dashed name in our kinds...  Perhaps io.l5d.k8s.external?\n. perhaps clearer with a specific type annotation on unstable... just because this is all a little subtle\n. tioli:\nscala\ncase  Some(address) =>\n  addr match {\n    case null =>\n      addr = Var(address)\n      update() = Some(addr)\n    case addr =>\n      addr() = address\n  }\ni could go either way on this, just trying to be clear about the handling of null\n. according to @mosesn, it's no longer necessary.  Leaving it commented as a reminder\n. I think we should simplify this now that we're not actually tracking a Stat -- can we just do a retries.incr() when an actual retry is attempted (i.e in the schedule block above)?  This will make it so that the counter is updated instantaneously, not only after retries have been exhausted.\nAlso we should probably rename retriesStat to retriesCounter or something...\n. It feels odd to have these scoped like this so that we have\nfoo/retries.count\nfoo/retries/total\nPerhaps scope the stat as statsReceiver.scope(\"retries\").stat(\"per_request\") so that they end up scoped similarly (also this makes it clearer what the stat is)\n. That's a good point!\nHowever, we already change scoping of finagle stats substantially, so I think it's better for our stuff to be internally consistent than finagle-compatible.\n. 'copy' conflicts with case classes\n. Ah, yeah...\nthe outer Future fires when the message's headers are written, and the inner Future fires when the stream has completed.  Needs docs, definitely.  I suppose this could also be modeled as (Future[Unit], Future[Unit])...\n. yeah i ended up doing ; () in this branch because it ends up being a lot less visually heavy -- I can keep one-liners as one liners (whereas having val _ = .. feels too cumbersome for a one-liner) and it also throws off vertical alignment in a bunch of cases, e.g.:\nscala\nfoo.bar()\nfoo.bah()\nval _ = foo.baz()\nvs\nscala\nfoo.bar()\nfoo.bah()\nfoo.baz(); ()\nI think the semicolon sends the same sort of i'm doing something intentional here warning.  Not super attached to this, but I do feel it made some parts of this read quite a bit nicer.\n. Yeah. This comes from the AsyncQueue API.  Synchronicity is all we really need in this implementation, it's possible that other use cases would benefit from an async API, though.  I have a follow-up branch related to this API, too.\n. whimsy\n. yep\n. it uses segments:\nscala\n      case Some(segments) =>\n        lazy val tooShort = {\n          val msg = s\"Path must have at least $segments segment(s)\"\n          Future.value(new UnidentifiedRequest[Request](msg))\n        }\n. Yeah, sure. I'm skeptical about that feature in general. But if we want to add it, it would go here.\n. Changed so that the default (when no segments is specified) requires a non-empty path.\n. HTTP/gRPC/thrift/etc\n. Alternatively, consider just making this a rewriting namer that rewrites to Path.Utf8(\"$\", \"inet\", host, port) -- the idea being that if the implementation of /$/inet changes, we'll get it for free.\n. The thought was that Frame is always read out of the stream, but the type of object you put into the stream is what's parameterized.  It can just be T though.\n. caused tests failures!\n. where does this magic string come from?  Should it be configurable?  At the least, I think it should be consted somewhere with a comment.\n. This is fragile and will break in CI.  we should use promises to control these sorts of updates\n. I think we can just do one Trace.letClear { ... } around this whole block\n. Is it appropriate to just ignore parse errors? I think it would be more user-friendly to fail to initialize when the environment variable is set but clearly invalid\n. Let's remove these logs in favor of logging centrally once for the failure (i.e where it's called).\nNote that it's preferred to use log.error(e, ... so that we properly formatted stack traces, etc.  Then, we don't need to have programmer-focused details like \"rspToAddrs\" in the log message, since that is captured in the stack trace.\nIdeally, would like the log message to be something like marathon at $host returned $code\n. I think this means we can issue a request with Authorization: token= -- is that ever appropriate?\n. Is it necessary to use an Activity here?  This is never pending, right?  It seems like a simple var/AtomicReference would be sufficient for authToken\n. these should be volatile or updates may not be visible across threads\n. this was in 0.8.2\n. referenced recursively\n. Yep. There are no IDs left to use on this connection.  Need to start again on a fresh connection.  Stream reset simply signals the error of one stream (but we don't even have a stream to reset at this point).\n. It's not reliable-reliable, but how do you test a negative...  This is really the inverse of eventually.  This is my hacky version of notEventually\n. it seems like this could be done as checkBound(tree: NameTree[Name]): Option[NameTree[Name.Bound]] to avoid the casting.  But I don't really see this as a severe issue...\n. this should be lazy, since name resolution can be side-effecty, do networked things.  That shouldn't start at instantiation.\n. also, re: use of Namer.global -- does this mean that configured namers can't be used?  should the transformer be configured with a Path => Activity[NameTree[Name] or something?\n. to speed up e2e tests (i.e. in ci).  we can always add more, but it felt like overkill for CI.\n. yeah, I'm coming to accept that stress testing doesn't really belong in the CI flow (i.e. shouldn't block commits)..\n. Can we make one further small change here:\nI think both curator and serviceDiscovery should be declared as private[this] lazy val so that network ops are not initialized during instantiation.\nAs it is, running linkerd-examples/test in SBT without a local zookeeper running causes a bunch of warnings:\nWARN 1101 20:15:28.993 pool-92333-thread-5-ScalaTest-running-ExamplesTest-SendThread(localhost:2181): Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect\njava.net.ConnectException: Connection refused\n    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n    at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:356)\n    at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1192)\nWARN 1101 20:15:30.100 pool-92333-thread-5-ScalaTest-running-ExamplesTest-SendThread(localhost:2181): Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect\njava.net.ConnectException: Connection refused\n    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n    at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:356)\n    at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1192)\nWARN 1101 20:15:31.203 pool-92333-thread-5-ScalaTest-running-ExamplesTest-SendThread(localhost:2181): Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect\njava.net.ConnectException: Connection refused\n    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n    at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:356)\n    at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1192)\n(and these are logged ~forever).\nOther namers avoid network initialization until it is needed.  Making these vals lazy should fix this.\n. I don't think that telemetry-core should have any dependencies on zipkin, or any other concrete telemetry implementation, for that matter.  Why was this necessary?\n. let's call this project 'zipkinKakfa' -- other tracing schemes could use kakfa differently\n. Please unify the import style with that  used elsewhere in the project:\n1. 1 section (no whitespace lines within imports)\n2. Alphabetically sorted\n3. Use scala-style imports for multiple imports e.g.\nscala\nimport org.apache.kafka.clients.producer.{Callback, ProducerRecord, RecordMetadata}\nor if there are more than ~5 imports:\nscala\nimport com.twitter.util._\n. Please make this debug-level logging.\n. I'd prefer that we not have log lines in constructors.  If this is really helpful (is it?), it should be at the debug level\n. it seems like it would be better for KakfaRawZipkinTracer to take a KafkaConfig object so that we don't have all of this stuff hardcoded in the tracer (ideally, this should all come from the TelemeterConfig)\n. If this is really helpful to log, I'd prefer that this include some more context.  Also I'd prefer something a bit terser:\nzipkin kafka brokers=%s retries=%d sampleRate=%f\n. prefer /* / style docstrings, especially for public members.  When using //comments, please put add a space like: // comments\n. what calls sendSpans?\n. can this be private[this]?\n. use promise.setDone()\n. I think this means that cleanup() needs to return a Future[Unit].  We must not use Await.result, unless you can guarantee this won't run in finagle/netty threads ever.\n. can these be private[this]?\n. I think this should be called io.l5d.zipkinKafka, so that all of the zipkin tracers sort lexically adjacent.  This also reinforces that the package name should be telemeter-zipkin-kafka or something\n. If this is part of the RawZipkinTracer interface, we tend to declare it as override def sendSpans so that it's clear that it's part of the parent type.\n. Why is this caching logic necessary?  Can this be simplified to:\nscala\nlazy val tracer: Tracer =\n  SamplingTracer(new KafkaRawZipkinTracer(brokerList, numRetrise, topic), sampleRate)\n. nit: preserve leading slash: /pfx/io/buoyant/foo/resource/name\n. Is this second sentence true? I thought marathon used slashes.  Either way, I might omit this sentence.  (I see this as useful in a bunch of cases where some portion of an FQDN is used to route -- hardly marathon-centric).\n. Similarly, I'd be inclined to leave out the consul references...\n. seems slightly odd for this to be in the config class...\nShould we have a MetadataGatewayTransformer?\n. tioli: .getOrElse(false) reads slighly clearer to me\n. we use this string a lot.  Worth constifying? (so that typos are compile errors)  Something like NodeNameMetaKey?\n. Worth a comment about what the field refers to.\n. tioli (i like pattern matching ;p):\nscala\n(authority, nodeName) match {\n  case (None, None) => None\n  case (authority, nodeName) =>\n    Some(...)\n}\n. consider caching prefix ++ path on the class (or better yet, just take a path in the constructor) so we don't have to do additional per-request work. nit: would you mind breaking some of these long lines on => so we keep things generally <100 chars?. consistency/clarity: use Stack.Transformer here, and don't import Transformer. If we're going to special-case the name, should we just move this into newInterpreter and use label?  Otherwise, getClass.getName. this isn't a namer, should that be interpreter/?. I'd imagine we want this interpreter marked as experimental?. hah yes, good catch. yep, i suppose i could use .poll to get a slightly nicer string. I'd hope so, but honestly the logging stuff gets wonky all the time, so i've taken to being OCD about clearing it out. iirc named parameters can only be used with optional params.  This commenting style is all over finagle's netty interactions, fwiw. i forgot to reenable the test ;). thanks. most of my scala knowledge comes from things i tried once in 2011 ;p. I gave it a shot but it's a little tricky to do without allocating. going to punt for now. honestly, just cargo-culted from finagle, i'll take a deeper look. It's an api breakage of sorts (may break linkerd-viz, etc, idk).  going to leave it in for now and we can remove it separately.. does making it a plain-object change that behavior, though?. well, i wanted this to be a package rather than an object, but i was being lazy and wanted to put it in one file without creating a directory...  hence package object.\nMoving it into its own directory doesn't really address your concern, though. how about i rename it context so it seems like less of temp val. you're right -- the local-clearing stuff only belongs at the top of the server. yeah, went that route. Is there a reason you think these all should be marked final?. looks like this needs to be updated from master. yeah (force of habit). i don't think that this is necessary in this case, though.. oh nm, this is why:\n[error] /Users/ver/b/l5d/router/h2/src/main/scala/com/twitter/finagle/buoyant/h2/Stream.scala:218: double definition:\n[error] def apply(pairs: Seq[(String, String)]): com.twitter.finagle.buoyant.h2.Frame.Trailers at line 215 and\n[error] def apply(pairs: (String, String)*): com.twitter.finagle.buoyant.h2.Frame.Trailers at line 218\n[error] have same type after erasure: (pairs: Seq)com.twitter.finagle.buoyant.h2.Frame.Trailers\n[error]     def apply(pairs: (String, String)*): Trailers =\n[error]         ^\nforce of habit for good. I think I'd prefer that we scope this into a Paths object and reference as e.g.  Paths.Percent, `Paths.stripTransfomerPrefix()?\nAlso perhaps better to call Hash->ConfiguredNamerPrefix, Dollar->LoadedNamerPrefix, Percent->TransformerPrefix, etc?. tioli:\nI like to do\nscala\npackage io.buoyant.namerd\npackage iface\nfor this sort of thing so that intra-project imports aren't necessary.. ... What does this do?  Are we setting the residual path of the transformed name?   And we set it to be only the bound id? should this be (effectively) bound.id ++ leaf.path?\n. Is this related to the finagle change? Do we need to change our jackson version to be consistent with finagle?. What changed in finagle related to this?. nit: Should the Paths object actually be in namer-core?. The service is called Eggman and the original method was Splat, and then I took it too far.  I think we'll replace this test in the future, so I'm not too worried about it for now.. We should ensure that LowMemSettings uses the assemblyJarName defined in appPackagingSettings and not that defined in BundleSettings so that the two assembly targets produce different files.  This probably applies to namerd too.\nFurthermore, would prefer 32b to 32bit.  Perhaps we should just rename \"lowmem\" to 32b, since we don't change any heap settings.?. I think we'll need to fix dockerTag/assemblyJarName/etc for Dcos as well?. As far as I can tell, this doesn't really do what we want.  As I mentioned, I think we should just leave all of this in for now.. can we clarify this?. \"does not support the boringssl TLS extensions required for ALPN, etc\". You should also note that histograms are windowed internally to the metrics registry and so it doesn't make sense to write histograms any more frequently than the window (1m). No and I had code to restrict this before, but I thought that it was simpler to provide a generic implementation than to have to describe the minor difference.. The spec says that, for the purposes of correlation, a random id can be used for the lifetime of the connection, which is how I implemented this in linkerd. I think it's to be used to show the existence of an intermediary and not any additional diagnostic information. We should probably trace these values (and even store them as records?). What does this do?. \"tracelog\" actually logs traces, whereas this module does not. Also, it's not named consistently with e.g. tracelog.  Any ideas for how to name this to more clearly differentiate it?. prefer to split this up:\nscala\nprivate[this] val cache = CacheBuilder.newBuilder\n  .maximumSize(capacity)\n  .build[SpanId, Seq[Record]]. tioli, could be simplified:scala\nrecords.collect { case Record(id, , Annotation.ServerAddr(addr), ) => id -> addr }\n```\nsame applies to other flatMaps below. There's a lot of logic in here.  What's going on?\nI'm confused that this is doing a nested iteration of three lists (clientAddrs, serverAddrs, and allServerAddrs), and then some filter is done on annotations for the given span? Anyway, this needs english. I don't understand it.. Is this defined on the instance?  AFAIU, this makes it hard for anything outside of this class to refer to this type (since it's a path-dependent type). Can this be made static?. This is racey and can lose records. ?. Thinking aloud, it'd be particularly nice if this could return a JSON representation. Not a blocker.. Can this be simplified as:\nscala\nval clientAddrs = ...\nval serverAddrs = ...\nallServerAddrs.flatMap {\n  case (dstId, dst) if dstId.spanId != span && dstId.parentId == span =>\n    (annotationNamed(annotations, \"router.label\"), annotationNamed(annotations, \"namer.path\"), annotationNamed(annotations, \"dst.id\")) match {\n      case (Some(label), Some(logical), Some(concrete)) =>\n        val timestamp = ...\n        clientAddrs.flatMap { ca =>\n          serverAddrs.map { sa =>\n            RequestMetadata(...)\n          }\n        }\n      case _ => Nil  \n    }\n  case _ => Nil\n}\nAm i correct that this is equivalent? If so, this seems to be preferable, since the top-level loop should do a lot less work?. Eh, you want /http/1.1 if you're routing on host.... correct, will update comment. IPv6 isn't going to work... host:port notation for ipv6 is e.g. [2001:db8:cafe::17]:8080 and braces are not legal in Paths...\n. i did mean to move this here (into the io.buoyant package). :+1:. Would you mind adding a test like \"/mysvc/subsvc/path1?foo=bar/\" -> \"/path1?foo=bar/\"?. eh? .replaceAll(\"[^A-Za-z0-9/]\", \"_\") replaces slashes with _ and then we replace slashes again.  Should replaceAll come after the first replaces?. Can we test name rewriting?. doh gotcha. nit: weird indentation. Do all Config types actually have a kind?  clients and servers, for instance, do not.  Perhaps call this PolymorphicConfig or something that indicates when it needs to be used?  It should have a comment that clarifies what it's for, what needs to extend it, etc.. it needs to read params so it has to implement make itself. newStack is used in finagle...H2.Client(), newStackWithoutTlsClientPrep is used in router.H2.clientStack. Note: I moved TlsEnabled to TransportSecurity, so this can later include tls protocol & cipher configuration.\nI moved to wrapper types (rather than making the param an ADT) because it simplifies type inference for params (also this tends to be what we see in finagle). Does this even need string interpolation?  Isn't this just \"\"\"^.*/srv/.*/requests$\"\"\".r?. Is there anything specific we're worried about exposing?. also typical to do:\nscala\nval LinkerConfig(config) = params[LinkerConfig]\ntioli: Also, fwiw, we typically don't have StackParam in param names. When we want to clarify this, we usually do so by sticking it in a param package/object so it's referenced like param.LinkerConfig. Yeah, you're right.  I think I'm leaning towards making these explicitly configured with prefixes, since I find the pfx-namers to be a bit awkward -- though these unconfigured namers are certainly more flexible.  Thoughts?. I sort of feel that we should require explicit prefixes for this namer?  Since it seems awkward to ever have /#/io.l5d.rewrite be meaningful.... Should we mark this as experimental? (could go either way). this is an extremely annoying emacsism, i'll try to fix. that should be rs (RecvState used to be called RecvBuffer). Follow-up change includes a RecvState.Reset. Yep... If you have any ideas about how to make this clearer, I'm all ears. This was the least-awful thing I could come up with.. AIUI, the main difference with slice is that it resets the start of the buffer so that bb.position == 0 and that it sets the capacity to the limit of the buffer.  I don't think buys us much but I'll take a look.. There may be multiple messages in the frame, yes.  We could introduce a new RecvState that can hold additional fully-parsed messages, but I think it simpler to just read the next header so that on the next call to recv(), the next message will be read and returned.. we could also not even read the next header and just update the state to RecvState.Buffer(None, ...). ACTUALLY this is a scalariformism so we're stuck with it for now. This should be changed.  Instead of blocking the final message's release future on prior futures, it should complete immediately -- but the last message to be released of a given frame should cause the flush.... These steps don't work as-is.  I think you don't want to cd into interop-testing?. Shouldn't maxEffort be exposed as a config option?. This only reads a single frame from the stream.  I think this should be something like:\nscala\n      def fullDuplexCall(reqs: Stream[StreamingOutputCallRequest]): Stream[StreamingOutputCallResponse] = {\n        val rsps = Stream[StreamingOutputCallResponse]\n        def process(): Future[Unit] = reqs.recv().transform {\n          case Throw(Stream.Closed) => rsps.close()\n          case Throw(e) => Future.exception(e)\n          case Return(req) =>\n            req.value.payload.flatMap(_.body) match {\n              case None => rsps.close()\n              case Some(body) =>\n                val chars = Array.fill(body.length) { 0.toByte } // {} specializes us to a Byte array\n                val toSend = Buf.ByteArray.Owned(chars)\n                val msg = StreamingOutputCallResponse(Some(Payload(None, Some(toSend))))\n                rsps.send(msg).before(process())\n            }\n        }\n        process()\n        rsps\n      }. similarly, this doesn't chain the writes (and so ignores backpressure) and doesn't ever close the response stream.  I think the following is correct:\n```scala\n      def streamingOutputCall(req: StreamingOutputCallRequest): Stream[StreamingOutputCallResponse] = {\n        val rsps = StreamStreamingOutputCallResponse\n    def process(params: Seq[ResponseParameters]): Future[Unit] = params match {\n      case Seq(param, tail@_*) =>\n        val size = param.size.getOrElse(0)\n        val chars = Array.fill(size) { 0.toByte } // {} specializes us to a Byte array\n        val toSend = Buf.ByteArray.Owned(chars)\n        val msg = StreamingOutputCallResponse(Some(Payload(None, Some(toSend))))\n        rsps.send(msg).before(process(tail))\n\n      case _ => rsps.close()\n    }\n\n    process(req.responseparameters)\n    rsps\n  }\n\n``. This won't work: we read from the stream (and updatesize`) asynchronously, but return a response immediately.  Need something like:\nscala\n      def streamingInputCall(reqs: Stream[StreamingInputCallRequest]): Future[StreamingInputCallResponse] = {\n        def process(processed: Int): Future[Int] = reqs.recv().transform {\n          case Throw(Stream.Closed) => Future.value(processed)\n          case Throw(e) => Future.exception(e)\n          case Return(req) =>\n            val sz = req.value.payload.flatMap(_.body).map(_.length).getOrElse(0)\n            process(processed + sz)\n        }\n        process(0).map { sz => StreamingInputCallResponse(Some(sz)) }\n      }. this looks like a codegen bug -- should be req.responseParameters. I don't think that's right. (Should document this):\nLet's say that we have an h2 frame containing two messages. If we parse and release the first message before we parse the second message, we should not call underlying.  We can only call underlying after we have created the releasable for all segments in the frame.\nFurthermore, these segments may be released out of order.  release() of the second message should not be blocked on release of the first message.  Instead, release of the second message should happen instantaneously and release of the first message should release the underlying frame.. If you look at the issue, I suggested this.  useNodeAddress is undesirable, since it always may use the node address.  What this flag does is disables use of the service address. That said, open to other names.. \ud83d\udc4d . unused import. is this still necessary?. let's please constify this like CommonMetricsConfig.kind so we're using one string everywhere.\nAlso I suppose this means that linkerd-core has to depend on telemeter-commonMetrics, which I don't particularly like, though this is not a hard blocker.\nlinkerd-core should only have to depend on telemeter-core. I'd prefer that dependencies on any implementation in from as high in the dependency chain as possible (probably linkerd-main?). but if you promise me this is being removed soon, i may be swayed.. Does this pull in common-metrics transitively? Can we safely exclude it?. tioli: I would probably opt to do this procedurally with a mutable.Buffer[(String, Metric)] to reduce garbage generated from hitting metrics.json (these things end up being megabytes in production).. style nit: this is awkard to read for me (indentation doesn't make control flow obvious)\npreferred:\nscala\nval flattened =\n  if (pretty) ...\n  else ...\nor\nscala\nval flattened =\n  if (pretty) {\n    ...\n  } else {\n    ...\n  }\n. i'd opt to just collapse these cases like:\nscala\n case (name, c: Counter) =>\n case (name, g: Gauge) =>. I'm not really a fan of the name io.l5d.admin -- \"the admin telemeter\" doesn't really tell me much about what it is or does.  Is the intent here \"the thing that exports stats on the admin page?\"  perhaps ...adminMetricsExport?. I'm not clear on this.  We always add CommonMetricsConfig?  This doesn't use MetricsTree, though, right? Are jvm stats, etc exported to both common metrics and metrics tree?  What happens if both this and AdminTelemeter are configured?. Please indicate what identifier is used by default now. That was not easy for me to determine (i get it now, io.l5d.header?). Default value should be {kind: io.l5d.header}. tioli: header key already constified in finagle. Releasing needs happen when the memory associated the returned value can be freed.  We're going to need to rely on this to get off of direct pools and onto buffered pools.  Furthermore, it allows for much more explicit provisioning -- if I say that I'll allow M connections with N streams each, each with a window size of W, then I'm able to estimate memory provisioning by MNW.  If an application updates the window immediately, they are effectively operating with a window of 2W.. server only. server only. this is a bugfix right? not worth mentioning in changes, but worth mentioning in commit. What's the thought on making this extend Admin.Handler directly?  It might be nicer to have a scoped member val handler: Admin.Handler = new Service[...] { ... }. wait... how do these tests pass?  I thought we changed the kind name?. I suppose these should be named like initialWindowBytes?. just an example, no preference (though this fails now, anyway). we don't advertise a setting, and so the default (unlimited) behavior is used\n```\n SETTINGS_MAX_CONCURRENT_STREAMS (0x3):  Indicates the maximum number\n      of concurrent streams that the sender will allow.  This limit is\n      directional: it applies to the number of streams that the sender\n      permits the receiver to create.  Initially, there is no limit to\n      this value.  It is recommended that this value be no smaller than\n      100, so as to not unnecessarily limit parallelism.\n  A value of 0 for SETTINGS_MAX_CONCURRENT_STREAMS SHOULD NOT be\n  treated as special by endpoints.  A zero value does prevent the\n  creation of new streams; however, this can also happen for any\n  limit that is exhausted with active streams.  Servers SHOULD only\n  set a zero value for short durations; if a server does not wish to\n  accept requests, closing the connection is more appropriate.\n\n```. correct. it's tail recursive-ish. (this is a common idiom in finagle)\nIt can't actually be marked tail-recursive because we're doing release().before(read(rest)) (which is equiv to release().flatMap(_ => read(rest))) -- since the recursive call is in the closure, the compiler can't know that it's in the tail position. Futures collapse chaining properly, though, and so this is all fine.. just noting that our test client is not yet able to enforce deadlines on streams (needs API changes). Our test client doesn't test unimplemented_*. I don't love adding these to the public interface for just the one use.... specifically, i don't like encouraging an allocation just for the sake of convenience. I'd rather callers make that decision explicitly.. I think we need to look at req.contentType here.... well, the PF should def be val'd... i meant the option --  usually this is just an intermediary allocation like\nt.metric match {\n  case c: Metric.Counter =>\n    // do the thing\n  case _ =>\n    // don't do it\n}\nBy providing counter, we encourage callers to do a needless option-allocation.  it's not a huge deal, i just tend to prefer public APIs that are minimal and don't encourage undesirable patterns. This idiom is borrowed from finagle (which does this all over the place). I'm fine with ClearContext.Enabled, though.. It is a little redundant, but I don't think it's harmful.  I think it's right for the ClearConext module to exhibit the behavior it does of checking whether it's enabled. And I think it's right that we only apply the protocol-specific transformation when ClearContext is enabled.  What may be confusing is using the same Param in both places... but I don't really think it's worth having an additional Param type for the protocol-specific setting.. Yeah, I as thinking about that... As much as I was trying to avoid iterating through headers, I think it's probably preferable to do it (additionally, after the targeted context clears have been performed). should be Sample.ClearServerFilter. This module is installed at the bottom of the server stack (i.e. after all trace initialization is preserved).  From the tracing point of view, it will appear as if this request originates from linkerd.. I suppose, if we want to maintain tracing from the server, we could simply not install this module for Http.  Then, we'd clear headers and let linkerd do what it wants with context... This module still seems like a good default behavior for other protocols. good catch. I think this should be if (closed) releasePrior(). You may just want to use a GlobalFlag[Duration] -- this allows specification of i.e. \"60.seconds\" \"1.minute\" etc -- and there's a default Flag handler for Duration so it all Just Works.. Yeah, I'm trying to use clearer language (and definitely move away from opaque finagle terms like Name, Addr, Address).  Replicas explicitly indicates that the returned set of endpoints can be used interchangeably and that this API does not support other schemes (i.e. by surfacing shard IDs, etc).. \"Replica\" describes the relationship.  This thing is an Endpoint.  Endpoints may be grouped in shards, etc.  (Really Replicas could be called ReplicatedEndpointSet or something if we want to be more verbose).. I don't think the resolver API should be used to surface arbitrary configuration parameters. \nInstead, the path forward is to build additional service interfaces to satisfy these other configuration domains. And maybe it will be appropriate for some these interfaces to support arbitrary metadata, I just don't think it's appropriate to hang this all off of (only) concrete addresses.\nI would even entertain the idea that metadata should be removed, though I think that, for instance, nodeName on the endpoint is a good use of metadata in that it provides additional diagnostic information and, realistically, won't be used to alter runtime behavior.. i switched to message types everywhere.  enums were a premature optimization. i think it's better to use messages, so they may be extended in the future to support additional fields.. For now, I've removed Replicas.Meta entirely.  We still have Endpoints.Meta. i think ideally it may be preferable to move this timer out into wherever the MetricsTree is configured, but definitely not a blocker. How does this end up scoped?. what do you think about using the kind here?  (or is the kind not namespaced?) either way, maybe just reference the configId directly?. Can this exist in the telemetry section?. i know that these can be costly at runtime. maybe worth a small note that this type is only used during configuration and not in the serving path?. yeah; reflection is slow. i think this is ok.. should we support multiple prefixes with multiple q params?. does this test verify that dtab-local does NOT impact the l5d-dtab?  (i.e. we should check the l5d-dtab-ctx or something?). can the payloadSize and uncaughtExceptionsFilter live outside of this?  Fewer allocations/resources per connection (request?).... It's a good instinct, but finagle paths have a restricted character set. And when an unsupported character is in a path element, the entire path element is turned into escaped hex codes! So sometimes Real World paths get really bad really quick.. Well, I think it prevents any of the configured http2 settings from being sent during connection establishment.  We're going against the grain in netty (as you'll see at the top of this file}, so we probably need to do some poking to figure out if this feature has been removed from netty entiirely.  I don't think it's a hard blocker, but we may need to remove a bunch of the settings from the config.. Actually, I may be wrong about that. Would be best to setup a simple test -- i usually run nghttpd --no-tls and have linkerd connect to it.  nghttpd will dump all of the settings received, which will be helpful.  The nghttp client can be used to initiate an h2 request through linkerd, and will dump any settings received.. copied from the thrift named interpreter config.  We should unify all of these.. Yep, that should definitely be added.. it would be Nice to check the sha of the binary (like we do for sbtlaunch) to check i.e. if the urlc hangges.. prefer ${protocversion} to make it clearer.  Also we should probably quote this Just In Case.. probably should quote protocbin. probably should quote $@. my only concern here is that we seem to be calling toLowerCase a lot, which makes me worried that it's easy to miss this in one place..  Is there any way we can do this somewhere once than throughout the code?  For instance, can we just alter the Path once at the start?. tioli: i like consts. imports should be sorted alphabetically with no spaces. this test should still be relevant, no?\nWhen a client connects to linkerd with HTTP/1.0 and no Connection header, we can't reuse that connection... right?. as a single commit.. this whole file could use some comments.  why are checksums based on these two files?  LinkerdBuild rarely changes.... unfortunately, LinkerdBuild isn't really a good proxy for dependency changes. All of the files in files in project/ basically need to be accounted for...  Some deps are stored in Deps... Some are in Grpc... Some things are in Base.  New linkerd subprojects will be in LInkerdBuild.... this doesn't need to extend Svc, since DefaultSvc extends Svc\n[edit] on second thought, i think this should extend Svc and DefaultSvc should only require it as trait DefaultSvc { _: Svc => ... }. should this extend Svc or perhaps trait StaticSvc { _: Svc =>?  Basically, would be nice for DefaultSvc and StaticSvc to be somewhat uniform in this regard. guaranteed with two a's. nit: i'd put service above clients because it's clearer from the sense servers -> service -> client. nit: i'd put service before client to reflect request flow..\nhow do we reconcile that servers is plural but client and service are singular?  I guess because with the default global module, there is only one?. how does this relate to requeues?. indentation looks off here, but might just be github. note re other client isn't necessary here since it's mentioned above. technically should be @volatile since accessed across threads. technically should be @volatile since accessed across threads. nit: let it breathe i % 2 == 0. i think this makes it clear that DefaultSvc should be trait DefaultSvc { _: Svc => ... }. nit: ordering. \"any\" or \"all\"? i think we mean all.... what does local refer to here?  that it's using the context?. what's this Map[String, String] all about?  perhaps a typedef would make this clearer?. please reuse the const. reuse const. I think we need to explain how this setting influences server and client modules.. i'm still stunned that writeFrame(f).before { doesn't work.... Doesn't this initiate two request loops?  i.e. initialCall is a task that will continue to submit updates every ~minute?\nShould the first one just be Future.sleep(jitter(1.minute).before(client(metrics)) ?\n. might as well append admin/src/main/resources/io/buoyant/admin/package.json into this as well.  also i might be inclined to put this in /tmp. and then this can be simplified. i don't think this is relevant any longer since we now use ./protoc (which is what's actually called from within sbt).  And, instead, we have to cache .protoc. hm, ok. fair.. meaning that this timeout spans requeues... ok not worth documenting just confirming my understanding. indent looks weird here:\nyaml\n  - prefix: /svc/foo\n      totalTimeoutMs: 200ms. probably worth a quick comment about what this is intended for. woa. prefer Seq.empty. prefer Seq.empty. So I understand... what does assert return now?. so I understand, why is this necessary?. maybe s/;/ orElse/?. Oh wait. Should this be requeues?  Do we depend on the retry behavior?. perhaps a comment that this removes Requeues and that the retries filter still applies? (It does, right?). i'd argue that this should be called idleTtl or something.  it doesn't limit the lifetime of the factory unless it's idle.. likewise, IdleTtl?. the prior format is used so that we print stack traces when they are enabled (this can be controlled at start time via a system property).. why is this necessary?  We prefer to use val'd functions when possible, since they save on unnecessary anonymous function allocations.. the prior form is used to be explicit that this allocates.  We use val'd functions and the paren-less form to indicate that there is no allocation performed, and we use parens to illustrate that an anonymous function is allocated (i.e. when we call a function rather than pass a val'd function).. Is this change related to the bugfix in some way or is this just a refactor?. Is it worth adding a comment here explaining how state.close is expected to be called?. Doesn't this make the match non-exhaustive?  Should we remove the StreamRemoteClosed state?. whoops, missed that. thanks. On second thought, is this safe?\nt0: closedId.get = 1\nt1: closedId.get = 1\nt1: max = 4\nt0: max = 2\nt0: cas(2, 1): true\nt1: cas(4, 1): false\nt2: closedId.get = 2\n...\nMaybe we don't actually care in this case, but it seems possible to miscalculate the max id. yes, that's right. carry on.. what does @silent do?. Is there any way for us to print some deprecation warning when legacy keys are used?. Seems like something we want to do for migration, but not a blocker.. any reason you prefer filter/map over collect? i find collect to be clearer (and i think it's slightly less costly).. Maybe spell this out a little more...  At the least, mention TLS.. Ideally, this should be generalized in linkerd-core or somewhere this module can be applied to other protocols (http for instance). What does this actually mean?  Perhaps a clearer comment like // TLS is disabled.\nBut I'm slightly mystified as to how ClientSsl.e.isDefined means that tls should be disabled... Are you sure that this shouldn't be isEmpty?. oh duh, i got confused with the matching blocks below... got it now. This seems like it might be a mistake?. Is it intended for transform to clear all other headers?  AIUI, the copied request would have only these 4 headers.. no newlines between imports, please.. this would probably benefit from an explanatory comment.  Specifically, what does it mean for it to be None? (should it be defaulted?)  Is it sufficient to make this a simple boolean that is defaulted to false?. not a huge fan of this abbreviation. Maybe just 'client'?. My gut feeling is that the conflict on \"client\" isn't really that onerous -- on one hand we're scoped under rt/_/... on the other, we're scoped under namer/...\nI don't feel that strongly, though. Generally, we want the output of ./protoc to actually be the output of protoc and not to include random debugging information.  Specifically, we may want to run something like ./protoc | foo where foo knows how to parse protoc output.  These sorts of log lines would interfere with that parser.\nPractically, this probably isn't an issue for protoc.  But, it's best practice to not have wrappers interfere with stdout.. I'd note the origin of this file and how it's differentiated.  If we have a finagle/util PR, note that here as well (i.e. \"Can be removed after #NNNN\"). you can avoid a function allocation here by valing up the body.\nFor example:\n```scala\ndef bucketAndCounts: Seq[BucketAndCount] = counts.synchronized {\n  counts.zipWithIndex.collect(bucketAndCount)\n}\nprivate[this] val mkBucketAndCount: PartialFunction[(Int, Int), BucketAndCOunt] = {\n  ...\n}\n```. nit: i'd be inclined to write this as\nscala\nval (index, valueToAdd) =\n  if (value >= Int.MaxValue) (countsLength - 1, Int.MaxValue)\n  else {\n    ...\n    (index, value)\n  }. eh, this is probably only called in metrics reporting so not high-priority. we tend to prefer dots and parens over spaces... but this is probably fine in this contextx. @zackangelo unsure. that's a very good question. could avoid doing getAddress twice with:\nscala\ncase FAddress.Inet(isa, meta) =>\n  Option(isa.getAddress).map { a => Address(a.getHostAddress, isa.getPort, meta) }. should these commented sections be removed?  or uncommented?. This is a bit of a red flag to me:  this stack param contains, effectively, a stack module.  (This isn't really what we want to use params for, if we can avoid it).\nIt would be more natural use of these APIs to just mutate the stack with the proper filters...\nI think this would work by modifying HttpConfig to override def router(params: Stack.Params): Router  to insert the stack module for each configured logger.  I think this would end up looking very similar to your current solution, and you wouldn't need the extra HttpLoggerConfig params/module -- just a plugin type that exposes a Stack.Module[ServiceFactory[Request, Response]]. doesnt' this need a @JsonIgnore?. I think these errors should probably count against failure accrual...  We'd want a load balancer to evict nodes sending these sorts of malformed responses.. I think we should update our classifiers to be aware of this type of error.\nHow about we do this:\n- Introduce a ProtocolError abstract type (in a protocol-agnostic module);\n- Update existing classifiers to treat protocol errors like 5xx;\n- Make the malformed framing filters throw a type that implements ProtocolError.\nI think it's correct to treat this as an exception that should be handled in an error handler: in the router's stack, a 5xx from the remote service should be distinct from linkerd deciding it won't handle the response.. i think, technically, we should be calling getCause on the exception to unwrap it (until getCause returns null).  Finagle does this some places already. i'd check rc before destructuring...\nscala\nclassifier(rr) match {\n  ResponseClass.NonRetryableFailure =>\n    rr match {\n      case ReqRep(req, Return(rsp)) if Headers.Retryable.get(rsp.headerMap) => ResponseClass.RetryableFailure\n      case _=> rc\n    }\n    case _ => rc\n}\nthis should be a bit less work in the common case. yeah, i'd probably write(..).before(close(deadline))\n. ideally, we would log on unexpected errors.  i'm not immediately sure if it's possible to get non-connection-related errors here, though.  Perhaps a good compromise is to at least log the exception's message?. i feel pretty good about making the change throughout linkerd.  (our stack traces are basically useless). if we want this to do that, it should be called something else. Why do we need an implicit?  Can't we just add onFrame to the type?  We own it. O_o is this necessary?. i don't think we  should bother with a histogram of frame counts/sizes.  maybe a global counter if we really want.  this will be a really wasteful/costly histogram in the vast majority of cases.. I do think we should track a histogram of total stream sizes (not individual frames). might be slightly clearer naming stream0 underlying or inner or something. . tioli:\ni tend to constify strings -- with, somewhere on an object:\nscala\n   private[pkgname] val LabelSelector = \"labelSelector\"\nonly because typos become compile errors.\nwe'd want to use these static keys throughout. Minor note, I prefer when comments end in punctuation so it's clear that it wasn't half a sentence.  I write lots of incomplete comments, so using proper capitalization and punctuation help me quickly get a sense for how complete my comments are.. technically, according to the h2 spec, the window size may change.  it may be worth modeling capacity as a Var.  but i wouldn't make this a blocker.  Also, i'd default this to 65535 bytes to match the rfc default window size. nit, prefer periods at the end of comments to indicate it's a complete thought. isn't this really onEnd?  read() doesn't return when it's fully buffered.... typo. scala golf: val-up bodies to eliminate needless alloc ;)\nscala\nprivate[this] val _toRsp = { case (rsp, _) => rsp }\ndef responseOnly = response.map(_toRsp)\n(and everywhere we can do this, we should.)  keep in mind that classifers will run multiple times for each req/rep, so we should be as careful as possible wrt allocation. nit: definitely prefer either all on one line or braces. fwiw, for the purposes of HTTP/2, only data frames are counted against flow control.  so this probably doesn't have to be todone. unclear to me if this is tested elsewhere, but should you verify that buffer capacity is reclaimed as frames are released?. tioli: perhaps worth (lazy?) valing these, since they'll be called multiple times per request. we should consider (lazy?) valing these up, since this will be run multiple times per request. we should probably have a default. we should probably have a better timeout (maybe enforced in config though). this should have a default -- ALSO we should ensure that it fits in the timer granularity ( i think has to be greater than 10ms?). This should really be the same value of the window size configurations (i.e. we can buffer up to an h2 server's window size if a request stream, and up to an h2 client's window size for the response stream.\nIf we can't enforce this in code, it should be clearly documented. . Why does this classifier need to be computed on each request?  Couldn't this be computed once on the parent class?. slight pref for capturing a temp val here -- the body is large enough that i've lost the context of what this failure pertains to.\nscala\nval factory = underlying(conn).map {\n  ???\n}\nfactory.onFailure(???). ah right.  i'd add a comment there because apparently \"Ctx.current\" wasn't enough of a clue for me...  just something like \"This must be computed on each request to get the the classifier from the request-local context\" etc. as for this, i'd just track a counter (rather than a distribution of idle times or something fancy like that). it'd be nice to have a comment here like \"A replacement for finagle's FailureAccrualFactory that ...\". wording nit.  clearer to me as:\n\"_ now serves on 127.0.0.1 by default (instead of 0.0.0.0).\"\n. nit: prefer braces or no newlines\neither\nscala\n  if (retryIndefinitely) infiniteRetryFilter\n  else Filter.identity[http.Request, http.Response]\nor\nscala\n  if (retryIndefinitely) {\n    infiniteRetryFilter\n  } else {\n    Filter.identity[http.Request, http.Response]\n  }. this comment probably needs updating, since the signature has changed significantly. this would be easier to read with temp vals\nscala\nval w = maybeObj.toSeq.flatMap { obj => Seq(od.toWatch(obj)) }\nval v = for {\n  obj <- maybeObj\n  meta <- obj.metadata\n  version <- meta.resourceVersion\n} yield version\n(w, v). tioli; but this may benefit from an explicit type annotation (very hard to know what the signature of this is by sight)\nscala\n private[this] val serviceEndpoints: String => Option[String =\n  ...\n(or whatever it is). I imagine that resolver.send is a potentially blocking call?  If so, then I think DnsSrvNamer should probably be constructed with a com.twitter.util.FuturePool so that these calls don't block the timer thread.\nFurthermore, it may be a good idea to record a stat around the lookup times.. just to confirm i understand this... timer.schedule returns a closable and the timer is canceled when the Var observation is released?. nit: prefer to break this very long line into a line per argument. probably better to measure in milliseconds?. as mentioned below, this should probably be measured in millis (and have suffix _ms like other finagle metrics). rather, so you have a static instance on which you can reference its members without allocating a new initializer instance. nit: i suspect these comment lines are a big longer than the usual 100 char limit we like to apply.\nWould you mind wrapping comments at 100 chars here & throughout?  Thanks!. nit: probably most idiomatic to call output state. i don't think we need to be that fancy, however, it's probably better to say something like \"Invalid dtab namespaces $ns: must contain only DNS-safe characters\" -- or something.  We should ideally indicate what set of characters are allowed.. We should definitely build the regex statically so that it doesn't need to be compiled on every invocation of this method.  Regex compilation is much slower than evaluating a compiled regex.\n```scala\nval validNs = \"[\\w+\\-?/?]+\".r\ndef namespaceIsValid(ns: Ns): Boolean = validNs.matches(ns)\n```\nOr something like that\n. Also, this regex doesn't make it very clear what character set we're supporting.\nCan this just be ^[A-Za-z0-9_-]+$, for instance?. probably should be private[this] and/or moved into companion object. nit: prefer braces on multi-line conditional statements.\ni.e.\n```\n// ok\nif (true) foo else bar\n// ok\nif (true) foo.withSomethingLongish()\nelse bar\n// ok\nif (true) {\n  foo\n} else {\n  bar.spanning(\n    mutliple,\n    lines\n  )\n}\n``. here & throughout. are we providing the client with a properly-scoped stats receiver? i assume so?. is HighResTimer required? is this just the default recommended thing? (used to seeing DefaultTimer.twitter or whatever it was). can we limit this to entries where _entry.addr_ is aNameorPathso that we don't encode weird things for clients that don't do name resolution?. Whyva.changes.toFuture()and notva.sample()?. wait what is the type ofstate`?  This seems a little annoying to decode if the type is \"a list or maybe a string\".\nPerhaps better to have a case class that encodes e.g. \"state\": \"bound\", \"addrs\": [] etc (i'd bet that we even have this somewhere already for namerd's http api)\n  . Are we confident that LoadBalancerFactory.Dest's Var is reference-counted?  Do we need to ensure this higher up in the stack when we get the result from an interpreter?. maybe warn instead of error? let's try to reserve error for things that we know are terrible. this could be something transient if i'm not mistaken.. nit: i tend to think of sample() as side-effecty, and so I use parens.. Note that we use FunSuite throughout this repo. I have a minor preference to keep it consistent, but it's in no way a blocker ;). tioli, but elsewhere we run apt-get install -y --no-install-recommends so that it does less work. not sure if that helps here or not. consider including q.fail(e, discard = false) in here -- since this is never called without also failing immediately prior afaict.  then can be renamed to failFrameQueue. similarly, slightly clearer to make this failQueue and have it do the fail(). we should probably do the same thing in consul, if we don't already?. is FlagBalancerFactory actually used? does it just need to be in scope or something?. would you mind briefly describing the problem/workaround.\n/**\n * finagle-thrift-18.04 incorrectly exports symbols from libthrift-0.10.0.\n * In order to build assembly packages, we chose an arbitrary copy of these files\n * on the assumption that they are identical.\n *\n * This can be safely removed once twitter/finagle#688 is resolved.\n */\n... or something. this type doesn't actually implement Var... perhaps say it holds a Var.async?. Do you think it's worth writing unit tests for InstrumenedVar?. tioli braces might make this a little clearer i.e. ${ns}/${svc}${labelStr}. Can this fail?  Does cancelling this future do anything?\nIf \"no\" to both of these, consider using an interface like def onCancel(f: Reset => Unit) and then have impls like override def onCancel(f: Reset => Unit) { cancelP.onSuccess(f) }.\nThis would have the benefit of hiding some of the complicated Future semantics from this part of the interface if they're not necessary.. I'm not clear enough on the semantics of fork() to know that this is safe...  This means that if any reader cancels then all other readers will also be reset? How does this interact with retries?  Does this need to refcount underlying?. Otoh, if we'd ever want to join/select over onCancels, the callback approach would be suboptimal.  worth asking the question though. space. this might be considered a ProtocolError?. On the other hand, consider that resets can be used to signal things like rate limiting.\nI think this behavior is Okay for now, but this may warrant some more thought in general.. i imagine we don't actually need to set the onCancel handler when sendState is SendClosed? (because this basically means we're in full-close///. here (and throughout), might be helpful to call this recvStream -- it can be a bit confusing which stream we're talking about at times.. It would be good, as a followup, to investigate which caching techniques are friendliest to the jvm (i.e. Map, mutable.Map, CHM, etc). Is it possible for dtab to be null? if not, it would be clearer as Some(...). I would expect the header be named l5d-max-forwards to be consistent with other linkerd-specific headers. Is the max-forwards header standardized more widely?. Excellent! thank you. it might be helpful to include that RFC link at the bottom of the docstring.. I would expect an HTTP-specifc module like this to live in an HTTP-specific subproject (and not core). Can this be moved? If not, why not? How will we offer similar support for H2?. spacing \"that  includes\"\nwording: \"This follow up release includes\".   I'd drop the line after it about not making 1.4.2 (not useful in the changelog). should this be val'd? or lazy val'd?. this indentation is a bit hard to read... i'd probably indent the && lines additionally. Does this need to be validated? It seems that we only should really support values on ~ [1-64]. If I understand correctly, this change means that all key/vals in labels0 now skip escapeLabelKey and escapeLabelVal. Do we need to take another pass over labels0 to do escaping?. Presumably, we just hand the request right back to the sampler. Can this API be simplified?. Why do we need to pass a request to the interface? so that it can modify headers? should the method name be updated to indicate it's usually side-effecty?. why does this need to be a mutable var at all?. worth including that link for next time we have to read this?. does this actually need to be volatile? it appears that all access is synchronized around mu. This sample will open and immediately close an unstable observation.  Then, below, we immediately open the observation up again. Is it possible to avoid redundant state transitions?. tioli: exists.getOrElse(false) and !exists.getOrElse(true) are a bit easier to read than e.g. exists != Some(false). ah but that's not the same thing, because the None type.  Consider using a dedicated marker enum with three stats rather than Option[Boolean]. It may be, but feel free to disregard if it's not. \"fixes some memory related issues\" sounds scary. Can we be clearer? \"reduces allocation\" or \"improves GC behavior\" or something to that effect. Simpler: Finally, this release adds a new gRPC response classifier that may be configured with gRPC status codes.. What versions of Java? I'd either take this sentence out or be more specific.. Consider renaming the method to sendPing(), which may help to clarify this.. My understanding is that failure detection is interacts with the load balancer & probation, so that \"failed\" nodes are not considered for new requests.\nThese pings can cause the operating system to notify the process that a closed connection was actually closed -- i.e. if writing to the socket fails. (Otherwise, we may not try to write to the socket and wouldn't have cause to be notified of the disconnect).\nSIGSTOPing a process for just a short time probably wouldn't be enough to cause the OS to tell the process the connection has closed. It should be sufficient to see the load balancer put the endpoint in probation; though this would only impact the LB, really, if there are more than one endpoint to choose from.. ",
    "siggy": ":star: rebase'd shipit\n. lgtm :+1: :star: \n. This is the best way I know to test sbt changes... :+1: :star: \n[info] All tests passed.\n[success] Total time: 34 s, completed Jan 14, 2016 5:42:10 PM\n. works on my machine :+1: :star: \n. lgtm other than ci failures :+1: :star: \n. Mind updating https://github.com/BuoyantIO/linkerd/blob/master/docs/config.md ?\n. Thanks! :star: :+1: \n. With this change, no 'servers' appear, only clients:\nver/fix-tracer-propagation:\n$ curl -s http://localhost:9990/admin/metrics.json|jq ' '|grep srv|wc -l\n       0\n$ curl -s http://localhost:9990/admin/metrics.json|jq ' '|grep clnt|wc -l\n     193\nmaster:\n$ curl -s http://localhost:9990/admin/metrics.json|jq ' '|grep srv|wc -l\n     177\n$ curl -s http://localhost:9990/admin/metrics.json|jq ' '|grep clnt|wc -l\n      16\n. ./sbt experimental:bundle produces both stable and experimental jars. Intentional?\nAlso a more meta-question re: stable + experimental... yagni?\n. lgtm :star: :+1: \nThat info in CONTRIBUTING.md is super helpful. At some point I'd like to see ./sbt examples/http:run or similar more prominently documented, ala git clone ... && ./sbt ..  && open ...\n. This is awesome! My comments are not blockers. :+1: :star: \nWe'll want a Contributor License Agreement process set up before we start accepting PR's. Not sure you'll want to address it in this review, but either way here are a couple examples:\nhttps://github.com/clahub/clahub\nhttps://www.hellosign.com/api/githubExample\n. :star: :+1: thanks !\n. This sounds good. I think we should complete https://github.com/BuoyantIO/linkerd/issues/16 prior to this work. Specifically, booting our own admin server, and serving our own metrics.json endpoint.\n. :+1: \n. Note to all: test locally.\n. :star: :+1: \n. :+1: :star: \n. i trust your rebase :star: \n. lgtm modulo other comments :star: :+1: \n. mind updating config.md?\n. :star: rebase\n. Thanks for the tioli's! :star: :+1: \nNot sure if this is your branch or unrelated, but the ip field is more than an ip, and sometimes just has a leading /.\n[\n  {\n    \"label\": \"http\",\n    \"protocol\": \"http\",\n    \"servers\": [\n      {\n        \"ip\": \"localhost/127.0.0.1\",\n        \"port\": 4140\n      }\n    ]\n  },\n  {\n    \"label\": \"ext\",\n    \"protocol\": \"http\",\n    \"servers\": [\n      {\n        \"ip\": \"/0.0.0.0\",\n        \"port\": 8080\n      }\n    ]\n  },\n  {\n    \"label\": \"thrift\",\n    \"protocol\": \"thrift\",\n    \"servers\": [\n      {\n        \"ip\": \"localhost/127.0.0.1\",\n        \"port\": 4141\n      }\n    ]\n  }\n]\n[\n  {\n    \"label\": \"http\",\n    \"protocol\": \"http\",\n    \"servers\": [\n      {\n        \"ip\": \"/0.0.0.0\",\n        \"port\": 4140\n      }\n    ]\n  },\n  {\n    \"label\": \"ext\",\n    \"protocol\": \"http\",\n    \"servers\": [\n      {\n        \"ip\": \"/0.0.0.0\",\n        \"port\": 8080\n      }\n    ]\n  }\n]\n. lgtm! :star: :+1: \n. With the nav breakage, probably best to remove the /adminlink from our own nav in AdminHandler.scala.\n. lgtm, thanks! :+1: :star: \n. Overall this looks great. TIOLI ui nit: there's an unclickable gap between the menu elements, feels weird:\n\n. thanks! :+1: :star: \n. super cool! :+1: :star: \n. mind documenting ./sbt integration:test ?\n. :+1: :star: \n. Following a demo, I am cool with the nav being on the left or centered, as long as local.css remains a copy from linkerd.io. :star: :+1: \n. :star: \n. :star:\n. :star: :+1: \n. works on my machine :+1: :star: \n. my comment was tioli, otherwise lgtm :star: :+1: \n. This is really slick. I think this is good to ship as-is. My comments below are more for down the road. :star: :+1: \nI'd like to see us move more docs and default usage in this direction. For developers, running a single compile command, and then having an executable is great. Perhaps have the exec file runnable via a more deterministic command.\nI think eventually we'll want to make the executable less noisy. For now it's probably nice to have extra info, but for example, when I don't specify a config, I get:\n$ ./linkerd/target/scala-2.11/linkerd-0.0.10-SNAPSHOT-exec\nPicked up JAVA_TOOL_OPTIONS: -Dfile.encoding=utf8\n-XX:InitialHeapSize=268435456 -XX:MaxHeapSize=4294967296 -XX:+PrintCommandLineFlags -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseParallelGC\nFeb 01, 2016 10:06:34 AM com.twitter.finagle.http.HttpMuxer$$anonfun$4 apply\nINFO: HttpMuxer[/admin/metrics.json] = com.twitter.finagle.stats.MetricsExporter(<function1>)\nFeb 01, 2016 10:06:34 AM com.twitter.finagle.http.HttpMuxer$$anonfun$4 apply\nINFO: HttpMuxer[/admin/per_host_metrics.json] = com.twitter.finagle.stats.HostMetricsExporter(<function1>)\nusage: linkerd path/to/config\n. Neat.\nMind documenting somewhere that tags must be in the form release-0.0.10?\n. :star: :+1: \n. :star: :+1: \n. :+1: :star: \n. awesome :+1: :star: \n. great change, my comments are advisory. :+1: :star: \n. The All isn't showing up in the menu.\n. lgtm :+1: :star:\nagree about delaying doc updates if things are in flux.\n. This issue no longer manifests following https://github.com/BuoyantIO/linkerd/pull/137, though an underlying bug may still exist.\n. Reported issue to finagle:\nhttps://github.com/twitter/finagle/issues/471\n. overall looks good, my comments were mostly cosmetic. :+1: :star: \n. hold off, working on tests now, will label it reviewable when ready.\n. :star: rebase\n. lgtm! :+1: :star: \n. :ship: :cry: \n. related: https://github.com/BuoyantIO/linkerd/pull/88\n. :star: :+1: \n. lgtm based on my limited understanding of tls. :+1: :star: \n. works for me! :star: :+1: \n. :star: rebase\n. :+1: :star: \n. We're making some changes to the marathon namer to more gracefully handle marathon availability changes, in https://github.com/BuoyantIO/linkerd/pull/164.\nThe marathon namer itself will continue to propagate java.net.ConnectException on first connection, if marathon is down.\nRepurposing this ticket to make the Delegator more explicitly handle these conditions:\n\n. :+1: :star: \n. This is resolved in upcoming the 0.2.0 release.\n. Fixed in https://github.com/linkerd/linkerd/pull/873. after i run ./sbt test, two files get reformatted:\n```\ndiff --git a/linkerd/core/src/main/scala/io/buoyant/linkerd/Linker.scala b/linkerd/core/src/main/scala/io/buoyant/linkerd/Linker.scala\nindex 5201805..76b35f2 100644\n--- a/linkerd/core/src/main/scala/io/buoyant/linkerd/Linker.scala\n+++ b/linkerd/core/src/main/scala/io/buoyant/linkerd/Linker.scala\n@@ -86,8 +86,6 @@ object Linker {\n           if (svrs.size > 1) throw ConflictingPorts(svrs(0).addr, svrs(1).addr)\n       }\n-\n   new Impl(routerImpls, interpreter.getOrElse(DefaultInterpreter), tracer, admin.getOrElse(Admin()))\n }\n\n}\ndiff --git a/linkerd/core/src/main/scala/io/buoyant/linkerd/TracerInitializer.scala b/linkerd/core/src/main/scala/io/buoyant/linkerd/TracerInitializer.scala\nindex 75e6f54..30de583 100644\n--- a/linkerd/core/src/main/scala/io/buoyant/linkerd/TracerInitializer.scala\n+++ b/linkerd/core/src/main/scala/io/buoyant/linkerd/TracerInitializer.scala\n@@ -9,8 +9,8 @@ import com.twitter.finagle.tracing.Tracer\n @JsonAutoDetect(fieldVisibility = Visibility.ANY)\n trait TracerConfig {\n   /\n-    * Construct a tracer.\n-    /\n+   * Construct a tracer.\n+   /\n   @JsonIgnore\n   def newTracer(): Tracer\n }\n```\n. :star: :+1: \n\n. thanks! :+1: :star: \n. :star: :+1: \n\n. lgtm :+1: :star: \n\n. lgtm :star: :+1: \ntioli: update config.md?\n\n. lgtm :star: :+1: \n\n. lgtm :+1: :star: \n\n. :star: :+1: coool\n$ curl http://localhost:4140\nUnknown namer: FOOio.buoyant.http.anyMethodPfx for path: /$/FOOio.buoyant.http.anyMethodPfx/host/GET/localhost:4140\n\n. :star: :+1: \n\n. :star: :+1: Looks awesome!\n\n. :star: rebase\n\n. :star:  :+1: awesome! \ntioli, mind adding this to marathon.l5d?\n\n. :star: :+1: \n\n. :star: :+1: just a couple questions but otherwise a great start!\n\n. :star: :+1: lgtm\n$ ./test-config.sh\n...\nSuccess!\n\n. On http://localhost:9990/delegator?router=http ...\n\n. :+1: :star: thanks!\n\n. :star: :+1: lgtm\n\n. :+1: :star: tests pass over here.\n\n. :star: :+1: \n\n. :+1: :star: \n\n. closing in favor of https://github.com/BuoyantIO/linkerd/pull/271\n. :+1: :ship: lgtm\n\n. :star: :+1: \n\n. mind adding a note in the README.md, maybe in the tests section, something like:\nRun integrated linkerd/namerd validation:\nbash\n./sbt assembly\n./sbt validator/run\n...also, for the above instructions to work, assembly should also build namerd, need a change like:\ndiff\ndiff --git a/project/LinkerdBuild.scala b/project/LinkerdBuild.scala\nindex b492a5a..2410cd8 100644\n--- a/project/LinkerdBuild.scala\n+++ b/project/LinkerdBuild.scala\n@@ -433,8 +433,12 @@ object LinkerdBuild extends Base {\n     .settings(unidocSettings)\n     .settings(\n       assembly <<= assembly in linkerd,\n+      assembly <<= assembly in namerd,\n       docker <<= docker in linkerd,\n+      docker <<= docker in namerd,\n       dockerBuildAndPush <<= dockerBuildAndPush in linkerd,\n-      dockerPush <<= dockerPush in linkerd\n+      dockerBuildAndPush <<= dockerBuildAndPush in namerd,\n+      dockerPush <<= dockerPush in linkerd,\n+      dockerPush <<= dockerPush in namerd\n     )\n }\n. Yeah, ZkDtabStore got configurability after this ticket was written:\nhttps://github.com/BuoyantIO/linkerd/commit/cb502a0f645135e265d670b94ba60dd680e2c1df\nI've updated the description accordingly.\nI think it's still worth considering for DcosBootstrap. I think we could make it a DCOS package config option on install. Would require changes in this repo, as well as in https://github.com/mesosphere/universe/blob/version-2.x/repo/packages/N/namerd/0/config.json.\n. :+1: :star: \n\n. :star: :+1: \n\n. :star: :+1: tioli update config.md\n\n. :star: :+1: \n\n. Similar behavior with marathon namer. Steps to repro:\n1. deploy linkerd + namerd + books-v2 to dcos\n2. successfully curl books-v2:\nbash\n   curl -H \"Host: books-v2\" http://$PUBLIC_URL/books.json\n3. remove books-v2 with:\nbash\n   dcos marathon app remove /books-v2\n4. curl books-v2, observe falling back to web with 404, as expected:\nbash\n   curl -H \"Host: books-v2\" http://$PUBLIC_URL/books.json\n5. redeploy books-v2 with:\nbash\n   dcos marathon app add books-v2.json\n6. curl -H \"Host: books-v2\" http://$PUBLIC_URL/books.json ... linkerd errors with:\nD 0414 22:22:18.750 THREAD26: ChannelStatsHandler caught an exception\n   java.net.ConnectException: Connection refused: /10.0.1.66:29574\n     at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n     at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n     at org.jboss.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:152)\n     at org.jboss.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:105)\n     at org.jboss.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:79)\n     at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)\n     at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)\n     at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n     at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n     at java.lang.Thread.run(Thread.java:745)\n7. observe namerd returns a newer, correct address:\n``` bash\n   $ curl $NAMERD_URL/api/1/bind/default/http/1.1/GET/books-v2\n   /io.l5d.marathon/books-v2\n$ curl $NAMERD_URL/api/1/addr/default/io.l5d.marathon/books-v2\n   Bound(Set(Inet(/10.0.1.66:12312,Map())),Map())\n   Bound(Set(Inet(/10.0.1.66:12312,Map())),Map())\n   ```\nlinkerd config\nyaml\nadmin:\n  port: 9990\nrouters:\n- protocol: http\n  httpAccessLog: access.log\n  identifier:\n    kind: default\n    httpUriInDst: true\n  servers:\n  - port: 4140\n    ip: 0.0.0.0\n  interpreter:\n    kind: io.l5d.namerd\n    dst: /$/inet/namerd-default.marathon.slave.mesos/4100\nnamerd config\nyaml\nadmin:\n  port: 9001\nstorage:\n  kind: io.buoyant.namerd.storage.experimental.zk\n  hosts:\n  - master.mesos:2181\n  pathPrefix: /dtabs\n  sessionTimeoutMs: 10000\nnamers:\n- kind:   io.l5d.experimental.marathon\n  prefix: /io.l5d.marathon\n  host:   marathon.mesos\n  port:   8080\n  ttlMs:  10\ninterfaces:\n- kind: thriftNameInterpreter\n  ip: 0.0.0.0\n  port: 4100\n- kind: httpController\n  ip: 0.0.0.0\n  port: 4180\ndtab:\n/srv        => /io.l5d.marathon;\n/www        => /srv/web;\n/host       => /srv | /$/io.buoyant.http.anyHostPfx/www;\n/method     => /$/io.buoyant.http.anyMethodPfx/host;\n/http/1.1   => /method;\n/http/1.0   => /$/io.buoyant.http.anyMethodPfx/www;\n. @adleong @olix0r: do we still want to do this? moving #295 to triage until we decide.. confirming that /api/1/delegate/ does not support a watch param? should it?\n. let's leave it out for now, if we need it, we can revisit.\n. note to change the commit message to not include /api/1/delegate\n. i noticed /api/1/dtabs streams the same response on a deterministic frequency:\n$ curl :4180/api/1/dtabs?watch=true\n[\"pandoracorn\"]\n[\"pandoracorn\"]\n[\"pandoracorn\"]\n...is this due to InMemoryDtabStore loop() ? if so, should we modify the underlying implementation to only stream out changes?\n. \u2b50 \ud83d\udc4d \n\n. \u2b50 lgtm\n\n. Per offline discussion, disabling permissive forked builds. Forks can still ci, but won't have access to DOCKER_CREDENTIALS:\nhttps://circleci.com/docs/fork-pr-builds/\n. \u2b50 \ud83d\udc4d \n\n. \ud83d\udc4d \u2b50  neat!\n$ curl :4180/api/1/dtabs?watch=true\n[\"pandoracorn\"]\n[\"pandoracorn\",\"baz\"]\n\n. agree using GET requests makes more sense in some ways. i considered doing this, then came across this:\nhttp://stackoverflow.com/questions/978061/http-get-with-request-body\nso, rather than modifying the existing API for GETs, i opted to only change how POSTS are handled.\n. \ud83d\udc4d \u2b50  pending risha's comment on x.x.x\n\n. \ud83d\udc4d \u2b50 \n\n. \u2b50 \ud83d\udc4d  lgtm pending other comments\n\n. full stack trace for reference:\nE 0511 17:38:26.529 THREAD60: Exception propagated to the root monitor!\nio.buoyant.router.RoutingFactory$UnknownDst: Unknown destination: invalid http request\n    at com.twitter.finagle.NoStacktrace(Unknown Source)\nCaused by: java.lang.IllegalArgumentException: invalid http request\n    at io.buoyant.router.http.DefaultIdentifier.apply(DefaultIdentifier.scala:39)\n    at io.buoyant.router.http.DefaultIdentifier.apply(DefaultIdentifier.scala:15)\n    at io.buoyant.router.RoutingFactory$RoutingService.apply(RoutingFactory.scala:87)\n    at com.twitter.finagle.Service$$anon$1.apply(Service.scala:16)\n    at com.twitter.finagle.tracing.ServerDestTracingProxy$$anon$1.apply(DestinationTracing.scala:32)\n    at com.twitter.finagle.Filter$$anon$1.apply(Filter.scala:79)\n    at com.twitter.finagle.Service$$anon$1.apply(Service.scala:16)\n    at com.twitter.finagle.service.DeadlineFilter.apply(DeadlineFilter.scala:176)\n    at com.twitter.finagle.Filter$$anon$1.apply(Filter.scala:79)\n    at com.twitter.finagle.Service$$anon$1.apply(Service.scala:16)\n    at com.twitter.finagle.filter.DtabStatsFilter.apply(DtabStatsFilter.scala:37)\n    at com.twitter.finagle.Filter$$anon$1.apply(Filter.scala:79)\n    at com.twitter.finagle.Service$$anon$1.apply(Service.scala:16)\n    at com.twitter.finagle.service.StatsFilter.apply(StatsFilter.scala:150)\n    at com.twitter.finagle.Filter$$anon$1.apply(Filter.scala:79)\n    at com.twitter.finagle.Service$$anon$1.apply(Service.scala:16)\n    at com.twitter.finagle.filter.ExceptionSourceFilter.apply(ExceptionSourceFilter.scala:42)\n    at com.twitter.finagle.Filter$$anon$1.apply(Filter.scala:79)\n    at com.twitter.finagle.Service$$anon$1.apply(Service.scala:16)\n    at com.twitter.finagle.filter.MkJvmFilter$$anon$1.apply(JvmFilter.scala:29)\n    at com.twitter.finagle.Filter$$anon$1.apply(Filter.scala:79)\n    at com.twitter.finagle.Service$$anon$1.apply(Service.scala:16)\n    at com.twitter.finagle.filter.ServerStatsFilter.apply(ServerStatsFilter.scala:43)\n    at com.twitter.finagle.Filter$$anon$1.apply(Filter.scala:79)\n    at com.twitter.finagle.Service$$anon$1.apply(Service.scala:16)\n    at com.twitter.finagle.tracing.AnnotatingTracingFilter.apply(TraceInitializerFilter.scala:118)\n    at com.twitter.finagle.Filter$$anon$1.apply(Filter.scala:79)\n    at com.twitter.finagle.Service$$anon$1.apply(Service.scala:16)\n    at com.twitter.finagle.http.filter.HttpNackFilter.apply(HttpNackFilter.scala:56)\n    at com.twitter.finagle.http.filter.HttpNackFilter.apply(HttpNackFilter.scala:49)\n    at com.twitter.finagle.Filter$$anon$1.apply(Filter.scala:79)\n    at com.twitter.finagle.Service$$anon$1.apply(Service.scala:16)\n    at com.twitter.finagle.buoyant.linkerd.HttpTraceInitializer$server$Filter$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(HttpTraceInitializer.scala:44)\n    at com.twitter.finagle.buoyant.linkerd.HttpTraceInitializer$server$Filter$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(HttpTraceInitializer.scala:44)\n    at com.twitter.finagle.buoyant.linkerd.HttpTraceInitializer$server$Filter.sample(HttpTraceInitializer.scala:62)\n    at com.twitter.finagle.buoyant.linkerd.HttpTraceInitializer$server$Filter$$anonfun$apply$1$$anonfun$apply$2.apply(HttpTraceInitializer.scala:43)\n    at com.twitter.finagle.buoyant.linkerd.HttpTraceInitializer$server$Filter$$anonfun$apply$1$$anonfun$apply$2.apply(HttpTraceInitializer.scala:43)\n    at com.twitter.util.Local.let(Local.scala:141)\n    at com.twitter.finagle.context.Context$class.let(Context.scala:176)\n    at com.twitter.finagle.context.MarshalledContext.let(Context.scala:232)\n    at com.twitter.finagle.tracing.Trace$$anonfun$letTracerAndId$1.apply(Trace.scala:216)\n    at com.twitter.util.Local.let(Local.scala:141)\n    at com.twitter.finagle.context.Context$class.let(Context.scala:176)\n    at com.twitter.finagle.context.LocalContext.let(LocalContext.scala:8)\n    at com.twitter.finagle.tracing.Trace$.letTracerAndId(Trace.scala:215)\n    at com.twitter.finagle.tracing.Trace$.letTracerAndNextId(Trace.scala:198)\n    at com.twitter.finagle.buoyant.linkerd.HttpTraceInitializer$server$Filter$$anonfun$apply$1.apply(HttpTraceInitializer.scala:42)\n    at com.twitter.finagle.buoyant.linkerd.HttpTraceInitializer$server$Filter$$anonfun$apply$1.apply(HttpTraceInitializer.scala:42)\n    at com.twitter.finagle.tracing.Trace$.letIdOption(Trace.scala:178)\n    at com.twitter.finagle.buoyant.linkerd.HttpTraceInitializer$server$Filter.apply(HttpTraceInitializer.scala:41)\n    at com.twitter.finagle.buoyant.linkerd.HttpTraceInitializer$server$Filter.apply(HttpTraceInitializer.scala:26)\n    at com.twitter.finagle.Filter$$anon$1.apply(Filter.scala:79)\n    at com.twitter.finagle.Service$$anon$1.apply(Service.scala:16)\n    at com.twitter.finagle.filter.MonitorFilter$$anonfun$apply$1.apply(MonitorFilter.scala:35)\n    at com.twitter.finagle.filter.MonitorFilter$$anonfun$apply$1.apply(MonitorFilter.scala:35)\n    at com.twitter.util.Future$$anonfun$monitored$1.apply$mcV$sp(Future.scala:156)\n    at com.twitter.util.Monitor$$anonfun$apply$1.apply$mcV$sp(Monitor.scala:38)\n    at com.twitter.util.Monitor$$anonfun$apply$1.apply(Monitor.scala:38)\n    at com.twitter.util.Monitor$$anonfun$apply$1.apply(Monitor.scala:38)\n    at com.twitter.util.Monitor$$anonfun$using$1.apply(Monitor.scala:110)\n    at com.twitter.util.Monitor$.restoring(Monitor.scala:117)\n    at com.twitter.util.Monitor$.using(Monitor.scala:108)\n    at com.twitter.util.Monitor$class.apply(Monitor.scala:37)\n    at com.twitter.util.Future$Monitored.apply(Future.scala:163)\n    at com.twitter.util.Future$.monitored(Future.scala:155)\n    at com.twitter.finagle.filter.MonitorFilter.apply(MonitorFilter.scala:34)\n    at com.twitter.finagle.Filter$$anon$1.apply(Filter.scala:79)\n    at com.twitter.finagle.Service$$anon$1.apply(Service.scala:16)\n    at com.twitter.finagle.filter.PayloadSizeFilter.apply(PayloadSizeFilter.scala:29)\n    at com.twitter.finagle.Filter$$anon$1.apply(Filter.scala:79)\n    at com.twitter.finagle.Service$$anon$1.apply(Service.scala:16)\n    at io.buoyant.linkerd.protocol.http.ErrorResponder$filter$.apply(ErrorResponder.scala:21)\n    at io.buoyant.linkerd.protocol.http.ErrorResponder$filter$.apply(ErrorResponder.scala:15)\n    at com.twitter.finagle.Filter$$anon$1.apply(Filter.scala:79)\n    at com.twitter.finagle.Service$$anon$1.apply(Service.scala:16)\n    at com.twitter.finagle.http.filter.ServerContextFilter$$anonfun$apply$1.apply(ContextFilter.scala:17)\n    at com.twitter.finagle.http.filter.ServerContextFilter$$anonfun$apply$1.apply(ContextFilter.scala:17)\n    at com.twitter.finagle.http.codec.HttpContext$.read(HttpContext.scala:52)\n    at com.twitter.finagle.http.filter.ServerContextFilter.apply(ContextFilter.scala:17)\n    at com.twitter.finagle.http.filter.ServerContextFilter.apply(ContextFilter.scala:13)\n    at com.twitter.finagle.Filter$$anon$1.apply(Filter.scala:79)\n    at com.twitter.finagle.Service$$anon$1.apply(Service.scala:16)\n    at com.twitter.finagle.http.filter.DtabFilter.apply(DtabFilter.scala:22)\n    at com.twitter.finagle.http.filter.DtabFilter.apply(DtabFilter.scala:11)\n    at com.twitter.finagle.Filter$$anon$1.apply(Filter.scala:79)\n    at com.twitter.finagle.http.exp.HttpServerDispatcher.dispatch(HttpServerDispatcher.scala:58)\n    at com.twitter.finagle.http.exp.HttpServerDispatcher.dispatch(HttpServerDispatcher.scala:19)\n    at com.twitter.finagle.http.exp.GenSerialServerDispatcher$$anonfun$com$twitter$finagle$http$exp$GenSerialServerDispatcher$$loop$1$$anonfun$apply$1.apply$mcV$sp(ServerDispatcher.scala:55)\n    at com.twitter.finagle.http.exp.GenSerialServerDispatcher$$anonfun$com$twitter$finagle$http$exp$GenSerialServerDispatcher$$loop$1$$anonfun$apply$1.apply(ServerDispatcher.scala:54)\n    at com.twitter.finagle.http.exp.GenSerialServerDispatcher$$anonfun$com$twitter$finagle$http$exp$GenSerialServerDispatcher$$loop$1$$anonfun$apply$1.apply(ServerDispatcher.scala:54)\n    at com.twitter.util.Local.let(Local.scala:141)\n    at com.twitter.finagle.context.Context$class.let(Context.scala:176)\n    at com.twitter.finagle.context.LocalContext.let(LocalContext.scala:8)\n    at com.twitter.finagle.http.exp.GenSerialServerDispatcher$$anonfun$com$twitter$finagle$http$exp$GenSerialServerDispatcher$$loop$1.apply(ServerDispatcher.scala:53)\n    at com.twitter.finagle.http.exp.GenSerialServerDispatcher$$anonfun$com$twitter$finagle$http$exp$GenSerialServerDispatcher$$loop$1.apply(ServerDispatcher.scala:48)\n    at com.twitter.util.Future$$anonfun$flatMap$1.apply(Future.scala:986)\n    at com.twitter.util.Future$$anonfun$flatMap$1.apply(Future.scala:985)\n    at com.twitter.util.Promise$Transformer.liftedTree1$1(Promise.scala:112)\n    at com.twitter.util.Promise$Transformer.k(Promise.scala:112)\n    at com.twitter.util.Promise$Transformer.apply(Promise.scala:122)\n    at com.twitter.util.Promise$Transformer.apply(Promise.scala:103)\n    at com.twitter.util.Promise$$anon$1.run(Promise.scala:381)\n    at com.twitter.concurrent.LocalScheduler$Activation.run(Scheduler.scala:178)\n    at com.twitter.concurrent.LocalScheduler$Activation.submit(Scheduler.scala:136)\n    at com.twitter.concurrent.LocalScheduler.submit(Scheduler.scala:207)\n    at com.twitter.concurrent.Scheduler$.submit(Scheduler.scala:92)\n    at com.twitter.util.Promise.runq(Promise.scala:350)\n    at com.twitter.util.Promise.updateIfEmpty(Promise.scala:721)\n    at com.twitter.util.Promise.update(Promise.scala:694)\n    at com.twitter.util.Promise.setValue(Promise.scala:670)\n    at com.twitter.concurrent.AsyncQueue.offer(AsyncQueue.scala:111)\n    at com.twitter.finagle.netty3.transport.ChannelTransport.handleUpstream(ChannelTransport.scala:55)\n    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n    at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:142)\n    at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)\n    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n    at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:142)\n    at com.twitter.finagle.netty3.channel.ChannelRequestStatsHandler.messageReceived(ChannelRequestStatsHandler.scala:32)\n    at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)\n    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n    at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)\n    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.messageReceived(SimpleChannelUpstreamHandler.java:124)\n    at com.twitter.finagle.http.codec.RespondToExpectContinue.messageReceived(RespondToExpectContinue.scala:30)\n    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.messageReceived(SimpleChannelUpstreamHandler.java:124)\n    at com.twitter.finagle.http.codec.PayloadSizeHandler.messageReceived(PayloadSizeHandler.scala:24)\n    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n    at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:108)\n    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n    at org.jboss.netty.handler.codec.http.HttpContentEncoder.messageReceived(HttpContentEncoder.java:82)\n    at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)\n    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)\n    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)\n    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)\n    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)\n    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n    at org.jboss.netty.handler.codec.http.HttpServerCodec.handleUpstream(HttpServerCodec.java:56)\n    at com.twitter.finagle.http.SafeHttpServerCodec.handleUpstream(Codec.scala:90)\n    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n    at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:142)\n    at com.twitter.finagle.netty3.channel.ChannelStatsHandler.messageReceived(ChannelStatsHandler.scala:68)\n    at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)\n    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)\n    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)\n    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)\n    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\n    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)\n    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)\n    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\n    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\n    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n. \u2b50 \ud83d\udc4d  works!\nthis is intended to be used in conjunction with a marathon namer, correct? if so, i'm having trouble crafting a config yaml that would leverage this setup, mind adding one to the repo?\n\n. This is really helpful, thanks!\nI'd prefer to preserve the original marathon.yaml, as a domain name like marathon.mesos is more user-friendly than /$/io.buoyant.namer.zkLeader/localhost:2181/marathon/leader. Mind setting this up as a separate example yaml?\n. \u2b50 \ud83d\udc4d great work!\n\n. Sure, default is /metrics.\n. @joerg84 correct, we can make the links relative. question for you: if we make this change, and the user can browse to dcos-url.com/service/linkerd, does that go to the public node, or does it load balance over all linkerd instances running in dc/os? if it's the latter, we'll need to rethink this, as the linkerd dashboard is designed to be a view of a single instance.. @lsjostro this is awesome, thanks for cleaning this up, particularly for adding tests!\nafter reading through your tests, i realized how subtle this transformation is. i found an edge case on our side which fails with this change (not your fault at all, as this test never existed until now ;) ):\nscala\n  test(\"Preserve non-histogram stats\") {\n    val key = PrometheusStatsHandler.formatKey(\"fd_count\")\n    assert(key == \"fd_count\")\n  }\ni took a crack at a couple more changes to try to address this, let me know what you think:\n``` scala\n  private[this] val statPattern = \"\"\"(.*).(count|sum|avg|min|max|stddev|p50|p90|p95|p99|p9990)$\"\"\".r\n  private[this] val disallowedChars = Escape(\"[^a-zA-Z0-9:]\".r, \"_\")\nprivate[this] def escapeKey(key: String) = {\n    disallowedChars.replaceAllIn(delimiter.replaceAllIn(key))\n  }\nprivate[admin] def formatKey(key: String) = key match {\n    case statPattern(label, stat) => s\"\"\"${escapeKey(label)}{stat=\"$stat\"}\"\"\"\n    case _ => escapeKey(key)\n  }\n```\nalso in LinkerdBuild.scala, to get ./sbt admin/test, i had to add .withTests() to the admin build:\nscala\n  val admin = projectDir(\"admin\")\n    .dependsOn(configCore, Namer.core)\n    .withTwitterLib(Deps.twitterServer)\n    .withTwitterLib(Deps.finagle(\"stats\"))\n    .withTests()\n. \u2b50 \ud83d\udc4d thank you for doing this @lsjostro !\n\n. (un)related: ticket to get ci to pass: #399\n. Unclear, I'm also not sure if that's the only missing env var that will fail ci.\n. Example prometheus marathon config:\njson\n{\n  \"id\": \"prometheus\",\n  \"acceptedResourceRoles\": [\n    \"slave_public\"\n  ],\n  \"instances\": 1,\n  \"container\": {\n    \"type\": \"DOCKER\",\n    \"docker\": {\n      \"image\": \"prom/prometheus:0.18.0\",\n      \"forcePullImage\": false,\n      \"network\": \"BRIDGE\",\n      \"portMappings\": [\n        {\n          \"containerPort\": 0,\n          \"hostPort\": 9090,\n          \"servicePort\": 0,\n          \"protocol\": \"tcp\"\n        }\n      ]\n    }\n  },\n  \"args\": [\n    \"-config.file=/mnt/mesos/sandbox/prometheus.yml\"\n  ],\n  \"cpus\": 0.25,\n  \"mem\": 256.0,\n  \"uris\": [\n    \"https://s3.amazonaws.com/HOST_YOUR_CONFIG_HERE/prometheus.yml\"\n  ],\n  \"healthChecks\": [\n    {\n      \"protocol\": \"HTTP\",\n      \"portIndex\": 0,\n      \"path\": \"/api/v1/query?query=up\"\n    }\n  ]\n}\nExample prometheus.yml:\n``` yaml\nglobal:\n  scrape_interval:     1m\n  evaluation_interval: 1m\nscrape_configs:\n  - job_name: 'l5d'\nmetrics_path: /admin/metrics/prometheus\n\nmarathon_sd_configs:\n  - servers: ['http://marathon.mesos:8080']\n\n``\n. We are initiating work on this at Buoyant. If you have opinions about the format/labeling of a statd telemeter, please share. We will likely use/admin/metrics.json` as a starting point, and modify as it makes sense for statsd.\n. Hey folks, we have a very-not-production-ready branch up for testing. If you have a moment, we'd love some feedback:\nhttps://github.com/BuoyantIO/linkerd/tree/siggy/statsd\nWith a hopsoft/graphite-statsd test setup, we have it writing three metrics types:\n- counters: stats_counts.linkerd_l5d-uuid-*\n- histograms: stats.timers.linkerd_l5d-uuid-*\n- gauges: stats.gauges.linkerd_l5d-uuid-*\nLet me know if you need help generating a jar/executable/plugin/docker image.. @robbfoster @olgert: nice catch, thanks! i've updated the PR with a fix: https://github.com/BuoyantIO/linkerd/pull/898/commits/20ab41fa2069976b75966b5a0bcebec7ef4682a1. @olgert: i think that sounds good. i'm inclined to ship this as-is to get more folks kicking the statsd tires, and then iterate with the various protocol extensions. i'm optimistic the extensions should be relatively straightforward.. Thanks for all that detail @leozc. Mind filing a ticket summarizing this request?. Hmmm, this might be more work than it's worth, but is it possible to not scale the elements on the left side of the red line?\n\n. \u2b50 \ud83d\udc4d i think it looks great! we can always double back at some point to get more responsiveness.\n. @halve, et al, here's a proposal for what an influxdb LINE telemeter output would look like:\nhttps://gist.github.com/siggy/1fa809ec5f3f2bd23be81665d5d5d788\nFeedback welcome. We have this working internally, and would appreciate some folks trying it out prior to release. Code is available on branch https://github.com/linkerd/linkerd/tree/siggy/influxdb, and at https://github.com/linkerd/linkerd/pull/1184.\na few notes:\n- very similar to the prometheus telemeter, but with each metric turned into a field and moved up into its parent as a measurement\n- the telemeter data would be available on /admin/metrics/influxdb\n- each measurement includes a host tag, set from the Host header in the collector's request\n- a few example queries:\n  - success rate for each app: SELECT DERIVATIVE(\"success\", 1s) / DERIVATIVE(\"requests\", 1s) FROM \"rt:dst_id\" GROUP BY \"dst_id\"\n  - success rate for each linkerd instance: SELECT DERIVATIVE(\"success\", 1s) / DERIVATIVE(\"requests\", 1s) FROM \"rt:srv\" GROUP BY \"host\"\n- until telegraf adds a LINE protocol input plugin, best practice will be to use the exec plugin with curl commands. example telegraf.conf:\n```\n[[inputs.exec]]\n  # the first command demonstrates using the default host header, yielding a \"host=192.168.1.129:9990\" tag\n  # the second command demonstrates setting a custom host header, yielding a \"host=custom_host\" tag\n  commands = [\n    \"curl -s http://192.168.1.129:9990/admin/metrics/influxdb\",\n    \"curl -H 'Host: custom_host' -s http://192.168.1.129:9990/admin/metrics/influxdb\",\n  ]\n  data_format = \"influx\"\n  timeout = \"5s\"\n# further examples of custom measurement and tag names\n  name_prefix = \"l5d:\"\n  [inputs.exec.tags]\n    l5d_tag1 = \"foo\"\n    l5d_tag2 = \"bar\"\n[[outputs.influxdb]]\n  urls = [\"http://127.0.0.1:8086\"]\n  database = \"l5d\"\n``. \ud83d\udc4d \u2b50 thanks!\n. \ud83d\udc4d \u2b50 confirmedNullTracer+Trace.letClear` works!\n. \u2b50 \ud83d\udc4d great cleanup!\ni noticed the navbars are still styled slightly differently between the main dashboard and the dtab page. is that expected? followup branch?\n. \u2b50 \ud83d\udc4d thanks!\n. \ud83d\udc4d \u2b50 looks great would expand again !\n. \ud83d\udc4d \u2b50 thanks!\n. \ud83d\udc4d \u2b50 change looks good! (modulo ci)\nAt some point it may be simpler to remove/combine some of these mostly similar APIs:\n1. http://localhost:9991/delegator.json?namespace=\n2. http://localhost:4180/api/1/delegate?namespace=\n3. http://localhost:4180/api/1/delegate/namespace?\nAt quick glance, consider removing #3, modify #1 to match name and params to #2, then make #1 a pass-through to #2 may help.\n. \ud83d\udc4d \u2b50 this format looks awesome.\n. \ud83d\udea2 :shipit: \n. ### linkerd change\nThe marathon namer can optionally check app/tasks/healthCheckResults/alive to determine instance health. Probably near https://github.com/BuoyantIO/linkerd/blob/master/marathon/src/main/scala/io/buoyant/marathon/v2/Api.scala#L89.\nMarathon API\nNote that according to the Marathon API Docs we may need to add ?embed=tasks soon anyway.\nAbridged response for /v2/apps/[appId]:\njson\n{\n  \"app\": {\n    \"id\": \"/toggle\",\n    \"tasks\": [\n      {\n        \"appId\": \"/toggle\",\n        \"healthCheckResults\": [\n          {\n            \"alive\": true,\n            \"taskId\": \"toggle.802df2ae-3ad4-11e4-a400-56847afe9799\"\n          },\n          {\n            \"alive\": true,\n            \"taskId\": \"toggle.802df2ae-3ad4-11e4-a400-56847afe9799\"\n          }\n        ],\n        \"host\": \"10.141.141.10\",\n        \"id\": \"toggle.7c99814d-3ad4-11e4-a400-56847afe9799\",\n        \"ports\": [\n          31234\n        ]\n      }\n    ]\n  }\n}\n. We don't necessarily need it tracked twice, but it would be helpful to have this data on the server.\n. This is the recommended process for all DC/OS packages. Note that to be a user on the parent OS means to have already have root on the machine.\ncode:\nhttps://github.com/mesosphere/universe/search?utf8=%E2%9C%93&q=DCOS_SERVICE_ACCOUNT_CREDENTIAL\ndocs:\nhttps://docs.mesosphere.com/1.8/administration/id-and-access-mgt/service-auth/universe-service-auth/\nhttps://docs.mesosphere.com/1.8/administration/id-and-access-mgt/service-auth/custom-service-auth/\n. it just feels like io.buoyant.marathon.v2.Api is kind of a odd place to have generic json parsing code.\n. The difference between the CoreOS and CentOS images seems to be the hostname value.\nCoreOS\n``` bash\n$ hostname\nip-10-0-3-101.us-west-1.compute.internal\n$ nslookup ip-10-0-3-101.us-west-1.compute.internal\nServer:     198.51.100.1\nAddress:    198.51.100.1#53\nNon-authoritative answer:\nName:   ip-10-0-3-101.us-west-1.compute.internal\nAddress: 10.0.3.101\n```\nCentOS\n``` bash\n$ hostname\nip-10-0-3-101\n$ nslookup ip-10-0-3-101\nServer:     198.51.100.1\nAddress:    198.51.100.1#53\n** server can't find ip-10-0-3-101: NXDOMAIN\n``\n. @camechis. this is a linkerd issue, not mesosphere. we're investigating a few approaches to fix this.\n. I believe it's this line:\nhttps://github.com/BuoyantIO/linkerd/blob/master/interpreter/per-host/src/main/scala/io/buoyant/transformer/perHost/LocalhostTransformerInitializer.scala#L15\n... interacting with myhostname` comment above.\n. @bflance: mind providing a bit more detail? best to drop into linkerd slack. http://slack.linkerd.io\n. Greetings @neethugeorge!\nA few things:\n- try a simpler linkerd config: https://github.com/BuoyantIO/linkerd-examples/tree/master/dcos/simple-proxy\n- is there any additional stack trace / messages? if so pass them on.\n- try our slack channel, you'll probably get a faster response\n. @adleong Yup, I thought it best to make the default dtab as generic as possible. It now matches most of the configs in the linkerd-examples repo.\n. Without namerd, given this dtab:\n/srv        => /#/io.l5d.marathon ;\n/host       => /srv & /$/empty ;\n/http/1.1/* => /host ;\nfirst request fails, all subsequent requests succeed:\nbash\n$ http_proxy=http://PUBLIC_DCOS_NODE:4140 curl webapp/hello\nUnknown namer: empty for path: /$/empty/webapp\n...\n$ http_proxy=http://PUBLIC_DCOS_NODE:4140 curl webapp/hello\nHello world. confirmed /$/nil works as expected.. may be a duplicate of https://github.com/BuoyantIO/linkerd/pull/821. @gaugau It's a bug in linkerd. You can verify this by hitting the /config.json endpoint on your linkerd admin port, or just open up your browser dev console while viewing the dtab page.. turns out this already exists in namerd's http api, probably a more appropriate place anyway:\nbash\n$ curl -s 192.168.99.100:4180/api/1/bound-names | jq\n[\n  \"/#/io.l5d.fs/authors\",\n  \"/#/io.l5d.fs/books\",\n  \"/#/io.l5d.fs/proxy\",\n  \"/#/io.l5d.fs/web\",\n  \"/#/io.l5d.fs/authors-v2\"\n]. @leozc This PR is already using the datadog Statsd client? Re: Datadog tagging, let me look into it. My intent was to provide a generic statsd exporter.\n@adleong: Load testing showed throughput nearly in half, and latency nearly doubled. I believe this was due to sending network calls on every counter increment and histogram event. For Counters, I've modified them to batch up and send periodically, similar to gauges. For Histograms, we now use a sample rate. Using these new defaults, performance change is negligible.. That's helpful @leozc! Do you know if the datadog tagging is compatible with the influxDB tagging?. For my reference and yours, some info on tagging...\nDatadog tagging\nWe're using java-dogstatsd-client, adding Datadog tagging should be fairly straightforward. \nhttp://docs.datadoghq.com/guides/dogstatsd/#datagram-format\nmetric.name:value|type|@sample_rate|#tag1:value,tag2\nInfluxDB tagging\nThis format appends the tag to the metric name itself. It should still be possible to implement this using the existing library, just a bit more work.\nhttps://www.influxdata.com/getting-started-with-sending-statsd-metrics-to-telegraf-influxdb/\nuser.logins,service=payroll,region=us-west:1|c\n. Hi @jacob-koren. By default linkerd retries connection failures, but not application failures, which may be what you are hitting. The linkerd DC/OS package is configured conservatively to avoid retrying requests that may not be idempotent.\nIf you'd like to configure more aggressive retries, have a look at:\nhttps://linkerd.io/config/0.8.5/linkerd/index.html#http-response-classifiers\nhttps://linkerd.io/config/0.8.5/linkerd/index.html#retries\nIf you do decide to deploy a custom-configured linkerd onto DC/OS, you can follow along here:\nhttps://linkerd.io/getting-started/dcos/#deploying-a-custom-linkerd. Thanks for the detailed report @jacob-koren. You're right, I think if useHealthCheck is enabled, linkerd should look at both \"state\": \"TASK_KILLING\" and \"healthCheckResults\": [ { \"alive\": true....\nA little more detail for my own context:\nhttps://mesosphere.github.io/marathon/docs/health-checks.html. Correct. To be slightly more specific, when useHealthCheck is enabled, we should check for \"healthCheckResults\": [ { \"alive\": true... and state is TASK_RUNNING. @adleong that's expected, we require the trailing the slash. this makes the root consistent with all other paths: /files..., /admin..., etc.. closing with #1025. this looks awesome! a few notes on the ui:\n\nconsider moving the numbers away from the start and end of the bar. it's confusing to me to see a bar/gauge with a 1 on both ends. maybe just one text element that says 1/1 load balancers available\ni'm not grocking the 3-color gauge. is it just meant to show a gradient? if so, i'd say just make it a gradient.\ni'm seeing a - in place of a number on the retries chart. does this mean the metric is absent? if so, does it make sense to default to 1?\nwhen i make the window wide, it looks a little bit weird that a big gap opens up between the bar charts. not sure the best approach here, perhaps pin retries to the left? or grow retries with the success rate graph? i'm not sure if either of those make sense, just thinking out loud.\n. @adleong believe it or not, i actually wrote my comment above re: UsageDataHandler before seeing your comment.\n\nmy initial implementation had this all on the server-side, but i found writing so much html in scala to be cumbersome. i noticed the namerd module already uses js and a template. is there a long-term plan for allowing plugins to add tabs and be able to leverage js/templating?. Agree that plugin systems should be modular, that's what motivated my original comment about it being weird to have plugin-specific assets and code in the admin project. This approach seemed viable because that abstraction was already broken, given the namerd-specific assets and routing code in the admin project which only makes sense when a namerd namer is loaded. I was simply following that pattern.\nGoing forward, I see 4 options:\n1) ship as-is with Usage assets and code in Admin\n2) render everything server-side, ala RecentRequestsAdminHandler\n3) move Usage assets to the Usage project, copy js/require/templating infrastructure from Admin to Usage\n4) refactor the admin plugin system to provide templating to plugins\nIf there are other options I am missing, I'm open to it.\nI think 1 and 2 are doable in this review, 3 is probably a bad idea all around, and 4 may be the correct long-term approach. If we do 2, it means admin plugins do not have the same access to assets and templating that the rest of admin does, making frontend development quite limited, and I think should motivate prioritizing 4.\n. closing for now. users can validate usage data by hitting the /admin/metrics/usage endpoint.. @kunalpathak14 delegator.json is a read-only endpoint, not sure if that's what you mean. recommend asking with a bit more detail on https://discourse.linkerd.io/ or in linkerd slack.. Some testing on k8s generates stats like:\nrequests{rt=\"outgoing\", dst_id=\"%/io.l5d.k8s.daemonset/default/incoming/l5d/#/io.l5d.k8s/default/http/hello\", type=\"counter\"} 7308\nrequests{rt=\"incoming\", dst_id=\"%/io.l5d.k8s.localnode/10.233.96.4/#/io.l5d.k8s/default/http/world-v1\", dst_path=\"http/1.1/GET/world\", type=\"counter\"} 70101\nIs it possible to break up the dst_id more? The hello and world-v1 bits are most relevant. I think generally it's helpful to err on the side of more, shorter labels, as it's easy to query across multiple labels on the consumption side.\n. I think if we're not matching prometheus' histogram format, the way you have it right now works best, at least all the components of the histogram are grouped under a single stat name.. i am seeing a similar issue in minikube. not sure if related, but the steps to reproduce are simpler.\nfull output of k8s endpoints api and linkerd debug log are at:\nhttps://gist.github.com/siggy/19c049a62bc8e9b65cac041c2921346b\nsteps to repro\nDeploy linkerd and app\nbash\nkubectl apply -f https://raw.githubusercontent.com/linkerd/linkerd-examples/master/k8s-daemonset/k8s/linkerd.yml\nkubectl apply -f https://raw.githubusercontent.com/linkerd/linkerd-examples/master/k8s-daemonset/k8s/hello-world-legacy.yml\nVerify routing works\nbash\nOUTGOING_PORT=$(kubectl get svc l5d -o jsonpath='{.spec.ports[?(@.name==\"outgoing\")].nodePort}')\nL5D_INGRESS_LB=http://$(minikube ip):$OUTGOING_PORT\nhttp_proxy=$L5D_INGRESS_LB curl -s http://world\nworld (172.17.0.9)!\nRedeploy app\nbash\nkubectl delete -f https://raw.githubusercontent.com/linkerd/linkerd-examples/master/k8s-daemonset/k8s/hello-world-legacy.yml\nkubectl apply -f https://raw.githubusercontent.com/linkerd/linkerd-examples/master/k8s-daemonset/k8s/hello-world-legacy.yml\nObserve routing failure\nbash\n$ http_proxy=$L5D_INGRESS_LB curl -s http://world\nNo hosts are available for /svc/world, Dtab.base=[/srv=>/#/io.l5d.k8s/default/http;/host=>/srv;/svc=>/host;/host/world=>/srv/world-v1], Dtab.local=[]. Remote Info: Not Available\nObserve delegator api returns healthy endpoints\n```bash\n$ ADMIN_PORT=$(kubectl get svc l5d -o jsonpath='{.spec.ports[?(@.name==\"admin\")].nodePort}')\n$ curl -H \"Content-Type: application/json\" -X POST -d '{\"namespace\":\"incoming\",\"dtab\":\"/srv=>/#/io.l5d.k8s/default/http;/host=>/srv;/svc=>/host;/host/world=>/srv/world-v1\",\"path\":\"/svc/world\"}' http://$(minikube ip):$ADMIN_PORT/delegator.json\n{\"type\":\"delegate\",\"path\":\"/svc/world\",\"delegate\":{\"type\":\"alt\",\"path\":\"/host/world\",\"dentry\":{\"prefix\":\"/svc\",\"dst\":\"/host\"},\"alt\":[{\"type\":\"delegate\",\"path\":\"/srv/world-v1\",\"dentry\":{\"prefix\":\"/host/world\",\"dst\":\"/srv/world-v1\"},\"delegate\":{\"type\":\"transformation\",\"path\":\"/#/io.l5d.k8s/default/http/world-v1\",\"name\":\"SubnetLocalTransformer\",\"bound\":{\"addr\":{\"type\":\"bound\",\"addrs\":[{\"ip\":\"172.17.0.13\",\"port\":7778,\"meta\":{\"nodeName\":\"minikube\"}},{\"ip\":\"172.17.0.11\",\"port\":7778,\"meta\":{\"nodeName\":\"minikube\"}},{\"ip\":\"172.17.0.12\",\"port\":7778,\"meta\":{\"nodeName\":\"minikube\"}}],\"meta\":{}},\"id\":\"/#/io.l5d.k8s/default/http/world-v1\",\"path\":\"/\"},\"tree\":{\"type\":\"leaf\",\"path\":\"/%/io.l5d.k8s.localnode/172.17.0.3/#/io.l5d.k8s/default/http/world-v1\",\"dentry\":{\"prefix\":\"/srv\",\"dst\":\"/#/io.l5d.k8s/default/http\"},\"bound\":{\"addr\":{\"type\":\"bound\",\"addrs\":[{\"ip\":\"172.17.0.13\",\"port\":7778,\"meta\":{\"nodeName\":\"minikube\"}},{\"ip\":\"172.17.0.11\",\"port\":7778,\"meta\":{\"nodeName\":\"minikube\"}},{\"ip\":\"172.17.0.12\",\"port\":7778,\"meta\":{\"nodeName\":\"minikube\"}}],\"meta\":{}},\"id\":\"/%/io.l5d.k8s.localnode/172.17.0.3/#/io.l5d.k8s/default/http/world-v1\",\"path\":\"/\"}}}},{\"type\":\"delegate\",\"path\":\"/srv/world\",\"dentry\":{\"prefix\":\"/host\",\"dst\":\"/srv\"},\"delegate\":{\"type\":\"neg\",\"path\":\"/#/io.l5d.k8s/default/http/world\",\"dentry\":{\"prefix\":\"/srv\",\"dst\":\"/#/io.l5d.k8s/default/http\"}}}]}}\n```\nObserve successful curl to world service from inside l5d container\nbash\n$ kubectl exec -it l5d-r6fv5 -c l5d curl 172.17.0.11:7778\nworld (172.17.0.11)!\n. updated gist with metrics.json https://gist.github.com/siggy/19c049a62bc8e9b65cac041c2921346b. Hi @hh! I took a crack at documenting linkerd's current ci state, and where we'd like to take it, leveraging CNCF's infrastructure. Let us know if this is the direction you had in mind: https://github.com/cncf/wg-ci/pull/14. @klingerf sgtm. all linkerd ci-related info for cncf is now at https://github.com/cncf/wg-ci/blob/master/projects/linkerd.mkd. Thanks for digging into this! I think it's totally acceptable to make these settings mutually exclusive. My assumption is that if someone is running DC/OS, they can use either setting, and if they're just on Marathon/Mesos, they can use this new setting.. i like 2. it provides the same level of flexibility as 3 and 4, correct?\noverrides within a config get super confusing, it's never obvious which things affect which other things.\nas we're aiming for 1.0, i'm all for breaking changes right now.. I think this general direction makes sense, glad we are doing this pre-1.0.\nI'm still struggling a little bit with io.l5d.static and io.l5d.global coupled with the use of a configs section or not, respectively. This shouldn't necessarily motivate a change, but perhaps just more explicit explanation/documentation on usage. The fact that it mirrors client is helpful, though.\nMy understanding is that these two configs blocks are valid:\nyaml\n  service:\n    kind: io.l5d.static\n    configs:\n    - prefix: /svc\n      retries:\nyaml\n  service:\n    kind: io.l5d.global\n    retries:\nI think it feels a bit weird that the kind field affects what is possible in the rest of the object. For example, would the following config blocks be invalid, and if so, is there a way to enforce it more structurally in the yaml:\nyaml\n  service:\n    kind: io.l5d.global\n    configs:\n    - prefix: /svc\n      retries:\nyaml\n  service:\n    kind: io.l5d.static\n    retries:\n. @adleong all sounds good. per offline convo, my concern stems from sibling objects affecting each other. this is not necessarily actionable, just something to consider as we document and articulate this approach. being explicit that kind is a special thing is helpful.. confirmed repro with gke 1.6.1:\nbash\ngcloud alpha container clusters create alpha --cluster-version 1.6.1 --zone us-central1-b --enable-kubernetes-alpha\nalso confirmed this works correctly on 1.6.0.. looks like this is not just an api difference between 1.6.0 and 1.6.1, but a difference between /api/v1/namespaces/{namespace}/endpoints and /api/v1/namespaces/{namespace}/endpoints/{name} in 1.6.1\n1.6.0\n```bash\n$ kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"6\", GitVersion:\"v1.6.0\", GitCommit:\"fff5156092b56e6bd60fff75aad4dc9de6b6ef37\", GitTreeState:\"clean\", BuildDate:\"2017-03-28T16:36:33Z\", GoVersion:\"go1.7.5\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"6\", GitVersion:\"v1.6.0\", GitCommit:\"fff5156092b56e6bd60fff75aad4dc9de6b6ef37\", GitTreeState:\"clean\", BuildDate:\"2017-03-28T16:24:30Z\", GoVersion:\"go1.7.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n$ kubectl -n=kube-system get ep kube-controller-manager -o jsonpath='{.subsets}'\n[]\n$ kubectl -n=kube-system get ep -o jsonpath='{.items[?(@.metadata.name==\"kube-controller-manager\")].subsets}'\n[]\n```\n1.6.1\n```bash\n$ kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"6\", GitVersion:\"v1.6.0\", GitCommit:\"fff5156092b56e6bd60fff75aad4dc9de6b6ef37\", GitTreeState:\"clean\", BuildDate:\"2017-03-28T16:36:33Z\", GoVersion:\"go1.7.5\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"6\", GitVersion:\"v1.6.1\", GitCommit:\"b0b7a323cc5a4a2019b2e9520c21c7830b7f708e\", GitTreeState:\"clean\", BuildDate:\"2017-04-03T20:33:27Z\", GoVersion:\"go1.7.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n$ kubectl -n=kube-system get ep kube-controller-manager -o jsonpath='{.subsets}'\n[]\n$ kubectl -n=kube-system get ep -o jsonpath='{.items[?(@.metadata.name==\"kube-controller-manager\")].subsets}'\n # <---- BOOM\n```\n. Fix is up for review.\nAlso reported this at https://github.com/kubernetes/kubernetes/issues/44593. review this issue after https://github.com/twitter/util/pull/189 has shipped.. Related: #1368.. We've pushed a fix to our private docs repo. Expect to see the change on the docs site shortly.. @elecnix have you tried https://github.com/linkerd/linkerd-viz ?. @elecnix Interesting ideas around scripted dashboards.\nI'd try linkerd-viz if I were you. It does per-service and per-linkerd stats out of the box. It can be further customized for specific metrics you are interested in.. Hi @christtrc. linkerd is at 1.0.0 in DC/OS universe master:\nhttps://github.com/mesosphere/universe/tree/version-3.x/repo/packages/L\nWhat version of DC/OS are you on? 1.8/1.9? Is your universe up to date?. @oysterpack you're correct this is a documentation issue. you need to add a section to your linkerd config like this:\nyaml\ntelemetry:\n- kind: io.l5d.prometheus\nexample:\nhttps://github.com/linkerd/linkerd-examples/blob/master/linkerd-tcp/linkerd.yml#L1\ni'm going to close this for now and get a fix going in our docs repo.. @magg: @elecnix is correct, you can use header-classifiers, or per-request headers to accomplish this.\nI'm going to close this issue. I've moved this topic to our new forum. If you have more questions/comments, let's keep the conversation going over there:\nhttps://discourse.linkerd.io/t/per-request-overrides-using-http-headers/66. To simulate authentication errors, I added a --accept-paths='/foo' param to the kubectl container.. @adleong \nE 0522 22:08:45.401 UTC THREAD20: k8s failed to list endpoints\nio.buoyant.k8s.Api$UnexpectedResponse: Response(\"HTTP/1.1 Status(403)\"): <h3>Unauthorized</h3>\n    at io.buoyant.k8s.Api$.parse(Api.scala:83). config file for reference:\n```yaml\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: l5d-mesh-config\n  namespace: linkerd\ndata:\n  config.yaml: |-\n    admin:\n      port: 9990\nnamers:\n  - kind: io.l5d.k8s\n    experimental: true\n    host: localhost\n    port: 8001\n\nrouters:\n  - protocol: http\n    label: http\n    dstPrefix: /http\n    interpreter:\n      kind: io.l5d.namerd\n      dst: /$/inet/namerd-sync.linkerd/4100\n      namespace: mesh\n      transformers:\n      - kind: io.l5d.k8s.daemonset\n        namespace: linkerd\n        port: incoming-http\n        service: l5d-mesh\n    servers:\n    - port: 4143\n      ip: 0.0.0.0\n    client:\n      kind: io.l5d.global\n      tls:\n        trustCerts:\n          - /infraca/ca.crt\n        commonName: linkerd\n\n  - protocol: http\n    label: api\n    dstPrefix: /api\n    identifier:\n      kind: io.l5d.header\n      header: x-ortoo-api-resource\n    interpreter:\n      kind: io.l5d.namerd\n      dst: /$/inet/namerd-sync.linkerd/4100\n      namespace: mesh\n      transformers:\n      - kind: io.l5d.k8s.daemonset\n        namespace: linkerd\n        port: incoming-http\n        service: l5d-mesh\n    servers:\n    - port: 4144\n      ip: 0.0.0.0\n    client:\n      kind: io.l5d.global\n      tls:\n        trustCerts:\n          - /infraca/ca.crt\n        commonName: linkerd\n\n  - protocol: http\n    label: incoming-http\n    dstPrefix: /\n    identifier:\n      kind: io.l5d.header\n      header: l5d-dst-client\n    interpreter:\n      kind: io.l5d.namerd\n      dst: /$/inet/namerd-sync.linkerd/4100\n      namespace: mesh\n      transformers:\n        - kind: io.l5d.k8s.localnode\n    servers:\n    - port: 4139\n      ip: 0.0.0.0\n      tls:\n        certPath: /certificates/tls.crt\n        keyPath: /certificates/tls.key\n\n\napiVersion: extensions/v1beta1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: l5d-mesh\n    ortoo.tls.infra: \"true\"\n  name: l5d-mesh\n  namespace: linkerd\n  annotations:\n    ortoo/tls.infra.hosts: '\n      .infra.ortootech.com\n      .default\n      linkerd\n    '\n    ortoo/tls.infra.secret: l5d-mesh-infra-ssl-cert\n    ortoo/tls.infra.ca.secret: l5d-infra-ca-cert\nspec:\n  template:\n    metadata:\n      labels:\n        app: l5d-mesh\n    spec:\n      volumes:\n      - name: l5d-mesh-config\n        configMap:\n          name: l5d-mesh-config\n      - name: l5d-mesh-infra-ssl-cert\n        secret:\n          secretName: l5d-mesh-infra-ssl-cert\n      - name: l5d-infra-ca-cert\n        secret:\n          secretName: l5d-infra-ca-cert\n      containers:\n      - name: l5d\n        image: buoyantio/linkerd:1.0.0\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        args:\n          - /io.buoyant/linkerd/config/config.yaml\n          # - -log.level=DEBUG\n        ports:\n        - name: incoming-http\n          containerPort: 4139\n          hostPort: 4139\n        - name: http\n          containerPort: 4143\n          hostPort: 4143\n        - name: api\n          containerPort: 4144\n          hostPort: 4144\n        - name: admin\n          containerPort: 9990\n          hostPort: 9990\n        volumeMounts:\n        - name: \"l5d-mesh-config\"\n          mountPath: \"/io.buoyant/linkerd/config\"\n          readOnly: true\n        - name: l5d-mesh-infra-ssl-cert\n          mountPath: /certificates\n          readOnly: true\n        - name: l5d-infra-ca-cert\n          mountPath: /infraca\n          readOnly: true\n  - name: kubectl\n    image: buoyantio/kubectl:v1.6.2\n    args:\n    - \"proxy\"\n    - \"-p\"\n    - \"8001\"\n\n```. @jamessharp That heap dump is consistent with the memory leaks you are seeing. We're trying to get a reproducible environment. Can you tell us a bit about the clients/workers that are connecting to this? Is it a load test tool? Do they timeout/disconnect?\nEclipse Memory Analyzer shows 433,641 com.twitter.finagle.Stack$Node objects:\n\nDigging into a com.twitter.finagle.Stack$Node using jhat jmap_dump.bin, and then navigating up via References to this object, it appears to reference scala.collection.immutable.$colon$colon at least 100 times up the chain.. @jamessharp closing this for now as the issue seems to be resolved. please reopen if you see the issue again.. @adleong I believe the curl call above was an example, and the source motivation for this was a Linkerd hung when querying Namerd. I don't remember the specific instance.. Note: This PR should not merge until we are ready to make a breaking release.\nThis change will definitely break many users' connections to namerd and to linkerd+namerd admin pages. Will likely also require changes to linkerd-examples.. abandoning this change, as alpine does not provide the libs we need, more details at https://github.com/linkerd/linkerd/issues/1581. i'lll work on creating a separate PR against master without all the istio changes.. linkerd interface change in new PR against master: https://github.com/linkerd/linkerd/pull/1463. Closing per https://github.com/linkerd/linkerd-examples/issues/148#issuecomment-310512139.. Hi @daftclouds. We have instructions for configuring linkerd to connect to an authenticated Marathon in DC/OS Enterprise, have a look at:\nhttps://github.com/dcos/examples/tree/master/linkerd/1.8#mesosphere-enterprise-dcos\nhttps://linkerd.io/config/1.1.0/linkerd/index.html#marathon-authentication\n. Thanks this detailed writeup @ashahan. We'll have a look.. @mtweten This is awesome! I just confirmed it works on our end. Please create a PR with your change.\nA couple comments:\n- please update namerd.md with the params you just added. you can reference client_tls.md, but as you mentioned, indicate that clientAuth is unused.\n- re-sort the com.twitter.finagle.buoyant.TlsClientConfig import line\n. Hi @kevholditch. Thanks for submitting this PR, it's helped us a ton in understanding what ECS integration looks like. After a bit of discussion, I think we'd like to try to support ECS via configs and examples, minimizing changes inside linkerd, and avoiding custom Docker containers, if possible. To that end, I just put up a pull request that demonstrates ECS+linked+Consul, using io.l5d.specificHost:\nhttps://github.com/linkerd/linkerd-examples/pull/169\nI'd love your feedback on this, particularly because your PR and blog post had significant influence on these examples (We've credited your blog at the bottom of the README).. @adleong Did not experimentally arrive at 16384. @olix0r said \"set it at 16k\" and I settled on 2^14.. @Taik Planning to release later today, stay tuned!. Fixed in #1580, thanks @cponomaryov !. Confirmed this is happening with alpine: https://github.com/linkerd/linkerd/pull/1596#issuecomment-323204885. The failure with alpine appears to be slightly different than the failure reported here. Both eventually fail with java.lang.UnsupportedOperationException: JDK provider does not support NPN_AND_ALPN protocol:\n- 1.1.3 fails to create a JdkSslClientContext, but gets much farther.\n- alpine fails to create a JdkSslServerContext.\n1.1.3\nbash\n$ nghttp https://localhost:4240/ --cert=linkerd-tls-e2e-cert.pem --key=linkerd-tls-e2e-key.pem -v\n[  0.004] Connected\nThe negotiated protocol: h2\n[  0.366] send SETTINGS frame <length=12, flags=0x00, stream_id=0>\n          (niv=2)\n          [SETTINGS_MAX_CONCURRENT_STREAMS(0x03):100]\n          [SETTINGS_INITIAL_WINDOW_SIZE(0x04):65535]\n[  0.366] send PRIORITY frame <length=5, flags=0x00, stream_id=3>\n          (dep_stream_id=0, weight=201, exclusive=0)\n[  0.366] send PRIORITY frame <length=5, flags=0x00, stream_id=5>\n          (dep_stream_id=0, weight=101, exclusive=0)\n[  0.366] send PRIORITY frame <length=5, flags=0x00, stream_id=7>\n          (dep_stream_id=0, weight=1, exclusive=0)\n[  0.367] send PRIORITY frame <length=5, flags=0x00, stream_id=9>\n          (dep_stream_id=7, weight=1, exclusive=0)\n[  0.367] send PRIORITY frame <length=5, flags=0x00, stream_id=11>\n          (dep_stream_id=3, weight=1, exclusive=0)\n[  0.367] send HEADERS frame <length=38, flags=0x25, stream_id=13>\n          ; END_STREAM | END_HEADERS | PRIORITY\n          (padlen=0, dep_stream_id=11, weight=16, exclusive=0)\n          ; Open new stream\n          :method: GET\n          :path: /\n          :scheme: https\n          :authority: localhost:4240\n          accept: */*\n          accept-encoding: gzip, deflate\n          user-agent: nghttp2/1.7.1\n[  0.797] recv SETTINGS frame <length=0, flags=0x00, stream_id=0>\n          (niv=0)\n[  0.797] recv SETTINGS frame <length=0, flags=0x01, stream_id=0>\n          ; ACK\n          (niv=0)\n[  0.797] send SETTINGS frame <length=0, flags=0x01, stream_id=0>\n          ; ACK\n          (niv=0)\n[  1.303] recv RST_STREAM frame <length=4, flags=0x00, stream_id=13>\n          (error_code=REFUSED_STREAM(0x07))\n[  1.303] send GOAWAY frame <length=8, flags=0x00, stream_id=0>\n          (last_stream_id=0, error_code=NO_ERROR(0x00), opaque_data(0)=[])\nSome requests were not processed. total=1, processed=0\nbash\n$ docker run --net=\"host\" -v /home/eliza/test:/certs buoyantio/linkerd:1.1.3 /certs/linkerd.yml\n...\nWARN 0818 18:19:29.599 UTC finagle/netty4-2: Failed to initialize a channel. Closing: [id: 0xfba2a7ee]\njava.lang.UnsupportedOperationException: JDK provider does not support NPN_AND_ALPN protocol\n    at io.netty.handler.ssl.JdkSslContext.toNegotiator(JdkSslContext.java:317)\n    at io.netty.handler.ssl.JdkSslClientContext.<init>(JdkSslClientContext.java:272)\n    at io.netty.handler.ssl.SslContext.newClientContextInternal(SslContext.java:770)\n    at io.netty.handler.ssl.SslContextBuilder.build(SslContextBuilder.java:446)\n    at com.twitter.finagle.netty4.ssl.client.Netty4ClientEngineFactory.apply(Netty4ClientEngineFactory.scala:66)\n    at com.twitter.finagle.netty4.ssl.client.Netty4ClientSslHandler.$anonfun$initChannel$1(Netty4ClientSslHandler.scala:114)\n    at com.twitter.finagle.netty4.ssl.client.Netty4ClientSslHandler.$anonfun$initChannel$1$adapted(Netty4ClientSslHandler.scala:111)\n    at scala.Option.foreach(Option.scala:257)\n    at com.twitter.finagle.netty4.ssl.client.Netty4ClientSslHandler.initChannel(Netty4ClientSslHandler.scala:111)\n    at io.netty.channel.ChannelInitializer.initChannel(ChannelInitializer.java:113)\n    at io.netty.channel.ChannelInitializer.handlerAdded(ChannelInitializer.java:105)\n    at io.netty.channel.DefaultChannelPipeline.callHandlerAdded0(DefaultChannelPipeline.java:597)\n    at io.netty.channel.DefaultChannelPipeline.addFirst(DefaultChannelPipeline.java:178)\n    at io.netty.channel.DefaultChannelPipeline.addFirst(DefaultChannelPipeline.java:143)\n    at com.twitter.finagle.netty4.channel.AbstractNetty4ClientChannelInitializer.initChannel(AbstractNetty4ClientChannelInitializer.scala:77)\n    at com.twitter.finagle.netty4.channel.RawNetty4ClientChannelInitializer.initChannel(RawNetty4ClientChannelInitializer.scala:18)\n    at io.netty.channel.ChannelInitializer.initChannel(ChannelInitializer.java:113)\n    at io.netty.channel.ChannelInitializer.handlerAdded(ChannelInitializer.java:105)\n    at io.netty.channel.DefaultChannelPipeline.callHandlerAdded0(DefaultChannelPipeline.java:597)\n    at io.netty.channel.DefaultChannelPipeline.access$000(DefaultChannelPipeline.java:44)\n    at io.netty.channel.DefaultChannelPipeline$PendingHandlerAddedTask.execute(DefaultChannelPipeline.java:1387)\n    at io.netty.channel.DefaultChannelPipeline.callHandlerAddedForAllHandlers(DefaultChannelPipeline.java:1122)\n    at io.netty.channel.DefaultChannelPipeline.invokeHandlerAddedIfNeeded(DefaultChannelPipeline.java:647)\n    at io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:506)\n    at io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:419)\n    at io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:478)\n    at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\n    at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:462)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run(BlockingTimeTrackingThreadFactory.scala:24)\n    at java.lang.Thread.run(Thread.java:748)\nAlpine\nbash\n$ nghttp https://localhost:4240/ --cert=linkerd-tls-e2e-cert.pem --key=linkerd-tls-e2e-key.pem -v\n[  0.004] Connected\nSome requests were not processed. total=1, processed=0\nbash\n$ docker run --net=\"host\" -v /home/eliza/test:/certs buoyantio/linkerd:openjdk-alpine2 /certs/linkerd.yml\n...\nWARN 0818 18:10:12.335 GMT finagle/netty4-1: Failed to initialize a channel. Closing: [id: 0x2ab1e070, L:/127.0.0.1:4240 - R:/127.0.0.1:52074]\njava.lang.UnsupportedOperationException: JDK provider does not support NPN_AND_ALPN protocol\n    at io.netty.handler.ssl.JdkSslContext.toNegotiator(JdkSslContext.java:317)\n    at io.netty.handler.ssl.JdkSslServerContext.<init>(JdkSslServerContext.java:239)\n    at io.netty.handler.ssl.SslContext.newServerContextInternal(SslContext.java:415)\n    at io.netty.handler.ssl.SslContextBuilder.build(SslContextBuilder.java:441)\n    at com.twitter.finagle.netty4.ssl.server.Netty4ServerEngineFactory.apply(Netty4ServerEngineFactory.scala:51)\n    at com.twitter.finagle.netty4.ssl.server.Netty4ServerSslHandler.$anonfun$initChannel$1(Netty4ServerSslHandler.scala:75)\n    at com.twitter.finagle.netty4.ssl.server.Netty4ServerSslHandler.$anonfun$initChannel$1$adapted(Netty4ServerSslHandler.scala:72)\n    at scala.Option.foreach(Option.scala:257)\n    at com.twitter.finagle.netty4.ssl.server.Netty4ServerSslHandler.initChannel(Netty4ServerSslHandler.scala:72)\n    at io.netty.channel.ChannelInitializer.initChannel(ChannelInitializer.java:113)\n    at io.netty.channel.ChannelInitializer.handlerAdded(ChannelInitializer.java:105)\n    at io.netty.channel.DefaultChannelPipeline.callHandlerAdded0(DefaultChannelPipeline.java:597)\n    at io.netty.channel.DefaultChannelPipeline.addFirst(DefaultChannelPipeline.java:178)\n    at io.netty.channel.DefaultChannelPipeline.addFirst(DefaultChannelPipeline.java:143)\n    at com.twitter.finagle.netty4.channel.Netty4RawServerChannelInitializer.initChannel(Netty4RawServerChannelInitializer.scala:54)\n    at io.netty.channel.ChannelInitializer.initChannel(ChannelInitializer.java:113)\n    at io.netty.channel.ChannelInitializer.handlerAdded(ChannelInitializer.java:105)\n    at io.netty.channel.DefaultChannelPipeline.callHandlerAdded0(DefaultChannelPipeline.java:597)\n    at io.netty.channel.DefaultChannelPipeline.addLast(DefaultChannelPipeline.java:226)\n    at io.netty.channel.DefaultChannelPipeline.addLast(DefaultChannelPipeline.java:392)\n    at io.netty.channel.DefaultChannelPipeline.addLast(DefaultChannelPipeline.java:379)\n    at com.twitter.finagle.netty4.Netty4Listener$$anon$1$$anon$2.initChannel(Netty4Listener.scala:158)\n    at io.netty.channel.ChannelInitializer.initChannel(ChannelInitializer.java:113)\n    at io.netty.channel.ChannelInitializer.handlerAdded(ChannelInitializer.java:105)\n    at io.netty.channel.DefaultChannelPipeline.callHandlerAdded0(DefaultChannelPipeline.java:597)\n    at io.netty.channel.DefaultChannelPipeline.access$000(DefaultChannelPipeline.java:44)\n    at io.netty.channel.DefaultChannelPipeline$PendingHandlerAddedTask.execute(DefaultChannelPipeline.java:1387)\n    at io.netty.channel.DefaultChannelPipeline.callHandlerAddedForAllHandlers(DefaultChannelPipeline.java:1122)\n    at io.netty.channel.DefaultChannelPipeline.invokeHandlerAddedIfNeeded(DefaultChannelPipeline.java:647)\n    at io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:506)\n    at io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:419)\n    at io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:478)\n    at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\n    at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:462)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run(BlockingTimeTrackingThreadFactory.scala:24)\n    at java.lang.Thread.run(Thread.java:748)\n^CI 0818 18:10:30.189 UTC THREAD19: Received SIGINT. Shutting down .... With log4j.rootLogger=ALL, stderr, we see:\nmaster\nbash\n$ docker run --net=\"host\" -v /home/eliza/test:/certs buoyantio/linkerd:log-all-test -log.level=DEBUG -com.twitter.finagle.tracing.debugTrace=true /certs/linkerd.yml\n...\nDEBUG 0818 20:04:08.806 UTC main: Successfully loaded the library: netty-tcnative-linux-x86_64\nDEBUG 0818 20:04:08.808 UTC main: netty-tcnative using native library: BoringSSL\nalpine\n(full startup output: https://gist.github.com/siggy/964494497529954c99832626d9eff899)\n$ docker run --net=\"host\" -v /home/eliza/test:/certs buoyantio/linkerd:openjdk-alpine3 -log.level=DEBUG -com.twitter.finagle.tracing.debugTrace=true /certs/linkerd.yml\n...\njava.lang.ClassNotFoundException: jdk.internal.misc.Unsafe\n...\njava.lang.UnsatisfiedLinkError: /tmp/libnetty-tcnative-linux-x86_646657845700759905707.so: Error loading shared library libcrypt.so.1: No such file or directory (needed by /tmp/libnetty-tcnative-linux-x86_646657845700759905707.so)\n...\nDEBUG 0818 19:51:43.378 GMT main: Unable to load the library 'netty-tcnative-linux-x86_64-fedora', trying other loading mechanism.\njava.lang.UnsatisfiedLinkError: no netty-tcnative-linux-x86_64-fedora in java.library.path\n...\nDEBUG 0818 19:51:43.380 GMT main: Unable to load the library 'netty-tcnative-linux-x86_64-fedora', trying next name...\njava.lang.UnsatisfiedLinkError: no netty-tcnative-linux-x86_64-fedora in java.library.path\n...\nDEBUG 0818 19:51:43.383 GMT main: Unable to load the library 'netty-tcnative', trying next name...\njava.lang.UnsatisfiedLinkError: no netty-tcnative in java.library.path\n...\nDEBUG 0818 19:51:43.387 GMT main: Failed to load netty-tcnative; OpenSslEngine will be unavailable, unless the application has already loaded the symbols by some other means. See http://netty.io/wiki/forked-tomcat-native.html for more information.\njava.lang.IllegalArgumentException: Failed to load any of the given libraries: [netty-tcnative-linux-x86_64, netty-tcnative-linux-x86_64-fedora, netty-tcnative]. a little more digging, it appears any linkerd h2 client configured with clientAuth exhibits this behavior.. reproduced on osx without docker:\nlinkerd\nbash\n./sbt linkerd-examples/client-auth:run -log.level=DEBUG -com.twitter.finagle.tracing.debugTrace=true\nstrest-server\nbash\nstrest-server -address=:1234\nstrest-client\nbash\nstrest-client -address localhost:4143\nlinkerd config\nyaml\nrouters:\n- protocol: h2\n  experimental: true\n  label: out\n  dtab: |\n    /svc/* => /$/inet/127.1/4144;\n  servers:\n  - port: 4143\n    ip: 0.0.0.0\n  client:\n    tls:\n      commonName: linkerd\n      disableValidation: true\n      trustCerts:\n      - finagle/h2/src/e2e/resources/cacert.pem\n      clientAuth:\n        certPath: finagle/h2/src/e2e/resources/linkerd-tls-e2e-cert.pem\n        keyPath: finagle/h2/src/e2e/resources/linkerd-tls-e2e-key.pem\n- protocol: h2\n  experimental: true\n  label: in\n  dtab: |\n    /svc/* => /$/inet/localhost/1234;\n  servers:\n  - port: 4144\n    ip: 0.0.0.0\n    tls:\n      certPath: finagle/h2/src/e2e/resources/linkerd-tls-e2e-cert.pem\n      keyPath: finagle/h2/src/e2e/resources/linkerd-tls-e2e-key.pem\n      caCertPath: finagle/h2/src/e2e/resources/cacert.pem\n      requireClientAuth: true\noutput\nD 0821 23:06:33.611 UTC THREAD317: [S L:/127.0.0.1:4143 R:/127.0.0.1:65405 S:69] initialized stream\n0821 23:06:33.613 508de878e78cfd7a.508de878e78cfd7a<:508de878e78cfd7a] Message(namer.success)\nWARN 0821 16:06:33.617 PDT finagle/netty4-8: Failed to initialize a channel. Closing: [id: 0x5ae1276b]\njava.lang.UnsupportedOperationException: JDK provider does not support NPN_AND_ALPN protocol\n    at io.netty.handler.ssl.JdkSslContext.toNegotiator(JdkSslContext.java:317)\n    at io.netty.handler.ssl.JdkSslClientContext.<init>(JdkSslClientContext.java:272)\n    at io.netty.handler.ssl.SslContext.newClientContextInternal(SslContext.java:770)\n    at io.netty.handler.ssl.SslContextBuilder.build(SslContextBuilder.java:446). error is coming from netty's JdkSslContext.java because it only supports ALPN and NPN, not NPN_AND_ALPN:\nhttps://github.com/netty/netty/blob/4.1/handler/src/main/java/io/netty/handler/ssl/JdkSslContext.java#L312\nlooks like finagle sets this protocol explicitly:\nhttps://github.com/twitter/finagle/blob/develop/finagle-netty4/src/main/scala/com/twitter/finagle/netty4/ssl/Netty4SslConfigurations.scala#L73\n. suspect ?\nhttps://github.com/twitter/finagle/blob/develop/finagle-netty4/src/main/scala/com/twitter/finagle/netty4/ssl/client/Netty4ClientEngineFactory.scala#L60\nhttps://github.com/linkerd/linkerd/blob/master/finagle/buoyant/src/main/scala/com/twitter/finagle/buoyant/TlsClientConfig.scala#L21. closing for now in favor of https://discourse.linkerd.io/t/high-cpu-and-memory-usage-with-frequent-k8s-state-changes/242. @adleong just fyi, i had consider making this part of https://github.com/linkerd/linkerd/issues/1578 but this fix does stand alone and is already in production use in the wild, so thought it prudent to get it merged.. encountered error in integration tests, investigating:\nWARN 0817 21:31:09.167 GMT finagle/netty4-8: Failed to initialize a channel. Closing: [id: 0x5c0a2612]\njava.lang.UnsupportedOperationException: JDK provider does not support NPN_AND_ALPN protocol\n    at io.netty.handler.ssl.JdkSslContext.toNegotiator(JdkSslContext.java:317)\n    at io.netty.handler.ssl.JdkSslClientContext.<init>(JdkSslClientContext.java:272)\n    at io.netty.handler.ssl.SslContext.newClientContextInternal(SslContext.java:770)\n    at io.netty.handler.ssl.SslContextBuilder.build(SslContextBuilder.java:446)\n    at com.twitter.finagle.netty4.ssl.client.Netty4ClientEngineFactory.apply(Netty4ClientEngineFactory.scala:66)\n    at com.twitter.finagle.netty4.ssl.client.Netty4ClientSslHandler.$anonfun$initChannel$1(Netty4ClientSslHandler.scala:114)\n    at com.twitter.finagle.netty4.ssl.client.Netty4ClientSslHandler.$anonfun$initChannel$1$adapted(Netty4ClientSslHandler.scala:111)\n    at scala.Option.foreach(Option.scala:257)\n    at com.twitter.finagle.netty4.ssl.client.Netty4ClientSslHandler.initChannel(Netty4ClientSslHandler.scala:111). abandoning this change, as alpine does not provide the libs we need, more details at https://github.com/linkerd/linkerd/issues/1581. @maruina @zackangelo Quick update: we're actively trying to reproduce this now.. Quick update: we're digging into this issue this week, stay tuned.. Other users asking for this:\nhttps://discourse.linkerd.io/t/targeting-a-specific-kubernetes-pod/400/6\nhttps://discourse.linkerd.io/t/grpc-http-dynamic-routing/532/\nTo help validate, here's an example that should work:\nhttps://github.com/linkerd/linkerd-examples/commit/b6f56920f979e34b90431ed696cc02b9d5740b2e\n. @ethanrubio we've posted a working linkerd+namerd+daemonset+cni+tls configuration example. would it be possible to test this configuration against your environment to confirm it works?\nhttps://github.com/linkerd/linkerd-examples/blob/master/k8s-daemonset/k8s/linkerd-namerd-cni-tls.yml. @ethanrubio yup: https://github.com/linkerd/linkerd-examples/blob/master/k8s-daemonset/k8s/namerd.yml. Let's ensure we have integration tests in place prior to making changes.. @mirosval We're looking into this this week.. @edio Quick update: we plan to look into this this week.. Quick update for folks watching this issue. We have reproduced a leak and are actively working on a fix. To confirm the leak you are seeing is the same one we have identified, have a look at your open_streams metrics. If they grow over time, that is the leak.. Quick update: we're digging into this issue this week, stay tuned.. Quick update for folks watching this issue. We have reproduced a leak and are actively working on a fix. To confirm the leak you are seeing is the same one we have identified, have a look at your open_streams metrics. If they grow over time, that is the leak.. Update on this issue. We have identified a specific leak in #1696, and are actively working to fix that. We believe this will at least fix one class of this leak, if not the full issue.\nAs mentioned earlier, if anyone can confirm they are seeing this issue with unbounded growth in the open_streams metric (or not), let us know.. @cb16 We've just released 1.3.3, containing a number of leak fixes. Would you mind confirming whether you still observer this issue?. Hi @christophetd, thanks for the detailed report. We're are currently investigating possibly related memory usage/leak issues, follow along at #1685 and #1690. Stay tuned.. Quick update for folks watching this issue. We have reproduced a leak and are actively working on a fix. To confirm the leak you are seeing is the same one we have identified, have a look at your open_streams metrics. If they grow over time, that is the leak.. Hi @christophetd, following up on this. Can you confirm whether you are seeing memory usage grow unbounded over time, or just high memory usage in general?\nA few things to try:\n- Set JVM_HEAP_MIN and JVM_HEAP_MAX environment variables to something like 256M or 512M. It's important to set them to the same value, to minimize memory fragmentation.\n- zipkin sampleRate: 1 is very resource intensive. Do you observe any difference in memory usage if you set this to 0.1 or 0.01?. Thanks for the detailed report @DukeyToo, we are investigating this.. Quick update for folks watching this issue. We have reproduced a leak and are actively working on a fix. To confirm the leak you are seeing is the same one we have identified, have a look at your open_streams metrics. If they grow over time, that is the leak.. @amitsaha We're looking for any metric ending in open_streams.\nFor example:\n- rt:client:stream:open_streams\n- rt:server:stream:open_streams\n- rt:io_l5d_mesh:stream:open_streams\n. Quick update: we are still actively investigating this. We believe it may be related to our usage of netty, in relation to reference counting, see http://netty.io/wiki/reference-counted-objects.html for more details.\nTo get additional information, if folks can try setting the environment variable LOCAL_JVM_OPTIONS to -Dio.netty.leakDetection.level=paranoid -Dio.netty.leakDetection.maxRecords=500, it should give us additional information around the nature of this leak.. @DukeyToo We have a fix up at: https://github.com/linkerd/linkerd/pull/1711 Is it possible to test this branch against your use case?. Thanks for the log @DukeyToo. Can you confirm what version of linkerd this was with?. @jftz Can you please include a bit more detail? Please try this with linkerd 1.3.5+ and provide the following:\n- linkerd config file\n- linkerd log\n- command(s) to reproduce the issue.. Ran this through our integration test suite. Results look good, memory is stable:\n\n. memory usage flat after 24 hours. that drop at the end is me forcing a gc with jmap:\n\n. Hi @sgrankin, just checking in to see how things are going. We'd love to get this merged for our 1.3.3 release this week.. Thanks for the PR @carloszuluaga, and also thanks @ccmtaylor for the reviews! Heads up we're targeting a 1.3.3 release by the end of this week. If this branch is ready by then we'd love to include it.. @carloszuluaga I agree the ci failure looks unrelated to your change. Can you hit rebuild on the circle ci dashboard? I don't have access because it's running under your user.. @carloszuluaga please send us your t-shirt size so we can thank you for this work!. closing in favor of https://github.com/linkerd/linkerd/pull/1722 (now featuring DCO check!). @dpetersen We've just released 1.3.4 with some fixes in the k8s namer. Would you mind testing with this release?. Quick update: we are continuing to investigate this issue. Our current test setup includes:\n- kubernetes 1.8.4 on GKE\n- linkerd configuration based on https://github.com/linkerd/linkerd-examples/blob/master/k8s-daemonset/k8s/servicemesh.yml\n- 1 strest-grpc client\n- 3 strest-grpc servers, redeployed every 60 seconds\n- prometheus + grafana for monitoring\nThus far we have not observed the issue. If anyone on this thread has a tighter reproduction case, we'd love to use it. Ideally a single kubernetes config file that manifests the issue.\nFor everyone's reference, here is the complete Kubernetes config file with which we are attempting to reproduce this issue:\nhttps://gist.github.com/siggy/9f283c853c3fea959a0007948d6fdcb4. Pinging this ticket to see if anyone has found a tighter reproduction case. In lieu of that, we are planning to implement #1780, which should give everyone far better insight into linkerd and namerd's internal service discovery and namer states.. @wmorgan it might.\nFor those following along at home, when you observe linkerd's routing state to be out of sync with Kubernetes, it would be interesting to observe this discrepancy directly. Some example commands to do this:\n```bash\nlinkerd\ncurl [ip-address-to-bad-linkerd]:9990/client_state.json\nkubernetes\nkubectl --all-namespaces=true describe svc\n``\n. right,BUILD.mdlooks like an appropriate place.. @klingerf this is unfortunately due to the base image upgrade from8u131-jreto8u151-jre`.\n$ docker images|grep jre\nopenjdk                     8u151-jre            c49bf7000580        3 weeks ago         538 MB\nopenjdk                     8u131-jre            02d99451775e        5 months ago        275 MB\nthese are the recommended base images from https://hub.docker.com/r/library/openjdk/. however, we could evaluate the \"slim\" or \"alpine\" variants if needed.. @hawkw h2.sh seems fine to me.. @hawkw that reminds me, update BUILD.md. thanks @dadadom ! mind adding DCO sign-off? you can do this with git commit -s. Noting UnpoolHandler is in the stack trace of each leak access record. We should consider this part of the code for removal.. Fixed in #1810. Please re-open if this issue resurfaces.. @negz this branch has been rebased on master to include #1828. Would you mind testing this?. Thanks for testing @negz ! Any update after a night of burn in?. Moving this out of 1.3.6. Let's do the finagle upgrade immediately after the release.. @fantayeneh Sorry about the DCO Bot, it's not very helpful here. It may be the malformed message in your first commit.\nSign the certificate of originSigned-off-by: Fantayeneh Asres Gizaw <fantayeneh@gmail.com>\nPerhaps try rebasing and squashing into a single commit.. Created #1840 to track subsequent work.. @jacob-go Appreciate all the detail. We're still digging into this on our side.\nThe tighter the reproduction case you can provide, the faster we can solve this. Ideally a specific set of config files and commands that reliably demonstrate the issue. For example:\n```bash\ndcos marathon app add linkerd-interal.json\ndcos marathon app add linkerd-external.json\ndcos marathon app add testapp.json\nwhile true; do curl -X POST \"http://marathon.mesos:8080/v2/linkerd/restart?force=true\" -H \"accept: application/json\"; sleep 30; done &\nhttp_proxy=$NODE:3128 curl -o /dev/null -s -w \"%{http_code}\\n\" http://testapp\n```\n... or, something like a self-contained docker-compose environment would be even better.. @jacob-go I agree that using fetch is a more elegant solution. We echo JSON in our examples because we don't want to be responsible for hosting linkerd config files for users. For your own production setup, switching to fetch is a great option.. @ylopatin-earnin Quick update: we are continuing to dig into this, stay tuned.. slow-cooker param modifications\nTesting various concurrency values:\nConcurrency: 20\nyaml\n- \"-qps=50\"\n- \"-concurrency=20\"\nbash\n2018-07-31T01:23:05Z   9440/0/0 10000  94% 10s   0 [  1  29  68  119 ]  119      0\n2018-07-31T01:23:15Z   8923/0/0 10000  89% 10s   0 [  1  55  85  252 ]  252      0\nConcurrency: 5\nyaml\n- \"-qps=200\"\n- \"-concurrency=5\"\nbash\n2018-07-31T01:26:02Z   8513/0/0 10000  85% 10s   0 [  1   1  42  111 ]  111      0\n2018-07-31T01:26:12Z   8515/0/0 10000  85% 10s   0 [  1   1  46   86 ]   86      0\n. Setting JVM_HEAP_ values improved throughput:\nyaml\nenv:\n- name: JVM_HEAP_MIN\n  value: 512M\n- name: JVM_HEAP_MAX\n  value: 512M\n2018-07-31T01:31:46Z   9959/0/0 10000  99% 10s   0 [  1   1   2   50 ]   50      0\n2018-07-31T01:31:56Z   9980/0/0 10000  99% 10s   0 [  1   1   2   26 ]   26      0\n2018-07-31T01:32:06Z   9950/0/0 10000  99% 10s   0 [  1   1   2   49 ]   49      0\n2018-07-31T01:32:16Z  10000/0/0 10000 100% 10s   0 [  1   1   2   12 ]   12      0\n2018-07-31T01:32:26Z   9930/0/0 10000  99% 10s   0 [  1   1   2   57 ]   57      0\n2018-07-31T01:32:36Z   9795/0/0 10000  97% 10s   0 [  1   1   2   81 ]   81      0. Confirmed setting FINAGLE_WORKERS to 32 (on a 24-core machine) increased throughput to nearly 1000qps.\n1000qps / Concurrency: 10\nyaml\nenv:\n- name: FINAGLE_WORKERS\n  value: \"32\"\n- name: JVM_HEAP_MIN\n  value: 1024M\n- name: JVM_HEAP_MAX\n  value: 1024M\nkubectl -n linkerd1-h1 logs -f $(kubectl -n linkerd1-h1 get po --selector=app=linkerd1 -o jsonpath='{.items[*].metadata.name}') slow-cooker\n...\n2018-08-01T22:09:05Z  10000/0/0 10000 100% 10s   0 [  1   1   2   17 ]   17      0\n2018-08-01T22:09:15Z  10000/0/0 10000 100% 10s   0 [  1   1   2   18 ]   18      0\n2018-08-01T22:09:25Z   9945/0/0 10000  99% 10s   0 [  1   1   2   55 ]   55      0\n2018-08-01T22:09:35Z   9984/0/0 10000  99% 10s   0 [  1   1   2   34 ]   34      0\n2018-08-01T22:09:45Z   9850/0/0 10000  98% 10s   0 [  1   1   2  136 ]  136      0. Increasing FINAGLE_WORKERS from 32 to 48 (on a 24-core machine) improved throughput a bit:\nyaml\n- name: FINAGLE_WORKERS\n  value: \"48\"\n- name: JVM_HEAP_MIN\n  value: 1024M\n- name: JVM_HEAP_MAX\n  value: 1024M\nbash\n$ kubectl -n linkerd1-h1 logs -f $(kubectl -n linkerd1-h1 get po --selector=app=linkerd1 -o jsonpath='{.items[*].metadata.name}') slow-cooker\n2018-08-06T22:31:13Z  10000/0/0 10000 100% 10s   0 [  0   1   2   10 ]   10      0\n2018-08-06T22:31:23Z  10000/0/0 10000 100% 10s   0 [  0   1   1   13 ]   13      0\n2018-08-06T22:31:33Z   9946/0/0 10000  99% 10s   0 [  0   1   1   65 ]   65      0\n2018-08-06T22:31:43Z   9980/0/0 10000  99% 10s   0 [  0   1   1   39 ]   39      0\n2018-08-06T22:31:53Z  10000/0/0 10000 100% 10s   0 [  0   1   1    9 ]    9      0\n2018-08-06T22:32:03Z  10000/0/0 10000 100% 10s   0 [  0   1   1    9 ]    9      0\n2018-08-06T22:32:13Z  10000/0/0 10000 100% 10s   0 [  0   1   1   10 ]   10      0\n2018-08-06T22:32:23Z   9402/0/0 10000  94% 10s   0 [  0   1   1 1043 ] 1043      0\n2018-08-06T22:32:33Z  10000/0/0 10000 100% 10s   0 [  0   1   1   15 ]   15      0. @zackangelo We're trying to pin down the exact code path that is throwing the Reset.Cancel. We've created a branch with additional logging around these code sites. If possible, can you try to reproduce with this branch, and then provide us the log output?\nhttps://github.com/linkerd/linkerd/tree/siggy/reset-cancel. hey @zackangelo just checking in to see if you were able to run this with the additional logging?. tcpdump from inside the bb-p2p-broadcast container:\nbash\ntcpdump -s 65535 'tcp port 4340' -w linkerd.pcap\nlinkerd1-lifecycle.pcap.zip\n. @zoltrain Really appreciate all this detail. We agree with your assessment that a reset is not being propagated, and/or Linkerd is not accepting a new stream. We're digging into this today.. @zoltrain quick update we are continuing to dig into this and have some promising leads, stay tuned.. @zoltrain This will definitely be in the next release. We're targeting in the next few days, stay tuned.. For reference, confirming that when maxConcurrentStreamsPerConnection is hit, Linkerd sends a RST_STREAM back to the client, with REFUSED_STREAM error.\nFrom strest-grpc:\nERRO[0056] stream.Recv() returned rpc error: code = Unavailable desc = stream terminated by RST_STREAM with error code: REFUSED_STREAM\n\n. @taer Thanks for the report. As a workaround, try setting Linkerd's JVM_OPTIONS environment variable to override the default flags:\nhttps://github.com/linkerd/linkerd/blob/b38a9a0f60fb9f92a49a0f17672928b6b02b5b77/project/LinkerdBuild.scala#L716\nhttps://github.com/linkerd/linkerd/blob/b38a9a0f60fb9f92a49a0f17672928b6b02b5b77/project/LinkerdBuild.scala#L237-L261\n. this endpoint enables users to include command line params in the gist:\nhttps://github.com/BuoyantIO/linkerd/blob/master/linkerd/admin/src/main/resources/io/buoyant/linkerd/admin/js/admin.js#L37\nlet me know if you'd still like it removed.\n. yeah, that's what i wrote initially, but then:\n[error] /Users/sig/code/linkerd/linkerd/admin/src/test/scala/io/buoyant/linkerd/admin/SummaryTest.scala:6: object TestNamer is not a member of package io.buoyant.linkerd\n[error] import io.buoyant.linkerd.{Build, Linker, NamerInitializers, TestNamer, TestProtocol, Yaml}\n. Confirming you intended these two lines to render on the same line, otherwise I think you need another line break.\n. TIOLI: I think you can do relative URLs via:\nhttps://help.github.com/articles/relative-links-in-readmes/\n...though I would understand if you wanted to keep all links consistent.\n. s/primaryB/serverB\n. s/metricsToRouterLists/metricsToRouterList\n. s/metrics/metricsJson\n. i get routerList.reduce is not a function\n. +1 it's taken me several minutes to unwind what's going on here.\n. idea to simplify this down the road: provide an endpoint that just gives a list of known routers, so the caller doesn't have to infer from the keys in metrics.json.\n. AFAICT, router and id only get used for composition in name: iface.router +\"/\"+ iface.id. What about just doing that here?\nAlso, there's a bit of duplication between the client and server blocks, mind DRY'ing that up? Conversely, the dst and server vars declared at the top are only used for a particular scope.\nAlso also, while you're in here, mind changing count to requests or something more explicit?\n. initializes\n. prepClients(clients)\n. some repetition between prepClients() and prepServers(), DRY?\n. i find this a bit hard to parse, how about:\n```\nObject.keys(metrics).forEach(function(key) {\n  var updated = false;\n  var tryUpdate = function(obj) {\n    if (key.indexOf(obj.prefix) == 0) {\n      var descoped = key.slice(obj.prefix.length);\n      obj.metrics[descoped] = metrics[key];\n      updated = true;\n    }\n  };\nrouterNames.forEach(function(rn) {\n``\n. rather than iterating over all routers/servers/clients and checkingupdated, how about more explicitly finding the object that needs updating and then returning?\n.clientanddstare being used kind of interchangeably. i get that in metrics.json it'sdst, and the ui showsclients, but perhaps pick one and use throughout?\n. all good, that's why i left the files whole and intact.\n. good catch! updated.\n. https://github.com/BuoyantIO/linkerd/blob/master/linkerd/admin/src/main/resources/io/buoyant/linkerd/admin/js/summary.js#L1\n. where exactly dostaticandunsafe` get used? related: they're nearly identical, refactor?\n. TIOLI: I get the intent here is to encapsulate implementation details within each object. I personally find this easier to follow:\nsetInterval(procInfo.update, UPDATE_INTERVAL);\nsetInterval(bigBoard.update, UPDATE_INTERVAL);\nsetInterval(interfaces.update, UPDATE_INTERVAL);\n...as it decreases the number of layers I have to browse through to get at what's happening.\n. something like Http.router.withStack(Http.router.stack.replace(PrepTls.role, PrepTls.unsafe)) is exactly what i had in mind, if not in code, in a doc or a comment, just so it's obvious how to use the thing.\nre refactor, i was thinking you could move more of the newEngine() logic into the module trait, but after giving it a go myself this morning, you're right, i don't see an obvious way.\n. okey 2 vs. 1 :+1: \n. consider .yml extension?\n. I think it's a bit confusing that these two commands do the same thing:\n./sbt examples/minimal:run\n./sbt \"linkerd/minimal:run examples/minimal.l5d\"\n...also not clear why I'm specifying minimal twice.\nPerhaps something like:\n./sbt 'project linkerd' \"run examples/minimal.yml\"\n. can this line go away?\n. maybe a comment showing the structure of the json?\n. i've had this change in my local repo for some time now. :stuck_out_tongue_winking_eye: \n. you may be able to remove this NotFoundView, since you're handling it below?\n. Is it possible to combine all of these into an Admin trait?\n. heads up merge conflict with /routers.json in https://github.com/BuoyantIO/linkerd/pull/25 /cc @esbie \n. s/AdminHttpHandler/LinkerdAdmin ?\n. Is it possible to merge in https://github.com/BuoyantIO/linkerd/pull/11 and then update the example config files with adminPort?\n. Ah, good point. Yeah, I suppose combining all the admin ones into a single trait makes sense, the others maybe not.\n. Yeah, I'd like to see it working end-to-end. I think of those example configs as part of our docs.\n. can you do $(\".nav .dropdown\").removeClass(\"hide\"); to be more explicit? or, is there a case where we're enabling hide?\nrelated: i assume this line will change / go away once we make this site-wide. if so, mind leaving a TODO here to that effect?\n. :+1: \n. related: https://linkerd.io/favicon.ico\n. I think we'll need to update this with io.l5d.serversets, and modify mention of ZooKeeper on line 174 above to say it's now supported.\n. Right. I'll go ahead and update the docs on the linkerd.io site, then we can migrate those changes back into this doc. :+1: :star: \n. these shortcuts don't work when i view this page as-is. should it be [Slack][slack]?\n. can you set this to 100% to match linkerd-site?\n. Thanks for consolidating devel back into readme, it's great having all this here.\nAs a developer I like to see this sort of  Running section, at the top. I think some of the Welcome to linkerd! stuff is useful, but then after that something like:\n```\nRunning\ndefaults\n$ ./run\nwith a specific config file\n$ ./run examples/http.l5d\nminimal build\n./sbt \"linkerd/minimal:run examples/http.l5d\"\n```\nI think all the text on sbt is great, but could work better further down the doc. Once a developer has the confidence they can compile and run, then they can go deeper with sbt.\nAlso, I think we should remove minimal build, unless there's a compelling reason to keep it. I think having build/config combos that don't work will confuse folks.\n. :heart: \n. i don't see this second param used anywhere. remove?\n. may be a bit prescriptive, but perhaps something like:\ndocker run -p 9990:9990 -v `pwd`:`pwd` -w `pwd` io.buoyant/linkerd:0.0.10-SNAPSHOT examples/thrift.l5d\n. for easier sublime rendering\n\"\"\"exec ${JAVA_HOME:-/usr}/bin/java -XX:+PrintCommandLineFlags $JVM_OPTIONS -server -jar $0 \"$@\";\"\"\"\n. consider un-linkifying these? i think it may be a bit confusing that they all point to the generic metrics page, which will display the top-most metric.\n. not for this PR, but we may want to consider providing aggregated stats across all routers further down the stack, so we wouldn't have to sum them here.\n. interfaces\n. nit: getAllServers is only used in this one place, and doesn't seem related to BigBoard. perhaps move it out?\n. i realize this was not your change, but #routing does not exist, i think we want #configuring-routing.\nsimilarly on line 237, i think we want #routing -> #basic-router-params\n. This example doesn't need to be exhaustive, but might be nice to include one client section just for demonstration.\n. maybe highlight the word client here, it's subtle that thriftFramed would go under client.\n. is thriftFramed still available on the server?\n. Can you elaborate on this? Are you referring to this being a polling model rather than event-driven?\n. I've never seen this pattern before, not that there's anything wrong with that. I had imagined just a thread with a loop+sleep. What are our other options?\n. I don't see this AppMonitor class used. Is it intended as part of the appMonitors map?\n. In the final implementation, I expected the client for the marathon rest api to live at something like:\nmarathon/src/main/scala/io/buoyant/marathon/v2.scala\n...and then this namer to live at something like:\nmarathon/src/main/scala/io/buoyant/marathon/MarathonNamer.scala\nDoes that make sense?\n. Where does _timer get used? It looks like ttl is sufficient for polling. Is this for something else?\n. I intended getAddrs to return an empty set when either:\n1) The app has no running tasks.\n2) The app does not exist.\nWill addr() = Addr.Bound(addrs) behave properly given an empty set?\nRelated: If an app goes away, do we want to stop polling for it? My reading of this code is that once an app exists we'll poll for it forever.\n. @olix0r, @esbie: this PR is ready.\nrelated: this toAddresses function is a bit hard to parse. any tips on simplifying would be much appreciated.\n. Wildcards and variable capture are allowed ?\n. is this intentional or did you mean ${host}?\n. Good point, I'll update config.md with some of this detail.\nThat API prefix can vary depending on how marathon is configured. \nFor example, running locally based on these instructions:\nhttps://mesosphere.github.io/marathon/docs/\n... there is no API prefix, you'd do:\n$ curl localhost:8080/v2/apps\nUsing the default DCOS / AWS setup:\nhttps://mesosphere.com/amazon/\n... you'd do:\ncurl $(dcos config show core.dcos_url)/marathon/v2/apps\n. yeah, i originally wrote it that way. @olix0r mentioned that it's preferable to tie AppIdNamer to a specific API version.\n. hmmm, this is a bit weird because the function both retrieves an address, and also initiates an ongoing watch as a side-effect. we could call it getMonitoredAddrAndStartWatching, but i'm inclined to just leave it as-is, to at least be explicit about the side-effect part.\n. ok i get that. updated with a super-duper-explicit getAndMonitorAddr.\nalso your comments made me think of this:\n\n. Per offline discussion, we want hex-encoded paths with slashes, at marathon enforces application IDs prepended with slashes. I've updated the PR per the rest of the feedback.\n. JsonProperty and Path unused\n. maybe put this back at the same level as baseDtab in the ext-http server, and then add /ext/http => /host/web; to it's dtab. i think this demonstrates nicely the distinction between an \"edge\" linkerd and an internal one.\n. for reference, in sublime: \"ensure_newline_at_eof_on_save\": true\n. move to client section?\n. it's nice that this is deterministic, but i'm concerned about cases where there are only two clients and they both end up the same color. we don't have to address this in this PR, but i think it should be addressed.\n. the router clients and related sections auto-update when clients are added/removed. i think the chart should at least add clients as they are created, removal is less important.\n. nit: ws\n. just confirming that we'll eventually seed the initial page load with this first call to metrics.json, rather than \"fill in content\" after a second call?\n. just curious: why not do success: render ?\n. that makes sense, agree routers.update(metrics) wouldn't make sense inside a render() method.\n. pretty sure it was scalariform, reverting may be futile, my system just made this change on the last 2 branches i tested.\n. there are some bits to do similar things with success rates in utils.js. consider re-using or consolidating?\n. no no, that's all good. look forward to all the forthcoming code deletion!\n. hmmm, could it be because ensurePath doesn't have a return type?\n. i don't see withRouters, allMetrics, withClient, withClients used. are these coming?\n. are you saying this is repeated in the namerd section and should be DRY'd up? if so, i agree. if not, i think it should.\n. now that requests is deterministic based on success+failure, the requests element of this data structure is redundant information. consider removing and allowing the caller to perform the addition?\n. i originally wrote it that way, changed it to finagle Paths after first review... https://github.com/BuoyantIO/linkerd/pull/225#discussion-diff-58152545\n. yeah, i was about to do that, then i noticed linkerd:v0.0.10-SNAPSHOT a few lines above. i think the whole thing should be automated: #278 \n. Two alternate ways. I'll make that more clear.\n. agree: #279 \n. i find this personally helpful for remembering how to run this stuff. it's possible we could move it into a separate file if we want to keep the README lean, but i'd like it documented somewhere.\n. maybe default to most restrictive?\n. i guess we want to support this use case, but i imagine folks may not be comfortable putting their password in the clear in our config files. a better option would be reading an env var, not sure the best way to accomplish.\n. per offline discussion, onboard with this for now.\n. agree with you both, ticketed here:\nhttps://github.com/BuoyantIO/linkerd/issues/326\ni thought about going down this route, but decided to limit the scope of this PR to just not introducing any more dependencies.\n. agreed: https://github.com/BuoyantIO/linkerd/issues/327\n. selfishly i find this quite helpful. i think for the uninitiated, it gets folks up and running very quickly, without having to know much about sbt. i'd like developers to go from clone to admin page in as few seconds as possible.\n. yup, agreed. fortunately all the code is in one place. ;)\nhttps://github.com/BuoyantIO/linkerd/issues/328\n. Fair point. I've noted this in #326. We may or may not bundle all that work together, but at least it's documented either way.\n. Fair, I've added links from here to those other two README's. I'd still like to keep these commands front and center, as it's the first thing folks see when they hit the repo.\n. tioli:\n* Pending-->Ok\n  * |         ^\n  * v        /\n  * Failed--`\n  * `-^\n. change this to random or hardcode 4001 below.\n. more specifically, code inside the new BuoyantMainTest { clause is not getting run. i tried to piece together when in git history this could have happened, may have been in a previous repo.\n. yeah, where debugTrace itself defaults to false. wasn't sure how explicit to be here. i'll update.\n. lol u guys https://github.com/BuoyantIO/linkerd/pull/354#discussion_r61657709\n. does this mean we can delete logo.svg?\n. would you mind adding cursor: pointer; ?\n. tioli: consider moving the register/deregister calls into toggleClientDisplay and removing show/hideClient methods.\nif this could cause handlers to be registered more than once, we could modify registerListener to check for existence before adding.\nif that causes registerListener to be expensive, maybe we could change its underlying implementation to a hash?\nrealizing this may be a lot of work as i'm typing this, just some things to think about.\n. hmm, yeah, that all makes sense. my thinking was that if the state of client display is tightly coupled to register/deregister, then be maximally explicit about that. if i were altering this code, i wouldn't be sure whether to call toggleClientDisplay directly or show/hide client. by removing that extra level of indirection, i wouldn't have a choice, which i think is a good thing.\nall that said, this is a minor thing, and i understand the intent. i think it's totally cool to leave as is, but thank you for the explanation!\n. namers ?\n. namers ?\n. a few lines above, replace [announce](config.md#announce) with \n[announce](#binding-cache) ?\n. is a transparent?\n. i was torn between having this return a fresh token, or just updating authToken, so i'm doing both. thoughts?\n. getAppIds and getAddrs are quite similar, though i could not come up with a great way to share code. ideas welcome.\n. alex and i discussed the right place for the environment variable parsing code. it's config-specific, which motivates it being here. it's also api-specific, which would motivate putting it down in Api.\n. making readJson public makes me think this could be useful across the broader codebase. thoughts about moving it out into some utility code?\n. is this too generic? considered calling this AuthConfig.\n. https://github.com/BuoyantIO/linkerd/issues/762\n. fair point, nothing i suppose, removed.\n. correct. we don't know a token has expired until we get UnauthorizedResponse, then we refresh and retry once. if it gets UnauthorizedResponse again something else is wrong.\n. importing RichActivity creates a dependency from linkerd (or, marathon namer, specifically) to namerd. is that ok? should RichActivity be moved somewhere more generic?\n. @jgehrcke that's definitely a nice optimization, thanks for pointing that out! will either change it here or in a subsequent PR.\n. ticketed at https://github.com/BuoyantIO/linkerd/issues/782\n. I think of both FA and HC as things that control where RPC calls can go based on some kind of health calculation. My goal here is to inform the user that it's okay, arguably preferable, to leave this disabled, as linkerd already provides a mechanism to handle unhealthy nodes.\nMarathon's default health check interval is 60 seconds, my comment about FA being more aggressive assumes high RPS with this default.\nWithout mention of FA, it may not be clear to the reader why you would ever leave this disabled. If there's another way to communicate this, I'm up for it.\n. Good point, I'll check.. From \nhttps://github.com/scala/scala/blob/v2.9.2/src/library/scala/collection/JavaConverters.scala#L330\nThe returned Scala <code>Iterator</code> is backed by the provided Java <code>Iterator</code>\nAlso, the ConcurrentHashMap section of https://twitter.github.io/scala_school/concurrency.html seems to imply thread-safety.. That's correct. The UUID is intended to distinguish the same stat coming from different linkerd processes. This can be useful if a single instance of a service is unhealthy. To evaluate the overall health of a service, these stats are intended to be rolled up, ignoring linkerd UUID.\nDoes this make sense in the StatsD world? In testing a StatsD+Graphite setup, I did not see another way to distinguish stats coming from multiple linkerd processes. As a counter-example, Prometheus identifies metric sources by IP address by default.. @adleong, @olix0r: thoughts on an experimental flag for telemeters?. fwiw i'm considering rolling this particular change back in favor of simple counter sampling.\ninitially i liked the idea of getting full-fidelity counters via batching, but counter sampling seems to be the statsd way. it will also will make this code simpler and avoid adding state to linkerd.. I think we're good wrt histograms? The statsd server handles histogram calculation. On our side we simply implement Stat.add() to forward the value to statsd.. yup, i grabbed all of it from an error output. i'll move it into its own file and use real var names for the relevant methods.. we don't replace slashes, there's a / in that regex. after pushing this change i actually considered writing it as [^/A-Za-z0-9] to make it more obvious for this very reason. will update.. there's a return _.sortBy(clients, 'label'); on line 204 of routers.js. is this sorting twice?. nit: i see routers.clients(routerName) repeated (and also assignned to clients), consider DRY up.. consider a comment here and/or in AdminHandler.scala that the nav's should stay in sync.. This may be moot given alex's ticket above, but consider just passing namerdNav into the html method, and generating the navBar using that. This way, AdminHandler can go back to being an object. Or, consider moving all the methods that do not depend on state into an object.. it feels a bit weird to have plugin-specific code here.. i considered passing UsageDataHandler into UsageDataAdminHandler and generating the usage json payload server-side, but i think leveraging the templating/js infrastructure on the client-side makes more sense for this use case.. right, i guess my question going forward is: should each project be responsible for serving its own static assets? today admin is the only project doing this.. quantile, per https://prometheus.io/docs/concepts/metric_types/#summary. looking at the go_gc_duration_seconds summary, maybe we should set min as quantile=\"0\" and max as quantile=\"1\"?\n```\nHELP go_gc_duration_seconds A summary of the GC invocation durations.\nTYPE go_gc_duration_seconds summary\ngo_gc_duration_seconds{quantile=\"0\"} 6.409200000000001e-05\ngo_gc_duration_seconds{quantile=\"0.25\"} 0.00010941100000000001\ngo_gc_duration_seconds{quantile=\"0.5\"} 0.000138934\ngo_gc_duration_seconds{quantile=\"0.75\"} 0.000173474\ngo_gc_duration_seconds{quantile=\"1\"} 0.00030891400000000003\ngo_gc_duration_seconds_sum 0.29322348800000003\ngo_gc_duration_seconds_count 1854\n``. Seeing this now, I think these two cases are sufficiently different that they warrant different variants ofAuthRequestandAuthenticatedobjects (and possiblyMarathonSecret). A lot of the logic inAuthenticatedis specific toDCOS_SERVICE_ACCOUNT_CREDENTIAL`, primarily because there are additional requests required to obtain a token, where in the basic auth case you already have a token.\nRecommend modifying the code around MarathonSecret.load() to something like:\nscala\nval service = MarathonSecret.load() match {\n  case None => client\n  case Some(secret) =>\n    secret match {\n      Some(secret.login_endpoint) =>\n        val auth = MarathonSecret.mkAuthRequest(secret)\n        new Authenticator.Authenticated(client, auth)\n      Some(secret.http_auth_token) =>\n        new BasicAuthenticator.Authenticated(client, secret.http_auth_token)\n    }\n}\nI realize this contradicts my earlier guidance to share the code where req.headerMap.set(\"Authorization\", ...) is called, but this wasn't clear to me until actually seeing this implementation, apologies for the extra work. I do think keeping all the logic here, and not in the API code, is a much better approach, so thank you for moving in this direction. It's possible we could still end up sharing that piece of logic with some kind of trait shared between Authenticator.Authenticated and BasicAuthenticator.Authenticated, but I wouldn't optimize for that approach until seeing how the implementations shake out.. i think we can rework some of the env var checking code and forego overloading this struct to serve both the basic and dc/os auth cases. this can remain specifically for dc/os auth.. i think this section may be a bit more readable if we move the env var checking into here, and make the load() method specific for dc/os auth. something like:\n```scala\n(sys.env.get(DCOSEnvKey), sys.env.get(basicEnvKey)) match {\n  case (Some(json), _) =>\n    val secret = MarathonSecret.load()\n    val auth = MarathonSecret.mkAuthRequest(secret)\n    new Authenticator.Authenticated(client, auth)\n  case (None, http_auth_token) =>\n    val filter = BasicAuthenticatorFilter(http_auth_token)\n    filter.andThen(client)\n  case (None, None) => client\n}\n```. i think we can do away with most of the state management, as most of that was motivated by the dc/os auth module hold the state of the authentication flow.\nscala\nclass BasicAuthenticatorFilter(http_auth_token: String) extends Filter[http.Request, http.Response] {\n  def apply(req: http.Request, svc: Service[http.Request, http.Response]): Future[http.Response] = {\n    req.headerMap.set(\"Authorization\", s\"Basic $token\")\n    svc(req)\n  }\n}\n. i get:\n[error] /Users/sig/code/linkerd/telemetry/influxdb/src/main/scala/io/buoyant/telemetry/influxdb/InfluxDbTelemeter.scala:116: discarded non-Unit value\n[error]       sb.append(\"\\n\")\n[error]                ^\nanother option would be to do:\nval _ = sb.append(\"\\n\"). I got it working with:\nval first = (tuple: (String, String)) => { tuple._1 }\n... it was complaining about the tuple not being named. if you know a better way i'm open to it.. That's correct. read on for the gritty details:\nAlthough we're providing an endpoint in LINE format, InfluxDB expects data to be pushed, so we need Telegraf. Telegraf has a number of input plugins, such as Prometheus, which, given a list of endpoints, collects data and automatically tags them with the source. Surprisingly, there is no input plugin specifically for LINE endpoints, so we must set the host tag ourselves. There is an open issue in Telegraf for this:\nhttps://github.com/influxdata/telegraf/issues/813\nBecause there is no LINE input plugin in Telegraf, we rely on the inputs.exec input plugin, which executes arbitrary commands, like this:\nhttps://github.com/linkerd/linkerd-examples/pull/124/files#diff-898d1cb8b1ac4b01cdfca408c9069081R2\n. will add some comments.\nre: LinkerdBuild.scala, i tried to pick a file which, when changed, would best motivate invalidating ~/.ivy2 in the circleci cache. if there's a better file, i'm open to it.\nsame comment for package.json, which motivates invalidating admin/src/main/resources/io/buoyant/admin/node_modules.\nrelated: any idea why we weren't caching ~/.ivy2 in circleci 1.0? it's a huge time saver.. ok makes sense. i think there's a way to key off of entire directories. will check.. possibly naive question: rather than keeping track of expired clients, is it possible to just remove all references to them?. ah, that's an interesting use case. what's the TTL on clients? i suppose the longer it is, the more ok it is to allow the color to change? i guess it's really a question of trading code complexity here for user friendliness in the ui.. AFAICT this was just a placeholder, as io.l5d.mesh wasn't supported on the server?. Similar to the other comment, my reading of the code was that we were only supporting TLS for the thrift interpreter, so in adding http and mesh, it made sense to make all 3 the same. Open to discussion though.. That's correct. Note the changes in HttpControlServiceConfig.scala. It does work, tested with:\nbash\n./sbt \"namerd-examples/tls:run -log.level=DEBUG -com.twitter.finagle.tracing.debugTrace=true\"\nbash\n./sbt \"linkerd-examples/namerd-tls:run -log.level=DEBUG -com.twitter.finagle.tracing.debugTrace=true\"\nHowever, I've updated mesh interface to use ApplicationProtocolNames.HTTP_2 and Netty4ServerEngineFactory(), and added a test to verfiy Netty4ServerEngineFactory.. You're correct we could avoid passing it through. The call sites are different enough where I thought this approach minimized code complexity, since we already are passing aplnProtocols through anyway.. this feels a bit overloaded. is there some way to factor out the common bits?. not sure i fully grock what's going on here. i see two calls to onAddedClients:\n1. metricsCollector.onAddedClients(addServices)\n1. metricsCollector.onAddedClients(addClients);\n... and the functions passed in have different signatures:\n1. function addServices(addedClients, metricsJson)\n1. function addClients(addedClients)\nis this expected?. in the istio/api repo i see this line as:\nimport \"google/protobuf/duration.proto\";\nis it possible for us to conform to a directory structure such that the files will not require modification?\nrelated: for the istio telemeter i'm going to need proto files under api/mixer/v1/config, can we move http_fault.proto and friends down into a directory structure that mirrors the api repo they came from?. Thanks for the feedback @ZackButcher ! I didn't realize the global dictionary was embedded in Mixer, we'll remove this duplication in that case. We may still end up reading/importing global_dictionary.yaml since we'll need to reference those strings in Attributes, but good to know we don't need to send them over the wire.. Makes sense. Our plan is to eventually bundle multiple Attributes, and will switch to default_words in that case. This code as it exists right now is mostly boiler to confirm we are speaking gRPC correctly. Thanks again!. yeah! i had thought about moving the body into IstioLogger to make MixerClient not http1-specific, so rather than logHttp, we'd have:\nMixerClient {\n  def log(rsp: ResposeRequest)\n}\nmy only concern was exposing the IstioLogger to the Mixer protobuf, but perhaps that's better than exposing MixerClient to all the http1-specifics ?. it was for istio > 0.1.6. i'll remove it for now.. Spent some time with @adleong trying to implement override def router(params: Stack.Params): Router. At this point in the code we don't have access to the stack without plumbing it through a few layers. Is this change warranted? Are there better approaches?. sort of related: how about serving on the existing admin ip:port ? could be useful/simpler for the admin ui down the road.. Hmm, what about serving it at a particular URL, consuming the expected path, and then using the rest of the path for identification? From a user's perspective this would feel similar to the delegation endpoints.\nMy primary concern is when we need this info (via admin ui or curl) our users will need to redeploy with a new port open in their k8s/ELB/whatever, and/or we update linkerd-examples to expose this new port. I guess this comes down to internal complexity vs. user complexity?. nice!. you may have already looked at this, but any thoughts on sharing code with HttpLoggerConfig? they're pretty similar, though i imagine the Request, Response types could be a pain to abstract.\nsame question for the IstioLogger classes.. nit, and realize this was already here, but, should this be value or stat or something, not mem?. i think if it was absent, it returns null, from:\nhttps://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ConcurrentHashMap.html\nReturns:\nthe previous value associated with the specified key, or null if there was no mapping for the key. i suppose that will work. it seems like we're trading a map.get on every call vs. an unnecessary allocation on some calls? i'd assume map.get is cheaper than allocation? what's the intent here?. is see, makes sense.. i would just hardcode this as https://github.com/linkerd/linkerd/releases, as it's something that shouldn't vary from branch-to-branch.. this section below should be updated/replaced. ah, the Deprecation Notice is good. i was referring to the section titled How to check ThirdPartyResource is enabled. if we're still supporting ThirdPartyResource's for k8s <= 1.7, i suppose this gets left as is?. CustomResourceDefinition. set your editor to add newlines to the end of files.. For *more. the block below is json ;). unneeded.. i get:\nPicked up JAVA_TOOL_OPTIONS: -Dfile.encoding=utf8\n\u28f7 sbt \u28f7 [info] Loading global plugins from /Users/sig/.sbt/0.13/plugins\n\u28f7 sbt \u28f7 [info] Loading project definition from /Users/sig/code/linkerd/project\n\u28f7 sbt \u28f7 [info] Compiling 2 Scala sources to /Users/sig/code/linkerd/project/target/scala-2.10/sbt-0.13/classes...\n\u28f7 sbt \u28f7 [warn] /Users/sig/code/linkerd/project/Base.scala:32: trait Build in package sbt is deprecated: Use .sbt format instead\n\u28f7 sbt \u28f7 [warn] class Base extends Build {\n\u28f7 sbt \u28f7 [warn]                    ^\n\u28f7 sbt \u28f7 [warn] one warning found\n\u28f7 sbt \u28f7 [info] Resolving key references (52323 settings) ...\n\u28f7 sbt \u28f7 [info] Set current project to all (in build file:/Users/sig/code/linkerd/)\n\u28f7 sbt \u28f7 [error] No such setting/task\n\u28f7 sbt \u28f7 [error] linkerd:compile\n\u28f7 sbt \u28f7 [error]                ^\nwhat about ./sbt linkerd/assembly ?. on my system i needed to to wget -O $H2SPEC_FILE... otherwise i end up with crazy long filenames with Amz-Credential's in it.. check for nghttpd on the system?. this swallowed an error, as $H2SPEC_FILE was not present.. consider LOGFILE =$(mktemp -t l5d-h2spec.log), to avoid gitignore and overwriting.. I think I'd just prefer echo \"${LOGFILE}\", as the logs are so noisy, and what I really want to see is the h2spec output.. consider checking for which h2spec, and, if not found, put it into ~/bin, and let the user know it's there. this also avoid gitignore foo.. dow nload. i'd say forego this step, as i would not expect h2spec to be in the linkerd dir.. i'd put the tar into a temp dir so it gets cleaned up:\nbash\nTMPDIR=$(mktemp -d -t h2spec)\nwget -O \"${TMPDIR}/${H2SPEC_TAR}\" .... recommend putting h2spec into ~/bin, as it's less invasive, and we may not have perms to for /usr/local/bin.. should be able to remove all changes to .gitignore.. it's not necessarily, but if you put it there, you can at least reference it directly.. sure, i don't think we want to modify the path, but checking for that install location seems reasonable.. no more than checking for ./h2spec ;). affect. nit, extra whitespace: >  show. curious the motivation here? what's log level usually set at? i see other tests setting it to OFF, TRACE, or ALL. \ud83d\udc4d . Ah, nice catch. I'm going to revert the Long -> Int change, and handle it in withEndpointParams.. ",
    "klingerf": "I might suggest calling the new role that you're adding \"AuthTokenFile\" instead of \"AuthToken\", but otherwise lgtm. :star: :ship: \n. This looks good to me -- much cleaner! :star: :honey_pot: \n. I still see two commits in this branch, with the same commit message.  Is that intended?\n. :star: :rocket: \n. @olix0r I took your suggestion on object grouping for common targets.  It basically works, but all of the tasks still have to be defined as top-level vals in the build file in order for them to be registered with SBT.  So see line 200 in the Build.scala file, and let me know if you think that's an acceptable approach.\n. :star: <= rebased\n. I do see the same failure as CI locally, if I run git clean -dfx . before ./sbt e2e:test.  Is it possible you have a cached version of the thrift scala bindings that are being picked up by sbt?  I don't immediately see the fix for the issue, but it will probably require some sbt rejiggering.\n. :star: for the rebase -- thanks for updating!\n. :+1: to @adleong's suggestion.  I think this branch would have been easier to review with the following renames:\n```\ngit mv router/http/src/main/scala/io/buoyant/router/http/Headers.scala \\\nlinkerd/protocol/http/src/main/scala/com/twitter/finagle/buoyant/linkerd/Headers.scala\ngit mv router/http/src/e2e/scala/io/buoyant/router/HttpEndToEndTest.scala \\\n  linkerd/protocol/http/src/e2e/scala/io/buoyant/linkerd/protocol/HttpEndToEndTest.scala\n```\n(assuming you meant to remove router/http/src/e2e/scala/io/buoyant/router/HttpEndToEndTest.scala, per my other comment)\n. Ah I see -- looks like the change in nesting in that file caused git to consider it an add/delete rather than rename.  My mistake.\n. Looks good to me. :star: :yen: \n. :star: :baseball: \n. This looks good to me. Can you add a test for the thrift identifier that you're adding, similar to router/http/IdentifierTest.scala?\n. Looks good to me. :star: :eyeglasses: \n. This looks great -- nice work! Totally tioli on my comment about imports. :star: :four_leaf_clover: \n. Thanks for updating! :star: :star: :star: \n. Thanks for submitting this issue, @caniszczyk! This is fixed by #12.\n. Cool -- this approach looks good to me, and will review fully once you've addressed the comments you left on Base.scala.\n. :star: \n. @siggy Thanks for reviewing. I took your suggestions, but I think we should add the CLA in a follow-up branch.\n. Have added a link from CONTRIBUTORS.md to https://buoyant.io/cla/.  Have also added a section about cleaning up git commit history, moved/added badges in the readme, and switched to relative links where appropriate. Will squash the commits on this branch prior to merging.\n. :star: <= rebased\n. Looks good! Thanks for updating. :star: :japanese_castle: \n. :star: \n. Looks good to me! Nice work. :star: :dancers: \nFwiw, I think it's fine to duplicate SetHostFilter to avoid a shared dependency, and I don't have any opinion on gitignore :-)\n. This looks great, but I think TlsPrep needs a test associated with it.\n. Cool, yeah, makes sense.  I'm good either way.\n. Looks good, and adding tests in a follow up branch works for me. I think you just need to squash history and this should be good to go. :star: :electric_plug: \n. Looks good to me! :star: :banana: \n. If it's not a ton of work I think it still might be worthwhile to default thriftFramed and thriftProtocol on the client to to match whatever is set on the server.  If somebody sets the server params but not the client params, then the client params should just inherit the server settings.\n. Fair enough, I forgot about the multiple servers case.\n. Thanks for updating! I think this needs one more rebase to pickup #29, but otherwise looks good to me, :star: :dragon: \n. :star: \n. I think this branch needs to merge master to pickup the changes from your lodash branch, but otherwise looks good to me. :star: :grapes: \n. Great, thanks! :star: \n. This looks like a huge improvement! :star: :frog: \n. Looks good! :star: :dromedary_camel: \n. :star: :linkerd:\n. :star: :soon: :linkerd:\n. :star: :art: \n. This is great! All of the additional info about sbt is really helpful. I just left some nit picky comments here and there. :star: :truck: \n. Looks good! :star: :honey_pot: \n. :star: :point_up: \n. :star: :back: \n. @olix0r Have updated with your suggestion. Looking at recent CI runs, I think this change adds about 45 seconds on each run.\n. :star: rebased\n. Fascinating :star: :cactus: \n. This looks good to me. I tested with the books app and the dtab playground works as expected! :star: :candy: \n. This looks good to me. Thanks for updating! :star: :sheep: \n. You could squash history on this branch, but would also be acceptable to just merge it straightaway. :star: \n. Looks good to me -- thanks for updating. :star: :snake: \n. :star: :pager: \n. When I build the image and test locally, I get:\n$ docker run --rm io.buoyant/linkerd:0.0.10-SNAPSHOT -help\n/io.buoyant/linkerd/0.0.10-SNAPSHOT/bundle-exec: 2: exec: /bin/java: not found\nI'd expect that to display the help message.  I think the issues is that your java_exec script does not set JAVA_HOME if it's not already set.\nif [ -z \"$JAVA_HOME\" ]; then\n    JAVA_HOME=/usr\nfi\nFWIW, this works:\n$ docker run --rm -e JAVA_HOME=/usr io.buoyant/linkerd:0.0.10-SNAPSHOT -help\n-XX:InitialHeapSize=16321536 -XX:MaxHeapSize=261144576 -XX:+PrintCommandLineFlags -XX:+UseCompressedClassPointers -XX:+UseCompressedOops \nFeb 02, 2016 7:24:49 PM com.twitter.finagle.http.HttpMuxer$$anonfun$4 apply\n...\n. Cool, looks good to me. Thanks for updating. :star: :running_shirt_with_sash: \n. Looks good! I think you need to rebase and pick up #67 to fix your failing CI run, but otherwise I say go for it. :star: :fireworks: \n. :star: pending passing CI\n. Quick question about the config file format for TLS.  I notice that all of your example configs look something like:\nrouters:\n- kind: http\n  tls:\n    kind: boundPath\nBut in master right now, we use protocol for each router, not kind, in which case the config ends up looking like:\nrouters:\n- protocol: http\n  tls:\n    kind: boundPath\nSince TLS is itself part of the protocol, how would you feel about nesting all of the tls options under the protocol section?  Having a protocol and a separate tls section feels confusing to me.\n. Yep, or we could keep the \"kind\" field, like:\nrouters:\n- protocol:\n    kind: http\n    tls:\n      kind: boundPath\n  servers:\n  - port: 0\n. I like it! :star: :dango: \n. :star: \n. Looks good to me! :star: :cocktail: \nJust a clarifying question (please don't go implement this :smile:) -- the default state of each page is to have \"ALL\" selected when it is loaded, right?  If I select one of the routers from the dropdown and then click to a different page, the dropdown reverts to ALL?  That's at least the behavior I was seeing and just wanted to confirm it's correct.\n. @esbie Thanks for the explanation! Yep, I figured it was complicated. This looks good to me. :star: :video_game: \n. Great, thanks for updating. :star: :flags: \n. Looks good! And thanks for the scaladocs updates. :star: :tea: \n. This seems great pending the one change that you called out. :star: :crystal_ball: \n. Thanks for the explanation. :star: :rocket: \n. Maybe we should call the new section clients: instead of client:?  I could go either way.\n. Yep, makes sense to me.\n. Looks good! :star: :satellite: \n. Ok, it looks like the root cause of this issue is actually #82.\n. Addressed @olix0r's feedback and rebased. :star: \n. :star: rebased (again)\n. Looks good to me. :star: :sake: \n. :sob: :star: \n. Good catch. :star: :panda_face: \n. :star: \n. :star: :hibiscus: \n. Looks good -- thanks for updating! :star: :100: \n. Looks good, totally optional style comment. :star: :panda_face: \n. :star: :boat: Thanks for making that update.\n. @olix0r Good suggestion -- have added wrapping as part of this branch.\n. Cool -- looks good to me. :star: :musical_keyboard: \n. :star: :musical_keyboard: \n\n. Ok, have addressed all of the review feedback in the most recent commit on this branch. Please take another look.\n. @olix0r d'oh. will squash that commit when this is ready to go.\n. Have fixed long lines and unused imports in the latest commit.  Running some external validations of this branch now, then will rebase before merging.\n. :star: <= rebased\n\n. :star: :horse_racing: Thanks for cleaning this up!\n\n. :star: :fireworks: First non-employee contribution! And you even signed the CLA -- nice work!\nI tested this locally and it works for me.  Mind also adding a bullet point in CHANGES.md that mentions the new config param?\n\n. @liamstewart I knew I was forgetting something. Can you also update this section of config.md to document the new param? Should be very similar to the documentation that we already have for thriftFramed:\nhttps://github.com/BuoyantIO/linkerd/blob/master/docs/config.md#thrift-protocol-parameters\n. :star: Looks good! Thanks for making those updates.\n\n. :star: :dango: Thanks for updating -- looks good to me.\n\n. When I try to use the fs namer with this branch, I get \"java.util.ServiceConfigurationError: io.l5d.fs not a subclass of io.buoyant.linkerd.NamerInitializer\".  Full error pasted below.\nkl ~/workspace/linkerd (greg/config-tweaks) $ ./sbt 'examples/http:run'\nPicked up JAVA_TOOL_OPTIONS: -Dfile.encoding=utf8\n[info] Loading global plugins from /Users/kl/.sbt/0.13/plugins\n[info] Loading project definition from /Users/kl/workspace/linkerd/project\n[info] Set current project to all (in build file:/Users/kl/workspace/linkerd/)\n[info] Running io.buoyant.Linkerd examples/http.l5d\nFeb 23, 2016 1:35:49 PM com.twitter.finagle.http.HttpMuxer$$anonfun$4 apply\nINFO: HttpMuxer[/admin/metrics.json] = com.twitter.finagle.stats.MetricsExporter(<function1>)\nFeb 23, 2016 1:35:49 PM com.twitter.finagle.http.HttpMuxer$$anonfun$4 apply\nINFO: HttpMuxer[/admin/per_host_metrics.json] = com.twitter.finagle.stats.HostMetricsExporter(<function1>)\nI 0223 21:35:49.962 THREAD53: linkerd 0.1.1-SNAPSHOT (rev=db4602ae88ffe502a100c7943bb87c88b8f3840f) built at 20160223-133548\nI 0223 21:35:50.143 THREAD53: Finagle version 6.33.0 (rev=21d0ee8b5070b735eda5c84d7aa6fbf1ba7b1635) built at 20160203-202859\njava.util.ServiceConfigurationError: io.l5d.fs not a subclass of io.buoyant.linkerd.NamerInitializer\n. Looks like ./sbt integration:test is failing with compilation errors on this branch. Mind investigating?\n. :star: :fireworks: Looks great -- thanks for updating!\n\n. :star: :zap: Thanks for fixing!\n\n. :star: :night_with_stars: Looks good to me.\n\n. :star: :bicyclist: Looks great! Thank you for updating.\n\n. :star: \n\n. :star: :hammer: Thanks for fixing this! I ran it through our acceptance test and it passed.\n\n. :star: :tada: Looks good. Tiniest of all tiny nits: your last bullet point is missing a period.\n\n. :star: :boom: \n\n. :star: :computer: Looks good.\n\n. :star: :turtle: Looks good. :+1: to @siggy's comment about config.md.\n\n. :star: :tada: \n\n. :star: Thanks for updating.\n\n. :star: :insecure: :star: \n\n. :star: Looks good to me.\n\n. :star: :tanabata_tree: Thanks for updating -- I like your fix a lot.\n\n. :star: \n\n. :star: :frog: \n\n. :star: :stars: Looks good. Just noting that changing the nesting on httpUriInDst is a breaking config change and will require a minor version bump when we release.\n\n. :star: :umbrella: \n\n. :star: :camel: \n\n. :star: :tangerine: Looks good. In the past I've used the Graphviz Brewer color schemes, but they're kinda retro... http://www.graphviz.org/doc/info/colors.html#brewer\n\n. Great, thanks for updating.\n. :star: :stars: Looks great -- thanks for tracking that js issue down.\n\n. :star: :racehorse: Looks good to me -- thanks for updating.\n\n. :star: :ship: Looks great -- thanks for updating!\n\n. :star: :shell: Nice cleanup!\n\n. :star: Looks good to me -- thanks for making those changes.\n\n. :star: :night_with_stars: \n\n. :star: :wrench: \n\n. :star: :white_flower: Nice -- looks good to me.\n\n. :star: :stars: Looks good to me.\n\n. :star: :balloon: Looks good to me.\n\n. :star: :sheep: Cool -- looks good to me.\n\n. :star: :honey_pot: Cool, I like this approach a lot.\n\n. :star: :rocket: Works for me.\n\n. :star: :flags: Thanks for updating.\n\n. :star: :panda_face: \n\n. :star: :bike: Looks good! My comments were totally optional.\n\n. :star: :night_with_stars: \n\n. :star: :green_heart: \n\n. Maybe put the bullet about namerd at the top, with special emoji? It's pretty special after all.\n. :star: Great, thanks for updating.\n\n. This was fixed by #261 -- closing.\n. :star: :sparkles: \n\n. I like this a lot. Just had that one comment about chained calls.\n. @esbie Thanks for updating! Am surprised that you need the _.isArray check throughout. What if you dropped support for the \"*\" character, and just reset to empty array in that case?  Then could you assume that metricNames and routerLabels are always arrays and skip the check? This is a big gray area of js for me, so apologies if I'm on the wrong track.\n. :star: :seedling: Looks good to me -- thanks for updating.\n\n. :star: :sparkler: Looks good to me, but I think it (now) needs a rebase.\n\n. :star: :turtle: Looks good to me, modulo @rmars's comment.\n\n. :star: :dragon: Cool, looking good to me.\n\n. \u2b50 \u26f5 Looks good to me.\n. \u2b50 \ud83c\udf20 \n. :star: :rabbit: Just had that trivial nit.\n. :star: :rabbit:\n\n. :star: :flags: Thanks for updating!\n\n. \u2b50 \ud83c\udf31  Looks good to me -- I just left some optional organizational comments, but didn't see any issues with the code.\n\n. \u2b50 Thanks for making those updates.\n\n. :star: :hammer: Thanks for putting this together -- just one optional comment.\n\n. For the zipkin bullet point, how about:\n- Support Zipkin tracer configuration via config file, to enable automatic export of tracing data from linkerd to a Zipkin collector.\n. :star: Thanks for updating.\n\n. :star: \ud83c\udf74 Much better!\n. \u2b50  Looks good to me.\n. :star: :boom:\n\n. :star: :seedling: Looks good to me, and I think 10 seconds is a reasonable default.\n\n. :star: :sunglasses: Looks good!\n\n. :star: :smile: Nice, am super excited about this change.\n. :star: :+1:\n\n. Looks like this was handled by #313, #314.\n. \u2b50 \ud83d\udc12 \n\n. \u2b50 \ud83d\udc22 \n\n. \u2b50 \ud83d\udc12 \n\n. \u2b50 \ud83d\udc18  Works for me.\n\n. \u2b50 \ud83c\udf0a \n\n. \u2b50 \ud83d\udca5 \n\n. \u2b50 \ud83d\udc11  Looks good to me.\n\n. \u2b50 \ud83d\udd27 \n\n\n. \u2b50 \ud83c\udf61 \n\n. \u2b50  Thanks for making those updates. My question aside, this looks good to me.\n\n. \u2b50 \ud83d\udc11 \n\n. \u2b50 \ud83d\udc34 \n\n. \u2b50 \ud83c\udf67 Great, thanks for making those updates.\n\n. \u2b50 \u267b\ufe0f \n\n. \u2b50 \ud83d\udc11  Looks good to me.\n\n. \u2b50  Thanks @tonyd3! Can we also bug you to sign the CLA, here: https://buoyant.io/cla/individual/\n\n. \u2b50 \ud83c\udf31  Afaict, this change is actually pretty small once #364 has shipped. \n\n. \u2b50 \ud83d\udc1a  Works for me.\n\n. \u2b50  Whoa, nice catch.\n\n. \u2b50 \ud83d\udc4d  Thanks for updating.\n\n. \u2b50 \ud83c\udd99 \n\n. \u2b50 \ud83d\udd27 \n\n. \u2b50 \u2712\ufe0f \n\n. \u2b50 \ud83d\udc3c  I like it -- just had that one documentation nit.\n\n. \u2b50 \n\n. Ok, have fixed merge conflicts with master and switched to io.l5d.path.\n. One thing I realized with this change is that it will be difficult for folks to know that they have to update their configs.  I had a namerd config with:\nnamers:\n    - kind: io.l5d.experimental.k8s\n      host: localhost\n      port: 8001\n    storage:\n      kind: io.buoyant.namerd.storage.inMemory\n      namespaces:\n        default: |\n          /ns         => /io.l5d.k8s;\n          /iface      => /ns/prod;\n          /srv        => /iface/incoming;\n          /host       => /srv;\n          /http/1.1/* => /host;\nAfter picking up this change, namerd still happily loaded that config, but routing was broken because the /ns => /io.l5d.k8s entry was no longer doing anything useful.  It's unfortunate that /ns => /io.l5d.k8s is still a valid entry, even though its effect has changed entirely.  It makes me think we might want to do something drastic like abort linkerd/namerd startup if we recognize that baseDtab has an old namer string that's not prefixed with a \"#\".\n. I encountered another subtle upgrade issue with this change and the tls bound path matcher.  I'm not sure there's anything we can do about it, but I wanted to post it here in case other folks run into similar.\nSuppose I have a linkerd config as follows:\nnamers:\n- kind: io.l5d.experimental.k8s\n  host: localhost\n  port: 8001\nrouters:\n- protocol: http\n  label: demo\n  identifier:\n    kind: io.l5d.path\n  baseDtab: |\n    /ns    => /io.l5d.k8s;\n    /iface => /ns/kltest;\n    /srv   => /iface/incoming;\n    /http  => /srv;\n  client:\n    tls:\n      kind: io.l5d.clientTls.boundPath\n      caCertPath: /io.buoyant/linkerd/certs/cacertificate.pem\n      names:\n      - prefix: \"/*/*/*/{service}\"\n        commonNamePattern: \"{service}\"\n  servers:\n  - port: 8080\n    ip: 0.0.0.0\nWhen I change the k8s namer dtab entry to /ns => /#/io.l5d.k8s, then I also have to change the tls boundPath prefix to /*/*/*/*/{service}, since the bound path for my service has changed from /io.l5d.k8s/kltest/incoming/svcname to /#/io.l5d.k8s/kltest/incoming/svcname.\n. \u2b50  Looks great -- thanks for making those updates.\n\n. \u2b50 \ud83d\udd28  Thanks for updating.\n\n. \u2b50 \ud83d\udc11 \n\n. \u2b50 \ud83d\udc1a  Looks good to me.\n\n. \u2b50 \ud83d\udc11  Thanks for updating.\n\n. \u2b50 \ud83d\udc1a \n\n. Good question -- it \"works\" without responseClassifier, in the sense that 10% of requests fail with a 503 (as expected), and I don't see any ChannelClosedExceptions in the logs.\n$ for i in {1..1000}; do curl -0 -so /dev/null -w '%{http_code}\\n' :4140/ping; done | sort | uniq -c | sort -rn\n 898 200\n 102 503\n$ curl -s :9990/admin/metrics.json | jq -S . | grep -E '(success|failures)' | grep -v zipkin\n  \"rt/http/dst/id/$/inet/127.1/9000/failures\": 102,\n  \"rt/http/dst/id/$/inet/127.1/9000/failures/com.twitter.finagle.service.ResponseClassificationSyntheticException\": 102,\n  \"rt/http/dst/id/$/inet/127.1/9000/success\": 898,\n  \"rt/http/srv/0.0.0.0/4140/failures\": 102,\n  \"rt/http/srv/0.0.0.0/4140/failures/com.twitter.finagle.service.ResponseClassificationSyntheticException\": 102,\n  \"rt/http/srv/0.0.0.0/4140/success\": 898,\n. \u2b50 \ud83d\udd27 \n\n. \u2b50  Can confirm that this fixes the original issue I reported, and look at those gorgeous stats:\n$ curl -s :9990/admin/metrics.json | jq -S . | grep -E '(success|failures)' | grep -v zipkin\n  \"rt/http/dst/id/$/inet/127.1/9000/failures\": 234,\n  \"rt/http/dst/id/$/inet/127.1/9000/failures/com.twitter.finagle.service.ResponseClassificationSyntheticException\": 234,\n  \"rt/http/dst/id/$/inet/127.1/9000/success\": 2000,\n  \"rt/http/dst/path/http/1.0/GET/success\": 1000,\n  \"rt/http/dst/path/http/1.1/GET/:4140/success\": 1000,\n  \"rt/http/srv/0.0.0.0/4140/success\": 2000,\n\n. \u2b50 \n\n. \u2b50 \n\n. \u2b50  (thanks also for breaking this up into multiple branches)\n\n. \u2b50  Great, thanks for updating.\n\n. \u2b50 \n\n. \u2b50 \ud83c\udf08  Thanks for putting this together!\n\n. \u2b50 \ud83c\udf76  Looks good. I might vote to add a link to the (yet to be published) blog post about upgrading to 0.6.0, but not required.\n\n. \u2b50 Looks good.\n\n. \u2b50 \ud83c\udf03 \n\n. \u2b50 \ud83c\udf3b \n\n. \u2b50 \ud83d\udc11  Looks good to me.\n. \n. \u2b50 \ud83d\udeb4  Looks great! And no more horizontal scrollbar at smaller widths.  One thing I noticed is that the scrollbar reappears in Chrome when the window is wider than ~1380px, but that's trivial and doesn't need to be fixed as part of this branch, or at all really.\n. Awesome -- I like this change a lot.  For the new success rate graphs that you're adding and all of the existing ones, would it be possible to add a gray label to say that the chart is displaying success rate?  Similar to this label on the RPS graph:\n\nAs it stands now there isn't a clear indicator in the UI of what is being displayed.  Also totally fine to do this in a separate branch or take a different approach altogether.\n. \u2b50  Tested this out and it works great.\n. \u2b50  Looks great.\n. \u2b50  Thanks for updating.\n. \u2b50 \ud83c\udf35 \n. \u2b50  Makes sense to me.\n. Rather that calling Trace.letClear everywhere, do you think it would be possible to turn off tracing in the client stack when the namer is initialized?  I haven't tested it, but if we don't want tracing in any of our namers, you might be able to just change this line to use a NullTracer instead:\nhttps://github.com/BuoyantIO/linkerd/blob/master/linkerd/core/src/main/scala/io/buoyant/linkerd/Linker.scala#L96\n. \u2b50  \u2668\ufe0f  Thanks for updating.\n. \u2b50 \ud83d\udeb2  Nice fix!\n. Related: https://github.com/BuoyantIO/linkerd/issues/188#issuecomment-200409787\n. \u2b50 \ud83d\udeb2  Awesome.\n. \u2b50  Nice catch.\n. \u2b50 \n. \u2b50 \ud83c\udf8b \n. \u2b50  Thanks for updating, looks good.\n. \u2b50  I tested out the new 32-bit images and everything appears to be working as expected. Nice work!\n. \u2b50  Thanks for updating, changes look good to me.\n. Hey @pypeng -- the dockerfile is available here: https://github.com/BuoyantIO/debian-32-bit\nWhen I run that command the container works as expected.\nkl ~ $ docker run -it --rm --entrypoint=java buoyantio/debian-32-bit | head -n4\nUsage: java [-options] class [args...]\n           (to execute a class)\n   or  java [-options] -jar jarfile [args...]\n           (to execute a jar file)\nkl ~ $ echo $?\n0\nIs it possible that you're modifying the container's environment in such a way (like setting JAVA_HOME) that would make it not load properly?\n. \u2b50 \u2712\ufe0f  Huge improvement!\n. They're \"server address\" and \"client address\", and I'm not positive they're wrong.  Adrian just mentioned them as one of the possible reasons why the dependencies aren't lining up.\n. \u2b50 \u270f\ufe0f \n. \u2b50  Makes sense to me.\n. \u2b50 Works for me.\n. \u2b50  \ud83d\udd2d  Thanks for updating.\n. \u2b50 \ud83d\udcdc \n. \u2b50 Looks good to me.\n. \u2b50 \nWould also recommend updating linkerd/core/src/test/scala/io/buoyant/linkerd/LoadBalancerTest.scala to set enableProbation: true, since the test is now testing setting the default value.\n. \u2b50  Thanks for updating!\n. \u2b50 \ud83d\udd27 \n. \u2b50  Looks great -- thanks for working on this! I just had tiny nits.\n. \u2b50 Great, thanks for updating!\n. \ud83d\udc4d  Here's the repo that contains all of the implemented finagle-zipkin transports:\nhttps://github.com/openzipkin/zipkin-finagle\n. I've put together a new repo that provides Zipkin HTTP and Kafka tracers by way of linkerd telemeter plugins. Reviewable here:\nhttps://github.com/linkerd/linkerd-zipkin/pull/1. Hi @mapix, very interesting.  This annotation is added by Finagle's JVM filter, here:\nhttps://github.com/twitter/finagle/blob/develop/finagle-core/src/main/scala/com/twitter/finagle/filter/JvmFilter.scala\nThat filter is present on the default Finagle server stack.  I think that means that any Finagle service, by default, will emit these GC annotations.  linkerd could be updated to specifically exclude that filter, but it seems like having GC annotations might be helpful when looking at traces.  Maybe Zipkin's UI needs to be updated to handle these types of annotations properly?\nWhich version of the Zipkin UI are you using, by the way?  When I send linkerd annotations to zipkin-1.11.1, I see these annotations represented a bit differently, as follows:\n\nAs far as I can tell the gantt chart only includes the starting value for the annotation, and does not display the ending value in line with the chart.\n. For completeness' sake, here are all of the annotations associated with the span in the previous screenshot:\n\n. @mapix Ok, great -- thanks for the additional info!\n. \u2b50  Looks good. Mind adding something to CHANGES.md as well, in case anybody is surprised that these flags went away?\nAlso, not part of your change, but while you're in here, it looks like there's a config issue in the namerd example k8s config -- it's missing the experimental: true flag for k8s storage.  Maybe add that as well?\n. Ah, sorry, was not specific in my last comment.  I think we need to add the flag here:\nhttps://github.com/BuoyantIO/linkerd/blob/master/namerd/examples/k8s.yaml#L3\n. Hi Abhi,\nIn order to create a pull request, you can push your branch to a fork of\nthe linkerd repo, rather than pushing directly to BuoyantIO's linkerd\nrepo.  You can fork the repo by clicking the fork button in the Github UI.\nThis document describes the fork / pull request workflow pretty thoroughly:\nhttps://gist.github.com/Chaser324/ce0505fbed06b947d962\nHopefully that's helpful, but let us know if you get stuck.\nKevin\nOn Tue, Nov 1, 2016 at 10:21 AM, AbhiMedallia notifications@github.com\nwrote:\n\nHi William,\nI have the changes on a branch called KafkaTelemeter but I do not have\npermissions to push these changes to remote branch on github.\nanigam-mbp:linkerd anigam$ git push origin KafkaTelemeter\nremote: Permission to BuoyantIO/linkerd.git denied to AbhiMedallia.\nfatal: unable to access 'https://github.com/BuoyantIO/linkerd.git/': The\nrequested URL returned error: 403\n-Abhishek\nOn Tue, Nov 1, 2016 at 9:44 AM, William Morgan notifications@github.com\nwrote:\n\nHi Abhishek,\nAwesome, I would love to have this feature! If you have it on a branch,\nyou can make a Github pull request against BuoyantIO/linkerd... that is\nthe\nstarting point for getting it into the main distro.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/BuoyantIO/linkerd/pull/717#issuecomment-257619233,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/\nAUEsEKHBf7DTtmtnsuvdMDk4Q0R9cdadks5q52xigaJpZM4KOOVO\n.\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/BuoyantIO/linkerd/pull/717#issuecomment-257630354,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAAkCh7q1QLLaoQCgxNqG74-GtlAk-Ujks5q53UugaJpZM4KOOVO\n.\n. Just wanted to add some of my own notes that will (hopefully) help with implementing and testing this later.\n\nI sent some h2 traces to zipkin. Here's what I noticed:\n\nAll server spans from all requests have the same (default) trace id. This is because we don't have an h2-protocol-specific trace initializer.\nServer and client spans are not connected. Client spans pick up a separate trace ID and appear as a separate trace in the UI. Again, this is related to not having an initializer.\nClient spans don't have an RPC label. This requires calling Trace.recordRpc somewhere in the h2 stack, which we're not doing.\n\nFor example, after sending 2 requests, I see only 1 server span, with multiple SR/SS annotations:\n\nAnd then I see two separate traces with the client spans. Both look like this:\n\n. Looks like retry budgets are per router, so tracking the budgets as a gauge doesn't give us the number of retries per path.  I've reintroduced the original retries histogram, and added a separate counter for tracking total retries per path.\n. Going to reopen this issue, since I still can't use linkerd to proxy requests to http2.golang.org, but this might just be user error, so feel free to re-close if so.\nAs stated in the initial description, I can send an insecure HTTP/2 request directly to http2.golang.org:\n$ curl -s --http2 --http2-prior-knowledge -k https://http2.golang.org:443/reqinfo\nMethod: GET\nProtocol: HTTP/2.0\nHost: http2.golang.org\n...\nBut I can't seem to configure linkerd to successfully make the same request.  Here's my linkerd config:\nrouters:\n- protocol: h2\n  experimental: true\n  baseDtab: |\n    /h2 => /$/inet/http2.golang.org/443;\n  servers:\n  - port: 4142\n  client:\n    tls:\n      kind: io.l5d.noValidation\nBut when I send a request to linkerd, I get:\n```\n$ curl -s --http2 --http2-prior-knowledge -v http://localhost:4142/reqinfo\n   Trying ::1...\n TCP_NODELAY set\n connect to ::1 port 4142 failed: Connection refused\n   Trying 127.0.0.1...\n TCP_NODELAY set\n Connected to localhost (127.0.0.1) port 4142 (#0)\n Using HTTP2, server supports multi-use\n Connection state changed (HTTP/2 confirmed)\n Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0\n Using Stream ID: 1 (easy handle 0x7ff04b00b000)\n\nGET /reqinfo HTTP/1.1\nHost: localhost:4142\nUser-Agent: curl/7.50.3\nAccept: /\n\nConnection state changed (MAX_CONCURRENT_STREAMS updated)!\nHTTP/2 stream 1 was not closed cleanly: REFUSED_STREAM (err 7)\nCurl_http_done: called premature == 1\nClosing connection 0\n```\n\n\nAnd there are a bunch of exceptions in linkerd's logs:\n```\nio.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: error:10000410:SSL routines:OPENSSL_internal:SSLV3_ALERT_HANDSHAKE_FAILURE\n    at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:442)\n    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:248)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:350)\n    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1334)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358)\n    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:926)\n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:129)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:571)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:512)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:426)\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:398)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:877)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at com.twitter.finagle.util.ProxyThreadFactory$$anonfun$newProxiedRunnable$1$$anon$1.run(ProxyThreadFactory.scala:19)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: javax.net.ssl.SSLHandshakeException: error:10000410:SSL routines:OPENSSL_internal:SSLV3_ALERT_HANDSHAKE_FAILURE\n    at io.netty.handler.ssl.OpenSslEngine.shutdownWithError(OpenSslEngine.java:620)\n    at io.netty.handler.ssl.OpenSslEngine.sslReadErrorResult(OpenSslEngine.java:823)\n    at io.netty.handler.ssl.OpenSslEngine.unwrap(OpenSslEngine.java:777)\n    at io.netty.handler.ssl.OpenSslEngine.unwrap(OpenSslEngine.java:855)\n    at io.netty.handler.ssl.OpenSslEngine.unwrap(OpenSslEngine.java:898)\n    at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1094)\n    at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:966)\n    at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:900)\n    at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:411)\n    ... 18 more\nW 1214 02:00:06.194 THREAD76: Unhandled exception in connection with http2.golang.org/130.211.116.44:443, shutting down connection\nio.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: error:10000410:SSL routines:OPENSSL_internal:SSLV3_ALERT_HANDSHAKE_FAILURE\n    at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:442)\n    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:248)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:350)\n    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1334)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358)\n    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:926)\n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:129)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:571)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:512)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:426)\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:398)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:877)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at com.twitter.finagle.util.ProxyThreadFactory$$anonfun$newProxiedRunnable$1$$anon$1.run(ProxyThreadFactory.scala:19)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: javax.net.ssl.SSLHandshakeException: error:10000410:SSL routines:OPENSSL_internal:SSLV3_ALERT_HANDSHAKE_FAILURE\n    at io.netty.handler.ssl.OpenSslEngine.shutdownWithError(OpenSslEngine.java:620)\n    at io.netty.handler.ssl.OpenSslEngine.sslReadErrorResult(OpenSslEngine.java:823)\n    at io.netty.handler.ssl.OpenSslEngine.unwrap(OpenSslEngine.java:777)\n    at io.netty.handler.ssl.OpenSslEngine.unwrap(OpenSslEngine.java:855)\n    at io.netty.handler.ssl.OpenSslEngine.unwrap(OpenSslEngine.java:898)\n    at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1094)\n    at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:966)\n    at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:900)\n    at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:411)\n    ... 18 more\nWARN 1213 18:00:06.196 finagle/netty4-2: An exceptionCaught() event was fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did not handle the exception.\nio.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: error:10000410:SSL routines:OPENSSL_internal:SSLV3_ALERT_HANDSHAKE_FAILURE\n    at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:442)\n    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:248)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:350)\n    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1334)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358)\n    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:926)\n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:129)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:571)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:512)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:426)\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:398)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:877)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at com.twitter.finagle.util.ProxyThreadFactory$$anonfun$newProxiedRunnable$1$$anon$1.run(ProxyThreadFactory.scala:19)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: javax.net.ssl.SSLHandshakeException: error:10000410:SSL routines:OPENSSL_internal:SSLV3_ALERT_HANDSHAKE_FAILURE\n    at io.netty.handler.ssl.OpenSslEngine.shutdownWithError(OpenSslEngine.java:620)\n    at io.netty.handler.ssl.OpenSslEngine.sslReadErrorResult(OpenSslEngine.java:823)\n    at io.netty.handler.ssl.OpenSslEngine.unwrap(OpenSslEngine.java:777)\n    at io.netty.handler.ssl.OpenSslEngine.unwrap(OpenSslEngine.java:855)\n    at io.netty.handler.ssl.OpenSslEngine.unwrap(OpenSslEngine.java:898)\n    at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1094)\n    at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:966)\n    at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:900)\n    at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:411)\n    ... 18 more\nW 1214 02:00:06.303 THREAD78: ChannelStatsHandler caught an exception\nio.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: error:10000410:SSL routines:OPENSSL_internal:SSLV3_ALERT_HANDSHAKE_FAILURE\n    at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:442)\n    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:248)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:350)\n    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1334)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358)\n    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:926)\n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:129)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:571)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:512)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:426)\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:398)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:877)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at com.twitter.finagle.util.ProxyThreadFactory$$anonfun$newProxiedRunnable$1$$anon$1.run(ProxyThreadFactory.scala:19)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: javax.net.ssl.SSLHandshakeException: error:10000410:SSL routines:OPENSSL_internal:SSLV3_ALERT_HANDSHAKE_FAILURE\n    at io.netty.handler.ssl.OpenSslEngine.shutdownWithError(OpenSslEngine.java:620)\n    at io.netty.handler.ssl.OpenSslEngine.sslReadErrorResult(OpenSslEngine.java:823)\n    at io.netty.handler.ssl.OpenSslEngine.unwrap(OpenSslEngine.java:777)\n    at io.netty.handler.ssl.OpenSslEngine.unwrap(OpenSslEngine.java:855)\n    at io.netty.handler.ssl.OpenSslEngine.unwrap(OpenSslEngine.java:898)\n    at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1094)\n    at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:966)\n    at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:900)\n    at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:411)\n    ... 18 more\n```. I should have noted in my last comment: linkerd no longer hangs. It returns right away, but with the REFUSED_STREAM error.. @olix0r Just checked, and I'm now seeing the same error as you in the netty debug log output.. @adleong Hmm, I see that now too.  When trying to figure this out I was looking at this line specifically:\nhttps://github.com/BuoyantIO/linkerd/pull/779/files#diff-6bfd786381a12996131585664726929eR38\nI don't see how that takes into account the default value, since the headers are read directly off the request object.  But it's entirely possible that I'm just missing it.\n. @adleong Ah, yep, you're totally write. Have updated accordingly.\n. Hey @abhinigam, just following up on this PR. I took a slightly different approach to implementing the Zipkin kafka tracer, by setting it up as a telemetry plugin in the linkerd-zipkin repo. It's reviewable here:\nlinkerd/linkerd-zipkin#1\nWant to give it a shot to see if it works for your use case? I've published a docker image from that repo that you can use for testing: buoyantio/linkerd:zipkin.\nUsing that image, you can add the following config option to your linkerd config file:\ntelemetry:\n- kind: io.l5d.commonMetrics\n- kind: io.zipkin.kafka\n  initialSampleRate: 1.0\n  bootstrapServers: 192.168.99.100:9092\nAnd it should work similar to what you have setup here.. @fantayeneh is correct that this is covered by maxInitialLineKB. Going to close.. Is #1391 possibly a dupe of this issue?. Relates to #766.. @olix0r Good call, added.. @olix0r Good suggestions -- I like none so I went with that, but happy to adjust. Also let me know if any of the class/object names in NoneInitializer.scala should change.. @adleong Makes sense. From an implementation standpoint, all of the labels on the dashboard are derived from the metrics.json response, which is unordered. Seems like we might need to make a separate API call back to the server to fetch router ordering?. Turns out routers are already sorted in the order that they are defined in the config file. \ud83c\udf89  I changed the title of this issue to reflect that.. In the failure accrual demo, I'm also displaying the client removed_for_ms stat, which tracks total time that instances have been removed from the pool, and that might be useful to surface in the dashboard as well.. @rmars Thanks for the suggestion! I added client sort-on-update in 7504903c. I also removed a few things that appeared to be unused, but please have a look to make sure I didn't break anything.. @adleong That's correct.. @adleong Ok, per discussion, have updated the behavior to append new clients and preserve the color of existing clients.. Just wondering aloud, should we also consider moving the linkerd-examples repo to the new org?. As far as I can tell, all of the initially loaded clients still show up in the graph, even if they start out collapsed. But any new client that is added in the collapsed state after the dashboard loads does not show up in the graph. I think that either a) no collapsed clients should show up in the graph, or b) all collapsed clients should show up in the graph. The current behavior where initially loaded collapsed clients show up and added collapsed clients do not seems broken.. Ok, have pushed the kl/span-ids branch, which collapses span IDs as described in the description for this issue.\nFor comparison, here's a trace in the Zipkin UI with the existing span behavior:\n\nAnd here's the same call pattern with the proposed new behavior:\n\nYou can see that the number of spans has dropped from 10 to 6, which is the result of the first and last spans being unmodified, and the middle 8 spans being collapsed to 4. I've also changed the span labels to provide more information.\nHere's my attempt to describe the spans in the first trace:\n1  external client         => external linkerd server  (server receive)\n2  external linkerd client => web service              (client send)\n3  web service             => outgoing linkerd server  (server receive)\n4  outgoing linkerd client => incoming linkerd server  (client send)\n5  outgoing linkerd client => incoming linkerd server  (server receive)\n6  incoming linkerd client => books service            (client send)\n7  books service           => outgoing linkerd server  (server receive)\n8  outgoing linkerd client => incoming linkerd server  (client send)\n9  outgoing linkerd client => incoming linkerd server  (server receive)\n10 incoming linkerd client => authors service          (client send)\nWhereas for the second trace, we have:\n1 external client         => external linkerd server                  (server receive)\n2 external linkerd client => web service => outgoing linkerd server   (client send / server receive)\n3 outgoing linkerd client => incoming linkerd server                  (client send / server receive)\n4 incoming linkerd client => books service => outgoing linkerd server (client send / server receive)\n5 outgoing linkerd client => incoming linkerd server                  (client send / server receive)\n6 incoming linkerd client => authors service                          (client send)\nOverall, the new trace format seems like an improvement to me, but I am concerned that by collapsing spans, we're losing two \"services\" in the zipkin UI: \"inet/web\" and \"inet/books\". \"inet/authors\" still appears as a service, since it does not make any downstream requests. But since web and books both make downstream requests, the downstream client annotations are being collapsed into their upstream callers, which creates a sort of gap in the UI where the services should be.\nPut another way, looking at the second trace description above, spans 2 and 4 seem incorrect to me, since they span a service boundary. On the other hand, collapsing spans 3 and 5 seems appropriate, since those represent linker-to-linker requests. It would be really great if we could distinguish between these two types of spans and make the decision to collapse or not collapse accordingly, but I don't know of a good way to do that offhand.. After thinking more about the collapsing issue outlined for the approach above, I think it's a blocker to collapse spans across service boundaries. Hence we will need to do some sort of conditional collapsing, or not collapse at all.\nI added a workaround (d8146f52) to my branch that detects whether or not the incoming request came directly from another linkerd process, and only collapses linker-to-linker request spans. This produces the following trace in the Zipkin UI:\n\nAs expected, this trace has 8 spans, since it's collapsing half as many spans as the original approach, and none of the spans cross service boundaries. I've also further tweaked the span labels to try to better represent what is going on. I like this result the best, but curious to know what others think.. @viglesiasce Per our discussion in the linkerd slack a few days ago, I created the kl/span-ids branch to experiment with span collapsing and relabeling. I'm pretty satisfied with where it's at now. Want to give it a try in your setup to see if it's an improvement in stackdriver?. @olix0r Good thinking. Yeah, I was definitely trying to come up with a naming convention to help prevent users from inadvertently forwarding headers that are not meant to be forwarded.. Ok, following an in-person discussion, we agreed that collapsing linker-to-linker spans would be a slight UI improvement that we should consider making configurable in the long run. In the meantime, we're just going to focus on relabeling the RPC names for each span, and keep the current collapsing behavior. Will post a branch for the relabeling change.. Here's a screenshot from the Zipkin UI that shows the updated labeling:\n\nCompare that to the same trace pattern with the existing labeling:\n\nWhat do folks think about this approach?. @viglesiasce Good thinking! I think we'll tackle that in a follow-up change though. I created #1014 to track it.. Sure, will close this now. We can revisit if we decide to do any span collapsing in the future.. Am going to call this one done.. @adleong Yeah, that's my main concern as well. It feels weird that by adding a telemetry section to a config that didn't previously have one, you could inadvertently disable the stats required for the admin dashboard to function.. Hey @viglesiasce, this is great, thanks for contributing! I've also been looking into getting http trace transport working, since our conversation last week.\nI talked it over with @olix0r, and we don't want to set a direct dependency on zipkin-finagle in this repo, since that project also depends on finagle, which would introduce a diamond dependency: we could end up depending on two incompatible versions of finagle, which would limit linkerd's ability to upgrade.\nThe solution we came up with is to move the new zipkin transports into an external telemeter plugin repo, which would produce new telemeters for writing trace data to zipkin via http and kafka. I actually have the new repo almost ready to go. Will try to get it up for review by the end of today. In the meantime, I can publish a docker image for you that includes linkerd with the zipkin http transport plugin, and you can try it out to see if it works in your setup. Does that work for you?\n. I also wanted to mention that we're in the process of streamlining our configurations in advance of the breaking 0.9.0 linkerd release. So all of the tracing functionality is being moved to the telemeters section of the config. This shouldn't be a huge issue, but you would need to change your config to use that section to configure the zipkin http tracer, instead of the tracers section. It will end up looking something like this:\ntelemetry:\n- kind: io.l5d.commonMetrics\n- kind: io.zipkin.http\n  host: 127.0.0.1:9411\n  initialSampleRate: 1.0\nWill publish a docker image that works with that config shortly.. Ok, if you wouldn't mind giving the buoyantio/linkerd:zipkin-http docker image a shot, that would be great. We'll eventually publish that image elsewhere, but I pushed it to that tag for testing purposes.\nNote, this image is built off of linkerd master, which includes some breaking changes in advance of the 0.9.0 release. Here are some notes about upgrading:\n\nThe tracers section of the config has been removed. Instead, you should add this top-level section to your config (replacing the host value with the location of your stackdriver-zipkin proxy):\n\ntelemetry:\n- kind: io.l5d.commonMetrics\n- kind: io.zipkin.http\n  host: 127.0.0.1:9411\n  initialSampleRate: 1.0\n\nThe baseDtab field has been renamed dtab.\n\nThere might be a few other miscellaneous changes too, but just ping us if you run into issues and we can help you get it sorted out.. The zipkin plugin repo is available for review here: linkerd/linkerd-zipkin#1. Awesome, congrats @viglesiasce! :champagne: There's no rush at all on this. I've been playing around with the other http tracer implementation and it's working for me.. Awesome, thanks @viglesiasce! I'll be moving that repo to 0.9.0 stable soon, too.. @adleong Makes sense, and probably something we could do with relabeling configs. But how would you feel about at least splitting the transformers out into a separate label?. @Ashald Good call -- I opened #1190 to track adding the documentation.. Hi @teodor-pripoae -- glad it's working for you. The h2 tracing issue is tracked in #732.. Hey @zhangmuxi -- this is still unimplemented in Linkerd. You might be interested in following this opentracing issue, which has a bit more context around trace implementations in proxies: opentracing/specification#86.. Hey @mqliang -- your understanding is correct. Right now Linkerd supports exporting tracing data in the Zipkin format, which can be collected by any Zipkin-compatible collector, such as Zipkin itself and Stackdriver. We'd need to add a separate telemeter plugin to export tracing data in the OpenTracing format, and that would enable OpenTracing-compatible collectors, such as Jaeger and LightStep.. @adleong Good question. I originally tried that and it compiles, but resulted in a bunch of runtime errors like this one:\n$ ./sbt linkerd-examples/zipkin:run\n...\ncom.fasterxml.jackson.databind.JsonMappingException: Can not construct instance of io.buoyant.telemetry.TelemeterConfig: abstract types either need to be mapped to concrete types, have custom deserializer, or contain additional type information\n at [Source: java.io.StringReader@5e779c2; line: 14, column: 3] (through reference chain: io.buoyant.linkerd.Linker$LinkerConfig[\"telemetry\"]->com.fasterxml.jackson.module.scala.deser.BuilderWrapper[0]). Looks good -- just an optional suggestion. There's already some code in admin.js for manipulating the router dropdown menu. Maybe it makes sense to move the decision as to whether to show or hide the menu into that file as well? You could pass that information as an argument to the initialize method (which already takes an argument for showing or hiding the \"all\" dropdown option). Something like:\nfunction initialize(showRouterDropdown, showAllOption) {\n. Closing as a dupe of #1182.. Hey @samek, thanks for reporting! I've verified that I'm seeing something similar to this too when I look at a Zipkin dependency graph for one of my test applications. \n\nIn this case I expect the call chain to go from 0.0.0.0/8080 (external linkerd) to $/inet/web/7000 (web service) to 0.0.0.0/4141 (internal linkerd) to $/inet/authors/7001 (authors service). But in this setup the internal linkerd is connected directly to external linkerd, bypassing the web service.\nLooking at Zipkin's dependency job, it seems like linkerd is not providing the right set of annotations for it to properly construct the tree. I'll dig into how the job is setup to see if we can provide more info to make the graph more presentable.. This Zipkin folks just added debug logging to the dependencies job, in openzipkin/zipkin-dependencies#62. I re-ran my test setup with debug logging enabled, and it produced the following output:\nRunning Dependencies job for 2017-03-06: 1488758400000000 \u2264 Span.timestamp 1488844799999999\n17/03/06 21:00:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/03/06 21:00:05 DEBUG DependencyLinker: traversing trace tree, breadth-first\n17/03/06 21:00:05 DEBUG DependencyLinker: processing {\"traceId\": \"38df1c6d25a13330\", \"id\": \"38df1c6d25a13330\", \"kind\": \"CLIENT\", \"peerService\": \"interpreter/io.buoyant.namerd.iface.namerdinterpreterconfig\"}\n17/03/06 21:00:05 DEBUG DependencyLinker: cannot determine parent, looking for first server ancestor\n17/03/06 21:00:05 DEBUG DependencyLinker: cannot find server ancestor; skipping\n17/03/06 21:00:05 DEBUG DependencyLinker: traversing trace tree, breadth-first\n17/03/06 21:00:05 DEBUG DependencyLinker: processing {\"traceId\": \"905d002f40f1fa22\", \"id\": \"905d002f40f1fa22\", \"kind\": \"CLIENT\", \"peerService\": \"interpreter/io.buoyant.namerd.iface.namerdinterpreterconfig\"}\n17/03/06 21:00:05 DEBUG DependencyLinker: cannot determine parent, looking for first server ancestor\n17/03/06 21:00:05 DEBUG DependencyLinker: cannot find server ancestor; skipping\n17/03/06 21:00:05 DEBUG DependencyLinker: traversing trace tree, breadth-first\n17/03/06 21:00:05 DEBUG DependencyLinker: processing {\"traceId\": \"f54d953dc87f0497\", \"id\": \"f54d953dc87f0497\", \"kind\": \"CLIENT\", \"peerService\": \"interpreter/io.buoyant.namerd.iface.namerdinterpreterconfig\"}\n17/03/06 21:00:05 DEBUG DependencyLinker: cannot determine parent, looking for first server ancestor\n17/03/06 21:00:05 DEBUG DependencyLinker: cannot find server ancestor; skipping\n17/03/06 21:00:05 DEBUG DependencyLinker: traversing trace tree, breadth-first\n17/03/06 21:00:05 DEBUG DependencyLinker: processing {\"traceId\": \"ac042cdb57568245\", \"id\": \"ac042cdb57568245\", \"kind\": \"CLIENT\", \"peerService\": \"interpreter/io.buoyant.namerd.iface.namerdinterpreterconfig\"}\n17/03/06 21:00:05 DEBUG DependencyLinker: cannot determine parent, looking for first server ancestor\n17/03/06 21:00:05 DEBUG DependencyLinker: cannot find server ancestor; skipping\n17/03/06 21:00:06 DEBUG DependencyLinker: traversing trace tree, breadth-first\n17/03/06 21:00:06 DEBUG DependencyLinker: processing {\"traceId\": \"246140402a483ee8\", \"id\": \"246140402a483ee8\", \"kind\": \"SERVER\", \"service\": \"0.0.0.0/8080\"}\n17/03/06 21:00:06 DEBUG DependencyLinker: root's peer is unknown; skipping\n17/03/06 21:00:06 DEBUG DependencyLinker: processing {\"traceId\": \"246140402a483ee8\", \"parentId\": \"246140402a483ee8\", \"id\": \"55519e9781ae5b44\", \"kind\": \"CLIENT\", \"peerService\": \"$/inet/web/7000\"}\n17/03/06 21:00:06 DEBUG DependencyLinker: cannot determine parent, looking for first server ancestor\n17/03/06 21:00:06 DEBUG DependencyLinker: processing ancestor {\"traceId\": \"246140402a483ee8\", \"id\": \"246140402a483ee8\", \"kind\": \"SERVER\", \"service\": \"0.0.0.0/8080\"}\n17/03/06 21:00:06 DEBUG DependencyLinker: incrementing link 0.0.0.0/8080 -> $/inet/web/7000\n17/03/06 21:00:06 DEBUG DependencyLinker: processing {\"traceId\": \"246140402a483ee8\", \"parentId\": \"55519e9781ae5b44\", \"id\": \"2aac09d6ab8bed4b\", \"kind\": \"SERVER\", \"service\": \"0.0.0.0/4141\"}\n17/03/06 21:00:06 DEBUG DependencyLinker: cannot determine parent, looking for first server ancestor\n17/03/06 21:00:06 DEBUG DependencyLinker: processing ancestor {\"traceId\": \"246140402a483ee8\", \"parentId\": \"246140402a483ee8\", \"id\": \"55519e9781ae5b44\", \"kind\": \"CLIENT\", \"peerService\": \"$/inet/web/7000\"}\n17/03/06 21:00:06 DEBUG DependencyLinker: processing ancestor {\"traceId\": \"246140402a483ee8\", \"id\": \"246140402a483ee8\", \"kind\": \"SERVER\", \"service\": \"0.0.0.0/8080\"}\n17/03/06 21:00:06 DEBUG DependencyLinker: incrementing link 0.0.0.0/8080 -> 0.0.0.0/4141\n17/03/06 21:00:06 DEBUG DependencyLinker: processing {\"traceId\": \"246140402a483ee8\", \"parentId\": \"55519e9781ae5b44\", \"id\": \"94f92cd41bba1d85\", \"kind\": \"SERVER\", \"service\": \"0.0.0.0/4141\"}\n17/03/06 21:00:06 DEBUG DependencyLinker: cannot determine parent, looking for first server ancestor\n17/03/06 21:00:06 DEBUG DependencyLinker: processing ancestor {\"traceId\": \"246140402a483ee8\", \"parentId\": \"246140402a483ee8\", \"id\": \"55519e9781ae5b44\", \"kind\": \"CLIENT\", \"peerService\": \"$/inet/web/7000\"}\n17/03/06 21:00:06 DEBUG DependencyLinker: processing ancestor {\"traceId\": \"246140402a483ee8\", \"id\": \"246140402a483ee8\", \"kind\": \"SERVER\", \"service\": \"0.0.0.0/8080\"}\n17/03/06 21:00:06 DEBUG DependencyLinker: incrementing link 0.0.0.0/8080 -> 0.0.0.0/4141\n17/03/06 21:00:06 DEBUG DependencyLinker: processing {\"traceId\": \"246140402a483ee8\", \"parentId\": \"2aac09d6ab8bed4b\", \"id\": \"fa0b9acf8ba84506\", \"kind\": \"CLIENT\", \"peerService\": \"$/inet/books/7002\"}\n17/03/06 21:00:06 DEBUG DependencyLinker: cannot determine parent, looking for first server ancestor\n17/03/06 21:00:06 DEBUG DependencyLinker: processing ancestor {\"traceId\": \"246140402a483ee8\", \"parentId\": \"55519e9781ae5b44\", \"id\": \"2aac09d6ab8bed4b\", \"kind\": \"SERVER\", \"service\": \"0.0.0.0/4141\"}\n17/03/06 21:00:06 DEBUG DependencyLinker: incrementing link 0.0.0.0/4141 -> $/inet/books/7002\n17/03/06 21:00:06 DEBUG DependencyLinker: processing {\"traceId\": \"246140402a483ee8\", \"parentId\": \"94f92cd41bba1d85\", \"id\": \"1f096189c740e359\", \"kind\": \"CLIENT\", \"peerService\": \"$/inet/authors/7001\"}\n17/03/06 21:00:06 DEBUG DependencyLinker: cannot determine parent, looking for first server ancestor\n17/03/06 21:00:06 DEBUG DependencyLinker: processing ancestor {\"traceId\": \"246140402a483ee8\", \"parentId\": \"55519e9781ae5b44\", \"id\": \"94f92cd41bba1d85\", \"kind\": \"SERVER\", \"service\": \"0.0.0.0/4141\"}\n17/03/06 21:00:06 DEBUG DependencyLinker: incrementing link 0.0.0.0/4141 -> $/inet/authors/7001\n17/03/06 21:00:06 DEBUG DependencyLinker: traversing trace tree, breadth-first\n17/03/06 21:00:06 DEBUG DependencyLinker: processing {\"traceId\": \"136de51a5bc5139d\", \"id\": \"136de51a5bc5139d\", \"kind\": \"CLIENT\", \"peerService\": \"interpreter/io.buoyant.namerd.iface.namerdinterpreterconfig\"}\n17/03/06 21:00:06 DEBUG DependencyLinker: cannot determine parent, looking for first server ancestor\n17/03/06 21:00:06 DEBUG DependencyLinker: cannot find server ancestor; skipping\n17/03/06 21:00:06 DEBUG DependencyLinker: traversing trace tree, breadth-first\n17/03/06 21:00:06 DEBUG DependencyLinker: processing {\"traceId\": \"16d1141ba07d48bb\", \"id\": \"16d1141ba07d48bb\", \"kind\": \"CLIENT\", \"peerService\": \"interpreter/io.buoyant.namerd.iface.namerdinterpreterconfig\"}\n17/03/06 21:00:06 DEBUG DependencyLinker: cannot determine parent, looking for first server ancestor\n17/03/06 21:00:06 DEBUG DependencyLinker: cannot find server ancestor; skipping\n17/03/06 21:00:06 DEBUG DependencyLinker: traversing trace tree, breadth-first\n17/03/06 21:00:06 DEBUG DependencyLinker: processing {\"traceId\": \"8c00ddee972f0e0b\", \"id\": \"8c00ddee972f0e0b\", \"kind\": \"CLIENT\", \"peerService\": \"interpreter/io.buoyant.namerd.iface.namerdinterpreterconfig\"}\n17/03/06 21:00:06 DEBUG DependencyLinker: cannot determine parent, looking for first server ancestor\n17/03/06 21:00:06 DEBUG DependencyLinker: cannot find server ancestor; skipping\n17/03/06 21:00:06 DEBUG DependencyLinker: traversing trace tree, breadth-first\n17/03/06 21:00:06 DEBUG DependencyLinker: processing {\"traceId\": \"c9d11977f6d7f1da\", \"id\": \"c9d11977f6d7f1da\", \"kind\": \"CLIENT\", \"peerService\": \"interpreter/io.buoyant.namerd.iface.namerdinterpreterconfig\"}\n17/03/06 21:00:06 DEBUG DependencyLinker: cannot determine parent, looking for first server ancestor\n17/03/06 21:00:06 DEBUG DependencyLinker: cannot find server ancestor; skipping\n17/03/06 21:00:06 DEBUG DependencyLinker: traversing trace tree, breadth-first\n17/03/06 21:00:06 DEBUG DependencyLinker: processing {\"traceId\": \"b20b77787099088c\", \"id\": \"b20b77787099088c\", \"kind\": \"CLIENT\", \"peerService\": \"interpreter/io.buoyant.namerd.iface.namerdinterpreterconfig\"}\n17/03/06 21:00:06 DEBUG DependencyLinker: cannot determine parent, looking for first server ancestor\n17/03/06 21:00:06 DEBUG DependencyLinker: cannot find server ancestor; skipping\n17/03/06 21:00:06 DEBUG DependencyLinker: traversing trace tree, breadth-first\n17/03/06 21:00:06 DEBUG DependencyLinker: processing {\"traceId\": \"10ea2135d9b12330\", \"id\": \"10ea2135d9b12330\", \"kind\": \"CLIENT\", \"peerService\": \"interpreter/io.buoyant.namerd.iface.namerdinterpreterconfig\"}\n17/03/06 21:00:06 DEBUG DependencyLinker: cannot determine parent, looking for first server ancestor\n17/03/06 21:00:06 DEBUG DependencyLinker: cannot find server ancestor; skipping\n17/03/06 21:00:06 DEBUG DependencyLinker: traversing trace tree, breadth-first\n17/03/06 21:00:06 DEBUG DependencyLinker: processing {\"traceId\": \"7ae8d773baa3b5dd\", \"id\": \"7ae8d773baa3b5dd\", \"kind\": \"CLIENT\", \"peerService\": \"interpreter/io.buoyant.namerd.iface.namerdinterpreterconfig\"}\n17/03/06 21:00:06 DEBUG DependencyLinker: cannot determine parent, looking for first server ancestor\n17/03/06 21:00:06 DEBUG DependencyLinker: cannot find server ancestor; skipping\n17/03/06 21:00:06 DEBUG DependencyLinker: traversing trace tree, breadth-first\n17/03/06 21:00:06 DEBUG DependencyLinker: processing {\"traceId\": \"f71c77832ecefe36\", \"id\": \"f71c77832ecefe36\", \"kind\": \"CLIENT\", \"peerService\": \"usagedata\"}\n17/03/06 21:00:06 DEBUG DependencyLinker: cannot determine parent, looking for first server ancestor\n17/03/06 21:00:06 DEBUG DependencyLinker: cannot find server ancestor; skipping\n17/03/06 21:00:07 DEBUG DependencyLinker: traversing trace tree, breadth-first\n17/03/06 21:00:07 DEBUG DependencyLinker: processing {\"traceId\": \"d746085c031ad11c\", \"id\": \"d746085c031ad11c\", \"kind\": \"CLIENT\", \"peerService\": \"interpreter/io.buoyant.namerd.iface.namerdinterpreterconfig\"}\n17/03/06 21:00:07 DEBUG DependencyLinker: cannot determine parent, looking for first server ancestor\n17/03/06 21:00:07 DEBUG DependencyLinker: cannot find server ancestor; skipping\n17/03/06 21:00:07 INFO CassandraDependenciesJob: Saving with day=2017-03-06\n17/03/06 21:00:07 INFO CassandraDependenciesJob: Done\nNow just need to dig in and figure out why the request to web isn't linked to authors and books in the graph from my previous comment.. Hmm, yeah, I looked into this awhile back and there isn't an easy Linkerd fix to apply unfortunately. The issue arises from the fact that Linkerd assigns separate span IDs to client and server spans, and Zipkin's DependencyLinker only links server spans. We're losing the client span labels, which contain essential info for the graph to display correctly. From a comment in the DependencyLinker file:\n\nThis implementation traverses the tree, and only creates links between server spans. One exception is at the bottom of the trace tree. client spans that record their peer are included, as this accounts for uninstrumented services.\n\nI think we'd probably need to follow-up with the Zipkin folks and figure out how to adjust the DependencyLinker job to work for our use case (and not break any of the existing use cases).. @apakulov Can I also ask you to sign our CLA? And apologies if you already signed it. https://buoyant.io/cla/individual/. Happy to close this, per @adleong's comment on #1135:\n\nIt's inscrutable, but I think it's most correct and I prefer inscrutable to misleading.. I've encountered another version of this issue, in the inverse. If I make an HTTP/1.0 request, and linkerd receives an HTTP/1.1 response, it sends the HTTP/1.1 response back to my client. Am not positive, but I think the right behavior in this situation would be for linkerd to return an HTTP/1.0 response.\n\nFor instance, with a router configured to forward to linkerd's admin port:\n```\n$ curl -v -0 localhost:4140/admin/ping\n   Trying 127.0.0.1...\n TCP_NODELAY set\n* Connected to localhost (127.0.0.1) port 4140 (#0)\n\nGET /admin/ping HTTP/1.0\nHost: localhost:4140\nUser-Agent: curl/7.50.3\nAccept: /\n< HTTP/1.1 200 OK\n< Content-Language: en\n< Content-Length: 4\n< Content-Type: text/plain;charset=UTF-8\n< Via: 1.1 linkerd\n< Connection: close\n< \n Curl_http_done: called premature == 0\n Closing connection 0\npong\n``. @adleong Good point about the path identifier. I had considered using the logical name for the client span'sServiceNameannotation, but we're already surfacing that as part of the parent server span'sRpc` annotation. So it felt a bit redundant to include it again.\n\nI tend to think about the division between server annotations and client annotations as being protocol-agnostic and protocol-specific. The server annotations are universally the same, regardless of protocol (e.g. dst.id, dst.path, router.label, etc.). And the client annotations vary widely by protocol (e.g. http.req.method, http.uri, etc.). So my preference would be to use a protocol-specific ServiceName annotation on client spans, and I think host header makes the most sense, even in the context of the path identifier.\nI grabbed some sample traces that show the current behavior for the path identifier, and the proposed new behavior. Here's what a trace of a demo app running locally using the path identifier and the current tracing code from master looks like:\n\nYou can see the dst ids in the Services column (in this demo setup they map pretty closely to the logical service names, but that's not the case in most environments, and especially not when transformers are present). Here's the same trace with the proposed change from this branch:\n\nI can't say this approach is necessarily better when using the path identifier, but it feels more consistent to me, at least. Anyway, happy to discuss more options too. Just wanted to expand on my initial reasoning for this change.. Ok, thanks for considering it. That works for me -- will close this PR.\nIf I get a chance I'll submit a separate PR to Zipkin so that their UI properly escapes our service names. Otherwise we'll see the \"Required String parameter 'serviceName' is not present\" error that's shown in #1132.\nIt's also a bit unfortunate that dst ids using the localnode transformer include the node IP. That means that we end up with a separate serviceName for each node in our cluster, even if the destinations are targeting the same service. I can't think of another way to fix that though, but will leave #1132 open to track it.. Hey @dls314, thanks for the question, and apologies for the delay in responding. Linkerd doesn't have support for websockets. It's something that could be added in the future, but at present we're more focused on HTTP/2 streaming support.\nWe recently released the linkerd-tcp, however, and websockets would be a good use case for that project. It's a TCP proxy, which can be used to proxy websocket connections. It won't give you all of the request-level features that you get with linkerd protocols, but it does handle service discovery and TLS, as well as connection-level observability.. @siggy Looks like cncf/wg-ci#14 was merged. Should we close this one as well?. I think our overall approach to this will be to try to move more parts of the config to dynamic backing stores, rather than make the config itself reloadable. For instance routing policy can already be modified without restart by using namerd, and logging levels can be set via the admin logging page. I think we'll have a lot more of this type of configuration post-linkerd-1.0.0, and I'm going to close this in the meantime.. Renamed this issue to \"HTTP/2 stream memory leak\", and we'll use it as a tracking ticket for fixing.. Hmm, I was mistaken in the original description of this issue. Looking at the code, it appears that the l5d-reqid header is already set to the root request id (the trace id), for both HTTP/1 and HTTP/2:\n\nHTTP/1\nHTTP/2\n\nSo I think the answer to @melgene's original question is to use the l5d-reqid header, which should be consistent for all hops in the request path, and doesn't require an linkerd code changes.. \ud83d\udc4d to @siggy's comment. I think the existing behavior of setting the client and server timeout to the same value is incorrect, since the client requests could be retried. Moving the timeout configurations into the client and server sections of the config makes the most intuitive sense to me.. It also looks like io.l5d.h2.nonRetryable5XX, io.l5d.h2.retryableIdempotent5XX and io.l5d.h2.retryableRead5XX are not mentioned anywhere in our documentation. We should fix that as part of this change.. Ok, I experimented with this a bit, and it looks like the ids must be globally unique for all instances of a given plugin. So we cannot have one response classifier id'd as io.l5d.h2.nonRetryable5XX for http, and a separate, h2 classifier with the same id.\nHence, I think we should change all of the h2 plugins to be scoped to io.l5d.h2 (most already are). This would require:\n\nRenaming io.l5d.headerPath to io.l5d.h2.path\nRenaming io.l5d.headerToken to io.l5d.h2.header.token\n\nThose identifiers feel more consistent (to me) to the HTTP/1.1 identifiers. We should also document all of the HTTP/2 response classifiers, which are currently not document. Happy to make this change if it works for other folks.. Ok, correction to my previous comment. It looks like response classifier IDs must be globally unique, but identifier IDs need only be unique per protocol. So we could have and io.l5d.path identifier for http, and a separate io.l5d.path identifier for h2. In that case... how do we feel about changing the h2 identifiers as follows:\n\nRename io.l5d.headerToken to io.l5d.header.token\nRename io.l5d.headerPath to io.l5d.path\nRename io.l5d.h2.ingress to io.l5d.ingress\n\nAnd for consistency I think we should also rename the http ingress identifier:\n\nRename io.l5d.http.ingress to io.l5d.ingress\n\n(sorry for all of the back-and-forth on this). Fixed by #1183.. Pretty sure this is a dupe of #1108, but seeing as how this one has more info maybe we should close that one.. Hey @Disturbing, thanks for the detailed write up! If I understand correctly, you want gRPC clients to connect to Linkerd, and Linkerd to route some endpoints to MicroserviceA, and other endpoints to MicroserviceB. This should already be possible using Linkerd's Dtab functionality. You would just need the client to use a combined service definition, and then break out the endpoints in your Dtab. Something like:\n/svc/combinedapp/Login => /srv/serviceA/Login\n/svc/combinedapp/LoginWithFB => /srv/serviceB/LoginWithFB. @ewilde It sounds like you're able to work around this. We'll also update the docs to include a description of the host key, but in the meantime I'll close this issue. Feel free to reopen though if need be.. @DukeyToo For your namerd interfaces, I see:\ninterfaces:\n- kind: io.l5d.thriftNameInterpreter\n  ip: 0.0.0.0\n  port: 4100\n- kind: io.l5d.httpController\n  ip: 0.0.0.0\n  port: 4181\n- kind: io.l5d.mesh\n  port: 4182\nIt looks like the io.l5d.mesh interface doesn't specify port: 0.0.0.0. Without that, I think namerd will only listen for connections to the mesh API from localhost. Can you try adding port: 0.0.0.0 to see if that fixes it?. I had started on this change awhile ago but didn't finish. You can see my branch here:\nhttps://github.com/linkerd/linkerd/compare/kl/configmap-interpreter\nI wanted to go back and refactor the changes in k8s/resources.scala, since I felt like my implementation was really messy. Let me know if you have any questions about it.. @olix0r There's more context for this change in #1176. And after a bit more investigation I might be making a few more changes to this branch -- see my last comment there.. Here's the config file that I'm using that causes the failure in the description:\n```\n    admin:\n      port: 9990\nnamers:\n- kind: io.l5d.k8s\n  host: localhost\n  port: 8001\n\nstorage:\n  kind: io.l5d.k8s\n  host: localhost\n  port: 8001\n  namespace: test\n\ninterfaces:\n- kind: io.l5d.thriftNameInterpreter\n  ip: 0.0.0.0\n  port: 4100\n- kind: io.l5d.thriftNameInterpreter\n  ip: 0.0.0.0\n  port: 4101\n  tls:\n    certPath: /io.buoyant/namerd/certs/namerdcertificate.pem\n    keyPath: /io.buoyant/namerd/certs/namerdkey.pem\n- kind: io.l5d.mesh\n  ip: 0.0.0.0\n  port: 4102\n- kind: io.l5d.httpController\n  ip: 0.0.0.0\n  port: 4180\n\n```\nThe interface on 4101 is the one that's not able to serve traffic.. Am also seeing the same behavior in one of our test environments, which is running:\n\nh2 routers with gRPC\nlinker-to-linker mode\nnamerd interpreter\n\nAfter a few hours, the test hangs with nothing interesting in any of the linkerd logs. I'll do some more debugging and update this ticket with findings.. Thanks @zackangelo, still looking around. In my setup I'm running the gob app on k8s. The \"web\" service talks to the \"word\" service and the \"gen\" service via gRPC. All requests to the web service start to fail at 18:50:26. In the word service's l5d logs, I see:\n```\nD 0427 18:50:26.323 UTC THREAD37: [S L:/10.196.2.7:6262 R:/10.196.2.6:55666 S:5361] stream closed\nD 0427 18:50:26.720 UTC THREAD10: hashedWheelTimerInternals.run took 157 us\nD 0427 18:50:26.812 UTC THREAD37: [S L:/10.196.2.7:6262 R:/10.196.2.6:55666 S:5363] initialized stream\nD 0427 18:50:26.812 UTC THREAD37: [S L:/10.196.2.7:6262 R:/10.196.2.6:55666 S:5363] admitting HEADERS in Open(com.twitter.finagle.buoyant.h2.netty4.Netty4StreamTransport$RemotePending@bbd29d)\nD 0427 18:50:26.813 UTC THREAD37 TraceId:950a42002378f09c: [C L:/127.0.0.1:60624 R:/127.0.0.1:8282 S:5363] initialized stream\nD 0427 18:50:26.813 UTC THREAD37 TraceId:950a42002378f09c: [L:/127.0.0.1:60624 R:/127.0.0.1:8282] write: HEADERS\nD 0427 18:50:26.814 UTC THREAD37 TraceId:950a42002378f09c: [L:/127.0.0.1:60624 R:/127.0.0.1:8282] wrote: HEADERS: Return(())\nD 0427 18:50:26.815 UTC THREAD37: [S L:/10.196.2.7:6262 R:/10.196.2.6:55666 S:5363] admitting DATA in Open(com.twitter.finagle.buoyant.h2.netty4.Netty4StreamTransport$RemoteStreaming@7d224ae2)\nD 0427 18:50:26.815 UTC THREAD37 TraceId:950a42002378f09c: [L:/127.0.0.1:60624 R:/127.0.0.1:8282] write: DATA\nD 0427 18:50:26.815 UTC THREAD37 TraceId:950a42002378f09c: [L:/127.0.0.1:60624 R:/127.0.0.1:8282] wrote: DATA: Return(())\nD 0427 18:50:26.821 UTC THREAD41: [C L:/127.0.0.1:60624 R:/127.0.0.1:8282 S:5363] admitting HEADERS in Open(com.twitter.finagle.buoyant.h2.netty4.Netty4StreamTransport$RemotePending@5840f4c)\nD 0427 18:50:26.821 UTC THREAD41: [L:/10.196.2.7:6262 R:/10.196.2.6:55666] write: HEADERS\nD 0427 18:50:26.821 UTC THREAD41: [C L:/127.0.0.1:60624 R:/127.0.0.1:8282 S:5363] admitting HEADERS in Open(com.twitter.finagle.buoyant.h2.netty4.Netty4StreamTransport$RemoteStreaming@71f16b68)\nD 0427 18:50:26.821 UTC THREAD37 TraceId:950a42002378f09c: [L:/10.196.2.7:6262 R:/10.196.2.6:55666] write: WINDOW_UPDATE\nD 0427 18:50:26.822 UTC THREAD37 TraceId:950a42002378f09c: [L:/10.196.2.7:6262 R:/10.196.2.6:55666] wrote: WINDOW_UPDATE: Return(())\nD 0427 18:50:26.822 UTC THREAD37 TraceId:950a42002378f09c: [C L:/127.0.0.1:60624 R:/127.0.0.1:8282 S:5363] stream closed\nD 0427 18:50:26.822 UTC THREAD37: [L:/10.196.2.7:6262 R:/10.196.2.6:55666] wrote: HEADERS: Return(())\nD 0427 18:50:26.822 UTC THREAD37: [L:/10.196.2.7:6262 R:/10.196.2.6:55666] write: DATA\nD 0427 18:50:26.822 UTC THREAD37: [L:/10.196.2.7:6262 R:/10.196.2.6:55666] wrote: DATA: Return(())\nD 0427 18:50:26.822 UTC THREAD37: [L:/127.0.0.1:60624 R:/127.0.0.1:8282] write: WINDOW_UPDATE\nD 0427 18:50:26.822 UTC THREAD41: [L:/127.0.0.1:60624 R:/127.0.0.1:8282] wrote: WINDOW_UPDATE: Return(())\nD 0427 18:50:26.823 UTC THREAD41: [S L:/10.196.2.7:6262 R:/10.196.2.6:55666 S:5363] stream read failed: Reset.NoError\nReset.NoError\nD 0427 18:50:26.823 UTC THREAD41: [S L:/10.196.2.7:6262 R:/10.196.2.6:55666 S:5363] stream closed\nD 0427 18:50:26.821 UTC THREAD41: [C L:/127.0.0.1:60624 R:/127.0.0.1:8282 S:5363] admitting DATA in Open(com.twitter.finagle.buoyant.h2.netty4.Netty4StreamTransport$RemoteStreaming@71f16b68)\nD 0427 18:50:26.823 UTC THREAD41: [S L:/10.196.2.7:6262 R:/10.196.2.6:55666 S:5363] resetting Reset.NoError in com.twitter.finagle.buoyant.h2.netty4.Netty4StreamTransport$RemoteClosed@75c07da2\n```\nThis l5d is running an incoming router on port 6262, forwarding to a local gRPC app on port 8282. You can see that some of the log lines apply to the router's server running on 6262, and some apply to the router's client, talking to the service running on 8282. As far as I can tell:\n\nthe server closed its previous stream at 18:50:26.323\nthe server opened a new stream at 18:50:26.812\nthe client opened a new stream at 18:50:26.813\nthe client closed its stream at 18:50:26.822\nthe client tried to send a WINDOW_UPDATE, also at 18:50:26.822\nthe WINDOW_UPDATE resulted in \"stream read failed: Reset.NoError\"\nthe server closed its stream at 18:50:26.823\nthe server attempted to reset at 18:50:26.823\n\nThis leads me to believe that the server is unable to successfully recover after Reset.NoError, and stops handling incoming requests. Am going to dig more into that.. Ok, a few more datapoints. Looking at the stats, each time a linkerd gets into this state, it looks like there is one fewer trailer sent than number of requests.\nStats from the outgoing l5d's h2 client:\n\"rt/outgoing/client/#/io.l5d.k8s/test/grpc/word2/stream/local/data/bytes.count\": 0,\n  \"rt/outgoing/client/#/io.l5d.k8s/test/grpc/word2/stream/local/data/frames\": 181,\n  \"rt/outgoing/client/#/io.l5d.k8s/test/grpc/word2/stream/local/reset\": 0,\n  \"rt/outgoing/client/#/io.l5d.k8s/test/grpc/word2/stream/local/trailers\": 0,\n  \"rt/outgoing/client/#/io.l5d.k8s/test/grpc/word2/stream/remote/data/bytes.count\": 0,\n  \"rt/outgoing/client/#/io.l5d.k8s/test/grpc/word2/stream/remote/data/frames\": 181,\n  \"rt/outgoing/client/#/io.l5d.k8s/test/grpc/word2/stream/remote/reset\": 0,\n  \"rt/outgoing/client/#/io.l5d.k8s/test/grpc/word2/stream/remote/trailers\": 180,\nAnd stats from the incoming l5d's h2 server:\n\"rt/incoming/server/0.0.0.0/6262/stream/local/data/bytes.count\": 0,\n  \"rt/incoming/server/0.0.0.0/6262/stream/local/data/frames\": 181,\n  \"rt/incoming/server/0.0.0.0/6262/stream/local/reset\": 0,\n  \"rt/incoming/server/0.0.0.0/6262/stream/local/trailers\": 180,\n  \"rt/incoming/server/0.0.0.0/6262/stream/remote/data/bytes.count\": 0,\n  \"rt/incoming/server/0.0.0.0/6262/stream/remote/data/frames\": 181,\n  \"rt/incoming/server/0.0.0.0/6262/stream/remote/reset\": 0,\n  \"rt/incoming/server/0.0.0.0/6262/stream/remote/trailers\": 0,\nBoth client an server report processing 181 frames, but only 180 trailers. Neither client nor server report processing any resets, even though the logs show that the reset is sent. Am going to investigate that next.. Thanks for the update @kenkouot. I unfortunately haven't made a lot of progress, but wanted to write up what I'm seeing to see if we can make sense of it.\nI think that what's happening is that for a non-streaming h2 request, the last frame of the response from the h2 server should indicate endStream=true. For instance:\n----------------INBOUND--------------------\n[id: 0xb0e4c45f, L:/127.0.0.1:39532 - R:/127.0.0.1:8282] HEADERS: streamId=1461, headers=DefaultHttp2Headers[grpc-status: 0, grpc-message: ], padding=0, endStream=true\n------------------------------------\nIn my test version of linkerd, I've enabled trace debug logging, and I can confirm that I see linkerd's h2 client receiving that frame. What I don't see, however, is a corresponding OUTBOUND frame on linkerd's h2 server, that tells the client that called linkerd to also terminate the stream. Whereas for a healthy request, I see:\n----------------OUTBOUND--------------------\n[id: 0x1e1e4ec7, L:/10.196.2.44:6262 - R:/10.196.2.43:40800] HEADERS: streamId=849, headers=DefaultHttp2Headers[grpc-status: 0, grpc-message: ], streamDependency=0, weight=16, exclusive=false, padding=0, endStream=true\n------------------------------------\nThat frame isn't present on the request that causes the server to stop processing requests. I think this happens because the stream gets into a reset state (from the logs \"send stream read failed: Reset.NoError\"), and the stream isn't properly reset, so the final frame is never transmitted. As far as I can tell, the response from the downstream service is successful, so I can't determine how linkerd determined that the stream needed to be reset.\nI've been investigating this issue in a linker-to-linker setup, where the reset is happening in the inbound router. This causes the outbound router, which called the inbound router, to hang forever waiting for the response to complete. I'm going to try adding timeouts on the outbound router, to see if it will recover, although this isn't the ultimate fix for this issue.. Still don't have a fix for this issue, but I wanted to provide another update.\nIn my test setup, I'm running an h2 router on port 6262, which forwards requests to a gRPC server running locally on port 8282. When looking at request patterns that trigger the error described in this issue, the bad behavior appears to be happening in the 8282 client.\nThe 8282 client has 2 different patterns of state transitions. The most common one is:\n\noutbound HEADER frame endStream=false => stream is Open/RemotePending\noutbound DATA frame endStream=true => stream changes to LocalClosed/RemotePending\ninbound HEADER frame endStream=false => stream changes to LocalClosed/RemoteStreaming\ninbound DATA frame endStream=false => stream stays LocalClosed/RemoteStreaming\ninbound HEADER frame endStream=true => stream changes to Closed/Closed\n\nA less common pattern (roughly 15% of requests in a random sample) is:\n\noutbound HEADER frame endStream=false => stream is Open/RemotePending\noutbound DATA frame endStream=true => stream stays Open/RemotePending\ninbound HEADER frame endStream=false => stream changes to Open/RemoteStreaming\ninbound DATA frame endStream=false => stream stays Open/RemoteStreaming\ninbound HEADER frame endStream=true => stream changes to Open/RemoteClosed\nstream changes to Closed/Closed\n\nBoth of these patterns result in successful requests, but it's only the second pattern that ever triggers the error in this issue. When the issue is triggered, the final inbound HEADER is received by the 8282 client, but it is never sent as an outbound HEADER on the 6262 server. Messages are passed from the client to the server using util's AsyncQueue. In the error situation, the queue refuses the final .offer of the last HEADER frame, but it is not clear to me why the offer is refused. This seems like a race condition, if the queue is reset before the offer is made, but I can't determine where the reset is coming from. Will keep investigating.\n. Looking at recent finagle commits, I found twitter/finagle@fa9d9e20de5409d3ffe981f499230edb9ca6756a, which seems to describe this issue. Am going test rebuilding linkerd with that commit to see if the problem still exists.. Hmm, ok -- linkerd built against the latest finagle still has the same issue.. Ok, I was able to track down the issue and have put together a fix in #1280. I've also published a docker image from that branch, to buoyantio/linkerd:h2-fix. @zackangelo, @kenkouot, if you have a chance can you verify that that image fixes the issue in your environments?. Hey @zillani, thanks for reporting this. I just merged linkerd/linkerd-examples#122, which ports all of the example configs to 1.0.0. Have another look and let us know if you run into any issues.. @prdoyle Good question. Check out the clearContext server option, documented here:\nhttps://linkerd.io/config/1.1.3/linkerd/index.html#server-parameters. @esbie Good catch -- this is from our k8s test env. Will ping you the link.. Just adding a bit more info. Looking at the inbound/outbound frames for each request, the grpc-java client and the grpc-go client are almost identical, except that the java client also sends:\nTRACE 0509 18:32:08.889 PDT finagle/netty4-1:\n----------------INBOUND--------------------\n[id: 0x82ec2c7f, L:/127.0.0.1:4140 - R:/127.0.0.1:64590] GO_AWAY: lastStreamId=0, errorCode=0, length=0, bytes=\n------------------------------------\nTRACE 0509 18:32:08.896 PDT finagle/netty4-1:\n----------------OUTBOUND--------------------\n[id: 0x82ec2c7f, L:/127.0.0.1:4140 - R:/127.0.0.1:64590] GO_AWAY: lastStreamId=3, errorCode=0, length=0, bytes=\n------------------------------------\nAnd the go client does not. Here are the full frame dumps:\n\ngrpc-go client frames\n\n  TRACE 0509 18:25:09.363 PDT finagle/netty4-1:\n  ----------------OUTBOUND--------------------\n  [id: 0x16d309ca, L:/127.0.0.1:4140 - R:/127.0.0.1:64529] SETTINGS: ack=false, settings={}\n  ------------------------------------\n  TRACE 0509 18:25:09.375 PDT finagle/netty4-1:\n  ----------------INBOUND--------------------\n  [id: 0x16d309ca, L:/127.0.0.1:4140 - R:/127.0.0.1:64529] SETTINGS: ack=false, settings={}\n  ------------------------------------\n  TRACE 0509 18:25:09.377 PDT finagle/netty4-1:\n  ----------------OUTBOUND--------------------\n  [id: 0x16d309ca, L:/127.0.0.1:4140 - R:/127.0.0.1:64529] SETTINGS: ack=true\n  ------------------------------------\n  TRACE 0509 18:25:09.378 PDT finagle/netty4-1:\n  ----------------INBOUND--------------------\n  [id: 0x16d309ca, L:/127.0.0.1:4140 - R:/127.0.0.1:64529] WINDOW_UPDATE: streamId=0, windowSizeIncrement=983025\n  ------------------------------------\n  TRACE 0509 18:25:09.385 PDT finagle/netty4-1:\n  ----------------INBOUND--------------------\n  [id: 0x16d309ca, L:/127.0.0.1:4140 - R:/127.0.0.1:64529] HEADERS: streamId=1, headers=DefaultHttp2Headers[:method: POST, :scheme: http, :path: /helloworld.Greeter/SayHello, :authority: localhost:4140, content-type: application/grpc, user-agent: grpc-go/1.3.0, te: trailers], padding=0, endStream=false\n  ------------------------------------\n  TRACE 0509 18:25:09.661 PDT finagle/netty4-1:\n  ----------------INBOUND--------------------\n  [id: 0x16d309ca, L:/127.0.0.1:4140 - R:/127.0.0.1:64529] DATA: streamId=1, padding=0, endStream=true, length=12, bytes=00000000070a05776f726c64\n  ------------------------------------\n  TRACE 0509 18:25:09.681 PDT finagle/netty4-1:\n  ----------------INBOUND--------------------\n  [id: 0x16d309ca, L:/127.0.0.1:4140 - R:/127.0.0.1:64529] SETTINGS: ack=true\n  ------------------------------------\n  TRACE 0509 18:25:09.741 PDT finagle/netty4-1:\n  ----------------OUTBOUND--------------------\n  [id: 0x16d309ca, L:/127.0.0.1:4140 - R:/127.0.0.1:64529] HEADERS: streamId=1, headers=DefaultHttp2Headers[:status: 200, content-type: application/grpc, l5d-success-class: 1.0, via: h2 linkerd], streamDependency=0, weight=16, exclusive=false, padding=0, endStream=false\n  ------------------------------------\n  TRACE 0509 18:25:09.743 PDT finagle/netty4-1:\n  ----------------OUTBOUND--------------------\n  [id: 0x16d309ca, L:/127.0.0.1:4140 - R:/127.0.0.1:64529] DATA: streamId=1, padding=0, endStream=false, length=18, bytes=000000000d0a0b48656c6c6f20776f726c64\n  ------------------------------------\n  TRACE 0509 18:25:09.748 PDT finagle/netty4-1:\n  ----------------OUTBOUND--------------------\n  [id: 0x16d309ca, L:/127.0.0.1:4140 - R:/127.0.0.1:64529] HEADERS: streamId=1, headers=DefaultHttp2Headers[grpc-status: 0, grpc-message: ], streamDependency=0, weight=16, exclusive=false, padding=0, endStream=true\n  ------------------------------------\n  \n\n\ngrpc-go server frames\n\n  TRACE 0509 18:25:09.684 PDT finagle/netty4-2:\n  ----------------OUTBOUND--------------------\n  [id: 0xe1eb88b4, L:/127.0.0.1:64530 - R:localhost/127.0.0.1:50051] SETTINGS: ack=false, settings={ENABLE_PUSH=0}\n  ------------------------------------\n  TRACE 0509 18:25:09.704 PDT finagle/netty4-2:\n  ----------------OUTBOUND--------------------\n  [id: 0xe1eb88b4, L:/127.0.0.1:64530 - R:localhost/127.0.0.1:50051] HEADERS: streamId=3, headers=DefaultHttp2Headers[:method: POST, :scheme: http, :path: /helloworld.Greeter/SayHello, :authority: localhost:4140, content-type: application/grpc, user-agent: grpc-go/1.3.0, te: trailers, l5d-dst-service: /svc/localhost:4140, via: h2 linkerd, l5d-dst-client: /$/inet/localhost/50051], streamDependency=0, weight=16, exclusive=false, padding=0, endStream=false\n  ------------------------------------\n  TRACE 0509 18:25:09.714 PDT finagle/netty4-2:\n  ----------------OUTBOUND--------------------\n  [id: 0xe1eb88b4, L:/127.0.0.1:64530 - R:localhost/127.0.0.1:50051] DATA: streamId=3, padding=0, endStream=true, length=12, bytes=00000000070a05776f726c64\n  ------------------------------------\n  TRACE 0509 18:25:09.722 PDT finagle/netty4-2:\n  ----------------INBOUND--------------------\n  [id: 0xe1eb88b4, L:/127.0.0.1:64530 - R:localhost/127.0.0.1:50051] SETTINGS: ack=false, settings={}\n  ------------------------------------\n  TRACE 0509 18:25:09.723 PDT finagle/netty4-2:\n  ----------------OUTBOUND--------------------\n  [id: 0xe1eb88b4, L:/127.0.0.1:64530 - R:localhost/127.0.0.1:50051] SETTINGS: ack=true\n  ------------------------------------\n  TRACE 0509 18:25:09.723 PDT finagle/netty4-2:\n  ----------------INBOUND--------------------\n  [id: 0xe1eb88b4, L:/127.0.0.1:64530 - R:localhost/127.0.0.1:50051] WINDOW_UPDATE: streamId=0, windowSizeIncrement=983025\n  ------------------------------------\n  TRACE 0509 18:25:09.723 PDT finagle/netty4-2:\n  ----------------INBOUND--------------------\n  [id: 0xe1eb88b4, L:/127.0.0.1:64530 - R:localhost/127.0.0.1:50051] SETTINGS: ack=true\n  ------------------------------------\n  TRACE 0509 18:25:09.724 PDT finagle/netty4-2:\n  ----------------INBOUND--------------------\n  [id: 0xe1eb88b4, L:/127.0.0.1:64530 - R:localhost/127.0.0.1:50051] HEADERS: streamId=3, headers=DefaultHttp2Headers[:status: 200, content-type: application/grpc], padding=0, endStream=false\n  ------------------------------------\n  TRACE 0509 18:25:09.741 PDT finagle/netty4-2:\n  ----------------INBOUND--------------------\n  [id: 0xe1eb88b4, L:/127.0.0.1:64530 - R:localhost/127.0.0.1:50051] DATA: streamId=3, padding=0, endStream=false, length=18, bytes=000000000d0a0b48656c6c6f20776f726c64\n  ------------------------------------\n  TRACE 0509 18:25:09.741 PDT finagle/netty4-2:\n  ----------------INBOUND--------------------\n  [id: 0xe1eb88b4, L:/127.0.0.1:64530 - R:localhost/127.0.0.1:50051] HEADERS: streamId=3, headers=DefaultHttp2Headers[grpc-status: 0, grpc-message: ], padding=0, endStream=true\n  ------------------------------------\n  \n\n\ngrpc-java client frames\n\n  TRACE 0509 18:32:08.318 PDT finagle/netty4-1:\n  ----------------OUTBOUND--------------------\n  [id: 0x82ec2c7f, L:/127.0.0.1:4140 - R:/127.0.0.1:64590] SETTINGS: ack=false, settings={}\n  ------------------------------------\n  TRACE 0509 18:32:08.329 PDT finagle/netty4-1:\n  ----------------INBOUND--------------------\n  [id: 0x82ec2c7f, L:/127.0.0.1:4140 - R:/127.0.0.1:64590] SETTINGS: ack=false, settings={ENABLE_PUSH=0, MAX_CONCURRENT_STREAMS=0, INITIAL_WINDOW_SIZE=1048576, MAX_HEADER_LIST_SIZE=8192}\n  ------------------------------------\n  TRACE 0509 18:32:08.330 PDT finagle/netty4-1:\n  ----------------OUTBOUND--------------------\n  [id: 0x82ec2c7f, L:/127.0.0.1:4140 - R:/127.0.0.1:64590] SETTINGS: ack=true\n  ------------------------------------\n  TRACE 0509 18:32:08.332 PDT finagle/netty4-1:\n  ----------------INBOUND--------------------\n  [id: 0x82ec2c7f, L:/127.0.0.1:4140 - R:/127.0.0.1:64590] WINDOW_UPDATE: streamId=0, windowSizeIncrement=983041\n  ------------------------------------\n  TRACE 0509 18:32:08.340 PDT finagle/netty4-1:\n  ----------------INBOUND--------------------\n  [id: 0x82ec2c7f, L:/127.0.0.1:4140 - R:/127.0.0.1:64590] SETTINGS: ack=true\n  ------------------------------------\n  TRACE 0509 18:32:08.375 PDT finagle/netty4-1:\n  ----------------INBOUND--------------------\n  [id: 0x82ec2c7f, L:/127.0.0.1:4140 - R:/127.0.0.1:64590] HEADERS: streamId=3, headers=DefaultHttp2Headers[:authority: localhost:4140, :path: /helloworld.Greeter/SayHello, :method: POST, :scheme: http, content-type: application/grpc, te: trailers, user-agent: grpc-java-netty/1.3.0, grpc-accept-encoding: gzip], streamDependency=0, weight=16, exclusive=false, padding=0, endStream=false\n  ------------------------------------\n  TRACE 0509 18:32:08.630 PDT finagle/netty4-1:\n  ----------------INBOUND--------------------\n  [id: 0x82ec2c7f, L:/127.0.0.1:4140 - R:/127.0.0.1:64590] DATA: streamId=3, padding=0, endStream=true, length=12, bytes=00000000070a05776f726c64\n  ------------------------------------\n  TRACE 0509 18:32:08.867 PDT finagle/netty4-1:\n  ----------------OUTBOUND--------------------\n  [id: 0x82ec2c7f, L:/127.0.0.1:4140 - R:/127.0.0.1:64590] HEADERS: streamId=3, headers=DefaultHttp2Headers[:status: 200, content-type: application/grpc, grpc-encoding: identity, grpc-accept-encoding: gzip, l5d-success-class: 1.0, via: h2 linkerd], streamDependency=0, weight=16, exclusive=false, padding=0, endStream=false\n  ------------------------------------\n  TRACE 0509 18:32:08.877 PDT finagle/netty4-1:\n  ----------------OUTBOUND--------------------\n  [id: 0x82ec2c7f, L:/127.0.0.1:4140 - R:/127.0.0.1:64590] DATA: streamId=3, padding=0, endStream=false, length=18, bytes=000000000d0a0b48656c6c6f20776f726c64\n  ------------------------------------\n  TRACE 0509 18:32:08.881 PDT finagle/netty4-1:\n  ----------------OUTBOUND--------------------\n  [id: 0x82ec2c7f, L:/127.0.0.1:4140 - R:/127.0.0.1:64590] HEADERS: streamId=3, headers=DefaultHttp2Headers[grpc-status: 0], streamDependency=0, weight=16, exclusive=false, padding=0, endStream=true\n  ------------------------------------\n  TRACE 0509 18:32:08.889 PDT finagle/netty4-1:\n  ----------------INBOUND--------------------\n  [id: 0x82ec2c7f, L:/127.0.0.1:4140 - R:/127.0.0.1:64590] GO_AWAY: lastStreamId=0, errorCode=0, length=0, bytes=\n  ------------------------------------\n  TRACE 0509 18:32:08.896 PDT finagle/netty4-1:\n  ----------------OUTBOUND--------------------\n  [id: 0x82ec2c7f, L:/127.0.0.1:4140 - R:/127.0.0.1:64590] GO_AWAY: lastStreamId=3, errorCode=0, length=0, bytes=\n  ------------------------------------\n  \n\n\ngrpc-java server frames\n\n  TRACE 0509 18:32:08.655 PDT finagle/netty4-2:\n  ----------------OUTBOUND--------------------\n  [id: 0x1fca26c1, L:/127.0.0.1:64591 - R:localhost/127.0.0.1:50051] SETTINGS: ack=false, settings={ENABLE_PUSH=0}\n  ------------------------------------\n  TRACE 0509 18:32:08.686 PDT finagle/netty4-2:\n  ----------------OUTBOUND--------------------\n  [id: 0x1fca26c1, L:/127.0.0.1:64591 - R:localhost/127.0.0.1:50051] HEADERS: streamId=3, headers=DefaultHttp2Headers[:authority: localhost:4140, :path: /helloworld.Greeter/SayHello, :method: POST, :scheme: http, content-type: application/grpc, te: trailers, user-agent: grpc-java-netty/1.3.0, grpc-accept-encoding: gzip, l5d-dst-service: /svc/localhost:4140, via: h2 linkerd, l5d-dst-client: /$/inet/localhost/50051], streamDependency=0, weight=16, exclusive=false, padding=0, endStream=false\n  ------------------------------------\n  TRACE 0509 18:32:08.701 PDT finagle/netty4-2:\n  ----------------OUTBOUND--------------------\n  [id: 0x1fca26c1, L:/127.0.0.1:64591 - R:localhost/127.0.0.1:50051] DATA: streamId=3, padding=0, endStream=true, length=12, bytes=00000000070a05776f726c64\n  ------------------------------------\n  TRACE 0509 18:32:08.811 PDT finagle/netty4-2:\n  ----------------INBOUND--------------------\n  [id: 0x1fca26c1, L:/127.0.0.1:64591 - R:localhost/127.0.0.1:50051] SETTINGS: ack=false, settings={MAX_CONCURRENT_STREAMS=2147483647, INITIAL_WINDOW_SIZE=1048576, MAX_HEADER_LIST_SIZE=8192}\n  ------------------------------------\n  TRACE 0509 18:32:08.811 PDT finagle/netty4-2:\n  ----------------OUTBOUND--------------------\n  [id: 0x1fca26c1, L:/127.0.0.1:64591 - R:localhost/127.0.0.1:50051] SETTINGS: ack=true\n  ------------------------------------\n  TRACE 0509 18:32:08.811 PDT finagle/netty4-2:\n  ----------------INBOUND--------------------\n  [id: 0x1fca26c1, L:/127.0.0.1:64591 - R:localhost/127.0.0.1:50051] WINDOW_UPDATE: streamId=0, windowSizeIncrement=983041\n  ------------------------------------\n  TRACE 0509 18:32:08.834 PDT finagle/netty4-2:\n  ----------------INBOUND--------------------\n  [id: 0x1fca26c1, L:/127.0.0.1:64591 - R:localhost/127.0.0.1:50051] SETTINGS: ack=true\n  ------------------------------------\n  TRACE 0509 18:32:08.852 PDT finagle/netty4-2:\n  ----------------INBOUND--------------------\n  [id: 0x1fca26c1, L:/127.0.0.1:64591 - R:localhost/127.0.0.1:50051] HEADERS: streamId=3, headers=DefaultHttp2Headers[:status: 200, content-type: application/grpc, grpc-encoding: identity, grpc-accept-encoding: gzip], streamDependency=0, weight=16, exclusive=false, padding=0, endStream=false\n  ------------------------------------\n  TRACE 0509 18:32:08.867 PDT finagle/netty4-2:\n  ----------------INBOUND--------------------\n  [id: 0x1fca26c1, L:/127.0.0.1:64591 - R:localhost/127.0.0.1:50051] DATA: streamId=3, padding=0, endStream=false, length=18, bytes=000000000d0a0b48656c6c6f20776f726c64\n  ------------------------------------\n  TRACE 0509 18:32:08.869 PDT finagle/netty4-2:\n  ----------------INBOUND--------------------\n  [id: 0x1fca26c1, L:/127.0.0.1:64591 - R:localhost/127.0.0.1:50051] HEADERS: streamId=3, headers=DefaultHttp2Headers[grpc-status: 0], streamDependency=0, weight=16, exclusive=false, padding=0, endStream=true\n  ------------------------------------\n  \n\n. Hmm, I just tried this again with log.level=DEBUG, and the java example is encountering the ChannelClosedException, but not logging it at log.level=INFO. The go example is encountering two ChannelClosedExceptions, only one of which is logged at log.level=INFO.\nWith debug logging enabled, here's the full output from a grpc-java request:\nD 0524 22:33:22.344 UTC THREAD226: [S L:/127.0.0.1:4140 R:/127.0.0.1:57957 S:3] initialized stream\nD 0524 22:33:22.346 UTC THREAD226 TraceId:837caa668db03a27: [C L:/127.0.0.1:57955 R:/127.0.0.1:11111 S:5] initialized stream\nD 0524 22:33:22.353 UTC THREAD225 TraceId:837caa668db03a27: [C L:/127.0.0.1:57955 R:/127.0.0.1:11111 S:5] stream closed\nD 0524 22:33:22.356 UTC THREAD226: [S L:/127.0.0.1:4140 R:/127.0.0.1:57957 S:3] stream closed\nD 0524 22:33:22.365 UTC THREAD226: [S L:/127.0.0.1:4140 R:/127.0.0.1:57957] resetting all streams: Reset.Cancel\nD 0524 22:33:22.366 UTC THREAD226: [S L:/127.0.0.1:4140 R:/127.0.0.1:57957] transport closed\ncom.twitter.finagle.ChannelClosedException: ChannelException at remote address: /127.0.0.1:57957. Remote Info: Not Available\n  at com.twitter.finagle.netty4.transport.ChannelTransport$$anon$1.channelInactive(ChannelTransport.scala:186)\n  (stacktrace truncated)\nAnd here's the full output from a grpc-go request:\nD 0524 22:32:38.938 UTC THREAD222: [S L:/127.0.0.1:4140 R:/127.0.0.1:57942 S:1] initialized stream\nD 0524 22:32:38.942 UTC THREAD222 TraceId:25c7760be0628380: [C L:/127.0.0.1:57934 R:/127.0.0.1:11111 S:5] initialized stream\nD 0524 22:32:39.062 UTC THREAD220 TraceId:25c7760be0628380: [C L:/127.0.0.1:57934 R:/127.0.0.1:11111 S:5] stream closed\nD 0524 22:32:39.066 UTC THREAD222: [S L:/127.0.0.1:4140 R:/127.0.0.1:57942 S:1] stream closed\nE 0524 22:32:39.069 UTC THREAD222: [S L:/127.0.0.1:4140 R:/127.0.0.1:57942] dispatcher failed\ncom.twitter.finagle.ChannelClosedException: ChannelException at remote address: /127.0.0.1:57942. Remote Info: Not Available\n  at com.twitter.finagle.netty4.transport.ChannelTransport$$anon$1.channelInactive(ChannelTransport.scala:186)\n  (stacktrace truncated)\nD 0524 22:32:39.069 UTC THREAD222: [S L:/127.0.0.1:4140 R:/127.0.0.1:57942] go away: GoAway.InternalError\nD 0524 22:32:39.069 UTC THREAD222: [S L:/127.0.0.1:4140 R:/127.0.0.1:57942] resetting all streams: Reset.Cancel\nD 0524 22:32:39.070 UTC THREAD222: [S L:/127.0.0.1:4140 R:/127.0.0.1:57942] transport closed\ncom.twitter.finagle.ChannelClosedException: ChannelException at remote address: /127.0.0.1:57942. Remote Info: Not Available\n  at com.twitter.finagle.netty4.transport.ChannelTransport$$anon$1.channelInactive(ChannelTransport.scala:186)\n  (stacktrace truncated). And just to clarify, I think the issue is that Linkerd logs the exception when it shouldn't; Linkerd still successfully serves these requests, even when the client closes the connection. So I'm pretty sure the fix is just to clean up the logs.. Thanks for writing this up @zackangelo!. The issue appears to be that the thrift protocol was previously set as a param on the router stack, but it's now set on the client stack instead. I think the correct fix is to move the thriftProtocol option back to the router config, and remove it from the server and client configs. Since linkerd does not modify payloads, I don't think it's possible configure a router client and server with different protocols and have it still successfully route the request. Consolidating the two places where protocol is configured back into one should hopefully fix this, but other ideas also welcome.. Hey @marccardinal -- I don't know the date of the next release but it should be soon. I think we'll have a firm target date in the next day or two and can update you. In the meantime, if you're using docker, we publish a nightly docker image from the master branch of this repo, which might help for evaluation purposes. The image is buoyantio/linkerd:nightly, found here: https://hub.docker.com/r/buoyantio/linkerd/tags/. Hi @superwen0001 -- this looks basically correct to me. I might suggest the following refactor to your dtab for readability:\n/marathonId => /#/io.l5d.marathon ;\n/srv => /$/io.buoyant.http.domainToPathPfx/marathonId ;\n/svc => /srv ;\n/svc/webapp => /srv/webapp & 5 * /srv/webapp2;\nI recommend trying the dtab playground in the linkerd admin UI, instead of the namerd admin UI. That will help you to determine if you've properly configured your linkerd router running on 4140 to properly connect to the namerd namespace that you've configured.. Each cancellation generates an h2.Reset.Cancel error, and Zack notes that based on the stats, it looks like those are being counted toward failure accrual, when they probably shouldn't be.. Re-ran my initial test with the fix from #1444, and verified that it is no longer leaking connections. Thanks @adleong! Will close this.. Hey @jamessharp -- sorry for the delay on this. Can you provide your linkerd config where you're seeing the leak? I can try to reproduce in our test env.. @DukeyToo Thanks for confirming that you're also seeing the issue. Can you provide a bit more info about your setup, as well as steps to reproduce if possible? If you don't mind sharing your linkerd and namerd configs, that would be very helpful. You can also send them to me directly; I'm @kl on the linkerd slack.. Thanks @Ashald! Here's the namerd debug log for that test:\nD 0607 23:15:49.049 UTC THREAD34: h2 server pipeline: installing framer: DefaultChannelPipeline{(channelStats = com.twitter.finagle.netty4.channel.ChannelStatsHandler), (AnyToHeapInboundHandler$#0 = com.twitter.finagle.netty4.channel.AnyToHeapInboundHandler$), (H2FrameCodec$ConnectionHandler#0 = io.netty.handler.codec.http2.H2FrameCodec$ConnectionHandler), (h2 framer = io.netty.handler.codec.http2.H2FrameCodec), (channelRequestStatsHandler = com.twitter.finagle.netty4.channel.ChannelRequestStatsHandler), (exceptionHandler = com.twitter.finagle.netty4.channel.ChannelExceptionHandler), (finagleChannelTransport = com.twitter.finagle.netty4.transport.ChannelTransport$$anon$1)}\nD 0607 23:15:49.070 UTC THREAD34: [S L:/127.0.0.1:5003 R:/127.0.0.1:61660 S:3] initialized stream\nD 0607 23:15:49.303 UTC THREAD34: [S L:/127.0.0.1:5003 R:/127.0.0.1:61660 S:5] initialized stream\nD 0607 23:15:50.394 UTC THREAD34: [S L:/127.0.0.1:5003 R:/127.0.0.1:61660 S:7] initialized stream\nD 0607 23:15:50.415 UTC THREAD34: [S L:/127.0.0.1:5003 R:/127.0.0.1:61660 S:9] initialized stream\nD 0607 23:15:56.520 UTC THREAD34: [S L:/127.0.0.1:5003 R:/127.0.0.1:61660 S:11] initialized stream\nD 0607 23:15:56.540 UTC THREAD34: [S L:/127.0.0.1:5003 R:/127.0.0.1:61660 S:13] initialized stream\nD 0607 23:16:02.627 UTC THREAD34: [S L:/127.0.0.1:5003 R:/127.0.0.1:61660 S:15] initialized stream\nD 0607 23:16:02.649 UTC THREAD34: [S L:/127.0.0.1:5003 R:/127.0.0.1:61660 S:17] initialized stream\nD 0607 23:16:08.727 UTC THREAD34: [S L:/127.0.0.1:5003 R:/127.0.0.1:61660 S:19] initialized stream\nD 0607 23:16:08.746 UTC THREAD34: [S L:/127.0.0.1:5003 R:/127.0.0.1:61660 S:21] initialized stream\nD 0607 23:16:14.831 UTC THREAD34: [S L:/127.0.0.1:5003 R:/127.0.0.1:61660 S:23] initialized stream\nD 0607 23:16:14.850 UTC THREAD34: [S L:/127.0.0.1:5003 R:/127.0.0.1:61660 S:25] initialized stream\nD 0607 23:16:20.926 UTC THREAD34: [S L:/127.0.0.1:5003 R:/127.0.0.1:61660 S:27] initialized stream\nD 0607 23:16:20.953 UTC THREAD34: [S L:/127.0.0.1:5003 R:/127.0.0.1:61660 S:29] initialized stream\nD 0607 23:16:27.037 UTC THREAD34: [S L:/127.0.0.1:5003 R:/127.0.0.1:61660 S:31] initialized stream\nD 0607 23:16:27.055 UTC THREAD34: [S L:/127.0.0.1:5003 R:/127.0.0.1:61660 S:33] initialized stream\nD 0607 23:16:33.137 UTC THREAD34: [S L:/127.0.0.1:5003 R:/127.0.0.1:61660 S:35] initialized stream\nD 0607 23:16:33.152 UTC THREAD34: [S L:/127.0.0.1:5003 R:/127.0.0.1:61660 S:37] initialized stream\nD 0607 23:16:39.233 UTC THREAD34: [S L:/127.0.0.1:5003 R:/127.0.0.1:61660 S:39] initialized stream\nD 0607 23:16:39.249 UTC THREAD34: [S L:/127.0.0.1:5003 R:/127.0.0.1:61660 S:41] initialized stream\nD 0607 23:16:45.348 UTC THREAD34: [S L:/127.0.0.1:5003 R:/127.0.0.1:61660 S:43] initialized stream\nD 0607 23:16:45.396 UTC THREAD34: [S L:/127.0.0.1:5003 R:/127.0.0.1:61660 S:45] initialized stream\nD 0607 23:17:02.522 UTC THREAD38: h2 server pipeline: installing framer: DefaultChannelPipeline{(channelStats = com.twitter.finagle.netty4.channel.ChannelStatsHandler), (AnyToHeapInboundHandler$#0 = com.twitter.finagle.netty4.channel.AnyToHeapInboundHandler$), (H2FrameCodec$ConnectionHandler#0 = io.netty.handler.codec.http2.H2FrameCodec$ConnectionHandler), (h2 framer = io.netty.handler.codec.http2.H2FrameCodec), (channelRequestStatsHandler = com.twitter.finagle.netty4.channel.ChannelRequestStatsHandler), (exceptionHandler = com.twitter.finagle.netty4.channel.ChannelExceptionHandler), (finagleChannelTransport = com.twitter.finagle.netty4.transport.ChannelTransport$$anon$1)}\nD 0607 23:17:02.523 UTC THREAD38: [S L:/127.0.0.1:5003 R:/127.0.0.1:61752 S:3] initialized stream\nD 0607 23:17:02.560 UTC THREAD38: [S L:/127.0.0.1:5003 R:/127.0.0.1:61752 S:5] initialized stream\nD 0607 23:17:08.659 UTC THREAD38: [S L:/127.0.0.1:5003 R:/127.0.0.1:61752 S:7] initialized stream\nD 0607 23:17:08.691 UTC THREAD38: [S L:/127.0.0.1:5003 R:/127.0.0.1:61752 S:9] initialized stream\nD 0607 23:17:14.790 UTC THREAD38: [S L:/127.0.0.1:5003 R:/127.0.0.1:61752 S:11] initialized stream\nD 0607 23:17:14.822 UTC THREAD38: [S L:/127.0.0.1:5003 R:/127.0.0.1:61752 S:13] initialized stream\nD 0607 23:17:20.921 UTC THREAD38: [S L:/127.0.0.1:5003 R:/127.0.0.1:61752 S:15] initialized stream\nD 0607 23:17:20.952 UTC THREAD38: [S L:/127.0.0.1:5003 R:/127.0.0.1:61752 S:17] initialized stream\nD 0607 23:17:27.050 UTC THREAD38: [S L:/127.0.0.1:5003 R:/127.0.0.1:61752 S:19] initialized stream\nD 0607 23:17:27.084 UTC THREAD38: [S L:/127.0.0.1:5003 R:/127.0.0.1:61752 S:21] initialized stream\nD 0607 23:17:33.174 UTC THREAD38: [S L:/127.0.0.1:5003 R:/127.0.0.1:61752 S:23] initialized stream\nD 0607 23:17:33.205 UTC THREAD38: [S L:/127.0.0.1:5003 R:/127.0.0.1:61752 S:25] initialized stream\nD 0607 23:17:39.300 UTC THREAD38: [S L:/127.0.0.1:5003 R:/127.0.0.1:61752 S:27] initialized stream\nD 0607 23:17:39.334 UTC THREAD38: [S L:/127.0.0.1:5003 R:/127.0.0.1:61752 S:29] initialized stream\nD 0607 23:17:45.437 UTC THREAD38: [S L:/127.0.0.1:5003 R:/127.0.0.1:61752 S:31] initialized stream\nD 0607 23:17:45.469 UTC THREAD38: [S L:/127.0.0.1:5003 R:/127.0.0.1:61752 S:33] initialized stream\nSo a new connection is initialized when the dyn binding error is encountered.. Ok, it looks like what's happening here is that when linkerd's client cache is full and it receives a request that requires building a new client, it evicts an existing client and tears it down. If the client has an open stream to namerd (as is the case with the io.l5d.mesh api), the stream is closed on the linkerd side.\nAs far as I can tell, the stream is not closed on the namerd side, and namerd continues to send updates on the stream, which look like this:\n----------------INBOUND--------------------\n[id: 0xe254a629, L:/127.0.0.1:64150 - R:/127.0.0.1:5003] DATA: streamId=5, padding=0, endStream=false, length=22, bytes=0000000011220f0a0d080012047f000001188e272200\n------------------------------------\nOnce one of these is sent to linkerd after the stream has been closed, the recv method is no longer able to successfully receive the frame, and it falls into an infinite loop trying to receive it. That happens here:\nhttps://github.com/linkerd/linkerd/blob/master/finagle/h2/src/main/scala/com/twitter/finagle/buoyant/h2/netty4/Netty4StreamTransport.scala#L479\nThe call to recvFrame returns false and prints \"remote offer failed\" over and over again:\nD 0608 01:31:28.714 UTC THREAD88: [C L:/127.0.0.1:64018 R:/127.0.0.1:5003 S:5] remote offer failed\nD 0608 01:31:28.714 UTC THREAD88: [C L:/127.0.0.1:64018 R:/127.0.0.1:5003 S:5] remote offer failed\nD 0608 01:31:28.714 UTC THREAD88: [C L:/127.0.0.1:64018 R:/127.0.0.1:5003 S:5] remote offer failed\nThe way in which remote streams are closed changed as part of #1280, and that's evidently where this bug was introduced.\nAm still investigating, but it seems like there are two approaches to fixing:\n\nFigure out why the remote stream on namerd's side is not being closed when the linkerd tears down the client and closes the stream.\nAutomatically close remote streams when the remote offer fails, rather than retrying indefinitely.\n\nI think we should probably do both but will keep poking around.. I'm able to reproduce the memory leak using just linkerd configured with the io.l5d.k8s namer (no namerd). Once linkerd establishes a watch on the kube-system namespace, it will leak memory until it runs out.\nHere's a kubernetes config that reproduces the issue:\n\nmem-leak.yml\n\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: l5d-config\ndata:\n  config.yaml: |-\n    admin:\n      port: 9990\n    namers:\n    - kind: io.l5d.k8s\n    routers:\n    - protocol: http\n      dtab: /svc => /#/io.l5d.k8s/kube-system/http/default-http-backend;\n      servers:\n      - port: 4140\n        ip: 0.0.0.0\n---\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: l5d\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: l5d\n    spec:\n      volumes:\n      - name: l5d-config\n        configMap:\n          name: \"l5d-config\"\n      containers:\n      - name: l5d\n        image: buoyantio/linkerd:1.1.2\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        args:\n        - /io.buoyant/linkerd/config/config.yaml\n        ports:\n        - name: http\n          containerPort: 4140\n        - name: admin\n          containerPort: 9990\n        volumeMounts:\n        - name: \"l5d-config\"\n          mountPath: \"/io.buoyant/linkerd/config\"\n          readOnly: true\n      - name: kubectl\n        image: buoyantio/kubectl:v1.6.2\n        args: [\"proxy\", \"-p\", \"8001\"]\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: l5d\nspec:\n  selector:\n    app: l5d\n  type: LoadBalancer\n  ports:\n  - name: http\n    port: 4140\n  - name: admin\n    port: 9990\n  \n\nThat configuration routes all requests to the default-http-backend service running in the kube-system namespace, with the following dtab:\n/svc => /#/io.l5d.k8s/kube-system/http/default-http-backend;\nAfter deploying that configuration, send a request with:\n$ curl $(kubectl get svc l5d -o jsonpath=\"{.status.loadBalancer.ingress[0].*}\"):4140/healthz\nok\nThat establishes the watch on kube-system. Then load the admin dashboard and watch the JVM heap size grow unbounded.\nIf I change the dtab to instead establish a watch on the default namespace, then we don't see the same memory leak issue:\n/svc => /#/io.l5d.k8s/default/admin/l5d;\n$ curl $(kubectl get svc l5d -o jsonpath=\"{.status.loadBalancer.ingress[0].*}\"):4140/admin/ping\npong. Talked this over with some folks, and we're going to change the way that the Kubernetes namer works, so that it establishes watches on individuals endpoints within a namespace, instead of all endpoints in a namespace. That should help us to avoid the issue where watching a namespace with too many endpoint events (e.g. kube-system) causes the namer to fall behind and leak memory. I wrote up the approach in #1534. We should still figure out why the namer can't handle a large volume of events in the first place.. Thanks for reporting this @ldemailly. Good suggestion for the jar xf fix -- we can go with that.\nThe [error] downloading .protoc messages that you saw after installing unzip aren't actually indicative of an error. The issue is that the protoc script writes that message to stderr instead of stdout, which causes sbt to display it as a scary error. Will fix that as well.. Fixed by #1514.. One of our test setups exhibited this bug over the weekend. Here's how the dashboard looked when it happened:\n\nI didn't manage to grab stats or a heap dump, but will work on reproducing again today. This is a linkerd-to-linkerd daemonset configuration running on kubernetes, with the following config:\n```\n    admin:\n      port: 9998\nrouters:\n  - label: h2-out\n    protocol: h2\n    experimental: true\n    servers:\n      - port: 4148\n        ip: 0.0.0.0\n    client:\n      failureAccrual:\n        kind: none\n      tls:\n        commonName: linkerd\n        trustCerts: [/io.buoyant/linkerd/certs/cacertificate.pem]\n    identifier:\n      kind: io.l5d.header.path\n      segments: 2\n    dtab: /svc/strest.Responder/Get => /srv/strest-server8;\n    interpreter:\n      kind: io.l5d.namerd\n      dst: /$/inet/namerd.test.svc.cluster.local/4100\n      namespace: default\n      transformers:\n      - kind: io.l5d.k8s.daemonset\n        namespace: test\n        port: incoming\n        service: l5d8\n\n  - label: h2-in\n    protocol: h2\n    experimental: true\n    client:\n      failureAccrual:\n        kind: none\n    servers:\n      - port: 5158\n        ip: 0.0.0.0\n        tls:\n          certPath: /io.buoyant/linkerd/certs/linkerdcertificate.pem\n          keyPath: /io.buoyant/linkerd/certs/linkerdkey.pk8\n    identifier:\n      kind: io.l5d.header.path\n      segments: 2\n    dtab: /svc/strest.Responder/Get => /srv/strest-server8;\n    interpreter:\n      kind: io.l5d.namerd\n      dst: /$/inet/namerd.test.svc.cluster.local/4100\n      namespace: default\n      transformers:\n        - kind: io.l5d.k8s.localnode\n\nusage:\n  enabled: false\n\ntelemetry:\n  - kind: io.l5d.prometheus\n  - kind: io.l5d.statsd\n    experimental: true\n    hostname: statsd.test.svc.cluster.local\n    sampleRate: 0.01\n\n``. @stevej I don't think our docker containers providejmap`:\n$ docker run --rm --entrypoint=/bin/sh buoyantio/linkerd:1.1.0 -c 'find / -name jmap'\n$\nMaybe we should think about publishing debug images that include the full JDK, as well as heapster.. Grabbed a heap dump from a gRPC linkerd in one of our test envs. This linkerd was not yet in the state described in this issue, but will grab another one once it enters that state. In the meantime I loaded this heap dump in MAT and ran the Leak Suspects report. It detected one leak in the Netty4ClientDispatcher, as follows:\n\nWe are going to focus on trying to resolve this leak, as we suspect that this may be the eventual cause of linkerd getting into the state where it stops forwarding requests.. Thanks @zackangelo! Super helpful. And here's a leak suspect report from one of our failing linkerd instances as well:\n\nThat instance was only receiving traffic on its inbound router, and stopped forwarding requests entirely. The inbound router is configured as:\n- label: h2-in\n  protocol: h2\n  experimental: true\n  client:\n    failureAccrual:\n      kind: none\n  servers:\n  - port: 5158\n    ip: 0.0.0.0\n    tls:\n      certPath: /io.buoyant/linkerd/certs/linkerdcertificate.pem\n      keyPath: /io.buoyant/linkerd/certs/linkerdkey.pk8\n  identifier:\n    kind: io.l5d.header.path\n    segments: 2\n  dtab: /svc/strest.Responder/Get => /srv/strest-server8;\n  interpreter:\n    kind: io.l5d.namerd\n    dst: /$/inet/namerd.test.svc.cluster.local/4100\n    namespace: default\n    transformers:\n    - kind: io.l5d.k8s.localnode\nWhen this failure happened, we also observed a large rate of connection growth between the outbound linkerd and this inbound linkerd, but the outbound client and inbound server stats don't have a consistent view of the number of connections. I've documented that in #1434.. Based on the OpenSsl objects in the heap dump that I grabbed yesterday, I went ahead and re-ran my test with linker-to-linker tls disabled. That test has now been running for 24+ hours and has not exhibited the failure from this issue. We are going to work on trying to determine where those openssl objects are coming from.\n@zackangelo, not sure if this is feasible for you or not, but I would be curious to know if the problem still reproduces in your setup with client/server tls removed. Also totally understand if you're not comfortable disabling tls for the sake of this test.. Update: still working on tracking down the SSL issue, and I opened #1441 to track a separate issue that seems to be related to SSL.. Update: @adleong has put together a branch in #1444 that fixes an issue that we have with h2 multiplexing. I believe that will also fix #1441, but am testing it overnight tonight to verify. There's also an issue with his branch where streams that are interrupted don't recover cleanly, so we are working on tracking that down as well.. Ok, using @adleong's fix from #1444, I've been able to reproduce the stream reset error locally. Here are the outbound linkerd's server frames leading up to the error, and the error itself:\nTRACE 0628 11:07:56.722 PDT finagle/netty4-2:\n----------------OUTBOUND--------------------\n[id: 0x1a1b4b41, L:/127.0.0.1:4140 - R:/127.0.0.1:58746] SETTINGS: ack=false, settings={}\n------------------------------------\nTRACE 0628 11:07:56.723 PDT finagle/netty4-2:\n----------------INBOUND--------------------\n[id: 0x1a1b4b41, L:/127.0.0.1:4140 - R:/127.0.0.1:58746] SETTINGS: ack=false, settings={}\n------------------------------------\nTRACE 0628 11:07:56.723 PDT finagle/netty4-2:\n----------------OUTBOUND--------------------\n[id: 0x1a1b4b41, L:/127.0.0.1:4140 - R:/127.0.0.1:58746] SETTINGS: ack=true\n------------------------------------\nTRACE 0628 11:07:56.724 PDT finagle/netty4-2:\n----------------INBOUND--------------------\n[id: 0x1a1b4b41, L:/127.0.0.1:4140 - R:/127.0.0.1:58746] WINDOW_UPDATE: streamId=0, windowSizeIncrement=983025\n------------------------------------\nTRACE 0628 11:07:56.724 PDT finagle/netty4-2:\n----------------INBOUND--------------------\n[id: 0x1a1b4b41, L:/127.0.0.1:4140 - R:/127.0.0.1:58746] HEADERS: streamId=1, headers=DefaultHttp2Headers[:method: POST, :scheme: http, :path: /strest.Responder/Get, :authority: localhost:4140, content-type: application/grpc, user-agent: grpc-go/1.4.0-dev, te: trailers, grpc-timeout: 28394292n], padding=0, endStream=false\n------------------------------------\nTRACE 0628 11:07:56.728 PDT finagle/netty4-2:\n----------------INBOUND--------------------\n[id: 0x1a1b4b41, L:/127.0.0.1:4140 - R:/127.0.0.1:58746] DATA: streamId=1, padding=0, endStream=true, length=5, bytes=0000000000\n------------------------------------\nTRACE 0628 11:07:56.729 PDT finagle/netty4-2:\n----------------INBOUND--------------------\n[id: 0x1a1b4b41, L:/127.0.0.1:4140 - R:/127.0.0.1:58746] SETTINGS: ack=true\n------------------------------------\nTRACE 0628 11:07:56.757 PDT finagle/netty4-2:\n----------------INBOUND--------------------\n[id: 0x1a1b4b41, L:/127.0.0.1:4140 - R:/127.0.0.1:58746] HEADERS: streamId=3, headers=DefaultHttp2Headers[:method: POST, :scheme: http, :path: /strest.Responder/Get, :authority: localhost:4140, content-type: application/grpc, user-agent: grpc-go/1.4.0-dev, te: trailers, grpc-timeout: 31945777n], padding=0, endStream=false\n------------------------------------\nTRACE 0628 11:07:56.762 PDT finagle/netty4-2:\n----------------INBOUND--------------------\n[id: 0x1a1b4b41, L:/127.0.0.1:4140 - R:/127.0.0.1:58746] DATA: streamId=3, padding=0, endStream=true, length=7, bytes=00000000021007\n------------------------------------\nTRACE 0628 11:07:56.763 PDT finagle/netty4-2:\n----------------INBOUND--------------------\n[id: 0x1a1b4b41, L:/127.0.0.1:4140 - R:/127.0.0.1:58746] RST_STREAM: streamId=1, errorCode=8\n------------------------------------\nTRACE 0628 11:07:56.766 PDT finagle/netty4-2:\n----------------OUTBOUND--------------------\n[id: 0x1a1b4b41, L:/127.0.0.1:4140 - R:/127.0.0.1:58746] GO_AWAY: lastStreamId=3, errorCode=1, length=53, bytes=526571756573742073747265616d2031206973206e6f7420636f727265637420666f722073657276657220636f6e6e656374696f6e\n------------------------------------\nDEBUG 0628 11:07:56.774 PDT finagle/netty4-2: [id: 0x1a1b4b41, L:/127.0.0.1:4140 - R:/127.0.0.1:58746] Sent GOAWAY: lastStreamId '3', errorCode '1', debugData 'Request stream 1 is not correct for server connection'. Forcing shutdown of the connection.\nE 0628 18:07:56.781 UTC THREAD20: [S L:/127.0.0.1:4140 R:/127.0.0.1:58746 S:1] ignoring exception\ncom.twitter.finagle.ChannelWriteException: com.twitter.finagle.UnknownChannelException: Request stream 1 is not correct for server connection at remote address: /127.0.0.1:58746. Remote Info: Not Available. Remote Info: Not Available\nCaused by: com.twitter.finagle.UnknownChannelException: Request stream 1 is not correct for server connection at remote address: /127.0.0.1:58746. Remote Info: Not Available\n    at com.twitter.finagle.ChannelException$.apply(Exceptions.scala:258)\n    at com.twitter.finagle.netty4.transport.ChannelTransport$$anon$2.operationComplete(ChannelTransport.scala:105)\n    at com.twitter.finagle.netty4.transport.ChannelTransport$$anon$2.operationComplete(ChannelTransport.scala:102)\n    at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507)\n    at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481)\n    at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420)\n    at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:122)\n    at io.netty.handler.codec.http2.DefaultHttp2ConnectionEncoder.writeHeaders(DefaultHttp2ConnectionEncoder.java:208)\n    at io.netty.handler.codec.http2.DefaultHttp2ConnectionEncoder.writeHeaders(DefaultHttp2ConnectionEncoder.java:146)\n    at io.netty.handler.codec.http2.H2FrameCodec.write(H2FrameCodec.scala:141)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730)\n    at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38)\n    at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1089)\n    at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1136)\n    at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1078)\n    at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\n    at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:462)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run(BlockingTimeTrackingThreadFactory.scala:24)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: io.netty.handler.codec.http2.Http2Exception: Request stream 1 is not correct for server connection\n    at io.netty.handler.codec.http2.Http2Exception.connectionError(Http2Exception.java:85)\n    at io.netty.handler.codec.http2.DefaultHttp2Connection$DefaultEndpoint.checkNewStreamAllowed(DefaultHttp2Connection.java:839)\n    at io.netty.handler.codec.http2.DefaultHttp2Connection$DefaultEndpoint.createStream(DefaultHttp2Connection.java:705)\n    at io.netty.handler.codec.http2.DefaultHttp2Connection$DefaultEndpoint.createStream(DefaultHttp2Connection.java:625)\n    at io.netty.handler.codec.http2.DefaultHttp2ConnectionEncoder.writeHeaders(DefaultHttp2ConnectionEncoder.java:156)\n    ... 16 more\n@adleong and @olix0r are investigating a fix.. This does appear to be fixed by #1606; closing.. \ud83d\ude0d  Ooh, I like option 2 -- vertical white lines.. Hey @cchatfield, we use finagle's TraceId.serialize method to serialize the trace id to bytes, which are passed via the l5d-ctx-trace header. You can see how the serialize method works here.\nMore generally though, we mostly expect users of linkerd to forward the l5d-ctx* headers without modification. The idea is that linkerd will produce it's own traces of the request path, and you're welcome to use a different set of headers, such as open tracing or zipkin's x-b3*, which linkerd will forward without modification. This unfortunately means that the two traces will not be linked, however. \nIf it seems like a common use case that folks would want to supply their own trace ids to linkerd, we could probably do this more ergonomically than requiring you to set the encoded l5d-ctx-trace header. Off the top of my head, maybe we could accept a plaintext l5d-trace header, and use that value to create the encoded l5d-ctx-trace header that's returned with the response. We do something similar already for the l5d-dtab header.. Glad to hear you got it working! I went ahead and opened #1447 to make it easier to set trace ids on inbound requests, and will close this issue out.. Digging a bit more deeply into this, unless I'm misunderstanding these stats, I'd expect the number of connections at any given time to equal the number of connects minus the number of closes. This is also clearly not the case.\nFor the client running on 10.196.1.125, I see:\n\nThe number of client connections remains basically constant, but the rate of closes exceeds the rate of connects, which does not seem correct.\nFor the server running on 10.196.0.42, I see:\n\nThis also seems problematic. Initially, the number of connections grows at the same rate as connects, and there are no closes, so the stats appear to line up. After awhile, however, the connect and close rates remain constant, but the number of connections drops.. Wanted to add a screenshot of a test where imbalanced traffic leads to an overall performance degradation when one of the backends stops serving requests:\n\nYou can see that one of the three backend instances was serving 50% of traffic, and when it stopped serving traffic, linkerd's overall success rate dropped to 50%, rather than removing the instance from the load balancer pool.. With @adleong's fix from #1444 applied, this issue has changed somewhat, although there's still a wider distribution of requests than I would typically expect.\nUnsurprisingly, the traffic balance changes based on choice load balancer. Here's a comparison in a test setup of ~85 rps balanced over 3 backends:\n\nThe balancing behavior also seems to be impacted by request cancelations. Here's the same setup, but with roughly 1 request per second being canceled:\n\nIn this case the variability in the default load balancer is pretty extreme.\n. I pulled the loadbalancer stats from an instance that was sending unbalanced h2 traffic across 3 backends, and they look like this:\n\"rt/h2-out/client/.../loadbalancer/adds\": 3,\n\"rt/h2-out/client/.../loadbalancer/algorithm/p2c_least_loaded\": 1,\n\"rt/h2-out/client/.../loadbalancer/available\": 3,\n\"rt/h2-out/client/.../loadbalancer/busy\": 0,\n\"rt/h2-out/client/.../loadbalancer/closed\": 0,\n\"rt/h2-out/client/.../loadbalancer/load\": 2035,\n\"rt/h2-out/client/.../loadbalancer/max_effort_exhausted\": 0,\n\"rt/h2-out/client/.../loadbalancer/meanweight\": 1,\n\"rt/h2-out/client/.../loadbalancer/rebuilds\": 1,\n\"rt/h2-out/client/.../loadbalancer/removes\": 0,\n\"rt/h2-out/client/.../loadbalancer/size\": 3,\n\"rt/h2-out/client/.../loadbalancer/updates\": 1,. Have seen this same behavior with an HTTP/1.1 test as well:\n\nAnd here are the loadbalancer stats for that client:\n\"rt/http-out/.../loadbalancer/adds\": 3,\n\"rt/http-out/.../loadbalancer/algorithm/p2c_least_loaded\": 1,\n\"rt/http-out/.../loadbalancer/available\": 3,\n\"rt/http-out/.../loadbalancer/busy\": 0,\n\"rt/http-out/.../loadbalancer/closed\": 0,\n\"rt/http-out/.../loadbalancer/load\": 3,\n\"rt/http-out/.../loadbalancer/max_effort_exhausted\": 0,\n\"rt/http-out/.../loadbalancer/meanweight\": 1,\n\"rt/http-out/.../loadbalancer/rebuilds\": 1,\n\"rt/http-out/.../loadbalancer/removes\": 0,\n\"rt/http-out/.../loadbalancer/size\": 3,\n\"rt/http-out/.../loadbalancer/updates\": 1,. @amitsaha Sorry for the delay. By RPC do you mean thrift? We could definitely make something work for TTwitter thrift, but not sure if there's an option for native thrift. For context, the l5d-dtab header is currently only supported for http protocols, so we'd probably start with that.. Hey @adriancole, good question. As far as I know there aren't any other open issues for adopting additional trace header formats. Since we use the l5d-ctx-* headers to propagate both tracing information and routing information, we think of them as being Linkerd-specific for the most part, and we generally encourage service owners for forward these headers without modification.\nSimilarly, Linkerd forwards tracing headers from all of the other tracing systems without modification, such that Linkerd's traces are non-overlapping with those systems' traces. This has the drawback that traces generated by Linkerd are not linked to traces generated by the applications themselves, but it also gives us the flexibility to propagate whatever type of context we need.\nI think that the l5d-trace header described in this issue could be a good middle ground, allowing applications to inject their own trace IDs if they have them, without wiring through context propagation for every tracing system.. Yeah, that makes sense. Linkerd could provide a pluggable server-side filter that converts inbound headers to a finagle Trace object, and a pluggable client-side filter that converts the Trace object back to headers on the outbound request. It's a bit more complicated because we'd still need to figure out a way for Linkerd to continue to propagate routing context, such as per-request routing rules, and we'd rather not rely on plugins to carry that forward. But something additive for advancing trace context for other systems would probably work.. Good thinking -- I'd be happy to join the discussion to talk about service mesh tracing as it relates to Linkerd. I indicated my interest in the document. Thanks!. Hi @xiaoerlyl -- linkerd is already setup to propagate context from any tracing system. For http traffic, context is usually passed via http request headers, and linkerd will forward those headers, regardless of tracing system. This ticket is setup to track linking traces generated externally to those generated by linkerd. This issue describes one such approach, but it's not being actively worked on at the moment.. Just wanted to mention that this is already mostly possible with the existing image.\nFor instance, if I have a shell script that generates a config file and does env var substitution:\n```\n$ cat gen-config.sh \n!/bin/sh\ncat <<EOF\nadmin:\n  port: $ADMIN_PORT\nrouters:\n- protocol: http\n  dtab: /svc => /$/inet/127.1/$ADMIN_PORT;\n  servers:\n  - port: 4140\n    ip: 0.0.0.0\nEOF\n```\nThen I can mount that into the released linkerd image and pipe the output of the script to start the linkerd process:\n$ docker run --rm -e ADMIN_PORT=9999 -p 4140:4140 -v `pwd`/gen-config.sh:/gen-config.sh --entrypoint='/bin/sh' buoyantio/linkerd:1.1.0 -c '/gen-config.sh | ./bundle-exec -- -'\n...\nI 0630 18:41:32.733 UTC THREAD1: serving http admin on /0.0.0.0:9999\nYou can see that it starts the admin server on the port specified by the ADMIN_PORT environment variable. And sending a request to the router running on port 4140 forwards to the correct admin port.\n$ curl $DOCKER_IP:4140/admin/ping\npong. @kevholditch Ok, makes sense. Sorry, posted by comment before reading the issue that you had created.. Sorry, should have mentioned with my review yesterday -- can you also update linkerd/docs/interpreter.md with a description of this interpreter?. @hawkw Sorry for the delay! Yep, am all for closing this in favor of #1532.. @adleong Heh, yes, there is broader context, which I sent to the linkerd-users mailing list, which no longer exists :-/ Here was my email:\n\nAh, this looks like an oversight in the native thrift (non-TTwitter) implementation. Good find!\nLinkerd will produce 2 spans for each thrift request: a server span and a client span. The server span's Rpc annotation is the name of the router that served the request, plus the request's identifier. For thrift, the Rpc annotation for a router with default configuration will be thrift /svc. However, if you set the thriftMethodInDst option on your thrift router, then the Rpc annotation will be thrift /svc/<method-name>.\nFor the client span, it looks like we're only setting the Rpc annotation if it's a TTwitter request. But it should be possible to set that field for non-TTwitter requests, since the method name exists in the native thrift payload. I went ahead and put together a branch to fix this:\nhttps://github.com/linkerd/linkerd/pull/1512\nThanks for bringing it to our attention!\n\nBasically, even for native thrift, we can still read the method name that's being called (we already do this in the thrift identifier), so it makes sense to use the method name as the Rpc annotation on the client span.\nBefore this change, a native thrift trace in Zipkin looked like:\n\nAnd with this change, it looks like:\n\n. @adleong Yeah, your understanding is correct. We're already providing a bunch of annotations anyway (DstTracing, for example), so we might as well fix the Rpc annotation.. @thedebugger Good point -- have updated tracelog.yaml.. I started to work on this by putting together the kl/k8s-endpoints-refactor branch, which rewrites EndpointsNamer to watch individual endpoints, based off of the kube-refactor branch. You can see the change here:\nhttps://github.com/linkerd/linkerd/compare/kube-refactor...kl/k8s-endpoints-refactor\nI tested the branch in Kubernetes and verified that it can at least route requests, but didn't test any more complicated routing or dtab overrides. Plus the code isn't very well written, so I didn't bother to put it up for review. But I wanted to at least post it here as a reference for whomever takes on making this change.. Hey @ievgen-kolomiiets, thanks for reporting this, and apologies for the delay in responding.\nI tested this a bit, and in my tests linkerd did fallback from service-a to service-b when all instances of service-a disappeared. That said, I think a better way to go about this would be to remove the service-a kubernetes service object prior to the deploy, which would force the fallback to service-b explicitly. Then you could deploy service-a and restore the service object to shift traffic back to service-a.\nIn investigating this, however, I realized that there's currently a bug in linkerd where it does not properly recognize when a service has been removed, and it continues to send traffic to instances of the service rather than falling back. I've described this behavior in more detail in #1612, and we're going to work on fixing that as part of the 1.2.0 release.. Great, thanks for the additional info @ievgen-kolomiiets. Your setup makes sense to me, and what you describe is how I expect it to work: the fallback should happen on Neg, but won't happen on Empty. But as I described in #1612, I think that's not actually how it works at present, and the behavior changed recently. Which version of linkerd are you running? We might actually want to fallback on Empty as well, but I think the fact that Linkerd behaves that way now was not a conscientious decision.. @amanjeev Thanks for spotting this! Have deployed the docs site with a fix.. Great, thanks @mejran. Will merge this now.. Also, if you're looking for a programatic way to verify this, you can use namerd's addr API to get all of the addresses for a given concrete name. More info here:\nhttps://linkerd.io/config/1.1.3/namerd/index.html#get-api-1-addr-lt-namespace-gt\nBy way of example:\n$ curl -s <namerd-ip>:4180/api/1/addr/default?path=%2F%23%2Fio.l5d.k8s%2Ftest%2Fhttp%2Fweb2 | jq -S .\n{\n  \"addrs\": [\n    {\n      \"ip\": \"10.196.0.61\",\n      \"meta\": {\n        \"nodeName\": \"gke-...\"\n      },\n      \"port\": 7272\n    },\n    {\n      \"ip\": \"10.196.6.168\",\n      \"meta\": {\n        \"nodeName\": \"gke-...\"\n      },\n      \"port\": 7272\n    },\n    {\n      \"ip\": \"10.196.6.177\",\n      \"meta\": {\n        \"nodeName\": \"gke-...\"\n      },\n      \"port\": 7272\n    }\n  ],\n  \"meta\": {},\n  \"type\": \"bound\"\n}\n. @danielBreitlauch Sorry for the delay. It's a bit less ergonomic, but you should be able to use the linkerd admin site's delegator.json endpoint to do something similar.\nFor instance, here's an identical request to my previous example, running against linkerd instead of namerd:\n$ curl -H \"Content-Type: application/json\" -X POST -d '{\"namespace\":\"outgoing\",\"path\":\"/#/io.l5d.k8s/test/http/web2\",\"dtab\":\"\"}' -s <linkerd-ip>:9990/delegator.json | jq -S .bound.addr\n{\n  \"addrs\": [\n    {\n      \"ip\": \"10.196.3.233\",\n      \"meta\": {\n        \"nodeName\": \"gke-...\"\n      },\n      \"port\": 7272\n    },\n    {\n      \"ip\": \"10.196.4.165\",\n      \"meta\": {\n        \"nodeName\": \"gke-...\"\n      },\n      \"port\": 7272\n    },\n    {\n      \"ip\": \"10.196.3.193\",\n      \"meta\": {\n        \"nodeName\": \"gke-...\"\n      },\n      \"port\": 7272\n    }\n  ],\n  \"meta\": {},\n  \"type\": \"bound\"\n}\nAm going to close this issue if that's ok by you.. Linkerd uses Consul's blocking queries to subscribe to events as soon as they happen. So once an instance is removed from Consul, Linkerd should stop sending it traffic almost immediately.\nIf you have any follow-up questions, mind posting them to discourse.linkerd.io? We'll do a better job staying on top of them if you post them there.. (fwiw, I think this is a dupe of #1361 -- follow along there). Oh and I verified that this branch fixes the memory leak described in #1361, which was the original motivation for this change. \ud83c\udf89 . Hey @vadimi, thanks for reporting this. Am seeing something similar with 1.1.3 in one of our test environments:\n\nWe should be able to reproduce and figure out what's going. Can you tell us a bit more about the environment where you're running linkerd? Is it Kubernetes? Are you using namerd? If you don't mind pasting your full linkerd config into this issue that would be super helpful. Thanks!. Great, thanks for the additional info. I was able to reproduce as well. Here are metrics and thread dumps from a linkerd process after it gets into this state:\n\ngrpc-metrics.txt\ngrpc-thread-dump.txt\n\nI also checked the linkerd debug logs and didn't see anything obvious that would cause this.. @adleong you did this! #1648. closing.. I talked this over with @adleong and it looks like with this change gRPC streaming is suffering from a memory leak, either as a result of this change, or due to a different issue that didn't manifest until we fixed the flow control issue. I think we should track that down before merging.. Ok, I merged this branch with latest master following the Kubernetes API refactor branch merging, and the memory leak is still happening. I grabbed a heap dump, and it contains one leak suspect, as follows:\n\nLooks like one or more unbounded AsyncQueue objects. Am happy to share the heap dump if folks are interested in looking at it.. @hawkw Yeah, as far as I can tell, this is a new leak introduced with this branch. It's a bit hard to be 100% sure, however, since without the fix from this branch, linkerd just stops processing all streaming requests after a short period of time.. Ah, heads up that if you update to twitter-server 1.31.0, the JvmStats class moved. You'll need to change imports of that class from com.twitter.server.util.JvmStats to com.twitter.jvm.JvmStats.. Great, thanks for updating @cponomaryov. We're going to try to pick this up for the 1.2.0 release, but we're going to hold off on merging it for the time being while we run a few more performance tests.\nRunning this branch in our test environment over the weekend showed one issue where memory usage was steady and success rate was good for about 48 hours, and then things went awry. It looked like this:\n\nIt's possible this is an instance of a separate issue, but we're going to run a few more tests to be sure.. I'm not sure I follow this. Should the example request be GET /book/api/published HTTP/1.1? Instead of GET /book HTTP/1.1?. Ahhh, ok, makes sense. Thanks for the clarification.. Hi @wjwinner, thanks for reporting this! I think it's a dupe of #1622, which we just fixed a few days ago. Would you mind trying out the buoyantio/linkerd:nightly image to see if it fixes the issue that you're seeing? . Great, thank you!. Hey @leozc -- thanks for reporting. This sounds like an issue with caching the DNS resolution for the CNAME for too long. Linkerd uses finagle, which I believe uses the default JVM implementation, to resolve CNAMEs. We'll need to investigate where the bad caching behavior might be coming from.\nI'm not too familiar with ELB, but is there any chance it publishes SRV records? We just added the io.l5d.dnssrv namer, which will do service discovery on a SRV record, and load balance against all IPs contained in the record. That actually sounds more appropriate for your use case. Otherwise, with CNAMEs, I think linkerd just picks the first IP based on the default DNS implementation, and uses that permanently. There isn't any load balancing for CNAMEs that resolve to more than one IP. If it's at all possible to switch to SRV, you might want to try using that instead.. Hey @mirosval -- just wanted to update to let you know that I haven't forgotten about this PR, but haven't had a chance to get to it yet. I will soon though!. Great, thanks @mirosval, and thanks for the fix!. Hi @xiaoerlyl, this shipped as part of the linkerd 1.3.2 release. So you should already have access to it, provided you're running a recently released version of linkerd.. I did a bit of investigation into this, I think that it's the same underlying issue as #1669. I've posted a longer update there, but will leave both of these open for the time being.. Hey @agunnerson-ibm -- we just merged a fix for this issue and are working on cutting a release candidate now. Can update in an hour or two once it's available, and then we'd love to have you verify it's fixed in your environment.. Ok, please give buoyantio/linkerd:1.3.1-rc1 / buoyantio/namerd:1.3.1-rc1 a shot to see if the fix your issue, thanks!. Hey @jsenon, thanks for reporting this -- we'll take a look and get back to you. Would you mind also adding Namerd's metrics payload if you have it?. @jsenon That's no problem. We can look into reproducing with the info you added in the description. In the meantime, how hard would it be for you to switch to namerd's io.l5d.mesh interface? That interface is newer and it might not have the same correctness problem, so I think it would be worth a shot.. I was able to reproduce this behavior in one of our Kubernetes test environments, running Kubernetes 1.6.10 and Linkerd/Namerd 1.3.0. It appears that Namerd fails to successfully re-establish watches after it encounters a 410 \"too old resource version\" error from the watch API. In the namerd logs, I see:\n```\nD 1012 20:59:08.359 UTC THREAD33 TraceId:08acbf261bc25b5e: json read eoc\nD 1012 20:59:08.365 UTC THREAD33 TraceId:08acbf261bc25b5e: json reading chunk of 8192 bytes\nD 1012 20:59:08.365 UTC THREAD33 TraceId:08acbf261bc25b5e: json read chunk: {\"type\":\"ERROR\",\"object\":{\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"too old resource version: 65030480 (65047701)\",\"reason\":\"Gone\",\"code\":410}}\nD 1012 20:59:08.365 UTC THREAD33 TraceId:08acbf261bc25b5e: json chunk reading: [179, 180] \nD 1012 20:59:08.366 UTC THREAD33 TraceId:08acbf261bc25b5e: json chunk read: [0, 179] {\"type\":\"ERROR\",\"object\":{\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"too old resource version: 65030480 (65047701)\",\"reason\":\"Gone\",\"code\":410}} EndpointsError(Status(Some(Status),Some(v1),Some(ObjectMeta(None,None,None,None,None,None,None,None,None,None,None)),Some(Failure),Some(too old resource version: 65030480 (65047701)),Some(Gone),None,Some(410)))\nD 1012 20:59:08.366 UTC THREAD33 TraceId:08acbf261bc25b5e: k8s returned 'too old resource version' error with incorrect HTTP status code, restarting watch\nD 1012 20:59:08.366 UTC THREAD33 TraceId:08acbf261bc25b5e: k8s restarting watch on /api/v1/watch/namespaces/test/endpoints/hello1, resource version Some(65030480) was too old\nD 1012 21:05:07.058 UTC THREAD37 TraceId:20a8f25b7c3e6bd5: json read eoc\nD 1012 21:05:07.164 UTC THREAD37 TraceId:20a8f25b7c3e6bd5: json reading chunk of 8192 bytes\n```\nAnd after that message is printed, I see no further updates from /api/v1/watch/namespaces/test/endpoints/hello1. If I restart all of the pods in the hello1 service, service success rate drops to 0 once the pods have been fully rolled. If I restart namerd, service success rate returns to 100%.\nWe'll need to investigate why the watch is not successfully re-established after encountering the \"too old resource version\" error. This relates to #1649.. Hey @activeshadow, we are actively working on it. Have a repro but not a fix yet, but should hopefully have the fix soon. Will update here when we do, at which point we can cut a bugfix release.. Hey @jsenon, @activeshadow, @Taik -- we just merged a fix for this issue and are working on cutting a release candidate now. Can update in an hour or two once it's available, and then we'd love to have you verify it's fixed in your environment.. Ok, please give buoyantio/linkerd:1.3.1-rc1 / buoyantio/namerd:1.3.1-rc1 a shot to see if the fix your issue, thanks!. Ok, thanks for all of the feedback folks! We will get this officially released as part of 1.3.1 next week.. @siggy I noticed that this change bumps up the image size on the non-jdk images. Is that expected? From testing locally, it looks like it adds about 230mb to each image.. @siggy Thanks for the explanation -- makes sense to me. I didn't realize that the base had bumped up so much.. Hey @hynek, thanks for such a detailed report! I've been trying to reproduce this issue locally, and haven't been able to exactly. But I did notice something in your setup that seems potentially problematic, based on the contents of your consul KV store:\n[\n  {\n    \"CreateIndex\": 137788444,\n    \"Flags\": 0,\n    \"Key\": \"namerd/dtabs/\",\n    \"LockIndex\": 0,\n    \"ModifyIndex\": 137788444,\n    \"Value\": null\n  },\n  {\n    \"CreateIndex\": 137788454,\n    \"Flags\": 0,\n    \"Key\": \"namerd/dtabs/default\",\n    \"LockIndex\": 0,\n    \"ModifyIndex\": 137801542,\n    \"Value\": \"L3N2YyAgICAgICAgICAgICAgID0+IC8kL2lvLmJ1b3lhbnQuaHR0cC5kb21haW5Ub1BhdGg7CgovYWcvdm0vbDVkL3Byb2QgICAgPT4gLyMvaW8ubDVkLmNvbnN1bC8ubG9jYWwvZW52LXByb2Q7Ci9hZy92bS9sNWQvb3RlICAgICA9PiAvIy9pby5sNWQuY29uc3VsLy5sb2NhbC9lbnYtb3RlOwovYWcvdm0vbDVkL3N0YWdpbmcgPT4gLyMvaW8ubDVkLmNvbnN1bC8ubG9jYWwvZW52LXN0YWdpbmc7\"\n  }\n]\nThe first key in that list is \"namerd/dtabs/\", which is also the folder where the second key, \"namerd/dtabs/default\", is located. When testing locally, if I create the first key first and the second key second, then namerd's admin UI doesn't hang, but it does tell me that there are no namespaces found:\n\nIf I use the consul API to delete the first key:\n$ curl -X DELETE http://localhost:8500/v1/kv/namerd/dtabs/\ntrue\nAnd then bounce namerd, its admin interface successfully loads the namespace from the second key:\n\nAm not positive you're experiencing the same issue, but can you try applying the same fix in your setup (delete the namerd/dtabs/ key and restart namerd)?\n\nWhen playing around with this locally, I setup a docker-compose env that might be useful. You can find it here:\nconsul-issue.tar.gz\nThere are some instructions for reproducing the issue in the README.\n. @hynek Thanks for confirming that the workaround works -- that's great! And huge thanks to @Ashald for putting together the fix. I tested the changes from #1816 in my local repro, and they do indeed fix the issue that I was seeing.. I can confirm that the fixes from this branch fix the issue that I was seeing locally. \ud83c\udf89  Will defer to @adleong for code review.. Hey @zsojma -- thanks for reporting this. I had previously opened #1132 to track changing the long span names to something more intelligible, but we ultimately decided against it. More context about that here: https://github.com/linkerd/linkerd/pull/1135#issuecomment-286858204\nThe underlying issue is that the client span name corresponds to the id used by linkerd to label the clients that it builds dynamically. If we strip out parts of the id, then they become ambiguous.\nBut I totally agree that in a kubernetes setup, where multiple transformers and namers are in use, the ids become unwieldy. Maybe we should think about a kubernetes-specific fix like you suggested, since the issue is most pronounced in that environment.. @yurishkuro That sounds totally reasonable. I haven't looked at Finagle's trace implementation in a while. Do you know if it's possible to set span names to the normalized structure like you suggest? I seem to remember that it required strings for span names.. Not your change, but I just noticed that import Keys._ is a relative import here.  I think it would be preferable to only used fully qualified imports and to sort them alphabetically, like so:\nimport com.typesafe.sbt.SbtScalariform._\nimport sbt.Keys._\nimport sbt._\nimport scala.language.implicitConversions\nimport scalariform.formatter.preferences._\n. Totally trivial but you could write this as:\nlibraryDependencies += Deps.finagle(\"thrift\")\n. Can you remove the ImmediateMetricsStatsReceiver dependency from this file, similar to what @olix0r is doing here:\nhttps://github.com/BuoyantIO/linkerd/pull/8/files#diff-55a222bb76bd2ba0c3f3e12164d10be9R6\n. I think sbt convention is no space after \"e2e,\" here.\n. Good question.  Now that #5 has shipped, how you would feel about adding a test dependency for this project on the \"router/thrift-idl\", and then using those bindings to encode the request?  That would be fewer lines of code (I think), and probably clearer overall.\n. You should remove this relative import, since the full package is already imported on the previous line.\n. Yeah -- good catch.  This was intended, since it creates more of a signature block in plaintext.  But happy to split it up if you prefer.\n. Optional suggestion, but how would you feel about using a non-breaking space in the name?  Otherwise there is weird wrapping on the admin page.\nname: `${iface.router}&nbsp;${iface.id}`,\n\n. Unused import\n. mkClientLabel returns a string that's prefixed with \"/\", and using that string to scope the stats receiver causes a \"//\" to appear in all of the resulting stats.  For example:\n$ curl -s http://192.168.99.100:9990/admin/metrics.json | jsonpp | sort | grep 'requests\"'\n  \"clnt/tracer/localhost/requests\": 18,\n  \"rt/ext/dst/id//$/inet/hosted_sessionproxy_1/8088/requests\": 69,\n  \"rt/ext/srv/0.0.0.0/8080/requests\": 69,\n  \"rt/http/dst/id//$/inet/hosted_accounts_1/8086/requests\": 22,\n  \"rt/http/dst/id//$/inet/hosted_auth_1/8082/requests\": 18,\n  \"rt/http/dst/id//$/inet/hosted_events_1/8085/requests\": 32,\n  \"rt/http/dst/id//$/inet/hosted_projects_1/8087/requests\": 84,\n  \"rt/http/dst/id//$/inet/hosted_sessions_1/8083/requests\": 64,\n  \"rt/http/dst/id//$/inet/hosted_users_1/8081/requests\": 101,\n  \"rt/http/dst/id//$/inet/hosted_web_1/8084/requests\": 69,\n  \"rt/http/srv/0.0.0.0/4140/requests\": 390,\nWhereas previously:\n$ curl -s http://192.168.99.100:9990/admin/metrics.json | jsonpp | sort | grep 'requests\"'\n  \"clnt/ext/$/inet/hosted_sessionproxy_1/8088/requests\": 24,\n  \"clnt/http/$/inet/hosted_auth_1/8082/requests\": 1,\n  \"clnt/http/$/inet/hosted_events_1/8085/requests\": 2,\n  \"clnt/http/$/inet/hosted_projects_1/8087/requests\": 8,\n  \"clnt/http/$/inet/hosted_sessions_1/8083/requests\": 15,\n  \"clnt/http/$/inet/hosted_users_1/8081/requests\": 17,\n  \"clnt/http/$/inet/hosted_web_1/8084/requests\": 24,\n  \"srv/ext/0.0.0.0/8080/requests\": 24,\n  \"srv/http/0.0.0.0/4140/requests\": 67,\n. Elsewhere in this file you're using stack.nilStack.  I'd probably leave this import as is and just use stack.nilStack throughout.\n. TIOLI, but I feel like the \"id\" stats scope here isn't really necessary.  rt/ext/dst/$/inet/hosted_sessionproxy_1/8088/requests seems clearer somehow than rt/ext/dst/id/$/inet/hosted_sessionproxy_1/8088/requests.\n. Nit -- I don't think it's documented anywhere but we are trying to avoid postfix ops where possible.  So I'd write this as:\nval addr = if (rest.nonEmpty && rest.last.contains(\":\")) {\n. Ok, makes sense to me.\n. An example service discovery backend that uses the filesystem to resolve endpoints.\n. Is this file complete?  I don't see a detry for /thrift anywhere.\n. Nvm\n. Small nit:\nval addr = if (segments.nonEmpty && segments.last.contains(\":\")) {\n. I still see an e2e test file at the following location:\nrouter/http/src/e2e/scala/io/buoyant/router/HttpEndToEndTest.scala\nWon't this change cause that test to not be run?  Should that file be fully removed in favor of linkerd/protocol/http/src/e2e/scala/io/buoyant/linkerd/protocol/HttpEndToEndTest.scala?\n. I don't actually see any test files under linkerd/protocol/http.  Was this change intended?\n. I think I'm just misunderstanding how this is setup.  As far as I can tell, adding this line causes us to try to run tests for this project, with the following output:\n[info] Tests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0\nIn which case it seems like it could just be omitted.\n. When I run ./sbt test on your branch, scalariform wants to reformat this section as follows:\n```\ndiff --git a/linkerd/admin/src/main/scala/io/buoyant/linkerd/admin/LinkerdAdmin.scala b/linkerd/admin/src/main/scala/io/buoyant/linkerd/admin/LinkerdAdmin.scala\nindex b4748f7..ab23d06 100644\n--- a/linkerd/admin/src/main/scala/io/buoyant/linkerd/admin/LinkerdAdmin.scala\n+++ b/linkerd/admin/src/main/scala/io/buoyant/linkerd/admin/LinkerdAdmin.scala\n@@ -43,9 +43,10 @@ class LinkerdAdmin(app: App, linker: Linker) {\n       baseRequestPath = \"/\",\n       baseResourcePath = \"twitter-server/img\"\n     )\n-  ).map { case (path, handler) =>\n-    path -> (new IndexView(path, path, () => Nil) andThen handler)\n-  }\n+  ).map {\n+      case (path, handler) =>\n+        path -> (new IndexView(path, path, () => Nil) andThen handler)\n+    }\nprivate[this] def linkerdAdminRoutes: Seq[(String, Service[Request, Response])] = Seq(\n     \"/\" -> new SummaryHandler(linker),\n```\nI'm actually ok with the formatting that you have.  I wonder if there's a way to tell scalariform to accept that?  Just a heads up that you'll need to resolve the issue one way or another before merging since the reformatting will cause issues in CI.\n. \"/admin/server_info\" is in this list twice.  Is that intentional?\n. Triple dash, huh? This doesn't render quite right on Github.\n\n. used (at|by) high-traffic companies...\n. Maybe ### or #### for this heading?  Otherwise it seems to just float out there on the page.\n. I'd call out the developer guide more explicitly here.\n4. Fork the linkerd repo. Follow our [Developer's Guide](devel) to write and test your code changes.\n. D'oh. Didn't read far enough before commenting.  Since you've called it out here, I wouldn't link it above.\n. This list is going to go out-of-date quickly.  Maybe just elide it somewhere around linkerd-admin?\n. I recommend calling this file DEVEL.md, to be consistent with README.me, CONTRIBUTING.md, etc.\n. Would also recommend upcasing CONFIG.md while you're at it.\n. I'm always confused when it comes to tagging in git, but should this be:\n`git tag release-0.0.10 && git push origin release-0.0.10`\n. TIOLI: this sentence feels redundant given the \"File-based service discovery\" section that immediately follows this section.\n. it can also be a convenient way to\n. I'd include rootDir in this example configuration, since it's basically required (maybe it has a default?). Either way, it seems like folks will need to include that most of the time.\n. :+1: to @esbie's suggestion.  I'd just route all HTTP 1.1 GET requests to the fs namer for the purpose of this example.\nbaseDtab: |\n  /http/1.1/GET => /io.l5d.fs\nA few other nits:\n- /$/io.l5d.fs doesn't resolve. That should be /io.l5d.fs\n- /ext/http is determined by the dstPrefix config option for each router, and defaults to /http.  For this example I think it's less confusing to just assume the default.\n. You have both the fs namer and the serversets namer here, which I think is confusing.  I'd do something like:\nbaseDtab: |\n  /http/1.1/GET => /io.l5d.serversets/discovery/prod;\n(note, have not actually tested that dtab to see if it works)\n(first version had a typo, fixed now)\n. And here:\nbaseDtab: |\n  /http/1.1/GET =>  /io.l5d.consul/dc1;\n. baseDtab: |\n  /http/1.1/GET => /io.l5d.k8s/prod/http;\n. Nit -- doesn't look like an example is actually provided here.\n. How would you feel about 9999 instead?  I have a slight concern about using the twitter server default admin port, in the event that the downstream is running twitter server.\n. Can you update this to:\n```\n!/bin/bash\nif [ -z \"$JAVA_HOME\" ]; then\n    JAVA_HOME=/usr\nfi\nexec $JAVA_HOME/bin/java -XX:+PrintCommandLineFlags $JVM_OPTIONS -server -jar \"$@\"\n```\nThe java_exec from our other images was updated yesterday to support multiple JVMs per image.\n. Non-blocking comment -- this line could use some wrapping.\n. Should that first comment actually be \"upstream client is framed\"?  As far as I understand it, the top-level thriftFramed option determines what protocol the routing server should use when handling requests from upstream clients.\n. How about calling this file / trait / object TlsInitializers?  That's my preference since it corresponds to the tls block from the config file.\nAlso, side note, mind wrapping all of your scaladoc comments at 80 chars?\n. I think I'm just missing this in the diff -- what does setting the EXEC (or L5D_EXEC) environment variable accomplish?\n. Ah, got it.  For my own understanding:\n$ docker run --entrypoint='env' --rm io.buoyant/linkerd:0.0.10-SNAPSHOT\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nHOSTNAME=7422c1a6cc13\nLANG=C.UTF-8\nJAVA_VERSION=8u66\nJAVA_DEBIAN_VERSION=8u66-b17-1~bpo8+1\nCA_CERTIFICATES_JAVA_VERSION=20140324\nL5D_HOME=/io.buoyant/linkerd/0.0.10-SNAPSHOT\nL5D_EXEC=/io.buoyant/linkerd/0.0.10-SNAPSHOT/bundle-exec\nHOME=/root\n. I think the default for failFast is false?  I had a tough time figuring it out, but it seems to be defined here:\nhttps://github.com/BuoyantIO/linkerd/blob/master/router/core/src/main/scala/io/buoyant/router/Router.scala#L275\n. Can you give an example of how to configure buffered or framed thrift routers now that the \"thriftFramed\" option is being removed?  I think this relates to what you put in the description, but I couldn't figure out how to run a buffered router without this config option available.\n. Related, it would be great to update the documentation docs/config.md as part of this branc, to reflect the updated way of configuring thrift clients and servers.\n. Trivial style nit (here and below) -- prefer:\nnamer.lookup(Path(Buf.Utf8(\"servicename\"), Buf.Utf8(\"residual\"))).states.respond(state = _)\n. I don't think you import anything from router/http inside the linkerd/protocol/benchmark project -- maybe just remove line 132?\n. Can you change this to:\n./sbt 'project linkerd-protocol-benchmark' 'jmh:run -i 20 -prof gc .*AccessLoggerBenchmark.*'\n. Oh man looks like the tests in this file don't compile and aren't being run.  Investigating now, but I think this is actually a legacy file and should be removed.\n. Good question. It looks like all routers pick up the same set of name interpreters, here:\nhttps://github.com/BuoyantIO/linkerd/blob/alex/config-madness/linkerd/core/src/main/scala/io/buoyant/linkerd/Linker.scala#L65\nAnd since we require at least one router per config, then this assumption is probably ok, although I agree that it is confusing.\n. Agreed, but as far as I can tell this exception is swallowed anyway by jackson's JsonMappingException, so I though it would be more confusing to provide an exception and not have that be the actual exception that is thrown.  I think we should address more sane config parsing error messages in a separate branch.\n. Ok, have addressed this in the latest commit.\n. Ok, really sorry to :bike: :house: on this branch, but I realized that this example config is pretty far out of date.  It doesn't include a namers section, and it has the old, unsupported way of invoking the filesystem namer, plus a few other misc errors that I think we should fix.  How do you feel about these additional changes?\n```\ndiff --git a/docs/config.md b/docs/config.md\nindex e6814ef..4466ce2 100644\n--- a/docs/config.md\n+++ b/docs/config.md\n@@ -15,23 +15,29 @@ below.\n admin:\n   port: 9990\n+namers:\n+- kind: io.l5d.fs\n+  rootDir: disco\n+\n routers:\n - protocol: http\n   label: int-http\n   baseDtab: |\n-    /host     => /$/io.l5d.fs;\n+    /host     => /io.l5d.fs;\n     /method   => /$/io.buoyant.http.anyMethodPfx/host;\n     /http/1.1 => /method;\n   httpUriInDst: true\n+  servers:\n+  - port: 4140\n+    ip: 0.0.0.0\n\nprotocol: http\n   label: ext-http\n   dstPrefix: /ext/http\n   baseDtab: |\n/host     => /$/io.l5d.fs;\n/method   => /$/io.buoyant.http.anyMethodPfx/host;\n/http/1.1 => /method;\n/ext/http => /host/web;\n/host         => /$/io.buoyant.http.anyHostPfx/io.l5d.fs/web;\n/method       => /$/io.buoyant.http.anyMethodPfx/host;\n/ext/http/1.1 => /method;\n   servers:\nport: 8080\n     ip: 0.0.0.0\n@@ -54,7 +60,7 @@ routers:\n     thriftFramed: true\n   thriftMethodInDst: false\n   baseDtab: |\n/thrift => /$/io.l5d.fs/thrift;\n/thrift => /io.l5d.fs/thrift;\n``\n. No \"echo\" here? Pretty sure this doesn't cleanup the test file as written.\n. Smallest of nits -- you're introducing an extra space between \"no\" and \"timeout\".\n. UnfortunatelyTBinaryProtocol.Factoryis not the same type that's produced by callingProtocols.binaryFactory()above, so this deserializer always returnsthriftProtocol: unknownwhenthriftProtocol: binary` is specified in a config.  How hard would it be to update ConfigHandlerTest to also test this field?\n. How would you feel about supporting \"/config.yaml\" as well? I'd accept \"bad\" as a valid answer :smile: \n. I think this actually belongs in the client section of this doc, starting on line 336 below.\n. Pretty sure this import should be:\n\nimport io.buoyant.router.http.{ForwardedFilter, DefaultIdentifier}\n. Same issue here?\nimport io.buoyant.router.http.DefaultIdentifier\n. Unneeded braces here.\n. I think I'm just missing this in the diff, but it seems like wherever you're using a color value you're always calling .join(\",\") -- why not just making this an array of strings to start with?\n. Maybe remove this comment from these 4 test config files?  We don't tend to include it in any of our other files.\n. Hmm, any idea why this compilation failure didn't trigger when we upgraded finagle?  This looks like the right fix, but just curious.\n. Instead of self, the convention we've used previously is to just reference LinkerdBuild directly.  See lines 83, 91, 95, etc. for examples of that.  I think we should either switch everything in this file to use self, or drop this variable and use LinkerdBuild throughout.  I'm good with either approach.\n. TIOLI: I have a slight preference for calling this project val config instead of configCore. configCore implies to me that there's a core sub-project inside the config project, which is not the case.\n. When I run the admin server with your changes, the prepClients method throws the following exception:\nUncaught TypeError: Cannot read property 'map' of undefined\nI think that means that calling routers.clients here returns undefined, but I don't see anything obvious in that method definition that would be causing this.\n. Ah ok, nevermind me.\n. Thanks for adding this! How would you feel about calling it test-config.sh or something similar? That might be more appropriate since we'll now be running it on every commit (hooray!).\n. How about defining withTwitterLibs to call out to withTwitterLib so that you don't have to do the revision mucking in multiple places?  Something like:\ndef withTwitterLibs(deps: Seq[ModuleID]): Project =\n  deps.foldLeft(project) { (_, mod) => withTwitterLib(mod) }\n. I couldn't easily figure this out so will just ask -- do these sbt commands not also require 'set developTwitterDeps in Global := true' in order to run against the finagle snapshots?\n. Mind adding http.pid in .gitignore?\n. And logs/ would probably also be appropriate.\n. :heart_eyes: :heart_eyes: :heart_eyes: Hooray for acceptance tests!\n. Is this just a whitespace change? The previous formatting looked right to me.\n. Hmm, yeah, you're right -- nevermind me. But scalariform seems kinda cray in this case:\nFuture.value(\n      linker.routers.map { router =>\n      val RoutingFactory.BaseDtab(dtab) = router.params[RoutingFactory.BaseDtab]\n      router.label -> dtab()\n    }.toMap\n    )\n. Do you need the metricsListeners var here?  Could it just be var metricsCollector = MetricsCollector([procInfo, dashboard]);?\n. That reasoning makes sense -- totally fine to leave as is.\n. Hmm, I don't think this test even belongs in this file. It appears to be defined in two places:\n$ grep -r 'static(\"hello\", None).tlsClientPrep' ./*\n./linkerd/tls/src/test/scala/io/l5d/clientTls/boundPathTest.scala:    static(\"hello\", None).tlsClientPrep\n./linkerd/tls/src/test/scala/io/l5d/clientTls/staticTest.scala:    static(\"hello\", None).tlsClientPrep\nCan you just remove it from here?\n. And then you can call the test you're adding \"sanity\".\n. Tiny tiny nit: margin: 0 30px; is more consistent with 0-values throughout the rest of this file.\n. Given that we might have more characters that we eventually want to replace here, I think it's more concise to use a bracket expression:\nvar id = obj.name.replace(/[/$]/g, \"-\");\nAnd that said, how about also escaping \".\"? I know that sometimes causes problems for jquery selectors. \nvar id = obj.name.replace(/[/$.]/g, \"-\");\n. This just stems from me not knowing javascript -- why are some function parameters prefixed with \"$\" and not others?  servers vs $serverEl etc.?  Is that to indicate that serverEl is a fully extended object?\n. Cool, yeah, totally fine by me. Just wanted to make sure I understood what was up.\n. Do you need the '{}' argument here? I'd expect merge to work with _.merge(stat, {value: value}).\n. Oh, maybe you don't want to modify stat?\n. Would it make sense to append to the routerLabels array here, rather than replace?  I could imagine somebody calling:\nQuery.clientQuery().withRouter(\"foo\").withRouter(\"bar\")\nAnd expecting that to match both routers.\n. Similar comment here, and for withMetric, withMetrics.\n. Good suggestion, have updated and inlined them.\n. Totally minor, but why use rgba for the grid.fillStyle, and hex for the labels.fillStyle?\n. Super minor -- I think it might make sense to bold the Latencies heading, so that it's more similar to the Requests, Failures, Connections, etc. heading from above.  Right now it looks like:\n\n. I'm pretty ambivalent on this. Finagle's http codec records \"http.uri\", so I'd like to keep that one for consistency.  I could go either way on the rest of them.  I also noticed that Finagle's annotations tend to be hierarchical, so we should probably use \"http.req\" and \"http.rsp\", instead of \"req.http\" and \"rsp.http\".\nSo that said, how do you feel about this as the set of annotations that we record:\n- http.uri\n- http.req.method\n- http.req.host\n- http.req.version\n- http.req.content-length\n- http.req.content-type\n- http.req.transfer-encoding\n- http.rsp.version\n- http.rsp.content-length\n- http.rsp.content-type\n- http.rsp.transfer-encoding\n- http.rsp.status\n. You're iterating over latencyData twice here.  It's not as elegant, but you could save yourself some time if you generate both tableData and chartData in the same pass.  Something like:\n```\nvar tableData = {};\nvar chartData = [];\n_.forEach(latencyData, function(latency, metricName) {\n  tableData[split(\".\")[1]] = metricName;\n  chartData.push({name: metricName, delta: latency})\n}\nreturn { tableData: tableData, chartData: chartData };\n```\n... but with proper javascript conventions :)\n. I don't think you need the wildcard match after the histogram value here, since those should be the end of the string that you're trying to match against.  As written, this regex will also match on p9999.  It would be better to include a trailing anchor to avoid that.\n```\n\nQuery.filter(/^request_latency_ms.(max|min|p9990|p99|p95|p50).*/, [{name: \"request_latency_ms.p9999\"}])[0];\n<- Object {name: \"request_latency_ms.p9999\"}\nQuery.filter(/^request_latency_ms.(max|min|p9990|p99|p95|p50)$/, [{name: \"request_latency_ms.p9999\"}])[0];\n<- undefined\n``\n. Ok, have updated to avoid string interpolation.\n. How would you feel about calling out the/dashboard` URL explicitly as part of this bullet point?\n. This file feels a bit buried to me -- we haven't previously included readme files nested this deeply within the project.  Maybe this information would be more visible as a scaladoc comment in k8s/src/main/scala/io/buoyant/k8s/Api.scala?\n. Why not define booklist within the context of ThirdPartyResourceTest, instead of putting it into a separate object that needs to be imported?\n\n```\nclass ThirdPartyResourceTest extends FunSuite with Awaits {\n  import ThirdPartyResourceTest._\nval bookList = Buf.Utf8(\"\"\"{\"kind\":\"BookList\"...\ntest(\"namespaced: get books list\") {\n    val service = Service.mk[Request, Response] { req =>\n```\n. For consistency with the zookeeper store description in this document, I think this should be written as:\n```\nK8sDtabStore\nexperimental\nio.buoyant.namerd.storage.experimental.k8s\n``\n. Since this isn't a namerd config, I think it's a bit confusing to put it in thenamerd/examplesdirectory.  For instance, we can't run./sbt namerd-examples/3rdparty:run`.  How about moving it to a namerd/examples/k8s subdirectory, or something similar?\n. These type aliases also exist in CatalogNamer.scala -- do you think it's worth trying to remove them from there as well?\n. Question: why do you have to assign this to a val?  It seems like you could just write:\nnamer.lookup(Path.read(\"/srv/thrift/sessions\")).states.respond(state = _)\nBut I think I'm probably misunderstanding what we're trying to do here.  Also, can you make a similar change to this one on line 49 of this file?\n. Ok great, thank for the explanation.  I saw that you were following the same convention in #323 but didn't have the context to understand why.\n. Is this intended to be alphabetical?  In that case I'd put testUtil after marathon.\n. Would it be worth mixing Awaits into this test class? 250ms is the default, so you could just write this as assert(await(v) == version).\n. Should the aggregate list for the all project include etcd? It seems to include everything else defined in this file. And I don't think we'll run tests for etcd by default if it's omitted.\n. TIOLI: would it be worth validating that the version changes after each update as part of this test?\n. Can you add stats to capture the size of each cache while they're running?  I think that will help to inform what the defaults should be.  Overall, I think the default for activeCapacity should be as high as is reasonably possible, since running out of capacity in that cache results in exception, not eviction. \n. Should the second get call here happen against inactiveCache? As written you're fetching twice from activeCache.\n. Would it be worth making the contents of this method explicitly synchronized?  As it is, it's only called inside a synchronized block above, but I'm concerned that we may not remember to wrap it with synchronized if we start calling it elsewhere.\n. Could you add a test to explicitly test that get fetches from inactive and promotes to active?\n. Ohhhhhh. Ok, that makes more sense. Maybe add a comment to explain that?\n. Ah, nevermind, I now see the \"observer cache reactivates observations\" test below.\n. I think this sentence is clearer as: \"If 2.0 is used then every non-retry call will be allowed 2 retries.\" \n. Typo (no requires): \"... and takes two configuration parameters:\"\n. As far as I can tell, both of these parameters are actually required, in which case I'd reword the sentence above, and remove \"Optional\".\n. Related to my comment below, I'm surprised that this config is valid without minMs specified.  Given that the jittered backoff case class is defined as JitteredBackoffConfig(minMs: Int, maxMs: Int), I thought that implies that both params are required -- what am I missing?\n. Ah, in that case I wonder if we'd be better off providing our own defaults.  Something like:\ncase class JitteredBackoffConfig(minMs: Option[Int], maxMs: Option[Int]) extends BackoffConfig {\n  def mk = Backoff.decorrelatedJittered(\n    start = minMs.map(_.millis).getOrElse(Duration.Zero),\n    maximum = maxMs.map(_.millis).getOrElse(100.millis)\n  )\n}\n. (apologies in advance for bike shedding on a 10-character change)\nI actually think this example would be clearer if you volume mounted the file instead of the whole directory, and got rid of the -w flag.  So...\n$ docker run -p 4140:4140 -v /absolute/path/to/myapp/linkerd.yml:/etc/linkerd.yml buoyantio/linkerd:0.0.10-SNAPSHOT /etc/linkerd.yml\n. Ah, hmm -- I think we're probably using different versions of docker.  Mine supports mounting files [link].  But no matter, if it's not universally supported, I agree that mounting a directory and using an absolute path to get rid of the -w flag is the right approach.  Thanks!\n. I see in the test that these stats are changing as the cache sizes change, but I can't actually figure out how that's happening based on initializing them here.  Can you help to shed some light?\n. Ah ok, thanks.\n. I think it would be worthwhile to change this to host: localhost port: 8001, since 443 is typically SSL and we stopped supporting that.\n. I think this header should be the fully qualified name: io.l5d.announcer.Serversets.  Otherwise there's nowhere else in this documentation that provides that information.\n. Yeah, I could go either way, but am inclined to explicitly fail the request since having fewer segments than expected is probably a sign that something is misconfigured.\n. The change in this branch is a result of scalariform reformatting.\n. Nice, this is much easier to follow. Tiny nit -- the parsedKey val doesn't need to be assigned, and instead you could just return directly.  Something like:\nprivate[admin] def formatKey(key: String) = {\n  val escapedKey = disallowedChars.replaceAllIn(delimiter.replaceAllIn(key))\n  escapedKey match {\n    case statPattern(label, stat) => s\"\"\"$label{stat=\"$stat\"}\"\"\"\n    case _ => escapedKey\n  }\n}\nOr perhaps even more concisely:\nprivate[admin] def formatKey(key: String) =\n  disallowedChars.replaceAllIn(delimiter.replaceAllIn(key)) match {\n    case statPattern(label, stat) => s\"\"\"$label{stat=\"$stat\"}\"\"\"\n    case escapedKey => escapedKey\n  }\n. It's auto formatting from scalariform.\n. Just an aesthetic comment, but it feels like the type signatures of recordRequest and recordResponse should be similar.  With your change we have:\nprivate[this] def recordRequest(req: Request): Unit\nprivate[this] val recordResponse: Response => Unit\nMaybe convert recordRequest to a functional val too?\nAlso, TIOLI, but rather than checking Trace.isActivelyTracing in both functions, you could just do it once inside the apply method.\n. I'm going to be sad if we can no longer use the collections API on options in our codebase.  Maybe we could test this change in isolation to see if it's necessary?  I feel like the big efficiency gains from this branch are going to come from the Trace.isActivelyTracing change and the change to only install the filter if tracing is configured.\n. In #388 I'm moving this file to:\nio/buoyant/linkerd/protocol/http/DefaultIdentifierConfigTest.scala\nInstead of:\nio/buoyant/linkerd/protocol/DefaultHttpIdentifierConfigTest.scala\nI think I have a slight preference for the former, given that's how we handle other http-specific configuration, but could switch to the latter if you prefer.\n. This is almost identical to Finagle's TimeoutAndWriteExceptionsOnly partial function, defined here:\nhttps://github.com/twitter/finagle/blob/develop/finagle-core/src/main/scala/com/twitter/finagle/service/RetryPolicy.scala#L162\nThat function does not catch ChannelClosedException.  But maybe you could do something like:\nTimeoutAndWriteExceptionsOnly.orElse(ChannelClosedExceptionsOnly)\nI think tying ourselves to their definition of retryable exceptions is worthwhile, since it may change over time.\n. Hmm, how about?\nobject RetryableException {\n    def unapply(e: Throwable): Boolean =\n      RetryPolicy.TimeoutAndWriteExceptionsOnly\n        .orElse(RetryPolicy.ChannelClosedExceptionsOnly)\n        .isDefinedAt(Throw(e))\n  }\nYou could constantize the RetryPolicy.TimeoutAndWriteExceptionsOnly.orElse(RetryPolicy.ChannelClosedExceptionsOnly).\n. Thanks for updating. These imports are no longer used.\n. Nor is UtilTimeoutException.\n. Big \ud83d\udc4d  to io.l5d.debug tracer.\n. These two tests are also present in linkerd/protocol/http/src/test/scala/io/buoyant/linkerd/protocol/http/MethodAndHostIdentifierConfigTest.scala -- maybe just delete this file?\n. nit: incomplete last sentence here\n. nit: add a space between \"the\" and \"io.l5d.nonRetryable5XX\" here\n. Should you mention the experimental parameter here, along with the host and port params?  Or maybe update the text on line 107 to say that the experimental param is required for this namer?\n. (same comment goes for the rest of the experimental namers in this file)\n. I think this comment should include the experimental config param.\n. Maybe add the experimental flag to the yaml, and assert(!consul.disabled) -- or alternatively leave the yaml as is and assert(consul.disabled)?\n. Unused input EnumeratingNamer?\n. Unused import?\n. Is this still required on the default server stack, now that it's on the default router stack?  Probably doesn't hurt to include it, but just curious.\n. Do you want to include an explanation after the colons here to explain what the deadline and trace headers represent?\n. Nit: missing backtick here after l5d-dtab.\n. Does it make sense to also update linkerd/examples/acceptance-test.yaml to include engine configs?\n. I find this a little bit misleading to appear in the Docker section, since the static_namer.yaml isn't actually \"supplied\" as part of the image -- it still needs to be volume mounted.  That makes it no different from any of the other example config files provided in linkerd/examples/.  In fact, without docker, there's a much more straightforward way to run this config, as follows:\n$ ./sbt linkerd-examples/static_namer:run\n. Sure -- maybe we should just modify the command on line 247 to do that though.\n. typo here: glogical => logical\n. Typo here: \"leve\" => \"level\"\n. It might be more consistent with other initializers to call this file TracelogInitializer.scala, instead of tracelog.scala.\n. Similar comment here as in #594 -- for consistency with other initializers, I'd capitalize the filename.  Would also consider calling it DefaultTelemetryInitializer.scala, similar to DefaultInterpreterInitializer.scala.\n. Question about this -- I think that if the linkerd config is setup to use a tracer (such as io.l5d.zipkin), then id.sampled will always be set to Some(true) or Some(false) before reaching this check, at which point the configured sample rate for this telemeter will always be ignored.  Is that the intended behavior? \n. I don't think the #namers anchor works with the latest => 0.7.5 redirect.  Not sure if there's a better approach though than hardcoding a version here.\n. Small typo here: server => serve\n. I think this might be a copy/paste issue -- pretty sure there is not a port option for storage.  Maybe this should be kind-specific?\n. Oh cool, head works, and that would be my preference.\n. Maybe this should be **Breaking Change** as well\n. Sorry for the confusion -- was commenting on line 18.  I blame github snippets.\n. Suuuper nit picky -- can you add a trailing period here? The rest of the Description cells in this table include it.\n. Hmm, if I revert this then nothing will be using the RetryFilter class that I'm adding in com.twitter.finagle.buoyant.\n. I'm a bit torn on this change. As currently written, our modifications to the stats exported by this filter are additive -- somebody using our filter instead of Finagle's would get all of the original Finagle stats plus one more.  If we rescope the retries stat though, that won't be the case.  Happy to make the change, just wanted to mention why it's written as it is.\n. TIOLI: I have a very slight preference for scoping this as: statsReceiver.scope(\"path\", name) or  statsReceiver.scope(\"dst\", \"path\", name), to more closely resemble the existing dst path stats.\n. Awesome, yeah, I figured there had to be a better way to go about it.. successRate works for me -- switching to that in the latest commit.. Tiniest of nits: can you capitalize this as LowMemSettings, to match the Namerd.LowMemSettings val?. I don't think this .split will play well with ipv6 addresses. Mind adding a test?. Ah, now I see the other comments on this branch... carry on.. Not positive, but I think it's possible that you need to always call Trace.letTracerAndNextId here, regardless of whether or not Trace.hasId.  Otherwise, you won't advance the span ID until the client span is created, and the server span will share its span ID with the caller's client span.. Ok, that makes sense to me too. I guess the question is moot for thrift anyway, since there is no context and no possible way that Trace.hasId would be true at this point in the server stack.. Tiny nit: no need to repeat \"Admin dashboard\" in this sub-bullet. Maybe just \"Now works if served at a non-root url\".. You can also remove the comment on lines 11-14 in this file. \ud83c\udf89 . TIOLI it might be worth mentioning here that this only works for the k8s and fs namers.. I'd like to add an additional scope to the metric name in this case, where the path is scoped by id.  With the current behavior, you're producing two stats that have the same metric name, with just one additional label to differentiate. For instance:\nrt:dst_id:requests{rt=\"external\", dst_id=\"$/inet/web/7000\", dst_path=\"svc/192.168.99.100:8080\", type=\"counter\"} 455\nrt:dst_id:requests{rt=\"external\", dst_id=\"$/inet/web/7000\", type=\"counter\"} 455\nI'm worried that with this setup it would be too easy to double count the stats from \"$/inet/web/7000\". You would need to setup a query that explicitly drops or deduplicates the dst_path label.  Instead, I would prefer for these two stats to end up as:\nrt:dst_id:dst_path:requests{rt=\"external\", dst_id=\"$/inet/web/7000\", dst_path=\"svc/192.168.99.100:8080\", type=\"counter\"} 455\nrt:dst_id:requests{rt=\"external\", dst_id=\"$/inet/web/7000\", type=\"counter\"} 455\nAnd in order to do that, I think you can just change line 68 to:\n(Seq(\"rt\", \"dst_id\", \"dst_path\"), labels0 :+ (\"dst_path\" -> path))\nPut another way, I think it would be a good rule of thumb to expect that every custom label produced by this code also shows up as a segment in the metric name.. Similar to the NamerdInterpreterConfig, I think the mesh config should implement the /namerd nav item in the admin dashboard. Right now, if you configure your routers with the io.l5d.mesh interpreter, we no longer display the \"namerd\" tab in the admin dashboard navigation.. I couldn't figure out a way to configure the mesh interface to serve over TLS. Maybe it's not supported yet? Or am I missing the server TLS option? Interpreter client TLS seems to work.. I'd prefer to unify this with JitteredBackoffConfig.. It's a bit confusing that these are different configuration parameters than those found in TlsClientConfig. Would be nice to standardize, but that would be a breaking change to the existing interpreter config.. In my shell this requires a template argument -- e.g. mktemp -d -t protoc.XXXXXX\n$ mktemp -d\nusage: mktemp [-d] [-q] [-t prefix] [-u] template ...\n       mktemp [-d] [-q] [-u] -t prefix \n$ echo $?\n1. Good catch -- fixed.. Removing the l5d-ci anchor breaks the link on the circle ci badge at the top of this document.. Not part of your change, but I'd recommend also linking to the official linkerd image on dockerhub in this section. Something like:\n\nWe also publish docker images for each release, which you can find on docker hub.. Rather than buoyant, it might be better to change this requirement to a \"linkerd organization member\" or something similar.. Should this be \"reference the github issue by number\"? The PR number won't be known at the time they're creating the commit message.. These should be bulleted or separated by an additional line. Otherwise github combines them into one paragraph.\n\n\n. Typo here: thought of as being the motivation.... #1191.. How do we feel about this change? Happy to add a test but wanted to run it by folks first. The previous code checked for uniqueness among all IDs for a given initializer class. The new code checks for uniqueness among all IDs for a given initializer class and the parent config class.. Ok, I added a test for this behavior in b7124fd.. Yep, TLS is configurable on the client, but the server doesn't support it yet, so there's no point in setting it here. I wanted to document that so that folks didn't see the option and try to use it. Tracking TLS server support in #1191.. For here and the rest of the docs changes, can you wrap these lines at 80 charactoers?. Typo here: there is a retry budget available.... It looks like requestBudget is of type RetryBudgetConfig rather than type RetriesConfig, so I think this should be:\nbudget | See [retry budget](#retry-budget-parameters) | A [requeue budget](#retry-budget-parameters) for connection-level retries.. This anchor isn't necessary since it matches the h2 header exactly. Same goes for the rest of the headers in this file. Can you remove all of them?. Maybe clearer to say \"with parameters defined later in the configuration file taking precedence over those defined earlier\" (if that's actually the case.. Thanks for updating! I'd also link the second requeue budget link to #retry-budget-parameters.. I think this comment should read l5d-dst-service.. l5d-dst-service. Ah, this is good to know. Which system property enables stack traces? Does that also world for Failure types (cases 3 and 4)? I can revert this change.. This is just a refactor. I kept getting confused that we process Http2HeadersFrame.isEndStream and Http2HeadersFrame in separate cases, but we process all Http2DataFrame in the same case. Breaking data frames out into two separate cases feels more consistent with how we're handling header frames.. Ok, good to know. Will revert.. I changed this for consistency with the writeHeaders and writeStream defs defined above. I can revert this, but I might also convert those other defs to vals.. If recvFrame returns false, that means that the call to remote.offer failed, due to the stream already being closed. So I think in this case calling remote.close again would be redundant.. There's gotta be a better way to go about this, but I couldn't figure it out.. Yep, am removing StreamClosed on line 31 in this file.. Cool, yeah, I switched this to a .find(...).map(...).. I removed it per alex's comment, but easy enough to revert this. I could also put together a benchmark to see which is faster.. Ah, ok -- thanks for the clarification. Will revert to the collect methods, but we can revisit later if this remains a hotspot.. Ok, with some console testing, looks like collect and .filter.map are roughly equal, but .find.map is faster than collectFirst. Will revert getAll to use collect. Sorry for all of the back-and-forth on this.. TIOLI: it might be worth hiding these two links if there are no clients, and therefore nothing to expand / collapse. Otherwise it looks like this:\n\n. TIOLI namers.getOrElse(Nil) is a bit more concise.. Ah, yeah, that makes sense. I'm inclined to just remove this echo from the script. Does that work for folks?. Maybe:\n* Add TLS support for `io.l5d.httpController` and `io.l5d.mesh` namerd interfaces.. Maybe:\n* **Breaking Change**: Convert `thriftProtocol` from a client/server param to a router param.. Created #1391 to track it.. Would you mind also changing this email address while you're here? I think we should point it to linkerd-users@googlegroups.com.. I have a slight preference for omitting this import and changing the reference to this class on line 324 to be:\npublic Http2FrameWriter.Configuration configuration() {\nThat's more consistent with how it is referenced on line 83:\nHttp2FrameWriter.Configuration config = configuration();. debug sounds good to me.. Super minor, but I don't think you need to closed.get check here, since the previous case is Throw(e) if closed.get. I'd be inclined to write this as:\ncase Throw(e) if closed.get =>\ncase Throw(e: ChannelClosedException) if streams.isEmpty =>\ncase Throw(e) =>. Yeah, works for me. So something like:\nlog.debug(\"[%s] transport closed: %s\", prefix, e)\nWhich will log:\nD 0714 22:21:54.271 UTC THREAD92: [S L:/127.0.0.1:4140 R:/127.0.0.1:60520] transport closed: com.twitter.finagle.ChannelClosedException: ChannelException at remote address: /127.0.0.1:60520. Remote Info: Not Available\nI think that's sufficiently not scary enough when logged at the debug log level. How do folks feel about me making this change throughout the finagle/h2 project?. Yeah, that seems like a good idea to me. And more generally you should feel free to revamp anything I wrote. Am sure I didn't give it a lot of thought at the time.. I think it's just to indicate that the var is a Closable, but yeah, a better name would be, well, better.. With the modifications that you made to the k8s setup, there's no longer a need for an un-namespaced api object in this file. So I'd just get rid of the nsApi val, and change the api val to:\nval api = {\n    val client = mkClient(Params.empty).configured(Label(\"configMapInterpreter\"))\n    Api(client.newService(dst)).withNamespace(nsOrDefault)\n  }. Just thinking out loud here: we do this type kubernetes activity construction elsewhere in the codebase (e.g. IngressCache and K8sDtabStore), and it all feels pretty convoluted to me. Maybe there's a more generic way to go from Watchable => Activity?. Also, as an aside, one thing we're not doing here that we are doing for IngressCache and K8sDtabStore, is fetching the current resource version before establishing the watch, and then passing that version as the resourceVersion param on the .watch call.\nIf I remember correctly, that's because when I tested this code on Kubernetes, I was running into an infinite loop where the call to watch with a given resource version would fail if the resource version is \"too old\" (more than ~1m old), which would trigger resetWatches, which would then re-fetch the same resource version, try to re-establish a watch, and run into the same error.\nThe reason why we don't encounter the loop with IngressCache and K8sDtabStore is that those are watching list endpoints, and I think it's the case that List endpoints always return the system-wide current resource version. For instance:\n```\ncurl -s localhost:8001/api/v1/namespaces/test/configmaps | jq '.metadata.resourceVersion'\n\"52871682\"\ncurl -s localhost:8001/api/v1/namespaces/test/configmaps | jq '.metadata.resourceVersion'\n\"52871686\"\n```\nWhereas Read endpoints return the resource version for the object is being read:\n```\ncurl -s localhost:8001/api/v1/namespaces/test/configmaps/namerd-config | jq '.metadata.resourceVersion'\n\"45625196\"\ncurl -s localhost:8001/api/v1/namespaces/test/configmaps/namerd-config | jq '.metadata.resourceVersion'\n\"45625196\"\n```\nSo I think it's the correct behavior here to pass in resourceVersion = None to the watch call, but I just wanted to call it out because it is weird. Maybe, to be consistent with the IngressCache and K8sDtabStore implementations, we should be calling a different list endpoint altogether (/api/v1/namespaces?), and using the resourceVersion from that endpoint to initiate this watch.. I think we need to make the same update in the IngressIdentifierTest.scala test file as well, here:\nhttps://github.com/linkerd/linkerd/blob/eliza/configmap-interpreter/linkerd/protocol/h2/src/test/scala/io/buoyant/linkerd/protocol/h2/IngressIdentifierTest.scala#L55. Totally, yeah, this stuff is pretty icky. I would take a look at the implementations of IngressCache and K8sDtabStore, since that's where I cribbed from when putting this branch together in the first place. And don't stress too much if there isn't an easy way to untangle this. I just wanted to give it some attention because it's not super user friendly at the moment.. The formatting of the return value here is a bit wonky, and scalariform is going to rewrite it. Try running ./sbt scalariformFormat on this branch.. This is super subtle, but I think the activity needs to updated as part of the first iteration over the stream, rather than in a separate iteration. As written, I don't think the activity will ever update, since the stream doesn't terminate.\nI tested this branch on Kubernetes and observed that the dtab was never picking up changes to the config map. Switching back to foldLeft fixes the issue, such that changes in the config map are reflected in the dtab as well.\nval _ = stream.foldLeft(initialState) { (s, watchEvent) =>\n            val newState = onEvent(s, watchEvent)\n            state.update(Activity.Ok(newState))\n            newState\n          }. The Closed object is no longer used in this class; I think you can remove it.. Looks like this is unused now as well and can be removed.. Closed is unused here as well.. I think we should log.warn when this happens, to make it easier to track down configuration errors.. Related to my other comment about logging, this case should probably log.warn as well, to let users know that they've configured the interpreter with a filename that does not exist in the configmap.. I think there's another typo here: \"not not\". Can you also standardize the capitalization of IPv6?. Same comment here as above.. I think more generally you want to supply the resourceVersion when calling Watch List endpoints, but you don't want to include it when calling Watch (object) endpoints, since it may be too old. In fact, maybe there's a way to enforce that in the code as well?. Missing return description here?. TIOLI -- could write this as watchPath = s\"$path/watch\".. Same comment here: s\"$path/watch\".. Tiny style nit -- I'd collapse this onto one line:\n).map { list =>\nSame goes for line 235 in this file.. I have a slight preference for just calling this listName; that's a bit more consistent with how we name option params elsewhere, but definitely not formalized in any way.. I'd include the changes in this file with your next branch, instead of this one.. And likewise would move these tests to the next branch as well.. IntelliJ's formatting tends to do this, but I think scalariform should be ok with having them on the same line.. I think that all of the changes in this file and k8s/resources.scala need to be reverted -- they contain reverts of changes that you made in the eliza/object-watchable.. The last part of this sentence isn't really clear to me. Does this mean that automatic retries only happen for requests or responses of a certain size? What's the size threshold after which a request is not retried?. Tiny nit: can you add a Activity[NumberedPortMap] type annotation to this function?. I think this log line should be:\nlogEvent.deletion(endpoints -- newEndpoints). My tendency is to annotate all defs, but skip it for single line vals. Not sure how consistently that's applied in the codebase though.. Can you also bump this netty dependency to match Finagle 7's version? It should be \"4.1.12.Final\".. Let's also bump to twitter-server \"1.31.0\" as part of this change.. Did you know? Having a code block without a format param is usually ok, but in this case totally breaks because of the # sign inside the block. So we're using yaml here and elsewhere in the document, even though the contents aren't yaml.. I can't seem to find this documented anywhere, but as far as I can tell, Kubernetes list resources (e.g. /api/v1/namespaces/ns/services) return the global resource version as part of the metadata response. This means that you can always use the version returned to establish a new watch.\nOn the other hand, Kubernetes object resources (e.g. /api/v1/namespaces/ns/services/mysvc) do not return the global resource version. They instead return the resource version from when the object was last modified, and that version might be too old to establish a new watch.\nFor instance, testing in one of our 1.6 clusters, I make the following request for an endpoints object:\n```\ncurl localhost:8001/api/v1/namespaces/test/endpoints/namerd\n{\n  \"kind\": \"Endpoints\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {\n    \"name\": \"namerd\",\n    \"namespace\": \"test\",\n    \"selfLink\": \"/api/v1/namespaces/test/endpoints/namerd\",\n    \"uid\": \"28d76695-af7d-11e7-a88a-42010af00004\",\n    \"resourceVersion\": \"65078108\",\n    \"creationTimestamp\": \"2017-10-12T18:42:54Z\"\n  },\n```\nThat \"resourceVersion\" reflects when the endpoints object was last modified, rather than the global resource version. If I then immediately make a watch request with the returned resource version, I get:\n```\ncurl localhost:8001/api/v1/watch/namespaces/test/endpoints/namerd?resourceVersion=65078108\n{\"type\":\"ERROR\",\"object\":{\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"too old resource version: 65078108 (65950030)\",\"reason\":\"Gone\",\"code\":410}}\n```\nSo when watching objects, we need to omit the version, rather than risk having the watch fail because the resource version is too old.\nWould love to get a bit more clarity on this though from folks with more familiarity of Kubernetes internals.. Have confirmed that this is the same behavior in Kubernetes 1.7 as well.. TIOLI -- all of the HTML outside of the <div class=\"container-fluid\"> tag here is the same as the HTML that's constructed on line 33 of this file. You could move the wrapper HTML to a separate string template so that you only have to define it once. Something like:\n```\ndef render(name: String, dtab: Dtab): Future[Response] = {\n  val content = s\"\"\"\n    ...\n  \"\"\"\n  return renderHtml(content)\n}\ndef renderError(name: String, e: Throwable): Future[Response] = {\n  val content = s\"\"\"\n    ...\n  \"\"\"\n  return renderHtml(content)\n}\ndef renderHtml(content: String): Future[Response] = {\n  val wrappedContent = s\"\"\"\n    <!doctype html>\n      ...\n      $content\n      ...\n    \n    \"\"\"\n    val response = Response()\n    response.contentType = MediaType.Html + \";charset=UTF-8\"\n    response.contentString = wrappedContent\n    Future.value(response)\n}\n```. Mind wrapping this line at 80 characters?. Hey @AsCat -- thanks for the PR! The previous version of this command works on my machine:\n$ ./sbt namerd-examples/basic:run\n...\nI 0302 11:13:15.570 PST THREAD83: serving http on /127.0.0.1:9991\nI 0302 11:13:16.048 PST THREAD83: Tracer: com.twitter.finagle.zipkin.thrift.ScribeZipkinTracer\nI 0302 11:13:16.111 PST THREAD83: serving io.l5d.thriftNameInterpreter interface on /127.0.0.1:4100\nI 0302 11:13:16.117 PST THREAD83: serving io.l5d.httpController interface on /127.0.0.1:4180\nWhereas the updated version does not:\n$ ./sbt namerd/examples/basic:run\n...\n[error] Expected configuration\n[error] Expected ':' (if selecting a configuration)\n[error] Expected key\n[error] Not a valid key: examples (similar: compilers, name, extra-loggers)\n[error] namerd/examples/basic:run\nThe issue is that \"namerd-examples\" is an sbt project name, and projects names can't contain \"/\" characters, so they don't map directly to filesystem paths.  In fact, our sbt configuration has a helper method for converting paths to project names, by replacing \"/\" characters with \"-\" characters.. ",
    "adleong": "resolved conflicts and rebased\n. Hmmm... I can't reproduce the circleci failure locally. \n. Moved thrift-idl into its own project.\n. resolved conflicts and rebased\n. I'm having trouble reviewing this because it's not clear what's a move and what's a change.\n. :star: looks good to me\n. addressed feedback and rebased\n. You are correct.  Moved it into linkerd/core.\n. I tested this locally with a self-signed cert.  It works perfectly with curl -k which does SSL but skips certificate validation.  If instead, I point curl at the cert and try to validate it, certificate validation fails.  Not sure if this is just an issue with curl and self-signed certs or if something is actually wrong.  I'd appreciate some help from someone who knows TLS a bit better than me (ie at all)\n. cert and key files were generated by following the instructions here: https://devcenter.heroku.com/articles/ssl-certificate-self\n. :star: rebase\n. This seems to work:\n@JsonTypeInfo(use = JsonTypeInfo.Id.NAME, include = JsonTypeInfo.As.PROPERTY, property = \"protocol\")\ntrait RouterConfig {\n  var label: Option[String] = None\n  var baseDtab: Option[String] = None\n  var dstPrefix: Option[String] = None\n  var failFast: Boolean = _\n  var timeoutMs: Option[Int] = None\n}\n```\n@JsonTypeName(\"http\")\ncase class HttpRouterConfig(\n  httpUriInDst: Boolean,\n  servers: Seq[ServerConfig]\n) extends RouterConfig\nclass HttpRouterConfigRegistrar extends ConfigRegistrar {\n  def register(mapper: ObjectMapper): Unit = {\n    mapper.registerSubtypes(classOf[HttpRouterConfig])\n  }\n}\n```\n. Nice, I like this approach a lot.\n. Juice. :star: :closed_lock_with_key: \n. TlsPrep in the commit message should be TlsClientPrep\n:star: othewise\n. :star: :star2: :star: \n. Doesn't look like there's any info in config.md on namers yet.\n. :star: rebase\n. I think I disagree.  If there are multiple servers, which server should the client inherit from?\nBetter would be some kind of config linter or warning if your client and server don't match.\n. :star: reabse\n. :star: rebase\n. :star: nice\n. :star: :closed_lock_with_key: openssl be crazy\n. Is does the consul namer use a streaming api like the k8s one does?  If I'm reading the code correctly, the retrying filter will retry any requests that fail outright (eg return a 500 or exception) but I don't think this handles the case where the request is successful and starts streaming data but then later the connection gets dropped and the stream cuts off.  I think the k8s retry stuff that Greg wrote had some logic to issue a new request if the stream got cut off.\n. Ah ok, that's good.  Looks good then modulo Oliver's style comments :+1: \n. :star: :balloon: \n. :star: :balloon: \n. :star2: \n. :star: :boat: \n. :star: :boat: \n. :star: :camel: \n. Fixed in https://github.com/BuoyantIO/linkerd/releases/tag/0.6.0\n. :star: lgtm\n. I don't really understand how tracing worked/works/will work.  Can you take me through this review at some point?\n. LGTM :star: :pencil2: \n. I wonder if other json parsing libraries might be easier to work with.  I've heard good things about argonaut, though I've never looked into it myself.  Not that there's anything especially wrong with jackson, just a a thought considering we probably don't need jackson's speed for just parsing configs.\n. What seems to be missing that I'd like to see is the step that converts a validated config into an actual io.buoyant.linkerd.Linker.  Thinking about that step raises the question about what is the difference (semantically) between a validated config (with defaults applied) and the final Linker object?  I think we can possibly drop a step here.  In my mind it goes something like:\njson/yaml string ---(jackson parsing)---> Config objects which mirror the structure of the json/yaml ---(validation and application of defaults)---> Linker\n. It seems redundant and confusing to me that you have validated types like LinkerConfig.Validated and also wrap that in ValidatedConfig. Isn't having a ValidatedConfig[LinkerConfig] sufficient to indicate that the LinkerConfig has been validated?  If that can work it lets us get rid of the whole parallel structure of Blah.Validated which would be nice.  I'm imagining something like:\n```\ntrait NamerConfig {\n    var kind: String = \"\"\ndef validate: ValidatedConfig[NamerConfig] = {\n    if (kind.length > 0) ValidatedConfig.Valid(this) else ValidatedConfig.Invalid(Nel(KindLengthError))\n}\n\n}\ntrait ServerConfig {\n    var port: Int = 0\ndef validate(otherPorts: Seq[Int]): ValidatedConfig[ServerConfig] = {\n    if (otherPorts.contains(port)) ValidatedConfig.Invalid(Nel(PortConflictError)) else ValidatedConfig.Valid(this)\n}\n\n}\ntrait RouterConfig {\n    def namers: Seq[NamerConfig]\n    def servers: Seq[ServerConfig]\ndef validate: ValidatedConfig[RouterConfig] = {\n    // cats-y foldy stuff to validate namers and servers and collect the validations together\n}\n\n}\ndef parse(yaml: String): RouterConfig = / jackson object mapper magic /\ndef validate(rc: RouterConfig): Validated[RouterConfig] = rc.validated\ndef mkLinker(vrc: Validated[RouterConfig]): Linker = / build the linker /\n```\n. We should also hash out our philosophy around defaults.  If we don't care about distinguishing between unset values which take on default values and values set to the default then we can do:\ntrait Foo {\n  var bar = 7 // <-- default value\n}\nThis means that when we print configs, we're printing configs with defaults applied instead of configs as written, which I think I like.  This also obviates the need for FooConfig.Defaults classes.\n. Got it.  Thanks.\n:star: :package: \n. I'm going to do Static and No Validation in this ticket.  I've filed #64 for per-service and we can delete the Static module then if it's just a special case of that.\n. Are you suggesting:\nrouters:\n- protocol:\n    name: http\n    tls:\n      kind: boundPath\n  servers:\n  - port: 0\nThat seems pretty reasonable to me.\n. All other path rewriting does prefix rewriting so it's weird that this rewrites to a fixed length path instead of just rewriting a prefix of it.  At the very least, I think we need a symbol to indicate \"the rest\" e.g:\nprefix: /http/1.1/\ndst: /host/$2/path/$3*\nThe star indicating that everything from segment 3 and onward should be written.  /http/1.1/GET/foo/bar/baz should rewrite to /host/foo/path/bar/baz\nOR\nWe do something pattern matchy:\npattern: /http/1.1/$1/$2\ndst: /host/$2/path\nWhich would prefix match the /http/1.1/GET/foo in the above example and rewrite it to /host/foo/path and therefore the final dest would be /host/foo/path/bar/baz.\n. What do you think of my second suggestion?  ie put capture variables in the pattern and substitute their values in the dst.  \nUnrelated: should we use a symbol other than $ to refer to variables?  $ already has a meaning to address namers...\n. Even for the TLS use-case, this seems a bit annoying to work with because you have to count path segments off the end of your prefix. \nprefix: /http/1.1/\nname: ${3}.buoyant.io\nmeans you have to think about what segment comes 3 positions after /http/1.1.  An more explicit alternative would be\nprefix: /http/1.1/$1/$2/$3\nname: $3.buoyant.io\nWhere you can explicitly name variables in the prefix.  In fact, you could choose meaningful names and possible have a special variable for ignoring segments\nprefix: /http/1.1/$_/$_/$host\nname: $host.buoyant.io\n(except, as mentioned above, we probably don't want to use $) \n. I dig it.  :star: :palm_tree: \n. :star: :watch: \n. TODO:\n- [x] fix tests\n- [x] scaladoc all the things\n. #26 is nice because it can be implemented just as a namer, but it still feels a bit verbose and awkward to me.  The thing that I find difficult to reason about with dtabs is that it's always a prefix that gets rewritten but you have to keep in your mind what's left over.  Every rule is really all about the unwritten stuff that comes after it.\nI don't think it would be crazy for us to provide a layer of syntactic sugar on top of the dtab syntax.\n/http/1.1/*/* => /path\ncould be sugar for\n/http/1.1 => /$/dropSegmentsNamer/2/path\nor something like that\n. Another option is to have a pre-processor that desugars our custom syntax into a legit dtab. \n. That's true.  I was thinking about this only in the context of configs.  But you're right, multiple syntaxes is a huge bummer.\n. Hi @jimmidyson, thanks for the info.  This behavior can happen if linkerd in unable to connect to the k8s API.  We've got a couple of issues tracking making this situation easier to debug including adding better logging (#272) and adding a timeout for name resolution (#273).\nIn your particular case it looks like you've got the k8s namer configured to talk to localhost.  Do you have the k8s master running on localhost?  Or are you running through a proxy?  In the case of a proxy, it's likely that TLS won't work and you'll need to turn that off in your config.\nCan you check if you're able to hit the k8s API manually on localhost:8443 over https?\n. thanks for digging into this, @jimmidyson!\nAgreed that timeouts are important, we're adding them soon.  As for the behavior you discovered with the filters: that's very strange.  The behavior you found suggests that something is going wrong in the AuthFilter, but if you take a look at the contents of that filter, it's pretty simple.\nWe're going to try setting up kuberenetes locally to see if we can reproduce the issue.  In the meantime, I encourage you to keep digging and let us know if you find anything.\n. Could you also post the full linkerd log when running with -log.level=DEBUG and -com.twitter.finagle.tracing.debugTrace=true and exercising the issue?\n. I've found a potential explanation.  Does your auth token file end with a newline?  Attempting to set a header value with a new line causes an exception.  Unfortunately, that exception is getting swallowed and not logged today, which makes this kind of thing very hard to debug.  We'll definitely fix that in #272.  In the meantime, try stripping out any newlines from your token file and see if that helps.\n. Glad to hear it!  This definitely showcases the need for better error logging.  Stay tuned.\n. :star: rebase\n. :star: rebase\n. Whoa.  I'm curious how this caching works.  The two requests resolve to the same connection, and the service factory for that connection is cached exception?  Should requests with different host headers resolve to the same connection?\n. Ah, I see.  So NameTree.Neg will always map to noBrokersAvailableFactory.  Makes sense.\n:star: :name_badge: :palm_tree: \n. :star: \n. Add example and rebase :star:\n. The marathon events api streams a non-chunked response using the Server-Sent Events format: https://www.w3.org/TR/eventsource/\nThis may be a problem for us because it seems finagle-http does not support non-chunked streaming.\n. :star: :flags: \n\n. :star: :christmas_tree: \n\n. :+1: \n\n. :star: :pencil2: nice\n\n. :star: :100: \n\n. The config refactor made the servers section mandatory but protocols still define default ports.  We should either keep the default port stuff in the docs, or remove it from the code.\n. :star: I dig this \u26cf\n\n. LGTM\n\n. Caution: Rabbit Hole Ahead\nAfter trying for way too long to add a test for this, I've given up.  It seems that in tests, Jackson adds a field for the subtype class name e.g.\n\"protocol\":\"Fancy\"\n(Note that the class name is potentially different from the property value used to identify it.  Typically we use fully qualified class names as identifiers whereas Jackson will just print the simple class name here.)\nThe Jackson documentation (https://fasterxml.github.io/jackson-annotations/javadoc/2.4/com/fasterxml/jackson/annotation/JsonTypeInfo.html#property()) states that if the object has a property with the same name, the value from the object should be printed instead.  This works as expected when linkerd is run normally (not in a unit test).  However, in unit tests, jackson will print the class name in addition to the value from the object.  eg\n{\"protocol\":\"Fancy\",\"servers\":[{\"port\":2}],\"protocol\":\"fancy\"}\n(The uppercase \"Fancy\" is the config class name, the lowercase \"fancy\" is the name of the protocol.)  I have no idea why it does this or how to stop it.\n. :star: rebase\n\n. :star: rebase\n\n. What about renaming HttpIdentifierInitializer to just IdentifierInitializer and moving it into core?  You would still have BasicHttpIdentifierInitializer extend IdentifierInitializer and you'd be able to change the load service line to LoadService[IdentifierInitializer] and kill the unwanted dependency.\n. HttpConfig would still contain a HttpIdentifierConfig so you would only be able to specify an http identifier in the http config.  HttpIdentifierConfig would still produce an Identifier[http.Request] so I think you're still good from a type-safety point of view.\nWith my suggestion, all IdentifierInitializers would get loaded (potentially for multiple protocols) but they would only be used if there was a config class that included them.\n. :star: nice! :id:\n\n. :star: rebase\n\n. :star: :mahjong: \n\n. I'm not totally sure, but I think it will work as you describe.  Zk2Resolver memoizes (hosts, path, endpoint) => Var[Addr] so both resolutions should get the same Var[Addr].  This is similar to if you did multiple calls to resolve the same path.\n. :bike: :house: \nSince the clients don't represent some kind of scale or gradient should we avoid a gradient color scheme?  What about a set* scheme?\n. :star: :hourglass_flowing_sand: :finnadie: \n\n. It should probably be configurable.  According to https://linkerd.io/config/1.1.0/linkerd/index.html#marathon-service-discovery the default poll interval is 5s so maybe a good default value for the jitter amount would be something like 500ms.. :star: :hamburger: nice\n\n. Maybe worth adding a test to LoadBalancerTest to test the presence of the param.\n. :star: :ship: \n\n. Is it possible for this to be moved to io.buoyant.admin instead of io.buoyant.linkerd.admin?  Eventually we'd want to be able to use this in namerd.\n. Makes sense!\n. :star: :headphones:\n\n. namespaces may contain slashes.  zk treats the name as a path.  this means that to create foo/bar/bas, foo and foo/bar must already exist\n. closed in favor of #848 . in the case of zookeeper storage, the version number doesn't come back in the response from create.  that means that it would require a second call to zk to get the version number.  we can definitely do this, I just wanted to point out that it's not free.\n. I agree in general that we need to move away from the finagle-serverset abstractions to have the flexibility we want.  In this case, setData returns a Stat but create does not.\n. My last comment wasn't clear.  Zookeeper's create method does not return a Stat so I don't think moving off of finagle-serversets helps in this particular case.\n. punting for now until we come up with a more reliable version of Validator\n. :star: :sparkles: \n\n. Note that I think we should continue to have validator run against assemblies instead of from source, to catch any packaging bugs.\n. :star: :fireworks: \n\n. :star: :memo: :penguin: \n\n. :star: nice, thanks for cleaning this up :curry: \n\n. This branch was a proof of concept and will not get merged, at least not in it's current form.  We're still figuring out the design of how dtabs could be combined.  I'd love to hear more details about your use case and what you want to be able to do with automation so that we can make sure that use case is accommodated. \n. :star: nice \ud83c\udf35 \n\n. I think I like this.  Seems like a nice separation of concerns.  Skipping delegation on the addr call makes perfect sense to me.\n. :star: \ud83d\udc53 \n\n. I just finished my second pass.  It makes a lot more sense now that I have the appropriate context, thanks!  Overall I think this looks solid.\n. :star: \ud83c\udfb1 I like it!\n\n. nice\n. :star: \ud83d\udc19 \n\n. Oh hell yes!\n. :star: I like it \ud83c\udf33 \n\n. :star: \ud83c\udfa3 \n\n. Thanks for the feedback!  Would you mind creating issues for the items you want addressed in followups?\n. :star: \ud83c\udf67 \n\n. [Q] just curious, how could you receive more responses than requests sent?\n. :star: \ud83d\udcc8 \n\n. oh right, windows.\n. I think interpreter pulling in linkerd-core is a mistake.  It looks like it can just depend on namer-core instead.  I don't think interpreter is linkerd specific and I think it makes sense at the top-level or possibly in namer.\n. :star: \ud83d\udd2d \n\n. :star: \ud83c\udfc1 \n\n. Seems like the \"integration\" tests don't run in CI.  I guess because it might be flaky?  Not sure.\n. I pushed alex/dcos-bootstrap which shows how this could be done without depending on plugins.  Siggy correctly pointed out that this could be brittle in the face of our config structure changing (it currently looks for a section named \"storage\") so there's a trade-off.  Let's discuss. \n. Looks great, just a few small questions and nits\n. :star: thanks for updating! \ud83c\udf63 \n\n. Does this specifically pertain to the DCOS bootstrap?  The regular ZkDtabStore supports configurable ACLs.\n. Fixed in #696\n. :star: \ud83d\udcfc \n\n. closing this in favor of moving towards the io.l5d.mesh api.. :star: \ud83c\udfb8 \n\n. Looks good.  Two questions:\n1. How do we test this?\n2. What happens if the namerd instance we're talking to goes down and we're forced to talk to a new one?\n. I guess the pathological case is where you reconnect to another namerd and the stamps happen to be the same even though the value is different.  In this case, the client would be stuck with a stale value until the next time it's updated.  Maybe we don't need to worry about this for now...\n. This is less important because we plan to switch to a gRPC API.  However, should we still do this anyway?. Closed in favor of io.l5d.mesh. :star: Thanks for updating.  Just nits remain.  I like the way this looks.  \ud83c\udf3e \n\n. :star: I just looked at the screenshots, not the code.  Looks really great, I think this is a huge improvement for readability. \n. :star: \nTIOLI what about \ud83d\udea2?\n\n. Can we decide on a project wide convention for how to name plugin kinds?  We're currently in a weird state where fully qualified class names are used in some places and not others, some kinds live in the io.l5d package while others live in the io.buoyant package.\n. https://docs.google.com/document/d/1_siXmv7mLrXlylFxzFWz3mtlJOwUdyrfS3tKgaCYrKQ/edit#\n. Similar to how the thrift interface works.  I'd imagine you can poll with a version to get an update when that version is out-of-date, or you can poll without a version to get the current state immediately.\n. Does anyone have opinions about using Guava cache for this?  Seems like a good fit since it's  thread-safe and also supports LRU semantics https://github.com/google/guava/wiki/CachesExplained\n. I think the purpose of caching is different in the streaming world vs in the stamped polling world.\nStreaming\nFor streaming, the purpose of caching Activities is simply to make sure that multiple requests for the same name can share an Activity and to avoid unnecessarily re-creating Activity objects.  If an Activity gets kicked out of cache while there is an open request against it, that request will keep the Activity open even though it's no longer in the cache and the request will continue to get updates.  Also note that having an Activity in cache does not necessarily mean that the Activity is being observed so the only cost to keeping Activities in cache is the memory they take up.\nFor this scenario I think we should use a Guava cache with LRU or time based eviction.\nOpen question: how do we put a limit on the number of observed Activities?\nStamped Polling\nWith stamped polling, on the other hand, caching also serves as a way to manage the observations.  If an observation is torn down while there are outstanding requests against it, those requests will hang forever.  Therefore, Observations need to be reference counted if we ever want to be able to tear them down.  I think in this case we actually want two caches:\n- A ConcurrentHashMap called the \"active\" cache for observations with open requests against them as well as their reference counts.  When a request is received, the observation is looked up (in both caches) or created, it's moved to the active cache if it isn't already there, and the reference count is incremented.  When that observation's value changes, responses are sent, the reference count is set to 0, and the observation is moved to the inactive cache\n- The inactive cache is a cache for observations that are not currently being observed.  This is similar to the cache in the streaming scenario in that it's purpose is just so that we don't have to recreate Activity objects.  This can be a guava cache with LRU or time based eviction.\nBy limiting the size of the active cache, we can limit the number of open observations and reject requests when that limit is reached.\n. Good question.  I don't think so because ports is exposed and used as an Activity elsewhere.  I'll see what else can be done to simplify, though.\n. :star: \ud83c\udf77 \n\n. It doesn't, currently.  It could, with some refactoring, if you think that would be useful.\n. @siggy that's correct.  We could implement de-dupping, but I'd prefer to do that in a follow-up.\n. Closed due to inactivity and lack of specifics.. Good points.  This requires some more thought.  I'll try to write a test to capture exactly the problem that I'm solving.  Will also update the description accordingly.\n. :star: Looks good to me, acknowledging that much of this will change as we mess around with the dtab namespace format.\n\n. Yes, I think that's right.  Actually, I think this was already the case since we flatMap the NameTree with the Addr.  Any changes to the NameTree will therefore result in an update of your observation.\nEdit: oh wait, I think I see what you mean.  The NameTree for a Path is not synonymous with the NsCache.  A change to the NsCache may or may not cause a change to a particular NameTree... Hmmm... let me think about this some more.  Perhaps the complexity in the original implementation was necessary.\n. I'm starting to see why the complexity in this implementation is necessary.  This was a helpful exercise.  Closing this PR.\n. \ud83d\udc4d \ud83d\udea2 \n\n. Can you add response classifiers to linkerd/CONFIG.md too?\n. :star: \ud83d\udc30 \n\n. :star: :rocket: :100:\n\n. :shipit: \ud83d\udc90 \n\n. :star: \ud83c\udf89 \ud83d\udcc7 \n\n. I'll bump the x.x.x in another commit when I bump the headVersion\n. I'd appreciate if someone could take another look at this.  I plumbed a StatsReciever through namerd so that the ObserverCache could report cache sizes.\n. \ud83d\udc4d looks good! could you also update config.md?\n\n. :star: \ud83d\udc1e \n\n. :star: \ud83c\udf49 \n\n. :star: Good catch!  Thanks for fixing, @topiaruss.\n\n. We'll actually need you to sign the CLA, @topiaruss, even for doc fixes.  https://buoyant.io/cla/  \nSorry for the trouble and thanks for the fix!\n. :star: \ud83d\udc18 \n\n. reminder to document this in config.md as part of #364 \n. :star: \ud83d\udc26 \n\n. :star: \u23f0 \n\n. Going to hold off on this until kind naming changes go through\n. hi @erdody.  we're still having some internal discussions as to what the right design for announcement in linkerd is.\n. Hi @erdody, good question regarding container address mapping.  Could you tell me more about the setup that you had in mind?  Would you have ips hardcoded into your linkerd config?  Are your containers being scheduled by any orchestration system?  Would the scheduler fill the ip into a linkerd config template when the container gets scheduled?\n. Thanks for the info, @erdody, that's really helpful.  If the inbound server is configured with the container IP, then does this just work as is?\nWe're planning to have this reviewed and merged early this week.\n. @olix0r what if we did away with configuring announcers and announce names separately?  Something like\nrouters:\n- protocol: http\n  servers:\n  - ip: 0.0.0.0\n    port: 4140\n    announcers:\n    - kind: ...\nThe Announcer interface would change to just accept an InetSocketAddress and any naming would be internal to the announcer plugin itself.  For example:\nrouters:\n- protocol: http\n  servers:\n  - ip: 0.0.0.0\n    port: 4140\n    announcers:\n    - kind: io.l5d.serversets\n      zkAddrs:\n      - host: zk.foo.co\n        port: 2181\n      name: /discovery/prod/foo-service\ndo you think that would be better?\n. :star:\n\n. :star: this is cool \ud83d\udc2f \nTo make sure I understand, suppose we had a union: /foo => /srv/a & /srv/b\nWith this change, a (retryable) failure from service a gets retried in the path stack and the retry could get sent to either a or b.  How does finagle load balance over the union?  I assume that a and b each have their own load balancer which is not shared, which means that 50% of requests will also go to each client, even if one is slower than the other?\n\n. is it possible to write a test that exercises the case where a request is retried on a different client?\n. :star: \ud83d\udce6 \n\n. :star: \ud83c\udfe1 \n\n. :star: \ud83d\udc56 cool!\n\n. Closing this until we have more specific requirements around an etcd service registry format.\n. :star: \n\n. :star:\n\n. :star: \ud83d\udc10 just a TIOLI on how to handle the case when there are fewer segments than requested\n\n. :star: \u26f2 \n\n. \u2b50 \ud83d\udca3 \n\n. Totally valid concern.  We'd have to hardcode a list of \"old namer strings\" and warn and/or abort if we see them.  We'd probably also only want to keep that check around for 1 or 2 versions: enough to give users a chance to transition but not keep it around forever.  \nIdeally this type of error would be trivial to debug by looking at the delegator UI, but I suspect most of our users wouldn't know to do that.\n. @siggy done: check out marathon.yaml\n. The idea here was to differentiate between global namers (which start with /$/) and configured namers (which start with /#/).  This isn't set in stone; it would certainly be possible to use /$/ for both.  To me, this is less confusing and makes it a bit more explicit why you need a io.l5d.fs namer section, for example, but there's no inet namer section.\n. Thanks all for the input.  I'll go forward with this for now but am happy to revisit if we find that having 2 different prefix symbols becomes confusing.\n. I haven't tried it yet, but it might just be a matter of updating pull-linkerd-docs to pull the whole docs directory instead of just config.md\n. It looks as if we'll have to do a bit of munging to get links to work correctly.  I'll make sure I can get that working before merging this.\n. :star: \nself-star\nI don't mind updating or reverting this if we later decide that the split out files aren't working well.\n\n. I feel like MethodAndHost captures the essence of the identifier even if it's not a totally complete description of everything the identifier uses.  Envelope reminds me of SOAP and meta reminds me of the html meta tag.\n. :star: \u270f\ufe0f \n\n. :star: \u2712\ufe0f \n\n. I've soft of lost track of where we've diverge from finagle's default stack in terms of requeues and retries.  A refresher would be helpful.  Are you sure that it's safe to not have any backoff for requeues?  What about if an entire cluster is overloaded and is trying to recover?  Is \"requeue-storm\" a problem?\n. :star: \ud83c\udfb1 \n\n. :star: \ud83d\udd17 \n\n. :star: \u26be\ufe0f \n\n. I don't think we need to update the config initializer name.  That convention that we've seemed to have fallen into is that configured namers can have more verbose names since these don't show up in configs ever (only kind names do), while classpath namers should have more concise (lowercase) names.\n. :star: \ud83c\udfed \n\n. Looks good.  Would feel more comfortable if there was a test for the dst binding factory change.\n. :star: LGTM\n\n. :star: :100:\n\n. :star: thanks, @obeattie \n\n. :star: \ud83c\udf04 \n\n. Looks good, just some nits\n. :star: \ud83c\udf86 \n\n. :star:\n\n. :star: \ud83d\udc53 \n\n. :star: \u231a \n\n. Nice.  Some questions about the screenshots:\n- In the 100% screenshot, I don't actually see a line at 100% in the line graph.\n- In the other screenshots, the y-axis labels range from 0.0 to 1.0 instead of 0% to 100%\n- The line graphs look very different client to client but the numerical success rate is the same for each client.  Is this just an artifact of the fake data?\n. :star: It would be cool if there was a way to force the y-axis range to have a minimum of 0.001%.  That way, a constant 100% would have a y-axis range of [99.999, 100] and the line would show on the top instead of the bottom.  \n\n. :star: awesome! \ud83c\udfef \n\n. :star: \n\n. :star: \ud83d\udcca \n\n. closing in favor of #453\n. Are you able to see what is being allocated in that method?  Looking at the code I see allocations of two closures (one passed to rescue and one passed to ensure) which probably show up a PartialFunction and Function0.  \nThose closures can't just be val'd up though, because they capture some local variables.  \nIf scala isn't smart about how for is desugared, there might be allocations of Function1 as well from the desugared flatMap calls.  Those should be easier to fix, if that's happening.\n. closing in favor of #453 \n. https://github.com/linkerd/linkerd/wiki/Debugging-a-linkerd-setup. :star: \ud83d\udcbb \n\n. Closing in favor of https://github.com/BuoyantIO/linkerd/pull/457\n. Good question.  We want the cache capacity to be big enough to accommodate the number of distinct paths and bounds that linkerd will be receiving.  In practice, I don't really know what that is.  I pulled the 500 paths number out of thin air, but it feels like the right ballpark.  I welcome any input, though.\n. I've bumped the linkerd binding cache size down to 100 and the namerd binding cache size up to 1000.  This is a configuration I've load tested and am confidant works.  It should also be sufficient for small/medium sized organizations.  A total of 1000 logical names is probably plenty for many use cases.\nI've also made all of these parameters configurable for those who these defaults aren't sufficient.\n. filed https://github.com/BuoyantIO/linkerd/issues/463 as well\n. linkerd was certainly able to handle a DstBindingFactory path cache of size 500 no problem.  My motivation for sizing this down was that a) in a side-car model, I'd imagine that any particular linkerd wouldn't target very many logical paths.  i.e. the number of logical downstreams of a service is probably < 100 (or at least that's my unqualified intuition) and b) this cache doesn't have any time-based eviction so I wanted some mechanism to limit the number of \"one-off\" watches that are kept active.  e.g. if a developer sends some test traffic through with a different logical path, we probably don't want to keep a watch open on that path forever.  Shrinking the cache limits the number of these one-off watches.\n. One possibility would be changing the identifier contract to Request => (Id, Request).  This allows the identifier to output a modified request.  Retries in the path stack would simply be retrying the modified request (the identifier wouldn't be re-run).\n. :star: \ud83d\ude85 \n\n. Admitting commas in paths feels like a more invasive sweeping change.  I could imagine there existing comma-separated lists of paths out there somewhere...\n. Documented here: https://linkerd.io/config/0.8.1/linkerd/index.html#context-headers\n. :star: \ud83d\udc87 \n\n. :star: \u25b6\ufe0f \n\n. :star: \ud83c\udf0c \n\n. Thanks for the fix, @obeattie!  I think it's a good idea to test this corner case.  Would you like to add tests to this PR in a few days when you have time?  Otherwise, one of us can take it over and add the tests.  \nThanks again!\n. Thanks, @obeattie, this is great!  Sorry for the delay getting this merged.\n. :star: \ud83c\udf16 \n\n. Closing in favor of https://github.com/BuoyantIO/linkerd/pull/580\n. :star: \ud83c\udf73 TIOLI: is possible to add a test where an exception is thrown in the client and verify the right stats get incremented?\n\n. :star: \ud83d\udd25 yeaaaahhhhhh!!\n\n. :star: Nice detective work \ud83d\udd0d \n\n. nope: https://github.com/BuoyantIO/linkerd-examples/compare/alex/plugin?expand=1\n. :star: \u23f0 \n\n. :star: looks great!  Are you planning on doing the \"help\" page as well in a followup?  \nThere's also the issue of the logging page which is straight up, twitter server.  Not sure if we want to bother restyling that or what it would take.\n\n. Interesting.  What do you think about making an offline copy of just the help page and styling it to match?    The links to the slack and github repo wouldn't work if you're offline, but I think that's probably okay...\n. I Am Not A Designer, but I think that logging page is an improvement!\n. Are client timeouts particularly useful?  For any of the built in request classifiers, timeouts are not retryable, so having a timeout underneath retries doesn't really accomplish anything.  Of course, it's a different story if we can classify requests as idempotent, but none of the built in classifiers do that.  And if we DID do that then I think BackupRequestFilter is probably a better way to cut down on tail latency for idempotent requests than retries are.\nI also think that server or client timeouts should override router timeouts instead of taking the minimum, in case you need to lengthen the timeout on a particular server for some reason.\n. :star: \ud83d\udc4d \n\n. :star: \ud83c\udf84 \n. Thanks @klingerf.  That explains it.  Bummer that netcat does that :(\n. :star: \ud83c\udf43 \n\n. :star: \ud83c\udd98 \n\n. :star: \ud83d\udcde \n. :star: \u2b05\ufe0f \n\n. :star: \ud83d\udebf \n\n. You're right.  I've made the cache a Var instead.\n. I've manually validated that there's no double-encoding.\nI tried sending a bunch of compressed request bodies but couldn't get any compression or inflation to show up on the profile.  Let me know if you see otherwise.\n. Hey @JonathanBennett, I'm having trouble reproducing this.  As I understand it, namerd's zk client sends a heartbeat to keep the zk session alive.  Do you have any idea what could cause the session to expire?  By default, the client requests a 10 second session timeout.\nIs the error message you pasted from linkerd or namerd?  If it's from linkerd, is there anything interesting in the namerd logs?\n. namerd should definitely recover by retrying on a new session.  I have a change that I suspect will fix it, but without being able to reproduce the error it's hard to be sure.\n. :star:\n\n. We should add timeouts to the delegator.(whatever).toFuture() calls in DelegateApiHandler and DelegateHandler.\n. Good catch!  I think we can remove this command altogether since it doesn't bundle up the namerd plugins and therefore isn't very useful.\n. This looks great!  Just a couple suggestions, mostly around caching.\n. :star: This is great!  Thanks for putting this together\n\n. :star: Awesome, thanks!\n\n. :star: good sleuthing \ud83d\udd0e \n\n. From giving this a read-through, it looks really good.  I have a bunch of style comments and suggestions but overall it looks great.\n. :star: This is great!  Thanks for all your hard work on this, @Ashald!\n\n. :star: \ud83d\ude92  pending moving the changelog entry\n\n. :star:\n\n. :star: \ud83d\udcca \n\n. This will significantly clean up debug level log output as well \ud83d\ude01 \n. Those branches are exploratory.  We still haven't settled on exactly how this should work.\n. To expand on William's first point, when you request a dtab from the namerd API, an Etag header is returned that represents the dtab's version.  When updating a dtab, you can set an \"If-Match\" header with that value and the update will only be applied if the version matches.\nWe'll work on getting this documented.\n. :star: Thanks for being patient with all my requests!  This looks good to me!\n\n. could you add this to CHANGES.md and the config docs?  Maybe with a warning in the docs that increasing this limit can leads to performance issues.\n. :star: \ud83c\udfe5 \n\n. With this change getting the class name no longer produces an excpetion:\nscala> io.buoyant.k8s.v1.Added.getClass.getSimpleName\nres2: String = Added$\n. This approach makes sense to me, and I'm okay with the name resolve.  The output format should be the same as addr, which is getting a facelift in #575.  The parameters (dtab and path) match the format of the other endpoints, so I think that's good.\n. :star: I think this turned out quite nicely\n\n. :star: Thanks!!\n\n. Our current theory is that finagle expects that Activities never go pending once they have gone ok.  As long as we obey that invariant, we should be ok (no pun intended).  Closing this issue until something contradicts this theory.. :star: \ud83c\udf70 great!\n\n. oh, almost forgot: would you mind adding something to CHANGES.md for this?\n. :star:\n\n. @siggy good call, I'll address in a follow-up.  You're right that #3 is redundant, but it is nice that it follows the same URI structure as bind and addr.  I think #2 is already essentially a pass through to #1, (it just calls into DelegateApiHandler)\n. I don't think your replacement of list with get is correct.  By specifying ?keys and ?separator, we get a list of keys only (not values) from a single directory (not recursive).  Take a look at https://www.consul.io/docs/agent/http/kv.html#single\n. Would you mind splitting this up into a PR for each of your bullet points in the description?  Some of these are obvious wins (ability to pass a dc into consul store, ability to add auth token, etc) whereas the larger refactors are more controversial. \n. This has been split into smaller PRs.\n. :star: This looks great, thanks!  Would you mind adding an entry to CHANGES.md?\n\n. :star: Makes sense!\n\n. :star: \ud83d\udd2d \ud83c\udf0c \n\n. How are you planning to discover the location of the auth service?  If it's in a known location that can be specified in the config block for your plugin, you can have your config block accept a host/port pair and then create a client which connects to that.  If you need to talk to service discovery to find the auth service, things get trickier.\n. Currently you can create a finagle client in your plugin using a classpath namer (e.g. /$/inet/1.2.3.4/8080) but I don't think you have access to the namers configured in the linkerd config.  I've filed https://github.com/BuoyantIO/linkerd/issues/596\n. :star: Great!\n\n. I like the convenience of having a single-value get that returns a String.  Rather than modifying get to support multi-gets, would you consider adding a new method called multiGet or something like that? \n. Is the first item guaranteed to be the one that matches the key exactly?  It seems simpler to keep them separate.\n. Unless the requested key doesn't exist \ud83d\ude1c \nIn that case, taking the first item from a recursive get is definitely not what you want.\nIs lexicographical ordering a guarantee of the api?  I couldn't find that in the docs.\n. This is good.  I think you need to update the title/description, however.\n. :star: looks good!\n\n. I think implementing a second consul namer that uses the health API is a good starting place.  That way we can evaluate it alongside the existing consul namer that uses the catalog API.  If we find that the namer that uses the health API is more useful, we can potentially remove the catalog namer at a later date.\n. @Ashald you may want to coordinate with @JustinVenus who has started on this in https://github.com/BuoyantIO/linkerd/pull/595\n. :star: \ud83d\udc77 \n\n. :star: \ud83c\udfec \n\n. :star: \ud83c\udf6d \n\n. As discussed in https://github.com/BuoyantIO/linkerd/issues/589, I think we'd like this to be a separate namer from the CatalogNamer (though it looks like it will be almost identical) so that we can get a health-based namer merged before validating that it is a suitable replacement for the catalog namer in all cases.\nAlso, I think the passing of passing should be configurable so that we can choose to respect consul's health checks or not.\n. I think if you omit the ?passing parameter entirely, the response won't be filtered to passing services.  As for why you would want that, you may not want to treat consul as the authority on which services are healthy.\n. @JustinVenus that sounds reasonable to me.  In that case I think we would rename CatalogNamer to ConsulNamer since it can be backed by either the CatalogApi or the HealthApi.\n. This looks really good to me, aside from a couple of small style nits.  Could you also update linkerd/docs/namer.md and CHANGES.md?\nAnd possibly add the useHealthCheck option to linkerd/examples/acceptance-test.yaml\n. Hi @JustinVenus, sorry I missed your response to my comment.  I've replied above.\n. Thanks for making those fixes, @JustinVenus.  I just had a couple of requests about the docs and then I think we can merge.\n. :star: Awesome!  Thanks for all your hard work on this, @JustinVenus \n\n. Good point, @olix0r.  I think that since namers are \"top-level\" in the config, making them available to router-level plugins keeps the dependencies one-directional and sane.\n. that's what I did\n. :star: \ud83d\udeaa \n\n. I think this is a fundamental restriction that should be enforced at a higher level than individual response classifiers.  What about wrapping the configured response classifier with a ChunkedRequestsCannotBeRetried classifier:\ncase (req,rep) if req.isChunked =>\n  val classification = underlying(req,rep)\n  if (classification == RetyableFailure) NonRetryableFailure\n  else classificaiton\ncase (req,rep) => underlying(req, rep)\n. :star: \ud83d\udeb0 \n\n. Here is the commit that adds that header logic: https://github.com/twitter/finagle/commit/761b860b89188c24f70406db26003129dbadb6a1\nIs the thrift method that you're calling a oneway method?  If so, the go client should not be attempting to read a response.  That said, I'm not sure why this would cause linkerd to have a degraded success rate until restart.\n. Ah, rats.  Would it be possible to capture a tcp dump, ideally of both the linkerd <-> python side and the golang <-> linkerd side?  Instructions for capturing a tcp dump are here: https://linkerd.io/help/\nA metrics dump of linkerd would also be helpful.\n. Thanks for the metics and pcap, @mapix.  \nLooking at the metrics I see a large number of channel closed exceptions:\n\"rt//dae_thrift_client_proxy/dst/id/#/io.l5d.serversets/dae/services/thrift/glorfindel/PickAd/failures/com.twitter.finagle.ChannelClosedException/java.io.IOException\":3479\nand an even larger number of connect exceptions:\n\"rt//dae_thrift_client_proxy/dst/id/#/io.l5d.serversets/dae/services/thrift/glorfindel/PickAd/service_creation/failures/com.twitter.finagle.ChannelWriteException/java.net.ConnectException\":26564043\nThis indicates that linkerd is unable to connect to the python service for some reason.  Perhaps some kind of network issue?  Or could the python service have been rescheduled but its location not updated in ZooKeeper?\nLooking at the pcap suggests this timeline of events:\n- client sends a thrift request to linkerd\n- linkerd fails to connect to server, throws java.net.ConnectException: Connection refused: hador7/192.168.1.67:43920\n- linkerd returns an exception to client\n- client attempts to parse the response as a list instead of as an exception, fails\nI also notice that the ip address that linkerd is attempting to connect to (192.168.1.67) is not in the list of servers in the readme.  Perhaps that is further evidence that the serverset in ZooKeeper is stale?\nFor next debugging steps, when you see this issue again, I would recommend sending a request to linkerd and noting the response.  It will likely be an exception containing a Connection refused message.  You can then attempt to manually connect to the address in the exception and try to diagnose why linkerd can't connect to it.\n. I've got some tweaks to the way the delegation trees are displayed that makes it more clear what's going on.  What do you think about this:\n\nThis is a separate issue, but I think that response from resolve is actually incorrect.  In the case of Neg, it should fall back to the next arm of the Alt so it should return Fail.\n. Not a bug as much as just a confusing way of displaying the information.  Hopefully what we have now is more clear.\n. I had to walk this back a bit to preserve the existing behavior of Neg.  Neg is slightly overloaded in that it can arise when there are no matching dentries or when a dentry explicitly maps to Neg.  \n\n. A bit more special casing to get both ! and /$/fail working as well as ~ when explicitly present on the right hand side of a dentry.\n\n\n. ```\nscala> Namer.global.lookup(Path.read(\"/$/fail\")).sample\nres6: com.twitter.finagle.NameTree[com.twitter.finagle.Name] = Fail\nscala> Namer.global.lookup(Path.read(\"/$/neg\")).sample\njava.lang.ClassNotFoundException: neg\n```\n. :star: makes sense to me\n\n. :star: \ud83d\udc78 \ud83d\udc02 \ud83d\udce7 \n\n. :star: \ud83d\udcc3 looks great!  the namerd docs should get the same treatment too\n\n. Looks good, just a couple of clean up items.  Thanks for doing this!\n. :star: thanks!\n\n. My understanding is that linkerd doesn't actually use the global NameInterpreter for anything.  The only place the global NameInterpreter would be used is for clients created manually in plugin code (e.g. to connect to the k8s api or the consul api etc.)  \nSo yes, if a namer plugin were to manually construct a client with it's own prefix as a dst, it would probably blow up.\nYes, we should probably do this for namerd as well.\n. I did some testing:\n- using a configured namer in a tracer: WORKS\n  - even though the tracers are passed to the namers, the tracer doesn't start issuing requests until the global NameInterpreter has been set.  so this actually works, though you might run into issues if you're trying to send requests right on startup\n- using a namer in a different namer: WORKS\n  - discovered the k8s api for the k8s namer using the fs namer\n- using a namer in itself: SPECTACULAR STACK-OVERFLOW CRASH\n. I think this would have to be up to the individual namer.  For example, if the io.l5d.foo namer takes a dst as a config parameter, it should validate that the dst doesn't start with it's own prefix.  Currently none of our namers accept dsts so this isn't an issue yet.\n. Thanks for the repro, @obeattie!  I think I now see what you meant the other day about AsyncStream and exceptions.  An exception in an AsyncStream will effectively halt traversal over the stream.  I think we need to make sure that exceptions don't make it into the stream, even if we encounter them while reading from the Reader.  I'll put together a unit test and fix, and make sure to validate it against the repro steps you posted.\n. (The circleci failure appears to just be a test timing out.  Feel free to ignore it.)\n. These docs have been restructured somewhat.  Can you merge or rebase onto master?\n. Thanks for this, @moleksyuk!\n. Here's a unit test for this https://github.com/BuoyantIO/linkerd/tree/alex/k8s-watch-test\n. I wonder if that test that expects a ReaderDiscarded exception is describing the behavior we even want.  In the case where the reader is discarded, I think we want to simply terminate the AsyncStream, not have it contain an exception.  This way, reconnect logic (or whatever else) can be appended to it.  \n@olix0r and @obeattie: what do you think about changing this back to swallowing all (maybe non-fatal) exceptions and changing that test to expect that the AsyncStream is empty instead of expecting a ReaderDiscarded?\n. :star: :100: \ud83c\udf86 \n\n. :star: it's a bit of a bummer that links will break when viewing the docs on github (https://github.com/BuoyantIO/linkerd/blob/master/linkerd/docs/config.md).  But I guess probably no one does that anyway. \ud83d\udc4d \n\n. :star: \ud83d\udcfa \n\n. This is awesome, thanks for taking this on, @Ashald.  Do you think the Addr Cache should get the same treatment as the Bind Cache?\n. If it makes sense to do then we might as well do it at the same time.\n. :star: Looks great!\n\n. Closed in favor of https://github.com/BuoyantIO/linkerd/pull/638\n. \u2795 \ud83d\udcaf \ud83d\udcaf \ud83d\udcaf  to this tabular format. \n. Hi @moleksyuk!  Just to confirm, this ticket is just about the noisy exceptions in the logs, right?  You aren't seeing any incorrect behavior?  \nI think we can clean up the log exceptions with something similar to this: https://github.com/BuoyantIO/linkerd/commit/8085c4c5b6a7dc5f4a6e77b4669cca383e1b63eb\n. I think we can keep this issue open to track cleaning up the noisy exceptions in the logs.  What's going on is that every time a observation on the k8s api is released, that Closed exception is hitting the root monitor and being logged.  Instead, we should change the io.buoyant.k8s.Api.Closed to be a finagle Failure with the Interruptedflag and also add a monitor to the k8s client to swallow interrupted failures.  See https://github.com/BuoyantIO/linkerd/commit/8085c4c5b6a7dc5f4a6e77b4669cca383e1b63eb where we did this for the consul client.\n. :star: seems reasonable\n\n. :star: looks like a good refactor to me.  a refactor of this size probably warrants some testing... do we have a good plan for that?\n\n. :star: \ud83d\udea2 \n. :star: \ud83d\udead \n\n. :star: yuck \ud83c\udf70 thanks for fixing\n\n. UnidentifierRequestException mixes in NoStackTrace, but I agree with you that it's not the best to do control flow this way.  \nAnd API break would be ideal, (either to ADT as your suggest or just plain Option) but I was hesitant because we know there are identifier plugins in the wild that would need to be updated.\n. Still think the API break is worth it? \ud83d\ude09 \n. Closely related to #863 . :star: \ud83d\udcd0 \n\n. cc @JustinVenus \n. :star: \ud83d\udcd1 \n\n. This may be getting off topic for this ticket, but there's some nuance to (3) because of the long polling interface that namerd exposes.  I think this could be accomplished by adding time-based eviction to the inactive observer cache.\n. :star: \ud83d\udea4 \n\n. :star: Looks good other than the value discard warning.\n\n. :star: \ud83d\udebf \n\n. What's the net effect here?\n- Endpoint list at startup moved from INFO to DEBUG level\n- Exceptions during admin initialization no longer print stack traces?\n- Slight refactor without any change of behavior\n- Anything else?\n. :star: \ud83d\udd07 \n. :star: \u2708\ufe0f \n\n. Let me see if I've got this straight:\nOld\n\nNo telemeters => Default tracer + default stats receiver\nSome telemeters but none that provide stats => default stats receiver\nSome telemeters but none that provide tracers => default tracer\n\nNew\n\nNo telemeters => Null tracer + default stats receiver\nSome telemeters but none that provide stats => default stats receiver\nSome telemeters but none that provide tracers => null tracer\n\nWhat even is the default tracer?  Is it ever useful to us?\n. :star: \ud83d\udc61 \n\n. :star: \ud83c\udf54 \n. :star: looks great \ud83c\udfb1 \n\n. thanks, @Ashald!\n. :star: This looks great, @Ashald!  Have you noticed that adjusting the consistency level helps reduce the amount of leader election?  If it makes a big difference on leader election and therefore on linkerd memory pressure, we could consider changing the defaults.\n\n. I'd say let's release as-is.  Once we have some data regarding how this behaves in prod we can consider changing the default in the next release.  Does that sound good?\n. :star: \ud83c\ude50 \n\n. Unable to repro.  Closing until this shows up again.. :star: I didn't try it out, but this looks good to me \ud83d\udd2d \n. This looks awesome!  Let me test it in my k8s setup...\n. :star: works great!\n. Hi @adambom, I believe that 2GB is the limit is for non-chunked requests.  If you are doing chunked streaming uploads, I don't think you will be subject to the MaxRequestSize.\n. :star: great! looks good to me!\n. :star: \ud83d\udd2c \n\n. This is done. This is done.. I haven't been able to reproduce this, but I'm wondering if it might be related to some incorrect retry logic in our k8s code.  \nAre you able to create a build off of this PR (https://github.com/BuoyantIO/linkerd/pull/698) and let me know if the problem still exists?\n. :star: good catch! Thanks, @OleksandrBerezianskyi \n. Could you please sign our CLA? https://buoyant.io/cla/\n. Thanks!!!\n. Hmmm... I don't see where the missing flag is supposed to go.  I see experimental: true on line 118.  Am I blind?\n. I agree that this is a powerful and dangerous tool for when we need to break the assumption that paths are hierarchical.  Let's keep this in our back pocket until a need arises.\n. @esbie given that the sample requests sometimes contain bodies and/or headers, do you have a preferred format for the requests?  curl command?  raw HTTP request?\nI'm also not sure of a good way to describe the response formats that isn't extremely verbose.\n. Fixed by #744. Hi @ManishMaheshwari!  This is a good idea but I think the implementation may be a bit more complex.  Instead of using .sample() we should use map to make sure that the NameTree always has the correct value.  Unfortunately, this means that the NameTree will need to update every time the Addr Var changes, which is not good.  \nIdeally, we would like to set the Addr to Addr.Neg when the Health API returns an empty list and linkerd should fall back to alternates in the NameTree.  However, I'm not sure if that's how Addr.Neg is handled today.  I've filed https://github.com/BuoyantIO/linkerd/issues/712 to investigate.\n. Closing this in favor of something like https://github.com/linkerd/linkerd/pull/744. The most obvious way to accomplish this is, in a namer, have the Activity[NameTree] be mapped off of the Var[Addr] such that when the Addr becomes Neg, we set the NameTree to Neg.  This is problematic because we don't want to trigger NameTree updates on every Addr update.\n@edma2 suggested that if we apply deduping to the Activity[NameTree], then any Addr updates that don't change the NameTree will not cause a NameTree update.  \n/**\n   * Filter out events that do not change the underlying Activity value.\n   */\n  def dedup[T](a: Activity[T]): Activity[T] =\n    new Activity(Var(Activity.Pending, a.states.dedup))\nWe could add this dedupping in each Namer individually, or in DstBindingFactory.\n. Thanks!  Really looking forward to testing this out.  I may not get to it until next week, however.\n. I did some testing with this and it works great!  I cleaned it up a bit in a new branch: https://github.com/BuoyantIO/linkerd/compare/alex/curator-namer?expand=1\n@halve let me know what you think\n. Cool.  What about clientside TLS with netty4?  Does that already work or is that TODO?\n. Closing this in favor of #1006 and #1255. Are you sure that the previous implementation, based on foldLeft identified requests eagerly?  By my reading, that checked the identifiers serially until an identification was found.\n. Your wish has already been granted: https://github.com/BuoyantIO/linkerd/pull/573\nI'm starting to have doubts about baking the dtab into the default interpreter.  This would make it so that the dtab is no longer editable in the dtab playground.  It's also worth pointing out that dtabs are stacked on top of each and therefore the baseDtab isn't useless for other interpreters, it's submitted and appended.  For example, if a baseDtab is specified on a router that uses the namerd interpreter, that baseDtab will be sent as part of namerd requests and namerd will append it to it's own dtab for resolving that request.\n. Done as of the 1.1.3 release. (discussed in-person)\nI'm hesitant to remove this histogram because it seems useful to know how retries are distributed across requests.  I suggested that perhaps we could expose the current retry budget's balance as a more direct indication of how much budget is currently used.\n. This is now the case.  Each path stack has its own individual retry budget.. Nothing, I just instinctively dumped it here.  I'll move it to router-core.\n. This has been sitting idle for a long time, but I really would like to get it in.  Let me see if I can come up with a testing plan for this branch.. Huge thanks to @Ashald for testing this!  As far as I can tell, there are no performance regressions here and I'd like to move forward with this change.  cc/ @olix0r . The usefulness of announcement depends on what kind of environment you're running in.  If you know the IP on which linkerd will serve, you can configure that explicitly in the servers section and that's what the announcer will announce.  If you don't know the IP on which linkerd will serve, (possibly because you're running with some kind of scheduler like marathon or kubernetes) then linkerd can't announce the IP because it doesn't know.  Instead, you should use a namer than knows how to query the scheduler to locate linkerd.\n. That seems reasonable, and probably more useful than announcing 0.0.0.0.  Of course, in a containerized world, the IP that the application can see isn't necessarily the same as the one where the application can be reached externally.\n. I think the session stickiness problem and the geo locality problem probably have different solutions.\nI think it would be possible to build a consistent hashing load balancer that accepts a hashing function and tries to route requests with the same hash value to the same endpoint.  \nFor the geo stuff, I'll echo what @olix0r said about setting up a service discovery entry for each zone/region.  Dtab rules can then be used to prefer more local clusters and fall back to more remote ones if necessary.\n. That's correct, fallback won't trigger until /default leaves service discovery (zk).\n. Some good discussion here but I don't think anything actionable.. I don't think this is a big enough issue to track by itself.  Please feel free to refactor and clean up code smell as you go.. This is done!. Thanks for working on this!  A couple of comments:\n- AppIdNamer accepts the ttl by-value (not by-name) which means that your getTtl and getRandomJitterMs will be called once and that fixed value will be used each time.  Consider modifying AppIdNamer to accept a Stream[Duration] instead of just a Duration.\n- Rather than using 10% jitter, consider making the jitter amount configurable.  For example, a user may want to specify 3000ms ttl with 50ms jitter.\n. Closed due to inactivity.. By my reading the Header stack param has default value of Headers.Authority.  So I think the written description is correct but the table needs to be updated to indicate that the default is :authority, not :path.\n. I think what that line is saying is that if the specified header is not present on the request, then fail identification.  So if no header name is specified in the config, it will default to :authority and then if no :authority header is present on the request, it will fail identification.\n. This could be accomplished by adding a ResponseClassifier to https://github.com/BuoyantIO/linkerd/blob/master/linkerd/protocol/http/src/main/scala/io/buoyant/linkerd/protocol/http/ResponseClassifiers.scala that classifies 404s as retryable failures.\nWhat version of linkerd are you running?  If you are using a version older than 0.7.5 you can also try setting enableProbation to false so that hosts will be removed from the load balancers as soon as they are removed from service discovery (zookeeper).\n. do we have a test for TLS with h2?\n. https://github.com/linkerd/linkerd-zipkin provides this. TODO: The nodeName metadata probably doesn't get propagated from namerd back to linkerd.\n. CI passed before I updated CHANGES.md so I'm just going to merge this.\n. I think this is due to namerd stamp collision when namerd is restarted.  This can potentially also happen if linkerd changes which instance of namerd it's talking to have happens to hit a stamp collision. \n. I think we'll fix this by moving to a streaming API (either HTTP/1.1 or H2, possibly gRPC).\n. This branch is back from the dead, rebased on master, and hungry for reviews/revenge!. Nice find!  PRs welcome :)\n. Fixed by https://github.com/BuoyantIO/linkerd/pull/824. Fixed by https://github.com/BuoyantIO/linkerd/pull/824. This namer is not very useful and doesn't seem to be referenced in any blog posts.  The pfx variant is more useful and should probably be used instead.. Hi @brendanyounger!  Sorry that you got tripped up on this.  This is a really common error so any suggestions on how to make this more user friendly would be appreciated.  I'll give you some background on each of your points.\n\nWhen linkerd sends a thrift request to namerd's http interface, namerd does not respond.  I'm not sure exactly why this is the case, perhaps at the protocol level an HTTP message is indistinguishable from an incomplete thrift request?  Normally we could just set a timeout on this request and fail it if namerd doesn't respond within the timeout but this interface uses long-polling where namerd isn't expected to respond until the data changes.  This makes setting a timeout impossible.  We could investigate exactly why namerd can't fail the request immediately when it receives a message of the wrong protocol but in the medium-term we want to move away from this thrift long-polling interface in favor of gPRC anyway (https://github.com/BuoyantIO/linkerd/issues/842).\nAdding a timeout on the delegator screen, on the other hand, makes total sense.  Issue is here: https://github.com/BuoyantIO/linkerd/issues/525.  PRs welcome :)\nI don't think a warning when the thrift namer interface is absent is appropriate because the other interfaces (http and eventually gRPC) can be used for naming (if the right interpreter is configured).\nThe docs mention that briefly here https://linkerd.io/config/0.8.5/linkerd/index.html#namerd \"The io.l5d.namerd interpreter uses namerd\u2019s long-poll thrift interface and the io.l5d.namerd.http interpreter uses namerd\u2019s HTTP streaming interface.\"  But I agree that this is pretty deeply buried and not 100% clear.  Suggestions and/or PRs for how to make this more clear are greatly appreciated.\n\nThanks for your interest in the project!. Closing in favor of more purposeful plugin interfaces.. I think this needs to be re-diffed against latest master. After some in-person discussion we will probably not pursue this.  Doing this kind of thing as a transformer instead of as a namer is appealing because it allows the initial naming to happen in namerd but still allows post-processing to happen in linkerd.  This is useful in the case where you want to resolve an id in namerd and then filter that set of addresses down based on some contextual information available in linkerd.  An example of this is the localnode namer/transformer.  In this use case we want to be able to resolve the destination service id in namerd but then filter that down to only endpoints which are node-local to the linkerd.  This is better accomplished using a transformer rather than a namer since the entirety of naming takes place in namerd (if namerd is used at all).. Fixed by #1886 . @olix0r I think this is already accomplished by the prefix property:\nkind: io.l5d.rewrite\nprefix: /hostMethod\npattern: ...\nname: ...\ndo you think that's sufficient?. I'm having a hard time reproducing #831 in order to validate this fix.  I'm considering just shipping this anyway since the new behavior seems better.  Thoughts?. This looks really great!\n@siggy would you mind having a look at this as well?  And possibly taking it for a test drive to make sure all the existing functionality still works as expected?. This seems to be stuck waiting for a non-existent circleci build.  @dmexe can you try pushing an empty commit to see if that triggers a ci run?. This has been merged.  Thanks, @dmexe!. @JustinVenus are you still experiencing this issue?. Closing due to inactivity.  Please reopen if this is still an issue.. The answer today is: it depends on the store.  In the redesigned API we'll be more thoughtful about it.. That's right.  We'd definitely love to hear about what you try and how it works out for you.. These duplicates are tricky to remove because the stats that they're generated from look like perfectly valid concrete names.  For example:\n\"rt/http/dst/id/#/io.l5d.fs/cat/path/http/1.1/GET/cat/requests\": 1\nThis is a duplicate of the /#/io.l5d.fs/cat client but it could just as well have been a legitimate client called /#/io.l5d.fs/cat/path/http/1.1/GET/cat.  We can notice that the former is a prefix of the latter, but there's still no reason the latter could not be a legitimate client as well.\n. @stevej is this issue still valid and is there anything we can do about it?. I don't think we're going to block this on #840.  . At the same time as we move h2 out of experimental, we should also move the namerd mesh interpreter out of experimental as well.. We fixed up the identifiers but not the probation option.. Thanks for the tip, @esbie!  I wasn't able to use onAddedClients because it seems like that triggers on clients added to any router so instead I just recompute the clients each time.  Let me know if I'm missing something, though.. Fixed by https://github.com/linkerd/linkerd/pull/1330. Transformers are not (necessarily) scoped under namers.  Transformers can be attached to namers, but more often they are attached to name interpreters.  I would expect the stat scoping to be something like [transformer, io.l5d.k8s.daemonset]. Taking a step back, the issue is that we want to be able to count requests that are delegated to a certain concrete id but that cluster cannot be connected to.  In this case we might expect rt/<dst id>/requests and rt/<dst id>/failures to be incremented, but they are not.  I'm guessing this is because those stats are incremented in the client stack but the request is aborted before then if no connections can be established at all.\nInstead, the service_creation/failures stat is incremented which is less useful for counting how many individual requests with that concrete id failed.. Perhaps moving the StatsFilter up and out of the endpoints stack and into the client stack above the StatsFactoryWrapper?. I think we might be missing Retries.  I'll open a ticket.. https://github.com/BuoyantIO/linkerd/issues/880. I think this is a bug in io.buoyant.linkerd.protocol.h2.HeaderPathIdentifier where if there is a uri path with params, it will attempt to parse the params instead of the base path.. $ echo $NO_PUSH\n1\n$ ci/docker-publish.sh nightly\n...\n[info] Tagging image 8013e16adf76 with name: buoyantio/linkerd:nightly\n[success] Total time: 59 s, completed Jan 6, 2017 10:52:37 AM\n...\n[info] Tagging image 117f68d8da04 with name: buoyantio/namerd:nightly\n[success] Total time: 38 s, completed Jan 6, 2017 10:53:24 AM\n...\n[info] Tagging image 462facd36422 with name: buoyantio/namerd:dcos-nightly\n[success] Total time: 26 s, completed Jan 6, 2017 10:53:59 AM. I think I may be missing something here.  Could you use a dtab like:\n/http/*/* => /$/inet\nwhich would map /http/1.1/GET/10.2.2.56/16992 to /$/inet/10.2.2.56/16992?. When a host with a port is specified in the Host header, it is a single path entry: /http/1.1/GET/10.2.2.56:16992.  We need a rewriting namer to split on the the : into two path segments.\neg.\n/srv => /$/inet;\n/http/*/* => /$/io.buoaynt.http.hostPortPfx/srv;\nWhich would delegate\n/http/1.1/GET/10.2.2.56:16992\n/$/io.buoyant.http.hostPortPfx/srv/10.2.2.56:16992\n/srv/10.2.2.56/16992\n/$/inet/10.2.2.56/16992\n. Fixed by https://github.com/BuoyantIO/linkerd/pull/914. @bashofmann great suggestion!  I'll take a crack at that in a follow up.. personally I would expect the routers to be displayed in the order that they appear in the config file.. Sounds like a pain in the butt.  I don't have strong feelings about this if it's too annoying to do.. The following per-client stats should be displayed on the dashboard:\n\nLoad balancer size\nLoad balancer available\nBreak failures down into individual exception counts\nRetry percentage\nRetry budget. If my understanding is correct, the default TraceInitializerFilter.role is slightly broken in the case where you have a server that accepts a protocol that does not propagate trace data.  In this situation a trace id is not generated until the client stack which means that annotations on the server stack don't have a trace id set and use the default trace id.  The result is that the client span is not a child of the server span.. @rmars is is possible to insert the new client into the correct position in the dom instead of clearing and re-appending?. Does this mean that when new clients are added, existing clients might change color?. \ud83d\udc4f \ud83d\udc4f \ud83d\udc4f  thank you, @klingerf!  Sorry for the runaround. We did some of this in https://github.com/linkerd/linkerd/issues/938.  We probably won't add namer specific labels such as namespace, service name, and port.. Could you also add an entry to CHANGES.md for this?. Thanks for the comment, @masonoise.  I think we can definitely support routing based on arbitrary request context field.  Quick question for you: how do you build your TTwitter thrift clients?  One thing I noticed was that even though the TTwitter header has field specifically for dest, finagle clients never set this field.. @raydin that's correct, unfortunately.  The modified version of TTwitterClientFilter in this change does set the dest header.. Closed by #1060 . Fixed by https://github.com/linkerd/linkerd/pull/1057. Fixed by https://github.com/linkerd/linkerd/pull/1007. Fixed by https://github.com/linkerd/linkerd/pull/1049. Namerd client metrics added here: https://github.com/linkerd/linkerd/pull/1060. If I browse to http://localhost:4140/foo (as opposed to http://localhost:4140/foo/ with the trailing slash) I get broken resource links.  . That's unfortunate, it would be nice to be able to accept either.  I wonder if in javascript land we can force the trailing slash somehow (edit the window location???)?\n\nThat said, this is a strictly better than the current state of affairs.  \ud83d\ude80 . I think this might be intended behavior?  In the case where there are many many clients, they'll all start collapsed and not show up on the combined requests graph.  I don't think you'd ever want 100 lines on the combined graph, for example.  cc/ @rmars . @wmorgan if you can come up with an initial list, I can put it in the repo. Hey wait a minute.  The fact that /admin/metrics only refreshes the deltas when queried probably explains https://github.com/linkerd/linkerd/issues/379.  Moving to client side deltas is probably a more robust solution.. @rmars @esbie what do you think about the feasibility/level-of-effort for this?. @Ashald I'd be interested in what you think of this. On my home network the call to https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300,600 takes about 500ms and blocks further loading of the page while we wait for a response.  If we could not block on this we could probably shave those 500ms off.. This relates to https://github.com/linkerd/linkerd/issues/1035.  Ideally we can find optimizations that play nicely with modularity.  Keep in mind that plugins outside the linkerd repo may provide functionality to the admin site, so some kind of \"dynamic linking\" is needed.  . I think we're happy with where performance is for now.  We can re-open or create a new issue if we feel like doing more in the future.. Alternatively this can get it's own tab (enabled by https://github.com/linkerd/linkerd/pull/1001). I think moving to MetricsTree should make this impossible.  Please reopen if you see it again.. I wondered the same thing, actually.  I went with this because it's slightly more flexible.  You could, for example, add nav items that link to external urls, but I'm not sure we'd ever do that.  I could easily go either way on it.. I don't know exactly what each of those params do but what it kind of seems like based on the report here is that when streaming is enabled (by default) then a non-chunked response is treated as if it is just a single chuck.  This would explain why responses larger than the max chunk size fail.. I believe I can explain the observed behavior.  While it's confusing and not totally consistent, I believe it is more or less correct.\n\nnetty3 seemingly ignoring maxResponseKB.  I'm guessing that the response was chunk encoded.  I believe that maxResponseKB only applies to non-chunked responses.\nnetty4 seemingly ignoring maxResponseKB.  What is the chunk size of the response?  If the chunk size is greater than the default maxChunkKB, then this error will occur no matter how big maxResponseKB is.\n\nOur best guidance is to never turn streaming off (there isn't really a good reason to ever do this) and make sure maxChunkKB is set large enough for the chunks you will be receiving.  This represents the maximum amount of a message linkerd will buffer and is a much more useful parameter than maxResponseKB.  We may even consider deprecating maxResponseKB in the future.. @olix0r: how do you think we should proceed here?. I believe this is working \"as intended\".. updated commit message and description above ^^. The wiggle drives me crazy.  No idea what's going on.. Agree that /s is pretty arcane.  But I think we also want something very concise.  /go and /id are interesting.  I'd also consider /name and /dst but those terms might be too overloaded.... Per offline discussion, going to change /s to /svc. wiggle BEGONE! \ud83d\udd7a . I feel like the point of a plugin system is to be modular.  Assets or code for a plugin shouldn't go in the core project.  If a plugin has static assets, I feel like it should either have handlers to serve them itself OR we should provide some API for plugins to expose static assets (like they expose handlers and nav items).\nAlternatively, we can avoid this entirely by rendering completely serverside and not needing any static assets in the first place.  FWIW, this is what the recent requests plugin does.\n(https://github.com/linkerd/linkerd/blob/master/telemetry/recent-requests/src/main/scala/io/buoyant/telemetry/recentRequests/RecentRequestsAdminHandler.scala). Briefly, what are the downsides to option 3?  It seems intuitive to me that each plugin would contain and serve their own static assets, but I don't know anything about js development so I'm probably missing something.  I took a stab at moving the namerd tab's assets (js file and template) into the namerd interpreter plugin just to see what it would take.  The place I got hung up was that without an explicit list of pages in linkerd-main.js, the namerd function is never invoked.  It feels like the namerd handler should be able to supply its own main somehow though?\nI feel like I don't really have a good enough grasp on requireJS (or on how js apps should be structured in general) to know how to solve this.  But I feel like the level of modularity I want should be possible.... I took a stab at option 3 here: https://github.com/linkerd/linkerd/pull/1027\n\nIs this a bad idea?. Ok, thanks for background.  I have a slight preference for doing this on the server-side (option 2) but I won't block on it.  Going forward I've ticketed https://github.com/linkerd/linkerd/issues/1035 for finding a better solution to this problem.. Closed in favor of https://github.com/linkerd/linkerd/issues/1035. I think this needs to be re-diffed against master.. Not going to do this in the near term.  Closing for now.. @rmars is this done?. I have no preference about removing tracers here vs in a followup.. @klingerf I started to go down this road in my original MetricsTree refactor but I felt like it was pretty weird and rolled it back.  But certainly it's not great that it's so easy to accidentally break the admin dashboard by not configuring any telemeters that provide admin/metrics.json.\n. I do want to address this, but I'm not sure the best way.  I'll create a separate issue where we can discuss.. Created https://github.com/linkerd/linkerd/issues/1055 to track.. Oh, it seems to be working now!  \ud83d\udc4d . Fixed by https://github.com/linkerd/linkerd/pull/1087. We should keep this in mind for our next round of performance regression testing/profiling.. TODO: \n fix typo in CHANGES.md\n add some scaladoc. @siggy not without adding namer specific logic.  Only a namer knows what the segments in its path represent.  That information is not encoded in the MetricsTree.. @klingerf it's possible, but it would have to be done via regex magic since the transformer prefix doesn't get any special treatment in the id.  I'm inclined to leave that to a post-processor.. I've updated the stat naming slightly to leave placeholders like \"rt\", \"dst_id\", \"dst_path\", and \"srv\".  Updated example gist: https://gist.github.com/adleong/633ac4e6eb88f59621b3d5e1d1e37102\nLet me know what you think.. @siggy As mentioned earlier, pulling segments out of the dst_id will require namer specific knowledge.  I don't think we can build that in here, unfortunately.\nI can definitely take out the type labels and change the histogram stats to include the stat (max, min, avg, etc) into the stat name instead of the label.  We won't match their format exactly since we export percentiles directly rather than cumulative bucket counts.. Is there a performance concern with showing all the clients on the combined request graph when there are hundreds of clients?  . Ok, just thinking about the case where there are hundreds of clients and wondering if drawing them all in the combined graph will cause the page to grind to halt.  I'll defer to your judgement.. @stevej are we okay to close this?. Possibly related to https://github.com/linkerd/linkerd/issues/1033. Hi @TheDukeVIP.  I think it's likely that #1138 fixed this.  Feel free to reopen if this is not the case.. I know @olix0r has been thinking a lot about this and the API around it. . It seems like the only actionable item here would be to add an option to disable auto-chunking of large messages (without disabling streaming altogether).  Does that sound about right?. Fixed by #1073. Hey @Ashald, would you mind updating the description here with what we discussed in slack?  Namely that the delegator UI should somehow indicate which path is the \"active\" path (but continue to display all branches). finagle upgrade is tracked here: https://github.com/linkerd/linkerd/issues/1107. Yes, load balancing, circuit breaking, and service discovery should work with the grpc client.. Fixed by #2025 . The need for this in linkerd is less now that linkerd-tcp exists.. The intersection of scala magic and jackson magic is truly a vortex of mystery and wonder.. We'd definitely welcome a PR that adds this!. No, I don't think so.. I've tried a new approach.  Now instead of coloring all the leaves (red for failure, green for success) we color just the primary path (the entire thing red for failure or green for success).  All nodes not in the primary path remain uncolored, regardless of success or failure.\n\n\nThoughts?\nPS: I have no idea why that last node is hanging out all on it's lonesome way down there.. @Ashald #1274 may help with this.  what do you think?. I like that idea a lot.  See: #1250 \nedit: oh, #1250 doesn't mention individual service/client pages.  But I still think it's a good idea.. @Ashald have you tried the expiring clients in version 1.1.0 yet?  Do they address this issue?. I can't reproduce this.  When I start a fresh linkerd and look at /admin/metrics/usage, this is what I see:\n{\n\"pid\": \"abcb0f0e-dfdd-45ac-b64e-d19f80340343\",\n\"linkerdVersion\": \"0.9.0-SNAPSHOT\",\n\"osName\": \"Mac OS X\",\n\"osVersion\": \"10.12.2\",\n\"startTime\": \"2017-03-13T23:58Z\",\n\"routers\": [\n{\n\"protocol\": \"http\",\n\"interpreter\": \"\",\n\"identifiers\": [\n\"io.l5d.header\"\n],\n\"transformers\": []\n},\n{\n\"protocol\": \"http\",\n\"interpreter\": \"\",\n\"identifiers\": [\n\"io.l5d.header.token\"\n],\n\"transformers\": []\n}\n],\n\"namers\": [\n\"io.l5d.fs\"\n],\n\"counters\": [\n{\n\"name\": \"srv_requests\",\n\"value\": 0\n},\n{\n\"name\": \"srv_requests\",\n\"value\": 0\n}\n],\n\"gauges\": [\n{\n\"name\": \"jvm_mem\",\n\"value\": 970160320\n},\n{\n\"name\": \"jvm/gc/msec\",\n\"value\": 970160320\n},\n{\n\"name\": \"jvm/uptime\",\n\"value\": 115918\n}\n]\n}. This is mutually exclusive with https://github.com/linkerd/linkerd/pull/1092. Closing this in favor of #1276 . Hi @pvcnt!  We require that the namerd/dtabs path exists (although it can be empty).  You should create this path as a setup step before running namerd.. We do infinite retries on namerd errors so that if namerd goes down temporarily and then comes back up, linkerd will recover.  In this case, specifically, if you create that path in consul, the next retry will be successful and linkerd will be in a good state.\nFor the namerd tab in the admin UI, we should probably add a timeout so that the page displays an error instead of hanging forever.  Would you mind filing a separate issue for the admin UI timeout issue?. The PR in general.. @klingerf should we close this as won't fix?. Are you still seeing this issue?. Ok, please reopen if you see this again or have any reproduction steps. . Ah yes, sorry, you're right.  My suggestion is to not create a Ns object that uses that API but just use that API directly.  \nNs is really purpose built to handle this 2 layer caching where changes to the namespaces are cached separately from changes to the actual resource.  This is necessary for namers where we want the NameTree activity to update independently from the Addr var.  But in this case, we just want a list of ingress resources so I don't think Ns is applicable.  \nPut another way, we don't really need an Activity[Map[String, VarUp[Ingress]]] what we actually want is just an Activity[Seq[Ingress]]. Stretching a tiny dtab to fit the full width of the page looks a little goofy imo.\n\n. This is blocked until https://github.com/twitter/finagle/commit/08c2171dd2189c0bc3eddd5b7d8db67b03274fe5 makes it into a finagle release.. Yeah, the above commit just barely missed 6.43. Fixed by https://github.com/linkerd/linkerd/pull/1257. I think we'll get this for free when we pull in https://github.com/twitter/finagle/commit/9a04ff30f684c32b34bb01f0aef31d50c26bcae8. I think we got this for free when we pulled in finagle 6.44. How would this look in the case of path based routing?  Would all the service names just be localhost:4140?  What about using the logical name instead?  e.g. /svc/hello. Thanks for the context.  I think it makes sense that there are protocol-specific client annotations since those annotations describe properties of the (protocol-specific).  Service name, on the other hand, represents the name of the service that the request is destined for; and that doesn't seem like a protocol-specific thing to me.  \nI think my personal preference would be to keep the current behavior of naming the service name on the client span as the concrete id.  It's inscrutable, but I think it's most correct and I prefer inscrutable to misleading.  My worry with hostname is that the service name looks like a resolvable address when it isn't one.  For example, in the screenshot above, spans with the service name \"localhost:8080\" are not actually destined for localhost:8080 at all.  I think it can be the job of additional tooling on top of the zipkin data to distill more human-readable information out of it and let the zipkin data itself just be low-level and accurate.\nThat's just my opinion, I'm happy to be outvoted on this.. This idea is especially interesting in a service mesh context.  All requests from linkerd on node A to the linkerd on node B could theoretically use the same connection pool.. I've started looking into this and it's a bit more challenging than I first anticipated because the service port is not listed in the endpoints api.\nIn order to do this we'd need to set up a watch on the services api to also maintain a map of port to targetPort for each service.. This would be awesome!  Definitely welcome any contributions from anyone with rancher experience.. I think this is satisfied by the PLUGINS.md file.. It looks like perhaps your thrift bindings are out of date.  Have you tried\nrunning clean and then compiling again?\nOn Mon, Mar 20, 2017 at 5:00 PM, Dennis Jung notifications@github.com\nwrote:\n\nI just caught on compile error while working on.\n\ncompile\n...\n[info] Compiling 2 Scala sources to /Users/kwangin/workspace/linkerd/namerd/iface/interpreter-thrift/target/scala-2.11/classes...\n[error] /Users/kwangin/workspace/linkerd/namerd/iface/interpreter-thrift/src/main/scala/io/buoyant/namerd/iface/ThriftNamerClient.scala:160: value endpointAddrWeight is not a member of io.buoyant.namerd.iface.thriftscala.AddrMeta\n[error]     val weight = thriftMeta.flatMap(.endpointAddrWeight).map(Metadata.endpointWeight -> )\n[error]                                       ^\n[info] Compiling 2 Scala sources to /Users/kwangin/workspace/linkerd/namerd/storage/etcd/target/scala-2.11/classes...\n[error] /Users/kwangin/workspace/linkerd/namerd/iface/interpreter-thrift/src/main/scala/io/buoyant/namerd/iface/ThriftNamerInterface.scala:421: too many arguments for method apply: (authority: Option[String], nodeName: Option[String])io.buoyant.namerd.iface.thriftscala.AddrMeta in object AddrMeta\n[error]       Some(thrift.AddrMeta(\n[error]                           ^\n[error] two errors found\n[info] Compiling 8 Scala sources to /Users/kwangin/workspace/linkerd/namerd/storage/k8s/target/scala-2.11/classes...\n[info] Compiling 2 Scala sources to /Users/kwangin/workspace/linkerd/namerd/storage/consul/target/scala-2.11/classes...\n[info] Compiling 4 Scala sources to /Users/kwangin/workspace/linkerd/namerd/main/target/scala-2.11/classes...\n[info] Compiling 6 Scala sources to /Users/kwangin/workspace/linkerd/namerd/iface/mesh/target/scala-2.11/classes...\n[info] Compiling 2 Scala sources to /Users/kwangin/workspace/linkerd/namerd/dcos-bootstrap/target/scala-2.11/classes...\n[info] Compiling 1 Scala source to /Users/kwangin/workspace/linkerd/linkerd/protocol/benchmark/target/scala-2.11/classes...\n[error] (namerd-iface-interpreter-thrift/compile:compileIncremental) Compilation failed\n\nIs this a known issue?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/linkerd/linkerd/issues/1151, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ADy6Ilbx5MryEJ_dKT11Krby3dZlpb8Nks5rnxMegaJpZM4MjKw3\n.\n. I so far have not been able to reproduce this.  I'm able to do 25 req/s through a linkerd with the recent request telemeter and a sample rate of 1.0 with memory sitting steady at 93M.  \n\nIs there anything unusual about your requests?  Could they be emitting extremely large trace annotations for some reason?. So far we haven't been able to reproduce this.  Any or all of the following would be helpful for us to dig in if you can provide it:\n\nSteps to reproduce\nHeap dump (when linkerd is in this state)\nlinkerd config file\ndump of /admin/metrics.json (when linkerd is in this state). Thanks for the great report!  I've been able to reproduce and I will update the ticket with my findings.. @siggy: Anything is possible \ud83c\udf08 but as @olix0r mentions there will be a number of different kinds that this block can be (global, per-client static, file watching, api watching, etc)\n\n@esbie io.l5d.global is a good suggestion, thanks.  I think we can still make io.l5d.global the default kind so that it doesn't need to be specified.  That way, we keep the TLS blocks as the only breaking change.  If you don't have a TLS block, your config won't need to be updated.. @olix0r from the client config docs:\n\nIf a client matches more than one config's prefix, all parameters from the\nmatching configs will be applied, with parameters from later configs taking\nprecedence.\n\nI will add an example as well.. Hi @teodor-pripoae, can you give a little more background on what you'd like to accomplish in this PR.  This change adds the withSessionPool method to Client but doesn't expose any way to invoke it.  . @olix0r could you take another look at this?. Closing this in favor of https://github.com/linkerd/linkerd/pull/1551. hey @macalinao, what are your thoughts on my suggestion above?  other than that one comment this is looking great and I'd like to get it merged soon.. @KarolisL Thanks for reporting this!  Yes, I think you're right that it's a bug.  It looks like perhaps this happens because of the ordering of the server stack.  Exceptions are turned into error responses in the ErrorResponder.module and bodies are scrubbed by the Headers.Ctx.serverModule.  But because bodies are scrubbed before exceptions are turned into error responses, the error bodies can escape.\nAre you interested in submitting a PR for this?  If so, I can point you in the right direction.. Awesome!  Take a look at defaultServer in HttpConfig.scala.  This is where the server stack is built out of modules assembled in a specific order.\nI think we want to modify this code to make the Headers.Ctx.serverModule come before the ErrorResponder module in the stack.. @KarolisL are you still planning to work on this?  is there anything else I can do to help point you in the right direction?. I'm realizing now that this setting used to set a timeout both on the client stack AND on the server stack.  This meant that if a timeout was set, each individual try of that request was subject to that timeout but also the entire request including all retries was also subject to the same timeout.  We have a few options on how to proceed:\n\nMove the timeout into the client config and remove the server stack timeout entirely.  This is what this change has done so far.\nMove the timeout into the client config but also allow a timeout to be configured per-server in the server config.\nKeep the timeout in the router config where it will apply to both the client and server stacks, but allow it to be overrideable in the client config.\nKeep the timeout in the router config where it will apply to both the client and server stacks, but allow it to be overrideable in the client config AND overrideable in the server config.\n\n3 and 4 would be non-breaking changes since they allow a timeout in the router block.  4 is most flexible, but also potentially the most confusing.  3 may be a good balance of flexibility and clarity.. Ok, as suggested here I have implemented option 2.  \ntimeoutMs in the client section now configures a timeout for individual requests (tries) which can be configured on a per-client basis.  \ntimeoutMs in the server section now configures a timeout for the whole request including all retries (note that retries may happen on different clients and each try is subject to the timeout of that client that it is tried on).  \ntimeoutMs can not be specified directly on the router section.. Client timeouts are per-try (pre-retry) and server timeouts are total across all retries (post-retry).  In your example:\nSuppose the request is dispatched to client A (which has a timeout of 1s) after 1s has elapsed and no response has been received, the client's 1s timeout is hit and the request is retried.  Retries happen in the path stack so the retry is dispatched to client B this time (which has a timeout of 3s).  After another 1s has elapsed, the client still has 2s left towards its timeout but the server has hit its 2s timeout and the request is failed.  \nI don't think we can do post-retry timeouts at the client level because retries happen in the path stack.\nThis is described briefly in the config docs.  It's a complex enough feature that it could probably use a section on linkerd.io/documentation describing it.. Merging this.  As discussed above, further work remains to be done.. @olix0r I could use a review on this now that 1.0.0 is out the door. Yup, @klingerf and I have both tested this manually.. @agunnerson-ibm that behavior is intentional.  we've found that treating service discovery as advisory allows us to be more resilient to service discovery outages.  The behavior can be configured with the enableProbation flag: https://linkerd.io/config/0.9.1/linkerd/index.html#load-balancer. Sorry, I got mixed up.  I believe there's an error in the docs for enableProbation.  I've filed https://github.com/linkerd/linkerd/issues/1206\n\nWhen enableProbation is true: service discovery is treated as advisory and endpoints are not eagerly evicted from the LB pool.\nWhen enableProbation is false: service discovery is treated as authoritative and endpoints are evicted from the LB pool as soon as they are removed from service discovery\nenableProbation defaults to false. @nik786 it doesn't seem like that has anything to do with this issue.  Posting on discourse.linkerd.io is the best way to get help debugging your setup.. @olix0r I could use a review on this. Nevermind, I've changed my mind on how I want to do this.  This approach has a few drawbacks:\n\nExpiring gauges is tricky because they have to be manually deregistered (which not everyone does) or rely on a WeakReferences to deregister when they are no longer referenced.  Unfortunately, there are gauges that seem to stick around forever even though they don't appear to be referenced anywhere.\nAdditionally, expiring idle counters may just be a bad idea in general.  Counters that update very rarely may still contain valuable information.  For example, binding cache one-shots: you may want to know if a one-shot has ever happened.  This information would be lost after the TTL period.\nInstead of expiring idle metrics based on a TTL I think we should more explicitly remove entire subtrees from the metrics tree when the relevant ServiceFactory is closed.. This sounds great!. After some investigation, this appears to be related to retries when the responses are chunk encoded.  My current theory is that when a chunk encoded response is retried, linkerd never actually reads the chunks of the discarded response and since the response has unread chunks, the connection is never released.\nI will attempt to test this theory and put together a fix.  I'll update this issue as I know more.. I've posted what I believe is a fix at https://github.com/linkerd/linkerd/pull/1256\n@taer @fantayeneh @leozc I would love your help testing this fix a verifying if it solves the issue for you.  Let me know if you'd like me to build a binary or docker image for you.\n@fantayeneh I wasn't able to reproduce this with non-chunked responses.  Do you have a sample implementation of a server that causes this?  . @fantayeneh thanks!  I've verified that my fix works on that service.  Apparently linkerd will automatically chunk messages and that is what's happening here (this was a surprise to me!). I've closed this as I believe that https://github.com/linkerd/linkerd/issues/1256 fixes the issue.  Please reopen this if you see this behavior again.. does anyone have a reliable set of commands to reproduce this?. This is known annoyingness that only occurs when protobuf changes (or something).\nDeleting the compiled scala folder from the target directory works around the issue.. @siggy thanks for the feedback!  Your understanding is correct about the valid/invalid configs.  I'm not sure what you mean about structural enforcement; invalid configs will fail to parse.  There's also president all over our configs for changing what's allowed in the object based on the value of the kind field.  Consider:\nnamers:\n- kind: io.l5d.fs\n  rootDir: /disco\n- kind: io.l5d.k8s\n  host: localhost\n  port: 8001. @esbie lmk if I've misunderstood your comment but client, service, and servers are all siblings.. Here's a proposal on what to do about retry budgets and response classification:\nRetry Budgets\nThe objective of retry budgets is to allow linkerd to retry as much as possible without overloading itself or downstream systems.  Currently linkerd has a single retry budget per router and both application failures (retries) and transport failures (requeues) count against this budget.  One problem with this is that a single failing client or service can eat up the entire retry budget and allow no other retries to take place.\nWe would like to continue to allow linkerd to retry as much as safely possible while also protecting the budget against failing clients and keeping the configuration sane.  Thankfully, retry budgets Just Work with the default settings and changing the budget should be a rarely used advanced feature.\nI propose that each service have its own retry budget that is not shared with other services and is not shared with any client.  Likewise, each client would have its own retry budget (for requeues) that is not shared with other clients or with any service.  Each retry filter and requeue filter would be responsible for its own depots and withdrawals.  This means that a failing client or service can only deplete its own budget.  A retries config may be specified as a service config that configures the retry budget and backoff for that service.  A requeueBudget config may be specified as a client config to configure the requeue budget for that client.\nResponse Classification\nResponse classification feeds into stats reporting (successes vs failures) and application retries.  The response classifier that is configured on the router today is used in 4 places: client level stats, service level stats, application failure retries, and server level stats.  However, having one response classifier for all services is problematic because the definition of failure or retryability/idempotency is different for different services.  Therefore, we would like to move the response classifier to the service config.  However, we also need the results of that classification to be reported in the client and server level stats.\nI propose that response classification be configured as a service config.  The response classification module in the path (service) stack should do 3 things:\n\nSet the response classifier into a LocalKey (ie Context).  The client stats module will then use the response classifier from the LocalKey to report success/failure.  In other words, it will use the response classifier request's service.\nUse the response classifier for service level stats and application failure retries.\nSet a l5d-ctx-success header on the response indicating the success or failure (or partial success?) of the request.  The server stats module will read this header and use it to report stats.\n\nIn this way, the response classifier is only set at the service level, but is used to report consistent stats at the client, service, and server levels.. Hi @Disturbing.  I don't think this is actually related to dark traffic.  I'm going to close this issue for now but please feel free to open a new one if @klingerf's suggestion doesn't work for you.. Thanks for the detailed information about your use case, @jgensler8.  When we were implementing the ingress identifier we found that there was a lot of underspecified behavior in the ingress spec so we made some somewhat arbitrary decisions.  So hearing about real use cases is very helpful!\nThe way that it's implemented (as you have probably noticed) is that we evaluate ingress resources one at a time.  In your example we evaluated gateway ingress before the subtraction-operator ingress.  Since the request didn't match any of the rules in gateway but gateway did have a default, that default was used.  We thought this behavior makes sense because it avoids the ambiguity of what to do if defaults are defined in multiple resources.\nOne way to get the desired behavior would be to remove the default from the gateway ingress and instead set up a fallback identifier:\nidentifier:\n- kind: io.l5d.ingress\n- kind: io.l5d.static\n  path: /my-http/gateway\nThis configuration should assign a static fallback name to the request if none of the rules in any of the ingress resources match.\n. I can see why this would be useful for finagle apps.  Would it work to simply have those apps include linkerd's Headers.Dtab.ClientFilter which will serialize the dtab local into the l5d-ctx-dtab header?. Linkerd already adds a host key to the Forwarded header.  Does that satisfy your use case?. Hi @DukeyToo.  Do you have things working now?. Awesome!  I'll close out the issue.. I think this was fixed in https://github.com/linkerd/linkerd/pull/1238\nPlease reopen if you see it again.. We should do this soon.. Closing as duplicate of #740.  But yes, we should still do this soon.. Amazing work!  I just tested it and verified that memory usage is way way down.  I think that we still may be leaking StreamClosed objects but these are tiny compared to leaking the actual streams (and we can fix that issue in a followup).. There are no such things as coincidences \ud83d\ude09 \nRight now my review just sets l5d-success-class to a number indicating success, partial success, or failure.  We should make that richer or add another header to indicate retryability. \n. Fixed by https://github.com/linkerd/linkerd/pull/1229. @rnorth: A response classifier that always treats 503s and 429s as retryable sounds like a good idea and it sounds like it solves issue 2 quite nicely.  We'd love if you wanted to contribute that back!. I agree that this would be good to have.  Finagle currently doesn't expose a way to override the default (https://github.com/twitter/finagle/blob/master/finagle-core/src/main/scala/com/twitter/finagle/service/FailFastFactory.scala#L33) so this would take some work to fix.. Relatedly, we could also strip off informational headers linkerd sends back on the response.  I'm not sure if there are any yet but https://github.com/linkerd/linkerd/pull/1185/files would add l5d-success-class to indicate response classification.. Thanks for the thorough review, @klingerf!  I think I've updated all the sites now, but let me know if you see anything else that I've missed.  The linkerd dashboard seems to work now, although there is an unrelated bug about certain failures being reported as success: https://github.com/linkerd/linkerd/pull/1228\n@rmars requesting a review from you since this touches a bunch of the admin js. I will update srv to server in a followup, if that works for you. The new test case catches this.  Notice that failureAccrual is not specified in the second config block.. Fixed by https://github.com/linkerd/linkerd/pull/1261. Maybe not a real solution but I can usually avoid this by bumping up the timeout\nenv CI_TERRIBLENESS=10.seconds. Do you think this could be the same file descriptor leak described here: https://github.com/linkerd/linkerd/issues/1235 ?. I think it's likely that this is a duplicate of https://github.com/linkerd/linkerd/issues/1256.  Please reopen this issue if you see the problem after using the above fix.. Thanks for the great report.  We will definitely investigate this.. Please let me know if https://github.com/linkerd/linkerd/pull/1256 fixes this issue.. Awesome!  The above PR has been merged.  Please reopen this issue if the problem reappears.. Great suggestion!. @SecretLiang Yes to both questions.. Thanks @fantayeneh, this is great!  We're prepping the 1.0.0 release right now but we'll get this merged soon after the release.. Would you mind squashing your commits and formatting the commit message as described in https://github.com/linkerd/linkerd/blob/master/CONTRIBUTING.md?\nThanks!. Unfortunately, moving to netty4 while still supporting pkcs#1 keys means that we need to use the LegacyKeyServerEngineFactory which depends on openssl.  We can't switch docker images until we drop support for pkcs#1.. Would dropping support for netty3 also be a solution?  Are we ready to do that?. Going to close this.  Please reopen if you continue to see issues.. Fixed by #1264 . We're continuing to investigate this.  We believe #1381 is related.. linkerd-tcp can already do this.\nAdding this functionality to linkerd is more difficult and would require finagle and netty changes.. This seems like a reasonable feature but it's not on our immediate roadmap.  We would definitely welcome a more detailed proposal for how this could be implemented.  A few items to consider:\n Exactly what interface would the plugin expose?\n Where should this be added into the linkerd router?  (path-stack vs client-stack?)\nThanks!. This is now possible with requestAuthorized plugins.. They are listed as \"loggers\" in the Linkerd docs but their name is changing to Request Authorizers in the next release.. Ah, sorry @owensk, I missed the part about this being for thrift.\nA logger/request authorizer gets the request before it is sent to the downstream server.  This means that it could choose to bypass the endpoint and hit a cache instead.  \nI think we should rename this ticket to \"Add request authorizer plugin interface to thrift protocol\" and reopen it.  What do you think?. Thanks, @fantayeneh!!!. 1. Liveness: linkerd has an /admin/ping endpoint\n2. Readiness: in the finagle philosophy, the best way to test readiness is to simply send an end-to-end request and see if it can be served.  For example, sending a health check request to the destination service through linkerd.  The benefit of this is that readiness is checked all the way through the system instead of just at the first hop.\n3. Resource requests/limits: we should add this to our production readiness guide https://github.com/linkerd/linkerd/wiki/Going-to-production\nI'll rename this ticket and keep it open to track number 3.. @betson-nuovo this is super interesting, thank you!  If you have more information about the failure states that Linkerd can get into, I'd love to hear more about that too (especially if you have reproduction steps!  \nIdeally, if we can fix the root cause of these failures, we wouldn't get into these states in the first place.. You have the identifier property set twice in your linkerd config which will cause the first identifier to be overriden/replaced by the second one.  Instead, you should provide a list of identifiers:\nrouters:\n- protocol: http\n  identifier:\n  - kind: io.l5d.header.token\n    header: Custom-Header\n  - kind: io.l5d.path\n    segments: 1\n    consume: true\nThe difference is pretty subtle.  I've filed https://github.com/linkerd/linkerd/issues/1262 to make it so that duplicate properties will raise an error.. Hi @smparekh.  That's right, you can have multiple identifiers but they must be defined as a list like the example in my last comment.  I know the documentation of this isn't 100% clear and could use an example.  \nThese docs live here in the linkerd repo and we gladly accept pull requests if you'd like to help us improve them.. If I put the path identifier first it uses the path segment if it's present, otherwise falls back to the header:\n$ curl localhost:4140/foo -H \"Custom-Header: bar\"\nNo hosts are available for /svc/foo, Dtab.base=[/svc=>/#/io.l5d.fs], Dtab.local=[]. Remote Info: Not Available\n$ curl localhost:4140 -H \"Custom-Header: bar\"\nNo hosts are available for /svc/bar, Dtab.base=[/svc=>/#/io.l5d.fs], Dtab.local=[]. Remote Info: Not Available\nHere's the config: \n```\nnamers:\n- kind: io.l5d.fs\n  rootDir: linkerd/examples/io.l5d.fs\nrouters:\n- protocol: http\n  dtab: /svc => /#/io.l5d.fs\n  identifier:\n  - kind: io.l5d.path\n    segments: 1\n    consume: true\n  - kind: io.l5d.header.token\n    header: Custom-Header\n  servers:\n  - port: 4140\n```\nThis is in line with what I would expect.  Does that answer your question?. That looks right to me.  If you have the path identifier first it will name the service based on the path segment if it is present.  So /svc/hello-world in the first example and /svc/api in the second example.  \nKeep in mind the identifier is just for assigning a service name to the request.  Later steps in the process will apply the dtab and check service discovery.  It's not possible to change which identifier is used based on dtab or service discovery state (unless you were to write a custom identifier that did this).. Yes, that will use the header if it is present and fall back to the path otherwise.. Fixed by #1267 . @halve: could you take a look at this and let me know if this works for you?. @halve were you able to take a look at this?. Closing due to inactivity.  Please reopen if you are still interested in this.. Hi @dhay.  Sorry for the inactivity here.  I would really like to get @halve's input on this as I believe they are using it in production and I'm personally a little unclear on the implications of this change.  However, I've been having trouble getting in contact with him.  I will try pinging again.. Good question.  If a service or client goes idle for the idleTtl and then becomes active again, the metrics will be deleted and recreated, effectively resetting the counters.  This could definitely impact the usage data.  We could lengthen the default idle ttl to longer than the 1 hour reporting interval.  . Closed by #1319 . @gsogol actually that sounds more like speculative/backup requests: https://github.com/linkerd/linkerd/issues/1069. Cool!  I'd love to see it!. I believe this is caused by https://github.com/twitter/finagle/pull/629. Blocked on https://github.com/linkerd/linkerd/issues/1595. Fixed!. Related to #1250. I just verified that this works with pkcs8 keys with both netty3 and netty4.  Looking at the code, it looks like it shells out to openssl to convert the key to pkcs12.  Does this have implications for which base docker image we can ship this with?. @zackangelo any update on this?. Closing this for now.  Please reopen if it's not working as expected.. Good catch, thanks!  Would you mind signing our CLA so that we can accept this fix? https://buoyant.io/cla/. The issue is specifically with ApplicationProtocols which non-H2 servers don't set.  I don't think there's anything to test clientside because we don't support client private keys (yet).. I notice that \"dispatcher\" is in metrics.js.  Is it possible to add a test that catches this error?. Mostly I just was looking for a way of testing that the issue is resolved.  If you've tested manually and are confident then \ud83d\ude80.. Posted prematurely.  There are still some things I need to fix.. I think it may be technically possible to have a server and client with different protocols because linkerd deserializes and re-serializes the thrift request.  That said, being able to bridge two different thrift protocols seems like a very niche feature and I'm not aware of anyone doing this or wanting to be able to do it.  So in the interest of simplification, I'd be in favor of consolidating this property on the router. . @lkysow: thanks for tracking this down!  that fix looks correct to me.  would you like to submit a pull request?  otherwise I can put one together based on your diff.. @lkysow I added tests and handled the case where the ingress is absent: https://github.com/linkerd/linkerd/pull/1324\nI'm not sure how attribution works when squashing commits so I added an empty commit by you at the end to hopefully give you attribution.  \nWould you mind signing our CLA so that we can accept this PR?  https://buoyant.io/cla/. I think going forward what I'd like to do is:\n Make Netty4 the default\n deprecate the maxResponseKB parameter\n* deprecate the streaming parameter\nWhat do you think? @lkysow @olix0r . I wasn't able to reproduce this.  Here's what I tried:\n\nStart a fresh namerd with consul dtab storage and mesh interface\nStart linkerd and attempt to resolve 5 names (1 existing and 4 non-existing)\nFrom the namerd stats note the number of connections to consul: \"clnt//$/inet/localhost/8500/connections\": 5,\nShutdown linkerd\nFrom the namerd stats, notice that the number of connections to consul has dropped to 0: \"clnt//$/inet/localhost/8500/connections\": 0,. Sounds good!. cc @Ashald . Awesome!  Maybe reword 1. slightly to say that no namespaces have been created in the backend store, since namespace aren't configured in the namerd config (except for the in-memory store).. Hi @superwen0001, thanks for your interest in linkerd!\n\nI recommend posting on https://discourse.linkerd.io for help with configuring and using linkerd.  Github issues are for bug reports and feature requests.  Thanks!. Thanks @rbaumgartner!\nUnfortunately this is a behavior of Java's InetSocketAddress that the hostname is resolved once when the InetSocketAddress is created and then never again.  This means that it will never pickup DNS changes.  (This is one of many reasons why we believe that DNS is not well suited for use as service discovery)\n. https://github.com/linkerd/linkerd/pull/1444 Should fix this issue. Thanks for the report!  Does memory use continue to grow if traffic is left on?  Or does it stabilize at 173MiB?. Any update here?  @jamessharp are you still seeing this and are you able to capture a heap dump?. Thanks for the report!. I am able to reproduce this in minikube with this config by turning on RBAC so that requests to the kubectl proxy fail.\nThis may be due to the kubernetes namer's infinite retry filter not discarding failed responses.. I believe linkerd can get away with much more restrictive permissions than the ones granted above.  It should only need read and watch permissions on the endpoints, namespaces, and services APIs. \nThanks for the issue, we'll definitely put something together.. I think adding this timeout makes sense on the namerd API (as opposed to on the namer itself). @siggy can you refresh my memory about the motivation for this?  Is this to do with Linkerd behavior when querying Namerd?  Or to do with users running manual requests against Namerd as in the curl command in the description?. Linkerd's 10 second dst binding timeout is supposed to protect against this, I think.\nI'm going to close for now.  Let's reopen this or a new issue if we come across cases where Linkerd behaves badly.. Hard to say without a repro of #1343. I found a solution that I like better than what I had before.  Now, instead of trying to avoid re-creating the client if mkNs is called multiple times, we simply don't call mkNs multiple times.  I think this is cleaner.. Thanks for the report!  A few more pieces of information that will help us track this down: what version of namerd are you running?  Is it possible to grab a dump of the :9991/admin/metrics.json from the namerd process when memory is on the rise but before it crashes?  I know there's a very limited window to grab this, but getting metrics from as close to the crash as possible will be very helpful to diagnose the issue.. Thanks for the stats dump.  I've taken a look but as far as I can tell, everything looks healthy and normal in there.  No smoking guns as to what might be responsible for the memory increase. \ud83e\udd14 . Aha!  Yes, it is almost certainly related.  I think that error is https://github.com/linkerd/linkerd/issues/1385 which we have a fix for here https://github.com/linkerd/linkerd/pull/1388. closing as duplicate of https://github.com/linkerd/linkerd/issues/1385. Do we still see this when there are no named target ports?. To clarify the current state of this issue: this issue is tracking the memory leak that occurs when routing to the kube-admin namespace in Kubernetes.  For the sake of clarity, I'm going to delete the comments on this issue that deal with the memory leak related to namerd and consul because that issue is being tracked at https://github.com/linkerd/linkerd/issues/1487. Just to give an update here.  The k8s namer refactor is quite complex but is nearing completion.  We should be able to see if it fixes this issue soon.. @olix0r we'd like to merge this for the next release. Could you take a look, please?. Netty4 is now the default as of finagle 6.45.0. The log level in the tracelog config indicates the level at which traces is logged at.  When set to TRACE, this means that traces are logged at the TRACE level.  If this is below your logging threshold, you won't see them.  The log threshold can be configured with the -log.level=TRACE flag or at runtime in the logging tab of the linkerd admin dashboard.. Thanks for the doc update!  Can you revert your changes to the example configs?. There's nothing explicitly coupled to k8s but the namespace, port, service concepts are k8s concepts.  In general, it seems like istio leaks a lot of k8s details through the abstraction.  I imagine that the istio apis are going to need to go through some refactors to make them work in other environments.  . Can we add a README.md or something indicating where these file are copied from?. Probably related to https://github.com/linkerd/linkerd/issues/2125. @rmars closing this PR as we're not actively working on this.  Go ahead and keep the branch around though.. Based on the stacktrace in the linkerd log above, it looks like namerd is encountering a NullPointerException and returning that to linkerd.  Do you have logs/stack traces for namerd as well?. Hi @leozc, were you able to get those namerd logs?. I'm not able to reproduce this but based on the stack trace, I believe this should fix it: https://github.com/linkerd/linkerd/pull/1460\n@leozc can you take a look and let me know if that fixes the issue for you?. It hasn't been merged yet.  Are you able to build off of my branch?  If not, let me know and I can send you a build or docker image.. @kumudt can you verify that this fixes the issue in your environment?. We no longer use AnyToHeapInboundHandler.  . \"fun\" fact: for some reason there's no parsing error when we attempt to parse a string into an Int typed field, the object with the Int field is happily constructed.  It's only much later when we attempt to use that field that a ClassCastException suddenly appears because (surprise!) it was a String all along.. https://github.com/linkerd/linkerd/pull/1444 should fix this issue. \ud83c\udf89 \ud83c\udf89 \ud83c\udf89 . We don't officially support windows but would likely accept pull requests for this.. Getting source.uid and target.uid is a bit tricky because linkerd is not running as a sidecar.  A couple options:\n\nFor outgoing traffic, linkerd gets the remoteAddress of the request (ie source.ip).  Linkerd keeps a watch on the k8s API to keep a mapping of IP to pod name.  When this request reaches the incoming router of the destination linkerd, that linkerd looks at the remoteAddress of the response (ie target.ip) and uses its mapping to get the target.uid.  It encodes this as a header on the response it sends back to the source linkerd.  The source linkerd now has the source.uid and the target.uid.  \nWe run linkerd-tcp as a sidecar in addition to the linkerd which is run as a daemonset.  Since linkerd-tcp is running as a sidecar, it can get the pod name from the downward API.  The source linkerd-tcp can write the source.uid onto a request header and the target linkerd-tcp can write the target.uid onto a response header.  This gives the linkerds access to both pieces of information.\n\nOption 1 requires each linked to keep an additional watch on the k8s api for the list of pods on its node.  To complicate things further, remoteAddress is not a field on response and we may need to make a finagle change to get access to this information. \nOption 2 is more complex since it requires both a sidecar linkerd-tcp and a daemonset linkerd (each request would make 5 hops from source app to destination app).  However, we may be moving in this direction anyway to have linkerd-tcp initiate and terminate TLS as a sidecar.  This option would require adding code in linkerd-tcp to read the environment variable and write the header.. related to #87 . That's correct that Linkerd and Envoy are very similar in the role they fill and have quite a bit of feature overlap.  Linkerd also has a control plane in the form of Namerd.\nIstio is a very new project and is evolving rapidly so it's tough to compare with Linkerd and draw any meaningful conclusions.  One of the reasons one might choose Linkerd is that it's been around a bit longer and has a wide array of companies already using it in production.  That said, Istio is super interesting and we're excited to be involved.. Fixed by https://github.com/linkerd/linkerd/pull/1444. Closed in favor of https://github.com/linkerd/linkerd/pull/1444. Do the external clusters exist in SDS?. I think (1) and (2) are both valid.  For (1) we would probably have to write an ExternalNameNamer or something like that..... Going to close this for now until we have a benchmark that demonstrates that this improves things.. Fixed by #1444 . It's not surprising to me that cancellations would cause more chaotic latency distributions and therefore more chaotic behavior of latency aware load balancers.. This is cool!  Can you tell me a little bit more about how it works?  Is this a scenario where each docker container has its own IP or do they all share the host IP?  Looking at the code, it looks like we get the host IP from the aws api and then only allow traffic to that IP.  Could the io.l5d.localhost transformer work here?  . Thanks for all the context, that's super helpful.  The solution you describe on your blog actually seems quite nice and elegant to me.  Assuming the IP of the host doesn't change over time, fetching it prior to starting linkerd and then passing it into the config with envsubst seems like the right thing to do.  Are there shortcomings to this solution that made you want to build this functionality into linkerd itself?\nOn a related note, we could definitely consider changing our docker image to include gettext so that users who want to envsubst don't have to roll their own.. I'd like to get our official docker image to the point where users can volume mount a templated linkerd config and have envsubst run on it without needing to build their own image.  And we should definitely document all of this so that folks don't have to work this out on their own.. Hey @kevholditch, we keep missing each other on Slack so I'll post my thoughts here.  I'm still having a hard time understanding what the drawback is to using a templated config file to accomplish this (as is done in the blogpost).\nIt sounds like distributing config files to ec2 hosts is difficult and therefore you need to roll the linkerd config into the docker image itself.  That's a bummer but it seems independent of whether the config is templated or not.  Unless I'm missing something, even with this transformer you would still need to distribute linkerd configs by bundling them into a docker image.\nSo I'm not really sure what this transformer buys you other than not needing to run envsubst as part of the Linkerd command.  To me, the task of fetching this kind of configuration information from the AWS api feels more natural as a startup command that feeds into Linkerd's configs than as something Linkerd needs to figure out during runtime.\nPlease let me know if I've misunderstood or am missing something.. You folks have done great work figuring out how to run Linkerd on ECS and I definitely want to build on that and make sure people don't have to re-invent the wheel.  What do you think about this:\n\nBuild gettext into the official Linkerd docker image. (I understand this is less important since ECS users probably have to build their own images anyway in order to include configs, but it's probably still more convenient for it to be in the base image.)\nAdd a Getting Started with ECS page to https://linkerd.io/getting-started/ that includes a sample templated linkerd config and a sample task definition or launch configuration user data script that fetches from the AWS API and uses envsubst.\n\nFor what it's worth, we do a somewhat similar thing for Kubernetes by setting the node ip into an environment variable before starting Linkerd.  \nLet me know what you think.. https://linkerd.io/advanced/namerd/. The io.l5d.fs namer uses Java's WatchService (https://docs.oracle.com/javase/7/docs/api/java/nio/file/WatchService.html) under the hood.  I've noticed that it can sometimes take a while for notifications to occur after the file has been changed.  This might be what you're experiencing.. @mnnit-geek: I'm not sure.  I've never had it take that long.  That's strange.. Perhaps try a different service discovery backend such as Consul, just to verify that the problem is limited to the filesystem namer.. I'm going to close this for now since I haven't been able to reproduce it.  Hopefully the move to consul makes this a moot issue.. Hi @jyothidat, is there any information in the linkerd pod's logs?  And can you also provide the output of: kubectl get svc?. Hi @SurajMenon.  This is a limitation of using the identifier plugin interface for authorization.  If you prefer, you can return an UnidentifiedRequest exception which will return a 400 Bad Request.  In the long term we'd like to add a new plugin interface specifically for authorization which would allow more control over the error responses: https://github.com/linkerd/linkerd/issues/1466. That's right, we currently only return plaintext error messages.  Adding support for json would be pretty easy, though.  Pull requests welcome!. Fixed by #1902 . Hi @pbagchi!\nOur default configuration assumes that node names are addresses resolvable from a pod.  ie that you could log into a pod and run ping k8s-worker-9 or curl k8s-worker-9:4140.  If this is not the case, then the hello-world-legacy configuration is more appropriate.. I'm closing this issue as inactive.  If you are still experiencing this issue, please reopen or create a post on discourse.linkerd.io. This is probably a duplicate of https://github.com/linkerd/linkerd/issues/1377 right?. Hi @eduponte, I deleted your stats dumps from #1361 in order to clean up that issue but I didn't realized that you were linking to them from here.  Do you still have them and can you upload them to this issue directly?  Thanks!. Fixed by https://github.com/linkerd/linkerd/pull/2006. Makes sense.  How did you come up with 16384 as the initial capacity.  Have you been able to experimentally measure the amount of allocation before and after this change?. Wdyt of this wording, @siggy?. I think we still need to manually test that the go_away is sent with wireshark, nghttp, and/or netty frame logging.. @hawkw any progress on manually testing this?. Whoa, that's pretty weird.  It sounds like getting a TCP dump might be the best way to dig into what's going on and see if Linkerd is being properly informed of the connection state.\nhttps://linkerd.io/overview/help/. Closing this as inactive.  If you're still having the issue, please reopen or post on discourse.linkerd.io. An added wrinkle here is that onEnd is a Future[Unit] which means we don't have access to the final frame of the stream.  Changing the type of onEnd to Future[Frame] is probably not a good solution either because this would cause the Stream to hold a reference to the final frame, even after the frame is released, which is a violation of flow control.\nI recommend creating a StreamProxy class that delegates all calls to an underlying stream but also takes a function onFrame: Try[Frame] => () which is called on each read.  By wrapping the response stream in a StreamProxy, we can give it an onFrame that increments the appropriate stats on the final frame.\npseudocode would be something like\nsvc(req).transform {\n  case Return(rsp0) =>\n    val stream1 = new StreamProxy(rsp0.stream) {\n      case Return(f) if f.isEnd => // consult classifier and incr stat\n      case Return(f) => // do nothing\n      case Throw(e) => // consult classifier and incr stat\n    }\n    Future.value(rsp0.copy(stream = stream1))\n  case Throw(e) => \n    // consult classifier and incr stat\n    Future.exception(e)\n}. We should also add some reasonable built-in classifiers:\n Successful iff response and response stream are successful\n Successful if response and response stream are successful and response status is < 500. That would be \"hella dope\".  Similarly: histogram of frame sizes.. This is the regex that Kubernetes uses for port names: https://github.com/kubernetes/kubernetes/blob/master/pkg/api/types.go#L40. Ah, thanks for the context.  Just to make sure I understand, for non-TTwitter thrift, we can emit traces like this from a single Linkerd but we can't join spans from multiple Linkerds?  If that's the case then tracing seems of very limited value.  We might as well fix this though.. :+1:. I'm happy to do a walkthrough if you think that would be helpful.  In many places we call read() to kick off reading/buffering from the underlying stream but drop the Future so we continue immediately instead of waiting until the stream is complete.. Here's what's left:\n\nWith this change, responses are held and not returned until either the buffer is overfull or the response stream completes.  This breaks a few tests which rely on the response returning before the stream is complete.  I could remove the tests that rely on that behavior but I'd prefer to instead wait and merge https://github.com/linkerd/linkerd/pull/1545 first, which will allow early classification in many cases.  We should ALSO probably add some kind of classification timeout where we give up on classification if the response stream does not complete in a certain amount of time.  Otherwise, we could be holding the response indefinitely.\nFix typos and nits\nWrite a commit message\nUse generated load and see what the allocation rate looks like before/after.. Some very preliminary allocation data points:\n\nmaster (baseline) with 0% failures and 300rps: 17.4 MiB/s\nh2-retries with 0% failures and 300rps: 19.9 MiB/s\nh2-retries with 10% failures and 270rps: 19.3 MiB/s\nh2-retries with 50% failures and 150rps: 16.0 MiB/s\nBecause I used strest-grpc to generate load, I wasn't able to keep rps constant at different retry levels.  As I increased the amount of retries, the rps from strest-grpc would drop.  What we do see, however, is that allocations per request goes down as we increase the retries.  This makes sense because a retry should be cheaper than a brand new request.\nThis isn't a comprehensive picture, but I think it demonstrates that we're in the right ballpark, at least for now.. Closing in favor of https://github.com/linkerd/linkerd/pull/1530. One thing to keep in mind is that this affects Linkerd's thrift proxying as well (it is susceptible to the same issue).  10MiB is way more than enough for the thrift namer interface but it seems like a reasonable default for thrift messages in general.. For reference, the default maximum size for a non-chunked HTTP request is 5120KB (approx 5MB) so this is pretty similar.\nhttps://linkerd.io/config/1.1.2/linkerd/index.html#http-1-1-protocol\n. I've tested this locally.. @edio could I get you to sign our CLA so that I can accept this PR?  thanks!!!\nhttps://buoyant.io/cla/. Thanks!. Fixed by https://github.com/linkerd/linkerd/pull/1548. I think latest master needs to be merged here.  The diff should be much smaller now, given the overlap with #1542. @hawkw: I'm very interested in your feedback for this proposal. Fixed by https://github.com/linkerd/linkerd/pull/1622. I think you need to merge master here.. @esbie do you know if the ingress controller spec (such as it is) says anything about this?  Is there any mechanism to do anything with the provided port, or should it always be ignored?. Would you mind providing your :9990/admin/metrics.json for an instance of linkerd that is experiencing this issue?  That will help us identify the problem.\nAs far as I know, thrift doesn't have any notion of streaming or chunked encoding.  This means that linkerd must buffer the entire message in memory.  It's possible this memory pressure is causing linkerd to do a lot of GC.  . Hi @perspectivus1, does that clear everything up for you?  Should we close this issue?. Would you mind signing our CLA so that we can accept this?. Thank you!!!. I think the issue here is that each path stack has a fixed terminal SF, even if the stack is rebuilt.  Looking the pathMk method in Router.scala we see there is a single sf (which is DynBoundFactory in this case) that is used as the Endpoint.  Even if this path stack is rebuilt, it will still have the same DynBoundFactory on the bottom.  This means that when the path SF is closed, it will close the DynBoundFactory.  When the path stack is rebuilt, it will get the same (now closed) DynBoundFactory.\nOne possible solution would be to have DynamicServiceFactory install a new RefCountedServiceFactory module immediately above the Endpoint in the next stack.  When DynamicServiceFactory builds a stack it calls open on the RefCountedServiceFactory which increments a counter.  When RefCountedServiceFactory gets close it decrements the counter and it only calls close on the underlying SF when the counter goes to zero.. Closed as duplicate of https://github.com/linkerd/linkerd-tcp/issues/71. This is really great!  scanLeft seems much more elegant than what we had before.  I'm not sure why your scanLeft ends up one element behind.  In my experiments with scanLeft it seems to do what I would expect.  I'll look at the code more closely and see if I can figure it out.. This is a fascinating bug.  I've filed: https://github.com/twitter/util/issues/195\nIt's not even clear to me how to fix it.  Perhaps someone from the twitter-util team will have some suggestions.. I think we can proceed with the workaround here with the intention of removing it once the bug in AsyncStream has been fixed.  Would you mind adding a comment that links to the AsyncStream issue and notes that the workaround can be removed once the AsyncStream issue is fixed?\nIn the meantime @klingerf is validating that performance improvement of this branch on our end.  Once that's done I think we should be able to merge this.\nThank you!. Would be interested to see if this still happens with this base image: https://github.com/linkerd/linkerd/issues/1369. If I'm reading this correctly, it seems that setting disableValidation: true forces the use of the JDK SSL provider which does not support client auth.  Therefore, disableValidation and clientAuth and incompatible.  \n@hawkw, you had the original repro of this issue.  Can you confirm that removing disableValidation: true makes the problem go away?\nIf this is the case, we should add documentation indicating that disableValidation and clientAuth are incompatible and potentially also add config validation that disallows both options from being set.. You should be able to see the address list through the dtab UI in the linkerd admin site.  This UI is powered by a json delegator UI that could also query directly.. \"This branch has conflicts that must be resolved\" :sob:. The finagle-h2 and grpc projects are already sbt subprojects in the Linkerd repo that can be built individually.  @oscar-stripe is that sufficient or are you still interested in them being moved into a separate repo?. Correct!. Finagle 7.1 is now out.. @hawkw I believe that https://github.com/linkerd/linkerd/issues/1593 refers to errors with the consul namer, not the consul dtab store.  In particular, I think the problem stems from the way that errors are handled in SvcAddr.scala. Hi @prdoyle, we still haven't been able to repro this internally.  Any additional repro instructions would be very helpful.. We can't really make any progress here unless we are able to repro.. This sounds pretty reasonable to me!  Are you interested in submitting a PR for this?. \ud83d\udc4b Very valid concerns.  We considered these factors when doing this refactor and I think performance should still be acceptable for reasons that I will explain.  However, this is only based on our intuition and reasoning since we don't have a k8s cluster of this magnitude to test against.  If you are able to try this out, any data that you can report back about performance would be extremely helpful.\nOn the linkerd side, this change went from having linkerd establish a single watch that watched all services to establishing individual watches on the services it needed to route to.  Even though this is a larger number of watches and connections, it should be a subset of the data volume and therefore decrease the load on linkerd.  Furthermore, linkerd expires idle watches (with a TTL of 10 minutes by default) so it's unlikely that Linkerd will keep watches on every service in the cluster, even if Linkerd is long running.  \nAs for load on the k8s api, namerd will effectively be used as a cache and should only establish 1 watch per service.  The Linkerds, in turn, establish watches on namerd.\nHopefully this makes sense.  But, as I said, theory and reasoning is no substitute for real world data; so please let us know if this doesn't match the actual behavior of Linkerd in prod.. Hey @zackangelo!  Does your request contain a single frame that is larger than the window size or is it many small frames whose sizes sum to larger than the window size?  A frame dump of a request that triggers this issue would be very helpful if you're able to provide one.  Thanks!. Close due to inactivity.. Here's my suggestion on how to reconcile useHealthChecks with healthStatuses:\nuseHealthChecks indicates whether filtering should take place at all with a default of false.  healthStatuses should be a list of statuses to accept with the default being [passing].  In this way, we get backwards compatibility with the current behavior but can also override the healthStatuses property to be something else.  This also means we don't need a special Any value; we can just set useHealthChecks: false to disable filtering altogether.\nAs for the implementation, I see the passing API parameter as an optimization.  If healthStatuses = [passing] then we can use the passing parameter to do server side filtering.  Otherwise, we have to do client side filtering.  \nWhat do you think?. Could the fallback issue in the second screenshot be an issue with the dtab resolution renderer?  If you send an actual request, does it fail or fallback to world1?. Fixed by: https://github.com/linkerd/linkerd/pull/1622\n. https://buoyant.io/2017/09/08/linkerd-1-2-0-announcement-migration-guide/. We believe that this is due to a kernel bug as described in https://github.com/netty/netty/issues/2616\nCan you try testing this on one of the following kernel versions to see if this resolves the issue?\n 3.10.45\n 3.14.10\n* 3.15. I don't think there's much we can do about this from our side.. Please update docs in linkerd/docs/telemetry.md as well. Hi @kumudt, I've got a few more questions to make sure I understand the issue.  Please let me know if this is correct:\nUnder Linkerd versions 1.1.2 and 1.1.3 everything works as expected (though there are perhaps some spurious messages in the logs).\nUnder Linkerd versions 1.2.0 and 1.2.1 there are two errors scenarios:\n\n\nWhen you attempt to send a request through Linkerd, No hosts are available for /svc/<hostname>, Dtab.base=[], Dtab.local=[]. Remote Info: Not Available is returned.  Does this start happening immediately after Linkerd is deployed?  Does it happen all the time, or only for a fraction of requests?  This type of error is usually a configuration error related to dtabs or transformers.  The best way to debug is to visit the admin dashboard that Linkerd serves on port 9990 and type /svc/<hostname> into the dtab playground to see how Linkerd is attempting to route the request.  If you can provide a screenshot of the result, we can help you make sense of it.\n\n\nWhen a service deployment happens, requests to that service start returning status code 504 and the Linkerd logs indicate that Linkerd is attempting to connect to the IP addresses of the deleted pods.  Does this only happen after deployments, or can it happen randomly as well?  If it happens not after a deployment, what do the IP addresses that Linkerd is attempting to connect to correspond to?  What you describe sound like the exact symptoms of an issue that was fixed in 1.2.1 so I'm surprised that you're still experiencing it.  If you use the dtab playground to resolve /svc/<hostname> does it show the new pod IPs or the old pod IPs?  Can you provide a screenshot of this as well?. That's great to hear that 1.2.1 seems to have resolved scenario two!  \n\n\nAre you able to narrow down at all the conditions under which scenario one happens?  It is very difficult for us to track down without either reproduction steps or some more specific information about what's different when the issue is happening compared to when it's not.. @Taik: yes, from those logs it looks like Linkerd cannot connect to the k8s master API.  This could be due to an issue with the k8s master or a network issue.  If Linkerd cannot communicate with the master, it will not be able to receive service discovery updates.. There's not much that can be done in terms of mitigation if Linkerd can't reach the master.  Linkerd already caches results and attempt to reconnect.  I recommend trying to diagnose the cause of the connection problem.  You may want to look at a TCP dump to see why the connection is being lost.. I'm going to go ahead and merge this, but option 1 above sounds good to me.. Yes, I think you are correct that this can be derived from the raw histograms (BucketAndCount).  Are you interested in working on this feature?. It was my (perhaps incorrect) understanding that the mixer could fill in (some? all?) of these attributes if given souce.uid.  source.uid should be sufficient to derive the other attributes using the k8s api.. I tried this out locally and it seems to work great.\n@Ashald: I know you maintain your own fork of the consul namer.  I'd love to hear your feedback on this change.. @Ashald I don't think the caching behavior of the consul namer will be changed by this branch.  It should still be the case that Linkerd will use last known information for any of the consul services that it's routing to in the case of a consul outage.. @Ashald Previously Namerd would keep an up-to-date list of all services/tags, but not the actual address sets for them.  So if Consul was unreachable, it would not be possible to route to a service that had never been routed to before.  . @Ashald Let me know if you would be willing to test this feature out and we can get you a snapshot image.  We're doing out best to test the performance of this on our side but there's no substitute for a real cluster.. This sounds like a good idea.  Pull requests welcome.. And resolve the merge conflicts \ud83d\ude09 . Hi @elecnix, are you still interested in this?. Actually it looks like you already did sign the CLA on 5/9/2017.  So I think once the merge conflicts are resolved this should be good to go.. Thanks for the PR, this is awesome!  I can't wait to take a look and dig in. \ud83d\udd0f . Sorry for the delay on reviewing this PR.  We're working on finalizing a Linkerd release that this PR will not make it into, but we'll be able to look at this as soon as the release is out the door.. Hi @cponomaryov, are you still interested in this?. Thanks for the contribution and double-thanks for being patient with us!  :star: :rocket:. To give some background here, this works like this because Linkerd uses the Kubernetes endpoints API: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#watch-145\nThis API gives back a list of addresses for a service.  Unfortunately, there's no way (that I'm aware of) to pass a label selector that will be used to filter the addresses that are returned.  Instead any label selectors that you supply will be used to select which services are queried.\nAs you say, this behavior is unintuitive and not useful.  I'm definitely open to ideas about how to make this better given the constraints of the Kubernetes API.. Closing due to inactivity.  @cb16 please re-open if you are still seeing this issue with the latest Linkerd version.. @mebe Thanks for the very excellent reproduction instructions!  I've tried to dig into this but I'm having trouble getting it to run for long enough to exhibit the issue.  I set MaxDirectMemorySize to 512M in an attempt to reproduce the issue faster.  I had to use only 2 of the 5G files in order for minio to not run out of space on my laptop and am able to run the upload for a while but it will eventually stop when Linkerd encounters:\nE 0322 10:23:07.654 PDT THREAD19 TraceId:17040cea6d9f4381: service failure: com.twitter.finagle.ChannelWriteException: com.twitter.finagle.ChannelClosedException: null at remote address: /127.0.0.1:9000. Remote Info: Not Available from service: $/inet/127.0.0.1/9000. Remote Info: Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /127.0.0.1:9000, Downstream label: $/inet/127.0.0.1/9000, Trace Id: 17040cea6d9f4381.02e1c836ab7aaf92<:17040cea6d9f4381\nindicating the minio has closed the connection to Linkerd for some reason.  This results in Linkerd responding to the client with 502 which stops the upload.\nWhat I have observed so far is that Linkerd memory usage shoots up to about 3.3GB relatively quickly but then seems to stay steady there.  I have not yet been able to reproduce either an unbounded memory growth or an OutOfDirectMemoryError.. tracked_streams might be a better name for this gauge.  I think we want to keep this as it indicates how many streams we are tracking in the hashmap.  \nAdding gauges for streams in particular states (open, reset, etc) sounds good in theory, but I worry about needing to iterate over the entire hashmap to compute this.... Closed until we decide what we want to do.. @sgrankin this looks great!  I've skimmed your change and I agree with the general approach.  I'd like to review it in greater detail so if you could create a PR against Linkerd that would be greatly appreciated and we can go through the nitty-gritty details there.\nThank you!!!. Hi @draveness, I'm sorry that I missed your original comment back in September.  This issue had been de-prioritized because there had not been any reports of this actually happening in production.  But if you're running into it, then I'd definitely like to get it fixed.\nWhat are you able to share about the production environment where this is occurring?  Do you have steps that can reliably reproduce the issue?  . Thanks for the update, @draveness.  I will close this issue until we get a reproducible report of this.  Please re-open if you run into this again!. All relevant changes are in https://github.com/linkerd/linkerd/pull/1698/files#diff-82c97ffce6d2e44e82d279b37b62865c\nThe rest is just scalariform. Poked this a bunch manually and everything looked fine.. It's incorrectly in H2ServerConfig in the code.  \nYeah, you should remove this from routers.md because it's protocol specific. . Because of the complexity of this issue, I think the description could benefit from being a lot more precise.  What are the exact sequence of events that leads to the issue?  I think it is:\n\nRequest headers are sent from the client to linkerd and then sent from linker to the server.\nResponse headers with EOS are sent from the server to linkerd and then sent from linkerd to the client.\nThe client disconnects.\n\nAt this point, what are the symptoms?  Does linkerd fail to send a Reset to the server (correctness issue)?  Are items permanently left in one of the dispatcher stream maps (memory leak)?  Which one?  What state are they left in?\nBased on reading the code, here is my interpretation of what I would expect to happen in this scenario:\n\nBefore the client disconnects, the Linkerd server dispatcher should have a StreamTransport in the Half-Closed (local) ie LocalClosed(RemoteStreaming) state.  It should be in this state because it has fully sent the response but is still receiving the request.  \nMeanwhile, the Linkerd client dispatcher should have a StreamTransport in Half-Closed (remote) because it has not fully sent the request but has finished receiving the response.\nWhen the client disconnects, the ClientDispatcher.demux method should see either a GoAway frame or a ChannelClosedException depending on how gracefully the client disconnected.  Either one should trigger a call to resetStreams which should iterate through the streams and call remoteReset on each one.\nWhen the Server dispatcher's StreamTransport gets remoteReset, it causes the LocalClosed(RemoteStreaming) to fail it's AsyncQueue with discard=true.\nMeanwhile, the client dispatcher's StreamTransport is attempting to write the request stream by calling stream.read in a loop in the writeStream method.  When the AsyncQueue is failed, this stream.read should result in a Reset.Cancel exception which is then wrapped in a StreamError.Local\nThis exception should cause the writeF future to become an exception and hit the Throw(StreamError.Local) case of respond in the send method.  This results in a call to localReset.\nThis causes resetP to become a StreamError.Local(Reset.Cancel).  In the client dispatcher, an onReset handler was added to this stream transport when it was added to the stream map.  This onReset handler causes the StreamTransport to be replaced by a StreamLocalReset in the streams map and causes a Reset to be sent to the server.\n\nDoes this description match what is actually happening?\nOne thing that stands out to me is that in step 7 when the stream is reset from local, the client dispatcher could theoretically look at the stream transport's state and see that it is already in RemoteClosed.  Therefore, a local reset is enough to fully close the stream and remove it from the streams map entirely rather than replacing it with StreamLocalReset.\n. A better question is \"why would this retain be necessary?\".  We're just moving a ByteBuf we already have a handle on into a new container.  \nOr another question: \"If this retain is appropriate, then were is the corresponding release?\".. Hi @zillani \nUnfortunately, due to the way that Linkerds are deployed per-host and shared between apps from potentially many namespaces, there isn't an easy way to route based on the originating namespace.\nThere are a couple of potential workarounds:\n Deploy a Linkerd per namespace, each listening on distinct ports.  Apps in namespace A would be configured to use the Linkerd port for namespace A.  That Linkerd would be configured to route to namespace A.\n Have apps pass a dtab override header that indicates which namespace to route to.. @zillani sorry for the extremely late response.  I would recommend starting with a multi-router configuration.. @jftz are you still seeing this issue?  Can you provide the information above so that we can help investigate?. Ah, that's super interesting!  Yes, I think you are correct that it is a fundamental property of Var (and Event) that they do not keep history.  With that in mind, your approach here makes sense.  I do like the updated signature you propose (moving state into the param list).. @dpetersen are you still seeing this issue with Linkerd 1.3.6?. Thanks, @dpetersen.  Hard to say for sure if this is fixed or not given our inability to reproduce it.  I'm going to close this issue for now but please re-open if you see this again after upgrading.. Hi @yoitsro, very sorry that no one responded to you until today.  I must have missed your comments because they were on a closed issue.  This thread, unfortunately, conflates several reports which may or may not have the same root cause.  \n@yoitsro and @ejwood79, if either of you are still interested in getting to the bottom of this, we're more than happy to help you debug.  If you could open new issues with as much detail as possible about your environment (Linkerd version, config, etc) and the symptoms you're seeing, that will help us to dig in and figure out what's going on.\nI'm going to re-close this particular issue because the original issue has already been fixed.  But I hope you do file new issues so that we can investigate.  Sorry, again, for the lack of communication and thanks for your understanding.. My understanding is that the Linkerd admin UI only makes short-lived observations of delegations to Namerd.  In effect, when the Linkerd admin UI does a delegation, it should cause Linkerd to make a thrift call to delegate with no stamp.  When Namerd returns a response with a stamp, Linkerd will make another delegate call with the returned stamp but should cancel the request very quickly (because the admin UI only required the initial value).  Therefore, the number of entries in Namerd's delegation ActiveCache should be roughly equal to the number of concurrent requests to Linkerd admin UIs.\nTwo potential solutions:\n Size the delegate active cache to be large enough to accommodate the number of concurrent admin UI requests.\n Add a non-long-polling method to retrieve only the initial value of a delegation, instead of using a short lived observation.. @jackkleeman yeah, you've hit the nail on the head.  the first would be very simple but would require an estimate on the number of concurrent requests.  However, it should be safe to set this number to be quite large.  For example, the default size for the binding and addr active caches are 1000 each.. :+1: I think that's a good idea.. I believe this should be fixed by https://github.com/linkerd/linkerd/pull/1862. Hi @aidansteele \nI'm able to run ./sbt compile on current Linkerd master (7c8499b9a0738211c9b160e9683479dbc2f78dee) and I don't see the error that you reported.\nHave you tried running ./sbt clean?  Could there potentially be something wrong in your ivy cache?. Closing this since I can't repro.. It would probably be a good idea for the consul namer to backoff on errors too.  But that doesn't have to be part of this change.. Hi @aidansteele, thanks for the pull request!  Unfortunately, I don't think we'll be able to merge this.  It's essential to Linkerd's routing model that the Host header not be changed.  Consider the following common scenario: in Linker-to-Linker mode, both the incoming and outgoing Linkerds must see the same Host header so that they can route consistently.  Having the first Linkerd mutate the Host header would violate this.\nWe're definitely interested in supporting ALB or IIS style host based routing though.  I think the best way to proceed would be to open an issue with as much detail as possible about the requirements in order to make this work and we can brainstorm together about a way to fit that into Linkerd's existing routing model.\nLet me know what you think!. @obeattie you are very right. Superseded by https://github.com/linkerd/linkerd/pull/1752. Hey @obeattie and happy new year!  I'm excited about this fix and I'd like to move forward but I could use some more help understanding the bug that's being fixed here (see my previous comment).  . Ah, makes sense.  I do think your refactors make the code more clear and increase the code quality level.  However, since we currently have active investigations into potential bugs with the k8s namer, I think I'd prefer to make the smallest change possible that fixes the bug.  Even though the code looks solid to me, I'm afraid of changing too many things at a time.. Thanks for this, @edio.  This actually relates pretty closely to https://github.com/linkerd/linkerd/issues/1731.  The pickFirst method you implement here is essentially just toFuture but coerced to an Activity.  If the Activity always just uses the first value, then the delegate method should really have a return type of Future instead of Activity.\nThis points to a larger change.  If we truly always only care about the initial value of a delegation (and I think this is probably the case) then we should change the signature of delegate in the Delegator trait to return a Future instead of an Activity.  This allows us to replace the use of pickFirst with toFuture.\nThis also allows us to change the namerd delegation thrift method to be a simple unary request instead of a long-poll.  In turn, this allows us to get rid of the delegation observation cache that is causing us trouble in https://github.com/linkerd/linkerd/issues/1731.  \nOf course, this is a much larger sweeping change and also a breaking API change to the namerd API so we should tread carefully.  But I think going in this direction is much preferable to the isolated change here that makes the delegate implementation incongruous with it's type signature.\nLet me know what you think.. Thanks, @edio.  I will discuss this with the team in the new year once folks are back at work.. Hi @edio, we may have an alternate solution for you.  Take a look at this PR: https://github.com/linkerd/linkerd/pull/1768 which adds a /client_state.json admin endpoint that returns the current address set for each client.\nThis should be more performant than delegator.json because it doesn't spin up a new Activity and it should also be more correct because it uses the existing Activity/watch rather than starting a new one.\nPlease take a look and let me know if that satisfies your requirements.. Hi @edio, sorry for the delay in getting back to you about this.  After some internal discussion I think the best path forward is:\nChange the signature of the delegate method on the Delegator trait to return a Future instead of an Activity.  Add a new thrift method to the namerd thrift interface that returns just the current delegation and have the thrift interpreter client use that for the delegate implementation.  Add a new grpc method to the namerd mesh interface that returns just the current delegation and have the mesh interpreter client use that for the delegate implementation.  I believe there is already a non-streaming version of the delegate http endpoint that the http interpreter client can use.\nThis is certainly a larger change that what you have proposed here, but I think it is more correct and will be easier to maintain in the long run.  If you'd like to take a crack at implementing my suggestion, I'd be happy to help out however I can.  Otherwise, we'll likely have time to do this sometime next week.\nLet me know what you think!. Hey @edio, I'm going to close this PR but I'm looking forward to working together with you on the solution we discussed above.. Thanks for reminding me about this!  If you folks don't have the bandwidth to do this, I'll make sure we allocate time for it as part of 1.3.7.  Does that work for you?. Thanks for the additional info, @yangzhares.  That should give us enough to try to reproduce it.  If we simply send a request to Linkerd for a consul service that does not exist, we can check to see if requests to consul for that service continue forever.\n@deebo91 can you attempt to reproduce this?. cc @obeattie . Closing as a duplicate of #1079 . Hi @malafeev!  Thank you for starting work on this!  We're really excited to get Open Tracing support into Linkerd.  Before we dig too much into the implementation details, could you tell us a little more about how this works?\n\nWhat are the steps for testing this?\nI don't see the OpenTracing telemeter in this branch, is that in progress?. Hi @malafeev, have you had a chance to publish the telemetry plugin?. Yikes.  Unfortunately, I don't have much advice to offer.  Perhaps some kind of dependency shadowing might help?. Hi @malafeev, any luck getting the shadowing to work?. @obeattie by chance have you had an opportunity to test this in your staging cluster?  This should hopefully improve the quality of your metrics.. Hi @xiaoerlyl, unfortunately the finagle thrift client does not set the dest request header.  Linkerd uses a special filter in order to set this header:\n\nhttps://github.com/linkerd/linkerd/blob/master/linkerd/protocol/thrift/src/main/scala/com/twitter/finagle/buoyant/linkerd/TTwitterClientFilter.scala. @mmrozek a simple timeout would not work for the active cache.  The active cache requires the guarantee that any observation that has a pending request MUST not be removed from the active cache.  Having a timeout could cause observations that have pending requests to be removed from the active cache, which would cause those requests to hang forever.. @mmrozek good question.  the most conservative thing to do would be to make this configurable with the default being no TTL (ie make the default the same as the current behavior).  Once a few users are running in prod using TTLs and don't experience any issues, we could then consider changing the default.  What do you think about that?. Fixed by https://github.com/linkerd/linkerd/pull/1923. It would also be good to know if this issue occurs when the fs namer is removed from the config entirely.. Thank you @nikolay-pshenichny for this incredibly helpful repro.  Using this we were able to quickly identify the problem.  When we fail to create a watch (due to inotify watch exhaustion) we never clean up the inotify instance we created which leads to the inotify instance leak you observed.  I'll put a PR up this afternoon to fix the issue.. Here is the aforementioned PR: https://github.com/linkerd/linkerd/pull/1787\nPlease let me know if you're able to verify that this fixes the issue on your end!. HTTP access logging (#1776) is not a prerequisite but we should (and will) do it anyway.. Hi @msiebeneicher!  Linkerd allows you to create custom response classifier plugins.  See: https://github.com/linkerd/linkerd-examples/tree/master/plugins/header-classifier for an example.\nI recommend that you go with case A but use a custom response classifier plugin that classifies 57x responses as successful.. Closing this for now.  Please re-open if you experience this issue again.. This seems like a pretty reasonable feature.  Is there any precedent for this kind of strict mode in other ingress controller implementations like nginx, traefik, or haproxy?  How do other ingress controllers deal with this non-determinism.. Thanks for starting a discussion on this.  There's a lot left unspecified in the K8s ingress spec (such as it is) so it's not super clear what the right behavior should be.  Preferring more specific ingress rules is one way to attack the ambiguity but a) it's not clear to me that more specific is always better and b) still leaves many cases ambiguous (where multiple rules match that are equally specific).\nOne approach is to explicitly state that if multiple rules match, the choice of rule is undefined.  This forces users to be careful and craft non-overlapping rules if they want predictable behavior.  This is more or less what we do now, although we perhaps could communicate it better.\nAnother approach would be to allow setting a \"priority\" value as an annotation on the ingress resource.  Not sure if this is a good idea or not, just brainstorming.\nAs before, it would be interesting to look at other ingress controller implementations to see if there is any consensus on how to handle this kind of ambiguity.. Better config validation is always a good idea!\nLinkerd will already exit immediately if the config file is absent or fails to parse.  There are a few things I can see where we can improve:\n\nFor client tls trustCerts we could use the File config type instead of String which will validate that the file exists\nDitto for server tls certPath, keyPath, caCertPath\n\nAre there other validations that you had in mind?  Are you interested in submitting a PR for this?. Fixed by https://github.com/linkerd/linkerd/pull/1854. My preference is option 1.  Yes, this does break compatibility with existing custom plugins that may exist outside the repo.  I think this is okay because a) plugin maintainers need to recompile against new versions of Linkerd anyway and b) the change they need to make is very simple and easy.. Thanks for this report @yennifehrrr!  There are some additional pieces of information that will help us track this down:\n\nAre you able to isolate the exact Linkerd version where the leak started?  There have a been a number of large changes/releases between 1.1.2 and 1.3.4 so pinpointing this to an exact release where the leak was introduced would be very helpful.\nDo you see any symptoms other than the Netty leak reports?  Do you see RSS grow over time?  Are you able to capture a heap dump when memory usage is severely high?\nAre you able to produce a stand-alone program that we could compile and run that exhibits the issue?. Hi @yennifehrrr, are you able to collect and provide any of the information above?. Hi @yennifehrrr, we have fixed a number of h2 memory leaks in version 1.3.7.  Can you try again with the latest version and let us know if you still see this?  Thanks!. Ah, darn. Thanks for checking, @yennifehrrr.  One more thing to try: can you try setting the JVM flags from this PR: https://github.com/linkerd/linkerd/pull/1889. @yennifehrrr any luck with the flags I suggested?. @yennifehrrr any updates here?  we've got our fingers crossed \ud83e\udd1e . I'm going to close this ticket due to inactivity.  Please re-open if you are still experiencing these memory leaks when running with the new settings.. Awesome!!!. Thanks for filing this, @briansmith.  I'll try to use this space to shed a bit of light on this topic and hopefully clear up some confusion.  I agree with you that Linkerd's behavior in this regard is surprising and unintuitive.\n\nIt is not actually possible to create a namer that mutates requests.  Namers operate on Finagle Names and NameTrees which means that a namer does not have access to the request object.  Any rewrites of the Name that a namer does are for the purposes of routing only and cannot affect the request itself.  Likewise, dtabs operate on Finagle Names and not requests and don't have access to the request object.\nI think a lot of the confusion around this comes from the fact that Finagle Names are slash delimited and look very much like request paths and it can be easy to conflate the two, especially when the io.l5d.path identifier is involved.\nIn contrast to Namers, identifiers (such as the io.l5d.path identifier) DO have access to the request object and therefore have the ability to mutate requests.  A detailed description of identifiers and namers is available on the routing in-depth doc.\nEven though identifiers have the ability to mutate requests, we've tried to avoid doing this as much as possible.  This is because a request can transit multiple Linkerds in a single service-to-service hop (2 Linkerds in a linker-to-linker setup, potentially more in more complex setups).  If each Linkerd that a request passes through modifies it, each Linkerd will potentially see a request with different properties and may therefore make different routing decisions.  This isn't necessarily wrong, but it makes routing much more difficult to reason about.  The one exception that we have made (by popular demand) is for path consumption in the io.l5d.path identifier.  But even this introduces a common pitfall where if the path is consumed by the first Linkerd then the second Linkerd will be unable to route.\nOf course, request modification is a powerful and sometimes necessary feature so we can't bury our head in the sand and avoid it forever.  But we have not yet spent the time thinking of a way to introduce it into Linkerd in a way that minimizes cognitive load and complexity, especially as it relates to routing and policy as a request transits multiple Linkerds.. Any progress on integration testing this?. @negz have you had a chance to test if #1817 resolves all the issues you were seeing or if you're still encountering this bug with DELETEs and resource versions?  Would it be helpful for us to cut an image that includes both #1817 and this change for you to test?. @negz #1828 also may or may not be related to this.  @deebo91 can you rebase this branch onto latest master so that when @negz cuts a test build off this branch, #1828 will be included?. Given that redundant updates when the resource versions are equal are probably not a problem, how do we feel about closing this in favor of #1810?. Wow yeah, this is super weird!  If you can reproduce it, it might be interesting to do some println debugging in the Throw(e) => block to print: \n The exception encountered\n The hashcode of the exception encountered\n The exception expected\n The hashcode of the exception expected\n The result of Failure.unapply on the exception encountered\n The hashcode of the result of Failure.unapply\nHopefully this might shed some light on why that earlier branch isn't getting matched.. Thanks for filing this!  In other situations like this, we apply the backoff in the Var.async loop by calling Future.sleep(backoff).flatMap { _ => loop(index) } instead of just loop(index).  See ThriftNamerClient for an example of this.  We should do this in SvcAddr too.. I think the idea solution would be to actually remove the infiniteRetryFilter altogether and use the loop cases to handle those retries.  A very similar thing was done in this PR: https://github.com/linkerd/linkerd/pull/1703/files. Ah, I see what you mean now.  You're right.\nI wouldn't except throwing an exception on every 5xx to be a serious performance problem if backoffs are working as intended because the backoffs would limit the number of requests being made.  It's also worth noting that these json parse exceptions are caught immediately in BaseApi.executeJson and turned into a Try.. Good question.  We've considered this but Finagle doesn't support Server-Sent-Events which the marathon event stream uses.  This makes using the marathon event stream in Linkerd considerably more difficult.  Of course, we would absolutely welcome a PR that implements this.. Thank you for investigating this @chrisgoffinet!\nIt sounds like Linkerd is getting many requests for many different service names.  There is a memory cost associated with each service that Linkerd maintains, even if the service cannot successfully route requests.  It is a design decision that Linkerd treat these services as valid because of the dynamic nature of binding.\nFor example, consider the service name /svc/foo and the dtab /svc => /#/io.l5d.consul/dc1.  If foo does not exist in consul, this request will fail.  However, Linkerd will keep the /svc/foo service because foo might be created in consul at a later date.  Or the dtab might be update to point /svc/foo at a destination that does exist.  \nThere are a couple of things that you can do to mitigate the memory cost of keeping all of these services.\n\nYou can try reducing the size of Linkerd's path cache: https://linkerd.io/config/head/linkerd/index.html#binding-cache  This will reduce the maximum number of services that Linkerd will keep cached.  Reducing this number too much can hurt Linkerd performance when Linkerd is connecting to many different valid services, but as long it is larger than the total number of valid services that you expect Linkerd to talk to, it should be okay.\nYou can try reducing the idleTtlSecs timeout: https://linkerd.io/config/head/linkerd/index.html#binding-cache  This reduces the amount of time Linkerd waits before closing idle services.  With a smaller number, Linkerd will more quickly close the idle invalid services.  However, there is a performance cost if this is shorter than the idle times between requests for valid services.. The consul connection leak is very similar to the service leak I described above.  When Linkerd attempts to resolve a name in consul, it establishes a watch on that name, even if the service does not currently exist.  This allows Linkerd to be notified immediately in case a service with that name is created.  The result is that Linkerd maintains a connection to Consul per name it is watching.  \n\nThis can be mitigated with the same techniques as above:\n\nLimiting the size of the client cache: https://linkerd.io/config/head/linkerd/index.html#binding-cache This will put a limit on the number of clients that Linkerd will cache and therefore put a limit on the number of connections to consul Linkerd will maintain.\nReducing the idleTtlSecs timeout: https://linkerd.io/config/head/linkerd/index.html#binding-cache will cause Linked to tear down watches on idle names more quickly.. @MirzaMerdovic @chrisgoffinet Let me know if my explanation makes sense and if it is consistent with your observations.  I also encourage you to experiment with the configuration options above and let me know if is sufficient to get memory/connections/performance to an acceptable level.. @MirzaMerdovic That's correct.  Linkerd should maintain one connection to Consul per service that is watching (regardless of if that service exists or not).  It should be independent of the number of requests.  If you are seeing new connections for each request to the same service, that is a bug.. Unfortunately, it looks like Finagle is still on Netty 4.1.16.  We won't be able to upgrade until they do.. Linkerd is now on Netty 4.1.28 so this should be fixed.  Please re-open if you see this again.. Thank you for this great contribution!. Hi @voki, thanks for this detailed report!  This does indeed look like a bug with our zk session management.. @voki do you have any advice on how to trigger a session expiry for the purposes of reproducing and testing this issue?. @voki I've created https://github.com/linkerd/linkerd/pull/1830 which sounds like it may fix your issue.  Please let me know if you're interested in testing this build.. Awesome!  Are you able to build off of my Pull Request?  Or would it be helpful for me to send you a binary or docker image?. @voki I've pushed buoyantio/linkerd:1.3.5-SNAPSHOT-zk-fix. Ah, sorry about that.  I've now pushed buoyantio/namerd:1.3.5-SNAPSHOT-zk-fix as well. Thanks for your help with testing this!  Those log messages are perhaps a bit too verbose, but do not indicate a problem.. Hi @jacob-go are you still investigating this?  Are you able to provide us with a more self-contained reproduction environment so that we can repro and help investigate?. @jacob-go does https://github.com/linkerd/linkerd/issues/1887 capture the issues the you're running into?  can this issue be closed?. Great!. Hi @etotten!  Can you try something like this in your Linkerd config:\n\n```\nnamers:\n- kind: io.l5d.serversets\n  zkAddrs:\n  - host: myhost01\n    port: 2181\n  - host: myhost02\n    port: 2181\n  - host: myhost03\n    port: 2181\nrouters:\n- interpreter:\n    kind: io.l5d.namerd\n    dst: \"/#/io.l5d.serversets/discover/yada/yada/namerd:thrift\n  ...\n```\nI believe this will use the locally configured serversets namer to discover Namerd, and then use the namers configured in Namerd to do all request routing.  \nPlease give that a try and let me know how it goes!. I haven't had a chance to look into this in depth but my instinct here would be to update the Activity to NameTree.Neg when we get a \"dc doesn't exist\" error but continue to run the polling loop in case the DC ever comes into existence.. If my understanding is correct, there are two different scenarios being discussed in this issue.  Let me discuss them individually, and please correct me if I have misunderstood.\n@krjensen is asking about sending all egress traffic through an HTTP proxy (squid).  This should be easy to do by adding a fallback rule to the dtab which sends requests directly to the HTTP proxy.  Something like:\n/svc/* => /$/inet/<http_proxy_ip>/<http_proxy_port>\nSince later dtabs rules have higher priority, this dtab rule should be added to the start of the dtab.  This will effectively act as a fallback.  If a request is made to Linkerd and Linkerd cannot resolve it through service discovery, it will fallback to sending it to the HTTP proxy.\n@krjensen does this answer your question?\n@sean-brandt on the other hand seems to be asking about sending traffic through a SOCKS proxy.  This is more complex because Linkerd needs to be made aware that it is talking to a SOCKS proxy and use the SOCKS protocol.  As you say, Finagle does have support for this.  The way I would imagine this working is by adding the ability to specify a SOCKS proxy on a Linkerd client configuration.  eg something like:\nrouters:\n- protocol: http\n  client: \n    socksProxy:\n      ip: 1.2.3.4\n      port: 4321\nIf this is something that you're interested in, @sean-brandt, would you mind opening a new issue specifically around SOCKS support?. @sean-brandt in that case, you should be able to take care of the routing entirely in your dtab:\n/svc/foo => /$/inet/<http_proxy_host>/<http_proxy_port>\nWill route requests for foo to the proxy.. Linkerd forwards the Host header unchanged which means that it needs to be set to the desired final destination by the application that is sending the request to Linkerd.  Something like this:\nApp sends request to Linkerd:\nGET /bar\nHost: foobar.nowhere.com\nLinkerd routes /svc/foobar.nowhere.com => /$/inet/<proxy_host>/<proxy_port and sends the request to the proxy.\nProxy sends the request to foobar.nowhere.com.\nDoes that make sense?. Good question.  Putting aside SOCKS proxying and CONNECT tunneling for the moment, I think this issue is specifically about configuring Linkerd's routing rules to use an HTTP proxy.  @sean-brandt and @krjensen: do my explanations here make sense?  Can we close this issue?. Ah, yes, this is expected because Linkerd does not support creating tunnels with the CONNECT method.  I've filed https://github.com/linkerd/linkerd/issues/1982 to track that separately.  . For HTTPS proxying, CONNECT tunneling is required and that is tracked in issue #1982.\nFor HTTP, Linkerd's routing rules can be configured to send the traffic to the external proxy (see discussion above).  @rasmus @krjensen does this cover your use-case or is there something else that I'm missing?. Fixed by #1851 . Hi @mebe, can you provide some more information about the severity and impact of this error?  Do you see failed requests from the client's (COSBench) perspective?  Approximately what percentage of requests fail?  Does Linkerd seem to be handling all other requests correctly, or does this exception seem to put Linkerd into a degraded or failing state?  Are the requests that fail safe to retry?. @chrisgoffinet have you had any success in reproducing this issue?. @mebe Closing this for now, please reopen it if you see this issue again.. @chris-goffinet-ck any update here?. @chris-goffinet-ck I know you're still investigating this one but I'm going to close it for now because it's not actionable until we have more information.  Please re-open when you have more to share.. This is a great issue, thanks!  It's true that we don't have a way to signal the explicit disabling of TLS.  We should fix this.. Fixed by https://github.com/linkerd/linkerd/pull/1904. I think the ideal fix here would be a change to twitter/util's flag parsing so that - is not treated as a flag.  I believe the offending code is here: https://github.com/BuoyantIO/twitter-util/blob/develop/util-app/src/main/scala/com/twitter/app/Flags.scala#L138\nIf possible, I'd like to avoid breaking backwards compatibility or introducing a second way to specify a Linkerd config.  \nAs a workaround, ./linkerd-x.y.z -- - might work to read the config from stdin?. The main method is already quite short so breaking it up hinders readability in my opinion.  And refactors that don't improve the quality of the code make navigating the git history a bit more difficult.  I think this should just be as simple as adding if (validate()) return to the existing main.  There are also some formatting changes in this diff that I think should be reverted.  Thanks again for working on this and for considering my feedback.. Hi @eroji!\nIt looks like Linkerd is able to find the hello pods fine but isn't able to find a Linkerd pod on the same host.  It will be looking for a service called l5d with a port named incoming in the default namespace.  \nDid you perhaps deploy Linkerd into a different namespace?  If so, you'll need to update this block in your Linkerd config to reference the namespace where Linkerd is deployed:\ntransformers:\n        - kind: io.l5d.k8s.daemonset\n          namespace: default\n          port: incoming\n          service: l5d\n          hostNetwork: true. @briansmith I could use some SSL expertise in understanding what the default_md is used for when generating pkcs#8 keys and how to configure the Netty SSLContext to be able to handle such keys.  Is the message digest the same as the cipher?  . Thanks, @briansmith!  I've dug into this a bit more and I cannot reproduce it.\n@Capitrium here are the steps that I am taking and I am unable to reproduce the issue:\nUsing this openssl.cfg:\n```\ndir = certificates\n[ ca ]\ndefault_ca = CA_default\n[ CA_default ]\nserial = $dir/serial\ndatabase = $dir/index.txt\nnew_certs_dir = $dir/newcerts\ncertificate  = $dir/cacert.pem\nprivate_key = $dir/private/cakey.pem\ndefault_days = 36500\ndefault_md  = sha512\npreserve = no\nemail_in_dn  = no\nnameopt = default_ca\ncertopt = default_ca\npolicy = policy_match\n[ policy_match ]\ncommonName = supplied\ncountryName = optional\nstateOrProvinceName = optional\norganizationName = optional\norganizationalUnitName = optional\nemailAddress = optional\n[ req ]\ndefault_bits = 2048\ndefault_keyfile = priv.pem\ndefault_md = sha512\ndistinguished_name = req_distinguished_name\nreq_extensions = v3_req\nencyrpt_key = no\n[ req_distinguished_name ]\n[ v3_ca ]\nbasicConstraints = CA:TRUE\nsubjectKeyIdentifier = hash\nauthorityKeyIdentifier = keyid:always,issuer:always\n[ v3_req ]\nbasicConstraints = CA:FALSE\nsubjectKeyIdentifier = hash\n```\nI run the following commands:\n$ mkdir -p certificates/private\n$ mkdir -p certificates/newcerts\n$ echo 1000 > certificates/serial\n$ touch certificates/index.txt\n$ openssl req -x509 -nodes -newkey rsa:2048 -config openssl.cfg \\\n                    -subj '/C=US/CN=My CA' -keyout certificates/private/cakey.pem \\\n                    -out certificates/cacertificate.pem\n$ openssl req -new -nodes -config openssl.cfg -subj \"/C=US/CN=alex\" \\\n                    -keyout certificates/private/alexkey.pem \\\n                    -out certificates/alexreq.pem\n$ openssl ca -batch -config openssl.cfg -keyfile certificates/private/cakey.pem \\\n                    -cert certificates/cacertificate.pem \\\n                    -out certificates/alexcertificate.pem \\\n                    -infiles certificates/alexreq.pem\n$ openssl pkcs8 -topk8 -nocrypt -in certificates/private/alexkey.pem  -out certificates/private/alexkey.pk8\nI then use this linkerd.yml config:\n```\nnamers: []\nrouters:\n- protocol: http\n  label: outgoing\n  dtab: /svc => /$/inet/localhost/4141\n  servers:\n  - port: 4140\n  client:\n    tls:\n      trustCerts:\n      - certificates/cacertificate.pem\n      commonName: alex\n\nprotocol: http\n  label: incoming\n  dtab: /svc => /$/inet/localhost/9990\n  servers:\nport: 4141\n    tls:\n      certPath: certificates/alexcertificate.pem\n      keyPath: certificates/private/alexkey.pk8\n```\n\nI run Linkerd version 1.3.6:\n$ ./linkerd-1.3.6-exec linkerd.yml -log.level=DEBUG\nand then curl\ncurl localhost:4140\nAnd everything seems to work correctly.  Note that default_md is set to sha512 for both the ca and the cert.  . I'm running\n$ openssl version\nLibreSSL 2.2.7\non MacOS 10.13.3. @edio . @Ashald that's an interesting idea.  Do you have a sense of how that would work, exactly?  My instinct is that relying on just one API call (service) is more robust than combining information from two calls (list datacenters and service) but I'm curious to hear more specifics about your idea.. I'm not sure I follow what you mean. Are you concerned about the Addr getting set to Neg for healthy services when the service API experiences a transient 5XX and suggesting that we could ignore a \"Datacenter does not exist\" 5XX if we know the DC is present in the DC list response?\nThis seems like it introduces more opportunities for inconsistency.  I'm not sure it makes sense to trust the DC list response more than the service response if they are inconsistent.  I think my preference would be to set the Addr to Neg on a 5XX and then rely on the retry loop to correct the Addr to a real value on the next retry.\nBut let me know if I've misunderstood you.. @Ashald the approach that @dadjeibaah outlines above basically treats the DC list API as the source of truth about DC existence instead of the response from the service API.  Is the DC list API considered more reliable?  In particular, that change would mean that failures from the DC list API could block service resolution.\nOverall, my current preference is the way this PR is currently implemented because it's simpler and doesn't involve combining information from multiple sources.  But if the DC list API is considered significantly more reliable or authoritative than the service API, perhaps we can do something like what @dadjeibaah describes.. Fixed by #1893 . I'm not sure about that.  Finagle explicitly differentiates between Empty (the service exists but has 0 instances) and Neg (the service does not exist).  I'm not sure if we always want to fallback on empty services.\nFor this issue in particular, take a look at AppIdNamer.scala.  We would want to return a NameTree.Neg in the case that the address set is empty.  You can find an example of this type of behavior in the Kubernetes namer in EndpointsNamer.scala in the mkNameTree method.. Fixed by https://github.com/linkerd/linkerd/pull/1921. Hi @Xcorpio.  This kind of routing is accomplished using dtabs.  I recommend using the io.l5d.header identifier to read a header of the format /<namespace>/<service> and then using a dtab to route the request accordingly.  \nPlease feel free to post on discourse.linkerd.io with as much information as possible if you get stuck and someone will be able to help you.. Thanks for this report!  Do you know what kind of traffic to Namerd triggers this behavior?  Do you have any concrete reproduction steps that cause Namerd to create connections to Consul that never get closed?. I'd be pretty surprised if Namerd is spontaneously generating connections to Consul.  More likely is that these are being triggered by certain requests from Linkerd.  If you're able to give concrete repro steps including the requests to Linkerd (using curl or something similar) that will really help us cut down the time it takes to investigate.  Thanks!. Hi @Adiqq!  In the Linkerd config above, clients should not specify a port number when sending requests through Linkerd to Kubernetes services.  Instead, the Kubernetes named port for the appropriate protocol will be used:\n```\nIf your application receives HTTP, HTTP/2, and/or gRPC traffic it must have a\nKubernetes Service object with ports named http, h2, and/or grpc\nrespectively.\n```\nSo I believe that your commands like: \nhttp_proxy=http://dmse02lx0680c:4140 curl -s mongodb-api.dev-ram:4321\nShould actually be\nhttp_proxy=http://dmse02lx0680c:4140 curl -s mongodb-api.dev-ram\nAnd the mongodb-api service should have a port named http.. This change looks good to me.  Can you also update the description to include what testing has been done on this?. @edio do you have a docker image for this?  if not, I can pull your branch and build one. @cchatfield I've pushed buoyantio/namerd:consul_observations_leak and buoyantio/linkerd:consul_observations_leak. Great!  Multiple commits are fine since they'll all get squashed when merged to master.  However, all of this commits need the DCO signoff so you'll need to amend them to add that.. By default Netty samples 1% of buffers to check for leaks and logs if leaks are detected.  There are a number of scenarios (particularly when using gRPC) that trigger this leak detection on master that no longer trigger it after this change.\nhttp://netty.io/wiki/reference-counted-objects.html#wiki-h3-11. Finagle has a dependency on the Hotspot JVM: https://github.com/twitter/util/blob/develop/util-jvm/src/main/scala/com/twitter/jvm/Jvm.scala#L262\nWhen Linkerd is started on the OpenJ9 JVM, the following error is thrown:\njava.lang.UnsatisfiedLinkError: sun/management/VMManagementImpl.getVersion0()Ljava/lang/String;\n    at sun.management.VMManagementImpl.<clinit>(VMManagementImpl.java:64)\n    at sun.management.ManagementFactoryHelper.<clinit>(ManagementFactoryHelper.java:455)\n    at java.lang.Class.forNameImpl(Native Method)\n    at java.lang.Class.forName(Class.java:297)\n    at com.twitter.jvm.Hotspot.liftedTree1$1(Hotspot.scala:37)\n    at com.twitter.jvm.Hotspot.<init>(Hotspot.scala:31)\n    at com.twitter.jvm.Jvm$.liftedTree2$1(Jvm.scala:262)\n    at com.twitter.jvm.Jvm$._jvm$lzycompute(Jvm.scala:262)\n    at com.twitter.jvm.Jvm$._jvm(Jvm.scala:261)\n    at com.twitter.jvm.Jvm$.apply(Jvm.scala:274)\n    at com.twitter.jvm.JvmStats$.register(JvmStats.scala:98)\n    at com.twitter.finagle.Init$.<init>(Init.scala:46)\n    at com.twitter.finagle.Init$.<clinit>(Init.scala)\n    at com.twitter.finagle.client.StackClient$.endpointStack(StackClient.scala:58)\n    at com.twitter.finagle.client.StackClient$.newStack(StackClient.scala:224)\n    at com.twitter.finagle.buoyant.H2$Client$.<init>(H2.scala:28)\n    at com.twitter.finagle.buoyant.H2$Client$.<clinit>(H2.scala)\n    at io.buoyant.router.H2$Router$.<init>(H2.scala:49)\n    at io.buoyant.router.H2$Router$.<clinit>(H2.scala)\n    at io.buoyant.router.H2$.<init>(H2.scala:77)\n    at io.buoyant.router.H2$.<clinit>(H2.scala)\n    at io.buoyant.linkerd.protocol.H2Initializer.<init>(H2Config.scala:40)\n    at java.lang.J9VMInternals.newInstanceImpl(Native Method)\n    at java.lang.Class.newInstance(Class.java:1773)\n    at com.twitter.app.LoadService$.$anonfun$apply$7(LoadService.scala:73)\n    at com.twitter.app.LoadService$$$Lambda$81.00000000CC7244F0.apply(Unknown Source)\n    at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:241)\n    at scala.collection.TraversableLike$$Lambda$24.00000000CC587E30.apply(Unknown Source)\n    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)\n    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)\n    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n    at scala.collection.TraversableLike.flatMap(TraversableLike.scala:241)\n    at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:238)\n    at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)\n    at com.twitter.app.LoadService$.apply(LoadService.scala:62)\n    at com.twitter.finagle.util.LoadService$.apply(LoadService.scala:14)\n    at io.buoyant.linkerd.Linker$.LoadedInitializers$lzycompute(Linker.scala:65)\n    at io.buoyant.linkerd.Linker$.LoadedInitializers(Linker.scala:64)\n    at io.buoyant.linkerd.Linker$.parse$default$2(Linker.scala:79)\n    at io.buoyant.linkerd.Main$.loadLinker(Main.scala:69)\n    at io.buoyant.linkerd.Main$.main(Main.scala:36)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at com.twitter.app.App.$anonfun$nonExitingMain$3(App.scala:233)\n    at com.twitter.app.App$$Lambda$178.00000000CCBD6FA0.apply(Unknown Source)\n    at scala.Option.foreach(Option.scala:257)\n    at com.twitter.app.App.nonExitingMain(App.scala:232)\n    at com.twitter.app.App.nonExitingMain$(App.scala:210)\n    at io.buoyant.linkerd.Main$.nonExitingMain(Main.scala:20)\n    at com.twitter.app.App.main(App.scala:198)\n    at com.twitter.app.App.main$(App.scala:196)\n    at io.buoyant.linkerd.Main$.main(Main.scala:20)\n    at io.buoyant.linkerd.Main.main(Main.scala)\nException thrown in main on startup\nSupport for the OpenJ9 would need to be added to twitter-util to have this work.  Or, at the very least, Finagle should not crash on an unknown jvm and should instead just be missing jvm stats.. Good question!  To test this I ran Linkerd and Namerd locally with the thrift namer interface.  Using Linkerd's delegator interface triggered this issue.  Having Linkerd resolve a request and then shutting down Linkerd also reproduced it.. Good point, @dadjeibaah.  Specifically, I think this section: https://github.com/linkerd/linkerd/blob/master/linkerd/docs/routers.md#static-client-config. It turns out the above failure was due to a badly behaved test.  I've now fixed the test.  @hawkw . I'm glad we got to the bottom of this!  I propose that we use #1910 to track fixing the documentation but that we also keep this issue open to track changing the behavior of the shutdown endpoint so that it respects the default shutdownGraceMs setting from the config.\nThis should hopefully be straightforward to implement by overriding defaultCloseGracePeriod in linkerd/main and namerd/main.. Fixed by #1951. These new settings look great!  I'm gonna run a few quick tests with these for my own sanity, but I don't anticipate any problems.. Thanks, @chrisgoffinet!  Everything looks good to me!. Hi @nikolay-pshenichny, thanks for your patience as I have investigated this.  It turns out there's a few things going on at multiple layers.  I'll attempt to write up my complete understanding of how this works.\nLinkerd caches \"clients\" which each represent a load balancer over a set of replicas.  These are cached until they have been idle for idleTtlSecs (10 minutes by default).  After this idle TTL has passed, the client will be removed from the cache and all of the connections in that load balancer will be closed.  Furthermore, the last entry in this cache will never be closed.  This means that there will always be one client that keeps its connections open, even if it has been idle for a long time.\nWithin each client, there is a load balancer which maintains a connection to each replica of the service.  In the case of HTTP/1.1 there's actually a pool of connections for each replica and these connections are established as needed and if a large number are established, those excess connections are closed once they have been idle for idleTimeMs.  For HTTP/2 it's a different story because HTTP/2 only requires one connection to each replica and therefore doesn't do connection pooling.  For HTTP/2 the singleton connection to each replica is never expired.\n@nikolay-pshenichny does this explanation match with what you've seen?  My recommendation for a fix would be to change the singleton connection pool that is used by HTTP/2 to close the singleton connection after it has been idle for idleTimeMs.  What do you think?. Great find!  ExpiringService sounds like a perfect fit.  I would be more than happy to take a look at such a PR.. Yes, overall direction looks great here!. My goal is to cut a release candidate this week, get some folks to test it, and hopefully do a full release sometime next week.  If you're able to help us test the RC once it's ready, that would be super helpful!. We will have a release candidate available very soon: early this week.. The release candidate is now available: https://github.com/linkerd/linkerd/releases/tag/1.4.0-rc1\nPlease try it out and let us know how it goes.. the artifacts have been pushed to maven central and hopefully should show up soon.. I've done a couple quick spot tests with client and server TLS as well as a longer running load test and everything looks great!. I was thinking that the easiest way to do this would be to simply create a second endpoint called metrics-summary.json or something like that which prints out only certain metrics from the metrics tree.. Fixed by #1909 . Yes, that's correct.. Looks like it was an issue with the CI system itself.  I've restarted the CI build.  Sorry about that!. This looks like a good fix for Namerd, though I think the issue still affects Linkerd.. Ah, nevermind, I see that the original issue refers to Namerd.  This looks great to me!. @nikolay-pshenichny Looks like there's a merge conflict with the latest master.  Would you mind resolving that and then we should be good to merge.. :star: Thank you so much for tracking this down and fixing it, @nikolay-pshenichny!. Having Linkerd always upgrade messages to HTTP/1.1 was a deliberate decision.  @olix0r probably has the most context on that decision and if it still makes sense.. Thank you for the detailed report and repro steps!!!  We will investigate.. I've done a little bit of digging and it's starting to look like this may be a bug in H2/gRPC where we're missing a mechanism to reset a response stream that we no longer need.  Since the mesh interface's response streams are effectively infinite, this means that we continue to build up more and more streams over time.\nI will continue to investigate and keep this thread updated.. @edio please try out #2051 and let me know if it resolves this issue for you. @dadjeibaah can you give a bit more detail about what tests were done?. Wireshark may be useful to examine the difference between the request that Linkerd makes (which the gateway responds to with a 403) and the request that curl makes (which the gateway responds to with a 200).  . oh yeah, of course.  What about from the gateway logs?  Basically determining why the gateway is returning a 403 is an application level issue.. I'm not sure we can help you very much with the 403s.  You'll need to understand your gateway's behavior in order to determine why it is returning those.  \nHowever, the SNI issue is a real shortcoming of the interface between Linkerd and Namerd where server name information is not passed along from Namerd to Linkerd.  This causes the TLS handshake to fail in cases where Linkerd needs to provide the server name but it had used Namerd to resolve the address.. I've renamed this issue to track the server name issue.. These test changes look good to me!. @leozc does @edio's comment give you what you need?. @leozc should we close this now that params are being passed to Announcer plugins?  Or are there other types of plugins that need this change as well?. @leozc whichever is easier for you is fine with me.  Looking forward to your PR.. @hynek Yikes, this is my top priority issue.  If you can provide any of the following information, it will greatly help tracking this down:\n\nWhat protocol? http or h2?\nCan you provide your Linkerd config?\nCan you provide your cert PEM file (without private key)?\nIf not, can you provide instructions for how to generate a cert with the same structure?\n\nIf you'd rather share any of the above privately instead of publicly, you can private message me on the Linkerd slack or email me alex@buoyant.io. I've been attempting to reproduce this but so far I can't even get it working on 1.3.7.  To my knowledge, we've never tested this particular configuration where the server serves a certificate chain of more than one cert.  When I try this, I run into null pointer exceptions in Netty's SSL code --- though it's possible I have misformatted or misconfigured my certs.\nThe workflow that we usually test for using intermediate CAs is that we have the server serve only the leaf cert and we configure the client with a CA chain that includes the intermediate and the root CA.  \nSo, long story short: I've very eager to see how your certs are structured so that I can reproduce this.. @pbagchi thanks for reporting!  Can you reproduce this reliably or does it only happen intermittently?  Do you see this same behavior if you use the io.l5d.mesh interface?. Ok, thanks @pbagchi.  Please keep this issue updated with your findings.. Thanks for the update!  We're currently working on a feature that will give us a lot more insight into these kinds of issues and help us debug better (https://github.com/linkerd/linkerd/pull/1943).  In the meantime, I'd still be very interested to hear if you see these same types of issues with the io.l5d.mesh interpreter.. Linkerd 1.4.1 has been released with the aforementioned diagnostics.  If you run this version you'll be able to visit /interpreter_state/io.l5d.namerd/<namespace>.json on Linkerd's admin port to see the current state of the Namerd watchs.  This should hopefully give us some hints about what is going on.\nhttps://github.com/linkerd/linkerd/releases/tag/1.4.1. Hi @pbagchi, have you been able to upgrade to 1.4.1 yet?. Hmmm... very mysterious.  What would you like to do with this issue?  Should we close it and re-investigate if this crops up again?. Cool.  Keep us in the loop!. Take a look at the data in the bind:<service name> section of the interpreter state.  This will show you the latest data returned from Namerd about that service.  You should ensure that running = true.  You should also check the lastUpdatedAt.  If this value doesn't match the approximate time when you made the dtab change, this indicates that Linkerd didn't get an update from Namerd.  Also take a look at the response/tree and make sure this is the correct (up-to-date) value.. @joeyb if I'm understanding correctly, you'll need to put your intermediate CA certs in the serverCaChainPath.  On the server side it would look like:\ncertPath: {{serverCert}} # <-- file contains ONLY the server cert (no intermediates)\nserverCaCertPath: {{serverChain}} # <-- file contains ONLY the intermediate CA certs\nkeyPath: {{serverKey}}\nand then on the client side:\ntrustCerts:\n- {{serverCa}} # <-- the root CA cert which issued the intermediates above\nDoes that make sense?. @joeyb Yes, unfortunately Finagle has changed the way that certs and cert chains are handled.  I think the simplest solution would be to add a startup script to the Linkerd container that splits your server PEM file into a file for the main cert and a file for the supporting chain.  \nLinkerd could conceivably do this itself in order to preserve backwards compatibility but the interaction with cert reloading may be tricky to get right.  PRs welcome but I suspect that a startup script to put the certs into the structure that Linkerd now expects would be easier.. @peterfich I know this is silly for a docs fix, but our DCO robot will be much happier if you could amend your commit to include the DCO signoff (the -s option in git will do this).. Yes, you can use\ngit commit --amend -s\ngit push -f\nto add the DCO signoff to your commit then force push.. Thank you!. This error Get http://world: proxyconnect tcp: dial tcp :4140: getsockopt: connection refused is from the Hello service trying to talk to its local Linkerd.  In the legacy configuration that you're using, Hello gets the Linkerd address from a hostIP.sh script which works by querying the Kubernetes API.  It appears that maybe this script is failing for some reason.  I recommend exec'ing into the Hello container and running hostIP.sh manually to see what the output is.. Awesome!  Before I review the code, I'll review the output:\n$ curl -s localhost:4140/linkerd -H \"l5d-req-evaluate: 1\" | jq\n{\n  \"identification\": \"/svc/linkerd\",\n  \"selectedAddress\": \"/104.24.112.2:80\",\n  \"addresses\": [\n    \"/104.24.113.2:80\",\n    \"/104.24.112.2:80\"\n  ],\n  \"dtabResolution\": {\n    \"type\": \"alt\",\n    \"path\": \"/svc/linkerd\",\n    \"alt\": [\n      {\n        \"type\": \"delegate\",\n        \"path\": \"/example/linkerd\",\n        \"dentry\": {\n          \"prefix\": \"/svc\",\n          \"dst\": \"/example\"\n        },\n        \"delegate\": {\n          \"type\": \"neg\",\n          \"path\": \"/#/io.l5d.fs/linkerd\",\n          \"dentry\": {\n            \"prefix\": \"/example\",\n            \"dst\": \"/#/io.l5d.fs\"\n          }\n        }\n      },\n      {\n        \"type\": \"delegate\",\n        \"path\": \"/example/default\",\n        \"dentry\": {\n          \"prefix\": \"/svc/*\",\n          \"dst\": \"/example/default\"\n        },\n        \"delegate\": {\n          \"type\": \"leaf\",\n          \"path\": \"/#/io.l5d.fs/default\",\n          \"dentry\": {\n            \"prefix\": \"/example\",\n            \"dst\": \"/#/io.l5d.fs\"\n          },\n          \"bound\": {\n            \"addr\": {\n              \"type\": \"bound\",\n              \"addrs\": [\n                {\n                  \"ip\": \"104.24.113.2\",\n                  \"port\": 80,\n                  \"meta\": {}\n                },\n                {\n                  \"ip\": \"104.24.112.2\",\n                  \"port\": 80,\n                  \"meta\": {}\n                }\n              ],\n              \"meta\": {}\n            },\n            \"id\": \"/#/io.l5d.fs/default\",\n            \"path\": \"/\"\n          }\n        }\n      }\n    ]\n  }\n}\nI think it would be awesome to have both human readable and json outputs.  What do you think about human readable by default but json if the Accept: application/json header is passed?  \nWe should also add the client name (/#/io.l5d.fs/default) as a top level field probably, rather than it just being in the delegate tree.\nDelegate trees, especially in json format, are pretty confusing.  I wonder if we can distill this into something more easily consumed.  Perhaps by walking the path from the root of the delegate tree to the bound leaf we can output just the delegation steps that lead to it.  e.g.:\n/svc/linkerd\n/example/default     (/svc/* => /example/default)\n/#/io.l5d.fs/default (/example => /#/io.l5d.fs)\nAnd/or we can print a list of rejected alternatives:\n/#/io.l5d.fs/linkerd. I agree that human readable is more important.  But if JSON is easy to add, we might as well.. I haven't looked at the code yet but I noticed a few issues with the output so:\n```\n$ curl localhost:4140 -H \"Host: default\" -H \"l5d-req-evaluate: true\"\nidentification: /#/io.l5d.fs/default\nselectedAddress: /127.0.0.1:9990\naddresses: /127.0.0.1:9990,/127.0.0.2:9990,/127.0.0.3:9990\nDtab Resolution:\n/svc/default\n1.0 * /x/default\n/a/default\n/#/io.l5d.fs/default\n    \u23ce\n```\nIt looks like the identification line is actually printing the client name instead of the service name.  I think this should be: /svc/default as the service name and /#/io.l5d.fs/default as the client name.\nIt looks like there's some leading a trailing whitespace.  The formatting of the addresses list could use some encasing characters and spacing the make it more readable.  Consider formatting like [127.0.0.1:9990, 127.0.0.2:9990, 127.0.0.3:9990] or something similar.  The dtab resolution could also use some indenting or something to distinguish the resolution output from the field names.. ```\n$ curl localhost:4140 -H \"Host: default\" -H \"l5d-req-evaluate: true\"\nidentification: /#/io.l5d.fs/default\nselectedAddress: /127.0.0.1:4140\naddresses: /127.0.0.1:4140\nDtab Resolution:\n/svc/default\n1.0 * /x/default\n/a/default\n/#/io.l5d.fs/default\n    \u23ce\n```\nWhen I use a dtab with a union, the output appears to be incorrect.  The above comes from this dtab:\n/b/* => /$/inet/127.1/4140;\n/a => /#/io.l5d.fs;\n/x => /a & /b;\n/svc => /x;\nThe /b path of the union has been chosen (thus the final address of 127.1:4140) but the delegation and client name still show the /a path being chosen which leads to /#/io.l5d.fs/default.. Thanks for bringing this up, @olix0r.  My concern about using TRACE for this is that it means that Linkerd can no longer proxy TRACE requests.  What I like about using an l5d-* header is that I think it's fair for us to assume Linkerd owns this part of the request space whereas it's more dubious for Linkerd to take ownership of all TRACE requests.\nWhat if Linkerd returned cache control headers in the evaluate response to prevent caching by intermediaries?  Would this take care of the caching concerns?  Are there other concerns to worry about here?. I like the idea of a custom Linkerd verb.  I just checked and it looks like Linkerd has no problem proxying custom verbs.  One limitation of this approach is that it doesn't work when you want to evaluate a request and you're using an identifier that keys off of the method such as the io.l5d.methodAndHost identifier.\nWe could return this information in a header or other side channel but I like the idea that this doesn't actually proxy the request.  It acts as a sort of \"dry-run\" mode.  \"Don't proxy this request, just tell me about it.\"\nA custom verb seems like the best compromise to me.  It's still useful in most cases but without any egregious spec violations (as far as I know \ud83d\ude2c).. I see this as a feature that has two main use-cases: when getting started with Linkerd and when debugging routing issues.\nWhen getting started with Linkerd, there is a probably a tight feedback loop where the user makes frequent changes to the Linkerd config/dtab, restarts Linkerd, and experiments.  In this case, the user is probably logged into the machine where Linkerd is running and may be testing requests by using curl to localhost.  This feature is useful here because it closes the gap between the request itself and the delegator UI.  \nWhen debugging routing issues, Linkerd is probably already set up and running, potentially serving production traffic.  Changing Linkerd config is costly because it requires a restart.  In this case, the user wants to get more information about how Linkerd would route a request in order to verify that routing is happening as expected or to diagnose why it is not.  In this case the test request may come from curl or from another application and it my pass through intermediaries such as proxies or load balancers before getting to Linkerd.. This is so cool!  Some quick notes on the updated output:\n```\n$ curl localhost:4140 -X TRACE -H \"Host: foo\" -H \"l5d-max-depth: 2\"\ndog\n--- Router: incoming ---\nservice name: /svc/foo\nclient name: /%/io.l5d.localhost/#/io.l5d.fs/foo\nselected address: 127.0.0.1:8888\naddresses: [127.0.0.1:8888]\ndtab resolution:\n  /svc/foo\n  /\n  /%/io.l5d.localhost/#/io.l5d.fs/foo\n--- Router: outgoing ---\nservice name: /svc/foo\nclient name: /%/io.l5d.port/4141/#/io.l5d.fs/foo\nselected address: 127.0.0.1:4141\naddresses: [127.0.0.1:4141, 127.0.0.2:4141, 127.0.0.3:4141]\ndtab resolution:\n  /svc/foo\n  /\n  /%/io.l5d.port/4141/#/io.l5d.fs/foo\u23ce\n```\n\nI would put addresses before selected address so that the order of service -> client -> addresses -> selected goes from more general to more specific\nWhat do you think of adding a timestamp field?  This gives some timing data about each hop\nLooks like the dtab resolution output is a bit broken with transformers\nConsider printing the dentry alongside each step in the delegation (like the dtab UI does)\nMissing newline at the end of the output. At the very least, having a trailing newline makes the output look nicer on a shell:\n\nbash-3.2$ curl localhost:4140 -X TRACE -H \"Host: foo\" -H \"l5d-max-depth: 0\"\n--- Router: outgoing ---\nservice name: /svc/foo\nclient name: /%/io.l5d.port/4141/#/io.l5d.fs/foo\nselected address: 127.0.0.1:4141\naddresses: [127.0.0.1:4141, 127.0.0.2:4141, 127.0.0.3:4141]\ndtab resolution:\n  /svc/foo\n  /\n  /%/io.l5d.port/4141/#/io.l5d.fs/foobash-3.2$. This is looking really awesome\n```\n$ curl -X TRACE localhost:4140 -H \"Host: default\" -H \"l5d-max-depth: 10\"\ndog\n--- Router: http ---\nservice name: /svc/default\nclient name: /#/io.l5d.fs/default\naddresses: [127.0.0.1:8888]\nselected address: 127.0.0.1:8888\ndtab resolution:\n  /svc/default (/\\x2f=>~)\n  /b/default (/svc=>2.00*/a & /b)\n  /bb/default (/b=>/bb)\n  /#/io.l5d.fs/default (/bb=>/#/io.l5d.fs)\n```\nA few quick nits on the output:\n Timestamps would be awesome\n Looks like the first line in the delegation is printing the Dentry.nop which looks weird\n* Probably don't need 2 blank lines between blocks. @olix0r The response here may be generated by either the final server (if max-forwards is sufficiently large) or by Linkerd (if max-forwards reaches 0).  In the case where Linkerd generates the response, it would make sense for us to set Content-Type: text/plain.\nIn the case where the final server generates the response, we should probably just leave the response headers alone.  The wrinkle being that by appending router context to the response body, we may be violating the content-type that the server sent.  I'm not sure what to do about that.. @dadjeibaah maybe!  If you notice a flakey test, please file an issue.. Hi @dvulpe!  The CI failures here are due to a memory issue in the Linkerd tests that has just been fixed on master.  If you merge latest master the issue should be resolved.  Sorry about that!. Thanks, @JustinVenus!  This is great feedback.. I think deprecated annotations are intended for library fields to signal to library users that those library fields are deprecated.  Given that Linkerd isn't a library, I'm not sure that it makes sense to use the scala deprecated annotation.\nWe should, however, warn in the logs when users use a config property that is deprecated.. Actually it looks like put the DelayedReleaseModule in 2 different places: replacing the prepFactory role in the path stack and replacing the prepConn role in the client stack.  I actually... have no idea why we put it in both.  Here's the PR where it got added: https://github.com/linkerd/linkerd/pull/1444/files\nI can't really think of a reason why it needs to be in both stacks but I'm nervous about changing something like that without a lot of testing.. I do see the node_modules when building locally:\n$ tar tf .ivy2/local/io.buoyant/admin_2.12/1.4.0-SNAPSHOT/jars/admin_2.12.jar | grep -i node_modules | wc -l\n    9796. It looks like the the node_modules directory will get included in the jar if it happens to be present in the source tree when the jar is built.  So the fact that it started showing up in 1.3.7 is just due to whoever built that release and what was on their computer at the time.  This change seems like a great way to guard against that.. Thanks!. @joeyb thanks for filing this and for the detailed repro steps!  Do you know if client auth is required to trigger this?  Does the client still fail to validate the server cert chain if it does not use client auth?. Based on that and the fact that your client cert file also contains an intermediate, it sounds like maybe it's the client that's not presenting its certificate chain properly.  A quick glance at the code makes me think this is plausible.\nI will investigate further.. I think this issue is essentially the same as #1926 except for the client certificates instead of the server certificates.  I've tested https://github.com/linkerd/linkerd/pull/1960 against the supplied repro and it appears to fix the issue.  @joeyb, can you confirm?. Yes, I believe it should work.  The client should use the trustCerts to validate the server chain.  clientCaChainPath is just used for the client to present its own certificate chain.. @yoitsro it's interesting that disk usage keeps growing like that.  Do you know where that disk usage is coming from?  Is there a particular file or directory that is growing?. Thanks, @yoitsro.  It seems like disk usage is correlated with the issue so I think that's the easiest place to start.. @yoitsro any luck finding out where that disk usage is going?. That error message is harmless and can be safely ignored.  It has been removed from the logs in Linkerd 1.4.0 (https://github.com/linkerd/linkerd/pull/1901).  . @yoitsro can you use unix tools such as du to determine where the disk usage is going?. Standard out, by default. @yoitsro sorry about the red hearing!\nThere are a few things to investigate next.\n Were you able to get the logs from the kubectl proxy pod as @dadjeibaah suggested?\n Can you tell me a bit more about what happens when Linkerd stops routing gRPC requests?  When you issue a request to Linkerd does it return an error response?  or hang?  or timeout?  can you supply some sample output from issuing some requests?\n* Can you provide the output of /admin/metrics.json from a Linkerd that has stopped routing gRPC requests?. I see a lot of http: proxy error: dial tcp 10.11.240.1:443: getsockopt: connection refused in your kubectl proxy logs.  Is that the address of your Kubernetes API server?  If kubectl is not able to connect to the Kubernetes API then Linkerd will not be able to get up-to-date information about the location of pods.. Do the connection errors to the Kubernetes API correlate with times where Linkerd failed to route requests?. It's hard to debug this because it's difficult to differentiate errors in Linkerd from errors in the application.  From your iOS logs we see that a client received a stream reset with code 2 (internal error).  We also see from the metrics that Linkerd sent a stream reset (internal error) on port 4140 but there are no client or service metrics to indicate where this error came from.  This is possibly because that service became idle and its metrics expired.\nIn order to debug this effectively, I think you need to isolate the problem a bit more.  If you look at logs (Linkerd, Kubectl, client application, and server application) immediately after the issue occurred, something might jump out at you.  You can also look at Linkerd's metrics immediately after the issue occurred and look for reset stats which are non-zero.  This should tell you where the reset is coming from.  If all else fails, you can look at tcpdumps of the H2 traffic and see if Linkerd is sending or receiving resets.\nI hope this helps!. com.twitter.finagle.RequestTimeoutException: exceeded 10.seconds to 0.0.0.0/4142 while dyn binding /svc/internal.Shoals\nThis error indicates that Linkerd was not able to resolve the service.  This could be because of the connectivity problem between kubectl proxy and the Kubernetes API.. Thanks, @briansmith, this is very helpful.  I do like the idea of extracting intermediates from the cert file but this is difficult to do with Finagle's existing API which expects a file for the cert and a file for the chain.  If we do pre-processing on the cert files and pass new files to Finagle, any changes to the original cert files would not get picked up by Finagle.  We would need to also watch the original files and update the files that we created on any change.  Doable, but potentially more complex and error prone.. See: https://circleci.com/gh/linkerd/linkerd/7461. @leozc Please take a look at https://github.com/linkerd/linkerd/compare/master...alex/params and let me know if that fixes it for you. Thanks for reporting!  I think I see how this artifact got missed... I'll get it uploaded asap.. I've released the missing artifacts and they should show up on maven within a few hours.  Thanks again for the prompt report!. Thanks @utrack!  Are there any errors messages in the Namerd logs when a delete fails like this?. I haven't been able to reproduce this.  Based on the output you pasted from the GET, it looks like the dtab contains many null dentries which is probably what is causing issues.  How did you create this dtab?  Did you POST to /api/1/dtabs/qwe?  Or edit the dtab resource directly?. @utrack can you give us steps to reproduce that include creating the dtab?  I am able to create and delete dtabs using the API and have not yet been able to reproduce this issue.. Now that we're tracking the null dentry issue in #1974, can we close this one? @utrack . Any update here, @utrack?. @JustinVenus and @bkreitch: Would you mind reviewing this?. This error message $/io.buoyant.rinet/11102/30-disableTLS.redact: name resolution is negative indicates that Linkerd could not resolve the DNS name 30-disableTLS.redact.  How did you independently test this for availability?  Are you able to resolve this DNS name from within the Linkerd pod?. Well now that is strange.  As far as I know, Linkerd uses the JVM's built-in DNS resolver.  I wonder if this is some kind of DNS caching issue in the JVM.  Do you know if these failures are corellated with changes to the DNS record?. Using some of the search options with the command line utility dig might help uncover if anything funny is going on with DNS resolution that the JVM is missing.. Any luck tracking this down?  Any signs pointing to whether this is a Linkerd issue or a DNS issue?. I'm not sure the heapdumps will be of much use since this appears to be an issue with DNS resolution in the JVM.. @betson-nuovo any update from your colleague who has been investigating the DNS issue?  On our side there's not much we can do to investigate further.. @zillani-nuovo are you still seeing this?  Should we close this issue?. @zillani-nuovo I believe that investigation of this issue has moved to email.  Do you mind if we close this issue on Github?. Updated.. I'd definitely appreciate suggestions for what the best status code is here.  502 seems weird to me but I can see the case for backwards compatibility.  I initially choose 400 because that's what we return for UnknownDst errors, but that may not be right either.  Inability to route is almost like a 404... but that also seems wrong....\nPlease help. \ud83d\ude27 . Thanks for the feedback!  I've reverted the status code back to 502.. I think this was caused by https://github.com/linkerd/linkerd/pull/1904\nWhat seems to be happening is that the HTTP/1.0 server is closing the connection to signal the end of the response (as opposed to using Content-Length or Transfer-Encoding).  However, Linkerd will set Connection: keep-alive on the response and keep the connection to the client (curl) alive.  Therefore, the absence of Content-Length, Transfer-Encoding, or connection termination will mean that curl cannot detect that the message has completed.. This could be done as a Request Authorizer plugin.. Given that the exact logging needs will vary quite a bit from user to user, I think this probably makes more sense as a 3rd party plugin rather than something built into Linkerd directly.  @mikebz does that make sense to you?. I'm going to close this issue since I don't think there's anything to be done here for Linkerd itself.  If you do put together a logging plugin we'd be more than happy to add it to the list of external plugins: https://github.com/linkerd/linkerd/blob/master/PLUGINS.md. Thank you, @dvulpe!  Removing our dependency on temp files will be so awesome.  I'm really looking forward to reviewing this.. @Pveasey can you share exactly how you interrupted Zookeeper?  I tried stopped ZK by running zkServer.sh stop which resulted in Namerd printing this error approximately once per second:\nWARN 0615 16:38:27.251 PDT run-main-0-SendThread(localhost:2181): Session 0x16405c7b50d0002 for server localhost/127.0.0.1:2181, unexpected error, closing socket connection and attempting reconnect\njava.net.ConnectException: Connection refused\n    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n    at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:356)\n    at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1192)\nI was not able to get Namerd into a state where it was printing thousands or millions of logs.. @Pveasey for what it's worth, I think you're right.  Looking at the code it's totally believable that certain zookeeper responses could put Namerd into a tight retry loop.  But without the ability to repro or at least without a better understanding of what those ZK responses are, it'll be tough to fix this with any amount of confidence.  I'll close this for now but please re-open it if you get more information.. Hi @mrezaei00, what is the structure of the wildcard-proxycert file?  If it contains more than one cert (ie a cert chain) then you'll need to use the intermediateCertsPath option which was added in Linkerd 1.4.1 (https://github.com/linkerd/linkerd/releases/tag/1.4.1).. Thanks for the info @mrezaei00.  In that case, no, intermediateCertsPath should not apply.  Let me see if I can reproduce this issue.. I wasn't able to reproduce this.  I used a slightly simplified setup, taking Namerd out of the equation and using self-signed certs.  With this setup, requests were routed correctly and Linkerd-Linkerd requests were encrypted as I'd expect.  Here's the Linkerd config I used:\n```\nnamers: []\nrouters:\n- protocol: http\n  label: out-http\n  identifier:\n    kind: io.l5d.path\n    segments: 2\n    consume: false\n  dtab: /svc/* => /$/inet/localhost/4141\n  servers:\n  - port: 4140\n  client:\n    tls:\n      commonName: \"foobar.domain.inc.com\"\n      trustCerts:\n      - certificates/cacertificate.pem\n\nprotocol: http\n  label: in-http\n  identifier:\n    kind: io.l5d.path\n    segments: 2\n    consume: true\n  servers:\nport: 4141\n    tls:\n      certPath: certificates/servercert.pem\n      keyPath: certificates/private/serverkey.pk8\n  dtab: /svc/* => /$/inet/localhost/8888\n```\n\nand here is the server cert:\nCertificate:\n    Data:\n        Version: 1 (0x0)\n        Serial Number: 4100 (0x1004)\n    Signature Algorithm: sha256WithRSAEncryption\n        Issuer: C=US, CN=My CA\n        Validity\n            Not Before: Jun 15 23:55:17 2018 GMT\n            Not After : May 22 23:55:17 2118 GMT\n        Subject: CN=*.domain.inc.com, C=US\n        Subject Public Key Info:\n            Public Key Algorithm: rsaEncryption\n                Public-Key: (2048 bit)\n                Modulus:\n                    00:d2:ba:9a:00:5f:c7:5f:f1:25:94:e3:28:16:81:\n                    20:fd:3d:a6:f2:0a:62:38:c0:6c:dc:45:ce:22:1e:\n                    3a:7d:3e:f0:d8:a8:68:b6:90:40:8b:c5:11:26:f3:\n                    6a:41:0a:5d:be:ff:63:fc:7e:f4:b6:4a:b3:72:76:\n                    06:5a:8c:8c:01:1a:29:61:f1:48:1a:7c:a3:ca:01:\n                    43:e2:f6:5b:66:aa:4a:4b:f7:26:56:0a:6f:a6:ac:\n                    f8:50:47:13:49:c4:4c:3f:ee:c2:92:e1:bf:32:b9:\n                    d8:2b:70:11:f1:1e:ad:90:ba:1a:49:4d:c5:36:ca:\n                    13:8e:94:25:20:90:a4:17:58:31:84:7a:9d:dd:95:\n                    39:b9:5f:b1:bf:30:92:05:09:7d:da:a9:36:93:95:\n                    4b:43:70:19:a5:1a:96:2f:4d:ee:cd:57:af:b7:b5:\n                    89:f6:80:67:e6:4c:6d:7f:90:7a:4b:3a:e0:87:31:\n                    16:0e:46:55:5a:d5:28:a6:b4:e1:5b:b0:3a:5d:d8:\n                    50:6c:e4:bb:3c:77:76:9d:d6:41:74:10:b6:0d:7d:\n                    50:aa:10:e5:0c:75:4d:d7:0d:6f:4c:9d:9c:51:c7:\n                    28:26:e0:10:a8:4b:18:b2:2a:5f:ac:9a:5a:68:2a:\n                    4e:fb:a2:03:10:00:2a:86:3b:39:3e:65:5b:0a:33:\n                    72:c7\n                Exponent: 65537 (0x10001)\n    Signature Algorithm: sha256WithRSAEncryption\n         4d:f3:32:dd:b5:33:42:5f:8a:e7:16:60:19:74:44:e0:cf:34:\n         f4:ce:74:b7:cf:30:f0:31:55:97:e3:f2:52:ad:b0:7a:c2:96:\n         e3:31:73:96:3f:07:9e:b4:10:89:38:3d:cd:16:e2:d4:98:3b:\n         c3:b3:4d:e5:74:02:5a:ba:55:28:37:9b:da:33:74:b1:84:90:\n         ff:be:8c:fc:2a:d9:e0:f1:37:56:b8:11:be:09:cf:18:b7:a7:\n         a4:49:7b:4a:0d:a0:f6:af:45:6e:0f:f6:ab:ec:ef:b5:ab:b8:\n         65:b5:08:79:ae:fb:3b:f2:da:e8:76:07:a6:b1:8b:98:f3:f6:\n         c4:d8:72:65:43:5b:b3:78:8c:83:84:74:69:bd:79:40:95:f8:\n         40:f8:c1:02:86:26:c3:5d:d5:51:6e:8c:16:cf:32:3b:61:b3:\n         ab:64:a3:9a:18:eb:33:27:c8:e1:67:eb:21:05:3c:9d:00:55:\n         f6:72:82:1c:89:75:76:58:27:9e:8f:78:18:88:fb:3c:7b:7a:\n         f9:be:d9:f6:e8:16:80:a9:f7:98:52:90:17:0f:8c:e1:93:10:\n         c4:80:c9:cc:2e:bd:af:1a:20:06:57:07:85:b1:3c:8f:6b:3d:\n         32:cd:89:3c:aa:8a:21:aa:ef:21:c2:76:4c:30:6f:19:49:d8:\n         84:1b:fc:53\n-----BEGIN CERTIFICATE-----\nMIICvDCCAaQCAhAEMA0GCSqGSIb3DQEBCwUAMB0xCzAJBgNVBAYTAlVTMQ4wDAYD\nVQQDDAVNeSBDQTAgFw0xODA2MTUyMzU1MTdaGA8yMTE4MDUyMjIzNTUxN1owKDEZ\nMBcGA1UEAwwQKi5kb21haW4uaW5jLmNvbTELMAkGA1UEBhMCVVMwggEiMA0GCSqG\nSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDSupoAX8df8SWU4ygWgSD9PabyCmI4wGzc\nRc4iHjp9PvDYqGi2kECLxREm82pBCl2+/2P8fvS2SrNydgZajIwBGilh8UgafKPK\nAUPi9ltmqkpL9yZWCm+mrPhQRxNJxEw/7sKS4b8yudgrcBHxHq2QuhpJTcU2yhOO\nlCUgkKQXWDGEep3dlTm5X7G/MJIFCX3aqTaTlUtDcBmlGpYvTe7NV6+3tYn2gGfm\nTG1/kHpLOuCHMRYORlVa1SimtOFbsDpd2FBs5Ls8d3ad1kF0ELYNfVCqEOUMdU3X\nDW9MnZxRxygm4BCoSxiyKl+smlpoKk77ogMQACqGOzk+ZVsKM3LHAgMBAAEwDQYJ\nKoZIhvcNAQELBQADggEBAE3zMt21M0JfiucWYBl0RODPNPTOdLfPMPAxVZfj8lKt\nsHrCluMxc5Y/B560EIk4Pc0W4tSYO8OzTeV0Alq6VSg3m9ozdLGEkP++jPwq2eDx\nN1a4Eb4Jzxi3p6RJe0oNoPavRW4P9qvs77WruGW1CHmu+zvy2uh2B6axi5jz9sTY\ncmVDW7N4jIOEdGm9eUCV+ED4wQKGJsNd1VFujBbPMjths6tko5oY6zMnyOFn6yEF\nPJ0AVfZyghyJdXZYJ56PeBiI+zx7evm+2fboFoCp95hSkBcPjOGTEMSAycwuva8a\nIAZXB4WxPI9rPTLNiTyqiiGq7yHCdkwwbxlJ2IQb/FM=\n-----END CERTIFICATE-----\nDo your certificates differ from the one I used significantly in structure?  Can you use openssl s_client -connect localhost:4141 to get more information about the certificates that Linkerd is serving?. @mrezaei00 I'm looking at the HTTP client config snippet you pasted in the description:\nservers:\n      - port: 4141\n        ip: 0.0.0.0\n        tls:\n          certPath: /certs/wildcard-proxycert\n          keyPath: /certs/wildcard-proxykey\nthis looks like it's actually configuring TLS on the server.  Is this just a typo in the description and the server and client snippets are switched?. I think I have managed to reproduce this using non-self signed certs.  I will investigate further.. I believe this may be related to https://github.com/twitter/finagle/issues/699. @mrezaei00 I think you just have a small typo in your description where you mixed up the server config and the client config which briefly confused me.  I will hopefully know more about the Finagle issue and how it relates to Linkerd after I speak with someone from the Finagle team.. @mrezaei00 are there any errors in the outgoing Linkerd's logs when this error occurs?  \nAn error log including:\nCaused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\nperhaps?. Another thing to double check is that the DigiCert High Assurance EV Root CA is in your JVM's trust store:\n$ keytool -list -rfc -storepass changeit -keystore $JAVA_HOME/jre/lib/security/cacerts  | grep -i DigiCert\nAlias name: digicertassuredidrootca\nAlias name: digicerthighassuranceevrootca\nAlias name: digicertglobalrootca. @mrezaei00 to be honest, I'm pretty stumped.  That error message means that the client Linkerd was not able to validate the server's certificate.  In particular, it wasn't able to find a certificate chain back to a trusted root.  But, based on the keytool command you ran above, it seems like DigiCert High Assurance EV Root CA is a trusted root.\nI'm not able to reproduce this, even when using Linkerd to make requests to servers that use that same root cert such as wepay.com.  For example with this Linkerd config:\n```\nnamers: []\nrouters:\n- protocol: http\n  dtab: /svc/* => /$/inet/wepay.com/443\n  servers:\n  - port: 4140\n    ip: 0.0.0.0\n  client:\n    tls:\n      commonName: wepay.com\n```\nI am able to curl -v localhost:4140 and have the request successfully go through to wepay.com which also has a cert issued by DigiCert High Assurance EV Root CA.. @mrezaei00 Are you still seeing this issue?  Do you have a self-contained reproduction that we would be able to run?. This looks like https://github.com/linkerd/linkerd/issues/1825\nThis has been fixed in Netty but we can't upgrade to pick up the fix until Finagle does.. I'm not aware of any workarounds, but let me see what I can find.  What is the impact of this for you folks?. I wasn't able to come up with any reasonable workarounds.  The original Netty issue (https://github.com/netty/netty/issues/7353) isn't much help either, as it says that the issue \"Happens randomly\".\nI think this means that we're pretty much stuck waiting for Finagle to pick up the fix from Netty.. This comment: https://github.com/twitter/finagle/pull/678#issuecomment-405775436 suggests that the Netty upgrade may happen in the next Finagle release.. That's awesome to hear!  What are the next steps here?. Thanks for this report!\nLinkerd keeps a cache of services that it talks to so creating a large number of services can definitely have a big impact on performance.  The old cached services should be cleaned up after being idle for 10 minutes (this ttl is configurable) so I would expect that performance should return to normal after 10 minutes.\nIt's interesting that the prometheus metrics endpoint becomes slow while the other ones do not.  This could point to some inefficiency that the prometheus metrics endpoint has when dealing with a large number of metrics.  If you can share the output from /admin/metrics/prometheus and from /admin/metrics.json when Linkerd is in this state, that will help us investigate.. Aha, it looks like the discrepancy between the json and prometheus metrics is that the Prometheus metrics will print empty histograms (ie when the count is 0) whereas the json metrics will skip them entirely.  Updating the prometheus metrics endpoint to not print empty histograms should bring it back in line with the json metrics.. @milesbxf are you able to evaluate if https://github.com/linkerd/linkerd/pull/2006 improves the performance of the prometheus endpoint in this scenario?. @milesbxf just wanted to check in with you and see if either of the above changes has helped the situation for you folks at all.  What would you like the next steps for this issue to be?. @milesbxf any thoughts here?  . Closing due to inactivity.  Please feel free to reopen if you're still seeing issues.. Somewhat related functionality (upgrading responses to HTTP/1.1) used to be in the ViaHeaderAppenderFilter.  However, I think it's actually more clear to separate functionality out into separate filters rather than jamming it all together in one.. Fixed by #1994 . Thanks, @mstoskus!!!. This is very cool.  We actually already do this in Conduit (soon to become Linkerd 2.0).  Adding this functionality to Linkerd, unfortunately, would be a large amount of work.. Thanks, @chrisgoffinet!  I will reproduce this locally and start investigating.  Serving a large number of metrics means a lot of string allocation... do you have evidence for or against the latency being GC related?. Can we close this now that https://github.com/linkerd/linkerd/pull/2006 has merged?. @mrezaei00 This is an interesting idea.  The delegator.json endpoint is intended to return the delegation tree for a particular path and dtab.  My interpretation of the HTTP status code 200 to mean that the delegation tree was successfully able to be returned (whereas, for example, 404 means the delegation tree was not found and 500 means there was an error producing the delegation tree).  \nSo I'm not sure if it makes sense to have an HTTP status code that describes the content of the delegation tree (ie whether the tree is neg or bound or fail).\nHere is the logic that we use to color the boxes in the delegator UI: https://github.com/linkerd/linkerd/blob/master/admin/src/main/resources/io/buoyant/admin/js/src/delegator.js\nIt seems reasonable to move that logic onto the server side and have the status of each node returned as part of the json structure.  Another possibility would be to encode the status (bound/neg/fail) of the tree in a response header.  \nWhat do you think?. I don't think 0.0.1.187 is a valid IP address.  What DNS server are you using?. Closing due to inactivity.  @utrack please re-open if you have are still having this issue and can give us more information about your DNS setup.. I've tested this locally with these steps:\n\nRun consul: consul agent -dev\nRun Linkerd: ./sbt linkerd-examples/consul:run\nIssue request: curl localhost:4140 -H \"Host: consul\"\nCheck state endpoint: open http://localhost:9990/namer_state/io.l5d.consul.json\n\nI get this serialization error:\ncom.fasterxml.jackson.databind.JsonMappingException: No serializer found for class com.twitter.collection.RecordSchema$Record and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: scala.collection.convert.Wrappers$MapWrapper[\"/dc1/consul\"]->scala.collection.convert.Wrappers$MapWrapper[\"poll\"]->io.buoyant.namer.consul.PollState[\"request\"]->com.twitter.finagle.http.Request$Impl[\"response\"]->com.twitter.finagle.http.Response$Impl[\"ctx\"])\nI think this is because PollState contains a Request member which doesn't serialize nicely.  Instead, PollState should probably hold a String member which contains a String representation of the request's method and uri.. Here is the output I got when testing locally:\n{\n  \"/dc1/consul\": {\n    \"state\": {\n      \"running\": true,\n      \"lastStartedAt\": \"2018-06-26 22:23:25 +0000\",\n      \"lastUpdatedAt\": \"2018-06-26 22:23:25 +0000\",\n      \"value\": {\n        \"value\": {\n          \"addr\": {\n            \"changes\": {}\n          },\n          \"id\": \"/#/io.l5d.consul/dc1/consul\",\n          \"path\": \"/\"\n        }\n      }\n    },\n    \"poll\": {\n      \"request\": {\n        \"method\": \"GET\",\n        \"uri\": \"/v1/catalog/service/consul?index=6&dc=dc1\",\n        \"headers\": {\n          \"X-B3-SpanId\": \"42276b1cdd0d006e\",\n          \"Finagle-Ctx-com.twitter.finagle.Retries\": \"0\",\n          \"Host\": \"localhost:8500\",\n          \"X-B3-TraceId\": \"42276b1cdd0d006e\"\n        }\n      },\n      \"lastRequestAt\": \"2018-06-26 22:23:25 +0000\",\n      \"response\": {\n        \"value\": [\n          {\n            \"Node\": \"Alexs-MBP.localdomain\",\n            \"Address\": \"127.0.0.1\",\n            \"ServiceID\": \"consul\",\n            \"ServiceName\": \"consul\",\n            \"ServiceTags\": [],\n            \"ServiceAddress\": \"\",\n            \"ServicePort\": 8300\n          }\n        ],\n        \"index\": \"6\"\n      },\n      \"lastResponseAt\": \"2018-06-26 22:23:25 +0000\"\n    }\n  }\n}. One thing that I notice is that we're using an InstrumentedActivity[NameTree[Name.Bound]].  The problem with that is that the value type of the InstrumentedActivity (NameTree[Name.Bound]) doesn't serialize nicely because it contains an embedded Var[Addr].  That's why we see:\n\"addr\": {\n            \"changes\": {}\n          },\ninstead of something useful.  Rather than storing and exposing an InstrumentedActivity[NameTree[Name.Bound]] I think we should instead store an expose an InstrumentedVar[Addr].  ie the result of SvcAddr.apply.. I'm also not sure if there's any value in recording the HTTP request headers in the API calls.  There isn't any relevant information stored in these headers and I think they may just be a distraction.  Consider the approach taken in InstramentedWatch.scala where only the method and uri are stored (and they are stored a string rather than as a case class).. @ivanopagano this is looking great!  I think once you change Option[PathScheme] in ConsulPath to not be an option and update the call from map to onSuccess in LookupCache this will be ready to merge.. Thanks for this, @jmalvarezf!  I agree with you that Linkerd is long overdue to play more nicely with other tracing systems like OpenTracing.  Unfortunately, there are a few complications.\nSo far, Linkerd has always used its own l5d-ctx-trace header for trace propagation and just blindly forwarded other headers such as the x-b3 ones.  This means that Linkerd tracing can peacefully co-exist in a system that also does x-b3 style tracing.  The two systems will basically be invisible to each other.  \nWith your change, Linkerd will read the x-b3 headers and prefer them to the l5d-ctx-trace header.  It will also write out both styles of headers.  The problem with this is that it's a backwards incompatible change.  In cases where both headers are present, this changes Linkerd's behavior to use the x-b3 header instead of the l5d-ctx-trace one.\nEven switching the precedence so that Linkerd prefers the l5d-ctx-trace header over the x-b3 headers doesn't solve the problem fully.  In cases where the request is sent through a tracing aware service that increments the x-b3 span header, that span would get lost at the next Linkerd because Linkerd would ignore it in favor of the l5d-ctx-trace header.\nThe complexity compounds even further if you want to support other tracing propogation headers such as jaeger's uber-trace-id or lightstep's ot-tracer- headers.\nThe TLDR is that I think this needs to be something configurable so that users can decide which tracing systems they want Linkerd to participate in and which they want Linkerd to ignore.  Would you be willing to hold off on this PR while I put together a Linkerd trace propagation plugin interface?  This would then allow you to resubmit your change as a trace propagation plugin.\nI hope this context is useful, please let me know what you think!. @jmalvarezf I'll leave it up to you.  Either way we can close this one once the replacement is ready.. Please see https://github.com/linkerd/linkerd/pull/2025 and let me know what you think.. @jmalvarezf the new interface has merged.  Please let me know if you need any help with this new interface!. Nice find, @pbagchi.  Are you interested in submitting a PR to correct this?  . Linkerd does connection caching and reuse.  What this means is that if a server sends a GOAWAY to Linkerd, that connection to the server will get torn down, but any connections from a client to Linkerd will remain intact, as you have described.  In fact, because Linkerd does routing, there isn't even a 1-to-1 mapping of client connections to server connections.\nClients should not need to reconnect to Linkerd if you add a new node.  Clients should be able to continue sending requests using their existing connection to Linkerd and Linkerd will load balance those requests to the servers, creating or reusing connections as necessary.\nLikewise in your second example, if connections from Linkerd to the servers are torn down by a timeout, clients do not need to reconnect to Linkerd.  They can simply continue sending requests and Linkerd will re-establish connections to the servers if necessary.. Hi @Aavon.  It looks like you're running into a DNS resolution issue, possibly from using the inet namer somwhere in your dtab.  \nHave you tried using the consolidated Kubernetes config: https://github.com/linkerd/linkerd-examples/blob/master/k8s-daemonset/k8s/servicemesh.yml\nThis might work better for you.. Now that #2015 has merged, can you merge latest master here?. Hi @flodiebold!  I believe this was fixed in https://github.com/linkerd/linkerd/pull/2005 which should go out with the next Linkerd release.. Ah, I think you're right!  Good catch, thanks!. @leozc was there a problem with returning both IPv4 and IPv6?  I haven't specifically tested IPv6 but, at least theoretically, it should work.. Can you add the DCO signoff?  It also looks like CI got stuck on your PR again but I think I've figured out a way to force it to re-run.  I'll do that once you add the DCO signoff.. @leozc looks like the bot isn't quite happy: Expected \"Leo Liang <leozc@leozc.com>\", but got \"leozc <leozc@leozc.com>\".. Good question, @robertpanzer.  Here's a list of the default admin endpoints:\n/\n/admin/announcer\n/admin/contention\n/admin/files/\n/admin/lint\n/admin/lint.json\n/admin/metrics.json\n/admin/metrics/usage\n/admin/ping\n/admin/pprof/contention\n/admin/pprof/heap\n/admin/pprof/profile\n/admin/registry.json\n/admin/server_info\n/admin/shutdown\n/admin/threads\n/admin/threads.json\n/admin/tracing\n/bound-names.json\n/client_state.json\n/config.json\n/delegator\n/delegator.json\n/favicon.ico\n/files/\n/help\n/logging\n/logging.json\nMost of these endpoints are pretty harmful if an attacker gains access to them.  At the very least, it leaks much data about Linkerd and its internal state.  It's recommended that the admin server not be externally accessible for this reason.\nThat said, /admin/shutdown is particularly dangerous.  Potentially the /admin/pprof endpoints could be considered dangerous too since accessing them will likely cause performance issues.  The logging endpoints also allow the log level to be changed which can also be used to degrade performance.\n@flatmap13 I think you were interested in this feature too.  What do you think?. @robertpanzer seems reasonable but we would want to figure out which endpoints are required for the admin UI.  Most configs that are whitelisting endpoints would want to whitelist everything for the admin UI.. Here's my suggestion based on the use-cases presented here:\nWe divide the admin endpoints into categories:\nUI:\n \"/\"\n \"/files/.\"\n \"/admin/files/.\"\n \"/config.json\"\n \"/metrics.json\"\n \"/admin/metrics.json\"\n \"/admin/metrics\"\n \"/delegator\"\n \"/delegator.json\"\n \"/logging\"\n* \"/help\"\nControl:\n \"/admin/shutdown\"\n \"/logging.json\"\nDiagnostics:\n* everything else\nThen we allow access to each of these categories to be enabled or disabled (enabled by default).  For finer grained control we also allow a blacklist of regexes to be specified.\nWe should acknowledge that this isn't bulletproof.  Even UI endpoints like /delegator can cause side-effects such as initiating service discovery watches.  Untrusted clients should not be granted at the admin server at all.  However, having this level of control is better than not having it, especially to make it harder to shoot yourself in the foot.\nWhat does everyone think?. Also a big thanks to @robertpanzer for taking the lead on this long outstanding issue! \ud83d\ude4f . @robertpanzer this is looking great!  I have a few suggestions but I know there is some time pressure to get this merged quickly.  In the interest of time, I've pushed a branch with my suggested changes: https://github.com/linkerd/linkerd/compare/alex/disable-shutdown\nPlease take a look and merge those changes into this PR if you agree.. You will also need to add the DCO signoff to all of your commits before we can merge this.. Thanks for filing this @ewilde!\nI think it would be great to have a response classifier that allows retrying of POSTs.  However, since POST requests may contain bodies, it may be necessary for Linkerd to buffer the bodies in order to be able to retry them.  Furthermore, we would only want to do this for sufficiently small bodies.\nSee ClassifiedRetryFilter to see how we do this in H2.. Hi @a4chet.  I believe this is because your header \"/svc/test\" contains a slash which is an illegal character for a path token.  You're using the io.l5d.header.token identifier which attempts to read the entire header (\"/svc/test\") as a single path segment.  I think you probably want to use the io.l5d.header.path identifier which reads the header as a slash-delimited path.. Thanks, @pbagchi!  I know this is silly for a documentation fix, but would you mind adding the DCO signoff to your commit to make our DCO bot happy?. Thanks for working on this!  A few early thoughts:\nThis classifier marks ONLY writes (POST and PUT) as retryable but all other methods as non-retryable.  Is this what you want?  Would it be more useful to have a response classifier which marks ALL methods as retryable?\nNote that chunk encoded requests are never retryable.  That means that even with this response classifier, a chunk encoded POST will not be retried.  Is this acceptable?. This looks good but I believe it will conflict with https://github.com/linkerd/linkerd/pull/2053. @AMCR you should now be able to merge master and then resolve the conflicts.  Thanks!. Thanks, @AMCR!  I think you may need to make sure your base branch is up to date.  The diff shown in Github is very large and includes changes from other branches that have already merged to master.. It also looks like the DCO signoff you provided is slightly different than what the bot was expecting: \nExpected \"pbagchi <priyasmita.bagchi@gmail.com>\", but got \"Priyasmita Bagchi <priyasmita_bagchi@cable.comcast.com>\".. Fixed by https://github.com/linkerd/linkerd/pull/2049. I've done manual testing of this on another branch related to https://github.com/linkerd/linkerd/issues/1906.  Essentially the setup is that there is a gRPC server which serves a long lived streaming response and a client which sends a request and then resets the stream.  This results in a memory leak on the server because it tracks all of the remotely reset streams.  After this change, memory usage is stable in the same scenario.. Fixed by https://github.com/linkerd/linkerd/pull/2063. My guess is that this would allow global dynamic routing by updating the dtabs in namerd.  But I don't believe this API allows per-lookup, or per-request overrides of any kind.. Unclear.  There are still a lot of more fundamental pieces to routing that need to be nailed down for Linkerd2.. This should already be possible with the rewrite namer\nYou can define:\nnamers:\n- kind: io.l5d.rewrite\n  prefix: /io.l5d.rewrite\n  pattern: \"/{service}\"\n  name: \"/#/io.l5d.dnssrv/{service}.staging.foo.com\"\nand then use the rewritng namer in your dtab:\n/dnssrv/us-east-1 => /#/io.l5d.rewrite. I can't think of any case where we'd want to forward the x-forwarded-client-cert, internally or at the edge.  @dadjeibaah did you have a scenario in mind where forwarding that header would be appropriate?. Thanks, @jmalvarezf!  This looks great, I'll try to test it today.\nOne quick question: I notice that this propagator looks for the x-b3 headers but if they're not present it falls back to reading the l5d-ctx headers.  Is this fallback behavior actually desirable?  At first glance it seems like this might make things more confusing if Linkerd could potentially be loading its trace context from one of two different places.. Thank you for this great contribution, @jmalvarezf!. starting point: https://www.ibm.com/support/knowledgecenter/SSYKE2_8.0.0/com.ibm.java.vm.80.doc/docs/diag_tools_intro.html\n. Adding the -Xdump:heap:events=user command line option allows a heapdump and javacore files to be created when a SIGQUIT signal is sent to the JVM.  This can be triggered by running kill -3 1.\nThe javacore contains detailed information about the current state of the JVM including a thread dump.  It can be analyzed with JCA: https://www.ibm.com/developerworks/community/groups/service/html/communitystart?communityUuid=2245aa39-fa5c-4475-b891-14c205f7333c\nThe heapdump is in PHD format and cannot be analyzed by Hotspot heap analyzer tools.  Instead, it can be inspected using HA: https://www.ibm.com/developerworks/community/groups/service/html/communitystart?communityUuid=4544bafe-c7a2-455f-9d43-eb866ea60091\n. Eclipse MAT can be made to understand PHD heap dumps by adding the DTFJ plugin:\n\nIn MAT, go to help -> Install new software\nAdd a site: http://public.dhe.ibm.com/ibmdl/export/pub/software/websphere/runtimes/tools/dtfj/\nSelect \"Diagnostic Tool Framework for Java\" and install it\n. @dhruvmphatek I believe that you need to manually create a znode at /icx/arpit.  This is the znode under which namerd will store your dtabs.  . Looking at your config it looks like you have /icx as your announcer path prefix.  This means that the announcer will only announce names which start with /#/icx.  Your server announce name /#/io.l5d.serversets/arpit doesn't start with this prefix and therefore does not get announced to ZK.  If you look at Linkerd's logs at startup, I believe you will see a message like: \"no announcer found for /#/io.l5d.serversets/arpit\"\n\nYou can read more about announcers in the docs.. I think there is some confusion here because ZK is being used for two different things in this scenario.  \nZK is storing a serverset which is a list of ephemeral znodes which represent the addresses of your service.  These are created and maintained by the serversets and exist at the path specified by the Linkerd server's announce name.  Note that the announcer's pathPrefix is stripped off of the announce name so if the announce name and the pathPrefix are equal, then the serverset will be stored at the ZK root (this is probably not what you want).  The serverset is read by the serversets namer and used for routing requests.\nAdditionally, Namerd stores dtabs in ZK.  By default, Linkerd will ask Namerd to use the dtab called default.  This is stored at the path <storage pathPrefix>/default.  If this dtab doesn't exist, you must create one using namerctl.  More details about how to use namerctl in this blog post: https://blog.buoyant.io/2016/05/04/real-world-microservices-when-services-stop-playing-well-and-start-getting-real/\nI hope that clears things up.. @dhruvmphatek By default Linkerd uses the Host header of the request for the service name: /svc/<host>.  In your case it looks like you're sending a request with a host header of 192.168.0.165:4140 and Linkerd is trying to look that name up in zookeeper.. I would recommend reading https://linkerd.io/1/advanced/routing/ to get an understanding of how routing works in Linkerd.  You typically set the host header to the logical name of the service that you wish to talk to.  Then the dtab transforms this name into a serversets lookup in ZK.. Hi @dhruvmphatek.  It looks like you're trying to read a service discovery endpoint as a dtab.  Taking a step back, I suspect that you may be trying to do too much at once which is why this is so difficult to configure correctly.  I recommend building up your config more incrementally and get something simple working first before layering on additional complexity.\nI'd recommend starting with the io.l5d.inMemory dtab store and the io.l5d.fs namer because these are very simple and don't interact with ZK.  Once you get that working, you can add announcers and manually verify that your proxies are getting registered in the correct location in ZK.  After you have verified that you can switch to the io.l5d.serversets namer instead of io.l5d.fs.  Finally, once that is working, you can try switching from the io.l5d.inMemory dtab store to the ZK store.  Keep in mind that you'll want to use separate znodes for you service registry from the znodes you use for dtab storage.\nI hope this is helpful.. Yay!  Way to go!. Is this a duplicate of #2076?. Thanks for the report, @ylopatin-earnin!  This error message means that Linkerd was not able to establish a connection to your service for some reason.  Are there any other errors in the Linkerd logs that could explain the problem?  It may also be helpful to use tcpdump to see if something is preventing the TCP handshake from occurring.. Ah, interesting.  It seems like Linkerd did not get the update from Kubernetes for some reason.  Are you able to share the full content of /namer_state/io.l5d.k8s.json?  In particular, I'm interested in the watch/lastStreamData and watch/lastStreamDataAt.  If this data and timestamp matches the previous update (the second most recent one) then that indicates that Linkerd simply did not get a message from Kubernetes about the change to the address set.. Thanks for the logs, @ylopatin-earnin.  In those logs I don't see 100.96.7.223 either before or after the deployment.  Which is the pod ip that Linkerd is erroneously routing to?  From what I can tell, Linkerd looks to be perfectly in sync with the Kubernetes API.. Hi @ylopatin-earnin \nThanks for the clarification.  This is a bit confusing but since the Kubernetes API is a streaming API, we track the last initial response body separately from the streaming data.  Therefore we would expect the \"response\" field to be the same before and after the deploy (assuming the Kubernetes stream was not reset) but the \"lastStreamData\" fields to be different and for the \"lastStreamData\" after the deploy to contain the new set of endpoints.  . Ah, that's great, that means that Linkerd DID get the endpoints update from Kubernetes.  The next thing to look at is the /client_state.json endpoint on the Linkerd admin server.  This shows the internal state of Linkerd's load balancers and contains the addresses that Linkerd uses for routing.  . @ylopatin-earnin hmmmm that's pretty weird that the k8s namer state shows that it has the correct current value but this value isn't propagated to the client state.  I've never seen that happen before.  Do you have a concrete set of steps that we can use to reproduce this?. Ok thanks.  We have not been able to reproduce this so far but we're going to spend some more time trying to get a reproduction.  Any detailed reproduction steps would be extremely helpful.. @negz how do the symptoms you describe here relate to https://github.com/linkerd/linkerd/issues/2148?  With that issue closed, are the problems that you describe here still valid?. Closing due to inactivity.  @cian-k please re-open if this is still an issue.. It's possible but the idea is to expose the underlying state as accurately as possible.  The file contents are just raw strings so I think that's how we should display them.. Hi @fixed-point!  This is a good question.  What's going on is an interaction between dtab unions (the & operator) and Linker-to-Linker mode.\nWhen the first Linkerd (outgoing) routes the request, it randomly picks one of the union arms to use: HOST_ONE or HOST_TWO, and sends the request to the Linkerd running on the node of that instance.  When the second Linkerd (incoming) gets that request, it again randomly picks one of the union arms to use and attempts to send the request an instance running on that node.  However, if the two Linkerds made different random choices, there may not be an instance running on the node for the second Linkerd to route to.\nThis can be resolved by changing the incoming configuring to use the choice made by the outgoing Linkerd instead of re-evaluating the dtab itself.  \nidentifier:\n    kind: io.l5d.header\n    header: l5d-dst-client\n  dtab: /svc/%/io.l5d.daemonset/*/*/* => /\nThis causes Linkerd to read the l5d-dst-client header, strip off the transformer prefix, and then send the request to that destination (the destination picked by the outgoing router).\nHope that helps!. Yes, the io.l5d.header.path identifier for H2: https://linkerd.io/config/head/linkerd/index.html#http-2-header-path-identifier. :tada: Can you also include sample contents of the endpoint?. I think you'll be able to improve performance further by setting FINAGLE_WORKERS to an appropriate value for the hardware.. Hey @zackangelo \ud83d\udc4b \nYikes, those scrape times.  It's not clear to me that these issues are necessarily related though.  Would you mind opening a separate issue about the scrape times and we can dig in further?. Hi @bdlk.  It looks like the port you define in your Endpoints resource is named \"mysql\" but your dtab routes to the port named \"http\".  The port name is the second segment in the /#/io.l5d.k8s name.  i.e. /#/io.l5d.k8s/<namespace>/<port>/<service>.  The port can be either a port name or a port number.. I think this docs error has been fixed by #2080 and #2082 . Typically we release the docs at the same time as we release Linkerd.. Thanks for the report, @vguhesan!  Please let us know if you see something like this again.. Very interesting.  @ccmtaylor any ideas?  Have you seen anything like this before?. Fixed by https://github.com/linkerd/linkerd/pull/2111. Things are pretty good with CircleCI for now.  We can re-open this if it becomes an issue.. Could we also add a logged deprecation warning whenever one of these deprecated config objects is loaded?  Similar to what is done for the deprecated trustCerts field in the TlsClientConfig config object.  Could you also update the description of this PR to give some context.. Thanks for this report @hsmade.  That is a surprising behavior.  I'd recommend taking a look in WatermarkPool to see if something funny is going on there.\nThe way that I would expect this to work is that when Linkerd issues the request, it should see that the WatermarkPool connection queue doesn't have any Services in it that are not in the Closed state.  Therefore, it should attempt to create a new connection.  You can attach a debugger and see if the service's in WatermarkPool's queue are closed (as they should be, if the connection has been closed by the remote).\nHope this is helpful.  I'm happy to chat about this more if you get stuck.. @hsmade any luck investigating this?. Hi @leozc, I haven't been following this issue very closely.  Would you mind updating the title and description of this PR with some context and the current state?. @ccmtaylor have you had a chance to review this?. @leozc I think you need to update the integration test: https://circleci.com/gh/linkerd/linkerd/8085#tests/containers/1. This LGTM!  @ccmtaylor since you're the codeowner for this, GitHub needs your explicit approval to merge this..  Hi @dhruvmphatek!  I have to admit, I don't totally follow all the details of the scenario that you're describing.  If Linkerd is sending traffic to service A, it should be load balancing requests to each instance of service A.  If a request fails and is retried, the retry will likely go to another instance of service A.\nI'm not sure if this answers your question but hopefully it is helpful.. @dhruvmphatek I hope my answer was helpful.  Please reopen if you have further questions.. @perrymanuk thanks for the report!  If you upload your heapdump somewhere, you can email it to me or send it to me on the Linkerd slack and I'd be happy to take a look.  It would also be useful to get the output from the prometheus endpoint in a case where it took 12-16 seconds to respond.  . Thanks, @perrymanuk, and sorry it took so long for me to get back to you.\nI'm looking at the metrics that you sent and here are a few relevant lines about the admin server:\nadminhttp:request_latency_ms{quantile=\"0.9\"} 1097\nadminhttp:handletime_us{quantile=\"0.9\"} 1094272\nadminhttp:response_payload_bytes{quantile=\"0.9\"} 1196789\nThis is telling me that the admin server is serving a payload that is about 1.1M, which matches with the size of metrics payload you sent.  It's also telling me that it serves it in under 1.1 seconds.  That's pretty slow, but it's not the 12-16 seconds that you reported.  Do you have any ideas on what might account for that discrepancy?. You mentioned that you're running a single instance of Linkerd 1.4.5 alongside your normal deployment of Linkerd 1.4.0.  After this issue occurs, are requests to /admin/metrics/prometheus slow on the Linkerd 1.4.0 hosts as well?  Is there any significant different between the contents or structure of the metrics from Linkerd 1.4.0 and Linkerd 1.4.5?\nIt would be interesting to see a side-by-side comparison to see if there are any relevant differences that could explain the latency.. I've attempted to reproduce this with a prometheus payload of approximately the same size, but I'm not seeing large handletimes or request latencies.  \n@perrymanuk If you can reproduce this issue, I recommend attaching a profiler to see where Linkerd is spending the time.  There are instructions for how to create a flame graph here: https://discourse.linkerd.io/t/linkerd-performance-tuning/447. Thanks, @adriancole!  I think this would be a very simple change to https://github.com/linkerd/linkerd/blob/master/linkerd/protocol/http/src/main/scala/io/buoyant/linkerd/protocol/ZipkinTracePropagator.scala  Are you interested in working on this?. Thanks @zackangelo.  Unfortunately, there's not much for us to investigate without more information about the conditions that trigger this.  Please let us know if you uncover any more details.. Closing this as a dupe of #2125 until we have information to suggest otherwise.. For reference, here is the output when I run this as non-root:\n$ linkerd/target/scala-2.12/linkerd-1.4.6-SNAPSHOT-exec linkerd/examples/fs.yaml\nmkdir: /var/log/linkerd: Permission denied\nGC_LOG must be set to a directory that user [alex] has write permissions on.  Unable to use [/var/log/linkerd] for GC logging.\n-XX:+AggressiveOpts -XX:+AlwaysPreTouch -XX:+CMSClassUnloadingEnabled -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled -XX:+CMSScavengeBeforeRemark -XX:GCLogFileSize=10485760 -XX:InitialHeapSize=33554432 -XX:MaxHeapSize=1073741824 -XX:MaxNewSize=348966912 -XX:MaxTenuringThreshold=6 -XX:NumberOfGCLogFiles=10 -XX:OldPLABSize=16 -XX:+PerfDisableSharedMem -XX:+PrintCommandLineFlags -XX:+PrintGC -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCDateStamps -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -XX:+PrintPromotionFailure -XX:+PrintTenuringDistribution -XX:+ScavengeBeforeFullGC -XX:-TieredCompilation -XX:+UseCMSInitiatingOccupancyOnly -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseGCLogFileRotation -XX:+UseParNewGC -XX:+UseStringDeduplication\nJava HotSpot(TM) 64-Bit Server VM warning: Cannot open file /var/log/linkerd/gc.log due to No such file or directory\nLinkerd then continues to start up normally (though presumably without gc logging).\nIt's maybe a bit confusing that the error here is broken across three different messages (the mkdir one, the GC_LOG echo, and the Cannot open file one) but it's probably okay.. @utrack I think this is the intended behavior.  When Linkerd is not able to initiate a request to the Kubernetes API (in this case because the stream limit has been reached) it will continuously retry until the binding timeout is exhausted.\nLinkerd should print error logs indicating the problem.\nOpening a second connection would defeat the purpose of the max-streams-per-connection.  I recommend simply increasing max-streams-per-connection on your API server.. This allows us to manage subscriptions individually.  The Kubernetes API should be able to handle a very large number of concurrent streams.  Is there any reason why you need to artificially limit it to 250?. @utrack interesting!  I'm surprised that the stream limit is so low by default.  Anyway, if you want to watch more services, you'll need to increase the stream limit.. Thanks for all the great info in this issue, @taer!. @peterfroehlich thanks for reporting this but I believe it is a different issue from what @taer is reporting.  @taer is using Consul whereas the deadlock you describe is specific to Zookeeper.  I have opened https://github.com/linkerd/linkerd/issues/2129 to track the Zookeeper deadlock issue.  . Hi @taer, sorry that it's taken me so long to dig in to this; thanks for your patience.\nI'm looking at the namer state files you sent but I'm a bit confused by their contents.  totalWatch.json, which is the state from the bad namerd includes an entry for the service /.local/live/serviceName-production but goodNamerd.json does not.  Instead it only contains entries for services like app39 etc.\nWas the good namerd resolving requests for /.local/live/serviceName-production?  How do you determine which namerds are good vs bad?. @taer ah, thanks for clarifying.\nIt seems like Namerd simply never gets a response from Consul, which causes it to get stuck like this.  I have not be able to reproduce this, however.  I have filed https://github.com/linkerd/linkerd/issues/2131 which would mitigate this by restarting watches after a certain timeout.  . Fixed by #2131. Closed by #2127 . I've been experimenting with a branch that adds a write queue to the transport, inspired by what was done in https://github.com/grpc/grpc-java/pull/431/files\nhttps://github.com/linkerd/linkerd/compare/alex/write-queue?expand=1\nThis uses a queue to buffer writes and schedules flushing them on the event loop.  This results in larger packets being sent with many frames per packet.  This gives us about a 30% increase in throughput and a slight improvement in latency.  Here are the raw results from slowcooker:\n```\nDIRECT\n$ strest-grpc client --address \"127.0.0.1:9999\" --totalRequests 200000 --streams 100\n2018-09-25T14:43:09-07:00    0.0B 190817/0 10s L:   0 [ 10  14 ]   64 J:   0   0\n2018-09-25T14:43:19-07:00    0.0B   9183/0 10s L:   0 [ 10  15 ]   33 J:   0   0\n{\n  \"good\": 200000,\n  \"bad\": 0,\n  \"bytes\": 0,\n  \"latency\": {\n    \"p50\": 4,\n    \"p75\": 6,\n    \"p90\": 8,\n    \"p95\": 10,\n    \"p99\": 14,\n    \"p999\": 64\n  }\n}\nLINKERD MASTER\n$ strest-grpc client --address \"127.0.0.1:4143\" --totalRequests 200000 --streams 100\n2018-09-25T14:38:41-07:00    0.0B    996/0 10s L: 542 [2119 2221 ] 2248 J:   0   0\n2018-09-25T14:38:51-07:00    0.0B   2000/0 10s L: 254 [905 1030 ] 1045 J:   0   0\n2018-09-25T14:39:01-07:00    0.0B   2764/0 10s L: 191 [758 842 ]  892 J:   0   0\n2018-09-25T14:39:11-07:00    0.0B   5380/0 10s L: 106 [282 453 ]  663 J:   0   0\n2018-09-25T14:39:21-07:00    0.0B   8871/0 10s L:  36 [282 301 ]  373 J:   0   0\n2018-09-25T14:39:31-07:00    0.0B   7329/0 10s L:  39 [261 390 ]  465 J:   0   0\n2018-09-25T14:39:41-07:00    0.0B   8674/0 10s L:  35 [193 251 ]  367 J:   0   0\n2018-09-25T14:39:51-07:00    0.0B   7172/0 10s L:  66 [259 323 ]  423 J:   0   0\n2018-09-25T14:40:01-07:00    0.0B  10084/0 10s L:  24 [146 202 ]  249 J:   0   0\n2018-09-25T14:40:11-07:00    0.0B  11993/0 10s L:  32 [124 152 ]  202 J:   0   0\n2018-09-25T14:40:21-07:00    0.0B   9164/0 10s L:  46 [218 345 ]  406 J:   0   0\n2018-09-25T14:40:31-07:00    0.0B   9173/0 10s L:  36 [181 201 ]  299 J:   0   0\n2018-09-25T14:40:41-07:00    0.0B  10339/0 10s L:  41 [134 156 ]  202 J:   0   0\n2018-09-25T14:40:51-07:00    0.0B   6543/0 10s L:  47 [301 362 ]  408 J:   0   0\n2018-09-25T14:41:01-07:00    0.0B  10744/0 10s L:  39 [133 181 ]  369 J:   0   0\n2018-09-25T14:41:11-07:00    0.0B  11476/0 10s L:  25 [132 168 ]  303 J:   0   0\n2018-09-25T14:41:21-07:00    0.0B   8619/0 10s L:  35 [250 291 ]  425 J:   0   0\n2018-09-25T14:41:31-07:00    0.0B  10814/0 10s L:  35 [127 173 ]  232 J:   0   0\n2018-09-25T14:41:41-07:00    0.0B  10284/0 10s L:  23 [205 289 ]  351 J:   0   0\n2018-09-25T14:41:51-07:00    0.0B  12546/0 10s L:  26 [128 197 ]  294 J:   0   0\n2018-09-25T14:42:01-07:00    0.0B  14762/0 10s L:  24 [ 99 123 ]  172 J:   0   0\n2018-09-25T14:42:11-07:00    0.0B  15359/0 10s L:  25 [ 88 108 ]  163 J:   0   0\n2018-09-25T14:42:21-07:00    0.0B   4914/0 10s L:  34 [185 301 ]  347 J:   0   0\n{\n  \"good\": 200000,\n  \"bad\": 0,\n  \"bytes\": 0,\n  \"latency\": {\n    \"p50\": 88,\n    \"p75\": 116,\n    \"p90\": 176,\n    \"p95\": 242,\n    \"p99\": 559,\n    \"p999\": 2249\n  }\n}\nLINKERD WITH WRITE QUEUE\n$ strest-grpc client --address \"127.0.0.1:4143\" --totalRequests 200000 --streams 100\n2018-09-25T14:30:47-07:00    0.0B   2794/0 10s L: 213 [517 1449 ] 1468 J:   0   0\n2018-09-25T14:30:57-07:00    0.0B   4699/0 10s L: 119 [373 437 ]  584 J:   0   0\n2018-09-25T14:31:07-07:00    0.0B   8635/0 10s L:  29 [237 316 ]  381 J:   0   0\n2018-09-25T14:31:17-07:00    0.0B  17322/0 10s L:  14 [ 94 116 ]  148 J:   0   0\n2018-09-25T14:31:27-07:00    0.0B  16772/0 10s L:  16 [ 97 115 ]  166 J:   0   0\n2018-09-25T14:31:37-07:00    0.0B  14754/0 10s L:  17 [133 178 ]  325 J:   0   0\n2018-09-25T14:31:47-07:00    0.0B  21097/0 10s L:  13 [ 71  85 ]  231 J:   0   0\n2018-09-25T14:31:57-07:00    0.0B  20959/0 10s L:  17 [ 70  95 ]  138 J:   0   0\n2018-09-25T14:32:07-07:00    0.0B  16081/0 10s L:  15 [133 180 ]  263 J:   0   0\n2018-09-25T14:32:17-07:00    0.0B  20414/0 10s L:  15 [ 74  93 ]  142 J:   0   0\n2018-09-25T14:32:27-07:00    0.0B  20515/0 10s L:  10 [ 71  89 ]  112 J:   0   0\n2018-09-25T14:32:37-07:00    0.0B  17408/0 10s L:  11 [119 218 ]  281 J:   0   0\n2018-09-25T14:32:47-07:00    0.0B  18550/0 10s L:  14 [ 72  88 ]  183 J:   0   0\n{\n  \"good\": 200000,\n  \"bad\": 0,\n  \"bytes\": 0,\n  \"latency\": {\n    \"p50\": 50,\n    \"p75\": 63,\n    \"p90\": 97,\n    \"p95\": 146,\n    \"p99\": 300,\n    \"p999\": 1468\n  }\n}\n```. Even with a write queue, the amount of overhead that Linkerd is adding here is much higher than I would expect.  As far as I can tell, most of the time seems to be tied up in slow socket writes.  Further investigation is required to determine why the socket writes are so slow.. @dadjeibaah I've added a workaround for the import conflict and filed https://github.com/twitter/util/issues/229\nI also had to disable the example tests because the CI job kept running out of memory.  We should investigate a long term solution for this so that we can re-enable those tests in CI.. It looks like this is due to a bug in finagle-serversets which was fixed recently: https://github.com/twitter/finagle/commit/78c17dc93339dd99fd2439de60ef1009a1ace722\nLinkerd will pick up this fix when we upgrade our Finagle dependency (in progress) and should be included in the next Linkerd release.. I think you're right about the current behavior not propagating the client id.  I'd consider that to be a bug and this PR is a great opportunity to fix it!. @corhere are you still interested in working on this?. I'm closing this due to inactivity.  Please re-open if you would like to resume.. It's not exactly the same because the K8S API uses streaming responses whereas the consul API uses blocking responses.  . We've been able to reproduce a direct memory leak with a similar stack trace using strest-grpc.  I'm not sure if this is the same issue that you're hitting here but I'll share what I've found in case it applies to you as well.\nI turned on Netty's leak detection but I kept getting the OutOfDirectMemory error without triggering any of Netty's leak detection.  This suggests to me that it was not an issue with reference counting of the direct ByteBufs but rather a \"true\" memory leak.  In other words, direct ByteBufs were being created and active references to them were sticking around.  I looked at Linkerd's stream/open_streams metrics and saw this number was very large.  This suggests a stream leak is holding references to direct ByteBufs.\nI added maxConcurrentStreamsPerConnection: 1000 to the server section in my Linkerd config and this seemed to fix the problem: I was no longer able to trigger the  OutOfDirectMemory error.\nI recommend monitoring the stream/open_streams metrics during your testing.  If you see this number growing over time, you're probably hitting this same issue.  You can also try setting maxConcurrentStreamsPerConnection to see if that fixes the issue.\nNext steps for me are to investigate the cause of the stream leak.  I also want to consider adding a default value for maxConcurrentStreamsPerConnection.. In our case, the stream leak was caused by an aberrant client behavior where the client was leaving streams open forever.  So technically, Linkerd was behaving correctly, although a default limit on maxConcurrentStreamsPerConnection would protect against this.\nI'm curious to know if your situation is at all similar.  Is it possible that you have clients which are leaving streams open?. hey @zackangelo!  any update on if setting maxConcurrentStreamsPerConnection helped this?. any update on this, @zackangelo?. Thanks @fantayeneh this is super interesting.  This is pretty similar to some of the work that's being done in Linkerd2 to get per-source metrics.\nI think a good first step before getting into the code would be to write up an issue describing what metrics Linkerd should output and describing the overall approach to how we determine the source.. I notice java.security.cert.CertificateException: could not find certificate file: /var/tmp/linkerd/certCollection2672780185168733830.tmp in your logs.  I wonder if we fail to clean up file descriptors when the file doesn't exist, or something like that.  Is it expected that your Linkerd is trying to load certificates that don't exist?. It would also be helpful to know if this problem is specific to epoll.  Can you reproduce this if you turn epoll off?. Does the number of open FDs correlate to the number of open connections?  Do you see a growth in the number of FDs while number of connections remains constant?. @zackangelo any new information here?  are you still seeing this?. cc @zackangelo . This change reduces the latency overhead added by Linkerd but it's still substantial (50ms p90 in my above test).  You can see the full data in the slow_cooker output above.. @zackangelo if you get a chance, I'd love to hear if this change yields the above performance benefits in your environment.. @fantayeneh can you go into more detail?  How do we identify the source of a request?  How do these sources get rendered into metric names?\nHow would this be configured?  It should probably be an opt-in feature that should only be enabled if the number of sources is known to be small.  Can this be done as a Linkerd plugin?. Thanks for reporting this @hsmade.  This is definitely not the intended behavior, Linkerd should continue to use the last known dtab if it cannot reach Namerd.  We will try to reproduce what you have described.\nJust so that we can follow your reproduction as accurately as possible, how did you block traffic between Linkerd and Namerd?  Firewall?  Kill the Namerd process?  Chop the cat5 cable with an axe?. It looks like this is probably a consequence of this change: https://github.com/linkerd/linkerd/pull/1863\nThe problem is that Consul overloads the semantics of a 500 response.  It could mean that the DC that we supplied does not exist, in which case we wish to return Neg.  Or it could also mean that there was some internal error in consul, in which case we would want to continue to use the previously known resolution.  It doesn't seem like there's a nice way to differentiate between these two states other that checking the human readable error in the response, which feels somewhat brittle to me.\n@dadjeibaah do you have any ideas here?. This is related to https://github.com/hashicorp/consul/issues/4901. I don't have any answers but I can add a bit of context around the various things that you're seeing.\nD 1011 06:05:08.402 UTC THREAD43 TraceId:4b87accf7f787c46: Failed mid-stream. Terminating stream, closing connection                                                                      \ncom.twitter.finagle.ChannelClosedException: ChannelException at remote address: localhost/127.0.0.1:8001. Remote Info: Not Available                                                      \n        at com.twitter.finagle.netty4.transport.ChannelTransport$$anon$2.channelInactive(ChannelTransport.scala:196)\nLinkerd receives a streaming response from the Kubernetes API.  This error suggests that the connection was idle long enough that it timed out and closed.  This should not be a problem as Linkerd will see the error and simply request a new stream.\nThe Dtab playground UI can give you a bit more detail on why /#/io.l5d.k8s.http/live/80/nginx (neg) is failing to resolve and if it's a problem with the Daemonset transformer.\nThe 404s are pretty mysterious.  Do you know if the Kubernetes API server has access logs?\n. Nice work getting to the bottom of this!  This feedback about debugability is really helpful too.\n\nI wonder whether it might be worth exposing the DaemonSet transformer state in a similar fashion to the k8s namer state.\n\nFantastic idea.  I agree 100%\n\nI don't understand why I was seeing ChannelClosedException from linkerd\n\nTearing down the connection is basically the only way to signal cancellation in HTTP/1.1.  So this will happen every time Linkerd stops a watch.  We did some work back around 1.4.0 to clean up harmless error messages out of the logs, but perhaps there's more to do.\n. Whoa!!!\nWell, you know what they say: \"It's never a Kubernetes bug -- except when it's a Kubernetes bug\". I've been messing around with various combinations of starting and stopping watches and I haven't been able to reproduce the Exception propagated to the default monitor (upstream address: /127.0.0.1:56278, downstream address: localhost/127.0.0.1:8001, $\nabel: client). error.  Do you have repro steps for how to trigger this error message?. I'll keep an eye out for this message but if you discover a way to reproduce it, please update this ticket.. I'm going to close this until we have a reliable way to reproduce it.  Please re-open if you are able to trigger this.. Hi @sh-miyoshi!  It looks like this is a Linkerd2 issue.  Would you mind opening this issue in the Linkerd2 repo instead?  https://github.com/linkerd/linkerd2/issues/\nThanks!. You can see the current number of observations by looking at the bindingcache/active stat in metrics.json.  Reaching the maximum will cause failures when resolving new destinations and failures in the dtab admin.  It is possible that some observations were released by the time you tried using the dtab admin page.\nThis limit can be configured in Namerd: https://api.linkerd.io/head/namerd/index.html#cache. Hi @nadilas!  I believe this is effectively a duplicate of https://github.com/linkerd/linkerd/issues/766.  Do you agree?. That is unfortunate.  I'm not familiar with the architecture of grpc-web, is it at all possible to disable the http/1.1 preflight requests?  \nUnfortunately, until Linkerd supports protocol detection or upgrading, I don't think there's anything we can do.. @nadilas I'm not aware of anyone actively working on it, so probably not in the near term.. Thanks, @robertpanzer!\nA few questions:\nAre you more interested in incoming (server side) rate limits?  i.e. if we receive more than X requests per period, Linkerd should protect the service by rejecting some of the requests.  Should the limit be global, or per client?\nOr outgoing (client side) rate limits?  i.e. we should be nice to our downstreams and not send them more than X requests for period.  \nAre you interested in limits per instance?  Or centralized/coordinated limits?. Hi @samek!  Per-instance and per-service rate limiting are pretty different in purpose (per-instance limits usually protect the service itself from excess load and allows the limit to scale with the number of instances whereas per-service limits usually protect some shared resources like a database and you don't want the limit to scale with the number of application instances) and in implementation (per-service limits generally need some kind of centralized coordination between instances).  \nFor the purposes of keeping them separate, let's focus on per-instance limits in this issue.  @samek if you could file another issue where we can discuss per-service limits, that would be great!. I think it should be already possible to implement per-instance rate limiting as a request authorizer plugin.  Let me lay out what I mean.\nWe could create a request authorizer plugin that has the following behavior.  When it receives a request, it increments an in-memory count.  If the count exceeds the limit, the request is rejected.  The count is reset to 0 at a regular period.  Since request authorizers are installed in the endpoint stack, each instance will get an separate independent count.  \n\nIn a daemonset deployment, the plugin should only be installed on the \"incoming\" routers.   \nThe limit could be read from an external service or from config\nthe plugin could be selectively installed on certain clients\nthe plugin can return a ResponseException which allows it to render a specific response to the caller\n\nThe config could look something like this:\nrouters:\n- label: incoming\n  protocol: http\n  [...]\n  client:\n    kind: io.l5d.static\n    configs:\n    - prefix: /%/io.l5d.localnode/#/io.l5d.k8s/default/http/serviceA\n      requestAuthorizers:\n      - kind: io.l5d.rateLimit\n        limit: 100\n        periodSecs: 15\nwhich would configure Linkerd to only allow 100 requests per 15 seconds to each instance of serviceA.\nThe pseudo-code of the plugin filter would be something like:\n```\nval limit = // Get from config or external service\nval count = new AtomicInteger()\ntimer.schedule(period) {\n  count.set(0)\n}\ndef apply(req: Request, svc: Service[Request, Response]): Future[Request] = {\n  if (count.get >= limit) {\n    return new ResponseException(/ insert response here /)\n  }\n  count.increment()\n  svc(req)\n}\n```\n@robertpanzer does that match up with what you were thinking?  If so, we can put together a proof-of-concept plugin along these lines.. Hi @Allen0710!\nLinkerd will only cache resolutions that it has already looked up (it doesn't keep a complete cache of Consul's entire registry).  This means that if you resolve a service while Consul is up, and then Consul goes down, Linkerd will continue to be able to route to that service.\nHowever, if you attempt to route to a service that Linkerd has not routed to yet while Consul is down, Linkerd will not be able to resolve it.  Is it possible that this is what you're experiencing?  This would be my guess based on the log message: Last known state is Pending.. Hi @Allen0710.  I'm going to close this due to inactivity.  Please feel free to re-open if you have more information to provide.. @ylopatin-earnin and @chrisgoffinet: if you'd like to help me validate this fix, you can pull this branch (alex/stab-stab-stab) or the docker image: buoyantio/linkerd:1.5.1-stab. After leaving some tests running over the weekend, this is looking pretty solid.  @ylopatin-earnin, is there a way for you to verify this fix in your environment?. @thedebugger am I understanding you correctly that you actually delete the dtab CRD itself (rather than a specific dtab resource)?  What would you expect the behavior to be in this case?  Do you eventually re-create the CRD?. Thanks for filing this PR, @jreichhold!  I originally took out this escaping because we explicitly control all label names and can ensure that they're properly escaped before they make it to this part.  A different bug allowed arbitrary label names in, which caused this problem.  Therefore, I believe that https://github.com/linkerd/linkerd/pull/2176 will fix this without needing to add another layer of escaping.\nDoes that sound right to you?. @yarosman I know this seems silly for just a docs change, but would you mind adding the DCO signoff to your commit so that we can accept it?  thanks!. DCO is just a line in your commit message that allows us to accept the change.  Details here: https://github.com/linkerd/linkerd/pull/2177/checks?check_run_id=32414978. Will be fixed by #2194 . Would you mind adding a config with this parameter to linkerd/examples so that this code path is tested?. I think 18.12.0 is coming out very soon.. Does this actually work with JDK 10?  My understanding was that these GC flags were removed in favor of a new system in JDK 9.  I would expect that these flags would only work on JDK 8 or earlier.. If we detect that the java version uses this new gc logging system, we should set the flags for it:\nhttps://dzone.com/articles/disruptive-changes-to-gc-logging-in-java-9. Hi @chenhaibocode, this looks like it might be a problem with the way the Linkerd binary was built.  Did you build the executable yourself or download it from the releases page?  Which version?  Can you provide steps to reproduce?. I think this may be the same issue as #2199. I'm actually a bit confused about the behavior of maxRequestKB and maxResponseKB.  I think the next steps for this ticket are to verify the behavior of these properties in all different cases:\n chunked message\n non-chunked message\n* non-chunked streaming message. Hi @chenhaibocode.  What library are you using to populate the serverset entries into Zookeeper?  Or are you populating it manually?  I believe the serversets protocol requires that entries in a serverset should be ephemeral znodes as children of the service name.  So in your example, the service endpoint data should live in an emphemeral child of /discovery/prod/test.. Hope my previous comment was helpful.  Please re-open this issue if that doesn't work for you.. I'm a bit torn on this.  Special casing the Pending -> Neg transition doesn't seem correct to me.  It also makes the logic more stateful and difficult to understand and reason about.\nIn particular, the proposed solution does not allow for DCs to be deleted.  This may be rare in practice, but it maps logically to the error message.\nI don't operate Consul in production so it's hard for me to know which use-cases to focus on, but I'm hesitant to making this code more complex than it already is.. After thinking very long on this, I'm not as opposed to it as I was initially.  This is the best solution that I can think of to deal with these intermittent consul errors.  Your argument about DC deletion is compelling as well.  @dadjeibaah, what do you think?. Truncating the response sounds great to me. Thanks for filling this!  I think this totally makes sense as a feature and seems pretty well scoped.  @chrismikehogan, are you interested in working on this?  If so, I can point you in the right direction.. @chrismikehogan KvApi is the class that defines how we interact with the Consul KV api.  I think you'll want to add a parameter to that class that controls if gzipping should be used.  You'll then need to modify the read and write methods in that file to gzip the values on the way in and unzip them on the way out.\nYou'll then need to add a property to the consul store config object to control the gzip behavior.  The value of that property would be passed into KvApi here: https://github.com/linkerd/linkerd/blob/master/namerd/storage/consul/src/main/scala/io/buoyant/namerd/storage/consul/ConsulDtabStoreInitializer.scala#L49\nHope that makes sense!  I'm happy to provide more guidance if you run into any problems.. Wow, great find, @waxie!  Are you interested in submitting a PR that updates our OpenJ9 base image?. Hey @waxie, I see that you opened and closed a PR for this.  Are you still interested in working on this or should we pick this up?. @adw12382 thanks! This heap report is hugely helpful.  We've got some theories about what might be happening that we're trying to validate.  I'll keep this issue updated with our findings.. Can you talk a bit about how to test this?  Have you used wireshark to observe the ping frames?  Is there a way to simulate a ping timeout and observe the behavior?. It seems that Linkerd is still attempting to send a request to 127.0.0.1:4322 (the Namerd which is stopped) and I guess this triggers a local goaway.  I would have expected the load balancer to not even attempt to send requests to that endpoint.. @adw12382: are you able to test if this change fixes the Namerd memory issue for you?  Would it be helpful for us to provide a docker image for you to test?. @thedebugger: does this look right to you?  Are you able to verify this fix?. Have you been able to test that the socket options get set correctly?\nOne suggestion for how to test this would be to build off this branch and run Linkerd in docker or on a Linux machine, establish a connection through Linkerd, and check the socket timer with netstat -o.  The socket timer should be controllable with the keepalive socket option.  See http://veithen.io/2013/12/19/inspecting-socket-options-on-linux.html for more info.. It looks like something changed related to what trace events we send: according to the failing tests there is now an extra wire receive.  We should figure out what caused that change and if it makes sense or not.. @wmorgan I think this needs your approval because it touches CHANGES.md. @wmorgan I think this needs your approval because it touches CHANGES.md. Thanks, @truongnh1992.  We already have a link to our code-of-conduct in the README.md. Hey @zackangelo!  If this is the same bug as https://github.com/linkerd/linkerd/issues/1696 (and I'm about 90% sure that it is) then I think the direct memory aspect is a bit of a red herring.  I believe that this is a leak of tracked stream objects.  Stream objects may hold on to some direct memory which would explain why this appears to manifest as a direct memory leak.. If haven't looked specifically, but if I had to guess, I'd look at:\nNetty4DispatecherBase -> streams -> Netty4StreamTransport -> stateRef -> SendStreaming/RecvStreaming -> Data -> Buf\nYour proposed experiment with clearing out the stream map sounds interesting.  I don't think there's anything special you need to do to tear it down other than dropping it from the map.  Obviously this will have implications for correctness, but I'm guessing it will resolve the leak.. This is fascinating, thanks @chrisgoffinet and @thedebugger!  Very nice find.\nIf I understand the implication, kubectl proxy is automatically upgrading the HTTP/1.1 requests from Linkerd to HTTP/2 requests to the kubernetes api server.  Is that correct?\nIt seems like two potential solutions could be:\na) Build kubectl from master as you describe above.\nb) Change the k8s namer to send HTTP/2 requests instead of HTTP/1.1 requests so that we can do our own stream management.\nDo those solutions make sense to you?  Do you have a feeling about which would be better to pursue?. Another potential solution is to simply increase the value of maxConcurrentStreams on the Kubernetes api server (which I believe can be set by a flag).  If Linkerd needs to watch more than 250 services, I think it would be better to raise this limit and have all of those watches on a single connection, rather than splitting them across multiple connections.. @chrisgoffinet switching to using http2 and our own stream management would require implementing something like https://github.com/linkerd/linkerd/issues/2253 which would be non-trivial.  I think we should explore using a recent kubectl build before resorting to that.. That's actually what I did here.  Unfortunately, I couldn't find a place where the scrooge generated bindings expose the encoding of the request, which is why I did this obtuse thing where I sneak the encoded request out of ThriftServiceIface.\n. intentional?\n. Another option would be to sanitize paths in stat names by replacing / with some other delimiter\n. Do these values have to be kept in sync with the default values elsewhere?  Or is this the source of truth for the default values?\nRelated question: what do we want to emit when a value is absent from the config?  Nothing?  Or the default value?\n. what's the point of this?  Couldn't you just omit the module if you don't want to do tls?\n. What kind of attribution do I need, given that this is mostly just a copy-and-paste job?\n. Do we want the zk hosts to be part of the path like this?  Or configured statically from the linkerd config?\n. ss, serversets, or s5s?\n. this module never does any deserialization, does it?\n. should this be rsp = req.response?\n. extra blank line\n. Sure, but I think Admin is the wrong name.  These are all the TwitterServer traits EXCEPT for the admin ones.  Maybe call it io.buoyant.App or something...\n. The admin port defaults to 9990 so omitting the config param has the same behavior as omitting the flag would have.  So I think this should be a nop with respect to the example config.\nThat said, I can add it to the example configs if we just want to demonstrate that the param exists.\n. I'll update the favicon in a followup.\n. TIOLI:\nSome(bound.path).filter(path => !path.isEmpty).map(_.show)\n. Ok, scalariform, you win.\n. nope, good catch, thanks\n. You're right.  I originally had a test in HTTP but when I moved it around I forgot to update this.  I'll remove it.\n. We use the com.twitter.finagle.serverset2.Zk2Resolver which doesn't look to support chroot.\n. this doesn't do anything?\n. I feel like the creation of Upstreams and Downstreams in the e2e tests can probably be DRYed up a bit.  Maybe that's the subject of another review though...\n. I don't think this admin stuff should be in the routers section.\n. The thriftMethodInDst parameter allows routing based on the thrift method name.\n. This bit about one port per Thrift service isn't really true if using thriftMethodInDst\n. You don't think it's a good idea?  What's wrong with routing \"AddFoo\" and \"GetFoo\" to the FooService and \"AddBar\" and \"GetBar\" to the BarService?\n. I agree that having Running closer to the top would be better, but I'm fine if that comes in a later review.\n. Can you explain why you need to make the underlying var private?\nWhat's wrong with just having a public\nvar label: Option[String] = None\n?\n. this value is bogus, right?  to actually run this you have to override this to specify the config (that was added in a volume mount or whatever)?\n. Can we indicate by naming, or scaladoc, or in the description that this is applicable to clients, not servers?\n. Yeah, it's a parameter on both server and client.\n. Should this be docker run?\nAlso, I think the volume mount source and dest need to be an absolute paths.\n. This is intentional.  \"{host}\" is the PathMatcher syntax for variable substitution.  The {host} variable is captured in the prefix and substituted here.\n. Agreed.  Fixed.\n. should this await?\n. Not sure, but I think we omit braces for single expression methods.\n. Should we move this default impl up into ConfigInitializer and just override it in ProtocolInitializer?\n. A bit dissonant to use for and foreach right next to each other.  Maybe use for here to be consistent?\n. This is unrelated to this review but I just thought of it.  If linker-level dtabs are gone and now dtabs are per-router, is a per-router dstPrefix still useful at all?\n. This is actually an interpreter section, right?\ninterpreter:\n  kind: default\n. I wish we could test more here, but finagle is pretty opaque about the load balancer.\n. Good point.  The default (p2c) comes from finagle so I'm a bit uneasy documenting something that could change out from under us.  But in this case I think it makes sense.\n. That's right.  What we call ewma here is actually p2c+ewma.  Similarly, what we call aperture is actually aperture+least loaded.  The finagle docs (which we link to) spell this out more explicitly.\n. These classes are so small that I find it more readable to put them in the same file.  I don't have a strong opinion about this, though.  Happy to change.\n(Precedent: we violate the \"one class per file\" guideline all over the place)\n. I don't think you can discover inner classes via ServiceLoader :cry: \nUnless someone starts having :muscle: feelings, I'm going leave this alone.\n. Good catch, thanks.  Fixed and added to the test.\n. bad\n. Should this be a basicHttp identifier specific option?\n. Should this move into the identifier/http project?  It's used as a default value for the identifier param so I guess not.\n. could all of these be grouped into a passthrough like \ncase act => act\n?\n. It is super lame that Activity doesn't have a rescue method.\n. Good catch.\n. Good catch.\n. Intellijaaaaaaaaaaaaaaaaay\n. unfortunately config is a name conflict with an sbt method.\n. we use Ms instead of Millis elsewhere.  we should be consistent\n. What do you think about calling this just probation instead of probationEnabled?\n. can you specify that this is optional and what the default is\n. TIOLI\nIf you want to default to whatever the param's default is instead of explicitly defaulting to true, you could do\n.maybeWith(enableProbation.map(LoadBalancerFactory.EnableProbation(_)))\n. yeah, matching finagle is nice.  but \"enabled\" on a boolean is pretty dumb and redundant.  shrug I don't feel strongly.\n. just noticed this says probationEnabled but it's enableProbation elsewhere\n. What do you think about calling these \"namespace\" instead of \"routerName\"?  In the context of linkerd each router has one namespace so it amounts to the same thing, but using \"namespace\" probably makes more sense if this is to be reused by namerd.\n. TIOLI might be cleaner to move this check down to around line 52:\nif (commonName.isEmpty && strict.getOrElse(true)) throw ...\n. Oh.  Good point.\nCould do \nval commonName = names.map ...\nif if (commonName.isEmpty && strict.getOrElse(true)) throw ...\ncommonName\nto avoid the awkward match if you wanted.  But this is fine too.\n. that would work in all the tests so far, but would fail for any Activity that starts out as pending.\n. I think this should return Future[Unit]\n. should this be wrapped in an Await.result?  Nothing in service is currently asynchronous, but this would turn flaky if that ever changes.\n. these request annotations won't ever conflict with response annotations, but should we namespace them anyway just so that all the request annotations have consistent naming?\n. that looks good to me\n. There's some discussion of that here: https://github.com/BuoyantIO/linkerd/issues/207.  Ideally yes, though the cost to implementing this depends on the backing store.\n. For clarity can we refine this pattern?  This is really case Activity.Ok(_), right?  And moreover, we expect this to be triggered when it goes to Activity.Ok(NameTree.Neg).  Any other value would be a violation of the assumption that the input is already a bound name, I think.\n. I don't think we want to remove this from the cache.  Anyone observing the discarded state var will no longer receive updates (eg if the namespace is recreated).  Instead, I fixed list to only list non-empty namespaces.\n. I think there's a MediaType.Json const for this\n. nit: reader.read(BufSize).flatMap {\n. nit: .flatMap\n. maybe rename to not conflict with java.lang.Object\n. Should the (Some(false), Some(_)) case emit some kind of error or warning since it's not really valid?  ie specifying tlsWithoutValidation doesn't have meaning if tls is off\n. maybe name this file Resource.scala to match this trait?  I know there are a bunch of resources in this file, but it might be good to have the file name match at least one of the things defined in it.\n. isn't this already defined in Resource?\n. should this be called withNamespace?\n. can you explain what this line is for?  Should the type param of Watchable specify that W <: Watch[O] : Manifest ?\n. I like to avoid O as an identifier because it can be confused for 0 although in this case there's probably not any room for confusion\n. Can this return type be made into a case class for clarity?\n. can we have a description of what this returns or a more self-descriptive return type?\n. this just takes the resourceVersion of the last watch?  maybe define a helper method def last[A](as: AsyncStream[A]): Future[Option[A]]\n. personal preference: I like Future.value\n. does calling reader.discard() throw a ReaderDiscarded?\n. maybe name this watchStream or watches?\n. nit: .flatMap\n. nit: avoid infix\n. seems like a lot of the core logic lives in here.  would it be worth splitting this into its own file Watchable.scala?  It also might help readability if the nested defs in watch were top level private[this] defs in Watchable.\n. it doesn't look like ListResource is actually abstract\n. rename to avoid conflict with scala.collection.immutable.List?\n. tioli: interpolation?\n. could call this DtabCodec maybe?\n. should namespace be used instead of \"default\"?\n. I think this can be act.values.toFuture.flatMap(Future.const).map(_.ketSet)\n. I agree.  The user can choose to retry the PUT and it should work once the state is no longer changing.\n. possibly factor some of this out for readability?\n. Could this be an abstract class?\n. Can the two codecs be consolidated?\n. sorry, I should have been more specific.  I meant to possibly factor out the Activity.State => Var[Resolution] mapping since it's long and interrupts the flow.\nNot a big deal, TIOLI.\n. we discussed possibly calling this Space so as not to be confused with the api version (v1) or the resource version\n. I think the case class name isn't too important.  What's more important is that the fields are named (eg watches and currentVersion).  What about Restart or RestartResult?\n. this took a bit to wrap my head around so it might be worth calling out in a comment or something like that:\ncurrentVersion forces the stream so it will not be satisfied for an in progress response.  This would be a problem because theStream is a streaming response, except for the fact that since this is all on the right-hand-side of theStream ++.  That means that for this to be evaluated, theStream must be already completed.\nThis code seems a bit brittle because it relies on this unstated requirement that currentVersion be only called with a completed stream.  I don't have a specific suggestion, but at the very least we should be careful.\n. Depending on the implementation, this can create an unobserved Activity and sampling it may not get the correct value.  A safer thing would be store.observer(namespace).values.toFuture.flatMap(Future.const).map { case Some(dtab) => ...\n. should this be called namerdAdminRoutes?\n. I think namerd-main doesn't depend on linkerd-admin so this might be a problem\n. unify this with the type alias in the io.buoyant.namerd package object?\n. personal preference: it would be more readable to me if these internal methods were instead private[this] methods on Watchable\n. this is an internal method inside an internal method \ud83d\ude22 \n. might factor out a helper method def last[T](as: AsyncStream[T]): Option[T] with a comment here about why it's safe to force the stream\n. this is crazy, but what happens if you put -1 here?\n. ThriftProtocolDeserializer imports import com.twitter.finagle.thrift.Protocols\nisn't finagle-thrift needed for that?\n. ah, I see now.  ignore me.\n. [S] namers could be a Map[Path, Namer] and you could move the .toSeq from NamerdConfig.scala into NamerdAdmin.scala\n. [N] not really a side-effecting method so I might ditch the ()\n. [N] don't need braces for a single expression\n. [N] don't need braces here\n. [I] This isn't really true... it's pretty special purpose\n. [I] Since this client is short-lived, I don't think we need a session timeout.\n. [Q] did you decide to go with the config parsing over just trying to extract the information you needed from a Map[String, Any]?  Needing to depend on every possible plugin is not great.\n. [S] could use a match here:\nconfig.storage match {\n  case zkStorage: zk => ...\n  case _ => exitOnError(...)\n}\n. [I] we should close the client when we're done.\ncould do this with a onExit block.\n. [C] :(\n. I think you want to Await.result this.\n. I think this ties together the syntax for finagle Paths and the syntax for zookeeper paths.  They're only the same by coincidence.  What about just path.split(\"/\")?\n. I found this was the easiest/fastest way to spin up a linkerd that used the k8s namer\n./sbt linkerd-examples/k8s:run\nI figured this might be useful to others as well.  I'm not super attached to it, though.\n. Are these two alternate ways of running namerd, or sequential commands?\n. should we put a placeholder version here so that this doesn't fall out of date?\n. TIOLI: remove unnecessary nesting\n}.toFuture.flatMap { _ =>\n  ensurePath(zkPrefix)\n}.flatMap { _ =>\n  zkw.create(...)\n}\n. What if customers want to set up less permissive ACLs on the zkPrefix node?  I think we'll want to revisit this in the future.\n. this can be handle and () instead of rescue and Future.Unit.\n. I think we should pass the deadline to zkw.close\n. good catch, thanks\n. That's a good point.  I don't think any of the examples other than acceptance-test.yaml are tested.  I created https://github.com/BuoyantIO/linkerd/issues/280 to track.\n. My thinking was that without the infinite retry filter catching all failures, a failure should bubble up and blow up or log at a higher level.  But probably there's no logging at the higher level either so I think you're probably right we should log failure here regardless of if we are swallowing them or not.\n. Actually I just noticed that there's already some logging in the infiniteRetryFilter's predicate.  I removed LogFailureFilter entirely.\n. Doesn't really matter either way in this case.  This ordering makes sense to me because zk.create is the fallible operation and I like to have the rescue directly on the operation that it rescues from.  The .unit call is just to discard the return value since it's not needed to satisfy the method signature so it makes sense to me to have it last.\n. yeah, pretty much everything in c.t.f.serverset2 is package private, including the ClientBuilder and exceptions.\n. Yes, I think that's the reason.\n. The default stream is Backoff.decorrelatedJittered(10.milliseconds, 10.seconds).  Would exponentialJitter be better?  We could also make the parameters configurable.\n. per offline discussion: digest auth is fundamentally insecure because it involves sending the password in cleartext: https://zookeeper.apache.org/doc/r3.1.2/zookeeperProgrammers.html#sc_BuiltinACLSchemes\nBut digest is probably the easiest to show off in an example.\n. Future already special cases this for us:\ndef raiseWithin(timer: Timer, timeout: Duration, exc: Throwable): Future[A] = {\n    if (timeout == Duration.Top || isDefined)\n      return this\nThe exception message contains the path so it can't be val'd on the class.  I expected that the exception would be pass-by-name, but it looks like it's not :(\nAnother option would be to make it extend NoStackTrace to somewhat reduce allocation.  Or remove the path from the message.\n. I reverted this because I think it's confusing behavior.  Path binding happens outside of the request path so the request timeout doesn't apply to it.\n. Yeah, it's confusing.  It would be helpful to me to talk through it in person to help get my thoughts straight before taking a stab at commenting it.\n. I see what you mean.  I don't immediately see how this could go wrong, but that doesn't mean that it can't.  To be safe I've expanded the synchronized block to the whole method.  In reality there should only be one updater so I don't think we'll really need to worry about contention.  Better thread-safe than sorry.\n. I removed an assert here where we were checking that clientAddr0 was Pending.  I think that once the NameTree changes to no longer include the old Var clientAdd0 then we can no longer make any assumptions about it.  \nIn practice we should never just sample an Activity[NameTree[_]] and watch a Var we find inside.  Instead we should always flatMap the Activity[NameTree[_]] to the Var[Addr] to get an observation that will always be correct.\n. That makes sense.  With clientAddr1 this test fails.  That's because we don't have an explicit pending state: when an Activity is pending, we simply don't send any updates.  This means that if the previous Var[Addr] becomes Neg, we'll keep that value until the new Var takes a non-pending value.  Let me know if you disagree, but I think this behavior is okay.\n. https://github.com/BuoyantIO/linkerd/issues/310\n. https://github.com/BuoyantIO/linkerd/issues/309\n. A small fraction of this file is actually Etcd.  Can we break this file up?\n. Putting all the interesting methods (get, set, etc.) on Key was surprising to me.  I would have found it more intuitive if these methods were on Etcd and took a Key as a parameter.  Can you talk about the motivation for this structure?\n. Using Finagle Paths as urls seems like a weird coupling.  Why not just Seq[String]?\n. suggestion:\nMap(quarum -> \"quarum\", recursive -> \"recursive\", ...).collect {\n  case (true, param) => \"param\" -> \"true\"\n}\n. Should aim for a consistent level of documentation on these operations.  Some have a lot, some have a little, some have none, some are scaladoc style, some aren't.\n. Can eliminate this var, similar to above.\n. avoid infix notation (here and elsewhere)\n. I think this starts a polling loop for every witness registered to the event.  What about creating a Var.async instead and then returning that.changes?\n. do we need both watch and events?  I think we should pick whether we'd prefer to expose an Activity or an Event and just do one.\n. should this be abstract?\n. suggestion: Try.orThrow(ByName.get(name))(() => Invalid(name))\n. could use some docs here, I'm not sure what index represents\n. could this return Try[NodeOp] instead of Future[NodeOp]?\n. good point\n. we already read namespaces from the URI eg /api/1/dtabs/<ns>.  Should we frontload the API churn for the sake of consistency and change these to /api/1/dtabs?namespace=default now?\nAlternatively, we could have a transition period where we support both...\n. The change in the return type is worth explaining.\nAddr has a Addr.Pending value.  Previously, we flatMapped the Activity[NameTree] directly to a Var[Addr] by mapping Activity.Pending to Addr.Pending.  This leads to some questions about how this pending state should be displayed in non-streaming responses.  Typically, we use Activity.toFuture to get the first non-pending value from an Activity but that doesn't work here because Addr.Pending is, in some sense, a non-pending value.\nTherefore, I changed this to flatMap the Activity[NameTree] into an Activity[Addr] so that it made sense to respond with the first non-pending value.\n. How can we detect this?  The Future[Response] is already complete by the time we're streaming data.\n. Good question.  I'm not sure what the best behavior is.  Print the error and keep streaming?  Print the error and close the connection?  Just close it?  Suggestions appreciated.\n. Good question.  Discarding values is a red-flag and usually not intentional.  I want to eventually turn on the compile option -Ywarn-value-discard which will cause a compiler error for discarded values.  In this case, though, we really do want to discard the return value from respond.  We can assign it to an unnamed variable to explicitly signal that discarding is intended here.\nI'll make the same change on line 49.\n. Probably.  I plan to take a pass through the consul namer as well.\n. These quickstart instructions are already in linkerd/README.md and namerd/README.md\n. fair enough\n. Comment: io.buoyant.namerd.RichActivity adds a toFuture method to Activity.  Unfortunately, we don't want to depend on namerd here.  I wonder if we should create a utility project that we can move RichFuture into...\n. being explicit about discarding values seems like good style to me, but you're right, we should discuss it.\n. I think what this exercise has led me to is that I think I don't understand why finagle has both an Activity for the NameTree and a Var for the Addr.  Just watching the addr Var directly seems problematic because a change to the NameTree could invalidate that Var.  Therefore, it seems like you would always want to watch the NameTree flatMapped to the addr... in which case, why have both?\n. This case applies when v is defined but is not Utf8 encoded.\n. is it intentional that prevExist only gets set if ttl is defined?\n. this can just be vp ++ tp\n. these are each used in one method respectively.  I might move them inside or nearby those methods for readability.\n. Thanks for spelling this out.  Makes sense now. \ud83d\udc4d \n. TIOLI: This could return Future[Unit] and use flatMap instead of respond.  That way you would't need currentOp and could just raise on the total Future[Unit] returned from the original invocation of loop.\n. I'd prefer spelling out Representation in full\n. already an int\n. I prefer Representation\n. I think the Version imported here conflicts with i.b.etcd.Version.  Also I think some of these imports are unused.\n. unrelated?\n. I think this blows up with a MatchError if value is not Utf8 encoded.  Maybe worth checking first or at least documenting?\n. In general, that seems reasonable.  In our case we don't stream back any json and each render method would end up having the same handling for Throw (ie just print the exception message).  I think we can maybe defer that until we are returning content types other than plain text.\n. I think this return type can just be (String, String)\n. should the witness be updated here?  If not, note is unused.\n. should we log the fatal error?\n. do we need this? Or is raising sufficient to stop things?\n. TIOLI:\nparams.map { case (k, v) => s\"($k -> $v)\" }.mkString(\", \")\n. Thanks for pointing me to that discussion.  Option's getOrElse also takes a closure:\ndef getOrElse[B >: A](default: => B): B\nAm I missing something?\n. good catch\n. This is served on all the legacy admin pages (dtabs, metrics, etc).  Maybe this should be a link to the legacy dashboard?  Users can already return to the new dashboard by clicking on the logo (which links to \"/\").\n. on the other hand, exists is more type-safe than contains  :bike: \ud83c\udfe0 \n. default is actually to fall back to the debugTrace flag, isn't it?\n. style nit:\nval tracer: Tracer = tracers.map(_.map { t =>\n. What if the key does not exist?  We should return a DtabNamespaceDoesNotExistException in that case, I think.\n. I have no idea what reasonable values for the cache sizes are.\nThe inactiveCache can be pretty small, I think, because observations will typically only stay there for the short period between when the observation fires and when clients re-query it.\n. maybe _responseClassifier.map(_.mk).getOrElse(baseResponseClassifier ?\nor \n_responseClassifier match {\n  case Some(config) => config.mk\n  case None => baseReponseClassifier\n}\n. typo porbably\n. we should check the value of the Retries.Budget stack param in the parsed router.\n. yup, good idea\n. This is intentional.  Once we enter the synchronized block we check again that the key hasn't entered the active cache before making it active.\n. Yeah, that's probably wise.\n. how is this stats filter scoped?\n. This test is a bit confusing.  It creates a RetriesConfig that will fail if .mk is called on it.  Since you test that error case elsewhere, I'd just give a fully valid config here.\n. Is hardcoding this going to be a problem?\n. Yeah, of course!  The signature of addGauge is def addGauge(name: String*)(f: => Float): Gauge so the second parameter is a closure that can be repeatedly invoked.\n. need a blank line between lines because sbt\n. extra blank line\n. Could fold the stat names in here maybe?\n\"\"\"(.*)\\.(count|sum|avg|min|max|stddev|p50|p90|p95|p99|p9990)\"\"\"\n. Two newlines at the end is intentional?\nIf so we can use the same StringBuilder\noutput ++= \"\\n\"\n. Do we need to worry about latching logic here?  Do we want deltas for counters?  \nIs there any value in building a SampledValues and feeding that through a Prometheus StatsFormatter?\n. I think we have a choice about how we want to handle names that contain slashes\n1. require that names start with / and treat them as paths\n2. require that names don't start with / and treat them as paths\n3. escape/encode slashes\n. In your example are you trying to announce the same server to two different names, foo and bar?\nIf registering multiple servers I'd expect the config to be\nannouncers:\n- kind: io.l5d.whatever\n  path: /role/env\nservers:\n- label: foo\n  announce: true\n  ...\n- label: bar\n  announce: true\n  ...\n. I like this idea as a refactor, but I think it's out of scope for this change.\n. You could make an argument for just taking as many segments as there are, instead of erroring out.  Not sure which is better, I could see it either way.\n. There are two gauges (one for active size, one for inactive size) in the same scope.\n. good call\n. getLeaderData returns a com.google.common.base.Optional.  This is this simplest way I could think of to convert it to a scala.Option\n. Let's use io.l5d.path\n. record checks isActivelyTracing so we already shouldn't be annotating the request.  This does save us from constructing the annotation though. \nWith this change. debugTrace() will no longer print annotations for non-sampled requests.\n. def convert(nothing: PartialFunction[Try[Nothing], Boolean]): PartialFunction[Try[Any], Boolean] = {\n  case th@Throw(_) if nothing.isDefinedAt(th.cast[Nothing]) =>\n    nothing(th.cast[Nothing])\n}\nI think this gives you a pf that's defined at the same places as the original.\n. TIOLI could use Retries.Budget.emptyBackoffSchedule instead of Back.const(Duration.Zero)\n. nit: factoryToService\n. TIOLI: could flatMap instead of awaiting twice\n. I think this should be a val\nActually, does this have to be a function at all?  Can this just be a method, as before?\n. TIOLI:\n(since the def syntax is nicer to read than the anonymous function syntax)\ndef recordResponse(rsp: Response): Unit = { ... }\nval recordResponseFn: Response => Unit = recordResponse _\n. should this be \n} else {\n  f\n}\n. Update scaladoc.  This isn't really a retry policy.\n. sentence fragment \n. TIOLI: this second bullet point could be a sub-bullet of the first\n. good idea\n. it's now on the linkerd admin and on the namerd http control iface.\n. role and description should indicate that other context is extracted as well (eg dtab)\n. I think there's a quoting typo here\n. just curious: why do we clear the header after reading it?\n. what's the purpose of differentiating these?  the contents of l5d-dtab will get moved into l5d-ctx-dtab after one hop, right?\n. Personally, I'd favor simplicity over brevity.\n. just curious: what's the difference between doing this vs <script id=\"data-router-name\" type=\"application/json\">$router</script>?\n. do we need ^ and $ anchors so that this doesn't match longer urls that include this?\n. Cool, thanks for the explanation.  I don't really have an opinion but I agree with you that it doesn't seem like a big deal either way, it's just always nice to be consistent.\n. Yeah, probably.  I'll try it.\n. This would require something in between linkerd and the outside world (like nginx).  If we want to position linkerd as being able to be used on the edge, maybe we need to configurably not emit these\n. Sure, it just moves where you have to handle the case where the interpreter is not a delegator.  Having that handled here means we can handle it in a consistent way.\n. Might be worth opening an issue for investigating why this happens.  Even if we never do it, at least there's a record documenting why we drop data points.\n. style nit:\nsvc(req).map { response =>\n  ...\n}\n. We haven't done this for previous releases:\nhttps://github.com/BuoyantIO/linkerd/blob/0.6.0/CHANGES.md\nhttps://github.com/BuoyantIO/linkerd/blob/0.5.0/CHANGES.md\n. oh, we did for 0.4.0... but just that one \ud83d\ude2c \n. The help page on linkerd.io was updated recently, I think this is missing those changes.\n. Just curious why we populate this with javascript instead of hardcoding it here\n. What about \n\"/help.html\" -> (StaticFilter andThen ResourceHandler.fromDirectoryOrJar(...)\n. good point\n. tiny non-blocking nit:\ncould use val instead of def\ncould omit braces for single expression\n. Unfortunately, caches isn't an Activity so this Activity is computed based on the namespaces in cache at the time of this call.  So we need to recompute the Activity in case namespaces have added to the cache.\n. Could use Activity.value(...) here instead of creating an activity with a witness.\n. I don't think we want to key off of the extra dtab in the cache.  What do you think about making extraDtab a Option[String] in getBind?  If the extraDtab is present, then skip the cache and just return delegate(ns).bind(...) directly.  If it's absent, then use the cache like before.\nDoes that seem reasonable?\n. req.params.get(\"dtab\") will return an Option[String] if that's easier to work with\n. I'd break up this line:\nval fullDtab = extraDtab match {\n  case Some(str) => ...\n  case None => ...\n}\n. style nit: omit semicolon\n. whatever you say, scalariform \n. I think these tests are not actually correct.\nRespect for the sampled bit of the trace is a property of the Tracer itself so I don't think it makes sense to test this here.  Indeed, BufferedTracer no longer respects it.\n. this should go in the x.x.x section above\n. TIOLI: it's probably just me, but for some reason I get confused when folding over options.  \n```\nengine match {\n  case Some(eng) => eng.mk(super.clientParams)\n  case None => super.clientParams\n}\nis maybe more obvious? FFTI\n`````\n. what does this do/is this for?\n. Could we refactor this into anApibase trait and haveCatalogApiandKvApiinherit from it?  Could also split these into 3 files (Api, CatalogApi, KvApi).\n. infiniteRetryFilter and retryClient could be factored into theApicommon trait\n. Can all the status code checking be moved intoIndexed.mk`?\nIdeally this could just be \nTrace.letClear(...).flatMap { rsp =>\n  Future.const(Indexed.mk[Seq[String]](rsp)\n}\n. might be worth noting in a comment/scaladoc that path must begin with a slash (I assume?)\n. style nit (here and elsewhere): avoid infix notation for methods that take a block.\ne.g.\ndo\nfoo.flatMap {\nnot\nfoo flatMap {\n. why not use Indexed.mk?  That way we can move all of the status code handling there.\n. Could use Indexed.mk here too.  Of course, the return type of this method would change to Future[Indexed[Boolean]] and the index would always be None, but I think that's okay.\n. could move all status code handling to here\n. it doesn't really impact this test, but I think path needs to start with a slash or else you'll get some weird uris.\n. this could be\nassertThrows[NotFound](await(...))\n(you have to import io.buoyant.test.Exceptions)\n. don't need this blank line\n. I don't think we would factor up apply, but definitely infiniteRetryFilter and retryClient could be factored up.\n```\ntrait Api {\n  ...\n}\nobject CatalogApi extends Api {\n  ...\n}\nobject KvApi extends Api {\n  ...\n}\n``\n. what about creating aIndexed.mkStringthat takes ahttp.Responseand returns aTry[Indexed[String]].  It would share the same status code and index handling withIndexed.mkbut would use the raw string instead of json parsing it.  what do you think?\n. don't need this blank line\n. I think we need to handle NotFounds here and turn them into an empty namespace list\n. consul throws a 500 if the cas value isn't a number.  we should check here that vstr can be parsed as a number, and immediately return a version mismatch exception if not\n. in the case of a NotFound, we still want to extract theX-Consul-Indexand set the index, otherwise we'll just send keep sending requests here.\n. I think this is unused\n. I think this should still be wrapped in aTrylike before, to help protect against malformed responses\n. I think the idiomatic way to mesh this withparseJsonreturningTry` would be\nfor {\n  rsp <- Trace.letClear(getClient(retry)(req))\n  value <- Future.const(parseJson[T](rsp.content))\n} yield Indexed[T](value, rsp.headerMap.get(Headers.Index))\n. What do you think about factoring this into execute which returns Future[Response], executeJson[T] which returns Future[Indexed[T]] and executeRaw which returns Future[Indexed[String]]?  (execute would be used by the other two methods)\n. by using the executeRaw method I suggest above, we could go back to using raw mode here, which seems nicer\n. good call making this a filter, I like it\n. prefer to use Future.exception instead of throw:\nservice(request).flatMap { ...\n  ...\n    case http.Status.Ok => Future.value(response)\n    case http.Status.Forbidden => Future.exception(Forbidden(response))\n    ...\n. nice, thanks\n. use absolute imports and put them at the top of the file\n. ).transform {\n. Closable.make\n. ).flatMap {\n. ).transform {\n. as far as I can tell, consul seems to always return 200 and \"true\", even if the key doesn't exist.  shrug\n. what about if (Try(vstr.toLong)).isThrow)?\n. ).flatMap {\n. ).transform {\n. Closable.make {\n. host.getOrElse(DefaultHost)\n. port.getOrElse(DefaultPort)\n. I think this file has the wrong name\n. oh, yeah, that makes sense.  good call\n. I think you're missing either a return Future.exception(...) or an else block.  Currently the Future.exception isn't returned, it's just dropped.\n. I notice we don't stat response size here.  Is that intentional?\nShould we just contribute memoizaiton to c.t.f.http.filter.StatsFilter upstream?\n. v1 contains implicit object EndpointsDescriptor and implicit classes can't live in packages.  I tried splitting this up so that some things went into the package and others into the package object, but then hit another weird issue with an illegal classfile dependency between EndpointsWatch and its companion object.\nSooo... \u00af_(\u30c4)_/\u00af\n. typo: Additionalyl\n. maybe link to https://linkerd.io/doc/0.7.2/routing/\n. I think this line should be case Some(p) => addrCache.get((ns, p))\n. I think you should use the renderAddr method here.  As @olix0r mentioned, our serialization format isn't great right now, but if you use renderAddr then we can add proper serialization in one place.\n. I think renderAddr can be used here too.\n. It's a little confusing that Key(k, v) expects a unencoded value but Key(Some(k), Some(v)) expects a base64 encoded value.  Consider naming this something other than apply (perhaps mk?).  This reinforces the idea that a Key stores base64 encoded values, but it has a helper method to construct it from an unencoded value.\n. lowercase key and value please.\n. You can use value.map here\n. it's idiomatic to avoid Option.get:\nval filters = token match {\n  case Some(t) => new SetHostFilter(...) andThen new SetAuthTokenFilter(t)\n  case None => ...\n}\nor\nval authFilter = token match {\n  case Some(t) => new SetAuthTokenFilter(t)\n  case None => Filter.identity\n}\nval filters = new SetHostFilter(...) andThen authFilter\n. can this be made private[this]?\n. can this be made private[this]?\n. Ah, yes, you're right.\n. should this be io.l5d.testTelemeter?\n. incomplete comment\n. should the Telemeter have access to the stack params?\n. avoid Option.get.  instead do something like:\nagentApi.self().flatMap { self =>\n  val domain = self.Config\n    .flatMap(_.Domain)\n    .map(_.stripPrefix(\".\").stripSuffix(\".\"))\n  domain match {\n    case Some(d) => Future.value(d)\n    case None => Future.exception(???)\n  }\n}\n. is this index refactor unrelated to the rest of the change?  If so, I'd prefer it split into a separate PR.\n. What do you think about calling this LocalAgent or something like that?  Self (and self above) is a bit confusing because of scala's self-types.\n. I think this Resolve stuff snuck in from a different PR\n. I don't think you need this asInstanceOf cast\n. Address.Inet is a subclass of Address so something weird must be going on here...\n. ah, I think scala just needs some help with the type unification.  Try:\naddr() = Addr.Bound(addrs.toSet[Address], convertMeta(boundMeta))\n. Sorry, I meant that you would use some real exception here to indicate that the consul response was malformed, or didn't contain the right info, or whatever.\n. Something to note: the domain request will be fired once when the catalog namer does init, and then never again.  This means if that request fails for any reason, the domain will never be populated.  Worse, anything flatMapped from the domain future (like the rest of the body of init will never be executed).  \nSimilarly, this also means that we will never pick up changes if the domain value in consul changes.\nDoes the consul self API support long polling?  Can we have the domain be populated by a Var.async instead?\n@olix0r any thoughts on this?\n. I'd rename this method to localAgent too\n. We try to avoid lazy vals in our build file to keep dependancies explicit.  Adding a dependency on namer/core actually tangles things up a bit.  I actually think the best way to resolve this is to move the CatalogNamer out of consul and into namer/consul.\n. Option 3) have domainFuture be a Future[Option[String]].  When you use this value elsewhere, you can substitute a default value if it's None.\n. 10s seems like a reasonable polling interval to me.  Just a warning that this is going to bit tricky to get right because Var.asyncs will stop updating if they don't have any listeners which will be true most of the time.  We probably don't want the Var.async to restart and fire a new request for every consul change.  (or maybe we do, in which case we can ditch Var.async and just make a Future call each time.)\n. I don't think this refactor is necessary.  You should be able to just add a call to domainFuture in update (if setHost is set, of course).\n. This isn't quite right; cycle returns a Future so it shouldn't be in the yield position.\n. I suggest rolling back the refactor and just adding the domain call in update.\n. update test name to not reference Self object\n. for multiGet, recurse should always be true, right?\n. should allow the separator to be specified as well\n. TIOLI but I think it's more ergonomic to have this be\nrecurse: Option[Boolean] = None\n. probably want to be able to specify separator here too\n. I think this should just be \"recurse\" -> recurse.map(_.toString)\n. ditto\n. I think you need to un-delete all the list and get tests in this file\n. According to https://github.com/twitter/finagle/blob/master/finagle-serversets/src/main/scala/com/twitter/finagle/zookeeper/ZkAnnouncer.scala#L134\nsetting shardId=0 is safe\n. The main reason to plumb this to main is so that we can closeOnExit the announcements.  But yes, I could imagine refactoring this so that routers expose a generic list of Closables.  I'm open to refactoring this in the future as the need arises.\n. do we need to close this observation too?\n. You've got the right idea, but I don't think this code is correct.  Have you tested it against consul?\nI think you want to map over the seq of HealthNodes and map them to ServiceNodes.  I think you're also missing the Address property in the Node case class.\n. style nit:\nfor map, we prefer this style:\nexecuteJson[Seq[ServiceHealth]](req, retry).map { indexed =>\n  ...\n}\n. I would write this as:\nindexed.value.map { health =>\n  val service = health.flatMap(_.Service)\n  val node = health.flatMap(_.Node)\n  SerivceNode(\n    node.Node,\n    ...\n. from a Var.async we return a closable:\nClosable.make { _ =>\n        running = false\n        pending.raise(new FutureCancelledException)\n        Future.Unit\n      }\nIs there a more idiomatic way to cancel a future?\n. Hmmm yeah, I think that makes sense for list and observe (i.e. the methods that return Activities).  Probably not for the methods that return Future\n. I don't understand this comment.  I think it should be something like Don't retry interrupted failures or something like that\n. update test name\n. rm\n. rm\n. Ah, I see.  What about\nindexed.value.map { health =>\n  val service = health.Service\n  val node = health.Node\n  SerivceNode(\n    node.flatMap(_.Node),\n    node.flatMap(_.Address),\n    service.flatMap(_.ID),\n    ...\n. Hmmm... I think that unit test may have incorrect json.  It has:\n\"\"\"[{\"Node\":{\"Node\":\"Sarahs-MBP-2\",\"Address\":\"192.168.1.37\"}},{\"Service\": {\"Service\":\"hosted_web\",\"Tags\":[\"master\"],\"Port\":8084, \"Address\":\"\"}}]\"\"\"\nThe Node object and are Service object are in different objects.  Shouldn't this be:\n\"\"\"[{\"Node\":{\"Node\":\"Sarahs-MBP-2\",\"Address\":\"192.168.1.37\"},\"Service\": {\"Service\":\"hosted_web\",\"Tags\":[\"master\"],\"Port\":8084, \"Address\":\"\"}}]\"\"\"```\n. This was not part of the 0.7.4 release, can you move this to the \"current\" section (0.x.x)\n. There is already an example a few lines up in this section.  Maybe just add the useHealthCheck property to that example?\n. I think buoyant style is just to have all imports alphabetically sorted without any breaks like this\n. Nothing wrong with this but we don't currently have an announcer that can announce /#/io.l5d.fs names.  So far we just have one for /#/io.l5d.serversets.  So it might make more sense to use that in this example.\n. what about case classes instead of type aliases for both of these?  that way the fields can be referred to by name instead of by ._1\n. I think we want to close the observation for any kind of removal, not just evictions\n. maybe add a comment here explaining that this is a handle to keep the activity observed as long as it's in the cache.\n. TIOLI: I would drop the ? from Required? and drop the Value from Value Description\n. Looks like this was a problem with the previous docs as well, but keys are case sensitive and these should be lowercase.\n. Good call breaking this out in its own file.\n. Should we have config literal strings  (http, thrift, mux) in a monospace font?\n. We're a bit inconsistent on whether the Description column contains complete sentences (starting with a capital and ending with a period).\n. These sizes are in number of entries, not KB.\n. There is no default value, supplying a CA cert is optional.  If no CA cert is supplied, then some well-known authority is used, I think.\n. Should this be an aside?\n. I think this link isn't correctly linkified\n. should this return a Future instead of an Activity?\n. probably worth noting that this is only reachable if getServices encounters a non-retryable error\n. Just to confirm my understanding: we have to fail out here because without an index, we can't long poll?  What situation would cause this to happen?  Is recovery just impossible?\n. I'm not sure if this type alias makes things more readable or less\n. Took me a couple of reads to get the point here:\nThe Var[Addr]s are created eagerly, but since they're Var.asyncs, they aren't started until observed.\n. Should we set state to Addr.Failed?\n. maybe factor this out into a method that produces a metadata from a service key?\n. Or make DcServices use a final handle block.  Either way, I don't see a reason to not be consistent.\n. This might come up if consul was behind a reverse-proxy that served an error page if consul was having issues... or something like that.  Conceivably we could retry with backoff until indexes come back but... probably not worth it.\n. use the previously computed meta val?\n. why lazy?\n. If https://github.com/BuoyantIO/linkerd/pull/636 merges first, update this to UnidentifiableRequestException\n. seems pretty cheap and lazy vals make things so much harder to reason about.  probably not a big deal either way in this case but I'd shy away from lazy vals when possible on principle\n. These stats are potentially confusing.  Since they're in the map block of an activity, they will get incremented for each listener registered to this activity.  Therefore the counter represents something like \"number of negs returned\" rather than \"total number of negs that exist\" or something that one might expect.\n. should the service stats be scoped by service name rather than all together?\n. You're right, this is confusing because request is in IdentifiedRequest and UnidentifiedRequest for different reasons.  In IdentifiedRequest, the request is part of the output of identification.  In UnidentifiedRequest it's there for informational purposes.  I think we can remove it since the original request should be available in any place where you're getting an UnidentifiedRequest.\n. I would make shutdownGraceMs optional by making it an Option[Int].  In Linkerd.scala we can apply a default value and convert to a Duration.\n. I don't think this is needed\n. Ah, I see.  val _ = is my personal preference to explicitly indicate value discard.\n. TIOLI: I know shadowing is a fairly common practice, but I think it's especially dangerous when the shadowed variables have the same type.  Here announcers refers to both the filtered and unfiltered seq of announcers, depending on the scope.\n. should f be named more descriptively? announcement?\n. This seems unreliable since cancelations are advisory.  Maybe closing of announcement is best-effort anyway so this is okay...?  But allowing the announcement to complete and then closing seems more reliable to me.\n. Calling this announce instead of apply seems more clear and explicit.\n. I have raised concerns about this in the parent branch.\n. I actually prefer your second example over your first.  I think apply is overused in scala.  But that's just, like, my opinion, man.  FFTI.\n. should this be r <- receivers?\n. should this name be singular instead of plural?\n. I think you need to signal value discard here\n. The motivation is the linkerd-to-linkerd case.  The outgoing linkerd writes the concrete name into the l5d-dst-concrete header.  The incoming linkerd uses this identifier to read that header and then forwards to a local destination.  \nWe can change the default value to be something else, but I think we need to set this to l5d-dst-concete for the linkerd-to-linkerd setup.\n. More details here: https://github.com/BuoyantIO/linkerd/issues/659\n. I'm fine with that, I was just trying to follow the spec in #373: \n\"if it's not a valid path, add it to dstPrefix as a path component\"\n. Ok I agree it's a confusing default.  What do you suggest?  How about just l5d-dst?\n. Our ability to handle those edge cases depends on InetAddress' ability to handle those edge cases.  In some of those situations, this transformation may not make sense or be applicable.  \nYes, I will be adding a k8s-local transformer in a future review that does the same thing but changes out the isLocal logic for a /24 subnet comparison.  No need to prematurely refactor.\n. I think this reads better as\nval consistencyMode = consistency.flatMap {\n  case ConsistencyMode.Constant =>\n    Some(\"consistent\" -> Some(true.toString))\n  case ConsistencyMode.Stale =>\n    Some(\"stale\" -> Some(true.toString))\n  case ConsistencyMode.Default =>\n    None\n}\nval params = (consistencyMode ++ optParams).collect { ...\n. This is weird because the \"default\" string is never used, it just gets filtered out.  See above suggestion.\n. I think this is fine, case objects get good toString methods.\n. @olix0r what do you think about this?\n. TIOLI: in scala collect evokes a pf version of map.  in finagle collect evokes combining a group of applicative functors.  might want to consider a term with less baggage.\n. just curious: do you actually need the (_)?\n. want to import/alias this to TApp as before?\n. call this extHandlers?\n. case object adds a better toString FWIW\n. What are these text index routes for?\n. what about handlers on non-top-level plugins (like identifiers, for example)?\n. I think (consistencyMode ++ optParams).collect { case (k, Some(v)) => (k, v) } should work since both consistencyMode and optParams are iterables.\n. cribbed from io.buoyant.namer.k8s.K8sInitializer.  if fixed we can fix it in both places.\n. can we move this to the utils object or the RichConsulClient companion object?\n. Make this a value class by adding extends AnyVal\n. explicit return types on public methods, please\n. Hmmm... this is a change in the default behavior so I might word it to emphasize that.  Something like \"failFast is now disabled by default for the consul namer and dtab storage but can be enabled with the failFastoption\"\n. The advice in the last sentence is a bit confusing.  Maybe say something about keeping it false if talking to consul via the local consul agent but setting to true if talking directly to a HA consul api server?\n. I'd say that's probably not necessary\n. http://docs.scala-lang.org/overviews/core/value-classes.html\n. revert please.  linkerd style is all imports together alphasorted\n. I'm torn on if we should include the rep.contentString as well here.  ConsulApiError does so maybe we should include that here as well.\n. \ud83d\udc4c \n. I think if we went the HostAndPort route then we would get rid of the AdminConfig class and then in LinkerConfig the admin field would simply be of type HostAndPort.\nI could go either way on doing that or leaving it as is.\n. sounds like we all agree \ud83d\ude04 \n. \ud83d\udc4d \n. Do you think we should?  It doesn't seem necessary to me but I'm open to it.\n. Yeah, Accept header is how to select which content-type you want.  My understanding is that's standard practice for http?  Should I call it out anyway?\n. I think I'll hold off for now, just because it either make things inconsistent if I only apply it to the new tables or it will make the diff a mess if I apply it everywhere.  It might be worth doing in a followup though.\n. can we name this agentConfig or localAgentConfig?\n. lowercase parameter name please.  more idiomatic would be\nif (localDc.contains(datacenter)) {\n   ...\n} else {\n  ...\n}\n. I found this pretty confusing because localDc isn't actually the local dc, it's the moniker.\n. it may clarify things to add a third activity called localDc which is config.map(_.flatMap(_.Datacenter))\n. Do you think this really needs to be configurable?  Could we just use .local?\n. I think hardcoding is my preference.  What do you think, @olix0r?\n. style nit: no braces for cases\n. this might be more clear if named trailingSlash since it's not a boolean\n. why not just make this a lazy val?\n. TIOLI: you could encode this unreachability in the type system by having this class take (hd: Identifier[Req], tl: Identifier[Req]*) instead of (all: Seq[Identifier[Req]])\n. isn't this just essentially a fold?\n. would be more direct to go straight to (req: Req) => fold(req, hd, tl) here\n. could add the @tailrec annotation I think\n. should this be commented out?\n. maybe add a comment reminding the reader what this comment should remind them of\n. It's unused.  I think it's here so that one day we can release observations.\n. I think this can be reverted\n. oh, you're right.  Ignore me.\n. move this lazy val out into the class?\n. should this be p.take(segments)?\n. potential future work: path consumption?\n. TODO: maybe put something in test utils to make it easy to toggle logging in tests\n. why Future[Future[Unit]]?\n. incomplete sentence\n. maybe worth noting this is a deep copy of headers but shallow copy of data.  Why not name this copy?\n. this is synchronous?  what is the return value?  success?\n. what is this actually for though?\n. semi-colons???\n. is this to avoid valid-discard warning?  elsewhere we do\nval _ = closeP.setDone()\n. or even better: a case class with 2 named Future[Unit] properties\n. oh.  variable shadowing rears its ugly head :)\nsegments doesn't depend on the request at all though, so I feel like this could be rejiggered.\n. fwiw this behavior of taking the whole path if segments is unspecified is different from the io.l5d.path identifier which defaults to 1 segment.\n. move this out onto the class or companion object?\n. doesn't technically need interpolation \n. This would require moving RewritingNamer up into router-core.  Not a big deal but I think we can do this later.\n. Maybe also add an entry for documenting the namerd HTTP api\n. I realize this is just copied from elsewhere, but it's probably worth fixing this comment\n. In seems like a weird choice of type variable names.  This is essentially just a queue, right?  It doesn't make assumptions about the directionality of the data.  Which way is In vs Out?\n. incomplete comment\n. just a few style nits:\n- addrs can be a val and doesn't need a type annotation\n- omit unnessary braces for single expression bodies\n- omit empty () for non-side-effecting methods\n- use == for string equality\n- that double for-loop can be simplified into a single\nsomething like this\nval addrs = if (server.ip.getHostAddress == \"0.0.0.0\" ) {\n      val addresses = for {\n        interface <- NetworkInterface.getNetworkInterfaces\n        if interface.isUp\n        inet <- interface.getInetAddresses\n        if !inet.isLoopbackAddress\n      } yield new InetSocketAddress(inet.getHostAddress, server.port)\n      addresses.toSeq\n    } else {\n      Seq(server.addr)\n    }\n. don't need the type annotation here\n. you can do announcers.flatMap { ... } instead of announcers.map { ... }.flatten\n. What about passing in a uid and secret instead of content and letting the Api implementation construct the json content body?\n. AuthRequest?\n. we probably want to log here (and probably also in the UnexpectedResponse case as well...)\n. ditto\n. I wouldn't do it as part of this change\n. private[this] please\n. I think we have to do some tricky synchronization here.  If multiple requests come in at once, they'll all see the token as invalid or missing and all kick off their own auth requests.\nI think that there are basically 3 states that the token can be in: absent, present, or pending.  This could be represented as an Activity[Option[String]].  \nIf the token is present, we can just use it.  If the token is absent, we set the state to pending and kick off an auth request (using synchronization to make sure things don't change out from under us).  If the token is pending, we can just map on the activity to wait for the value.\nState is hard!\n. no braces for cases\n. no braces for cases\n. what if you get UnauthorizedResponse back from this one?  Do we assume that there's no way the token could expire in this span of time?\n. don't need this case, rescue takes a partial function\n. what exception(s) are we guarding against here?\n. what about conditioning on the path?  If the path is the authPath, serve the authBuf.  otherwise assert that the request has a valid token and serve the buf.\n. string interpolation inside of string interpolation is crazy, yo\nI'd break the token value into it's own val\n. this could be\nsys.env.get(secretKey).flatMap { secret =>\n   ...\n}\n. this is just a single expression so you don't need braces here\n. is this still needed?\n. Use wildcard _ import for 5 or more items.\n. I think you shouldn't need the braces on ${e}\n. if you import RichActivity then you can do authToken.toFuture\n. In the case where token refresh comes back with None, something must have gone wrong.  Probably return Future.exception here.\n. for readability, consider factoring the response handling code into it's own method\n. RichActivity\n. This method could use a comment\n. could use a comment here about how we don't want to kick off an auth request if one is in progress\n. could potentially use a comment here about how we want to force refresh the token, even if one already exists (it might be expired).\n. single expression doesn't need braces\n. TIOLI:\nsys.env.get(secretKey).flatMap { secret =>\n  Api.readJson[MarathonSecret](Buf.Utf8(secret)).toOption\n}.collect {\n  case MarathonSecret(...) => Api.AuthRequest(...)\n}\n. might want to call this authTokenState or something similar.\n. minor typo\n. what's the purpose of testing this 10000 times?  is this reliable? \n. On stream id overflow, you send a GOAWAY?  Not a stream reset?\n. is this TODO complete?\n. what happens if the stream id overflows but the CAS fails?\n. is it intentional that this is a double?\n. Integer overflow kicks in before stream id overflow does:\n```\nscala> val max = math.pow(2,31)-1\nmax: Double = 2.147483647E9\nscala> val id = new AtomicInteger(max.toInt)\nid: java.util.concurrent.atomic.AtomicInteger = 2147483647\nscala> id.getAndAdd(2) > max\nres10: Boolean = false\nscala> id.getAndAdd(2) > max\nres11: Boolean = false\nscala> id.getAndAdd(2) > max\nres12: Boolean = false\nscala> id.getAndAdd(2) > max\nres13: Boolean = false\nscala> id.get\nres14: Int = -2147483641\n```\n. why lazy?\n. missing\n. use i.b.t.FunSuite?\n. it does go pending.  see refreshToken\n. TIOLI alternative: you could keep the AtomicInt and then check \nif (id > MaxStreamId || id < 0)\n. just curious why?\n. ugh\n. This should have been NameInterpreter.global; I've updated.  We set NameInterpreter.global so this will be able to use configured namers.\n. I don't think this description is quite right.  This option will just turn on filtering at the service discovery layer such that we only consider endpoints that marathon considers healthy.  Failure accrual still applies to the remaining endpoints as normal.  This is an important distinction because a service can be alive and responding to health checks, but still return errors when servicing requests.\n. marathonHealth makes me think that this variable is an indication of marathon health.  Maybe rename to checkMarathonHealth or useMarathonHealth or something?\n. style nit:\nhealthCheckResults.forall { \n  case HealthCheckResult(Some(alive)) if (alive) => true\n  case _ => false\n}\nor even\nhealthCheckResults.forAll(_ == HealthCheckResult(Some(true))\n. This is nice.  Better than my filter suggestion.\nStyle nit: don't need the parens around the if predicate, I don't think.\n. I'd be interested to hear what other people think but I don't think we need to talk about failure accrual here.  F.A. isn't more aggressive than marathon health checking, it's just a different thing.\n. What about something like:\nIf true, exclude app instances that are failing Marathon health checks.  Even if false, linkerd's built-in resiliency algorithms will still apply.\n. Maybe note that these namers don't have to be included in the namers: list in your config file.  Since they are on the classpath and referenced with the /$/ syntax, they will be automatically available.\n. I think setting a labelSelector on this EndpointsNamer is going to cause problems.  The only name we ever bind with this namer is the hardcoded name on the next line /ns/port/service which does not include a label value.\n. nit: don't need braces for simple interpolated variables.  s\"$label=$labelValue\"\n. the label should be part of the id, right?  so should this just be id instead of id.take?\n. the id may or may not contain a label so I think you need to do id.take(3) here\n. TIOTLI golf: PrefixLen + labelName.size\n. since this is duplicated below you could probably factor it out into a toNameTree method\n. should path.drop(PrefixLen) just be residual?\n. ditto re interpolation variables\n. residual?\n. It looks like we're just always using ttl.head which will only use the first value of the stream over and over.  I think we probably want to modify loop to accept a stream and sleep based on the head of the stream and pass the tail of the stream into the recursive call to loop.\n. I think you could simplify this to something like Stream.continually(ttl + (scala.util.Random.nextDouble() * 2 - 1) * jitter)\n. feel free to make this package private if you want to be able to test it (private[marathon]). since this is a val a value will be computed once and then used for every position in the stream.  I think this should be a def so that each position is recomputed.. Would probably also be useful to grab a handful (maybe a few hundred?) of items off the stream and test that they're not all the same, that they all lie between 250 and 350, etc.. Less invasive would be to just mark as @JsonIgnore. can you explain this commented out value?. imo this is a pretty confusing scala behavior that this is two promises instead of two references to the same promise.  consider splitting onto two lines for clarity. this is taken care of by the after test harness, isn't it?. should extend Throwable for clarity and consistency with Error above.. typo. is $onEnd going to try to toString a Future here?. is this a debug handler?  should this be deleted, or add a comment explaining?. how about using a named parameter?\n, eos = true). empty doc string. tioli: transport.updateWindow(streamId, _). alex-helloworld@ def foo(x: Int, y: Int, z: Int) = x + y + z\ndefined function foo\nalex-helloworld@ foo(1,2, z = 3)\nres4: Int = 6. rm or switch to log. rm or switch to log. rm or switch to log. Possible to DRY up usages of this block?. Consider just making this an object instead of a package object?  When I see references to ctx without an import it's not immediately obvious how that got into scope.. Is this right?  Doesn't let already take care of saving and restoring?  Isn't the thread local referred to by Local.save actually different from Contexts.local?. This provides path level request stats right?  Can this now be removed?. You'd have to capitalize Ctx.  \ud83d\ude1c \nplus package objects are just weird.. what about moving them into a directory AND importing DstPath and DstBound directly?  Just trying to follow the principle of least surprise.. pro-tip:  make this a var and default it to None and then you don't need to include it in any of the subclasses.. is this to specifically disallow an empty list of headers?  ie Trailers(). This type seemed odd to me so I dug into FailureAccrualFactory to figure out why.  It turns out that the FailureAccrualPolicy isn't just a policy but also keeps the failure accrual state, which is why you would want to be able to create new ones on demand, which explains this type.  \u2604\ufe0f :star:. is sr too cryptic for a config param?  do you think successRate would be too verbose?. wild. if you wanted to offer the full suite of ergonomics you could also define def apply(): Trailers = apply(Nil). finagle now calls .hashCode on ids which means that using null for an id is a no-no.. Yep.  Now updated to 2.8.4.. I've factored this common code into the DelegatingNameTreeTransformer companion object and renamed variables for clarity.\nHere we are extracting the transformed id from the transformed Name.Bound.  \nbound.id should be equal to prefix ++ orig.id.  . oxford comma. Don't need to make this a case class, and you can call it NoneConfig for consistency.. Capitalize file name to match class name.. I get that U is for unary and S is for stream... but what is plat?. consider moving this inside the mk method so that the client is created when the telemeter is created instead of when the config is parsed.. Do you know if the thread-safety of ConcurrentHashMap is preserved if you .asScala it?. I'm confused about the purposes of these random labels.  If these are generated for each request (as the spec suggests), how can they be correlated for tracing or debug purposes?  How is this useful?. This makes sense for Forwarded by, but does it make sense for Forwarded for?. I wrote a big long comment about how I didn't understand what this was for, and in the process of writing that I figured it out.  (the values for forwarded for and forwarded by need to be computed here since they potentially depend on linkerd related values such as router label.  those values can then be passed down to the http router filter module that actually appends the headers)\nSo anyway, this is probably non-obvious enough to need some comments.. What about just Math.floor(ms/this.msInYear)?. per https://github.com/BuoyantIO/linkerd/pull/914 this is going away. this is a pretty old-school dtab\nwhat about just /http/*/* => /#/io.l5d.fs?. I think we can omit config options like httpAccessLog, label, and dstPrefix that aren't really relevant to demonstrating statsd. @JsonIgnore. @JsonIgnore. all-a dees, @JsonIgnore. it looks like : are replaced by _ below.  Not sure if this covers all invalid characters.  Maybe a whitelist is better than a blacklist?. were these variable names autogenerated by something?  that's probably fine for the unimplemented methods but for the ones where we provide implementations here it might be helpful to have real variable names.. Consider moving this to its own file because its big and boilerplatey and it breaks up the flow.. it's actually the labelers that are computed once per connection, right?  This is what allows ObfuscatedRandom.PerRequest to work.. I got really excited and then really disappointed. I don't think you meant to move this here.. unused?. think its worth adding a test for ipv6 addresses?. ah, I see.  Pretty confusing that it's in hostport.scala though.  How about giving it its own file?. Good suggestions, thanks!. Added scaladoc.  Lmk if you think this needs more explanation beyond that.. How about recentRequests?. Slightly refactored and added comments.. Good call.  Moved to top level.. Fixed per @leozc's suggestion above.. Gonna punt on this for now.. My thinking is that if the trace context is loaded from the request, it's at that point that the span ID should be advanced.  If that has already happened by this point we don't want to advance the span ID again.  If it hasn't happened yet then the trace ID generated here will simply be overwritten by the trace loaded from the request.. Well, unless TTwitter is being used.  But otherwise, yeah.. TIOLI: could create a Config trait that defines this var and have each Config class extend it.  That would reduce the boilerplate a bit.. Whoa!  Are annotations inherited?  If this works, that's pretty awesome.\nThere are many more config classes (e.g. RouterConfig and many others) and we should probably be consistent about having them all inherit this.. should this scaladoc be on TlsTrustModule?. This module could use some scaladoc, I think.. Consider moving this below finagleModule and add a comment indicating that this is an alternative to finagleModule that disables the finagle tls params.. could this extend TlsTrustModule?. is this used?. damn you, scalariform. I'm actually surprised this works because as far as I can tell, annotations are not inherited.  But it does seem to work and is a great reduction in boilerplate.  Must be some jackson magic going on.  \ud83d\ude80 . It might be nice to explicitly call out certain data that will NOT be collected (labels, dtabs, service addresses, request data of any kind, etc.). Perhaps add a note saying that the exact data that will be transmitted can be viewed on the admin site.. This should be a single identifier used for the whole company/organization, right?  Maybe something like \"Optional string of your choosing that identifies your organization\".. Not sure if this is the right place but somewhere we should probably stress that this is opt-in and no data will be transmitted until this telemeter is added to the config.. I think the calculation of this pid should be moved up to UsageDataTelemeter so that it's not recalculated for each message.. I think you're missing character classes before the kleenes.\ns\"\"\"^.*/srv/.*/requests$$\"\"\".r maybe?. I think this extractor pulls out capture groups from the regex, but there aren't any.. more concise style \nmetrics.collect {\n  case (RequestPattern(key), v) => Counter(...)\n}. style nit:\n``\nconfig.routers.map { r =>\n  ...\n}. You can use MediaType.json. some scaladoc would be nice on this. explicit return type on public members, please. can we also include a \"dryRun\" parameter that will let users see what data we would send but without actually sending anything. I think you can just doparams[LinkerConfigStackParam].config. might be more idiomatic to do:Name.bound(Address(\"130.211.174.30\", 80)). I think UsageDataTelemeter could use an e2e test where you spin up a Service and receive data from it.. I think the pfx is supposed to be before the host:port segment.. This is somewhat similar to other pfx-namers so consistency suggests making this a pfx-namer as well.  But I agree they're somewhat awkward and having both /$ namers and /# namers is perhaps unnecessary complexity.  I wouldn't be opposed to converting all of our /$ namers into configured namers in the future (perhaps in 0.9.0). I have no attachment to the namedryRun` if we can think of a clearer name.  Given that usage data is sent so infrequently, it shouldn't have any real performance impact.. Even though we're not 100% on how we feel about this namer, it should be safe for production use from a performance point of view.  I would lean towards not marking it as experimental.. Just wondering aloud if we should name this in a way to indicate it does not reflect all event states.  LossyEventStream?  EventuallyConsistentEventStream?. This is a pretty unfortunate area of the code.  Any time that you're converting a NameTree to a DelegateTree, the path property is meaningless.  We could set it to Path.empty all the time but DelegateTrees are actually used to render the namerd json API (uggggh) and we need to set path to null to make sure it doesn't show up in the json output.  \nWhat about changing the method signature to:\ndef fromNameTree[T](names: NameTree[T], path: Path = Path.empty): DelegateTree[T]\nand most callers can leave path unspecified.  Only the json api handlers need to explicitly pass null in.. ahh, good catch.  thanks for fixing!. style: consider if-else instead of match for a simple boolean.. nit: space before type. This sealed trait only has one subtype.  Are we planning to add more RecvState states?  Or should we just rename Buffer to RecvState and ditch the sealed trait?. I don't understand this part.  If this frame isn't yet releasable, we return a Func that, when invoked, allows this Frame to be released?. msgSz might be clearer. can we grab a slice of the original bb?. can we grab a slice of size h0.size instead of duplicating the buffer?. Could there be multiple messages per frame?  Should this recurse?. Oh, I get it.  The doRelease func returned here is associated with the previous message.  That message must be released before we can release this frame.  This stuff is hard to get my head around.. I know duplicate doesn't copy the bytes (I presume) but maybe the slice api makes managing limits easier or make the code more readable?  Not sure.. s/dream/stream. what does rb stand for?  Receive buffer?. Ah, yeah.  I don't see the benefit of reading the next message's header in advance.. This seems like a reasonable stopgap fix.  I've ticketed a more long term solution here: https://github.com/linkerd/linkerd/issues/972. should this be sealed?. This trait and it's implementations are unused?. both these methods need docs. If I'm following this correctly, I think this can be broken into it's two codepaths and put directly into segmentReleaseFunc and lastSegmentReleaseFunc.  I think the canReleaseUnderyling variable is unnecessary . { () => synchronized { \n  if (pending < 1) throw ...\n  pending -= 1\n  Future.Unit\n} }`.\n{ () => synchronized { \n  if (pending < 1) throw ...\n  pending -= 1\n  if (pending == 0 ) underlying() else Future.Unit\n} }\n``. Ok, I think I get it now. It started making more sense when I started renamingcanReleaseUnderlyingtoallSegmentsConsumedin my head.  I keep thinking that there's some subtle race condition here but I thiiiiiiiiiiiiiiiiink the synchronization protects you from everything I can think of.. I generally prefer boolean options to be stated positively instead of negatively.  What do you think about calling thisuseNodeAddressor reversing it (default to true) and calling itpreferServiceAddress?. Thoughts onpreferServiceAddress?. it seems like 0s display as \"-\".  I think a 0 should display as \"0\".. I think I'm going to revert this change to the way the default telemeter is handled.  My motivation was that it's really easy to mistakenly not configure a telemeter that provides/admin/metrics.json` in which case the admin dashboard does not work.  In some sense, the admin dashboard should REQUIRE a telemeter that can provide that endpoint.  But I think what I have here is too weird.  \nI'll revert this part and think on it some more.. Even if it is pulled in, it shouldn't be used... right?. sort of... it's the io.l5d.header identifier with \"host\" as the header.  I think I'll just make \"host\" the default header for the io.l5d.header identifier which makes this much easier to explain.. wait nevermind, this is just the io.l5d.header.token identifier.. prior to sorting here, are the routers in an arbitrary order, or the order they appear in the config file?  Preserving the order from the config file might be nice.. The tiniest of nitpicky bike sheds: I think in the mocks the corners are rounded.. The other namerd iface plugins don't have namerd in the kind name.  I think we should be consistent.. We should retry the request on a stream failure.  Is that what your TODO alludes to?. can you explain why we shouldn't release right after we've read the value?. these methods all follow the same pattern.  is it possible to DRY this up?. Ah, interesting.  This means that the window size must be big enough for 2 values or else you'll get stuck.  Is the window size really meant to represent the full amount of memory holding messages?  Note that memory usage is only 2 messages worth for the short duration from when the message is received to when it is stored in the state.  Should the window size perhaps not include this temporary memory use?. I think to do this without adding the templates and js to the admin project, you'll need to add them to the usage project and also add handlers to serve them.. TIOLI: We could display on here whether or not dryRun is enabled.  That is, whether data is actually being sent to Buoyant or not.\nTIOLI, probably out of scope: We could even make dryRun mutable and settable from here.  That way, after seeing the payload, users can turn on sending data with a flick of a switch.. or maybe, more generally, that it only works for namers that support this enumeration.. To make sure I understand: this means that any frames that are released will immediately trigger a window update?  Whereas a ratio of 0.95f would mean that releasing frames will not trigger a window update until 5% of the window size has been released?. these tests weren't even being run because I forgot to update the build file hahahaha whoooooooops. should probably add the io.l5d.commonMetrics telemeter here as well so that the admin dashboard works. Can you add a link to the github of the file this was forked from?. does this work if the admin page is served at a non-root url?. the allocation of the PF?  How would you feel about this if the PFs were val'd up?. or the allocation of the Option?. This will get the current time when the client is created which isn't exactly the same as the linkerd start time (though maybe it's close enough?).  What about using the jvm uptime metric?  \nAlso: would we rather send startTime or uptime in the payload?  I guess one can always derive the other if the message timestamp is known.. I think this conflicts with https://github.com/linkerd/linkerd/pull/1047.\nSorry!!! \ud83d\ude22 . Let's go with toLong to be safe.. whoooops, thank you.. I think what you want is to pass a statsReceiver into the constructor here.  In NamerdInterpreterInitializer's newInterpreter you can extract the stats receiver from the params and pass it in.  At that point you can also scope it to indicate that it's for the namerd interpreter.. I believe these load balancer metrics are gauges also. done!  great suggestion, thanks!. oops. Calling this Param is a bit confusing with Stack.Param.  What about ClearContextEnabled or something?. Is this the right place to be clearing the trace context?  Has server receive already been recorded at this point?. We only include the module if the param is enabled, and then check the param again in the module itself.  Isn't this redundant?. extra space. Do we want to do a more blanket clear of l5d-ctx-* or even l5d-* headers?. I guess if there's precedent then I'm okay with it.  It just made me do a double take when reading.. We don't want to accept trace headers but we probably do want to, for example, record the remote client address.. what's the difference between this and a single valued enum?. \"replicas\" is a new piece of terminology we don't use elsewhere.  Likewise, in the existing namerd API, \"resolve\" means to go from a logical name directly to an address set.  The existing nomenclature isn't sacred but we should be deliberate about terminology.. If the Var becomes unobserved, the prior message will never be released, even if the Var becomes observed again.  I think this is fine because when the Var is observed again a new stream will be created.  Will the old stream be torn down and collected if it has unreleased frames on it?. typo \"ACtivity\". Endpoint or Replica?. I'm still not thrilled about explicitly listing the metadata fields in the proto and am still in favor of an untyped String->String map.. rm if not needed. rm if not used. or add a TODO that retries will be added in a followup. ditto. Here is how this updated terminology would affect our current nomenclature.\n\n. The telemetry section is a list of telemeter configs.  Having a list where there's a default item always included but that default item can be replaced if you add an item to the list that is the same kind... is pretty weird and complex.  I think having usage as a separate thing is cleaner.. what is costly?  the reflexive calls?  these are actually used \"in the serving path\" but that path is only exercised once per hour (or when requested from the admin endpoint). insert better styling here. yeah, that last assert checks that l5d-dtab-ctx is absent.. I think this could use a commend explaining why we are replacing the host in the response headers.. style nit: map on the same line\nsvc(req).map { rsp =>. this is elegant but I think it might be less allocate-y and easier to read as:\nfor {\n  header <- headers\n  value <- rsp.headerMap.get(header)\n} {\n  rsp.headerMap.set(header, value.replace(host, originalHost))\n}. style nit: this can only be None so I prefer to be explicit about that:\ncase None =>. I think this can be greatly simplified if we make namespace mandatory (or optional with a default value) instead of allowing the watching of all namespaces.  If we do this I think we no longer need to subclass Ns or keep a 2-level cache.  The result should be very very similar to K8sDtabStore.  \nRather than exposing an Activity[Map[NamespacedName, VarUp[IngressSpec]]] we could simply expose an Activity[IngressSpec].. I would ask if clearing the URI is part of the ingress controller spec, except that I'm guessing the ingress controller spec is not a thing that exists.  Does the golang ingress controller clear the uri like this?. Does this check that the namespace of the ingress resource is the same as the namespace that we use to fetch the ingress resource?  Isn't that always true?. In the absence of a real ingress control spec, I guess we should just treat the golang implementation as the spec.  Does that do prefix matching?. I prefer that we don't treat URI paths as finagle Paths except for where we have to.. uncaughtExceptionsFilter is outside of the map.  payloadSize being inside is cargo-cult from finagle https://github.com/twitter/finagle/blob/695694ea710bda551ed3da7702a2bd5776ce4e20/finagle-thrift/src/main/scala/com/twitter/finagle/thrift/ThriftServerPreparer.scala\nI'm not sure if there's a good reason but I feel like mirroring finagle is safer.  . I think this class specifically deals with a namescaped API so I don't think it makes sense to make namespace optional here.. a fatal error. should we set the state here?. We should either set the state on exceptions or change the signature of this to S => Option[T]. I think we want retries on the client itself in addition to retries within the stream processing.  That way if the initial request fails, we can retry it.. @olix0r: this API has been removed.  what was the intention behind setting the decoder's local settings?  does removing this cause any problems?. whoa, I didn't know about corresponds.  TIL.  I think startsWith is probably simpler in this case, though.. We already optionally specify the namespace when making the API call so the list of ingresses should already be exactly the ones we want to consider (the ones from a particular namespace if specified, or all namespaces otherwise).  Therefore I don't think we need to consider namespace here.  . TIOLI: consider moving the path matching logic into a method on the IngressPath class:\ndef matches(hostHeader: Option[String], requestPath: String): Boolean = ???\nand then you can define a getMatchingPath method on the IngressSpec class and then finally this method can simply become something like\ningresses\n  .toIterator // to make this lazy so we can stop after we find a match\n  .flatMap(_.getMatchingPath(...)) // see if there's a match\n  .take(1) // take the first match\n  .toSeq.headOption // convert back to option. I think those first parameters default to None so we can omit them.. more idiomatic is to use the syntactic sugar for update which is state() = Activity.Failed(e). style nit:\n.flatMap(mkItem). I think we can get the resource version out of the initial ingressList and pass it in here.. TIOLI: => ingresses ++ mkItem(a). TIOLI:\nval item = mkItem(m)\nval withItemRemoved = ingresses.filterNot(i => item.exists(isNameEqual(i, _)))\nwithItemRemoved ++ item. should this be called mkIngress?. ingress.spec.map { spec =>. if you use { instead of ( here, you don't need semicolons below. each time this is called, an observation is opened on the Activity until it produces a non-pending value, which then populates the Future, and closes the observation.  The result of this is that for each call to matchPath the Activity will be started and then stopped.  \nSince we want identification to be fast, I think we should just hold the Activity open forever.  One way to do this would be to change the ingresses val to a lazy val and also hold open an observation:\nprivate[this] lazy val ingresses: Activity[Seq[IngressSpec]] = {\n  val act = Activity(state)\n  val _ = act.states.respond(_ => ()) // register a listener forever to keep the Activity open\n  act\n}. FWIW, this says that the path is actually a regex.  We don't necessary have to support that, but it's worth being aware of.. I think this should use com.twitter.finagle.buoyant.h2.Headers.Authority as the header namer. flatMap and match can be collapsed:\nmatchingPath.flatMap {\n  case None => \n  case Some(path) =>\n}. probably good practice to pass params into mkClient.. req.host. I think if no host or path is specified, the rule should match everything.. for performance reasons we may want to compile this regex and store it as a val on the class.. I don't think this should be removed.. This is identical to the ThriftTraceInitializer except for the type parameters.  Could you parameterize the ThriftTraceInitializer on Request and Response type and use that instead?. As of 0.9.0 we no longer should set this.. I think we want to be able to configure all the same options that available on the thrift protocol config. e.g. buffered vs framed, binary vs compact, attemptTTwitterUpgrade etc.. I think we want to include the client and server prep modules from the thrift protocol so that ttwitter can be used.. let's add override val experimentalRequired = true to mark this protocol as experimental for now.. would be a different sha depending on os and arch so I didn't bother.. I think this should be an Option[String] throughout and only add the header if the Option is a Some.. I think this config requires the experimental property to be set. I don't think this example routes correctly, but it looks like neither does the thrift.yaml example.  Perhaps we can just fix both of them in a followup review.. Tiny style nit: we usually prefer service(reqBytes).map { over service(reqBytes) map {. here too. Another option would be to call .toLowerCase before Path.read in Api.toAppIds to make sure all ids in appsActivity are lowercase.  Then we can convert path to lowercase by doing \nval Path.Utf8(segs@_*) = path\nval lowercasePath = Path.Utf8(segs.map(_.toLowerCase): _*)\nthis way we can avoid relying on path.show which is probably unreliable for equality. You can update these two lines to:\nval service = (MarathonSecret.load(), sys.env.get(MarathonSecret.basicEnvKey)) match {\n  case (Some(secret), _) =>\nThis way secret is a String instead of an Option[String] and you don't need to call .get below.. Good question.  A transformation object has 2 children: obj.tree which is the result of the transformation and obj.bound which is the bound address pre-transformation.  We show both so that we can see the effect of the transformation.  \nIn the case where the transformation results in a failure we want to color the whole path red, even if the pre-transformed value is valid.  That's what this does.  Without it, the pre-transformed value would be green while the rest of the path would be red.. I hesitate because returning Neg here doesn't necessarily mean something is wrong.  For example, you could imagine building a fallback dtab that attempts to route to a daemonset but falls back to something else if no matching daemonset pods exist.  In that case we'd log on every request even though the requests get successfully routed to the fallback.. should this be h2?. I... don't know.  I guess I assumed that the client would close the connection after receiving the response.  linkerd returns a 1.1 response and there's no reason why it couldn't accept another request on the same connection if the client were to send one.  But I don't know what the spec dictates in this case.... I deleted this. We squash when we merge the PR anyway, but having the commits pre-squashed gives the contributor better control of the commit message.. what does this mean?. whoooooooops. fyi this is deprecated in the next finagle release.  Should just do Response() to build a response instead.. This will allocate a function for each call to labelExists.  Not sure if there's a super great way to avoid this, best I could come up with is:\nval first = ((String, String)) => String = _._1\nprivate[this] def labelExists(labels: Seq[(String, String)], name: String) = labels.toIterator.map(first).contains(name)\nUnfortunately that's not super readable, but I THINK it saves on allocation which might matter depending on how often this endpoint is going to be scraped.. Seq().mkString(\",\") produces an empty string so I don't think you have to special case when the seq is empty.. can val this function up (see below).. How about flatMap here and return just Seq[(String, String)] below instead of Seq[Option[(String, String)]]?  Then you can get rid of BOTH calls to flatten!. Don't need this. I don't think you need this brace for a single expression. Oh.  The problem is that the method return type is Unit but append returns a stringbuilder.  You can do val _ = sb.append(\"\\n\") to explicitly discard the returned stringbuilder.. This works in the repl for me val first: ((String, String)) => String = _._1. .flatMap {. does this write the host header of the request to the metrics endpoint as a tag?  influx is weird.. TIOLI: I like arrow notation for pairs to avoid the double parens.  Total TIOLI though.\nSeq(\"host\" -> host). Could use arrow notation here to avoid the double parens if you wanted.. I think conflicting kinds between response classifiers for different protocols is going to be a problem at deserialization time because they are all subtypes of ResponseClassifierConfig.  So I think we want to keep this check.. It doesn't.  Timeouts are not requeuable but they may be retryable.  . Yeah?  I don't think we talk about stacks or modules anywhere else in this documentation.. Oh I see what you mean.  Retries are above timeouts in the client stack.  So timeouts don't get requeued but if a requeue happens, the requeue gets a new timeout.. yeah that it gets the classifier from the local context. These are the values for the captured path segments from the prefix matcher.  E.g. if the prefix is /svc/{service} and the path is /svc/foo then this map is populated with \"service\" -> \"foo\".  None of the service configs use this yet, but I included it to be consistent with client configs.. to cut through finagle's insanely restrictive visibility . We could also potentially strip the l5d-err, l5d-success-class, and l5d-retryable headers off the response here.  What do you think?. Does this fix https://github.com/linkerd/linkerd/issues/1203 ?. what?. Instead of adding an extra layer of retrying here, would it be possible to add ConnectionFailedException to the RetryPolicy in BaseApi.infiniteRetryFilter so that it will retry on ConnectionFailedExceptions instead of giving up?. Thanks!  Even though it might not be strictly necessary, I think we're going to mirror the scala version that finagle uses for now.. hmmmm... strange.  The only thing I can think of is that RetryFilter will skip any Failures that are flagged f.isFlagged(FailureFlags.NonRetryable) without consulting the retry policy.  Maybe the connection failed exceptions are flagged this way?  Not sure why they would be though..... Eta-expansion of zero-argument method values is deprecated.\nIt works if we replace usages of retryIn with retryIn _ but I figured just val'ing up the function to begin with would be easier to understand.. It returns Assertion which is just a marker trait.. This is Retries.moduleRequeueable which is the requeues module, included by the stack client by default.  . Maybe add a note here and in the scaladoc for the multins lookup method about the optional label segment.. what do you think about using the latest tag so that we don't have to update this for each version bump?. Would triple quotes make this easier to read?  I'm not sure what's going on here.. I can't even.. Why 4 backslashes instead of 2?  Is there another layer of escaping that I'm missing?. omg it's backslashes all the way down. Whoops!  Sorry about that.  Fixed.. Might be worth mentioning that tracing doesn't apply to namerd and any tracers provided by these telemeters will not be used.. might be able to avoid allocating a new IntUnaryOperator here by doing something like defining \ndef addClosedId(id: Int): Unit = {\n  val i = closedId.get\n  val max = if (id > i) id else i\n  if (!closedId.compareAndSet(i, max)) addClosedId(id()\n}\nwhich, I think, is pretty similar to what updateAndGet does anyway, based on the scaladoc.. wouldn't this cause t1 to try again with id = 4 (and keep trying until successful)?. silences the deprecation warning\n. Not easily, I don't think.  We'd need to open up the key ourselves and figure out the format.. I think we should probably just make a clean break in a future major release where we drop support for netty3 and require all keys to be pkcs#8. whooops. If this is a super hot method that we're trying to optimize, you could try getting rid of the partial function allocation and just iterate over the seq and build the result.  . Sorry, I should have been more clear.  I think we can probably do this without allocating any functions by doing it purely imperatively (eg no higher order functions).  But I would only bother doing that for the hottest of codepaths.. Did anything change in this method other than taking the ns, port, and service names from method parameters instead of extracting them from the id?  The diff is a bit tough to read because it looks like some cases got re-ordered.. TLS is enabled for this branch. This is in finagle-h2 which doesn't depend on linkerd-core.  I'm open to refactoring this up in the future if it's used elsewhere.. default is clients expire after being idle for 10 minutes.  but it's configurable.. I believe this config format is only used by the namerd thrift interpreter.  Why move it here?. Isn't this a breaking change?  I think we'd like to standardize on TlsClientConfig as the proper way to specify client tls configuration.  It's somewhat unfortunate that the namerd thrift interpreter uses a different format but we're stuck with it until a breaking release I think.. It looks like sslServerEngine is just passed through.  Could it just be added as a param at the call site instead of being wired through this method?. This comment is specific to LegacyKeyServerEngineFactory and should be moved to the place where the LegacyKeyServerEngineFactory is specified.. Just to confirm my understanding: this is a breaking change of something that was already broken, right?  Namerd's http interface never supported TLS on the server side?  . rm. Does this work for the mesh interface?  I would have thought that Some(Seq(ApplicationProtocolNames.HTTP_2)) would need to be specified for the alpnProtocols instead of None.. whoops, I think this should have been ALPN all along: https://en.wikipedia.org/wiki/Application-Layer_Protocol_Negotiation. I'm not sure if this method needs to be added to Request itself.  I think you can accomplish this by simply doing something like\nval req1 = req0.dup()\nreq1.headers.set(...). Yeah... I don't love it either.  \"clnt\" is the scope that finagle uses for its ClientStatsRecevier so there is some precedent for this abbreviation.  My main motivation was to distinguish stats for clients that we explicitly create (e.g. for talking to the k8s api etc) from stats for clients that are provisioned by linkerd routers.  ie \"client\" has a specific meaning in the context of linkerd and I don't want to conflate that with the one-off finagle clients created by namers and transformers.\nAdvice appreciated.. I'd replace \ntry { find(...).map(...).get } catch { ... throw ... } \nwith \nmapMessages.collectFirst {\n  case mp if msg.endsWith(mp.name) => ...\n}.getOrElse(throw new IllegalArgumentException(...)). TIOLI: could use collectFirst instead of find -> map. I thought there was some reason why I deliberately avoided logging to stdout here... But I now can't imagine why that would be a problem.  So this is probably fine.. wfm. I agree with your in-person remark that we can DRY up all these clients.  Let's get them merged first and refactor second.. I think this can just be\nprivate[this] val routeRules: Activity[Map[String, RouteRule]] = api.watchRouteRules(pollInterval).map(mkRouteMap)\nalthough you'd lose the logging.... I think this can just be\nrouteList.collect {\n  case RouteRuleConfig(...) => name -> spec\n}. You might be able to leave off the type annotation here. Or even make this a lazy val and do the observation here.. We're going to need the RouteManager in the identifier AND in the interpreter.  What do you think about having an apply method on the companion to create RouteManagers that caches based on the URL for the client or something like that?  That way when the identifier and the interpreter both try to create a RouteManager, only one will be created and they can share it.. Should we fail the request if no routes match?  Or forward the request to it's stated destination (I think this is what istio does) ?  I was thinking we could use /svc/dest/<hostname>/<port> in the case where no rules match.. maybe add a TODO comment here about checking more route conditions. This seems like the right thing to do for now.  Is there a ticket for figuring out buffer management in the long term?. TIOLI: could use com.twitter.util.Memoize instead of a Var.  Or even just a mutable Map.. IstioNamers still can be configured directly if you wanted to use one standalone, but for the IstioInterpreter it has an IstioNamer that it uses internally that is not configured directly.  This is similar to how in a linkerd config that uses namerd, namers won't be configured directly in the linkerd config because the namers are \"internal\" to namerd.\nWe pass the istioNamer in here so we can pass it to the ConfiguredDtabNamer.. stream and req are unused I think.. Might just use Logger() here since io.l5d.istio could refer to a number of different things. 8082?. In the future we may want to send this to the egress linkerd instead of directly to the internet.. I think scala gets annoyed if you don't put in the Unit return type and the =.. TIOLI: naming this something like headerMatches would make it a bit more clear to me that it returns true for a match and false if it does not match.. It's too bad we can't compile and store the regex on the StringMatch object.  This is not actionable, I'm just complaining.. TIOLI: what about matchesAllConditions or matchesCondition?. could do:\nmatchCondition.httpHeaders.forall {\n  ...\n}\ninstead of an explicit fold. rm println before shipping please. could also do\nreq.headerMap.get(headerName) match {\n  case Some(headerValue) => headerMatches(headerValue, stringMatch)\n  case None => false\n}\n(.get.map.getOrElse is also fine, just throwing this out there.  go with whatever seems clearer to you). could do match instead of .map.getOrElse here as well if you think it improves clarity.. style nit: avoid infix notation\nconfigs.map(_.mk()).reduce(_ andThen _). I think there's additional information we want to pass in here such as request duration, dst, and possibly other params from the endpoint stack.  We could add all of those things as parameters to this method or we could move the body of this method into the istio logger so it has direct access to that information.. probably want to add params: Stack.Params as a parameter to mk so that the filter has access to the stack params (like dst, and endpointAddr, for example).. if you make this take a Try[Response] instead of an Option[Response] then we'll have access to the exception in the case of a failure. . I think we want this to be somewhere in the endpoint stack so that is has access to the endpoint addr.  How about \n.insertAfter(ClientDestTracingFilter.Role, HttpLoggerConfig.module). TIOLI: if (rr.route == null) { ... } else { ... } might be more clear? not sure.. I'm pretty sure filter composition is associative so this doesn't really matter but it's more common to do A.andThen(B).andThen(C) instead of A.andThen(B.andThen(C)). I think you need to merge master.. I'm mildly worried about this being flaky.  Back of the napkin math suggests the expected number of distinct elements is about 95 so this is probably ok, but to be ultra-conservative I'd be tempted to only check that there's more than 1 distinct value.  What do you think?. Looks great!. this is probably fine but it might be more technically correct for this to be a def so that it remains correct if the backoff var changes.. rm this comment please. For consistency with our other tests you should be able to omit the annotation and extend io.buoyant.test.FunSuite.. shouldBe is cool but I'd recommend assert(...isInstanceOf[...]) to be consistent with other tests. Since I haven't used scalacheck much, can you briefly explain what the difference between for { n <- Gen.choose(1, Integer.MAX_VALUE) } yield n and simply Gen.choose(1, Integer.MAX_VALUE) is?. I think this is a property with an assertion in it.  I think we should either iterate over kinds and assert for each one or remove the assertion and just have this be a property.. Wait nevermind, is this forAll from TableDrivenPropertyChecks or from org.scalacheck.Prop?. does this test actually fail when one of these cases fails?  or does it succeed and produce a sequence of failed Outcomes?. Doesn't this just define a property?  Don't we have to check it as well?  And assert that the check was successful?. Use assertThows or intercept for consistency. how about checking that probeDelays.drop(4) only contains the Somes. For my education: what's the advantage of table based checks rather than iterating over a collection and asserting?. Does the aws api always live at this ip address?  I think this should be configurable.. Let's make this configurable.. GET is default, so I think we can omit this.. We always want to avoid blocking the thread with Await.result.  Instead, I think you'll need to write a new transformer (perhaps it's possible to subclass SubnetLocalTransformer) that turns this Future into an Activity (using Activity.future) and uses that.. Ideally the parser would spit out an immutable config, but, between jackson and inheritance, and polymorphism, that's not the world we live in.\nGiven that backoff has to be a var (sad face) I really think this should be a def.  Otherwise a stray early access of this field could lock it to an incorrect value.  In general, lazy vals in scala are kind of dangerous and hard to reason about so I like to avoid them except as a last resort.. I think we can drop Matchers and OutcomeOf now. Can drop extra parens. So Table + forAll is essentially Seq + foreach but with nicer test output?  Unclear whether this is worth the cognitive overhead of doing things differently than other tests in the repo.. Lets drop Matchers from this file too. based on my updated understanding of egress, I think we should delete this dentry.  /svc/ext names are generated when the host header doesn't match any clusters (even \"external\" ones).  These requests should fail.. rm empty line please. certs.map { please. we should be able to omit the explicit type annotation on the inner certs. fold makes sense conceptually here but the fact that StreamIO.copy is side-effecting rather than pure means that this ends up being a bit more awkward than simply iterating over the trustStreams.. Add a brief comment explaining why this test is commented out.. is this method useful?. type parameter should be able to be inferred here. type parameter should be able to be inferred here. This gets hit for every request so maybe trace level is more appropriate. I think we usually do Logger(). code in finagle for this kind of thing usually uses com.twitter.util.Stopwatch.  Not sure what the difference is.... maybe stopwatch is more accurate?. maybe something like\nret.toOption.map(_.statusCode).getOrElse(Status.InternalServerError). this is an exception from the target service, right? not from istio?  probably shouldn't warn on that. A bit worried about this being brittle.  I think this suffix might actually be configurable in kubernetes.  I'd be in favor blindly trusting what's in that path segment.. could also do e.startsWith. perfect opportunity for flatMap here:\nDstBoundCtx.current.flatMap { bound =>. Let's defer the default value of \"unknown\" until later.  Instead, we can make this an Option[String] by using istioPath.map.  This will pay off shortly.... ...and the payoff!  targetService.flatMap(_.split('.').headOption). we can make this an Option too. here's where we can .getOrElse(unknown) on all our Option[String] values. I think this field is supposed to be the value of the \"app\" label of the target pod.  We populate it with the name of the target service.  This is good enough for now because (by convention) folks will usually name these the same.  But we should keep in mind (and probably add a comment) that we should use the correct value once we're able to get label values (or uids).. I think our updated logic should be:\n\nIf matching route exists: /svc/route (send to target service)\nIf cluster exists but no matching route: /svc/dest (send to target service; fallback to istio-egress)\nIf cluster does not exist: /svc/ext (send to istio-egress). @esbie do you think we should change this line to send /svc/ext names to the egress linkerd?  I think, as is, this will send directly from the daemonset to the outside world.. Isn't still called the manager as of the 1.6 release?  And should we use the fully qualified name in case linkerd is deployed in a different namespace from istio?. Your fear is probably more accurate than my fear.. I think L5D_SERVICE_HOST is the vip of the L5D kubernetes service which balances over all linkerd pods.  I think you want to get the node name from the downward API and use that.. A bunch of similar defaults were just moved into https://github.com/linkerd/linkerd/pull/1458/files#diff-cc9d18fb542f89cb0c372bf6de429a58\n\nWe should do the same for consistency.. Yes, you're right, what you suggest is much better.. Could we pass in the request itself rather than a meta structure?. Could you use request.path for this?. Actually, what does this do?  A comment would be helpful.. Oh, now I see, this is to bridge the gap between H1 and H2.  A comment on IstioRequestMeta explaining that would be helpful.. A case class return type here or a comment explaining that this is (uri, authority) would be helpful.  Potentially also a comment explaining that we can't modify the uri and authority directly in this method because it is shared between H1 and H2.. should we validate that uri starts with pfx?. This is probably similar to pathFromUri but I don't understand it.. We should probably change the name of this method now that it's no longer just returning the name.. I don't think this information is available on the HTTP request (unless it's in the request.uri).  We may have to punt on this.. Not a big deal but I realized after the fact that com.twitter.finagle.stack.nilStack exists and we could probably use that instead.. Replace will replace all occurrences, I think we just want to replace the first (ie the prefix):\n```\nscala> \"hotshots\".replace(\"hots\", \"nots\")\nres1: String = notsnots\nscala> \"hotshots\".replaceFirst(\"hots\", \"nots\")\nres2: String = notshots\n``. Not sure if the istio spec spells out what this behavior should be if the match type is prefix but the prefix doesn't match.  I would expect the uri to be left unmodified, rather than replaced.  . I think this can move even higher in the path stack.  Since the request is invalid, we want to reject it as quickly as possible so that we don't waste resources processing it.  I would say you could just prepend it here to put it at the top of the stack.  Or maybe right below ErrorResponder just in case this filter ever throws an exception (even though it shouldn't).. Doing upfront initialization of the Logger is probably fine and lets us avoid the evillazykeyword.. I think this will fail requests with multiple content length headers that all have the same value.  If I'm reading the spec right, I think that should be valid.. PreferFuture.value(resp). By convention we usually usereasonas the name of the String for stuff like this.  We can also passreasonintoExcpetion(...)to populate the exception's message.. avoid infix notation. The meaning of \"downstream\" is controversial.  We could say something more unambiguous like \"Received conflictingContent-Lengthheaders from remote server.\". Since this deals with invalid responses, I think we want this deep in the client stack so that we can skip response processing.. Thinking about it some more... we might actually want to return a 503 response directly here instead of returning an exception.  That way the classified retries module can decide if the 503 is retryable or not (based on the request verb, for example) rather than us returning an exception that we must explicitly mark as retryable or not.  What do you think, @olix0r?. I don't think that works because we want the configured response classifier to make the determination of if the request is retryable.  For example, a GET that results in malframed response might be retryable but a POST that results in a malframed response might not be.. rm blank line please. I think this also should be after the RequestStats module so that these malframed responses show up as failures there and also below FailureAccrual so that malframed responses contribute to failure accrual.. Do you know if this is above or below the StatsFilter?  We should make sure this is below both the StatusCodeStatsFilter and the StatsFilter. rm blank line. for clarity I'd move this up above the flatMap since this exception can only come from filterMessage.  I'd also switch this to use handle instead of rescue.. I think it would be helpful for this error message to indicate that it's a Request that is malframed (as opposed to a Response). This exception is always the same so we can store it in a val to avoid allocating a new one each time.. ah yeah, good point. It's probably not a big deal.  The response that linkerd responds will have a different status code depending on if it's a malframed request or response.. typo: ServerFIlter. I was following the pattern from here: https://github.com/twitter/finagle/blob/72cac55fecfd13daf00a4cfed4dad11b926baa8d/finagle-core/src/main/scala/com/twitter/finagle/service/RetryPolicy.scala#L169. If I'm understanding correctly, streams should be removed fromstreamswhen they are closed.  So I think this can just beclosed.get || streams.isEmpty.. I'm torn about this.  Conceptually, I agree with you.  However, since some of the most widely used clients have this bad behavior, I worry about linkerd's logs just getting clogged with these messages..closedIdis the highest closed stream id.  If we encounter a stream id that's not in our map, it might not be there because the stream was closed and removed from the map, or it might not be there because the stream hasn't been created yet.  closedId helps us differentiate those cases.  I think.. Unless you feel strongly I'd prefer to leave this as is, just because it's annoying to test.. Should we wait until the GoAway is written before closing?  ie flatMap these together?  I don't really know this part of the spec.... Instead can we just importcom.twitter.finagle.buoyant.ParamsMaybeWithwhere ever it's used?  It shouldn't be THAT many places.. Let's move this as well so it all stays together.. I think the comments on lines 2-5 are not really relevant to this example and can be removed.. Let's remove this identifier to simplify the config. lets remove this router to simplify the config. lets remove this router as well. for my sanity, lets make these two vals eager rather than lazy. alphasort imports please. I don't _think_ this is needed to enable implicit classes. alphasort imports please. This is more than just precondiditons now, how about calling it something likeIstioIdentifierBase?. This always returns a Future.exception so we get to use one of my favorite types as the return type!Future[Nothing]. I think this can just be a trait or abstract class with no type param or constructor args.  The child class will contain the response anyway and we don't have useful information about the type at this level.. This lead my down a bit of a rabbit hole trying to figure out how Monitors work but I think we can have the ResponseException extendcom.twitter.logging.HasLogLevel` and set the log level to TRACE or something like that.. TIOLI: could move all the non-config related code here into a separate object for a cleaner separation of concerns and (possibly more importantly) so that we don't need @JsonIgnore on everything.. What does \"withClose\" mean in the name of this val?. This isn't in the hot path so allocations of a few strings won't matter.  Begone, fowl lazy keyword!. should this be removed?. should this be removed?. might split this onto two lines for readability:\nval stream = result match {\n}\nval _ = stream.respond. We can just scope the StatsReceiver before passing it into StreamStats.. not super obvious what these names are so I'd use the parameter names:\ndurationName = Some(\"...\"), \nsuccessName = Some(\"...\"),. I'd switch the order here and scope \"stream\" under \"request\" or \"response\".. I'd consider not scoping this under \"response\" and calling the duration \"request_latency\".  It's a less accurate name but it's more consistent with other protocols and will make play more nicely with the admin dashboard.. protip: statsReceiver.scope(\"request\", \"stream\"). description should reference thrift instead of http. We probably want to have this serve on the same IP as the admin service (or make the ip configurable). TIOLI: I think there's a setting that makes Jackson not serialize fields with null values.  It might be nice to omit the \"error\" field if there's no error.. Keep in mind this will create a second identifier.  This is fine but we should be aware of this and try to make sure identifiers don't create expensive resources.. Be careful of what? If the underlying classifier is partial, this should be partial as well, right?. We probably want this to be total and mark everything else as nonretryable failure.. scala golf: .respond(onFrame). This should extend AnyVal to be a value class. s/not unlike/like. the final Frame. We might actually want to make this one the default..... what is the difference between onEnd and apply?. Oh, I think I see, you override apply in the subclass.\nI would keep this all in apply and then in the subclass you can override it and call out to super.apply.\nOR you can embrace \"composition over inheritance\" and have FrameStats contain a StreamStats instead of extending it.. This class level object is pretty confusing.  I think we can get rid of it and move its contents up into the class directly.. Personally I don't love this mixin pattern.  Would prefer to pass arguments into FrameStats' constructor directly.. I think we should count trailers towards frame count too.. We have two StreamProxies, one here and then one in rspStreamStats.  Can we move this logic into FrameStats' onFrame?. I think we also need to handle the case where the stream isEmpty separately.  I think it should add(0) to the frameCount stat and should call the classifier with None as the final frame.. Or, alternatively, move the logic from FrameStats out to here.. it's pretty confusing that this has both an apply that takes a stream and an apply that takes a Try.  I think this can be clearer/simpler.. scala golf: frame.release().before {. The issue with putting it on the existing admin port, I think, is that it would have to be served at a particular URL.  But the URL can factor into identification.. \ud83d\udeb2 \ud83c\udfe1 \nWhat about calling this ExceptionsAsFailures. :bike: \ud83c\udfe1 \nWhat about calling this ServerErrorsAsFailures (this mirrors its H1 counterpart).. TIOLI this might be clearer as \n{\n  case H2ReqRep(_, Return((response, Return(_)))) if response.status.code >= 500 => ResponseClass.NonRetryableFailure\n}.orElse(ExceptionsAsFailures). These \"classic\" finagle ResponseClassifiers don't take into account the streams and can't be used with any of the observability and retry logic we're building for H2.  We should delete this or potentially fold it into the file above.. I think this should be Option[Try[Frame]] instead of Try[Option[Frame]] because the emptiness of a Stream can be determined safely.. I don't really follow what this comment means.  StreamStats is really just a collection of metrics and a convenience method for updating them.. We evaluate the Stopwatch immediately in the body of this method so we might as well just pass a Duration in instead.  The only aspect of result that we use is whether it's defined so this can simply be a Boolean.  I'd simplify this to:\ndef apply(streamDuration: Duration, success: Boolean)\nor\ndef success(streamDuration: Duration)\ndef failure(streamDuration: Duration). We have access to startT here, I don't think we need to partially apply streamStats above.. potentially racy.  perhaps use an AtomicLong for streamFrameBytes. I think we could simplify by moving the contents of StreamStats into this class, deleting StreamStats, and renaming this to StreamStats.. Are you able to put together a standalone repro of this bug?. This is weird.  The signature of the method doesn't exactly match what we want to express.  At this point we know the second argument will be discarded so it's confusing that we pass in an irrelevant values (we might as well be passing null).. that would surprise me.  if there's something improper happening here we should get a repro of it.. Yeah, they're settable via the H2 config and they feed into Finagle's retries and metrics filters.  As part of this change we want to rip all of that out and replace it with our stream aware versions.. I think this change allows you to delete a bunch of .toString calls on port later in this file.. I don't think you need to sign and date comments, git blame has your back. probably would want it to return Try.  You never want callers to be able to return Activity.State.Pending here or Bad Things will happen.. is this even possible with retryIndefiniately = true?. This is not in a hot path so the goal should be to maximize clarity not minimize allocations.  . intellij formatter: r u ok?. Since this is H2 specific, we can refine the type in RetryableResult.unapply to (Response, Option[Try[Frame]]).  We also want to return true if the final frame is a retryableThrow.. I think we want to classify (_, Return((_, Throw(_)))) as a failure too.  eg when we get a 200 OK but the response stream fails.. I think this is the same as NonRetryableServerFailures. Why this type alias?. This represents the number of bytes of data in the whole stream right? not in a single frame?  I think the name is misleading.. maybe call this onFinalFrame or something. I can't find where Stream.empty(Int) is defined.. scala golf: response.map(_._1). I think the return type here should be Try[FinalFrame] (ie Try[Option[Try[Frame]]]). at which point you can scalagolf this to response.map(_._2). (Response, FinalFrame)?. I think extending ReqRep is dangerous.  We really don't want an H2ReqRep to be used in any finagle code that is expecting a ReqRep because the semantics are different.  Finagle's response classifiers fire when the response Future is complete and the rep position is given the response.  If the classifier were an H2ReqRep it would be expecting a RepAndFrame in the rep position and would never match.  . Because of this, I think we still need our own H2ResponseClassifier type (or h2.ResponseClassifier or whatever).  . I don't think we want this method anyway.. I'm not sure if these convenience methods actually add much convenience.. I still find it a bit icky that this signature admits invalid inputs:\napply(r, Throw(e), Some(f)) // f is specified as the final frame but is unused. I think we gotta bring this back. Any progress on this?  I'd like to remove this (and related) comment if it's not a real thing.  Or fix it if it is.. I'd explicitly name onFinalFrame since it's sort of an optional parameter:\nrspStreamStats(rspT, onFinalFrame = classify(Return(response)))\nThis helps clarify that reqStreamStats is the same thing but doesn't provide any onFinalFrame callback. In addition to this setter, I think you also need to define a getter with the same JsonProperty annotation so that this can be serialized correctly.. This is a nice hack to get around the fact that the response classifier is a different type in H2.  Let's add a brief comment here explaining why we have to treat responseClassifier differently than the other properties in this class.. cool but unrelated to the rest of this PR.  would you mind splitting it out?. could even throw an exception here so that this fails loudly if this is ever called. I think we can uncomment this, right?  This should mark all things as successful as long as both the response and stream are Returns.. For most dynamically loaded plugins we list all the plugin types in Linkerd.scala in linkerd-core.  However, we can't do that for this one since it's H2 specific and we don't want linked-core to depend on linkerd-protocol-h2.  The easiest thing to do for now is to explicitly list all subclasses here with a @JsonSubTypes annotation.  This is what we do for LoadBalancerConfig, for example.. revert please. rm extra newline. I believe that jackson will happily parse a config that is missing these properties and fill them with nulls.  We typically add an assert to make sure they're non-null.. I think getResponseClassifier is fine.  We're outside the idiom because of this little hack.  As long as it has the right JsonProperty annotation so that it serializes correctly.. sure. rm or uncomment. ah yeah, ResponseClassifiers.Default. For Option, jackson will fill in a None.  . 10:52:54 ~/w/linkerd (alex/h2-retry \u26a1$\u2621)$ ag -c \"Future.Unit\" | wc -l\n      78\n10:52:59 ~/w/linkerd (alex/h2-retry \u26a1$\u2621)$ ag -c \"Future.Done\" | wc -l\n       8\nFuture.Unit seems to be preferred.\nAlso no need for braces on this line. I like this. ty, scalariform. I think we want to replace the ResponseClassifier.Setter module with this one.\n. should this be \"Report request statistics using local H2 stream classifier\". Should this be StatsFilter.role?  Or possibly LocalClassifierStatsFilter.role?  (the choice is academic, they have the same value). isNull checks to see if the StatsReceiver is the NullStatsReceiver.  In that case there's no point adding the StatsFilter so we just skip it.. I don't think we want to pass an addition Try[Unit] in here to capture the potential exception.  Even if we did, Option[Throwable] is probably a better type than Try[Unit].  See below for suggestions on how to eliminate this.. I believe an empty Stream always has onEnd satisfied.  I think we can just immediately and synchronously call onFinalFrame(None).. e should be purely derivable from rsp and frame.\n// something like...\ndef failed[T](t: Try[T]): Option[Throwable] = t match { case Return(_) => None; case Throw(e) => Some(e) }\nval e = failed(rsp).orElse(frame.flatMap(failed)).getOrElse(SSF.SE)\n. buffer capacity is never reclaimed.  If a child releases a (RefCounted) frame while the frame is still in the buffer, the underlying frame is not released because the buffer holds a reference to it.  If the buffer has been discarded there is no way to recover the beginning of stream so there's no point putting anything in the buffer.  Once it's gone, it's gone.. cool. it looks like frame is unused here.  ie if the response class is written into the trailers, we never check it.  https://github.com/linkerd/linkerd/pull/1545 would actually make this easier, allowing us to classify based on the response class header if it exists, and then based on the response class trailer, if it exists.. https://github.com/linkerd/linkerd/pull/1545 would probably help with this too.  If we can early classify the response, then we should write the response class into a header.  If not, we should wait until the final frame, classify, and write the response class into the final frame if it's a trailers.\nThe one case that doesn't get covered is if we need the final frame for classification but it's not a Trailers frame.  It's a pretty weird case and I'm okay with punting on it.. This doesn't work.  By the time we get to the final frame, the response has already been transmitted so mutating the headers here will have no effect.. or flatMap even.  But I dont know if we should be adding Trailers frames to responses that don't have them to begin with. keep in mind that classifiers are partial functions.  in this case I think we just fail to classify. I think so.  The responseClassifier only looks at the request and response.  The stream classifier looks at the final frame as well, which may or may not exist.  Technically, once the response is complete we'll be able to tell if the stream is empty and could fold the empty stream case into early classification but... conceptually it feels cleaner to handle the empty stream classification as part of stream classification.  By doing this, responseClassifiers never have to look at response.stream at all.. We just need to make sure the responseClassClassifier can handle this value being absent.  And likewise we need to make sure everything that uses H2Classifiers can handle the case where the PF is undefined.  We may want a \"unclassified\" stats counter in addition to \"success\" and \"failures\".. probably. NonRetryableStream went away.  We can do retries on streams!  Or we will be able to soon.. The difference between H2ReqRep and H2ReqRepFrame is subtle and the two are easily confused.  I think being explicit about which classifier you're using helps with clarity.. ReqRep isn't type parameterized.  Given that we don't want to reuse any of finagle's classification machinery, better to have our own type.. This lets us be more specific with the types and I think we want to explicitly prevent ourselves from accidentally using any of the finagle classification machinery.  There are semantic differences that are subtle.  For example, finagle might classify a 200 response as successful whereas we want the responseClassifier for a 200 response to be undefined so that we can wait and see if the response stream fails.. This seemingly fixed a flaky test failure:\nThrow(Reset.Closed) did not equal Throw(Reset.Cancel) (RouterEndToEndTest.scala:159)\nBut I don't really understand why this was here in the first place or how it could have non-deterministically caused that failure.. Don't we want to wait and make sure the stream doesn't Throw also?. I think this should never match, right?  An H2ReqRepFrame is never an H2ReqRep.. Oh, I just saw the custom extractor you added to H2ReqRep.  You're gonna have to sell me on this change.  It seems very misleading.. I don't think we want this orElse.  If the response is a 5XX with no l5d-response-class header, we want to responseClassifer to be undefined so that we defer to streamClassification and check for a l5d-response-class trailer.  Instead, this will immediately fall back to the Default responseClassifier and mark the response as a failure.\nI'm considering removing orElse since it never seems to be the thing we actually want to do.. this can throw a NumberFormatException.  Probably want to wrap it in a Try.. can probably DRY up this header parsing code.. Adding a ResponseSuccessClass extractor for Response would let you avoid this .get. shouldn't this just be if (classifier.responseClassifier.isDefinedAt(...)) ?. dont want to do a getOrElse here, we can just check above that the pf is defined here.. add te: trailers header.. in this case we want to add a Trailers frame to the stream.  To do this we probably need something like\nclass StreamFlatMap(underlying: Stream, f: Frame => Seq[Frame]) extends Stream { \n  val q = new QueueOfSomeKind\n  def read(): Future[Frame] = {\n    if (q.nonEmpty) Future.value(q.pop)\n    else underlying.read().map(f).map { fs => q.push(fs) ; q.pop }\n  }\n}. this will match all responses, regardless of if the stream is empty or not.\nif the stream is empty, this should be undefined and we should call the stream classifier with Return(req, Return((rep, None))) which should fall into the _ => ResponseClass.Success case. theoretically, if the responseClassifier is defined for this H2ReqRep, we would never even call the streamClassifier.... See above, the responseClassifier should be undefined in this case.. Response is not a Headers so I don't see how case H2ReqRep(_, Return(ResponseSuccessClass(c))) could match.  But yeah, if you add an extractor for Response, I think it would work.. This file is identical to FailureAccrualFactory except for the addition of classifyResponse and classifyStream and a modified apply method.. Maybe?  Currently it's a json string, but I'm open to suggestions.  . is that initialStreamWindowBytes?  TODO would be using the current window size. is mutable.Queue threadsafe?. I think this method probably needs to be synchronized. rm. This case should get caught by the responseClassifier... but I guess there's no harm in having it here too. it feels conceptually cleaner to me to handle this case as part of the stream classifier.  That's the reason why H2ReqRepFrame contains an Option.. weird formatting. this comment still stands. s/spurious/invalid. }.getOrElse. Frame is not an instance of Message so I don't think this will ever match.. can store the lifted function as a val on the class to reduce allocations. I think we want f to be of type Frame => Seq[Frame].  In the case where we fail when reading a Frame off of the underlying stream, I think we want to also fail the resulting stream.  \nTo put this in context: if we read part of a response stream and then hit an error, I think we want to transmit that error to the caller (or retry the whole request).  I don't think we want to tack a Trailers frame onto the end of the incomplete stream.. I think we just want to pass the error along (see above comment on flatMap). I think we need to add the \"te: trailers\" header here.. We need to handle the case where rep.stream.isEmpty here. Let's add some scaladoc here including the restrictions on f (namely that it has to give back the input frame in the output Seq or take ownership of releasing it). I'm launching a background thread to make some changes to FAF to make it possible for us to subclass, but that will take a while to land in finagle and be released.  Hopefully this whole-cloth duplication is a temporary measure.. Nope.  The FAF tests are quite a bit more extensive but obviously don't cover the stream classifier aspect.. I think this would clobber state since we can read theirs but not update it.  . personally I find match easier to read and understand than .map.getOrElse.  . thank you! will update.. r... really?? \ud83d\ude31 . because the classifier comes from the request local context.  In other places we memoize this keyed off the service Path with the assumption that the classifier is static for each service but.... we're gonna have to rip that all out when we violate that assumption with dynamic service policy.. Instead of splitting factory into a val, I changed the Service into a named inner class.  Let me know if you think this improves readability.. I'm on the fence about extractors like this.  Code like\nframe match {\n  case GrpcStatus(status) =>\n  case _ =>\n}\nwould be surprising to me because frame is not a GrpcStatus.  Or maybe more to the point, a method GrpcStatus.apply(status: GrpcStatus): Frame wouldn't make sense, so inverting it to an unapply maybe doesn't make sense either.. formatting nit: I prefer:\nfor {\n  headerValue <- ...\n  code <- ...\n  status = ...\n} yield status\nor even\nfor {\n  headerValue <- ...\n  code <- ...\n} yield GrpcStatus(...). We can classify Throws here as non-retryable failures (there will be no stream).. rm extra newline. I think we need to add a _ case that leads to Non-retryable failure.  This covers the cases where the stream is empty and where the stream is a throw and where the stream doesn't have a grpc status.. ditto for other classifiers here. It's kinda nice that this refines the return type of mk down to GrpcClassifier but this ends up adding a lot of boilerplate.  I think its fine to just have the individual grpc classifier configs extend H2ClassifierConfig directly.. errant file got included here. and errant rename of H2ClassifiersTest. is this needed?. flatMappppppppped. s/release/eventually release. Might reword this slightly to indicate that this returns a new stream which is backed by this stream with f applied on it.  imo \"inserted\" isn't really correct.. I'm a little unclear on the semantics of isEmpty but I'm pretty sure that this should be isEmpty iff the underlying isEmpty (regardless of q).. unrelated, but we can rm this comment. I think if the response is a throw we can classify as nonretryable failure.. I think this case should get caught in early classification (but I guess there's no harm in having it here too). I think we need a catchall case that goes to non-retryable failure.. I think we need to add te: trailers to the headers here.  At this point we're committed to adding a trailers frame (if one doesn't already exist).. want to set this on headers, not trailers.. rm. You can get rid of the JsonSubTypes annotation. oh yeah, that's true.  I forgot that these aren't plugins.. I think you accidentally deleted this file.. Should we log.error when we get a null key or value?. TIOLI:\nif (status.message != null && status.message.nonEmpty) {\n  Seq(... both items ...)\n} else {\n  Seq(... status only...)\n}\nAvoids smelly var and might be fewer allocations at the expense of a small amount of code duplication.\n. agree that this is confusing but we should be consistent with the existing semantics.  If you pass a non-empty queue to Stream.empty(q), isEmpty is true.. TIOLI: you could use 1 fewer map here if you want\nread().map { frame => \n  q.enqueue(f(frame): _*). weird space. It's worth considering if we can just have a Activity[Seq[PathParams]].  The static and global configs could simply return an Activity.value(...) that never changes.  . This can put the Activity back into a pending state if the file is deleted.  Instead, I think we probably want to either go into a failed state or use the default SvcPrefixConfig or something.  . not a dtab. probably need this for all other protocols as well, I think. This could probably use a bunch of scaladoc.. whenever the Activity updates, we also need to call close() on the previous ServiceFactory.. We also want to close the current ServiceFactory as well as closing the observation.. I think we can just make this non-lazy and remove the hashCode() call.. We might want some stats in here.  Frequency of updates probably?  Not sure what else.... I think we need this for other protocols as well. I think the body of this case needs to be in a synchronized block. I think we want to move the dedup up to the dynamicParams so that we don't build a new stack unnecessarily . in the error case we probably still want to close the previous sf.. I think we want to combine the closables with Closable.all or something. or maybe closable.sequence... we may want to close the obs first so that we don't create any new SFs before closing the current one.. Good point.  I don't really want to get into the nitty-gritty so I removed this bit.. I don't think that will work the way this is written.  If the params activity goes to error, then the sf activity will go to error and the toFuture will return an exception.  If we get into this state we've lost the previous sf anyway so we should probably close it.\nWe might be able to solve this at the FsSvcConfig layer by not updating the activity if the file cannot be parsed, or something like that.. Since this is intended to be mixed into tests, we should use a scalatest primitive such as assert for output instead of println. (This comment applies to all println statements in this change). Are these test names backwards?  This test has many ingresses configured, right?. should we also test the case where there are multiple ingresses with the linkerd class configured?  Or is that tested elsewhere?. I think these last 4 bullets are rarely relevant.  I think we can ask for these if we need them.. I think having Cache in the name here is misleading.  Would prefer to call this Svc or KubeService or something. would call this lookupPort to be consistent with lookupNumberedPort.. prefer type annotation hint instead of cast:\n} yield Address.Inet(isa, nodeName.map(Metadata.nodeName -> _).toMap): Address. might be more useful to keep these as ConcurrentHashMaps instead of asScalaing them so we get CHM's atomic methods. can use CHM.putIfAbsent here to avoid race conditions. ditto. I think you can DRY this up a little bit by factoring out methods logAddtions, logDeletions, and logModifications.  . perhaps a bit more concise: newEndpoints -- endpoints. this should probably be higher than debug. why protected specifically?  Seems like this could be either public or private.. I think putIfAbsent returns the value.  so no need for this line. I'd call this just endpoints or endpointsAct or something.  The fact that the activity is cached is an implementation detail.. I think the destructuring here doesn't need the double-parens.. don't need double-parens. should this be private?. similar comments as below: can probably clean and/or DRY this up by factoring out the logging.  also don't nee double-parens here.. TIOLI: use com.twitter.util.Memoize instead of HashMap.. as far as I can tell, this isn't used.. Or ServiceEndpoints. I think if we move ServiceNamer off of Ns, then we can delete this file entirely.. I think this class needs to be refactored to be very similar to the EndpointsNamer.  That is, it should use NsApi.service(serviceName).activity instead of ServiceCache.  Ideally, ServiceCache can be deleted entirely.. I'm not a huge fan of providing utility methods as mixins like this.  I'd prefer to put these utility methods in a static object or on the package object.. weird indentation. closing paren on its own line please. this seems like maybe intellij got some weird IDEAs in it's head about formatting (see what I did there?). No need to have it in the superclass if it's not used there, though.. oh, yup.  I misread.. It might be easier to this all at once.  I'll leave it up to you.. Inheritance is a powerful tool that we don't actually need here.  stabilize is just a static function.  This is the first step down a dark path that eventually leads to the cake pattern.. unused import. shouldn't this be intersect?. I think you can use .keys on the RHS instead of .keySet when subtracting from a map.. (here and throughout this file). isn't this just \nkeptPorts.filter { p => newPorts.get(p) != ports.get(p) }. Lots of changes to the logging logic in this branch.  Do you have any ideas about the best way to test these changes?. Maybe creating a StringLogger and using Logger.withLoggers in the test and validating the log output. I'm intentionally not reviewing this file for now, as it will change.. uncomment or rm. whoops, apparently I can't read.  sorry.  what about:\nval newCounter = new Metric.counter(...)\nval counter = ...putIfAbsent(..., newCounter)\nif (counter != null) counter else newCounter\nor something like that. well, we're already doing the potentially unnecessary allocation anyway: putIfAbsent takes arguments by value, not by name.  To avoid this we could switch to computeIfAbsent but that's a bit more of a pain.  Unclear if the optimization is worth it.\nThe thing that I care about here is atomicity.  As written, the value in the hashmap could change after putIfAbsent is called but before get is called.. rm this annotation. rm. I would sleep better at night if this used flatMap and Activity.excpetion instead of throw.. dope. use Future instead of Promise. Need some scaladoc here explaining what this is.. TIOLI: I think we might save a tiny bit of boilerplate by extending ServiceFactoryProxy. This took us a while to figure out so it's definitely worth explaining in a comment.. This is not technically a closable.  I might call it closeLeaf or something.  Would also make it private[this]. I think to avoid rebuilding ServiceFactories we have to dedup the params.  ie\nval dedupParams = ... // dedup dynamicParams\nval sf = dedupParams.map { .... I think it makes sense to handle this in the same block as we are building the stacks.  We probably still need a (maybe nop) observation to keep the activity open.. I didn't notice this until now but it looks like the number of format variables is mismatched here.... ditto. oldMap.keys (and similar throughout this file). You were right: I do think this is over-engineered.  I love typeclasses as much as the next person but I think it's overkill here given that:\n There are only two implementations of Loggable\n EventLogger is package private\nI think it would be simpler and easier to understand to simply have 2 overloaded versions of addition (and each other method) that take the concrete types (Endpoint and PortMap).  \nSorry.. Is there a purpose for this method?  Could callers just call servicesMemo instead?. Might be more idiomatic to use rep.status and Status.NotFound.. is this related to the rest of the PR?. Future.selectIndex runs both Futures in parallel and returns the index of the one that completes first.  The first Future is scheduled to return at the deadline.  So if we hit the deadline first, we go to the first case and if we read the final frame before hitting the deadline, we go the second case.  Does that help?. Peut-\u00eatre \"without\" au lieu de \"sans\". It looks like you're expecting the path to have exactly 2 segments here.  I think this should call .take(1) before the match and then match that one segment.. I think you should also pass the residual into this method path.drop(1).  (see related comment below). Which part of this method is expected to throw exceptions?  It would be nice to scope the Try block more narrowly if possible. I think this method should take the path residual as a parameter as well.. I don't think this log is correct. It's debatable if you want NameTree.Neg here or a leaf with Addr.Bound containing an empty set.  The former will consider the name invalid and will fall back to any alternatives in the NameTree.  The later will be considered a valid name but any requests will fail because there are no addresses to send to.. you want to set the residual as the path of the Name.Bound here. I think this should be a Throw(...) instead of a Return(NameTree.Fail). move imports to top of file, please. I think this would be a better black-box test if you use namer.lookup instead.. If you use an empty address set then half of the time you'll pick the first branch of the union and fail the request and half of the time you'll pick the second branch of the union and use myservice2\nIf you use NameTree.Neg then you'll skip to the myservice2 every time.. It can be useful if you want to artificially halt evaluation of a name tree, but it's rarely used in practice.. if you import io.buoyant.namer.RichActivity then you can do await(namer.lookup(...).toFuture). does passing=false mean filter to only non-passing nodes? or skip filtering?. rm reference to any. rm any. Probably worth commenting that this is NOT thread-safe and must be externally synchronized (as it is in fork::read). Now that we have to pass child in here, I don't think this really accomplishes any separation of concerns.  Since the flow of the program bounces from Fork to here and then back to Fork, I think this is actually more confusing.. I wonder if the istio identifiers should be in a separate project somewhere?\nistio-identifiers depends on [\n  * linkerd-protocol-h2\n  * linkerd-protocol-http\n  * istio\n]\nSomething like that?. I'm worried about this being quite brittle but... I can't really see a better way. Perhaps remove the message equality check and make the log more generic?  \"HTTP/2 router could not handle non-HTTP/2 request!\"?  WDYT?. :+1:. if I'm being reallllllly nitpicky, a long-running stream should never accept new messages, only new frames.  :neckbeard: \nWe could also probably use less technical terminology: \"... eventually hang.\". I think that since the netty3 implementation has gone away, we can simply remove Thrift.ThriftImpl.Netty4 entirely.  I don't think we want to replace it with Thrift.client.params. (unless there's something I'm missing?). I think this can just be Future.value(Response()). see above.  I think we can just remove this.. what is this line needed for?. This will create an observation on the activity that will last forever and hold the activity open.  I don't think we want that.. rm?. bound.addr is the Var whose identity you want to make sure is preserved.. I don't think this belongs on the config class.  This was added for testing purposes I assume?  Could you instead extend ConfigMapInterpreterConfig in your test and override mkClient?. Just my 2c but I think this extractor makes this code harder to follow rather than easier.  I would just keep those cases inline in the match above, even if it is a tiny amount of duplication.. TIOLI: I think you could leave the existing signature the same if you changed this to\nTry(Dtab.read(data)).getOrElse(Dtab.empty)\nor\ntry { Dtab.read(data) } catch { case _: IllegalArgumentException =>\n  log.warning(...)\n  Dtab.empty\n}. Doesn't this create a new client for every time api is referenced?. Scala: where the only thing more insane than the problems are the solutions.\nobject TestConfigMapInterpreterConfig extends {\n  override val api = v1.Api(service).withNamespace(\"test\")\n} with ConfigMapInterpreterConfig(\n  None, None, Some(\"test\"), \"test-config\", \"test.dtab\"\n) {. These little corners of the language are so disgusting yet fascinating.  . I think extractors are useful when it's extremely obvious from context or naming what they do.  In this case, I don't think it's immediately obvious and I end up jumping around the code to figure out what's going on.  Instead, how about factoring the common match arm bodies into a method and calling that method from both arms?  This avoids duplication while also making it clear that there are two match possibilities that both map to the same behavior.. This is mostly unrelated to this change but it just now occurs to me that in large clusters these are potentially expensive set operations.  Might be good to factor this so that the set operations happen inside of a ifTrace block.. (here and elsewhere). Is this just a cosmetic/style change?. unused?. duplicated from elsewhere, and effectively unused?. Prefer to explicitly list the last case instead of wildcarding it. This looks scary but the Activity de-dupping that Linkerd does means that the NameTree won't get rebuilt on addr changes.\nI'm just saying this to reassure myself.  . Yeah . Can we check e.status.code for 410 instead of inspecting the message?. Is it true that this k8s bug has no adverse effects now that we're detecting \"too old\" events with status code 200?  If so, I don't think we need to log.  (we will continue to log at trace level that a watch was restarted). I think that looking for a code is actually more future proof than looking at a human readable string.  Plus it's already the case that we look for the HTTP status code 410.. I suspect this is pretty cheap, actually.  The objects have already been parsed, after all.  And we will never be able to assume that all users have upgrade off of the affected K8s version.  The best we can do is eventually drop support for certain k8s versions.. This feels a bit verbose.  How do you feel about something like ApiClusterCache?. As far as I can tell, nothing there depends on the current object's state.  Could these methods just be static method instead?  (ie turn this trait into an object)\nI tend to shy away from the mixin pattern when possible.. All of the information this trait provides is derived from the DstBoundCtx and not from any underlying request or response object.  So it seems a bit misleading to attach this to IstioRequest and IstioResponse objects.  It is also confusing that the vals on this trait are computed from the DstBoundCtx and saved at constructor time.  This means that the exact position in the stack where this object is constructed will determine this values, which is pretty non-intuitive.  \nI would try to keep IstioDataFlow separate from the Request/Response objects and try to make it clear that constructing an IstioDataFlow takes a snapshot of the current DstBoundCtx.. Values from the IstioDataFlow trait are derived from the local context so response.targetService could just as easily be request.targetService or even new IstioDataFlow {}.targetService. Any particular reason why IstioRequest is parameterized on the underlying request type and contains the underlying request but IstioResponse is not parameterized and does not include the underlying response?. Name feels a bit verbose.. rm blank line between imports. what is the root prefix?. At this point the DstBoundCtx context has not been populated so attempting to call any of the methods from IstioDataFlow on this IstioRequest will fail.. I assume this is factored this way for test-ability.  \nInstantiating a ClusterCacheBackedByApi with a test client that returns mock data would test more of the codepath than implementing a test ClusterCache.  But it would also be a lot less ergonomic.  I just wanted to bring it up so we can consider the tradeoff.. values is immediately evaluated here regardless of log level so I don't think we benefit at all from passing by name.  Ideally we would avoid evaluating unless the log level was high enough.. To make matters worse, we actually evaluate values twice, incurring the cost of the set operations twice.. This should be an Option[Seq[TagWeight]] since this is coming from a json list and duplicates will be eliminated anyway when this is transformed into a map.. please alphabetize imports and remove blank lines separating them. this is not typically how we align parameters.  these should be indented one level from the line with the def.  take a look at the surrounding code for examples.. I think this test needs to be updated to use Seq instead of Set.. should we close this PR then?. I think this is always toString (either explicitly with _.toString or by default with the default formatter.  Can we just remove it?. I don't think we ever partially apply this?  Combine into one argument list?. I don't really understand the naming convention here.  Why mk?. any change here or just a move?. in the case of a straight up deletion of the whole object, I don't think we need to list all elements of the object that was deleted.. we may not need this?. what about on object creation?  (ie where old.nonEmpty)  shouldn't we log then?. ditto. unzip is cute but this is probably more readable by building up :scream: mutable sets.. We're paying a price here for being more general than is necessary.  There are a very small number of nouns (service, endpoint, portMapping?).  Instead, you could have functions like val logEndpointAdded: Endpoint => Unit defined on the trait instead of being allocated on each newState call.  I think the goal should be to not allocate any functions in the logging path; not because allocating functions is necessarily that expensive, but because the result will be more straightforward code.. I still think building these sets mutably will help readability and performance. if this is supposed to be fire-and-forget, maybe the return type of this method should just be Unit?. I have a lot of opinions about testing that we can discuss at great length sometime in the future.  But I think this is fine.. I don't have a super strong opinion about naming.  I have a mild preference for a more concise name.  Using package namespace to differentiate would also be fine.. rename file to IstioServices.scala?. I think this is equivalent to Logger() which is more common. I like the way this has been factored.  Feels much more explicit now :+1:. TIOLI: this feels more like a dependency and less like something that needs to be implemented.  Could make this an abstract class instead of a trait and have MixerClient be a constructor argument.. What about something like: \n```\nsealed abstract class IstioAttributeT\nsealed abstract class StringAttribute(val name: String, val description: String, val value: String) extends IstioAttributeString\n...\ncase class SourceIpIstioAttribute(val value: String) extends StringAttribute(\"source.ip\", \"blah blah\", value)\n...\n. With the change to IstioAttributes I suggest above, this could be something like:\nval allStringAttributes = attributes.collect { case attr@StringAttribute(_) => attr }\n``. typo: ixer'. Instead of get and getOrElse, I think we can chain all these options together with flatMap and then convert to a Future with Future.const. we also have to call stream.release(). I think aStream.Releasableis an item from the stream, not the stream itself.. I think we should passOption[IstioPath]` in here so that callers are explicit about when they are pulling the local context.. This means that code in the identifiers that construct an HttpIstioRequest can explicitly pass None for IstioPath. it looks like this class is supposed to be type agnostic but this assumes that all mappings are port mappings?. is this ever anything other than toString?. could combine the first two argument lists of logAction if you write this as\nprivate val endpointWasAdded = logAction[Endpoint](\"endpoint\", \"added\")(_). Hmmm... I don't understand your comment about symmetric return types.  What would be the difference between having the return type be Unit instead of Future[Unit]?. no need for val on case class params. I like explicit return type on public methods. I like explicit return types on public methods. Ah, I see.  I think Unit is more aligned with my personal philosophy anyway.  (since this method is fire-and-forget). :heart_eyes: explicit None. we've got 3 format thingies and 2 arguments. the naming of to and from seems backwards. any reason for the backtick escaped name?. Would prefer to make this a class with nsName and serviceName as constructor arguments instead of a trait. would prefer to have the PortMapLogging as a private member rather than a parent.. Oh!  I definitely missed that.  Would case Some(value) if value == oldValue => be more clear perhaps?. this is pretty confusing.  would prefer case Some(value) if value == oldValue =>. from, to. Should this be >= 0?  I mean, element 0 should always be $ or # but I assume this is intended to just avoid index out of bounds.. It would be helpful to have a comment with the expected structure of path so that these offsets are easier to understand.. I think it's safe to just skip Metric.None in flatten.  Can't think of a reason why we'd want both behaviors.. consider a counter instead of a trace. consider counters for the error cases too (probably in addition to logging). can we move the digest up to be a class level val?. is this match exhaustive?  don't you need to handle the case where the cert is not an X509Certificate to avoid a match error?. Can you factor out common code between this an the H2 version to reduce code duplication?. linkerd import style is to have a single import block (no blank lines between imports) all alphabetically sorted together. I think it makes more sense to put this module on the client stack instead of the server stack.  That way, sending of the client cert header can be configured per-destination rather then being on/off for the entire router.. maybe also check that the value can be parsed as a long?. I think this param should contain an Option[String] so that this filter can be disabled.  (and it should be disabled by default). I don't think we need this since this should be disabled by default. prefer to avoid infix notation. prefer Filter.mk or new Filter. to ensure that this request was even routed to the downstream correctly, should assert the status code is ok. close the downstream server too. it looks like this puts the header on the response, not the request?. I think this might have to be @volatile since it's shared across threads. I think we can remove lines 10-11. s/message/request. I don't really understand.. I don't think this works.  Consider:\nstream = [3, 1, 2]\npaired = [(x,3), (3,1), (1,2)]\nfiltered = [(x,3), (1,2)]\nmapped = [3,2]\nI think you could probably accomplish this with a scanLeft...\nstream.scanLeft[(W, Boolean)]((Zero, false)) { case ((largest, incr), w) =>\n  if (w > largest) (w, true)\n  else (largest, false)\n}.filter(_._2).map(_._1)\nor something like that. defining this as a method def increasingSubsequence[T <: Ordered[T]](stream: AsyncStream[T]): AsyncStream[T] would make the logic of this method easy to test (since you can just test it on an AsyncStream of numbers).. what is the visibility scope here?  Is this just equivalent to private?. stopped should already be true at this point.. so you don't have to type new to use it :). looks like this removed the newline from the end of the file.  please revert this.. Since we're now specifying a resource version, I think we now need to handle:\ncase io.buoyant.k8s.Api.Conflict(_) => Future.exception(new DtabVersionMismatchException())\nWe may also want to update the documentation or add a comment indicating that this method can return a DtabVersionMismatchException if the dtab was changed during the update and that the put should be simply retried.. rm. https://github.com/linkerd/linkerd/blob/master/namerd/docs/storage.md. linkerd style is to have all the imports grouped together (no blank line) and sorted alphabetically.. I'm not 100% sure but I think you can do implicitly[HeadersLike[H]] instead of having this method.. (this comment applies to other files in this review as well). group and alphasort imports please. rm. what about \nif (cert.delete() && key.delete()) Return(()) else Throw(new SomethingSomething). ditto-ish. * Technically, frames larger than WINDOW_SIZE - CLASSIFIED_BUFFER_SIZE can hang.  So if, for example, someone were to set the initial window on a stream to 8k, they're going to have a bad time.\n* I believe the stream will hang, not the connection.. The parameter is called retryBufferSize: https://linkerd.io/config/1.3.1/linkerd/index.html#http-2-service-parameters and if configurable.  8kb is just the default.\nAlso we need to update the docs at https://linkerd.io/config/1.3.1/linkerd/index.html#http-2-service-parameters with the new default value.. rm. rm: This should be set to the server's window size.. unneeded . Perhaps: \"Added the ability to weight addresses based on Consul tags.\". I've just discovered some issues with the forwardedClientCert.  We should fix them, but I don't think we need to block the release.  This property is missing documentation, has an invalid test (that isn't run in CI), and is extraneously settable on H2 servers.\nAs for this line item:\n The property applies to client configurations, not server.\n \"... configurations which causes Linkerd to forward client TLS certificates in the x-forwarded-client-cert header.. 11-17?. Could this return type just be Activity[W] instead?  I think this should be as simple as moving Activity(Var.async(...) { ... }) from the caller into this method.  That would make this method signature a little easier to understand.. consider calling this \"largestVersion\" or something since it may not actually be the previous event. is the _: W type annotation necessary?. I think this variable shadows the resourceVersion passed to watch which is a bit confusing.. this anchor should be switched with the next one I think. should this notice be in the http and h2 docs instead of here?. scalariform. rather than getting and re-encoding the query string, I would just do something like:\nval query = {\n  val u = req.uri\n  u.indexOf('?') match {\n    case -1 => \"\"\n    case n => u.substring(n)\n  }\n}. Wanna give this exception a message, just it case it gets printed somehow?. what do you think about making this a private[this] val ServiceRelease = new Exception(\"fjdklasf\") with NoStackTrace since SvcAddr is already a singleton anyway. I think what you have right now is an equality check, not a type check.  (since ServceRelease is an object and not a type). unused. The convention is usually that backoffs0 represents the prior backoffs and backoffs1 represent the next backoffs.  Alternatively, rest or tail is sometimes used to denote the tail of a stream.  So this could be\nval sleep #:: backoffs1 = backoffs0 or val sleep #:: rest = backoffs.. TIOLI: flatMap is perfectly fine here but there is also Future.sleep(sleep).before(cycle(None, backoffs0)) that is equivalent.  Take your pick.. I don't think this does anything.  I think you either need to return Future.exception(...) here or (my preference) put the rest of this method in an else block.. remove newlines. We define custom deserializers in io.buoyant.config.types. I think you'll want to make the host and port that the rancher client connects to configurable.. you should be able to use .transform instead of .liftToTry.flatMap. style nit: no need for braces on case bodies. I'm not familiar with the rancher API but let me just say, that is a wacky path.. I think if you make this a @volatile var v: Option[String] = None you can get rid of the initialized boolean.. Depending on how this Var is intended to be used, you may not want to set it into an error state here.  It might be preferable to keep it in pending and keep retrying until you get a successful response.. I think you should be able to use flatMap here. linkerd import style is to group all imports together (no blank lines between imports) and sort them alphabetically.. you are correct. yup.  final stat names look like namer/#/io.l5d.k8s/client/http/status/4XX. I think this test is mistitled.  it looks like it tests that when the dtab directory is absent, an empty list of dtabs is returned . it doesn't look like this tests the invalid namespace check at all.  that check applies to the create, delete, put, and get methods.. lets make this a Duration instead of an Int. This should be an Option[BackoffConfig] instead of an Option[Int].  . I think you can do implicit val timer: Timer = DefaultTimer here. lets have this accept a Stream[Duration]. rm this file, I think. I like that the default is defined in one place only.. Not sure there exists a package that's a perfect fit.  It needs to be visible from both linkerd and namerd and it depends on finagle and config.. BackoffConfig only produces decorrelatedJittered and constant.  Never exponentialJittered.  This is more consistent with elsewhere in linkerd.. Personally, I usually wildcard it after about 5 imports in the same package.. you shouldn't nee this type annotation. alphasort :). TIOLI: this would be a bit more testable if we passed in the Random as a constructor argument.  Not that it really matters since we don't currently have good tests for this \ud83d\ude43 . Is this case possible?  Could the offsetAfter ever exceed the buffer length?  Seems like no, right?. What do you think about logging the exception here?. TIOLI: could factor the \"8u151\" version number into a constant to avoid repetition and ensure they stay in sync.. alphasort imports please. no blank line in imports please. should be optional as well, I think. This will only build a ThresholdConfig if all properties are set.  Instead, I think we want to fall back to defaults for each property.\nThresholdConfig(\n  minPeriod = minPeriodMs.map(_.millis).getOrElse(DefaultMinPeriod),\n  ... etc\n)\n  . Let's explicitly set our own default values instead of inheriting from Finagle.  It's okay if the default values are the same, but this way they won't unexpectedly change out from under us if do a Finagle upgrade.. Needs @JsonIgnore. needs @JsonIgnore. needs @JsonIgnore. Let's fall back to our defaults if the failureTreshold is absent, instead of omitting the param entirely and letting Finagle pick the default.. tioli 5.seconds. tioli 4.seconds. Use consistent capitalization at the beginnings of the descriptions please\n  . tioli: \"... the health of the connection to a Namerd instance.\". After discussing with @olix0r, I think we should make this default to Duration.Top. broken link formatting. lets be consistent about inline links vs named links. Maybe silly but this is exciting enough to potentially use an \"emoji\". I think you can get the remote host with this:\nimport com.twitter.finagle.context.RemoteInfo\nRemoteInfo.Upstream.addr. To avoid printing the port here I think you want to attempt to cast to InetSocketAddress and use getHostString.  Something like\nRemoteInfo.Upstream.addr match {\n  case Some(isa: InetSocketAddress) => isa.getHostString\n  case _ => \"-\"\n}. I think this might be more clear written as:\nif (strict) {\n  spec.backend...\n} else {\n  None\n}. What do you think about calling this parameter \"ignoreDefaultBackends\" or something that's more self-explanatory than \"strict\"?. And changing the description to \"Identify requests only when explicit ingress rules exist.\". Scala is weird but you totally can!\nval fallback = if (strict) {\n  None\n} else {\n  log...\n  spec.backend.map(b => ...)\n}. it doesn't seem to me like additionally logging the IngressPath is very useful.. Rather than keeping this deprecated method, how would you feel about just updating all of the existing DtabStore implementations?  There are only a small handful in the codebase and it should be a very mechanical change.. Linkerd style is to group all imports together (no blank lines between imports) and sort in alphabetical order.. This might benefit from an example to help see what's going on here.\nIf key is /namerd/dtabs/foo and root is /namerd/dtabs, then doesn't s\"/$key\" start with a double slash?. I think this can be written more simply as:\nval altNames = names.zipWithIndex.map { case(name, i) => s\"DNS.${i+1} = $name\" }\naltNames.mkString(\"\\n\"). Linkerd import style is all imports grouped together (no blank lines) in alphabetical order.. Can we make this always take a Stream[Duration] instead of an Option[Stream[Duration]] and pick the default backoffs at the call sites where appropriate?  Similarly, I think we should not provide a default log level here and be more explicit about the log level at the call sites.   Since this is a local method, this should be pretty easy.. I think it's more idiomatic to do something like\nval backoff #:: nextBackoffs = backoffs\nFuture.sleep(backoff).before(loop(None, nextBackoffs, Level.DEBUG)). I'm not sure that is makes sense to die here if there is no previous good state, especially given that we are removing the retry filter.  In the case of an error when there is no previous good state, I think we should not set state() (e.g. leave it as pending) and do a retry with backoff, just like the case above.  \n@hawkw what do you think?. I don't think we need this method.. Given my comment above, I'm not sure this is the desirable behavior.... group imports together. remove printlns please. Linkerd style is to group all imports together (no blank lines) in alphabetical order. What are we synchronizing on here?  I think it's essentially unstable.synchronized since ExistentiaAct is a value class.  Really, we probably want to synchronize on the Var.async closure.  We could effectively do this by creating val mu = new {} alongside current and exists and then doing mu.synchronized. Fair points.  It's hard to differentiate between an invalid request (such as from a security scanner or for a nonexistent DC) from a transient failure that should be retried.  Do you know what Linkerd's behavior is when it falls into this case?  Will it cache the Addr.Failed and continue to fail subsequent requests without retrying the consul API call?  If so, I don't think that's the behavior that we want since it would effectively make transient failures put Linkerd into a \"stuck\" state for that name.\nRegarding unnecessary load, Linkerd will stop watching these names once they have been idle for 10 minutes, so the load won't be unbounded.  And hopefully the exponential backoff will mean that the added load from these API calls will be not too much.. This will be more testable if you pass the Timer in as a param into apply.  This allows us to pass in a MockTimer in tests.. I think this comment needs to be updated.  As far as I can tell, lastGood is only used for logging purposes now. I don't think we need a new label param, we can just use the existing one (com.twitter.finagle.param.Label). same here, can use com.twitter.finagle.param.Label instead of this one. I think you can use io.buoyant.linkerd.TlsClientConfig here instead of defining a new TlsConsulConfig class.. Tiny style nit: Linkerd style is to group all imports together (no blank line inbetween) in alphabetical order.. I find this refactor to be much harder to follow and understand than what was here before.  I'm guessing that you did this refactor to facilitate testing?  Unfortunately, it looks like the test doesn't really test the signal handling at all so I'm not sure it's worth refactoring the code for such a test.\nIdeally we would be able to leave the structure of the code the same and test the behavior with Signal.raise but I'm not sure if this is possible within a unit test or how the testing framework handles it.  If such a test is not feasible, I think it's okay to resort to manual testing for features like this.. This utility object is nice!  I might name it differently so as not to confuse it with java.net.InetSocketAddress.. Nice cleanup!. I've got some concerns about this trait but I think it can just go away entirely if the refactor that I objected to above is reverted.. I think that would be my preference.  This kind of thing is, unfortunately, pretty tricky to test.  I do appreciate your work on this anyway.. I think we probably want to include all instances of UnexpectedResponse here.  That should cover more cases and be less brittle.. Good question.  Unfortunately, consul doesn't give us a good machine readable way to differentiate.  Either way, you are correct that the retry loop would continue and the state would be updated.. typo. I think this should be await. oh, I just noticed all the Await.readys in this file.  I think... they should all be await instead?. unused. I think we can remove this line and use await(addr.changes.toFuture) below. un-un-sort please. good idea. can this be private[this]?. for consistency, we usually use Future.value. there is a comment above on line 80 that is no longer accurate and needs to be updated. I don't think the updated test name is correct.  It looks like this tests that the state remains pending while waiting for a response from consul.. I think we should add a test for the case when a DC is created and destroyed.  This would use a TestApi that returns a 500 response for the first request (representing that the DC does not exist yet), a valid 200 response for the second request (representing that the DC has been created), and then a 500 response again for the third request (representing that the DC has been deleted). \nWe should check that the state is Neg after the first request, Bound after the second, and Neg again after the third.. I think you need to recursively call _extract in order to continue processing the rest of the path. It's a bit weird to allow captures of the form {foo}:bar but not any other form that includes {foo} as a substring such as {foo}-bar or www.{foo}.com for examples.  Could we maybe use a regex to make this a bit more general?. I'm not sure, but you might want to wrap this in eventually as well.  Depending on the timing I think this could potentially still be pending. perhaps use \"datacenter was deleted\" or something as the exception message. I don't think we ever really use this..... yup, an unrelated \"clean-as-I-go\" change\n. yup, an unrelated \"clean-as-I-go\" change. Yeah, the functions are basically identical except that one operates on Frames and the other operates on Releasables.  We could consolidate them by having Frame and Releasable implement a common trait or by having failAndDrainQueue take a releaser function as an input.  But, given the small size of this method, I think I'd rather avoid adding an abstraction.. Just to confirm that I understand, we only escape \".\" because that's the only valid Path character which also has a regex meaning?  . I think it would be clearer to factor this code so that it happens when the Matcher is constructed, instead of at _extract time.  I'm thinking something like this:\nWhen the Matcher is constructed and the exprSegments are split out, each one is stored as a Regex and a list of capture names.  For example, /{foo}-{bar} is stored as \nSeq(\n  ExprSegment(regex = \"(.*)-(.*)\".r, variables = Seq(\"foo\", \"bar\"))\n)\nThis lets us compile all of the regexs ahead of time when the matcher is constructed.. What do each part of this return type represent?  As written, I think the second part represents the list of capture keys when the segment contains captures but it represents the literal segment when there are no captures.  This reassignment of meaning is confusing.  To make this explicit, consider a case class:\ncase class MatchSegment(\n  segment: String,  \n  regex: Option[Regex],\n  captureKeys: Seq[String]\n)\nor something like\nsealed trait MatchSegment\ncase object Wildcard extends MatchSegment\ncase class Literal(segment: String) extends MatchSegment\ncase class Captures(regex: Regex, keys: Seq[String]) extends MatchSegment. I think this var should be a val. there's a lot going on in this line which makes it hard to understand.  consider breaking it up:\n// '.' is the only valid Path character that also has a regex meaning.  Any '.' characters must be escaped.\nval escaped = exprSegment.replace(\".\", \"\"\"\\.\"\"\") \n// Replace capture keys with regex captures.\nval withCaptures = keys.foldLeft(escaped)(_.replace(_, \"(.*)\")\nval keyNames = keys.map { key =>\n  key.drop(1).dropRight(1) // Remove the encasing '{' and '}' characters.\n}.toSeq. What do you think about:\nsegments.zipWithIndex.map { case (s, i) => s -> v.group(i+1) }. Can you explain this bit?  This looks like when we receive a settings frame we write that settings frame back to the peer that sent it to us?  Is this the settings ack?. is this above TODO still accurate/relevant?. typo? l5d-dtab. What change caused this change in behavior?. we do await above so I don't think we need eventually. Are we confident that CLOSED is the right state here rather than one of the half-closed states?  Asking because I honestly have no idea.  (likewise for the other reset calls in this file). counterpoint: I actually like these \"clean as you go\" changes as long as they're not too distracting from the change at hand.  Especially when they're made my autoformatting tools like scalariform or an IDE.  It's a pain to re-break style errors just to get them out of the diff.. my personal vote is to LI since this way is more explicit rather than delegating to Option's null handling.. We don't encode any logic about these states and just pass them on to Netty so I don't think there's much to gain from more types here.  I think this probably can be made more concise...\ncase class H2FrameStream(streamId: Int, streamState: Http2Stream.State) extends Http2FrameStream\nSince the case class will autogenerate vals for streamId and streamState that should satisfy the interface.. ah, great!  I was reading it backwards and thinking this was a frame coming FROM the transport.  My bad.. rm. rm. rm. can this assertion be reverted?. can these assertions be reverted?. We do something similar.  Consul uses long-polling instead of streaming so it's not quite the same.. They are similar but slightly different.  This connection timeout is only applied to connections between the minSize and maxSize whereas the clientSessions timeouts apply to all connections.  The connectionPool timeout can be thought of as the amount of time \"extra\" connections are kept around.. Although, in the case where minSize=0 (which is the default) then these two timeouts do exactly the same thing.. I don't know how this extra whitespace got here in the first place or why scalariform suddenly decided it needed to fix it right now.. it's used on line 27 and line 48. Yeah. I don't think util has anything like this.  We could consider contributing it back, but I'm not sure if they'd be interested.. I think everything in the AddForwardedHeaderConfig and AddForwardedHeaderConfig.module is request/response type agnostic.  This means that we should be able to just have one of these (not separate ones for H1/H2) and it can live in http-base.. can parameterize this on the request/response types:\ndef module[Req, Rep]: Stackable[ServiceFactory[Req, Rep]] =. I think it would be nice to avoid depending on http and H2 in http-base if possible. I think rather than move this into http-base, we should split it into two: one in router/http and one in router/h2. possibly with common functionality factored into a trait or util object that can stay in http-base. These filters are protocol specific so they should be moved to router/http and router/h2 respectively.. LabelingProxy can be made generic by accepting the filter as a parameter:\nclass LabelingProxy[Req, Rep](\n  byLabeler: Labeler.By,\n  forLabeler: Labeler.For,\n  addForwardedHeader: Filter[Req, Rep, Req, Rep],\n  underlying: ServiceFactory[Req, Rep]\n). The module is protocol specific (and I think this is correct since it should instantiate the appropriate AddForwarededHeader filter to pass to LabelingProxy) so should live in router/http and router/h2. Just curious if scala can infer these type parameters if you omit them?. alphasort, no blank lines in imports. typo in test name. Is any of this H2 specific?  Isn't this just tests of the AddForwardedHeaderConfig, which lives in http-base?. should this move to the http-base project?. Maybe use Stream.empty so that you don't need to worry about releasing the data frames. Maybe use Stream.empty so that you don't need to worry about releasing the data frames. Weird!  I would have expected type unification to be able to infer the type parameters since this is being called on a Stack[ServiceFactory[Request, Response]].  Anyway, this is fine, I was just curious.. Can this file be moved into linkerd/core?  Since it's a config file it probably belongs in linkerd rather than router.  This used to be part of AddForwardedHeaderConfig which is also moving to linkerd/core so I think it makes sense for this to move into linkerd/core as well.. No need for braces on cases \ud83d\ude09 . To make the types work out, I think you want to change this map to flatMap and have the function return an Activity[NameTree[Name]] (see commend below). This is a Var[NameTree[Name]] but I think it needs to be Activity[NameTree[Name]] to match with the flatMap (see comment above).\nYou can use addr.map to map Addr states into Activity.State and get a Var[Activity.State[NameTree[Name]]].  Then you can use Activity.apply to turn that into an Activity[NameTree[Name]].. e.g. Addr.Pending should map to Activity.State.Pending.. And probably Addr.Failed should map to Activity.State.Failed. using flatMap above lets us avoid needing sample() here.. I think 10.minutes is probably a fairly reasonable default.  (if you import com.twitter.conversions.time._ then you can use 10.minutes, 7.days, etc.). This interface would be a little bit nicer if it took a com.twitter.util.Duration here instead of a Long or Int.. I think this interface can be used to make a faster and more robust test https://github.com/google/guava/wiki/CachesExplained#testing-timed-eviction. should this indicate that the default is 600 now?. ditto. please format imports in alphabetical order with no blank lines. No need for braces here. I think this should be Activity.Pending. This can be shortened to just Activity(observation). This plays a bit more nicely with Finagle if we have this take a com.twitter.util.Stopwatch with a default value of Stopwatch.systemNanos.  We can then create a Ticker based on the stopwatch:\nnew Ticker() {\n  val start = stopwatch.start()\n  override def read(): Long = start()\n}\nThen, in the test we can instead pass in a Stopwatch.timeNanos.  This is a stopwatch that respects time control which means we can then use Time.withCurrentTimeFrozen and use time control for the test.. nice!. This works but Stopwatch.start() results in some unnecessary allocations.  My suggestion of passing in an instance of Stopwatch as a parameter was so that by default we could use Stopwatch.systemNanos which is very performant and avoids unnecessary allocations but also allows us to pass in Stopwatch.timeNanos in the test which allows us to do time control.  These Stopwatch instances also return Long directly which allows us to avoid boxing and unboxing a Duration.. should we assert that state == Activity.Pending here?. now that this no longer checks the Addr, I think this should be renamed.. I think this could use a little more information about how to determine the full client name including the transformer prefix.  Probably the easiest way would be click the client name to expand it in the Linkerd admin dashboard.  An example would probably be helpful.. This method had no purpose.  To close a Stream, simply write a frame with eos=true.. An empty Stream should never have frames written to it or read from it.  The only reason for this queue was to allow cancellation by polling the queue and then immediately interrupting it.  We have a better mechanism for cancellation now (the cancel method).. These made for very messy logs when logging StreamState.. We now use the StreamError type only in onReset to signal to the dispatcher if a reset needs to be written to the remote.. Reset.Cancel seems like a better default when a Future is interrupted than Reset.InternalError to me.. We now change the stream state before writing the frame.  I think that's okay???. These frames have illegal headers and are not admitted.. Writing a trailers frame is sufficient to close the stream.. Should this be /%/io.l5d.port/4141/#/io.l5d.fs/{service}?. I think this can benefit from a bit more background:\n\nIf you use transformers, each transformer will prepend a transformer prefix to the client name to indicate that it has been transformed.  Transformer prefixes always start with /%.  When using the io.l5d.static client configuration, ensure that the client prefix includes the transformer prefix.  For example..... missing a leading slash. shouldn't the port number be part of the port transformer prefix?. Maybe give a bit more explanation:\nSince the base client name is /#/io.l5d.fs/hello and the port transformer applies /%/io.l5d.port/4141 as a transformer prefix, the transformed client name is /%/io.l5d.port/4141/#/io.l5d.fs/hello.. I'd remove the bit about multiple transformers... it can be difficult even if there is a single transformer.. > will reveal the full client name including any transformer prefixes.. Maybe add a comment here that explains that we use ThreadLocal because MessageDigest is not thread safe. good suggestion, thanks. The semantics of resets when retries are involved aren't really externally specified so it's somewhat up to us.  I think it makes sense to reset everything once a single reader cancels.\n\nConsider when the client which sends a small but unterminated stream.  If the server resets that stream, I would interpret that to be a signal to the client to stop producing.. maybe a bug in the UI if the leading slash is not being displayed?. I think this is unused. Would you mind updating this e2e test such that the request passes through a Linkerd router with the appropriate protocols configured?. Good question.  If we get a timeout error but we're not in Pending or Named states (i.e. we're in Closed or Failed) then the timeout doesn't really apply anymore and we should leave the error untransformed.  Let me rework this code to make that more clear.... DynBoundTimeoutException is a new exception created in this PR so that we can control it's error message.. I just noticed that if you try to use the request evaluator on an address that Linkerd cannot connect to, the connection fails before getting to the evaluator:\n$ curl localhost:4140 -H \"Host: foo\" -H \"l5d-req-evaluate: 1\"\nconnection timed out: /1.2.3.4:8888 at remote address: /1.2.3.4:8888. Remote Info: Not Available\u23ce\nPerhaps if we move the Request higher the stack (above connection establishment) but below the load balancer, this will work?  Perhaps .insertAfter(LoadBalancerFactory.role)?. should this just be a def evaluateRequest(identification: String, ...): String?. I tend to prefer private[this] just because it's often the narrowest scope that is needed.. unused?. Should this be identificationPath?. Instead of using .sample() you should generally do .toFuture to get a Future[Addr] and then map on that to produce the Future[Response].. this can be more condensed I think:\na.map {\n  case inetAddr: Address.Inet => ...\n  case _ =>\n}. is this unused?. all recursive calls to this method are called with a Some here.  Could this just be a DelegateTree[Name.Bound] and have the Option handling outside of the method?. this looks like it always takes the first branch of a union. I was never super clear on this... should this key off the request's accept header instead?. ah, nevermind, I see this exists so that it can be serialized to json.. I think this can be\nDstPathCtx.Setter.module +: RequestEvaluator.module +: Stack.Leaf(...). bind and resolve use pattern matching to destructure the Option from map.get.  In this case we're just doing a null test.. it is possible for dtab to be null.  . This is to mirror the serverCaChainPath.  Of course, that might be a poor name also.  My intention was to try and avoid confusion between the CA that the server uses to validate the client identity (caCertPath in the server tls struct) and the CA that issued the server's cert (serverCaChainPath in the server tls struct).  \nLikewise I wanted to differentiate between the CA that the client uses to validate the server identity (trustCerts in the client tls struct) and the CA that issued the client's cert (clientCaChainPath in the client auth section of the client tls struct).\nAny suggestions for better names for clientCaChainPath and serverCaChainPath would be appreciated.  The names of any other properties are more difficult to change because they have already been released and require a breaking config change to rename.. Great suggestion, thanks!. We should think about what we want to call this.  I don't think Evaluator is quite right.  This is a kind of tracing but we should be careful not to conflate it with the type of tracing already in Linkerd (zipkin etc).  \nPerhaps a name related to ActiveTracing (as opposed to the \"passive\" tracing that zipkin does).  Or maybe RoutingInfoFilter... hmmm.... is this the routerLabel?  let's call it that if so. I'd explicitly annotate the return type of String here to signal that this method isn't side-effecting.. I think we can omit this here and access the classes constructor parameter directly.. perhaps formatDelegation?  since we're not printing the tree, just one path through it. I think this is not quite right.  Technically the weight belongs to the subtree (wd.tree) instead of this node (path).  This causes delegations like \n/svc/default\n  1.0 * /x/default\n  /b/default\n  /$/inet/127.1/4140\nwhen really it should be\n/svc/default\n  /x/default\n  1.0 * /b/default\n  /$/inet/127.1/4140\nRegardless, I'm not sure if it's even valuable to print the weight at all.  We can probably just omit it.. protip:\nhead is unsafe because it fails on empty lists.  prefer headOption:\n.filter(!_.isEmpty).headOption.toList.flatten\nfilter+headOption can be combined into find:\n.find(!_.isEmpty).toList.flatten. I'm a bit confused about the name.  What is RouterCtx?  The return type is Future[Response], right?. consider just appending to prevResp.contentString instead of creating a new Response. Need some error handling here:\n$ curl localhost:4140 -X TRACE -H \"Host: foo\" -H \"l5d-max-depth: alex\"\nFor input string: \"alex\"\u23ce. do we need this val or can we just inline req.method?. is this infinitely recursive?  should we pass all of the params EXCEPT the identifier param to the identifier?. This class could use some scaladoc here.. I'd hesitate to call this print since it doesn't print.  And it seems like we're moving away from the evaluate terminology.\nformatRoutingContext or something maybe?. s/prints out/returns ?. fill out these scaladoc properties with descriptions or remove them entirely. but filling them out might be good because it's not super clear what they are.  tree is some kind of accumulator, I assume?. this method is pretty dense.  breaking it up with comments or vals with intermediate values might help make it more readable.. this is to perform a transformation on the last node of tree?  personally, I'd find this easier to follow if it were something like\nval lastNode = tree.lastOption.collect {\n  case Node(...) => Node(...)\n}\ntree.dropRight(1) ++ lastNode. This dentry juggling we have to do here is pretty confusing and could use a comment. is this just tree.map(_.toString)?. might just call this response instead of prevResp since we're mutating it and not creating a new one. should resp have a non 200 status code? 400?. update comment. can use com.twitter.finagle.http.Fields.MaxForwards. typo. can collapse the matches:\ncase (Method.Trace, Return(Some(0)), Some(isAddRouterCtx)) if StringUtil.toBoolean(isAddRouterCtx) =>\n  ...\ncase (Method.Trace, Return(Some(0)), _) =>. We probably want to be more strict than allowing any value to be treated as true.  com.twitter.finagle.util.StringUtil.toBoolean seems like a good thing to use here.  It will map \"1\" and \"t\" and \"true\" to true and everything else to false.. we usually abbreviate context to ctx.. consider \nreq.headerMap.get(...) match {\n  case Some(s) => StringUtil.toBoolean(s)\n  case None => false\n}. can this just be svc(req).flatMap(getRequestTraceResponse(_, stopwatch)).ensure {. I think if we get a TRACE with l5d-add-context but no Max-Forwards, we should still add the context.  I think we basically treat this as Max-Forwards = infinity.. Yes, although this is already an issue with Linkerd: https://github.com/linkerd/linkerd/issues/1411. update comment: no such thing as l5d-max-depth anymore. this represents one step in the delegation, rigth?  should this be called DelegationNode or something similar?\n. Scaladoc links using [[ and ]] have a tendency to break artifact publishing.  I don't know why, but I've had to remove them in the past.  . try to be consistent about private vs private[this] unless there's a reason this method needs to be accessed from another instance.  I prefer using the smallest visibility scope necessary.. TIOLI I used to use this opt.map(f).getOrElse(default) pattern a lot but more recently I'm starting to favor\nopt match {\n  case Some(x) => f(x)\n  case None => default\n}\nThey're identical in effect but I find the second more readable.  Either way is fine, just wanted to point out the alternative.. could be case (Method.Trace, ..., true) =>. could be case (Method.Trace, ..., false) =>. ditto for remaining cases. Yes, from https://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html\n\nThe final recipient is either the origin server or the first proxy or gateway to receive a Max-Forwards value of zero (0) in the request (see section 14.31).. > The Max-Forwards request-header field provides a mechanism with the TRACE (section 9.8) and OPTIONS (section 9.2) methods to limit the number of proxies or gateways that can forward the request to the next inbound server. This can be useful when the client is attempting to trace a request chain which appears to be failing or looping in mid-chain. . space after : for return type. is this just s\"\\n${ctx.formatRouterContext.trim}\\n\"?. fyi there is a Frame.Data.eos(content) method. be consistent about Frame.Data vs Data. I don't think we want to add an empty trailers frame here.  there was no trailers frame in the original response.. Do we want to return an error string in the response, like we do in the h1 case?. I think this is invalid.  I'm guessing that the response needs to have certain response headers (like status code) and cannot have certain request headers (like authority, path, method, etc).. The H2 request type is better about mutability than H1 and so we can do something nicer here:\n\nval req1 = request.dup()\nreq1.headers.set(...)\nservice(req1)\nsince we never mutate the original request, we never have to worry about un-mutating it!. any reason this needs to be a case class?  can this just be a method on RouterContextBuilder?. RouterContextFormatter perhaps?  Builder suggests the builder pattern to me.. are the type annotations needed here?. Does this need to be separate object from RequestContextBuilder?  Can these methods just go into that object?. What you have doesn't save you from Windows, unfortunately:\n@ \"\"\"|\n                        |banana\n                        |\"\"\".stripMargin.toSeq\nres3: Seq[Char] = WrappedString('\\n', 'b', 'a', 'n', 'a', 'n', 'a', '\\n'). indentation is weird on this line. nit s\"$path\" is just path. extra newline. add a blank line here, please. I'd prefer to be a bit more defensive here and throw an IllegalArgumentException if trustCerts and trustCertsBundle are both set.  Then we can handle them in separate cases:\ncase TlsClientConfig(_, _, Some(cn), Some(certs), Some(bundle), _, _) =>\n  throw ...\ncase TlsClientConfig(_, _, Some(cn), Some(certs), None, _, _) =>\n  // use certs\ncase TlsClientConfig(_, _, Some(cn), None, Some(bundle), _, _) =>\n  // use bundle\ncase TlsClientConfig(_, _, Some(cn), None, None, _, _) =>\n  // use Unspecified. double space between \"that\" and \"includes\". diagnostic tracing for H2 should be in this list. Consider using similar wording for this feature from a previous release:\n* Pass stack params to announcer plugins, allowing them to report metrics correctly.. good catch. possibly, although this code is short enough that I'm not sure how much there is to be gained by factoring it out.  . From https://www.w3.org/Protocols/rfc2616/rfc2616-sec4.html\n\n2.If a Transfer-Encoding header field (section 14.41) is present and has any value other than \"identity\", then the transfer-length is defined by use of the \"chunked\" transfer-coding (section 3.6), unless the message is terminated by closing the connection. . Would you mind adding a comment here that the Ctx.serverModule needs to be before the ErrorResponder module so that errors responses from the ErrorResponder will be cleared when clearContext is set?. I notice that we never use rsp except to hold its headers.  This is basically equivalent but it might be a bit more clear:\nval headers = resp.headers.dup()\n...\nclearLinkerdHeaders(headers)\nResponse(headers, Stream.empty()). maybe write this as val _ = Stream.readToEnd(rsp.stream) to explicitly signal that we're doing a fire-and-forget future here.  Or add a comment.. Is it possible to make to avoid this refactor in favor of a smaller change?  For instance, to keep the TaggedNamer and UntaggedNamer classes but have them both extend a trait which adds the handlerPrefix and adminHandler implementations.  I think this lets us avoid needing to create and pass this parser function.. I think we need to expand the scope of this synchronized block to include the creation of the Activity (ie the call to lookup) and also add a second check to see if the entry is in the cache inside the synchronized block.\n\nThis is to avoid the race condition where two different Activities could be returned for the same path.. Why is this an Option?  Isn't it always called with Some?. extra blank line here is not needed. I added an end-to-end test in linkerd/main which uses the http protocol, the namer interpreter, and the per-host interpreter.  Therefore I need to depend on those projects in the e2e configuration.. that's correct but all the label names are defined as literals in this file and none of them contain any characters that need escaping.. I agree with you on all points.  However, my preference is to avoid refactors like this unless\na) The refactor is clearly superior or b) the refactor is inextricably connected to the change at hand.\nOtherwise, I'd prefer to keep this PR focused on the new functionality and relegate any non-essential refactors to separate PRs.. Why is this an Option?  As far as I can tell it is always Some.. I'm not sure about this refactor. With this change, only the Var[Addr] is cached, the Activity[NameTree[Name.Bound]] is recreated on each call.. I think we want to reorder this so that the work of creating this Var[Addr] is only done if it's definitely not in cache.. should be able to do this without a var. This cache within a cache is confusing to reason about.  I think this lookup will always fail because lookupAddress will insert this item into cache.. I think this is always going to be the same value as was just put into cache so this can simply be addr.  In fact, since this is the original value of the future anyway, this whole map block can be replaced by an onSuccess block.. This results in a URL like /namer_dtabs_store/namerd/dtabs.json.  I we want to have a URL like /namer_dtabs_store/namerd/default.json where default is the name of dtab namespace.  This should be as simple as using ns instead of root here.. I think we should also add a custom serializer for VersionedDtab so that this gets printed more nicely in the output.  In particular, we would want to serialize the version byte array as a string:\n\"value\": {\n        \"dtab\": \"/svc=>/#/io.l5d.fs;/svc/cat=>/#/io.l5d.fs/dog | /#/io.l5d.fs/cat\",\n        \"version\": \"MjAw\"\n      }. arbitrary throwables cannot be serialized.  Let's make this an Option[String] and in the Throw case of recordResponse, we should populate this with the throwable's message.. I just noticed this in the consul dtab store PR but it applies here too:\narbitrary throwables cannot be serialized. Let's make this an Option[String] and in the Throw case of recordResponse, we should populate this with the throwable's message.. wait nevermind, I think what you have here with using root is actually correct.  Ignore my previous comment.. how about storage for the first segment?  so that this becomes /storage/dtabs.json?. this is an inner class of object DecodingStream which is a singleton.  So private and private[this] are equivalent, I think.. good catch. including the type is less important here than it is for namers or interpreters because there can only be one storage module configured in Namerd (whereas there may be many namers configured).  . This comment is no longer accurate, there's not \"double checking\" anymore.  I think this is fine because this function is only ever called from inside of a memoize block, but we should fix the comment.. IndexedSeq[Dentry] renders weirdly in the exception message: Vector(null, Dentry(/foo=>/#/io.l5d.fs))\nWhat about having this just be the input string that triggered the exception?  e.g. req.contentString. this is just e.getMessage, no need for string interpolation. We'll also want to create a WatchState, pass it into activity, and serialize it the admin handler.. This documentation belongs in namerd/docs/storage.md. I don't think this needs to be a var OR a mutable.Seq.  What if this was just a val and val act was defined in terms of this?\nval instrumentedDtabStorage = {\n  val watchState = ...\n  val act = watchApi.dtabs.activity...\n}\nval act = instrumentedDtabStorage.underlying. is this a total function?  could we use map instead of collect?. Not quite.  Sampler takes a Long as input, not a request.  We feed the request's trace id into the sampler.  I think we want to ensure Sampler is used here rather than just returning Option[Boolean] to ensure that sampling is based on the trace id and is properly salted.. I think this class is now unused.. So that the sampler may be based on properties of the request.  For example, in the default trace propagator this method reads the l5d-trace header and returns a sampler with a sample rate equal to the value of l5d-trace.  Please let me know if you think the comment on this method needs to be reworded to make this more clear.. Base64StringEncoder imported twice. Ah, you're right, I got mixed up because of the name.  retryableAll5XX sounds good to me.. As described here, I think setting \"diagnosticsEnabled: true\" and \"controlEnabled: false\" would still allow access to the control endpoints, which doesn't seem like the right behavior.  I think diagnostics should be everything else, not included in ui or control.\nWhat do you think about this algorithm:\n1. determine which category a request is for: ui, control, or diagnostics\n2. if that category is enabled and the path doesn't match the blacklist, the request is allowed\n3. if that category is disabled but the path matches the whitelist and not the blacklist, the request is allowed\n4. otherwise the request is rejected. style nit: no blank lines in imports, please. what does this comment mean?. do you also need to do resp.reader.discard()?. based on that comment, does the reader.discard need to happen before setting chunked to false.  Otherwise the reader.discard will be ignored by finagle?. We need to override a def defaultCloseGracePeriod with this value.  However, the value is loaded from the config file in the main method.  This means that when main is invoked and the config is read, this value must be stored somewhere so that it can be returned from defaultCloseGracePeriod.  Attempting to call defaultCloseGracePeriod before main has run will result in an NPE.. it's still state and it still holds a releaser, even if it's less state than we were storing before.. TIOLI: just my personal preference to be explicit with None here. We're inconsistent about this, but it looks like etcd should not start with an uppercase letter.. I can't tell, what's the diff on this line?. is this unused?. why the package move?. this is cheap, any reason why it has to be lazy?. I think you also want to set the content type to json. instead of passing in an Option[String] and Option[Service], what about just passing in a Seq[Admin.Handler]?. should this be called stateHandler or something along those lines?. probably add a note that this file is copied and indicate where it is copied from and that it should not be modified directly.  Bonus points for a script in this directory which pulls down these files from their authoritative source.. Consider something like:\n// non-pending states have priority\ncase (Addr.Pending, x) => x \ncase (x, Addr.Pending) => x\n// combine bound\ncase (Addr.Bound(prev, _), Addr.Bound(cur, _)) => ...\n// Bound has priority over other states\ncase (x@Addr.Bound(_, _), _) => x\ncase (_, x@Addr.Bound(_, _)) => x\n// otherwise, just use the first state (e.g. in a list of negs or fails)\ncase (x, _) => x. is name unused?. could make this configurable if you wanted. using respond here and below is a problem because respond creates an observation that we never close.  The really nice thing about VarEventStream is that it handles creating and closing observations properly.  I'd highly recommend trying to massage this into a final Var[Update] using combinators like map and then passing that to VarEventStream than trying to use respond and managing the observations yourself.. I don't think this is right, if the Activity is pending we should just not send any update.. In this case we probably want to stream.close with some error code. I think this is the case that will get if we have NameTree.Neg.  So really this means that the service does not exist.  NoEndpoints(false) should be sent here, I think.. This is a wrinkle when trying to convert Event[AddressDiff] to Event[Update] because one AddressDiff can turn into potentially two Updates.  There's no built-in Event combinator that helps us with this, but we can write our own:\ndef eventFlatMap[T, U](ev: Event[T], f: T => Seq[U]): Event[U] = new Event[U] {\n  def register(w: Witness[U]): Closable = {\n    ev.respond { t => \n      for(u <- f(t)) w.notify(u)\n    }\n  }\n}\nand then we can do:\nval diffs: Event[AddressDiff] = ...\nval updates = eventFlatMap(diffs, { case AddressDiff(add, remove) => \n  val addUpdate = if (add.nonEmpty) Seq(mkAddUpdate(add)) else Nil\n  val removeUpdate = if (remove.nonEmpty) Seq(mkRemoveUpdate(remove)) else Nil\n  addUpdate ++ removeUpdate\n}. I don't think this is right.  if there are no adds and no removes, then that means \"no change\".  in other words, no update.. I think overall the pipeline will look something like:\nActivity[NameTree[Name.Bound]]\nActivity[Option[Set[Name.Bound]] //eval\nActivity[Var[Set[Addr]]] // Var.collect\nActivity[Var[Addr.Bound]] // foldAddr\nVar[Addr] // flatMap  (e.g. Activity.Pending -> Var(Addr.Pending))\nEvent[Addr] // changes\nEvent[Addr.Bound] // collect, dropping types other than Bound\nEvent[Set[Address]] // map\nEvent[Diff] // diff\nEvent[Update]  // eventFlatMap\nOne wrinkle that you'll run into is how to prepend a NoEndpoints(true) before the first Add in the event stream.  A custom combinator could do the trick:\ndef prependEvent[T](ev: Event[T], init: T): Event[T] = new Event[T] = {\n  def register(w: Witness[T]): Closable = {\n    w.notify(init)\n    ev.respond(w.notify)\n  }\n}. typo. I think this can be cleaned up if you define type TrySet[T] = Try[Set[T]] and turn your Var[Addr] into an Event[TrySet[Address]].  This avoids the need for the FAddress class.\nYou would then define a Diffable[TrySet] which pattern matches on the left and right TrySets.  If both are return, it returns a SetDiff as usual.  If either of them are Throw then it returns a new class called Failed[T] or something like that which extends Diff[TrySet, T].\n. Should this just be called SetDiffable?. SetDiff?. this line should have a comment explaining what we're doing here and why.. Should this be Addr.Neg?. I think you're missing the case where both addrs are bound and need to be combined.. dont want a var of AtomicBoolean.  a val of AtomicBoolean is mutable using its mutating methods.. won't this set the AB even if f(t) is false?. wildcard import should already cover Address, I think?. wildcard import should already cover Event, I think?. I'm no bash expert but https://www.shellcheck.net/ gives a few warnings about this script. this can allow multiple threads in before the AB is set to true.  Consider something like:\n// if this is already set, we can skip ahead\nif (!prependOnce.get) { \n  // check f(t) first and if it returns true then do CAS\n  if (f(t) && prependOnce.compareAndSet(false, true)) {\n    w.notify(init)\n  }\n}\nw.notify(t). it doesn't seem to me like figuring out how to add this to sbt is a good use of time. This is an unapply method that we actually want to always match, so it should always return Some, as it does here.\nA potentially less confusing way to implement this would be as a plain method:\ndef healthCheckCount(healthChecks: Option[Seq[HealthCheck]]): Int = healthChecks match {\n  case Some(checks) => checks.size\n  case None => 0\n}\nand then invoking this method directly instead of in as part of a case destructuring.\nBut this section of the code relies heavily on unapply methods so take it or leave it.. @rogoman correct me if I'm wrong but I believe that Task is a class that matches the Marathon API and API responses are automatically deserialized to it.  This means that we can't add the health check count to that class.. do you need to assign this to a dummy val?  Can you just do Logger.get(...).warning(...)?. do you think we should use an env var here so this is overridable?. What about the case where users are running the executable directly (not in docker)?. should this be /var/log/namerd?. namerd?. Do we have to worry about permissions here?  If you run the assembly directly as a non-root user, what happens?  What about in docker?\nIf this works out-of-the-box in docker, it's probably fine.  It's probably okay to require that to run the binary directly as non-root, you'll need to set GC_LOG to a location that you have permissions on, as long as the error message is clear..... I don't think you need these to be interpolation (s) strings. Don't think you need string interpolation here either. was pool just unused?. This is a lot of detail.  do you think it's actually helpful to print the thread name/id in this log message?  what does printing the gauge do?. Caution that the number of counters could blow up here if the number of distinct error strings is large.  if there are only a few potential errors messages, it's probably fine.\nAlso note that looking up/creating counters like this can be costly, especially in the hot path.  It may not be an issue, but if performance is a concern here, consider caching the counters so you don't have to keep creating them.. Even if the number is bounded, this is potentially a lot of metrics.  How important is it to scope these counters by dns entry?. I can't find the TraceId.serializeIt method.  Am I missing something?. The motivation is that this test generates a huge amount of output at the WARNING level that I wanted to suppress.  Setting this to OFF could have worked too.. the _parseString method no longer exists :(. It has to do with propagating future interrupts but, TBH, I copied this setting value from Finagle's ThriftMux's singleton pool.. com.twitter.finagle.thrift.ThriftService no longer exists. wait... I use _parseString in other deserializers.... what's going on...... I got myself very confused, please ignore these comments.\nI will fix the formatting here to not create a val.. I think the name of this parameter is backwards.  It maps to the finagle argument noDelay.\nMore generally, I think we should remove the enabled or disable words and just make all of these positive:\ntcpNoDelay = true, resuseAddr = true, reusePort = false. I think this is backwards.  If set to true it enables TCP_NODELAY. Cargo culted from grpc-java: https://github.com/grpc/grpc-java/pull/431/files#diff-7f048858dab93d58f2bcac583626abddR49\nThis is mostly just a safety net to ensure that we don't buffer up arbitrarily large writes without flushing.  In most cases I would expect us to flush before hitting this limit.. I don't think so.  All writes will add the msg to the writeQueue and schedule a flush if one has not already been scheduled.  During the scheduled flush, we will drain off the writeQueue 128 items at a time and write them out as a group.. Cargo culted from java-grpc: https://github.com/grpc/grpc-java/pull/431/files#diff-7f048858dab93d58f2bcac583626abddR132\nI'm not actually sure, but I suspect this is just to guarantee that we can't have writes that get stuck in the Channel buffer forever and never get flushed.. I think we can simply omit this line and the Var will just keep whatever value it already has.. this log message can be more specific, indicating that the request to consul timed out after x amount of time. I'd add a third case here so that we can check that in the case of a timeout, SvcAddr will issue a new request and the address will get updated with that new data. you'll also want to be careful to make sure you don't make this test flakey.  As written, SvcAddr may go through cases 1 and 2 (and beyond) before the value of the addr is checked.  (depending on the value of the backoff and the specific timing of execution).\nPerhaps a safer way to do this would be to use time control to advance through the backoffs and timeouts.. Would it work to do [-w \"$GC_LOG\"] instead of touch?  i.e. check that the directory is writable?  Touch leaves behind an empty file:\n$ ls -l *gc.log*\n-rw-r--r--  1 alex  staff      0 Oct  4 16:17 gc.log\n-rw-r--r--  1 alex  staff  82106 Oct  4 16:18 gc.log.0.current. This change is not technically required to fix the above issue.  However, when this issue is triggered this line causes exceptions to be thrown because endP becomes satisfied by cancel and endP.become causes f.onRelease to be satisfied.  This leads to f.onRelease being set again when f.release() is called, which is an illegal operation.\nWe change this code slightly to protect against such an exception.  We only allow updates to flow from f.onRelease to endP and not the other way around.. Consider a scenario where the stream is in RemoteClosed (i.e. the remote has finished streaming but the local has not).  It is possible for the remote to send a stream reset at approximately the same time as the local sends an EOS.  When the local sends the EOS it transitions the state to closed, but it will then receive the stream reset from the remote.\nThe somewhat unintuitive thing about that scenario is that the remote can still send resets even after it sends an EOS.. If f.isEnd == false then nothing happens.  We leave endP alone.. I don't think this comment is accurate anymore. Do we need both of this tc.advance call and the next one?  Should they be combined?. there seems to be a large amount of overlap here with ConsulDtabStore.  Is there a way to refactor things to share code between the two of them?  Or even better, can the Consul interpreter just use a ConsulDtabStore under the hood?. A bonus of this approach is that it helps to ensure compatibility between the consul dtab store and the consul interpreter.. Maybe just require the handler url to be explicitly passed in, rather than passing in an override.. TIOLI: s\"storage${root.show}.json\". This is so gross \ud83d\udc4d \ud83d\udc4d \ud83d\udc4d . typo on this line. don't think these braces are necessary. rm. typo: modules. typo: that. I'd reword this away from saying that it removes the dependence on Namerd.  Instead, I'd say that using this interpreter allows Linkerd to read dtabs from Consul KV directly instead of using Namerd.. \"degrade HTTP/2 streaming\" is vague and I don't really know what it means.  Should probably be more specific (or less specific and just let the changelist describe it in more detail). The condition is more narrow than that.  Rather than get into the details, maybe just say \"... no longer route HTTP/2 frames in certain cases.\". I wouldn't say \"Add support\" because we already had support.  Maybe \"Sets max... to 1000 by default...\". The problem with this test is that we only sample the vaddr, never observe it long enough for it to reach its final value.  Previously this was fine because observing the outer Var drove updates to the vaddr.  With our change to stabilizeExistence, we need to observe the vaddr itself if we want it to receive updates.  . Good question.  With the synchronized blocks just within the arms there is a potential race condition where two threads could pick the same arm because the case matching happens outside of the synchronize block.  I never observed this race condition happening in practice but to be safe I expanded the synchronized block to make sure this can never happen.. This actually doesn't work because state is of type Activity.State[Option[T]] and update expects type Activity.State[Option[Var[T]].\nI bet you might be able to pull this off with Scala3's union types because (Activity.Pending | Activity.Failed) is a subtype of Activity.State[Nothing] but I don't think this can be done in scala2.. gonna LI because makeInner returns a Var.  Moving the null check into makeInner means we have to move the assignment into makeInner as well which seems less clear to me.. done. I was able to remove this by taking @dadjeibaah's suggestion below of using None as the initial value.. Good idea.. Added a new unit test.. volatile removed.. I don't think peer is the right name for this.  I'd call these \"stacks\".  e.g. the service stack, the client stack, and the server stack.. Will the exception name ever be multiple segments?  e.g. in the case of nested exceptions?  do we need to capture metrics like:\nSeq(\"rt\", \"client\", \"exn\", \"com.twitter.finagle.SomeParentException\", \"java.net.SomeChildException\") ?. Related to my question above, is this how these types of nested exceptions are actually recorded?  Or would it be more like stats.scope(\"rt\", \"incoming\", \"service\", \"/#/foo\", \"failures\", \"bar\").counter(\"baz\").incr() ?. I considered this, but though it might be more boilerplate than necessary.  But you're probably right, it will be more clear.. I opted for a sealed trait because scala enums give me the heebie jeebies. I think this could be more simply written as something like:\ncase Seq(\"rt\", stack, \"failures\", exception@_*) => (..., labels0 :+ (\"exception\" -> escapeLabelVal(exception.mkString(\":\")))))\n. lets take this opportunity to rename this.  (they used to be called \"loggers\" before they were renamed to \"requestAuthorizers\"). ditto. we should perhaps remove this example file, considering Istio support is deprecated.. A link in this comment to the underlying Finagle and/or Netty issues that cause this would be good.. TIOLI: I think you can make this more concise with\ncase Seq(\"rt\", stack, identifier) if Seq(\"service\", \"client\", \"server\").contains(stack) && !labelExists(labels0, stack) =>\n``. nit: probably call this variablemillisecondsinstead ofseconds.. I think this indented farther than we typically do for lists (see other configs in this directory).  The fact that both are apparently valid yaml is crazy :exploding_head: !  We should be consistent.. I think this should still be namerd?. I think this should be namerd?. This isn't a timeout, right?  it's just the consul API reporting that the requested DC does not exist.  I think we can improve the error message here to reflect that.. maybe include a link to the consul issue you filed in a comment here, as justification for why we need to do this.. Are you sure? I don't see any reason why a DC couldn't become unavailable.  I would expect in that case that the vaddr would should go from having a value to being neg.. This is a property of response classification, so it doesn't just apply to failure accrual, it also applies to metrics, for example.  Therefore, I wouldn't name thisnonAccruableStatusCodesbut would instead name it something likesuccessStatusCodes.  . I believe that status code 0 will also be counted as a success, which is a bit confusing because it doesn't appear in this list.  What do you think of removing the special casing for 0?  i.e. making it so ONLY status codes in this list are counted as success (but making[0]` the default value for this property). might be a bit more general to make this\ndef success(status: GrpcStatus): Boolean. this is the special casing I mentioned above. I think this could then become:\nclass AlwaysRetryable(override val success: Set[GrpcStatus] = Set(Ok())) extends .... consider adding a def that implements the defaulting so that you don't have to repeat it.\ndef _successStatusCodes = nonAccruableStatusCodes.getOrElse(Set(Ok())). this promise is never satisfied?  why shouldn't this be a pass through to reader.onClose?. for the sake of symmetry, it would be nice to not define this val and just to leave the success def abstract, just like we do for retryable.. for the sake of symmetry, it would be nice to do this the same way that we do for retryable: by overriding the def. Isn't this already the behavior?  None.toSeq.flatMap{ ... } will be Seq.empty.  If you just want to add the logging, you can hide it behind an if (watch.isEmpty). typo: to can. I think this means that we have to release this as 1.6.0. maybe: \"... will be streamed instead of being fully buffered in memory.\". This allows for the streaming of large messages, even when chunk-encoding is not used.. consider moving breaking changes up to the top. Add some context: Rather than enforcing a hard size limit, Linkerd will now stream chunks that exceed streamAfterContentLengthKB.. s/routing/streaming. I think you're right.  The difference in behavior from today would be that if we get a 5XX which is not a \"No path to DC\" error while the Addr is pending, we would immediately update it to Neg (and then retry) instead of leaving it as Pending and retrying.  But I think that's okay.\nIt's been a while since I've read this code so I'm trying to remember how it all works.  I don't see anything in here for dealing with 404s.  Do you remember how SvcAddr deals with service deletion?  When a service is deleted, does that catalog call just return an empty address list?. @edio This is a great write-up!  Yes, I agree, I think it makes sense to merge these 3 cases.. @edio I think you're right.  It looks like these two branches used to do different things (a 5XX would update the state to Neg whereas a timeout would not) but a recent change (https://github.com/linkerd/linkerd/commit/a17406b43a928a59c8d1bdd94ddcb0f0bc8588cb) effectively make these branches identical.  I think we can safely unify them.. whoops, you're right.  I don't think this was added for any specific use-case, but just as standard defensive programming.  See: https://github.com/linkerd/linkerd/pull/632/files#diff-ea0487e1d8c6b88af1a70cdbf4dc02f6R55. I see where you're coming from.  This branch also is questionable because it halts the request loop instead of restarting the watch or something like that, which is probably not great.  But still, since it's mostly unrelated to this change, I'd prefer we don't make changes unless we have a clear motivation for doing so.. Alternatively, just put SOMAXCONN in the Default value column instead of None.. The semantics of this are a bit unexpected, so this should definitely have a comment.  I think the returned future is satisfied when the ping is written, not when the ack is received as one might expect.. the second two arguments are ignored when NullConfig is passed in.  Consider passing null for them so that their values aren't red herrings.. Do we ever expect to hit this branch?  If so, consider making eventLoop an Option.  Otherwise, consider throwing a more explicit error here instead of letting this trigger an NPE somewhere else.. possible race condition where done could be set to null before this is called. personally, I like writing val _ = write.ping() to signal that there is a fire-and-forget future being dropped on the floor here.. Consider making this an AtomicReference and using cas state transitions to avoid race conditions.. This will register an additional callback every time this dispatcher receives a frame.  I think this should probably be set up when the detector is created.. I'm looking at the mux implementation for comparison and it doesn't explicitly tear down the connection on close of the detector.  Instead, it just sets its status to the detector's status.\nI guess some other layer of the stack closes the service if the status becomes closed, maybe?. possible race condition where this could become null. do we need this? we never need to handle pings ourselves, right?. perhaps add a comment that we need to propagate ping acks so that we can inform the failure detector that an ack has been received.. This is a direct mirror of https://github.com/istio/api/blob/6e481630954efcad10c0ab43244e8991a5a36bfc/proxy/v1/config/dest_policy.proto#L62 so we shouldn't edit this directly, even to fix typos. ah, you're right. This only occurred to me just now but: could this class extend com.twitter.finagle.serverset2.client.apache.ApacheZooKeeper and override the getDataWatch method?  much less duplicated code if we can do that. I think we should do a null check here to defend against malicious acks. :+1: to calling this sendPing.  I still think a comment on the semantics wouldn't hurt.. might be helpful to pass these as named parameters. This is so much more readable than what was here before!  :star:. This method really is the heart of the zk watch code and is very semantically dense.  Perhaps adding some comments will help future spelunkers understand it more easily.. This could use a comment explaining that this map is just to perform side effects (logging and updating the activity). This filter could use a comment explaining that this, in combination with the toFuture() below, creates a future that is satisfied when the state becomes Determined, ConnectedReadOnly, SaslAuthenticated, SyncConnected, or an unknown state.. I think this is unused. Failed is a member of Activity so I think this was correct without the space: Activity.Failed.. ",
    "rmars": ":star: (*I am not an expert on this stuff)\n. :+1:\n. :+1: \n. Some things, like .toc are from the linkerd site, and don't make sense here, but I suppose the simplest/easiest way to keep the files/styles in sync is just to copy the whole file.\n. I have to put the router list dropdown inside div#navbar for the collapsing to work properly, but now the top menu looks a little weird.\n\nI could use container-fluid to have the top menu span the whole page, what do you think?\n\n. Ok! I'm waiting until I merge https://github.com/BuoyantIO/linkerd-site/pull/31 so I can copy over the local.css.\n. :+1: \n. :+1: \n. :+1: \n. :star: yayyyy so much less confusing\n. :star:\n. :+1: \n. This is my fault: the navbar is now a little too narrow for long router names.... perhaps I should use container-fluid for this navbar instead.\n\n. :+1: \n. :star: the examples are super helpful\n. :+1: lgtm\n. :+1: yay!\n. :+1: \n. :star: :ship: \n. I think it's because this is in the pathCache and that path doesn't resolve, so we're caching a NameTree.Neg, with value noBrokersAvailableFactory (found in com/twitter/finagle/buoyant/XXX_NameTreeFactory.scala)... and neither of those paths resolve so they're both cached as NameTree.Neg => exception factory which is why it's returning the same exception.... @olix0r explained the binding/caching to me but I'm still shaky on some of it.\n. Sure! Let me rebase!\n. :star: <- rebase\n. :star: :(\n. :star:\n. :star: <- rebase\n. :+1: \n. :star: \n. :star:\n\n. :+1: \n\n. :+1: \n\n. :+1: \n\n. :star: lgtm\n\n. :+1: \n\n. :+1: \n\n. :+1: \n\n. Would you say that this is fixed in the new dashboard?  Or do we still need some work on the naming?\n. :star: \n\n. :star: :v: \n\n. I've used http://colorbrewer2.org/ in the past :fried_shrimp: \n. :star: :rainbow: works for me now!\n\n. Question - why enable it in Router.scala instead of putting a default value in LoadBalancerConfig.scala?\n. Ooh, I'll add it.  Thanks!\n. :star: :rainbow: :art: \n\n. LGTM! yayyy\n\n. :star: :cake: yay\n\n. Latest design specs: \ncolor_guide.pdf\ndefault_view_grid.pdf\ndefault_view_spec_typography.pdf\nrouter_detail_sizing_spec.pdf\nrouter_detail_styles.pdf\nrouter_detail_typography_spec.pdf\n. The initial dashboard (rough version) is in master, so I am closing this.\nWork towards a general release will be tracked in #275\n. Combined graph was made in #227 - I'll close this and we can make another ticket for adding the latency graph to the client module.\n. Note: as soon as #189 passes CI I'm going to merge it into master, and then put this back up again with parent master.  (Since I can't change the parent of this review once it's posted).\n. Rebased off master: see #192 \n. :star: :art: :8ball: yay\n\n. :star: :ghost: \n\n. I think our error message colours need to be updated too:\n\n\n. :star: :grapes: yay\n\n. :star: :package: :rabbit:  I like this branch a lot\n\n. \n. :star: :grapes: :dart: cool!\n\n. :star: :clipboard: \n\n. I checked Safari and Firefox, and they look ok.\nI'm going to mark this as complete, and we'll just have to keep in mind that we should check browser compatibility, since this is more ongoing.\nFirefox:\n\nSafari:\n\n. :star: :ship: :baby_chick: \nThanks for removing the dashboard code! I was thinking about doing that too.  And the controller code looks much better.\n\n. :+1: :octopus: :dog: yay graphs!\n\n. Yeah, agree about the duplication! \n. :star: :performing_arts: :truck:  yay!\nI know we're not supporting responsiveness for now, but maybe we can revisit that in the future.  When I downsize the window to about 2/3 of my screen, it gets weird.\n\n\n. Made #234 \n. It's mostly a cleanup/polish ticket - It's not super important, but might be nice to address\n. Moving metrics on the frontend to using the metricstree structure will remove the need to create query regexes, so this will soon be unneeded.. :star: lgtm!\n\n. I think I\"m just going to make this part of #242\n. I rebased on top of esbie's branch that moves all the resources.\nAlso fixed bug where client legends were the wrong colour.\n\n. :star: :deciduous_tree: \n. There's no way to apply exclusions currently.  There is an open issue in pullapprove:\nhttps://github.com/pullapprove/support/issues/52\n. Won't fix for now...\n. :star: :eyes: works for me locally\n\n. :star: :abcd: \n\n. :star: lgtm\n\n. This was fixed as part of #304 \nWe decided to use font weights rather than font-medium, font-light, etc.\n. Agreed.  With longer names and more clients, I think we'll run out of URI space... Though there do seem to be things we can do with the current state that would make it a little better.  But we will have to solve this more robustly.\nExample of a current URI:\n- Some metrics are repeated\n- Some query params are blank\nm:\nm:\nm:jvm/uptime\nm:jvm/thread/count\nm:jvm/mem/current/used\nm:jvm/gc/msec\nm:rt/divider/srv/0.0.0.0/4117/load\nm:rt/multiplier/srv/0.0.0.0/4116/requests\nm:rt/adder/dst/id/$/inet/127.1/9090/connections\nm:rt/adder/dst/id/$/inet/127.1/9089/connections\nm:rt/adder/dst/id/$/inet/127.1/9088/connections\nm:rt/divider/dst/id/$/inet/127.1/9093/connections\nm:rt/subtractor/dst/id/$/inet/127.1/9091/connections\nm:rt/multiplier/srv/0.0.0.0/4116/load\nm:rt/adder/srv/0.0.0.0/4114/connections\nm:rt/subtractor/srv/0.0.0.0/4115/connections\nm:rt/multiplier/srv/0.0.0.0/4116/connections\nm:rt/multiplier/dst/id/$/inet/127.1/9092/connections\nm:rt/divider/srv/0.0.0.0/4117/requests\nm:rt/subtractor/srv/0.0.0.0/4115/load\nm:rt/subtractor/srv/0.0.0.0/4115/requests\nm:rt/adder/srv/0.0.0.0/4114/load\nm:rt/divider/srv/0.0.0.0/4117/connections\nm:rt/adder/srv/0.0.0.0/4114/requests\nm:rt/multiplier/dst/id/$/inet/127.1/9092/loadbalancer/load\nm:rt/multiplier/dst/id/$/inet/127.1/9092/success\nm:rt/multiplier/dst/id/$/inet/127.1/9092/load\nm:rt/multiplier/dst/id/$/inet/127.1/9092/requests\nm:rt/multiplier/srv/0.0.0.0/4116/success\nm:rt/divider/dst/id/$/inet/127.1/9093/success\nm:rt/divider/dst/id/$/inet/127.1/9093/load\nm:rt/divider/dst/id/$/inet/127.1/9093/loadbalancer/load\nm:rt/divider/dst/id/$/inet/127.1/9093/requests\nm:rt/divider/srv/0.0.0.0/4117/success\nm:rt/subtractor/dst/id/$/inet/127.1/9091/load\nm:rt/subtractor/dst/id/$/inet/127.1/9091/loadbalancer/load\nm:rt/subtractor/dst/id/$/inet/127.1/9091/success\nm:rt/subtractor/dst/id/$/inet/127.1/9091/requests\nm:rt/subtractor/srv/0.0.0.0/4115/success\nm:rt/adder/dst/id/$/inet/127.1/9090/requests\nm:rt/adder/dst/id/$/inet/127.1/9090/success\nm:rt/adder/dst/id/$/inet/127.1/9089/success\nm:rt/adder/dst/id/$/inet/127.1/9088/load\nm:rt/adder/dst/id/$/inet/127.1/9089/loadbalancer/load\nm:rt/adder/dst/id/$/inet/127.1/9089/requests\nm:rt/adder/dst/id/$/inet/127.1/9090/loadbalancer/load\nm:rt/adder/dst/id/$/inet/127.1/9088/success\nm:rt/adder/dst/id/$/inet/127.1/9090/load\nm:rt/adder/dst/id/$/inet/127.1/9088/requests\nm:rt/adder/dst/id/$/inet/127.1/9088/loadbalancer/load\nm:rt/adder/dst/id/$/inet/127.1/9089/load\nm:rt/adder/srv/0.0.0.0/4114/success\nm:rt/adder/dst/id/$/inet/127.1/9089/requests\nm:rt/adder/dst/id/$/inet/127.1/9089/connections\nm:rt/adder/dst/id/$/inet/127.1/9089/success\nm:\nm:rt/adder/dst/id/$/inet/127.1/9090/requests\nm:rt/adder/dst/id/$/inet/127.1/9090/connections\nm:rt/adder/dst/id/$/inet/127.1/9090/success\nm:\nm:rt/adder/dst/id/$/inet/127.1/9088/requests\nm:rt/adder/dst/id/$/inet/127.1/9088/connections\nm:rt/adder/dst/id/$/inet/127.1/9088/success\nm:\nm:rt/divider/dst/id/$/inet/127.1/9093/requests\nm:rt/divider/dst/id/$/inet/127.1/9093/connections\nm:rt/divider/dst/id/$/inet/127.1/9093/success\nm:\nm:rt/multiplier/dst/id/$/inet/127.1/9092/requests\nm:rt/multiplier/dst/id/$/inet/127.1/9092/connections\nm:rt/multiplier/dst/id/$/inet/127.1/9092/success\nm:\nm:rt/subtractor/dst/id/$/inet/127.1/9091/requests\nm:rt/subtractor/dst/id/$/inet/127.1/9091/connections\nm:rt/subtractor/dst/id/$/inet/127.1/9091/success\nm:\n_:1460405457705\n. :star: :bookmark_tabs: \n\n. \ncolor_guide.pdf\ndefault_view_grid.pdf\ndefault_view_spec_typography.pdf\nrouter_detail_sizing_spec.pdf\nrouter_detail_styles.pdf\nrouter_detail_typography_spec.pdf\n. I'm going to close this out, as we no longer need it to keep track of a long list of dashboard TODOs :)\n. :star: lgtm!\n\n. :star: :clipboard: \n\n. :star: :pencil2: \n\n. :star: :ice_cream: \n\n. :star: we do advise java 8 in our instructions (https://linkerd.io/doc/getting-started/index.html)\n\n. Added new font:\n\n. :star: \ud83c\udf33 \n\n. :star: \ud83c\udf43 \n\n. \u2b50 \u2650 \n\n. Added prefixing clients with / here\n\n. :star: \ud83d\udc63 \n\n. lgtm! do we want to update x.x.x to 0.3.2 and create a new x.x.x section?\nalso, do we mention the finagle upgrade? I know we have done so in the past\n\n. :ship: \ud83d\udc30 \n\n. :star: woww nice.  idk if you want to call out the new config file in changes.md\n\n. :star: \ud83d\udd11 \n\n. Things I will do:\n- Add back the horizontal client colour line\n- Have the graph scaling be \"downward\" ie we want to compare against 100%\n. :star: \ud83d\udc9b  Nice! thanks for doing this! \nagreed with esbie about ci \n\n. :star: \ud83d\udd11 \n\n. >  In the 100% screenshot, I don't actually see a line at 100% in the line graph.\nIn the 100% case, the top and bottom of the smoothie charts show 100% so the bottom line in that case is the 100% line.  I am looking into how to make this less weird, I don't think smoothie deals well with this.\n\nIn the other screenshots, the y-axis labels range from 0.0 to 1.0 instead of 0% to 100%\n\nMy fault!  I updated the axes late, after I'd taken some screenshots.\n\nThe line graphs look very different client to client but the numerical success rate is the same for each client. Is this just an artifact of the fake data?\n\nCorrect!  I mocked the numerical data to show 3 cases - good (green) warn (yellow) and bad (red) and I mocked the graph data with a Math.random() - I should have been more clear about this!\n. @olix0r here are some more screenshots demonstrating what it would look like in various situations close to 100%! \n (Graph data only is mocked, the other numbers won't make sense.)\nSuccess rate between 99.0% and 99.9%:\n\nSuccess rate between 99.90% and 99.95%:\n\nAt 100%:\n\nAt 99.9%:\n\n. I added a chart hack that will create the reference line at 99.99% if the successRate is 100% - let me know what you think:\n\n\n. @siggy - I've now coerced it into doing that.  \nIt kiiiiind of breaks the idea that we were doing responsive columns, a la bootstrap, but it does look less weird.\nThis is what it looks like now, let me know what you think:\n\n\n. :star: \n\n. Ooooh yes good point - let me add that.\nAdded client success labels and server success labels:\n\n. @wmorgan here you go:\n\n. When I tested it out, it looked like more than the first data point had an unusually large value - so I left out the first couple to be sure. Let me check again. It does mean a more seconds of waiting for the graph after loading the dashboard page.\n. Yep, checked again... the first 4 ish values are way too high.\nDelaying by 4 data points at our current polling rate means a 4 second delay until correct data is showing up from metrics.json...\n. I've added a little indicator to the request totals when we're in a specific router's page - hopefully it will make it clearer that the request totals are still for all routers.  It looks weird but I think maybe less confusing. lmk what you think:\n\nWe could also do things like hide the request totals row in a Router Page, I wonder if the context of all routers is needed/wanted\n. Updated summary banner: (hide on router page)\n\n. Fixed by #1053 . Yes indeed, I intend to keep unifying!\nFonts, backgrounds still to come. Wanted to keep it incremental.\n. Thanks!! Good questions.\n- The Help section is a link to linkerd.io - so I think if we wanted to really unify it we would make a version of the docs available offline. IIRC we did want to do this at some point?\n- You're correct about the logging page! So I looked into restyling it and got this far:\n  \nI do this by writing a filter before LoggingHandler that injects our own css and a bit of js into the logging page html, such that I can now control the theming. It is maybe a sketchy solution, what do you think?\n. I think that would be fine. If we want the complete site's docs I think we'll need to figure out overall navigation, updating content, etc which is probably not trivial, but I will look into it too.\n. (Addressing esbie's comment in #505)\n. (addressing esbie's comments in #505)\n. closing this, issues have been addressed. Suggestions for the dashboard changes (wordsmithing appreciated):\n* Admin site/dashboard UI improvements:\n  * Added server success rate graphs to the dashboard, improved responsiveness\n  * Added the ability to navigate to a specific router's dashboard\n  * Standardized the look and feel of the admin pages\n. Most of these are now addressed. Filed #1321 for the point about styling the rules.. :star:\nlgtm! I think it's failing because we need to specify in our eslintrc that we'd like es6 syntax: http://eslint.org/docs/user-guide/configuring\n\n. oh! though we don't use es6 here yet... might just wanna take out that default syntax\n. Seems quite likely! Let me try making the clients collapsible as you mentioned in slack, so that for large numbers of clients this page isn't doing so much.\n. Since > 6 clients are now automatically collapsed, we have max ~10 listeners per router... and maybe you expand a bunch of listeners but if it does get to the point that _.find is slow, I think the other components of the page are limiting us more. I don't think it makes sense to optimize this (yet).. :star: :art: good find!\n\n. lgtm!\n\n. lgtm!\n\n. Oops, yes, this was fixed in https://github.com/BuoyantIO/linkerd/pull/889. \n. Looks awesome, thanks for doing this.\nI think we'll also need to add something in router_clients.js - when new clients show up they'll just be appended to the dom... then the clients won't appear in sorted order.  \nSo we'd need to clear and reappend all the client-containers. This could be expensive if there's a lot of clearing and reappending going on so maybe that's not worth it, since you could always refresh to get a sorted page. (Though maybe we don't want people to have to do this).\nexample:\nadd a bunch of clients\n\nneed to refresh to resort:\n\n. I think we'll also need to change assignColorsToClients in router-clients.js(my fault for making it order-dependent) - the reordering is causing color mapping issues, were more than 1 client is assigned the same color, even though we have more colors available:\n\n. yeah! good point! we can use $.insertBefore and the like to just do inserts. Agreed with both of you - if we have 100 clients we probably don't want that many lines on the graph. But also, the behaviour should be consistent. I vote that no collapsed clients should show up in the graph.\nThe reason all the initially loaded clients show up on the combined client graph is because we call router.clients() in combined_client_graph.js. Shouldn't be too bad to unify the behaviour.. (the  requirejs optimizer http://requirejs.org/docs/optimization.html will probably also be helpful). Options:\n- [x] #1028 precompiling templates \n    - reduces current load times by ~1/3 \n    - this is easy to do\n    - requires slight changes to how we develop when adding a new Handlebars template\n- [ ] using r.js to optimize requirejs (serve all our js in a bundle)\n    - reduces current load times by ~1/2\n    - easy enough to do on the js side BUT\n    - requires changes in how we develop/deploy\n    - requires us to either set up a build tool or very manually switch between dev and the bundle we serve UNLESS we add some sort of flag when we run linkerd that will serve unbundled js\n- (related to ^) adding a build tool e.g. grunt, gulp, webpack etc \n    - would add more complication to our dev process \n    - however then we could have nice things like es6 transpiling\n    - If we have es6 modules, we can get rid of require (though my hesitancy to add more build tools was why I chose require in the first place)\n-  [x] #1037 for fonts, serve font locally. @olix0r I do like the idea of the admin UI in a separate repo that produces an asset bundle. Seems like it could be a clean separation. Not sure yet how that's going to work with plugins that provide functionality to the admin site. Would it be too messy to have contributors of a plugin also contribute to linkerd-ui and have some way for the ui to know which plugins are being used?\n@esbie what do you think?. Yep! Plugins can provide their own compiled or not compiled templates since all we do is include them with require.\nYou can still compile templates yourself, by using the old way, as I haven't removed the text plugin:\ndefine(['Handlebars', 'text!template/logging_row.template'], function(Handlebars, notCompiledTemplate) {\n  var template = Handlebars.compile(notCompiledTemplate);\n}\nOr by running handlebars -a js/template/ > js/template/compiled_templates.js to compile those templates in whatever plugin directory they are in and then including those.\n. hmmmm not if we want to also support manually adding and compiling templates (e.g. https://github.com/linkerd/linkerd/pull/1025). if we want to force everyone to precompile templates, then yeah we can!. admin endpoints as seen at http://localhost:9990/admin\n\n[x] /\n[ ] /admin/announcer\n[x] /admin/contention\n[x] /admin/lint (https://github.com/linkerd/linkerd/pull/1086)\n\u2757\ufe0f seems to work but has a js error\n\n\n[x] /admin/lint.json\nworks fine via curl, though doesn't display nicely (like metrics.json) when accessed from a browser\n\n\n[x] /admin/metrics\n[x] /admin/metrics.json\n[x] /admin/metrics/prometheus\n[x] /admin/ping\n[x] /admin/pprof/contention\nit... downloads something after a couple seconds.\n\n\n[ ] /admin/pprof/heap\n[x] /admin/pprof/profile\n[x] /admin/registry.json\n[x] /admin/server_info #1130 \n\u2757\ufe0f it works but the info doesn't seem very useful (#1058)\nthe class-package-name/build.properties file is not being read for linkerd-main...\n\n\n[x] /admin/shutdown\n[x] /admin/threads (https://github.com/linkerd/linkerd/pull/1086)\n\u2757\ufe0f \"/admin/files/js/threads.js\" does not exist\n\u2757\ufe0f the toggling doesn't work because no jquery is loaded\n\n\n[x] /admin/threads.json\nworks fine via curl, though doesn't display nicely (like metrics.json) when accessed from a browser\n[x] /admin/tracing\n[x] /bound-names.json\n[x] /config.json\n[x] /delegator\n[x] /delegator.json\nif you don't provide a path you get 400 Bad Request, by design\n\n\n[x] /favicon.png\n\u2757\ufe0f 404\n\n\n[x] /files/\n\u2757\ufe0f doesn't return anything on the index /, but serves files.\n\n\n[x] /help\n[x] /logging\n[x] /logging.json\n[x] /namerd\n[x] /requests . I think we still need to test \n[x] /admin/tracing\n[ ] /admin/announcer\n[ ] /admin/pprof/heap. Hmmm it's lisenced under an Open Font License\n\n```\nCopyright 2010, 2012 Adobe Systems Incorporated (http://www.adobe.com/), with Reserved Font Name 'Source'. All Rights Reserved. Source is a trademark of Adobe Systems Incorporated in the United States and/or other countries.\nThis Font Software is licensed under the SIL Open Font License, Version 1.1.\nThis license is copied below, and is also available with a FAQ at:\nhttp://scripts.sil.org/OFL\n\nSIL OPEN FONT LICENSE Version 1.1 - 26 February 2007\nPREAMBLE\nThe goals of the Open Font License (OFL) are to stimulate worldwide\ndevelopment of collaborative font projects, to support the font creation\nefforts of academic and linguistic communities, and to provide a free and\nopen framework in which fonts may be shared and improved in partnership\nwith others.\nThe OFL allows the licensed fonts to be used, studied, modified and\nredistributed freely as long as they are not sold by themselves. The\nfonts, including any derivative works, can be bundled, embedded, \nredistributed and/or sold with any software provided that any reserved\nnames are not used by derivative works. The fonts and derivatives,\nhowever, cannot be released under any other type of license. The\nrequirement for fonts to remain under this license does not apply\nto any document created using the fonts or their derivatives.\nDEFINITIONS\n\"Font Software\" refers to the set of files released by the Copyright\nHolder(s) under this license and clearly marked as such. This may\ninclude source files, build scripts and documentation.\n\"Reserved Font Name\" refers to any names specified as such after the\ncopyright statement(s).\n\"Original Version\" refers to the collection of Font Software components as\ndistributed by the Copyright Holder(s).\n\"Modified Version\" refers to any derivative made by adding to, deleting,\nor substituting -- in part or in whole -- any of the components of the\nOriginal Version, by changing formats or by porting the Font Software to a\nnew environment.\n\"Author\" refers to any designer, engineer, programmer, technical\nwriter or other person who contributed to the Font Software.\nPERMISSION & CONDITIONS\nPermission is hereby granted, free of charge, to any person obtaining\na copy of the Font Software, to use, study, copy, merge, embed, modify,\nredistribute, and sell modified and unmodified copies of the Font\nSoftware, subject to the following conditions:\n1) Neither the Font Software nor any of its individual components,\nin Original or Modified Versions, may be sold by itself.\n2) Original or Modified Versions of the Font Software may be bundled,\nredistributed and/or sold with any software, provided that each copy\ncontains the above copyright notice and this license. These can be\nincluded either as stand-alone text files, human-readable headers or\nin the appropriate machine-readable metadata fields within text or\nbinary files as long as those fields can be easily viewed by the user.\n3) No Modified Version of the Font Software may use the Reserved Font\nName(s) unless explicit written permission is granted by the corresponding\nCopyright Holder. This restriction only applies to the primary font name as\npresented to the users.\n4) The name(s) of the Copyright Holder(s) or the Author(s) of the Font\nSoftware shall not be used to promote, endorse or advertise any\nModified Version, except to acknowledge the contribution(s) of the\nCopyright Holder(s) and the Author(s) or with their explicit written\npermission.\n5) The Font Software, modified or unmodified, in part or in whole,\nmust be distributed entirely under this license, and must not be\ndistributed under any other license. The requirement for fonts to\nremain under this license does not apply to any document created\nusing the Font Software.\nTERMINATION\nThis license becomes null and void if any of the above conditions are\nnot met.\nDISCLAIMER\nTHE FONT SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT\nOF COPYRIGHT, PATENT, TRADEMARK, OR OTHER RIGHT. IN NO EVENT SHALL THE\nCOPYRIGHT HOLDER BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\nINCLUDING ANY GENERAL, SPECIAL, INDIRECT, INCIDENTAL, OR CONSEQUENTIAL\nDAMAGES, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\nFROM, OUT OF THE USE OR INABILITY TO USE THE FONT SOFTWARE OR FROM\nOTHER DEALINGS IN THE FONT SOFTWARE.\n```. Yeah was gonna say the same thing. I think we should include the license but then I think it's fine(?). Related: #1036 . Updated to show all clients if all clients are collapsed: \n\n. Hmmmm unclear. I tested a plain smoothie chart and it handled 400 plotted lines easily. I suspect our data processing to do other things in the dashboard might cause us problems rather than the chart. Maybe we should test this further!\n\n. Yeah it may be a good idea to just test ourselves and see what happens!. Totally! I did do that just to see and I do get deprecated errors. Still so weird that we don't use any deprecated things tho!\n\n. Oops, reverting that behaviour was not intentional. I'll check it out. . doesn't that give us counters per linkerd instance, rather than counters per individual configured router?. ohhhh nvm, you're replying to the first comment!. Btw I am able to reproduce this using the http example: ./sbt linkerd-examples/http:run - when I run the usage collector locally and look at the messages it sends, I see counters:<name:\"srv_requests\" > counters:<name:\"srv_requests\" value:2 >\n. blocked on finagle 6.43.0 being released. Ok.  This is how it looks if I remove the columns. For dtabs with more than one entry, I feel like columns are easier to read. wdyt? We could remove the columns for things with only one entry, though I'm sure if you have multiple short entries, it'd still look weird.\nLong dentries (no columns):\n\nLong dentries (columns):\n\nMany short dentries (no columns):\n\nMany short dentries (columns, still looks strange):\n\n. Okay, took a stab at the combined approach!. @klingerf do you still want to leave this open for tracking?. reset is now a link:\n\n. ooooooooh, good find! this was caused by \"dispatcher\" being among the clients returned in metrics.json, and me not accounting for the case that things can be returned in metrics.json that don't have valid client names. added a check and a test for this case.. Okay, I removed the handling of expired clients from router_clients, and each client now controls its own isExpired and updates it on calls to metricsHandler. One result of this is that, since metricsHandler isn't called on minimized clients, a collapsed client won't be hidden unless the user expands it, reactivating the metricsHandler, enabling us to tell whether the client is expired.\nGood idea, re the states of RouterClients. Since I'll be revamping expand/collapse in that branch, my plan is to leave the above behaviour as is for now. . @ethanrubio yeah I think you can do this without granting cluster-admin access. We're working on adding example RBAC configs (see configs at https://github.com/linkerd/linkerd-examples/pull/165 for granting namerd access to the third party resource).. Okay, this should now be behaving as expected.\nNotes: if the expand all/collapse all links are clicked, when clients are unexpired, they'll obey this rather than 'custom'. However /newly added/ clients will still be collapsed if there are more than 6.. I like middot!\n\nHere's slash for comparison:\n\n. Yeah! I think the border around the client sections makes it weird to also add the green/red vertical lines there (there's slightly less space). There are some simple things we could do without thinking too much about the design. wdyt?\nI could make it uniform and take away the vertical lines from the server section:\n\nOr, I could add vertical white lines in the server section, further differentiating it from the client:\n\nAlternatively, we could have some sort of border around the server section:\n\n. Moved to https://github.com/linkerd/linkerd-inject/pull/1. hey @maruina! thanks for letting us know! as @adleong said, a frame dump would help! . Thank you for the report @cb16! We'll investigate.. Thanks for filing this, @LukaszMarchewka! We'll take a look.. Thanks for the detailed bug report, @Capitrium - we'll take a look!. question - there isn't a basic-concepts section in this file... is this just something we're adding later? Should we put a blank section and a TBD?\n. do these comments about primary servers apply now that we're showing metrics for all of them?\n\"Big summary board of \"most active\" server (by number of requests)\" <-- seems like that's no longer the case\n. line 17 of this file still refers to the most active server :'( \n. is recommend a strong enough word here? don't they /have/ to match? I think you'll get unexpected results if they're different (https://github.com/BuoyantIO/linkerd/issues/27)\n. total nit = line 39 uses ' and line 40 uses \"\n. Whoops, yep, I was using the wrong snapshot in my testing and didn't pick this up :(\n. ooh, thanks for catching. let me update.\n. Yeahhh this happens for the calculator app\n\n. oh no! my editor does this :(\n. Sure!\n. should we also test the other tls plugin (io.l5d.clientTls.boundPath)?\n. tioli - \"Running tests...\"?  This statement appears between two \"Starting http server\" statements so looks a little incomplete without an object\nStarting http server on 9999...\nRunning...\nServing HTTP on 0.0.0.0 port 9999 ...\n. Sure! It is a long name as is.\n. yep! thanks!!!\n. Though, if we left it, it would match finagle....\n. I was trying to be explicit about defaulting to true, but I am totally open to changing it.\n. Yeah, I totally agree with you re: boolean and that name... right now I'm still leaning towards matching finagle, lmk if you have a stronger opinion :)\n. correct! that's my intent.\n. ah! eventually I wanted it to call various update functions on success, eg calling a render method on various dashboard components.  Also, calling routers.update(metrics); seemed to make more sense from here than from the render() method inside the dashboard.  I could just move those function calls to render() though.\n. Yes!  will do in a follow up branch.  Thanks!\n. mine keeps doing this too.  it is driving me nuts\n. Oh sure! I just wanted to be clear what they were, so people adding future modules would know explicitly what those were.  Would you like me to update this to not have the var?\n. I think for the summaries we actually wanted to display the delta.\n. Totally agree\n. This can go outside the map()\n. I didn't want to invest too much time in consolidating this code because I'm 90% sure we're not going to use failure rates... however I wanted failure rates to be in here so we could see how it looked before making a decision.  Happy to consolidate/refactor, lmk what you think.  \nI should add a todo to remove this also.\n. yeah, this simplifies the code a lot! thanks!\n. We don't need these any more\n. errant console.log :)\n. Ah yes! It's a naming convention - if it is a jquery object, I name it $variable.  I don't know if we actually use this convention in this codebase.  Should I change it?\n. Can you update (or delete :smile:) the comments at the top of this file?\n. argument metricName is unused\n. now that each listener registers itself with the metrics collector, I don't think we need to keep track of all these any more\n. yeah that does simplify things!\n. Fixed this... the intermediate div was unnecessary\n. Ok! Will revisit once that branch lands, and possibly remove in a later branch.\n. ah! indicates that it's a wrapped jquery object. just a convention.\n. yesss I totally just noticed this too :) :(\n. I think we are? I will change it.\n. That seems reasonable.  I'll address in a future branch.\n. Okay! I am not tied at all to using a table - I could change this.\n. I'm getting the metrics here from client.metrics (from routers.js), which just returns \"request_latency_ms.max: 0\" etc...  So the client name isn't in name in this case.  \nI'm not actually sure whether I should be using client.metrics from routers or if I should be getting latency from metrics.json.\n. Then pullapprove requires approval from /all/ members :(\nA value of -1 means that approval is required from all members.\n. true, though I think naming it \"requests\" makes it a little clearer to read\n. data.failures isn't always present in the returned metrics (if there haven't yet been any failures) so sometimes the dashboard will display NaN% where it should display N/A.  We should (I can do this later) do some checks on data.failures.\n. Oliver's branch (#258) adds || 0 to these arguments, so I think I'll remove this line\n. Oh, the failure rate calculation also uses it, so I will just leave as is.\n. tioli: it might be clearer to say:\nGet a dtab for the default namespace\nsince default is a common word...\n. Yeah, this is served from the old dashboard - in theory people shouldn't arrive on this page unless they visit /legacy-dashboard\n. Oh, or the dtabs I guess.  Okay, I will make it point to the new one!\n. Yeah, that might be good\n. good point\n. I added an update() before we call setInterval so now, when we load the page, we shouldn't see blank spots in the client section.  We still see it when we add a new client, as we need to wait for the next call to update()... so I'll leave \"metric\" in as a placeholder for this case.\n. I don't think you meant to leave this commented - it takes out the totals summary :)\n. since we don't actually use these names we could just avoid naming them and do \nProcInfo(metricsCollector, $(\".proc-info\"), Handlebars.compile(overviewStatsRsp[0]), buildVersion);\n. These are from summary.css, which I believe we can delete once we delete the old dashboard.\n. omg, yes let me update that\n. Yeah, this maxValue doesn't make sense any more now that we're fully customizing the y range.  I got rid of this maxValue - the .001 is so that the graph line markers extend past the 100% success rate line (to make the graph look less confusing in the case of a pure 100% success rate). Before I was using a custom yRangeFunction, I was using the maxValue of 100.000001 to pin the graph's upper limit so that the 100% line would appear at the top and look reasonable, but the custom yRangeFunction does this better. I can pull these out into constants!\n. Ah yes! Thanks :)\n. Hm, good question! I am not really sure what the most idiomatic way to pass these variables along is, between building up an object, and passing that, or using html5 data attributes. Since we're only passing 2 variables, it probably doesn't make too much of a difference. If someone has a strong opinion they can chime in :)\nWe could make global json object for our constants, something like <script>var DASHBOARD_CONSTANTS = { router: \"\", linkerdVersion: \"\"}</script>, and since our application's js is loaded after our $content it'd work... but if we ever moved those scripts, then the variables would be defined before they're used. If we did <script id=\"data-router-name\" type=\"application/json\">$router</script> we'd need to find the div to get the router name anyway, similar to what we're doing now.\nPassing the data as part of the div does mean we're responsible for naming/obtaining the variables in the dashboard js code instead of here, which I kind of like, though it does mean we use an extra jquery selector to find the div.\nWe could make it its own js file, which we'd have to generate in the scala and then pass in in our list of javaScripts - but since we're only passing 2 variables it seems much. \nThis did make me think, though, I don't need a separate div for the new data, so I'll just put all the data under one div, so I can reuse the jquery selector on the frontend.\n. Good point! Created #485, I'll add it here.\n. Ooh, yes we can!\n. If we use js / load this from a file, then we can make changes to the html and not have to rebuild to pick up changes!\n. Good spot, updated!\n. hmmmm that's a good idea, though I wanted to use the AdminHandler to avoid copying the navbar html, as well as the stylesheets to also be inserted into the html for this page...\n. It is true - it won't change too often besides this first time. I can hardcode it!\n. ooh yes, good call\n. oo, interesting. would that consolidation be less confusing?\noriginally I separated it into showClient/hideClient to make the steps clearer:\n- handle the listener\n- update the UI\nadditionally, in the initialization section in line 100, I didn't want to have to call deregisterListener on a listener that we know doesn't exist if on initialization we don't want to register that listener.\nwhat do you think?\n. ah thanks for explaining, that's helpful. \nthe extra indirection probably isn't worth it. will move!\n. Yep indeed it's a no-op, I just didn't want to go through the list of listeners when I already know we won't find the item.\nAgreed that it would read more clearly in the if/else if it is just show vs hide.\n. Yeah! I do think we should get to it at some point. I think I'll make a ticket to keep track of this for later.\nTicket: #577\n. tioli: you can use the map shorthand here\n_.map(routers.clients(routerName),'label')\nhttps://lodash.com/docs/4.17.2#map. I think this is where karma likes to load its assets from:\nhttp://karma-runner.github.io/1.0/config/files.html\nBy default all assets are served at http://localhost:[PORT]/base/\n...\nNotice the base in the URL, it is a reference to your basePath. You do not need to replace or provide your own base.\nSo if our basePath was \"/foo\" karma would serve from \"/base/foo\" - but I am not sure we want to change the /base part. It loads our assets with that /base/ in the url, e.g. http://localhost:9876/base/js2/lib/lodash.min.js with relative requires.\nLooks like I can get around this by doing something like\nproxies: {\n  '/': '/base',\n},\nbut we don't rely on the urls anyway... so it doesn't matter if we are using /base. What do you think?. great point. if I use separate entrypoints I don't need a data attr to know we should initialize namerd admin. You're right! doing this means we can get rid of loading all the templates and passing them down from the router_controller too. I've moved templates/ to js/ templates and gotten rid of most of these $gets.\nI would like to stop passing these down, and instead directly include the templates in the modules that they are used. This will involve a little refactoring (the module initializations) so I will make a branch right after this one, so the changes will actually be reflected in the diffs.. we only instantiate RouterClients once (in RouterController) - so I think we are safe!. doh! yep that works!. +1 to this, would be nice to not nave to generate the table here. Good idea... I've added a simple toHaveLength matcher in this file, but may look at just including something (e.g. https://github.com/just-boris/jasmine-collection-matchers/blob/master/src/toHaveLength.js) instead. (Or write more of our own matchers and also have specs for these matchers).. (I'm working on that right now as part of #505!). in this file, <li><a href=\"admin/logging\">logging</a></li> on line 34 needs to be updated. I refactored these - they were being passed in originally:\nreturn function (metricsCollector, routers, client, $metricsEl, routerName, $chartEl, colors, $toggleLinks, shouldExpandInitially)\nThey're used later on in the function - maybe github has cut the rest of the function off:\n```\n    return function (metricsCollector, routers, client, $container, routerName, colors, shouldExpandInitially) {\n      var metricPartial = Handlebars.compile(metricPartialTemplate);\n      Handlebars.registerPartial('metricPartial', metricPartial);\n  var $contentContainer = $container.find(\".client-content-container\");\n\n  var $metricsEl = $container.find(\".metrics-container\");\n  var $chartEl = $container.find(\".chart-container\");\n  var $toggleLinks = $container.find(\".client-toggle\");\n  var $lbBarChart = $container.find(\".lb-bar-chart\");\n\n  var clientColor = colors.color;\n  var latencyLegend = createLatencyLegend(colors.colorFamily);\n  var metricDefinitions = getMetricDefinitions(routerName, client.label);\n\n  var $expandLink = $toggleLinks.find(\".client-expand\");\n  var $collapseLink = $toggleLinks.find(\".client-collapse\");\n\n  renderMetrics($metricsEl, client, [], [], clientColor);\n  var chart = SuccessRateGraph($chartEl.find(\".client-success-rate\"), colors.color);\n\n``. acursor: pointer` might be nice here to indicate we can click this. omg I like this a lot!. oops yeah, agreed!. Hmmmm what about passing the colorBorder? do you prefer that we create a class for each shade and then toggle that?. oh! right, I think they are in config order, let me remove this sort. (agree with you on separation of js/css, maybe we can refactor later). yeah idk. I don't mind it that much, I do think it's informative... though it makes us have to do this sometimes. great question. it does! I tested it out with admin.yaml and it works. I asked @siggy and he says the admin scala is unaware of the root url.\nif you use admin.yaml that serves admin at /foo/ and inspect request.path, it's unaware that it's at /foo/... so request.path here is still just \"/requests\", \"/namerd\" etc.... okay! I'll get rid of it!\n. hooray!. a .compact() here might be nice to get rid of the \"\" and undefineds. I would also maybe update the description of specific: {} in the comment. that's better, thanks!. I checked this out and tested it in the UI - since we're now checking for content type json on the server, as is, I get a POST http://localhost:9990/delegator.json 415 (Unsupported Media Type) error when I try to test a delegation... so we'll need to switch this to use $.ajax  so we can specify a content type:\nvar request = $.ajax({\n            url: 'delegator.json',\n            method: 'POST',\n            contentType: 'application/json',\n            data: JSON.stringify({ namespace: namespace, dtab: dtabViewer.dtabStr(), path: path })\n        }).done(renderAll.bind(this));. s/reties/retries. total nit: the tense here is inconsistent with that below. s/Added/Add?. if we define subtreeType in here, of it's the case \"union\" etc then subtreeType isn't defined.....\nUncaught TypeError: subtreeType is not a function\n    at http://localhost:9990/files/js/src/delegator.js:33:52\n    at Array.some (native)\n    at treeType (http://localhost:9990/files/js/src/delegator.js:33:25)\n    at renderNode (http://localhost:9990/files/js/src/delegator.js:53:19)\n    at HTMLButtonElement.renderAll (http://localhost:9990/files/js/src/delegator.js:17:25). I think this could use an indent one level?. yeahhh good point!. agreed!. yep, intentional - though maybe less confusing if I use limitWidth = $namespacesContainer.width() / 2 - 60;. I think this is unnecessary and I forgot to remove it.... oh yeah, they would! let me fix. yeah - we want to replace any \\ with \\\\, and each \\ here has to be escaped, even in the triple quotes. the backslash character in the replace string is still interpreted as an escape character...! \nhttps://www.scala-lang.org/api/current/scala/util/matching/Regex.html\n```\nscala> (\"abc\".r).replaceAllIn(\"abcdef\", \"\"\"\\\"\"\")\nres106: String = \\def\nscala> (\"abc\".r).replaceAllIn(\"abcdef\", \"\"\"\\\\\"\"\")\nres107: String = \\def\nscala> (\"abc\".r).replaceAllIn(\"abcdef\", \"\"\"\\\"\"\")\njava.lang.IllegalArgumentException: character to be escaped is missing\n  at java.util.regex.Matcher.appendReplacement(Matcher.java:809)\n  at java.util.regex.Matcher.replaceAll(Matcher.java:955)\n  at scala.util.matching.Regex.replaceAllIn(Regex.scala:450)\n  ... 33 elided\n``. typo?. tioli:_.chain()this to the map in the previous line so these don't get separated. oh yeah, good idea!. that is definitely possible - however, if we keep track of them we can put them back in their original color (both the client section and the combined graph) - which is possibly less confusing than picking a new color every time this client is unexpired. I guess we don't really know how likely it is that clients will come back, or go for good. ah! I guess what I was saying was that since clients can now be expired and unexpired at any time, we'd call register/deregister more often than before, where we never deregistered anything unless the user minimized the client section. good point, moved!. Oooh good catch, thanks!. ah! yeah expected.addServicesandaddClients` are handlers that could be called if new clients are detected (one is for the Service-focussed dashboard and one is for the current (client) dashboard. they both listen of the 'addedClients' event, but since they belong to different dashboards they won't both be triggered).\nI modified the handler in metricsCollector to include the most current metricsJson because I needed to check which services added clients belong to in order to fit them under the correct service in the Serivce dashboard. the clients dashboard doesn't care about this, so it ignores that second argument (metricsRsp) passed in on line 60 of this file. \nMaybe onAddedClients(addServices) is confusingly named - the addServices method is really \"check if any of the added clients are under a service we haven't seen before, and if they are, add that service\" - perhaps I should name it something other than addServices?. Yeah, I think isServiceMap was also a bad arg name :) I originally called the service dashboard \"serviceMap\" but forgot to rename it in all places.\nWould it be less confusing if this argument was called dashboardType instead?\nRenaming RouterController to ClientDashboard and RouterServices to ServiceDashboard could also help. (They both need the same setup things, e.g. initializing ProcInfo and RequestTotals)\nThen line 43 would be something like:\nif(dashboardType === \"service\") {\n  ServiceDashboard(metricsCollector, initialData, $(\".service-dashboard-container\"));\n} else {\n  ClientDashboard(metricsCollector, selectedRouter, initialData, $(\".client-dashboard-container\"), routerConfig);\n}\nThis file could be called something better, like dashboard_initializer which is maybe more indicative of what it is doing.. totally can name this better :). s/retires/retries :). ",
    "wmorgan": "@gtcampbell we should do both :smile: \n. Nice.\n. Squash :star: \n. I'd like to leave the dtab complexity for another PR.\n. Believe I've addressed everything except dtab complexity.\n. Squash merged.\n. Didn't look at the code but I did review the picture. :star: :star2: \n. Nice to have in 0.0.10 but not critical.\n. Sounds like a proper bug. Reopening and assigning to Oliver.\n. After some internal discussion, Alex will take a look at this.\n. Amazing.\n. Thanks for reporting this!\n. Thanks all. We'll be taking a look into this shortly.\n. Apologies for the bikeshed, but I think it's really valuable when items in the changelog describe not just what changed, but what the end user gets as a result of the changes.\nE.g. \"Added zipkin trace initializer for configuring Finagle's zipkin-tracer. Now you can automatically export Zipkin traces across all your services without having to write any code!\" (Or whatever. I have no idea what that feature actually enables.)\n. sweet!\n. It's not on the short-term roadmap AFAIK. Are there specific formats you want to support?. @bashofmann good idea! FYI, you can get aggregate statistics of that info through the metrics.json endpoint currently (and raw data in individual traces).. Hi @amitkumarj441. @adleong probably has an idea of how relevant & valuable this would be.. Bumping this up, since I think this could help address some NUX issues. Could we have, e.g., identifiers provide some example paths as part of their API, that could be exposed on a per-router basis in the dtab playground?. Closing this since the recent requests telemeter is a sufficient solution.. I see a few non-trivial examples in config.md, but they're largely described in term os \"how to use the X namer when I have it configured\". Which is fine, that needs to be documented too.\nI think there's still a gap between the generic/ice-cream doc, and \"I have a working dtab for my setup\".\n. @clhodapp: thanks for the feedback! Can you give us a few more details about the use case you want to support? We're actively working on some identifier stuff (like this ticket) right now.\n. Screenshots with and without.\n\n\n. I kinda like the # vs $ distinction, but IANADE.\n. @lsjostro I think you're the Prometheus expert here--is it difficult or irritating to use the existing /admin/metrics/prometheus endpoint?\n. Gotcha. Probably makes sense to have it be configurable including the port, then. (At the expense of adding yet another configuration option.)\n. Are we waiting on a stats dump (e.g. using the above prometheus config) from @JonathanBennet  to resolve this issue?\n. @JonathanBennet sounds like the ball is in your court on this--we haven't been able to repro on our side.\n. I see. As a starting point, maybe we show a per-server success rate in the UI--that will capture retries, and we have some space for it visually on the page. That's not a perfect solution, but should be useful in the common case of one server per router.\n(Moving these changes to another ticket.)\n. Updated the description with the latest Product Vision. This ticket is scoped to changes to the client section. I'll make another ticket to capture changes to the server section.\n. This might be a dupe of #234. \n. FWIW I wouldn't call this \"compatible\". All services are linkerd compatible if they speak HTTP, thrift, mux, etc. It's only if you want extended capabilities like distributed tracing and per-request routing that you have to do header forwarding.\nBut I agree it should be documented.\n. Yeah, I think so. Closing. (BTW we're changing some of these headers in the next release.)\n. @Ashald @andersschuller @taion809 Is this issue still important to you guys?\n@masonoise you noted on Slack that the telegraf Prometheus input plugin wasn't a great solution. Can you clarify a little bit what the problem with that approach is? (Not a telegraf expert.)\n. @masonoise Thanks, that's helpful. Could you write a script to transform the metrics before telegraf ingestion to the desired state? Or maybe modify the telegraf Prometheus plugin to have template specifications similar to the statds plugin? Not sure if either of those are good long-term solutions, but they might be reasonable short-term ones to get you unblocked.\n. @masonoise @olgert @leozc would love to get your input on the prototype above ^.. Golden Buoy Award for this branch\n. (Later work might included a fancier error page with some diagnostic tips for other types of errors.)\n. Should the linkerd dtab UI just call out to namerd for resolution when it's configured, and we ditch the namerd ui entirely?\n. How about putting something like <linkerd address>:9991? That would give a hint about how to debug this.\nRe: not being useful when namerd is in play, per my comment in #447, I think the linkerd dtab playground should be able to use namerd for resolution.\n. And put the URL in the error message(s) that triggered #445?\n. Upon further investigation, issue was that the top-most graph grows horizontally when switching tabs. (Under certain window sizes, in Chrome). So switching back and forth between tabs can cause \"lag\" because the newest portion of the graph is hidden from view.\n(I.e. this is unrelated to how long it's been running.)\n. @endzyme and others: thanks for the feedback. Is setting a Host header (but still connecting to locahost:4140) prohibitive?\nI ask because in order to get fancy things like distributed tracing (\u00e0 la https://blog.buoyant.io/2016/05/17/distributed-tracing-for-polyglot-microservices/) and per-request routing to work, you'll need to pass some other headers along at request time anyways. So I'm trying to get a sense of whether Host: is different for some reason, or if this mainly an ease-of-integration feature.\n. @endzyme (and others): Thanks for the feedback. I'm not sure Host header overwriting is really violation of the RFC (at least not by my reading), but the barrier to entry point is a good one. We're taking a look at this.\n. Thanks, Mason! I think this was an oversite in 28f74e6bd9 -- definitely not a loss of functionality!\n. Yay!\n. W00t! I have a minor tweak I'd like to make--I think the y axis should extend to 101% or something so that the 100% line doesn't look so much like a UI element--but that doesn't have to be part of this branch.\n. IT'S BEAUTIFUL\n. Nice corner case. Thanks, Oliver!\nOn Monday, June 13, 2016, Oliver Beattie notifications@github.com wrote:\n\nWhen a watch against the k8s API is terminated (because of a time-out,\nconnection failure, etc), it must be re-established. Kubernetes only allows\nwatching from a \"recent\" version, so when re-establishing the watch, it's\npossible it will be rejected with a \"resource too old\" error. In this case,\nit's necessary to refresh the resource list to verify no updates were\nmissed, and then re-establish the watch from the newly-received version\nnumber.\n\ud83d\udd0b Apologies again this one doesn't include tests. I will write some, but\nI'll do that as a separate PR in a few days if that's okay.\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/BuoyantIO/linkerd/pull/473\nCommit Summary\n- Handle \"too old\" error when re-establishing Kubernetes watches\nFile Changes\n- M k8s/src/main/scala/io/buoyant/k8s/Watchable.scala\n  https://github.com/BuoyantIO/linkerd/pull/473/files#diff-0 (16)\nPatch Links:\n- https://github.com/BuoyantIO/linkerd/pull/473.patch\n- https://github.com/BuoyantIO/linkerd/pull/473.diff\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/BuoyantIO/linkerd/pull/473, or mute the thread\nhttps://github.com/notifications/unsubscribe/AAAPutvvl96X-spF_nCkXvKIhGoznxtGks5qLU_fgaJpZM4I0SXS\n.\n\n\n-William\n. Thanks, Chris! (And hi!!!!)\n. Hi Nicholas,\nThanks for the issue! It would be possible to standardize these formats, and we're certianly not opposed to it, but it would be a fair amount of work--various underlying frameworks do various things for logging and unifying them all would be non-trivial. (And historically, we've moved away from log parsing in favor of metrics to understand aggregate behavior, so we haven't had much motivation to do this in the first place.)\nDo you have a specific use case you want to address? E.g. if you're looking to track exceptions, these are exposed in failures/* counters in metrics.json.\n. Yep, I think that makes sense. I have never used logstash--but I did find some search results about parsing JVM exceptions, e.g. https://sematext.com/blog/2015/05/26/handling-stack-traces-with-logstash/. Would that help with your use case?\nWe can also investigate smashing these things into one line within linkerd. I'm not sure what that would entail.\n. Alright, feel free to ping or reopen if necessary. Also, if you get a viable solution going with logstash, please post on Slack or dump it here... I'd love to add it to the docs somewhere. I'm sure you're not the only person who'll be doing this.\n. In the absence of being able to repro on our end, we would like a tcpdump from @jacob-koren to diagnose.\n. @jacob-koren Thanks, we're taking a look at this now. It looks like the pcap file doesn't include linkerd -> server traffic, which we'd like to take a look at. Are you able to capture that traffic as well? (If it's helpful, we can give you a pcap command if you tell us the ports of the upstream service -- since they're on Mesos it will be something weird. Maybe 22393 in the example above?)\n. The other thing that might be useful is some stats dumps so we can get a sense of # nodes, how long connections live, etc. You can run this small script when generating the errors:\n```\n!/bin/bash\nwhile true; do\n  curl -s http://localhost:9990/admin/metrics.json > l5d_metrics_date -u +'%s'.json\n  sleep 60;\ndone\n```\nSorry for all the requests. It's not easy to debug this sort of thing remotely!\n. Ok, we discussed this a bit internally. I think there are basically two viable options:\n1. Write a custom identifier that maps request URLs to names in the way that you want. (And potentially, we could come up with something more generic that you could then configure.)\n2. Keep this logic in nginx, but have nginx set a Host header and pass through to linkerd, and have linkerd do routing based on the Host header.\nOf these, I actually prefer the second, if you have any non-trivial nginx config, or want to do cookie parsing or arbitrary header parsing or whatever. But we can help you with the first one if you want.\n(As a side note, it's technically possible to accomplish this sort of routing today with httpUriInDst, but it's almost definitely not what you want. It turns each distinct URI into a separate logic name and, since we cache and track stats per logical name, you're gonna have a Bad Time unless your URI space is severely constrained.)\n. Yeah, this is a bit outside the current use case, where we assume that load-balancing happens over a pool of identical replicas.\nAs a short-term hack, would it make sense to introduce something like nginx to rewrite those  headers? Not a great solution, but might be enough for testing purposes to see if EWMA buys you a performance gain.\n. @garysweaver this ticket is a bit old... are you still interested in pursuing this? I would like to talk to you a bit offline if so.\n. Forgot to say, this is all against the UI in 0.7.0.\n. Some of these are fixed. Assigned to @rmars for triage.. Whoops, dupe of #486.\n. Thanks for the feedback, @mielientiev! Announcing is on the tentative roadmap and we're putting some thought into how to do this best. (E.g. see https://github.com/BuoyantIO/linkerd/issues/38).\nCould you give us a couple details about your use case and environment? How would you like this to work? How would it interact with your deploy tooling? How would you want to configure it? Etc.\n. I believe that the race condition issue is avoided currently thanks to the Etag header enforced by the namerd API (which, I'll admit, is totally undocumented).\nUse case 2 is an area of active discussion for us. Some of this use case can be captured by the override header (https://linkerd.io/doc/0.7.1/linkerd/protocol-http/#user-headers), but there's definitely more to be done.\n. Sweeet\n. @seekely Note that using this flag to handle large responses is \"not great\" for performance reasons (they take up a big blob of memory). Using chunked encoding is better in the long run.\n. Hi Calvin,\nlinkerd is modeled as an out-of-process client-side network stack, so HA concerns are intended to be deferred to another layer. If a linkerd instance is down, it should be handled as the client being down. Then you can treat linkerd upgrades or failures just as you do with client upgrades or failures--rolling deploys, rescheduling via something like Mesos or Kubernetes, etc.\nThe other option is that you can have clients talk to multiple linkerd instances, and load balance over them. Then you can do a rolling upgrade. This is not ideal in some ways (linkerd is designed to save you from implementing your own load-balancing logic), but it is useful in some cases.\nPlease let me know if this answers your question.\n. Hi @moleksyuk. Sounds like a bug (but not Kubernetes specific). We'll take a look. Thanks!\n. Thank you for a very detailed repro!\n. Thanks for this, @moleksyuk!\n. @OleksandrBerezianskyi thanks for the details! Really helpful. \n. Found another way to get the host IP via the Kubernetes slack: use the default route for the pod (e.g. by reading /proc).\nhttps://github.com/DataDog/dd-agent/blob/e1086b0c52d3438ea841762623ecb5a0db28645f/utils/dockerutil.py#L157\n. @adambom if you really want to go down this path we can point you in the right direction (like Oliver says, increase MaxRequestSize and change some of the JVM settings to get more memory). But it's almost definitely better to get chunking in there, unless you have some very specific reasons why that's not feasible.\n. Nice! Thank you!\n. Hi Abhishek,\nAwesome, I would love to have this feature! If you have it on a branch, you can make a Github pull request against BuoyantIO/linkerd... that is the starting point for getting it into the main distro.\n. @irachex I believe this is fixed in 0.8.4 (per #785). Please feel free to reopen if not.. This should be fixed in 0.8.4. Please reopen if not.. Current status: not fixed yet, but on our radar.. @ademaria it's on the long-term roadmap. We're making progress towards it, but we're focusing on h2 perf first. In the short term, a separate router for each should do it :). Hi @bflance!\nThe instructions in https://blog.buoyant.io/2016/10/10/linkerd-on-dcos-for-service-discovery-and-visibility/ should walk you through the steps for installing linkerd on DC/OS.\nIf you have problems following those instructions, please feel free to hop into the Slack channel or to file a Github issue. Be sure to include the error messages you're seeing so that we can help you diagnose the problems!\n. Thanks @sevein!\n. Great, have been missing these!\n. Awesome, thanks @abhinigam! We will take a look.\n. Looks pretty interesting though!\n. Http/1 and http/2\nOn Sat, Mar 25, 2017 at 12:11 AM Leo Liang notifications@github.com wrote:\n\nWhat are H1 and H2 referring to ?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/linkerd/linkerd/issues/841#issuecomment-289194719,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAAPulAGUXiBpOtfg2j_ctnQ8FGQ2JVlks5rpL4igaJpZM4LG634\n.\n-- \n-William\n. Thanks for the contribution, Dmitry!. @pinak if you have a moment to confirm the fix with 0.8.5, that would be great!. Thanks, @andrewrothstein !. Thanks, @fantayeneh!. Thanks, Adrian!. Thanks, Maciej!. Thanks, Andrew!. +1. Thanks, @hangfu! I thought we had this in place already, but apparently we do not. . @apakulov that's awesome, maybe @olix0r or @adleong can write up a few pointers here.. @apakulov awesome!. As discussed on Slack. Thanks for filing this, @somejfn! . +1. Here's what @siggy said in Slack. (See https://linkerd.io/features/service-discovery/ for some more motivation about this advisory treatment.)\n\nRegarding our Marathon namer not honoring the RUNNING state, that is by design. Marathon apps are available more than just when the state is RUNNING, due to lag between periodic Marathon health checks. Also, if an instance becomes unavailable, linkerd will notice immediately upon a failed request, and retry a healthy instance. We expect this to give you better zero downtime behavior during upgrade than using the RUNNING state, because you instances will be available earlier during startup and later during shutdown.\nAll that said, if you still only want to hit instances marked as RUNNING, you can enable the useHealthCheck flag in your linkerd config: https://linkerd.io/config/0.8.5/linkerd/index.html#marathon-service-discovery-experimental. To clarify: the desired behavior is that when useHealthCheck is enabled for the marathon namer, linkerd should only use endpoints where \"healthCheckResults\": [ { \"alive\": true... and state is not TASK_KILLING.\nIs that right?. It's a nice font but blocking for 500ms seems like a lot. We could serve it locally.... @andrejvanderzee not explicitly. Circuit breaking and latency-aware load balancing will kick in as instances slow down / start failing under load, but setting an explicit limit is still on the roadmap. Watch this ticket for updates, though!. @leozc status quo for now. PRs welcome though :). There's a rumor we fixed some of this in 0.9.0. Is this still reproducible?. Also document namerd endpoints. No documentation even of namerd's metrics.json currently.. Chatted with @caniszczyk briefly about this. My understanding is that inclusion is fine as long as we respect any attributions in the license (e.g. by adding to the README).. @dpetersen congrats, you now know Scala!. Confirmed in 0.9.0. Nice work.. Thanks for the detailed writeup, @ewilde!. @anuradhacse great, this is going to be a cool GSOC project!. OMG. @Ashald great! Please close the ticket if you are satisfied :). @huggsboson we discussed this a bit internally today. Will update the ticket in a bit.. Great, glad you figured it out!. Great idea, but it looks like we'll need someone with over 1500 points to do that. Anyone a heavy StackOverflow user?. @DukeyToo thank you very much!. @klingerf and I chatted--closing was an oversight. Sorry. This is still on the roadmap. @valdemon do you have a use case for this feature?. @valdemon Thanks, that's very helpful. . Thanks @jtaylor32!. @tsandall thanks for this! We haven't thought too much about what a contrib/ might look like, but @adleong @olix0r might have some ideas.\nIn terms of talking about this more widely, my suggestion is to have a blog post with some concrete examples of using OPA + linkerd for something interesting. Check out what we do in our service mesh series, e.g. https://blog.buoyant.io/2016/10/04/a-service-mesh-for-kubernetes-part-i-top-line-service-metrics/.\n(And we'd be happy to have a guest post on blog.buoyant.io if you like!)\n. Thanks @Oded-B ! Per Slack, we're looking into this.. These might be separate issues? @Oded-B's heap dump points squarely at the recent requests telemeter, whereas yours points at h2.. I know I'm late to this, but I think changing the \"Save*\" to \"Apply\" might help. Save implies storage, at least to my ancient brain.\n(I also think the path should be underneath the dtab area, but that's another discussion). @agunnerson-ibm thanks for the report with detailed repro instructions! Very helpful. Namespace thing is probably a bug. We'll look into it. Probation thing, like Alex says, is working as intended.. @agunnerson-ibm awesome, glad to hear it!. Thanks for the very detailed report, @taer!. Thanks for the repro, @taer! This is super helpful.. Thanks for the detailed report, @jgensler8!. Agreed. I think maintaining our own JRE image would be out of scope for Linkerd.\nI think we should be responsible for using the best possible public JRE image as the basis for our Linkerd images, and we should be aware of the issues with those images, but that's where the line should be drawn. (At least in open source; commercial relationships could potentially be different.)\n. Nice!. We owe you a hoodie and a beer :). Hi @ahmetb! Thanks for the note. To be clear, the root cause of that incident was a Kubernetes bug. This Linkerd bug compounded the problem, which is bad, but \"caused an outage\" is an overstatement.\n(I'll also add that this bug has been fixed since Linkerd 1.0, released over 6 months ago.). Thanks for pointing this out, @rothgar!. Thanks for the report. We will take a look!. Hi @danielBreitlauch. There are many reasons this might happen. We'll need many more details to be helpful.\nHow are you running Linkerd? In a Docker container, or as a JVM process? What are the nature of these requests? HTTP, Thrift, something else? Are they concurrent? If they're HTTP, are you doing keepalives or restarting the connection every time? Are they big requests? Small? Chunked? Are you using TLS? What's the CPU usage and memory footprint of Linkerd at this point?\nWe've been able to get a single instance to ~40k HTTP RPS under ideal conditions.\nYou might find this doc useful: https://github.com/linkerd/linkerd/wiki/Debugging-network-performance\n. Sounds good!. What's the recommendation for how to use Kubernetes readiness probes with Linkerd? We may want to add that as well to the going-to-prod doc.. @SamFleming that doesn't sound unreasonable. how's it working out?. Great. I'm going to label this as a documentation issue, since it doesn't sound like we will be doing feature work to support this.. Thanks, @yoshi-taka! Do you mind signing the CLA? https://buoyant.io/cla/. Thanks, @dhay! We'll take a look!. Thanks @elecnix! We'll take a look.. @ale-batt are you still seeing this with Linkerd 1.1?. @ale-batt thanks!. @arbarlow From the go-grpc issue, it looks like you are left with doing this \"manually\". Hopefully it can be done as a config change rather than as a code change, at least!. Blast from the past. I will defer this decision to @sterlingwhite.. (And I don't think I realized they were clickable when I originally made this issue.). Hi @kigsmtua. Mutual auth will probably be a part of the next release. You can follow along here: https://github.com/linkerd/linkerd/pull/1319. Thanks for the report, @rbaumgartner. We'll take a look!. @rbaumgartner how often do you make this change? And OOC, are any of the other Zookeeper-dependent services that were able to make the change JVM applications?. @jamessharp please do, we would love to dig into this.. Thanks @jamessharp ! We're on it.. Hm, maybe this was fixed in 1.1.0?\nOOC, with the advent of clearContext (https://linkerd.io/config/1.1.0/linkerd/index.html#server-parameters), do you still need nginx in front? . @pawelprazak Just following up on this: if there's a path forward where you think Linkerd might satisfy your use case, please feel free to reach out. It's a continually evolving project and we'd be happy to have a conversation with you about your requirements!. Thanks, @nvartolomei! We'll take a look!. Very cool!. @dario-simonetti Thank you! We'll take a look!. You can turn this off easily. See  https://linkerd.io/config/1.0.2/linkerd/index.html#usage. I hear you. What do you suggest instead?. Thanks! Appreciate the thoughtful feedback. I would be open to PRs for options 3 and 4.. FWIW, I totally understand @kalyan02's point of view. I am also uncomfortable with software that phones home. But we use these metrics to justify investment in Linkerd, so... we have to walk a fine line here.\nWe need to keep this enabled by default, but I would like it to be very obvious how to turn it off, and very clear about what it sends. (We've also tried very hard to make sure the info it sends is non-identifying.)\n. Just to summarize for future passersby, I'd be open to accepting PRs for better documentation around this feature and more configuration surface area around it.. Thanks @leozc. We'll take a look!. @leozc great, thanks!. Thanks, @zackangelo!. Ok, we finally have a consistent, local repro thanks to @klingerf, and we have identified the root cause of this issue in netty: https://github.com/netty/netty/issues/6906\nThere is a fix that just made it into netty \"master\": https://github.com/netty/netty/pull/6916. Our belief is that this will fix this behavior in Linkerd as well. The next step is to apply this fix directly to Linkerd and verify.\n. Thanks, @rake36! We'll take a look!. @rake36 that's great! Sorry for the pain here, none of us are developing on Windows at the moment so this sort of friction occasionally happens. Glad we found a workaround.. Is this closed by #1606 ?. Thanks @edio! Great writeup and thank you for all the alternatives. We'll take a look.. Thanks, @rmichela! Looks like this message originates in Finagle's ResponseConformanceFilter. I'll leave it to the experts to determine how easy this is to fix.. @nitishm we're no longer maintaining that integration (which never made it out of experimental status anyways).. Thanks, @gitcarter! We'll take a look.. Thanks @dario-simonetti!. We're currently targeting week of July 10th for the next Linkerd release. (Please note this is an estimate, not a guarantee!). Thanks @hammerOwen! We'll take a look. . @vadimi Interesting. Thanks for the pointer. We'll see if we can repro with a self-signed cert.. Would this also apply to upgrading Linkerd itself?. Ok. Longer-term I think we'll support this via an API (see https://blog.buoyant.io/2017/05/24/a-service-mesh-for-kubernetes-part-x-the-service-mesh-api/ for some notes on this). In the short term, we could consider consider /admin/offline-style behavior for graceful shutdown.. @MrTravisB PRs weclome :). Thanks for submitting this @kevholditch!. Agreed, namerd is generally undocumented, both deployment but also when you do / don't need it. \n@rickardrosen just to answer your immediate questions:\n Yes, any number of instances can be started simultaneously. All the hard coordination logic is deferred to the backing store; namerd is an API layer on top of that.\n You could run one per host but that's probably overkill. Our typical deployment is to run N >= 3 (usually N == 3) as a separate service somewhere.\n* On consul cluster nodes vs as a separate thing: up to you!\nLet's keep this ticket open in the meanwhile to track documenting this, as @esbie suggests.. @rickardrosen yes that's right. Consul does all the hard work. :). Are you pushing enough throughput through Linkerd that it actually needs multiple destinations?\nAnything in the logs?. We have some examples on https://github.com/linkerd/linkerd-examples/tree/master/consul\nFWIW I run the fs namer for various demos on a volume-mounted dir on Docker for Mac (the worst possible combination of filesystems). It can take 5-10 seconds to see a change. If it's taking more than that, there's definitely something weird going on.\n. @mnnit-geek hopefully this issue is not a blocker if you're using Consul in production, but it would be good to figure out what's going on here.\nWe haven't seen this on our side, so the best way to help us is to find a reproduction case we can dig into. Is there a specific OS that this happens on? Are there specific changes to the file that don't get picked up, but others do? Does using touch do anything? If you run Linkerd with debug flags (see https://discourse.linkerd.io/t/debugging-a-linkerd-setup/52) are there any interesting messages in the logs? All that info would help us.\n. @eduponte Thanks for capturing these details. We will dig into this. . @mtweten thank you! if you send us your address & size we'll happily send you a t-shirt and some stickers. william at buoyant.io or DM me on the linkerd slack . @b-hoyt we're going to discuss / scope this today. @b-hoyt This is doable but not 100% trivial. Current plan it to take a look at this in a bit; for the next release, at least, we'd like to focus on H2 features and perf.. @vadimi glad you figured it out!. Thanks for pointing that out! Whoops! . Thanks for the report, @jiju-git. We'll take a look!. @fantayeneh thanks. @zackangelo can you confirm you are not seeing the host field in this header in 1.1.3?. @ievgen-kolomiiets thanks for filing this! We'll take a look.. @mejran thanks for the report. We'll take a look.. @amanjeev thanks for pointing that out! We'll take care of it.. Thanks @owensk!. Thanks @prdoyle. We'll dig into this.. @d10i that's awesome. nice work. we'll dig into this.. Thanks Zack. We'll take a look!. A repro script would be immensely helpful. Thank you!\nOn Wed, Nov 8, 2017 at 4:35 AM James Sharp notifications@github.com wrote:\n\nWe've just run into this as well - can probably get a repro script up and\nrunning if it would be helpful?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/linkerd/linkerd/issues/1605#issuecomment-342804881,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAAPuuAgK7JI2MlJvzEWsGoi6FCBSLffks5s0aAigaJpZM4PAXpF\n.\n-- \n-William\n. @jamessharp thank you, that's incredibly helpful.. @blacktoe thanks for the PR! if you send us your mailing address and t-shirt size we'll send a shirt your way. Email my first name @ buoyant.io or DM me on the Linkerd slack.. Nice.. Thank you team. @robb-foster-ck thanks for the report! We'll take a look. . Thanks @ccmtaylor! We'll take a look.. Thanks @jsurdilla! We'll dig into this.. Thanks @kumudt, @mejran.  We'll take a look.. @Ashald thanks for the suggestion!. @adleong what do you think?. @amitsaha thanks for filing this!. @amitsaha nice work!. @joshleeb thanks for submitting this! We will take a look!. @joshleeb thanks again! email me your address and preferred size and I will send a t-shirt your way! My first name at buoyant.io. Thanks @0x656b694d! We'll look into this.. @zsojma we should be taking a look at this this week. Thanks @mirosval! We'll take a look! . Update: we have identified a possible root cause and are working to verify. This is a high priority issue.. Hi @blakebarnett! This is on our long-term roadmap. Would you be interested in submitting a PR for this? We would be happy to help you through the process, and that would probably be the quickest way to get this functionality in.\n\n. @zackangelo does your open_streams metric grow over time?. I'm ok with this.. Specifically, I'm ok with switching to DCOs entirely. . Thanks for filing this, @matt-turner-skyscanner! . @christophetd I'm going to close this issue, since I believe we've fixed the underlying problem. If you still see this on 1.3.7 or beyond, please feel free to reopen.. @chrismikehogan thanks! @DukeyToo, it would be great to have you try 1.3.4 or 1.3.5.. Thank you! Standing by eagerly.. Bummer. We're going to allocate some time to dig into this later this month.. @mebe thank you for a really detailed repro case! . @mebe That's interesting. Let us know what you find out with your tests on the internal server.. @sgrankin yes, thank you for digging into this!. Thanks @vadimi. Good to know that it's something in 1.3.3.\n@sawyerzhu can you provide some more details about your environment please?\n@vadimi @sawyerzhu It will be much easier to figure this out if we can reproduce it on our side, so if you have some [more] time to spend on working on a repro case, that would really speed up our ability to track this down.\n. @vadimi that's really helpful. Thank you!. @vadimi @sawyerzhu Preliminary investigation suggests this is non-trivial and will take us some time to debug. In the meantime, any kind of repro case would be helpful, so if you do end up coming up with something, please let us know.. @vadimi That's a really good clue. Thank you. . Thank you for all the detail. Very helpful. We continue to investigate.. @adleong @siggy would #1768 be helpful for diagnosing this?. @ejwood79 are you running into an increase in disk usage? Or into Linkerd talking to old K8s endpoints? If so, can you clarify which version of Linkerd you are running, and which version of K8s?. Bummer. This should be fixed in more recent versions, and if not, as @dadjeibaah, we have better diagnostics to help us understand what's going on.\n@yoitsro can you provide a few more details about what you're seeing? As for the disk size increase, can you figure out what file is getting bigger?\n. We have a really cool idea for this that @dadjeibaah is working on now. Stay tuned!. @obeattie we're going to blow your mind with this. Awesome. Thanks for the PR, @fangel! We'll take a look.. I love bug reports like this. Thanks, @obeattie! We're looking into this.. Thanks for submitting this! We'll take a look.. Amazing. Thanks for this, @obeattie. We'll take a look.. Thanks, @rclayton-the-terrible. We'll take a look.. @rclayton-the-terrible do you have a way of verifying #1760, e.g. by building from that branch?. Hi @jippi! Thanks for the feature request! You might be interested in the discussion that's happening on #1079.\n. FYI we did some digging into this today. We'll have more to say next week.. More debugging power = good. I'd rather have the better approach (that doesn't require parsing). A smooth Envoy -> Linkerd transition is not very important to the project IMO.. @shakti-das would you be interested in submitting a PR following @briansmith's proposed format?. Glad to hear you got it working. Closing this issue.. @yennifehrrr thanks for the detailed report! Bummer we still have a few leaks floating around. We'll dig into this.. Whoops! Thanks @leozc! . @drichelson it's really nice to see you here btw!. @hynek we're taking a look at this. W00t!. W00t! Thanks again @negz!. Thanks for the report, @MirzaMerdovic! . So is this effectively \"working as intended\", or is there an underlying bug that has been uncovered?. Adding tentatively to 1.3.6.. PRs welcome :). Thanks for the PR! We'll take a look!. Thanks for filing this, @krjensen!. Maybe @adleong or @dadjeibaah can share some pointers.. Thanks for the PR, @LukaszMarchewka! We will take a look.. Thanks for filling this. We'll take a look!. You're on a roll! Thanks for filing this.. Thanks for capturing this, @hynek!. @shakti-das @drichelson it's a pleasure working with you, as always. @a-kinder Great! And thanks for the writeup.. Nice to see you here @mrezaei00 :). @edio nice! Does it make sense to have @cchatfield validate this fix?. There is some prior art: https://github.com/kubernetes/charts/tree/master/stable/linkerd\n. (Also this: https://github.com/littlemanco/helm-linkerd). Have a report from a customer that they've successfully used this in production and see noticeably reduced memory footprint and startup times. Let's bump this up the priority queue.. Great debugging, @jacob-go! At a minimum, we should add this to our DC/OS docs and examples.. @joeyb Great to hear!. @nikolay-pshenichny nice work!. @xinyuliu-glu thanks for filing this. It would be helpful to understand why the 403 is happening, since that might provide a clue about the other behavior.\nCan you paste in your complete dtab?\n. Should we capture the lack of server name passing in an issue?. Thanks @peterfich!. @zackangelo Thanks for taking a stab. Some other folks have been looking into this too. Would you be interested in joining our nascent working group?. @Hugh-ifly glad you figured it out! We'll update the docs to point out when you have to set this.. git commit \u2014amend -s should do it\nOn Fri, May 18, 2018 at 12:06 PM Robert Panzer notifications@github.com\nwrote:\n\nSure!\nThat means I have to change the commit message to add the \u201eSigned off\nby...\u201c line?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/linkerd/linkerd/pull/1954#issuecomment-390303575, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAPurqeYwNIwOS5cLS9atz2NuYZDLD-ks5tzxuwgaJpZM4UERPD\n.\n-- \n-William\n. Is this a dup of #1926?. Thanks, @yoitsro. We'll dig intpo this.. @yoitsro are you setting httpAccessLog in your config, or anything else like that?. Setting that option would be a reason that Linkerd would write files to disk. If it's off, then no reason to add it.\n\nAt this point, it's hard to further diagnose this until we can find out where the disk usage is coming from. I found this really good issue where someone documented how they tracked down disk space consumption in a Kubernetes container: https://github.com/kubernetes/kubernetes/issues/32542\nMy suggestion would be to try and replicate the steps in that issue, and see if you can identify which file is eating up disk space.. Reading back to the top of the issue: the graphs there seem to say that disk usage caps out at 7mb. Is that right? Am I looking at the wrong graph?. Sounds like disk usage was a red herring. . Still looking good?. Well... that's one way to fix the issue :). Thanks for doing this, @leozc! We'll take a look.. Thanks for filing this, @utrack!. We've got an example you may be able to crib from here:\nhttps://github.com/linkerd/linkerd/blob/master/linkerd/protocol/http/src/main/scala/io/buoyant/linkerd/protocol/http/istio/IstioRequestAuthorizer.scala\n(and \nhttps://github.com/linkerd/linkerd/blob/master/istio/src/main/scala/io/buoyant/k8s/istio/IstioRequestAuthorizerFilter.scala and some other related files)\nAnd docs on how to build plugins are here: https://linkerd.io/advanced/plugin/. Wild. Thanks @chrisgoffinet.. Awesome!. Welcome to the cool kids' club, @utrack!. cc @Ashald @zackangelo. This might be a starting point for using the linkerd2 proxy outside of K8s.. Hm, I don't think the rinet namer is documented anywhere. We should fix that. The short story is that it does a DNS lookup.\nIn the dtab config you give, Linkerd will first look for a service name in Kubernetes (in the default namespace only), then will fall back to a DNS lookup, using port 80 if not specified.. @chrisgoffinet nice hack!. We have a concrete repro (thanks, @chrisgoffinet!) and we're working on this actively.. The current state is that Istio support hasn't really advanced beyond the work in 1.1.1 (the 1.3.1 thing was a one-off to support a specific API). Linkerd's Istio integration effectively deprecated and we should probably mark it as such, outside of the the 1.3.1 precondition check stuff. Though if there are more specific API features like that, I'm certainly not opposed to adding them.\n. In short, we never found anyone who was actually interested in using it. (Plus it was a huge amount of work and there were some real impedance mismatches.)\nDo you have a specific use case for Linkerd + Istio? Or is this more of an academic question?. That sounds 100% academic :)\nIf we mark this integration as deprecated we should probably also put a note on the blog post.. Addressing in #2103.. Thanks @chrisgoffinet! Don't forget a DCO signoff.. For non-SSD hosts, given that /var/log is typically not a candidate for tmpfs, is this the best place for the logs to go? cc @chrisgoffinet . Possibly related: https://github.com/linkerd/linkerd/issues/2088#issuecomment-410809972. cc @zackangelo. Thanks @zackangelo!. Wow. Thank you for the excellent writeup!. @nadilas are you on Kubernetes? we have protocol upgrade in Linkerd 2.x, but that's K8s-specific right now. Alternatively, #766 is the issue to track. . @robertpanzer ping! We are planning on taking a stab at this next week.\n. Thanks for reporting this, Sumit! We'll dig in.. Thanks @evhfla! Adding this is a good idea.. Dunno. What's the right number?\n. I would leave out the overhaul to the internals, since it's not really user-facing, and talk about the motivation for the breaking config change instead.\n. Various (unless that was a @mariusae homage)\n. Streaming bugfix should be in here too\n. Also header changes--especially if they are potentially breaking.\n. I like the idea of having a one-liner \"just run the docker image\" in here though.\n. D'oh!\n. Email mismatch. I'd say something like \"This release focuses on quality, and on improving the debugging process. It includes improvements and fixes for Linkerd's Kubernetes support, administrative UI, and Namerd control plane. It officially graduates HTTP/2 support out of experimental, and also features a number of community contributions!\". Maybe: Add an /admin/client_state.json debugging endpoint to expose the current address set of each client, allowing you to easily inspect where Linkerd thinks it can send traffic to.. I think it would be really helpful to say why this namer exists, and why we end up using it everywhere instead of the slightly-less-obscurely-named inet namer. Just language tweaking, but I think you can write this as something like:\n\nLinkerd 1.5.1 adds a new io.l5d.consul.interpreter that allows Linkerd to read dtabs directly from a Consul KV store agent instead of using Namerd. In addition, this release fixes critical bugs in the HTTP/2 router where Linkerd would get stuck handling connections in certain cases.\n\nAlso, is \"critical\" the right word?. \"first 1.x release\". Formatting. Remove \"external\". We are all part of the Linkerd community. Replace \"these\" with \"the\". formatting, and below as well. ",
    "IssuehuntBot": "@boostio funded this issue with $10. Visit this issue on Issuehunt. @boostio funded this issue with $10. See it on IssueHunt. @boostio funded this issue with $10. See it on IssueHunt. @boostio funded this issue with $10. Visit this issue on Issuehunt. @boostio funded this issue with $10. See it on IssueHunt. @boostio funded this issue with $10. Visit this issue on Issuehunt. @boostio funded this issue with $10. Visit this issue on Issuehunt. @boostio funded this issue with $10. Visit this issue on Issuehunt. @boostio funded this issue with $10. See it on IssueHunt. @boostio funded this issue with $10. See it on IssueHunt. @boostio funded this issue with $10. See it on IssueHunt. @boostio funded this issue with $10. Visit this issue on Issuehunt. ",
    "benley": "This looks great :-)\n. it might be - I can test that and see if it works.  I seem to recall running into strange problems with using those in the past, but I don't remember what it was.\n. I did sign the CLA in \"Company\" mode - but I can sign as an individual too. Done.\n. I'm neutral on this one - if putting it at the start violates the standards then I suppose let's not do that :-)\n. shall I update the patch to put it at the end?\n. ",
    "jimmidyson": "I'm still seeing this issue with 0.3.0. Any tips on the best way to debug this?\n. Thanks for the tips. Very possible this is just a misunderstanding about the dtab - have found it slightly confusing so have tried to make it as simple as possible:\n``` yaml\nadmin:                                                                        \n  port: 9990                                                                    \nnamers:                                                                       \n- kind: io.l5d.experimental.k8s                                               \n  host: localhost                                                             \n  port: 8443                                                                  \n  tls: true                                                                   \n  tlsWithoutValidation: true                                                                                                                                                                \n  authTokenFile: /home/jdyson/tmp/token                                         \nrouters:                                                                      \n- protocol: http                                                              \n  baseDtab: |                                                                 \n    /http/1.1/GET => /io.l5d.k8s/zipkin/http;                                 \n  label: int                                                                  \n  servers:                                                                    \n  - port: 4140                                                                \n    ip: 0.0.0.0\n```\nRunning curl -H 'Host: zipkin-web-console' localhost:4140. The service zipkin-web-console does exist in the namespace zipkin with the named port http. It's almost like it's not querying at all.\nHere's the (hopefully) relevant part of the logs:\nD 0411 21:25:53.631 THREAD19: k8s lookup: /zipkin/http/zipkin-web-console /zipkin/http/zipkin-web-console\nD 0411 21:25:53.649 THREAD19: k8s initializing zipkin\n0411 21:25:53.652 36e4dfa7003f8c8b.36e4dfa7003f8c8b<:36e4dfa7003f8c8b] BinaryAnnotation(namer.dtab.base,/http/1.1/GET=>/io.l5d.k8s/zipkin/http)\n0411 21:25:53.652 36e4dfa7003f8c8b.36e4dfa7003f8c8b<:36e4dfa7003f8c8b] BinaryAnnotation(namer.dtab.local,)\n0411 21:25:53.652 36e4dfa7003f8c8b.36e4dfa7003f8c8b<:36e4dfa7003f8c8b] BinaryAnnotation(namer.path,/http/1.1/GET/zipkin-web-console)\nHere's the output from :9990/admin/metrics.json: https://gist.github.com/jimmidyson/c05a0e7959b52a5dde50bb2837ea3782\nIf it does turn out to be config problems, hanging isn't great of course.\n. @adleong I'm running this just for dev, local builds from master & running locally, & I have Kubernetes (well OpenShift actually) running locally too, accessible at https://localhost:8443. TLS works but of course with cert validation disabled (as I tried to do with the config). So yeah curl https://localhost:8443 works fine. I've actually also tried setting up a little proxy to capture requests & see that no requests reach the proxy on localhost. The timeout for name resolution is really important - it would help with quick diagnosis of issues like this but still in production consumers would want to have a quick response if lookups fail, just like DNS failures.\nTrying to debug through the namer but being a bit of a n00b to scala it makes it a bit difficult. One thing I have noticed is that the filters don't seem to work properly. I added a quick System.out (yeah, dirty but effective :-b) to both the SetHostFilter & AuthFilter & the SetHostFilter doesn't get called for some reason:\nAuthfilter Request(\"GET /api/v1/namespaces/zipkin/endpoints\", from 0.0.0.0/0.0.0.0:0)\nWhen I swap round the order of filtered calls in namer/k8s/src/main/scala/io/l5d/k8s.scala it does actually call the SetHostFilter too:\nSetHostFilter Request(\"GET /api/v1/namespaces/zipkin/endpoints\", from 0.0.0.0/0.0.0.0:0)\nAuthfilter Request(\"GET /api/v1/namespaces/zipkin/endpoints\", from 0.0.0.0/0.0.0.0:0)\nSo it could be something odd in the filter stack going on?\n. So testing the calls via curl:\ncurl -k -H \"Authorization: Bearer $(cat ~/tmp/token)\" https://localhost:8443/api/v1/namespaces/zipkin/endpoints\nworks fine so I know both the API server URL & token file are correct.\n. I've turned off auth in the API server & removed the auth filter... & querying works... I cannot explain that at all as you said AuthFilter is ridiculously simple.\nDisabling auth in API server & leaving the auth filter in place hangs as before.\n. It's this line in AuthFilter that's hanging:\nreq.headerMap(\"Authorization\") = s\"Bearer $token\"\n. Here's the full log.\n. You've got it - new line removed & works great. Well it's a newline char without a new line (just learnt about the eol option in vim). That is really annoying - sorry for the noise (but yeah something in the logs would be great).\n. Thanks for your help figuring this one out - now I can get on & play more :)\n. ",
    "moderation": "You don't have to use SSE - there is a HTTP callback method - https://mesosphere.github.io/marathon/docs/rest-api.html#get-v2-eventsubscriptions\n. Are these announcer branches from @adleong part of this work? https://github.com/BuoyantIO/linkerd/search?q=announcer&type=Issues\n. Woohoo. Works with a bunch of com.twitter.finagle.UnknownChannelException: Response entity too large: DefaultHttpResponse(decodeResult: success, version: HTTP/1.1) errors, but it does work. Hopefully the Netty 4 stuff improves as Finagle integration matures. Thanks @olix0r \n. https://mesosphere.github.io/marathon/docs/generated/api.html#v2_eventSubscriptions_get ?\n. The Finagle 6.41 release notes that mention the support for Scala 2.12.\n\nScala 2.12 Support is Here\nFinagle, Util, Ostrich, Scrooge, and Twitter-Server are all now cross-compiled for Scala 2.11 and 2.12(!). Blog post is out.. \n",
    "ashald": "Close it?\n. This is great! Any plans to have it merged anytime soon?\nAlso, another idea: it'd be awesome if if we could combine multiple dtabs somehow. For instance, for stores that support 'dirs' (I believe, all of them should) if namespace points to a dir then the dtab would be composed as a concatenation of values inside sorted alphabetically by key. That would give a tremendous power for automation!\n. Closed via #1030. I'd be happy to contribute Consul announcer (actually, port this one into linkerd https://github.com/ncbi/finagle-consul/blob/develop/src/main/scala/gov/nih/nlm/ncbi/finagle/consul/catalog/ConsulAgentCatalogAnnouncer.scala) once this PR is merged\n. @andersschuller are you going to work on this? We also need this and I thought that someone from our team would give it a try soon. OTOH, if you're working on it we would be happy to help you with testing.\n. @wmorgan yeah, we actually working on setting up metrics aggregation right now and linked/namerd is on the short list for this or next week.\n\nyou noted on Slack that the telegraf Prometheus input plugin wasn't a great solution\n\nMeh, that was our plan. :( Well, another good reason for contribution, I guess? :) cc @olgert. @wmorgan sure! I'll spin up a build and ask @olgert to play with it, hopefully today or tomorrow. IDK since when, but at least 0.7.5 supports absolute paths in rootDir. Moreover since fe5943bc53a3e305275b023abc19fc2a86c1ae3b @adleong added a test that uses absolute path for rootDir.\n. NP. ;)\n. This reminds about what we did in out Finagle proxy PoC that was designed to allow service overrides with Dtabs. We were stripping the part of the request path and as it's mentioned above and were setting it as the SCRIPT_NAME HTTP header - the one from CGI spec (and propagated to its derivatives such as WSGI). Given that apps/app servers honor the SCRIPT_NAME header properly we were able to 're-mount' our apps on arbitrary sub-paths with 0 changes to apps themselves. In other words, cool feature!\n. Addressed in #529 \n. I'm a super-newbie in Scala and I believe this code make use of a lot of refactoring and optimizations. I'd highly appreciate code review and change requests so I can bring this code to the level of quality you will agree to merge into the upstream.\nThanks!\n. @samuel The Consul-enabled io.l5d.consul namer in already implemented in Linkerd https://linkerd.io/doc/0.7.0/linkerd/namer/ and it uses Catalog API.\nThis PR targets the Dtab storage interface rather than namer. And it cannot be implemented with the Catalog API.\n. @samuel you're welcome ;)\n. @adleong once you have some time, can you please share your comments and suggestions so I can improve the code?\n. @adleong I hope I didn't miss anything and addressed all your comments. Can you please take another look? Although now there are so much changes I believe it makes more sense to look just at the code under consul and namerd/storage/consul rather than at the diff.\n. Another Try. :D\n. Thanks for the help!\n. @olix0r please take a look at this PR whenever you'll have time. I'm interested in your 'conceptual' comments along with code review as I feel that my code if far away from being pretty. :/\n. Hi @olix0r, thanks for taking your time to look a this one.\nThe short answer that configured value in namer is not a static thing that will be set for all requests but rather a required suffix to generate Consul-ish host value. I tired to explain that briefly within linkerd/docs/namer.md.\nWhen Consul is used as a DNS server it resolves names that end with pre-configured TLD. By default it's .consul but can be changed to anything, for instance, acme.co. Given that TLD Consul will resolve following name 'patterns':\n1) $name.service.$dc.$tld - some service (e.g., foobar.service.us-east.acme.co)\n2) $node.node.$dc.$tld - some machine (e.g., web-worker1.node.us-east.acme.co)\nAs I said above, you can configure TLD in Consul config but unfortunately it doesn't return it when you query its HTTP API for service info. It only uses it to distinguish between names it's supposed to handle and others that it cannot handle.\nWe're heavily relying on HTTP Host header in order to distinguish between different services/versions of service hosted on single ip:port. I believe other Consul users might have similar use-cases.\nThe idea behind this PR is to allow to set the aforementioned TLD in Consul namer and use it to set Host header in the form $name.service.$dc.$tld whenever linkerd forwards request based on Consul concrete name.\nHaving this feature and services foo-1 and foo-2 we will be able to switch a version by just setting a dtab like /foo=>/foo-1 and we won't need to generate additional web server configs to handle a 'canonical' name like foo.service.us-east.acme.co. Moreover it enables us to do an a/b or canary testing with different versions sitting on the same ip:port. I think it won't be useful only to us but is the crucial feature for many Consul users who rely on Host value.\n. @olix0r upon further digging Consul API I found an endpoint that reports the domain value we need. I think it will make a lot of sense to integrate with it so we can fetch the domain from Consul directly. Given that the option in Namer will become a flag. The question is how to name it: override_host? Or something else?\n. @olix0r thanks for the review. So right now I have it implemented following way:\n1) if given l5d-http-host meta field on bound addr linekrd will always set that as a host\n2) there is an option on ConsulCatalogNamer whether to set that meta field or not\nI see the benefit of such implementation in (1) that other components may use this feature as well (2) and you can enabled/disable that on per-namer basis.\nAlthough what you mentioned above is supposed to work in an 'opposite direction' I don't mind changing the PR to make it work as you suggested. Though I'll need your advice on how to add that parameter to linkerd.\nBTW, are you OK with meta field called l5d-http-host?\n. @olix0r I pushed some changes and updated the PR as per your comments. Can you please take another look?\n. @adleong I addressed you latest comments - can you take another look please?\n. Does the update look good now? Or you'd prefer the domainFuture call in some other form?\n. Thanks for your review guys! And sorry that this PR took so much of your time. :/ I'll try to go with smaller PRs in future.\n. @olix0r, another PR to your attention - as we agreed yesterday :)\n. @olix0r pushed an updated - please take another look\n. For an expanded explanation of the feature please see the change in namerd/docs/storage.md or for example - io.buoyant.namerd.storage.consul.ConsulDtabStoreTest\n. Within our infrastructure we have dozens of services that we deploy in automated way.\nAt any given moment of time we usually have dozens of versions of each service deployed.\nAll of them go through various stages of testing and eventually can get activated - promoted to production.\nGiven all of that our dtabs are usually structured following way (an abstract example):\n1) general rules (environments, datacenters, fallbacks etc)\n2) dtab for an application A\n3) dtab for an application B\n4) dtab for an application C\nand so on.\nFor each application the dtab can be just a simple mapping like /service/A=>/service/A-version-1 or it can be more complicated if we need a canary testing/cross-datacenter fallbacks/etc.\nWe need to update configuration for each app during our deployment (and activation) which is fully automated and doesn't involve humans.\nHaving the whole Dtab as a single peace of text (that's how ConsulDtabStore and others work right now) is not very convenient for automation. This means that we need somehow parse the Dtab contents, find the right place to to edit, drop the old peace of dtab and put a new one.\nWith proposed optional feature for ConsulDtabStore it becomes possible to split Dtab into peaces that are assembled together in a stable and predictable way. In our case we would do something like this (another abstract example):\nnamerd/\n       dtabs/\n             production = \"/current=>/datacenter/us-east; /fallback=>/datacenter/us-west;\"  # some super-generic stuff\n             production/\n                        A-services = \"/service=>/current\"  # some other generic stuff\n                        ...\n                        X-app-foo = \"/app/foo=>/service/foo-version-1\"\n                        X-app-bar = \"/app/bar=>/service/bar-version-23\"\n                        X-app-search = \"/app/search=> 0.5 * /app/search/a & 0.5 * /app/search/b \"\n                        X-app-search/\n                                     a = \"/app/search/a=>/service/bio-search-version-3\"\n                                     b = \"/app/search/b=>/service/new-bio-search-version-1\"\nThe example is totally made up but I think it gives an idea: we separate 'generic' dtabs parts into separate keys and each application is 'configured' by its own key. So during deployment/activation we just need to override a specific key and that's all.\nOf course those 'app configs' can also be 'dirs' so we can configure different aspects of app version resolution by overwriting only those peaces that are required.\nGiven comments that I received last week I made the feature optional and also adjusted behavior so it's preserves semantics of other backends and can be freely used with namerd to CRUD any parts.\n. I'm working on additional tests but would love to hear your opinion on this and would appreciate code-review.\n. For convenience, here is the quote from updated readme for Consul storage:\nConsul\nio.l5d.consul\nexperimental\nStores the dtab in Consul KV storage.  Supports the following options\n- host -- Optional. The location of the etcd API.  (default: localhost)\n- port -- Optional. The port used to connect to the etcd API.  (default: 8500)\n- pathPrefix -- Optional.  The key path under which dtabs should be stored.  (default: \"/namerd/dtabs\")\n- recursive -- Optional.  Whether to use recursive strategy to fetch namespaces.  (default: \"false\")\nIn recursive mode the storage will look not only at keys that are immediate children of the pathPrefix\nbut it will traverse the whole tree beneath recursively. The resulting dtab is composed from all non-empty\nvalid values beneath the given namespace (sorted in alphabetical order of keys).\nThis may be very useful when you need to modify your dtab in runtime in automated manner and you don't want\nto bother with dealing with dtabs syntax - you can just put mutable parts into separate keys and update them as needed.  \nAlthough recursive strategy may sound as something complicated at first it's actually very simple. Let's just\nlook at an example and see how it will work in recusrive and non-recursive modes. \nLet's assume that we configured storage pathPrefix=/namerd/dtabs and the KV in Consul is as below:\nnamerd/\n       dtabs/\n             sample = \"/foo=>/bar\"\n             nested/\n                    example = \"/fiz=>/baz\"\n                    broken = \"invalid dtab\"\n             mydtab = \"/1st=>line\"\n             mydtab/\n                    a = \"/2nd=>/line\"\n                    a/\n                      a = \"/3rd=>/line\"\n                      b = \"/4th=>/line\"\n                    b = \"/5th=>/line\"\n                    c/\n                      1 = \"/6th=>/line\"\n                    empty = \"\"\nIf we will list namespaces in non-recursive mode we will see just 2 of them (with their Dtabs given in brackets):\n- sample (\"/foo=>/bar\")\n- mydtab (\"/1st=>line\")\nOn the other hand in recursive mode we will see slightly bigger list of namespaces:\n- sample (\"/foo=>/bar\")\n- nested/ (\"/fiz=>/baz\")\n- nested/example (\"/fiz=>/baz\")\n- nested/broken (\"\")\n- mydtab (\"/1st=>line\")\n- mydtab/ (/1st=>/line;/2nd=>/line;/3rd=>/line;/4th=>/line;/5th=>/line;/6th=>/line;)\n- mydtab/a (\"/2nd=>/line\")\n- mydtab/a/ (\"/3rd=>/line;/4th=>/line\")\n- mydtab/a/a (\"/3rd=>/line\")\n- mydtab/a/b (\"/4th=>/line\")\n- mydtab/b (\"/5th=>/line\")\n- mydtab/c/ (\"/6th=>/line\")\n- mydtab/c/1 (\"/6th=>/line\")\n- mydtab/empty (\"\")\nSo the rule is simple: everything that doesn't end with / is a key that can has some value.\nEverything that ends with / is a 'virtual namespace' that aggregates other keys.\nWith regards to namerd API there is only 1 restriction - you cannot create/update 'virtual namespaces'.\nBut you can read and delete them (removal of a 'virtual namespace' will recursively remove everything beneath it).\n. Hey @olix0r , thanks for your feedback.\nWith regards to Dtab Syntax - it looks like we have a little miscommunication there. Although what you described sounds like a great feature it's not on the list of our requirements.\nOur real requirement is being able to decompose a Dtab into set of independent peaces so that each of them can be edited separately. \n\nI think there needs to be a way to have an application use multiple dtabs\n\nCan you explain this in more details please?\nIs it about using different namespaces for different services in runtime without prior configuration? \nBecause with the proposed feature whenever we need to add a service we don't have to pre-configure anything - just put an extra key (or in terms of namerd API - create a new 'nested' namespace) with few Dentries in it and we're done. What also important is that we neither introduce any new syntax (e.g., how to define 'composed' dtabs) nor we break the namerd API semantics - assuming that namespaces are opaque strings for namerd with recursive mode enabled you can do exactly the same set of CRUD operations.\n\n@adleong has done some design exploration on this front which seems really promising\n\nAre you talking about #235 or something else? The #235 really looks interesting but I think it's clear that it solves a totally different problems compared to our use case/this PR.\n\nWe're being careful about introducing this, since we're hoping that we can begin to stabilize the namerd API over the next few months, and so we have been reticent to introduce a public API only to have to break it shortly thereafter. \n\nThat's really sounds great. And we do care about APIs stability a lot as well - that's exactly the reason why we tried our best to implement the proposed feature in least intrusive way (making it optional and keeping the existing API semantics) hoping that you can consider it at least as a temporary solution for a use-case such as ours unless namerd will get some even better way to deal with it (which might take more time after API stabilizes).\nI hope this clarifies our use cases a bit more.\nIn our situation, we need to have a solution for the problem we described. We would gladly prefer some generic solution on the overall namerd API level but since it doesn't provide such an option we though about an optional non-intrusive feature that's 100% compatible with the existing API. Our assumption is that it can be dropped in the future when the problem would be solved on namerd level and an optional feature in experimental backend shouldn't hurt.\nI understand that you hesitant about this feature since you have much broader perspective than me but I also would like to understand what are the real stoppers here.\n. The more I think about it the more I realize that I failed to describe our use-case properly.\nWithin our environment we have a lot of services. Right now we run about 100 services registered in Consul be we also have ~7K legacy services that pending migration to Consul.\nGiven that we want to use linkerd / namerdfor service routing and, obviously, configure it using Dtab.\nAnother thing that I need to mention that we're doing a lot of deployments (which are fully automated): with 100 services being currently running we have about 50 deployments per day.\nWith this approach the resulting Dtab can be pretty huge and so we have few concerns:\n1) getting all of the data each time we need to change the little peace of it and then putting it back has a lot of overhead\n2) with concurrent updates we need be careful version checks so we won't discard other concurrent changes (which would be a huge problem with frequent deployments with 7K services)\n3) we will be veeery close to Consul KV limit for max key's value: it's 512k - having ~7K services we get about 75 bytes per service config in Dtab\nAll of that being said, I believe that our scale is by far is not the biggest in the world and other people using Consul at such or greater scale will have same concerns.\nAnalyzing the situation we figured out that it'd be great if we could split Dtab into tiny peaces that can be edited separately (reducing concurrency), reduce overhead traffic and discard any concerns about size restrictions in Consul KV.\nLooking at namerd and its API we made a few assumptions:\n1) from namerd perspective namespace names are opaque strings - given the diverse nature of backends that seems to be the case as otherwise it can limit integration options with some backends\n2) using namerd API one should be able to do full CRUD with any namespace exposed by a backend\n3) any namerd backend shouldn't rely/require anything else except what's defined in API/trait\n4) the proposed feature might be an 'advanced mode' that must be enabled separately (an analogy to experimental flag - confirm that you know what you're doing)\n5) this is an optional feature that may be obsoleted in future and as well removed given the fact namerd will get a 'generic' way to deal with such a use-case\nGiven all of that we implemented a proposed feature and presented it within the current PR. I think you see how our assumptions are reflected into the implementation but let me know if any of that requires additional clarification.\nHope this use-case description will make more sense the previous one. :)\n. Thanks for looking at the PR.\nThe issue with delegate API is that it returns the whole tree and consumers of the API will have to implement the logic to interpret it.\nAlthough that might be suitable for the needs you mentioned it will make consuming the API harder.\nWe though about using bind + addr but that doesn't work for few reasons:\n1) if bind returns and alt (like 0.50*/#/io.l5d.consul/dc1/myservice & 0.50*/#/io.l5d.consul/dc2/myservice) that its output cannot be directly fed to addr endpoint\n2) that would require 2 calls and will make workign with the API harder\nAs I mentioned this before, in #linkerd during discussion with Alex, and during our chat - the use-case for this is to allow apps that cannot benefit from linkerd (due to various reasons - legacy code, protocols not supported by linkerd and so on) to use namerd as a generic resolver. IIRC, you and William agreed that it sounds like a valid usecase.\nAs for output format - yeah, I agree that it's by far not the best one. I just used the same idea as in addr endpoint. Now looking again, as per your comment, at delegate API output I see that we can re-use the same serialization logic from DelegateApiHandler.getDelegateRsp for this proposed endpoint and we will get a pretty easy parseable JSON.\n. @olix0r @adleong can you please take another look at this PR? In first place I'm interested in the interface part - are you OK with calling the endpoint resolve? Are you OK with such output format? And are you OK with parameters it takes and their names?\nOnce you OK with all those things I'd like to ask you whether I need to improve code so it could be merged. Thanks!\n. @adleong @olix0r I rebased the PR on master after #575 was merged - please let me know what you think about it now\n. As per agreement with @adleong I pushed an extra commit to fix JsonAddr.mk behavior and include Address.Inet meta into the JSON representation.\n. Sorry, it's my fault. :(\n. \ud83d\udc4d \n. > I don't think your replacement of list with get is correct. By specifying ?keys and ?separator, we get a list of keys only (not values) from a single directory (not recursive). Take a look at https://www.consul.io/docs/agent/http/kv.html#single\nYeah, that's how it was originally. As I mentioned above, the idea behind this refactoring to switch to from GET .../kv/some-key?raw to GET .../kv/some-key so it would be easier to implement extensions for ConsulDtabStore (make API more generic).\nBut once I had that in place I realized that (1) GET .../kv/namerd/dtabs/?recursive would give us exactly the same result as ?keys. Yeah, you're right about separator but:\n1) it actually doesn't filter out dirs - we still need to do manual filtering\n2) since namespaces expected to be simple keys the current implementation if sufficient enough\nAlthough I can put check like .filter(!_.Key.contains(\"/\")) back.\n. > Would you mind splitting this up into a PR for each of your bullet points in the description? Some of these are obvious wins (ability to pass a dc into consul store, ability to add auth token, etc) whereas the larger refactors are more controversial.\nYeah, will do.\n. @adleong as per your request in #576 I started splitting it into smaller chunks. This one is about auth token and essentially is pretty small one - could you please take another look? I changed that way how filters are composed as per your advice.\n. Sure! Sorry that I'm constantly forgetting about it. :(\n. I updated the commit to include CHANGES.md entry. Waiting on this to be merged so I can follow-up with other pieces of #576 \nThanks for the review!\n. That's interesting point. There are lot of things that we might consider on the 'security' side of the things - including the fact that the token would be passed in plain text along with request unless HTTPS is used.\nWhile working on this PR I made few assumptions that are common for environment I have to deal with - such us dedicated cluster of instances namerd (therefore you don't need to pass the token around linkerd instance configs) as well as automated deployment with config files filled with token stored in secure store and a service account to run the process together with restricted access on namerd config file.\nAt the end of the day that's up to you: would you prefer tokenFile - let me know and I will update the PR. Or we can proceed with a simple token field and think about tokenFile option if there will be demand for it.\n. @adleong another piece of #576 - a micro change to make Namerd UI a bit more convenient. Let me know what you think please.\n. What would be the next step for this PR? Merge or we're waiting for an additional review?\nThanks!\n. @adleong Another piece of #576 - please take a look whenever you will have time. Thanks\n. @adleong can you take a look at this PR as well please?\n. It turned out that it's doable by adding the same namer with another prefix and config.\n. How about making it multiGet but making get to work work as a call to multiGet and taking 1st item from the list?\n. The short answer is: yes.\nThe long answer:\n1) in non-recursive mode (default-one) it's always either 1-item list or 404\n2) in recursive mode it's a list of lexicographically ordered items filtered by prefix match therefore the requested item is always 1st in the list\nThe CatalogNamer doesn't care about recursive mode pretty much.\nOn a side note, IMO it's better to have a single execute that works with the json rather than execute and executeJson as it's now.\n. > Unless the requested key doesn't exist \ud83d\ude1c \n\nIn that case, taking the first item from a recursive get is definitely not what you want.\n\nBut that applies only to recursive mode - in non-recursive mode that would result in 404. ;)\nMy proposal is to 'use default mode by default' but add a flag to switch to recursive mode.\nWith it CatalogNamer's observe (or the simple get as proposed above) will use the non-recursive mode.\n\nIs lexicographical ordering a guarantee of the api? I couldn't find that in the docs.\n\nI don't recall seeing that in docs as well but it appears that that's the case. That how items displayed in Consul UI and that's how they are returned in API calls. I will ask their support and if yes maybe they can add that to docs. But, again, that applies only to recursive mode which is not used by CatalogNamer but will be used by the extension we planing to build with it (we applying an extra sort just in case in it).\n. @adleong thanks - totally forgot about description. :( I updated both title and description.\nPlease let me know whether I need to fix anything else so it can be merged.\nThanks!\n. I see a failure on CircleCI but it failed due to dependency resolution error. Can you please re-run the build?\n. Thank you!\n. Thanks guys!\n. Checked - works like a charm!\n. The ?passing parameter is not supported by Catalog HTTP API. :(\n. @adleong I have plans to submit a PR [hopefully] somewhere next week implementing it as an extra namer. Want to bounce an idea for it with you first:\n1) By default behaves same way as CatalogNamer but has an option to respect Consul's health status (by adding ?passing)\n2) By default filters out services in 'maintenance mode' but doesn't pay attention to other health checks and has an option to return only healthy services (by adding ?passing)\n3) Just returns only healthy services all the time (always add ?passing)\n4) Accepts names/patterns for health check names/IDs that it should respect and ignore other\n5) Something else?\nThanks! \n. I'd be happy to submit a PR implementing the feature in any form. It'd be great to hear other opinions so we can decide on best approach to handle the problem.\n. Sure\n. You're welcome ;)\n. @JustinVenus I'll give it a try later today\n. Hm, just noticed 1 issue: looks like it's not possible to rely on catalog api but do respect the maintenance health check, right? Or in other words, filter out only those services that are in maintenance mode.\n. @JustinVenus I verified your custom build and it works as expected\n. We would glad to have this feature as well! :)\n. Yes, exactly the same behavior.\n. Yeah I like the new representation better - it it expands alternatives in Dentry dst's and it's easier to analyze the output. \ud83d\udc4d \n. So, was it the issue of rendering? Or there is also a bug in delegate API?\n. Ah, I see. Cool - thanks!\n. I'm a bit busy right now putting linkerd in production but once I'm done with I'd be happy to contribute it if it won't be done by that time.\n. What kind of health-check do we want use when announcing?\n1) TTL - we will need to configure TTL and send heartbeats\n2) HTTP - Consul is going to make HTTP requests to given endpoint and look for 2XX status codes\n3) TCP - Consul is going to try to connect via TCP to given endpoint periodically\n. @olix0r do you have an opinion on the question above?\n. Looks like you never run out of neat features! Huge like - we would use it!\n. \ud83d\udc4d \n. This is a follow-up for #552 that we noticed after starting using it to route our services - @olix0r / @adleong what do you think?\n. I'm glad you liked it. :)\n. I hope this will get into next release? :)\n. Yay! Thanks!\n. @olix0r / @adleong please take a look at updated code.\n. Cleanup - done. :)\n. Done, waiting on the build.\n. Yaaay! :)\n. Updated PR according to comments.\n. Tests failed but the failure looks unrelated. :/\n. C'mon, CircleCI, tests are \ud83d\udcaf . :)\n. Hm, in theory - yes. In practice - let me try to implement it and see the performance impact.\nDo you want that as addition to this PR or as a new one?\n. Yeah, this gives 20K QPS almost constantly. I'll update PR over weekend - hope you won't do release today. :)\nThe only question - how to name those case classes? :) NameActClsoe and AddrActClos? :)\n. Actually was able to do that while riding in metro. :) How does it look to you?\n. Yaaay! :)\n. @olix0r does it look better now? :)\n. If the exception will be thrown while in handler wouldn't it result into a failed future (I assume Future.exception)?\n. And if not - we probably can wrap handler into try-catch.\n. Yaaay! Thanks! :)\n. Wow, this is amazing! I'll take a more detailed look a bit later today. One of things that, frankly, I didn't like from the very beginning is using a var index in caches - IMHO a \"recursive future\" (as in DtabStore) serves such purpose better.\nAlso I have a small guess that there might be another issue in caches (although I'm not 100% sure). I did number of load tests on Friday involving CatalogNamer and left namerd running over the weekend - on Tuesday we noticed few 10Ks of open connections reported by Nagios on that host. As I understand, neither DcCache nor SvcCache are explicitly closing \"watch process\" - IIRC Alex mentioned something similar on Friday. I wonder whether fetching can be done with closable Var.async and those caches can be done in similar way to what we have in HttpControlService - CacheBuilder-based cache that keeps observations open but stopes them upon eviction.\nJust few ideas.\n. Great, I have few questions/comments about it as well - please cc me on the PR once it's ready. :)\n. Ha-ha, deal. :) Let me know when code is ready for a test-drive - I'll make a build and try it in our environment.\n. Please correct me if I'm wrong, but it looks like it will require modification of the container image that is used to run tests... Or do we want to download Consul agent dynamically during each test?\n. This is another tricky problem as there are now way to distinguish between temporarily disconnected datacenter and non existing one. So re-trying sounds legit.\nOTOH, if we want to have a bulletproof solution we can think about an optional config field where we can list DCs that are known to exist for sure and enable re-try for them but fail for any other DCs.\n. Oh yeah, being able to terminate observations based on some conditions would be great. :) And I agree, it would be better solution.\n. Once this is done https://github.com/hashicorp/consul/issues/2736 I'd like to submit a PR to improve Consul namer in a way so that it'll check datacenter availability before fetching DC services - this would eliminate meaningless retries may DC outage occur. Does it sound good to you?. Done. ;)\n. @JustinVenus yeah, that's exactly how it's being implemented - please check out #666 \n. Thanks for comments, I'll do adjustments you proposed. How it looks like in general to you? Is it \"good enough\" so I can proceed with introducing consistency for dtab store? If yes, do you want a single consistency parameter for it or 2 separate - for read and for write?\n. BTW, love PR # :D\n. So, 2 independent options then? I mean, no \"smart\" behavior like \"if write consistency is not set then it's same as read consistency\"?\n. 2nd part is done as well - readConsistencyMode & writeConsistencyMode for consul dtab store. Please let me know how it looks to you guys?\n. Actually I haven't tried it yet in our environment. I can give it a try but I cannot replace all namerd instances in a datacenter with dev build. :( Our security guys are pretty strict :)\nBut without that I don't think we will see a difference. So I see 2 options:\n1) We replace default value for reads to stale since it's just makes sense\n2) We release it as is, I'm switching to stale to see the impact and if it's as great as expected then we change the default in next release\n. Yeah, sounds great to me!\n. I rebased the PR over latest master - waiting for it to be merged so I can submit another about failFast.\n. Thanks!\n. IIRC the problem is a bit deeper - there is fundamental limitation within netty. org.jboss.netty.handler.codec.http.HttpChunkAggregator has int maxContentLength as a constructor parameter. Given that it's signed it means that HttpChunkAggregator cannot handle requests larger than 2GB at all. So unless you ready to hack your own version of codec there is no easy way to achieve that.\n. Not sure whether docs/changelog are good enough though. :/\n. Pushed re-base to latest master + fixes according to review\n. @olix0r do you have something in mind? :)\n. I think it can be closed now as the PR was merged.\n. Also rebased brunch on master.\n. Will rebase once I'll get home in few hours ;)\n. Another attempt ;)\n. Meeerge, I neeed somebody meeerge... me... :)\n. FWIW, I'm about to release an OSS lib with a set of rewriting namers inspired by what exists in Linkerd now. If you fill hesitant about adding such namers to linkerd core we can put them into that lib and advertise it to whoever needs them (of course that lib can be sued as a Linkerd plugin). And later we can see whether there is a demand and whether it's worth adding them to the core.\n. Branch was out of sync with master so I updated it just in case.\n. Meh, MuxEndToEndTest failed again. Can you please re-run tests on circleci please?\n. Yeah, we didn't notice any changes in behavior/resource utilization over ~24 hours running this code and the upstream in parallel. Happy to help! ;). Each time when I think about it there is 1 idea that comes to my mind each time: IIRC, in HTTP proxy spec there is a part that says that for the sake of HTTPS support a client can use a CONNECT method with some special parameters (headers?). In return proxy must figure out the proper destination, open a TCP connection to it and reply with a local endpoint info (IP + port?) that client should use to send arbitrary data as through normal TCP connection. I'm wondering whether this can be used as a \"simple\" solution to handle TCP traffic. If it's indeed possible I could imagine some kind of micro-utility that would wrap a target process and request required endpoints for it. For instance, let's say we want to talk to Redis that is known as /db/redis - then the assumed usage would be something like:\n$/ request-service --name=/db/redis --output=REDIS_ENDPOINT -- ./my-program-that-uses-redis\nIn this example request-service is responsible requesting an \"endpoint\" from a proxy (Linkerd) and exporting it as env var REDIS_ENDPOINT (let's say, it's value is 127.0.0.1:42356). After this it can just exec whatever is given after --. In turn, ./my-program-that-uses-redis is expected to read REDIS_ENDPOINT env var and connect to Redis using provided endpoint.\nThis, definitely, won't allow such fancy stuff as dynamic load balancing and so on but at least will be able to 100% rely on Linkerd for service discovery. In terms of forwarding TCP traffic from local port to remote - that expected to be easy.\nI'm wondering how this idea sounds to others? Do you think it's something feasible or rather sounds crazy? The only other \"good\" (IMHO) approach would be to implement SOCKS5 proxy that (a) allows proxying of arbitrary traffic, (b) can learn about clients original desired destination (e.g., replacement for host header on a lower level) and (c) IIRC has some capabilities for protocol negotiation.. 1) IIRC there was a fix for Mux in 0.8.2 (we didn't see this issue since we updated)\n2) To best of my knowledge as per now there is an HTTP/1.1 streaming API that can be used instead of Mux (which is state-less and shouldn't have such issues by design).\nTL;DR: I think we can close this issue. :). From our experience, we stopped seeing such errors for real only after switching to gRPC.. Sounds like a big thing. It also reminds me that we have similar discussion last, hm, Spring? :)\nLet me share what we came up with - maybe it will help in some way. As you know, we heavily use Consul so we decided to leverage Consul KV storage to solve this problem. ATM we don't use Namerd control API to modify Dtab and rather update things directly in Consul (although, with some adjustments to our Dtab storage plugin that would be possible as well).\nEssentially we've separated concepts of namespace and Dtab:\n1) a namespace X is a list of \"references\", for simplicity let's describe them as simple strings:\n/namerd/dtabs/base\n/namerd/dtabs/shared?dc=us-east1\n/namerd/dtabs/shared\n/namerd/dtabs/local\nso essentially each entry in namespace definition is a path in Consul KV store and optional parameter for cross-datacenter query. Obviously, a namespace Dtab is composed in the order paths defined.\nEach of paths is queried using a recursive query and we rely on lexicographical sorting to ensure stable, repeatable and easy to grasp sorting/behavior.\nWe found that it's essential to be able to define some (a) generic stuff, (b) something unique to environment and (c) something that can be shared with other environments.\n2) Service definitions\nAmong aforementioned paths most are \"more or less static\" and essentially version-controlled and populated with Terraform. The \"automatic\" part is contained within /namerd/dtabs/shared/services.\nGiven a service name foobar the one would expect to find its definition under /namerd/dtabs/shared/services/foobar.\nOf course it's an opinionated approach tied to our needs, but if we put away details such as naming of particular things I think we can derive an \"API\" or a \"framework\" for managing Dtabs. Judging from our experience we found that the most important part that allows a lot of flexibility is an ability to combine Dtabs (that can be manipulated separately) into \"bigger dtabs\".\nThe only problem that is not addressed with this approach is \"change of view based on client\" but I believe it can be implemented using a special identifier that would select particular \"namespace\" based on some properties of request (this comes from an assumption that 1 L5d per host deployment approach is used for efficiency).\nHope this helps. May you need any other details - feel free to ping me.. @elecnix there are just a few idea that I can give you as of today:\n1) implement a custom identifier plugin that will do the trick\n2) put another proxy (like nginx lol) in front of Linkerd that will do that\n3) try to handle that with Dtab-magic (you can get some inspiration from https://blog.buoyant.io/2017/04/25/announcing-linkerd-1.0/ and the PER-CLIENT CONFIGURATION section)\nIMO this is another use cases for a \"new plugin interface\" for Linkerd that would allow modifying requests or even rejecting them even before the identification happens so that we don't need to build custom identifiers for such cases.. @elecnix yeah, that's another option - and great idea!. So, what is the answer to:\n\nCan namerd namespaces contain slashes?\n\n? :). So no decision so far for the future API, right? We're working on a new Dtab-Store plugin and want to ensure compatibility with the new API. We're planning to play with a new idea for it so we will share our experience with you once it's ready. ;). Current implementation is more of PoC and is based on \"least intrusion\" strategy. :) I'm not sure that passing Dtab in request body while passing other parameters via query string is a good idea... Maybe we should move all parameters to post? And add a case class for de-serializaing that stuff with a bunch of matches to get around Options while returning HTTP 400 on missing/invalid input.. OK, thanks for feedback.\nIs it OK to return HTTP 400 on JSON validation errors?\nAlso, the only usage of delegator.json with GET method Is aw was in Namerd Admin so I though it's OK to move it to POST. I'll just add a case for POST then.. @olix0r may I ask you to take another look at this PR?. @olix0r I think adding a test shouldn't be an issue. Should it be a test for DelegateApiHandler within admin module?. @olix0r please take another look. I found some tests for the DelegateApiHandler in linkerd-admin module so I just extended them rather then adding one more (4th :) ) test class for the DelegateApiHandler. @rmars pushed a fix, please take another look. Yaaay! Thanks!. @adleong happy to help! ;). Meh, I didn't notice this earlier, but there where no update to the docs. :(. Whaaat? I just fixed typo in commit message and tests failed T_T. @adleong hooray! Thanks for the review! So can this fix get into the release next week?. @olix0r thanks! Maybe not the most elegant one but I tried to keep it simple and concise :). @adleong yeah, sure. I'm wondering whether it can be handled like:\n1) if there is a \"successful active\" path - highlight all steps in it with green\n3) leave \"possible alternatives\" white all the way down\n3) tint Neg/Fail with red\n4) If there are no \"successful path\" - show all possible paths in red (all steps from the root to leaves)\nOr would you prefer just a toggle to switch between \"show only active path\" / \"show all possible paths\"?. \ud83d\udc4d\ud83d\udc4d\ud83d\udc4d. Sorry for jumping in but I like it! :). Hoooray! Thanks!. @wmorgan @olix0r I promised to submit this issue a long time ago but just got a moment to process the screenshot :D. YES PLEASE! :)\nBTW, how do you like an idea of an endpoint where you can see UI metrics for an individual service/client?. @adleong was out last week but hope to be able to check how it looks like this week. Will let you know - thanks!. This helps tremendously - thanks a lot!. Reminds me about #1074 :). I'm also wondering whether it makes sense to set the X-Forwarded-Host header on the request:\n\na de facto standard for identifying the original host requested by the client in the Host HTTP request header, since the host name and/or port of the reverse proxy (load balancer) may differ from the origin server handling the request.\n. > I think this makes sense. Just a couple nits.\n\nYou referr to the PR in general or to my comment saying that we might as well set X-Forwarded-Host?\nBTW, tests have passed - that's just good luck after N-th re-run? :)\nThanks for the review! I'll do the changes tomorrow.. OK, thanks!. @olix0r deal :D. The main issue with including such a module is that every Finagle app that used to work \"as is\" will now require (1) an extra dependency on Linkerd codebase and (2) a re-build.\nI think one of the beauties of Linkerd is that it can be used absolutely transparently from the application perspective. Even though it's very convenient for Linkerd itself, enforcing such changes means not only deep integration with Linkerd on the level of apps, but also that the main principle of being non-intrusive is not holding anymore. :(. We have a very similar situation where we need to let the backend application know what host was used by the original user. It can be injected by the web-servers such as Apache/Nginx if there is one but when there are no web-server in front this becomes problematic.\nAs Wikipedia says, this is\n\na de facto standard for identifying the original host requested by the client in the Host HTTP request header, since the host name and/or port of the reverse proxy (load balancer) may differ from the origin server handling the request.\n\nand to the best of my knowledge there are tons of applications out there that rely on it.\n. @adleong I didn't see labelers docs before so reading through right now but is it possible to have host key one very request set to the actual value?\nAlso, one of the problems is that, as I mentioned before, there are tons of applications that rely on X-Forwarded-Host so in terms of backward compatibility it'd be great if it was possible to inject it as well.. I'm wondering whether it's theoretically possible to extended labelers functionality in a way that it will allow setting any sorts of headers on demand.... Hm, I think we have actually seen similar behavior but just didn't have time to figure out a reproducible scenario. For now we workaround this by restarting Linkerd each time Consul is restarted.... Hm, there is no entry in CHANGES.md for this - is it intentionally or you just forgot to add it? :). Thanks for fixing it! :). cc @adleong . cc @adleong . If that's the case maybe we've got it in a wrong way. Let's close it for now and if we will be able to reproduce it we will re-open the issue. Thanks!. So if I understand correctly, this should be fixed now (it should take less than 5 mins to recover), right?\nAFAIK, you can adjust the wait time so maybe it makes sense to set it to few seconds when we retry on failure? Just an idea for the future. Hm, yeah, that makes sense.\nAnd yeah, I cannot think about a use-case that would require adjusting the wait time [yet].. This looks great to me! I don't know how important is this, but I noticed that unless you set the concurrency level within cache to 1 there is a pretty good chance to end with several duplicated activities.. \nAround the same time there were few stack-traces in namerd02\nMay 25 15:33:52 namerd02 namerd: W 0525 19:33:52.350 UTC THREAD37: Unhandled exception in connection with /10.10.1.1:46384, shutting down connection\nMay 25 15:33:52 namerd02 namerd: io.netty.handler.codec.http2.Http2Exception$StreamException: Stream closed before write could take place\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.Http2Exception.streamError(Http2Exception.java:149)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController$FlowState.cancel(DefaultHttp2RemoteFlowController.java:492)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController$FlowState.cancel(DefaultHttp2RemoteFlowController.java:473)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController$1.onStreamClosed(DefaultHttp2RemoteFlowController.java:103)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection.notifyClosed(DefaultHttp2Connection.java:349)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection$ActiveStreams.removeFromActiveStreams(DefaultHttp2Connection.java:957)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection$ActiveStreams.deactivate(DefaultHttp2Connection.java:913)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection$DefaultStream.close(DefaultHttp2Connection.java:469)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection.close(DefaultHttp2Connection.java:153)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.Http2ConnectionHandler$BaseDecoder.channelInactive(Http2ConnectionHandler.java:191)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.Http2ConnectionHandler.channelInactive(Http2ConnectionHandler.java:392)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)\nMay 25 15:33:52 namerd02 namerd: at com.twitter.finagle.netty4.channel.ChannelStatsHandler.channelInactive(ChannelStatsHandler.scala:113)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1329)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:908)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:744)\nMay 25 15:33:52 namerd02 namerd: at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\nMay 25 15:33:52 namerd02 namerd: at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)\nMay 25 15:33:52 namerd02 namerd: at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\nMay 25 15:33:52 namerd02 namerd: at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nMay 25 15:33:52 namerd02 namerd: at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nMay 25 15:33:52 namerd02 namerd: at com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run(BlockingTimeTrackingThreadFactory.scala:24)\nMay 25 15:33:52 namerd02 namerd: at java.lang.Thread.run(Thread.java:745)\nMay 25 15:33:52 namerd02 namerd: W 0525 19:33:52.351 UTC THREAD37: Unhandled exception in connection with /10.10.1.1:46384, shutting down connection\nMay 25 15:33:52 namerd02 namerd: io.netty.handler.codec.http2.Http2Exception$StreamException: Stream closed before write could take place\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.Http2Exception.streamError(Http2Exception.java:149)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController$FlowState.cancel(DefaultHttp2RemoteFlowController.java:492)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController$FlowState.cancel(DefaultHttp2RemoteFlowController.java:473)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController$1.onStreamClosed(DefaultHttp2RemoteFlowController.java:103)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection.notifyClosed(DefaultHttp2Connection.java:349)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection$ActiveStreams.removeFromActiveStreams(DefaultHttp2Connection.java:957)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection$ActiveStreams.deactivate(DefaultHttp2Connection.java:913)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection$DefaultStream.close(DefaultHttp2Connection.java:469)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection.close(DefaultHttp2Connection.java:153)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.Http2ConnectionHandler$BaseDecoder.channelInactive(Http2ConnectionHandler.java:191)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.Http2ConnectionHandler.channelInactive(Http2ConnectionHandler.java:392)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)\nMay 25 15:33:52 namerd02 namerd: at com.twitter.finagle.netty4.channel.ChannelStatsHandler.channelInactive(ChannelStatsHandler.scala:113)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1329)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:908)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:744)\nMay 25 15:33:52 namerd02 namerd: at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\nMay 25 15:33:52 namerd02 namerd: at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)\nMay 25 15:33:52 namerd02 namerd: at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\nMay 25 15:33:52 namerd02 namerd: at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nMay 25 15:33:52 namerd02 namerd: at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nMay 25 15:33:52 namerd02 namerd: at com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run(BlockingTimeTrackingThreadFactory.scala:24)\nMay 25 15:33:52 namerd02 namerd: at java.lang.Thread.run(Thread.java:745)\nMay 25 15:33:52 namerd02 namerd: W 0525 19:33:52.354 UTC THREAD37: Unhandled exception in connection with /10.10.1.1:46384, shutting down connection\nMay 25 15:33:52 namerd02 namerd: io.netty.handler.codec.http2.Http2Exception$StreamException: Stream closed before write could take place\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.Http2Exception.streamError(Http2Exception.java:149)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController$FlowState.cancel(DefaultHttp2RemoteFlowController.java:492)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController$FlowState.cancel(DefaultHttp2RemoteFlowController.java:473)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController$1.onStreamClosed(DefaultHttp2RemoteFlowController.java:103)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection.notifyClosed(DefaultHttp2Connection.java:349)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection$ActiveStreams.removeFromActiveStreams(DefaultHttp2Connection.java:957)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection$ActiveStreams.deactivate(DefaultHttp2Connection.java:913)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection$DefaultStream.close(DefaultHttp2Connection.java:469)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection.close(DefaultHttp2Connection.java:153)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.Http2ConnectionHandler$BaseDecoder.channelInactive(Http2ConnectionHandler.java:191)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.Http2ConnectionHandler.channelInactive(Http2ConnectionHandler.java:392)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)\nMay 25 15:33:52 namerd02 namerd: at com.twitter.finagle.netty4.channel.ChannelStatsHandler.channelInactive(ChannelStatsHandler.scala:113)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1329)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:908)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:744)\nMay 25 15:33:52 namerd02 namerd: at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\nMay 25 15:33:52 namerd02 namerd: at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)\nMay 25 15:33:52 namerd02 namerd: at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\nMay 25 15:33:52 namerd02 namerd: at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nMay 25 15:33:52 namerd02 namerd: at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nMay 25 15:33:52 namerd02 namerd: at com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run(BlockingTimeTrackingThreadFactory.scala:24)\nMay 25 15:33:52 namerd02 namerd: at java.lang.Thread.run(Thread.java:745)\nMay 25 15:33:52 namerd02 namerd: W 0525 19:33:52.355 UTC THREAD37: Unhandled exception in connection with /10.10.1.1:46384, shutting down connection\nMay 25 15:33:52 namerd02 namerd: io.netty.handler.codec.http2.Http2Exception$StreamException: Stream closed before write could take place\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.Http2Exception.streamError(Http2Exception.java:149)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController$FlowState.cancel(DefaultHttp2RemoteFlowController.java:492)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController$FlowState.cancel(DefaultHttp2RemoteFlowController.java:473)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController$1.onStreamClosed(DefaultHttp2RemoteFlowController.java:103)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection.notifyClosed(DefaultHttp2Connection.java:349)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection$ActiveStreams.removeFromActiveStreams(DefaultHttp2Connection.java:957)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection$ActiveStreams.deactivate(DefaultHttp2Connection.java:913)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection$DefaultStream.close(DefaultHttp2Connection.java:469)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection.close(DefaultHttp2Connection.java:153)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.Http2ConnectionHandler$BaseDecoder.channelInactive(Http2ConnectionHandler.java:191)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.Http2ConnectionHandler.channelInactive(Http2ConnectionHandler.java:392)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)\nMay 25 15:33:52 namerd02 namerd: at com.twitter.finagle.netty4.channel.ChannelStatsHandler.channelInactive(ChannelStatsHandler.scala:113)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1329)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:908)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:744)\nMay 25 15:33:52 namerd02 namerd: at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\nMay 25 15:33:52 namerd02 namerd: at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)\nMay 25 15:33:52 namerd02 namerd: at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\nMay 25 15:33:52 namerd02 namerd: at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nMay 25 15:33:52 namerd02 namerd: at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nMay 25 15:33:52 namerd02 namerd: at com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run(BlockingTimeTrackingThreadFactory.scala:24)\nMay 25 15:33:52 namerd02 namerd: at java.lang.Thread.run(Thread.java:745)\nMay 25 15:33:52 namerd02 namerd: W 0525 19:33:52.357 UTC THREAD37: Unhandled exception in connection with /10.10.1.1:46384, shutting down connection\nMay 25 15:33:52 namerd02 namerd: io.netty.handler.codec.http2.Http2Exception$StreamException: Stream closed before write could take place\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.Http2Exception.streamError(Http2Exception.java:149)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController$FlowState.cancel(DefaultHttp2RemoteFlowController.java:492)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController$FlowState.cancel(DefaultHttp2RemoteFlowController.java:473)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController$1.onStreamClosed(DefaultHttp2RemoteFlowController.java:103)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection.notifyClosed(DefaultHttp2Connection.java:349)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection$ActiveStreams.removeFromActiveStreams(DefaultHttp2Connection.java:957)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection$ActiveStreams.deactivate(DefaultHttp2Connection.java:913)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection$DefaultStream.close(DefaultHttp2Connection.java:469)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection.close(DefaultHttp2Connection.java:153)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.Http2ConnectionHandler$BaseDecoder.channelInactive(Http2ConnectionHandler.java:191)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.Http2ConnectionHandler.channelInactive(Http2ConnectionHandler.java:392)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)\nMay 25 15:33:52 namerd02 namerd: at com.twitter.finagle.netty4.channel.ChannelStatsHandler.channelInactive(ChannelStatsHandler.scala:113)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1329)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:908)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:744)\nMay 25 15:33:52 namerd02 namerd: at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\nMay 25 15:33:52 namerd02 namerd: at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)\nMay 25 15:33:52 namerd02 namerd: at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\nMay 25 15:33:52 namerd02 namerd: at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nMay 25 15:33:52 namerd02 namerd: at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nMay 25 15:33:52 namerd02 namerd: at com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run(BlockingTimeTrackingThreadFactory.scala:24)\nMay 25 15:33:52 namerd02 namerd: at java.lang.Thread.run(Thread.java:745)\nMay 25 15:33:52 namerd02 namerd: W 0525 19:33:52.358 UTC THREAD37: Unhandled exception in connection with /10.10.1.1:46384, shutting down connection\nMay 25 15:33:52 namerd02 namerd: io.netty.handler.codec.http2.Http2Exception$StreamException: Stream closed before write could take place\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.Http2Exception.streamError(Http2Exception.java:149)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController$FlowState.cancel(DefaultHttp2RemoteFlowController.java:492)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController$FlowState.cancel(DefaultHttp2RemoteFlowController.java:473)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController$1.onStreamClosed(DefaultHttp2RemoteFlowController.java:103)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection.notifyClosed(DefaultHttp2Connection.java:349)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection$ActiveStreams.removeFromActiveStreams(DefaultHttp2Connection.java:957)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection$ActiveStreams.deactivate(DefaultHttp2Connection.java:913)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection$DefaultStream.close(DefaultHttp2Connection.java:469)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.DefaultHttp2Connection.close(DefaultHttp2Connection.java:153)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.Http2ConnectionHandler$BaseDecoder.channelInactive(Http2ConnectionHandler.java:191)\nMay 25 15:33:52 namerd02 namerd: at io.netty.handler.codec.http2.Http2ConnectionHandler.channelInactive(Http2ConnectionHandler.java:392)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)\nMay 25 15:33:52 namerd02 namerd: at com.twitter.finagle.netty4.channel.ChannelStatsHandler.channelInactive(ChannelStatsHandler.scala:113)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1329)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:908)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:744)\nMay 25 15:33:52 namerd02 namerd: at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\nMay 25 15:33:52 namerd02 namerd: at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\nMay 25 15:33:52 namerd02 namerd: at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)\nMay 25 15:33:52 namerd02 namerd: at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\nMay 25 15:33:52 namerd02 namerd: at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nMay 25 15:33:52 namerd02 namerd: at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nMay 25 15:33:52 namerd02 namerd: at com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run(BlockingTimeTrackingThreadFactory.scala:24)\nMay 25 15:33:52 namerd02 namerd: at java.lang.Thread.run(Thread.java:745)\n. Finally, me and @edio were able to reproduce the issue using following steps:\n1. Set Vars\n```\nexport BASE=$((UID + 1000))\nexport NAMERD_ADMIN_PORT=$((BASE + 1))\nexport NAMERD_HTTP_CONTROLLER_PORT=$((BASE + 2))\nexport NAMERD_GRPC_PORT=$((BASE + 3))\nexport LINKERD_ADMIN_PORT=$((BASE + 4))\nexport LINKERD_PROXY_PORT=$((BASE + 5))\nexport SERVICE_PORT=$((BASE + 6))\n```\n2. Prepare configs\nNamerd\n```\ncat > namerd.yaml << EOF\nadmin:\n  port: ${NAMERD_ADMIN_PORT}\nstorage:\n  kind: io.l5d.inMemory\n  namespaces:\n    default: |\n      /http=>/$/io.buoyant.hostportPfx/inet;\n      /inet=>/$/inet;\nnamers:\n- kind: io.l5d.rewrite\n  prefix: /unused\n  pattern: \"/{service}/api\"\n  name: \"/srv/{service}\"\ninterfaces:\n- kind: io.l5d.httpController\n  ip: 0.0.0.0\n  port: ${NAMERD_HTTP_CONTROLLER_PORT}\n- kind: io.l5d.mesh\n  ip: 0.0.0.0\n  port: ${NAMERD_GRPC_PORT}\nEOF\n```\nLinkerd\n```\ncat > linkerd.yaml << EOF\nadmin:\n  port: ${LINKERD_ADMIN_PORT}\nrouters:\n- protocol: http\n  label: egress\n  dstPrefix: /http\n  identifier:\n    - kind: io.l5d.header.token\n      header: Host\n  interpreter:\n    kind: io.l5d.mesh\n    dst: /$/inet/127.0.0.1/${NAMERD_GRPC_PORT}\n    root: /default\n    experimental: true\n  bindingCache:\n    paths: 10\n    trees: 10\n    bounds: 10\n    clients: 10\n  servers:\n  - port: ${LINKERD_PROXY_PORT}\n    ip: 0.0.0.0\nEOF\n```\n3. Start namerd, linkerd and a service\nNamerd\n./namerd-1.0.2-exec namerd.yaml\nLinkerd\n./linkerd-1.0.2-exec linkerd.yaml\nService\npython -m SimpleHTTPServer ${SERVICE_PORT}\nRun Test\nPrepare Test\n```\ncat > test.sh << EOF\n!/usr/bin/env bash\nexport http_proxy=localhost:${LINKERD_PROXY_PORT}\nSTART=1\nEND=\\${1}\nfor ((i=START; i<=END; i++))\ndo\n    echo -n \"\\${i} \"\n    echo \\$(curl -s \"http://127.0.0.\\${i}:${SERVICE_PORT}\" | grep 10.seconds)\n    sleep 1\ndone\nEOF\nchmod +x ./test.sh\n```\n4. Run Test\n$ ./test.sh 20\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16 exceeded 10.seconds to unspecified while dyn binding /http/127.0.0.16:58080. Remote Info: Not Available\n17\n18\n19\n20\n. ZOMG, it looked obscure, but I never thought it might be so deep. o_O Thanks for looking into it!. I cannot say for 100% but I have a feeling that this might be a different issue. I tried running the bash snippet from above modified in a way that it generates a lot of distinct names but all of them are short. I ran test up until I resolved >20k names and it didn't crash till then. So this might have something to do with long names.... Given migration to Netty 4 this might not be an issue anymore, let me re-run the test with v1.1.1.... Was able to reproduce the issue with 1.1.1, between 2000 and 2100 iterations.. Thanks!. Thanks for a quick reply! I think #1597 is an important update (and I believe it can be improved even more ignoring \"empty\" replies to guard against possible situations of empty KV store during disaster recovery) but I think there is more to the issues than that.\nFirst of all, pardon me, I forgot to mention that we use a custom Consul DtabStore that already has stabilization in place. I'm more concerned with namers.\nTo add to that, I think that it's safe to say that we expect that Linkerd/Consul will be properly configured and therefore anything but 2XX status codes most probably will highlight some misconfiguration. And rather than blindly retrying on such statuses it would help logging some human-friendly warnings so that operator will be able to spot the issues (misconfiguration or an issue with Consul) and resolve it more quickly.. C'mon, there is nothing you should be sorry about. ;) That's in fact my fault not giving enough details within the original issue description. And having stabilization for DtabStore is also important IMO.\nThanks for looking into the issue!. Great, thanks! I think this definitely should improve stability in face of Consul failures and give us more perspective on what exactly is happening! Looking forward to seeing this in next release. :). Well, as far as I understand, we shouldn't fall-bak only when we have a normal reply (200 OK with a proper index), everything else is safe to be considered untrustworthy.. @adleong we actually do not - we have only a custom Dtab store plugin.\nSpeaking about this PR - it's definitely interesting one and I have few thoughts on it.\nIn our experience we used to assume that Consul is rock-solid and stable and we can rely on it a lot. Nevertheless we did appreciate Consul namer's ability to \"cache\" the state from Consul so that in case of Consul outage we can not only keep processing requests to services that were requested previously but we also can send requests to services that haven't been used before. So instead of being completely down we can keep operating in some degraded mode.\nIMO this is definitely a feature and I wouldn't just throw it away. It might cause issues in large setups but when it can be afforded I think it provides a lot of value.\nSpeaking about huge number of services - this is clearly might be an issue. In our setup we use Consul namer in 2 ways:\n1) Namerd discovers services that Linkerd will use\n2) Linkerd discovers Namerd\nIn (1) we obviously want to see all the services, but in (2) we used Consul ACLs to hide all services but Namerd from Consul thus reducing the load. While such an approach is not applicable in all cases I believe ACLs might help with performance in some of them. \nOn the other hand I think we also noticed high CPU utilization with cluster growth to the degree that we might be willing to sacrifice \"caching\" for performance.\nTL;DR of all of this is that IMO it'd be better to keep both old a new namer or add an option to switch between different operational modes. Speaking of performance, I wonder if it's possible to identify what exactly takes so much of CPU and whether we can deal with that by adding some throttling when it's OK to sacrifice speed of updates.. I think we're talking about slightly different things. Previously Namerd used to \"watch all services\" so in case of Consul being unavailable it was able to resolve even those names that weren't looked up properly. As far as I understand this cache now goes away.. I see. Yeah, this way it seems that behavior remains the same.\nThanks for details! Turns out I was assuming wrong thing about Consul namer. Good to know. :). @adleong I'd love to try that out but not sure whether we will have tome for that this week. As we're not using containers to run Linkerd, just a binary would be enough. Alternatively, we can build it on our own.. @adleong @olix0r so far this issue is the most frequent one bringing Namerd down in our environment. As far as I understand, you proposed some sort of a fundamental solution that requires some work to be implemented. Unfortunately at the moment @edio don't have availability to contribute that so I wonder if you can reconsider accepting this PR as is as as simple remedy to the issue or maybe we can ask you to implement a proper fix in the next version? Thanks. Would be amazing, thanks a lot!. Wow, @klingerf, great catch! I tried to repeat your steps and was able to reproduce the issue exactly as  @hynek reported it! In my case, as well as in @hynek's case, Consul has ACLs enabled with default policy set to \"deny-all\" and ACL permissions set to\nkey \"namerd/dtabs/\" { policy = \"write\" }\nwhich may explain the difference in behavior we see.\nLet me see if I can figure out what causes it.... The cause of the issue as well as the fix turned out to be embarrassingly simple and it looks like this issues has been in place since first implementation of the store that I pushed almost 2 years ago. \ud83d\ude1e. Thanks! \ud83d\ude0d It's a pleasure to be part of such a great thing as Linkerd and work with such amazing people as you!. Decided to celebrate becoming Linkerd maintainer by fixing my own bug from 2016. \ud83d\ude42. I'm trying to get to review this PR but cannot make it out of meetings.. Hope to be done today.. In LookupCache we still have agentConfig that relies on ConsulApi's retries. I wonder if it makes sense to generalize back-offs approach used in the namer and use it for that call as well. I think I saw someone suggesting getting rid of \"InfiniteRetryFilter\" for ConsulApi and if there were to happen we would need to reconsider how we handle agentConfig.. Just realized that this change affects only Consul namer and not a Dtab store. @LukaszMarchewka can you add a TLS option to a Consul Dtab Store as well please? I think the change should be quite similar.. @LukaszMarchewka thanks for addressing TLS config for Dtab store!. Since @edio verified that it works I'm more than happy with it! \ud83c\udf89 . I wonder if we should use https://www.consul.io/api/catalog.html#list-datacenters to collect the list of all known datacenters (maybe even a cumulative one over the whole lifetime of Namerd)  and make a decision about a retry based on if that datacenter exists.. > that's an interesting idea. Do you have a sense of how that would work, exactly? My instinct is that relying on just one API call (service) is more robust than combining information from two calls (list datacenters and service) but I'm curious to hear more specifics about your idea.\n@adleong replying here - cannot find a way to reply in-place.\nFrom our experience, sometimes we observe network issues. And when there is a cross-data center polling, it's not atomic - we have a \"polling cycle\" per service name plus an overall one for the whole list of services. This in turn causes a sort of inconsistent behavior that causes a lot of confusion as some services are \"on\" and some are \"off\", some are flapping, some have latest updates and some are not.\nIf we were to use this endpoint as some sort of a flag to add some atomicity into the behavior it might help a bit.. First of all, @adleong, my biggest concern that when there is some sort of intermittent network issues between datacenters then some of the cross-DC long-polling processes might fail while others will succeed. Imagine you are in DC A and polling DC B for 10 services. Due to network issues, polling will succeed for 7 out of 10 services while will fail for remaining 3. From a user perspective, we will observe that Linkerd can handle requests for 7/10 services and it will be confusing (the \"atomicity\" I was referring to).\nAs for the concern expressed by @dadjeibaah, I think if we not going to retry when DC is not listed then we might save a lot fo bandwidth.. > IIUC, you are suggesting something like this\n@dadjeibaah yeah! And then, potentially, later down the road we even can optimize and close watches do datacenters that are not reachable at the moment.\n\nOverall, my current preference is the way this PR is currently implemented because it's simpler and doesn't involve combining information from multiple sources.\nAgree to that. Sorry for delaying the PR with this discussion.\nBut if the DC list API is considered significantly more reliable or authoritative than the service API, perhaps we can do something like what @dadjeibaah describes.\nI'll try to do a little test to see how quickly it reflects changes.. @dadjeibaah I'm super sorry but turned our I was very wrong with my assumptions about how it works. It does not update when DC goes down. It's more like \"all known DC that we presumably should be able to talk to\". Or, in other words, my ides is completely wrong. Sorry for confusing things up. :(. This sounds like an amazing feature! Although, I'm a bit concerned with the status code change. We have number of things that rely on the fact that Linkerd returns 502 when it cannot route a request so this would be a breaking change for us. It'd be great if it was configurable instead.. I personally find 502 to be a pretty good match. While I understand the reasoning behind \nInability to route is almost like a 404\n\nI think the fact that Linkerd serves as a gateway/proxy is of bigger importance here. It's not necessarily a user error - while user indeed may have requested a service that does not exist, it's also possible that it's just all instances of requested service failed this very moment. If we were able to distinguish between an unbound name and and a bound one withAddr.Neg inside then we could've used more appropriate status codes (400 and 502 respectively seem as good matches in such case, IMHO).\nLast but not least, 4XX are usually non-retryable while 5XX are. And given the above, I think it's safer to report a retryable error in such case if we're not sure why exactly we were not able to route the request.. Thank you for the amazing feature!. We had to put Linkerd behind nginx and allow only certain endpoints under admin w/o auth:\n^/admin/(metrics.*|registry\\.json|server_info)\nI believe what we really need it to separate UI and metrics endpoints from admin endpoints that manage the instance or debug it.\nAlso, I think it'd be great to have that as a simple option rather than list of whitelisted URLs. While that one is indeed flexible, it would add extra complexity to config.. I'd love to give it a try if/when there is a PoC. \ud83d\ude0d\nBTW, I haven't had a chance to look into Conduit/Linkerd2 but I wonder if this setup will support dynamic request routing via delegation table overrides?. Hm, do you think it's possible per-request dynamic routing will work ever with linkerd2-proxy & namerd? We rely on that feature heavily.. Given that Dtab.read works well with an empty string I feel like req.getParam(\"dtab\", \"\") is good enough. OTOH if you prefer that Option is used I can change the code.\n. Thanks!\n. Done. ;)\n. Done\n. You mean classes, right? Or objects as well? I may miss something but with regards to objects I see only 1 'generalization option' - add a type parameter to the base one and implement apply there...\n. Indexed.mk tries to parse data with JSON - this endpoint with given parameters returns value as is. If it's an arbitrary string JSON will fail since not everything can be a top-level object in JSON. But I like the idea...\n. You mean an alternative version of execute that doesn't require de-serializng? Ok.\n. Ooops... :)\n. Another ooops.. :)\n. Thanks! :)\n. Yeah, that's for delete. And that's why I added a get first so we can raise a DtabNamespaceDoesNotExistException as needed.\n. I like your version better - I'm just not sure about consul index boundaries. But it should fit into Long, right? :)\n. Thanks!\n. Argh, I'm angry at myself\n. I'm sorry. :(\n. I'm so sorry. :(\n. I'm super sorry. :(\n. I just copied it from etcd. :D of course will fix. :)\n. Ooops, thanks\n. Yeah, that makes sense. At first I thought about keeping it simple but I was still in doubts. I'll dig API, thanks for an advice.\n. Also though about that - will do. :)\nSo I see to options: \n1) always drop all Proxy-* headers\n2) always drop Proxy-Connection header and deal with other Proxy-* headers if we're in 'proxy mode'\nwhat would you prefer?\n. OK, will do.\n. Looks like I failed and put it in wrong place although you initially told me to put it onto the server side. %) Will fix, thanks.\n. Regarding case of field names - here I followed example from https://github.com/BuoyantIO/linkerd/blob/master/consul/src/main/scala/io/buoyant/consul/v1/CatalogApi.scala#L62 - aren't field names should match JSON field names for parser to work properly? Or it can figure out the thing? Because in Consul JSON output they're starting from upper case.\n. The same as for other APIs (I inherited it from the original implementation) - from consul/src/main/scala/io/buoyant/consul/v1/package.scala - it defines package object v1 with common stuff.\n. Thanks!\n. For now I changed it to call to DelegateApiHandler.Codec but I see #575 - once it's merged (assuming it would be merged before this one) I'd be happy to change it to updated renderAddr that makes the same thing.\n. Got the idea - thanks, will refactor it into mk.\n. Those fields must match the case of fields in JSON produced by Consul for mapper to be able to decode it. I'll add the comment explaining that. Other Consul APIs in the same package using the same approach - that's not 'my invention' but was done like that before my first PR. I'd be happy to change that to something else if you can advice me on another way of decoding JSON objects.\n. Ah, great! Thanks!\n. Your version looks definitely better - thanks! :)\n. Yeah, will do.\n. Are you ok with making it protected? The reason why I'm asking is one of the reasons for this refactoring - when we had chat with @olix0r and @wmorgan I asked about submitting a PR to make this refactoring and make it easier to extend ConsulDtabStore so we can get our own plugin that would do #555 but without the need to re-write the whole store.\n. That's neat - thanks!\n. You mean, within this file? Or across the whole linkerd codebase?\n. Woul it be OK if I'll put it into an object in consul namespace for starters?\n. Sure\n. Thanks!\n. Yeah, a bit of it. I just saw a way to get rid of variable and changed the code a bit. But can put this into a separate PR.\n. Ooops, you right - I'll drop changes these from this PR\n. It didn't compile without it. :( Is there is a better way around?\n. According to Consul API spec the Domain field cannot be empty and thus Future.exception(???) should never be executed. Would be OK to leave it as is or I should replace ??? with some concrete exception (and which one would be appropriate in such case)? \n. [error] .../linkerd/namerd/iface/interpreter-thrift/src/main/scala/io/buoyant/namerd/iface/ThriftNamerClient.scala:185: overloaded method value apply with alternatives:\n[error]   (addrs: scala.collection.immutable.Set[com.twitter.finagle.Address],metadata: com.twitter.finagle.Addr.Metadata)com.twitter.finagle.Addr.Bound <and>\n[error]   (addrs: com.twitter.finagle.Address*)com.twitter.finagle.Addr\n[error]  cannot be applied to (scala.collection.immutable.Set[com.twitter.finagle.Address.Inet], com.twitter.finagle.Addr.Metadata)\n[error]             addr() = Addr.Bound(addrs.toSet, convertMeta(boundMeta))\n[error]                           ^\n[error] one error found\n[error] (namerd-iface-interpreter-thrift/compile:compileIncremental) Compilation failed\n. I asked about that in a comment above. :)\nI see 2 options:\n1) Some concrete exception - in that case can you recommend one? :)\n2) Rather than exception fallback to consul - the default value according to Consul docs.\nWhat would you prefer?\n. Unfortunately it doesn't support long polling. Would you like a Var.async with some interval? If yes, what the interval would be ok? Something like 10.seconds?\n. OK, will do.\n. Thanks!\n. Hm, sounds more complicated than I though first. In this case, would you mind making it 'just an another Future call each time' for now. Once we will get some usage experience with this we will be able to see the performance impact and if it will be significant then we will contribute another PR with an improvement.\nWhat do you think?\n. Thanks!\n. So should I roll-back or this way it also OK?\n. Done\n. > should allow the separator to be specified as well\nIt works only with list query parameter.\n. > for multiGet, recurse should always be true, right?\nI'd say it can but not must. Do you mind leaving it optional?\nI expect usage as:\ndef getNs(path: String, index: Option[String] = None) =\n    api.get(path, blockingIndex = index, recurse = path.endsWith(\"/\"))\nso it'd be much more convenient to switch the recurse flag rather than use another method.\n. The same situation here as well - separator is accepted only for get method and only when list query parameter is used.\n. Ah, when I will learn? %) Thanks!\n. Done\n. From the source code:\n@deprecated(\"Provided for API compatibility; use raise() instead.\", \"6.0.0\")\n  def cancel() { raise(new FutureCancelledException) }\nSo although Future.cancel is deprecated the FutureCancelledException is not.\n\nWe should probably return a Failure with Interrupted set -- I think Failure is the new idiomatic way to do this\n\nCan you please explain this in ore details please? How we can influence currently running future this way?\n. Do you want me to update the way we cancel futures within this PR? Or should I just adjust this PR so retry filter will handle Interrupted failures properly and then submit another PR changing the way we canceling futures?\n. @adleong Do we want to add retry = true in ConsulDtabStore to all api calls within this PR as well?\n. Deal. ;)\n. That was an old comment from previous version of the code. :/ Will fix, thanks.\n. Ooops. :)\n. IMO it should stay as is - there is no evidence that requested service is unavailable - Consul agent may be just being restarted.\nOTOH maybe it's better to put this into retryHandler?\n. For my own reference: do you care about name overlapping in situations like this one? I mean the \"\"outer Option called domain and the inner domain value as a String. I saw such pattern number of times through linkerd codebase and used to consider this as error-prone pattern but maybe in Scala world it's not the case. :)\n. By saying loop you mean:\nprivate[this] def run(): Future[Unit] =\n  mkRequest().flatMap(update).handle(handleUnexpected)\n?\nFor me it looks like it will brake on 1st unexpected error: next cycle is supposed to be triggered within update while flatMap executes given function only when the original future succeeds. Or am I wrong?\n. Thanks for explanations!\n. Although such \"loop break\" can occur on any error that is not considered re-triable, right? For instance 403 HTTP response... My concern here is that as long as we're not caching and persisting activities we don't care much as failure-recovery would happen on its own. But if those activities are cached then it's a bit more tricky. One of the possible solutions that I see is to implement \"cache eviction\" upon unexpected failure.\n. IIRC, 403 is considered non-retryable right now...\n. Wouldn't it be easier to look for v1.NotFound here instead?\n. I'm just curious what Try(...).toOption gives us here compared to Some\n. wouldn't it be more 'idiomatically correct' to do:\ntry { ... } catch { e: Failure if e.isFlagged(Failure.Interrupted) => Future.Unit }\n? @volatile var stopped seems redundant for me. :/ Probably minor stuff.\n. Also, it's wrong to expect 404 in such situation (as per comment below) because:\n```\n$ curl -i \"http://localhost:8500/v1/catalog/services?dc=WRONG\"\nHTTP/1.1 500 Internal Server Error\nX-Consul-Index: 0\nX-Consul-Knownleader: false\nX-Consul-Lastcontact: 0\nDate: Wed, 07 Sep 2016 20:09:43 GMT\nContent-Length: 32\nContent-Type: text/plain; charset=utf-8\nrpc error: No path to datacenter\n``\n. Which actually will be considered a retryable error - a valid behavior for those cases where connectivity between datacenters is briefly interrupted.\n. Yeah, unfortunately it doesn't look like there is a way to distinguish between non-existing datacenter and temporarily disconnected one. It's not clear whether/v1/catalog/datacenters` lists only datacenters availbale right now or any \"ever seen\". I think it worth an investigation so maybe put a TODO for now?..\nMaybe we can make some sense from:\nX-Consul-Index: 0\nX-Consul-Knownleader: false\nX-Consul-Lastcontact: 0\nor from /v1/catalog/datacenters results - I'll try to check that out once I'll have some time.\n. I'd expect it to retry:\n1) in our use-case that usually means misconfigured ACLs\n2) you might see 403 when ACL datacenter is disconnected from current one\nAlthough I can imagine situation where people might use ACLs with other purpose and in that case retry on 403 doesn't make sense. Another configuration flag?.. :)\n. Cool, thanks!\n. Great, thanks! I'll update the comment with explanation (about good toString then). Also as we rely on toString to operate in some way - maybe it worth a test?..\n. If you will happen to have a better idea - please let me know, not huge fan of current situation as well. :/\n. As an alternative, we can switch to object but define an explicit toString - how does it sounds?\n. Hm, so with this we have consistencyMode: Option[(String, Option[String])] - what would be the idiomatic way of using it on the following line where we have val params = (consistencyMode +: optParams).collect { case (k, Some(v)) => (k, v) }? Add getOrElse?\n. (consistencyMode.toSeq ++ optParams) worked - it was failing to compile without toSeq :/\n. Hm, I think (consistencyMode ++ optParams).collect { case (k, Some(v)) => (k, v) }.toSeq would be better in such case...\n. OK. I'd be very grateful if you could briefly explain what it gives us or point me to some doc about that.\n. Yeah, makes sense.\n. Oooops, sorry. Will fix.\n. Do we need an extra *8 Breaking Chnage ** note? :)\n. Thanks!\n. Fixed\n. Yeah, that's great idea!\n. How do you like it now?\n. Let me take a look...\n. How would you like field called? endpoint?\n. Yeah, for backward compatibility we either need to get rid of AdminConfig class and replace it with HostAndPort or stick to current implementation.\nIMHO, it might be worth sticking to current implementation as AdminConfig might need extra config parameters later. And we always can refactor it and do what you suggested.\nPlease let me know what you think and what should I do.\n. Thanks! Will fix\n. Should I rename it to localDcMoniker or localDcAlias?\n. I used upper-case to make it stable identifier so I can use it in match but it's really not needed with the approach you proposed - thanks\n. Sure\n. I thought about that but wasn't sure where we need an extra activity just for that... But if you say that it will clarify things - then sure, will do!\n. Well, I don't mind hardcoding .local (as I don't see how that could be a valid DC name in Consul) - I though you won't like that. :) So, should I remove this option then and update the doc?\n. With default to true I assume?. We want it to be set to some particular value?. @rmars shame on me :(. http://stackoverflow.com/a/4858211. Yeah, sure!. OK, thanks.. ZOMG, I forgot there is for in Scala. Thanks!. Roger that. :). Was just passing buy and thought that maybe using YAML here as you do for all the configs would've been nice... :)\n/me goes away. I tried to illustrate that by an example in tests but probably that's to subtle.\nConsul KV always returns paths without a leading / so we never going to get a double slash here. Should I add a comment explaining that?\nAnother way to achieve same behavior would be:\nscala\ndef namespace(key: String): Option[Ns] =\n  Some(s\"/$key\")  // $key never has a leading slash\n    .filterNot(_.endsWith(\"/\")) // filter out dirs\n    .map(_.stripPrefix(s\"${root.show}/\")) // strip $root\n    .filter(namespaceIsValid). This is probably super minor but would it make sense to move lastGood out of Var.async so we can use it as a default value for, well, Var.async[Addr](lastGood). At this point I'm obviously nitpicking so feel free to ignore me, but I had to checkout the code and open an IDE to figure out the diff between index0 and index1, some sort of semantic naming would've made it way easier, at least for me.. We don't care about counting cancelations, right?.... The more I look at it the more I wonder if we need yet another var just to pass the lastGood state -  maybe it's better to pass it as a parameter in loop. Maybe I'm just too cautious about vars though.. ",
    "liamstewart": "will do!\nOn Fri, Feb 19, 2016 at 5:48 PM, Oliver V. Gould notifications@github.com\nwrote:\n\nthis looks great, @liamstewart https://github.com/liamstewart. thanks\nfor sharing. once you've updated config.md, would you mind rebasing this\ninto one commit with a commit message that describes the motivations?\nNothing fancy, just a sentence or two saying that you're adding support for\nthrift's compat protocol.\n\u00a1muchos gracias!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/BuoyantIO/linkerd/pull/115#issuecomment-186456937.\n\n\nLiam Stewart :: liam.stewart@gmail.com\n. ptal\n. ",
    "hawkw": "WDYT an appropriate min/max time range to jitter in would be?. @adleong I borrowed the values from PR #772, which I hadn't seen at the time of that comment. I've already opened a PR #1448 with my changes but can change the default interval to 500ms (it's currently 50ms).. I'm working on a tool to automate this, and so far, I've found at least one case we're not handling correctly.\nAccording to the spec:\n\nIf a message is received without Transfer-Encoding and with\neither multiple Content-Length header fields having differing\nfield-values or a single Content-Length header field having an\ninvalid value, then the message framing is invalid and MUST be\ntreated as an error to prevent request or response smuggling. \n...\nIf this is a response message received by a proxy, the proxy MUST \ndiscard the received response, send a 502 (Bad Gateway) status \ncode as its downstream response, and then close the connection.\n\nWe are, as you might have guessed, Not Doing This.\nTesting with flossy, where the upstream server sends first bad response message in the blog post @olix0r quoted above (with multiple Content-Length headers), linkerd happily forwards the bad response, multiple headers intact. \n(N.B. for potential users of flossy \u2013 it's currently a minimum working proof-of-concept. you probably don't want to use this yet). I did a manual telnet session to confirm (I don't entirely trust flossy yet; there's probably several lurking bugs in there):\n```\nGET http://127.0.0.1:7777/test1 HTTP/1.1\nHTTP/1.1 200 OK\nServer: Example\nContent-Length: 31\nDate: Mon, 03 Jul 2017 15:15:07\nContent-Length: 45\nContent-Length: 20\nl5d-success-class: 1.0\nVia: 1.1 linkerd\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n``\nSo yeah. This is real.. Additional testing with the [latestflossy](BuoyantIO/flossy@94135561dcc71b834f5c4aeffcdde7e641a6f786) reveals that we're also mishandling the case of a request with multipleContent-Lengthheaders \u2013 I'll open an issue for that too.. Still need to add similar tests inflossy` for chunked encodings (BuoyantIO/flossy#2).. Issue #1776 now tracks adding access logging for HTTP/2.. Hi @sfroment, just wanted to let you know I have a PR open that adds this feature (#1502). Hopefully we can get it merged soon! :). Aww, thanks @sfroment, we appreciate it! \u2764\ufe0f \nI'm pretty sure we'll get this merged by the next release. We're right at the beginning of a release cycle right now, though, so it'll probably be a couple of weeks out.\nThanks for your support!. Merged in c5a3b42f7d8fac76d8b8928ff9a369e803fe7be7! @sfroment, when this is released, you'll be able to add TLS to the admin page by adding \nyaml\ntls:\n   certPath: path/to/cert.pem\n   keyPath: path/to/key.pem \nunder admin: in the configuration file. \nThere's a sample configuration in linkerd/examples/admin-tls.yaml. Any options you can add to TLS server configurations elsewhere in linkerd are also valid here. Also, note that you can now add TLS to namerd's admin dashboard as well!\nLet me know if you have any questions or issues!. Great, thanks @klingerf! I'll let you know if I have any trouble.. Implemented in #1603! \ud83c\udf89. @urtrcc this issue is on my to-do list; I'll be starting on it shortly. Sorry to keep you waiting!. I've done some research, and it looks like this is technically a bug in the gRPC client implementation(s) that cause this behavior.\nThis error is occurring when the client closes a connection but fails to send a GOAWAY frame. According to the HTTP/2 spec:\n\nEndpoints SHOULD always send a GOAWAY frame before closing a connection so that the remote peer can know whether a stream has been partially processed or not. For example, if an HTTP client sends a POST at the same time that a server closes a connection, the client cannot know if the server started to process that POST request if the server does not send a GOAWAY frame to indicate what streams it might have acted on.\n\nSince the clients are in error (because they don't comply to the spec), I think it's reasonable for linkerd to complain about this? I can see about changing the log message to something more informative than a stack trace, though. . Hi @urtrcc, @olgert, and @ale-batt, just wanted to let you all know that I've pushed up a fix for this issue (#1498), hopefully we can get it merged shortly!. Aaaand merged (99d06576c65de6e2e9db3d5987d2a4cbd2a14870). You should see this in the next release. :). @urtrcc and @ale-batt, we cut release v1.1.2 right before I merged these changes in, so I'm afraid I can't give you an exact time-frame. It'll probably be within two weeks or so, though!. Trying to test PR #1496, which re-enables this commented-out code, it's looking like it may not actually be necessary:\nLinkerd on the current master appears to be already sending GO_AWAY frames, at least in some cases. I'm running linkerd-examples/h2 proxying a connection between nghttp and nghttpd. \nRunning nghttp http://localhost:4142/ -v (when nghttpd is serving on port 8888) outputs the following logs:\n[ERROR] Could not connect to the address ::1\nTrying next address 127.0.0.1\n[  0.002] Connected\n[  0.002] send SETTINGS frame <length=12, flags=0x00, stream_id=0>\n          (niv=2)\n          [SETTINGS_MAX_CONCURRENT_STREAMS(0x03):100]\n          [SETTINGS_INITIAL_WINDOW_SIZE(0x04):65535]\n[  0.002] send PRIORITY frame <length=5, flags=0x00, stream_id=3>\n          (dep_stream_id=0, weight=201, exclusive=0)\n[  0.002] send PRIORITY frame <length=5, flags=0x00, stream_id=5>\n          (dep_stream_id=0, weight=101, exclusive=0)\n[  0.002] send PRIORITY frame <length=5, flags=0x00, stream_id=7>\n          (dep_stream_id=0, weight=1, exclusive=0)\n[  0.002] send PRIORITY frame <length=5, flags=0x00, stream_id=9>\n          (dep_stream_id=7, weight=1, exclusive=0)\n[  0.002] send PRIORITY frame <length=5, flags=0x00, stream_id=11>\n          (dep_stream_id=3, weight=1, exclusive=0)\n[  0.002] send HEADERS frame <length=38, flags=0x25, stream_id=13>\n          ; END_STREAM | END_HEADERS | PRIORITY\n          (padlen=0, dep_stream_id=11, weight=16, exclusive=0)\n          ; Open new stream\n          :method: GET\n          :path: /\n          :scheme: http\n          :authority: localhost:4142\n          accept: */*\n          accept-encoding: gzip, deflate\n          user-agent: nghttp2/1.24.0\n[  0.012] recv SETTINGS frame <length=12, flags=0x00, stream_id=0>\n          (niv=2)\n          [SETTINGS_MAX_CONCURRENT_STREAMS(0x03):300]\n          [SETTINGS_INITIAL_WINDOW_SIZE(0x04):1048576]\n[  0.012] recv SETTINGS frame <length=0, flags=0x01, stream_id=0>\n          ; ACK\n          (niv=0)\n[  0.012] send SETTINGS frame <length=0, flags=0x01, stream_id=0>\n          ; ACK\n          (niv=0)\n[  0.020] recv RST_STREAM frame <length=4, flags=0x00, stream_id=13>\n          (error_code=CANCEL(0x08))\n[  0.020] send GOAWAY frame <length=8, flags=0x00, stream_id=0>\n          (last_stream_id=0, error_code=NO_ERROR(0x00), opaque_data(0)=[])\nwhile the tail end of the linkerd log, with the log level set to TRACE so Netty logs inbound and outbound frames, says:\nTRACE 0807 16:04:36.371 CDT finagle/netty4-2: \n----------------OUTBOUND--------------------\n[id: 0xfceee452, L:/127.0.0.1:4142 - R:/127.0.0.1:61248] RST_STREAM: streamId=13, errorCode=8\n------------------------------------\nTRACE 0807 16:04:36.380 CDT finagle/netty4-2: \n----------------INBOUND--------------------\n[id: 0xfceee452, L:/127.0.0.1:4142 - R:/127.0.0.1:61248] GO_AWAY: lastStreamId=0, errorCode=0, length=0, bytes=\n------------------------------------\nTRACE 0807 16:04:36.383 CDT finagle/netty4-2: \n----------------OUTBOUND--------------------\n[id: 0xfceee452, L:/127.0.0.1:4142 - R:/127.0.0.1:61248] GO_AWAY: lastStreamId=13, errorCode=0, length=0, bytes=\n------------------------------------\n@zackangelo, do you think this issue is still valid?. @zackangelo I'm not sure, haven't really looked into it yet; I suspect it could be somewhere in Netty?. I'm removing this from the 1.2.0 milestone until we hear back on https://github.com/linkerd/linkerd/issues/1296#issuecomment-321355706.. \ud83d\ude4c thanks for looking into that @zackangelo !. Moved from 1.2.0 milestone to 1.2.1 so we can have additional time for beta testing this feature without holding back 1.2.0. Hi @rmichela, just wanted to let you know that this issue was fixed in 797716b73acee0b5ab647ad9976a0627aaf5354a \u2013 we now return an empty response for HEAD requests. It'll be in the next release. Thanks for reporting!. Hey @gitcarter and @dario-simonetti, just wanted to let you know that we just merged a fix for this issue in commit d879758ee8865bd78703342affd64c935a546299. It should be in the next release. \nThanks for reporting, and let us know if you have any other issues!. Okay, an update: as of 7f8c027ef7f07e6c67cb97be8de5e4e778e157c9, I've added a great deal of unit tests for the parsing of failure accrual configs, and I can say with some certainty that this bug is not in config parsing.. Hi @hammerOwen, just wanted to let you know that I added a fix for this issue in 038c223bf55b854d18c5771555fae690b1ea9290. It should be in the next release. Thanks for reporting!. This was a fun one \u2013 finding the issue was a bit trickier than I expected, and then the fix was deceptively simple :). I did some additional poking around this morning, modifying existing TLS integration tests to use TrustCredentials.Unspecified, and it appears that this setting really just does not work inside of ScalaTest. Very curious about why.. While I'd eventually like to look at fixing this behavior downstack from us in Finagle, I think for now the Right Thing to Do is just to add a filter that turns these requests into 502 errors. @olix0r, does that sound good to you?. Closed by #1475. I've attempted to reproduce this issue by sending HTTP/2 requests to a HTTP/1.1 Linkerd router. I'm using nghttp to send H2 requests, and when I point nghttp at Linkerd, I don't get this exception - instead, Linkerd throws an exception because it doesn't recognize the PRIORITY frames that nghttp sends.. However, it looks like this exception can be reproduced by sending HTTP/1.1 requests to a HTTP/2 router; here's what @adleong got when he curled a Linkerd h2 router:\nI 0831 21:34:14.309 UTC THREAD80: serving h2 on localhost/127.0.0.1:4142\nI 0831 21:34:14.370 UTC THREAD80: initialized\nE 0831 21:34:41.961 UTC THREAD95: [S L:/127.0.0.1:4142 R:/127.0.0.1:55072] dispatcher failed\njava.lang.ClassCastException: Transport.cast failed. Expected type io.netty.handler.codec.http2.Http2Frame but found io.netty.handler.codec.http.DefaultHttpRequest. Closed by #1489. @adleong I thought I'd handle badly-framed responses in a separate PR, since they have a different issue (#1468). Does that seem appropriate to you?. @adleong I'm not sure why there's an ErrorResponder in both stacks, that was there when I got here :)\nI think even if we continue having two filters, they could still throw different subclasses of MalframedWhateverException and encode the response status that way, rather than based on the error responder's position in that stack \u2013 I'd be happy to make that change. @olix0r, does that seem reasonable?\nRe: having a single filter, it would definitely be simpler that way; IIRC Oliver suggested I do it this way. I'm open to suggestions.. I'll probably add this functionality to the RequestFramingFilter I added in 312309f86d0912e4976070d42cc4cb7d37f29b83.. Well, I have a suspect:\n\n\n\n. And I'm pretty sure that the Vars in question live in here somewhere. @eduponte, do you have the ability to get me a metrics dump corresponding to the heap dump you posted? That could really help with tracing this issue.. @eduponte Okay, that's fine, I'll just keep digging.. @eduponte, after continuing to talk it over, it looks like we're not really going to be able to proceed on this without either a metrics dump or steps to reproduce this issue. \nThe metrics dump doesn't need to come from the same linkerd process as the one that produced the heap dump you gave us earlier, but it should come from a process that died due to this issue. Is that possible?. @eduponte that's fine, take your time! We just probably won't be making much progress on this issue without more information; I've determined more or less how the OOM happens, but in order to fix it, we also need to figure out why \u2013 what specific linkerd/Consul interaction(s) are causing out of control Var proliferation.. @eduponte, that's great, thanks. I can't promise that'll be enough info to resolve this issue, but I'll keep digging. If you can get me metrics at some point, though, that would still be highly appreciated.. @eduponte I'm not sure yet, but I think it seems likely. Even if there's more going on here, though, I suppose fixing the JsonStreamParser leak in #1579 might improve the memory footprint enough that issues with Consul no longer causes OOMs.\nI'd be interested to see if this issue persists after #1579 lands \u2013 if it does, we can certainly keep looking into it.. Hi @eduponte, are you still experiencing OOM errors with Consul? I'm currently working on improving some performance issues with Consul, and if you're still having this issue, would you be interested in testing out a snapshot build of Namerd?. Hi @eduponte, what version are you referring to? The snapshot build I mentioned above is not yet officially released.. @eduponte ah, okay; there is now a snapshot 1.2.1-CONSUL-FIX-SNAPSHOT that fixes another Consul-related performance issue. If you'd like to try this snapshot, I'd be interested to know if it fixes your issue as well.. I'm closing this due to inactivity. @eduponte, if you consider seeing this issue on the latest Namerd release, please re-open.. This PR extends on and obsoletes my existing PR #1478, so if we merge this, we should close that PR. Is this the correct convention for obsoleting a PR, and should I add the summary of changes in that PR to this one, so that it shows up in the commit history?. @adleong I manually verified the retries behavior locally, and it looks like if 68701cda69cbb44f7aeec56c4cd4bf1e2a0e9277 builds successfully, this should be done! \ud83c\udf8a . Your PR summary doesn't have a section for validation :). I haven't added any new tests for the GO_AWAY frame, but I can, if anyone thinks I ought to \u2013 might need some advice on what behavior to expect?. @adleong okay, sounds good. I'll get on that, then!. @adleong, sorry, I've been so busy with response classification that I completely forgot about this \u2013 I'll look into it, thanks for the reminder.. @adleong nah, I just told Safari to trust the cert even though it was unknown. I figured as long as it was offering the right cert, it's probably working right, since the TlsClientConfig is used all over linkerd, and we have a lot of tests for it. \nI think curl has an option where you can specify a path to a CA cert, so I probably ought to test this using that, as well... although I think it would only work if I generated a new demo cert with the correct hostname for the admin page. \nAs it is, I can at least go into Safari and see that it's advertising the demo cert as expected:\n\n. I've been rerunning this test against master over and over on my laptop and I don't think it's passed a single time, so far?. @esbie issue renamed :). This seems like it's probably the same issue as/linked to #1225?. Regarding the question \"do we want to log stack traces for recovered exceptions at DEBUG in general\", I think that's a bit above my pay grade. \ud83d\ude0a. I think I may need to add some additional tests for the ConfigMapInterpreter before this is ready to merge; if so, I'll probably save that for tomorrow/Monday.. I forgot to mention yesterday but this should be ready for review as of 50f515368b971bcf51a97b00b51ab7667eae638c.. @klingerf & @adleong, I think all requested changes have been addressed? Sorry this one took so long.. @klingerf, should we close this in favor of #1532 to merge the same changes into kube-refactor?. It occurs to me that with the addition of an onFrame function, we also have the ability to collect some pretty granular metrics, such as, say, the number of frames per request, . Is this data we're interested in collecting @adleong?. Consider it done! :D. Closed b81efb74a48aecda7a064af20aec1362a6e7b6f7. Build 5327 failed, but it looks spurious (the failed test should be nowhere near anything I've touched...), so I'm restarting it.. Btw @adleong, I left the StreamStatsFilter in io.buoyant.router.h2 because that's where the original code was \u2013 should it go there, or in com.twitter.finagle.buoyant.h2?. @adleong I've addressed all your feedback in a5a0a413a768e3aea7b7e3e5e7f0d350f895e501 \ud83d\ude0a. @adleong what's a reasonable token to parse the port with? alphanumerics-only?. @adleong & @olix0r, I've made a bunch of additional changes to this branch \u2013 I think I've addressed everything you've requested. It should be ready to be re-reviewed?\n(I've also updated the PR summary to reflect changes in the metrics collected, as Oliver requested). @adleong, I did a bunch more work on this branch this morning; H2 ResponseClassifiers have now been brought much more in line with how protocol-specific ResponseClassifiers look elsewhere in Finagle. It should be ready for another round of review \u2013 also, do we want to talk about trying to integrate this with your work on retries in #1523 ?. @olix0r I'm on board with the renaming suggestion \u2013 it also means we'd be able to refer to H2StreamClassifiers and ResponseClassifiers in the same file w/o renaming imports, which I'm in favor of. WDYT @adleong?. Agh. Grammar backwards compatibility. I'll fix that!. @adleong, shall I go ahead & merge this?. 'course, I think we're also waiting on #1534 before we merge this back to master.. This is great, thanks for including the links to background \u2013 I'm not terribly familiar with gRPC, but this should give me enough to get started!. @adleong, 139cd2a413a29a97a20844fb7778462e7f16a7bd adds a framework for this, although the lists of retryable and retryable-if-idempotent status codes are placeholders. WDYT?. Oh, I've also added a time interval stack param to StreamStatsFilter, since the regular StatsFilter has one. Functionality should remain unchanged by default.. @adleong, I think 438780e36fcb22e33dceb7555e83b40d7fa7c434 addresses all your comments?. (trying to figure out why this broke on CI, it builds in dev. retrying the build & if that fails, I'll do some additional digging). Okay, this is real weird: the test appears to be failing when run from SBT, but not when run from IDEA (which is why I pushed this).... Okay, finally fixed the failing test \u2013 it turns out the tests were expecting exceptions to be in the io.buoyant.router package and not io.buoyant.router.h2, which I totally overlooked. Now I feel silly.. @adleong does this seem right, now?. Okay, for the time being, I think I can safely say this is blocked on #1545.. @adleong thanks for answering all of my questions; I'm totally open to merging this whenever you're ready!. \ud83c\udf89 . I think before trying to find a fix for this, we'll want to ensure that the issue persists after #1603 is merged. That branch rewrites enough of the Kubernetes namers that it's possible this issue was fixed as well.. I think before this can be merged, I also should add it to the docs, but I'm opening this PR so it can be reviewed.. @adleong  try it now?. @adleong 5586 certainly looks like a spurious failure to me, restarted it and we'll see.... N.B. that the Build trait is also deprecated, so we will also want to migrate to build.sbt before sbt 1.0 is released; I thought that ought to be a separate PR.. @olix0r yup, everything appears to be nominal!. Note that the documentation does state that although using the Build trait for the build definition is deprecated,\n\nyou can still use project/*.scala file to organize your build and/or define ad-hoc plugins\n\nSo this won't require a complete restructuring of our build definition. We will just need to rewrite LinkerdBuild.scala as build.sbt.. Since sbt 1.0.0 was released 12 days ago, I'd really like to get this done before we release Linkerd 1.2.0.. Oh, wow, it looks like most of these links were added by me, sorry! \nI vaguely remember there being a fix for this besides removing links, I'll see if I can find any information on that.. Okay, now that  #1589 is merged, what else is necessary to consider this issue finished for 1.2.0?. ### Notes to reproduce:\nThe VM instance in which I reproduced the bug is still available in the buoyant-hosted GCE project, named linux-test; the Linkerd configs and SSL certs I used for the repro live in /home/eliza/test on that machine.\nI launched Linkerd with\n shell\n$ docker run --net=\"host\" -v /home/eliza/test:/certs buoyantio/linkerd:latest /certs/linkerd.yml\nand nghttpd with\nshell\n$ nghttpd 8888 linkerd-tls-e2e-key.pem linkerd-tls-e2e-cert.pem -v\nand then hit Linkerd with a request from nghttp:\nshell\n$ nghttp https://localhost:4240/ --cert=linkerd-tls-e2e-cert.pem --key=linkerd-tls-e2e-key.pem -v. Ugh. Thanks for looking into that, @siggy.. @adleong Confirmed, removing disableValidation: true fixes this issue on the test GCE instance I set up to reproduce this. I'll add that to the documentation.. (n.b. that CI builds on this branch are breaking because I messed with the git history too much, and not for any real reason). @adleong try it now?. @adleong, I think I've addressed all your feedback as of 9928679?. @adleong, this was fixed in #1598, correct?. I've opened a PR #1597 that stabilizes the Consul observation activities in ConsulDtabStore. Hopefully that should solve this issue.\nIf we want to start retrying 403s, I think that will require additional changes, but this should fix the issue for now?. > First of all, pardon me, I forgot to mention that we use a custom Consul DtabStore that already has stabilization in place. I'm more concerned with namers.\nYeah, @adleong cleared that up for me here; sorry about that, that was a misunderstanding on my end \u2013 I was afraid such a simple fix was probably too good to be true.\nI'll start looking at making these changes to Consul namers.. Hi again @ashald, I've added some modifications to PR #1597, hopefully it now makes the changes you originally requested?. Okay, that's good to hear! \ud83d\ude00 Please let me know if there's anything that still needs additional changes; wasn't entirely sure what cases should not fall back but I think I got it right?. Yeah, what I meant was that in the current implementation, there are a few error cases, such as when Consul didn't return an index,\nhttps://github.com/linkerd/linkerd/blob/e1002c6f1b11ca17d86dc24b1347aa685b1547e2/namer/consul/src/main/scala/io/buoyant/namer/consul/SvcAddr.scala#L82-L87\nwhere we don't fall back to the last known-good state and still return Addr.Failed. I thought it seemed reasonable to only fall back from errors where it seems plausible that we could eventually resume observation safely?. Hi @Ashald, just wanted to let you know I've merged https://github.com/linkerd/linkerd/commit/ef38c6ad20071a57294bae863923aae99572b5dd, it should be in Linkerd 1.2.0, which we're hoping to release by the 31st!. A quick update: as of 0b333dd6382954e6915b86318b862e4ab57c5de4, I've got this half-done: the ReaderDiscarded exception is ignored. The IOException is still being thrown \u2013 trying to figure out where that exception needs to be caught in order to only ignore it when it's being thrown spuriously.. @adleong oh, whoops, I can look into that. Sorry, my bad.. Okay @adleong and @ashald, I've updated this PR; hopefully it now makes the correct changes.. Okay, have tested this against https://github.com/linkerd/linkerd-examples/tree/master/consul and nothing appears to be obviously broken; going to go ahead and merge. This is great, thanks @klingerf \u2013 I'll look into this in a moment?. @klingerf just to confirm, do we know for sure that this leak did not exist before 9deb5f7?. @adleong, I've updated your PR summary to note that this fixes #1613 (I also fixed a typo \ud83d\ude1b). Hi @prdoyle, I strongly suspect PR #1609 will fix this issue as well, since it adds handling for the exception that's being thrown here. \nWhen we've tried to reproduce this issue, we don't see the 502 responses you're describing. Since I don't have your HTTP service to test against, can I get you to confirm whether that PR fixes this issue?\n+ Check out the eliza/fix-spurious-exception branch \n+ Build a Linkerd docker image by running ./sbt docker from the root dir of the Linkerd repo. The image created will be tagged 1.1.3-SNAPSHOT.\n+ Change your docker-compose.yml to use the buoyantio:linkerd:1.1.3-SNAPSHOT image for Linkerd \n+ Run docker-compose up and check whether this issue persists.. Quick update @prdoyle: we've recently cut a release candidate build of 1.2.0 for canarying some Kubernetes changes, and you should be able to test using that rather than building a new docker image. Just change yourdocker-compose.yml to use the buoyantio:linkerd:1.2.0-rc1 image, & let me know if this problem is still occurring?. @prdoyle okay, thanks \u2013 I'll continue looking into this issue, then. Are you still getting 502 responses?. Thanks @prdoyle, that will make diagnosing this issue a lot easier! I haven't yet been able to reproduce this failure, and I suspect the bug is only revealed by the particular behavior of your service.. @d10i hm, that's interesting \u2013 off the top of my head, I can't think of any changes in 1.3.2 that would fix this issue, but we can take a closer look. @prdoyle, can you confirm whether or not you're seeing this error on 1.3.2?. @d10i that certainly makes sense; when I was looking into this issue a couple weeks ago, it seemed like it was happening somewhere in Finagle.  I didn't realise this was fixed on 1.3.0 as well, but I suspect you're right that the Finagle 7 upgrade fixed this.\n@prdoyle I'm going to go ahead and close this issue; if you still see this error on a version >1.3.0, please feel free to reopen.. Hi @cponomaryov, just wanted to give you a quick update on this: we'd love to get this merged in as part of the 1.2.0 version that we're planning on releasing by the end of the month. I've asked @klingerf to deploy this branch to our test cluster to sanity-check that there aren't any subtle issues introduced by the upgrade to Finagle 7. Hopefully, everything will check out & we can get this merged to master.\nThanks again for your contribution!. Hi @amitsaha, @adleong is currently out of the office. I'm running point on the 1.2.0 release & would be happy to go over your PR if you like. When it's ready for review, we might want to loop @esbie in as well.. And #1608 is merged! Thanks to @blacktoe for all the work on this!. @amitsaha yes, that's correct! We're looking at cutting 1.2.0 by September 14th, if not before.. Since this is a pretty big set of changes, it'll need some serious scrutiny before we can merge this into master.\nEach individual feature contained in this branch has already been reviewed by @adleong and @klingerf, and tested on the Kubernetes test cluster by @klingerf.\nPer Alex's recommendation, before we merge branch, we should:\n\n[x] code review all the changes in this PR\n[x] run it for 24h in the test environment (at a minimum)\n[x] see if we can come up with a test that exercises rapidly-changing services?\n[x] see if we can get one or more of our users to test an RC1?. lgtm!. Hi @zackangelo, @maruina, @coleca, and @jamessharp, just wanted to give you all a quick update. We've found the probable cause of this issue (issue #1697) and are putting together a fix. It should be in the next Linkerd release, which we're expecting to get out by the end of the week.\n\nThanks to @jamessharp for the repro script, that was very useful!. I'd like to hear back from @adleong to confirm, but I think this is now ready to merge!. I strongly suspect that this patch also fixes #1599; I've asked @prdoyle to confirm.. Hi @ccmtaylor, this is great, thank you for your contribution!\nI'll find some reviewers for this PR; in the mean time, would you mind making sure you've signed our Contributor License Agreement so we can merge this?. @ccmtaylor:\n\nSoundCloud recently signed a CLA. I used my SC email in the commits, but if you prefer, I can close this PR and re-submit from a fork under github.com/soundcloud.\n\nOh, okay; I wasn't aware of that and \"please sign the CLA\" is just in my boilerplate response for all first-time contributors. You're fine, then!. @ccmtaylor we don't currently collect any metrics from namers, although we probably should. . Hi @ccmtaylor, thanks for those last commits yesterday! I'm going to go ahead and merge this, and it'll show up in Linkerd 1.2.0 (which we're expecting to release by September 14th, if not before). Thanks again for your contribution!. Thanks for the fix, @ccmtaylor!. Quick update: I'm about 80% certain I know what's going on. It looks like the issue is related to BufferedStream/ClassifiedRetryFilter interaction. ClassifiedRetryFilter.apply() has a lot of calls to BufferedStream.fork(), and it seems that as a long stream remains open, the ClassifiedRetryFilter repeatedly forks the stream, and the list of _forks (a List[AsyncQueue[Frame]]) on the BufferedStream grows and grows and grows. Note that forks are added to the list but never removed.\nMy guess is that we can probably find a fix by discarding old forks that are no longer in use?. Another update: @adleong and I have been debugging this for the last 3 days; Alex found the leak yesterday:\n\nit\u2019s a bug with BufferedStream::fork::read and pull\nif 2 forks read, that will cause 2 pulls which will create 2 pollers\nwhen a frame arrives, it will be fanned out to both forks and the fork::read will be satisfied\nbut 1 poller will remain on the underlying q\nthen, presumably, both forks will call read again\nand now you have 3 pollers on the underlying q\n\nI have a preliminary version of a fix: 6ce546c77cff03585ea64c99e54a4c34e2594352. I've spent some time with the GC logs in the dumps @leozc gave us, and I can say with some certainty that it's not GC-related.. Closing this as I've talked it over with @leozc a bit and it appears that the issue is caused by interaction with a custom plugin \u2013 none of the data we have indicates a bug in Linkerd.. @pcalcado it looks like 3a843eb has broken the examples/istio.yml test on CI, so I think your changes to io.buoyant.namer.NamerInitializer have prevented the Istio namer from being loaded.\nI also think it seems odd that the (non-Istio) K8s namer & interpreter initializers are now being provided by the istio module? I'd like to see all of the non-Istio-specific k8s services provided by the k8s module, so that it could (theoretically) be used without the istio module.. \ud83c\udf89 . ### Alternative Solutions\n+ Ideally, we'd like to actually perform a protocol upgrade negotiation (see #841), but that isn't supported yet.\n+ I suggested sending  the H1 client either a 429 (\"upgrade required\") error or a 400 error with a message stating that the H2 router can't handle H1 requests at this time, but @adleong said he'd prefer not to see H2 routers returning any H1 responses.... I will ~Would like to~ add validation for this as well.. Update: as of https://github.com/linkerd/linkerd/pull/1621/commits/8f57a53d8381bd270aee246265c419438caea0cb, TlsClientConfig now validates configs \u2013 it now requires that if disableValidation is set to true, clientAuth must not be present. I've also added tests for this validation (https://github.com/linkerd/linkerd/pull/1621/commits/086ba9d4a9375016acd86ba1c3f2fc2c03d31ae8).. Hi @wjwinner,\nI've attempted to reproduce this issue in Minikube with the following steps:\n+ created a service world-v1 with kubectl apply -f linkerd-examples/master/k8s-daemonset/k8s/hello-world-legacy.yml\n$ kubectl get endpoints world-v1                                                                                                                                                             \nNAME       ENDPOINTS                                         AGE\nworld-v1   172.17.0.2:7778,172.17.0.3:7778,172.17.0.5:7778   1m\n+ deleted service kubectl delete service world-v1: couldn't reach it as expected\nhttp_proxy=$L5D_INGRESS_LB curl -s http://world\ninvalid response 502\nNo hosts are available for /svc/world, Dtab.base=[/srv=>/#/io.l5d.k8s/default/http;/host=>/srv;/svc=>/host;/host/world=>/srv/world-v1], Dtab.local=[]. Remote Info: Not Available%\n+ re-created service kubectl apply -f linkerd-examples/master/k8s-daemonset/k8s/hello-world-legacy.yml\n+ kubectl get endpoints world-v1 gave me different endpoints as expected:\nkubectl get endpoints world-v1                                                                                                                                                             \ue0b2 \uf00c \nNAME       ENDPOINTS                                         AGE\nworld-v1   172.17.0.2:7778,172.17.0.3:7778,172.17.0.5:7778   6s\n+ was able to curl service:\nhttp_proxy=$L5D_INGRESS_LB curl -s http://world\nHello (172.17.0.22) world (172.17.0.3)!!world (172.17.0.3)!%\nAs you can see, I wasn't able to reproduce the issue.\nWhat Kubernetes provider are you using? It's possible that I can't recreate this issue in Minikube, but it's occurring due to something specific to whichever provider you're using.. Okay, thanks for that information @weitzj!. @wjwinner ah, you're right, I must have missed that. I'll see if I can reproduce this with Calico. . Okay, thanks @weitzj, @wjwinner, and @ihac for the additional information \u2013 it looks like this issue only occurs with Calico/Flannel. . Hey folks, can I get you to try the buoyantio/linkerd:1.2.0-SNAPSHOT and buoyantio/namerd:1.2.0-SNAPSHOT images I just pushed and let me know if the problem persists?. thanks, that's too bad. we'll keep trying to fix this. . Hi @wjwinner, @weitzj, and @ihac, just wanted to let you all know I have a pull request #1638 that fixes this issue. We'll try to get a hotfix out for this ASAP.. Excellent \u2013 we had confirmed that internally, but it's nice to hear. We'll get an official release out soon!. Hi @Taik, after seeing these warnings, can you still route to the correct endpoints for your services?\nWatches on Kubernetes API objects should be restarted automatically when Linkerd requests a watch with a resource version that's too old. This warning is usually benign unless you're having other issues, in which case it might help in determining what else is going on.. Hi @Taik, I just wanted to give you a quick update on this. First of all, we've officially released Linkerd version 1.2.1, so you'll want to upgrade your images from 1.2.0.fix4-SNAPSHOT to linkerd:1.2.1. \nSecondly, regarding the \"too old resource version\" warnings, we've seen those warnings as well in our tests, and we're continuing to look into it. However, we've determined pretty conclusively that they're not related to this issue, as we're also seeing them on Linkerd 1.1.2 and 1.1.3. We suspect this may be related to a recent Kubernetes change, as we hadn't seen this happening in our test deployments of 1.1.2/1.1.3 previously.\nAre you also seeing ChannelClosedExceptions in the logs? If so, that would likely indicate that some requests aren't going through, and I'd recommend you open a new issue. Otherwise, the \"watch error\" warning is probably benign.. Hi @weitzj, by any chance, have you seen \"too old resource version\" error messages in your logs? If so, it's possible you're experiencing a different issue (#1636), which we've recently fixed. Otherwise, I'll be happy to keep looking into this for you.. @adleong done 4deb53a. it keeps happening!. It seems possible to me that this issue is related to/shares the same root cause as #1626 .... Great, thanks @Taik! Please let me know if you can reproduce the issue discussed in #1626 as well.. @jsurdilla yeah, that is (unfortunately!) not too surprising...#1626 also still exists after that snapshot.\nJust to double-check, though, are you using buoyantio/linkerd:1.2.0-SNAPSHOT or buoyantio/linkerd:nightly? The 1.2.0-SNAPSHOT tag contains patches that aren't on master (which the nightly tag is built from).. @jsurdilla okay, thanks for confirming, I've done the same, and determined that the SNAPSHOT image doesn't fix the problem. We're working on trying to diagnose what's going on here, stay tuned!. Hi @jsurdilla and @Taik, pull request #1638 should fix this issue. We'll try and get the fix out as soon as possible!. Hi @mejran and @kumudt, this looks like the same issue as #1626 and #1635. We're trying to find the cause of this problem, and hopefully we'll have a fix soon.\nThanks for reporting!. Hi again @kumudt and @mejran, just wanted to let you know that I've opened pull request #1638, which should fix this issue (and #1626 and #1635). We'll try and get the fix out to you as soon as possible!. Hi @kumudt, in the meantime, you can try using the Docker image buoyantio/linkerd:1.2.0.fix4-SNAPSHOT, but please note this is not a production ready release yet.\nAlso, please note that there isn't (yet) a Namerd tag with these fixes, and this bug effects both Linkerd and Namerd. So, because you're using Namerd, you may still see incorrect behavior after deploying the snapshot Linkerd tag.\nWe'll try to get a production ready release for this fix out as soon as possible!. Hi @kumudt, that's disappointing to hear. Your deployments must be effected by another issue besides #1626. We'll keep looking into this.. Could you possibly post the configuration file for Namerd as well?. Thanks!. Hi again @kumudt, I'm working on a reproduction for this issue, can I get you to give me an example of what steps you're using to deploy your application? I'm guessing it's kubectl deploy, but it would be nice to know exactly what you're doing. Thanks!. @kumudt, you've said you first observed this issue upon upgrading to Linkerd 1.2.0. Can you tell me what Linkerd version you were upgrading from?. Are you still seeing this issue on 1.1.3?. Hi @kumudt, I haven't been able to reproduce this issue with Linkerd 1.2.1 and Namerd 1.2.1. Here's my Linkerd configuration:\n```yaml\n    routers:\n    - protocol: http\n      label: outgoing\n      identifier:\n        kind: io.l5d.header.token\n        header: Host\n      interpreter:\n        kind: io.l5d.namerd\n        dst: /$/inet/namerd.liza.svc.cluster.local/4100\n        namespace: internal\n        transformers:\n        - kind: io.l5d.k8s.daemonset\n          namespace: liza\n          port: incoming\n          service: l5d\n      servers:\n      - port: 4140\n        ip: 0.0.0.0\n      client:\n        kind: io.l5d.global\n        loadBalancer:\n          kind: ewma\n          enableProbation: false\n          maxEffort: 5\n          decayTimeMs: 10\n        failureAccrual:\n          kind: io.l5d.consecutiveFailures\n          failures: 5\n      service:\n        kind: io.l5d.global\n- protocol: http\n  label: incoming\n  identifier:\n    kind: io.l5d.header.token\n    header: Host\n  interpreter:\n    kind: io.l5d.namerd\n    dst: /$/inet/namerd.liza.svc.cluster.local/4100\n    namespace: internal\n    transformers:\n    - kind: io.l5d.k8s.localnode\n  servers:\n  - port: 4141\n    ip: 0.0.0.0\n  client:\n    kind: io.l5d.global\n    loadBalancer:\n      kind: ewma\n      enableProbation: false\n      maxEffort: 5\n      decayTimeMs: 10\n    failureAccrual:\n      kind: io.l5d.consecutiveFailures\n      failures: 5\n  service:\n    kind: io.l5d.global\n\n- protocol: http\n  label: external\n  interpreter:\n    kind: io.l5d.namerd\n    dst: /$/inet/namerd.liza.svc.cluster.local/4100\n    namespace: external\n  servers:\n  - port: 4142\n    ip: 0.0.0.0\n  client:\n    kind: io.l5d.global\n    loadBalancer:\n      kind: ewma\n      enableProbation: false\n      maxEffort: 5\n      decayTimeMs: 10\n    failureAccrual:\n      kind: io.l5d.consecutiveFailures\n      failures: 5\n  service:\n    kind: io.l5d.global\n\nand my Namerd configuration isyaml\n    namers:\n    - kind: io.l5d.k8s\n      experimental: true\n      host: localhost\n      port: 8001\nstorage:\n  kind: io.l5d.k8s\n  host: localhost\n  port: 8001\n  namespace: default\n\ninterfaces:\n- kind: io.l5d.thriftNameInterpreter\n  ip: 0.0.0.0\n  port: 4100\n- kind: io.l5d.httpController\n  ip: 0.0.0.0\n  port: 4180\n\n```\nI've taken the following steps to reproduce:\n\ndeployed Linkerd and Namerd with kubectl -n liza apply -f\ndeploy the hello and world services with kubectl -n liza apply -f hello_world.yml\ncurl -H 'Host: hello' -s $L5D_INGRESS_LB:4140, Linkerd routes correctly to the hello and world services, as expected\nchange the image versions in the config file for the hello and world to ensure they will be redeployed\ndeploy again with kubectl -n liza apply -f hello_world.yml\ncurl -H 'Host: hello' -s $L5D_INGRESS_LB:4140 / curl -H 'Host: world' -s $L5D_INGRESS_LB:4140\nLinkerd routes correctly to the new endpoints, and I don't see any error messages in the logs\n\nI've repeated the deployment a few times, and I'm still not seeing any problems.\nPlease let me know if there's something I'm missing, I've tried to match your configuration as closely as possible.. Interesting. I retried curling repeatedly but couldn't get it to fail. When you say that it worked fine after the deployment but started to fail later, about how long after the deployment did you start to see failures?. @kumudt Hmmm. To be honest, I'm really not sure what's going on here, except that it looks like there's some aspect of your configuration that I haven't been able to accurately replicate in order to reproduce this issue. Thanks for your persistence, and I'll keep working on this.\nDo you know what version of Kubernetes you're using? It's a stretch, but this could only be occurring on a specific Kube version.... Also, @mejran and @Taik, can you confirm that you're no longer seeing this kind of issue on 1.2.1? That could help us to pin down any aspects of @kumudt's deployment that are linked with this problem. Thanks!. Ah, interesting \u2013 I was trying to reproduce with Kubernetes 1.6.9. I'll see if there were any changes that may effect this issue.. Thanks @Taik!. @kumudt it's certainly worth trying!. @bseibel I've been looking into this some more today and I agree that this issue is almost certainly related to kubernetes/kubernetes#35068. That also explains why our unit tests haven't caught this issue, as the tests for handling the \"too old resource version\" response set the response status code to 410.. Hi @bseibel, thanks for following up! \nThe namer should watch each service Kubernetes API object individually, so restarting a watch on a given service shouldn't effect a different service that was added after the watch is restarted. It's possible something else is up. I'll be happy to continue looking into this.\nJust so that I'm sure, what Linkerd/Namerd/Kubernetes versions are you using?. Okay, thank you @bseibel. I'll try to determine whether what you're experiencing is the same problem or something different, and I'll re-open this issue or create a new one as appropriate.. Note that this branch also fixes a very minor issue where the namespace and service names were swapped in some log messages, and ensures that the correct semantics are used for MODIFIED events.  . Thanks for reporting! I think this should be a pretty quick fix.... Hi @0x656b694d, I've just opened pull request #1640 which should fix this issue. We're planning on cutting a hotfix later today with a patch to address some serious issues on Kubernetes (#1638), and hopefully we'll finish reviewing this PR in time for it to be merged before we release the hotfix. If not, it'll be in 1.2.1 for sure.. @adleong, I think I've addressed all the changes you've requested?. @adleong I think now that I've made the changes you've asked for in e8e7eb1, this should be ready to merge?. @adleong, I think I've made all the changes you've requested?. I'd like to deploy this to the test environment for a while before merging to master, in order to make sure the namer is properly exercised.. Deployed to test11.. Btw, this has been running on test11 for a while with no signs of error so far. This has been running in the test env for about 24 hours and there don't seem to have been any issues so far.. N.B. that this doesn't really make any kind of noticeable performance improvement on test11, but I think that's because that test isn't really a stress test for the Consul namer \u2013 the Consul dc in the test has only two services, and the services list doesn't really ever change.. > It should still be the case that Linkerd will use last known information for any of the consul services that it's routing to in the case of a consul outage.\nThat's correct. @Ashald, I believe the caching you're referring to is occurring here \u2013 which should be unchanged in this PR.. aaaah my build finally succeeded! \ud83d\ude4c . I'd like to refactor the logging code in K8s some more (it's a little inelegant) but this should improve performance in the short term.. @adleong, WDYT now?. Okay @adleong, you were right, this could have been much simpler (though I was having fun over-engineering it...). It turns out the code for handling port remappings was the only thing that was really complex enough to be factored out \u2013 for everything else, it turns out that just logging directly from the update function took about as many lines of code as having EventLogging do it...\nLet me know what you think of fa68e421c9326022e9fed839f52b411d675e282f? I also changed namer event handling slightly to try and make it more consistent between different namers.. Closed by #1657. Hi @ethanrubio, I was actually just sitting down to start looking into this issue. I'll definitely let you know if there's anything else we need from you!. Hi @jippi, thanks for letting me know! I think must have been some miscommunication about what kind of New Relic integration you were interested in. I think supporting the New Relic request queue shouldn't take too long, so expect to see that soon.\nI do think there's still a potential use-case for a New Relic plugin like in this PR as well, though that might be better off focusing on metrics related to Linkerd performance rather than on the application itself?. @jippi no worries, and yeah, we can definitely do that.. Hi again @jippi, just wanted to follow up and let you know that we've just merged #1672, which adds an optional timestampHeader configuration field to HTTP servers, which will allow you to add the X-Request-Start header. Sorry it took a little while \u2013 I actually spent most of last week moving \u2013 but it should be in the next Linkerd release!. Hi @agunnerson-ibm, thanks for opening an issue to track this! We'll continue investigating and I'll let you know if we have any questions.. Hi @edio, do the errors you're seeing in 1.3.1 still have the message \"service observation released\"?. Thanks @edio, I'll get back to you if I need any additional information!. @adleong I think I've addressed all requested changes?. Okay, I have a branch up that implements this behaviour, but it looks like we have some tests in ServiceNamerTest that seem to expect that we don't do this.. Further investigation will be necessary in order to determine whether this also resolves any of the resource leaks that have been plaguing the Consul namer for a while.. @adleong i'm working on that test as we speak.. Okay, update on this: it looks like there's no resource leak here, since I've added a test in ae0662ed3e4973850ac6ef2a797a472b6cef25d8 to assert that Consul isn't polled again after releasing a service.. Okay, I don't know if this test is written correctly \u2013 @adleong, I might need your advice on this tomorrow.. Hi @christophetd, we suspect the leak you're seeing may be the same as #1694, which was fixed by @sgrankin in #1714. We've just published a release candidate Docker tag buoyantio/linkerd:1.3.3-RC1-jdk; can we get you to try using that image and confirm that this issue is fixed?. Hi @DukeyToo, I just wanted to let you know that you can now test a release candidate for 1.3.3 by using the buoyantio/linkerd:1.3.3-RC1-jdk Docker image, which should make testing your use case against the fix much easier.. Hi @DukeyToo, the gradual increase in rt:{client, server}: stream:open_streams metric is a symptom of an unrelated issue that only affects HTTP/2 routers, which track those metrics. As your configuration doesn't contain any H2 routers,  it's correct for you not to see those metrics, since HTTP/1.1 is not a streaming protocol. I believe @siggy was mistaken when he told you to look at that metric. Sorry for any confusion!. @siggy, I believe in the logs that @DukeyToo posted, the Linkerd reports itself as linkerd 1.3.2-SNAPSHOT. A few comments ago (https://gist.github.com/DukeyToo/582285e2b3603ce9468e4f2c2f21be75#file-linkerd-log-L8) @DukeyToo says he did a build of master as of revision c6f0d2eaeecca80c60314e6f6cb852a31870877a, I'm assuming that's what he's running currently?. Hi @DukeyToo, following up on this issue: we've fixed a number of memory leaks since the version where this issue was observed. Are you still seeing this leak in the latest release, Linkerd 1.3.4?. That, at least, is good to hear!. cc @adleong, @olix0r . I think we probably don't want to count these streams as \"open\", and we could easily change the open_streams gauge to only count streams in the StreamOpen state. We might want to also add new metrics to track streams in the reset and failed states.\nI'd be happy to make these changes but would like a confirmation that the current behaviour is incorrect first.. > tracked_streams might be a better name for this gauge.  I think we want to keep this as it indicates how many streams we are tracking in the hashmap.  \nDo you think renaming this gauge at this point would be a compatibility issue?\n\nAdding gauges for streams in particular states (open, reset, etc) sounds good in theory, but I worry about needing to iterate over the entire hashmap to compute this...\n\nWe could, at least, count streams in each state in one iteration, but I'm fairly sure even a single iteration is still costlier than .size()...alternatively, we could add counters that are updated when a stream transitions from one state into another, and have the gauges sample those counters?. Hi @sgrankin, thanks for reporting! I think the theory you've proposed seems quite possible, but would love to take a look at your heap dumps to confirm.. Hi @sgrankin, once again, thanks for your help. We'll take a look at your changes and get back to you on that as soon as possible!. \ud83d\ude4c thanks @sgrankin!. Alright, I've figured out exactly why this is happening. Consider the following code, in Netty4ServerDispatcher.serveStream():\nhttps://github.com/linkerd/linkerd/blob/b5cd1c168873d1d668b5f4fdbd729a44122135d2/finagle/h2/src/main/scala/com/twitter/finagle/buoyant/h2/netty4/Netty4ServerDispatcher.scala#L83-L91\nWhen a server stream is reset, it raises the reset to serveF, a future chain which should include the Service passed to the Netty4ServerDispatcher constructor; namely, the corresponding Netty4ClientDispatcher (and its underlying stream transport). If that future is still pending, raising the reset exception will reset that stream. HOWEVER, if the future has finished, the client stream will not be reset. It appears that streams entering the half-closed (remote) state satisfy the future, and thus are never reset.\nI added some debug logging to verify this theory, and it looks like this is indeed the case:\nD 1114 23:15:53.966 UTC THREAD27: [S L:/127.0.0.1:4140 R:/127.0.0.1:61757 S:1] st.onReset -> serveF.raise(Reset.Cancel); serveF.isDone=true\nD 1114 23:15:53.966 UTC THREAD27: [S L:/127.0.0.1:4140 R:/127.0.0.1:61757 S:1] stream reset from remote: Reset.Cancel\nD 1114 23:15:53.966 UTC THREAD27: [S L:/127.0.0.1:4140 R:/127.0.0.1:61757 S:17] st.onReset -> serveF.raise(Reset.Cancel); serveF.isDone=true\nD 1114 23:15:53.966 UTC THREAD27: [S L:/127.0.0.1:4140 R:/127.0.0.1:61757 S:17] stream reset from remote: Reset.Cancel\nD 1114 23:15:53.966 UTC THREAD27: [S L:/127.0.0.1:4140 R:/127.0.0.1:61757 S:3] st.onReset -> serveF.raise(Reset.Cancel); serveF.isDone=true\nD 1114 23:15:53.966 UTC THREAD27: [S L:/127.0.0.1:4140 R:/127.0.0.1:61757 S:3] stream reset from remote: Reset.Cancel\nD 1114 23:15:53.966 UTC THREAD27: [S L:/127.0.0.1:4140 R:/127.0.0.1:61757 S:7] st.onReset -> serveF.raise(Reset.Cancel); serveF.isDone=true\nD 1114 23:15:53.966 UTC THREAD27: [S L:/127.0.0.1:4140 R:/127.0.0.1:61757 S:7] stream reset from remote: Reset.Cancel\nD 1114 23:15:53.966 UTC THREAD27: [S L:/127.0.0.1:4140 R:/127.0.0.1:61757 S:9] st.onReset -> serveF.raise(Reset.Cancel); serveF.isDone=true\nD 1114 23:15:53.966 UTC THREAD27: [S L:/127.0.0.1:4140 R:/127.0.0.1:61757 S:9] stream reset from remote: Reset.Cancel\nD 1114 23:15:53.966 UTC THREAD27: [S L:/127.0.0.1:4140 R:/127.0.0.1:61757 S:11] st.onReset -> serveF.raise(Reset.Cancel); serveF.isDone=true\nD 1114 23:15:53.966 UTC THREAD27: [S L:/127.0.0.1:4140 R:/127.0.0.1:61757 S:11] stream reset from remote: Reset.Cancel\nD 1114 23:15:53.966 UTC THREAD27: [S L:/127.0.0.1:4140 R:/127.0.0.1:61757 S:13] st.onReset -> serveF.raise(Reset.Cancel); serveF.isDone=true. ```\nD 1115 23:58:32.350 UTC THREAD35: [S L:/127.0.0.1:4140 R:/127.0.0.1:65533] go away: GoAway.InternalError\nD 1115 23:58:32.350 UTC THREAD35: [S L:/127.0.0.1:4140 R:/127.0.0.1:65533] resetting all streams: Reset.Cancel\nD 1115 23:58:32.353 UTC THREAD35: [S L:/127.0.0.1:4140 R:/127.0.0.1:65533 S:1] resetting Reset.Cancel in LocalClosed(remote: RemoteStreaming)\nD 1115 23:58:32.353 UTC THREAD35: [S L:/127.0.0.1:4140 R:/127.0.0.1:65533] reset Netty4StreamTransport.Server(state: Closed(error: Reset.Cancel)): Reset.Cancel\nD 1115 23:58:32.353 UTC THREAD35: [S L:/127.0.0.1:4140 R:/127.0.0.1:65533 S:17] resetting Reset.Cancel in LocalClosed(remote: RemoteStreaming)\nD 1115 23:58:32.353 UTC THREAD35: [S L:/127.0.0.1:4140 R:/127.0.0.1:65533] reset Netty4StreamTransport.Server(state: Closed(error: Reset.Cancel)): Reset.Cancel\nD 1115 23:58:32.354 UTC THREAD35: [S L:/127.0.0.1:4140 R:/127.0.0.1:65533 S:19] resetting Reset.Cancel in RemoteClosed(15 offers)\nD 1115 23:58:32.354 UTC THREAD35: [S L:/127.0.0.1:4140 R:/127.0.0.1:65533 S:19] RemoteClosed(15 offers).reset(Reset.Cancel); q failed\nD 1115 23:58:32.354 UTC THREAD35: [S L:/127.0.0.1:4140 R:/127.0.0.1:65533] reset Netty4StreamTransport.Server(state: Closed(error: Reset.Cancel)): Reset.Cancel\nD 1115 23:58:32.354 UTC THREAD35: [S L:/127.0.0.1:4140 R:/127.0.0.1:65533 S:21] resetting Reset.Cancel in RemoteClosed(0 offers)\nD 1115 23:58:32.354 UTC THREAD35: [S L:/127.0.0.1:4140 R:/127.0.0.1:65533 S:21] RemoteClosed(0 offers).reset(Reset.Cancel); q failed\nD 1115 23:58:32.354 UTC THREAD35: [S L:/127.0.0.1:4140 R:/127.0.0.1:65533] reset Netty4StreamTransport.Server(state: Closed(error: Reset.Cancel)): Reset.Cancel\nD 1115 23:58:32.354 UTC THREAD35: [S L:/127.0.0.1:4140 R:/127.0.0.1:65533 S:13] resetting Reset.Cancel in LocalClosed(remote: RemoteStreaming)\nD 1115 23:58:32.354 UTC THREAD35: [S L:/127.0.0.1:4140 R:/127.0.0.1:65533] reset Netty4StreamTransport.Server(state: Closed(error: Reset.Cancel)): Reset.Cancel\nD 1115 23:58:32.354 UTC THREAD35: [S L:/127.0.0.1:4140 R:/127.0.0.1:65533] streams={1=StreamRemoteReset$, 17=StreamRemoteReset$, 19=StreamRemoteReset$, 21=StreamRemoteReset$, 13=StreamRemoteReset$}\nD 1115 23:58:32.355 UTC THREAD35: [S L:/127.0.0.1:4140 R:/127.0.0.1:65533 S:1] reset by Reset.Cancel, raising to serveF\nD 1115 23:58:32.355 UTC THREAD35: [S L:/127.0.0.1:4140 R:/127.0.0.1:65533 S:1] stream reset from remote: Reset.Cancel\nD 1115 23:58:32.355 UTC THREAD35: [S L:/127.0.0.1:4140 R:/127.0.0.1:65533 S:17] reset by Reset.Cancel, raising to serveF\nD 1115 23:58:32.355 UTC THREAD35: [S L:/127.0.0.1:4140 R:/127.0.0.1:65533 S:17] stream reset from remote: Reset.Cancel\nD 1115 23:58:32.355 UTC THREAD35: [S L:/127.0.0.1:4140 R:/127.0.0.1:65533 S:19] reset by Reset.Cancel, raising to serveF\nD 1115 23:58:32.355 UTC THREAD35: [C L:/127.0.0.1:65534 R:localhost/127.0.0.1:8888 S:21] remote message interrupted: Reset.Cancel\nD 1115 23:58:32.355 UTC THREAD35: [C L:/127.0.0.1:65534 R:localhost/127.0.0.1:8888 S:21] resetting Reset.Cancel in Open(remote: RemotePending)\nD 1115 23:58:32.355 UTC THREAD35: [S L:/127.0.0.1:4140 R:/127.0.0.1:65533 S:19] stream reset from remote: Reset.Cancel\nD 1115 23:58:32.355 UTC THREAD35: [S L:/127.0.0.1:4140 R:/127.0.0.1:65533 S:21] reset by Reset.Cancel, raising to serveF\nD 1115 23:58:32.355 UTC THREAD35: [C L:/127.0.0.1:65534 R:localhost/127.0.0.1:8888 S:23] remote message interrupted: Reset.Cancel\nD 1115 23:58:32.355 UTC THREAD35: [C L:/127.0.0.1:65534 R:localhost/127.0.0.1:8888 S:23] resetting Reset.Cancel in LocalClosed(remote: RemotePending)\nD 1115 23:58:32.356 UTC THREAD35: [S L:/127.0.0.1:4140 R:/127.0.0.1:65533 S:21] stream reset from remote: Reset.Cancel\nD 1115 23:58:32.356 UTC THREAD35: [S L:/127.0.0.1:4140 R:/127.0.0.1:65533 S:13] reset by Reset.Cancel, raising to serveF\nD 1115 23:58:32.356 UTC THREAD36 TraceId:a6e34d95c3925567: [C L:/127.0.0.1:65534 R:localhost/127.0.0.1:8888 S:21] remote write failed: Reset.Cancel\nD 1115 23:58:32.356 UTC THREAD35: [S L:/127.0.0.1:4140 R:/127.0.0.1:65533 S:13] stream reset from remote: Reset.Cancel\nI 1115 23:58:32.368 UTC THREAD35 TraceId:a6e34d95c3925567: [C L:/127.0.0.1:65534 R:localhost/127.0.0.1:8888 S:21] onReset.onFailure: StreamError.Local(Reset.Cancel)\nD 1115 23:58:32.368 UTC THREAD35 TraceId:a6e34d95c3925567: [C L:/127.0.0.1:65534 R:localhost/127.0.0.1:8888 S:21] stream reset from local: Reset.Cancel\nD 1115 23:58:32.368 UTC THREAD35 TraceId:a6e34d95c3925567: [C L:/127.0.0.1:65534 R:localhost/127.0.0.1:8888 S:21] stream reset from local; resetting remote: Reset.Cancel\nI 1115 23:58:32.370 UTC THREAD35 TraceId:fd56186acc091384: [C L:/127.0.0.1:65534 R:localhost/127.0.0.1:8888 S:23] onReset.onFailure: StreamError.Local(Reset.Cancel)\nD 1115 23:58:32.370 UTC THREAD35 TraceId:fd56186acc091384: [C L:/127.0.0.1:65534 R:localhost/127.0.0.1:8888 S:23] stream reset from local: Reset.Cancel\nD 1115 23:58:32.370 UTC THREAD35 TraceId:fd56186acc091384: [C L:/127.0.0.1:65534 R:localhost/127.0.0.1:8888 S:23] stream reset from local; resetting remote: Reset.Cancel\nD 1115 23:58:32.375 UTC THREAD35 TraceId:8950e59880c20995: [C L:/127.0.0.1:65534 R:localhost/127.0.0.1:8888 S:19] stream read failed: Reset.Cancel\nD 1115 23:58:32.375 UTC THREAD35 TraceId:8950e59880c20995: [C L:/127.0.0.1:65534 R:localhost/127.0.0.1:8888 S:19] resetting Reset.Cancel in RemoteClosed(0 offers)\nD 1115 23:58:32.375 UTC THREAD35 TraceId:8950e59880c20995: [C L:/127.0.0.1:65534 R:localhost/127.0.0.1:8888 S:19] RemoteClosed(0 offers).reset(Reset.Cancel); q failed\nI 1115 23:58:32.377 UTC THREAD35 TraceId:8950e59880c20995: [C L:/127.0.0.1:65534 R:localhost/127.0.0.1:8888 S:19] onReset.onFailure: StreamError.Local(Reset.Cancel)\nD 1115 23:58:32.377 UTC THREAD35 TraceId:8950e59880c20995: [C L:/127.0.0.1:65534 R:localhost/127.0.0.1:8888 S:19] stream reset from local: Reset.Cancel\nD 1115 23:58:32.377 UTC THREAD35 TraceId:8950e59880c20995: [C L:/127.0.0.1:65534 R:localhost/127.0.0.1:8888 S:19] stream reset from local; resetting remote: Reset.Cancel\nW 1115 23:58:32.379 UTC THREAD35 TraceId:a6e34d95c3925567: Exception propagated to the default monitor (upstream address: /127.0.0.1:65533, downstream address: localhost/127.0.0.1:8888, label: $/inet/localhost/8888).\nReset.Cancel\nW 1115 23:58:32.379 UTC THREAD35 TraceId:fd56186acc091384: Exception propagated to the default monitor (upstream address: /127.0.0.1:65533, downstream address: localhost/127.0.0.1:8888, label: $/inet/localhost/8888).\nReset.Cancel\nD 1115 23:58:32.386 UTC THREAD35: [S L:/127.0.0.1:4140 R:/127.0.0.1:65533] transport closed: com.twitter.finagle.ChannelClosedException: ChannelException at remote address: /127.0.0.1:65533. Remote Info: Not Available\nD 1115 23:58:59.292 UTC THREAD11: [S L:/127.0.0.1:4140 R:/127.0.0.1:65533] streams={1=StreamRemoteReset$, 17=StreamRemoteReset$, 19=StreamRemoteReset$, 21=StreamRemoteReset$, 13=StreamRemoteReset$}\nD 1115 23:58:59.611 UTC THREAD11: [C L:/127.0.0.1:65534 R:localhost/127.0.0.1:8888] streams={17=StreamOpen(Netty4StreamTransport.Client(state: RemoteClosed(0 offers))), 3=StreamOpen(Netty4StreamTransport.Client(state: RemoteClosed(0 offers))), 19=StreamLocalReset$, 5=StreamOpen(Netty4StreamTransport.Client(state: RemoteClosed(0 offers))), 21=StreamLocalReset$, 7=StreamOpen(Netty4StreamTransport.Client(state: RemoteClosed(0 offers))), 23=StreamLocalReset$, 9=StreamOpen(Netty4StreamTransport.Client(state: RemoteClosed(0 offers))), 11=StreamOpen(Netty4StreamTransport.Client(state: RemoteClosed(0 offers))), 13=StreamOpen(Netty4StreamTransport.Client(state: RemoteClosed(0 offers))), 15=StreamOpen(Netty4StreamTransport.Client(state: RemoteClosed(0 offers)))}\n^X^CI 1115 23:59:10.969 UTC THREAD44: Received SIGINT. Shutting down ...\nI 1115 23:59:10.995 UTC THREAD44: Reaping /svc/localhost:4140\nI 1115 23:59:11.028 UTC THREAD44: Reaping $/inet/localhost/8888\n``. Issue occurs whenClassifiedRetriesFilter.DefaultBufferSizeis 65k. If I set it to 8k, all my frames get released and everything is fine.. Proposed solution as per @adleong: ~add classification timeout to request stream as well as response stream (and possibly~ reduce default buffer capacity~)~.. @zackangelo it seems pretty unlikely; this only occurs when very large data frames (>= than 1/4th of the flow control window size) are buffered for response classification, and they aren't being released because they're still in the buffer. It's definitely causing #1605, but isn't the kind of general failure to release frames that might cause the memory leaks we've been seeing.. @deebo91 hmm, maybe open an issue for that as well?. @adleong beat me to it :P. @adleong, now that the changes you've requested have been made in 85ccc37135c4d8cae49e9beedd7b86ec437d7dff, is this ready to merge? I'd like to get it in for 1.3.3.. TODO:\n - [x] probably need to also removeStreamClosedstreams from thestreams` hashmap when the corresponding client stream finishes and/or not put them in the map unless we know the client stream is still open (done 47b888eb2c109a8ff542eb0ee67743a1e0b206f2)\n - [x] reap reset/errored streams after a TTL (done in its own branch: c906c4f709f86c01033e10ffd83e264ce9f138d5). @adleong thanks for the review, I've edited this PR to describe the issue more accurately. \nYour description of what should be happening in this scenario is correct. However, when we reach step 5, the observed behaviour no longer lines up with your description:\n\n\nMeanwhile, the client dispatcher's StreamTransport is attempting to write the request stream by calling stream.read in a loop in the writeStream method.  When the AsyncQueue is failed, this stream.read should result in a Reset.Cancel exception which is then wrapped in a StreamError.Local\n\n\nFor streams which are leaked due to this bug, the closure flatMapped over stream.read() in writeStream is never called again after the client disconnects. When I was investigating this issue, I added logging to AsyncQueue, and I believe that the future returned by read() never finishes, because the stream's AsyncQueue has 0 offers.. Closing this for now as we are going to try and find a better solution.. @adleong agreed, any suggestions?. Some improvements in allocations vs 1.3.2 in some test runs:\ntest 7\n\ntest 2\n\n(my branch is on the left and 1.3.2 is on the right). Some of the other test runs appear to have broken after I redeployed 1.3.2, or to have not been reporting metrics for my branch, so I'm still working on getting a trustworthy validation of this.. Correct me if I'm wrong, but we think this leak is likely the cause of #1690, right? Can we try to verify that this fixes that issue?. @sgrankin yeah, that's one of a handful of integration tests that are known to be somewhat flaky on CI (https://github.com/linkerd/linkerd/issues/1504), please feel free to restart any builds failed by that test!. Hi @vadimi and @sawyerzhu, I've been investigating some problems potentially linked to the root cause of this issue, and I've made some changes that I think may resolve this. \nSince we don't currently have a way to reproduce this failure to test against, would you be interested in trying a snapshot? I've pushed a docker image tagged as buoyantio/linkerd:1.3.3-SNAPSHOT-grpc. If you're able, please try running that image and let us know if the issue persists. Thanks!. Ah, sorry about that, @vadimi. Unfortunately, the executable is too large to attach to this comment. I've pushed a branch, which you can check out and build from by running ./sbt linkerd/assembly. Otherwise, I can try to try and find some other way to get you the binary?  \n. That's fantastic @vadimi! Please feel free to open a PR (off of my branch?) and we can look at getting this merged \u2013 we can review your changes there.. Hi @dpetersen, thanks for writing this enormous ticket! I really appreciate seeing the detailed description of the problem. \nWe'll look into this and get back to you if I need any more information. . Okay @dpetersen, I can reproduce the log messages you're seeing, but we're only observing a single failed request every time we redeploy the service (which appears to have been in flight when the rolled pod terminates) \u2013 we're not seeing the service go down and stay down after a deployment.. @dpetersen, what sequence of commands is actually being used to do the deployment?. As much metrics data as you can provide would be highly appreciated!. Morning @dpetersen, something else that could be useful for debugging this issue is if you could set the log level for the k8s scope to TRACE rather than DEBUG. You can do this by opening the Linkerd admin interface, clicking on \"logging\" in the top menu bar, and scrolling down to find k8s. You probably don't want to set the TRACE log level globally - it'll make your logs very verbose.. Hi again @dpetersen, thanks for all the additional information! Hopefully the fixes in 1.3.4 will help, but if not, we can also cut you a snapshot release with the changes in #1755. And if that doesn't work, the information you've provided will help as we continue debugging this issue.. Okay, thanks for keeping us informed. Would you be interested in trying a snapshot release while we continue investigating this issue?. Alright, should I publish a snapshot Docker image or send you a binary? We don't have a working reproduction of the issue you're seeing yet, so we can't yet verify that #1755 definitely fixes it, but the symptoms seem similar.. Okay, I've pushed a new Linkerd image tagged as buoyantio/linkerd:1.3.4-SNAPSHOT-k8s-json-fix. Please let us know if you continue seeing issues with this snapshot!. > Unfortunately, it didn't take long to get a failure.\nJust to confirm, this is with the snapshot?. > Yes, the two most recent failures are running the snapshot.\nOkay, just making sure. I guess that at least we now know the issue you're seeing is different from the one @obeattie describes in #1755. \nI'm sorry you're continuing to see these failures! While it definitely seems like a Linkerd bug is involved, your theory that something about your service is interacting with Linkerd in some way that triggers it is definitely likely from my perspective, especially given that I still haven't been able to reproduce these issues on my end.. Just wanted to give you all a little update: we've finally seen this type of failure in our test cluster, and that's helped to provide a little more insight into where the bug might be. I'm still investigating the issue, but there has been some progress!. Alright folks, here's another update. \nFirst, we've determined the cause of an issue where Linkerd may fail to correctly register Kubernetes DELETED events (#1791).  @dpetersen, from the information you've given us, we do not believe that this bug is also the cause of the issues you're seeing, since modifying a Deployment object should only trigger MODIFIED events. However, for others in this thread, if deleting a service or deployment is part of your deploy process, it's possible the issues you're seeing are caused by #1791.\n\nNow that that's out of the way, here's the important bit: while we've seen this issue occur in our test cluster, we still don't have a solid reproduction yet. However, we have found a potential race condition which may cause Linkerd's Kubernetes namer to become \"stuck\" in a way that seems similar to this issue. We can't easily verify that fixing this race will fix this behavior. \nHowever, we've cut a snapshot build with a fix for the potential race, which should hopefully also fix this bug. If you're interested in testing it out, the Docker image is buoyantio/linkerd:1.3.5-k8s-namer-SNAPSHOT --- please let me know if it fixes these issues!. @bourquep in that case, it sounds very likely that the issue you're experiencing is in fact #1791. Fortunately, we have a fix for that bug, as well -- if you'd like, I can publish a snapshot image for that issue as well. \nIf you do switch to a build with the fix for #1791 and continue to see occasional failures, it's likely that any remaining routing failures are caused by this bug (#1730) --- this may be the case if you are seeing intermittent routing failures on the prod cluster despite never deleting k8s resources. However, I would expect that a build with the fix for incorrect DELETED event handling will lead to a significantly reduced rate of incorrect routing in your test cluster.. @bourquep no, that's quite alright! I'm happy to hear that your Conduit adoption has been so successful! :D. @adleong added to the description & made the changes you requested!. @deebo91 If you were to just copy and paste the code example I gave above, yes. But if you were to also add an implicit conversion from Ns back to String, consumers of Ns would still be able to call all the String methods on it. Something like\nscala\nimplicit def nsToString(ns: Ns): String = ns.ns\nand then just make sure the implicit is in scope at the call site, and all consumers of Ns that call String methods on it will still work.. Of course, that does break some of the advantages of type-level validation; if I can implicitly convert an Ns back into a string then I can edit the string to include invalid characters again. You might prefer to have your Ns class only implement the methods on String that consumers of Ns will actually be able to use safely.. @fangel NetworkedInteropTest.cancel_after_begin is one of a small handful of tests that sometimes fail on CI for reasons we haven't been able to pin down yet; so that's probably spurious. I'll restart the build, and we'll review your most recent changes. Thanks again!. @siggy I've made all the changes you've requested except for the docs change --- do you think that should go in README.md or BUILD.md?. (sorry, this one's probably my fault, isn't it?). Hi @vadimi, it looks like your CI builds are failing due to an issue that was fixed on master in 9f037db949e5f61edf5837e94386db455ef732f0, would you be willing rebase your branch from the current master so we can get a clean CI build?. hi @vadimi, yeah, I merged your changes back into my branch so I could add some comments & test against the latest changes. I'm happy to take it from here, but if you'd like to get credit for the final merge commit, feel free to merge my changes back into your branch and open a PR against master?. @vadimi cool, I've gone ahead and opened #1751; this should end up in the 1.3.4 release!. @adleong I completely agree; I have just enough understanding of this change to understand why it might fix some issues, but not enough to feel confident that it will fix all ref-counting issues.. @adleong, if this supersedes the match errors branch, can we get a \"closes #1748\" in the PR description?. ah, nevermind; i see you've already closed it \ud83d\udc4d \n. @obeattie, this is great! I'll get back to you with more detailed review soon. \ud83d\ude04 . @siggy how do you feel about changing the name of the script to h2.sh? i thought it was reasonable because the subcommands are now load and spec; but wasn't sure if that seemed clear to everyone else?. @siggy yeah, just started on that :). Hi @rclayton-the-terrible  I've successfully reproduced this issue using the steps you've provided (thanks!) and can confirm that it's happening with Namerd 1.3.4. I'll look into getting it fixed.. Okay, I've figured out what's going wrong here. Should have a fix ready soon! :). Hi again @rclayton-the-terrible, merging PR #1760 should improve this significantly \u2013 Namerd will now log that a dtab parsing error occurred and describe why the dtab was invalid, like\nE 1220 20:46:06.770 UTC THREAD31 TraceId:30cbad82deebb433: consul ns consul dtab parsing failed: java.lang.IllegalArgumentException: '/' expected but 'a' found at '[a]sjgfdskjhsdfg;'; dtab: 'asjgfdskjhsdfg;'\nI've also opened a follow-on issue #1761 to display dtab parse error messages in the web UI as well as in the logs.. @rclayton-the-terrible, I'm happy to build a snapshot Namerd executable or Docker image for you if it makes verifying these changes any easier.... @rclayton-the-terrible, just pushed an image tagged buoyantio/namerd:1.3.4-SNAPSHOT-consul-parse-errors to Docker Hub.. @rclayton-the-terrible that's great! We'll also look into making the UI present the error in a way that's a little more informative than HTTP ERROR 500 (see #1761).. merged from the Buoyant holiday party cable car \ud83d\ude42. @adleong I'm not sure how the malformed dtabs were being written to Consul; while I was testing this fix I was using the Consul command line consul kv put ... to write them to a local Consul instance. We might want to ask @rclayton-the-terrible, who reported #1759, how unparsable dtabs ended up in Consul?. @deebo91 ah, yeah, that's more or less what I suspected.. Hi @yangzhares, to answer your questions:\n1. The main difference between  the io.l5d.thriftNameInterpreter, io.l5d.mesh, and io.l5d.httpController Namerd interfaces is the protocol used by Linkerd to communicate with Namerd (Thrift, gRPC, and HTTP, respectively). Of course, there are some differences in the respective implementations, and the different protocols may have different performance characteristics.\n\nIf you're referring to the active and inactive caches mentioned in issue #1778, those caches are used to improve the performance of requests to Namerd, not for handling cases where the service discovery backend goes down. That behaviour is implemented at the namer level, not in the Namerd interface level.  Specifically, since you're using Consul for service discovery, you'll be happy to note that the io.l5d.consul namer will fall back to the last good state received from Consul if Consul goes down, and resume receiving updates from Consul when it comes back up (this behaviour was added in #1597).. @edio in the event of a Namerd outage, Linkerds will continue routing using the last routing data observed from Namerd, and will resume receiving updates when Namerd comes back online.  \n\nThis is because all Namerd interfaces are implemented as instances of Finagle's NameInterpreter trait, which returns an Activity for name resolution. If an error (such as Namerd going offline) occurs, rather than setting the Activity to Activity.Failed, the interface will simply not update it, leaving the previous Activity.Ok state intact. This behaviour should be the same regardless of what Namerd interface is used.\nSidenote: if you want to see this behaviour in action, you can easily demonstrate it by:\n1. spin up Namerd backed by the io.l5d.fs namer. \n  ./sbt namerd-examples/basic:run\n2. spin up a Linkerd using that Namerd for name resolution.\n   ./sbt linkerd-examples/namerd:run in a different terminal \n3. curl -H \"Host: default\" http://localhost:4140 (you should see the Linkerd admin page)\n4. kill the Namerd process\n5. curl Linkerd again --- it should continue to route correctly even though Namerd is offline.\n6. modify namerd/examples/disco/default to point to a different port (maybe the Namerd admin port, :9991)\n7. restart Namerd\n8. after Namerd comes back up, curl Linkerd again. it should now reflect the new state in Namerd, indicating that changes after a Namerd outage do update routing data.\nYou can then modify the linkerd/examples/namerd.yaml and namerd/examples/basic.yaml configuration files to use different Namerd interfaces, to demo that they all exhibit the correct behaviour.. I'm going to go ahead and close this issue, since #1778 and #1779 have now been opened to track the issue that @yangzhares reported. @yangzhares and @edio, if either of you have any additional questions, I'm definitely happy to continue discussing Linkerd and Namerd with you, on Slack, Discourse, or wherever you prefer!. @nikolay-pshenichny whoops! Will fix that.. Yay, thank you!. @adleong do we still consider HTTP access logging (#1776) to be a prerequisite for graduating H2?. I believe merging this PR closes #854.. @olix0r, I'd appreciate getting your \u2705 on this before it makes it into the release.. This is still waiting on #1782, when that lands, I'll rebase this PR and update the changelog. Figured I'd push it up now so @adleong can start reviewing the changelog.. @adleong, I think 3fa78bcd45a8521560304da51f8ef7b8eb37758d makes all the changes you requested.. We may also want to consider setting more code owners - note that this feature also allows owners to be assigned to directories, or to all files matching a certain pattern.. I've also checked the \"Require review from Code Owners\" box in the branch protection setting for master, so any change to the changelog will not be mergeable unless approved by @olix0r or @wmorgan.\n. @franziskagoltz \n\nThe current Request object does not store a protocol version, we could potentially hard code this, as this code is only executed when http/2 are being made.\n\nI think this seems reasonable for now.. \ud83c\udf10\ud83d\udcf2\ud83d\udcdd\ud83d\udc4d. CI failure for 6efeca5da632c7c1a9b938c7450b4f24142c973d appears to be spurious, restarting.. Yay!. @deebo91 Yeah, Watchable is involved in pretty much all of our code that communicates with Kubernetes, not just ingress. Although that does mean that we'll have to do a lot of testing to be confident in shipping this change, it also means that there's a distinct possibility that this fixes other k8s watch issues such as #1730, as well as the ingress-specific issue #1791!. @negz \n\nmy understanding is that MODIFIED objects will have a higher resourceVersion, while DELETED objects will have an unchanged resourceVersion because they return \"the state of the object immediately before deletion\".\n\nThat seems right to me --- thanks for putting that together. I have some thoughts on how to fix Linkerd's handling of watch events based on your observation, so hopefully we'll be able to start testing some changes soon! Thanks again.. Thanks for sharing the repro steps, @negz, that's very helpful.. @adleong sounds good to me!. @shakti-das We're not planning on cutting 1.3.7 until next week, and we'd like to get this PR merged today pending @olix0r's sign-off. :). And it's merged! Thanks @shakti-das!. Should we consider also adding @ccmtaylor to .github/CODEOWNERS for the file paths associated with the SRV namer?. Hi @eroji, I'm not entirely clear on what you're trying to do, so please bare with me while I ask some clarifying questions. Are you saying that you want to configure Linkerd to route requests to the DNS name https://services.example.com, which corresponds to the Nginx ingress controller?. Ah, okay, that makes more sense. Can I see your Linkerd config file in order to better understand what's going on here?. Okay, right now it looks like the dtab for Linkerd's incoming router is configured to route requests to a Kubernetes service called \"world-v1\":\n/srv        => /#/io.l5d.k8s/default/http;\n        /host       => /srv;\n        /svc        => /host;\n        /host/world => /srv/world-v1;\nWhat you need to do is change the dtab to route to your services. If you want routing like\napp1 -> /app1/api/v1/...\napp2 -> /app2/api/v1/...\napp3 -> /app3/api/v1/...\nyou want to use the path identifier rather than the host identifier, which is currently configured. So you'll want to change the incoming router to something like this: \nyaml\n- protocol: http\n  label: incoming\n  identifier:\n    kind: io.l5d.path\n    segments: 1\n    consume: true\n  dtab: |\n    /srv        => /#/io.l5d.k8s/default/http;\n    /svc        => /srv\n  interpreter:\n    kind: default\n    transformers:\n    - kind: io.l5d.k8s.localnode\n      hostNetwork: true\n  servers:\n  - port: 4141\n    ip: 0.0.0.0\nThis will configure Linkerd to take the first path segment from the request path, and use that as the name of a Kubernetes service to route the request to. The io.l5d.k8s.localnode transformer will ensure that Linkerd routes to a pod in that service that's running on the same node as the Linkerd instance.\nLet me know if you have any additional questions!. The outgoing router in this case controls how requests made by services running on the same node as a Linkerd instance are routed. If you want to route these requests the same way you route incoming requests, then yes. If these requests should be routed differently, then you'll want the configuration for this router to reflect that. If your services are not routing outgoing requests through Linkerd, then you can remove that router from the config file.. @eroji you'll need to configure your application to use Linkerd as a proxy. Many (but not all) applications and frameworks support a http_proxy environment variable; otherwise, they'll likely have their own method of configuring a HTTP proxy. See https://linkerd.io/getting-started/k8s/#configure-your-application for more information.\nYou might also be interested to note that, unlike Linkerd, the Conduit service mesh can be transparently injected into Kubernetes pods without requiring you to set it as a proxy; it will set iptables rules so that all outgoing requests are routed through Conduit automagically. However, it doesn't currently support the kind of policy-based routing we discussed above, but this is on the roadmap.. Hi @DipeshMitthalal, thanks for the feature request!. I think it might be worthwhile to consider making this change in a way that can benefit more than just the Marathon namer; similar failover-on-empty-addrs might be useful with other service discovery backends as well?. @adleong \n\nI'm not sure about that. Finagle explicitly differentiates between Empty (the service exists but has 0 instances) and Neg (the service does not exist). I'm not sure if we always want to fallback on empty services.\n\nI'm thinking this behaviour should be opt-in with a configuration setting --- sorry if that wasn't clear in the issue description. My understanding is that (according to @olix0r) is that multiple people have expressed wanting to be able to set a fallback-on-zero-replicas kind of behaviour in Linkerd.. @adleong Any test that was mentioned in one of the two currently-open flaky tests issues (#1225 and #1504) was marked as retryable, as well as grpc.NetworkedInteropTest, which was not mentioned in an issue but has frequently observed to be flaky.. \ud83d\ude4c :tada:\ud83c\udf8a . This is excellent! I had thought removing UnpoolHandler. would require significantly more work, but it's awesome that it can just be deleted. I also thought this was likely to be the kind of change that we'd want to be extremely cautious about making, but given your description above it seems like this is fairly unlikely to break anything?. Ah, of course \u2013 I'll make it prettier :). Also good to know, thanks; I'll keep that in mind. I wasn't sure about coding style, all I saw was \"use Effective Scala\". Yeah, I screwed up the merge, my bad.. Yeah, I had this expecting 150 for a while (I hadn't had coffee yet, and instead of doing the math, I thought \"half seems reasonable, right?\" \ud83d\ude13) and it was failing with lengths around 95-98. \nAfter I worked out the percentage to actually expect, I was thinking about setting it to 95 but figured I'd lowball it because I didn't want to have a test that breaks occasionally due to probability. I can take it down to 2 but I suspect the odds of it dipping under 50 are pretty low...hmm.. Changed this in 5d781156316f1ceb7d58e21a7fd8489c89e8aaaf. I took down the expected number of unique values to 2, and added a (perhaps too) long comment for historical reasons.\nI think that seems fair?. Okay, should I go ahead and merge this, or wait for @rmars?. Oops! I should probably tell IDEA to stop making those.... This should already extend io.buoyant.test.FunSuite. I think you're right that the annotation can be removed, though.. This is the forAll from PropertyChecks, so it needs the assertion.. Personally, I prefer should because I think it's more expressive, and produces more readable failure messages, but I'd be happy to rewrite this to use assert.. Whoops. Fixed that here: 0192b58e485c191112b447cd8aff57e2a14a2650. Hmm. I used lazy val since the original code used a val and it seemed like the intention was for this value to not change after the config is parsed. Are we expecting that configs will ever be parsed?. I think that \u2013  if we're not in the habit of mutating these configs post-parsing\u00a0\u2013 ideally, it would be nice if the config was immutable, and was instantiated by the parser with all the values set, rather than instantiated and then mutated by the parser (this error could have been avoided entirely!). I'm not sure if our parsing code would easily allow that in this instance, though.... The main advantage of table-based checks is that they allow naming of columns and make somewhat more readable failure messages, and use the same verbs as generator-based property checks (for consistency). I think in this case, since the table has a single column, this is basically the same thing as iterating over a Seq...I could easily replace this with a Seq if you'd prefer.. Ah, yeah, that definitely shouldn't be there; I haven't written ScalaTest tests for a bit & misunderstood something from the docs. Fixed in d16e001e88abf2882cacb304fa354e9603c00616. They should be equivalent; the for expression is completely vestigial. I think this started out its life as for { n <- Arbitrary[Int] if n >= 1} yield n or something like that, before I rewrote it with Gen.choose and forgot to remove the for-expression. The map is just doing the identity operation at this point; I'll get rid of it. Thanks for catching that, now I feel silly.. Okay, if we're limited by Jackson in this case, I'll definitely make it a def for now. It would be nice to use a parsing library that generates immutable immutable config classes but that sounds like it would be a major structural change at this point.\nI tend to make fairly liberal use of lazy vals in my own code, but this is because I generally try to make everything immutable \u2013 i've been known to set WartRemover to make var into a compile error. In this case, you're right, and I'll change it to a def.. Changed: b1f480edd0ed9a452dbdd7950b820afb7158cabd. There's a couple places in this file where I'm actually still using Matchers for testing the structure of collections. I think they're more expressive than assertions & that code would get a bit more complex with just assert. Ah, yeah, I forgot to remove unused imports. My bad.. I think IDEA adds those automatically - I'll have to find the config to make it not do that. . ~~If I recall correctly, it wouldn't compile without that annotation? I'll double-check though.~~\nNever mind, it was removable. Fixed.. @adleong re certs.map {, I wasn't sure about that one \u2013 I know we avoid using infix notation, but I thought it was generally preferred for combinators like map, flatMap, etc, so I wasn't sure. Thanks for clearing that up.. Whoops, that was IDEA's fault; it looks like the default config in my new install automatically reformats code on commit. Gotta turn that off!. @olix0r, Alex & I discussed this, and we weren't sure if this ought to go above or below the failure accrual module in the client stack. What do you think?. \ud83d\udc4d yeah, that makes sense.... I asked Oliver about it & he says we probably should just add the FramingException to the exceptions that the classifier knows about, so I can do that.. Hmm...so, should I just make this return a 503, or wait for @olix0r to see this?. @olix0r Okay, that sounds like a plan. While I'm at it, do you think there are other places we ought to refactor to use a new ProtocolError type?. (I'm calling it ProtocolException rather than ProtocolError, since IIRC the Java/Scala convention is for Errors to be fatal errors, while Exceptions are recoverable). @olix0r, what do you think about the current iteration of this (824c06ea189b93198feaef36940c521b05487b7)?. I'd like to rewrite these tests to run with both the clientModule and the serverModule, but I don't think it's terribly important.. fixed 1bc8676a02a62f1db3187c835ddbe0ada0386c73. And you'll be glad to know I found the IntelliJ pref to turn that off. I can do that, but do you think the exception ought to have information specific to the request added to the message, instead?. As it's currently written, the flatMap has to be before the rescue/handle, since it changes the type from Request to Response \u2013 that's why changing it broke the build in 7dd76a64c7c1246b1243683a76a212322f7eacb4 (sorry).. I think I can rewrite this to just use transform though.. Okay, fixed 73cba51fe755d9072cd68805cb564406a2de6b5b. (Turns out FailureAccrual is lower). Okay, looks like StatusCodeStatsFilter is actually at the top of the stack, and RequestStats towards the bottom. Fixed in 33d05c3c3b255f44fb42b05ff119f88afc51bc19. agh. Ah, I wasn't sure \u2013 I thought that was what the closedId was for?. Hmm. I could either make this completely silent, or have it log to debug if that seems more correct?. (an alternative, and perhaps more principled, solution to this issue: \"we submit patches to grpc-go and the Python grpcio package fixing this behavior, but I think that would take work). Okay, logs as debug in d65c1dd5c3b60e6df34b06ed0408b1f96e4d488f.. Fixed d65c1dd5c3b60e6df34b06ed0408b1f96e4d488f. oh, you're right! l'll remove that, thanks. @adleong I suspect you're probably right. I wasn't really sure whether the commented-out code here was correct, so I was hoping to get review feedback on it.... Moved in 4b90419fa1b953ed483433ed353b49488c983584, but a quick code search indicates that we never seem to actually use maybeTransform anywhere?. Fixed 52c3f8dc88b4a9c24bf866733590318543b5a923. Fixed 555937d3c380dd15a90587e8c2f4740f8881d0b6 :). can confirm, about all I've learnt from a linkerd stack trace is \"we use netty\". \ud83d\ude22. Ask @klingerf \u2013 this is his code, untouched by me.. Hmm, not a bad idea, trying to figure out how that would look (once again, this bit of code was written by @klingerf).. :0. @klingerf yeah, I think the main reason I didn't touch too much of your code was that I didn't really understand what some of it was doing (c.f. stateWithClose, which seems like it uses a bunch of finagle stuff I wasn't really familiar with). I'll see if I can refactor some of this & hopefully make it clearer?. Done 4463b6699c70ef2a77da6d0abbbbc3fb0c3dc3cb. Okay, I've factored out the Activity code from ConfigMapInterpreterInitializer, IngressCache, and K8sDtabStore and moved it all to Watchable; let me know what you think of this PR as of 14191e996b9621a7468eaf8c3ab8c29fb2d28120?. I think returning an Activity[T] here means that I could theoretically move the Watch event unpacking here and have this return an Activity[W] instead, and move onEvent and the flatMap to the call site, but I'm not sure if that's a better or worse API to expose, though. I feel like it might just result in more code repetition.... Oh, gosh, that's...really bad! Glad you caught that! \nI think this bug came about because I was expecting Rust \"iterator adapter\" behavior here \u2013 both combinators would be called on every iteration \u2013 and I forgot Scala's iterators had different semantics. I rewrote it to use scanLeft because I  thought passing onEvent points-free was more aesthetically pleasing, but I'll definitely change it back!. Ah, you're right \u2013 I overlooked that.. in Haskell, we tend to pronounce this as state'' \ud83d\ude1b. yeah, that seems correct \u2013 I was wondering about that, just wasn't sure if I should change the behavior of the existing code. \ud83d\udc4d. ...I copied the comment from ResponseClassifier & forgot to delete that?. \ud83d\udc4d I kind of thought so...was hoping you'd confirm in code review.. It was point-free for a while & changed because I thought it needed to free the frame after, realized that was wrong, and this never got changed back :). probably not \u2013 I think IntelliJ makes these, and I've committed it accidentally.. Are these used elsewhere? If not, I can go ahead and delete them.. This is just my thought process out loud, I guess  \u2013 iteration one of StreamStatsFilter had all of the stats floating around as instance variables, so I was conceptualizing this as sort of being like a closure over them?. I'll see what I can do!. Yeah, I think it's this way primarily because classifiers can match on Return(_) instead of having to have separate cases for Some(Return(_)) and None, but the semantics of having it be like that is a little wonky.. Oh, you're probably right \u2013 I'll remove those.. Done 6460c2e2d3f0dffc9a32298e3aaa16ff1932d017 \u001f\u2013 I think this should be merge-able whenever?. Okay, I'll keep that in mind for the future, thanks.. good call!. Yeah, it was like that for a while, but then I changed these to lazy vals based on the observation that we probably wouldn't have to allocate both strings. @adleong quashed that & I never changed it back.... I think Scalariform decided it should be like that?. I think I called it that because I wanted to use the identifier listName somewhere else, but that may be vestigial \u2013 I'll check.. probably not, but who knows?. So the types of some of the defs in this trait can be polymorphic \u2013 H2SvcConfig overrides this type with h2.ResponseClassifier. Yeah, that's why I changed it from bytes to total_bytes - I'll probably remove the data_frame namespace (the other stat in that namespace was removed so it's kind of meaningless anyway), does that seem clearer? . Whoops!. I try to avoid using the ._n syntax for tuples \u2013 generally, I feel like it reduces clarity. Though in this case I suppose it's pretty clear.. @olix0r I (very briefly) had these methods as lazy vals instead of defs, but I figured it would make Alex mad. Since this is in the hot path, and the parameters are vals, can I pleeeease? :). Fixed 1bb5bc842cb6da4866c91ab42a54c94345e202f2, my bad. sure, I was just using it to debug which tests were failing. \ud83d\udc4d . yeah \u2013 I was going to add one, but the conventional name for it, responseClassifier(), clashes with the responseClassifier() def & I didn't want to rename that? I suppose it could be called getResponseClassifier but that doesn't seem v. idiomatic?. yeah that's probably a good call. What about renaming this field to responseClassifierConfig? Although it's a little wordier, I feel like it makes the difference between this field and the responseClassifier() method clearer, and allows the getter and setter to have more idiomatic names?. ClassifiedRetries.Default is of type http.ResponseClassifier, not h2.ResponseClassifier. This could make ResponseClassifiers.Default which should have that behaviour?. agh, sorry. is it alright if I add build.properties to the .gitignore? IntelliJ seems dead-set on leaving these everywhere.. for the optional properties, should I be asserting that the option is not null, or that the option isn't Some(null)? . oh, okay, so this is just for the non-optional properties. got it!. whoops I did not mean to commit this. Ah, okay, so that's not actually checking for null, gotcha. Good to know. :D. ah, yeah, that's much better, thanks!. If we have separate H2ReqRep and H2ReqRepFrame types, is there any reason for this to still be an Option?. This field should probably be renamed to h2Classifier if the class is being renamed.... We probably want to include tests for the stream classifiers as well, right?. Is there a reason these tests were removed instead of being rewritten to use stream classifiers?. TIOLI: how do you feel about overloading apply(H2ReqRep) and apply(H2ReqRepFrame) so we don't have to explicitly call streamClassifier and responseClassifier?. (IMO that better leverages the type-level distinction between ReqRep and ReqRepFrame).... Curious to know if there's any particular rationale for not just using (or extending) ReqRep here?. Any reason the responseClassifier def can't just have the type ResponseClassifier? That might let us re-use some existing code for classifying without streams?. I like your work on #1545 and I think that seems right \u2013 I'll happily make that change when we land #1545.. Yeah, you're right, that won't work, I wasn't thinking. I'm unsure what to do if the final frame is not a Trailers frame \u2013 you're right that it's a pretty weird case, but I'd like to cover it nonetheless.  Unfortunately, we can't just add a Trailers frame from inside of the onFrame method.... I'm wondering if we can add a method that is to onFrame as map is to foreach.... Yeah, I just can't think of any other way to handle that case?. Will failing to set the l5d-success-class header cause problems elsewhere?. Fair enough!. lazy val! lazy val! lazy val! \ud83d\ude34 . The H2ReqRep extractor may not be the clearest way to handle this, you're right. I wanted streamClassifier to have a way to fall back to the responseClassifier if it gets a H2ReqRepFrame with an empty stream, which was happening...somewhere (can't remember precisely atm) and causing some of our existing tests to break.. This case is for handling responses with no response stream \u2013 otherwise, a bunch of responses that should be considered successful were being marked as failures.. I was considering that, just wasn't sure what the correct way to handle the error was in this case.. I'm not sure if this will work like that \u2013 in order to call responseClassifier.isDefinedAt, I'd have to take the Response and wrap it in a H2ReqRep, and whether or not the response also has a non-empty stream, the responseClassifier will be defined.... \u261d\ufe0fsee above. You mean like this? . I think I only removed that because it seemed overengineered at the time, but there might have been a reason it didn't work that I'm forgetting. I can possibly see about re-adding it.. ag, you're right, that's why this didn't work \u2013 Response has a Headers, and I must've somehow gotten it into my head that it is a Headers. \ud83d\ude44 . Are these tests based on Finagle's tests for FailureAccrualFactory?. It would be really nice if there was a way of doing this other than completely duplicating FailureAccrualFactory, but because didFail is private[this], I get that there may not be an easy option.... Do you think we should just reuse the stat definitions in FailureAccrualFactory?. Any reasons we can't extend FailureAccrualFactory to at least reuse the protected defs?. TIOLI: why not make this protected here, since this being private[this] in Finagle's FailureAccrualFactory caused trouble? we probably won't have to extend this class, but maybe we should be future-proof nonetheless?. TIOLI: personally, I think classifier.responseClassifier.lift(h2ReqRep).map{...}.orElse{...} might be clearer than this match; your mileage may vary.. wondering if there's an easy way to collapse these match arms...probably there isn't.. Very minor quibble, but it seems to me this should be H2FailureAccrualFactory.role, just in case the parent's role changes?. \ud83d\udc4d. I wrote it like this so that we could tack a trailers frame on the end of a failing stream, but if we don't want to do that, you're definitely right.. scalariform did this. well, I certainly don't remember doing this. at least not on purpose. that's fair, it also (probably) compiles to more performant bytecode?. I was wondering about this myself for a hot second.. oh, good point, hadn't thought of that.. I mean, it's about three lines of boilerplate, right? Or is there something I'm missing?. Oh, yeah, I guess you're right.. @adleong isn't the annotation still needed, though, but on H2ClassifierConfig instead?. Hmm...I decided to override isEmpty because it seems to me that a Stream probably shouldn't claim to be empty if read()ing from it will yield more frames? If q is non-empty, read() on this stream will yield frames, even if the underlying stream isEmpty. Which seems...surprising, if not actually incorrect?. If I recall correctly, there was some test that wasn't working correctly w/o this case. I can double check and see if it's removable?. Oops.. Nevermind, I was completely wrong, this is removable.. okay, will change this to just document that a stream may be \"empty\" but still return frames.. Oh, huh, don't know how that happened.. agh, can't believe I missed that.. this is much clearer, I should have written it that way :). TIOLI, but as far as I can tell, the renaming import shouldn't be necessary here?. In general, I think our convention is to rename imports only when necessary. If you were importing both finagle.http.Request and a type from Linkerd called Request, then I think we would rename the type from Finagle. But in most of our code that I've seen where we don't have to deal with namespace clashes, we tend not to rename imports by default.. Can't believe I missed that...thanks for fixing this, @pcalcado !. I went back and forth on that for a while and ended up using the name from the previous code. Can definitely rename it. . I was wondering if something like that existed, but all I could find was stuff from scalaz which I figured you probably wouldn't approve of us depending on \ud83d\ude43. aren't nested defs always restricted to the scope they're created in, or am I missing something?. ServiceEndpoints is a nice solution to the problem of having a whole million different things in this file whose names are some variation of \"service\" or \"endpoints\" \ud83d\ude1b. was considering that, too lazy. will add. . \ud83d\udc4d. yeah, my initial plan was to save that for a separate PR?. okay, I can change it \u2013 for my education, are there any particular reasons why this is considered harmful?. hey, that was a pun, I understood that! and yeah, I blame IntelliJ. the concrete namer implementations use it, I moved it to the EndpointsNamer class because I was planning on trying to factor out more of lookup from the concrete classes.. that's fair....would still like to factor out some stuff from lookup but it is probably less possible than I originally thought.. whoops, my bad. Oh, yeah, that's a lot clearer, thanks.. \ud83d\ude22 \n(but you're right). Yes, the changes in 5bea97b65964b5b301f91ffbb99642741bef743a (which are necessary to make one of the tests for ServiceNamer pass) broke this.. Just remembered that Function.untupled is a thing, so, yeah, there's no reason to have this.. oh, yeah, definitely.  do we always prefer explicit type annotations on private functions?. I feel like this is the only thing in this PR that isn't immediately clear to me, but I'm not sure if I can think of any clearer way to express it.. Yeah, I was able to figure out what was going on here after looking at it for a minute, I just meant that the control flow with selectIndex is a little hard to follow at a glance?. How would you feel about something like\nscala\nval timeoutF = \n  // Classification timeout.  Immediately return a classificationTimeoutException but\n  // ensure the final frame is eventually released.\n  Future.sleep(deadline - now).onSuccess { \n    frameF.onSuccess { f => f.foreach(_.release()); () }\n    Future.exception(classificationTimeoutException)\n  }\ntimeoutF.or(frameF)\ntake it or leave it, of course.. Would be nice if this test was more comprehensive... (though the function under test is simple enough it probably doesn't actually matter). It looks like this addition is causing compile errors in ConsulNamerTest (which is what's currently breaking the CI build), since the test creates some ServiceNodes without passing an argument for the Status field. \nWe'll want to add the Status field to the ServiceNodes here \nhttps://github.com/linkerd/linkerd/blob/ef38c6ad20071a57294bae863923aae99572b5dd/namer/consul/src/test/scala/io/buoyant/namer/consul/ConsulNamerTest.scala#L15-L32\nso the test can compile.. I'm not against using Enumeration and a custom deserializer here, but just for reference, I think we've generally represented JSON enums using a sealed class and JsonSubTypes, as in: \nhttps://github.com/linkerd/linkerd/blob/a9c1627d389fab206faac44d1f674695207ff5cd/namerd/storage/k8s/src/main/scala/io/buoyant/namerd/storage/kubernetes/DtabWatch.scala#L7-L23. Ah, I see you've already mentioned that in your PR, sorry \u2013 haven't had my coffee yet.. yeah, I realized that: d215b9838261a022bc8ad9764812c9ac1fdd20a0 :). I think Enumeration is probably the right call in this case, just wanted to put that out there. . I think anything that makes NameTree.Fail probably ought to be logged at the warning level.. I'd feel somewhat more comfortable if tests that create external network connections went in integration rather than unit tests.. Take it or leave it: consider com.twitter.util.Memoize as an alternative to map.getOrElseUpdate.. nit: maybe put the level, payload size, etc, in a constant?. :) \ud83c\uddeb\ud83c\uddf7. Set experimentalRequired = true in DnsSrvNamerInitializer. See (for example) K8sNamerInitializer:\nhttps://github.com/linkerd/linkerd/blob/5d22b56489c4e646672e5d7fce6ef799d7574397/namer/k8s/src/main/scala/io/buoyant/namer/k8s/K8sExternalInitializer.scala#L34-L35. I think rather than making the namer/k8s project depend on istio, it might make more sense to move the Istio namers to a new namer/istio project?. Same comment as above.. Yeah, I think I'm inclined to agree w/ @adleong on this \u2013 I feel like just replacing every dependency on k8s with istio doesn't really accomplish the goal of this changeset (i.e. to decouple our Kubernetes and Istio modules)?. Can we add a comment here indicating that please?. Our preferred code style tends to discourage the use of whitespace-delimited function calls. Prefer:\nscala\nresult.filter(statuses.contains(_.Status.getOrElse(HealthStatus.Passing))). TIOLI:\nscala\nval checks = \n  for {\n    checks <- health.Checks.toSeq\n    check <- checks\n    status <- check.Status\n } yield HealthStatus.withNameSafe(status). rm parens from name.toLowerCase() here please \u2013 parens on a zero-argument function should be used to indicate that the function is executed for side effects. Take it or leave it, but I feel like these three tests (\"HealthApi supports filtering by health status {warning, passing, critical }\") could be made DRYer, maybe like this:\n``scala\ndef filterTest(status: HealthStatus, name: String) = \n  test(s\"HealthApi supports filtering by health status$name`\") {\n    val service = stubService(nodesWithChecksBuf)\n    val api = HealthApi(service, Set(status))\nval response = await(api.serviceNodes(\"hosted_web\")).value\nassert(response.size == 1)\nassert(response.head.ServiceName == Some(\"hosted_web\"))\nassert(response.head.Node == Some(s\"node-$name\"))\nassert(response.head.Status == Some(status))\n\n}\n``. I considered that, but it seemed unwise to assume _all_ClassCastExceptions were due to a mismatched request protocol. If aClassCastException` was thrown for some other reason, logging the protocol error message would obscure that failure...\nWhat about a test that the message contains \"Transport.cast failed.\"?. Very minor, but: would like to see a test that the config entry without a path configuration uses the correct default path.. ah, yeah, this was added for debugging purposes and should have been removed, my bad!. yeah, added a test for this (183d08834fe75ed9832ac8da3d1f7d91aa06897a) but evidentially forgot to push it up? anyway, rectified that.. Ah, yeah, that makes a lot more sense, I really wish I had thought of that.. but Alex, DRY! :(. Hmm, this changes the semantics somewhat: if we want to fall back to the previous dtab on an error, rather than returning Dtab.empty, the error needs to at least propagate up to Activity.onEvent, where the previous dtab value is known.... Yes, but api is only referenced once, and leaving this as a val causes the test interpreter to throw a NullPointerException.. That fixes it???? I didn't even know you could extend { .. }, I thought you could only do new ConfigMapInterpreter(...) { ... }. The \"g\" in gRPC apparently no longer stands for Google :). Sure!. Yeah, I've wanted to rewrite the logging code for a bit, thanks for reminding me!. Yeah, this class was changed to use the NewState extractor, and then it was changed back, but I had restructured the pattern match slightly. Can just revert.. Ah, yeah, I thought I removed that. Whoops.... ah, I did not mean to commit this - I had made some changes locally for testing.. do you think this will necessitate a 1.3 release (since it's breaking configs?). Hmm. I thought it was worthwhile to encourage users to upgrade their Kubernetes version, so that we can eventually remove this workaround? I suspect it adds at least a minor performance hit.. That's definitely better than string comparison from a performance perspective. I felt like inspecting the message might be more future-proof \u2013 what if Kubernetes were to change the status code returned for this event, or added additional events with status code 410 that should be handled differently \u2013 but if you don't think that's as big a concern, I can change this?. Yeah, I'm not really a big fan of the trait X/class XBackedByApi pattern that shows up a couple times in this PR. Elsewhere in Linkerd, when we need to mock an external API for testing, I think the general pattern is to inject a mock API with overrides, like this. Like Alex said, this would exercise more of the implementation, and I think it also results in a less complex non-test codebase?. this might be overly nitpicky, but can we insert some line breaks here? I think lines this long are hard to read especially when my editor wraps them weirdly.. once again, I'd really like to see some more line breaks in this file. I'd find something like\n```scala\npackage io.buoyant.k8s.istio\nimport com.twitter.util.Duration\ncase class IstioResponseResp extends IstioAttributesHolder {\ndef responseCode: ResponseCodeIstioAttribute = \n    ResponseCodeIstioAttribute(statusCode)\ndef responseDuration: ResponseDurationIstioAttribute = \n    ResponseDurationIstioAttribute(duration)\n}\n```\nmuch more readable. \nNote that this is entirely a matter of personal taste & I won't hold it against you :). Is there any way to do this w/o casting?. This whole function seems really convoluted, is there any way it can be simplified or broken apart?. is there a reason for the capitalization in istioREquest?. Also, TIOLI, but it seems to me that a lot of this function might be better represented as a series of functions on IstioRequest?. third option: ClusterCache with Api (Alex will hate this :D). Ah, you're right \u2013 I think I'll have to rewrite the whole event logger. Which is fine, this implementation is kind of ossified.. I'm working on actually fixing this, but would be fine with closing this PR & opening a new one?. ah, you're right, whoops.. Oh, okay, that'll make this a lot simpler, then.. noooooo, the horror. an (unnecessary) move for changes I didn't end up making. Didn't mean to commit this.. Hmm. What about a function on IstioAttribute taking an attribute type and optionally returning that attribute if it is of the given type?. Another potential concern here \u2013 which might be a premature optimization \u2013 is that the repeated calls to filter mean that we traverse the sequence of attributes 4 times. \nAs an alternative, consider using mutable collections (gasp!) and traversing the sequence a single time and appending the attribute to a different collection for each attribute type? If this is in the hot path, it might be worth using mutability. . Alternatively, if you really want to avoid mutable collections, could always do something like this\nscala\nval (stringAttrOpts, int64AttrOpts, stringMapAttrOpts, durationAttrOpts) = \n  attributes.map { \n    case attr: IstioAttribute[String] => (Some(attr), None, None, None)\n    case attr: IstioAttribute[Long] => (None, Some(attr), None, None)\n    case attr: IstioAttribute[Map[String, String]] => (None, None, Some(attr), None)\n    case attr: IstioAttribute[Duration] => (None, None, None, Some(attr))\n  }.unzip\n// the resultant collections are now `Seq[Option[IstioAttribute[T]]]`, which you'd then have to `.flatten`...\nwhich is kind of ugly, but is purely functional!. Should this return a finished Future, or return a future that's satisfied when the report stream finishes?. fair enough - I just have a pathological dislike for writing the same code twice. I'll simplify it.. I like that alignment style for parameters, but most of the rest of the Linkerd codebase puts parameter lists on the next line, one tab level in, like\nscala\nclass Foo(\n  a: A,\n  b: B\n) extends Bar {\n...\n}\nrather than \nscala\nclass Foo(a: A,\n          b: B) \n  extends Bar {\n...\n}. I agree that we definitely ought to have a shared config file for formatting. Do note that, while we don't have one you can plug into IDEA at the moment, running sbt test will also run Scalariform on everything.. yeah, this is matching the case where the value contained in the Some is equal to the value of oldValue, not shadowing the binding of oldValue to a new value.. (how many times can I say \"value\" in one sentence?). this phrase gives me unpleasant FORTRAN flashbacks. argh. i thought i fixed that.. yeah, was going to mention on your earlier comment that if this wasn't immediately clear, it probably ought to be changed.. good call!. Yeah, I just wasn't sure, since you'd originally written it that way. Happy to make this the default behaviour\n. Added tests for initializers in 7fdfa1d, will add tests for telemeter reporting as well. looks like Scalariform modified this and I committed it by mistake. will fix.. I originally had wanted to test using Time.withCurrentTimeAt but that wasn't practical in the e2e test; that seems like a good compromise.. Yeah, you're right; 0208ec44f22688d16fd14786916f25736b78daf4 should be more correct.. Smart!. yeah, i just thought that \u2013 since Scala's visibility modifiers can be confusing \u2013 it was clearer to make the scope explicit?. What exactly is the purpose of the empty companion object here?. I don't think it's correct to replace the existing ThirdPartyResourceTest with a new CustomResourceTest, since we will need to continue supporting third-party resources as well in order to remain backwards-compatible. I think that rather than replacing the test, we ought to have separate tests for TPR and CRD.. I'm not a fan of this naming. I don't think V2 is very descriptive \u2013 I think the class name should make it clear that the DtabStore created by this config is the CRD-based dtab store. Consider K8sCrdConfig?. Similarly to my above comment, I think this should be io.l5d.k8s.crd. We want the config id to make it clear that this is the CRD dtab store. Also, I think \"k8s\" should come before the part of the config id that designates what type of dtab store this is, since it's less specific.. Once again, since not all users will be immediately upgrading to a version of Kubernetes that supports CRDs, we need to support both TPRs and CRDs. Thus we can't just replace the third party resource code, we need both.. I think this Future.Unit should be removed \u2013 shouldn't the value returned from this function just be the result of the flatMap expression above?. Shouldn't the case where the dtab doesn't already exist be handled by creating it, rather than doing nothing?. I think whether CRDs or TPRs are used reflects both the implementation of the object and its purpose.. @esbie yeah, @deebo91 explained to me that the APIs are equivalent, so I retract that comment. I do think it ought to be documented in that these are equivalent, though.. Oh, okay, I didn't realise this was configurable; I looked at the config classes, but I was looking in the wrong place (H2ClassifierConfig). I'll update the docs. Do you think this warning should go next to where we describe those config fields instead of where it is at the moment?. whoops. Maybe add backtick () characters around the former links so they'll be typeset as code?. A comment summarizing this function's behaviour might be nice.. Take it or leave it, but I think this _might_ be a little clearer if we collapsed the nestedmatchexpression into onematch` with guards, like:\n```scala\n// special case to handle Kubernetes bug where \"too old\n// resource version\" errors are returned with status code 200\n// rather than status code 410.\n// see https://github.com/kubernetes/kubernetes/issues/35068\n// for details\ncase Some((e: Watch.Error[0], ws))  if e.status.code.contains(410) =>\n  log.debug(\n   \"k8s returned 'too old resource version' error with \" +\n     \"incorrect HTTP status code, restarting watch\"\n  )\n  _resourceVersionTooOld()\ncase Some((event, ws)) =>\n  import Ordering.Implicits.\n  if (previousEvent.forall((: W) < event)) {\n    state(Activity.Ok(event))\n    _processEventStream(ws, event.resourceVersion, Some(event))\n  } else {\n    // Ignore any events where we receive a resource version lower\n    // than the last received resource version.\n    _processEventStream(ws, resourceVersion, previousEvent)\n  }\n// if the stream ends (k8s will kill connections after ~30m), restart it.\ncase None if resourceVersion.isDefined => _watch(resourceVersion)\n// In this case, we want to try loading the initial information instead before\n// watching again.\ncase None => resourceVersionTooOld()\n``. SinceIncreasingOnlyAsyncStreamwas removed, I'd like to see another test somewhere that asserts that the events onWatchable.Activityhave monotonically-increasing resource versions even if the Kubernetes API sends events out of order.... Thanks for flattening the match arms like I asked, this looks great. The rephrased comments are much clearer too!. Thanks for adding this test!. Very minor, but I notice the type of exception here was changed fromIOExceptionto genericException, which is a loss of specificity --- is there a more specific exception that can be set here, or do you thinknew Exceptionis the most correct?. @carloszuluaga I agree thatIOExceptionis not the correct type here, I'm just wondering if there's something that conveys more meaning thanException.. This looks good to me \u2013 I might consider having separate subtypes for the \"lookup errored\" and \"lookup contained no SRV records\" cases, but that's bikeshedding at this point... :). - [x] can we add a comma after \"carloszuluaga\" here? like \"Special thanks to @sgrankin and @carloszuluaga, just to name a few.\"\n- [x] can we turn the GitHub usernames into Markdown links?\n- [x] once the blog post is live, can this link to it?. can we turn the PR issue numbers into Markdown links to those PRs?. This looks like an unrelated change (although the new indentation is more correct... \u00af\\_(\u30c4)_/\u00af). same as above, unrelated formatting change. move this to a separate PR?. lgtm \ud83d\udc4d . TIOLI but it would be nice if these test names were more descriptive than just the name of the method being tested.... Real minor, but: it might be nice if this exception also told me what the invalid characters were. Like,\"the dtab namespace in&val^id*nam@space contains invalid characters: &, ^, * @\"` or somethinbg.. I wonder if this is the right place for this validation. Do we want to apply this validation to _all namespaces, not just Consul ones? \nIf that's the case, consider the following: right now, Ns is a type alias for String:\nhttps://github.com/linkerd/linkerd/blob/dacdb21729cbc1152ad715085be42b53618c2703/namerd/core/src/main/scala/io/buoyant/namerd/package.scala#L4\nWhat if we were to change Ns from a type alias to a new type, and require that in order to construct a Ns from a string, it has to go through this validation step? Like\n```scala\nclass Ns \n  // private constructor so you can't just make a new Ns with an arbitrary string`\n  private[Ns] (val ns: String) \n  // see https://docs.scala-lang.org/overviews/core/value-classes.html\n  extends AnyVal\nobject Ns {\n  private[this] val validNsRegex = \"[\\w+\\-?/?]+\"\n  def apply(s: String): Try[Ns] = \n    if (Pattern.matches(validNsRegex, s)) Return(new Ns(s))\n    else Throw(new InvalidNsException(...))\n}\n```\nThat way, any method taking a ns knows the namespace has already been validated.. tioli, but can we use the scala stdlib's regex stuff instead?. Thanks for adding a section to the docs!. since PortMapping is a case class, making its fields vals is extraneous; the parameters to a case class are always public vals.. Yeah, that's fine too \u2013 I think if we're going to have restrictions on what characters are allowed, they should be in the namerd docs as well.. Also, if we want a valid name to consist only of canonical DNS label characters, we already have a regex for matching them: \nhttps://github.com/linkerd/linkerd/blob/642e14fd7e7f0d961a023cbf55ecaa23b070f649/namer/core/src/main/scala/io/buoyant/hostport.scala#L12-L20\nWe could put this regex someplace where it's publicly visible and just reuse it.... Oh, also, TIOLI, but if you use a raw string you won't have to escape the \\ characters in your regex, which might make it a little more readable.. At the very least, you can split the mapping a single time and destructure it:\nscala\nstring.split(':') match {\n  case Array(ipString, privatePortString, publicPart) =>\n      publicPart.split('/') match {\n         case Array(publicPortString, protocolString) => \n           // here i would `Try` the calls to `toInt` and handle the cases where the string\n           // couldn't be parsed, instead of throwing an exception...\n         case _ => // handle the case where the split returned something other than a 2-element array\n      }\n  case _ => \n    // handle the case where `split` doesn't return an array with three elements \u2013 I would \n    // make the method return an Option and return None here\n}. Honestly though, Scala's regexes are nice enough that i would probably end up doing something more like this:\nscala\nval Mapping = raw\"([0-9a-zA-Z]+):([0-9]+):([0-9]+)/([0-9a-zA-Z]+)\".r\ndef this(mapping: String) = \n  mapping match {\n    case Mapping(ip, privatePort, publicPort, protocol) => \n      this(ip, privatePort.toInt, publicPort.toInt, protocol) \n      // will still throw an exception if the calls to `toInt` fail - but they shouldn't, \n      // since the regex already ensures that these parts of the string contain only\n      // numerals...\n    case _ => \n      // handle the case where the input doesn't match the regex. Should this be configurable?. I made it an object because I was concerned that if it was a singleton val the equality might still get munged, but the type test would still pass?. I think that for a case class, rather than providing a constructor, you probably want a companion object with an apply() method.. Correct me if I'm wrong, but the _resourceVersionTooOld function is now being called due to errors other than just the resource version being too old? \ud83e\udd37. maybe it needs a different name.... yeah, was considering that - will do.. WHOOPS my bad, I forgot the task name. is ~/bin always on the path?. hmm \u2013 and forgive me if i'm entering into bikeshed territory here \u2013 it seems to me that if ~/bin isn't necessarily on the path, we'll need to either add it to the path, or add a special case to check whether h2spec is installed in that location?. yeah, i didn't think we'd want to have the script modify the path either, but seems like checking if it exists in ~/bin complicates this logic a bit... happy to add that though. This lgtm \ud83d\udc4d . would be cute to use ScalaCheck here & generate an arbitrary status code...definitely not necessary though.. Should the default value removed from the constructor be added here? (i.e, \nscala\ndef apply(\n  c: Client,\n  backoff: Stream[Duration] = Backoff.exponentialJittered(1.milliseconds, 5.seconds)\n): KvApi = new KvApi(c, s\"/$versionString\", backoff)\n. this would also make it no longer necessary to change all the tests in KVApiTest.... I'm not sure if I'm a fan of having this live in the namer package, since I'm pretty sure the backoff config is used in places other than in namers? I think maybe it should be moved someplace where it's visible to the namer module, but not necessarily into that package?. Was the default backoff (for KvApi) not previously exponentialJittered rather than decorrelatedJittered? Is there a motivation behind this change?. Yeah, I saw that after scrolling through the rest of the PR; disregard that.. Glad this could be removed! \ud83d\ude4c . I think this renaming makes the tests a bit clearer, thanks! \ud83d\udc4d . Ah, no, this is an unintentional change \u2013 I added a log line in this match arm while debugging, and then removed it, but forgot to remove the line break. I'll remove this change, thanks for catching it!. Added some docs in b891c92813823fb988a3ce3100421df66d1449cf that should help explain how this is useful \u2013 let me know if you have any further questions!. I will \"add\" an \"emoji\". I believe the referer header field may be present on h2 requests, so this probably ought to be \nscala\nreqHeaders.get(\"referer\").getOrElse(\"-\"). I think it would be good to log something for the remoteHost field, but as far as I can tell, this would probably require changes in h2.Request to store the remote address like Finagle's HTTP Request does, so it's reasonable to punt on that for now.. As we discussed, i think that this header is not always set, but can be present depending on the request --- I think we should attempt to log it if it is present.. nice, i didn't know that existed!. expression languages are wild. TIOLI, but I think it would be nice if this log line included the following information:\n+ the path of the watched resource\n+ the received resource version\n+ the previous resource version\nIf we log all this information, it might not be necessary to log the entire event as well (which can be somewhat wordy). Consider something like:\nscala\nlog.debug(\n  \"k8s watch on '%s' skipping event with resource version %s (older than most recent resource version %s)\",\n  watchPath,\n  event.resourceVersion,\n  largestEvent.map(_.resourceVersion)\n)\nor similar.. Note also that the sprintf-style log formatting (%s and friends) is preferable over Scala string interpolation ($name etc) in twitter.util.logging, as it's only evaluated if that log level is enabled, which will have a reduced performance impact. See: https://github.com/twitter/util#using. TIOLI, but I think this can probably be logged at the trace level. Also see my below comments on logging. . @negz totally agree --- I thought I said something about that in my original review of this PR, but I think I forgot to mention it.. In this case, I think the doWatch promise is unnecessary --- in EndpointsNamerTest, we use those Promise to allow the test body to signal to the mock API service when a particular event should be sent, but here you immediately set the doWatch promise to be done inside of the mock API service. Since doWatch is done, the line\nscala\ndoWatch before rsp.writer.write(Buf.Utf8(watchResponse))\nshould be equivalent to just\nscala\nrsp.writer.write(Buf.Utf8(watchResponse))\nso I think you can remove the doWatch promise entirely.. Thanks for adding this test!. Very minor, so feel free to take it or leave it, but I would probably try to find a way to phrase this without the .get call, maybe\nscala\nassert(await(cache.matchPath(host, \"/linkerd-2\")).contains(_.svc == \"echo2\"))\nor importing org.scalatest.OptionValues and using \nscala\nassert(await(cache.matchPath(host, \"/linkerd-2\")).value.svc == \"echo2\"). Yeah, I believe that's true --- this should probably be an ADDED event. Since Linkerd handles those events in essentially the same way, I just wanted to reuse the existing test data, but it would probably be more accurate to send an ADDED event here.. \ud83d\udc4d. Do you think it would better express intent to, say, bundle current and exists into an object and synchronize on that, instead?. Is the lastGood variable still necessary, or can we just use the current state of the activity for logging the last good state?. Passing the log level so that it's changed to debug on subsequent calls is very elegant, I like that \ud83d\udc4d . I definitely agree that logging the state is likely to be useful, and I think you're right that there's no easy way to avoid having the lastGood variable --- just wanted to make sure there wasn't an obvious way to remove it.. As we discussed previously, releasing the frame here was almost certainly incorrect. This LGTM \ud83d\udc4d . Were these imports unused? This looks like an unrelated change at a glance?. Would be nice if there was a short comment here explaining why this is different from calling q.fail(..., discard = true) and why it's used instead.. Seems like there's some duplication between this function and the failAndDrainQueue function in io.buoyant.grpc.runtime.Stream; is there any reasonable way to factor it out so we can use the same code in both places?. Unrelated change?. Oh, I didn't realise the functions were operating on different types; carry on, then!. Non-blocking, but it might be nice if this repetition could be reduced?. It looks like the reordering of these imports is unrelated --- can we back off the formatting change to a separate PR?. Nit: It's generally considered good Scala style to omit parentheses from arity-0 method calls unless the method is side-effecting: https://docs.scala-lang.org/style/method-invocation.html#arity-0. Nit: would prefer not to use parameters with the same name as the type of the parameter. Can this be made more descriptive or at least less repetitive?. Nit: I would have omitted the \"get\" from the name of this method: https://docs.scala-lang.org/style/naming-conventions.html#accessorsmutators. Is this ever mutated, or can it be a val?. Is there any way to implement this without using asInstanceOf, or is that necessary because we're interacting with a Java API?. TIOLI: rather than \nscala\nmatch somethingThatCanBeNull {\n  case null => ...\n  case thing => thing\n}\nyou could also say\nscala\nOption(somethingThatCanBeNull).getOrElse(...). Do you think that it would be worthwhile to encode the states a stream can be in as separate types rather than as a field? That way, we could encode valid state transitions at the type level. Consider something like this:\n```scala\nsealed abstract class H2FrameStream(override val state: Http2Stream.State) extends Http2FrameStream\nobject H2FrameStream {\n  case class Open(id: Int) extends H2FrameStream(Http2Stream.State.OPEN)\n  case class Closed(id: Int) extends H2FrameStream(Http2Stream.State.CLOSED)\n  case class LocalClosed(id: Int) extends H2FrameStream(Http2Stream.State.HALF_CLOSED_LOCAL)\n  case class RemoteClosed(id: Int) extends H2FrameStream(Http2Stream.State.HALF_CLOSED_REMOTE)\n}\n``\nThen you can nicely pattern match on the variants without having to call.state, but you still get thestateandidaccessors that are required to implementHttp2FrameStream`.. According to the state transition diagram in http://httpwg.org/specs/rfc7540.html#StreamStates, I believe sending or receiving a reset should always transition the stream to CLOSED.\nTIOLI: if CLOSED is the only valid transition on a reset, consider encapsulating writer.reset in another function that takes an id and always constructs a CLOSED stream to pass to writer.reset..... Yeah, that's fair @adleong, I suppose that logic is already encoded in Netty4DispatcherBase's state type.... okay, carry on, then!. I think that rather than wrapping the map in a new class, the key and value types ought to be case classes (or at least type aliases).\nIn particular, I think a case class for (nsName, serviceName, labelSelector) keys might clear up a lot of the code in this file significantly.. Once again, even if making the map keys a case class is unergonomic, I think at least a type alias could make these type signatures less long.... TIOLI: maybe the repeated code in this map closure and in the similar map over endpointsState could be factored out into a function or something?. IIRC it's also more highly optimized by the Scala compiler (at least for fields rather than functions). (Sorry for the drive-by review, but) I kind of wonder if this is something that we can do from sbt rather than with a bash script?. IMO, this match is complex enough that I feel like it would be nice to have some additional comments explaining the various cases.... Nit/TIOLI: maybe we should match Pending and Failed in the same match arm, like this:\nscala\ncase state =>\n  exists = None\n  update() = state\nThat makes the code a little less repetitive.\nOn the other hand, this does rely on the rest of the match exhaustively matching over all the possible Activity.Ok(...) patterns...which it does, but it might be possible to miss one in a future change... \ud83e\udd37\u200d\u2640\ufe0f . TIOLI: It looks like we do this null check every time we call makeInner...maybe we should factor that out so that it's part of makeInner?. I notice there's a change in this branch from synchronizing each match arm separately to synchronizing the whole match block...is that also a necessary part of this change? If so, it would be great to get a mention of that in the commit message for future reference.... Turns out I totally forgot how pattern matching works in Scala. Man, languages with subtyping are wild.... ",
    "dschobel": "api is breaking on next monday's sync but the message mapping already landed https://github.com/twitter/finagle/blob/develop/finagle-http-netty4/src/main/scala/com/twitter/finagle/http-netty4/Bijections.scala\n. not yet, it's been cut internally and should be published shortly. Looks like we moved some TLS config api around (https://github.com/twitter/finagle/commit/9c4677c8578d634ecd8bb45395392369db6539b0) so the router isn't compiling.\n```\nrouter/core/src/main/scala/com/twitter/finagle/buoyant/TlsClientPrep.scala:81: not found: type Netty3TransporterTLSConfig\n[error]           val cfg = new Netty3TransporterTLSConfig(mkEngine, cn)\n[error]                         ^\n[error] one error found\n[error] (router-core/compile:compileIncremental) Compilation failed\n```\nAlso, HttpControlServiceTest has failing tests\n[info] - bind *** FAILED ***\n[info]   \"{\"type\":\"leaf\",\"[dentry\":null,\"]bound\":{\"addr\":{\"typ...\" did not equal \"{\"type\":\"leaf\",\"[]bound\":{\"addr\":{\"typ...\" (HttpControlServiceTest.scala:291)\n[info] - bind with an extra dtab *** FAILED ***\n[info]   \"{\"type\":\"leaf\",\"[dentry\":null,\"]bound\":{\"addr\":{\"typ...\" did not equal \"{\"type\":\"leaf\",\"[]bound\":{\"addr\":{\"typ...\" (HttpControlServiceTest.scala:316)\n[info] - bind watch *** FAILED ***\n[info]   Some(\"{\"type\":\"leaf\",\"dentry\":null,\"bound\":{\"addr\":{\"type\":\"bound\",\"addrs\":[{\"ip\":\"127.0.0.1\",\"port\":1,\"meta\":{}}],\"meta\":{}},\"id\":\"/#/io.l5d.namer/foo\"\") did not equal Some(\"{\"type\":\"leaf\",\"bound\":{\"addr\":{\"type\":\"bound\",\"addrs\":[{\"ip\":\"127.0.0.1\",\"port\":1,\"meta\":{}}],\"meta\":{}},\"id\":\"/#/io.l5d.namer/foo\",\"path\":\"/\"}}\n[info]   \") (HttpControlServiceTest.scala:47)\n[info] - bind an invalid path\n[info] - errors are printed\n[info] - addr\n[info] - addr watch\n[info] - addr an invalid path\n[info] - delegate an invalid path\n[info] - delegate a path given a namespace *** FAILED ***\n[info]   \"...,\"path\":\"/yeezy\",\"de[ntry\":null,\"de]legate\":{\"type\":\"neg...\" did not equal \"...,\"path\":\"/yeezy\",\"de[]legate\":{\"type\":\"neg...\" (HttpControlServiceTest.scala:476)\n[info] - delegate a path given a dtab *** FAILED ***\n[info]   \"...e\",\"path\":\"/foo\",\"de[ntry\":null,\"de]legate\":{\"type\":\"neg...\" did not equal \"...e\",\"path\":\"/foo\",\"de[]legate\":{\"type\":\"neg...\" (HttpControlServiceTest.scala:495)\n[info] - delegate a path given a namespace and a dtab *** FAILED ***\n[info]   \"...t\",\"path\":\"/yeezy\",\"[dentry\":null,\"]alt\":[{\"type\":\"neg\",...\" did not equal \"...t\",\"path\":\"/yeezy\",\"[]alt\":[{\"type\":\"neg\",...\" (HttpControlServiceTest.scala:514)\n[info] - resolve w/ dtab\n[info] - resolve watch\n[info] - resolve an invalid path\n[info] Run completed in 4 seconds, 175 milliseconds.\n[info] Total number of tests run: 40\n[info] Suites: completed 1, aborted 0\n[info] Tests: succeeded 34, failed 6, canceled 0, ignored 0, pending 0\n[info] *** 6 TESTS FAILED ***\n[error] Failed tests:\n[error]     io.buoyant.namerd.iface.HttpControlServiceTest\n[error] (namerd-iface-control-http/test:test) sbt.TestsFailedException: Tests unsuccessful\n[error] (router-core/compile:compileIncremental) Compilation failed\n[error] Total time: 387 s, completed Nov 26, 2016 8:38:52 PM\n@olix0r can you check whether those failures are legit finagle induced or due to linkerd sneaking into finagle/twitter namespaces and doing Unsupported Things?\nI can take a stab at fixing up the config code because it's not great that we rm'd it w/o a deprecation cycle.. ",
    "mosesn": "@olix0r is this bleeding finagle or release finagle?  I think the netty multiplexing codec has a framelogger on all the time, so you should be able to see something if you make it to there.\nhttps://github.com/netty/netty/blob/4.1/codec-http2/src/main/java/io/netty/handler/codec/http2/Http2MultiplexCodec.java#L119\n. Very cool!  I'd suggest you look at c.t.f.http.StreamTransport for the finagle.Message <=> netty.DataFrame adapter.  I don't think MultiplexCodec works on the client-side yet, fwiw: https://github.com/netty/netty/issues/4913.  It shouldn't be too bad to implement the Http2Frame converter when that happens though.\n. I have a review up internally for clients that can upgrade or downgrade, but streaming still doesn't work (because of how the netty InboundHttp2ToHttpAdapter works).  I have some ideas for how to implement that, if you want to meet up at some point in time.  Good job on the server-side stuff\u2013can you downgrade too?  Or does it only support http/2?\n. @olix0r tentatively looks like first or second week of October.\n. Cool!  On the finagle side, we have cleartext working, both h2c and prior knowledge (prior knowledge must be configured server-side\u2013we're trying to work with netty https://github.com/netty/netty/issues/5857 to improve this, but folks don't seem terribly interested, so we might have to just write a hack into finagle).  We also have server-side http/2 with TLS over ALPN, but have not hooked up client-side http/2 with TLS.\nWe're planning on doing a release in the next two weeks.\n. @olix0r ah, if grpc needs trailers we'll need to add it to the finagle-http Message type.  Shouldn't be too hard other than that though.  \ud83d\udc4d  yeah, would love to standardize so we can get to a point where we can benefit from each other's work.\n. Aperture size one?\n. You can specify a maxLoad of Int.MaxValue or similar, and it'll behave like it's of size one.\n. ",
    "samuel": "This is great! I'm excited to see HTTP/2 is being worked on. Is it safe to assume that after HTTP/2 then gRPC specific support could follow? We (@sprucehealth) are eager to use linkerd but are waiting for gRPC.\n. This is awesome.\nThe one thing it seems though is that this PR is using the KV store rather than Consul's service registration. Using the services rather than kv would probably be better as it's the standard way to handle this with Consul as it then allows using the health checks and DNS resolution.\nhttps://www.consul.io/docs/agent/http/catalog.html\n. Ahh, cool, my bad. Thanks for the link.\n. ",
    "buchgr": "@mosesn yes we require trailers to deliver status information for an RPC.\n. @zackangelo thanks for the additional details! I was actually refering to the gRPC protocol [1]. Trailers are required by the gRPC protocol to transmit gRPC status codes. Even without your own application level status codes, one needs trailers for gRPC to work correctly.\nThe application level status codes you are refering to are the (optional) *Custom-Metadata in the response specification.\nEDIT: Just to be clear, trailers are required not only for \"failed\" RPCs, but for all RPCs.\n[1] https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#responses\n. @olix0r \nIt's great to hear that you are using Netty's Http2Codec :). It's highly experimental. Would love to get some feedback!\n\nThough, it looks like netty/netty#5914 should just fix this in Netty.\n\nThe PR you are talking about does two things in that regard:\n1. Promises are only completed once the frames have actually been written. So after they have passed stream and connection level flow control and so on.\n2. A child channel's writability changes depending on the available stream-level flow control window. So you will be able to observe that via ChannelHandler.writabilityChanged. \nCheers!\n. @olix0r \n\nINF [20161017-22:57:05.189] http2: [id: 0xae5561fd, L:/192.168.1.187:55378 - R:/192.168.1.187:55379] WINDOW_UPDATE: streamId=0, windowSizeIncrement=3276\n\nYou are seeing a flow control window update for the connection window (stream 0). Yes, Netty will do that automatically and there are currently not plans on changing that.\nFurthermore, the child channel will also automatically update the stream-level flow control window after a channelRead. If you don't want that, you can turn off autoRead [1] on the child channel and thereby apply back pressure (I think that's what you want). \nEssentially, after having read some data you turn auto read off, forward the data and once you are done you turn auto read on again, or call read manually.\nPlease let me know if that helps or if you have any further questions.\n[1] http://netty.io/wiki/new-and-noteworthy-in-4.0.html#sensible-and-less-error-prone-inbound-traffic-suspension\n. ",
    "zackangelo": "Just to add a little detail around what @buchgr said, the most common way to convey error details for a failed RPC is in an HTTP/2 trailer. The gRPC java stack even does this by default if you craft the exception in the right way, see https://github.com/grpc/grpc-java/issues/681 for details. \n. Seems like a duplicate of #785.\n. The error appears to change when building against Finagle 6.40.0: \nI 1126 20:43:34.714 THREAD680 TraceId:1f19ba7bc5937d00: FailureAccrualFactory marking connection to \"$/inet/127.0.0.1/9897\" as dead. Remote Address: Inet(/127.0.0.1:9897,Map())\nE 1126 20:38:00.044 THREAD680: service failure\ncom.twitter.finagle.ChannelClosedException: null at remote address: /127.0.0.1:9897 from service: $/inet/127.0.0.1/9897. Remote Info: Upstream Address: Not Available, Upstream Client Id: Not Available, Downstream Address: /127.0.0.1:9897, Downstream Client Id: $/inet/127.0.0.1/9897, Trace Id: fde0d31c44d52c0e.fde0d31c44d52c0e<:fde0d31c44d52c0e\nCaused by: java.nio.channels.ClosedChannelException\n    at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source). Seeing these whenever a linkerd process is SIGINT'd: \n```I 1128 17:25:07.119 THREAD1687: Received SIGINT. Shutting down ...\nE 1128 17:25:07.815 THREAD1687: [S L:/127.0.0.1:4144 R:/127.0.0.1:64426] dispatcher failed\nFailure(com.twitter.finagle.buoyant.h2.Reset$Cancel$, flags=0x02) with NoSources\nCaused by: Reset.Cancel\nE 1128 17:25:08.105 THREAD1687: [C L:/127.0.0.1:64437 R:/127.0.0.1:9897] dispatcher failed\nFailure(com.twitter.finagle.buoyant.h2.Reset$Cancel$, flags=0x02) with NoSources\nCaused by: Reset.Cancel```. I also believe we're experiencing this issue. Our heap dump does not contain any sensitive information and would be happy to share it if it would help. \nI've tried setting the sample rate of the telemeters to zero, the leak still occurs. Additionally, linkerd starts logging these messages as things start to go south. \nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: E 0411 22:49:00.940 UTC THREAD48: [S L:/169.254.1.1:4142 R:/169.254.1.1:56352 S:317] ignoring exception\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: com.twitter.finagle.ChannelWriteException: com.twitter.finagle.UnknownChannelException: Request stream 317 is not correct for server connection at remote address: /169.254.1.1:56352. Remote Info: Not Available. Remote Info: Not Available\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: Caused by: com.twitter.finagle.UnknownChannelException: Request stream 317 is not correct for server connection at remote address: /169.254.1.1:56352. Remote Info: Not Available\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at com.twitter.finagle.ChannelException$.apply(Exceptions.scala:259)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at com.twitter.finagle.netty4.transport.ChannelTransport$$anon$2.operationComplete(ChannelTransport.scala:102)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at com.twitter.finagle.netty4.transport.ChannelTransport$$anon$2.operationComplete(ChannelTransport.scala:99)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:514)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:488)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:427)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:120)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:87)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.handler.codec.http2.DefaultHttp2ConnectionEncoder.writeHeaders(DefaultHttp2ConnectionEncoder.java:199)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.handler.codec.http2.DefaultHttp2ConnectionEncoder.writeHeaders(DefaultHttp2ConnectionEncoder.java:145)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.handler.codec.http2.H2FrameCodec.write(H2FrameCodec.scala:135)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:749)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:741)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:39)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1100)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1147)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1089)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:418)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:454)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:873)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at com.twitter.finagle.util.ProxyThreadFactory$$anonfun$newProxiedRunnable$1$$anon$1.run(ProxyThreadFactory.scala:19)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at java.lang.Thread.run(Thread.java:745)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: Caused by: io.netty.handler.codec.http2.Http2Exception: Request stream 317 is not correct for server connection\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.handler.codec.http2.Http2Exception.connectionError(Http2Exception.java:85)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.handler.codec.http2.DefaultHttp2Connection$DefaultEndpoint.checkNewStreamAllowed(DefaultHttp2Connection.java:1026)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.handler.codec.http2.DefaultHttp2Connection$DefaultEndpoint.createStream(DefaultHttp2Connection.java:883)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.handler.codec.http2.DefaultHttp2Connection$DefaultEndpoint.createStream(DefaultHttp2Connection.java:901)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.handler.codec.http2.DefaultHttp2Connection$DefaultEndpoint.createStream(DefaultHttp2Connection.java:811)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.handler.codec.http2.DefaultHttp2ConnectionEncoder.writeHeaders(DefaultHttp2ConnectionEncoder.java:156)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: ... 16 more\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: E 0411 22:49:00.940 UTC THREAD48: [S L:/169.254.1.1:4142 R:/169.254.1.1:56352 S:317] unexpected error\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: com.twitter.finagle.ChannelWriteException: com.twitter.finagle.UnknownChannelException: Request stream 317 is not correct for server connection at remote address: /169.254.1.1:56352. Remote Info: Not Available. Remote Info: Not Available\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: Caused by: com.twitter.finagle.UnknownChannelException: Request stream 317 is not correct for server connection at remote address: /169.254.1.1:56352. Remote Info: Not Available\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at com.twitter.finagle.ChannelException$.apply(Exceptions.scala:259)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at com.twitter.finagle.netty4.transport.ChannelTransport$$anon$2.operationComplete(ChannelTransport.scala:102)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at com.twitter.finagle.netty4.transport.ChannelTransport$$anon$2.operationComplete(ChannelTransport.scala:99)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:514)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:488)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:427)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:120)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:87)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.handler.codec.http2.DefaultHttp2ConnectionEncoder.writeHeaders(DefaultHttp2ConnectionEncoder.java:199)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.handler.codec.http2.DefaultHttp2ConnectionEncoder.writeHeaders(DefaultHttp2ConnectionEncoder.java:145)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.handler.codec.http2.H2FrameCodec.write(H2FrameCodec.scala:135)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:749)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:741)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:39)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1100)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1147)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1089)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:418)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:454)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:873)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at com.twitter.finagle.util.ProxyThreadFactory$$anonfun$newProxiedRunnable$1$$anon$1.run(ProxyThreadFactory.scala:19)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at java.lang.Thread.run(Thread.java:745)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: Caused by: io.netty.handler.codec.http2.Http2Exception: Request stream 317 is not correct for server connection\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.handler.codec.http2.Http2Exception.connectionError(Http2Exception.java:85)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.handler.codec.http2.DefaultHttp2Connection$DefaultEndpoint.checkNewStreamAllowed(DefaultHttp2Connection.java:1026)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.handler.codec.http2.DefaultHttp2Connection$DefaultEndpoint.createStream(DefaultHttp2Connection.java:883)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.handler.codec.http2.DefaultHttp2Connection$DefaultEndpoint.createStream(DefaultHttp2Connection.java:901)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.handler.codec.http2.DefaultHttp2Connection$DefaultEndpoint.createStream(DefaultHttp2Connection.java:811)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: at io.netty.handler.codec.http2.DefaultHttp2ConnectionEncoder.writeHeaders(DefaultHttp2ConnectionEncoder.java:156)\nApr 11 17:49:01 apiproxy-app3-p.dal10sl.bigcommerce.net linkerd[17668]: ... 16 more. @adleong thanks! yeah, that table still grows without bound, but will get gc'd when the channel closes. @olix0r pointed out that the StreamClosed object is a singleton, so we're only accumulating stream IDs at that point. . \nmetrics.json\n\n{\n  \"jvm/start_time\": 1.49247309E+12,\n  \"jvm/application_time_millis\": 6.0543571E+8,\n  \"jvm/classes/total_loaded\": 9651.0,\n  \"jvm/classes/current_loaded\": 9594.0,\n  \"jvm/classes/total_unloaded\": 57.0,\n  \"jvm/postGC/Par_Survivor_Space/max\": 35782656,\n  \"jvm/postGC/Par_Survivor_Space/used\": 4877568.0,\n  \"jvm/postGC/CMS_Old_Gen/max\": 7.1584973E+8,\n  \"jvm/postGC/CMS_Old_Gen/used\": 82497024,\n  \"jvm/postGC/Par_Eden_Space/max\": 286326784,\n  \"jvm/postGC/Par_Eden_Space/used\": 0.0,\n  \"jvm/postGC/used\": 87374592,\n  \"jvm/nonheap/committed\": 69271552,\n  \"jvm/nonheap/max\": -1.0,\n  \"jvm/nonheap/used\": 67800712,\n  \"jvm/tenuring_threshold\": 6.0,\n  \"jvm/thread/daemon_count\": 43.0,\n  \"jvm/thread/count\": 44.0,\n  \"jvm/thread/peak_count\": 63.0,\n  \"jvm/mem/postGC/Par_Survivor_Space/max\": 35782656,\n  \"jvm/mem/postGC/Par_Survivor_Space/used\": 4877568.0,\n  \"jvm/mem/postGC/CMS_Old_Gen/max\": 7.1584973E+8,\n  \"jvm/mem/postGC/CMS_Old_Gen/used\": 82497024,\n  \"jvm/mem/postGC/Par_Eden_Space/max\": 286326784,\n  \"jvm/mem/postGC/Par_Eden_Space/used\": 0.0,\n  \"jvm/mem/postGC/used\": 87374592,\n  \"jvm/mem/metaspace/max_capacity\": 1.0968105E+9,\n  \"jvm/mem/buffer/direct/max\": 861816.0,\n  \"jvm/mem/buffer/direct/count\": 51.0,\n  \"jvm/mem/buffer/direct/used\": 861817.0,\n  \"jvm/mem/buffer/mapped/max\": 0.0,\n  \"jvm/mem/buffer/mapped/count\": 0.0,\n  \"jvm/mem/buffer/mapped/used\": 0.0,\n  \"jvm/mem/allocations/eden/bytes\": 2.12353863E+12,\n  \"jvm/mem/current/used\": 454854816,\n  \"jvm/mem/current/CMS_Old_Gen/max\": 7.1584973E+8,\n  \"jvm/mem/current/CMS_Old_Gen/used\": 364261056,\n  \"jvm/mem/current/Metaspace/max\": -1.0,\n  \"jvm/mem/current/Metaspace/used\": 50707904,\n  \"jvm/mem/current/Par_Eden_Space/max\": 286326784,\n  \"jvm/mem/current/Par_Eden_Space/used\": 1.791548E+7,\n  \"jvm/mem/current/Par_Survivor_Space/max\": 35782656,\n  \"jvm/mem/current/Par_Survivor_Space/used\": 4877568.0,\n  \"jvm/mem/current/Compressed_Class_Space/max\": 1.07374182E+9,\n  \"jvm/mem/current/Compressed_Class_Space/used\": 8267464.0,\n  \"jvm/mem/current/Code_Cache/max\": 50331648,\n  \"jvm/mem/current/Code_Cache/used\": 8825344.0,\n  \"jvm/num_cpus\": 6.0,\n  \"jvm/gc/msec\": 538972.0,\n  \"jvm/gc/eden/pause_msec.count\": 2,\n  \"jvm/gc/eden/pause_msec.max\": 11,\n  \"jvm/gc/eden/pause_msec.min\": 8,\n  \"jvm/gc/eden/pause_msec.p50\": 8,\n  \"jvm/gc/eden/pause_msec.p90\": 11,\n  \"jvm/gc/eden/pause_msec.p95\": 11,\n  \"jvm/gc/eden/pause_msec.p99\": 11,\n  \"jvm/gc/eden/pause_msec.p9990\": 11,\n  \"jvm/gc/eden/pause_msec.p9999\": 11,\n  \"jvm/gc/eden/pause_msec.sum\": 19,\n  \"jvm/gc/eden/pause_msec.avg\": 9.5,\n  \"jvm/gc/ParNew/msec\": 123080.0,\n  \"jvm/gc/ParNew/cycles\": 27893.0,\n  \"jvm/gc/ConcurrentMarkSweep/msec\": 415892.0,\n  \"jvm/gc/ConcurrentMarkSweep/cycles\": 669.0,\n  \"jvm/gc/cycles\": 28562.0,\n  \"jvm/fd_limit\": 4096.0,\n  \"jvm/compilation/time_msec\": 199495.0,\n  \"jvm/uptime\": 6.0616294E+8,\n  \"jvm/safepoint/sync_time_millis\": 34743.0,\n  \"jvm/safepoint/total_time_millis\": 726695.0,\n  \"jvm/safepoint/count\": 264276.0,\n  \"jvm/heap/committed\": 1.03795917E+9,\n  \"jvm/heap/max\": 1.03795917E+9,\n  \"jvm/heap/used\": 387054112,\n  \"jvm/fd_count\": 173.0,\n  \"zk2/inet/dns/queue_size\": 0.0,\n  \"zk2/inet/dns/successes\": 0,\n  \"zk2/inet/dns/cache/evicts\": 0.0,\n  \"zk2/inet/dns/cache/size\": 0.0,\n  \"zk2/inet/dns/cache/hit_rate\": 1.0,\n  \"zk2/inet/dns/dns_lookups\": 0,\n  \"zk2/inet/dns/failures\": 0,\n  \"zk2/inet/dns/dns_lookup_failures\": 0,\n  \"zk2/inet/dns/lookup_ms.count\": 0,\n  \"zk2/observed_serversets\": 0.0,\n  \"zk2/session_cache_size\": 0.0,\n  \"retries.count\": 206,\n  \"retries.max\": 0,\n  \"retries.min\": 0,\n  \"retries.p50\": 0,\n  \"retries.p90\": 0,\n  \"retries.p95\": 0,\n  \"retries.p99\": 0,\n  \"retries.p9990\": 0,\n  \"retries.p9999\": 0,\n  \"retries.sum\": 0,\n  \"retries.avg\": 0.0,\n  \"retries/budget_exhausted\": 0,\n  \"clnt/zipkin-tracer/retries/requeues_per_request.count\": 0,\n  \"clnt/zipkin-tracer/retries/request_limit\": 0,\n  \"clnt/zipkin-tracer/retries/budget_exhausted\": 0,\n  \"clnt/zipkin-tracer/retries/cannot_retry\": 0,\n  \"clnt/zipkin-tracer/retries/not_open\": 0,\n  \"clnt/zipkin-tracer/retries/budget\": 100.0,\n  \"clnt/zipkin-tracer/retries/requeues\": 0,\n  \"clnt/zipkin-tracer/tries/request_latency_ms.count\": 0,\n  \"clnt/zipkin-tracer/tries/success\": 0,\n  \"clnt/zipkin-tracer/tries/pending\": 0.0,\n  \"clnt/zipkin-tracer/tries/requests\": 0,\n  \"clnt/zipkin-tracer/loadbalancer/size\": 1.0,\n  \"clnt/zipkin-tracer/loadbalancer/rebuilds\": 1,\n  \"clnt/zipkin-tracer/loadbalancer/closed\": 0.0,\n  \"clnt/zipkin-tracer/loadbalancer/load\": 0.0,\n  \"clnt/zipkin-tracer/loadbalancer/meanweight\": 1.0,\n  \"clnt/zipkin-tracer/loadbalancer/adds\": 1,\n  \"clnt/zipkin-tracer/loadbalancer/p2c\": 1.0,\n  \"clnt/zipkin-tracer/loadbalancer/updates\": 1,\n  \"clnt/zipkin-tracer/loadbalancer/available\": 1.0,\n  \"clnt/zipkin-tracer/loadbalancer/max_effort_exhausted\": 0,\n  \"clnt/zipkin-tracer/loadbalancer/busy\": 0.0,\n  \"clnt/zipkin-tracer/loadbalancer/removes\": 0,\n  \"clnt/zipkin-tracer/service_creation/service_acquisition_latency_ms.count\": 0,\n  \"namer/#/io.l5d.consul/connect_latency_ms.count\": 1,\n  \"namer/#/io.l5d.consul/connect_latency_ms.max\": 2,\n  \"namer/#/io.l5d.consul/connect_latency_ms.min\": 2,\n  \"namer/#/io.l5d.consul/connect_latency_ms.p50\": 2,\n  \"namer/#/io.l5d.consul/connect_latency_ms.p90\": 2,\n  \"namer/#/io.l5d.consul/connect_latency_ms.p95\": 2,\n  \"namer/#/io.l5d.consul/connect_latency_ms.p99\": 2,\n  \"namer/#/io.l5d.consul/connect_latency_ms.p9990\": 2,\n  \"namer/#/io.l5d.consul/connect_latency_ms.p9999\": 2,\n  \"namer/#/io.l5d.consul/connect_latency_ms.sum\": 2,\n  \"namer/#/io.l5d.consul/connect_latency_ms.avg\": 2.0,\n  \"namer/#/io.l5d.consul/failed_connect_latency_ms.count\": 0,\n  \"namer/#/io.l5d.consul/sent_bytes\": 456103969,\n  \"namer/#/io.l5d.consul/service_creation/service_acquisition_latency_ms.count\": 206,\n  \"namer/#/io.l5d.consul/service_creation/service_acquisition_latency_ms.max\": 6,\n  \"namer/#/io.l5d.consul/service_creation/service_acquisition_latency_ms.min\": 0,\n  \"namer/#/io.l5d.consul/service_creation/service_acquisition_latency_ms.p50\": 0,\n  \"namer/#/io.l5d.consul/service_creation/service_acquisition_latency_ms.p90\": 0,\n  \"namer/#/io.l5d.consul/service_creation/service_acquisition_latency_ms.p95\": 0,\n  \"namer/#/io.l5d.consul/service_creation/service_acquisition_latency_ms.p99\": 0,\n  \"namer/#/io.l5d.consul/service_creation/service_acquisition_latency_ms.p9990\": 6,\n  \"namer/#/io.l5d.consul/service_creation/service_acquisition_latency_ms.p9999\": 6,\n  \"namer/#/io.l5d.consul/service_creation/service_acquisition_latency_ms.sum\": 7,\n  \"namer/#/io.l5d.consul/service_creation/service_acquisition_latency_ms.avg\": 0.03398058252427184,\n  \"namer/#/io.l5d.consul/connection_received_bytes.count\": 1,\n  \"namer/#/io.l5d.consul/connection_received_bytes.max\": 2240083,\n  \"namer/#/io.l5d.consul/connection_received_bytes.min\": 2240083,\n  \"namer/#/io.l5d.consul/connection_received_bytes.p50\": 2240083,\n  \"namer/#/io.l5d.consul/connection_received_bytes.p90\": 2240083,\n  \"namer/#/io.l5d.consul/connection_received_bytes.p95\": 2240083,\n  \"namer/#/io.l5d.consul/connection_received_bytes.p99\": 2240083,\n  \"namer/#/io.l5d.consul/connection_received_bytes.p9990\": 2240083,\n  \"namer/#/io.l5d.consul/connection_received_bytes.p9999\": 2240083,\n  \"namer/#/io.l5d.consul/connection_received_bytes.sum\": 2248546,\n  \"namer/#/io.l5d.consul/connection_received_bytes.avg\": 2248546.0,\n  \"namer/#/io.l5d.consul/connection_duration.count\": 1,\n  \"namer/#/io.l5d.consul/connection_duration.max\": 359031,\n  \"namer/#/io.l5d.consul/connection_duration.min\": 359031,\n  \"namer/#/io.l5d.consul/connection_duration.p50\": 359031,\n  \"namer/#/io.l5d.consul/connection_duration.p90\": 359031,\n  \"namer/#/io.l5d.consul/connection_duration.p95\": 359031,\n  \"namer/#/io.l5d.consul/connection_duration.p99\": 359031,\n  \"namer/#/io.l5d.consul/connection_duration.p9990\": 359031,\n  \"namer/#/io.l5d.consul/connection_duration.p9999\": 359031,\n  \"namer/#/io.l5d.consul/connection_duration.sum\": 359153,\n  \"namer/#/io.l5d.consul/connection_duration.avg\": 359153.0,\n  \"namer/#/io.l5d.consul/failure_accrual/removals\": 0,\n  \"namer/#/io.l5d.consul/failure_accrual/probes\": 0,\n  \"namer/#/io.l5d.consul/failure_accrual/removed_for_ms\": 0,\n  \"namer/#/io.l5d.consul/failure_accrual/revivals\": 0,\n  \"namer/#/io.l5d.consul/connects\": 12370,\n  \"namer/#/io.l5d.consul/pool_num_waited\": 0,\n  \"namer/#/io.l5d.consul/success\": 1983657,\n  \"namer/#/io.l5d.consul/request_latency_ms.count\": 206,\n  \"namer/#/io.l5d.consul/request_latency_ms.max\": 2879,\n  \"namer/#/io.l5d.consul/request_latency_ms.min\": 9,\n  \"namer/#/io.l5d.consul/request_latency_ms.p50\": 406,\n  \"namer/#/io.l5d.consul/request_latency_ms.p90\": 1261,\n  \"namer/#/io.l5d.consul/request_latency_ms.p95\": 1733,\n  \"namer/#/io.l5d.consul/request_latency_ms.p99\": 2456,\n  \"namer/#/io.l5d.consul/request_latency_ms.p9990\": 2879,\n  \"namer/#/io.l5d.consul/request_latency_ms.p9999\": 2879,\n  \"namer/#/io.l5d.consul/request_latency_ms.sum\": 116700,\n  \"namer/#/io.l5d.consul/request_latency_ms.avg\": 566.504854368932,\n  \"namer/#/io.l5d.consul/pool_waiters\": 0.0,\n  \"namer/#/io.l5d.consul/dc/errors\": 0,\n  \"namer/#/io.l5d.consul/dc/adds\": 25133,\n  \"namer/#/io.l5d.consul/dc/updates\": 779062,\n  \"namer/#/io.l5d.consul/dc/closes\": 0,\n  \"namer/#/io.l5d.consul/dc/opens\": 1,\n  \"namer/#/io.l5d.consul/dc/removes\": 24794,\n  \"namer/#/io.l5d.consul/retries/requeues_per_request.count\": 206,\n  \"namer/#/io.l5d.consul/retries/requeues_per_request.max\": 0,\n  \"namer/#/io.l5d.consul/retries/requeues_per_request.min\": 0,\n  \"namer/#/io.l5d.consul/retries/requeues_per_request.p50\": 0,\n  \"namer/#/io.l5d.consul/retries/requeues_per_request.p90\": 0,\n  \"namer/#/io.l5d.consul/retries/requeues_per_request.p95\": 0,\n  \"namer/#/io.l5d.consul/retries/requeues_per_request.p99\": 0,\n  \"namer/#/io.l5d.consul/retries/requeues_per_request.p9990\": 0,\n  \"namer/#/io.l5d.consul/retries/requeues_per_request.p9999\": 0,\n  \"namer/#/io.l5d.consul/retries/requeues_per_request.sum\": 0,\n  \"namer/#/io.l5d.consul/retries/requeues_per_request.avg\": 0.0,\n  \"namer/#/io.l5d.consul/retries/request_limit\": 0,\n  \"namer/#/io.l5d.consul/retries/budget_exhausted\": 0,\n  \"namer/#/io.l5d.consul/retries/cannot_retry\": 0,\n  \"namer/#/io.l5d.consul/retries/not_open\": 0,\n  \"namer/#/io.l5d.consul/retries/budget\": 105.0,\n  \"namer/#/io.l5d.consul/retries/requeues\": 0,\n  \"namer/#/io.l5d.consul/received_bytes\": 12931607545,\n  \"namer/#/io.l5d.consul/namer/nametreecache/evicts\": 0,\n  \"namer/#/io.l5d.consul/namer/nametreecache/misses\": 1,\n  \"namer/#/io.l5d.consul/namer/nametreecache/oneshots\": 0,\n  \"namer/#/io.l5d.consul/namer/dtabcache/evicts\": 0,\n  \"namer/#/io.l5d.consul/namer/dtabcache/misses\": 1,\n  \"namer/#/io.l5d.consul/namer/dtabcache/oneshots\": 0,\n  \"namer/#/io.l5d.consul/namer/namecache/evicts\": 0,\n  \"namer/#/io.l5d.consul/namer/namecache/misses\": 1,\n  \"namer/#/io.l5d.consul/namer/namecache/oneshots\": 0,\n  \"namer/#/io.l5d.consul/namer/bind_latency_us.count\": 0,\n  \"namer/#/io.l5d.consul/failfast/marked_available\": 0,\n  \"namer/#/io.l5d.consul/failfast/unhealthy_num_tries\": 0.0,\n  \"namer/#/io.l5d.consul/failfast/marked_dead\": 0,\n  \"namer/#/io.l5d.consul/failfast/unhealthy_for_ms\": 0.0,\n  \"namer/#/io.l5d.consul/connection_sent_bytes.count\": 1,\n  \"namer/#/io.l5d.consul/connection_sent_bytes.max\": 87397,\n  \"namer/#/io.l5d.consul/connection_sent_bytes.min\": 87397,\n  \"namer/#/io.l5d.consul/connection_sent_bytes.p50\": 87397,\n  \"namer/#/io.l5d.consul/connection_sent_bytes.p90\": 87397,\n  \"namer/#/io.l5d.consul/connection_sent_bytes.p95\": 87397,\n  \"namer/#/io.l5d.consul/connection_sent_bytes.p99\": 87397,\n  \"namer/#/io.l5d.consul/connection_sent_bytes.p9990\": 87397,\n  \"namer/#/io.l5d.consul/connection_sent_bytes.p9999\": 87397,\n  \"namer/#/io.l5d.consul/connection_sent_bytes.sum\": 87252,\n  \"namer/#/io.l5d.consul/connection_sent_bytes.avg\": 87252.0,\n  \"namer/#/io.l5d.consul/connection_requests.count\": 1,\n  \"namer/#/io.l5d.consul/connection_requests.max\": 505,\n  \"namer/#/io.l5d.consul/connection_requests.min\": 505,\n  \"namer/#/io.l5d.consul/connection_requests.p50\": 505,\n  \"namer/#/io.l5d.consul/connection_requests.p90\": 505,\n  \"namer/#/io.l5d.consul/connection_requests.p95\": 505,\n  \"namer/#/io.l5d.consul/connection_requests.p99\": 505,\n  \"namer/#/io.l5d.consul/connection_requests.p9990\": 505,\n  \"namer/#/io.l5d.consul/connection_requests.p9999\": 505,\n  \"namer/#/io.l5d.consul/connection_requests.sum\": 503,\n  \"namer/#/io.l5d.consul/connection_requests.avg\": 503.0,\n  \"namer/#/io.l5d.consul/service/opens\": 1,\n  \"namer/#/io.l5d.consul/service/updates\": 1204593,\n  \"namer/#/io.l5d.consul/service/closes\": 0,\n  \"namer/#/io.l5d.consul/service/errors\": 0,\n  \"namer/#/io.l5d.consul/pool_num_too_many_waiters\": 0,\n  \"namer/#/io.l5d.consul/socket_unwritable_ms\": 0,\n  \"namer/#/io.l5d.consul/closes\": 12367,\n  \"namer/#/io.l5d.consul/pool_cached\": 1.0,\n  \"namer/#/io.l5d.consul/pool_size\": 2.0,\n  \"namer/#/io.l5d.consul/available\": 3.0,\n  \"namer/#/io.l5d.consul/request_payload_bytes.count\": 206,\n  \"namer/#/io.l5d.consul/request_payload_bytes.max\": 0,\n  \"namer/#/io.l5d.consul/request_payload_bytes.min\": 0,\n  \"namer/#/io.l5d.consul/request_payload_bytes.p50\": 0,\n  \"namer/#/io.l5d.consul/request_payload_bytes.p90\": 0,\n  \"namer/#/io.l5d.consul/request_payload_bytes.p95\": 0,\n  \"namer/#/io.l5d.consul/request_payload_bytes.p99\": 0,\n  \"namer/#/io.l5d.consul/request_payload_bytes.p9990\": 0,\n  \"namer/#/io.l5d.consul/request_payload_bytes.p9999\": 0,\n  \"namer/#/io.l5d.consul/request_payload_bytes.sum\": 0,\n  \"namer/#/io.l5d.consul/request_payload_bytes.avg\": 0.0,\n  \"namer/#/io.l5d.consul/socket_writable_ms\": 0,\n  \"namer/#/io.l5d.consul/cancelled_connects\": 0,\n  \"namer/#/io.l5d.consul/response_payload_bytes.count\": 206,\n  \"namer/#/io.l5d.consul/response_payload_bytes.max\": 5778,\n  \"namer/#/io.l5d.consul/response_payload_bytes.min\": 5664,\n  \"namer/#/io.l5d.consul/response_payload_bytes.p50\": 5664,\n  \"namer/#/io.l5d.consul/response_payload_bytes.p90\": 5778,\n  \"namer/#/io.l5d.consul/response_payload_bytes.p95\": 5778,\n  \"namer/#/io.l5d.consul/response_payload_bytes.p99\": 5778,\n  \"namer/#/io.l5d.consul/response_payload_bytes.p9990\": 5778,\n  \"namer/#/io.l5d.consul/response_payload_bytes.p9999\": 5778,\n  \"namer/#/io.l5d.consul/response_payload_bytes.sum\": 1176250,\n  \"namer/#/io.l5d.consul/response_payload_bytes.avg\": 5709.9514563106795,\n  \"namer/#/io.l5d.consul/dtab/size.count\": 0,\n  \"namer/#/io.l5d.consul/lookups\": 3,\n  \"namer/#/io.l5d.consul/requests\": 1983657,\n  \"namer/#/io.l5d.consul/loadbalancer/size\": 3.0,\n  \"namer/#/io.l5d.consul/loadbalancer/rebuilds\": 120980,\n  \"namer/#/io.l5d.consul/loadbalancer/closed\": 0.0,\n  \"namer/#/io.l5d.consul/loadbalancer/load\": 2.0,\n  \"namer/#/io.l5d.consul/loadbalancer/meanweight\": 1.0,\n  \"namer/#/io.l5d.consul/loadbalancer/adds\": 12211,\n  \"namer/#/io.l5d.consul/loadbalancer/p2c\": 1.0,\n  \"namer/#/io.l5d.consul/loadbalancer/updates\": 120980,\n  \"namer/#/io.l5d.consul/loadbalancer/available\": 3.0,\n  \"namer/#/io.l5d.consul/loadbalancer/max_effort_exhausted\": 0,\n  \"namer/#/io.l5d.consul/loadbalancer/busy\": 0.0,\n  \"namer/#/io.l5d.consul/loadbalancer/removes\": 12208,\n  \"namer/#/io.l5d.consul/pending\": 2.0,\n  \"namer/#/io.l5d.consul/dispatcher/serial/queue_size\": 0.0,\n  \"namer/#/io.l5d.consul/connections\": 3.0,\n  \"namer/#/io.l5d.consul_to_linker/lookups\": 0,\n  \"namer/#/io.l5d.consul_to_linker/request_payload_bytes.count\": 0,\n  \"namer/#/io.l5d.consul_to_linker/response_payload_bytes.count\": 0,\n  \"namer/#/io.l5d.consul_to_linker/namer/nametreecache/evicts\": 0,\n  \"namer/#/io.l5d.consul_to_linker/namer/nametreecache/misses\": 0,\n  \"namer/#/io.l5d.consul_to_linker/namer/nametreecache/oneshots\": 0,\n  \"namer/#/io.l5d.consul_to_linker/namer/dtabcache/evicts\": 0,\n  \"namer/#/io.l5d.consul_to_linker/namer/dtabcache/misses\": 0,\n  \"namer/#/io.l5d.consul_to_linker/namer/dtabcache/oneshots\": 0,\n  \"namer/#/io.l5d.consul_to_linker/namer/namecache/evicts\": 0,\n  \"namer/#/io.l5d.consul_to_linker/namer/namecache/misses\": 0,\n  \"namer/#/io.l5d.consul_to_linker/namer/namecache/oneshots\": 0,\n  \"rt/http1-in/bindcache/path/evicts\": 0,\n  \"rt/http1-in/bindcache/path/misses\": 0,\n  \"rt/http1-in/bindcache/path/oneshots\": 0,\n  \"rt/http1-in/bindcache/bound/evicts\": 0,\n  \"rt/http1-in/bindcache/bound/misses\": 0,\n  \"rt/http1-in/bindcache/bound/oneshots\": 0,\n  \"rt/http1-in/bindcache/tree/evicts\": 0,\n  \"rt/http1-in/bindcache/tree/misses\": 0,\n  \"rt/http1-in/bindcache/tree/oneshots\": 0,\n  \"rt/http1-in/bindcache/client/evicts\": 0,\n  \"rt/http1-in/bindcache/client/misses\": 0,\n  \"rt/http1-in/bindcache/client/oneshots\": 0,\n  \"rt/http1-in/srv/10.142.54.113/4141/success\": 0,\n  \"rt/http1-in/srv/10.142.54.113/4141/request_latency_ms.count\": 0,\n  \"rt/http1-in/srv/10.142.54.113/4141/nacks\": 0,\n  \"rt/http1-in/srv/10.142.54.113/4141/transit_latency_ms.count\": 0,\n  \"rt/http1-in/srv/10.142.54.113/4141/status/1XX\": 0,\n  \"rt/http1-in/srv/10.142.54.113/4141/status/4XX\": 0,\n  \"rt/http1-in/srv/10.142.54.113/4141/status/2XX\": 0,\n  \"rt/http1-in/srv/10.142.54.113/4141/status/error\": 0,\n  \"rt/http1-in/srv/10.142.54.113/4141/status/3XX\": 0,\n  \"rt/http1-in/srv/10.142.54.113/4141/status/5XX\": 0,\n  \"rt/http1-in/srv/10.142.54.113/4141/time/1XX.count\": 0,\n  \"rt/http1-in/srv/10.142.54.113/4141/time/4XX.count\": 0,\n  \"rt/http1-in/srv/10.142.54.113/4141/time/2XX.count\": 0,\n  \"rt/http1-in/srv/10.142.54.113/4141/time/error.count\": 0,\n  \"rt/http1-in/srv/10.142.54.113/4141/time/3XX.count\": 0,\n  \"rt/http1-in/srv/10.142.54.113/4141/time/5XX.count\": 0,\n  \"rt/http1-in/srv/10.142.54.113/4141/nonretryable_nacks\": 0,\n  \"rt/http1-in/srv/10.142.54.113/4141/dtab/size.count\": 0,\n  \"rt/http1-in/srv/10.142.54.113/4141/requests\": 0,\n  \"rt/http1-in/srv/10.142.54.113/4141/pending\": 0.0,\n  \"rt/http1-in/srv/10.142.54.113/4141/handletime_us.count\": 0,\n  \"rt/http1-out/bindcache/path/evicts\": 0,\n  \"rt/http1-out/bindcache/path/misses\": 0,\n  \"rt/http1-out/bindcache/path/oneshots\": 0,\n  \"rt/http1-out/bindcache/bound/evicts\": 0,\n  \"rt/http1-out/bindcache/bound/misses\": 0,\n  \"rt/http1-out/bindcache/bound/oneshots\": 0,\n  \"rt/http1-out/bindcache/tree/evicts\": 0,\n  \"rt/http1-out/bindcache/tree/misses\": 0,\n  \"rt/http1-out/bindcache/tree/oneshots\": 0,\n  \"rt/http1-out/bindcache/client/evicts\": 0,\n  \"rt/http1-out/bindcache/client/misses\": 0,\n  \"rt/http1-out/bindcache/client/oneshots\": 0,\n  \"rt/http1-out/srv/169.254.1.1/4140/success\": 0,\n  \"rt/http1-out/srv/169.254.1.1/4140/request_latency_ms.count\": 0,\n  \"rt/http1-out/srv/169.254.1.1/4140/nacks\": 0,\n  \"rt/http1-out/srv/169.254.1.1/4140/transit_latency_ms.count\": 0,\n  \"rt/http1-out/srv/169.254.1.1/4140/status/1XX\": 0,\n  \"rt/http1-out/srv/169.254.1.1/4140/status/4XX\": 0,\n  \"rt/http1-out/srv/169.254.1.1/4140/status/2XX\": 0,\n  \"rt/http1-out/srv/169.254.1.1/4140/status/error\": 0,\n  \"rt/http1-out/srv/169.254.1.1/4140/status/3XX\": 0,\n  \"rt/http1-out/srv/169.254.1.1/4140/status/5XX\": 0,\n  \"rt/http1-out/srv/169.254.1.1/4140/time/1XX.count\": 0,\n  \"rt/http1-out/srv/169.254.1.1/4140/time/4XX.count\": 0,\n  \"rt/http1-out/srv/169.254.1.1/4140/time/2XX.count\": 0,\n  \"rt/http1-out/srv/169.254.1.1/4140/time/error.count\": 0,\n  \"rt/http1-out/srv/169.254.1.1/4140/time/3XX.count\": 0,\n  \"rt/http1-out/srv/169.254.1.1/4140/time/5XX.count\": 0,\n  \"rt/http1-out/srv/169.254.1.1/4140/nonretryable_nacks\": 0,\n  \"rt/http1-out/srv/169.254.1.1/4140/dtab/size.count\": 0,\n  \"rt/http1-out/srv/169.254.1.1/4140/requests\": 0,\n  \"rt/http1-out/srv/169.254.1.1/4140/pending\": 0.0,\n  \"rt/http1-out/srv/169.254.1.1/4140/handletime_us.count\": 0,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/success\": 2097107,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.count\": 6163,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.max\": 25,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.min\": 1,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p50\": 4,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p90\": 5,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p95\": 6,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p99\": 10,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p9990\": 14,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p9999\": 24,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.sum\": 26253,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.avg\": 4.2597760830764235,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.count\": 6162,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.max\": 0,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.min\": 0,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.p50\": 0,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.p90\": 0,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.p95\": 0,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.p99\": 0,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.p9990\": 0,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.p9999\": 0,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.sum\": 0,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.avg\": 0.0,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/total\": 0,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/budget_exhausted\": 0,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures\": 13879,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 13826,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/interrupted\": 36,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/interrupted/com.twitter.finagle.Failure\": 36,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/interrupted/com.twitter.finagle.Failure/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 36,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/com.twitter.finagle.buoyant.h2.Reset$InternalError$\": 1,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/com.twitter.finagle.CancelledConnectionException\": 9,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/com.twitter.finagle.CancelledConnectionException/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 9,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/restartable\": 31,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/restartable/com.twitter.finagle.Failure\": 31,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/restartable/com.twitter.finagle.Failure/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 31,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/com.twitter.finagle.buoyant.h2.Reset$Refused$\": 7,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/requests\": 2110986,\n  \"rt/h2-out/dst/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/pending\": 0.0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/sent_bytes\": 136516243,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/failures\": 31,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/failures/com.twitter.finagle.Failure\": 31,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/failures/com.twitter.finagle.Failure/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 31,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.count\": 6161,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.max\": 2,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.min\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.p50\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.p90\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.p95\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.p99\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.p9990\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.p9999\": 1,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.sum\": 4,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.avg\": 0.0006492452523940919,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/connection_received_bytes.count\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/connection_duration.count\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/failure_accrual/removals\": 19,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/failure_accrual/probes\": 44,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/failure_accrual/removed_for_ms\": 3977202,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/failure_accrual/revivals\": 17,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/connects\": 145,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/pool_num_waited\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/success\": 2097107,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/success\": 2097107,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.count\": 6162,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.max\": 25,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.min\": 1,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p50\": 4,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p90\": 5,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p95\": 6,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p99\": 10,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p9990\": 14,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p9999\": 23,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.sum\": 25321,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.avg\": 4.109217786432977,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures\": 13834,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 13826,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/com.twitter.finagle.buoyant.h2.Reset$Refused$\": 7,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/com.twitter.finagle.buoyant.h2.Reset$InternalError$\": 1,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/requests\": 2110941,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/path/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/pending\": 0.0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/request_latency_ms.count\": 6162,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/request_latency_ms.max\": 25,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/request_latency_ms.min\": 1,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/request_latency_ms.p50\": 4,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/request_latency_ms.p90\": 5,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/request_latency_ms.p95\": 6,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/request_latency_ms.p99\": 10,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/request_latency_ms.p9990\": 14,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/request_latency_ms.p9999\": 23,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/request_latency_ms.sum\": 25255,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/request_latency_ms.avg\": 4.0985069782538135,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/pool_waiters\": 0.0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.count\": 6162,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.max\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.min\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.p50\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.p90\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.p95\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.p99\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.p9990\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.p9999\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.sum\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.avg\": 0.0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/request_limit\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/budget_exhausted\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/cannot_retry\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/not_open\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/budget\": 100.0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/requeues\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/received_bytes\": 236661254,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/read_timeout\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/write_timeout\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/connection_sent_bytes.count\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/connection_requests.count\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/pool_num_too_many_waiters\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/socket_unwritable_ms\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/closes\": 195,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/pool_cached\": 43.0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/data/frames\": 2083049,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.count\": 6162,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.max\": 141,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.min\": 5,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.p50\": 72,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.p90\": 84,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.p95\": 86,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.p99\": 92,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.p9990\": 115,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.p9999\": 116,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.sum\": 444553,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.avg\": 72.14427134047388,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/reset\": 14127,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/trailers\": 2083049,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/data/frames\": 2110758,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.count\": 6160,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.max\": 17,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.min\": 12,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.p50\": 14,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.p90\": 17,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.p95\": 17,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.p99\": 17,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.p9990\": 17,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.p9999\": 17,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.sum\": 86235,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.avg\": 13.999188311688311,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/reset\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/trailers\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/failures\": 13834,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/failures/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 13826,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/failures/com.twitter.finagle.buoyant.h2.Reset$Refused$\": 7,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/failures/com.twitter.finagle.buoyant.h2.Reset$InternalError$\": 1,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/pool_size\": 0.0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/available\": 1.0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/socket_writable_ms\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/dtab/size.count\": 0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/requests\": 2110941,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/loadbalancer/size\": 1.0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/loadbalancer/rebuilds\": 1230,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/loadbalancer/closed\": 0.0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/loadbalancer/load\": 0.0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/loadbalancer/meanweight\": 1.0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/loadbalancer/adds\": 11,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/loadbalancer/p2c\": 3.0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/loadbalancer/updates\": 27,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/loadbalancer/available\": 1.0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/loadbalancer/max_effort_exhausted\": 1203,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/loadbalancer/busy\": 0.0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/loadbalancer/removes\": 10,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/pending\": 0.0,\n  \"rt/h2-out/dst/id/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/connections\": 43.0,\n  \"rt/h2-out/bindcache/path/evicts\": 0,\n  \"rt/h2-out/bindcache/path/misses\": 1,\n  \"rt/h2-out/bindcache/path/oneshots\": 0,\n  \"rt/h2-out/bindcache/bound/evicts\": 0,\n  \"rt/h2-out/bindcache/bound/misses\": 1,\n  \"rt/h2-out/bindcache/bound/oneshots\": 0,\n  \"rt/h2-out/bindcache/tree/evicts\": 0,\n  \"rt/h2-out/bindcache/tree/misses\": 1,\n  \"rt/h2-out/bindcache/tree/oneshots\": 0,\n  \"rt/h2-out/bindcache/client/evicts\": 0,\n  \"rt/h2-out/bindcache/client/misses\": 1,\n  \"rt/h2-out/bindcache/client/oneshots\": 0,\n  \"rt/h2-out/srv/169.254.1.1/4142/sent_bytes\": 236606986,\n  \"rt/h2-out/srv/169.254.1.1/4142/connection_received_bytes.count\": 0,\n  \"rt/h2-out/srv/169.254.1.1/4142/connection_duration.count\": 0,\n  \"rt/h2-out/srv/169.254.1.1/4142/connects\": 33,\n  \"rt/h2-out/srv/169.254.1.1/4142/success\": 2097107,\n  \"rt/h2-out/srv/169.254.1.1/4142/request_latency_ms.count\": 6162,\n  \"rt/h2-out/srv/169.254.1.1/4142/request_latency_ms.max\": 25,\n  \"rt/h2-out/srv/169.254.1.1/4142/request_latency_ms.min\": 1,\n  \"rt/h2-out/srv/169.254.1.1/4142/request_latency_ms.p50\": 4,\n  \"rt/h2-out/srv/169.254.1.1/4142/request_latency_ms.p90\": 6,\n  \"rt/h2-out/srv/169.254.1.1/4142/request_latency_ms.p95\": 7,\n  \"rt/h2-out/srv/169.254.1.1/4142/request_latency_ms.p99\": 10,\n  \"rt/h2-out/srv/169.254.1.1/4142/request_latency_ms.p9990\": 14,\n  \"rt/h2-out/srv/169.254.1.1/4142/request_latency_ms.p9999\": 24,\n  \"rt/h2-out/srv/169.254.1.1/4142/request_latency_ms.sum\": 27029,\n  \"rt/h2-out/srv/169.254.1.1/4142/request_latency_ms.avg\": 4.386400519311912,\n  \"rt/h2-out/srv/169.254.1.1/4142/received_bytes\": 194894957,\n  \"rt/h2-out/srv/169.254.1.1/4142/read_timeout\": 0,\n  \"rt/h2-out/srv/169.254.1.1/4142/write_timeout\": 0,\n  \"rt/h2-out/srv/169.254.1.1/4142/connection_sent_bytes.count\": 0,\n  \"rt/h2-out/srv/169.254.1.1/4142/connection_requests.count\": 0,\n  \"rt/h2-out/srv/169.254.1.1/4142/transit_latency_ms.count\": 0,\n  \"rt/h2-out/srv/169.254.1.1/4142/socket_unwritable_ms\": 0,\n  \"rt/h2-out/srv/169.254.1.1/4142/closes\": 8,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/remote/data/frames\": 2110924,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/remote/data/bytes.count\": 6160,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/remote/data/bytes.max\": 17,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/remote/data/bytes.min\": 12,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/remote/data/bytes.p50\": 14,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/remote/data/bytes.p90\": 17,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/remote/data/bytes.p95\": 17,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/remote/data/bytes.p99\": 17,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/remote/data/bytes.p9990\": 17,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/remote/data/bytes.p9999\": 17,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/remote/data/bytes.sum\": 86235,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/remote/data/bytes.avg\": 13.999188311688311,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/remote/reset\": 26135,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/remote/trailers\": 0,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/local/data/frames\": 2082341,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/local/data/bytes.count\": 6162,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/local/data/bytes.max\": 141,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/local/data/bytes.min\": 5,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/local/data/bytes.p50\": 72,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/local/data/bytes.p90\": 84,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/local/data/bytes.p95\": 86,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/local/data/bytes.p99\": 92,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/local/data/bytes.p9990\": 115,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/local/data/bytes.p9999\": 116,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/local/data/bytes.sum\": 444553,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/local/data/bytes.avg\": 72.14427134047388,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/local/reset\": 0,\n  \"rt/h2-out/srv/169.254.1.1/4142/stream/local/trailers\": 2082333,\n  \"rt/h2-out/srv/169.254.1.1/4142/failures\": 13879,\n  \"rt/h2-out/srv/169.254.1.1/4142/failures/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 13826,\n  \"rt/h2-out/srv/169.254.1.1/4142/failures/interrupted\": 36,\n  \"rt/h2-out/srv/169.254.1.1/4142/failures/interrupted/com.twitter.finagle.Failure\": 36,\n  \"rt/h2-out/srv/169.254.1.1/4142/failures/interrupted/com.twitter.finagle.Failure/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 36,\n  \"rt/h2-out/srv/169.254.1.1/4142/failures/com.twitter.finagle.buoyant.h2.Reset$InternalError$\": 1,\n  \"rt/h2-out/srv/169.254.1.1/4142/failures/com.twitter.finagle.CancelledConnectionException\": 9,\n  \"rt/h2-out/srv/169.254.1.1/4142/failures/com.twitter.finagle.CancelledConnectionException/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 9,\n  \"rt/h2-out/srv/169.254.1.1/4142/failures/restartable\": 31,\n  \"rt/h2-out/srv/169.254.1.1/4142/failures/restartable/com.twitter.finagle.Failure\": 31,\n  \"rt/h2-out/srv/169.254.1.1/4142/failures/restartable/com.twitter.finagle.Failure/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 31,\n  \"rt/h2-out/srv/169.254.1.1/4142/failures/com.twitter.finagle.buoyant.h2.Reset$Refused$\": 7,\n  \"rt/h2-out/srv/169.254.1.1/4142/exn/java.io.IOException\": 3,\n  \"rt/h2-out/srv/169.254.1.1/4142/socket_writable_ms\": 0,\n  \"rt/h2-out/srv/169.254.1.1/4142/dtab/size.count\": 0,\n  \"rt/h2-out/srv/169.254.1.1/4142/requests\": 2110986,\n  \"rt/h2-out/srv/169.254.1.1/4142/pending\": 0.0,\n  \"rt/h2-out/srv/169.254.1.1/4142/handletime_us.count\": 6160,\n  \"rt/h2-out/srv/169.254.1.1/4142/handletime_us.max\": 3729,\n  \"rt/h2-out/srv/169.254.1.1/4142/handletime_us.min\": 9,\n  \"rt/h2-out/srv/169.254.1.1/4142/handletime_us.p50\": 20,\n  \"rt/h2-out/srv/169.254.1.1/4142/handletime_us.p90\": 39,\n  \"rt/h2-out/srv/169.254.1.1/4142/handletime_us.p95\": 57,\n  \"rt/h2-out/srv/169.254.1.1/4142/handletime_us.p99\": 182,\n  \"rt/h2-out/srv/169.254.1.1/4142/handletime_us.p9990\": 701,\n  \"rt/h2-out/srv/169.254.1.1/4142/handletime_us.p9999\": 1407,\n  \"rt/h2-out/srv/169.254.1.1/4142/handletime_us.sum\": 179005,\n  \"rt/h2-out/srv/169.254.1.1/4142/handletime_us.avg\": 29.059253246753247,\n  \"rt/h2-out/srv/169.254.1.1/4142/connections\": 2.0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/connect_latency_ms.count\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/failed_connect_latency_ms.count\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/sent_bytes\": 3527580,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/service_creation/service_acquisition_latency_ms.count\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/connection_received_bytes.count\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/connection_duration.count\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/connects\": 4,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/success\": 202,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/request_latency_ms.count\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/current_lease_ms\": 3.6893488E+13,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/drained\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/marked_busy\": 102,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.count\": 47,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.max\": 2795,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.min\": 790,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.p50\": 1119,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.p90\": 2633,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.p95\": 2740,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.p99\": 2795,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.p9990\": 2795,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.p9999\": 2795,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.sum\": 82775,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.avg\": 1761.1702127659576,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping\": 435157,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/revivals\": 106,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/close\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/leased\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/draining\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/retries.count\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/retries/budget_exhausted\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/received_bytes\": 3506068,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/nametreecache/evicts\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/nametreecache/misses\": 1,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/nametreecache/oneshots\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/dtabcache/evicts\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/dtabcache/misses\": 1,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/dtabcache/oneshots\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/namecache/evicts\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/namecache/misses\": 1,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/namecache/oneshots\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/bind_latency_us.count\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/bind/failures\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/bind/success\": 175,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/bind/requests\": 176,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/connection_sent_bytes.count\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/connection_requests.count\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/addrcache.size\": 0.0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/socket_unwritable_ms\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/closes\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/addr/failures\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/addr/success\": 27,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/addr/requests\": 28,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/available\": 4.0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/singletonpool/connects/fail\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/singletonpool/connects/dead\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/bindcache.size\": 0.0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/request_payload_bytes.count\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/socket_writable_ms\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/cancelled_connects\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/response_payload_bytes.count\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/dtab/size.count\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/requests\": 202,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/size\": 4.0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/rebuilds\": 1085195,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/closed\": 0.0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/load\": 2.0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/meanweight\": 1.0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/adds\": 4,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/p2c\": 1.0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/updates\": 1085195,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/available\": 4.0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/max_effort_exhausted\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/busy\": 0.0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/removes\": 0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/pending\": 2.0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/protocol/thriftmux\": 2.0,\n  \"rt/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/connections\": 4.0,\n  \"rt/h2-in/bindcache/path/evicts\": 0,\n  \"rt/h2-in/bindcache/path/misses\": 0,\n  \"rt/h2-in/bindcache/path/oneshots\": 0,\n  \"rt/h2-in/bindcache/bound/evicts\": 0,\n  \"rt/h2-in/bindcache/bound/misses\": 0,\n  \"rt/h2-in/bindcache/bound/oneshots\": 0,\n  \"rt/h2-in/bindcache/tree/evicts\": 0,\n  \"rt/h2-in/bindcache/tree/misses\": 0,\n  \"rt/h2-in/bindcache/tree/oneshots\": 0,\n  \"rt/h2-in/bindcache/client/evicts\": 0,\n  \"rt/h2-in/bindcache/client/misses\": 0,\n  \"rt/h2-in/bindcache/client/oneshots\": 0,\n  \"rt/h2-in/srv/10.142.54.113/4143/sent_bytes\": 0,\n  \"rt/h2-in/srv/10.142.54.113/4143/connection_received_bytes.count\": 0,\n  \"rt/h2-in/srv/10.142.54.113/4143/connection_duration.count\": 0,\n  \"rt/h2-in/srv/10.142.54.113/4143/connects\": 0,\n  \"rt/h2-in/srv/10.142.54.113/4143/success\": 0,\n  \"rt/h2-in/srv/10.142.54.113/4143/request_latency_ms.count\": 0,\n  \"rt/h2-in/srv/10.142.54.113/4143/received_bytes\": 0,\n  \"rt/h2-in/srv/10.142.54.113/4143/read_timeout\": 0,\n  \"rt/h2-in/srv/10.142.54.113/4143/write_timeout\": 0,\n  \"rt/h2-in/srv/10.142.54.113/4143/connection_sent_bytes.count\": 0,\n  \"rt/h2-in/srv/10.142.54.113/4143/connection_requests.count\": 0,\n  \"rt/h2-in/srv/10.142.54.113/4143/transit_latency_ms.count\": 0,\n  \"rt/h2-in/srv/10.142.54.113/4143/socket_unwritable_ms\": 0,\n  \"rt/h2-in/srv/10.142.54.113/4143/closes\": 0,\n  \"rt/h2-in/srv/10.142.54.113/4143/socket_writable_ms\": 0,\n  \"rt/h2-in/srv/10.142.54.113/4143/dtab/size.count\": 0,\n  \"rt/h2-in/srv/10.142.54.113/4143/requests\": 0,\n  \"rt/h2-in/srv/10.142.54.113/4143/pending\": 0.0,\n  \"rt/h2-in/srv/10.142.54.113/4143/handletime_us.count\": 0,\n  \"rt/h2-in/srv/10.142.54.113/4143/connections\": 0.0,\n  \"larger_than_threadlocal_out_buffer\": 0,\n  \"inet/dns/queue_size\": 0.0,\n  \"inet/dns/successes\": 362940,\n  \"inet/dns/cache/evicts\": 0.0,\n  \"inet/dns/cache/size\": 0.0,\n  \"inet/dns/cache/hit_rate\": 1.0,\n  \"inet/dns/dns_lookups\": 362940,\n  \"inet/dns/failures\": 0,\n  \"inet/dns/dns_lookup_failures\": 0,\n  \"inet/dns/lookup_ms.count\": 36,\n  \"inet/dns/lookup_ms.max\": 3,\n  \"inet/dns/lookup_ms.min\": 0,\n  \"inet/dns/lookup_ms.p50\": 0,\n  \"inet/dns/lookup_ms.p90\": 0,\n  \"inet/dns/lookup_ms.p95\": 3,\n  \"inet/dns/lookup_ms.p99\": 3,\n  \"inet/dns/lookup_ms.p9990\": 3,\n  \"inet/dns/lookup_ms.p9999\": 3,\n  \"inet/dns/lookup_ms.sum\": 11,\n  \"inet/dns/lookup_ms.avg\": 0.3055555555555556,\n  \"toggles/com.twitter.finagle.mux/checksum\": 1.65079066E+9,\n  \"toggles/com.twitter.finagle.netty4/checksum\": 2.18031974E+9,\n  \"zipkin/zipkin/log_span/ok\": 0,\n  \"zipkin/zipkin/log_span/try_later\": 0\n}\n  \n\n. We've tried a few other configurations, with no luck. Our original configuration had two intermediate linkerd instances doing TLS upgrade. Both configurations resulted in the same behavior. \n\nWe removed the TLS upgrade and tried with one linkerd instance. \nWe switched the client load balancer to be round robin. . @klingerf if there's anything I can do to lend a hand, let me know. I can test some additional configuration permutations on our end if you have any ideas. . @klingerf I can confirm that our logs indicate a similar issue, we see a Reset.NoError near the end of the last log I captured during this issue. . @hawkw maybe not, any idea how/where GO_AWAY would've been implemented without us adding it back? . @hawkw ok, I'll verify that we're seeing GO_AWAYs on our end and let you know \ud83d\udc4d . Verfied that we are, indeed receiving a GO_AWAY frame. Better late than never. :) \n\nApologies for the delay. . \ud83c\udf86 . Full metric dump: \njson\n{\n  \"jvm/start_time\": 1.49737374E12,\n  \"jvm/application_time_millis\": 1.78152528E8,\n  \"jvm/classes/total_loaded\": 9682.0,\n  \"jvm/classes/current_loaded\": 9421.0,\n  \"jvm/classes/total_unloaded\": 261.0,\n  \"jvm/postGC/Par_Survivor_Space/max\": 3.5782656E7,\n  \"jvm/postGC/Par_Survivor_Space/used\": 578784.0,\n  \"jvm/postGC/CMS_Old_Gen/max\": 7.1584973E8,\n  \"jvm/postGC/CMS_Old_Gen/used\": 2.928892E7,\n  \"jvm/postGC/Par_Eden_Space/max\": 2.86326784E8,\n  \"jvm/postGC/Par_Eden_Space/used\": 0.0,\n  \"jvm/postGC/used\": 2.9867704E7,\n  \"jvm/nonheap/committed\": 7.8045184E7,\n  \"jvm/nonheap/max\": -1.0,\n  \"jvm/nonheap/used\": 7.0783176E7,\n  \"jvm/tenuring_threshold\": 6.0,\n  \"jvm/thread/daemon_count\": 47.0,\n  \"jvm/thread/count\": 48.0,\n  \"jvm/thread/peak_count\": 71.0,\n  \"jvm/mem/postGC/Par_Survivor_Space/max\": 3.5782656E7,\n  \"jvm/mem/postGC/Par_Survivor_Space/used\": 578784.0,\n  \"jvm/mem/postGC/CMS_Old_Gen/max\": 7.1584973E8,\n  \"jvm/mem/postGC/CMS_Old_Gen/used\": 2.928892E7,\n  \"jvm/mem/postGC/Par_Eden_Space/max\": 2.86326784E8,\n  \"jvm/mem/postGC/Par_Eden_Space/used\": 0.0,\n  \"jvm/mem/postGC/used\": 2.9867704E7,\n  \"jvm/mem/metaspace/max_capacity\": 1.1051991E9,\n  \"jvm/mem/buffer/direct/max\": 1204600.0,\n  \"jvm/mem/buffer/direct/count\": 82.0,\n  \"jvm/mem/buffer/direct/used\": 1204601.0,\n  \"jvm/mem/buffer/mapped/max\": 0.0,\n  \"jvm/mem/buffer/mapped/count\": 0.0,\n  \"jvm/mem/buffer/mapped/used\": 0.0,\n  \"jvm/mem/allocations/eden/bytes\": 4.5079095E12,\n  \"jvm/mem/current/used\": 1.07524088E8,\n  \"jvm/mem/current/CMS_Old_Gen/max\": 7.1584973E8,\n  \"jvm/mem/current/CMS_Old_Gen/used\": 3.499188E7,\n  \"jvm/mem/current/Metaspace/max\": -1.0,\n  \"jvm/mem/current/Metaspace/used\": 5.5117384E7,\n  \"jvm/mem/current/Par_Eden_Space/max\": 2.86326784E8,\n  \"jvm/mem/current/Par_Eden_Space/used\": 1170248.0,\n  \"jvm/mem/current/Par_Survivor_Space/max\": 3.5782656E7,\n  \"jvm/mem/current/Par_Survivor_Space/used\": 578784.0,\n  \"jvm/mem/current/Compressed_Class_Space/max\": 1.07374182E9,\n  \"jvm/mem/current/Compressed_Class_Space/used\": 8795776.0,\n  \"jvm/mem/current/Code_Cache/max\": 5.0331648E7,\n  \"jvm/mem/current/Code_Cache/used\": 6870016.0,\n  \"jvm/num_cpus\": 8.0,\n  \"jvm/gc/msec\": 1319755.0,\n  \"jvm/gc/eden/pause_msec.count\": 229,\n  \"jvm/gc/eden/pause_msec.max\": 17,\n  \"jvm/gc/eden/pause_msec.min\": 1,\n  \"jvm/gc/eden/pause_msec.p50\": 3,\n  \"jvm/gc/eden/pause_msec.p90\": 4,\n  \"jvm/gc/eden/pause_msec.p95\": 6,\n  \"jvm/gc/eden/pause_msec.p99\": 14,\n  \"jvm/gc/eden/pause_msec.p9990\": 17,\n  \"jvm/gc/eden/pause_msec.p9999\": 17,\n  \"jvm/gc/eden/pause_msec.sum\": 728,\n  \"jvm/gc/eden/pause_msec.avg\": 3.1790393013100435,\n  \"jvm/gc/ParNew/msec\": 1296818.0,\n  \"jvm/gc/ParNew/cycles\": 506480.0,\n  \"jvm/gc/ConcurrentMarkSweep/msec\": 22937.0,\n  \"jvm/gc/ConcurrentMarkSweep/cycles\": 886.0,\n  \"jvm/gc/cycles\": 507366.0,\n  \"jvm/fd_limit\": 4096.0,\n  \"jvm/compilation/time_msec\": 72975.0,\n  \"jvm/uptime\": 1.79823104E8,\n  \"jvm/safepoint/sync_time_millis\": 50585.0,\n  \"jvm/safepoint/total_time_millis\": 1670997.0,\n  \"jvm/safepoint/count\": 568688.0,\n  \"jvm/heap/committed\": 6.7026944E7,\n  \"jvm/heap/max\": 1.03795917E9,\n  \"jvm/heap/used\": 3.6753104E7,\n  \"jvm/fd_count\": 381.0,\n  \"rt/h2-out/server/169.254.1.1/4142/sent_bytes\": 3215393905,\n  \"rt/h2-out/server/169.254.1.1/4142/connection_received_bytes.count\": 0,\n  \"rt/h2-out/server/169.254.1.1/4142/connection_duration.count\": 0,\n  \"rt/h2-out/server/169.254.1.1/4142/connects\": 14,\n  \"rt/h2-out/server/169.254.1.1/4142/success\": 27339599,\n  \"rt/h2-out/server/169.254.1.1/4142/request_latency_ms.count\": 10160,\n  \"rt/h2-out/server/169.254.1.1/4142/request_latency_ms.max\": 2013,\n  \"rt/h2-out/server/169.254.1.1/4142/request_latency_ms.min\": 1,\n  \"rt/h2-out/server/169.254.1.1/4142/request_latency_ms.p50\": 8,\n  \"rt/h2-out/server/169.254.1.1/4142/request_latency_ms.p90\": 18,\n  \"rt/h2-out/server/169.254.1.1/4142/request_latency_ms.p95\": 24,\n  \"rt/h2-out/server/169.254.1.1/4142/request_latency_ms.p99\": 62,\n  \"rt/h2-out/server/169.254.1.1/4142/request_latency_ms.p9990\": 1993,\n  \"rt/h2-out/server/169.254.1.1/4142/request_latency_ms.p9999\": 2013,\n  \"rt/h2-out/server/169.254.1.1/4142/request_latency_ms.sum\": 176387,\n  \"rt/h2-out/server/169.254.1.1/4142/request_latency_ms.avg\": 17.360925196850392,\n  \"rt/h2-out/server/169.254.1.1/4142/received_bytes\": 1329548255,\n  \"rt/h2-out/server/169.254.1.1/4142/read_timeout\": 0,\n  \"rt/h2-out/server/169.254.1.1/4142/write_timeout\": 0,\n  \"rt/h2-out/server/169.254.1.1/4142/connection_sent_bytes.count\": 0,\n  \"rt/h2-out/server/169.254.1.1/4142/connection_requests.count\": 0,\n  \"rt/h2-out/server/169.254.1.1/4142/transit_latency_ms.count\": 0,\n  \"rt/h2-out/server/169.254.1.1/4142/socket_unwritable_ms\": 0,\n  \"rt/h2-out/server/169.254.1.1/4142/closes\": 856,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/remote/data/frames\": 27422589,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/remote/data/bytes.count\": 10157,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/remote/data/bytes.max\": 17,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/remote/data/bytes.min\": 11,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/remote/data/bytes.p50\": 13,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/remote/data/bytes.p90\": 17,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/remote/data/bytes.p95\": 17,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/remote/data/bytes.p99\": 17,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/remote/data/bytes.p9990\": 17,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/remote/data/bytes.p9999\": 17,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/remote/data/bytes.sum\": 138150,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/remote/data/bytes.avg\": 13.60145712316629,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/remote/reset\": 82842,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/remote/trailers\": 0,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/local/data/frames\": 27339095,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/local/data/bytes.count\": 10132,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/local/data/bytes.max\": 122,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/local/data/bytes.min\": 5,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/local/data/bytes.p50\": 71,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/local/data/bytes.p90\": 86,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/local/data/bytes.p95\": 89,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/local/data/bytes.p99\": 98,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/local/data/bytes.p9990\": 121,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/local/data/bytes.p9999\": 122,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/local/data/bytes.sum\": 728689,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/local/data/bytes.avg\": 71.91956178444532,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/local/reset\": 0,\n  \"rt/h2-out/server/169.254.1.1/4142/stream/local/trailers\": 27339079,\n  \"rt/h2-out/server/169.254.1.1/4142/failures\": 82988,\n  \"rt/h2-out/server/169.254.1.1/4142/failures/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 79813,\n  \"rt/h2-out/server/169.254.1.1/4142/failures/interrupted\": 3130,\n  \"rt/h2-out/server/169.254.1.1/4142/failures/interrupted/com.twitter.finagle.Failure\": 3130,\n  \"rt/h2-out/server/169.254.1.1/4142/failures/interrupted/com.twitter.finagle.Failure/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 3130,\n  \"rt/h2-out/server/169.254.1.1/4142/failures/com.twitter.finagle.buoyant.h2.Reset$InternalError$\": 39,\n  \"rt/h2-out/server/169.254.1.1/4142/failures/restartable\": 3130,\n  \"rt/h2-out/server/169.254.1.1/4142/failures/restartable/com.twitter.finagle.Failure\": 3130,\n  \"rt/h2-out/server/169.254.1.1/4142/failures/restartable/com.twitter.finagle.Failure/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 3130,\n  \"rt/h2-out/server/169.254.1.1/4142/failures/com.twitter.finagle.buoyant.h2.Reset$Refused$\": 6,\n  \"rt/h2-out/server/169.254.1.1/4142/exn/java.io.IOException\": 1,\n  \"rt/h2-out/server/169.254.1.1/4142/socket_writable_ms\": 0,\n  \"rt/h2-out/server/169.254.1.1/4142/dtab/size.count\": 0,\n  \"rt/h2-out/server/169.254.1.1/4142/requests\": 27422587,\n  \"rt/h2-out/server/169.254.1.1/4142/pending\": 6.0,\n  \"rt/h2-out/server/169.254.1.1/4142/handletime_us.count\": 10157,\n  \"rt/h2-out/server/169.254.1.1/4142/handletime_us.max\": 1164,\n  \"rt/h2-out/server/169.254.1.1/4142/handletime_us.min\": 7,\n  \"rt/h2-out/server/169.254.1.1/4142/handletime_us.p50\": 16,\n  \"rt/h2-out/server/169.254.1.1/4142/handletime_us.p90\": 25,\n  \"rt/h2-out/server/169.254.1.1/4142/handletime_us.p95\": 30,\n  \"rt/h2-out/server/169.254.1.1/4142/handletime_us.p99\": 48,\n  \"rt/h2-out/server/169.254.1.1/4142/handletime_us.p9990\": 174,\n  \"rt/h2-out/server/169.254.1.1/4142/handletime_us.p9999\": 798,\n  \"rt/h2-out/server/169.254.1.1/4142/handletime_us.sum\": 184720,\n  \"rt/h2-out/server/169.254.1.1/4142/handletime_us.avg\": 18.18647238357783,\n  \"rt/h2-out/server/169.254.1.1/4142/connections\": 2.0,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/success\": 27339599,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.count\": 10161,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.max\": 2013,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.min\": 1,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p50\": 8,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p90\": 18,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p95\": 24,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p99\": 62,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p9990\": 1993,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p9999\": 2013,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.sum\": 175485,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.avg\": 17.27044582226159,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.count\": 10161,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.max\": 0,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.min\": 0,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.p50\": 0,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.p90\": 0,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.p95\": 0,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.p99\": 0,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.p9990\": 0,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.p9999\": 0,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.sum\": 0,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.avg\": 0.0,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/total\": 0,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/budget_exhausted\": 0,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/budget\": 405.0,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures\": 82988,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 79813,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/interrupted\": 3130,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/interrupted/com.twitter.finagle.Failure\": 3130,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/interrupted/com.twitter.finagle.Failure/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 3130,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/com.twitter.finagle.buoyant.h2.Reset$InternalError$\": 39,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/restartable\": 3130,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/restartable/com.twitter.finagle.Failure\": 3130,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/restartable/com.twitter.finagle.Failure/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 3130,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/com.twitter.finagle.buoyant.h2.Reset$Refused$\": 6,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/requests\": 27422587,\n  \"rt/h2-out/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/pending\": 6.0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/connect_latency_ms.count\": 8,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/connect_latency_ms.max\": 1633,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/connect_latency_ms.min\": 112,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/connect_latency_ms.p50\": 286,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/connect_latency_ms.p90\": 1523,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/connect_latency_ms.p95\": 1633,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/connect_latency_ms.p99\": 1633,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/connect_latency_ms.p9990\": 1633,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/connect_latency_ms.p9999\": 1633,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/connect_latency_ms.sum\": 6388,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/connect_latency_ms.avg\": 798.5,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/failed_connect_latency_ms.count\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/sent_bytes\": 3346641247,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/failures\": 5041,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/failures/com.twitter.finagle.Failure\": 5041,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/failures/com.twitter.finagle.Failure/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 3130,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/failures/com.twitter.finagle.Failure/com.twitter.finagle.ConnectionFailedException\": 1911,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/failures/com.twitter.finagle.Failure/com.twitter.finagle.ConnectionFailedException/io.netty.channel.ConnectTimeoutException\": 394,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/failures/com.twitter.finagle.Failure/com.twitter.finagle.ConnectionFailedException/java.nio.channels.ClosedChannelException\": 1,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/failures/com.twitter.finagle.Failure/com.twitter.finagle.ConnectionFailedException/io.netty.channel.AbstractChannel$AnnotatedConnectException\": 1516,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/failures/com.twitter.finagle.Failure/com.twitter.finagle.ConnectionFailedException/io.netty.channel.AbstractChannel$AnnotatedConnectException/java.net.ConnectException\": 1516,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.count\": 10157,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.max\": 1633,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.min\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.p50\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.p90\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.p95\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.p99\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.p9990\": 4,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.p9999\": 1523,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.sum\": 6423,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.avg\": 0.6323717633159397,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/connection_received_bytes.count\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/connection_duration.count\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/failure_accrual/removals\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/failure_accrual/probes\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/failure_accrual/removed_for_ms\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/failure_accrual/revivals\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/connects\": 3381,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/pool_num_waited\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/success\": 27339599,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/request_latency_ms.count\": 10160,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/request_latency_ms.max\": 2013,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/request_latency_ms.min\": 1,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/request_latency_ms.p50\": 8,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/request_latency_ms.p90\": 17,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/request_latency_ms.p95\": 23,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/request_latency_ms.p99\": 59,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/request_latency_ms.p9990\": 1993,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/request_latency_ms.p9999\": 2013,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/request_latency_ms.sum\": 167824,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/request_latency_ms.avg\": 16.518110236220473,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/pool_waiters\": 0.0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.count\": 10160,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.max\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.min\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.p50\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.p90\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.p95\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.p99\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.p9990\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.p9999\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.sum\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.avg\": 0.0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/request_limit\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/budget_exhausted\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/cannot_retry\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/not_open\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/budget\": 405.0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/retries/requeues\": 1911,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/received_bytes\": 3215499413,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/read_timeout\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/write_timeout\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/connection_sent_bytes.count\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/connection_requests.count\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/success\": 27339599,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.count\": 10160,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.max\": 2013,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.min\": 1,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p50\": 8,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p90\": 17,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p95\": 23,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p99\": 59,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p9990\": 1993,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p9999\": 2013,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.sum\": 167909,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.avg\": 16.526476377952754,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures\": 79858,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 79813,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/com.twitter.finagle.buoyant.h2.Reset$Refused$\": 6,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/com.twitter.finagle.buoyant.h2.Reset$InternalError$\": 39,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/requests\": 27419457,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/pending\": 6.0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/pool_num_too_many_waiters\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/socket_unwritable_ms\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/closes\": 14403,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/pool_cached\": 63.0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/nack_admission_control/dropped_requests\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/data/frames\": 27339583,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.count\": 10131,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.max\": 122,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.min\": 5,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.p50\": 71,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.p90\": 86,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.p95\": 89,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.p99\": 98,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.p9990\": 121,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.p9999\": 122,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.sum\": 728627,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.avg\": 71.92054091402626,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/reset\": 173,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/remote/trailers\": 27339570,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/data/frames\": 27419419,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.count\": 10157,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.max\": 17,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.min\": 11,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.p50\": 13,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.p90\": 17,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.p95\": 17,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.p99\": 17,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.p9990\": 17,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.p9999\": 17,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.sum\": 138150,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.avg\": 13.60145712316629,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/reset\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/stream/local/trailers\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/failures\": 79858,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/failures/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 79813,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/failures/com.twitter.finagle.buoyant.h2.Reset$Refused$\": 6,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/failures/com.twitter.finagle.buoyant.h2.Reset$InternalError$\": 39,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/pool_size\": 7.0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/exn/java.io.IOException\": 6,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/available\": 4.0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/socket_writable_ms\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/cancelled_connects\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/dtab/size.count\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/requests\": 27419457,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/loadbalancer/size\": 4.0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/loadbalancer/rebuilds\": 13,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/loadbalancer/closed\": 0.0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/loadbalancer/load\": 7.0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/loadbalancer/meanweight\": 1.0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/loadbalancer/adds\": 10,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/loadbalancer/updates\": 13,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/loadbalancer/algorithm/p2c_least_loaded\": 1.0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/loadbalancer/available\": 4.0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/loadbalancer/max_effort_exhausted\": 0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/loadbalancer/busy\": 0.0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/loadbalancer/removes\": 6,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/pending\": 7.0,\n  \"rt/h2-out/client/%/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig/connections\": 69.0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/connect_latency_ms.count\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/failed_connect_latency_ms.count\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/sent_bytes\": 290531,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/service_creation/service_acquisition_latency_ms.count\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/connection_received_bytes.count\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/connection_duration.count\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/connects\": 1,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/success\": 14,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/request_latency_ms.count\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/current_lease_ms\": 9.223372E12,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/drained\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/marked_busy\": 21,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.count\": 12,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.max\": 2530,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.min\": 660,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.p50\": 790,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.p90\": 1538,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.p95\": 1538,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.p99\": 2530,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.p9990\": 2530,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.p9999\": 2530,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.sum\": 12445,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.avg\": 1037.0833333333333,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping\": 35901,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/revivals\": 21,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/close\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/leased\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/draining\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/retries.count\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/retries/budget_exhausted\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/received_bytes\": 289695,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/read_timeout\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/write_timeout\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/nametreecache/expires\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/nametreecache/evicts\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/nametreecache/misses\": 1,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/nametreecache/oneshots\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/dtabcache/expires\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/dtabcache/evicts\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/dtabcache/misses\": 1,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/dtabcache/oneshots\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/namecache/expires\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/namecache/evicts\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/namecache/misses\": 1,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/namecache/oneshots\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/bind_latency_us.count\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/bind/failures\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/bind/success\": 1,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/bind/requests\": 2,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/connection_sent_bytes.count\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/connection_requests.count\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/addrcache.size\": 0.0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/socket_unwritable_ms\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/closes\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/nack_admission_control/dropped_requests\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/addr/failures\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/addr/success\": 13,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/addr/requests\": 14,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/available\": 1.0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/singletonpool/connects/fail\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/singletonpool/connects/dead\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/bindcache.size\": 0.0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/request_payload_bytes.count\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/socket_writable_ms\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/cancelled_connects\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/response_payload_bytes.count\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/dtab/size.count\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/requests\": 14,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/size\": 1.0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/rebuilds\": 7152,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/closed\": 0.0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/load\": 2.0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/meanweight\": 1.0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/adds\": 1,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/updates\": 7152,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/algorithm/p2c_least_loaded\": 1.0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/available\": 1.0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/max_effort_exhausted\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/busy\": 0.0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/removes\": 0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/pending\": 2.0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/protocol/thriftmux\": 1.0,\n  \"rt/h2-out/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/connections\": 1.0,\n  \"rt/h2-out/bindcache/path/expires\": 0,\n  \"rt/h2-out/bindcache/path/evicts\": 0,\n  \"rt/h2-out/bindcache/path/misses\": 1,\n  \"rt/h2-out/bindcache/path/oneshots\": 0,\n  \"rt/h2-out/bindcache/bound/expires\": 0,\n  \"rt/h2-out/bindcache/bound/evicts\": 0,\n  \"rt/h2-out/bindcache/bound/misses\": 1,\n  \"rt/h2-out/bindcache/bound/oneshots\": 0,\n  \"rt/h2-out/bindcache/tree/expires\": 0,\n  \"rt/h2-out/bindcache/tree/evicts\": 0,\n  \"rt/h2-out/bindcache/tree/misses\": 1,\n  \"rt/h2-out/bindcache/tree/oneshots\": 0,\n  \"rt/h2-out/bindcache/client/expires\": 0,\n  \"rt/h2-out/bindcache/client/evicts\": 0,\n  \"rt/h2-out/bindcache/client/misses\": 1,\n  \"rt/h2-out/bindcache/client/oneshots\": 0,\n  \"rt/http1-out/server/169.254.1.1/4140/success\": 0,\n  \"rt/http1-out/server/169.254.1.1/4140/request_latency_ms.count\": 0,\n  \"rt/http1-out/server/169.254.1.1/4140/nacks\": 0,\n  \"rt/http1-out/server/169.254.1.1/4140/transit_latency_ms.count\": 0,\n  \"rt/http1-out/server/169.254.1.1/4140/status/1XX\": 0,\n  \"rt/http1-out/server/169.254.1.1/4140/status/4XX\": 0,\n  \"rt/http1-out/server/169.254.1.1/4140/status/2XX\": 0,\n  \"rt/http1-out/server/169.254.1.1/4140/status/error\": 0,\n  \"rt/http1-out/server/169.254.1.1/4140/status/3XX\": 0,\n  \"rt/http1-out/server/169.254.1.1/4140/status/5XX\": 0,\n  \"rt/http1-out/server/169.254.1.1/4140/time/1XX.count\": 0,\n  \"rt/http1-out/server/169.254.1.1/4140/time/4XX.count\": 0,\n  \"rt/http1-out/server/169.254.1.1/4140/time/2XX.count\": 0,\n  \"rt/http1-out/server/169.254.1.1/4140/time/error.count\": 0,\n  \"rt/http1-out/server/169.254.1.1/4140/time/3XX.count\": 0,\n  \"rt/http1-out/server/169.254.1.1/4140/time/5XX.count\": 0,\n  \"rt/http1-out/server/169.254.1.1/4140/nonretryable_nacks\": 0,\n  \"rt/http1-out/server/169.254.1.1/4140/dtab/size.count\": 0,\n  \"rt/http1-out/server/169.254.1.1/4140/requests\": 0,\n  \"rt/http1-out/server/169.254.1.1/4140/pending\": 0.0,\n  \"rt/http1-out/server/169.254.1.1/4140/handletime_us.count\": 0,\n  \"rt/http1-out/bindcache/path/expires\": 0,\n  \"rt/http1-out/bindcache/path/evicts\": 0,\n  \"rt/http1-out/bindcache/path/misses\": 0,\n  \"rt/http1-out/bindcache/path/oneshots\": 0,\n  \"rt/http1-out/bindcache/bound/expires\": 0,\n  \"rt/http1-out/bindcache/bound/evicts\": 0,\n  \"rt/http1-out/bindcache/bound/misses\": 0,\n  \"rt/http1-out/bindcache/bound/oneshots\": 0,\n  \"rt/http1-out/bindcache/tree/expires\": 0,\n  \"rt/http1-out/bindcache/tree/evicts\": 0,\n  \"rt/http1-out/bindcache/tree/misses\": 0,\n  \"rt/http1-out/bindcache/tree/oneshots\": 0,\n  \"rt/http1-out/bindcache/client/expires\": 0,\n  \"rt/http1-out/bindcache/client/evicts\": 0,\n  \"rt/http1-out/bindcache/client/misses\": 0,\n  \"rt/http1-out/bindcache/client/oneshots\": 0,\n  \"rt/h2-in/server/10.176.65.111/4143/sent_bytes\": 4127116303,\n  \"rt/h2-in/server/10.176.65.111/4143/connection_received_bytes.count\": 0,\n  \"rt/h2-in/server/10.176.65.111/4143/connection_duration.count\": 0,\n  \"rt/h2-in/server/10.176.65.111/4143/connects\": 756,\n  \"rt/h2-in/server/10.176.65.111/4143/success\": 35099254,\n  \"rt/h2-in/server/10.176.65.111/4143/request_latency_ms.count\": 17421,\n  \"rt/h2-in/server/10.176.65.111/4143/request_latency_ms.max\": 240,\n  \"rt/h2-in/server/10.176.65.111/4143/request_latency_ms.min\": 0,\n  \"rt/h2-in/server/10.176.65.111/4143/request_latency_ms.p50\": 7,\n  \"rt/h2-in/server/10.176.65.111/4143/request_latency_ms.p90\": 16,\n  \"rt/h2-in/server/10.176.65.111/4143/request_latency_ms.p95\": 22,\n  \"rt/h2-in/server/10.176.65.111/4143/request_latency_ms.p99\": 39,\n  \"rt/h2-in/server/10.176.65.111/4143/request_latency_ms.p9990\": 94,\n  \"rt/h2-in/server/10.176.65.111/4143/request_latency_ms.p9999\": 201,\n  \"rt/h2-in/server/10.176.65.111/4143/request_latency_ms.sum\": 154350,\n  \"rt/h2-in/server/10.176.65.111/4143/request_latency_ms.avg\": 8.859996555880834,\n  \"rt/h2-in/server/10.176.65.111/4143/received_bytes\": 4268922461,\n  \"rt/h2-in/server/10.176.65.111/4143/read_timeout\": 0,\n  \"rt/h2-in/server/10.176.65.111/4143/write_timeout\": 0,\n  \"rt/h2-in/server/10.176.65.111/4143/connection_sent_bytes.count\": 0,\n  \"rt/h2-in/server/10.176.65.111/4143/connection_requests.count\": 0,\n  \"rt/h2-in/server/10.176.65.111/4143/transit_latency_ms.count\": 0,\n  \"rt/h2-in/server/10.176.65.111/4143/socket_unwritable_ms\": 0,\n  \"rt/h2-in/server/10.176.65.111/4143/closes\": 1197,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/remote/data/frames\": 35101114,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/remote/data/bytes.count\": 17423,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/remote/data/bytes.max\": 17,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/remote/data/bytes.min\": 11,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/remote/data/bytes.p50\": 13,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/remote/data/bytes.p90\": 17,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/remote/data/bytes.p95\": 17,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/remote/data/bytes.p99\": 17,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/remote/data/bytes.p9990\": 17,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/remote/data/bytes.p9999\": 17,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/remote/data/bytes.sum\": 236817,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/remote/data/bytes.avg\": 13.592205705102451,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/remote/reset\": 1869,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/remote/trailers\": 0,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/local/data/frames\": 35099188,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/local/data/bytes.count\": 17420,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/local/data/bytes.max\": 133,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/local/data/bytes.min\": 5,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/local/data/bytes.p50\": 71,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/local/data/bytes.p90\": 86,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/local/data/bytes.p95\": 89,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/local/data/bytes.p99\": 97,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/local/data/bytes.p9990\": 121,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/local/data/bytes.p9999\": 122,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/local/data/bytes.sum\": 1250659,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/local/data/bytes.avg\": 71.79443168771527,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/local/reset\": 0,\n  \"rt/h2-in/server/10.176.65.111/4143/stream/local/trailers\": 35099155,\n  \"rt/h2-in/server/10.176.65.111/4143/failures\": 2390,\n  \"rt/h2-in/server/10.176.65.111/4143/failures/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 2329,\n  \"rt/h2-in/server/10.176.65.111/4143/failures/interrupted\": 2,\n  \"rt/h2-in/server/10.176.65.111/4143/failures/interrupted/com.twitter.finagle.Failure\": 2,\n  \"rt/h2-in/server/10.176.65.111/4143/failures/interrupted/com.twitter.finagle.Failure/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 2,\n  \"rt/h2-in/server/10.176.65.111/4143/failures/com.twitter.finagle.ChannelWriteException\": 9,\n  \"rt/h2-in/server/10.176.65.111/4143/failures/com.twitter.finagle.ChannelWriteException/com.twitter.finagle.ChannelClosedException\": 9,\n  \"rt/h2-in/server/10.176.65.111/4143/failures/com.twitter.finagle.ChannelWriteException/com.twitter.finagle.ChannelClosedException/java.nio.channels.ClosedChannelException\": 9,\n  \"rt/h2-in/server/10.176.65.111/4143/failures/com.twitter.finagle.NoBrokersAvailableException\": 5,\n  \"rt/h2-in/server/10.176.65.111/4143/failures/nonretryable\": 38,\n  \"rt/h2-in/server/10.176.65.111/4143/failures/nonretryable/com.twitter.finagle.Failure\": 38,\n  \"rt/h2-in/server/10.176.65.111/4143/failures/nonretryable/com.twitter.finagle.Failure/com.twitter.finagle.ChannelWriteException\": 38,\n  \"rt/h2-in/server/10.176.65.111/4143/failures/nonretryable/com.twitter.finagle.Failure/com.twitter.finagle.ChannelWriteException/com.twitter.finagle.ChannelClosedException\": 38,\n  \"rt/h2-in/server/10.176.65.111/4143/failures/nonretryable/com.twitter.finagle.Failure/com.twitter.finagle.ChannelWriteException/com.twitter.finagle.ChannelClosedException/java.nio.channels.ClosedChannelException\": 38,\n  \"rt/h2-in/server/10.176.65.111/4143/failures/restartable\": 9,\n  \"rt/h2-in/server/10.176.65.111/4143/failures/restartable/com.twitter.finagle.Failure\": 9,\n  \"rt/h2-in/server/10.176.65.111/4143/failures/restartable/com.twitter.finagle.Failure/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 2,\n  \"rt/h2-in/server/10.176.65.111/4143/failures/restartable/com.twitter.finagle.Failure/com.twitter.finagle.ConnectionFailedException\": 7,\n  \"rt/h2-in/server/10.176.65.111/4143/failures/restartable/com.twitter.finagle.Failure/com.twitter.finagle.ConnectionFailedException/io.netty.channel.AbstractChannel$AnnotatedConnectException\": 7,\n  \"rt/h2-in/server/10.176.65.111/4143/failures/restartable/com.twitter.finagle.Failure/com.twitter.finagle.ConnectionFailedException/io.netty.channel.AbstractChannel$AnnotatedConnectException/java.net.ConnectException\": 7,\n  \"rt/h2-in/server/10.176.65.111/4143/failures/rejected\": 7,\n  \"rt/h2-in/server/10.176.65.111/4143/failures/rejected/com.twitter.finagle.Failure\": 7,\n  \"rt/h2-in/server/10.176.65.111/4143/failures/rejected/com.twitter.finagle.Failure/com.twitter.finagle.ConnectionFailedException\": 7,\n  \"rt/h2-in/server/10.176.65.111/4143/failures/rejected/com.twitter.finagle.Failure/com.twitter.finagle.ConnectionFailedException/io.netty.channel.AbstractChannel$AnnotatedConnectException\": 7,\n  \"rt/h2-in/server/10.176.65.111/4143/failures/rejected/com.twitter.finagle.Failure/com.twitter.finagle.ConnectionFailedException/io.netty.channel.AbstractChannel$AnnotatedConnectException/java.net.ConnectException\": 7,\n  \"rt/h2-in/server/10.176.65.111/4143/sourcedfailures//svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash\": 1,\n  \"rt/h2-in/server/10.176.65.111/4143/sourcedfailures//svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/com.twitter.finagle.NoBrokersAvailableException\": 1,\n  \"rt/h2-in/server/10.176.65.111/4143/sourcedfailures/10.176.65.111/4143\": 4,\n  \"rt/h2-in/server/10.176.65.111/4143/sourcedfailures/10.176.65.111/4143/com.twitter.finagle.NoBrokersAvailableException\": 4,\n  \"rt/h2-in/server/10.176.65.111/4143/sourcedfailures/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig\": 9,\n  \"rt/h2-in/server/10.176.65.111/4143/sourcedfailures/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/com.twitter.finagle.ChannelWriteException\": 9,\n  \"rt/h2-in/server/10.176.65.111/4143/sourcedfailures/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/com.twitter.finagle.ChannelWriteException/com.twitter.finagle.ChannelClosedException\": 9,\n  \"rt/h2-in/server/10.176.65.111/4143/sourcedfailures/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/com.twitter.finagle.ChannelWriteException/com.twitter.finagle.ChannelClosedException/java.nio.channels.ClosedChannelException\": 9,\n  \"rt/h2-in/server/10.176.65.111/4143/exn/java.io.IOException\": 4,\n  \"rt/h2-in/server/10.176.65.111/4143/socket_writable_ms\": 0,\n  \"rt/h2-in/server/10.176.65.111/4143/dtab/size.count\": 0,\n  \"rt/h2-in/server/10.176.65.111/4143/requests\": 35101644,\n  \"rt/h2-in/server/10.176.65.111/4143/pending\": 4.0,\n  \"rt/h2-in/server/10.176.65.111/4143/handletime_us.count\": 17423,\n  \"rt/h2-in/server/10.176.65.111/4143/handletime_us.max\": 5836,\n  \"rt/h2-in/server/10.176.65.111/4143/handletime_us.min\": 10,\n  \"rt/h2-in/server/10.176.65.111/4143/handletime_us.p50\": 16,\n  \"rt/h2-in/server/10.176.65.111/4143/handletime_us.p90\": 25,\n  \"rt/h2-in/server/10.176.65.111/4143/handletime_us.p95\": 30,\n  \"rt/h2-in/server/10.176.65.111/4143/handletime_us.p99\": 44,\n  \"rt/h2-in/server/10.176.65.111/4143/handletime_us.p9990\": 155,\n  \"rt/h2-in/server/10.176.65.111/4143/handletime_us.p9999\": 2908,\n  \"rt/h2-in/server/10.176.65.111/4143/handletime_us.sum\": 334679,\n  \"rt/h2-in/server/10.176.65.111/4143/handletime_us.avg\": 19.209034035470356,\n  \"rt/h2-in/server/10.176.65.111/4143/connections\": 106.0,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/success\": 35099254,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.count\": 17420,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.max\": 240,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.min\": 0,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p50\": 7,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p90\": 16,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p95\": 22,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p99\": 39,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p9990\": 94,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p9999\": 201,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.sum\": 152829,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.avg\": 8.773191733639495,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.count\": 17420,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.max\": 0,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.min\": 0,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.p50\": 0,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.p90\": 0,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.p95\": 0,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.p99\": 0,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.p9990\": 0,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.p9999\": 0,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.sum\": 0,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/per_request.avg\": 0.0,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/total\": 0,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/budget_exhausted\": 0,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/retries/budget\": 606.0,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures\": 2390,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 2329,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/interrupted\": 2,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/interrupted/com.twitter.finagle.Failure\": 2,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/interrupted/com.twitter.finagle.Failure/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 2,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/com.twitter.finagle.ChannelWriteException\": 9,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/com.twitter.finagle.ChannelWriteException/com.twitter.finagle.ChannelClosedException\": 9,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/com.twitter.finagle.ChannelWriteException/com.twitter.finagle.ChannelClosedException/java.nio.channels.ClosedChannelException\": 9,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/com.twitter.finagle.NoBrokersAvailableException\": 5,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/nonretryable\": 38,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/nonretryable/com.twitter.finagle.Failure\": 38,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/nonretryable/com.twitter.finagle.Failure/com.twitter.finagle.ChannelWriteException\": 38,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/nonretryable/com.twitter.finagle.Failure/com.twitter.finagle.ChannelWriteException/com.twitter.finagle.ChannelClosedException\": 38,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/nonretryable/com.twitter.finagle.Failure/com.twitter.finagle.ChannelWriteException/com.twitter.finagle.ChannelClosedException/java.nio.channels.ClosedChannelException\": 38,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/restartable\": 9,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/restartable/com.twitter.finagle.Failure\": 9,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/restartable/com.twitter.finagle.Failure/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 2,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/restartable/com.twitter.finagle.Failure/com.twitter.finagle.ConnectionFailedException\": 7,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/restartable/com.twitter.finagle.Failure/com.twitter.finagle.ConnectionFailedException/io.netty.channel.AbstractChannel$AnnotatedConnectException\": 7,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/restartable/com.twitter.finagle.Failure/com.twitter.finagle.ConnectionFailedException/io.netty.channel.AbstractChannel$AnnotatedConnectException/java.net.ConnectException\": 7,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/rejected\": 7,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/rejected/com.twitter.finagle.Failure\": 7,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/rejected/com.twitter.finagle.Failure/com.twitter.finagle.ConnectionFailedException\": 7,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/rejected/com.twitter.finagle.Failure/com.twitter.finagle.ConnectionFailedException/io.netty.channel.AbstractChannel$AnnotatedConnectException\": 7,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/rejected/com.twitter.finagle.Failure/com.twitter.finagle.ConnectionFailedException/io.netty.channel.AbstractChannel$AnnotatedConnectException/java.net.ConnectException\": 7,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/sourcedfailures//svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash\": 1,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/sourcedfailures//svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/com.twitter.finagle.NoBrokersAvailableException\": 1,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/sourcedfailures/10.176.65.111/4143\": 4,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/sourcedfailures/10.176.65.111/4143/com.twitter.finagle.NoBrokersAvailableException\": 4,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/sourcedfailures/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig\": 9,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/sourcedfailures/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/com.twitter.finagle.ChannelWriteException\": 9,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/sourcedfailures/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/com.twitter.finagle.ChannelWriteException/com.twitter.finagle.ChannelClosedException\": 9,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/sourcedfailures/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/com.twitter.finagle.ChannelWriteException/com.twitter.finagle.ChannelClosedException/java.nio.channels.ClosedChannelException\": 9,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/requests\": 35101644,\n  \"rt/h2-in/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/pending\": 4.0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/connect_latency_ms.count\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/failed_connect_latency_ms.count\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/sent_bytes\": 4306750804,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service_creation/failures\": 56,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service_creation/failures/com.twitter.finagle.NoBrokersAvailableException\": 5,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service_creation/failures/com.twitter.finagle.Failure\": 51,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service_creation/failures/com.twitter.finagle.Failure/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 2,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service_creation/failures/com.twitter.finagle.Failure/com.twitter.finagle.ConnectionFailedException\": 49,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service_creation/failures/com.twitter.finagle.Failure/com.twitter.finagle.ConnectionFailedException/io.netty.channel.AbstractChannel$AnnotatedConnectException\": 49,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service_creation/failures/com.twitter.finagle.Failure/com.twitter.finagle.ConnectionFailedException/io.netty.channel.AbstractChannel$AnnotatedConnectException/java.net.ConnectException\": 49,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.count\": 17422,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.max\": 5,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.min\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.p50\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.p90\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.p95\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.p99\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.p9990\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.p9999\": 4,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.sum\": 49,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service_creation/service_acquisition_latency_ms.avg\": 0.0028125358741820684,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/connection_received_bytes.count\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/connection_duration.count\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/failure_accrual/removals\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/failure_accrual/probes\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/failure_accrual/removed_for_ms\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/failure_accrual/revivals\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/connects\": 335,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/pool_num_waited\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/success\": 35099254,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/request_latency_ms.count\": 17420,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/request_latency_ms.max\": 240,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/request_latency_ms.min\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/request_latency_ms.p50\": 6,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/request_latency_ms.p90\": 16,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/request_latency_ms.p95\": 22,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/request_latency_ms.p99\": 39,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/request_latency_ms.p9990\": 94,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/request_latency_ms.p9999\": 201,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/request_latency_ms.sum\": 150928,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/request_latency_ms.avg\": 8.66406429391504,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/pool_waiters\": 0.0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.count\": 17420,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.max\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.min\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.p50\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.p90\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.p95\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.p99\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.p9990\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.p9999\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.sum\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/retries/requeues_per_request.avg\": 0.0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/retries/request_limit\": 32,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/retries/budget_exhausted\": 6,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/retries/cannot_retry\": 9,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/retries/not_open\": 7,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/retries/budget\": 595.0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/retries/requeues\": 3040,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/received_bytes\": 4047191353,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/read_timeout\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/write_timeout\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/connection_sent_bytes.count\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/connection_requests.count\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/success\": 35099254,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.count\": 17420,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.max\": 240,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.min\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p50\": 6,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p90\": 16,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p95\": 22,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p99\": 39,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p9990\": 94,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.p9999\": 201,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.sum\": 151009,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/request_latency_ms.avg\": 8.668714121699196,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures\": 5374,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 2329,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/com.twitter.finagle.ChannelWriteException\": 3045,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/com.twitter.finagle.ChannelWriteException/com.twitter.finagle.ChannelClosedException\": 3045,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/failures/com.twitter.finagle.ChannelWriteException/com.twitter.finagle.ChannelClosedException/java.nio.channels.ClosedChannelException\": 3045,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/requests\": 35104628,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/service/svc/bigcommerce.rpc.storeconfig.StoreConfig/GetStoreByHash/pending\": 4.0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/pool_num_too_many_waiters\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/socket_unwritable_ms\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/closes\": 2666,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/pool_cached\": 54.0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/nack_admission_control/dropped_requests\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/remote/data/frames\": 35099243,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.count\": 17420,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.max\": 133,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.min\": 5,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.p50\": 71,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.p90\": 86,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.p95\": 89,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.p99\": 97,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.p9990\": 121,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.p9999\": 122,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.sum\": 1250659,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/remote/data/bytes.avg\": 71.79443168771527,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/remote/reset\": 395,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/remote/trailers\": 35099241,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/local/data/frames\": 35100999,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.count\": 17421,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.max\": 17,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.min\": 11,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.p50\": 13,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.p90\": 17,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.p95\": 17,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.p99\": 17,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.p9990\": 17,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.p9999\": 17,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.sum\": 236788,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/local/data/bytes.avg\": 13.59210148671144,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/local/reset\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/stream/local/trailers\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/failures\": 5374,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/failures/com.twitter.finagle.buoyant.h2.Reset$Cancel$\": 2329,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/failures/com.twitter.finagle.ChannelWriteException\": 3045,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/failures/com.twitter.finagle.ChannelWriteException/com.twitter.finagle.ChannelClosedException\": 3045,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/failures/com.twitter.finagle.ChannelWriteException/com.twitter.finagle.ChannelClosedException/java.nio.channels.ClosedChannelException\": 3045,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/pool_size\": 4.0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/exn/java.io.IOException\": 44,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/available\": 1.0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/socket_writable_ms\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/cancelled_connects\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/dtab/size.count\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/requests\": 35104628,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/loadbalancer/size\": 1.0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/loadbalancer/rebuilds\": 13,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/loadbalancer/closed\": 0.0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/loadbalancer/load\": 4.0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/loadbalancer/meanweight\": 1.0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/loadbalancer/adds\": 3,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/loadbalancer/updates\": 13,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/loadbalancer/algorithm/p2c_least_loaded\": 1.0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/loadbalancer/available\": 1.0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/loadbalancer/max_effort_exhausted\": 0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/loadbalancer/busy\": 0.0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/loadbalancer/removes\": 2,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/pending\": 4.0,\n  \"rt/h2-in/client/%/io.l5d.localhost/#/io.l5d.consul/.local/storeconfig/connections\": 58.0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/connect_latency_ms.count\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/failed_connect_latency_ms.count\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/sent_bytes\": 290531,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/service_creation/service_acquisition_latency_ms.count\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/connection_received_bytes.count\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/connection_duration.count\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/connects\": 1,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/success\": 14,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/request_latency_ms.count\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/current_lease_ms\": 9.223372E12,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/drained\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/marked_busy\": 10,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.count\": 12,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.max\": 2480,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.min\": 641,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.p50\": 881,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.p90\": 1973,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.p95\": 1973,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.p99\": 2480,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.p9990\": 2480,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.p9999\": 2480,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.sum\": 15117,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping_latency_us.avg\": 1259.75,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/ping\": 35901,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/revivals\": 10,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/failuredetector/close\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/leased\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/mux/draining\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/retries.count\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/retries/budget_exhausted\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/received_bytes\": 289695,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/read_timeout\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/write_timeout\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/nametreecache/expires\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/nametreecache/evicts\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/nametreecache/misses\": 1,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/nametreecache/oneshots\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/dtabcache/expires\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/dtabcache/evicts\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/dtabcache/misses\": 1,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/dtabcache/oneshots\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/namecache/expires\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/namecache/evicts\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/namecache/misses\": 1,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/namecache/oneshots\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/namer/bind_latency_us.count\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/bind/failures\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/bind/success\": 1,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/bind/requests\": 2,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/connection_sent_bytes.count\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/connection_requests.count\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/addrcache.size\": 0.0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/socket_unwritable_ms\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/closes\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/nack_admission_control/dropped_requests\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/addr/failures\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/addr/success\": 13,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/addr/requests\": 14,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/available\": 1.0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/singletonpool/connects/fail\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/singletonpool/connects/dead\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/bindcache.size\": 0.0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/request_payload_bytes.count\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/socket_writable_ms\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/cancelled_connects\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/response_payload_bytes.count\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/dtab/size.count\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/requests\": 14,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/size\": 1.0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/rebuilds\": 7152,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/closed\": 0.0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/load\": 2.0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/meanweight\": 1.0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/adds\": 1,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/updates\": 7152,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/algorithm/p2c_least_loaded\": 1.0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/available\": 1.0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/max_effort_exhausted\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/busy\": 0.0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/loadbalancer/removes\": 0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/pending\": 2.0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/protocol/thriftmux\": 1.0,\n  \"rt/h2-in/interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig/connections\": 1.0,\n  \"rt/h2-in/bindcache/path/expires\": 0,\n  \"rt/h2-in/bindcache/path/evicts\": 0,\n  \"rt/h2-in/bindcache/path/misses\": 1,\n  \"rt/h2-in/bindcache/path/oneshots\": 0,\n  \"rt/h2-in/bindcache/bound/expires\": 0,\n  \"rt/h2-in/bindcache/bound/evicts\": 0,\n  \"rt/h2-in/bindcache/bound/misses\": 1,\n  \"rt/h2-in/bindcache/bound/oneshots\": 0,\n  \"rt/h2-in/bindcache/tree/expires\": 0,\n  \"rt/h2-in/bindcache/tree/evicts\": 0,\n  \"rt/h2-in/bindcache/tree/misses\": 1,\n  \"rt/h2-in/bindcache/tree/oneshots\": 0,\n  \"rt/h2-in/bindcache/client/expires\": 0,\n  \"rt/h2-in/bindcache/client/evicts\": 0,\n  \"rt/h2-in/bindcache/client/misses\": 1,\n  \"rt/h2-in/bindcache/client/oneshots\": 0,\n  \"rt/http1-in/server/10.176.65.111/4141/success\": 0,\n  \"rt/http1-in/server/10.176.65.111/4141/request_latency_ms.count\": 0,\n  \"rt/http1-in/server/10.176.65.111/4141/nacks\": 0,\n  \"rt/http1-in/server/10.176.65.111/4141/transit_latency_ms.count\": 0,\n  \"rt/http1-in/server/10.176.65.111/4141/status/1XX\": 0,\n  \"rt/http1-in/server/10.176.65.111/4141/status/4XX\": 0,\n  \"rt/http1-in/server/10.176.65.111/4141/status/2XX\": 0,\n  \"rt/http1-in/server/10.176.65.111/4141/status/error\": 0,\n  \"rt/http1-in/server/10.176.65.111/4141/status/3XX\": 0,\n  \"rt/http1-in/server/10.176.65.111/4141/status/5XX\": 0,\n  \"rt/http1-in/server/10.176.65.111/4141/time/1XX.count\": 0,\n  \"rt/http1-in/server/10.176.65.111/4141/time/4XX.count\": 0,\n  \"rt/http1-in/server/10.176.65.111/4141/time/2XX.count\": 0,\n  \"rt/http1-in/server/10.176.65.111/4141/time/error.count\": 0,\n  \"rt/http1-in/server/10.176.65.111/4141/time/3XX.count\": 0,\n  \"rt/http1-in/server/10.176.65.111/4141/time/5XX.count\": 0,\n  \"rt/http1-in/server/10.176.65.111/4141/nonretryable_nacks\": 0,\n  \"rt/http1-in/server/10.176.65.111/4141/dtab/size.count\": 0,\n  \"rt/http1-in/server/10.176.65.111/4141/requests\": 0,\n  \"rt/http1-in/server/10.176.65.111/4141/pending\": 0.0,\n  \"rt/http1-in/server/10.176.65.111/4141/handletime_us.count\": 0,\n  \"rt/http1-in/bindcache/path/expires\": 0,\n  \"rt/http1-in/bindcache/path/evicts\": 0,\n  \"rt/http1-in/bindcache/path/misses\": 0,\n  \"rt/http1-in/bindcache/path/oneshots\": 0,\n  \"rt/http1-in/bindcache/bound/expires\": 0,\n  \"rt/http1-in/bindcache/bound/evicts\": 0,\n  \"rt/http1-in/bindcache/bound/misses\": 0,\n  \"rt/http1-in/bindcache/bound/oneshots\": 0,\n  \"rt/http1-in/bindcache/tree/expires\": 0,\n  \"rt/http1-in/bindcache/tree/evicts\": 0,\n  \"rt/http1-in/bindcache/tree/misses\": 0,\n  \"rt/http1-in/bindcache/tree/oneshots\": 0,\n  \"rt/http1-in/bindcache/client/expires\": 0,\n  \"rt/http1-in/bindcache/client/evicts\": 0,\n  \"rt/http1-in/bindcache/client/misses\": 0,\n  \"rt/http1-in/bindcache/client/oneshots\": 0,\n  \"namer/#/io.l5d.consul/client/connect_latency_ms.count\": 1,\n  \"namer/#/io.l5d.consul/client/connect_latency_ms.max\": 2,\n  \"namer/#/io.l5d.consul/client/connect_latency_ms.min\": 2,\n  \"namer/#/io.l5d.consul/client/connect_latency_ms.p50\": 2,\n  \"namer/#/io.l5d.consul/client/connect_latency_ms.p90\": 2,\n  \"namer/#/io.l5d.consul/client/connect_latency_ms.p95\": 2,\n  \"namer/#/io.l5d.consul/client/connect_latency_ms.p99\": 2,\n  \"namer/#/io.l5d.consul/client/connect_latency_ms.p9990\": 2,\n  \"namer/#/io.l5d.consul/client/connect_latency_ms.p9999\": 2,\n  \"namer/#/io.l5d.consul/client/connect_latency_ms.sum\": 2,\n  \"namer/#/io.l5d.consul/client/connect_latency_ms.avg\": 2.0,\n  \"namer/#/io.l5d.consul/client/failed_connect_latency_ms.count\": 0,\n  \"namer/#/io.l5d.consul/client/sent_bytes\": 6115857,\n  \"namer/#/io.l5d.consul/client/service_creation/service_acquisition_latency_ms.count\": 10,\n  \"namer/#/io.l5d.consul/client/service_creation/service_acquisition_latency_ms.max\": 7,\n  \"namer/#/io.l5d.consul/client/service_creation/service_acquisition_latency_ms.min\": 0,\n  \"namer/#/io.l5d.consul/client/service_creation/service_acquisition_latency_ms.p50\": 0,\n  \"namer/#/io.l5d.consul/client/service_creation/service_acquisition_latency_ms.p90\": 1,\n  \"namer/#/io.l5d.consul/client/service_creation/service_acquisition_latency_ms.p95\": 7,\n  \"namer/#/io.l5d.consul/client/service_creation/service_acquisition_latency_ms.p99\": 7,\n  \"namer/#/io.l5d.consul/client/service_creation/service_acquisition_latency_ms.p9990\": 7,\n  \"namer/#/io.l5d.consul/client/service_creation/service_acquisition_latency_ms.p9999\": 7,\n  \"namer/#/io.l5d.consul/client/service_creation/service_acquisition_latency_ms.sum\": 8,\n  \"namer/#/io.l5d.consul/client/service_creation/service_acquisition_latency_ms.avg\": 0.8,\n  \"namer/#/io.l5d.consul/client/connection_received_bytes.count\": 1,\n  \"namer/#/io.l5d.consul/client/connection_received_bytes.max\": 27013,\n  \"namer/#/io.l5d.consul/client/connection_received_bytes.min\": 27013,\n  \"namer/#/io.l5d.consul/client/connection_received_bytes.p50\": 27013,\n  \"namer/#/io.l5d.consul/client/connection_received_bytes.p90\": 27013,\n  \"namer/#/io.l5d.consul/client/connection_received_bytes.p95\": 27013,\n  \"namer/#/io.l5d.consul/client/connection_received_bytes.p99\": 27013,\n  \"namer/#/io.l5d.consul/client/connection_received_bytes.p9990\": 27013,\n  \"namer/#/io.l5d.consul/client/connection_received_bytes.p9999\": 27013,\n  \"namer/#/io.l5d.consul/client/connection_received_bytes.sum\": 26903,\n  \"namer/#/io.l5d.consul/client/connection_received_bytes.avg\": 26903.0,\n  \"namer/#/io.l5d.consul/client/connection_duration.count\": 1,\n  \"namer/#/io.l5d.consul/client/connection_duration.max\": 53141,\n  \"namer/#/io.l5d.consul/client/connection_duration.min\": 53141,\n  \"namer/#/io.l5d.consul/client/connection_duration.p50\": 53141,\n  \"namer/#/io.l5d.consul/client/connection_duration.p90\": 53141,\n  \"namer/#/io.l5d.consul/client/connection_duration.p95\": 53141,\n  \"namer/#/io.l5d.consul/client/connection_duration.p99\": 53141,\n  \"namer/#/io.l5d.consul/client/connection_duration.p9990\": 53141,\n  \"namer/#/io.l5d.consul/client/connection_duration.p9999\": 53141,\n  \"namer/#/io.l5d.consul/client/connection_duration.sum\": 53266,\n  \"namer/#/io.l5d.consul/client/connection_duration.avg\": 53266.0,\n  \"namer/#/io.l5d.consul/client/failure_accrual/removals\": 0,\n  \"namer/#/io.l5d.consul/client/failure_accrual/probes\": 0,\n  \"namer/#/io.l5d.consul/client/failure_accrual/removed_for_ms\": 0,\n  \"namer/#/io.l5d.consul/client/failure_accrual/revivals\": 0,\n  \"namer/#/io.l5d.consul/client/connects\": 3584,\n  \"namer/#/io.l5d.consul/client/pool_num_waited\": 0,\n  \"namer/#/io.l5d.consul/client/success\": 34016,\n  \"namer/#/io.l5d.consul/client/request_latency_ms.count\": 10,\n  \"namer/#/io.l5d.consul/client/request_latency_ms.max\": 31992,\n  \"namer/#/io.l5d.consul/client/request_latency_ms.min\": 26,\n  \"namer/#/io.l5d.consul/client/request_latency_ms.p50\": 4927,\n  \"namer/#/io.l5d.consul/client/request_latency_ms.p90\": 30138,\n  \"namer/#/io.l5d.consul/client/request_latency_ms.p95\": 31992,\n  \"namer/#/io.l5d.consul/client/request_latency_ms.p99\": 31992,\n  \"namer/#/io.l5d.consul/client/request_latency_ms.p9990\": 31992,\n  \"namer/#/io.l5d.consul/client/request_latency_ms.p9999\": 31992,\n  \"namer/#/io.l5d.consul/client/request_latency_ms.sum\": 116317,\n  \"namer/#/io.l5d.consul/client/request_latency_ms.avg\": 11631.7,\n  \"namer/#/io.l5d.consul/client/pool_waiters\": 0.0,\n  \"namer/#/io.l5d.consul/client/received_bytes\": 192325263,\n  \"namer/#/io.l5d.consul/client/namer/nametreecache/expires\": 0,\n  \"namer/#/io.l5d.consul/client/namer/nametreecache/evicts\": 0,\n  \"namer/#/io.l5d.consul/client/namer/nametreecache/misses\": 1,\n  \"namer/#/io.l5d.consul/client/namer/nametreecache/oneshots\": 0,\n  \"namer/#/io.l5d.consul/client/namer/dtabcache/expires\": 0,\n  \"namer/#/io.l5d.consul/client/namer/dtabcache/evicts\": 0,\n  \"namer/#/io.l5d.consul/client/namer/dtabcache/misses\": 1,\n  \"namer/#/io.l5d.consul/client/namer/dtabcache/oneshots\": 0,\n  \"namer/#/io.l5d.consul/client/namer/namecache/expires\": 0,\n  \"namer/#/io.l5d.consul/client/namer/namecache/evicts\": 0,\n  \"namer/#/io.l5d.consul/client/namer/namecache/misses\": 1,\n  \"namer/#/io.l5d.consul/client/namer/namecache/oneshots\": 0,\n  \"namer/#/io.l5d.consul/client/namer/bind_latency_us.count\": 0,\n  \"namer/#/io.l5d.consul/client/failfast/marked_available\": 0,\n  \"namer/#/io.l5d.consul/client/failfast/unhealthy_num_tries\": 0.0,\n  \"namer/#/io.l5d.consul/client/failfast/marked_dead\": 0,\n  \"namer/#/io.l5d.consul/client/failfast/unhealthy_for_ms\": 0.0,\n  \"namer/#/io.l5d.consul/client/connection_sent_bytes.count\": 1,\n  \"namer/#/io.l5d.consul/client/connection_sent_bytes.max\": 701,\n  \"namer/#/io.l5d.consul/client/connection_sent_bytes.min\": 701,\n  \"namer/#/io.l5d.consul/client/connection_sent_bytes.p50\": 701,\n  \"namer/#/io.l5d.consul/client/connection_sent_bytes.p90\": 701,\n  \"namer/#/io.l5d.consul/client/connection_sent_bytes.p95\": 701,\n  \"namer/#/io.l5d.consul/client/connection_sent_bytes.p99\": 701,\n  \"namer/#/io.l5d.consul/client/connection_sent_bytes.p9990\": 701,\n  \"namer/#/io.l5d.consul/client/connection_sent_bytes.p9999\": 701,\n  \"namer/#/io.l5d.consul/client/connection_sent_bytes.sum\": 704,\n  \"namer/#/io.l5d.consul/client/connection_sent_bytes.avg\": 704.0,\n  \"namer/#/io.l5d.consul/client/connection_requests.count\": 1,\n  \"namer/#/io.l5d.consul/client/connection_requests.max\": 6,\n  \"namer/#/io.l5d.consul/client/connection_requests.min\": 6,\n  \"namer/#/io.l5d.consul/client/connection_requests.p50\": 6,\n  \"namer/#/io.l5d.consul/client/connection_requests.p90\": 6,\n  \"namer/#/io.l5d.consul/client/connection_requests.p95\": 6,\n  \"namer/#/io.l5d.consul/client/connection_requests.p99\": 6,\n  \"namer/#/io.l5d.consul/client/connection_requests.p9990\": 6,\n  \"namer/#/io.l5d.consul/client/connection_requests.p9999\": 6,\n  \"namer/#/io.l5d.consul/client/connection_requests.sum\": 6,\n  \"namer/#/io.l5d.consul/client/connection_requests.avg\": 6.0,\n  \"namer/#/io.l5d.consul/client/pool_num_too_many_waiters\": 0,\n  \"namer/#/io.l5d.consul/client/socket_unwritable_ms\": 0,\n  \"namer/#/io.l5d.consul/client/closes\": 3581,\n  \"namer/#/io.l5d.consul/client/pool_cached\": 1.0,\n  \"namer/#/io.l5d.consul/client/nack_admission_control/dropped_requests\": 0,\n  \"namer/#/io.l5d.consul/client/pool_size\": 2.0,\n  \"namer/#/io.l5d.consul/client/available\": 3.0,\n  \"namer/#/io.l5d.consul/client/request_payload_bytes.count\": 10,\n  \"namer/#/io.l5d.consul/client/request_payload_bytes.max\": 0,\n  \"namer/#/io.l5d.consul/client/request_payload_bytes.min\": 0,\n  \"namer/#/io.l5d.consul/client/request_payload_bytes.p50\": 0,\n  \"namer/#/io.l5d.consul/client/request_payload_bytes.p90\": 0,\n  \"namer/#/io.l5d.consul/client/request_payload_bytes.p95\": 0,\n  \"namer/#/io.l5d.consul/client/request_payload_bytes.p99\": 0,\n  \"namer/#/io.l5d.consul/client/request_payload_bytes.p9990\": 0,\n  \"namer/#/io.l5d.consul/client/request_payload_bytes.p9999\": 0,\n  \"namer/#/io.l5d.consul/client/request_payload_bytes.sum\": 0,\n  \"namer/#/io.l5d.consul/client/request_payload_bytes.avg\": 0.0,\n  \"namer/#/io.l5d.consul/client/socket_writable_ms\": 0,\n  \"namer/#/io.l5d.consul/client/cancelled_connects\": 0,\n  \"namer/#/io.l5d.consul/client/response_payload_bytes.count\": 10,\n  \"namer/#/io.l5d.consul/client/response_payload_bytes.max\": 6576,\n  \"namer/#/io.l5d.consul/client/response_payload_bytes.min\": 1508,\n  \"namer/#/io.l5d.consul/client/response_payload_bytes.p50\": 6447,\n  \"namer/#/io.l5d.consul/client/response_payload_bytes.p90\": 6576,\n  \"namer/#/io.l5d.consul/client/response_payload_bytes.p95\": 6576,\n  \"namer/#/io.l5d.consul/client/response_payload_bytes.p99\": 6576,\n  \"namer/#/io.l5d.consul/client/response_payload_bytes.p9990\": 6576,\n  \"namer/#/io.l5d.consul/client/response_payload_bytes.p9999\": 6576,\n  \"namer/#/io.l5d.consul/client/response_payload_bytes.sum\": 49980,\n  \"namer/#/io.l5d.consul/client/response_payload_bytes.avg\": 4998.0,\n  \"namer/#/io.l5d.consul/client/dtab/size.count\": 0,\n  \"namer/#/io.l5d.consul/client/requests\": 34016,\n  \"namer/#/io.l5d.consul/client/loadbalancer/size\": 3.0,\n  \"namer/#/io.l5d.consul/client/loadbalancer/rebuilds\": 35892,\n  \"namer/#/io.l5d.consul/client/loadbalancer/closed\": 0.0,\n  \"namer/#/io.l5d.consul/client/loadbalancer/load\": 2.0,\n  \"namer/#/io.l5d.consul/client/loadbalancer/meanweight\": 1.0,\n  \"namer/#/io.l5d.consul/client/loadbalancer/adds\": 3591,\n  \"namer/#/io.l5d.consul/client/loadbalancer/updates\": 35892,\n  \"namer/#/io.l5d.consul/client/loadbalancer/algorithm/p2c_least_loaded\": 1.0,\n  \"namer/#/io.l5d.consul/client/loadbalancer/available\": 3.0,\n  \"namer/#/io.l5d.consul/client/loadbalancer/max_effort_exhausted\": 0,\n  \"namer/#/io.l5d.consul/client/loadbalancer/busy\": 0.0,\n  \"namer/#/io.l5d.consul/client/loadbalancer/removes\": 3588,\n  \"namer/#/io.l5d.consul/client/pending\": 2.0,\n  \"namer/#/io.l5d.consul/client/connections\": 3.0,\n  \"namer/#/io.l5d.consul/#/io.l5d.consul/lookups\": 4,\n  \"namer/#/io.l5d.consul/#/io.l5d.consul/service/opens\": 1,\n  \"namer/#/io.l5d.consul/#/io.l5d.consul/service/updates\": 7152,\n  \"namer/#/io.l5d.consul/#/io.l5d.consul/service/closes\": 0,\n  \"namer/#/io.l5d.consul/#/io.l5d.consul/service/errors\": 0,\n  \"namer/#/io.l5d.consul/#/io.l5d.consul/dc/errors\": 0,\n  \"namer/#/io.l5d.consul/#/io.l5d.consul/dc/adds\": 32822,\n  \"namer/#/io.l5d.consul/#/io.l5d.consul/dc/updates\": 26863,\n  \"namer/#/io.l5d.consul/#/io.l5d.consul/dc/closes\": 0,\n  \"namer/#/io.l5d.consul/#/io.l5d.consul/dc/opens\": 1,\n  \"namer/#/io.l5d.consul/#/io.l5d.consul/dc/removes\": 32460,\n  \"namer/#/io.l5d.consul/dispatcher/serial/queue_size\": 0.0,\n  \"namer/#/io.l5d.consul_to_linker/client/request_payload_bytes.count\": 0,\n  \"namer/#/io.l5d.consul_to_linker/client/response_payload_bytes.count\": 0,\n  \"namer/#/io.l5d.consul_to_linker/client/namer/nametreecache/expires\": 0,\n  \"namer/#/io.l5d.consul_to_linker/client/namer/nametreecache/evicts\": 0,\n  \"namer/#/io.l5d.consul_to_linker/client/namer/nametreecache/misses\": 0,\n  \"namer/#/io.l5d.consul_to_linker/client/namer/nametreecache/oneshots\": 0,\n  \"namer/#/io.l5d.consul_to_linker/client/namer/dtabcache/expires\": 0,\n  \"namer/#/io.l5d.consul_to_linker/client/namer/dtabcache/evicts\": 0,\n  \"namer/#/io.l5d.consul_to_linker/client/namer/dtabcache/misses\": 0,\n  \"namer/#/io.l5d.consul_to_linker/client/namer/dtabcache/oneshots\": 0,\n  \"namer/#/io.l5d.consul_to_linker/client/namer/namecache/expires\": 0,\n  \"namer/#/io.l5d.consul_to_linker/client/namer/namecache/evicts\": 0,\n  \"namer/#/io.l5d.consul_to_linker/client/namer/namecache/misses\": 0,\n  \"namer/#/io.l5d.consul_to_linker/client/namer/namecache/oneshots\": 0,\n  \"namer/#/io.l5d.consul_to_linker/#/io.l5d.consul_to_linker/lookups\": 0\n}. Here's what the admin looks like on the server-side linkerd at 10.143.147.85, no failures (for clarification, the stats above are for the client-side linkerd in a linker-to-linker setup). \n\nI should also add that this behavior is not uniform across all linkerd instances. Some client linkerds seem to be fine, and even most connections on a linkerd instance seem to be fine. In the case of the node above, it's only requests to 10.143.147.85 that are failing. This suggests that there are individual connections in the client's load balancer that are entering a bad state and not being evicted for some reason. \n. Here's a leak suspects report from a misbehaving linkerd on our end: \n\n. Testing these fixes now, will report back.. Seeing this, currently investigating what the source is. \nJul 03 18:38:43 nomad-client8-p.dal9sl.bigcommerce.net linkerd[41298]: Failure(com.twitter.finagle.CancelledConnectionException, flags=0x03) with RemoteInfo -> Upstream Address: /172.17.0.3:56724, Upstream Client Id: Not Available, Downstream Address: /10.173.117.229:4143, Downstream Client Id: %/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig, Trace Id: 459fe3e40fdf0b9d.459fe3e40fdf0b9d<:459fe3e40fdf0b9d with Service -> 169.254.1.1/4142\nJul 03 18:38:43 nomad-client8-p.dal9sl.bigcommerce.net linkerd[41298]: Caused by: com.twitter.finagle.CancelledConnectionException\nJul 03 18:38:43 nomad-client8-p.dal9sl.bigcommerce.net linkerd[41298]: I 0703 23:38:43.319 UTC THREAD128: [S L:/169.254.1.1:4142 R:/172.17.0.3:56724 S:119] interrupted; resetting remote: CANCEL\nJul 03 18:38:43 nomad-client8-p.dal9sl.bigcommerce.net linkerd[41298]: Failure(com.twitter.finagle.CancelledConnectionException, flags=0x03) with RemoteInfo -> Upstream Address: /172.17.0.3:56724, Upstream Client Id: Not Available, Downstream Address: /10.173.117.229:4143, Downstream Client Id: %/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig, Trace Id: 459fe3e40fdf0b9d.459fe3e40fdf0b9d<:459fe3e40fdf0b9d with Service -> 169.254.1.1/4142\nJul 03 18:38:43 nomad-client8-p.dal9sl.bigcommerce.net linkerd[41298]: Caused by: com.twitter.finagle.CancelledConnectionException\nJul 03 19:30:50 nomad-client8-p.dal9sl.bigcommerce.net linkerd[41298]: W 0704 00:30:50.701 UTC THREAD132 TraceId:ed4d2a398cbbb05c: Exception propagated to the default monitor (upstream address: /172.17.0.3:56724, downstream address: /10.176.36.173:4143, label: %/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig).\nJul 03 19:30:50 nomad-client8-p.dal9sl.bigcommerce.net linkerd[41298]: Reset.CancelJul 03 18:38:43 nomad-client8-p.dal9sl.bigcommerce.net linkerd[41298]: Failure(com.twitter.finagle.CancelledConnectionException, flags=0x03) with RemoteInfo -> Upstream Address: /172.17.0.3:56724, Upstream Client Id: Not Available, Downstream Address: /10.173.117.229:4143, Downstream Client Id: %/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig, Trace Id: 459fe3e40fdf0b9d.459fe3e40fdf0b9d<:459fe3e40fdf0b9d with Service -> 169.254.1.1/4142\nJul 03 18:38:43 nomad-client8-p.dal9sl.bigcommerce.net linkerd[41298]: Caused by: com.twitter.finagle.CancelledConnectionException\nJul 03 18:38:43 nomad-client8-p.dal9sl.bigcommerce.net linkerd[41298]: I 0703 23:38:43.319 UTC THREAD128: [S L:/169.254.1.1:4142 R:/172.17.0.3:56724 S:119] interrupted; resetting remote: CANCEL\nJul 03 18:38:43 nomad-client8-p.dal9sl.bigcommerce.net linkerd[41298]: Failure(com.twitter.finagle.CancelledConnectionException, flags=0x03) with RemoteInfo -> Upstream Address: /172.17.0.3:56724, Upstream Client Id: Not Available, Downstream Address: /10.173.117.229:4143, Downstream Client Id: %/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig, Trace Id: 459fe3e40fdf0b9d.459fe3e40fdf0b9d<:459fe3e40fdf0b9d with Service -> 169.254.1.1/4142\nJul 03 18:38:43 nomad-client8-p.dal9sl.bigcommerce.net linkerd[41298]: Caused by: com.twitter.finagle.CancelledConnectionException\nJul 03 19:30:50 nomad-client8-p.dal9sl.bigcommerce.net linkerd[41298]: W 0704 00:30:50.701 UTC THREAD132 TraceId:ed4d2a398cbbb05c: Exception propagated to the default monitor (upstream address: /172.17.0.3:56724, downstream address: /10.176.36.173:4143, label: %/io.l5d.port/4143/#/io.l5d.consul/.local/storeconfig).\nJul 03 19:30:50 nomad-client8-p.dal9sl.bigcommerce.net linkerd[41298]: Reset.Cancel. We've been running this against a mirror of our production traffic since Monday at 1:30pm PST and it appears to be stable.  \ud83d\udc4d\ud83c\udffb. We've been struggling to get by without this for a while, but we're running enough traffic through linkerd now where it'd probably be worth trying to implement. \n@olix0r @adleong thoughts on trying to do this the same way HAProxy does it? \nhttps://www.haproxy.com/blog/truly-seamless-reloads-with-haproxy-no-more-hacks/\nBasically we'd set SO_REUSEPORT on the server sockets which would let multiple PIDs bind and accept connections on same socket. We'd then change our restart behavior to bring up a 2nd linkerd, wait for it to be healthy, then gracefully shutdown the old linkerd. \nIt looks like Netty has support for this sockopt on the kernels that support it (mainly linux 3.9+). I'm going to have to do some more digging on how we'd make this a configurable option on linkerd servers (or if we'd even want it to be configurable?). \nThoughts? . @DukeyToo can you share how many CPUs are on the linkerd hosts? . @DukeyToo are you seeing this exception in your linkerd logs? \nNov 03 17:36:55  WARN 1103 17:36:55.421 CDT finagle/netty4-6: An exceptionCaught() event was fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did not handle the exception.\nNov 03 17:36:55  java.io.IOException: Connection reset by peer\nNov 03 17:36:55  at sun.nio.ch.FileDispatcherImpl.read0(Native Method)\nNov 03 17:36:55  at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\nNov 03 17:36:55  at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\nNov 03 17:36:55  at sun.nio.ch.IOUtil.read(IOUtil.java:192). @hawkw we don't think this is causing a memory leak in the h2 stack anymore? . Awesome! \ud83d\udc4f\ud83c\udffb \ud83d\ude0e \ud83d\udcab \ud83c\udf89 . Did a quick GraalVM compile run to see if there were any obvious issues. Looks like we're doing some reflection-ey/classloader-ey things that it doesn't like: \n```\n./native-image --no-server -cp linkerd-1.4.0-SNAPSHOT.jar io.buoyant.linkerd.Main\n   classlist:  12,929.97 ms\n       (cap):   2,482.33 ms\n       setup:   3,897.65 ms\nMay 15, 2018 3:31:42 PM com.twitter.finagle.http.HttpMuxer$ $anonfun$new$1\nINFO: HttpMuxer[/admin/metrics.json] = com.twitter.finagle.stats.MetricsExporter()\nMay 15, 2018 3:31:42 PM com.twitter.finagle.http.HttpMuxer$ $anonfun$new$1\nINFO: HttpMuxer[/admin/per_host_metrics.json] = com.twitter.finagle.stats.HostMetricsExporter()\nRecomputeFieldValue.FieldOffset automatic substitution failed. The automatic substitution registration was attempted because a call to sun.misc.Unsafe.objectFieldOffset(Field) was detected in the static initializer of com.twitter.util.Promise$. Add a RecomputeFieldValue.FieldOffset manual substitution for com.twitter.util.Promise$.\nwarning: unknown locality of class Lcom/twitter/io/Buf$HashingProcessor$1;, assuming class is not local. To remove the warning report an issue to the library or language author. The issue is caused by Lcom/twitter/io/Buf$HashingProcessor$1; which is not following the naming convention.\n    analysis:  16,244.00 ms\nerror: unsupported features in 6 methods\nDetailed message:\nError: Must not have a started Thread in the image heap.\nTrace:  object java.util.concurrent.ForkJoinPool$WorkQueue\n    object java.util.concurrent.ForkJoinPool$WorkQueue[]\n    object java.util.concurrent.ForkJoinPool\n    object com.github.benmanes.caffeine.cache.LocalCacheFactory$WSLi\n    object com.github.benmanes.caffeine.cache.BoundedLocalCache$BoundedLocalManualCache\n    object com.twitter.finagle.stats.StatsReceiverWithCumulativeGauges$$anon$2$$anon$3\n    object java.util.concurrent.ConcurrentHashMap$Node\n    object java.util.concurrent.ConcurrentHashMap$Node[]\n    object java.util.concurrent.ConcurrentHashMap\n    object com.twitter.finagle.stats.MetricsStatsReceiver\n    object com.twitter.finagle.stats.LoadedStatsReceiver$\n    field com.twitter.finagle.stats.LoadedStatsReceiver$.MODULE$\nError: Must not have a started Thread in the image heap.\nTrace:  object java.util.concurrent.ForkJoinPool$WorkQueue\n    object java.util.concurrent.ForkJoinPool$WorkQueue[]\n    object java.util.concurrent.ForkJoinPool\n    object com.github.benmanes.caffeine.cache.LocalCacheFactory$WSLi\n    object com.github.benmanes.caffeine.cache.BoundedLocalCache$BoundedLocalManualCache\n    object com.twitter.finagle.stats.StatsReceiverWithCumulativeGauges$$anon$2$$anon$3\n    object java.util.concurrent.ConcurrentHashMap$Node\n    object java.util.concurrent.ConcurrentHashMap$Node[]\n    object java.util.concurrent.ConcurrentHashMap\n    object com.twitter.finagle.stats.MetricsStatsReceiver\n    object com.twitter.finagle.stats.LoadedStatsReceiver$\n    field com.twitter.finagle.stats.LoadedStatsReceiver$.MODULE$\nError: com.oracle.graal.pointsto.constraints.UnsupportedFeatureException: Unsupported field java.lang.reflect.Proxy.proxyClassCache is reachable\nTo diagnose the issue, you can add the option -H:+ReportUnsupportedElementsAtRuntime. The unsupported element is then reported at run time when it is accessed the first time.\nTrace:\n    at parsing java.lang.reflect.Proxy.getProxyClass0(Proxy.java:419)\nCall path from entry point to java.lang.reflect.Proxy.getProxyClass0(ClassLoader, Class[]):\n    at java.lang.reflect.Proxy.getProxyClass0(Proxy.java:412)\n    at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:719)\n    at com.twitter.util.SunSignalHandler.handle(Signal.scala:58)\n    at com.twitter.util.HandleSignal$.$anonfun$apply$1(Signal.scala:85)\n    at com.twitter.util.HandleSignal$.$anonfun$apply$1$adapted(Signal.scala:85)\n    at com.twitter.util.HandleSignal$$$Lambda$828/1664204892.apply(Unknown Source)\n    at scala.Option.foreach(Option.scala:257)\n    at com.twitter.logging.FileHandler.publish(FileHandler.scala:283)\n    at java.util.logging.Logger.log(Logger.java:738)\n    at java.util.logging.Logger.doLog(Logger.java:765)\n    at java.util.logging.Logger.log(Logger.java:876)\n    at com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$notifyRemoval$1(BoundedLocalCache.java:288)\n    at com.github.benmanes.caffeine.cache.BoundedLocalCache$$Lambda$857/1148776417.run(Unknown Source)\n    at com.oracle.svm.core.JavaMainWrapper$JavaMainSupport.executeHooks(JavaMainWrapper.java:105)\n    at com.oracle.svm.core.JavaMainWrapper$JavaMainSupport.executeStartupHooks(JavaMainWrapper.java:91)\n    at com.oracle.svm.core.JavaMainWrapper.run(JavaMainWrapper.java:198)\n    at Lcom/oracle/svm/core/code/CEntryPointCallStubs;.com_002eoracle_002esvm_002ecore_002eJavaMainWrapper_002erun_0028int_002corg_002egraalvm_002enativeimage_002ec_002etype_002eCCharPointerPointer_0029(generated:0)\nError: com.oracle.graal.pointsto.constraints.UnsupportedFeatureException: Unsupported field java.lang.reflect.Proxy.proxyClassCache is reachable\nTo diagnose the issue, you can add the option -H:+ReportUnsupportedElementsAtRuntime. The unsupported element is then reported at run time when it is accessed the first time.\nTrace:\n    at parsing java.lang.reflect.Proxy.isProxyClass(Proxy.java:791)\nCall path from entry point to java.lang.reflect.Proxy.isProxyClass(Class):\n    at java.lang.reflect.Proxy.isProxyClass(Proxy.java:791)\n    at sun.reflect.misc.ReflectUtil.isNonPublicProxyClass(ReflectUtil.java:289)\n    at java.lang.reflect.Proxy.checkNewProxyPermission(Proxy.java:757)\n    at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:726)\n    at com.twitter.util.SunSignalHandler.handle(Signal.scala:58)\n    at com.twitter.util.HandleSignal$.$anonfun$apply$1(Signal.scala:85)\n    at com.twitter.util.HandleSignal$.$anonfun$apply$1$adapted(Signal.scala:85)\n    at com.twitter.util.HandleSignal$$$Lambda$828/1664204892.apply(Unknown Source)\n    at scala.Option.foreach(Option.scala:257)\n    at com.twitter.logging.FileHandler.publish(FileHandler.scala:283)\n    at java.util.logging.Logger.log(Logger.java:738)\n    at java.util.logging.Logger.doLog(Logger.java:765)\n    at java.util.logging.Logger.log(Logger.java:876)\n    at com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$notifyRemoval$1(BoundedLocalCache.java:288)\n    at com.github.benmanes.caffeine.cache.BoundedLocalCache$$Lambda$857/1148776417.run(Unknown Source)\n    at com.oracle.svm.core.JavaMainWrapper$JavaMainSupport.executeHooks(JavaMainWrapper.java:105)\n    at com.oracle.svm.core.JavaMainWrapper$JavaMainSupport.executeStartupHooks(JavaMainWrapper.java:91)\n    at com.oracle.svm.core.JavaMainWrapper.run(JavaMainWrapper.java:198)\n    at Lcom/oracle/svm/core/code/CEntryPointCallStubs;.com_002eoracle_002esvm_002ecore_002eJavaMainWrapper_002erun_0028int_002corg_002egraalvm_002enativeimage_002ec_002etype_002eCCharPointerPointer_0029(generated:0)\nError: com.oracle.graal.pointsto.constraints.UnsupportedFeatureException: Unsupported method java.lang.ClassLoader.getParent() is reachable: The declaring class of this element has been substituted, but this element is not present in the substitution class\nTo diagnose the issue, you can add the option -H:+ReportUnsupportedElementsAtRuntime. The unsupported element is then reported at run time when it is accessed the first time.\nTrace:\n    at parsing com.twitter.app.ClassPath.getEntries(ClassPath.scala:66)\nCall path from entry point to com.twitter.app.ClassPath.getEntries(ClassLoader):\n    at com.twitter.app.ClassPath.getEntries(ClassPath.scala:65)\n    at com.twitter.app.ClassPath.browse(ClassPath.scala:57)\n    at com.twitter.app.GlobalFlag$.getAll(GlobalFlag.scala:169)\n    at com.twitter.app.GlobalFlag$.getAllOrEmptyArray(GlobalFlag.scala:146)\n    at com.twitter.app.Flags.usage(Flags.scala:331)\n    at com.twitter.app.App.$anonfun$main$1(App.scala:309)\n    at com.twitter.app.App$$Lambda$691/320891981.apply(Unknown Source)\n    at com.twitter.logging.Logger$.$anonfun$configure$1(Logger.scala:455)\n    at com.twitter.logging.Logger$$$Lambda$760/2144147057.apply(Unknown Source)\n    at scala.collection.immutable.List.foreach(List.scala:378)\n    at com.twitter.util.Duration.toString(Duration.scala:338)\n    at java.lang.String.valueOf(String.java:2994)\n    at java.lang.StringBuilder.append(StringBuilder.java:131)\n    at com.oracle.svm.core.amd64.AMD64CPUFeatureAccess.verifyHostSupportsArchitecture(AMD64CPUFeatureAccess.java:163)\n    at com.oracle.svm.core.JavaMainWrapper.run(JavaMainWrapper.java:180)\n    at Lcom/oracle/svm/core/code/CEntryPointCallStubs;.com_002eoracle_002esvm_002ecore_002eJavaMainWrapper_002erun_0028int_002corg_002egraalvm_002enativeimage_002ec_002etype_002eCCharPointerPointer_0029(generated:0)\nError: com.oracle.graal.pointsto.constraints.UnsupportedFeatureException: Unsupported method java.lang.ClassLoader.getParent() is reachable: The declaring class of this element has been substituted, but this element is not present in the substitution class\nTo diagnose the issue, you can add the option -H:+ReportUnsupportedElementsAtRuntime. The unsupported element is then reported at run time when it is accessed the first time.\nTrace:\n    at parsing sun.reflect.misc.ReflectUtil.isAncestor(ReflectUtil.java:207)\nCall path from entry point to sun.reflect.misc.ReflectUtil.isAncestor(ClassLoader, ClassLoader):\n    at sun.reflect.misc.ReflectUtil.isAncestor(ReflectUtil.java:205)\n    at sun.reflect.misc.ReflectUtil.needsPackageAccessCheck(ReflectUtil.java:233)\n    at sun.reflect.misc.ReflectUtil.checkProxyPackageAccess(ReflectUtil.java:269)\n    at java.lang.reflect.Proxy.checkProxyAccess(Proxy.java:402)\n    at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:713)\n    at com.twitter.util.SunSignalHandler.handle(Signal.scala:58)\n    at com.twitter.util.HandleSignal$.$anonfun$apply$1(Signal.scala:85)\n    at com.twitter.util.HandleSignal$.$anonfun$apply$1$adapted(Signal.scala:85)\n    at com.twitter.util.HandleSignal$$$Lambda$828/1664204892.apply(Unknown Source)\n    at scala.Option.foreach(Option.scala:257)\n    at com.twitter.logging.FileHandler.publish(FileHandler.scala:283)\n    at java.util.logging.Logger.log(Logger.java:738)\n    at java.util.logging.Logger.doLog(Logger.java:765)\n    at java.util.logging.Logger.log(Logger.java:876)\n    at com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$notifyRemoval$1(BoundedLocalCache.java:288)\n    at com.github.benmanes.caffeine.cache.BoundedLocalCache$$Lambda$857/1148776417.run(Unknown Source)\n    at com.oracle.svm.core.JavaMainWrapper$JavaMainSupport.executeHooks(JavaMainWrapper.java:105)\n    at com.oracle.svm.core.JavaMainWrapper$JavaMainSupport.executeStartupHooks(JavaMainWrapper.java:91)\n    at com.oracle.svm.core.JavaMainWrapper.run(JavaMainWrapper.java:198)\n    at Lcom/oracle/svm/core/code/CEntryPointCallStubs;.com_002eoracle_002esvm_002ecore_002eJavaMainWrapper_002erun_0028int_002corg_002egraalvm_002enativeimage_002ec_002etype_002eCCharPointerPointer_0029(generated:0)\nError: Image building with exit status 1\n```. I think a secondary issue we may be observing is that the data structures required to write out the prometheus response are taking up a relatively large amount of heap. In this dump below, you can see that although they are marked for collection, there's ~48MB of responses sitting in the heap. \n\nFor this particular instance, the current committed heap is 212MB so it represents roughly 23%. \nOne thing that's puzzling is that even though we have the max heap set to 1GB the JVM has not raised the committed heap on any of our instances. We'll definitely tune around this by setting Xms=Xmx. But I suspect that either these responses are unnecessarily fragmenting the young generation (which is 48MB in this case) or the JVM allocator may be deciding to drop these buffers directly into the old generation. \n@adleong your recent PR to raise the efficiency of the prometheus endpoint should help us here, I think.. Related to the previous comment, it does appear that linkerd is a lot happier with a constant committed heap.\nThe dramatic drop in full GCs (indicated by red triangles) can be seen here: \n\nWe'll measure and report back if the GC behavior changes further with the new changes (efficiency and thread pool PRs). \n . GC performance seems much improved on a node we're running with Alex's two PRs. In the three hours we've been running it we've only seen one full GC \ud83d\ude2e:\n\n. I know that @adleong did some work to separate the finagle worker pools for the traffic path and the admin endpoint path, but I wonder if we're still getting contention on the metrics data structures. @chrisgoffinet observed some pretty high scrape times on our deployment which would indicate there's something dramatically slowing down the construction of the prometheus response: \n\nIt also seems to degrade quickly, but I have no idea what could be triggering it. . This may be related to #2125, especially if it coincides with an uptick in h2-out traffic on that particular linkerd node. Feel free to close until we can provide more concrete evidence.. After looking at the circumstances under which linkerd will raise a Reset.Cancel error in the code, I think we could've gained some insight if I had raised the log verbosity on a few classes. Next time this happens I'll do that and add the output here. . After digging into the Prometheus metrics from around this time, I noticed the counter for rt:server:exn:io_netty_handler_codec_DecoderException for h2-in increase from 1 to 9. This exception didn't appear to be logged by linkerd, so I don't have a stack trace. . An update here: we stopped seeing this issue and haven't seen any new log messages.\nOur environment is pretty dynamic and changing all the time, but one important change we made is we removed a service that was creating HTTP requests and then force-closing the socket from the linkerd path. \nI'm going to close for now. If I can reproduce this issue by recreating the bad service's behavior, I'll reopen with more detail.. Anecdote: we've manually switched over to using native epoll in production and haven't seen any issues with it.. @evhfla yes, we see the issue regardless of JVM (we use OpenJ9 in production).. We will evaluate the maxConcurrentStreamsPerConnection workaround. \nWanted to post some information from our environment that compares open_streams with OOM events. \nh2-in router: \n\nh2-out router: \n\n\nOne interesting observation is that the server streams count isn't correlated with the client stream count on a router. I would expect that if there is client streams, there would be a corresponding server stream for it and that they would be roughly correlated in the stats.\n. Awesome! \ud83d\udc4f . 10 day look at FD usage for this instance: \n\nIt appears to grow slowly over time. Also, I thought that we raised the limit to 4096 before, but I was mistaken. My initial hunch is this correlates with the connect rate for the h2 routers on this instance but will have to do more digging to verify.. @adleong do you know if this resulted in a latency improvement versus connecting directly? . Here's a log snippet when the verbosity on h2 is turned up: \n```\nD 0308 16:09:33.338 PST THREAD32: [S L:/127.0.0.1:4142 R:/127.0.0.1:50137 S:689] stream reset from local; resetting remote: Reset.Cancel\nD 0308 16:09:33.338 PST THREAD32: [S L:/127.0.0.1:4142 R:/127.0.0.1:50137 S:689] stream reset from local\nD 0308 16:09:33.327 PST THREAD27 TraceId:6aee4a006a74d2d5: [C L:/127.0.0.1:50138 R:/127.0.0.1:9999 S:663] initialized stream\nD 0308 16:09:33.342 PST THREAD32: [S L:/127.0.0.1:4142 R:/127.0.0.1:50137 S:691] stream reset from local; resetting remote: Reset.Cancel\nD 0308 16:09:33.341 PST THREAD32: [S L:/127.0.0.1:4142 R:/127.0.0.1:50137 S:691] stream reset from local\nD 0308 16:09:33.342 PST THREAD32 TraceId:0743dd9782f53fae: [C L:/127.0.0.1:50138 R:/127.0.0.1:9999 S:659] writing message failed\nReset.Cancel\nW 0308 16:09:33.342 PST THREAD32 TraceId:0743dd9782f53fae: Exception propagated to the default monitor (upstream address: /127.0.0.1:50137, downstream address: /127.0.0.1:9999, label: $/inet/127.1/9999).\nReset.Cancel\nD 0308 16:09:33.344 PST THREAD32 TraceId:890c14394a9db297: [C L:/127.0.0.1:50138 R:/127.0.0.1:9999 S:665] initialized stream\nD 0308 16:09:33.346 PST THREAD32: [C L:/127.0.0.1:50138 R:/127.0.0.1:9999 S:661] stream reset from remote\nD 0308 16:09:33.346 PST THREAD32 TraceId:a8fab39e44da5344: [C L:/127.0.0.1:50138 R:/127.0.0.1:9999 S:661] writing message was interrupted\nStreamError.Remote(Reset.Cancel)\nD 0308 16:09:33.346 PST THREAD32 TraceId:a8fab39e44da5344: [C L:/127.0.0.1:50138 R:/127.0.0.1:9999 S:661] stream reset from remote: Reset.Cancel\nD 0308 16:09:33.347 PST THREAD32: [S L:/127.0.0.1:4142 R:/127.0.0.1:50137 S:693] stream reset from local\nD 0308 16:09:33.348 PST THREAD32: [S L:/127.0.0.1:4142 R:/127.0.0.1:50137 S:693] stream reset from local; resetting remote: Reset.Cancel\nD 0308 16:09:33.348 PST THREAD32 TraceId:a8fab39e44da5344: [C L:/127.0.0.1:50138 R:/127.0.0.1:9999 S:661] writing message failed\nReset.Cancel\nW 0308 16:09:33.348 PST THREAD32 TraceId:a8fab39e44da5344: Exception propagated to the default monitor (upstream address: /127.0.0.1:50137, downstream address: /127.0.0.1:9999, label: $/inet/127.1/9999).\nReset.Cancel\nD 0308 16:09:33.341 PST THREAD27: [S L:/127.0.0.1:4142 R:/127.0.0.1:50137 S:699] initialized stream\nD 0308 16:09:33.354 PST THREAD32: [C L:/127.0.0.1:50138 R:/127.0.0.1:9999 S:663] stream reset from remote\nD 0308 16:09:33.354 PST THREAD32 TraceId:6aee4a006a74d2d5: [C L:/127.0.0.1:50138 R:/127.0.0.1:9999 S:663] writing message was interrupted\nStreamError.Remote(Reset.Cancel)\nD 0308 16:09:33.354 PST THREAD32 TraceId:6aee4a006a74d2d5: [C L:/127.0.0.1:50138 R:/127.0.0.1:9999 S:663] stream reset from remote: Reset.Cancel\nD 0308 16:09:33.356 PST THREAD32: [S L:/127.0.0.1:4142 R:/127.0.0.1:50137 S:695] stream reset from local\nD 0308 16:09:33.356 PST THREAD32: [S L:/127.0.0.1:4142 R:/127.0.0.1:50137 S:695] stream reset from local; resetting remote: Reset.Cancel\nD 0308 16:09:33.356 PST THREAD32 TraceId:6aee4a006a74d2d5: [C L:/127.0.0.1:50138 R:/127.0.0.1:9999 S:663] writing message failed\nReset.Cancel\nW 0308 16:09:33.357 PST THREAD32 TraceId:6aee4a006a74d2d5: Exception propagated to the default monitor (upstream address: /127.0.0.1:50137, downstream address: /127.0.0.1:9999, label: $/inet/127.1/9999).\nReset.Cancel\nD 0308 16:09:33.360 PST THREAD27 TraceId:e081ba5068d1ae47: [C L:/127.0.0.1:50138 R:/127.0.0.1:9999 S:667] initialized stream\nD 0308 16:09:33.358 PST THREAD32: [C L:/127.0.0.1:50138 R:/127.0.0.1:9999 S:665] stream reset from remote\nD 0308 16:09:33.361 PST THREAD32 TraceId:890c14394a9db297: [C L:/127.0.0.1:50138 R:/127.0.0.1:9999 S:665] writing message was interrupted\nStreamError.Remote(Reset.Cancel)\nD 0308 16:09:33.361 PST THREAD32 TraceId:890c14394a9db297: [C L:/127.0.0.1:50138 R:/127.0.0.1:9999 S:665] stream reset from remote: Reset.Cancel\nD 0308 16:09:33.375 PST THREAD32: [S L:/127.0.0.1:4142 R:/127.0.0.1:50137 S:697] stream reset from local\nD 0308 16:09:33.375 PST THREAD32: [S L:/127.0.0.1:4142 R:/127.0.0.1:50137 S:697] stream reset from local; resetting remote: Reset.Cancel\nD 0308 16:09:33.375 PST THREAD32 TraceId:890c14394a9db297: [C L:/127.0.0.1:50138 R:/127.0.0.1:9999 S:665] writing message failed\nReset.Cancel\nW 0308 16:09:33.376 PST THREAD32 TraceId:890c14394a9db297: Exception propagated to the default monitor (upstream address: /127.0.0.1:50137, downstream address: /127.0.0.1:9999, label: $/inet/127.1/9999).\nReset.Cancel\nD 0308 16:09:33.377 PST THREAD27: [S L:/127.0.0.1:4142 R:/127.0.0.1:50137 S:701] initialized stream\nD 0308 16:09:33.389 PST THREAD32: [C L:/127.0.0.1:50138 R:/127.0.0.1:9999 S:667] stream reset from remote\nD 0308 16:09:33.389 PST THREAD32 TraceId:e081ba5068d1ae47: [C L:/127.0.0.1:50138 R:/127.0.0.1:9999 S:667] writing message was interrupted\nStreamError.Remote(Reset.Cancel)\nD 0308 16:09:33.389 PST THREAD32 TraceId:e081ba5068d1ae47: [C L:/127.0.0.1:50138 R:/127.0.0.1:9999 S:667] stream reset from remote: Reset.Cancel\nD 0308 16:09:33.390 PST THREAD32: [S L:/127.0.0.1:4142 R:/127.0.0.1:50137 S:699] stream reset from local\nD 0308 16:09:33.390 PST THREAD32: [S L:/127.0.0.1:4142 R:/127.0.0.1:50137 S:699] stream reset from local; resetting remote: Reset.Cancel\nD 0308 16:09:33.390 PST THREAD32 TraceId:e081ba5068d1ae47: [C L:/127.0.0.1:50138 R:/127.0.0.1:9999 S:667] writing message failed\nReset.Cancel\nW 0308 16:09:33.390 PST THREAD32 TraceId:e081ba5068d1ae47: Exception propagated to the default monitor (upstream address: /127.0.0.1:50137, downstream address: /127.0.0.1:9999, label: $/inet/127.1/9999).\nReset.Cancel\n```. Plot twist: I tried running the test with the number of netty direct arenas set to zero, effectively disabling the usage of pooled direct memory buffers. Taking a heap dump confirms that there are no direct arenas in use anywhere in the process (nor are there any direct memory buffers owned by arenas or of the size that netty typically allocates: 1MB). Linkerd still runs out of direct memory somehow: \nW 0308 17:16:14.981 PST THREAD461: Unhandled exception in connection with /127.0.0.1:51174, shutting down connection\nio.netty.util.internal.OutOfDirectMemoryError: failed to allocate 1024 byte(s) of direct memory (used: 134216789, max: 134217728)\n    at io.netty.util.internal.PlatformDependent.incrementMemoryCounter(PlatformDependent.java:640)\n    at io.netty.util.internal.PlatformDependent.allocateDirectNoCleaner(PlatformDependent.java:594)\n    at io.netty.buffer.UnpooledUnsafeNoCleanerDirectByteBuf.allocateDirect(UnpooledUnsafeNoCleanerDirectByteBuf.java:30)\n    at io.netty.buffer.UnpooledUnsafeDirectByteBuf.<init>(UnpooledUnsafeDirectByteBuf.java:68)\n    at io.netty.buffer.UnpooledUnsafeNoCleanerDirectByteBuf.<init>(UnpooledUnsafeNoCleanerDirectByteBuf.java:25)\n    at io.netty.buffer.UnsafeByteBufUtil.newUnsafeDirectByteBuf(UnsafeByteBufUtil.java:625)\n    at io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:327)\n    at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:185)\n    at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:176)\n    at io.netty.buffer.AbstractByteBufAllocator.ioBuffer(AbstractByteBufAllocator.java:137)\n    at io.netty.channel.DefaultMaxMessagesRecvByteBufAllocator$MaxMessageHandle.allocate(DefaultMaxMessagesRecvByteBufAllocator.java:114)\n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:147)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:646)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:581)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:498)\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:884)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run(BlockingTimeTrackingThreadFactory.scala:23)\n    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n    at java.lang.Thread.run(Thread.java:748)\n. Running linkerd with -Dio.netty.noPreferDirect=true still results in direct memory exhaustion. This maybe indicates we're leaking an I/O buffer somewhere?\nE 0308 17:33:07.456 PST THREAD22: [S L:/127.0.0.1:4142 R:/127.0.0.1:51345] dispatcher failed\ncom.twitter.finagle.UnknownChannelException: failed to allocate 1024 byte(s) of direct memory (used: 134217390, max: 134217728) at remote address: /127.0.0.1:51345. Remote Info: Not Available\n    at com.twitter.finagle.ChannelException$.apply(Exceptions.scala:271)\n    at com.twitter.finagle.netty4.transport.ChannelTransport$$anon$2.exceptionCaught(ChannelTransport.scala:175)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:285)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:264)\n    at io.netty.channel.AbstractChannelHandlerContext.fireExceptionCaught(AbstractChannelHandlerContext.java:256)\n    at io.netty.channel.ChannelInboundHandlerAdapter.exceptionCaught(ChannelInboundHandlerAdapter.java:131)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:285)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:264)\n    at io.netty.channel.AbstractChannelHandlerContext.fireExceptionCaught(AbstractChannelHandlerContext.java:256)\n    at io.netty.channel.ChannelInboundHandlerAdapter.exceptionCaught(ChannelInboundHandlerAdapter.java:131)\n    at com.twitter.finagle.netty4.channel.ChannelExceptionHandler.exceptionCaught(ChannelExceptionHandler.scala:69)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:285)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:264)\n    at io.netty.channel.AbstractChannelHandlerContext.fireExceptionCaught(AbstractChannelHandlerContext.java:256)\n    at io.netty.channel.ChannelInboundHandlerAdapter.exceptionCaught(ChannelInboundHandlerAdapter.java:131)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:285)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:264)\n    at io.netty.channel.AbstractChannelHandlerContext.fireExceptionCaught(AbstractChannelHandlerContext.java:256)\n    at io.netty.channel.ChannelInboundHandlerAdapter.exceptionCaught(ChannelInboundHandlerAdapter.java:131)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:285)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:264)\n    at io.netty.channel.AbstractChannelHandlerContext.fireExceptionCaught(AbstractChannelHandlerContext.java:256)\n    at io.netty.channel.ChannelInboundHandlerAdapter.exceptionCaught(ChannelInboundHandlerAdapter.java:131)\n    at com.twitter.finagle.netty4.channel.ChannelStatsHandler.exceptionCaught(ChannelStatsHandler.scala:163)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:285)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:264)\n    at io.netty.channel.AbstractChannelHandlerContext.fireExceptionCaught(AbstractChannelHandlerContext.java:256)\n    at io.netty.channel.DefaultChannelPipeline$HeadContext.exceptionCaught(DefaultChannelPipeline.java:1401)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:285)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:264)\n    at io.netty.channel.DefaultChannelPipeline.fireExceptionCaught(DefaultChannelPipeline.java:953)\n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.handleReadException(AbstractNioByteChannel.java:125)\n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:174)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:646)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:581)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:498)\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:884)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run(BlockingTimeTrackingThreadFactory.scala:23)\n    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n    at java.lang.Thread.run(Thread.java:748)\nCaused by: io.netty.util.internal.OutOfDirectMemoryError: failed to allocate 1024 byte(s) of direct memory (used: 134217390, max: 134217728)\n    at io.netty.util.internal.PlatformDependent.incrementMemoryCounter(PlatformDependent.java:640)\n    at io.netty.util.internal.PlatformDependent.allocateDirectNoCleaner(PlatformDependent.java:594)\n    at io.netty.buffer.UnpooledUnsafeNoCleanerDirectByteBuf.allocateDirect(UnpooledUnsafeNoCleanerDirectByteBuf.java:30)\n    at io.netty.buffer.UnpooledUnsafeDirectByteBuf.<init>(UnpooledUnsafeDirectByteBuf.java:68)\n    at io.netty.buffer.UnpooledUnsafeNoCleanerDirectByteBuf.<init>(UnpooledUnsafeNoCleanerDirectByteBuf.java:25)\n    at io.netty.buffer.UnsafeByteBufUtil.newUnsafeDirectByteBuf(UnsafeByteBufUtil.java:625)\n    at io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:327)\n    at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:185)\n    at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:176)\n    at io.netty.buffer.AbstractByteBufAllocator.ioBuffer(AbstractByteBufAllocator.java:137)\n    at io.netty.channel.DefaultMaxMessagesRecvByteBufAllocator$MaxMessageHandle.allocate(DefaultMaxMessagesRecvByteBufAllocator.java:114)\n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:147)\n    ... 10 more\n. I've tried to configure Netty in such a way that it strongly prefers heap memory for arena allocation (buffer recycling): \n\n-Dio.netty.allocator.numDirectArenas=0 \n-Dio.netty.noPreferDirect=true\n-Dio.netty.noUnsafe=true\n\nThe net result is, hopefully, that Netty will still use reference counted buffers but that they will be on the heap so we can observe any leaks. \nThis appears to be the case. Here's a baseline summary showing the biggest consumers of heap being Netty heap arena pool chunks: \n\nHere's a follow-on heap dump ~20 minutes later showing further growth of the arena size: \n\nI wondered if it was possible to get Netty to do away with reference counted memory altogether and if the leak would go away if we did. This also appears to be the case. If I also set the number heap arenas to 0 (-Dio.netty.allocator.numHeapArenas=0), I can't observe any further leaks on the heap. \nBaseline: \n\n~T+5 minutes: \n\n. @adleong I couldn't find a strong reference to a direct ByteBuffer in the heap for the half closed streams in the tracking table. Do you know where else I might look? I think it has to be strongly referenced somewhere or else the Netty leak detector would have spat out some warnings. \nI can also write a quick test for this theory by writing something that periodically purges the stream tracking ConcurrentHashMap. Do you know what I would have to do to tear down the streams in there before removing them from the Map? . @adleong another interesting data point from this weekend is when I cracked open the heap buffers in a hex editor, the data frames from the H2 packets were in there. That makes me strongly suspect we're hanging onto references to network I/O data somewhere.. @adleong The reason I thought there might be something we have to do to the stream state before we kick it out of the map is most of the stream IDs already look like this (replaced with $StreamLocalReset): \n\nand $StreamLocalReset is a singleton (no other state attached). \nIs there something I can do here where we move the stream state over?https://github.com/linkerd/linkerd/blob/master/finagle/h2/src/main/scala/com/twitter/finagle/buoyant/h2/netty4/Netty4DispatcherBase.scala#L126\n. I wanted to test the theory that we're transitioning the state in the streams CHM while the stream dispatcher is still hanging onto some direct memory. \nIt's pretty crude, but maybe a useful data point: I put a finalize method on the stream dispatcher class and printed out the state of send and recv side of the stream to see if we were still in some kind of open state when the streams are being GC'd. I couldn't find any indication that this was the case. . Just a heads up, 2.12.2 is out now.. Should there be remote.close() here as well if the stream queue is closed?. Unless you want to pull in scala-java8-compat, there probably isn't. Java 8 lets you specify these operators as lambdas. \nhttps://github.com/scala/scala-java8-compat#converters-between-scalafunctionn-and-javautilfunction. @olix0r do you know if 2.12 still performs a function allocation in cases like this? . ",
    "frank-zg": "why? i`m copy guide code running.\n. ",
    "endzyme": "Directing to /dev/stdout worked for me on docker container in the hub. +1\n. +1 would love to see tag support here. \nPersonal preference would be nice to be able to route inline with Dtabs to tags:\n/consul => /#/io.l5d.consul/dc1\n/http/1.1/* => /consul\n/http/1.1/*/tag => /consul/tag\nWe would like to route in the following manor:\n- We could register a webappname and point it to the current version:\n/consul/webappname => /consul/tag/1.0/webapp;\n- We could then do canary with the same format:\n/consul/webappname => 9 * /consul/tag/1.0/webapp & 1 * /consul/tag/2.0/webapp;\nOur current method is to register our services in consul as webapp-version and then namerd can handle which should be the active version.\n. https://linkerd.io/doc/0.7.0/linkerd/protocol-http/#http-headers - I think this can be closed now, if I've understood the ask correctly. \n. +1 this feature would be awesome - otherwise we're mounting all our apps to listen on /servicename instead of just /.\n. @wmorgan - Thanks for the rapid reply!\nSo we can handle the forwarding of headers fairly easily from our end. It's a little easier to reconcile than having to override a Host header. Overriding the Host header doesn't 100% jive with RFC for http headers. We get a lot of feedback from developers concerned about having to retrofit lots of existing applications in order to use linkerd. This is a barrier to entry for us and, from a product perspective, I can see it being an obstacle to adoption for others, even just to POC. \n. Thanks @wmorgan! I appreciate your groups diligence and attention to this!\n. ",
    "pawelchcki": "Thanks for thorough reply.\n\n\nyou should be able to query services by tags,\n\nThis is one place where I feel that consul's model does not mesh well with Dtabs. I'm interested for input here.\n\nOne of the reasons we initially get into consul was its builtin DNS interface. I'm mentioning that because maybe following that interface schema might be how we could implement tags.\nThe schema is [<tag>.]<service-name>.service.[<dc>.]consul\nThis is still somewhat limited but I think for now [<tag>.]<service-name> could work well, at least it would give users flexibility they already have when using consul.\n\nWould you mind elaborating on how you lay out your services with tags? It would be helpful to have some concrete cases in mind so that we can figure out a solution for this.\n\nRecently we started using tags to distinguish environments e.g. dev.svc vs staging.svc\nWe also use consul DNS interface pretty extensively so we are somewhat limited by DNS in what we do. So e.g we also have tags like dev-master where otherwise we could use 2 tags and filter accordingly.\nSome of the consul based tools I've seen in the wild also parse tags content.\n\nAlternatively, perhaps we could give special treatment to local dc, so that dc does not need to be named explicitly in a Dtab like:\n\nI'm sure there will be at least one company out there that will decide to name its DC local.\nI'm sure there will be at least one company out there that will decide to name its DC local :- )\n\nnamers:\n- kind: io.l5d.experimental.consul\n  host: 127.0.0.1\n  port: 2181\n  prefix /io.l5d.consul.local\n- kind: io.l5d.experimental.consul\n  host: 127.0.0.1\n  port: 2181\n  dc: dc2\n  prefix /io.l5d.consul.dc2\nI really like this approach. Also maybe adding requiredTags: dev could work too.\nload balancer is able to maintain its own view of what is healthy via latency measurements and circuit breaking\n\nIn consul you can trigger maintenance mode on node or service that should be able to mark each affected service as unhealthy with expectation being that traffic should stop being routed there.\nAlso I think current Linkerd consul impl doesn't do that but I think it would help with reaction time if Linkerd was also using blocking queries to watch for updates in consul services.\n\nAgain, thanks for the feedback!\n\nThanks for building linkerd! With zipkin and consul support it certainly seems to be the missing piece I was looking for.\n. ",
    "awwithro": "+1\n. ",
    "stevej": ":star: :+1:\n\n. \ud83d\udc4d LGTM\n. Java 8 required! MERGING\n. \u2747\ufe0f \ud83d\udc4d \n. Based on a blog post, this is the Design of the watcher, there's a Thread running that busy-waits on poll\nSource of poll from jdk7 (this seems unchanged in 8)\nAnd evidence that we're sitting waiting for poll.\n\"35\" : {\n      \"priority\" : 5,\n      \"state\" : \"RUNNABLE\",\n      \"daemon\" : true,\n      \"thread\" : \"Thread-3\",\n      \"stack\" : [\n        \"sun.nio.fs.LinuxWatchService.poll(Native Method)\",\n        \"sun.nio.fs.LinuxWatchService.access$600(LinuxWatchService.java:47)\",\n        \"sun.nio.fs.LinuxWatchService$Poller.run(LinuxWatchService.java:314)\",\n        \"java.lang.Thread.run(Thread.java:745)\"\n      ]\nAre we just hosed using the filesystem watcher? Should we push people harder to use Service Discovery? We don't intend for the fs watcher to be used full time. Maybe we should just document this behavior?\n. We could use a different java inotify library but would we have to release a different binary for linux?\n. How to reproduce: startup l5d and send a single request to kick the FS watcher into action.\n. I threw together a quick branch that used poll(timeout) instead of take() and it made no difference, we're still busywaiting in a native method.\n. When we upgraded slow_cooker, (our load tester) to reuse connections, this issue went away so we believe there's a Promise chain being created due to connection churn. I'm currently tracking down whether this is in l5d code or inside of finagle.\n. This appears fixed with #402.\n. (adding myself to this ticket)\n. Ideally, we'd love a tcpdump during the error event where we can see both sides of the problem request. Getting stats would also be helpful, of course!\n. l5d is much much colder when connections are churning with this change. I'll keep the test running overnight so we can see if things unexpectedly melt down.\n. \u2b50 I don't think pullapprove cares what I think but here we go.\n\n. Our time to failure doubled but we're still full GC'ing after about 4 hours. I'll hook up YourKit and see if I can find this function in the hot path. Before, YourKit was claiming that this was like 8% of our allocations.\n. As I said in Slack, Alex and I didn't find evidence that it was directly helpful for reducing allocations. Considering closing this issue with a WELPWETRIED\n. Here's a compact3 profile based on OpenJDK 8. I need to double check the license.\nhttps://github.com/ofayau/docker-openjdk/blob/master/openjdk-compact3/Dockerfile\n. Notes on modifying our sbt-docker setup to run commands (for configuring multiarch)\nhttps://velvia.github.io/Docker-Scala-Sbt/\n. The compact3 openjdk 8 binaries are under a dual license.\nhttps://jdk8.java.net/java-se-8-ri/\nThey make a big stink about how it's for reference only and not for production use. We should ponder that.\n. (here's the commit where we removed docker publishing from io.buoyant, it contains how to build the multiarch docker image)\nhttps://github.com/BuoyantIO/io.buoyant/commit/818d2da2bcaf24fbc0e656bc0679cf4b7d9f2a20\n. Here is the commit where I added support: https://github.com/BuoyantIO/io.buoyant/pull/395/files\n. Fixed in https://github.com/BuoyantIO/linkerd/pull/554/\n. Another workaround is to whitelist specific libraries using SELinux although I'm not clear how to do this with containers\nhttp://www-01.ibm.com/support/docview.wss?uid=swg21268209\n. I don't think we want to be responsible for building and updating a custom JVM image, I think the workaround is to run linkerd on a stock 64-bit JVM. I think we should close this ticket.. When l5d is setup to route all gRPC requests to a gRPC backend, interop is successful.\n$ ./bin/test-client --server_host=proxy-test-4b --server_port=4141 --use_tls=false --test_case=unimplemented_service\nJan 11, 2017 9:19:32 PM io.grpc.internal.ManagedChannelImpl <init>\nINFO: [io.grpc.internal.ManagedChannelImpl-1] Created with target directaddress:///proxy-test-4b/10.240.0.10:4141\nRunning test unimplemented_service\nTest completed.\nShutting down\nFor reference, here's the config file I was using with linkerd to test this\n```\nrouters:\n\nprotocol: h2\n  experimental: true\n  label: h2c\n  servers:\nport: 4141\n    ip: 0.0.0.0\n  dstPrefix: /grpc\n  identifier:\n    kind: io.l5d.headerPath\n  baseDtab: |\n    /srv => /$/inet/10.240.0.4/8080;\n    /grpc => /srv;\n```. it's worth noting that gRPC won't send credentials over an unencrypted channel and this test was done without TLS so I will re-run all of these credential failure test cases with TLS.. This was fixed in #977 . Thank you! That was unexpected.. I've noticed both grpc-go and grpc-java set monster window sizes.\n\npublic static final int DEFAULT_FLOW_CONTROL_WINDOW = 1048576; // 1MiB\nhttps://github.com/grpc/grpc-java/blob/65e4d9f47abdad2a0a7a67fc5f1002bcf6f6fab1/netty/src/main/java/io/grpc/netty/NettyChannelBuilder.java#L69\n. Yes, some of this is in-progress in the stevej/grpc-interop branch. This was released with today's 0.9.0 release, please let us know if you find other log lines that aren't uniform or are lacking a timezone. Specific examples are the most helpful. Thanks!. Do you think that there's a gRPC implementation that encodes multiple messages in a single DATA frame? That would be very surprising.. \ud83d\udc4d  lgtm approval granted.. If there's a cleaner way to build to the grpc-go interop client, please let me know. The current method is pretty ugly.. Yes, your fix does work for empty_stream but not for ping_pong. Yes, empty_stream appears fixed with that change!. I think we should leave this open until the interop test passes. If you disagree, we can chat about it tomorrow morning, no biggie.. Yeah, I'll close this. We can revisit it at a later date if the need arises.. For your different test scenarios, are they all going through the networking stack? It sounds like they are but I want to make sure localhost isn't a factor.\nTBH, I haven't seen a 26k RPS throughput limitation with linkerd, I routinely test near gigabit ethernet's limits.\nWould you mind sharing with us your metrics during one of your test runs? You can find it in the linkerd admin instance under /admin/metrics.json You should be able to attach it to a comment on this github issue.\nAlso, it would be helpful to know which version of linkerd you're using, which JVM you're using. (the easiest way is probably with ps auxwww |grep linkerd), and your config file.\nThanks!. Also, how much CPU use did you see at 26k RPS?. I spent some time this morning running load tests on the last 9 months of linkerd releases (at 10k qps) and there's definitely been a mild performance regression. I'm going to test at 40k again and try and replicate this 26k qps bottleneck.\n\n. Here are some representative slow_cooker log lines from my testing\n0.7.5 2017-05-10T18:18:55Z  99065/0/0 100000  99% 10s   0 [ 11  19  36   73 ]   73\n0.8.0 2017-05-10T17:27:10Z  97720/0/0 100000  97% 10s   0 [ 12  30  45   97 ]   97\n0.8.2 2017-05-10T18:09:01Z  96489/0/0 100000  96% 10s   0 [ 13  33  51  233 ]  233\n0.8.6 2017-05-10T17:45:39Z  98578/0/0 100000  98% 10s   0 [ 12  25  42  232 ]  232\n0.9.1 2017-05-10T17:59:17Z  88270/0/0 100000  88% 10s   0 [ 15  45  65  248 ]  248\n1.0.0 2017-05-10T18:38:10Z  84635/0/0 100000  84% 10s   0 [ 16  51  70  129 ]  129. I did some digging into whether there is a throughput limitation: with linkerd 1.0, I was able to sustain 65k qps with a few caveats: linkerd needed 22 cores and I had to use 1000 connections from my load testing tool. (I'll note that the Google Cloud network offers a lot of bandwidth on a large core VM so I didn't need to configure any fancy bonding to go over 1Gbps)\nSo our verdict is: there isn't strictly a throughput limitation but there is a performance regression that's making high qps harder to achieve without more cores and sockets.\n. Thanks to this ticket, we're going to prioritize tracking down the source of the regression, we should have an update in a few weeks. Thank you!. Hi @jamessharp, so far we haven't been able to reproduce this particular issue. Would it be possible to provide a heap dump when linkerd is taking a gig or more memory? We've found heap dumps are a pretty reliable technique for tracking down memory leaks.\nHere are two ways to get a heap dump: https://blog.gceasy.io/2015/08/14/how-to-capture-heap-dump-jmap/\nThanks a bunch!. Digging through the heap dump (thank you!) with Eclipse MAT:\nThe thread java.lang.Thread @ 0xd4e12328 finagle/netty3-7 keeps local variables with total size 835,314,744 (98.33%) bytes.\nLooks like a scala List we keep consing onto.\nscala.collection.immutable.$colon$colon @ 0xd2ad4eb8\n24  835,291,688 98.32%\nUnfortunately, it's in a Promise$Transformer which is just a generic container of computation so we'll need to do some digging as to what code is actually consing.\n. The relevant thread stack\nfinagle/netty3-7\n  at com.twitter.util.Promise.runq(Lcom/twitter/util/Promise$K;Lscala/collection/immutable/List;Lcom/twitter/util/Try;)V (Promise.scala:406)\n  at com.twitter.util.Promise.updateIfEmpty(Lcom/twitter/util/Try;)Z (Promise.scala:806)\n  at com.twitter.util.Promise.update(Lcom/twitter/util/Try;)V (Promise.scala:775)\n  at com.twitter.util.Future.$anonfun$proxyTo$1(Lcom/twitter/util/Promise;Lcom/twitter/util/Try;)V (Future.scala:1328)\n  at com.twitter.util.Future.$anonfun$proxyTo$1$adapted(Lcom/twitter/util/Promise;Lcom/twitter/util/Try;)Ljava/lang/Object; (Future.scala:1328)\n  at com.twitter.util.Future$$Lambda$258.apply(Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)\n  at com.twitter.util.ConstFuture$$anon$5.run()V (Future.scala:1469)\n  at com.twitter.concurrent.LocalScheduler$Activation.run()V (Scheduler.scala:200)\n  at com.twitter.concurrent.LocalScheduler$Activation.submit(Ljava/lang/Runnable;)V (Scheduler.scala:158)\n  at com.twitter.concurrent.LocalScheduler.submit(Ljava/lang/Runnable;)V (Scheduler.scala:272)\n  at com.twitter.concurrent.Scheduler$.submit(Ljava/lang/Runnable;)V (Scheduler.scala:108)\n  at com.twitter.util.Promise.runq(Lcom/twitter/util/Promise$K;Lscala/collection/immutable/List;Lcom/twitter/util/Try;)V (Promise.scala:406)\n  at com.twitter.util.Promise.updateIfEmpty(Lcom/twitter/util/Try;)Z (Promise.scala:801)\n  at com.twitter.util.Promise.update(Lcom/twitter/util/Try;)V (Promise.scala:775)\n  at com.twitter.util.Promise.setValue(Ljava/lang/Object;)V (Promise.scala:751)\n  at com.twitter.concurrent.AsyncQueue.offer(Ljava/lang/Object;)Z (AsyncQueue.scala:123)\n  at com.twitter.finagle.netty3.transport.ChannelTransport.handleUpstream(Lorg/jboss/netty/channel/ChannelHandlerContext;Lorg/jboss/netty/channel/ChannelEvent;)V (ChannelTransport.scala:56)\n  at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(Lorg/jboss/netty/channel/DefaultChannelPipeline$DefaultChannelHandlerContext;Lorg/jboss/netty/channel/ChannelEvent;)V (DefaultChannelPipeline.java:564)\n  at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(Lorg/jboss/netty/channel/ChannelEvent;)V (DefaultChannelPipeline.java:791)\n  at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(Lorg/jboss/netty/channel/ChannelHandlerContext;Lorg/jboss/netty/channel/MessageEvent;)V (HttpContentDecoder.java:135)\n  at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(Lorg/jboss/netty/channel/ChannelHandlerContext;Lorg/jboss/netty/channel/ChannelEvent;)V (SimpleChannelUpstreamHandler.java:70)\n  at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(Lorg/jboss/netty/channel/DefaultChannelPipeline$DefaultChannelHandlerContext;Lorg/jboss/netty/channel/ChannelEvent;)V (DefaultChannelPipeline.java:564)\n  at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(Lorg/jboss/netty/channel/ChannelEvent;)V (DefaultChannelPipeline.java:791)\n  at org.jboss.netty.channel.Channels.fireMessageReceived(Lorg/jboss/netty/channel/ChannelHandlerContext;Ljava/lang/Object;Ljava/net/SocketAddress;)V (Channels.java:296)\n  at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(Lorg/jboss/netty/channel/ChannelHandlerContext;Ljava/net/SocketAddress;Ljava/lang/Object;)V (FrameDecoder.java:459)\n  at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(Lorg/jboss/netty/channel/ChannelHandlerContext;Lorg/jboss/netty/channel/Channel;Lorg/jboss/netty/buffer/ChannelBuffer;Lorg/jboss/netty/buffer/ChannelBuffer;Ljava/net/SocketAddress;)V (ReplayingDecoder.java:536)\n  at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(Lorg/jboss/netty/channel/ChannelHandlerContext;Lorg/jboss/netty/channel/MessageEvent;)V (ReplayingDecoder.java:485)\n  at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(Lorg/jboss/netty/channel/ChannelHandlerContext;Lorg/jboss/netty/channel/ChannelEvent;)V (SimpleChannelUpstreamHandler.java:70)\n  at org.jboss.netty.handler.codec.http.HttpClientCodec.handleUpstream(Lorg/jboss/netty/channel/ChannelHandlerContext;Lorg/jboss/netty/channel/ChannelEvent;)V (HttpClientCodec.java:92)\n  at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(Lorg/jboss/netty/channel/DefaultChannelPipeline$DefaultChannelHandlerContext;Lorg/jboss/netty/channel/ChannelEvent;)V (DefaultChannelPipeline.java:564)\n  at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(Lorg/jboss/netty/channel/ChannelEvent;)V (DefaultChannelPipeline.java:791)\n  at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(Lorg/jboss/netty/channel/ChannelHandlerContext;Lorg/jboss/netty/channel/MessageEvent;)V (SimpleChannelHandler.java:142)\n  at com.twitter.finagle.netty3.channel.ChannelStatsHandler.messageReceived(Lorg/jboss/netty/channel/ChannelHandlerContext;Lorg/jboss/netty/channel/MessageEvent;)V (ChannelStatsHandler.scala:68)\n  at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(Lorg/jboss/netty/channel/ChannelHandlerContext;Lorg/jboss/netty/channel/ChannelEvent;)V (SimpleChannelHandler.java:88)\n  at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(Lorg/jboss/netty/channel/DefaultChannelPipeline$DefaultChannelHandlerContext;Lorg/jboss/netty/channel/ChannelEvent;)V (DefaultChannelPipeline.java:564)\n  at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(Lorg/jboss/netty/channel/ChannelEvent;)V (DefaultChannelPipeline.java:791)\n  at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(Lorg/jboss/netty/channel/ChannelHandlerContext;Lorg/jboss/netty/channel/MessageEvent;)V (SimpleChannelHandler.java:142)\n  at com.twitter.finagle.netty3.channel.ChannelRequestStatsHandler.messageReceived(Lorg/jboss/netty/channel/ChannelHandlerContext;Lorg/jboss/netty/channel/MessageEvent;)V (ChannelRequestStatsHandler.scala:32)\n  at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(Lorg/jboss/netty/channel/ChannelHandlerContext;Lorg/jboss/netty/channel/ChannelEvent;)V (SimpleChannelHandler.java:88)\n  at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(Lorg/jboss/netty/channel/DefaultChannelPipeline$DefaultChannelHandlerContext;Lorg/jboss/netty/channel/ChannelEvent;)V (DefaultChannelPipeline.java:564)\n  at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(Lorg/jboss/netty/channel/ChannelEvent;)V (DefaultChannelPipeline.java:559)\n  at org.jboss.netty.channel.Channels.fireMessageReceived(Lorg/jboss/netty/channel/Channel;Ljava/lang/Object;Ljava/net/SocketAddress;)V (Channels.java:268)\n  at org.jboss.netty.channel.Channels.fireMessageReceived(Lorg/jboss/netty/channel/Channel;Ljava/lang/Object;)V (Channels.java:255)\n  at org.jboss.netty.channel.socket.nio.NioWorker.read(Ljava/nio/channels/SelectionKey;)Z (NioWorker.java:88)\n  at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(Ljava/nio/channels/Selector;)V (AbstractNioWorker.java:108)\n  at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run()V (AbstractNioSelector.java:337)\n  at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run()V (AbstractNioWorker.java:89)\n  at org.jboss.netty.channel.socket.nio.NioWorker.run()V (NioWorker.java:178)\n  at org.jboss.netty.util.ThreadRenamingRunnable.run()V (ThreadRenamingRunnable.java:108)\n  at org.jboss.netty.util.internal.DeadLockProofWorker$1.run()V (DeadLockProofWorker.java:42)\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(Ljava/util/concurrent/ThreadPoolExecutor$Worker;)V (ThreadPoolExecutor.java:1142)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run()V (ThreadPoolExecutor.java:617)\n  at com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run()V (BlockingTimeTrackingThreadFactory.scala:24)\n  at java.lang.Thread.run()V (Thread.java:745). I believe this is what's in the cons list:\n```\nClass Name                                           |    Objects | Shallow Heap\n\ncom.twitter.util.Return                              | 12,318,090 |  197,089,440\ncom.twitter.util.Promise                             | 12,317,865 |  197,085,840\ncom.twitter.concurrent.AsyncStream$Cons              |  4,241,180 |  135,717,760\ncom.twitter.concurrent.AsyncStream$Embed             |  8,079,806 |  129,276,896\ncom.twitter.concurrent.AsyncStream$$Lambda$1306      |  4,238,057 |  101,713,368\ncom.twitter.concurrent.AsyncStream$Cons$$$Lambda$1305|  4,238,199 |   67,811,184\n\n``. I\u2019ve dug about 20 deep into the cons list and it\u2019sJsonStreamParserinterleaved withEndpointsModified` so it\u2019s probably whatever is driving reading.\nAttached is a screen grab from Eclipse MAT to help clarify things.\n\n. Hi @aguilbau, which version of kubernetes are you running? I suspect this is a bad interaction between k8s and our k8s namer.. I think a heap dump is the best way to achieve some clarity here. If the process RSS is north of 500MB, we should be able to spot what's filling up the heap pretty easily.. The offending method: https://github.com/linkerd/linkerd/blob/9f29a70b0a1adfad3aa81232cd119353976a7ab6/telemetry/core/src/main/scala/io/buoyant/telemetry/Metric.scala#L30. I see the same lock contention on a http/1.1 router using wrk2 and the same linear slow down with number of connections.\nPlease note: each load test sends the same amount of RPS regardless of concurrency level, (unlike many slow_cooker tests). This is against linkerd 1.0.2.\nconcurrency 2:\n```\n       0.900     0.950000        13906        20.00\n       1.252     0.990625        14500       106.67\n       3.193     0.999023        14623      1024.00\n```\nconcurrency 10:\n1.335     0.950000        47481        20.00\n       5.539     0.995313        49742       213.33\n      12.887     0.999023        49931      1024.00\nconcurrency 50:\n3.583     0.950000        47381        20.00\n       9.783     0.995313        49640       213.33\n      20.447     0.999023        49824      1024.00\n. linkerd-1.0.0\nconcurrency 2:\n0.670     0.950000        32547        20.00\n       1.777     0.990625        33921       106.67\n       2.671     0.999023        34209      1024.00\nconcurrency 10:\n3.183     0.990625        49512       106.67\n       4.247     0.995313        49745       213.33\n       6.939     0.999023        49932      1024.00\nconcurrency 50:\n3.195     0.950000        47383        20.00\n       9.687     0.995313        49639       213.33\n      19.631     0.999023        49825      1024.00. linkerd-0.8.5\nconcurrency 2:\n0.671     0.950000        33102        20.00\n       1.916     0.990625        34512       106.67\n       3.037     0.999023        34805      1024.00\nconcurrency 10:\n1.158     0.950000        47481        20.00\n       3.287     0.990625        49511       106.67\n       6.851     0.999023        49931      1024.00\nconcurrency 50:\n3.297     0.950000        47379        20.00\n       7.791     0.990625        49405       106.67\n      17.743     0.999023        49824      1024.00. It turns out I ran numbers at 10k qps back to 0.7.5 and left them in this comment: https://github.com/linkerd/linkerd/issues/1251#issuecomment-300591713. So I put together a branch to reduce how long we're holding this lock. Good news: the lock has dropped from #1 to #6 in thetop20 lock contention profile. Bad news: there was no improvement in the performance numbers.\n(pprof) top\nTotal: 6253 samples\n    1544  24.7%  24.7%     1550  24.8% com.twitter.finagle.pool.CachingPool.apply\n    1018  16.3%  41.0%     2568  41.1% com.twitter.finagle.pool.WatermarkPool.apply\n     936  15.0%  55.9%      977  15.6% com.twitter.finagle.pool.WatermarkPool$ServiceWrapper.close\n     639  10.2%  66.2%      639  10.2% sun.nio.ch.EPollSelectorImpl.doSelect\n     494   7.9%  74.1%      494   7.9% sun.nio.ch.EPollSelectorImpl.wakeup\n     336   5.4%  79.4%      336   5.4% com.twitter.finagle.stats.buoyant.MetricsBucketedHistogram.add\n      96   1.5%  81.0%       96   1.5% com.twitter.finagle.filter.NackAdmissionFilter.$anonfun$afterSend$1\n      94   1.5%  82.5%       94   1.5% com.twitter.util.TimeFormat.format\n      54   0.9%  83.3%       54   0.9% com.twitter.finagle.liveness.FailureAccrualFactory.com$twitter$finagle$liveness$FailureAccrualFactory$$stopProbing\n      48   0.8%  84.1%     5519  88.3% com.twitter.util.Promise.runq. So this patch plus other changes in master reduces the number of total threads blocked on a lock by roughly 20%. Here's the same numbers from the 1.1.0 release.\nUsing local file ./locks_100_release.\nTotal: 7928 samples\n    1972  24.9%  24.9%     1972  24.9% io.buoyant.telemetry.Metric$Stat.add\n    1367  17.2%  42.1%     1377  17.4% com.twitter.finagle.pool.CachingPool.apply\n     600   7.6%  49.7%     1981  25.0% com.twitter.finagle.pool.WatermarkPool.apply\n     556   7.0%  56.7%      556   7.0% sun.nio.ch.EPollSelectorImpl.doSelect\n     554   7.0%  63.7%      554   7.0% com.twitter.finagle.loadbalancer.LoadBalancerFactory$StackModule$$anon$2.status\n     543   6.8%  70.5%      607   7.7% com.twitter.finagle.pool.WatermarkPool$ServiceWrapper.close\n     495   6.2%  76.8%      495   6.2% sun.nio.ch.EPollSelectorImpl.wakeup\n     130   1.6%  78.4%      130   1.6% com.twitter.finagle.filter.NackAdmissionFilter.$anonfun$afterSend$1\n     110   1.4%  79.8%      110   1.4% com.twitter.util.TimeFormat.format\n      93   1.2%  81.0%       93   1.2% com.twitter.finagle.liveness.FailureAccrualFactory.com$twitter$finagle$liveness$FailureAccrualFactory$$stopProbing\n. I haven't verified this yet but getting a heap dump from docker containers should be doable with linkerd since we use a full jdk that supplies jmap\nhttps://dkbalachandar.wordpress.com/2016/07/05/thread-dump-from-a-docker-container/\n. Copied the tests over and I somehow broke the bucketing logic. diff leads me to believe that I didn't change anything in the logic.. Occam's razor strikes again: the bug was caused by my changes. This latest version simplifies the logic for easier merging back into finagle.. This test is invalid because it tested two different versions. I didn't notice that until I re-ran the numbers.. We're changing this to $JAVA_HOME/bin/java in another branch in java_exec.\n. I think that makes sense and have now made that change. I wouldn't expect it to make a difference (and it doesn't seem to) but there's no chance of a timing bug now.\n. Oh, I think Oliver means that +CMSClassUnloadingEnabled is on by default in Java 8. I missed that.\n. replaceAll's first parameter is a regex that is compiled each time this is called. IIRC, you can create a Pattern in a static variable and use that instead to reduce allocations and cpu time.\n. Since these are all single characters, there's probably a more efficient way than using a regex. I'll dig around.\n. can you add backticks to RequeueFilter and RetryFilter for code styling?\n. fixed!\n. Ok, I have changed the wording to make it clear this is for local testing convenience. I added this config because I wanted to test running the container with a simple static config and I felt new users would appreciate that.\nHappy to make other changes.\n. yeah, you're totally right. fixing it.. \ud83d\udc4d . Filed as #992. This fixes server_streaming. Unfortunately, client_streaming is still broken with this but only after a certain size is hit so I suspect it's related to the issue you mentioned in #980.. don't forget to remove this before checkin.. why not default to 1?. What does None mean here? No limit? (that seems ok to me). I have the same question here about why it's not 1.0f. Was this comment meant for me? I don't understand.. is this tail recursive?. you got it.. Doesn't (a,b) allocate a Tuple2 on the heap? That was why I avoided it.. I changed this to be a single var to make the diff from finagle smaller.. Happy to make the change either way but it's not called that often, on the order of O(# of stats) per minute. (that's a good trick to keep in my back pocket, though). \ud83d\udc4d . ",
    "leozc": "Do we have any plan on this ? \nMay be parsing a param from route down to Finagle that will help  ?. Hello all,\nSorry for being late to the party, any plan/progress on the statd plugin for linkers/namerd ? We are on the same boat :-), and would love to see if there is any work been done.. @wmorgan sure, I will look at end of this week or early next week. . @siggy  Thanks very much for working on this :-)\nMy 2 cents feedback has been given in the CR.\n. @olgert @siggy  \nThanks very much to make this happen, it is certainly an important feature in the coming release!\nIn the next step, I would suggest we add a new flag called\n\nprovider_type\n\nwhich add provider specific tags into the metrics. \nHere are the examples I've found, they are pretty much the same code, only differences are the  string formatter in the send and the format of the tags.\nDataDog:\nhttps://github.com/DataDog/java-dogstatsd-client/blob/master/src/main/java/com/timgroup/statsd/NonBlockingStatsDClient.java\nInfluxDB\nhttps://github.com/njawalkar/java-influx-statsd-client/blob/master/src/main/java/com/timgroup/statsd/NonBlockingStatsDClient.java\n. A test to replicate the error\nscala\n test(\"preserve trailing slash\") {\n    val identifier = PathIdentifier(Path.Utf8(\"http\"), 2, consume = true)\n    val req0 = Request()\n    req0.uri = \"/mysvc/subsvc/path1/path2/\"\n    val (dst, req1) = await(identifier(req0))\n    assert(dst == Dst.Path(Path.read(\"/http/mysvc/subsvc\")))\n    assert(req1.uri == \"/path1/path2/\")\n  }\n. YES closed :). What are H1 and H2 referring to ?. @siggy \nRe: This PR is already using the datadog Statsd client\nThank you!\nJust FYI\nBoth InfluxDB and Datadog speaks StatsD, and they BOTH have tagging extension. (the tagging is ideal way to solve the problem we discussed above e.g. host, service etc - they can support operations like group by)\nUnfortunately, the wire protocol is slightly different (not much).\nWe can leave the tag support later.\nPossible tasks look like\ne.g.\n1. Adding new option statsd_backend = [default|datadog|influxDB]\n2. Adding new option for tags: String[] which only valid for non-default statsd backend\n3. Extend the NonBlockingUDP client to emit tags into corresponding wire format based on different backend \n. Tags support should be fairly easy to implement:\nDatadog version:\nhttps://github.com/DataDog/java-dogstatsd-client/blob/master/src/main/java/com/timgroup/statsd/NonBlockingStatsDClient.java\nAn influxDB client implemented using NonBlockingStatsDClient:\nhttps://github.com/njawalkar/java-influx-statsd-client\nSomehow it is worth for the community to merge them together.\n. SUPER looking forward for this PR to release!!. 1. Yes I agree with \"Forwarded\" header is more appropriate \n2. How does it interact with XFF based header - will linkerD ever log it or update or convert to \"Forwarded\"\n3. Reason for asking is to ensure LinkerD access log has sufficient data in all cases, so that individual service can skip implementing access.log and rely on LinkerD access log. . We don't need this now . This would be related to a bug in this fix:\nhttps://github.com/BuoyantIO/linkerd/pull/711/files/0281c61224af19498f212c7b76a8af6ffe1e04b6. Any plan on this ? Rate limiting per service/api level would be a very useful feature . \"per-client rate-limit\" is a global limit  - aka assume we have n boxes - each client on a box is 1/n of the quota.\nCan we reuse the storage model/interface in Dtab storage?\n. Not claim to be 100% related to this issue\nroot@ip-172-16-22-62:/home/ubuntu# lsof -p 2414 | grep CLOSE_W | sed -e 's/.*>//' | sort | uniq -c\n142 ip-172-16-31-45.ec2.internal:http (CLOSE_WAIT)\n997 ip-172-16-31-70.ec2.internal:http (CLOSE_WAIT)\n1 ip-172-16-31-75.ec2.internal:http (CLOSE_WAIT)\n190 ip-172-16-32-14.ec2.internal:http (CLOSE_WAIT)\n523 ip-172-16-32-21.ec2.internal:http (CLOSE_WAIT)\n48 ip-172-16-33-91.ec2.internal:http (CLOSE_WAIT)\nthe IPs that have the high number of CLOSE_WAITs are service ELBs (172.16.3*, i.e. prod-xxxx.foobar.net). i could not find these IPs in any of the obvious ELBs.\nHere it is run on a current routing instance:\nroot@ip-172-16-22-228:/home/ubuntu# lsof -p 2438 | grep CLOSE_W | sed -e 's/.*>//' | sort | uniq -c\n27 ip-172-16-32-83.ec2.internal:http (CLOSE_WAIT)\n27 ip-172-16-33-95.ec2.internal:http (CLOSE_WAIT)\nessentially we see large number of CLOSE_WAIT (which is fine , finagle behavior) and some CLOSE_WAIT to dead ELB.\nAny concern  on this - and will these CLOSE_WAIT connection being reclaim ?. Feature request : possible to support data dog as well?\nhttp://docs.datadoghq.com/guides/dogstatsd/\nIt should be fairly similar beside the string format\n```\nIf you want to send metrics to DogStatsD in your own way, here is the format of the packets:\nmetric.name:value|type|@sample_rate|#tag1:value,tag2\nHere\u2019s breakdown of the fields:\nmetric.name should be a String with no colons, bars or @ characters and fit our naming policy.\nvalue should be a number\ntype should be c for Counter, g for Gauge, h for Histogram, ms for Timer or s for Set.\nsample rate is optional and should be a float between 0 and 1 inclusive.\ntags are optional, and should be a comma seperated list of tags. Colons are used for key value tags. Note that the key device is reserved, tags like \u201cdevice:xyc\u201d will be dropped by Datadog.\nHere are some example datagrams and comments explaining them:\n```. Another incident came in while I was writing this up.\nThanks for @klingerf 's help, we shed some light\nOn  EC2(Ngx -> L5d), I did\nsudo lsof |grep java  |wc -l\nand I found we had 200K + file descriptor, it is very likely the root cause.. Possible - but I didn't catch jvm/fd_count .\nI enabled the statsd plugin, and I don't see this metrics got collected :-/. Thanks when would this CR be released?. Thanks, I will do coordination of release cycle on my side.\nCheers\nLeo\nOn Tue, May 2, 2017 at 8:58 PM, Oliver Gould notifications@github.com\nwrote:\n\n@leozc https://github.com/leozc i expect that we'll do a release by\nearly next week, depending on progress of some other pending issues.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/linkerd/linkerd/pull/1256#issuecomment-298818389, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABcXyJpXvuAJcrq5mZtFFu8-ZkMMMFLhks5r1_tXgaJpZM4NK0rm\n.\n. Repro on \nL5D 1.1 and Namerd 1.1. Interesting that means the DNS resolving is on namerd ? Let me dig ...\nOn Wed, Jun 14, 2017 at 11:38 Alex Leong notifications@github.com wrote:\nBased on the stacktrace in the linkerd log above, it looks like namerd is\nencountering a NullPointerException and returning that to linkerd. Do you\nhave logs/stack traces for namerd as well?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/linkerd/linkerd/issues/1387#issuecomment-308521249,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABcXyGu78k1vBUI3FFi-Wq0jXOW7oRUNks5sECitgaJpZM4N5OZa\n.\n. I have been out of town - will try to get the log when I am back.\n\nCheers\nLeo@mobile\n\nOn Jun 21, 2017, at 05:03, Alex Leong notifications@github.com wrote:\nHi @leozc, were you able to get those namerd logs?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. W 0628 23:24:22.620 UTC THREAD20: Exception propagated to the default monitor (upstream address: /172.23.32.183:62799, downstream address: n/a, label: adminhttp).\njava.lang.NullPointerException\n        at io.buoyant.admin.names.DelegateApiHandler$Address$.mk(DelegateApiHandler.scala:34)\n        at io.buoyant.admin.names.DelegateApiHandler$Addr$.$anonfun$mk$1(DelegateApiHandler.scala:57)\n        at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:241)\n        at scala.collection.immutable.Set$Set1.foreach(Set.scala:95)\n        at scala.collection.TraversableLike.flatMap(TraversableLike.scala:241)\n        at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:238)\n        at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)\n        at io.buoyant.admin.names.DelegateApiHandler$Addr$.mk(DelegateApiHandler.scala:57)\n        at io.buoyant.admin.names.DelegateApiHandler$Bound$.$anonfun$mk$3(DelegateApiHandler.scala:79)\n        at com.twitter.util.Future.$anonfun$map$2(Future.scala:1145)\n        at com.twitter.util.Try$.apply(Try.scala:15)\n        at com.twitter.util.Future$.apply(Future.scala:163)\n        at com.twitter.util.Future.$anonfun$map$1(Future.scala:1145)\n        at com.twitter.util.Promise$Transformer.liftedTree1$1(Promise.scala:107)\n        at com.twitter.util.Promise$Transformer.k(Promise.scala:107)\n        at com.twitter.util.Promise$Transformer.apply(Promise.scala:117)\n        at com.twitter.util.Promise$Transformer.apply(Promise.scala:98)\n        at com.twitter.util.Promise$$anon$2.run(Promise.scala:823)\n        at com.twitter.concurrent.LocalScheduler$Activation.run(Scheduler.scala:200)\n        at com.twitter.concurrent.LocalScheduler$Activation.submit(Scheduler.scala:158)\n        at com.twitter.concurrent.LocalScheduler.submit(Scheduler.scala:272)\n        at com.twitter.concurrent.Scheduler$.submit(Scheduler.scala:108)\n        at com.twitter.util.Promise.runq(Promise.scala:406)\n        at com.twitter.util.Promise.updateIfEmpty(Promise.scala:801)\n        at com.twitter.util.Promise.update(Promise.scala:775)\n        at com.twitter.util.Promise.setValue(Promise.scala:751)\n        at com.twitter.concurrent.AsyncQueue.offer(AsyncQueue.scala:123)\n        at com.twitter.finagle.netty3.transport.ChannelTransport.handleUpstream(ChannelTransport.scala:56)\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n        at org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:142)\n        at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n        at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n        at com.twitter.finagle.http.SafeServerHttpChunkAggregator.handleUpstream(Codec.scala:27)\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.messageReceived(SimpleChannelUpstreamHandler.java:124)\n        at com.twitter.finagle.http.codec.RespondToExpectContinue.messageReceived(RespondToExpectContinue.scala:28)\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.messageReceived(SimpleChannelUpstreamHandler.java:124)\n        at com.twitter.finagle.http.codec.PayloadSizeHandler.messageReceived(PayloadSizeHandler.scala:22)\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n        at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:108)\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n        at org.jboss.netty.handler.codec.http.HttpContentEncoder.messageReceived(HttpContentEncoder.java:82)\n        at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)\n        at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)\n        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)\n        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n        at org.jboss.netty.handler.codec.http.HttpServerCodec.handleUpstream(HttpServerCodec.java:56)\n        at com.twitter.finagle.http.SafeHttpServerCodec.handleUpstream(Codec.scala:46)\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)\n        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\n        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)\n        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)\n        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\n        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\n        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run(BlockingTimeTrackingThreadFactory.scala:24)\n        at java.lang.Thread.run(Thread.java:745). Cool , which build is it?\nOn Fri, Jun 30, 2017 at 14:51 Alex Leong notifications@github.com wrote:\nI'm not able to reproduce this but based on the stack trace, I believe\nthis should fix it: #1460 https://github.com/linkerd/linkerd/pull/1460\n@leozc https://github.com/leozc can you take a look and let me know if\nthat fixes the issue for you?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/linkerd/linkerd/issues/1387#issuecomment-312380637,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABcXyKsQD2sCY10SZ2N682Bt2HDafUI7ks5sJW3NgaJpZM4N5OZa\n.\n. +1 for this task. Thanks heaps for the quick response - I will work on that tomorrow\nOn Tue, Oct 10, 2017 at 18:59 Kevin Lingerfelt notifications@github.com\nwrote:\nHey @leozc https://github.com/leozc -- thanks for reporting. This\nsounds like an issue with caching the DNS resolution for the CNAME for too\nlong. Linkerd uses finagle, which I believe uses the default JVM\nimplementation, to resolve CNAMEs. We'll need to investigate where the bad\ncaching behavior might be coming from.\nI'm not too familiar with ELB, but is there any chance it publishes SRV\nrecords? We just added the io.l5d.dnssrv namer\nhttps://linkerd.io/config/1.3.0/linkerd/index.html#dns-srv-records,\nwhich will do service discovery on a SRV record, and load balance against\nall IPs contained in the record. That actually sounds more appropriate for\nyour use case. Otherwise, with CNAMEs, I think linkerd just picks the first\nIP based on the default DNS implementation, and uses that permanently.\nThere isn't any load balancing for CNAMEs that resolve to more than one IP.\nIf it's at all possible to switch to SRV, you might want to try using that\ninstead.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/linkerd/linkerd/issues/1666#issuecomment-335658884,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABcXyDVSfI1cKZvvkXqX0DbQMO_xXALyks5srCDzgaJpZM4PzaoE\n.\n. Another incident -> 1 of the 3 Namerd instances didn't pick up the change.\n\nThe step as follow:\nfoo.bar is alias of elb.bar\nand we update the foo.bar points to alias of elb2.bar \n1 of the 3 namerd box didn't pick up the change.\nrestart that specific Namerd Box fixed the issue. This is a good request, for now, to work around we are using logrotated . Which GC did you use for the experiment - great to know too!. I am working on plugins base onAnnouncerInitializer and IdentifierInitializer  - I don't see the factory methods receiving Stack.Params.\nPS: my IdentifierInitializer minics StaticIdentifierInitializer\nObservation\nThe factory for H1 identifier - \ndoesn't have the Stacks.Param injected\n@JsonIgnore\n  private[this] val combinedIdentifier = identifier.map { configs =>\n    Http.param.HttpIdentifier { (prefix, dtab) =>\n      RoutingFactory.Identifier.compose(configs.map(_.newIdentifier(prefix, dtab)))\n    }\n  }\nin contrast h2 counter part has it.\nprivate[this] def identifierParam: H2.Identifier = identifier match {\n    case None => h2.HeaderTokenIdentifier.param\n    case Some(configs) =>\n      H2.Identifier { params =>\n        val identifiers = configs.map(_.newIdentifier(params))\n        RoutingFactory.Identifier.compose(identifiers)\n      }\n  }\nAs expected Announcer initializer doesn't pass the Stacks.Param along neither - even the Stack.param is just two lines above:\n```\n  @JsonIgnore\n  def router(params: Stack.Params): Router = {\n    val prms = params ++ routerParams\n    val param.Label(label) = prms[param.Label]\n    val announcers = announcers.toSeq.flatten.map { announcer =>\n      announcer.prefix -> announcer.mk\n    }\n    protocol.router.configured(prms)\n      .serving(servers.map(.mk(protocol, label)))\n      .withAnnouncers(announcers)\n  }\n```. Added a pull of concept fix for Announcer\nhttps://github.com/linkerd/linkerd/pull/1944. We need to work on the identifier type as well - should I open another bug\nor reuse this one ?\nI am happy to work on the identifier plugin improvement as well.\nOn Tue, May 22, 2018 at 13:38 Alex Leong notifications@github.com wrote:\n\n@leozc https://github.com/leozc should we close this now that params\nare being passed to Announcer plugins? Or are there other types of plugins\nthat need this change as well?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/linkerd/linkerd/issues/1922#issuecomment-391132579,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABcXyMXbE_aVryP30q5mHB0R86k6CKkRks5t1HdGgaJpZM4TrQf3\n.\n. Working perfectly - after picking up 1.4.1 and rebuild - now my announcer can emit the stat to the router it attaches to.\ne.g.\n  \"rt/svc_ingress/announcer_hc_cln/http/response_size.p9990\" : 166,\n  \"rt/svc_ingress/announcer_hc_cln/http/response_size.p9999\" : 166,\n  \"rt/svc_ingress/announcer_hc_cln/http/response_size.sum\" : 3320,\n  \"rt/svc_ingress/announcer_hc_cln/http/response_size.avg\" : 166.0,\n  \"rt/svc_ingress/announcer_hc_cln/http/status/400\" : 131,\n  \"rt/svc_ingress/announcer_hc_cln/http/status/4XX\" : 131,\n  \"rt/svc_ingress/announcer_hc_cln/http/time/400.count\" : 20,\n  \"rt/svc_ingress/announcer_hc_cln/http/time/400.max\" : 65,. Resolve this - and keep Identifier work in #1963. Resolved!. ```\n  @JsonIgnore\n  def router(params: Stack.Params): Router = {\n    val prms = params ++ routerParams\n    val param.Label(label) = prms[param.Label]\n    val announcers = announcers.toSeq.flatten.map { announcer =>\n      announcer.prefix -> announcer.mk(params)\n    }\n    protocol.router.configured(prms)\n      .serving(servers.map(.mk(protocol, label)))\n      .withAnnouncers(announcers)\n  }\n\n```\nWill do another push when I get back. Added missing file - Route.scala.. Done. @dadjeibaah - I am working on some test on my end - and we have the Announcer done.\nEssentially what it does is to announce L5D node to Dynamodb. https://github.com/linkerd/linkerd/blob/fe822302d38f249533d1af326ea06efd842bf9b4/linkerd/protocol/http/src/main/scala/io/buoyant/linkerd/protocol/HttpConfig.scala#L247\nHere should be where the identifier being construed . PR: https://github.com/linkerd/linkerd/pull/1964. Reenable - pull the latest linkerd and debug through and found out the statReceiveer is NullStatReceiver:\nHere is the corresponding section from http://linkerd:9990/admin/registry.json\n\"NackAdmissionFilter\" : {\n              \"statsReceiver\" : \"NullStatsReceiver/common_auth/l5d_authIdentifier\",\n              \"window\" : \"2.minutes\",\n              \"nackRateThreshold\" : \"0.5\"\n            },. Thanks @adleong , this seems to be a right fix - now I can also see adminhttp/* metrics in additional to private metrics.. May be worth to note  InetAddress.getAllByName does return IPv4 and IPv6 address entries for the same host - we may want to add a parameter to indicate which class of address to use.\n```\n$ export _JAVA_OPTIONS=\"-Djava.net.preferIPv4Stack=true\"\n$ scala\nPicked up _JAVA_OPTIONS: -Djava.net.preferIPv4Stack=true\nWelcome to Scala 2.12.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_172).\nType in expressions for evaluation. Or try :help.\nscala> import java.net.{InetAddress, InetSocketAddress, UnknownHostException}\nimport java.net.{InetAddress, InetSocketAddress, UnknownHostException}\nscala> InetAddress.getAllByName(\"api.mixerbox.com\").toTraversable.map(singleIP => new InetSocketAddress(singleIP, 443))\nres0: Traversable[java.net.InetSocketAddress] = ArrayBuffer(api.mixerbox.com/54.88.94.148:443, api.mixerbox.com/54.152.138.221:443, api.mixerbox.com/107.23.90.88:443)\n```\nwithout the IPv4 only Option\n```\nscala> InetAddress.getAllByName(\"api.mixerbox.com\").toTraversable.map(singleIP => new InetSocketAddress(singleIP, 443))\nscala> InetAddress.getAllByName(\"api.mixerbox.com\").toTraversable.map(singleIP => new InetSocketAddress(singleIP, 443))\nres22: Traversable[java.net.InetSocketAddress] = ArrayBuffer(api.mixerbox.com/54.88.94.148:443, api.mixerbox.com/107.23.90.88:443, api.mixerbox.com/54.152.138.221:443, api.mixerbox.com/2600:1f18:6705:7c1f:7c20:eac8:de83:4048:443, api.mixerbox.com/2600:1f18:6705:7c18:732e:d3c0:71:4862:443, api.mixerbox.com/2600:1f18:6705:7c20:6a79:c479:c7e0:b573:443)\n```. It works - my concern may be artificial - as in the example on working with\nELB, each ELB instance returns one IPv4 and IPv6 address - from load\nbalancing prospective - it should still fine.\nLeo\nOn Mon, Jul 2, 2018 at 2:15 PM, Alex Leong notifications@github.com wrote:\n\n@leozc https://github.com/leozc was there a problem with returning both\nIPv4 and IPv6? I haven't specifically tested IPv6 but, at least\ntheoretically, it should work.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/linkerd/linkerd/issues/2030#issuecomment-401938787,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABcXyB0s932BtNGcIY1nxM-DSIL5RGBKks5uCo2BgaJpZM4U-lDY\n.\n. Fixed. Before:\n\n\nAfter:\n\n. Yes, I did pass the ipv4 param as a boot parameter - the problem is when people who prefer to use IPv6 only - in this case they set the preferIPv4Stack false, and they will get the mixed of v4 and v6 addresses.. @adleong  DCO done. Please try again... . This is a better solution -> closing this issue!. This shows the DNS record is there but Namerd failed to resolve - please note when it fails - it doesn't fail all destination but only a random specific one (e.g. while resolving foo1.srv.bar fails ,  foo2.srv.bar can be resolvable - the failure is partial.\n\n\n. When a namerd got into an error status\nWe can see this from tracing\nAug 09 11:30:24 ip-172-25-181-97 namerd[14732]: D 0809 11:30:24.778 UTC THREAD10 TraceId:d92a95fa1320eb49: SRV lookup failure: no results for webapp.staging.offerup.services\nfrom the code \nhttps://github.com/linkerd/linkerd/blob/db8d40628d747857f4f6d24b9c6f5d00aff572cf/namer/dnssrv/src/main/scala/io/buoyant/namer/dnssrv/DnsSrvNamer.scala#L40\nAnd from DTAB - SRV failed to resolve\n\nAfter restarting\n\nHence I am pretty sure the bug is in where SRV namer. Added:\nEnvironment=\"JVM_OPTIONS=-Dsun.net.inetaddr.ttl=30 -Djava.net.preferIPv4Stack=true\" to namerd startup script and continue to monitor.... Add more verbose logging:\nAug 09 17:56:58 ip-172-25-181-97 namerd[20367]: D 0809 17:56:58.491 UTC THREAD9 TraceId:981ecf34ec0ff32b: SRV lookup failure: no results for shipping.staging.offerup.services return type: 4 address: shipping.staging.offerup.services\n4 means\n/** The host exists, but has no records associated with the queried type. */\npublic static final int TYPE_NOT_FOUND = 4;\nDig it\n```\ndig srv shipping.staging.offerup.services\n; <<>> DiG 9.10.3-P4-Ubuntu <<>> srv shipping.staging.offerup.services\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 38932\n;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 4096\n;; QUESTION SECTION:\n;shipping.staging.offerup.services. IN  SRV\n;; ANSWER SECTION:\nshipping.staging.offerup.services. 10 IN SRV    1 1 4141 ip-172-25-41-9.ec2.internal.\n;; Query time: 5 msec\n;; SERVER: 172.25.0.2#53(172.25.0.2)\n;; WHEN: Thu Aug 09 18:00:02 UTC 2018\n;; MSG SIZE  rcvd: 109\n```\n\n. When capturing the packet:\nsudo tshark -f \"udp port 53\"\nfor the DNS experiencing the bug - I cannot see the query being dispatched\nyet for normal one I do see the proper response:\n581 150.320323586 172.25.181.97 \u2192 172.25.0.2   DNS 91 Standard query 0x5e38 SRV webapp.staging.offerup.services\n  582 150.323167736   172.25.0.2 \u2192 172.25.181.97 DNS 139 Standard query response 0x5e38 SRV webapp.staging.offerup.services SRV 1 1 4141 ip-172-25-41-71.ec2.internal\nBy restarting the Namerd process - query the DATB again and we see the recovery and observe the dump from wireshark capture.. Catch something here... \nWhen observing \nAug 14 00:24:33 ip-172-25-182-80 namerd[19118]: D 0814 00:24:33.141 UTC THREAD11 TraceId:3038e6a7cee4e966: SRV lookup failure: no results for search.staging.offerup.services return type: 4 type not found  address: search.staging.offerup.services\nWe see:\n\n\nSuspect something went wrong in the memorization block\n```scala\nprivate val memoizedLookup: (Path) => Activity[NameTree[Name]] = Memoize { path =>\n    path.take(1) match {\n      case id@Path.Utf8(address) =>\n        val vaddr = watchDns(address, timer).run.map {\n          case Activity.Ok(rsp) =>\n            Addr.Bound(rsp: _*)\n          case Activity.Pending =>\n            Addr.Pending\n          case Activity.Failed(e) =>\n            log.debug(s\"SRV lookup failure: %s address: %s pattern: %s\" , e.getMessage, address, id)\n            Addr.Failed(e)\n        }\n    val state: Var[Activity.State[NameTree[Name]]] = vaddr.map {\n      case Addr.Bound(addrs, _) if addrs.isEmpty =>\n        Activity.Ok(NameTree.Neg)\n      case Addr.Bound(addrs, _) =>\n        Activity.Ok(NameTree.Leaf(Name.Bound(vaddr, prefix ++ id, path.drop(1))))\n      case Addr.Pending =>\n        Activity.Pending\n      case Addr.Failed(_) =>\n        Activity.Ok(NameTree.Neg)\n    }\n\n    Activity(state)\n  case _ =>\n    Activity.value(NameTree.Neg)\n}\n\n}\n```\nnote (@ccmtaylor): I fixed the markdown syntax in the comment. It seems it is related to the cache in the DNS.Lookup cache - which if I\ndisabled the cache (set it null), the code works fine (in past 6 hours)\nI also investigated the bad response - as far as I can tell - the TTL looks\nreasonable.\nAs it is for now - I am 90% sure it is cache related.\nTo @ccmtaylor  questions:\n\n@leozc, in your debugger screenshot, is the cache you highlighted part of the dnsjava library? NXRRSET is a very unexpected response from the DNS server: afaict, it's related to dyndns updates, so I don't know how that should occur. The value of the expire field looks like a timestamp. This is just a guess, but if dnsjava is caching bad responses for a long time (could be verified by comparing that timestamp to \"now\"), that may explain the behaviour we're seing here.\nEach DNS has a task guarded by a timer and keep looping - the error message is from the task \nscala\ndoUnit()\n - potentially I can see this as design bug which allow infinite number DNS being looped in the timer;\n@leozc another question: are the \"SRV lookup failure: no results for ...\" log messages repeated every refresh interval, or do you see the message once, and then namerd is stuck forever?\nNope.\n. A fix is verified as below:\nscala\nval lookup = new DNS.Lookup(dsnsrvRecord, DNS.Type.SRV, DNS.DClass.IN)\nlookup.setResolver(resolver)\nlookup.setCache(null) // Not to use the default cache. Still some problems - go deeper in the problem by digging into Lookup class within dnsjava lib - when a DNS gets into an issue - in the response we observed SOA record instead of a SRV record (!) - when I intercepted the code path - manually run the query again (resolver,send(query) before the actual query code path - the resolver returns correct result and THEN the subsequent resolution is also correct  -- amazing - seems some serious pollution somewhere\n\n\nThis makes me to think about if we handle the error case consistently:\nExperiment A - \ntry to resolve a domain name doesn't exist say payments5.blah....\nI see the DNS is being resolved ONCE and no longer being refreshed (observed from print log)\n\nAug 20 21:58:14 ip-172-25-182-212 namerd[15348]: D 0820 21:58:14.646 UTC THREAD25 TraceId:650430c92e680d00: thread admin-1 id: 25 dns: payments5.staging.offerup.services lookup org.xbill.DNS.Lookup@2fc5735d iteration: 1\nAug 20 21:58:14 ip-172-25-182-212 namerd[15348]: D 0820 21:58:14.646 UTC THREAD25 TraceId:650430c92e680d00: in doUnit: message no results for payments5.staging.offerup.services return type: 3 host not found iteration: 1\nAug 20 21:58:14 ip-172-25-182-212 namerd[15348]: D 0820 21:58:14.646 UTC THREAD25 TraceId:650430c92e680d00: SRV lookup failure: no results for payments5.staging.offerup.services return type: 3 host not found iteration: 1  address: payments5.staging.offerup.services pattern: Path(payments5.staging.offerup.services)\n\nExperiment B:\nfor domain name autocomplete, we deleted the SRV record, and then we start to observe\n\nAug 20 21:45:06 ip-172-25-182-212 namerd[15348]: D 0820 21:45:06.470 UTC THREAD11 TraceId:8196d5a926ac3348: thread Netty 4 Timer-1 id: 11 dns: autocomplete.staging.offerup.services lookup org.xbill.DNS.Lookup@115f6971 iteration: 722\nAug 20 21:45:06 ip-172-25-182-212 namerd[15348]: D 0820 21:45:06.470 UTC THREAD11 TraceId:8196d5a926ac3348: in doUnit: message no results for autocomplete.staging.offerup.services return type: 4 type not found iteration: 722\nAug 20 21:45:06 ip-172-25-182-212 namerd[15348]: D 0820 21:45:06.471 UTC THREAD11 TraceId:8196d5a926ac3348: SRV lookup failure: no results for autocomplete.staging.offerup.services return type: 4 type not found iteration: 722  address: autocomplete.staging.offerup.services pattern: Path(autocomplete.staging.offerup.services)\nAug 20 21:45:06 ip-172-25-182-212 namerd[15348]: D 0820 21:45:06.471 UTC THREAD11 TraceId:8196d5a926ac3348: SRV lookup failure: no results for autocomplete.staging.offerup.services return type: 4 type not found iteration: 722  address: autocomplete.staging.offerup.services pattern: Path(autocomplete.staging.offerup.services)\n\nThen insert back the record - it then recovered after 2 mins.\nExperiment A and Experiment B are both responsible by the same code path \nhttps://github.com/linkerd/linkerd/blob/db8d40628d747857f4f6d24b9c6f5d00aff572cf/namer/dnssrv/src/main/scala/io/buoyant/namer/dnssrv/DnsSrvNamer.scala#L70\nWhy would they behave differently aka. one stops to continue the loop yet another one continue to pull.\n. @dadjeibaah \n\nThat is strange @leozc, you should be seeing a lookup for a non-existent DNS to always be called based on the refreshInterval configured for the DNS Namer. The Lookup is always executed and is not dependent on the results of the DNS query. One way to verify that the namer is periodically running the lookup is to view the DNS namer stats on admin/metrics.json and look for the lookup_failures_total field. That total should be always increasing.\n\nThis is what I thought, but the fact is - it is not the case - for domain name doesn't exist - the loop only runs once.\nThe trace looks like this:\n\nAug 21 01:56:27 ip-172-25-182-212 namerd[15348]: D 0821 01:56:27.845 UTC THREAD28 TraceId:0dd0da7dcf39f614: thread admin-2 id: 28 dns: abcdefg.staging.offerup.services lookup org.xbill.DNS.Lookup@3dc45a4a iteration: 0\nAug 21 01:56:27 ip-172-25-182-212 namerd[15348]: D 0821 01:56:27.846 UTC THREAD28 TraceId:0dd0da7dcf39f614: in doUnit: message no results for abcdefg.staging.offerup.services return type: 3 host not found iteration: 0\nAug 21 01:56:27 ip-172-25-182-212 namerd[15348]: D 0821 01:56:27.846 UTC THREAD28 TraceId:0dd0da7dcf39f614: SRV lookup failure: no results for abcdefg.staging.offerup.services return type: 3 host not found iteration: 0  address: abcdefg.staging.offerup.services pattern: Path(abcdefg.staging.offerup.services)\n\nAs you may notice I have a Sequence number iteration: 0 which I put that in to keep trace of the life cycle of the timer loop, and this number increases if I repeat the same search for the non-exist DNS (+1) - hence I am pretty sure the loop is there but doesn't run.\nTo make this problem more measurable here is the metrics (which added in the PR #2111 )\n\"namer/#/io.l5d.dnssrv/#/io.l5d.dnssrv/lookup_dns/iteration/a.staging.offerup.services\" : 1.0,\n  \"namer/#/io.l5d.dnssrv/#/io.l5d.dnssrv/lookup_dns/iteration/auth.integration.offerup.services\" : 866.0,\n  \"namer/#/io.l5d.dnssrv/#/io.l5d.dnssrv/lookup_dns/iteration/b.staging.offerup.services\" : 1.0,\n  \"namer/#/io.l5d.dnssrv/#/io.l5d.dnssrv/lookup_dns/iteration/boards.integration.offerup.services\" : 203.0,\n  \"namer/#/io.l5d.dnssrv/#/io.l5d.dnssrv/lookup_dns/iteration/bulkrequests.integration.offerup.services\" : 880.0,\n  \"namer/#/io.l5d.dnssrv/#/io.l5d.dnssrv/lookup_dns/iteration/c.staging.offerup.services\" : 1.0,\n  \"namer/#/io.l5d.dnssrv/#/io.l5d.dnssrv/lookup_dns/iteration/chat.integration.offerup.services\" : 813.0,\nAs you can see a.staging.offerup.services b.staging.offerup.services, acstaging.offerup.services are the DNS non-resolvable  - and the iteration counter is not being increased.\n@ccmtaylor  this may interest you ^^ . https://github.com/linkerd/linkerd/pull/2111 is my work in progress\n@dadjeibaah this may help you to understand what I discussed above - I tried the best to isolate each Lookup/resolver object from each domain name ( to avoid pollution). By disabling DNS Lookup cache from dnsjava lib (within java process) - I am not seeing any perf issue\nalso based on the implementation we are using (a background pulling model) - perf shouldnt be an issue\n\"namer/#/io.l5d.dnssrv/#/io.l5d.dnssrv/request_duration_ms.count\" : 132,\n  \"namer/#/io.l5d.dnssrv/#/io.l5d.dnssrv/request_duration_ms.max\" : 13,\n  \"namer/#/io.l5d.dnssrv/#/io.l5d.dnssrv/request_duration_ms.min\" : 0,\n  \"namer/#/io.l5d.dnssrv/#/io.l5d.dnssrv/request_duration_ms.p50\" : 1,\n  \"namer/#/io.l5d.dnssrv/#/io.l5d.dnssrv/request_duration_ms.p90\" : 2,\n  \"namer/#/io.l5d.dnssrv/#/io.l5d.dnssrv/request_duration_ms.p95\" : 3,\n  \"namer/#/io.l5d.dnssrv/#/io.l5d.dnssrv/request_duration_ms.p99\" : 9,\n  \"namer/#/io.l5d.dnssrv/#/io.l5d.dnssrv/request_duration_ms.p9990\" : 13,\n  \"namer/#/io.l5d.dnssrv/#/io.l5d.dnssrv/request_duration_ms.p9999\" : 13,\n  \"namer/#/io.l5d.dnssrv/#/io.l5d.dnssrv/request_duration_ms.sum\" : 158,\n  \"namer/#/io.l5d.dnssrv/#/io.l5d.dnssrv/request_duration_ms.avg\" : 1.196969696969697,. @dadjeibaah  I have added some metrics of for the state watcher in #2095  PR. My change mainly rolls the code to conservative side,  which have isolated Resolver, Lookuo, Cache and Counter (seq number) for each of the DNS.. Tested on production for 2 weeks - everything seems fine. @adleong  sorry for taking a while - now it should be ready . Remove per dns metrics to reduce # of counter. > @leozc I think you need to update the integration test: https://circleci.com/gh/linkerd/linkerd/8085#tests/containers/1\nsorry didnt see that - done.. fixed\n. opps, fixed\n. should we mark some of the kind\" \"experimental\" \nlinkerd doc example\n\nexperimental  false   Set this to true to opt-in to experimental XXX support.. Sorry, I am not sure about the purpose of having a random UUID as the prefix;\nWon't this result different metrics name for different linkerd process even for the same service? . Also it may be worth to catch StatsDClientException, since this exception may be thrown upon initiation according to \nNonBlockingStatsDClient.java. Suggestion: We Should use the datadog Statsd client?\nA statsd client library implemented in Java. Allows for Java applications to easily communicate with statsd.\nThis version was originally forked from java-dogstatsd-client and java-statsd-client but it is now the canonical home for the java-dogstatsd-client. Collaborating with the former upstream projects we have now combined efforts to provide a single release.\n\nDataDog Statsd Client\n\nhttps://github.com/DataDog/java-dogstatsd-client/blob/master/src/main/java/com/timgroup/statsd/NonBlockingStatsDClient.java\n\n// We can then support Datadog tagging (well we are using Datadog) for free.\n```\n private static final StatsDClient statsd = new NonBlockingStatsDClient(\n   statsDPrefix,                       / prefix to any stats; may be null or empty string /\n   statsDHost,                        / common case: localhost /\n   statsDPort,                         / port /\n    MaxQueueSize,                   /queuesize/\n   listofTagFromConfig         // String[]  \n            / Datadog extension: Constant tags, always applied, this can be passed down from config optionally/\n  );\n```\n. I would suggest leaving this to user, user can always generate machine specific prefix (using sed for example, per instance).\nUser probably will encode things like this in prefix, by doing sed for the config file.\nl5d.${stage}.${service}.${instanceId} \nMapping random UUID to actual box and shard is also painful anyway\n. ISO_8859_1 indicates we can only have single byte char in requestMeta, I think it is fine in general.. How often is this being called?\nMaybe we can use lock free structures?\nConcurrencyMap has putIfAbsent and by combining with a concurrent linear structure e.g. ConcurrentLinkedQueue, then we should be able to remove the synchronized block?\nSomething like:\ncache.putIfAbsent(traceId, new ConcurrentLinkedQueue())\ncache.get(traceId).add(record). +1. +1. Ya silly mistake - fixed and ran test - all passed. fixed. yes the pool wasn't in use. agree, it was the debug artifacts... I removed them - only leave the dns and iteration .. A good call @adleong , what is the better way to manage these counters - since as you see, the # of counter scale with the DNS entries.\nI can see the problem here, if large number of DNS is flowed into the request path, then the counter number won't be in control.. Sorry for the late response - I do agree the DNS entries is not 100% necessary and it can cause problem - I will remove them . We have tested on production for 3 weeks handling many 10k/s req with 3\nnamerd boxes - they are small instance - so I would think the scalability\nis fine :). ",
    "bashofmann": "It would be great to include the time taken to serve the request (like the %{UNIT}T flag in apache's format strings). As an alternative: We are using diamond (https://github.com/python-diamond/Diamond) to collect system metrics and push them to graphite, so a diamond is installed on every server anyways. I just pushed the small collector for diamond that we wrote to call the linkerd metrics endpoint here https://github.com/researchgate/diamond-linkerd-collector. It would also be great to add the time the request took to the log. In production we saw hosts that did not connect back to consul for days. I also just tested this with the above mentioned test setup and waited for 10 minutes and linkerd still tries to route the request to the downed instance.\nI'll try to get you some metrics tomorrow.. Unfortunately this issue is not fixed with 1.0.2\nWith the reproduction case I posted above, if my-service goes down I normally get this in the linkerd log:\ncom.twitter.finagle.NoBrokersAvailableException: No hosts are available for /svc/my-service, Dtab.base=[/svc=>/#/io.l5d.consul/dc1], Dtab.local=[]. Remote Info: Not Available\nBut after I restart consul and then stop my-service again, the not running instance still stays in the list of up and running instances, and the log says instead\n```\nE 0515 11:05:48.964 UTC THREAD29 TraceId:47779e72188a85f0: service failure\nFailure(Connection refused: /127.0.0.1:9012 at remote address: /127.0.0.1:9012. Remote Info: Not Available, flags=0x09) with RemoteInfo -> Upstream Address: /127.0.0.1:50808, Upstream Client Id: Not Available, Downstream Address: /127.0.0.1:9012, Downstream Client Id: #/io.l5d.consul/dc1/my-service, Trace Id: 47779e72188a85f0.47779e72188a85f0<:47779e72188a85f0\nCaused by: com.twitter.finagle.ConnectionFailedException: Connection refused: /127.0.0.1:9012 at remote address: /127.0.0.1:9012. Remote Info: Not Available\n    at com.twitter.finagle.netty3.ChannelConnector$$anon$2.operationComplete(Netty3Transporter.scala:79)\n    at org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:409)\n    at org.jboss.netty.channel.DefaultChannelFuture.notifyListeners(DefaultChannelFuture.java:400)\n    at org.jboss.netty.channel.DefaultChannelFuture.setFailure(DefaultChannelFuture.java:362)\n    at org.jboss.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:109)\n    at org.jboss.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:79)\n    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)\n    at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)\n    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run(BlockingTimeTrackingThreadFactory.scala:24)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.ConnectException: Connection refused: /127.0.0.1:9012\n    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n    at org.jboss.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:152)\n    at org.jboss.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:105)\n    ... 9 more\n```. ",
    "amitkumarj441": "@gtcampbell Would like to know if this task is still in backlog, I'm willing to take up this task.\nPlease let me know a bit more about this issue.\nCheers,\nAmit Kumar Jaiswal. Thanks @wmorgan for the prompt response.\nI think it's time to renew hacktoberfest label and let's give this issue a shot.. ",
    "lsjostro": ":+1:\n. Another option could be to listen on a specific port?\n. @wmorgan no it's fine as it is.. just more convenient not to have special config for each service in your prometheus configuration, especially when you have hundreds(maybe thousands?) of micro services. You just want to deploy and get automatic metrics from it.\n. @olix0r np, signed! \ud83d\udc4d \n. @klingerf more cleaner and scala:ish now. ;-)\n. nice catch @siggy! I like your solution. I'll go ahead and add that.\n. @siggy should be fixed now. \n. Thanks! much cleaner! I will fix!\n. ",
    "JustinVenus": "Maybe it would be easier to explain delegations tables by showing another proxy configuration that uses linkerd as a backend and then show how a few utilities would work when a 'Dtab-Local' header is present. \nWhen I explain delegation tables to my peers I use a similiar linkerd.yaml and haproxy.cfg as a starting point.  (Disclaimer probably contains syntax errors)\nlinkerd.yaml\n``` yaml\nadmin:\n  port: 9990\nnamers:\n- kind: io.l5d.consul\n  experimental: true\n  host: 127.0.0.1\n  port: 8500\nrouters:\n- protocol: http\n  baseDtab: |\n    /http/1.1/POST/localhost:10006 => /s/service6-write;\n    /http/1.1/PUT/localhost:10006 => /s/service6-write;\n    /http/1.1/DELETE/localhost:10006 => /s/service6-write;\n    /http/1.1/* => /s;\n    /s/localhost:10001 => /s/service1;\n    /s/localhost:10002 => /s/service2;\n    /s/localhost:10003 => /s/service3;\n    /s/localhost:10004 => /s/service4;\n    /s/localhost:10005 => /s/service5;\n    /s/localhost:10006 => /s/service6-read;\n    /s => /s#;\n    /s# => /#/io.l5d.consul/cluster_1 | /#/io.l5d.consul/cluster_2;\n  label: http-rpc\n  dstPrefix: /http\n  servers:\n  - port: 4041\n    ip: 0.0.0.0\n```\nhaproxy.cfg\n``` sh\nglobal\n        log 127.0.0.1   local0\n        log 127.0.0.1   local1 notice\n        stats socket /tmp/haproxy\n        spread-checks 7\n        maxconn 4096\n        user haproxy\n        group haproxy\n        daemon\ndefaults\n        log     global\n        mode    http\n        option  httplog\n        option  dontlognull\n        option  dontlog-normal\n        retries 2\n        option redispatch\n        option httpclose\n        maxconn 2000\n        contimeout      5000\n        clitimeout      50000\n        srvtimeout      50000\n        stats enable\n        stats uri     /statspage\n        stats refresh 5s\n        stats show-node\nfrontend service1\n        bind 127.0.0.1:10001\n        default_backend linkerd\nfrontend service2\n        bind 127.0.0.1:10002\n        default_backend linkerd\nfrontend service3\n        bind 127.0.0.1:10003\n        default_backend linkerd\nfrontend service4\n        bind 127.0.01:10004\n        default_backend linkerd\nfrontend service5\n        bind 127.0.01:10005\n        default_backend linkerd\nfrontend service6\n        bind 127.0.01:10006\n        default_backend linkerd\nbackend linkerd\n        option httpclose\n        server localhost 127.0.0.1:4041\n```\nshell\n``` sh\n$ curl http://localhost:10006/index.html\nroutes to \"/s/service6-read\" using the consul datacenter \"cluster_1\" with a fallback to \"cluster_2\"\n$ curl -X POST -d @foo.json http://localhost:10006/nomnom\nroutes to \"/s/service-6-write\" using the consul datacenter \"cluster_1\" with a fallback to \"cluster_2\"\n$ curl -H 'Dtab-Local: /s/localhost:10005=>/s/service1' http://localhost:10005/index.html\nroutes to \"/s/service1\" using the consul datacenter \"cluster_1\" with a fallback to \"cluster_2\"\n```\n. I hit this same issue this past Friday, but didn't have time to look into it until this morning.  I think the easiest solution is to add '?passing' as suggested in the initial report to the code in consul/src/main/scala/io/buoyant/consul/v1/CatalogApi.scala.  I take back my initial comment '?passing' has no effect on catalog requests.\n. FWIW this seems inter changeable in response as long as the additional json can be easily ignored.  \nPlease ignore duplicate entry as that is a different issue on my side.\nThe node that is down in my environment is mesos-slave05of2.  Here is some output.\nHealth Endpoint (Passing Only)\nhttp://consul01dv2:8500/v1/health/service/cadence?dc=zerozero&tag=devel&passing\njavascript\n[\n{\nNode: {\nNode: \"mesos-slave01of2\",\nAddress: \"10.150.150.227\",\nTaggedAddresses: null,\nCreateIndex: 4477041,\nModifyIndex: 4596032\n},\nService: {\nID: \"v2.sd-bright-devel-cadence-member_0000000015\",\nService: \"cadence\",\nTags: [\n\"devel\",\n\"shard:0\",\n\"user:bright\"\n],\nAddress: \"\",\nPort: 31204,\nEnableTagOverride: false,\nCreateIndex: 4478435,\nModifyIndex: 4478446\n},\nChecks: [\n{\nNode: \"mesos-slave01of2\",\nCheckID: \"serfHealth\",\nName: \"Serf Health Status\",\nStatus: \"passing\",\nNotes: \"\",\nOutput: \"Agent alive and reachable\",\nServiceID: \"\",\nServiceName: \"\",\nCreateIndex: 4477041,\nModifyIndex: 4477041\n},\n{\nNode: \"mesos-slave01of2\",\nCheckID: \"service:v2.sd-bright-devel-cadence-member_0000000015\",\nName: \"Service 'cadence' check\",\nStatus: \"passing\",\nNotes: \"\",\nOutput: \"TCP connect localhost:31204: Success\",\nServiceID: \"v2.sd-bright-devel-cadence-member_0000000015\",\nServiceName: \"cadence\",\nCreateIndex: 4478436,\nModifyIndex: 4478446\n}\n]\n},\n{\nNode: {\nNode: \"mesos-slave01of2\",\nAddress: \"10.150.150.227\",\nTaggedAddresses: null,\nCreateIndex: 4477041,\nModifyIndex: 4596032\n},\nService: {\nID: \"v2.sd-bright-devel-cadence-member_0000000016\",\nService: \"cadence\",\nTags: [\n\"devel\",\n\"shard:0\",\n\"user:bright\"\n],\nAddress: \"\",\nPort: 31204,\nEnableTagOverride: false,\nCreateIndex: 4482833,\nModifyIndex: 4482864\n},\nChecks: [\n{\nNode: \"mesos-slave01of2\",\nCheckID: \"serfHealth\",\nName: \"Serf Health Status\",\nStatus: \"passing\",\nNotes: \"\",\nOutput: \"Agent alive and reachable\",\nServiceID: \"\",\nServiceName: \"\",\nCreateIndex: 4477041,\nModifyIndex: 4477041\n},\n{\nNode: \"mesos-slave01of2\",\nCheckID: \"service:v2.sd-bright-devel-cadence-member_0000000016\",\nName: \"Service 'cadence' check\",\nStatus: \"passing\",\nNotes: \"\",\nOutput: \"TCP connect localhost:31204: Success\",\nServiceID: \"v2.sd-bright-devel-cadence-member_0000000016\",\nServiceName: \"cadence\",\nCreateIndex: 4482834,\nModifyIndex: 4482864\n}\n]\n}\n]\nHealth Endpoint Everything\nhttp://consul01dv2:8500/v1/health/service/cadence?dc=zerozero&tag=devel\njavascript\n[\n{\nNode: {\nNode: \"mesos-slave01of2\",\nAddress: \"10.150.150.227\",\nTaggedAddresses: null,\nCreateIndex: 4477041,\nModifyIndex: 4596032\n},\nService: {\nID: \"v2.sd-bright-devel-cadence-member_0000000015\",\nService: \"cadence\",\nTags: [\n\"devel\",\n\"shard:0\",\n\"user:bright\"\n],\nAddress: \"\",\nPort: 31204,\nEnableTagOverride: false,\nCreateIndex: 4478435,\nModifyIndex: 4478446\n},\nChecks: [\n{\nNode: \"mesos-slave01of2\",\nCheckID: \"serfHealth\",\nName: \"Serf Health Status\",\nStatus: \"passing\",\nNotes: \"\",\nOutput: \"Agent alive and reachable\",\nServiceID: \"\",\nServiceName: \"\",\nCreateIndex: 4477041,\nModifyIndex: 4477041\n},\n{\nNode: \"mesos-slave01of2\",\nCheckID: \"service:v2.sd-bright-devel-cadence-member_0000000015\",\nName: \"Service 'cadence' check\",\nStatus: \"passing\",\nNotes: \"\",\nOutput: \"TCP connect localhost:31204: Success\",\nServiceID: \"v2.sd-bright-devel-cadence-member_0000000015\",\nServiceName: \"cadence\",\nCreateIndex: 4478436,\nModifyIndex: 4478446\n}\n]\n},\n{\nNode: {\nNode: \"mesos-slave01of2\",\nAddress: \"10.150.150.227\",\nTaggedAddresses: null,\nCreateIndex: 4477041,\nModifyIndex: 4596032\n},\nService: {\nID: \"v2.sd-bright-devel-cadence-member_0000000016\",\nService: \"cadence\",\nTags: [\n\"devel\",\n\"shard:0\",\n\"user:bright\"\n],\nAddress: \"\",\nPort: 31204,\nEnableTagOverride: false,\nCreateIndex: 4482833,\nModifyIndex: 4482864\n},\nChecks: [\n{\nNode: \"mesos-slave01of2\",\nCheckID: \"serfHealth\",\nName: \"Serf Health Status\",\nStatus: \"passing\",\nNotes: \"\",\nOutput: \"Agent alive and reachable\",\nServiceID: \"\",\nServiceName: \"\",\nCreateIndex: 4477041,\nModifyIndex: 4477041\n},\n{\nNode: \"mesos-slave01of2\",\nCheckID: \"service:v2.sd-bright-devel-cadence-member_0000000016\",\nName: \"Service 'cadence' check\",\nStatus: \"passing\",\nNotes: \"\",\nOutput: \"TCP connect localhost:31204: Success\",\nServiceID: \"v2.sd-bright-devel-cadence-member_0000000016\",\nServiceName: \"cadence\",\nCreateIndex: 4482834,\nModifyIndex: 4482864\n}\n]\n},\n{\nNode: {\nNode: \"mesos-slave05of2\",\nAddress: \"10.150.150.231\",\nTaggedAddresses: null,\nCreateIndex: 4458480,\nModifyIndex: 4611589\n},\nService: {\nID: \"v2.sd-bright-devel-cadence-member_0000000014\",\nService: \"cadence\",\nTags: [\n\"devel\",\n\"shard:0\",\n\"user:bright\"\n],\nAddress: \"\",\nPort: 31308,\nEnableTagOverride: false,\nCreateIndex: 4478149,\nModifyIndex: 4478418\n},\nChecks: [\n{\nNode: \"mesos-slave05of2\",\nCheckID: \"serfHealth\",\nName: \"Serf Health Status\",\nStatus: \"passing\",\nNotes: \"\",\nOutput: \"Agent alive and reachable\",\nServiceID: \"\",\nServiceName: \"\",\nCreateIndex: 4458480,\nModifyIndex: 4458480\n},\n{\nNode: \"mesos-slave05of2\",\nCheckID: \"service:v2.sd-bright-devel-cadence-member_0000000014\",\nName: \"Service 'cadence' check\",\nStatus: \"critical\",\nNotes: \"\",\nOutput: \"dial tcp [::1]:31308: getsockopt: connection refused\",\nServiceID: \"v2.sd-bright-devel-cadence-member_0000000014\",\nServiceName: \"cadence\",\nCreateIndex: 4478149,\nModifyIndex: 4478418\n}\n]\n}\n]\nCatalog Endpoint (current source of truth)\nhttp://consul01dv2:8500/v1/catalog/service/cadence?dc=zerozero&tag=devel\njavascript\n[\n{\nNode: \"mesos-slave01of2\",\nAddress: \"10.150.150.227\",\nServiceID: \"v2.sd-bright-devel-cadence-member_0000000015\",\nServiceName: \"cadence\",\nServiceTags: [\n\"devel\",\n\"shard:0\",\n\"user:bright\"\n],\nServiceAddress: \"\",\nServicePort: 31204,\nServiceEnableTagOverride: false,\nCreateIndex: 4478435,\nModifyIndex: 4478446\n},\n{\nNode: \"mesos-slave01of2\",\nAddress: \"10.150.150.227\",\nServiceID: \"v2.sd-bright-devel-cadence-member_0000000016\",\nServiceName: \"cadence\",\nServiceTags: [\n\"devel\",\n\"shard:0\",\n\"user:bright\"\n],\nServiceAddress: \"\",\nServicePort: 31204,\nServiceEnableTagOverride: false,\nCreateIndex: 4482833,\nModifyIndex: 4482864\n},\n{\nNode: \"mesos-slave05of2\",\nAddress: \"10.150.150.231\",\nServiceID: \"v2.sd-bright-devel-cadence-member_0000000014\",\nServiceName: \"cadence\",\nServiceTags: [\n\"devel\",\n\"shard:0\",\n\"user:bright\"\n],\nServiceAddress: \"\",\nServicePort: 31308,\nServiceEnableTagOverride: false,\nCreateIndex: 4478149,\nModifyIndex: 4478418\n}\n]\n. I have a pull request for this issue open https://github.com/BuoyantIO/linkerd/pull/595.\n. As luck would have it '?passing=false' does not have the effect one would hope for.  Even so, why would anyone want down services (that are failing health checks) to be considered as valid endpoints?\n. I see your reasoning with '?passing', and in that case I think it makes sense to use the CatalogAPI.  How do you feel about adding a paramenter useHeathCheck: Option[Boolean] = Some(False) to case class ConsulConfig so I can just switch out CatalogApi for HealthApi based on user configured preference.  I ask b/c I feel implementing an entire namer for such a small change is not worth the effort.\n. This is what I'm considering for HealthApi.\npatch\ndiff --git a/consul/src/main/scala/io/buoyant/consul/v1/HealthApi.scala b/consul/src/main/scala/io/buoyant/consul/v1/HealthApi.scala\nnew file mode 100644\nindex 0000000..2235433\n--- /dev/null\n+++ b/consul/src/main/scala/io/buoyant/consul/v1/HealthApi.scala\n@@ -0,0 +1,79 @@\n+package io.buoyant.consul.v1\n+\n+import com.twitter.conversions.time._\n+import com.twitter.finagle.http\n+import com.twitter.finagle.service.Backoff\n+import com.twitter.finagle.stats.{DefaultStatsReceiver, StatsReceiver}\n+import com.twitter.util.{Duration, Future}\n+\n+object HealthApi {\n+  def apply(c: Client): HealthApi = new HealthApi(c, s\"/$versionString\")\n+}\n+\n+class HealthApi(\n+  override val client: Client,\n+  override val uriPrefix: String,\n+  override val backoffs: Stream[Duration] = Backoff.exponentialJittered(1.milliseconds, 5.seconds),\n+  override val stats: StatsReceiver = DefaultStatsReceiver\n+) extends CatalogApi(\n+  client,\n+  uriPrefix,\n+  backoffs,\n+  stats\n+) {\n+\n+  val healthPrefix = s\"$uriPrefix/health\"\n+\n+  // https://www.consul.io/docs/agent/http/health.html#health_service\n+  override def serviceNodes(\n+    serviceName: String,\n+    datacenter: Option[String] = None,\n+    tag: Option[String] = None,\n+    blockingIndex: Option[String] = None,\n+    retry: Boolean = false\n+  ): Future[Indexed[Seq[ServiceNode]]] = {\n+    val req = mkreq(\n+      http.Method.Get,\n+      s\"$healthPrefix/service/$serviceName\",\n+      \"index\" -> blockingIndex,\n+      \"dc\" -> datacenter,\n+      \"tag\" -> tag,\n+      \"passing\" -> Some(\"true\")\n+    )\n+    executeJson[Seq[ServiceHealth]](req, retry).map(\n+      indexed => {\n+        val service = indexed.value.flatten(_.Service).head\n+        val node = indexed.value.flatten(_.Node).head\n+        val result = List(\n+          ServiceNode(\n+            node.Node,\n+            service.Address,\n+            service.ID,\n+            service.Service,\n+            service.Tags,\n+            service.Address,\n+            service.Port\n+          )\n+        )\n+        Indexed[Seq[ServiceNode]](result, indexed.index)\n+      }\n+    )\n+  }\n+}\n+\n+case class Node(\n+  Node: Option[String]\n+)\n+\n+case class Service_(\n+  ID: Option[String],\n+  Service: Option[String],\n+  Tags: Option[Seq[String]],\n+  Address: Option[String],\n+  Port: Option[Int]\n+)\n+\n+case class ServiceHealth(\n+  Node: Option[Node],\n+  Service: Option[Service_]\n+)\ndiff --git a/consul/src/test/scala/io/buoyant/consul/v1/HealthApiTest.scala b/consul/src/test/scala/io/buoyant/consul/v1/HealthApiTest.scala\nnew file mode 100644\nindex 0000000..e4c8829\n--- /dev/null\n+++ b/consul/src/test/scala/io/buoyant/consul/v1/HealthApiTest.scala\n@@ -0,0 +1,33 @@\n+package io.buoyant.consul.v1\n+\n+import com.twitter.finagle.Service\n+import com.twitter.finagle.http.{Request, Response}\n+import com.twitter.io.Buf\n+import com.twitter.util.Future\n+import io.buoyant.test.Awaits\n+import org.scalatest.FunSuite\n+\n+class HealthApiTest extends FunSuite with Awaits {\n+  val nodesBuf = Buf.Utf8(\"\"\"[{\"Node\":{\"Node\":\"Sarahs-MBP-2\",\"Address\":\"192.168.1.37\"}},{\"Service\": {\"Service\":\"hosted_web\",\"Tags\":[\"master\"],\"Port\":8084, \"Address\":\"\"}}]\"\"\")\n+  var lastUri = \"\"\n+\n+  def stubService(buf: Buf) = Service.mk[Request, Response] { req =>\n+    val rsp = Response()\n+    rsp.setContentTypeJson()\n+    rsp.content = buf\n+    rsp.headerMap.set(\"X-Consul-Index\", \"4\")\n+    lastUri = req.uri\n+    Future.value(rsp)\n+  }\n+\n+  test(\"serviceNodes endpoint returns a seq of ServiceNodes\") {\n+    val service = stubService(nodesBuf)\n+\n+    val response = await(HealthApi(service).serviceNodes(\"hosted_web\")).value\n+    assert(response.size == 1)\n+    assert(response.head.ServiceName == Some(\"hosted_web\"))\n+    assert(response.head.Node == Some(\"Sarahs-MBP-2\"))\n+    assert(response.head.ServiceAddress == Some(\"\"))\n+    assert(response.head.ServicePort == Some(8084))\n+  }\n+}\n. @adleong I think this meets the requirements that we discussed.  Please advise if i should add any other members to trait ConsulApi extends BaseApi.  Also sorry git chose to treat the rename as delete + add.\n. @adleong @Ashald I've uploaded linkerd-0.7.3-SNAPSHOT-exe to dropbox if you'd like to verify the same binary sha256 below.  I'm deploying this version to my test environment.\nbash\nshasum -a 256 /Users/jvenus/Projects/linkerd/linkerd/target/scala-2.11/linkerd-0.7.3-SNAPSHOT-exec\n4a5cffe054b3e988bbb173c356ca13d2915d3d1334d6a96456e054ffa0eec5ca  /Users/jvenus/Projects/linkerd/linkerd/target/scala-2.11/linkerd-0.7.3-SNAPSHOT-exec\n. @Ashald  I do not see a way to use the consul catalog api to filter out anything for us.  From my very quick reading of the consul health api, it seems we can get the entireor the passing status of a service group.  It appears that we'd have to filter unhealthy out ourselves.\nFwiw, I am happy to accept deferring the knowledge of what is in maintenance to something else.  I personally don't feel like linkerd should care about the health of consul provided services when useHealthCheck is explicitly enabled.\n. To follow up on the maintenance checking I think it would be relatively easy to implement a case Class Checks and do something with the result when we are mapping the other case classes to ServiceNodes.\nHowever, I think a larger discussion about what to do with the result would need to be had.\nI'm mostly interested in getting failing nodes out of linkerd's consideration.  Zipkin in my environment shows me significant time spent, retrying to find working services (when consul returns unhealthy services).\n. @adleong thanks for the feedback.  I will make the changes and update this pull request.\n. @adleong next steps?\n. @Ashald awesome, thanks for trying it out.\n. @adleong I think this is ready to go.\n. @adleong documentation updates have been made.\n. @Ashald if I get to vote I'd prefer option 1 and nice find on the leader election behavior ... I was beginning to think I was seeing a consul bug.\n. I missed the experimental kv :(\n. Ok, sorry for the spam experimental has no effect.\nsh\nOct 14, 2016 3:19:15 PM com.twitter.finagle.http.HttpMuxer$$anonfun$4 apply\nINFO: HttpMuxer[/admin/metrics.json] = com.twitter.finagle.stats.MetricsExporter(<function1>)\nOct 14, 2016 3:19:15 PM com.twitter.finagle.http.HttpMuxer$$anonfun$4 apply\nINFO: HttpMuxer[/admin/per_host_metrics.json] = com.twitter.finagle.stats.HostMetricsExporter(<function1>)\ncom.fasterxml.jackson.databind.JsonMappingException: Could not resolve type id 'io.l5d.zkLeader' into a subtype of [simple type, class io.buoyant.namer.NamerConfig]: known type ids = [NamerConfig, io.l5d.consul, io.l5d.fs, io.l5d.k8s, io.l5d.marathon, io.l5d.serversets]\n at [Source: java.io.StringReader@1e636ea3; line: 38, column: 9] (through reference chain: io.buoyant.namerd.NamerdConfig[\"namers\"]->com.fasterxml.jackson.module.scala.deser.BuilderWrapper[0])\n    at com.fasterxml.jackson.databind.JsonMappingException.from(JsonMappingException.java:148)\n    at com.fasterxml.jackson.databind.DeserializationContext.unknownTypeException(DeserializationContext.java:967)\n    at com.fasterxml.jackson.databind.jsontype.impl.TypeDeserializerBase._handleUnknownTypeId(TypeDeserializerBase.java:277)\n    at com.fasterxml.jackson.databind.jsontype.impl.TypeDeserializerBase._findDeserializer(TypeDeserializerBase.java:159)\n    at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:108)\n    at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:93)\n    at com.fasterxml.jackson.databind.deser.AbstractDeserializer.deserializeWithType(AbstractDeserializer.java:131)\n    at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:247)\n    at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:217)\n    at com.fasterxml.jackson.module.scala.deser.SeqDeserializer.deserialize(SeqDeserializerModule.scala:76)\n    at com.fasterxml.jackson.module.scala.deser.SeqDeserializer.deserialize(SeqDeserializerModule.scala:59)\n    at com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:520)\n    at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeWithErrorWrapping(BeanDeserializer.java:463)\n    at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:378)\n    at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1099)\n    at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:296)\n    at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:133)\n    at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3736)\n    at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2764)\n    at com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper$class.readValue(ScalaObjectMapper.scala:184)\n    at io.buoyant.config.Parser$$anon$1.readValue(Parser.scala:71)\n    at io.buoyant.namerd.NamerdConfig$.loadNamerd(NamerdConfig.scala:95)\n    at io.buoyant.namerd.NamerdConfig$.loadNamerd(NamerdConfig.scala:99)\n    at io.buoyant.namerd.Main$.loadNamerd(Main.scala:49)\n    at io.buoyant.namerd.Main$.main(Main.scala:16)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at com.twitter.app.App$$anonfun$nonExitingMain$3.apply(App.scala:176)\n    at com.twitter.app.App$$anonfun$nonExitingMain$3.apply(App.scala:175)\n    at scala.Option.foreach(Option.scala:257)\n    at com.twitter.app.App$class.nonExitingMain(App.scala:175)\n    at io.buoyant.namerd.Main$.nonExitingMain(Main.scala:9)\n    at com.twitter.app.App$class.main(App.scala:141)\n    at io.buoyant.namerd.Main$.main(Main.scala:9)\n    at io.buoyant.namerd.Main.main(Main.scala)\nException thrown in main on startup\n. I may have a fix ... i'm compiling namerd atm\n. PR https://github.com/BuoyantIO/linkerd/pull/755 should address this issue.\n. Fixed.\n. Imo the case of \"0.0.0.0\" should be easy to announce by enumerating all the interfaces NetworkInterface.getNetworkInterfaces and publish all the addresses (except 127.0.0.1) as opposed to \"0.0.0.0\".\n. @adleong I'll take a stab at implementing this and put it behind a default false announcer key.\n. I've made some progress, but I have to figure out how to dry up my changes.\nsh\nI 1018 21:08:04.100 THREAD1: SessionState. Session 257a6d82ef90782. State com.twitter.finagle.serverset2.client.SessionState$SyncConnected$@764cba\nI 1018 21:08:04.344 THREAD1: serving http admin on /0.0.0.0:9990\nI 1018 21:08:04.354 THREAD1: serving http on /0.0.0.0:4140\nI 1018 21:08:04.371 THREAD1: announcing /10.111.254.195:4140 as /#/io.l5d.serversets/prod/linkerd to zk-serversets\nI 1018 21:08:04.455 THREAD1: announcing /10.111.255.50:4140 as /#/io.l5d.serversets/prod/linkerd to zk-serversets\nI 1018 21:08:04.460 THREAD1: initialized\nI 1018 21:08:04.514 THREAD20: Set group member ID to member_0000000012\nI 1018 21:08:04.547 THREAD20: Set group member ID to member_0000000013\n. Fixes https://github.com/BuoyantIO/linkerd/issues/747\n. @adleong Thanks for the feedback ... I didn't know I could flatten multiple sequence comprehensions into one \ud83d\ude04 \n. @olix0r It's possible there needs to be some wall time (a little more than 5 minutes) between the ending of step 4 and starting of step 5.. @jacobrichard and I spent a little time coming up w/ an automated recovery mechanism.  The following conditions are currently only recoverable w/ a linkerd restart.  Again the entire ensemble has to be lost to get linkerd into this state.  That said it may just be easiest to crash and let some other mechanism recover linkerd when it occurs.\nThis script ran as a systemd-unit will recover/fail/restart linkerd ran as a systemd-unit.\n```python\n!/usr/bin/python3\nimport select\nimport dbus\nfrom systemd import journal\nimport sys\nsysbus = dbus.SystemBus()\nsystemd1 = sysbus.get_object('org.freedesktop.systemd1', '/org/freedesktop/systemd1')\nmanager = dbus.Interface(systemd1, 'org.freedesktop.systemd1.Manager')\nif name == 'main':\n    j = journal.Reader()\n    j.log_level(journal.LOG_INFO)\n    j.this_boot()\n    j.this_machine()\nj.add_match(\n    _SYSTEMD_UNIT=u'linkerd.service',\n)\n\nj.seek_tail()\nj.get_previous()\np = select.poll()\n\njournal_fd = j.fileno()\npoll_event_mask = j.get_events()\np.register(journal_fd, poll_event_mask)\n\nwhile True:\n    if p.poll(250):\n        if j.process() == journal.APPEND:\n            for entry in j:\n                if ('ZkResolver reports unhealthy.' in entry['MESSAGE'] or\n                    'A HostProvider may not be empty!' in entry['MESSAGE']):\n                    manager.RestartUnit('linkerd.service', 'fail')\n\n```. @adleong This issue seems like a duplicate of #1178.. @adleong seems to fix the issue for me.. I just this hit this bug.. Apologies ... I've been swamped lately.  I'll try to update the doc soon.. @adleong the announcer feature is pretty useful for the group I work with.  We run our linkerd's on the host OS and announce via serversets so we can discover them for prometheus metrics.  We also been kicking around the idea of using the announcer interface (with a plugin) to register linkerd's with Amazon application load balancers.\nquote\nOn a somewhat unrelated note, I've been wondering if people actually find announcers useful and I'd love to chat with you about them if you use them.. +1. @adleong I can confirm that I am able to retrieve this artifact.. @adleong lgtm, thanks for the fix.. @yeyincai I think you are looking for delegation tables.  Which would apply to this section of a static namer config. . I have an unexpected response exception when I run locally.  I'm pretty sure it's the same reason you've pointed out.  I'll send a follow up when I get a chance to dig in.\n. @adleong this now works as I expect.  @Ashald would you like to confirm if this change works as you'd expect?  Just add useHealthCheck to your consul namer once you've built my branch.\nyaml\nnamers:\n- kind: io.l5d.consul\n  experimental: true\n  includeTag: true\n  useHealthCheck: true\n. @adleong Yeah that does read much clearer.  However I'm unable to make it work without first flattening the members Node and Service of\ncase class ServiceHealth(\n  Node: Option[Node],\n  Service: Option[Service_]\n)\nIt seems the result of executeJson[Seq[ServiceHealth]](req, retry) is like this.\n[ServiceHeath(Some(Node), Some(None)), ServiceHealth(Some(None), Some(Service)), ...]\nI'm not sure why this happens, I'll attach a json response at the end [1].  The list always has an even number of elements where element (0, 1), (2,3), (3,4), etc ... always belong together information wise.  This seems to be the only way I can get the expected behavior.\nexecuteJson[Seq[ServiceHealth]](req, retry).map(\n      indexed => {\n        val result = (indexed.value.flatten(_.Service), indexed.value.flatten(_.Node))\n          .zipped.map { (service, node) =>\n            ServiceNode(\n              node.Node,\n              node.Address,\n              service.ID,\n              service.Service,\n              service.Tags,\n              service.Address,\n              service.Port\n            )\n          }\n        }\n        Indexed[Seq[ServiceNode]](result, indexed.index)\n      }\n    )\n  }\nI suspect I may be doing something wrong, but I can't see it.\n1)\njavascript\n[\n  {\n    \"Node\": {\n      \"Node\": \"mesos-slave01of2\",\n      \"Address\": \"10.150.150.227\",\n      \"TaggedAddresses\": null,\n      \"CreateIndex\": 4478199,\n      \"ModifyIndex\": 4674987\n    },\n    \"Service\": {\n      \"ID\": \"v2.sd-bright-devel-cadence-member_0000000015\",\n      \"Service\": \"cadence\",\n      \"Tags\": [\n        \"devel\",\n        \"shard:0\",\n        \"user:bright\"\n      ],\n      \"Address\": \"\",\n      \"Port\": 31204,\n      \"EnableTagOverride\": false,\n      \"CreateIndex\": 4478435,\n      \"ModifyIndex\": 4478446\n    },\n    \"Checks\": [\n      {\n        \"Node\": \"mesos-slave01of2\",\n        \"CheckID\": \"serfHealth\",\n        \"Name\": \"Serf Health Status\",\n        \"Status\": \"passing\",\n        \"Notes\": \"\",\n        \"Output\": \"Agent alive and reachable\",\n        \"ServiceID\": \"\",\n        \"ServiceName\": \"\",\n        \"CreateIndex\": 4478199,\n        \"ModifyIndex\": 4478199\n      }\n    ]\n  }\n]\n. @adleong This is what I tried.\nscala\n    executeJson[Seq[ServiceHealth]](req, retry).map { indexed =>\n      val result = indexed.value.map { health =>\n        val service = health.Service\n        val node = health.Node\n        ServiceNode(\n          node.flatMap(_.Node),\n          node.flatMap(_.Address),\n          service.flatMap(_.ID),\n          service.flatMap(_.Service),\n          service.flatMap(_.Tags),\n          service.flatMap(_.Address),\n          service.flatMap(_.Port)\n        )\n      }\n      Indexed[Seq[ServiceNode]](result, indexed.index)\n    }\n  }\n}\nThis is the failed unit.\n[info] HealthApiTest:\n[info] - serviceNodes endpoint returns a seq of ServiceNodes *** FAILED ***\n[info]   List(ServiceNode(Some(Sarahs-MBP-2),Some(192.168.1.37),None,None,None,None,None), ServiceNode(None,None,None,Some(hosted_web),Some(List(master)),Some(),Some(8084))) had size 2 instead of expected size 1 (HealthApiTest.scala:27)\nSep 01, 2016 3:24:50 PM io.buoyant.consul.v1.BaseApi$$anonfun$1 applyOrElse\n. @adleong good eyes, I couldn't for the life of me see that.  (even w/ jq).  I'll post a follow up.\n. fixed\n. fixed\n. fixed\n. yes .. I can look at focusing on just the imports needed as opposed to blinding importing everything.\nNetworkInterface.getNetworkInterfaces()  returns an Enumeration and interface.getInetAddresses() returns an Enumeration.  I get a compiler error w/o this import when attempting to loop over either Enumeration.\n. sure enough.\n. Thank you for the feedback/suggestions.  I'll work on a more general solution.. Correct.. I see what you are saying ... this makes a lot of sense to me.  Thanks for the feedback.. ",
    "topiaruss": "Done.\nNP.\n\u2014r\n\nOn 2 May 2016, at 21:02, Alex Leong notifications@github.com wrote:\nWe'll actually need you to sign the CLA, @topiaruss https://github.com/topiaruss, even for doc fixes. https://buoyant.io/cla/ https://buoyant.io/cla/\nSorry for the trouble and thanks for the fix!\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub https://github.com/BuoyantIO/linkerd/pull/359#issuecomment-216346143\n\nRuss Ferriday -- Software Product Architect, Developer, Mentor\nFounder & CTO Topia Systems Ltd.\nrussf@topia.com  --  +44 7429 518822\n. ",
    "drewrobb": "@olix0r would a reasonable solution be to make a class that implements namer for the twitter commons's leader election class? Then rather than modifying the marathon namer configuration options directly, one could just make a http router for marathon and then use that? something like:\n```\nnamers:\n- kind:      io.l5d.experimental.marathon\n  prefix:    /marathon-test\n  host: 127.0.0.1\n  port: 11111\nrouters:\n- protocol: http\n  servers:\n  - port: 11111\n    ip: 0.0.0.0\n  interpreter:\n    kind: io.l5d.namerd\n    dst: /$/some.new.class.extending.namer/zkhost:2181/marathon/leader\n```\n. Thanks @adleong !\n. done. ",
    "erdody": "Is this something that can be merged or is it still blocked on something else?\n. Thanks @adleong. I saw some movement recently, does that mean you've decided to go ahead with merging this or are you still discussing? \nAlso, what do you think about adding and optional address to AnnouncerName, to have the ability to override the one coming from the Server? The use case is: we need the server to bind to all interfaces (0.0.0.0) to cover container IP and localhost but that, of course, won't work for announcement.\n. @adleong, we are planning to run linkerd as an in-container side-car (initially), linkerd-to-linkerd, and the IP is passed as an environment variable. We have a container initialization script that runs confd and renders the linkerd yaml file with ip, service name, etc.\nWe use aurora and per-container IPs (IPs belong to the container). After I sent the message I realized that the outbound server is what needs to be bound to localhost. The inbound server can work with the container IP only (which is the one we want to announce), nobody needs to connect to it internally. Sorry about the confusion.\nAre you planning to merge this soon?\n. @adleong Yes, it would work as-is, we would use the container IP for the inbound server which is what the announcer would advertise.\nThanks for the PR timeline, looking forward to it.\n. Thanks @olix0r for the context.\nCan you elaborate on what exactly is unreliable? Is it the triggering of the hook or the execution?\nMy main goal is to have a way to cleanly shutdown on a SIGTERM signal. Of course, SIGKILL won't be captured and there's not much you can or want to do about it. \nSpecific example: I want to use linkerd as an in-container sidecar. On shutdown, init frameworks (s6-overlay / phusion) will kill all side process and wait for a graceful termination. I can write a specific shutdown script that calls /admin/shutdown and then waits for the process to die, but I\n'd rather move that logic to the process. \n. Thanks @olix0r for the detailed reply.\nI noticed the logging problems, which is what I mention in my original comment. After further investigation I could find the root cause: java.util.logging.LogManager (which is what twitter logging uses) adds a shutdown hook to close handlers (reported in many places, e.g. this bug).\nSince all shutdown hooks are run in parallel, there's a race condition and other hooks log lines might be ignored. The closing code is otherwise executed.\nThere are many workarounds to the logging problem, but I agree, even if we solve this specific issue there could be other misbehaving libraries (e.g. log4j2 seems to have a similar problem and I saw com.twitter.finagle.zipkin.core.TracerCache also using shutdown hooks, which might not be a major issue).\nOne thing I just successfully tried is using sun.misc.Signal.handle, which is run before all shutdown hooks. \nSomething like:\nscala\n  def main() {\n    Signal.handle(new Signal(\"INT\"), shutdownHandler)\n    Signal.handle(new Signal(\"TERM\"), shutdownHandler)\nscala\n  private def shutdownHandler(): SignalHandler = new SignalHandler() {\n    def handle(sig: Signal): Unit = {\n      log.info(\"Closing all ...\")\n      Await.result(close(Duration.fromSeconds(graceFlag())))\n      log.info(\"All closed.\")\n    }\n  }\nOf course it has the downside of not being officially supported which means it can go away (unlikely) and it might not work in all OSes. \nI still think it's better than just dying when the OS kindly asks the process to gracefully terminate :)\nWhat do you think about this option?\n. Good, I just changed to SignalHandler and moved parameter to admin section in the config file.\n. Thanks, just pushed the updates.\n. @olix0r CLA signed, finally. I just synced with master.\n. @olix0r sorry, this was meant to be merged to our fork's branch. \nHaving a plugin system would be great. \nThanks!\n. If I don't add that, sbt linkerd/assembly complains about:\n[error] /Users/diego/Repo/linkerd-medallia/linkerd/main/src/main/scala/io/buoyant/Linkerd.scala:104: discarded non-Unit value\n[error]     Signal.handle(new Signal(\"TERM\"), shutdownHandler)\n[error]                  ^\n[error] one error found\nOne alternative is to add a val _ = ...\n. Sure, I was trying to emulate what you do with port. Any reason why you want to it differently in this case?\n. Fixed.\n. Added to the Main class\n. ",
    "tonyd3": "signed!\n. signed!\n. I think it works slightly differently than I expected.\nIt *looks like tony/devel/test/health gets resolved to tony/devel/test and the residual path (health) isn't getting picked up.\n. This is a big weakness in Docker(and I think the same in most container) volume mounting is that you can only mount a directory, not specific files. \nI do like getting rid of the -w and using absolute path, and it's what I did when I tried to run the example. I'll make that change.\n. ",
    "clhodapp": "I want to express that I think this would be tremendously helpful in making the request routing design that I'm working on more clean.\n. Sure, but you're going to get a lot more details about what I'm doing, not just\na few.\nI'm currently working on putting together the pieces for a kubernetes-native\nmicroservice application for my employer. The application is to have two layers,\na \"frontend\" layer, which provides RESTful APIs over HTTP, and a \"backend\"\nlayer, which sits behind the frontend layer, providing RPCish APIs over pieces\nof the data model over a multiplexed protocol (Mux for now, HTTP/2 when it's\nready). The intention is to use linkerd heavily for routing within this system:\nThese services will be run in the context of Kubernetes pods fronted by\nKubernetes Services. Each pod will contain a linkerd container, deployed in what\nyour docs call the \"sidecar linker-to-linker\" model. It's my intention to\nconfigure these linkerd sidecars to connect to a namerd running the k8s\nEndpoints Namer.\nThis works great for routing requests within the cluster, which makes sense,\nas I think this closely matches the use-case Bouyant has been focusing on, but I\nthink there's another very significant potential use case for \"Finagle in a\nbox\": HTTP ingress routing. First, I'll describe the vision of how I'd like this\ndeployment model to look. Then I'll describe what I'm actually planning to do to\ncome as close to this vision as possible using present day linkerd.\nWhat I'd like to have happen is for Kubernetes Endpoints that declare an HTTP\nport and are tagged with appropriate Kubernetes Labels to be automatically\n\"mounted\" as versioned API groups on a virtual API host, which is actually\nbacked by the whole Kubernetes cluster (well a namespace therein). That is, if I\ncreate a Kubernetes Service with an HTTP port and Kubernetes Labels,\n{\"api-group\": \"users\", \"api-version\": \"v1\", \"exposed-api\": \"true\"}, I want\nthat API group to automatically become exposed at, e.g.,\napi.example.com/users/v1 on the public internet. I want a request for, e.g.,\napi.example.com/users/v1/profiles/123 to pass through my public routing and\nbalancing layer, where it will and have its path rewritten to /profiles/123\nand for it to get routed to one of my service instances.\nIn order to come as close to this as is feasible with present-day linkerd, I've\ngone ahead and set up a Kubernetes Deployment of NGINX-linkerd duo pods, which\nare exposed to the public internet via a more-basic connection-level load\nbalancer. The NGINX part of these \"public balancer\" pods strips the first two\nsegments from the path and rewrites the Host header to a hyphenated\nconcatonation. For example, a POST to the path /users/v1/profiles is\nrewritten to a POST to /profiles with a Host header of users-v1. The\nrequest is then handed off to the linkerd running on localhost to be routed to\nits final destination. That linkerd is configured to use the Endpoints Namer to\nroute the request based on a Kubernetes Endpoint with the name from the HOST\nheader. To ensure that only services which we fully intend to expose on the\npublic internet can recieve traffic via this mechanism, require that the port we\nare routing to be named public-http.\nThe reason that this issue is of interest to me is that it will allow me to stop\noverwriting the Host header and still route the request where it's supposed to\ngo. This means that the only non-additive mangling of the request that I'll have\nto do in my routing layer is to strip the first two path segments.\nNow some disconnected thoughts:\nOn my TODO list is to investigate what this Namer does when the Kubernetes\nmaster (and thus the Kubernetes API) goes down. Specifically, I am wondering\nwhether the Endpoints Namer can continue to resolve names against cached data\nwhile the Kubernetes master is down and whether it will correctly reconnect to\nthe API and resume consuming it when it comes back up. It would be great for the\nintended k8s Endpoint Namer behavior to be described here and for the actual\nbehavior to go into the docs if possible, as it seems pretty important for users\nof pretty much any Kubernetes API based infrastructure component to understand\nwhat happens to that component when the API goes down.\nIt would be great if the Endpoints Namer could be more dynamic. Specifically, it\nwould be wonderful if it had support for Kubernetes Labels and Label-based\nfiltering. Suppose I could define a namer config like so:\n- kind: io.l5d.k8s\n    experimental: true\n    host: localhost\n    port: 8001\n    prefix: k8s\n    namespaceFilters: [ ]\n    endpointFilters:\n      - kind: label\n        name: exposed-api\n        value: true\n    portFilters:\n      - kind: name\n        value: http\n    pathSegments:\n      namespaceFilters:\n        - kind: name\n      endpointFilters:\n        - kind: label\n          name: api-group\n        - kind: label\n          name: api-version\n      portFilters: [ ]\nThis looks complex at first but it's also super powerful and I don't think\nit'd be all that hard to implement. What it says is to recognize paths that look\nlike:\n/#/k8s/<namespace_name>/<endpoint_api-group_label_value>/<endpoint_api-version_value>\nThat is, the path /#/k8s/ns/users/v1 would be balanced across all the pods in\nall the services in the namespace ns with the Label values: exposed-api=true,\napi-group=profiles, api-version=v1. I think in NameTree-land, the really\ncorrect form of this is just a four-level Union (do note that single-child\nUnions get optimized away): one to allow balancing across all criteria-matching\nnamespaces, another for balancing across matching Endpoints, another for\nbalancing across Pods within these Endpoints, and a final level for balancing\nacross exposed ports on those pods. It would be quite feasible for the Namer to\nread Annotations on these objects to get weights for balancing as well. It\nshould also not be all that difficult to maintain the data structure to be able\nto efficiently perform lookups, given that the potential query structure is\nstatically defined in the configuration. I actually have some ideas about the\nparticular way that this could be implemented if it's of any use, but I don't\nthink they are particularly novel, so I'll leave off on laying them out for now.\nIf this (admittedly-at-least-medium-sized) improvement plus the one mentioned in\nthis issue were made, linkerd would TRULY excel at routing within a Kubernetes\ncluster, both as an RPC proxy and as an http ingress router (when coupled with\nan NGINX helper).\n. ",
    "ccmtaylor": "this is fixed in #1634. I think this issue can be closed now.. @hawkw thaks for the quick review! I've addressed your comments; let me know if things look alright.\n\nwould you mind making sure you've signed our Contributor License Agreement so we can merge this?\n\nSoundCloud recently signed a CLA. I used my SC email in the commits, but if you prefer, I can close this PR and re-submit from a fork under github.com/soundcloud.. @hawkw @adleong: what kind of instrumentation would you expect on namers? I'm passing in a StatsReceiver, but not using it atm. I couldn't see anything consistent across the other namers, but I'm not sure if there exist some metrics across all kinds of namers (e.g. lookup latency, success/failure counts)?. lifting this to a top-level comment, so it doesn't get lost in revisions:\n\n@olix0r: I imagine that resolver.send is a potentially blocking call? If so, then I think DnsSrvNamer should probably be constructed with a com.twitter.util.FuturePool so that these calls don't block the timer thread.\n\nvery good point, thanks @olix0r! I tried (and failed) to do this today. It's straight forward to make lookupSrv async with a FuturePool by doing\nscala\nfuturePool(resolver.send(query)) flatMap { message =>\n  //...\n  Future.value(NameTree.whatever)\n  //...\n  Future.exception(e)\n}\nbut I can't figure out how to fit Futures into the Activity/Var/Timer APIs :(. Here's what I tried:\n\nI found Activity.future(Future[T]), but that seems to run a future once, and then stop.\nI tried to use val (act, witness) = Activity[NameTree[Name]() like this:\n  scala\n  // with lookupSrv modified to return Future[Nametree[Name]] as outlined above\n  case id@Path.Utf8(address) =>\n    val (act, witness) = Activity[NameTree[Name]]()\n    def loop(): Future[NameTree[Name]] = lookupSrv(address, prefix ++ id, path.drop(1)).respond { result =>\n      witness.notify(result)\n      timer.doLater(refreshInterval)(loop())\n    }\n    act\n  but I'm left with no place to start and evaluate the loop -- calling Await.result(loop()) would miss the point :)\n\nI'm pretty fluent in using Futures, but the Activity and Var APIs are unfamiliar to me, so I'd appreciate any hints.. @olix0r thanks for the feedback, that worked great! I found Future.whileDo, which is a nice wrapper around the loop, and I used an AtomicBoolean as shown in the twitter-util cookbook.. Thanks for your time and feedback, @hawkw, @adleong and @olix0r! I just noticed that I made an editing mistake while writing the docs. #1623  fixes it, sorry!. I can think of two improvements:\n1) add an optional search domain setting to the plugin (default to .), and resolve relative records relative to that. This would have the advantage of of removing some duplication, e.g. if my org serves SRV records under srv.dc-1.example.com, we could use something like (cf. the example from #1611):\n```yaml\nrouters:\n- protocol: http\n  dtab: |\n    /dnssrv => /#/io.l5d.dnssrv;\n    /svc => /dnssrv;\n    /svc/myservice =>\n               /dnssrv/myservice | // relative to search domain\n               /dnssrv/myservice.srv.dc-2.example.org.; // absolute records also work\nnamers:\n- kind: io.l5d.dnssrv\n  experimental: true\n  domain: srv.dc-1.example.org.\n```\n2) resolve relative names according to the host's DNS settings (e.g. /etc/resolv.conf on linux). The advantage here would be that it's \"least surprise\" from a DNS/networking point of view. OTOH, hosts often have multiple search domains, so this might involve multiple DNS queries. In addition, the system resolver is OS-specific (e.g. macOS provides a resolv.conf for compat, but doesn't use it internally; and windows does something completely different), and I'd like to avoid re-implementing it.. I've thought about this some more; I quite like option (1) from my comment above. I'll push it as a separate commit in this PR, let me know what you think (or if I should submit it as a separate PR).. sorry for the late response.\n\nAre you interested in working on this feature?\n\nI could spend some time on this tomorrow if you're interested. It looks like it might be a chunk of work, so I wanted to ask before I go ahead. I haven't looked into how many buckets finagle keeps internally for each stat, so this might cause the count of timeseries to increase quite a lot.\nDo you think I should replace the summaries by histograms? This will break dashboards that expose the summaries (though those dashboards probably aren't showing what people think they are, unless there's only one linkerd instance).. lgtm, :shipit: (Modulo @hawkw's comment). you can list ccmtaylor+linkerd@gmail.com as my email address. > Very interesting. @ccmtaylor any ideas? Have you seen anything like this before?\nno, we haven't seen this behaviour. Our setup is a bit different though; we don't run namerd and configure linkerd to use the dnssrv directly.\n@leozc, in your debugger screenshot, is the cache you highlighted part of the dnsjava library? NXRRSET is a very unexpected response from the DNS server: afaict, it's related to dyndns updates, so I don't know how that should occur. The value of the expire field looks like a timestamp. This is just a guess, but if dnsjava is caching bad responses for a long time (could be verified by comparing that timestamp to \"now\"), that may explain the behaviour we're seing here.\n@leozc another question: are the \"SRV lookup failure: no results for ...\" log messages repeated every refresh interval, or do you see the message once, and then namerd is stuck forever?. @leozc great work, thanks! Would you like to submit a PR with the fix?. thanks for the contribution, @leozc! :tada: . I've seen other namers hand around a residual path, but I'm not sure how to use it, or why they would.. ok. I actually wrote it that way first, but thought it looked a bit nicer like this because it's one fewer level of indentation (though I guess Utf8.unapply() might need to do more work.. a major linkerd use-case for us is to load-balance across two or more deployments of a service (with different SRV records), i.e. the dtab looks like this:\n/svc/myservice =>\n               /dnssrv/myservice1.srv.example.org &\n               /dnssrv/myservice2.srv.example.org;\nif myservice1 resolves to Addr.Bound(Set.empty), would linkerd still call myservice2?. I agree, but I couldn't figure out how to do something like Future.await() for an Activity. The closest I could find is Activity.sample, but I'm not sure if that would race if the lookup is too slow. I'll push something, but please take a look if that's correct.. resolver.send(query) throws IOException. I'll scope the Try{} to that call.. ok. I'm a little unclear on the behaviour of the different NameTree.* variants. When would I use NameTree.Fail?. as mentioned in a previous comment, I'd appreciate a +1 that this should work and doesn't just race on the Activity being in Pending state.. should I make this required? If so, where?. sounds like NameTree.Neg is the right choice then :). In general, why would linkerd not fallback/load-balance for empty sets? I see this has come up in #1612 and #1549, too.. yes, that is correct. maybe replace SDS with SRV for consistency? I've only run across the term SDS in Envoy docs, but I don't think it applies here.. I don't think you need to provide DefaultTimer here; there should be an implicit timer in scope for the whole class -- see line 20. related: if you use the timer explicitly, maybe pass it as a regular parameter?. if you don't use the return value, maybe give the method a type of doUnit(): Unit?. I would keep the existing order here to ensure that we only schedule subsequent runs once the first one has completed.. I would keep using ExtendedResolver here, since according to the JavaDocs, it supports multiple servers if the default configuration (e.g. /etc/resolv.conf) specifies them. If you want to ensure only one server is queried, you can pass a one-element array of hosts instead.. agreed with @adleong. I'd log the message and keep a counter without the per-DNS scope. If you want to keep the lookup.getErrorString scope, please make sure that it is bounded and doesn't include e.g. the DNS name or other dynamic data in the string. Ditto for dnsSuccess below :). this creates a new resolver in every iteration. Probably ok, since this isn't a hot loop (it's scheduled to run once every couple of seconds, per DNS address), but it's some file I/O nonetheless. Might be worth investigating if we can use one resolver per address instead (e.g. create it outside doUnit). Ah, we ran into this issue too. It should be fixed in the next release: https://github.com/twitter/util/issues/239. Are these imports used? The issue I linked should prevent them from working.... ",
    "joerg84": "I guess it is sufficient to make the links relative to the current directory or?. @siggy your current implemention sets the labels for each linkerd task (See here mesosphere/universe/blob/version-3.x/repo/packages/L/linkerd/8/marathon.json.mustache#L42). Hence admin router will redirect to one of those tasks running (Not really loadbalanced). So it might serve from different instances. See also the my  last sentence on the original issue https://github.com/mesosphere/universe/issues/911. FYI people can already browse there but they will see a broken site. . ",
    "andersschuller": "@olix0r Do you have any thoughts on how you would like this (and to some extent the other telemeter plugins) to be implemented? Would you be happy to depend on an external library (for example something like the java-statsd-client), or would you prefer to have it implemented from scratch? I suppose either way it should become a separate telemetry-statsd project, like the existing telemetry-tracelog?\n. @Ashald I was considering working on this a bit over the next few days. I just thought I would first confirm what I've been thinking so far. Since I'm quite new to the project, I have pretty much just been getting familiar with how the project is set up and how the metrics work. I haven't done any actual coding yet, so I would be just as happy to pick up one of the other related issues.\n. ",
    "masonoise": "Yes, we're having pain because of the lack of statsd telemetry as well. We need to feed the metrics into InfluxDB using telegraf, and without statsd format it's very difficult to make proper use of the data.. @wmorgan I'm not a telegraf/Prometheus expert at all either, but what I know is that our metrics pipeline is set up to ingest statsd-format metrics. I took the telegraf plugin for Prometheus data and modified it to convert the Prometheus-style metrics coming out of Linkerd into statsd-format, and also added a prefix (\"linkerd.\") to help filter. That's got metrics flowing into our system and we hoped it would do the trick but it's not quite good enough. The telegraf statsd plugin supports template specifications which allows metrics to get filtered into the appropriate places before getting sent down the pipe, which is the key to organizing the data coming in. That's not possible using the Prometheus plugin unfortunately. So we really do need the statsd metrics coming out of linkerd. The only other option would be for me to take the telegraf statsd plugin and hack it to read the JSON or Prometheus metrics out of linkerd, which is clearly not the right way to go.... @wmorgan Modifying the Prometheus plugin is what I did, but it's not so easy. We have metrics going in now because of my modification, but it's not sufficient because we can't properly filter the metrics in Grafana this way. The statsd plugin supports templating, which is what we need. As-is we can't filter sufficiently to build useful dashboards. And adding the templating support would be significant work from what I can see, and at that point it's basically rebuilding the statsd plugin which doesn't seem worthwhile.. Thanks for the thought, @bashofmann -- I'm not sure that would do it for us because we're storing metrics in Influxdb for Grafana dashboards so the statsd metrics format (and the telegraf templating) is essential for us to get proper dashboarding and alerting. I'll look into it in case it might do the trick, though.. This is great news, thanks. We'll be happy to test it out when it's ready to go. We use telegraf so a key for us is the templating in order to feed to Grafana for dashboards. Being able to filter on labels such as service is probably obvious, but it's what we're looking forward to: having dashboards that can show latency/error rates broken down by service and by host will be fantastic.. Great, I didn't think so -- I'm testing it out with 0.6.0 and having some trouble but I can tell the header is being processed. Some examples in the documentation of using it would definitely not be a bad thing as well. Thx.\n. We're looking forward to this. I wanted to suggest that if it's not much more difficult, rather than supporting routing specifically on one field, given that the TTwitter envelope can contain arbitrary key-value pairs it would be great if it's possible to route on any field. If that adds significant complexity then it's maybe a future enhancement, but I thought I'd mention it. Thx.. We have a custom framework that's used by our clients -- I've asked someone else to chime in with any relevant details that they can provide. I'm not the expert on that part! Thanks.. ",
    "olgert": "Great news! We are also using telegraf, and it will be amazing, if linkerd could push stats in \"Influx Statsd\" protocol as an option, which is statsd enhanced with tags. However, transformation templates will also work for us.. @robbfoster The problem is that : is reserved by statsd protocol (it separates metric name from its value) and can't be used in metric name. xref: https://github.com/BuoyantIO/linkerd/pull/898#discussion_r95265487. Thanks for taking care of this! IMO, this is pretty good as a first implementation. As a next step we could add tagging support and create 3 different statsd_backend implementations for each statsd dialect:\n1. etsty (default)\n2. influxdb\n3. datadog\nHow does it sound for you guys?. Thanks everyone for the effort!. @siggy DataDog StatsD and InfluxDB/Telegraf StatsD are not compatible, and I'd better treat them as a separate dialects. I also like the idea @leozc suggested: \n\nAdding new option statsd_backend = [default|datadog|influxDB]\n\nwhere \"default\" is etsy's original statsd dialect, e.g. encode tags into measurement name.. Thank you for fixing this!. Similar behavior with gRPC examples for Python (grpcio).. @hawkw Changing log message looks good to me. Thanks!. I agree with @leozc, this shouldn't be included in metric name by default, as it makes initial kick-off more complex: one should explicitly configure statsd/telegraf to skip prefix to be able to aggregate stats from multiple instances, which is (at least to us) the main goal of using statsd for linkerd monitoring. To distinguish between instances it is better to use tags. Another thing to consider: it is not recommended to have tags with unlimited number of possible values or encode this data into measurement name, known as hitting series cardinality limit. This will eventually put InfluxDB instance into extremely slow and memory consuming state. In other words, instance id should be if not disabled at all, then at least not included in measurement name/tags by default.. mkName should also escape characters that are invalid for statsd metric name, such as :, otherwise it will result and error on statsd/telegraf side.. ",
    "robb-foster-ck": "Yes, thanks very much for doing this.  It seems to be mostly working well.  I've got one little problem where I am getting errors in telegraf along the lines of (these are just a sample)\n\n2017/01/10 12:01:05 E! Error: splitting '|', Unable to parse metric: linkerd.rt.int-http.dst.path.http.1_1.GET.10_0_2_15:4140.pending:0|g\n2017/01/10 12:01:05 E! Error: splitting '|', Unable to parse metric: linkerd.rt.int-http.dst.id.$.inet.10_0_2_15.8888.path.http.1_1.GET.10_0_2_15:4140.pending:0|g\n\nThe theory is that the parser doesn't like the :4140 and the :0 in the same metric name, but that is only a guess.. And all my errors have gone away.  Thanks for the quick turnaround.. Sorry, but I can't in any practical way.  The version of the OS that we are running is the only one that's approved for use in production and development.. ",
    "obeattie": "Done \ud83d\udd8b \ud83d\ude04 \n. Sure, I'm more than happy to add the tests to this PR in a few days :)\nOn Wed, 15 Jun 2016 at 8:07 pm, Alex Leong notifications@github.com wrote:\n\nThanks for the fix, @obeattie https://github.com/obeattie! I think it's\na good idea to test this corner case. Would you like to add tests to this\nPR in a few days when you have time? Otherwise, one of us can take it over\nand add the tests.\nThanks again!\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/BuoyantIO/linkerd/pull/473#issuecomment-226288662,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AAAXZJggoyDfU1FIJIN4wCFZ_w54DKOAks5qME1vgaJpZM4I0SXS\n.\n. I've added a test for this. Apologies about the amount of copy + paste, feel free to refactor the test if you like.\n. I can understand the desire not to buffer large bodies, but I think this (and any other conditions which would make a request non-retryable) should be documented very clearly. \n. Yep, I just saw the test failure. Will fix. \ud83d\udd27 \n. Yep, I agree. I've changed that behaviour, and the tests too. I do notice a lot of k8s-related pollution in the test log now though\u2026\u00a0not sure if related. \ud83e\udd14\n. Apologies, this is my fault. I am more than happy to look at this as soon as I can, but I suspect you'll be a lot quicker at it than me \ud83d\ude13 \n. Sorry to come so late to this and I'm happy to discuss this elsewhere if it's better for you. I was just looking at upgrading linkerd in our environment to 1.2.0 and saw this change. The \"k8s namers watch individual objects, rather than the entire ns\" part of this PR concerns me a little. It would be good to understand better what kind of testing has been done on it.\n\nWe have \u2248300 distinct service deployments running in k8s and we use linkerd to mediate RPC communication between all of them, running as a DaemonSet on each of our \u2248250 nodes. If I understand correctly, before this change there would have been 250 watches (one per linkerd), but with this change that will become 300 * 250 = 75,000 unique watches established against the k8s API. Granted, this is a worst case where every linkerd is processing requests bound for every service, but given that linkerd's are long-lived in our platform while services are fairly ephemeral, the number of watches does seem like it would trend toward this. As far as I can tell, once a watch is established for a destination it will never be evicted from the cache.\nI've done no testing but this seems like rather a lot of watches which is likely to cause pressure on both the k8s apiservers and linkerd itself. I understand that in the mentioned ticket, linkerd is doing a lot of work processing events and caching name trees for destinations it'll never contact, but for environments in which linkerd's are dealing with requests to a large number of destinations this change seems like it could worsen performance significantly.\nOf course, I may be missing something significant here \ud83d\ude07. Just one more thing to rule out: you definitely have probation disabled, right?\nI don't think you are configuring it explicitly from what you posted above, meaning it should take the default (false), but I just wanted to be sure.. \ud83d\udc40. I think the test failure is unrelated to this change \ud83e\udd14 . @adleong Sorry, I thought I had already responded to this. If you run the stream: empty chunk reads test that I added against the old version of the code, you will see that it fails \u2013\u00a0specifically, the stream stalls and the test hangs. The new version of the code fixes this.\nThe underlying reason for this is the old Incomplete extractor wouldn't match an exception that was thrown by Jackson: \"No content to map due to end-of-input\" because it expected such messages to begin \"Unexpected end-of-input\". This may have worked better in an older version of Jackson but I think it's likely that this always happened, it was just hard to trigger.\nWe caught this \"in the wild\" where a HTTP chunk would be terminated with a newline (something that k8s does), with the newline positioned in such a way that it would be read in a separate read operation due to the buffer size. When this case was hit the stream would never recover and no further endpoint updates would be processed.\nThe rest of the refactor is, in a sense, window dressing, but when I was tracking this down I found the old code a little hard to follow; I think the refactor simplifies things.. Thank you so much for this \ud83d\ude4c. Either a whitelist or a blacklist would be good for this. It would be ideal if there could be some mechanism so someone who does want all the functionality enabled could do something like:\nyaml\nenabledEndpoints:\n- /admin/*. Given the offset is only ever updated to the parser's last location, I don't think this is possible. Even if it were, Buf.slice's doc states that:\n\nOut of bounds indices are truncated.. \n",
    "apakulov": "@Ashald @olix0r thanks!\n. @wmorgan @olix0r we would be happy work on this implementation, if it is not a priority for you at the moment. Can you share your vision on how you would like to get it done? \nWe're also looking for the following scenario with service topology like: finagle (mux) -> linkerd-sidecar-a (mux) -> service-a (thrift, non-jvm) -> linkerd-sidecar-b(mux) -> service-b (thrift, non-jvm). And we want to have mux all the way to service-b to preserve tracing for zipkin.\nIt would be great to have linkerd to handle protocol upgrade from thrift to mux and maintain zipkin traces for the service internally.. @wmorgan @olix0r just created PR for thriftmux support, please check it out: https://github.com/linkerd/linkerd/pull/1022. @olix0r @wmorgan you can close this ticket, thanks!. @olix0r @adleong is there a chance to get your feedback on this PR? :-). @olix0r completely understandable, thanks for the update.. @adleong will add the following things soon:\n- [x] Example\n- [x] Docs\n- [x] Changes.md. @adleong ready for another review round.. @adleong are you planning to maintain your own debian repo for linkerd project?. @klingerf just signed CLA.. My bad, will bring it back.. Done.. Done.. Done.. Removed.. Using ThriftConfig as configClass. Added.. Done.. Done.. Done.. ",
    "wei-hai": "+1. ",
    "halve": "Hello Buoyant. We have a strong need for a LINE protocol telemeter. Prometheus is not in the picture yet.\nThe Grafana dashboard in linkerd-viz is very compelling, but we cannot simply dump /admin/metrics/prometheus metrics into Influx using Telegraf's Prometheus plugin. The data has to be transformed carefully so that we can use InfluxQL's math functions. By default, the plugin will create a separate measurement for each metric (think table).\nIn InfluxDB 0.9+, JOINs are not possible: https://docs.influxdata.com/influxdb/v0.9/concepts/08_vs_09/#joins - \"In InfluxDB 0.9 neither the MERGE nor JOIN operations are supported.\"\nConsequently, to pull off the slick success graphs in linkerd-viz, it seems the success and request counter must be 'fields' of the same measurement. Then we could get the best analog to the PromQL query used to power the panel in linerd-viz: irate(rt:dst_id:success{rt=\"$router\",service=~\"$service\",instance=~\"$instance\"}[1m]) / irate(rt:dst_id:requests{rt=\"$router\", service=~\"$service\",instance=~\"$instance\"}[1m])\nThis would translate to something like\nSELECT irate(success) / irate(request) FROM \"rt:dst_id\"\nNotice that this query is only references one measurement.. Your changes look good. Thanks for helping polish this PR, @adleong .\n. Should we add a disclaimer in the Curator doc blurb saying that it doesn't yet support Curator's custom payload objects?\n. @adleong On my initial CuratorNamer implementation, I handled the JSON deserialization manually. We might want to put that code back in there. We are using a Curator service instance payload, but that class is not on Linkerd's classpath. This results in\n\njava.lang.IllegalArgumentException: Invalid type id 'x.y.z.ServicePayload' (for id type 'Id.class'): no such class found\n\nNot a great experience.\n. Paging @olix0r ... Could we get this in before you rework your package structure?\n. +1 for a header modification interface. Maybe this is still too open-ended for you @olix0r ...\nWe are using linkerd in concert with nginx as an edge service. Nginx is mapping all the public paths to service urls, then l5d does its thing. We are hoping to move more responsibility to l5d like header modification. A simple plugin interface would be super slick.. Sorry for going MIA. This branch looks great. It wont affect us.. ",
    "cacoco": "@wmorgan (hey Will!) No problem.\n. ",
    "njohns-grovo": "Hello,\nSorry I took a dive into the code and I see changing the log format for everything is a bit of work, specifically i'm looking into having a linkerd instance per host and have logs shipped to logstash (which is how our current infrastructure is setup).  One issue that we have had come up with finagle based services is that an exception can generate a lot of broken log entries in logstash.\nI think it might be easier to answer \"how can i integrate linkerd's logs with logstash/syslog/{insert-log-aggregation-here}\" instead of rummaging through finagle, twitter-server, etc \ud83d\ude2c \n. Thanks for that, i'll be doing some experimentation\nThis is what i've done with logback for finagle stuff so far\nxml\n<appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\">\n        <encoder>\n            <pattern>ts=\"%date\" level=\"%level\" traceId=\"%-25X{traceId}\" ctx=\"%-25logger{0}\" msg=\"%msg\"%replace(%xException){\"\\n\", \"\\\\n\"}%nopex%n</pattern>\n        </encoder>\n</appender>\nI think this will end up in a cookbook somewhere, fitting finagle into our infrastructure has been more difficult than expected but I dont think its something linkerd needs to solve \ud83d\ude00   I'm going to close this one if I find a solution i'll just ping slack :)\n. ",
    "gudyo": "Does anyone know if any improvements have been made on this? Is it possible to get all logs to be in a single format, like JSON?. ",
    "dadjeibaah": "@gudyo we haven't done any work on this issue, mostly because of the reasoned specified in the previous comments. What is your particular use case for having log output in JSON format?. @adleong We would need to do an integration test to make sure things don't break with namerd especially in Kubernetes 1.7 and 1.8. I definitely like the idea since its a \"small\" code change in the existing functionality. Smaller than this PR. I have tested this manually on a minikube on my local machine and all CRUD operations for the DTabs seem to work in using namerctl. Next step would be to run integration tests.. Should I also add other changes that are going to be in this release as well? @siggy  . Thanks for all the useful info @krak3n. We are taking a look at this.. I tested this manually with namerctl, and etcd on docker. I will have to do it with Kubernetes to match the original environment the bug was found in. I am actually seeing an issue were if the etcd has no dtabs it gives me a 500 on namerctl. I would like that to be more informative and say that there are no dtabs available or something.. > This parameter is incorrectly present on H2ServerConfig, it should be removed from there.\nIs this in the routers.md file I edited or is this in the code? Also do I need to remove this parameter from the routers.md when adding it protocol-h2.md and protocol-http.md?. \u2b50\ufe0f LGTM. @krak3n your issue may be related but I am not sure. Could you give us more information about the environment namerd is running in? I am assuming you are using k8s 1.8 since you referred to CRDs. What version of Namerd are you using?. Thanks @krak3n, everything in your configs look fine to me. Do you think you could create a small docker setup that exhibits the issues you mentioned. It could help us pinpoint were exactly were the error is happening. Also, I think I am going to create a separate issue as I think this may not be related to k8s talking to old endpoints.. @yoitsro, it is a bummer that you are running into this. Since this issue is not easy to reproduce, we have added new functionality that inspects the state of the k8s namer. We would love to see how we could use this new feature to debug this issue if that is something you are willing to try. the k8s namer inspector has not been released yet but is available on latest master. I believe there is a nightly linkerd docker image you could use to diagnose the issue. If there is any additional information you may have that might help resolve this, please do let us know!. All great points @olix0r and @hawkw! thanks for the review. Should have another commit by end of day.\n. @hawkw I like the idea of moving the validation to the a new Ns class, but I am a little apprehensive about changing Ns from a type to a class. Would changing that introduce breaking changes to other classes that use Ns?. @adleong after changing the ConsulConfig to add a configurable backoff setting, I realized that ConsulConfig is used for the consul Namer plugin in linkerd. This change is for the Namerd Dtabstore plugin. Do we need to also change the Namer plugin in Linkerd to reflect this behavior?. I was able to successfully test the config value by providing a max backoff of 1ms. The config was able to override the default value. Would love feedback on the config field name. I tried to make sure it made sense but also didn't to have a really long field name.. @obeattie I can try rerunning the CI job to see if we run into the same thing.. @jackkleeman thanks for the PR, we will take a look and provide any feedback if necessary!. @adleong I think @rclayton-the-terrible mentioned that they manually wrote dtabs into consul. I think this is the slack message\n\nWe create DTabs directly in Consul as a part of our deployments.  So namerd would not get a chance to validate.. Thanks for submitting the PR @edio we are taking a look at the changes you provided.. @yangzhares I am curious to see your namerd and linkerd configuration. Do you mind sharing your  configs so we get more insight as to what could be causing the issue?\n  . @yangzhares thank you for the config info. At first glance, I don't see anything wrong with your configs. What we know is that we have other users that are experiences the same 5 minute spike in requests to consul. Is there way you can identify the requests that are being sent to consul?. @adleong I can setup a test environment to reproduce. thanks for the info @yangzhares . Spent a good chunk of time trying to reproduce this but couldn't get promising results. I tried calling a service that did not exist in consul service discovery and it did not retry based on a Linkerd request. I also setup the service to register with consul and work under normal conditions then I killed the service so consul would deregister the service so that it no longer existed according to consul and then made a bunch of Linkerd requests. That still didn't give a promising result. To get a better understanding of the parameters this issue manifests itself in \n- What version of consul are you using? \n- What environment are you running linkerd, namerd and consul? Is it in k8s? AWS ECS etc?\n- Can you reproduce the issue in a docker environment? \n. Thanks @yangzhares I will give an update once I have new findings. @yangzhares we are able to reproduce the issue you are seeing. It looks like an issue that arises in the io.l5d.thriftNameIntepreter. We have opened #1778 and #1779 which describes the bug in more detail. We have identified that it will take a little bit of refactoring, planning and testing to make sure we fix the bug and make sure we don't introduce any new ones. Our recommendation is to use the io.l5d.mesh when using Namerd with Linkerd. \n\nWe've got a couple of questions though. How big of an impact does this bug create in your production environment? I know you mentioned that it happens every time your Security Team runs its test against Linkerd. Does this happen frequently? Would a Namerd restart after a security test be a possible fix for the issue?\nThanks so much for your cooperation in helping us work on this issue!. PR for this enhancement is ready for review, tested the change on both the branch and master. I was able to configure the closeTimeout  from 4 seconds to various other timeout values (1 minute, 30 seconds, 1 second, 40 seconds). @malafeev thank you very much for starting a PR for this feature. We always love it when we get a contribution from the community. Let us not when you are ready for us to review this PR!. Thanks for adding this issue @b-hoyt. We will take a look at what's going on. Generally, we recommend that fs namer shouldn't be used for production.. Did some further investigation and I think I may found a possible clue in to what is going on. The issue seems to in the IngressCache watch activity. \nscala\nval witness = Var\n            .async[Activity.State[W]](Activity.Pending) { updatable =>\n              watch(labelSelector, fieldSelector, version, updatable)\n            }\n            .changes\n            .foldLeft(initialState) {\n              case (state, activity) =>\n                activity match {\n                  case Activity.Ok(event) => onEvent(state, event)\n                  case _ => state\n                }\n            }.register(note => state.update(Activity.Ok(note)))\nWhile debugging, when an ingress is added, the activity is notified with of the IngressAdded added event. However, if an ingress is deleted IngressDeleted, no event is detected by the activity and therefore the IngressCache is never updated. . Update: We may have found a possible fix for this issue (#1810). We are currently testing out to make sure the fix does not cause any regression.\n. @krak3n I have not been able to reproduce this issue unfortunately. Here is a detailed step of my attempted. \n- Started up a fresh instance of minikube with the namerd.yaml below\n- Initiated a series of GET and PUT requests manually to the HTTP API\n- Admin page and GET requests seemed to always show the most recent DTab\nIs there any additional step that I may have missed?\nThis is the namerd.yml file I used:\n```yaml\n\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-namespace\n\nkind: CustomResourceDefinition\napiVersion: apiextensions.k8s.io/v1beta1\nmetadata:\n  name: dtabs.l5d.io\nspec:\n  scope: Namespaced\n  group: l5d.io\n  version: v1alpha1\n  names:\n    kind: DTab\n    plural: dtabs\n    singular: dtab\n\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: namerd-config\ndata:\n  config.yml: |-\n    admin:\n      ip: 0.0.0.0\n      port: 9991\nnamers:\n- kind: io.l5d.k8s\n  experimental: true\n  host: localhost\n  port: 8001\n\nstorage:\n  kind: io.l5d.k8s\n  host: localhost\n  port: 8001\n  namespace: my-namespace\n\ninterfaces:\n- kind: io.l5d.thriftNameInterpreter\n  ip: 0.0.0.0\n  port: 4100\n- kind: io.l5d.thriftNameInterpreter\n  ip: 0.0.0.0\n  port: 4101\n- kind: io.l5d.httpController\n  ip: 0.0.0.0\n  port: 4180\n- kind: io.l5d.httpController\n  ip: 0.0.0.0\n  port: 4181\n- kind: io.l5d.mesh\n  ip: 0.0.0.0\n  port: 4321\n- kind: io.l5d.mesh\n  ip: 0.0.0.0\n  port: 4322\n\n\nkind: ReplicationController\napiVersion: v1\nmetadata:\n  name: namerd\nspec:\n  replicas: 1\n  selector:\n    app: namerd\n  template:\n    metadata:\n      labels:\n        app: namerd\n    spec:\n      dnsPolicy: ClusterFirst\n      volumes:\n      - name: namerd-config\n        configMap:\n          name: namerd-config\n      containers:\n      - name: namerd\n        image: buoyantio/namerd:1.3.5\n        args:\n        - /io.buoyant/namerd/config/config.yml\n        ports:\n        - name: thrift\n          containerPort: 4100\n        - name: thrift-tls\n          containerPort: 4101\n        - name: http\n          containerPort: 4180\n        - name: http-tls\n          containerPort: 4181\n        - name: mesh\n          containerPort: 4321\n        - name: mesh-tls\n          containerPort: 4322\n        - name: admin\n          containerPort: 9991\n        volumeMounts:\n        - name: \"namerd-config\"\n          mountPath: \"/io.buoyant/namerd/config\"\n          readOnly: true\n      - name: kubectl\n        image: buoyantio/kubectl:v1.8.5\n        args:\n        - \"proxy\"\n        - \"-p\"\n        - \"8001\"\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: namerd\nspec:\n  selector:\n    app: namerd\n  type: LoadBalancer\n  ports:\n  - name: thrift\n    port: 4100\n  - name: thrift-tls\n    port: 4101\n  - name: http\n    port: 4180\n  - name: http-tls\n    port: 4181\n  - name: mesh\n    port: 4321\n  - name: mesh-tls\n    port: 4322\n  - name: admin\n    port: 9991\n\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: namerctl-script\ndata:\n  createNs.sh: |-\n    #!/bin/sh\nset -e\n\nif namerctl dtab get external > /dev/null 2>&1; then\n  echo \"external namespace already exists\"\nelse\n  echo \"\n  /host       => /#/io.l5d.k8s/default/http/hello;\n  /svc/*      => /host;\n  \" | namerctl dtab create external -\nfi\n\nif namerctl dtab get internal > /dev/null 2>&1; then\n  echo \"internal namespace already exists\"\nelse\n  echo \"\n  /srv        => /#/io.l5d.k8s/default/http;\n  /host       => /srv;\n  /tmp        => /srv;\n  /svc        => /host;\n  /host/world => /srv/world-v1;\n  \" | namerctl dtab create internal -\nfi\n\n\nkind: Job\napiVersion: batch/v1\nmetadata:\n  name: namerctl\nspec:\n  template:\n    metadata:\n      name: namerctl\n    spec:\n      volumes:\n      - name: namerctl-script\n        configMap:\n          name: namerctl-script\n          defaultMode: 0755\n      containers:\n      - name: namerctl\n        image: linkerd/namerctl:0.8.6\n        env:\n        - name: NAMERCTL_BASE_URL\n          value: http://namerd.default.svc.cluster.local:4180\n        command:\n        - \"/namerctl/createNs.sh\"\n        volumeMounts:\n        - name: \"namerctl-script\"\n          mountPath: \"/namerctl\"\n          readOnly: true\n      restartPolicy: OnFailure\n``. Hey @krak3n. Just wanted to check in and see if you were still experiencing this issue. I would love to see any log messages that come up when the issue occurs.. Thanks for the feature request @negz! The team will look at how we can add this feature to linkerd.. Closing as a result of merged PR #1794. Wow, I was about to mention on issue you created earlier that you could also submit a PR but you beat me to it! Thanks so much! we will take a look. :). No problem @negz. In the case of CircleCI we usually just retry the job. I can do that for you.. @peterfroehlich, this sounds like a good feature request! Thanks for bringing this to our attention. We will scope out and evaluate what is required.. Thanks for creating the issue @edio. Looking forward to your PR. Thanks for filing this issue @shakti-das! We will take a look.. Hi @negz I think this might be similar to #1791. From my investigation, it looks like the Watch API isn't getting notified of delete events for ingress.  We are continuing to investigate a possible solution but thanks for filing this detailed issue! . Ahh I see so the _Service_ is removed rather than the _Ingress_. Thanks for clarifying, it very well might be the same bug we can test that out to see if we get the same results.. Commenting on this issue just so everyone is in the loop. PR #1810 has been created as a possible fix for this issue and is currently being tested. . @hawkw I agree with you on testing. I would want to make sure we aren't affecting code that usesWatchable.scala. We probably have to test k8s service discovery extensively.. @negz thanks for offering to help! It would be interesting to see your findings with this PR. There is a new docker image https://hub.docker.com/r/buoyantio/linkerd/tags/1.3.6-watch-change/ that includes this change. It would be helpful to see if this image fixes the ingress issue you have been seeing.. @negz thanks for testing and fixing the new bug you found! I will give #1817 a review when I get the chance.\n. Sorry @adleong, only saw this now. I will rebase right now.. @hynek thanks so much for filing this detailed report. These kinds of issues make debugging linkerd/namerd much easier. We will take a look at this issue as soon as we can. Also, if you have any suggestions on where you would love to see more detail in namerd's documentation, please do let us know!. It looks like this error happens when theConsulNamerbinds addresses with the help of consul. When it tries to discover the addresses for/#/io.l5d.consul/nonexistent-dc/[SERVICE]`, it receives a 500 error from consul. The HTTP request is then retried indefinitely even though the path will never resolve to anything. This will then cause the request to linkerd or in my case, a dtab lookup in the admin interface to timeout and paths are never resolved. \nThere may be a couple of solutions to fix this. We could proactively check for correct parameters e.g. dc, that are going to be used in the consul API  service lookup.  We could immediately raise an error or give a NameTree.Neg to signify that the path being resolved is faulty. The other solution may be to go ahead with path resolution and when we encounter a HTTP 500 error with a response that says No path to datacenter, we could tell the consul namer that the path shouldn't resolve to anything.\n/cc @adleong . I think there is an additional tradeoff in relying on the the DC list. Right now, the consul namer spends a lot of its time polling consul endpoints. If we were to add a and additional network call to check the \"DC list\", it would increase the network traffic between Linkerd and the consul. This may not be an issue for a environments that can handle heavy network traffic but I think its good to keep that in mind when considering adding network calls to get info about the state of consul. . @Ashald IIUC, you are suggesting something like this:\n```scala\nprivate[consul] class DataCenterWatch{\n  def apply(consulApi: v1.ConsulApi)\n   (implicit timer: Timer = DefaultTimer): Activity[Seq[String]] = {\n    Var.asyncActivity[Seq[String]]{state =>\n      @volatile var stopped: Boolean = false\n      def loop(backoffs: Stream[Duration]):Future[Unit] = {\n        consulApi.datacenters().transform{\n      case Throw(e) =>\n        log.error(\"error getting datacenter list\")\n        val backoff #:: nextBackoffs = backoffs\n        Future.sleep(backoff).before(loop(nextBackoffs))\n\n      case Return(dcs) =>\n        state.update(Activity.value(dcs))\n        loop(backoffs)\n    }\n    val pending = loop(backoffs)\n    Closable.make { _ =>\n      stopped = true\n      Future.Unit\n    }\n  }\n}\n\n}\n}\nprivate[this] class UntaggedNamer(lookup: LookupCache, dcWatch: DataCenterWatch, prefix: Path) extends Namer {\ndef lookup(path: Path): Activity[NameTree[Name]] =\n  path.take(2) match {\n    case id@Path.Utf8(dc, service) =>\n      val k = SvcKey(service.toLowerCase, None)\n      dcWatch().flatMap { dcsList =>\n        if (dcsList.contains(dc))\n          lookup(dc, k, prefix ++ id, path.drop(2))\n        else Activity.value(NameTree.Neg)\n\n      }\n\n\n    case _ => Activity.value(NameTree.Neg)\n  }\n\n}\n```\nWe add a datacenter list watch to monitor the datacenters available. When we do a lookup during name resolution, we would check to see if the datacenter exists and if it does we go ahead with our NameTree lookup, if it doesn't we resolve with NameTree.Neg.. > I'll try to do a little test to see how quickly it reflects changes.\n@Ashald did you by change have the time to test out the changes mentioned above? Curious to see what you have discovered.. @Ashald its no worries at all! There was totally no confusion whatsoever :) I will fix up the merge conflicts and hopefully can get this merged. Thanks for your feedback!. Since Linkerd is built on Finagle, work is needed to ensure the J9 JVM works with Finagle. Unfortunately, Finagle is dependent on some features present in the Hotspot JVM. This causes an UnsatisfiedLinkerdError when attempting to run Linkerd with OpenJ9. The issue is fixed by this PR. Until this PR is merged into twitter-util, you will need to build twitter-util from it\u2019s Github source code.\nBuilding Linkerd with OpenJ9.\n\nFork and clone twitter-util, finagle, twitter-server from the Twitter OSS repo on Github\nMake the changes similar to this PR in twitter-util.\nBuild and publish the Twitter OSS libraries to your local environment by using dodo with this command ./dodo/bin/build --no-test finagle\nJust to be safe, cd into the twitter-util repo and run ./sbt publishLocal to make sure the changes you made are placed in your local cache.\nGo into the Linkerd repo and build a linkerd docker image with the base image as the adoptopenjdk openj9 image.\n./sbt 'set Base.dockerJavaImage in (linkerd, Bundle) := \"adoptopenjdk/openjdk8-openj9:jdk8u162-b12_openj9-0.8.0\"' linkerd/docker\n\nComparing vanilla Linkerd 1.4.3 and Linkerd 1.4.3+OpenJ9\nThe OpenJ9 indeed has significant improvements in memory footprint. We run a simple hello world service with slow_cooker sending 500 qps to Linkerd in docker. Using top, we see that Linkerd 1.4.3 uses ~ 200MB.\n\nLinkerd 1.4.3 with OpenJ9 memory footprint shows\n\nPretty cool! That\u2019s almost half the size in memory!\nFast and the Furious: Vanilla JDK vs OpenJ9\nNext, we test out the startup time for the J9 JVM to see if we do gain a performance improvement. Since Linkerd starts up and does not exit, it\u2019s a going to be a little tricky testing out how fast Linkerd starts up with J9. The method used is a little hacky however, it does provide us some sense of how fast the two JVMs are. Here is how we tested it:\n\nWe first set up a curl loop to poll the admin/ping endpoint every second like so:\nwhile sleep 1; do curl  http://localhost:<admin_port>/admin/ping; done\nWe run linkerd from in a docker environment\nWe wait to see how long Linkerd responds with a \u201cpong\u201d message. You will see three different curl\n  messages when running the test. Initially, you should see:\nFailed to connect to localhost port 9991: Connection refused\nThe Linkerd container isn\u2019t up yet.\n\n\nEmpty reply from server\nLinkerd is in the process of initializing\n\n\npong\nLinkerd is is fully initialized.\n\nThe results are determined by a pretty easy albeit rudimentary way by counting the number of \u201cEmpty reply from server\u201d we get before receiving \u201cpong\u201d.  Since each message is displayed every second, we can get an idea, in seconds, of how long it takes for Linkerd to startup with both JVMs.\nWith the vanilla Linkerd we get about 30 x \u201cEmpty reply from server\u201d messages which equate to roughly about 30 seconds. With OpenJ9 it takes about 10 x \u201cEmpty reply from server\u201d = 10 seconds!. That\u2019s like a 60% increase in startup speed. Crazy! I think the vanilla JDK owes OpenJ9 a 10 second car.\n. That's great to hear! I think the next steps for this is to see if we can squeeze more out of the JVM. I found out there are various flags that can be used with J9 like -Xquickstart and enabling AOT. It would be good to also get some benchmarks to see what we are working with.. @leozc, Linkerd is configured to use the CMS GC. I didn't make changes on that front. I know J9 has a GC related flag -XX:+IdleTuningGcOnIdlebut not really sure how that affects garbage collection. I am planning on including that in my second set of experiments.. If folks would like to try out Linkerd+OpenJ9, there is a docker image available at Buoyant's docker repo with the image tag 1.4.3-SNAPSHOT-jdk-openj9. It'd be interesting to see experiment results from the community. Also, experimental Linkerd+OpenJ9 docker images will be available in upcoming Linkerd releases. This should make it easy to include the new build artifact in your test environments.. Is there anyway to reproduce this issue? i.e. How was the change tested?. > We should also update the documentation in linkerd/docs/routers.md that describe match patterns.\nI think we should update the routers section with this behavior it hasn't already been done. Unless, this behavior is not new and is only an enhancement.. Thanks for submitting the issue @jacob-go!  We will take a look at what is going on.. Hi @jacob-go, thanks for checking in! I have not had the chance to look into this issue yet. However, I was able to reproduce the issue pretty easily based on your description. We will update the issue once we come around to a possible fix for this issue.. I've taken a look at the shutdown grace period issue and this is what I understand so far based on the three tests I ran:\n\nTest 1: If no requests are in flight and linkerd gets a SIGTERM, it will shutdown immediately even if shutdownGraceMs is set.\nTest 2: If a request is in flight and is completed before the graceful shutdown period completes, linkerd will shutdown immediately after the completed request. Meanwhile, no requests will be processed during the graceful shutdown period.\nTest 3: If the request is never completed within the graceful shutdown window, Linkerd will force shutdown and drop the in-flight request. Just like the scenario above, any request that comes in during the graceful period shutdown will not be processed.\n\nNo in-flight requests with 10 second graceful shutdown window\nbash\nI 0412 13:57:48.114 PDT THREAD1: initialized\nD 0412 13:57:48.498 PDT THREAD20:\nI 0412 13:58:14.322 PDT THREAD23: Received SIGTERM. Shutting down ...\nRequest with a 8 second latency and linkerd with a 10 second graceful shutdown window\nbash\nI 0412 13:44:03.681 PDT THREAD1: initialized\nD 0412 13:44:04.069 PDT THREAD22:\nD 0412 13:44:57.328 PDT THREAD24 TraceId:883e1c5975bad4db: LoadService: loaded instance of class com.twitter.server.FlagResolver for requested service com.twitter.finagle.Resolver\nD 0412 13:44:57.333 PDT THREAD24 TraceId:883e1c5975bad4db: LoadService: loaded instance of class com.twitter.finagle.zookeeper.ZkResolver for requested service com.twitter.finagle.Resolver\nD 0412 13:44:57.352 PDT THREAD24 TraceId:883e1c5975bad4db: LoadService: loaded instance of class com.twitter.finagle.serverset2.Zk2Resolver for requested service com.twitter.finagle.Resolver\nI 0412 13:44:57.366 PDT THREAD24 TraceId:883e1c5975bad4db: Resolver[inet] = com.twitter.finagle.InetResolver(com.twitter.finagle.InetResolver@28428c1f)\nI 0412 13:44:57.366 PDT THREAD24 TraceId:883e1c5975bad4db: Resolver[fixedinet] = com.twitter.finagle.FixedInetResolver(com.twitter.finagle.FixedInetResolver@4982f0e1)\nI 0412 13:44:57.367 PDT THREAD24 TraceId:883e1c5975bad4db: Resolver[neg] = com.twitter.finagle.NegResolver$(com.twitter.finagle.NegResolver$@76802a73)\nI 0412 13:44:57.367 PDT THREAD24 TraceId:883e1c5975bad4db: Resolver[nil] = com.twitter.finagle.NilResolver$(com.twitter.finagle.NilResolver$@4a1e72c8)\nI 0412 13:44:57.367 PDT THREAD24 TraceId:883e1c5975bad4db: Resolver[fail] = com.twitter.finagle.FailResolver$(com.twitter.finagle.FailResolver$@14459692)\nI 0412 13:44:57.367 PDT THREAD24 TraceId:883e1c5975bad4db: Resolver[flag] = com.twitter.server.FlagResolver(com.twitter.server.FlagResolver@6456b4db)\nI 0412 13:44:57.367 PDT THREAD24 TraceId:883e1c5975bad4db: Resolver[zk] = com.twitter.finagle.zookeeper.ZkResolver(com.twitter.finagle.zookeeper.ZkResolver@13f2c990)\nI 0412 13:44:57.368 PDT THREAD24 TraceId:883e1c5975bad4db: Resolver[zk2] = com.twitter.finagle.serverset2.Zk2Resolver(com.twitter.finagle.serverset2.Zk2Resolver@2a436114)\nI 0412 13:44:59.028 PDT THREAD29: Received SIGTERM. Shutting down ...\nI 0412 13:45:05.586 PDT THREAD24: Reaping /svc/dog\nI 0412 13:45:05.589 PDT THREAD24: Reaping $/inet/127.1/7777\nRequest with a 15 second latency with linkerd shutdown grace period of 10 seconds\n```bash\nI 0412 13:51:29.506 PDT THREAD1: initialized\nD 0412 13:52:05.671 PDT THREAD28 TraceId:9c28ecd4a090f466: LoadService: loaded instance of class com.twitter.server.FlagResolver for requested service com.twitter.finagle.Resolver\nD 0412 13:52:05.676 PDT THREAD28 TraceId:9c28ecd4a090f466: LoadService: loaded instance of class com.twitter.finagle.zookeeper.ZkResolver for requested service com.twitter.finagle.Resolver\nD 0412 13:52:05.693 PDT THREAD28 TraceId:9c28ecd4a090f466: LoadService: loaded instance of class com.twitter.finagle.serverset2.Zk2Resolver for requested service com.twitter.finagle.Resolver\nI 0412 13:52:05.709 PDT THREAD28 TraceId:9c28ecd4a090f466: Resolver[inet] = com.twitter.finagle.InetResolver(com.twitter.finagle.InetResolver@3694dcec)\nI 0412 13:52:05.710 PDT THREAD28 TraceId:9c28ecd4a090f466: Resolver[fixedinet] = com.twitter.finagle.FixedInetResolver(com.twitter.finagle.FixedInetResolver@6bb562c6)\nI 0412 13:52:05.710 PDT THREAD28 TraceId:9c28ecd4a090f466: Resolver[neg] = com.twitter.finagle.NegResolver$(com.twitter.finagle.NegResolver$@3e4bcd6f)\nI 0412 13:52:05.710 PDT THREAD28 TraceId:9c28ecd4a090f466: Resolver[nil] = com.twitter.finagle.NilResolver$(com.twitter.finagle.NilResolver$@7cc3af3a)\nI 0412 13:52:05.710 PDT THREAD28 TraceId:9c28ecd4a090f466: Resolver[fail] = com.twitter.finagle.FailResolver$(com.twitter.finagle.FailResolver$@6c206ff2)\nI 0412 13:52:05.710 PDT THREAD28 TraceId:9c28ecd4a090f466: Resolver[flag] = com.twitter.server.FlagResolver(com.twitter.server.FlagResolver@2cc383e2)\nI 0412 13:52:05.710 PDT THREAD28 TraceId:9c28ecd4a090f466: Resolver[zk] = com.twitter.finagle.zookeeper.ZkResolver(com.twitter.finagle.zookeeper.ZkResolver@4c8303a0)\nI 0412 13:52:05.711 PDT THREAD28 TraceId:9c28ecd4a090f466: Resolver[zk2] = com.twitter.finagle.serverset2.Zk2Resolver(com.twitter.finagle.serverset2.Zk2Resolver@2d58df75)\nI 0412 13:52:07.346 PDT THREAD32: Received SIGTERM. Shutting down ...\nD 0412 13:52:17.357 PDT THREAD31 TraceId:9c28ecd4a090f466: Failed mid-stream. Terminating stream, closing connection\ncom.twitter.finagle.ChannelClosedException: ChannelException at remote address: /127.0.0.1:7777. Remote Info: Not Available\n    at com.twitter.finagle.netty4.transport.ChannelTransport$$anon$1.channelInactive(ChannelTransport.scala:188)\n...\nD 0412 13:52:17.375 PDT THREAD28 TraceId:9c28ecd4a090f466: Exception propagated to the default monitor (upstream address: /127.0.0.1:56577, downstream address: /127.0.0.1:7777, label: $/inet/127.1/7777).\ncom.twitter.finagle.CancelledRequestException: request cancelled. Remote Info: Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /127.0.0.1:7777, Downstream label: $/inet/127.1/7777, Trace Id: 9c28ecd4a090f466.c31ef3bf3fb5c7fb<:9c28ecd4a090f466\nI 0412 13:52:17.473 PDT THREAD28: Reaping /svc/dog\nI 0412 13:52:17.478 PDT THREAD28: Reaping $/inet/127.1/7777\nE 0412 13:52:17.481 PDT THREAD28 TraceId:9c28ecd4a090f466: service failure: com.twitter.finagle.CancelledRequestException: request cancelled. Remote Info: Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /127.0.0.1:7777, Downstream label: $/inet/127.1/7777, Trace Id: 9c28ecd4a090f466.c31ef3bf3fb5c7fb<:9c28ecd4a090f466\n``. @jacob-go from the scenario you described above, it could that Linkerd stops receiving the non-stop inflight request and then is able to immediately shutdown even with theshutdownGraceMsset. Are the requests short lived? meaning, are they able to complete within 10 seconds of the shutdown window?. It is interesting to see the linkerd process consistently shuts down within 2 seconds.  My experiments were conducted locally. I had a dummy server that accepted a number of requests that responded with some preconfigured latency i.e. one request with an 8 second latency and another with a 15 second latency.  For my first test, I simply sent aSIGTERMto linkerd with no in-flight requests . I then ran two tests where I started linkerd, issued a request through linkerd to the dummy server, initiated aSIGTERM` on the linkerd process and watched the logs to observe how long it took for the process to terminate. I did not use the shutdown endpoint but that might be something I could possibly introduce in my tests. \nI am a little curious about this line in the logs you provided:\nbash\nW0423 13:32:33.847641 31006 logging.cpp:91] RAW: Received signal SIGTERM from process 3921 of user 0; exiting\nI don't recall seeing that come up in the linkerd logs. I am not entirely sure of how DC/OS works but is that a process that runs along side linkerd? If so, does that process interact with Linkerd in anyway?. I don't think, OTTOH, that sending a SIGTERM vs using the /admin/shutdown are any different.  I would like to make sure the tests closely match how the issue is manifested just in case the /admin/shutdown has some additional tear down tasks that I am not aware of. . Thanks, I am going to rerun my tests but this time with the admin endpoint and I will keep you posted on what I find. I was looking through the admin endpoint handler and it looks like you can add a URL parameter grace that gives you the option to provide a grace period for shutdown. For example, if you issue a POST request to linkerd with the URL admin/shutdown/grace=10.seconds.  Is there a way you could test that out?. @jacob-go, I ran some tests and it looks like the admin/shutdown endpoint has its own configurable grace period. I had the same test setup I described earlier and used the admin endpoint to initiate a SIGTERM. When I sent admin/shutdown on the first try with an in-flight request, Linkerd quit within 2 seconds. When I sent admin/shutdown?grace=10.seconds to Linkerd, it shut down right after the request completed which was somewhere around 8 seconds after the SIGTERM was triggered.  I can document that behavior in the docs for clarity. Hopefully, this solves the issue you are running into.. I tested this branch with various test scenarios in minikube that exercised both H1 and H2 routing as well as the namerd thrift interpreter integration and everything looks good.. Not so great news on the h2spec though, I ran that and it looks in the baseline logs we had 12 failed tests but now we have 16 :(. h2spec was rerun on this PR based on the last two commits. Things are looking great! more eyes and more tests on this PR are always welcome!. Thanks for this PR @mmrozek! we will review it as soon as we get the chance.. Adding the related issue.\nfixes #1477 . @mmrozek are you able to run the e2e tests locally to see if you get the same issue that the CI server is failing on? You can run the specific test by running sbt 'router-http/e2e:test' on your machine.. Sure. the bug can be reproduced locally. I set up an H2 server with Linkerd pointing to it. The H2 server responds with a stream of integers up to 11 and closes the connection. After the server responds with a GOAWAY frame, Linkerd throws a ClassCastException in the logs with the log.level set to INFO. After the code change, I run the same test with the H2 server and no longer see any errors. To be sure, I ran the test with Linked set a DEBUG log level and no errors show up.. @jacob-go Yes. I think that should be a good place to highlight the steps you outlined in #1887 . This issue is fixed based on PR https://github.com/linkerd/linkerd-examples/pull/220. Closing this issue.. @yoitsro good question. I haven't tested this with remote IPs. One way I could test that is to deploy linkerd in a k8s environment with the forwarded header configuration set. Does that suffice as an appropriate test for remote IPs? I would be happy to get any suggestions on what you think would be best. As for your question about this working with IPv6 addresses, the answer is yes. It should work exactly like the Http/1 feature which does display IPv6 addresses in the Forwarded header field. I think I can set up a test for that as well.. @yoitsro actually found a faster way to confirm that this works with IPv6, there is a unit test that tests for that specific scenario.  here\nThe output for the specific header value looks like this:\nby=_http;for=\"[2001:db8:cafe:0:0:0:0:17]\";authority=svc/name;proto=http. @jayeve your PR is looking good! I wanted to add that once you are done with the necessary changes, please update the PR description to follow the Linkerd PR contribution guidelines. Thanks, @mmrozek for this PR! We will take a look.. Thanks for filing this issue @hynek! We will take a look at what's going on\n. @shakti-das thanks for filing this issue! PRs are always welcome :). Hi @shakti-das, to make our DCO bot happy, do you mind signing off your last commit like so \nbash\ngit commit --amend --no-edit -s\ngit push -f origin <branch name>\nThat should take your previous commit and added your signoff so you can pass the DCO bot check. Thanks!. whoops added this to issue and not PR\nHi @shakti-das, to make our DCO bot happy, do you mind signing off your last commit like so \nbash\ngit commit --amend --no-edit -s\ngit push -f origin <branch name>\nThat should take your previous commit and added your signoff so you can pass the DCO bot check. Thanks!. Having both human readable and a JSON formatted response sound like great ideas. What do you think about not having the JSON format output and just rely on the human readable format? I can think of users wanting to use JSON in scripts as a way to monitor the client state in linkerd. I am thinking the non-JSON format might be sufficient for such use cases. I don't think it would be hard to implement both, I just feel it might be good to have only one way of getting request evaluation info.. Thanks, @olix0r for the highlighting the potential spec violation. I wasn't aware of that. \ud83d\ude0c I am curious to learn more about this violation. Do you mind pointing me to where  RFC 2616 highlights this spec?  (I am assuming that the spec info resides in this RFC).\nI like the idea of creating a custom HTTP verb for this feature as it gives the evaluator its domain to do its work. I am not sure if Linkerd does not automatically proxy custom HTTP methods. However, I don't think that would be difficult to change if it ends up doing that. I read that proxies and caches treat custom HTTP methods as unsafe and therefore do not cache them. So, maybe that handles the caching problem. Are there any other compatibility issues that we need to worry about when dealing with custom HTTP methods, proxies and caches?. Great suggestions @adleong.\n\n\nI would put addresses before selected address so that the order of service -> client -> addresses -> selected goes from more general to more specific\n\n\nthat's a good point. I will do that.\n\n\nWhat do you think of adding a timestamp field?  This gives some timing data about each hop\n\n\nI think that could be useful. It would give users information about how long each TRACE request takes. At one point, I ran the tracer and it seemed to be taking longer than expected. I can imagine there could be serious latency that may be encountered while doing this.\n\n\n\nLooks like the dtab resolution output is a bit broken with transformers\nDarn! I will take a look to see what's going on.\n\n\nConsider printing the dentry alongside each step in the delegation (like the dtab UI does) \n\n\n\nConsider it done. \n\n\nMissing newline at the end of the output\n\n\nRight now, the code strips empty watch space at the start and end of the final output. Do we not want this behavior? Is a newline expected on plaintext responses from servers?. I think my latest commit has a different output from what you have. The weights are no longer displayed in the Dentry, Also Dentry.nop is no longer visible. I forgot to add the timestamps. Watch for my next commit.\n. Sure, I can do that.\n. According to the RFC the Content-Type needs to be message/http. I am not really sure what that means really.. @adleong, we could set the Content-Type if we check and see if it isn't present, otherwise, we can send back whatever the final server sent?. @olix0r, according to the RFC we would need to send  \n\na 200 (OK) response with a Content-Type of \"message/http\"\n\nA \"message/http\" content-type would need to be encoded in either 7bit, 8bit or binary. I think for now we could release with this feature and then down the line, we could encode a TRACE request and set the Content-Type to make sure we are matching the spec. I will create a ticket so we can track that. . Thanks for submitting this @dvulpe! We will take a look at your PR.. Thanks for submitting this @bmcmilla! We will be sure to take a look.. Interested in getting thoughts about using @deprecated annotations in Linkerd. I know finagle uses them and have encountered them while performing upgrades. I think it is a nice reminder to have for users. WDYT?. Interesting, do we ever use a PrepFactory module in the H2 client stack? What are the uses of a PrepFactory module?. Hey @robertpanzer thanks for submitting this PR! I was trying to see if I could reproduce this locally prior to this commit but I don't seem to be running into the issue. I may be doing something wrong \ud83d\ude2c. I pulled down latest master and ran ./sbt admin/publishLocal to publish the admin_2.12.jar to my local ivy cache. I then ran jar tf admin_2.12.jar | grep -i node_modules to see if I could find any node module directories in the jar but didn't find any. Do you see the same results? . Thanks for filing this issue @yoitsro. Out of curiosity, do you have any log information that you could provide w.r.t the problematic linkerd instance(s)? Do you see any errors pop up when Linkerd stops routing gRPC traffic?. @yoitsro, I took a look at the logs you sent me, and I didn't seem to find what the cause might have been. One thing that was weird was, Linkerd failed to connect to the kubectl proxy sidecar. I am not sure if it is a red herring, but I think it would be useful to view the container logs for the kubectl proxy to see if there is anything fishy going on.. Thanks for submitting this @bkreitch. We will take a look at this.. @dunjut I believe we resolved the issue over slack. If the fix worked out for you, do you mind closing this issue? thanks!. Thanks for submitting the issue @utrack! We will take a look at this.. It looks like this is a non-issue, so I'm closing this ticket. Linkerd clears any headers that have a prefix of l5d- from incoming requests. In this case, if a TRACE request with l5d-add-context were set to true, and, if clearContext is configured in Linkerd, a TRACE request would be simply forwarded without any router information added to the response.. @robertpanzer thanks for all the detail you've included in this issue. We will run through your steps and see what might be going on.. @mikebz thanks for submitting this feature request. We love to see the community come up with feature requests such as this. We also love to see PRs from the community too! Would you be open to submitting a PR for this?. Thanks so much for this PR @mstoskus! Be sure to check out our CONTRIBUTING docs for PR submission style.. @mstoskus one final thing and this branch is ready to merge. To comply with DCO, you will need to sign each of your commits. Rebase your branch and edit all your commits. Then sign each commit with the-s flag.. @mstoskus thanks for that. I think you also need to add the \"Signed Off By:\" portion to your commits as well. so something like git commit -s --amend --no-edit. It looks like you used -S, --gpg-sign[=<key-id>]. @aywrite, thanks for filing this issue and for the detailed reproducible steps! We take a look at this.. @aywrite I tried following the steps you outlined to reproduce this issue, but I didn't quite get the same results you had. In the process, I may have found something that may the cause of the problem you are seeing. \nI tried to replicate the first io.l5d.consul namer config in my local tests. When I used this section of the config you provided.\nyaml\n - kind: io.l5d.consul\n   host: 127.0.0.1\n   useHealthCheck: true\n   prefix: /default\nI couldn't get Linkerd to fail routing the request. Even with the start up sequence you provided.\nHowever,  when using the second io.l5.consul namer, it consistently failed to route requests regardless of how I executed the start up sequence.\nyaml\n- kind: io.l5d.consul\n   host: 127.0.0.1\n   useHealthCheck: true\n   healthStatuses:\n   - warning\n   prefix: /fallback\nIt seems as if adding passing to the healthStatus list configuration fixed the issue. It looks like when Linkerd filters services based on what is considered \"healthy\" or \"routable\" by the user. You must list passing as a healthy status. When Linkerd gets a list of services from Consul, it checks to see that each service's status must be present in the healthStatuses set. For instance, the configuration above would filter out services with passing health because it's not present in the healthStatuses field.\n. Ahh! I guess I should have first asked how you were using the two namers. I would have to check and see if the connection to consul is retried upon connection failure. We will keep investigating. In the meantime, if there is any more information you may have or if you can come up with a repro environment to help us figure out what is going on, we would love to check that out!. Yea, sounds like something more complex is going on. I can confirm on my side that Linkerd should retry requests to consul if it is unreachable and that after consul comes back up, Linkerd's service discovery polling requests show up in the consul logs. Can you confirm if that is the case on your side? Does Linkerd keep logging Connection refused even after consul comes back up?. @aywrite thanks so much for the repo! We will run through the steps you provided and see if we can reproduce it. That's weird that requests to a locally running consul are never retried.. @aywrite, I think we may have found and fixed the bug causing the issues you are seeing. Feel free to test the branch fix out and see if that works.. Hi, @rmichela thanks for filing this issue. We have taken a look at it and have this WIP PR you can track or leave any helpful feedback. Thanks again!. Thanks for filing this issue @utrack. It looks like something special is going on when Linkerd is handling HTTPS traffic that prevents the header from being propagated. We will look into this.. @utrack we tried reproducing this issue and we do not run into it when we set up client TLS on the router that is configured with egress. Do you have client TLS set up in your config?. Strange, your config looks correct. I am wondering if service name does not resolve with the /$/io.buoyant.rinet namer. Is the IP 0.0.1.187 from Google? When using the admin UI to test the name resolution, what IP addresses does Linkerd display?. Marking this closed based on #2043 . Thanks for submitting this issue @vadimi and thanks for all the detailed tests you ran. We will take a look at this.. @vadimi after reproducing a scenario similar to what you described, it seems like things are working as intended AFAIK. I set up Linkerd to route requests from nghttp to an HTTP/2 server that sent no frames to the nghttp client. I added a client configuration similar to use with an idleTimeMs of 5 seconds.\nWhen firing a request to the \"idle\" HTTP/2 and waiting for about 6 seconds. Linkerd didn't send a GOAWAY frame to the nghttp client. I believe Linkerd, in this scenario, assumes the request is not satisfied, so the connections between the client <-> Linkerd and Linkerd <-> HTTP/2 server are still considered active. However, when terminating the client's request, Linkerd cached the connection to the HTTP/2 and terminated that connection after the configured 5 seconds. Let us know if you can reproduce this or if there is more information you may have on this issue.. @vadimi glad we could be of help! Let us know if you have other questions, in the meantime, I will go ahead and close this issue.. Hi @Aavon, because minikube doesn't have external load balancer support, you cannot use the NODE_NAME environment variable provided by the k8s Downward API. Instead, you can replace the NODE_NAME environment variable with a HOST_IP env. It would look something like this in your k8s yaml.\nyaml\n         - name: HOST_IP\n            valueFrom:\n              fieldRef:\n                fieldPath: status.hostIP\n          - name: http_proxy\n            value: $(HOST_IP):4340\nI think that should do the trick. Do let us know if that fixes it. Thanks!. @leozc thanks for submitting this PR. This is a very interesting case. I do agree that having to rely on the JVM\u2019s DNS resolver brings an additional layer of redirection to the consul namer. Maybe we could make this option configurable so that it is explicit for other consul namer users? \nIn regards to the behavior of the DNS returning IPv4 vs IPv6, IMO, this should be configurable at the JVM level and not the Namer level. However, I think Linkerd already has the -Djava.net.preferIPv4Stack JVM setting set to true in LinkerdBuild.scala. So this might be a non-issue but may require additional testing.. @utrack thanks for filing this! You are right that Linkerd should handle chunked responses as a special case when running a diagnostic trace. We will work on fixing this.. True. The idea was to have the configuration similar to the LowMem docker image build process. Or we could manually use the dockerJavaImage setting at build time and push the images manually.. As of 1.4.6, Namerd now implements the destination API from Linkerd 2 https://github.com/linkerd/linkerd/pull/2091. Closing this issue.. I am curious to know if there are certain cases where we want this behavior to happen and whether it should be tied to a configuration setting.  Could the x-forwarded-client-cert be cleared when clearContext is set to true? i.e. When Linkerd is configured for ingress? . IIUC, based on #1153,  the x-forwarded-client-cert is intended to allow downstream services make some decision about a request forward by a TLS terminated Linkerd. If you had two Linkerd's routing a request in a servicemesh configuration, the first Linkerd (outgoing) would generate the header but, the second Linkerd (incoming) would clear that header and the terminal service would not be able to handle the request appropriately. Would that scenario be affected by this change?. @evhfla thanks for filing this. Could you try again and see if the issue is fixed? I just rebuilt the images and pushed them to docker.. It looks like we may not need to add a specific JvmStats for OpenJ9. When starting up Linkerd on the OpenJ9 JVM, the admin/metrics.json endpoint is reachable and showing a majority of the stats. I compared the Linkerd stats with those collected by VisualVM and I do see that there is a much for some of the data points.\nStats from Linkerd:\njson\n{\n  \"jvm/start_time\": 1532366420000,\n  \"jvm/application_time_millis\": 0,\n  \"jvm/classes/total_loaded\": 8810,\n  \"jvm/classes/current_loaded\": 8810,\n  \"jvm/classes/total_unloaded\": 0,\n  \"jvm/postGC/used\": 13862072,\n  \"jvm/postGC/tenured_LOA/used\": 0,\n  \"jvm/postGC/tenured_SOA/used\": 13271992,\n  \"jvm/postGC/nursery_survivor/used\": 458752,\n  \"jvm/postGC/nursery_allocate/used\": 131328,\n  \"jvm/nonheap/committed\": 337056032,\n  \"jvm/nonheap/max\": -1,\n  \"jvm/nonheap/used\": 51105992,\n  \"jvm/tenuring_threshold\": 0,\n  \"jvm/thread/daemon_count\": 33,\n  \"jvm/thread/count\": 34,\n  \"jvm/thread/peak_count\": 34,\n  \"jvm/mem/current/used\": 65907688,\n  \"jvm/mem/current/JIT_data_cache/max\": -1,\n  \"jvm/mem/current/JIT_data_cache/used\": 2097152,\n  \"jvm/mem/current/miscellaneous_non_heap_storage/max\": -1,\n  \"jvm/mem/current/miscellaneous_non_heap_storage/used\": 7201856,\n  \"jvm/mem/current/tenured_LOA/max\": 26843136,\n  \"jvm/mem/current/tenured_LOA/used\": 0,\n  \"jvm/mem/current/tenured_SOA/max\": 510027776,\n  \"jvm/mem/current/tenured_SOA/used\": 13271992,\n  \"jvm/mem/current/JIT_code_cache/max\": -1,\n  \"jvm/mem/current/JIT_code_cache/used\": 0,\n  \"jvm/mem/current/nursery_survivor/max\": 13421568,\n  \"jvm/mem/current/nursery_survivor/used\": 458752,\n  \"jvm/mem/current/nursery_allocate/max\": 120796160,\n  \"jvm/mem/current/nursery_allocate/used\": 864624,\n  \"jvm/mem/current/class_storage/max\": -1,\n  \"jvm/mem/current/class_storage/used\": 42013312,\n  \"jvm/mem/buffer/mapped/max\": 0,\n  \"jvm/mem/buffer/mapped/count\": 0,\n  \"jvm/mem/buffer/mapped/used\": 0,\n  \"jvm/mem/buffer/direct/max\": 48,\n  \"jvm/mem/buffer/direct/count\": 7,\n  \"jvm/mem/buffer/direct/used\": 49,\n  \"jvm/mem/postGC/used\": 13862072,\n  \"jvm/mem/postGC/tenured_LOA/used\": 0,\n  \"jvm/mem/postGC/tenured_SOA/used\": 13271992,\n  \"jvm/mem/postGC/nursery_survivor/used\": 458752,\n  \"jvm/mem/postGC/nursery_allocate/used\": 131328,\n  \"jvm/num_cpus\": 1,\n  \"jvm/gc/scavenge/msec\": 685,\n  \"jvm/gc/scavenge/cycles\": 258,\n  \"jvm/gc/msec\": 757,\n  \"jvm/gc/eden/pause_msec.count\": 0,\n  \"jvm/gc/global/msec\": 72,\n  \"jvm/gc/global/cycles\": 4,\n  \"jvm/gc/cycles\": 262,\n  \"jvm/fd_limit\": 1024,\n  \"jvm/compilation/time_msec\": 19520,\n  \"jvm/uptime\": 2119074,\n  \"jvm/safepoint/sync_time_millis\": 0,\n  \"jvm/safepoint/total_time_millis\": 0,\n  \"jvm/safepoint/count\": 0,\n  \"jvm/heap/committed\": 20905984,\n  \"jvm/heap/max\": 536870910,\n  \"jvm/heap/used\": 14595368,\n  \"jvm/fd_count\": 73,\n...\n\nThere are some stats that are Hotspot specific that don't show values e.g. jvm/safepoints. Some work may need to be done to evaluate what the equivalent metrics measurement could be for OpenJ9.\n. Argh, totally didn't catch that! thanks! I will update the config docs.. Thanks for submitting this issue @msiebeneicher, we will get this fixed as soon as we can.. @dhruvmphatek It looks like Linkerd is having issues reaching the Namerd instance. Are you able to confirm that Namerd is running on localhost on port 8080? Are you seeing any errors in the Namerd logs?. @dhruvmphatek can you confirm that you can reach your zookeeper server on 192.168.0.90 from your Namerd host?. That's great @sahilbadla27, anyway you can document how you reproduced this?. @cian-k updating this issue with what we figured out. So, it seems like Linkerd uses the maxInitialSizeKB value as intended. The one thing you will need to ensure is that the service you are trying to reach is configured to accept requests with large initial lines. \nWhen reproducing this issue, the test server I used was configured to accept initial line sizes of 4KB. After setting the maxInitialSize to 20MB, the server returned an HTTP 200 to my test client through Linkerd. . Hi, @cian-k just checking in to see if you have resolved the issue of maxInitialSizeKB. I will close the issue in a couple of days. Feel free to reopening it when you run into this again.. @mjschmidt we have a few places where you can find helpful documentation on how to get started with Linkerd in k8s. I highly recommend going the blog series A Service Mesh for Kubernetes. The blog series has detailed instructions on how to use the different features Linkerd provides in Kubernetes. \nAnother great resource is the linkerd k8s examples repo which contains a variety of YAML configurations you can use to set up Linkerd in k8s.\nTo answer your questions:\n1. You can set up your application to send traffic through Linkerd by setting an http_proxy environment variable for your containers in you application.yaml. Here is an example of how to do that. This allows you not to have to change your application code to send traffic to Linkerd.\n\nThe host/authority label is an HTTP header set to the value immediately after the http:// protocol section of a URL. HTTP/2, uses an :authority header instead of Host. The :authority header is set to the hostname of an HTTP/2 server. \n\nLinkerd's default identification behavior is to read the Host or :authority header of either HTTP/1.1 and HTTP/2  requests to figure out what service names it will use to route incoming requests to their destination servers.\nThere is documentation that dives deeper into how Linkerd routes requests here. Thanks for filing this @leozc, Let us know if you run into the issue after settings change.. That is strange @leozc, you should be seeing a lookup for a non-existent DNS to always be called based on the refreshInterval configured for the DNS Namer. The Lookup is always executed and is not dependent on the results of the DNS query. One way to verify that the namer is periodically running the lookup is to view the DNS namer stats on admin/metrics.json and look for the lookup_failures_total field. That total should be always increasing.. Thanks for filing this @rogoman! If you are up for it, we would be happy to review a PR with the change you described above? We love receiving PRs from the community.. Thanks for submitting this @rogoman! We will take a look and recommend any suggestions we have.. You are all set @rogoman I will merge this in right now.. Thanks for filing this issue @hsmade and thanks for the detail repro steps! We will take a look.. @DevoRia that is more a command line error rather than an error in linkerd. Check the current directory you are in to make sure the linkerd-1.4.6-exec binary is present.. Also, do you mind describing how you have your environment set up? i.e. Are you using docker? is this command run in a docker container? Are you just running this locally? From my understanding, you do not need to add sudo to your the linkerd-exec binary.. Thanks for submitting this @utrack! And thank you for the concise reproduction scenario. We will dig into this.. Possibly, or it could be in the stream cancelation management for gRPC connections. Hard to tell until we examine  http2 connection frames on both sides of Linkerd.\nRunning Linkerd and Namerd locally is easy. You can run Namerd backed but a io.l5d.fs namer and then run Linkerd locally to connect to Namerd. This can be done through the ./sbt command.\nIn your Linkerd directory, run ./sbt namerd-examples/all:run to run a Namerd instance with a filesystem namer . The configuration file is located at /linkerd/namerd/examples/all.yaml. To run a Linkerd instance that talks to a Namerd instance on localhost:4180, run ./sbt linkerd-examples/namerd:run. The configuration file associated with that command is at /linkerd/linkerd/examples/namerd-http.yaml.. @utrack an update on what the team has discussed. We believe this may be behaving as intended if retries are configured on the HTTP/2 router. Linkerd catches H2 errors it receives from the downstream services and modifies the response to add error information to H2 response headers.  Which is why you see Unable to route request!. The error is generated by  ErrorReseter.scala in the h2 protocol module.. @zackangelo Sorry, there hasn't been much progress on this issue but, I wanted to check in and see if there is still a need for this feature. Are you still running into issues were you are seeing stale Dtabs in Namerd?. @zackangelo Thanks for filing this issue. It looks like that diag trace may give us a clue a where linkerd might be inappropriately sending a Reset.Cancel. We will dig into this.. @zackangelo, @kleimkuhler and I are working on this issue and we have some quick questions. Do you mind sharing the scenario in which Linkerd's prometheus endpoint records rt:server:exn:io_netty_handler_codec_DecoderException? Is this caused by invalid HTTP/HTTP2 request reaching Linkerd on the server-side? We tried sending an HTTP request to LInkerd with a malformed HTTP request but didn't get any exn lines. We did, however, get Linkerd to display exceptions returned by a downstream service and have that show up as failures in the Prometheus endpoint. . @utrack thanks for filing this. By default, linkerd has no limit to the number of streams it creates per connection. https://api.linkerd.io/1.4.6/linkerd/index.html#http-2-server-parameters. Out of curiosity, how do you have your H2 router configured?. @peterfroehlich thanks for letting us no, are you able to provide watch states for your namerd instances that are running into this issue?. @peterfich this is great! Thanks for giving us the info. We will look into this.. It also looks like the CI job is failing on some merge strategy issues:\nbash\n[error] 3 errors were encountered during merge\njava.lang.RuntimeException: deduplicate: different file contents found in the following:\n/root/.ivy2/cache/org.slf4j/slf4j-simple/jars/slf4j-simple-1.7.21.jar:org/slf4j/impl/StaticLoggerBinder.class\n/root/.ivy2/cache/org.slf4j/slf4j-log4j12/jars/slf4j-log4j12-1.7.5.jar:org/slf4j/impl/StaticLoggerBinder.class\ndeduplicate: different file contents found in the following:\n/root/.ivy2/cache/org.slf4j/slf4j-simple/jars/slf4j-simple-1.7.21.jar:org/slf4j/impl/StaticMDCBinder.class\n/root/.ivy2/cache/org.slf4j/slf4j-log4j12/jars/slf4j-log4j12-1.7.5.jar:org/slf4j/impl/StaticMDCBinder.class\ndeduplicate: different file contents found in the following:\n/root/.ivy2/cache/org.slf4j/slf4j-simple/jars/slf4j-simple-1.7.21.jar:org/slf4j/impl/StaticMarkerBinder.class\n/root/.ivy2/cache/org.slf4j/slf4j-log4j12/jars/slf4j-log4j12-1.7.5.jar:org/slf4j/impl/StaticMarkerBinder.class\n    at sbtassembly.Assembly$.applyStrategies(Assembly.scala:140)\n    at sbtassembly.Assembly$.x$1$lzycompute$1(Assembly.scala:25)\n    at sbtassembly.Assembly$.x$1$1(Assembly.scala:23)\n    at sbtassembly.Assembly$.stratMapping$lzycompute$1(Assembly.scala:23)\n    at sbtassembly.Assembly$.stratMapping$1(Assembly.scala:23)\n    at sbtassembly.Assembly$.inputs$lzycompute$1(Assembly.scala:67)\n    at sbtassembly.Assembly$.inputs$1(Assembly.scala:57)\n    at sbtassembly.Assembly$.apply(Assembly.scala:83)\n    at sbtassembly.Assembly$$anonfun$assemblyTask$1.apply(Assembly.scala:241)\n    at sbtassembly.Assembly$$anonfun$assemblyTask$1.apply(Assembly.scala:238)\n    at scala.Function1$$anonfun$compose$1.apply(Function1.scala:47)\n    at sbt.$tilde$greater$$anonfun$$u2219$1.apply(TypeFunctions.scala:40)\n    at sbt.std.Transform$$anon$4.work(System.scala:63)\n    at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:228)\n    at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:228)\n    at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:17)\n    at sbt.Execute.work(Execute.scala:237)\n    at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:228)\n    at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:228)\n    at sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:159)\n    at sbt.CompletionService$$anon$2.call(CompletionService.scala:28)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:748)\n[error] (linkerd/bundle:assembly) deduplicate: different file contents found in the following:\n[error] /root/.ivy2/cache/org.slf4j/slf4j-simple/jars/slf4j-simple-1.7.21.jar:org/slf4j/impl/StaticLoggerBinder.class\n[error] /root/.ivy2/cache/org.slf4j/slf4j-log4j12/jars/slf4j-log4j12-1.7.5.jar:org/slf4j/impl/StaticLoggerBinder.class\n[error] deduplicate: different file contents found in the following:\n[error] /root/.ivy2/cache/org.slf4j/slf4j-simple/jars/slf4j-simple-1.7.21.jar:org/slf4j/impl/StaticMDCBinder.class\n[error] /root/.ivy2/cache/org.slf4j/slf4j-log4j12/jars/slf4j-log4j12-1.7.5.jar:org/slf4j/impl/StaticMDCBinder.class\n[error] deduplicate: different file contents found in the following:\n[error] /root/.ivy2/cache/org.slf4j/slf4j-simple/jars/slf4j-simple-1.7.21.jar:org/slf4j/impl/StaticMarkerBinder.class\n[error] /root/.ivy2/cache/org.slf4j/slf4j-log4j12/jars/slf4j-log4j12-1.7.5.jar:org/slf4j/impl/StaticMarkerBinder.class\n[error] Total time: 111 s, completed Sep 20, 2018 6:12:31 PM. @Bachmann1234 it's been a while so can't really remember. But I would assume opening up the PR and testing it out to see if the current code fixes the issue would be worthwhile. Are you interested in working on it?. /cc @zackangelo . Tests were done using a consul agent that contained a dtab in consul's kv store. Requests were sent periodically through Linkerd as dtabs in consul were updated. Requests were routed correctly on every dtab change.. Yea this is tricky. I also do not like the idea of checking the human-readable error from the 500 response and using that info to determine what state the namer should be in. I am in favor of reverting this change back so that all 500 responses always use the last known good state and always retry.  \n1863 was meant to fix a case where nonexistent dcs would never resolve the namer state. It feels as if that is an edge case scenario and that more often than not a dc should always exist. . @antonmatsiuk I created a new [https://github.com/linkerd/linkerd2/issues/1791] in the linkerd2 that is a direct copy of this one. Please feel free to comment on that issue. I am going to go ahead and close this one.. Hi @robertpanzer, we think we have a good first POC for rate limiting. The rate limiting functionality has been build as a separate plugin and can be found here. In order to get this working we had to move the requestAuthorizers config section to the client portion of the Linkerd config.\nTo test it the plugin, checkout the rate limiter plugin code and then build it using sbt like so:\nbash\n./sbt rate-limiter:assembly\nThen, when running Linkerd, you would need to ensure that there is a plugins directory alongside Linkerd and that you set an L5D_HOME environment variable to point to that plugins directory. The directory should contain a rate-limiter jar that is produced after the command specified above completes.\nIn your Linkerd config file, you can specify a rate limit for a client like so:\nyaml\n...\nclient:\nkind: io.l5d.static\n- prefix: /svc/<service_name>\nconfigs:\n  rateLimiter: \n    limit: 100\n    intervalSecs: 1\nFinally running everything together would look like this:\nbash\n$ cp /path/to/ratelimiter/plugin/target/scala-2.12/rate-limiter-assembly-0.1.0-SNAPSHOT.jar plugins/\n$ L5D_HOME=$(pwd) ./linkerd-1.5.1-SNAPSHOT-exec basic.config.yml\nWe would love to hear any feedback you may have and if this fits your use case.\n. Thanks for opening this issue @bansalrajat. Linkerd is proxy for managing service-to-service communication between microservices and can't serve static files like a web server. Since this issue was more of a question rather than a bug or feature request, I am going to close this issue for now. If you have any more additional questions, please feel free to join our Linkerd Slack where you will find a friendly Linkerd maintainer ready to answer your questions \ud83d\ude04.. Code to disable logger programmatically thanks to @adleong \n```scala\ndiff --git a/linkerd/main/src/main/scala/io/buoyant/linkerd/Main.scala b/linkerd/main/src/main/scala/io/buoyant/linkerd/Main.scala\nindex 056605ee5..9a0f86db9 100644\n--- a/linkerd/main/src/main/scala/io/buoyant/linkerd/Main.scala\n+++ b/linkerd/main/src/main/scala/io/buoyant/linkerd/Main.scala\n@@ -1,6 +1,7 @@\n package io.buoyant.linkerd\nimport com.twitter.finagle.Path\n+import com.twitter.logging.{Level, Logger}\n import com.twitter.util._\n import io.buoyant.admin.{App, Build}\n import io.buoyant.linkerd.admin.LinkerdAdmin\n@@ -33,6 +34,9 @@ object Main extends App {\n     val build = Build.load(\"/io/buoyant/linkerd/build.properties\")\n     log.info(\"linkerd %s (rev=%s) built at %s\", build.version, build.revision, build.name)\n\n// TODO: comment\nLogger.get(\"com.twitter.finagle.http.codec.ResponseConformanceFilter$\").setLevel(Level.OFF)\n+\n     args match {\n       case Array(path) =>\n         val config = loadLinker(path)\n``\n. @thedebugger looking into this a little bit I see that namerd does retry if the watch request receives a 4xx response. This is based on [this](https://github.com/linkerd/linkerd/pull/1744) PR which should be in 1.3.4. With the issue that you are seeing, does the API respond with 404 sometime during the watch stream? i.e. after receiving an initial response of HTTP 200, do you see the apiserver responding with 404 during the stream? . @thedebugger, I've added some additional log lines to for each dtab watch event and can be viewed here in this PR #2192. I looked through the code once more to make sure I wasn't missing something in regards to Namerd not restarting watches and it looks like even in situations where we get a 404 and Namerd is issuing arestartWatch, the API still makes a call to the apiserver to make sure that the resource is in sync. Here is the [specific line that does that](https://github.com/linkerd/linkerd/blob/master/k8s/src/main/scala/io/buoyant/k8s/Watchable.scala#L242). Let me know if you have any more info.. @thedebugger, I looked into this more and I think you were correct that there is indeed some weird behavior going on during therestartWatchesmethod. It looked like during, restartWatch, namerd would silently fail and stop watching the apiserver. I will put a PR for you to review and see if that fixes your issue.. @thedebugger I created a PR #2192  for this and thought that might fix this issue, however, the tests I ran didn't give me the confidence to ship it. When running the tests, receiving a 404 initially would cause Namerd to logk8s API request for Dtab list returned 404!` and any other times namerd would receive a 404, the watch would be restarted. Let me know if there is something I am missing or if you have additional info on reproduction steps.. fixed by PR #2169 . > This looks correct to me. Have you been able to test it? I recommend writing a simple request authorizer plugin the just logs something and making sure you can load it and that it runs on each request.\n\nI was able to start Linkerd and observe the /config.json endpoint. It looked like it was able to load just fine. Writing a plugin and testing it is a good idea. I will try that out.. Good point, I will not that down for the next release.. @jreichhold, might sound silly to ask you this for a simple PR but do you mind adding some more information on the PR that describes the problem you are saying and outlining how your change fixes that problem. This CONTRIBUTING.md is the guideline we use for PRs. . Managed to track this down and found out that this memory increase is caused by this netty issue and has been fixed since Netty 4.1.31. The finagle repo also has an issue that reported a similar problem here and they also have recently picked up the Netty fix.\nIt looks like the most recent version of Finagle (18.12.0) corrects this increased non-heap memory and upgrading Linkerd to use Finagle 18.12.0 will ultimately fix this issue. Yay to library dependencies \ud83d\ude05!\nIssue references:\nhttps://github.com/twitter/finagle/issues/738\nhttps://github.com/netty/netty/issues/8159. Can confirm that the finagle upgrade fixes this issue. Running a docker image of this PR #2194 yields this native memory summary:\n```bash\nPrinting VM memory summary...\n1:\nNative Memory Tracking:\nTotal: reserved=2290251KB, committed=1200727KB\n-                 Java Heap (reserved=1048576KB, committed=1048576KB)\n                            (mmap: reserved=1048576KB, committed=1048576KB)\n\n\nClass (reserved=1092594KB, committed=50454KB)\n                            (classes #8180)\n                            (malloc=1010KB #7916)\n                            (mmap: reserved=1091584KB, committed=49444KB)\n\n\nThread (reserved=24777KB, committed=24777KB)\n                            (thread #25)\n                            (stack: reserved=24672KB, committed=24672KB)\n                            (malloc=77KB #125)\n                            (arena=28KB #48)\n\n\nCode (reserved=50186KB, committed=2802KB)\n                            (malloc=266KB #1127)\n                            (mmap: reserved=49920KB, committed=2536KB)\n\n\nGC (reserved=8398KB, committed=8398KB)\n                            (malloc=4630KB #212)\n                            (mmap: reserved=3768KB, committed=3768KB)\n\n\nCompiler (reserved=169KB, committed=169KB)\n                            (malloc=39KB #105)\n                            (arena=130KB #2)\n\n\nInternal (reserved=5283KB, committed=5283KB)\n                            (malloc=5283KB #13480)\n\n\nSymbol (reserved=11712KB, committed=11712KB)\n                            (malloc=10266KB #80804)\n                            (arena=1447KB #1)\n\n\nNative Memory Tracking (reserved=1631KB, committed=1631KB)\n                            (malloc=7KB #82)\n                            (tracking overhead=1624KB)\n\n\nArena Chunk (reserved=185KB, committed=185KB)\n                            (malloc=185KB)\n\n\nUnknown (reserved=46740KB, committed=46740KB)\n                            (mmap: reserved=46740KB, committed=46740KB)\n. @adleong I added `socket-options.yaml` to `linkerd/examples`. Thanks for the suggestion!. Keeping this open until version 18.12.0 comes out. Current master now has Finagle 18.11.0.. Running a linkerd docker image with this change displays this error:bash\n$ docker run -p 9990:9990 -v pwd/basic.config.yml:/config.yaml -v pwd/disco:/disco buoyantio/linkerd:1.5.2-SNAPSHOT /config.yaml\n/io.buoyant/linkerd/1.5.2-SNAPSHOT/bundle-exec: 15: /io.buoyant/linkerd/1.5.2-SNAPSHOT/bundle-exec: [[: not found\n``\nIt might the fact that you the docker image runs the script withshand notbash`.\n\n\nA quick search on the interwebs shows that you can do logical regex shell expressions with grep but I'm not sure if its the best way.. @edio, thanks for shedding light on this. Interesting that Consul still sends this message even when there is seemingly nothing wrong with the server. We could modify this so that it only returns Neg when the previous state is Pending.. Hi @chenhaibocode, there are tons of examples of how to run Linkerd in various configuration here. One example is running Linkerd with a few grpc apps. You can take a look at the Readme here https://github.com/linkerd/linkerd-examples/tree/master/docker/helloworld. I am going to close this since I am unable to reproduce the issue this PR is intended to fix. Trying to create a unit test also did not recreate the scenario. The issue was that if namerd tried to watch a dtab and the first call receives a 404 and then, after a retry receives a 404 again, Namerd will stop watching dtabs. However, during my tests, when namerd received a 404 for the first time from, the watch was unable to be established and a log line was displayed through this line of code\nhttps://github.com/linkerd/linkerd/blob/6702ab00c1baf025a7695dcab8208be6330d98f5/namerd/storage/k8s/src/main/scala/io/buoyant/namerd/storage/K8sDtabStore.scala#L67\nPlease add anymore context if there is something I am missing @thedebugger.. Ah, looks like I can't merge it without @ccmtaylor's approval. I was wondering why I couldn't see the big green button haha.. Thanks for filing this @edio. My understanding is that maxRequestKB and maxResponseKB were initially added to tame large requests from being sent to the router according to this PR https://github.com/linkerd/linkerd/pull/557. Having these configurations with streamAfterContentLengthKB seems a bit redundant, however, I do agree with you that these configs could be used to protect services from large requests.\nI did some digging and it looks like some if not all of these http configs in the past were discussed in some form. Issue #1327 further discusses the maxResponseKB and maxRequestKB params.. This investigation may result in the removal of maxRequestKB and maxResponseKB as config parameters and may be a breaking change in Linkerd just as a heads up.. Thanks for filing this @chenhaibocode, this is strange since you have everything wired up correctly and that logical path results in a (neg). We will dig into this.. Thanks for filing this @edio! Should be easy to get a PR up for this. We will take a look at this soon.. \ud83d\udc4b @chrismikehogan, Thanks for filing this. FWIW, there is a config parameter called clearContext that can be set at the router level of the config. It's a server side configuration so you won't have the ability to dynamically turn it on or off per request.. Ah, that's right, it also gets applied to inbound requests as well so that wouldn't work.. Thinking about this a little bit more, do you think it would be better to mimic what Linkerd does to the l5d-err header? So instead of sending back the entire dtab, Linkerd would truncate the dtab sent in the body if it is more than a specific number of lines? \nThe config parameter could definitely help but I feel that adding more implicit behavior through a header might increase the complexity of Linkerd's per-request handler.  I am thinking that an error response with that much verbosity might not be as useful anyway and so it might be better to just truncate it. WDYT?\n/cc @adleong . Fixed by #2229 . @mrezaei00 there is a PR that fixes the missing forward slash issue. However, I was unable to reproduce the issue of JAVA_HOME not having the path to the java binary at runtime. I tried starting up a linkerd docker image with JAVA_HOME set to nothing and it picked up the default path of /usr/bin/java. . Thanks for filing this issue @waxie! We'll take a look and have this fixed.. Fixed by #2232 . Yep, I'll add the two external contributors we got for a few of the commits in this release.. Thanks for pointing that out. While working on this the assumption I worked\nwith was that the client side would ping Namerd. But it shouldn\u2019t be\ndifficult for Namerd to do that as well.\nOn Mon, Feb 11, 2019 at 9:14 PM Sumit Vij notifications@github.com wrote:\n\nI just want to add that both client (linkerd) and server (namerd) should\nsupport enabling the ping frame\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/linkerd/linkerd/issues/2219#issuecomment-462619085,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACGGcHz2bWScUn8e_JE6fluspAcwsonUks5vMk21gaJpZM4al422\n.\n. @chrisgoffinet @thedebugger #2221 is a working PR that implements H2 PINGs and implements a failure detector. . I looked into this some more and it looks like there might be some behavior in theSingletonPool module that forces the service associated with 127.0.0.1:4322 to always stay open because the pool ensures that there is a most one service that is available for that connection. I will investigate further to see why that is the case.. @chrisgoffinet @thedebugger have you been able to test out this branch? What are your thoughts on the current implementation for failure detection between Linkerd and Namerd connections? RIght now, if the failure detector on the client side is triggered, the client will terminate the connection and establish a new one. WDYT?. @adw12382 Namerd exposes JVM flags through a LOCAL_JVM_OPTIONS environment variable. You could add whatever GC configuration you are interested through this variable and Namerd should pick it up.. Interesting @adw12382, are the sharp cliffs caused by GC or is that Namerd being restarted? We should do more memory analysis over time and see what might be the cause.. @thedebugger, We don't have a build for it out, let us know if you'd like us to upload that image to docker.. @thedebugger wanted to circle back and check to see if this PR fixes the issue you were seeing.. Hi @yeyincai, does @JustinVenus' suggestion help out in anyway?. That's interesting, do you mind sharing how you solved the issue?. That's pretty neat! Thanks for the explanation. I am glad to know you were able to get that working. I am going to go ahead and mark this as closed then.. @trungnvfet, do you mind resolving the merge conflicts? It looks like the conflicts are a result of the other doc PRs that made it onto master.. @trungnvfet do you mind resolving the merge conflict after that this is ready to merge.. Thanks for submitting this, this is a great idea @huynq0911. We have a PR template described in CONTRIBUTING.md that describes how PRs should be formatted. I think it would be good to follow that format.. @truongnh1992 is the .swp file being inadvertently added to your Linkerd commits? Is there anyway you can include this file in your global .gitignore file?. Thanks for the detailed issue @zackangelo, we will look into the findings you posted and investigate.. Alright will do, thanks\n. Thats a good point. I was thinking that we calling it K8sCrdConfig would give details about the underlying implementation of the k8s storage plugin but now that I think of it. This is an open source project. :facepalm: . Good point. I will be sure to add that.\n. @adleong you mentioned documentation, Is that in the Readme?. In terms of manual testing, I was using namerctl and minikube using both TPRs and CRDs. I mainly tested whether an update and create was successful. I am going to test delete to see if it works as well but I am confident that it will.. I added the Deprecation Notice as a way to notify people of the change so that they could migrate their DTabs if they were using K8s 1.7. I can remove this if it is no longer necessary.. We may have to make the distinction on that line, something like \"for Kubernetes <= v1.7\". Hopefully it shouldn't be too confusing.. ahh yes, I can fix that\n. This was probably my fault and happened when I added the CustomResourceDefinition changes, sorry \ud83d\ude28 . I can definitely include what characters are allowed in the docs and possibly in the log message. . I can do that, wasn't aware scala had its own regex lib. I can tests for that functionality. I was under the impression that we were trying to get this out the door by today so I just wrote basic tests for the ConsulDTabStore. oops thought I deleted it. thanks. So this change was not related to what you fixed, was it intentional? I am unfamiliar with what the styleguide is for match statements so what you have might be right.. Would love a mini tutorial to see how this works with your unit tests!. Just for my curiosity, Is there a case where we wouldn't want to update the state when a HTTP 500 error occurs? If we received a 500 other than an invalid datacenter, should we update the state? I briefly looked through the consul agent source code and one error that might return a 500 is errRPCRateExceeded. Which I assume is rate limiting. I would think we would want to retry and not update the state to Addr.Neg since a subsequent request would generally end up being a HTTP 200. I may be totally wrong though.\n\nRegardless, I understand that updating the Addr state to Neg is not permanent and eventually it would be updated to Ok once we get a good value from consul. . I believe this passes the settingsFrame forward to the remote so that it can deal with the remote can deal with the frame accordingly. When working through this change, the Netty4ServerDispatcher would receive this frame and then throw an exception because saw it as unexpected. I checked to see if the same behavior happened before the Netty upgrade and it seemed to work fine. I believe the new netty version reworked the class hierarchy so that the settingsFrame was no longer a Http2StreamFrame and made it as a subclass of Http2Frame. . This is still an issue, I ran into this problem and when testing the H1 to H2 upgrade pipeline. The issue referenced above is still open and the only way to get around it is through the workaround used in the here. Thanks! Will get that fixed!\n. This is necessary because the Java API stores generic values in the connection.stream map we have to do the type cast to get Http2FrameStream. . This sounds like a good idea, I am not familiar with how this would work but I can try it out and see what that is like.. I didn't quite pin down where exactly this behavior changed. But I can work on that to figure out exactly where.. That's a great idea! I could just change writer.reset to accept only and id and just generate a stream object that is in the CLOSED state. My only concern is that the method signature in NettyH2Writer would look different than i.e. writer.reset only accepts an id while write(...) accepts a H2FrameStream and other parameters.. sure!, Actually, I thought this tests were no longer needed since we remove the ServerDtabContextFilter and ClientDtabContextFilter. I was under the impression that these assertions were testing that filters were essentially testing Finagle server functionality rather than Linkerd.. and sure!. I am trying to understand this comment. How do HostConnectionPool's idleTimeMs differ from ClientSession's idleTimeMs? Do they effectively do the same thing? If they do, then we may no longer need to have this setting in HostConnectionPool.. I may be wrong, but maybe we could add a test that exercises both hostConnectionPool and clientSession configs. I am still unclear of how the two different kinds of configurations would work together if that ever is the case. . Agreed! OTTOH that may be more of a challenge given that its difficult to identify what is a legit retry error vs an innocuous error. The consul API is not really explicit in its API errors . Ahh I see, thanks for the clarification!. I don't recall but was this a result of scalariform?. This InstrumentedVar looks pretty neat. Is this something twitter util already has? If not, would this be useful to submit a PR to finagle?. This value looks like it contains a lot of information that isn't easily recognizable at first glance. Do you think it would be helpful for this to be a class of some sort?. Good point, I can try that out.. That is correct, thanks for catching that. It is definitely redundant to have to test for this all over again.. So, I tried to omit the types but Scala infers that the type is com.twitter.finagle.Stackable[com.twitter.finagle.ServiceFactory[Nothing,Nothing]]. I tried to mess with the module method definition to add upper type bounds but it was not successful. The Request and Responsetype classes for HTTP/1 and HTTP/2 do not share a parent class.. I noticed here that the type forinactiveTTLSecswasLongbut in theThriftIntepretererConfig, the type forinactiveTTLSecsis of typeOption[Int]. Should these two variables be of the same base type? As inLongforinactiveTTLSSecsinObserverCacheandOption[Long]forinactiveTTLSecsinThriftIntepreterInterfaceConfig?. Out of curiosity, what is the motivation for having the binding cache TTL set to a week?. I think it would be good add a test that checks to make sure an item is removed from the inactive cache once the TTL timer is elapsed.. Just made the change and looks like it is possible. \ud83d\udc4d . Good point, I could add information that points to the link to the client name. Hopefully, that is available in the admin endpoint.. I am not sure of how the logs looked with this toString method. Could there have been some way to edit the toString method to log more useful info?. That is correct. I will make update the config example.. A strange thing encountered was that the client name in the UI did not have a beginning forward slash. The user that reported this issue mentioned that they added a prefix that began with%/. From my understanding, it should start with the%/but I am not entirely sure.. In the admin UI, the port number came after the fs namer prefix. It might be that I did not set up the config properly.. IIUC does this mean a stream cannot receive a headers frame in the middle of a stream? i.e. If the initial headers frame was sent and then a bunch of data frames was sent after that followed by another header. Would this reset the stream? Is that considered correct behavior? Asking because I do not quite understand the rules of Http/2.. please remove spaces and alphasort imports. I am wondering, what catches this exception when all other cases fail for this particular match statement? Is there additional information we can give to theDynBoundTimeoutexce[topm so that the object that eventually catches it can log with more information?. What is the difference betweenRequestTimeoutExceptionandDynBoundTimeoutException?. It looks like there is missing information in the comment. I understand if fixing this isn't a priority since it's related to this PR.. NIT: noticed that this is in the style ofif...elsebutbindandresolvemethods use pattern matching.. Well, since the new implementation won't rely on JSON, we no longer need the case class, and I have changed it to a method.. That makes sense. I will add that.. Thanks for finding that. I will remove it.. TheFuture[Addr]is pretty neat! Using this made it much easier to read the code IMO.. You're right. I think I could also haveAddr.Bound...be the first case. I could also condensecase Addr.Neg | Addr.Failed(_) | Addr.Pendingto be the default value forlbSet. That way, we no longer need strict type matching used on line 67.. I think I hadTry...because I couldn't figure out how to get the client name without using theDelegateTree. With your help, I was able to get the client name from theDstBoundCtx.. You're right. I mistook theContent-Typeheader for theAcceptheader.. I changed the recursive code a little bit to be clearer. I also removed the use ofOption.. Yep. I will make the change.. Agreed this is much clearer.. Makes sense. Will make that change.. Good point.. I liked the name you mentioned earlier, RequestTracer. We could call it RequestActiveTracer (RAT haha) so we don't confuse it with passive tracing.. That's right.treeis an accumulator and eventually will represent the path of thedTreethat was are searching. I will flesh out the scaladocs for this method to highlight that and give descriptions to the param.. When we are building out the path, and we get to a leaf. We need to check to see if the parent of the leaf we are evaluating is a transformation. If it is a transformation, we switch the set theDentryof the transformation to be theDentryof the leaf and then, set theDentryof the leaf to theDentryof the transformation. I think your suggestion makes it clearer. Separating the creation of the node from the modification of the tree path is a good call.. Yea. It doesn't make sense for this to be 200.. Yea I thought about that as well. Could we run into a scenario were we have Linkerd set up in a linker to linker fashion and when a request with noMax-Forwardscould eventually make the request hang in a linker to linker loop? I guess the timeout exception might make this a non-issue.. Darn, I thought I renamed that variable. using the underscore is even better..StringUtil.toBooleanreturns false by default if the string being converted is not \"1\", \n\"t\" or \"true\". I am thinking maybereq.headerMap.get(AddRouterContextHeader).map(StringUtil.toBoolean)should suffice?. Yea,Max-Forwardsis the default header associated with theTRACEmethod according to the spec. The header is what proxies use to figure out if they should forward a trace request or just reflect it back to the client.. Yea, I was worried about using\\nwhich excludes Windows newline format. Hopefully formatting the string the way I did takes care of that issue. Or maybe, it might not be too much of a concern.. Nice! I will use that instead.. I was under the impression that the Status code would suffice, but I do see the point of being consistent. Also, it's useful information.. Ahh! good point.. I was futureproofing this functionality. I reason that there are cases where we may want a view ofRouterContextand not necessarily represent that as a string. That way, you can pass around aRouterContextobject to do whatever work you may need to do.. Yea, I can see that. I realize I can also see that I could move the contents of theapplymethod from RouterContextBuilder toActiveTracer. Darn, well, I will go withs\"\\n${ctx.formatRouterContext.trim}\\n\"anyways. Unused import \"import io.buoyant.namer.DelegateTree._\". It might have been left over from the initial commit of this code.. Is this suppose to be \"argument\" or \"augment\"?. This code is very similar to [formatCtx](https://github.com/linkerd/linkerd/pull/1988/files#diff-7f2b008c320604fb7424f5702567777cR16). Might be a good idea to DRY up some of the code likeval resolutionorval addresses. I don't see this as a PR blocker. . According to [RFC 7230, section 3.3.1: Transfer-Encoding](https://tools.ietf.org/html/rfc7230#section-3.3.1), Transfer-Encoding header values can be a comma separated string. Is it possible we may run into a value likechunked, identity? If so, this may not satisfy the condition. I am not entirely sure ifchunked, identityis an actual field value that would be generated by an Http/1.0 client.. Sincearg.statusthrows an exception instead of returningnullbased on thearg.status` definition\nscala\noverride def status = headers.get(Headers.Status).headOption match {\n      case Some(code) => Status.fromCode(code.toInt)\n      case None => throw new IllegalArgumentException(s\"missing ${Headers.Status} header\")\n    }\nWould it work to do this instead?\nscala\ndef unapply(arg: Response): Option[Status] =\n    Some(arg.status). I think this can be shortened to:\nscala\n   case Reset.Refused => true. What is the reason for adding Protocol.http % \"e2e\", Interpreter.namerd % \"e2e\", Interpreter.perHost % \"e2e\"? Where this tests that weren't added to the End to End test suite?. Yea, that is much more straightforward. Thanks for the suggestion!. unused import. unused import. Do we no longer need the Header to be marked with private[this]?. unused import on Line 5.. Should we update addSbtPlugin(\"com.twitter\" %% \"scrooge-sbt-plugin\" % \"18.4.0\") in plugin.sbt as well?. I am currently working on the k8s version of this implementation and was thinking about how the endpoint would look like in the k8s scenario. I like the /storage/dtab.json. What about including the storage type as well in the link like /storage/io.l5d.consul/dtabs.json or /storage/consul/dtabs.json?. Ahh I see, I was under the impression that you could have multiple storage modules in namerd.. s/NilTracePropagagtor/NilTracePropagator. Unused import InstrumentedActivity. Is this supposed to read as chunked maybe?. That's a good point. I can make that change. . whoops! good catch!. Yea, you are right! I got mixed up with the first sentence in description of resp.setChunked(...)\nscala\n/**\n   * Manipulate the `Message` content mode.\n   *\n   * If `chunked` is `true`, any existing content will be discarded and further attempts\n   * to manipulate the synchronous content will result in an `IllegalStateException`.\n   *\n   * If `chunked` is `false`, the synchronous content methods will become available\n   * and the `Reader`/`Writer` of the message will be ignored by Finagle.\n   */\nI will add that in.. nit: you probably could format the class definition for readability to something like:\nscala\nclass Admin(\n  val address: InetSocketAddress,\n  tlsCfg: Option[TlsServerConfig],\n  workers: Int, \n  stats: StatsReceiver, \n  securityConfig: Option[AdminSecurityConfig]) {\n.... Should we add a test that validates that we are receiving a StatsReceiver in an identifier to show that it propagates through initialization?. Is this still considered a state of the stream or is it now just functioning as a buffer? Maybe the comments need to be updated?. Uh...I have no clue either, I think it may have been a newline character but I reverted that line so it could match master. \ud83e\udd37\u200d\u2642\ufe0f . Good catch! thanks.. I needed to move this because I was running into an issue where adding the admin module as a dependency to the marathon module was causing some sort of circular dependency issue at build time. It returned a NPE when trying to build marathon.. This was set up to match the handlerPrefix in the ConsulNamer. I can add the comment that describes why this is needed.. Thanks, I will add that.. That is much cleaner actually, Thanks!. Good point, my initial thinking was that this needed to be a happens once event but I can see that calling the .getAndSet method is too early and doesn't align with the description of the method.. @hawkw. I tried adding this LinkerdBuild.scala, but a bash script was easier. I can continue researching on how to do this through sbt and create a separate PR.. Thanks, I think I ran into this while running my tests on k8s but couldn't figure out what was causing it. This change fixed it.. scala's map function on the Option can be pretty handy in these situations. You can achieve the same Option[Int] method result like this:\nscala\ndef unapply(healthChecks: Option[Seq[HealthCheck]]): Option[Int] = healthChecks.map(_.size). I think we can leave this method largely unchanged if we incorporate the number of health checks into the Task case class.. Kudos for writing unit tests! \ud83d\ude04 . Ahh, it looks like we don't. I thought that since this was a function of type Unit we would need to capture the expression in a dummy variable to avoid the discarded Unit error. TIL.. This line creates a var/log/linkerd directory if it doesn't exist.. Good point. I'll get that set up in the next commit.. Yea. When running this as a non-root user the JVM gives a permission denied error when trying to create logs. Running this in docker works just fine since docker runs the process as root. I can add a warning message in the script.. To be consistent with other deserializers, should this be:\nscala\noverride def deserialize(jp: JsonParser, ctxt: DeserializationContext): Directory = {\n       Directory(Paths.get(_parseString(jp, ctxt)))\n}. From reading the finagle docs. I am not really sure what this parameter is actually doing. Would it be helpful to add a comment as to why this is set to true?. Could we still keep the strict type for iface as ThriftService using import com.twitter.finagle.thrift.ThriftService. Is there a reason why this should be AnyRef.. Yea, good point. I was running through a couple of different versions to see what names would describe the options well. I do agree that disable- makes the word more negative.. According to the docs in Finagle's Transport.scala it looks like if it is set to true it is disabled and enabled if it is false.\n/**\n   * $param the options (i.e., socket options) of a `Transport`.\n   *\n   * @param noDelay enables or disables `TCP_NODELAY` (Nagle's algorithm)\n   *                option on a transport socket (`noDelay = true` means\n   *                disabled). Default is `true` (disabled).\n   *\n   * @param reuseAddr enables or disables `SO_REUSEADDR` option on a\n   *                  transport socket. Default is `true`.\n   *\n   * @param reusePort enables or disables `SO_REUSEPORT` option on a\n   *                  transport socket (Linux 3.9+ only). This option is only\n   *                  available when using finagle-netty4 and native epoll support\n   *                  is enabled. Default is `false`.\n   */. Curious to know how we came about this number? Is this the optimal number that gives us the best latency when writing messages?. Does scheduleFlush() need to happen when writeQueue has reached MaxFlushSize of 128? Does this need to be guarded against a condition?. Curious as to why we need to flush at least once? Is this true in the case where we do not have any messages to write?. Wanted to make a note about the the line touch \"$GC_LOG/gc.log\". It looks like even mkdir -p $GC_LOG will have an exit status of 0 if the directory already exists and has strict permissions. We actually need to do an additional check to see if our process can create a file in the directory since that is what the JVM will do when its writing logs to that directory. . Yea, I think that could work.. What happens in the case where f.IsEnd == false? Do we need to do anything special or is that an illegal state?. Ahh, the \"Dr. Strange\" package, is there an example file I can look at to see how this works?. I am curious about this case, Does this assume that we can receive frames from streams that are considered closed in Linkerd's \"Stream registry\" but are still active on the remote peer? Or does this happen immediately after the remote stream is closed and the close frame just reached Linkerd? . Yea, that is correct. My dilemma was that I wanted to just use the observe method present in ConsulDtabStore, but my options were either to use the entire class or create a shared class that could be used by the consul interpreter and consul dtab storage. I didn't want to use the ConsulDtabStore class since it had other  CRUD operations for dtabs which I didn't want to include. I also had issues creating a shared class in that it would require me to restructure the dependencies for the consul, namer and linkerd modules. That would have caused a huge diff. I was thinking about following up with a separate PR that does the actual restructuring but probably the best bet is to just reuse ConsulDtabStore in the consul interpreter module.. I think we may need to include the @JsonDeserialize annotation since this is a long similar to classificationTimeoutMs. It would look something like this:\nscala\n @JsonDeserialize(contentAs = classOf[java.lang.Long])\n  var maxConcurrentStreamsPerConnection: Option[Long] = None. Yea that could work, but I wanted the default cause to always to be the storage/.... endpoint optionally set the URL if we wanted it changed. I guess since the ConsulDtabStore isn't used anywhere else it'd be fine to make it more explicit.. \ud83e\udd26\u200d\u2642\ufe0f Thanks\n. What do you think about using the word exceptions instead of exn? To me, it seems better to have it spelled out so that its descriptive.. Can this .sample() run into a race condition with the makeInner definition? Is that something we should be concerned about?. What happens if we let this Var.asnyc's init value be None?. Does this test exercise the code that once had the bug? Are we confident that if we reproduce the issue at the unit test level, we do not see the issue come up?. @kleimkuhler I think I understand what @adleong means when referring to how nested exceptions are recorded. I took a look at how the failure stats get generated for a Finagle HTTPServer. Check out this code in the Finagle code base. It looks like counter receives a list of exception class name strings and then is incremented. That means that your test would have something like:\nscala\nstats.scope(\"rt\", \"incoming\", \"service\", \"/#/foo\", \"failures\").counter(\"Exception1\", \"Exception2\", \"Exception3\").incr(). s/Unbounded/unbounded. I also think we should shorten the new options. For example, we could shorten readTimeoutSeconds to readTImeoutSecs to keep it consistent with other configuration field names.. Thanks for modifying the test with the new change! For completeness, I think we should also include writeTimeoutSeconds as well.. We no longer need this test since this tests the behavior of having a datacenter becoming unavailable. We don't expect this to happen at runtime.. Looking through the comments in #2145, the user mentioned that \n\nA DC doesn't usually cease to exist during runtime\n\nBased on that, I felt that the test wasn't necessary since this issue only happens on the initial call to resolve the logical. But since this can be considered edge case, it might be helpful to have this test.. Would it be possible to add ensime-langserver.log, pc.stdout.log and .ensime* to your .gitignore_global instead?. Any reason why we can't call this nonAccruableStatusCodes?. Do we need to specify a default value here? I think we might be able to override the val without having to specify Set.empty. . We may want to add in more details to this yaml so that it's easier to see where this new config value fits in. i.e.\nyaml\n    servers:\n      - port: 4140\n        ip: 0.0.0.0\n    service:\n      responseClassifier:\n        kind: io.l5d.h2.grpc.default\n        nonAccruableStatusCodes:\n        - 3\n        - 5. Ah, I see. That's fine. Whatever you feel makes more sense. Consider it a nit \ud83d\ude04 . Makes sense, thanks for clarifying. I think its fine the way you have it.. Oh awesome, glad this was fixed. \ud83d\ude04 . Super nit: I feel kinda silly even mentioning it \ud83d\ude02. I think we should undo the formatting that happened on this comment to keep it consistent with the other comments and reduce the diff for this.. I was in the middle of reviewing this and was about to comment about the reasoning behind some this logic but haven't gotten the chance to write up everything. A big part of why we are special casing for UnexpectedResponse is that there are cases where we do get a 5xx error but it's not one where we want to send Addr.Neg. \nAccording to issue #2145, Consul can send a 5xx error with a response body that does not contain No path to datacenter. In that issue, the user describes that resolution was successful prior to this error and we wanted to keep that state if we experienced intermittent failures. If we were to change this case, would this use case still hold true? In other words will we be able to keep state on intermittent 5xx errors?. TIOLI: Reworded this a little bit. If set, this config will adjust the backlog queue size of the socket, a default of None falls back to the OS defined value of SOMAXCONN. That's right. I think this must have been left over code. . Looking at this a little more, the done references the promise within the method and in this case, when we set the exception here, we are notifying the Failure detector that things went wrong.. Testing this out with the SIGSTOP scenario we discussed. Without this, the client side stops pinging but the connection is still ESTABLISHED. I am not really sure if there is some kind of cleanup layer in the stack that detects if a service is closed. But I think we may have to tear down the connection manually.. I think we need to remove this space and then sort the imports in alphabetical order.. In addition to @adleong's comment, I think we could also place the result of state.map in a val to help with readability.. The HTTPS link for this give me a warning saying that the link is not secure. I think we can leave this as HTTP for now.. Same with this one.. Let's also add the keepAlive option here. It doesn't hurt to have more documentation \ud83d\ude04 . ",
    "jacob-koren": "I uploaded the tcpdump in slack channel\n. Including linkerd-->service:\nhttps://linkerd.slack.com/files/jacobk/F1HBGUH6F/linkerd.pcap\n. Archive.zip\n. Added stats of about 10 minutes\n. Do you have the source? I need to build a docker image to use  in dc/os\n. Yes. It's fixed!\n. Thanks, Is the retry a default behavior? Because right now we do see failed requests. Hi, I tried the useHealthCheck option. It didn't solve the problem mainly because TASK_KILLING state is not considered unhealthy. After marathon send the sig TERM there were no additional health check calls.\nHere's an example of one of the tasks of the application that is KILLING state during the upgrade:\n\"tasks\": [\n   {\n     \"id\": \u201csomeid\u201d,\n     \"slaveId\": \u201csomeslaveid\u201d,\n     \"state\": \"TASK_KILLING\",\n     \"version\": \"2017-01-19T09:11:38.714Z\",\n     \"healthCheckResults\": [\n       {\n         \"alive\": true,\n         \"consecutiveFailures\": 0,\n         \"firstSuccess\": \"2017-01-19T09:11:44.272Z\",\n         \"lastFailure\": null,\n         \"lastSuccess\": \"2017-01-19T09:21:11.473Z\",\n         \"lastFailureCause\": null,\n       }\n     ]\n   },\nI'll continue with the retry and see if it's solves the issue . ",
    "garysweaver": "@wmorgan Thanks! We\u2019re looking into alternate solutions for now until this can be fixed.\n. ",
    "aclemmensen": "Couldn't you just add additional bindings to the site in IIS? Make both servers bind the site to the same name and it should work, right? Except if you can't control what Host header Linkerd will use when forwarding the requests as this will have to match the binding you are configuring.\n. ",
    "JonathanBennett": "Hey,\nI don't have a reason why zk would have failed to respond to a session. Rather annoyingly due to our log rotation in dev I don't have the logs for this as it looks to have been failing for quite a while so I'll be on the lookout to try and reproduce it; only thing I can think of is maybe ZK was underload and therefore failed to respond but I would imagine that as unlikely.\nIn such a case how should namerd recover? Should I be restarting namerd or should it be more resilient than this in failing from a zk timeout? From my logs I do have (about 4 days of failures over a weekend) I just see all lookups/sets failing with an error similar to the following:\nE 0701 01:47:17.364 THREAD25: binding name /http/1.1/GET/zeppelin.xxxxx.co\njava.lang.Exception: session expired\n    at com.twitter.finagle.serverset2.buoyant.ZkSession$$anonfun$watchedOperation$1$$anonfun$com$twitter$finagle$serverset2$buoyant$ZkSession$$anonfun$$loop$2$1$$anonfun$2.apply(ZkSession.scala:162)\n    at com.twitter.finagle.serverset2.buoyant.ZkSession$$anonfun$watchedOperation$1$$anonfun$com$twitter$finagle$serverset2$buoyant$ZkSession$$anonfun$$loop$2$1$$anonfun$2.apply(ZkSession.scala:142)\n    at com.twitter.util.Witness$$anon$17.notify(Event.scala:434)\n    at com.twitter.util.Var$$anon$3$$anonfun$register$1.apply(Var.scala:99)\n    at com.twitter.util.Var$$anon$3$$anonfun$register$1.apply(Var.scala:99)\n    at com.twitter.util.Var$Observer.publish(Var.scala:151)\n    at com.twitter.util.UpdatableVar$$anonfun$update$2.apply(Var.scala:474)\n    at com.twitter.util.UpdatableVar$$anonfun$update$2.apply(Var.scala:469)\n    at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:778)\n    at scala.collection.immutable.RedBlackTree$._foreachKey(RedBlackTree.scala:109)\n    at scala.collection.immutable.RedBlackTree$.foreachKey(RedBlackTree.scala:105)\n    at scala.collection.immutable.TreeSet.foreach(TreeSet.scala:154)\n    at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:777)\n    at com.twitter.util.UpdatableVar.update(Var.scala:469)\n    at com.twitter.finagle.serverset2.client.EventDeliveryThread$.run(EventDeliveryThread.scala:18)```\n. ",
    "pypeng": "Is there a Dockerfile for buoyantio/debian-32-bit ?\nWe have a problem running java using this image:\ndocker run -it --rm --entrypoint=java buoyantio/debian-32-bit\nError occurred during initialization of VM\njava/lang/NoClassDefFoundError: java/lang/Object\n. thx a lot.\nwe are still working on it. it seems to be a xfs/overlayfs/kernel(3.18) related problem.\n. ",
    "kmt": "First, thanks @olix0r for looking into this, highly appreciated!\nMy thoughts are that while \"Dtab Syntax\" perhaps partially helps our use case (well, at least outsources the parsing to a json parser, after which you'd just navigate a structure, modify a node and serialize again), the \"Dtab composition\", at least as described in @adleong 's RFC sounds like a different feature from what we need.\nTo be clear about our feature request: given that we'd be dealing with a large number of frequently deployed services, we'd like to be able to update only the part of the Dtab relevant to the service being currently deployed, rather than read/deserialize/modify/serialize/write the whole Dtab.  Is \"Dtab composition\", the way you envision it, going to address that?  If yes, we'd be happy, regardless of how exactly it's done technically.\n. ",
    "seekely": "Thanks for adding the option!   \n. ",
    "asheshambasta": "Thanks for the pointers @olix0r. What would be the best way to access a Namer inside the plugin -- basically, I'll need to read some configuration that defines the hostname of the Auth service and then creates a service after resolving the service address, which then would instantiate an instance of the plugin.\nAlso, I wanted to do a quick check with you guys to see if I'm headed the right way.\n. @adleong yeah, that was what I was worried about \u2013 I wanted something that could just accept the hostname and resolve it through SD the same way it would do for other services. That makes more sense for us because the service handling authentication in our case is one of the services registered on consul and can move around.\n. @adleong ok, seems like a no go then \u2013 I'm keeping my eyes on #596. In our scenario, its impossible to know the location of the authentication service without resolving it first.\n. This will definitely be great for our use case.\n. ",
    "ajagnanan": "@asheshambasta were you guys able to finish the plugin by chance? . ",
    "zircote": "\ud83d\udc4d Looking forward to this one as well, thank you @adleong \n. It is not so much about securing the znodes for me as it is about making it less likely that one team/engineer can not harm or interfere with others. If someone unwanted gets to the zk servers we have bigger problems than zk. We currently have our discovery tree under acls to disallow accidental/unwanted mutation by individuals.. ",
    "mapix": "@adleong  thanks for your reply, but it is not a One-Way request , description file like this:\npickad.thrift\n```\nstruct Query {\n    1: i32 uid,\n    2: string bid,\n    3: i32 group_id,\n    4: string ip,\n    5: i32 hour,\n    6: string pid,\n    7: i32 unit_id,\n    8: list page_tags,\n}\nservice PickAd {\n    list get(1: Query query,\n                  2: list ad_ids,\n                  3: string model_typ,\n                  4: string model_collection),\n}\n```\n. @adleong  Yeah,  i will do both tcp dump and metrics dump, and will paste here in hours.  Thanks for your advise.\n. @adleong\nuploaded more tracking data as linkerd_dump.tar.gz\n``\n.\n|-- collect_metrics.sh\n|-- collect_tcpdump.sh\n|-- l5d_metrics_1472025753.json\n|-- l5d_metrics_1472025813.json\n|-- linkerd.2016-08-24-16:02.pcap-- README\n0 directories, 6 files\n```\ntcpdump file include all traffic from client <-> linkerd & linkerd <-> server\n. @adleong   Thanks for the invaluable analysis.  \nI simulated the zookeeper register logic in develop environment,  find a very long deregister which may lead the \"Connection refused\".  But still can't tell why TTwitter protocol broken,  the simulate shows that Linkerd always send back a ResponseHeader frame even connection refused occurs.\n. @adleong  \nAfter reimplement our service register logic use standalone golang daemon, It works now.  Thank you very much for help.\n. @klingerf \nYeah, we use a self-implemented Zipkin toolsets . If this GC annotation is collected by intent , i will update our Zipkin UI to fix it. Thanks for reply.\n. closed for only affect our infrastructure\n. ",
    "moleksyuk": "@wmorgan, np :)\nAll examples based on real setup ;)\n. @esbie \n\nThis example is extremely useful! It feels more guide-like than reference-like, so I think we should add it as part of the existing k8s guide https://linkerd.io/doc/0.7.4/k8s/\n\nLater on (probably with other pull request) I'll prepare complete guide how to configure Linkerd+Namerd in Kubernetes from real example and list all pitfalls and how to resolve them.\n. @olix0r I've replaced version numbers with placeholders.\n. @adleong done \ud83d\udc4d \n. Fixed\n. Replaced 1. to '*'. I think this \"How to check...\" should be as a sub-part of item 3. [ThirdPartyResource](http://kubernetes.io/docs/api-reference/extensions/v1beta1/definitions/#_v1beta1_thirdpartyresource) feature should be enabled in Kubernetes cluster. and a list is the one possible way to visualize it in such way.\nWhat do you think @esbie ?\n. @olix0r good point \ud83d\udc4d \nWhat about such proposal?\nyaml\ncontainers:\n+        - name: namerd\n+          image: buoyantio/namerd:<version> # specify required version or remove to use the latest\n. ",
    "OleksandrBerezianskyi": "hey Alex, the problem we observed was that (for some reason) Namerd stopped to change the dtab in underlying k8s storage through API (while still returning 204). Maybe the real issue was buried under these exception messages. Initially we thought that these exceptions were the cause, but after the restart we could not reproduce the behaviour, so I think we should just close this ticket and reopen it if we have any reasonable steps to reproduce.\n. Hey Alex, that is exactly what we together with @moleksyuk are planning to do. So 1 and 2 is solved by having linkerd running as a deamonset on the host network. In this case linkerd still can connect to namerd to  resolve endpoints (2.) and every pod should connect on the well known port of the hosting node.\nThere are a few ways to pass the hosting node domain name to the pod:\n1. Use downward API and parse out host name or IP: http://stackoverflow.com/questions/30690186/how-do-i-access-the-kubernetes-api-from-within-a-pod-container/30739416#30739416\n   The ugly part of this is that you need to add a script that will do that on the pod startup and push the result into some environment variable, like K8S-NODE-NAME\n2. Wait for k8s 1.4 and use https://github.com/kubernetes/kubernetes/pull/27880 to do the same but without the script - directly from pod spec\nAs for 3. we were thinking about getting the information about /24 CIDR from downward API as well and using that on linkerd plugin to filter enpoints to the node-local\nIn our approach we have not used linkerd-to-linkerd model, instead we use source-linkerd-destination model\n. Hey @adleong, sorry for the delay, was busy with the other things. I'll try to retest this with the build from master sometime this week or early next week.\n. hi @olix0r unfortunately (for this ticket) our main config was switched to etcd backend and I can't reproduce it any more on test cluster. I'll consider this closed.\n. Fixed with the referenced pull\n. @adleong - just signed\n. In our topology linkerd talks to many different endpoints managed by different team and all of those have different performance/load requirements. Having that said it would be very good to expose every single config from: https://twitter.github.io/finagle/guide/Clients.html for each path stack. There should be a default config that is used if a particular config is missing. \nAlso please note that because linkerd config and a particular service has different lifecycles we want to reload or at least add particular client config dynamically, and preferably via API and watched persistence (the same way as namerd works).\n. @olix0r so, let's say we are using any service discovery. In this case the destination is dynamic and the only way to support \"session stickiness\" is it add custom loadbalancer, correct?\n. @olix0r so the use case. We have a ton of microservices. Each microservice is caching downstream data. Each microservice have at least 4 instances to support high availability (have instances in all availability zones). We have an idea to develop some sort of session stickiness to improve cache hit statistics. In our case 'userId' is the most important part of the context and a part of cache key. The idea itself was to always route requests for the same session to the same endpoints if possible. I understand that such idea conflicts the main purpose of load balancing to distribute the load as evenly as possible. So the stickiness rule should only be applied up to a certain threshold. \nThe same story about physical distance to the endpoints. We want to route traffic to the node-local microservices first, then to endpoints in the same availability zone and only then to other instances in the same region. \nAll in all we want to optimize the route for the best possible latency.\n. Also this can cause a huge CPU load on the side of linkerd\n. Thanks! (feeling miserable)\n. ",
    "adambom": "The upload is not currently being chunked, which would explain why I'm hitting the limit. Is there any reason to always limit the non-chunked uploads?\n. Ok thanks for the explanation. I will look into chunking the request body.\nOn Thursday, September 15, 2016, Borys Pierov notifications@github.com\nwrote:\n\nIIRC the problem is a bit deeper - there is fundamental limitation within\nnetty. org.jboss.netty.handler.codec.http.HttpChunkAggregator has int\nmaxContentLength as a constructor parameter. Given that it's signed it\nmeans that HttpChunkAggregator cannot handle requests larger than 2GB at\nall. So unless you ready to hack your own version of codec there is no easy\nway to achieve that.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/BuoyantIO/linkerd/issues/672#issuecomment-247506981,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAR-xT-6R8gLrqZcrqnbLiQkvjy3RrgIks5qqgTVgaJpZM4J-bR3\n.\n. \n",
    "fantayeneh": "Yes, that was exactly what i would have done. :)\n. Another use case we are trying to use the plugin system, is for injecting a/b test headers.\nIs the plan to extending the plugin system still on?\n. Is this not covered by the config param maxInitialLineKB? \nsee https://github.com/BuoyantIO/linkerd/blob/master/linkerd/docs/protocol-http.md . @olix0r yep, i saw that. :). I did some digging, with streamingEnabled=true\nNett3\nIt appears that the issue lies in netty message decoder.\nhttps://github.com/netty/netty/blob/3.10/src/main/java/org/jboss/netty/handler/codec/http/HttpMessageDecoder.java#L218-L226\nNetty4\nIt just crashes with Response entity too large:.\nIt feels it is not honoring the streamingEnabled config\n@olix0r \nIt worth checking why with netty4 there is this issue.\n@dpetersen \nif the resource you are proxying is too big, disabling streaming will create you problems . Yep, make sense.. On this same issue we realized that chunked error responses are responsible in the first place for\nthe unusual high number of connection created.\n```\ntest(\"chunked error responses should not create new connection for each retries\") {\n    val stats = new InMemoryStatsReceiver\n    val tracer = NullTracer\nval downstream = Downstream.mk(\"dog\") { _ =>\n    val rsp = Response()\n    rsp.status = Status.InternalServerError\n    rsp.setChunked(true)\n    rsp.close()\n    rsp\n}\n\nval label = s\"$$/inet/127.1/${downstream.port}\"\nval dtab = Dtab.read(s\"/svc/dog => /$label;\")\nval yaml =\n  s\"\"\"|routers:\n      |- protocol: http\n      |  dtab: ${dtab.show}\n      |  responseClassifier:\n      |    kind: io.l5d.retryableRead5XX\n      |  servers:\n      |  - port: 0\n      |\"\"\".stripMargin\nval linker = Linker.load(yaml)\n  .configured(param.Stats(stats))\n  .configured(param.Tracer(tracer))\nval router = linker.routers.head.initialize()\nval server = router.servers.head.serve()\nval client = upstream(server)\n\ntry {\n\n  val req = Request()\n  req.host = \"dog\"\n  val errrsp = await(client(req))\n  assert(errrsp.status == Status.InternalServerError)\n  assert(stats.counters.get(Seq(\"http\", \"srv\", \"127.0.0.1/0\", \"requests\")) == Some(1))\n  assert(stats.counters.get(Seq(\"http\", \"dst\", \"id\", label, \"requests\")) == Some(101))\n  assert(stats.counters.get(Seq(\"http\", \"dst\", \"id\", label, \"connects\")) == Some(1), \"expecting only one connects to downstream server\")\n\n\n} finally {\n  await(client.close())\n  await(downstream.server.close())\n  await(server.close())\n  await(router.close())\n}\n\n}\nThis fails\nExpected :Some(1) expecting only one connects to downstream server\nActual   :Some(101)\n```\nThe same test passes if we return the following response\n```\n val downstream = Downstream.mk(\"dog\") { _ =>\n        val rsp = Response()\n        rsp.status = Status.InternalServerError\n        rsp\n    }\n```\nWe will continue investigating this but any input on why this is happening is greatly appreciated\n. @adleong Unfortunately it leaks also for non-chunked response ;(\nFor an origin service responding with \nHTTP/1.1 500 Internal Server Error\ncache-control: no-cache\ncontent-type: text/html; charset=utf-8\ncontent-length: 10605\nvary: accept-encoding\nDate: Thu, 27 Apr 2017 13:46:24 GMT\nConnection: keep-alive\na single request leaked the following number of connections and counting\n\nOur theory also is the same, on retry the client is not consuming the body. \n. @adleong Here it is\nval downstream = Downstream.mk(\"dog\") { _ =>\n      val rsp = Response()\n      rsp.status = Status.InternalServerError\n      val body = (1 to 1000).map(_ => \"Hello Linkerd\").mkString(\"\\n\")\n      rsp.setContentString(body)\n      rsp.close()\n      rsp\n    }. Will create a new one. I think host is already being forwarded.\nhttps://github.com/linkerd/linkerd/blob/1e2c1e42bf38477648361376523baee0bcc58bfb/router/http/src/main/scala/io/buoyant/router/http/AddForwardedHeader.scala#L26. @pcalcado Just did that, not sure why is not picking it up. Fixes #1840 . NP. @adleong We are very interested on this one, do you have some idea how you want to do this. \ne.g a configuration . with the selected metrics? I can def would work on this. Great, raised https://github.com/linkerd/linkerd/issues/2137\nWill close this.. So \nQ. How do we identify the source of a request?\nA. This should be configurable/pluggable solution \n     - Injecting this info in a configurable header name, can be on solution\n     - If you are using tracing, you might have already the desired info\nQ. How do these sources get rendered into metric names?\nA.  route.src.dest.metric e.g route.frontend.backend.status\nQ. How would this be configured? It should probably be an opt-in feature that should only be enabled if the number of sources is known to be small.\nA. Absolutely this should be an opt-in  feature.\n    Not sure how/where should go the config but i see see some options for the config\nrouters:\n- protocol: http\n  route_stats:\n    # max_size: maximum number route stats allowed\n    max_size: 10\nQ. Can this be done as a Linkerd plugin?\nA.  I am not sure if it is possible to have hold the stats object in a Plugin\nYou were saying that there is some work being done for Linkerd2. \nIs this different from what you folks thinking?\n. Uhm, How you \"val-ing up\" if msg need to be passed around\n. Yes make sense.. I think so. :). Yes, I tried hard to come up with something the can be testable but it felt the same to me.\nIt makes the code unreadable.\nI am happy to close the pull request until we find a better strategy  for it.. ",
    "dhay": "when I put the ServiceInstance payload class in the classpath ($L5D_HOME/plugins), I get past the InvalidArgumentException, but then I get \"Exception: Cannot cast com.servicemagic.curator.discovery.ServiceDetails to java.lang.Void\"  I think this is because the curator ServiceDiscovery instance is instantiated with a Void type instead of something like Object.. Was there something you were waiting for from me on this pull request?  It's still something we would like to see in the core product.. ",
    "abhinigam": "My bad.\n. Oliver can you take a look at this again. I have updated the diff based on your comments.\n. ",
    "AbhiMedallia": "Hi Oliver,\nSorry about that. I am just figuring out the github stuff and did not\nrealize I was sending the diff to you guys.\nThese are medallia specific changes. As far as I am concerned the headers\nwork pretty well as they are. This is just for policy/aesthetic reasons why\nwe are changing the header names.\n-Abhishek\nOn Tue, Oct 4, 2016 at 3:26 PM, Oliver Gould notifications@github.com\nwrote:\n\nThanks for the patch, @AbhiMedallia https://github.com/AbhiMedallia!\nThis is really cool.\nI don't think we should add Medallia-specific logic to linkerd; but I\nunderstand the need to support an alternate header scheme. This seems like\na good target for a new plugin subsystem, so that you can maintain this\noutside of the linkerd repo. If we could provide such a plugin interface,\nwould that work for you?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/BuoyantIO/linkerd/pull/717#issuecomment-251531947,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AUEsEHSD1WBoOTfjV1YmnaytQZQrTdCYks5qwtKqgaJpZM4KOOVO\n.\n. Hi Oliver,\nIf I want to contribute this kafka telemeter patch back to buoyant linkerd\nmaster how should I proceed.\nI think this patch will be really useful for others considering kafka as\nthe transport protocol for tracing.\n\n-Abhishek\nOn Tue, Oct 4, 2016 at 4:01 PM, Oliver Gould notifications@github.com\nwrote:\n\n@AbhiMedallia https://github.com/AbhiMedallia no problem. If you've\nfound the need to change this sort of thing, I expect other users will as\nwell. Ideally, you should be able to run vanilla-released linkerd with a\nset of medallia specific plugins (provided at runtime) to get all of the\nfeatures you need. This should substantially reduce your maintenance\noverhead in the long run.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/BuoyantIO/linkerd/pull/717#issuecomment-251538291,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AUEsEEt_WaxHMDg5LUG1xLY8F7qPhMeZks5qwtq9gaJpZM4KOOVO\n.\n. Here is what the config looks like:\n\ntelemetry:\n- kind: io.l5d.kafkazipkin\nsampleRate: 1\nbrokerList: 192.168.99.100:9092\nnumRetries: 10\nI am attaching the patch to this email thread itself to make it easier for\nsomeone to use.\n-Abhishek\nOn Mon, Oct 31, 2016 at 11:40 PM, Abhishek Nigam anigam@medallia.com\nwrote:\n\nHi Oliver,\nIf I want to contribute this kafka telemeter patch back to buoyant linkerd\nmaster how should I proceed.\nI think this patch will be really useful for others considering kafka as\nthe transport protocol for tracing.\n-Abhishek\nOn Tue, Oct 4, 2016 at 4:01 PM, Oliver Gould notifications@github.com\nwrote:\n\n@AbhiMedallia https://github.com/AbhiMedallia no problem. If you've\nfound the need to change this sort of thing, I expect other users will as\nwell. Ideally, you should be able to run vanilla-released linkerd with a\nset of medallia specific plugins (provided at runtime) to get all of the\nfeatures you need. This should substantially reduce your maintenance\noverhead in the long run.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/BuoyantIO/linkerd/pull/717#issuecomment-251538291,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AUEsEEt_WaxHMDg5LUG1xLY8F7qPhMeZks5qwtq9gaJpZM4KOOVO\n.\n. Hi William,\nI have the changes on a branch called KafkaTelemeter but I do not have\npermissions to push these changes to remote branch on github.\n\n\nanigam-mbp:linkerd anigam$ git push origin KafkaTelemeter\nremote: Permission to BuoyantIO/linkerd.git denied to AbhiMedallia.\nfatal: unable to access 'https://github.com/BuoyantIO/linkerd.git/': The\nrequested URL returned error: 403\n-Abhishek\nOn Tue, Nov 1, 2016 at 9:44 AM, William Morgan notifications@github.com\nwrote:\n\nHi Abhishek,\nAwesome, I would love to have this feature! If you have it on a branch,\nyou can make a Github pull request against BuoyantIO/linkerd... that is the\nstarting point for getting it into the main distro.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/BuoyantIO/linkerd/pull/717#issuecomment-257619233,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AUEsEKHBf7DTtmtnsuvdMDk4Q0R9cdadks5q52xigaJpZM4KOOVO\n.\n. I have created a pull request:https://github.com/BuoyantIO/linkerd/pull/796\nThanks for the guidance.\nhttps://github.com/BuoyantIO/linkerd/pull/796\n\nyaml looks like:\ntelemetry:\n- kind: io.l5d.kafkazipkin\nsampleRate: 1\nbrokerList: 192.168.99.100:9092\nnumRetries: 10\n-Abhishek\nOn Tue, Nov 1, 2016 at 11:05 AM, Kevin Lingerfelt notifications@github.com\nwrote:\n\nHi Abhi,\nIn order to create a pull request, you can push your branch to a fork of\nthe linkerd repo, rather than pushing directly to BuoyantIO's linkerd\nrepo. You can fork the repo by clicking the fork button in the Github UI.\nThis document describes the fork / pull request workflow pretty thoroughly:\nhttps://gist.github.com/Chaser324/ce0505fbed06b947d962\nHopefully that's helpful, but let us know if you get stuck.\nKevin\nOn Tue, Nov 1, 2016 at 10:21 AM, AbhiMedallia notifications@github.com\nwrote:\n\nHi William,\nI have the changes on a branch called KafkaTelemeter but I do not have\npermissions to push these changes to remote branch on github.\nanigam-mbp:linkerd anigam$ git push origin KafkaTelemeter\nremote: Permission to BuoyantIO/linkerd.git denied to AbhiMedallia.\nfatal: unable to access 'https://github.com/BuoyantIO/linkerd.git/': The\nrequested URL returned error: 403\n-Abhishek\nOn Tue, Nov 1, 2016 at 9:44 AM, William Morgan <notifications@github.com\nwrote:\n\nHi Abhishek,\nAwesome, I would love to have this feature! If you have it on a branch,\nyou can make a Github pull request against BuoyantIO/linkerd... that is\nthe\nstarting point for getting it into the main distro.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/BuoyantIO/linkerd/pull/717#issuecomment-257619233\n,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/\nAUEsEKHBf7DTtmtnsuvdMDk4Q0R9cdadks5q52xigaJpZM4KOOVO\n.\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/BuoyantIO/linkerd/pull/717#issuecomment-257630354,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-\nauth/AAAkCh7q1QLLaoQCgxNqG74-GtlAk-Ujks5q53UugaJpZM4KOOVO\n.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/BuoyantIO/linkerd/pull/717#issuecomment-257644027,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AUEsEGsZr3Eq2QlU3aQyJIIDDF3dyBxaks5q539ngaJpZM4KOOVO\n.\n. \n",
    "rsimonfitbit": "Thanks. Let me give a little more background on what we are trying to accomplish with linkerd.\nWe have several Finagle-based services up and running. These are mostly Java. We are making use of Finagle's extensibility to add in some of our own functionality. One example is a rate limiting filter. This filter will extract the client ID from the request then check with a rate limiting service to see if the client is over the limit across all instances of the service it is trying to talk to. Currently rate limits are applied per service but we can imagine supporting per service method rate limits in the future.\nAnother filter we have is for extracting the service method name being called. Finagle does not provide service method level metrics out of the box. Therefore, we added a filter that extracts the service method name and sticks it in a com.twitter.finagle.context.Contexts object. We can then use the service method name when generating metric labels like requests per second, error rate, etc.\nThese are just two examples of how we have extended Finagle. There are others built or in the process of being built like custom load balancing algorithms, custom admin pages, authentication filters, custom metrics, etc.\nWe now need to support some non-JVM services that we have. We'd like them to get all the goodness of Finagle as well as our Finagle extensions. Linkerd certainly seems like an ideal fit for this use case. In fact, in the long term I could imagine only using Linkerd instead of \"embedded Finagle\".\nProbably the most important use case for us to start with is rate limiting support. However, I wanted to provide the additional context because I thought it might be useful for you.\nIs this something you are looking to address soon or should we propose some changes?\n. ",
    "jacktuck": "@fantayeneh +1.\nI was thinking about whether it would be possible to add a caching plugin to linkerd. It could help improve latencies and even fault tolerance. Not sure if that's something that would be wanted broadly enough to warrant open a feature request for. Any thoughts on this @olix0r ? :)\n. @ewilde Those links are great. I think hedged requests would give an edge over istio/envoy since they do not have this yet. \ud83d\ude0d . @amitsaha You'd probably only want to hedge idempotent requests.. ",
    "ejona86": "@olix0r the behavior you are seeing is expected. Easiest thing to do is disable AUTO_READ on the child channel when you want to push-back. Anything read is automatically considered \"consumed\" and returns flow control window. This is effectively the same behavior you would see when using HTTP/1 in Netty.\nI was thinking the child channel could have an option that disables the auto return on read. Your handler would then be responsible for sending WINDOW_UPDATEs when you consumed the data. That isn't there today, but it is relatively easy to add. But it's also lower on the priority list since you can already disable AUTO_READ.\n. ",
    "rnorth": "It seems that this issue affects the filesystem namer as well - e.g. if a name file is empty, we're seeing the same behaviour as for Consul (mentioned below). It seems counterintuitive. Does the FS namer need to be adjusted as well?\nNote that if the name file is absent it works as we'd expect.\ncc @jakubka\nFrom Slack:\n\ndoes the alternate (|) routing take Consul healthchecks into account? We\u2019re trying to do something similar to achieve failover to another region if the local region\u2019s instances are down.\nOur linkerd config looks a bit like this:\n```namers:\n- kind: io.l5d.consul\n  host: consul\n  useHealthCheck: true\n\nrouters:\n- protocol: http\n  dtab: |\n      /svc      =>  /#/io.l5d.consul/.local | /$/inet/other-dc-ingress-linkerd/4140 ;\n```\n\nThe Consul healthchecks seem to be doing the right thing for selecting a healthy node within the same DC. We were expecting that the alternate routing dtab above would direct requests to the other DC ingress point if all of the local instances failed their healthchecks. However, actually we\u2019re getting an error back (No hosts are available).\nFWIW everything seems to work OK when no instances of the service are registered with Consul - it\u2019s just when there are registered but unhealthy instances that things start to behave unexpectedly.\nAre we doing something wrong, or is there a better way to do what we\u2019re trying to do here?. We encountered the same issue and were actually wondering about suggesting an alternative solution. Our use case is actually two-fold:\n\n\nAs above, when service B fails (or starts to shutdown) before our namer is updated, we DO want linkerd A to attempt a retry when linkerd B indicates a failure. It seems that linkerd B returns a 503 status code when it's failed to establish a connection to the service.\nSecondly, we also want to implement back pressure via service instances returning 429 status codes when overloaded. Similarly, if service B returns a 429 response, we want linkerd A to attempt a retry against another linkerd-service pair.\n\nSo our current approach is to create an alternative response classifier for linkerd Abased on io.l5d.retryableRead5XX. The difference is that a 503 response and a 429 response will always be regarded as retryable regardless of method. This seems to be working pretty nicely in our test environment so far - we were actually wondering about contributing this back!\nI can see how l5d-success-class could help us with the first scenario but probably wouldn't help with the second. What do you think?. ",
    "dpetersen": "So I tried this out today (thanks to @klingerf building me a Docker image with this code). I'm running k8s, I have a Rails app speaking gRPC to a golang backend. There's a linkerd sidecar in each pod, with the Rails-to-golang communication happening linker-to-linker. None of this is using TLS right now. Everything looks good and works as expected (save for missing stats in the admin, but I see #731). Metadata is coming through just fine.\nI was unable to test whether linkerd respects the h2 authority header, since I can't figure out how to wrangle the ruby gRPC client into setting that header for me. Hopefully I'll figure it out at some point.\nI did notice that the httpAccessLog setting isn't available for the h2 router, which I didn't expect. It's pretty handy to point that to /dev/stdout when you're first setting things up.\nThanks for the work on this, it's looking great!\n. I know I haven't had any issues recently with this, and we're still running this version of linkerd (I haven't had time to upgrade).. We have some CI/CD automation that is updating the Deployment object using the K8s go client library. We've had a dozen deploys go through successfully today without any failures, and we're still running 1.3.3. I was hoping to have it happen again since I still have verbose logging turned on.\nI can provide more details of the precise code if you'd like. We also have Prometheus metrics for this instance, including for last night when we had our issue. I can put some of those in this ticket if it helps. I mostly looked at rt:client:failure_accrual:removals and rt:service:failures:rejected:com_twitter_finagle_Failure:com_twitter_finagle_ConnectionFailedException, since those two jumped out at me while I was reading the metrics endpoint during troubleshooting.\n. That was one of the first things I did after we had our issue. I was watching the logs while our service was still down. Some of the logs I posted above are with k8s at a trace level while we were doing subsequent deploys trying to fix the service.\nOne thing I did not post is that the logs are full of this:\nD 1207 19:33:32.593 UTC THREAD28: k8s returned 'too old resource version' error with incorrect HTTP status code, restarting watch\nD 1207 19:33:32.593 UTC THREAD28: k8s restarting watch on /api/v1/watch/namespaces/content-staging/endpoints/assets, resource version None was too old\nI'm assuming that's normal behavior.\nAnother change I didn't think of is that we upgraded the buoyantio/kubectl sidecar pod from v1.4.0 to v1.6.2 (the most recent published version), although our k8s version is actually 1.7.2. Those logs will occasionally spew:\nI1207 18:59:35.249296       1 logs.go:41] httputil: ReverseProxy read error during body copy: context canceled\nI1207 18:59:35.254915       1 logs.go:41] httputil: ReverseProxy read error during body copy: context canceled\nI1207 19:29:42.662030       1 logs.go:41] httputil: ReverseProxy read error during body copy: context canceled\nI1207 19:29:42.680564       1 logs.go:41] httputil: ReverseProxy read error during body copy: context canceled\nI1207 19:29:42.697225       1 logs.go:41] httputil: ReverseProxy read error during body copy: context canceled\nI1207 19:29:42.707671       1 logs.go:41] httputil: ReverseProxy read error during body copy: context canceled\nI1207 19:29:42.725691       1 logs.go:41] httputil: ReverseProxy read error during body copy: context canceled\nI1207 19:29:42.735628       1 logs.go:41] httputil: ReverseProxy read error during body copy: context canceled\nI1207 19:47:35.331696       1 logs.go:41] httputil: ReverseProxy read error during body copy: context canceled\nI1207 19:47:35.350363       1 logs.go:41] httputil: ReverseProxy read error during body copy: context canceled\nI1207 19:47:35.359299       1 logs.go:41] httputil: ReverseProxy read error during body copy: context canceled\nFrom the timestamps, it's not constant but just a few times an hour. I looked at that during our issue the other night and none of the timestamps seemed to correspond to the same time as our outage. Our production linkerd is still running old linkerd with the v1.4.0 against that Kubernetes 1.7.2 and has no such errors, but that might just be down to logging changes in kubectl.\nI'm still digging into the metrics. Since it's time series I can't just dump numbers in here and provide anything useful, so I'm trying to look at all the metrics for the time period of the event to see if I find anything that looks weird. So far the only curious thing I found was that during the window of the outage, the rt:service:request_latency_ms metric for that particular service capped out at 1s and never went above (or much below). Here is a graph from +/-1hr of the outage.\n\nI don't know what it means, it just stuck out to me as looking kind of funny. I will continue looking at the metrics.\nI should also point out that I am still running the same config and we've had over 30 deploys in the last two days that have not had the issue. As far as I know, there was nothing special about that deploy (that app has been deployed a few times since).. For completeness sake, here are the graphs from the same time period for linkerd_rt:client:failure_accrual:removed_for_ms and linkerd_rt:service:failures:rejected:com_twitter_finagle_Failure:com_twitter_finagle_ConnectionFailedException that I referred to earlier.\n\n. I will deploy it now. I was just about to tell you that we haven't had a recurrence of this issue in the last two weeks, but I just checked my dashboard and we actually had a failed deployment last night. I went in the slack channel of the team involved and they were trying to figure out why their service was unresponsive, but it just sort of healed itself after a few hours. I went into my logs, and here are the relevant lines. I have k8s at a trace log level, unlike my previous log posting. Unfortunately, there isn't much new information here:\n{\"log\":\"D 1218 04:35:22.595 UTC THREAD31: k8s returned 'too old resource version' error with incorrect HTTP status code, restarting watch\\n\",\"stream\":\"stderr\",\"time\":\"2017-12-18T04:35:22.597129146Z\"}\n{\"log\":\"I 1218 04:35:26.607 UTC THREAD23: FailureAccrualFactory marking connection to \\\"#/io.l5d.k8s/integrations-staging/http/inventory\\\" as dead. Remote Address: Inet(/100.97.4.75:3000,Map(nodeName -\\u003e ip-172-20-61-49.us-west-2.compute.internal))\\n\",\"stream\":\"stderr\",\"time\":\"2017-12-18T04:35:26.608727757Z\"}\n{\"log\":\"E 1218 04:35:26.609 UTC THREAD23: service failure: Failure(connection timed out: /100.97.4.75:3000 at remote address: /100.97.4.75:3000. Remote Info: Not Available, flags=0x09) with RemoteInfo -\\u003e Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /100.97.4.75:3000, Downstream label: #/io.l5d.k8s/integrations-staging/http/inventory, Trace Id: 7f31992890b7e429.7f31992890b7e429\\u003c:7f31992890b7e429\\n\",\"stream\":\"stderr\",\"time\":\"2017-12-18T04:35:26.610478624Z\"}\nThis is what we've seen before. They deployed this service and 100.97.4.75 is the old endpoint. It continued to try and talk to that endpoint for the next 3 hours, throwing a bunch of ConnectionFailedException errors.\n\nIt finally recovered on its own, with no work from any person. I found this in the logs for the time when the connection errors stopped (the Grafana graphs are PST and the logs are UTC):\n{\"log\":\"I 1218 06:48:41.142 UTC THREAD10: Reaping #/io.l5d.k8s/integrations-staging/http/inventory\\n\",\"stream\":\"stderr\",\"time\":\"2017-12-18T06:48:41.147723284Z\"}\nAfter this, it picked up the new endpoints and has been working happily.\nHopefully, that helps. I will try running the new version, and also will pay attention to issue #1755 as well.. Bad news, the issue occurred again tonight and we're running v1.3.4. The metrics and logs for this event are identical to what I've already posted, so I won't post them again.. Sure, we're willing to try a snapshot if it has potential fixes for this issue.. Docker image would be the easiest, thanks!. Unfortunately, it didn't take long to get a failure. We've had two failed deployments with the usual symptoms in the last hour since I've deployed. I would say that the move from 1.3.3 to 1.3.4 has exacerbated the problem. We went weeks without any issues after the initial failure, and in the last 24 hours (since first upgrading to 1.3.4 and then to this branch) we've had 6.\nAll that said, whatever triggers this behavior must be something special about one of our services. I think we've had 8 total failures and 7 of them have been with the same app. Off the top of my head, I know this app is unreliable. It regularly gets inundated with requests, even on staging, so multiple times a day it will be briefly removed from linkerd's internal load balancer due to failure accrual. That's not desirable, but in that case, linkerd behaves as I expect. Our other services are usually less flakey.\nI will take a look at recent development on that service to make sure that our increased failure rate isn't something new in the service instead of our 1.3.4 update. The deployment issues still seem like misbehavior from linkerd, but it must be something special we're doing here that triggers it. There were a dozen successful deploys today for other services.. Yes, the two most recent failures are running the snapshot.. @obeattie I just double-checked and we're not overriding any of the defaults for enableProbation, so it should be disabled. Thanks for looking!. @siggy I don't, sorry. Since I last commented on this I haven't had time to revisit it, and we're continuing to run an older version of Linkerd. Definitely willing to experiment again if a new version has a prospective fix or just additional logging.. Honestly, I have been so busy we haven't been able to make another upgrade attempt. We're still running the same version we have been running. Not sure when I'm going to be able to revisit it. I'm fine if you think it's fixed and want to close this issue. I can re-open or open something new if it's not fixed for me.. ",
    "jchauncey": "Closing til I can better articulate my problem :)\n. ",
    "alinvasile": "Do you have an example of client IP-based load balancing with the Identifier plugin?\n. Having entries for each zone sounds like a good idea, so I gave it a try:\nrouters:\n protocol: http\n  identifier:\n    kind: io.l5d.methodAndHost\n    httpUriInDst: true\n  responseClassifier:\n    kind: io.l5d.retryableIdempotent5XX\n baseDtab: |\n    /default => /#/io.l5d.serversets/deployments/192.168.10.1;\n    /standby => /#/io.l5d.serversets/deployments/192.168.10.10;\n    /http///* => /default | /standby;\n/deployments/192.168.10.1 will contain a serverset entry, same for /deployments/192.168.10.10\nWith this configuration I discovered that the requests are never sent to /standby if the /default server responds with 5xx or suddenly goes down (until the zookeeper timeout deleted the server set in zk).\n. That is acceptable when the server dies, but if  it returns 500s they will be retried on the same server until they are echoed to the client.\n. I'm using 0.8.2.\nI will try to update the code and submit a patch if possible.\n. ",
    "bobbymicroby": "@olix0r  Hi Oliver, here is one use-case. We need to route requests based on user location to the most appropriate backend server. Most appropriate in our case means a server that have announced  itself via service discovery as the handler for a certain geo region.  So based on the service discovery records we need to calculate the closest route for the user's request.\n. ",
    "elecnix": "For what it's worth, I experimented with a Finagle patch that allows requests to be routed to /standby when /default \"breaks-circuit\".. I'm looking for a way to inject a dtab entry that is specific to the host where Linkerd is running. That is similar to per-request routing with the l5d-dtab HTTP header, but here it would be static for a client. \nIn consul at least, perhaps that could be done by merging two namespaces: one that contains a global namespace, and the other a host-specific dtab.. Thanks @Ashald. I don't understand what dtab magic you're referring to. Let me just throw in an other idea: a rewriting namer that inserts the local host name into the path.. Finagle 6.44 is out.. If it was possible to enable immediate requeue on alternate matching clients (instead of the same client), and somehow classify responses as \"requeuable\" (similar to connection-level requeues on clients) we might not need a new plugin interface.\n- protocol: http\n  service:\n    kind: io.l5d.static\n    configs:\n    - prefix: /svc\n      requeues:\n        next-client: true\nThen a dtab could simply give higher priority to the cache backend:\n/svc => /#/namer/backend\n/svc => /#/namer/cache. I dream of an API allowing a Grafana instance to auto-discover Linkerd-generated dashboards.. @siggy, I have only looked at the code and screenshots. There is no drill-down into service and client pages, which is what I see mentioned by @rmars in the issue description. I think we would need Linkerd to generate Grafana dashboards from its configuration and runtime state.. You may be able to get something like this with scripted dashboards.. I will!. You can identify requests using a header, then match it in your dtab with negative resolution or failure.. You may want to hold this PR until I sign the Contributor Agreement.. Hold on! I'm still waiting for a response from our legal department.. Alright. I'll fix the merge conflicts and it should be OK to pull.. I like to delegate the formatting to my IDE. Do you have a settings file for IntelliJ ?. ",
    "irachex": "Another problem is that linkerd seems doesn't handle GOAWAY properly.\nAccording to https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#goaway-frame\n\nServers should send GOAWAY before terminating a connection to reliably inform clients which work has been accepted by the server and is being executed.\n\nWe add a keepalive options for server. server will send GOAWAY then close if client doesn't send any data in keepalive seconds. But linkerd report error:\nE 1019 04:04:33.988 THREAD69: demuxer\njava.lang.ClassCastException: io.netty.handler.codec.http2.DefaultHttp2GoAwayFrame cannot be cast to io.netty.handler.codec.http2.Http2StreamFrame\n    at com.twitter.finagle.buoyant.h2.netty4.Netty4ClientDispatcher$$anonfun$com$twitter$finagle$buoyant$h2$netty4$Netty4ClientDispatcher$$loop$1$1.apply(Netty4ClientDispatcher.scala:64)\n    at com.twitter.util.Future$$anonfun$flatMap$1.apply(Future.scala:1092)\n    at com.twitter.util.Future$$anonfun$flatMap$1.apply(Future.scala:1091)\n...\n. ",
    "ghost": "Hello, any update on this? We are being hit by the same problem. We would like to have 1 h2 router being able to handle both grcp and http/1. ",
    "nadilas": "+1. Hi @adleong, technically yes. If you can upgrade http/1.1 to http/2.0 that would take care of the preflight requests as well. However the difference is in the use case, all my requests are http/2.0 only the ones generated by the browser which I can\u2019t control are http/1.1. \nTo be honest it\u2019s completely blocking my project, because no grpc calls are working from the browser. . I would if I could :) those preflight requests are generated by the browser (e.g. Chrome) not grpc-web. I have no control over that. . @adleong When do you expect this feature to land?. @adleong thanks for the info. That means I have to take Linkerd out of my stack for now. Where can I track when this feature will be implemented?. @wmorgan great, thanks! I\u2019m not on Kubernetes. So I\u2019ll just have to wait. \ud83d\ude42. ",
    "satybald": "@olix0r thanks for the warm welcome. I don't quite get your first question ;( Can you please rephrase a bit?\nI think request can fail due to validation errors or invalid marathon configuration. In the case of validation errors, it will get some 4xx error. But in the case of invalid configuration, it gets timeouts exception. Am I on the right track? \n. @olix0r I guess you're referring to \"request storm\", when clients simultenisly start polling marathon API. Added possible implementation for that. \n. @adleong thank for suggestions. I added a stream based computation of TTL with jiitter as you recommended.\nShould I make getJitteredNextTtl method a package level to cover it with the tests? \n. @adleong I added necessary changes && resolved merge conflicts. Could you please look at a PR? . @esbie yes, unfortunately I don't have enough time at this moment ;(. thanks @adleong for review. I'll work on in on the weekends.\n. ",
    "camechis": "Definitely looks like its It.  Is this fixable via Linkerd or should I\ncontact the mesophere guys?\nOn Wed, Oct 26, 2016 at 8:44 PM Andrew Seigner notifications@github.com\nwrote:\n\nThe difference between the CoreOS and CentOS images seems to be the\nhostname value.\nCoreOS\n$ hostname\nip-10-0-3-101.us-west-1.compute.internal\n$ nslookup ip-10-0-3-101.us-west-1.compute.internal\nServer:     198.51.100.1\nAddress:    198.51.100.1#53\nNon-authoritative answer:\nName:   ip-10-0-3-101.us-west-1.compute.internal\nAddress: 10.0.3.101\nCentOS\n$ hostname\nip-10-0-3-101\n$ nslookup ip-10-0-3-101\nServer:     198.51.100.1\nAddress:    198.51.100.1#53\n** server can't find ip-10-0-3-101: NXDOMAIN\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/BuoyantIO/linkerd/issues/774#issuecomment-256517003,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAM4e_lomNh4-T0xO_uE4dGbJo0COBb-ks5q3_P-gaJpZM4Kh01F\n.\n. \n",
    "bflance": "Hi Guys,\nis there any simple guide for a beginner on how to install this on DC/OS 1.8.6?\nthe included linkerd 0.8.1 simply doesnt work (after installing from universe)..\na short video on youtube could really be helpful..\nthanx\n. ",
    "qb-neethug": "Hi Guys,\nI am facing the same issue with coreos on AWS. I tried installing linkerd on DC/OS 1.8.6 from universe. The containers fail to start with following exception:\njava.net.UnknownHostException: ip-10-x-x-xx.ap-southeast-1.compute.internal: ip-10-x-x-xx.ap-southeast-1.compute.internal: unknown error\nIs there any work around for this issue?\nThanks\n. Thank you @siggy !\nI tried with  https://github.com/BuoyantIO/linkerd-examples/tree/master/dcos/simple-proxy , and its working fine. \n. ",
    "mmolimar": "Thanks @olix0r \nOne use case I found was using Kafka in DC/OS. Kafka brokers are deployed on private agents inside a Mesos cluster and they are not accessible from the \"outside world\" of DC/OS. So, the idea would be to use Linkerd as a proxy in order to produce/consume messages in Kafka and take advantage of all features that Linkerd provides.. ",
    "iMartyn": "For consistency it would be nice to only have one ingress to care about in kubernetes, so for your average VPS for instance you'd be looking at smtp(+s), imaps, as well as http and https and auxiliary services.. ",
    "forsandeep": "I agree with jar349.It will solve many more problems and help running operations even more smoothly. ",
    "bagelswitch": "Apologies, did not intend to push this upstream, hit the wrong button. \n. ",
    "momania": "I've been looking into linkerd + namerd combination too and ran into the same issues.\nI might add that I think in general the documentation is very unclear, and mostly not up to date with the implementation, and that the logging of both services is extremely bad. I case something doesn't work, the logs give no direction as to what the issue can be.. How about when we run Linkerd as docker image, single instance per host, but with bridged network? Then the host ip address can never be found, so the filter doesn't work.\nIs it an idea to have the ip address to match with as a configurable property? Then we can just fill this in at deploy time.. ",
    "gaugau": "@siggy Does it mean that it's a bug from linkerd? Or the configuration from examples is not correct?!. ",
    "kamilchm": "Any plans for this?. ",
    "dmexe": "@adleong I had pushed an empty commit, but nothing changed. Perhaps ci builds from non BuoyantIO repo disabled?. @olix0r changelog updated. No, there was not reasons, I removed a final modifier. ",
    "jacobrichard": "Here are the relevant logs from the issue. You'll be most interested in logging after Dec 08 18:00:00. \nlinkerd.log.gz\n. Here is the linkerd.yml\n```\nadmin:\n  port: 9990\nnamers:\n- kind: io.l5d.serversets\n  zkAddrs:\n  - host: mesos-masterc01fu2\n    port: 2181\n  - host: mesos-masterc02fu2\n    port: 2181\n  - host: mesos-masterc03fu2\n    port: 2181\ntracers:\n- kind: io.l5d.zipkin\n  host: 127.0.0.1\n  port: 9410\n  sampleRate: 0.01\nrouters:\n- protocol: http\n  interpreter:\n    kind: io.l5d.fs\n    dtabFile: /etc/linkerd/regions/fu2.dtab\n  responseClassifier:\n    kind: io.l5d.retryableRead5XX\n  servers:\n  - port: 4140\n    ip: 172.31.10.88\n```. At Dec 08 19:25:15 in the log snippet I've attached here, another failure mode seems to have been reached, whereby linkerd has consumed all its available filehandles. \nThis appears to be connections to one of the Aurora endpoints -- 16000+ connections to port 8081 on one of the machines running aurora all in CLOSE_WAIT. \nThis may be a run-on condition that is caused by the aforementioned failure, but I figured I'd mention it here in case it is of interest. . ",
    "thewmo": "Thanks for filing the issue. Here is a grab-bag of information from one of our nodes:\n```\n~]$ uname -r\n3.10.0-327.36.3.el7.x86_64\n~]$ cat /etc/redhat-release \nCentOS Linux release 7.2.1511 (Core) \n~]$ sudo docker version\nClient:\n Version:         1.10.3\n API version:     1.22\n Package version: docker-common-1.10.3-46.el7.centos.14.x86_64\n Go version:      go1.6.3\n Git commit:      cb079f6-unsupported\n Built:           Fri Sep 16 13:24:25 2016\n OS/Arch:         linux/amd64\nServer:\n Version:         1.10.3\n API version:     1.22\n Package version: docker-common-1.10.3-46.el7.centos.14.x86_64\n Go version:      go1.6.3\n Git commit:      cb079f6-unsupported\n Built:           Fri Sep 16 13:24:25 2016\n OS/Arch:         linux/amd64\n~]$ openshift version\nopenshift v1.2.1\nkubernetes v1.2.0-36-g4a3f9c5\netcd 2.2.5\n```. Another workaround may be a 32-bit JVM built with PIC so it doesn't need to be reloc'ed.. Did some poking around today on this question - some poking and prodding with objdump and friends had me convinced the 32-bit libjvm.so in CentOS7 might not require a reloc, but after building an image and trying it out I hit the same issue. Later I found a number of old OpenJDK tickets and discussions about libjvm.so not being built with -fPIC on ia32 and why. Long story short I don't expect there will be a workaround on the JVM end, not without building our own JVM (um, no thanks).. ",
    "adrian": "@adleong You're absolutely right. That gives me exactly what I need. Thanks very much.\n@olix0r Do you still think this issue is worth keeping open? I'm happy with the solution proposed by adleong.. Does commit 70502c2 address this issue?. ",
    "akreiling": "thanks -- this works like a charm.\n-a. I will apologize in advance that I my tests need some help. After opening this, I realized they aren't really testing how/what I wanted. Any advice is welcome.. turns out that this is already supported via io.buoyant.http.subdomainOfPfx\nthanks!\n-a. ",
    "raydin": "I don't think you can populate the dest header field when using the standard Scala Finagle thrift client. It add the header using TTwitterClientFilter which does not populate this nor allows you to supply an initial version of the header.. @adleong, that would require forking finagle or creating a filter that deserializes the header and re-serializes it with the field set.. ",
    "ManishMaheshwari": "+1. Different time formats posing issues with Splunk.. ",
    "caniszczyk": "I'd vote yes :)\nOn Wed, Jan 18, 2017 at 7:10 PM, Kevin Lingerfelt notifications@github.com\nwrote:\n\nJust wondering aloud, should we also consider moving the linkerd-examples\nhttps://github.com/BuoyantIO/linkerd-examples repo to the new org?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/BuoyantIO/linkerd/issues/949#issuecomment-273652664,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAD5IXhSYMFAE3EXuuvNqx7o-gICACRxks5rTrgPgaJpZM4LnjQt\n.\n\n\n-- \nCheers,\nChris Aniszczyk\nhttp://aniszczyk.org\n+1 512 961 6719\n. Thanks @olix0r, called the results today for the linkerd project:\nhttps://lists.cncf.io/pipermail/cncf-toc/2017-January/000557.html\nWelcome to the CNCF community! We will work on bringing you within the community via our events and other activities. The official PR should go out in about 10 minutes.. friendly ping here @olix0r and folks, this is a small ask from CNCF and a requirement for graduation :). Thank you xoxo\nYou can enable DCO checking via prbot: https://github.com/probot/dco. ",
    "markeijsermans": "I agree, ignoreServiceAddress is clearer with it's intention as setting useNodeAddress: false will still give you the node address if no service address is defined.. @Ashald yes, it was added to solve a situation where we have a \"linker to linker\" type of setup with our existing routing mesh based on consul. #967 explains it a bit better.\nOption is now renamed preferServiceAddress and defaults to true.. I'm experiencing the same errors (0.8.6): \ncom.twitter.finagle.UnknownChannelException: Response entity too large: DefaultHttpResponse(decodeResult: success, version: HTTP/1.1)\nCan confirm, as per @Pveasey comment that with streamingEnabled: false netty4 respects maxResponseKB limits\nyaml\nrouters:\n- protocol: http\n  maxRequestKB: 20480\n  maxResponseKB: 20480\n  streamingEnabled: false\n  client:\n    engine:\n      kind: netty4\n  servers:\n  - engine:\n      kind: netty4\n  .... ",
    "viglesiasce": "Sounds good @klingerf, will test when its ready.. This looks great!! Is it possible for it to be configurable (templatized)?. For posterity here is what I see in StackDriver trace now. Thanks for putting this in!\n\n. Thanks @klingerf! I just started paternity leave so will be about 3-4 weeks before I come around to this.. @klingerf worked a treat!!!! Thanks for making this happen. Will close out this issue.. ",
    "andrejvanderzee": "This would be useful. Is there any way yet how to enforce global or per-client rate limiting?. @hawkw Any rough estimates on next release? I am running into the exact same issue.. ",
    "ondrej-smola": "Same here (version 0.8.6), using go grpc client/server. Cancelling client hangs all following requests. \nFrom what i observed after first cancel: Client -> Linkerd -> Server. \nC opens connection and sends M1 to S through L, M1 arrives at S and S reply with M2. M2 is send but never arrives at C. C hangs at recv and S hangs at recv. \nFirst request\n```\n0205 19:44:57.645 9ec900a6de5a7faf.9ec900a6de5a7faf<:9ec900a6de5a7faf] ServiceName(#/io.l5d.marathon/test-services/v2t-engine-proxy)\n0205 19:44:57.645 9ec900a6de5a7faf.9ec900a6de5a7faf<:9ec900a6de5a7faf] BinaryAnnotation(clnt/finagle.version,6.41.0)\n0205 19:44:57.645 9ec900a6de5a7faf.9ec900a6de5a7faf<:9ec900a6de5a7faf] ClientSend()\n0205 19:44:57.646 9ec900a6de5a7faf.9ec900a6de5a7faf<:9ec900a6de5a7faf] WireSend\n0205 19:44:57.646 9ec900a6de5a7faf.9ec900a6de5a7faf<:9ec900a6de5a7faf] ServerAddr(/192.168.91.61:21716)\nD 0205 19:44:57.646 THREAD57 TraceId:9ec900a6de5a7faf: [C L:/172.17.0.3:44066 R:/192.168.91.61:21716 S:3] initialized stream\n0205 19:44:57.647 9ec900a6de5a7faf.9ec900a6de5a7faf<:9ec900a6de5a7faf] WireRecv\n0205 19:44:57.648 9ec900a6de5a7faf.9ec900a6de5a7faf<:9ec900a6de5a7faf] BinaryAnnotation(l5d.success,1.0)\n0205 19:44:57.648 9ec900a6de5a7faf.9ec900a6de5a7faf<:9ec900a6de5a7faf] ClientRecv()\n0205 19:44:57.648 b926bc1d3375b04d.b926bc1d3375b04d<:b926bc1d3375b04d] ServerSend()\nD 0205 19:45:02.082 THREAD76: [S L:/172.17.0.3:8080 R:/192.168.91.56:39734 S:11] stream reset from remote: Reset.NoError\nD 0205 19:45:02.083 THREAD76 TraceId:9ec900a6de5a7faf: [C L:/172.17.0.3:44066 R:/192.168.91.61:21716 S:3] stream read failed: Reset.NoError\nReset.NoError\nD 0205 19:45:02.083 THREAD76 TraceId:9ec900a6de5a7faf: [C L:/172.17.0.3:44066 R:/192.168.91.61:21716 S:3] resetting Reset.NoError in Open(com.twitter.finagle.buoyant.h2.netty4.Netty4StreamTransport$RemoteStreaming@19b5eba4)\nD 0205 19:45:02.083 THREAD76 TraceId:9ec900a6de5a7faf: [C L:/172.17.0.3:44066 R:/192.168.91.61:21716 S:3] stream closed\nD 0205 19:45:02.083 THREAD76: [S L:/172.17.0.3:8080 R:/192.168.91.56:39734 S:11] remote write failed: Reset.NoError\nReset.NoError\n```\nSecond\n0205 19:45:27.821 b926bc1d3375b04d.b926bc1d3375b04d<:b926bc1d3375b04d] Message(namer.success)\n0205 19:45:27.821 b926bc1d3375b04d.b926bc1d3375b04d<:b926bc1d3375b04d] BinaryAnnotation(dst.id,/#/io.l5d.marathon/test-services/v2t-engine-proxy)\n0205 19:45:27.821 b926bc1d3375b04d.b926bc1d3375b04d<:b926bc1d3375b04d] BinaryAnnotation(dst.path,/)\n0205 19:45:27.822 9450aad50b1abbee.9450aad50b1abbee<:9450aad50b1abbee] ServiceName(#/io.l5d.marathon/test-services/v2t-engine-proxy)\n0205 19:45:27.822 9450aad50b1abbee.9450aad50b1abbee<:9450aad50b1abbee] BinaryAnnotation(clnt/finagle.version,6.41.0)\n0205 19:45:27.822 9450aad50b1abbee.9450aad50b1abbee<:9450aad50b1abbee] ClientSend()\n0205 19:45:27.822 9450aad50b1abbee.9450aad50b1abbee<:9450aad50b1abbee] WireSend\n0205 19:45:27.822 9450aad50b1abbee.9450aad50b1abbee<:9450aad50b1abbee] ServerAddr(/192.168.91.61:21716)\nD 0205 19:45:27.822 THREAD62 TraceId:9450aad50b1abbee: [C L:/172.17.0.3:44066 R:/192.168.91.61:21716 S:9] initialized stream\nThen it hangs.\nIf somebody can lead me how to debug this or point me to relevant parts of project.\nWe are  building prototype and this is show stopper for us.\n. ",
    "sfroment": "Thanks a lot :) (buoyantio team your the best!) !!! I close this issue reopen if finally it isn't merged !. Thanks you so much for your work on this one https://github.com/linkerd/linkerd/issues/1289#issuecomment-314878636 <3 that was a Real pain for me and i believe many people arround l5d . The only thing that i thinknis missing is \u00e0 http based auth but i'll try to do it my self when i'll have the time to learn scala \ud83d\ude48. Hello, even with a telemetry block \ntelemetry:\n    - kind: io.l5d.prometheus\nit's trying to connect to zipkin in debug log level I got this error \nD 0912 11:09:40.812 UTC THREAD68: ChannelStatsHandler caught an exception\njava.net.ConnectException: Connection refused: localhost/127.0.0.1:1463\n    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n    at org.jboss.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:152)\n    at org.jboss.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:105)\n    at org.jboss.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:79)\n    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)\n    at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)\n    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run(BlockingTimeTrackingThreadFactory.scala:24)\n    at java.lang.Thread.run(Thread.java:748). ",
    "kunalpathak14": "\u2b50\ufe0f thanks this is really helpful.  @siggy my delegator.json curl command  is not dynamically changes the endpoints after i am flipping namer endpoint and need to kick start linkerd service to get the correct IPs. any inputs?. ",
    "rake36": "Hi, I'm having this same problem. Your protoc script does not handle windows at all. To get around this, I manually copied in both 3.0.0 and 3.3.0 of protoc.exe for Win32. Both give exact error as described by @TheDukeVIP .. Output from showing the last exception:\n`[info] Done updating.\n[trace] Stack trace suppressed: run last linkerd-core/protobuf:protobufGenerate for the full output.\n[trace] Stack trace suppressed: run last grpc-interop/protobuf:protobufGenerate for the full output.\n[trace] Stack trace suppressed: run last grpc-eg/protobuf:protobufGenerate for the full output.\n[trace] Stack trace suppressed: run last mesh-core/protobuf:protobufGenerate for the full output.\n[error] (linkerd-core/protobuf:protobufGenerate) protoc returned exit code: 1\n[error] (grpc-interop/protobuf:protobufGenerate) protoc returned exit code: 1\n[error] (grpc-eg/protobuf:protobufGenerate) protoc returned exit code: 1\n[error] (mesh-core/protobuf:protobufGenerate) protoc returned exit code: 1\n[error] Total time: 290 s, completed Jun 15, 2017 4:34:54 PM\n\nlast linkerd-core/protobuf:protobufGenerate\n[info] Compiling 1 protobuf files to C:\\Users\\drakestraw\\development\\src\\linkerd\\linkerd\\core\\target\\scala-2.12\\src_mana\nged\\main\\compiled_protobuf\n[debug] protoc options:\n[debug]         --io.buoyant.grpc_out=plugins=grpc:C:\\Users\\drakestraw\\development\\src\\linkerd\\linkerd\\core\\target\\scala\n-2.12\\src_managed\\main\\compiled_protobuf\n[info] Compiling schema C:\\Users\\drakestraw\\development\\src\\linkerd\\linkerd\\core\\src\\main\\protobuf\\usage.proto\njava.lang.RuntimeException: protoc returned exit code: 1\n        at scala.sys.package$.error(package.scala:27)\n        at sbtprotobuf.ProtobufPlugin$.sbtprotobuf$ProtobufPlugin$$compile(ProtobufPlugin.scala:88)\n        at sbtprotobuf.ProtobufPlugin$$anonfun$sourceGeneratorTask$1$$anonfun$5.apply(ProtobufPlugin.scala:115)\n        at sbtprotobuf.ProtobufPlugin$$anonfun$sourceGeneratorTask$1$$anonfun$5.apply(ProtobufPlugin.scala:114)\n        at sbt.FileFunction$$anonfun$cached$1.apply(Tracked.scala:253)\n        at sbt.FileFunction$$anonfun$cached$1.apply(Tracked.scala:253)\n        at sbt.FileFunction$$anonfun$cached$2$$anonfun$apply$3$$anonfun$apply$4.apply(Tracked.scala:267)\n        at sbt.FileFunction$$anonfun$cached$2$$anonfun$apply$3$$anonfun$apply$4.apply(Tracked.scala:263)\n        at sbt.Difference.apply(Tracked.scala:224)\n        at sbt.Difference.apply(Tracked.scala:206)\n        at sbt.FileFunction$$anonfun$cached$2$$anonfun$apply$3.apply(Tracked.scala:263)\n        at sbt.FileFunction$$anonfun$cached$2$$anonfun$apply$3.apply(Tracked.scala:262)\n        at sbt.Difference.apply(Tracked.scala:224)\n        at sbt.Difference.apply(Tracked.scala:200)\n        at sbt.FileFunction$$anonfun$cached$2.apply(Tracked.scala:262)\n        at sbt.FileFunction$$anonfun$cached$2.apply(Tracked.scala:260)\n        at sbtprotobuf.ProtobufPlugin$$anonfun$sourceGeneratorTask$1.apply(ProtobufPlugin.scala:117)\n        at sbtprotobuf.ProtobufPlugin$$anonfun$sourceGeneratorTask$1.apply(ProtobufPlugin.scala:112)\n        at scala.Function9$$anonfun$tupled$1.apply(Function9.scala:35)\n        at scala.Function9$$anonfun$tupled$1.apply(Function9.scala:34)\n        at scala.Function1$$anonfun$compose$1.apply(Function1.scala:47)\n        at sbt.$tilde$greater$$anonfun$$u2219$1.apply(TypeFunctions.scala:40)\n        at sbt.std.Transform$$anon$4.work(System.scala:63)\n        at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:228)\n        at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:228)\n        at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:17)\n        at sbt.Execute.work(Execute.scala:237)\n        at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:228)\n        at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:228)\n        at sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:159)\n        at sbt.CompletionService$$anon$2.call(CompletionService.scala:28)\n        at java.util.concurrent.FutureTask.run(Unknown Source)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n        at java.util.concurrent.FutureTask.run(Unknown Source)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n        at java.lang.Thread.run(Unknown Source)\n[error] (linkerd-core/protobuf:protobufGenerate) protoc returned exit code: 1`. Thanks. If you need me to try anything, just let me know. In case it matters, running on Windows 7 x64. Can recreate whether running inside git bash or Windows command prompt.. Ok, thanks. We've come up with a work-around to run linkerd on windows.\n\nIf you run the following, it works fine.\njava -cp linkerd-1.1.0-exec io.buoyant.linkerd.Main config/linkerd.yaml\n. ",
    "gsogol": "This is a great request. Something I've been thinking about as well. Would be an awesome feature. . Another use case could be used to send to two different data centers, whichever one returns first, send back to the consumer based on latency. Or could that be configured using different load balancing strategies?Thoughts?. It's exactly that. \n\nOn May 24, 2017, at 6:05 PM, Alex Leong notifications@github.com wrote:\n@gsogol actually that sounds more like speculative/backup requests: #1069\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \n",
    "amitsaha": "@ewilde I am curious to know how would one approach adopting this for requests which has a side-effect? (. I would be curious to know what would your thoughts be regarding making the same possible for RPC services (i.e. when the incoming request is from a RPC service).?. Cool, we will work on it and submit a PR. Thanks @adleong . hey @adleong do you mind taking a look at the in progress PR please? . Hi @hawkw thanks for the merge! Are we looking at this being in the upcoming 1.2.0 release?. Thanks, perfect.. @wmorgan happy to have been able to get my colleague to work on a PR as well :). hey @mirosval (linkerd user here). I am curious why you are doing this:\n- protocol: thrift\n  experimental: true\n  label: thrift\n  dtab: |\n    /consul => /#/io.l5d.consul/small-virtualbox;\n    /svc => /consul;\ninstead of:\n- protocol: thrift\n  experimental: true\n  label: thrift\n  dtab: |\n        /svc => /#/io.l5d.consul/.local/thrift;\n(The thrift above is your label in the router configuration)\nWe are currently running Python thrift services without the attemptTTwitterUpgrade:True, but everything works. We set the consul host explicitly as well (which is different from your config here).. May be worth trying the approach that has been working for us? (unless you have already tried and failed).. hi @DukeyToo do you mind sharing the specific metric you are tracking for the memory usage? Thanks :). Hi @siggy thanks, which specific metric is this? I tried searching in the prometheus exported metrics, but can't seem to find it?. Thanks @DukeyToo . ",
    "ewilde": "@amitsaha as @jacktuck mentioned I'm not sure this approach would work for apis that are not idempotent. Perhaps we could include a response classifier section in the configuration similar to https://linkerd.io/config/1.3.5/linkerd/index.html#http-response-classifiers, this then allows the linkerd user to determine which types of requests can be hedged. @adleong thanks for that I wasn't aware of the host key. For reference if you configure the addForwardedHeader i.e.:\nyaml\n  servers:\n  - port: 4140\n    addForwardedHeader:\n      by: {kind: \"ip:port\"}\n      for: {kind: ip}\nYou will get the following Forwarded headers added, example value:\nForwarded: by=\"172.16.238.9:4140\";for=172.16.238.254;host=linkerd:4140\nI couldn't see the host key mentioned in the docs, should I update the docs?. I've also observed that if the X-Forwarded-Host is already set, linkerd forwards it on. Which in my particular case solves my scenario. I have kong api gateway infront of linkerd and have used the request transformer to add the X-Forwarded-Host header. @adleong i've rebased @AMCR commit and reviewed it with him. Hopefully it's all good. > This classifier marks ONLY writes (POST and PUT) as retryable but all other methods as non-retryable. Is this what you want? Would it be more useful to have a response classifier which marks ALL methods as retryable?\n@adleong I thought the because we define Write based on Idempotent.withMethods it would take the verbs from Idempoent as well i.e.  put, delete, get, head, options, trace. if that is the case maybe a better name would be All and call the response classifier retryableAll5XX ? Our intention was to retry regardless of the verb with this PR. ",
    "teodor-pripoae": "Hi,\nThank you for your help !\nI've been able to successfully run grpc client/server by using the code from 0.9.0-rc1.  I've put an example on github here: https://github.com/teodor-pripoae/finagle-grpc-example.\nI can see it doesn't work with finagle 6.42.0 yet. Are there any plans to support in the near future ?\nAlso, I spotted a bug involving tracing, currently all the spans for a server are in one big trace.\n. Hi,\nIs it currently possible to implement load balancing/circuit braker/service discovery on top of the grpc client ?. Hi,\nI'm trying to use the grpc client outside linkerd, from scala code. I've asked on the slack channel about configuring grpc clients with load balancing and session pooling and @olix0r told me that currently it can be configured using configure method but will accept a PR which exposes this functionality further.\nCurrently I've been able to set it like this after exposing this 2 traits:\nscala\n val h2Settings = H2.client\n      .withLabel(name)\n      .withRequestTimeout(timeout)\n      .withRetryBackoff(retryBackoff)\n      .withRetryBudget(retryBudget)\n      .withLoadBalancer(buildLoadBalancer(config))\n      .withSessionPool\n      .minSize(poolMin)\n      .withSessionPool\n      .maxSize(poolMax)\n      .withSessionPool\n      .maxWaiters(maxWaitersPerHost)\nval client = h2Settings.newClient(dest). @olix0r I've updated the PR.. ",
    "jbripley": "@olix0r has there been any progress of breaking out the gRPC generation into a separate SBT plugin yet? Otherwise, I'd like to help out with making that happen. We're planning to use this in a new gRPC based project at work, which means I can dedicate some time to help out with this.. ",
    "johnynek": "yeah, at Stripe we may also be interested in a finagle-grpc and could possibly spend some cycles on it. It there was a stand-alone project for that we would likely contribute some time to it (and at least send bug reports. :). any update on this @olix0r ?. ",
    "penland365": "+1 to @jbripley and @johnynek - I know it's extra dev cycles for y'all, but it would be great if buoyant was able to break out a finagle-grpc so we can contribute some cycles to it. We're kicking off a new project as well and want to go full bore gRPC / Finagle / Linkerd with it. . Any update here @olix0r ? Novolabs is more than willing to allocate some development cycles to help in any way we can with this. . ",
    "ChaoMai": "Hello, is project still available for application?\nI just find the linkerd project under the cncf today. But it's about to meet the deadline of the student application. So I  want to know whether there is a chance to apply this project?\nThank you. ",
    "anuradhacse": "Hi I saw this issue as a gsoc idea. I am interested in doing this. so I would like to know in detail about the issue. As I understood QUIC codec for netty is the requirement. Can you suggest any tips to get started. So as I understood I have to write a QUIC codec for Netty so that it can be\nused in linkerd. I briefly went through the draft specification document of\nQUIC protocol and looked at Netty codec. so do you have any suggestion on\nhow to proceed on this project.\nOn 11 March 2017 at 03:13, William Morgan notifications@github.com wrote:\n\n@anuradhacse https://github.com/anuradhacse great, this is going to be\na cool GSOC project!\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/linkerd/linkerd/issues/1078#issuecomment-285792771,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ALn-Y-n2Vo0JAT1sOoK71gNC1S3jmJYWks5rkcQUgaJpZM4MDoxL\n.\n. \n",
    "zhangmuxi": "Have you guys implemented this or not?. ",
    "mqliang": "@olix0r @klingerf IIUC, currently, we already have a linkerd-zipkin plugin, which is a linkerd telemeter plugins for writing trace data to zipkin, but this is dedicated to zipkin. And, we want implement a linkerd-opentracing plugin, which writes trace data to all tracing vendor (as long as it support OpenTracing) ?. ",
    "malafeev": "I think plugin is not enough. \nWe need a filter to have access to http headers of incoming and outgoing requests.\nFilter should:\n1. Extract span context from http headers of incoming requests (and later use this span context as parent for created span)\n2. Create span and inject it into http headers of outgoing request.\n3. Call Trace.recordBinary(\"opentracing-span\", span). It will be intercepted by OpenTracing linkerd plugin (using TelemeterInitializer)\nThe problem that there isn\u2019t a way to inject arbitrary filters via plugin (https://discourse.linkerd.io/t/is-it-possible-to-set-common-client-and-server-filter-via-plugin/493/2?u=malafeev). @SecretLiang linkerd-zipkin plugin based on finagle tracing (finagle inject/extract b3 http headers).\nI can create PR which will add Filter to inject/extract opentracing http headers.. @deebo91 please review it.\nMaybe you will propose another approach.. @adleong I will publish telemeter plugin.. @adleong I'm currently stuck because linkerd uses libthrift version 0.5\nbut Jaeger tracer uses libthrift version 0.9.2\nMost of OpenTracing tracers come with dependencies on transport libraries (thrift, grpc etc)\nand they sometimes conflict with application dependencies (jar hell).\n. yeah, trying to solve issue with shadowing (https://github.com/jaegertracing/jaeger-client-java/issues/332). Hi @adleong, no luck.\nProblem that only Jaeger tracer supports Tracer Resolver, but there is an issue\nwhich probably will not be fixed nearest time.\nI think we can close this PR for now. \n. ",
    "SecretLiang": "@malafeev Then, how linkerd-zipkin plugin handle this? Hardcoding the HTTP header in the core codebase, instead of injecting via plugin? And you mentioned at https://discourse.linkerd.io/t/is-it-possible-to-set-common-client-and-server-filter-via-plugin/493/2?u=malafeev that you can create a PR, do you mean a PR that add filter to have access to http headers ? . Two questions:\n\nWill the file system interpreter watch file modification events via Inotify ?\nShould the ZK interpreter watch a Znode ? You know, ZK provides a watch mechanism which can inform client data update.. \n",
    "chriscoomber": "Do you know when this might make it in? It looks like finagle 6.44 is soon to be released (given that the last one was about a month ago).\nI'm looking at using linkerd for HTTPS uplift (and other cool things, undoubtedly) and I like mutual TLS as a simple way to do service-service authentication (by checking that the client is trusted before serving it).. That last bit is interesting, I just read through your blog post and maybe linkerd-tcp is sufficient for my use case. Thanks for the info - I'll keep an eye on the todo!. ",
    "pvcnt": "Hi, thanks for your answer! Still, it's strange to get an infinite loop,\ninstead of an error, isn't it? And it's the same thing if I reference in\nlinkerd a not-yet created namespace in namerd (with Consul storage), the\nnamerd's linkerd tab takes forever to load (because of that infinite loop I\nguess).\n2017-02-27 23:51 GMT+01:00 Alex Leong notifications@github.com:\n\nHi @pvcnt https://github.com/pvcnt! We require that the namerd/dtabs\npath exists (although it can be empty). You should create this path as a\nsetup step before running namerd.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/linkerd/linkerd/issues/1105#issuecomment-282883226,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA5pejztjv-SEULOC_f3YiVZxfkQkazSks5rg1OEgaJpZM4MMJm9\n.\n. \n",
    "djKooks": "@olix0r Thanks! It works.. @adleong Oh Thnx. It was my mistake. I'm not used to in thrift...\nI'll close this one.. ",
    "zillani": "This helped! I have uninstalled the 2.x and upgraded to 3.x with proper instructions here, https://github.com/google/protobuf/blob/master/src/README.md. @DukeyToo Awesome!. Awesome! thanks @klingerf . ok, got it. What is recommended b/w\n- single linkerd daemonset with multi-router configuration? \n(https://discourse.linkerd.io/t/no-hosts-available-error-with-multi-router-configuration/433)\n- multiple linkerd daemonset as you suggested?. @adleong Sure, thanks for the follow-up.. @adleong done :) thanks for approving the smallest PR. initially, I thought deprecation was wrongly put there & overlooked the description.\ntrustCert accepts a LIST of filePaths\ntrustCertBundle accepts a cacert bundle file. ",
    "DukeyToo": "Done - http://stackoverflow.com/questions/tagged/linkerd. Increasing max memory seemed to work.  . Yes @olix0r  the zookeeper runs on same nodes as namerd.  \nTo be clear, this configuration works, until it doesn't.  At some point, linkerd => namerd communication breaks down and things break.. Thanks for the pointer @olix0r - I found what was causing this, and problem is no longer reproducible.   In my original report, I omitted the linkerd config that is to blame \ud83d\udc4e :\nidentifier:\n    - kind: io.l5d.path\n      segments: 5\n    - kind: io.l5d.path\n      segments: 4\n    - kind: io.l5d.path\n      segments: 3\n    - kind: io.l5d.path\n      segments: 2\n    - kind: io.l5d.path\n      segments: 1\nWe did the above because we have varying lengths of paths that we need to support.  And it worked.  The 5 segment identifier would be presented with unique paths e.g. /a/b/c/cust-123/things, fail to match, and the next identifier would try to match, and eventually it would fall through to the 3 segment identifier and be matched successfully.\nI think that the problem was that the 5 segment identifier still asked namerd to resolve the 5 segments, which caused a lot of load as each one was unique (because it contained a customer id).  Each unique 5 segment path was also being tracked individually in metrics and probably in in-memory cache.  Eventually, load got too much and things broke.. The dtabs seem to work well, although they can be sensitive to the ordering.  We keep it all in a single dtab namespace.  Some sanitized examples:\n/http/app/v15/x=>/#/io.l5d.marathon/microservices/stage/black/app-x-edge-v15\n/http/app/v12/specialcase/a=>/#/io.l5d.marathon/microservices/stage/black/app-specialcase-a-edge-v15\n/http/app/v11/specialcase/b=>/#/io.l5d.marathon/microservices/stage/black/app-specialcase-b-edge-v15\n/http/admin/ping => /linkerd-admin\n/http/1.1/*/cwl-linkerd.internal-csp1-e1-npe-someorg.com => linkerd-admin\n/linkerd/admin => /$/inet/0.0.0.0/9990\nMost microservices have a 3 segment context path like app/version/endpoint but we've inherited some with more (e.g. specialcases above), and we have some non-microservices with less.  We also have some host-based routing.  . Yes, I do.. I am able to reproduce this as well in 1.0.2, but not in 1.0.0 linkerd.   Sanitized linkerd logs look like this:\nJun  3 00:46:06 cwl-mesos-minions-v019-1343 linkerd: com.twitter.finagle.RequestTimeoutException: exceeded 10.seconds to unspecified while dyn binding /http/something. Remote Info: Not Available\nJun  3 00:46:06 cwl-mesos-minions-v019-1343 linkerd: E 0603 00:46:06.705 UTC THREAD10 TraceId:15a4bd604b272638: service failure\nJun  3 00:46:13 cwl-mesos-minions-v019-1343 linkerd: E 0603 00:46:13.686 UTC THREAD10 TraceId:aeb9dffa88f5b1e6: service failure\nJun  3 00:46:13 cwl-mesos-minions-v019-1343 linkerd: com.twitter.finagle.RequestTimeoutException: exceeded 10.seconds to unspecified while dyn binding /http/something. Remote Info: Not Available\nJun  3 00:46:43 cwl-mesos-minions-v019-1343 linkerd: E 0603 00:46:43.875 UTC THREAD10 TraceId:ab394c9057dc9fdc: service failure\nJun  3 00:46:43 cwl-mesos-minions-v019-1343 linkerd: com.twitter.finagle.RequestTimeoutException: exceeded 10.seconds to unspecified while dyn binding /http/something. Remote Info: Not Available\nJun  3 00:47:42 cwl-mesos-minions-v019-1343 linkerd: I 0603 00:47:42.929 UTC THREAD10 TraceId:c73fac633d696d92: FailureAccrualFactory marking connection to \"io.l5d.mesh\" as dead. Remote Address: Inet(cwl-mesos-masters.service.consul/172.21.0.21:4182,Map())\nJun  3 00:47:42 cwl-mesos-minions-v019-1343 linkerd: W 0603 00:47:42.930 UTC THREAD10 TraceId:c73fac633d696d92: Exception propagated to the default monitor (upstream address: /172.21.0.32:15090, downstream address: namerd.service.consul/172.21.0.21:4182, label: io.l5d.mesh).\nJun  3 00:47:42 cwl-mesos-minions-v019-1343 linkerd: Reset.Cancel\nJun  3 00:47:43 cwl-mesos-minions-v019-1343 linkerd: E 0603 00:47:43.685 UTC THREAD10 TraceId:2a0e066147086bb4: service failure\netc.. @klingerf , configs below.  We're running a linkerd per host, with a cluster of 3 namerd hosts, which also run zookeeper and some other stuff.  The linkerd hosts come and go but the namerd hosts typically stay for a long time.\nLike @Ashald we did not see the issue immediately.  It occurred after some undetermined time.  Restarting all of the linkerd processes fixed it for a while, until the next occurrence.  We reverted pretty quickly to 1.0.0, so it only occurred for us about 3 times total over 2 days.   \nLinkerd yaml:\nadmin:\n    port: 9990\n  routers:\n  - protocol: http\n    label: microservices_dev\n    dstPrefix: /http\n    httpAccessLog: /var/log/linkerd_access_microservices_dev.log\n    maxHeadersKB: 16\n    interpreter:\n      kind: io.l5d.mesh\n      dst: /$/inet/namerd.service.consul/4182\n      experimental: true\n      root: /microservices-dev\n    identifier:\n      - kind: io.l5d.path\n        segments: 3\n      - kind: io.l5d.path\n        segments: 2\n      - kind: io.l5d.path\n        segments: 1\n    servers:\n    - port: 4140\n      ip: 0.0.0.0\n  - protocol: http\n    label: microservices_stage\n    dstPrefix: /http\n    httpAccessLog: /var/log/linkerd_access_microservices_stage.log\n    maxHeadersKB: 16\n    interpreter:\n      kind: io.l5d.mesh\n      dst: /$/inet/namerd.service.consul/4182\n      experimental: true\n      root: /microservices-stage\n    identifier:\n      - kind: io.l5d.path\n        segments: 3\n      - kind: io.l5d.path\n        segments: 2\n      - kind: io.l5d.path\n        segments: 1\n    servers:\n    - port: 4160\n      ip: 0.0.0.0\n  - protocol: http\n    label: proxy_by_host\n    dstPrefix: /http\n    httpAccessLog: /var/log/linkerd_access_host.log\n    maxHeadersKB: 16\n    interpreter:\n      kind: io.l5d.mesh\n      dst: /$/inet/namerd.service.consul/4182\n      experimental: true\n      root: /default\n    identifier:\n      - kind: io.l5d.header.token\n        header: Host\n    servers:\n    - port: 4141\n      ip: 0.0.0.0\n    - port: 80\n      ip: 0.0.0.0\n  - protocol: http\n    label: proxy_by_path3\n    dstPrefix: /http\n    httpAccessLog: /var/log/linkerd_access_path3.log\n    maxHeadersKB: 16\n    interpreter:\n      kind: io.l5d.mesh\n      dst: /$/inet/namerd.service.consul/4182\n      experimental: true\n      root: /default\n    identifier:\n      - kind: io.l5d.path\n        segments: 3\n    servers:\n    - port: 4153\n      ip: 0.0.0.0\n  - protocol: http\n    label: proxy_by_path2\n    dstPrefix: /http\n    httpAccessLog: /var/log/linkerd_access_path2.log\n    maxHeadersKB: 16\n    interpreter:\n      kind: io.l5d.mesh\n      dst: /$/inet/namerd.service.consul/4182\n      experimental: true\n      root: /default\n    identifier:\n      - kind: io.l5d.path\n        segments: 2\n    servers:\n    - port: 4152\n      ip: 0.0.0.0\n  - protocol: http\n    label: proxy_by_path1\n    dstPrefix: /http\n    httpAccessLog: /var/log/linkerd_access_path1.log\n    maxHeadersKB: 16\n    interpreter:\n      kind: io.l5d.mesh\n      dst: /$/inet/namerd.service.consul/4182\n      experimental: true\n      root: /default\n    identifier:\n      - kind: io.l5d.path\n        segments: 1\n    servers:\n    - port: 4151\n      ip: 0.0.0.0\n  telemetry:\n  - kind: io.l5d.recentRequests\n    sampleRate: 1.0\n  - kind: io.l5d.prometheus\n  - kind: io.l5d.influxdb\nNamerd config:\nadmin:\n  port: 9991\nstorage:\n  kind: io.l5d.zk\n  zkAddrs:\n  - host: 127.0.0.1\n    port: 2181\n  pathPrefix: /dtabs\n  sessionTimeoutMs: 10000\nnamers:\n- kind:      io.l5d.marathon\n  prefix:    /io.l5d.marathon\n  host:      127.0.0.1\n  port:      8080\n  useHealthCheck: true\n  ttlMs:     5000\n- kind: io.l5d.rewrite\n  prefix: /ipport\n  pattern: \"/{ip}/{port}\"\n  name: \"/$/inet/{ip}/{port}\"\n- kind: io.l5d.consul\n  prefix: /consul\n  useHealthCheck: true\n  consistencyMode: stale\n  failFast: true\n  preferServiceAddress: true\n- kind: io.l5d.consul\n  prefix: /consultag\n  useHealthCheck: true\n  consistencyMode: stale\n  includeTag: true\n  failFast: true\n  preferServiceAddress: true\ninterfaces:\n- kind: io.l5d.thriftNameInterpreter\n  ip: 0.0.0.0\n  port: 4100\n- kind: io.l5d.httpController\n  ip: 0.0.0.0\n  port: 4181\n- kind: io.l5d.mesh\n  ip: 0.0.0.0\n  port: 4182. Mostly we have tweaked the client and service parameters, for example change the service totalTimeoutMs/responseClassifier or client failure params.  It is not something we have a need to do often - as you say, most of the stuff that changes often is outside of that config, in namerd in our case.. @wmorgan Not for us - we do deploys of a whole new cluster of machines in that scenario.  It is a  zero-downtime in many cases - it is managed outside of linkerd though, and it is slow (hours instead of minutes).. @zackangelo there are 8 cpu cores on the hosts.  \n@amitsaha we're analyzing /proc data on linux, RSS memory only (using https://github.com/influxdata/telegraf/tree/master/plugins/inputs/procstat).   My assumption is that the 2G+ usage shown on the chart is approx 1G heap + 1G direct memory used by Netty + some MetaSpace.   The 2nd line of the stack trace shows the specifics of how much total direct memory was allowed/allocated and how much was used.. @zackangelo no, I cannot find that error in our logs, however old logs have rotated out from when we last saw the error, so it might have been there.. @siggy sure, I will try. @siggy @hawkw I took a build from master at https://github.com/linkerd/linkerd/tree/c6f0d2eaeecca80c60314e6f6cb852a31870877a but was unable to reproduce in our test environment, because it takes more than 24 hours of traffic until I can see the issue occurring - and our test environment linkerd instances are restarted every day by an automated process.\nI did plot rt:io_l5d_mesh:stream:open_streams over time for various routers in test environment, and it does trend upward over time each day - so if the bug is indicated by gradual increases in that metric, then it is not seem like it is fixed for me.\nExample (the drops are due to instances restarting as-scheduled): \n\nI also looked for rt:client:stream:open_streams and rt:server:stream:open_streams in /admin/metrics.json, but did not find those as metrics.. I have captured some logs with LOCAL_JVM_OPTIONS=-Dio.netty.leakDetection.level=paranoid -Dio.netty.leakDetection.maxRecords=500 at https://gist.github.com/DukeyToo/582285e2b3603ce9468e4f2c2f21be75. Yes, that is correct. I am busy testing 1.3.4.  It takes a few days to reproduce, so I'm hoping for the best at this stage.  Should know soon.. I restarted linkerd this afternoon, before it reached the memory usage where we have seen this issue in the past ... so it will be a while longer before I know if it still happens. \nIf nothing else, it seems to be taking longer to get to the memory usage threshold where the bug occurred :). This is not fixed.  Took 5 days to reproduce, but it happened again.\nLet me know if you need any more info.  Metrics are logged to an InfluxDb so I can post whatever we gather that might be relevant.  . Thanks @ganasubrgit for help reproducing - the above \"docker linkerd\" minimal example is from the following linkerd yaml: \n```\nadmin:\n  ip: 0.0.0.0\n  port: 9990\nrouters:\n- protocol: http\n  interpreter:\n    kind: io.l5d.namerd.http\n    experimental: true\n    dst: /$/inet/n5d/4180\n  servers:\n  - port: 4140\n    ip: 0.0.0.0\nusage:\n  enabled: false\n```\nThe \"internal\" committed memory on that is about 450MB, which is not as bad as my first example, but still illustrates the issue of linkerd reserving a lot of memory on top of what we assigned for heap.. On linkerd 1.4.2, this is the equivalent output, showing much lower internal memory usage:\n```\nNative Memory Tracking:\nTotal: reserved=2286556KB, committed=1196144KB\n-                 Java Heap (reserved=1048576KB, committed=1048576KB)\n                            (mmap: reserved=1048576KB, committed=1048576KB)\n\n\nClass (reserved=1088411KB, committed=45383KB)\n                            (classes #7272)\n                            (malloc=923KB #7015)\n                            (mmap: reserved=1087488KB, committed=44460KB)\n\n\nThread (reserved=24777KB, committed=24777KB)\n                            (thread #25)\n                            (stack: reserved=24672KB, committed=24672KB)\n                            (malloc=76KB #127)\n                            (arena=28KB #48)\n\n\nCode (reserved=50162KB, committed=2778KB)\n                            (malloc=242KB #1169)\n                            (mmap: reserved=49920KB, committed=2536KB)\n\n\nGC (reserved=12840KB, committed=12840KB)\n                            (malloc=9404KB #209)\n                            (mmap: reserved=3436KB, committed=3436KB)\n\n\nCompiler (reserved=165KB, committed=165KB)\n                            (malloc=35KB #105)\n                            (arena=130KB #2)\n\n\nInternal (reserved=5362KB, committed=5362KB)\n                            (malloc=5330KB #12053)\n                            (mmap: reserved=32KB, committed=32KB)\n\n\nSymbol (reserved=10603KB, committed=10603KB)\n                            (malloc=9316KB #72675)\n                            (arena=1287KB #1)\n\n\nNative Memory Tracking (reserved=1468KB, committed=1468KB)\n                            (malloc=6KB #77)\n                            (tracking overhead=1461KB)\n\n\nArena Chunk (reserved=186KB, committed=186KB)\n                            (malloc=186KB)\n\n\nUnknown (reserved=44008KB, committed=44008KB)\n                            (mmap: reserved=44008KB, committed=44008KB)\n```. Full reproducible example is here: https://github.com/DukeyToo/linkerd-docker-example - thanks again @ganasubrgit for putting that together.\n\n\nTo reproduce, change linkerd version as needed in the docker compose file, then run:\ndocker-compose up -d\ndocker exec -it docker-linkerd_l5d_1 /bin/bash\napt-get update && apt-get install -y openjdk-8-jdk\njcmd 1 VM.native_memory summary. ",
    "chriswessels": "We're using gRPC and obviously service names are defined in PascalCase, and we'd like to define our k8s services with kebab-case.\nFor example, we may define our service name in our proto as FraudModel, and we'd like that to resolve to the fraud-model k8s service, but currently it only resolves to the fraudmodel service.\nIs slightly more intelligent case conversion/resolution possible? . Thanks @esbie! Will take a look \ud83d\ude04 . ",
    "jbkc85": "updated configMap, forgot I changed it to 'test-linkerd-config' to avoid interrupting my work on Linkerd. ",
    "ppwfx": "Isn't the path identifier intended to be used for routing? At least that's how the mainstream implementations are behaving.\nE.g. given the configuration below, /foo/yo would be routed to the service listed in -path: /foo. \napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: test\nspec:\n  rules:\n  - host: foo.bar.com\n    http:\n      paths:\n      - path: /foo\n        backend:\n          serviceName: s1\n          servicePort: 80\nI actually don't see any need for providing additional vendor specific configuration. Please correct me if I'm wrong.. :). Alright, got it. Makes total sense :). ",
    "FuzzOli87": "I completely agree with this being an annotation similar to what the \"rewrite\" annotation does on NGINX controller configurations.\nIt could allow us to namespace apps by path which could help in directing to different versions etc.. ",
    "pawelprazak": "We are currently using Traefik and we need this feature, so we'd like to implement it.\nAny pointers regarding API and implementation are very much welcome.\nI propose the simplest implementation, with a similar behavior to Traefik's pathprefixstrip: \n\nMatch request prefix path and strip off the path prefix prior to forwarding the request to the backend\n\nIn our case, adding linkerd.io/pathprefixstrip: true annotation on Ingress would match any path that starts with the defined prefix and strip this prefix.\nExample, given ingress definition:\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: my-ingress\n  annotations:\n    kubernetes.io/ingress.class: \"linkerd\"\n    linkerd.io/pathprefixstrip: true\nspec:\n  rules:\n  - http:\n      paths:\n      - path: /hello\n        backend:\n          serviceName: hello\n          servicePort: 7777\n\nand a request for path /hello/world, service hello will only get /world.\nAs an additional or alternative configuration method we could use:\n...\n  identifier:\n    kind: io.l5d.ingress\n    pathprefixstrip: true\n.... Thank you for your thoughts, esp. for pointing out the k8s API, I was not aware it explicitly allows regexps.\n\nOur current quick and dirty implementation just strips the request path using the ingress path as the prefix. As far as I understand what you wrote, it would brake the existing behavior.\nIMHO the segments consumption, like in path identifier, is not very useful, it fails for a simple use case we have, e.g.:\n/v1/hello\n/v2/world\n/heathcheck\n\nYou can either match the first 2 path prefixes or the last one, not all 3.\nA third option would be to use capturing groups, so with a flag use-capturing-groups e.g.:\n/test/path/                    and  /test/path/qwer/asdf/1234  ->  '' (empty - edge case)\n/test/path/(.+)                and  /test/path/qwer/asdf/1234  ->  /qwer/asdf/1234\n/test/path/(.+)/asdf/([0-9]+)  and  /test/path/qwer/asdf/1234  ->  /qwer/1234\n/test/path/(.+)/(asdf)/(.+)    and  /test/path/qwer/asdf/1234  ->  /qwer/asdf/1234\n/test/.+/(.+)/(asdf)/(.+)      and  /test/path/qwer/asdf/1234  ->  /qwer/asdf/1234\n\nWe could also skip the flag and just leave the path as is if there is no capture group hits, but that feel error prone.. I've created a PR with an initial version to get some feedback early.\nTODO:\n- real tests on Kubernetes\n- does h2.IngressIdentifier also needs to be changed? If so, how to deals with the lack of uri field\n- update docs. I've added / striping, and I agree this might be tricky for someone to figure out.\nI corrected the local: false + global:true corner case, also local: none + global: true still evaluates to true.\nTo support the above situations I need to be able to differentiate between false and none for local, but I could change it for global.. I've implemented h2 ingress identifier, added more tests, signed the CLA. Also there was a manual test for previous version, need to retest.. We retested manually on k8s 1.6.2 and 1.5.4, the feature looks fine.\nWe've noticed what looks like unrelated problem with k8s returning \"too old resource version\" and causing finagle to fail:\n[l5d-1jlmb l5d] I 0529 10:29:13.782 UTC THREAD20: k8s no suitable rule found in 'hello-world' for request 'l5d.ingress.k8s.dev.redacted.com' '/v4/redacted/ping'\n[l5d-1jlmb l5d] I 0529 10:29:13.782 UTC THREAD20: k8s found rule matching 'l5d.ingress.k8s.dev.redacted.com' '/v4/redacted/ping': 'IngressPath(None,Some(/v4/redacted(/?.*)),Some(true),identity,redacted,80)'\n[l5d-1jlmb l5d] E 0529 10:29:13.793 UTC THREAD20: service failure\n[l5d-1jlmb l5d] com.twitter.finagle.NoBrokersAvailableException: No hosts are available for /svc/identity/80/redacted, Dtab.base=[/svc=>/#/io.l5d.k8s], Dtab.local=[]. Remote Info: Not Available\n[l5d-1jlmb l5d] \n[l5d-1jlmb l5d] I 0529 10:29:18.609 UTC THREAD17: k8s no suitable rule found in 'hello-world' for request 'l5d.ingress.k8s.dev.redacted.com' '/v4/redacted/ping'\n[l5d-1jlmb l5d] I 0529 10:29:18.610 UTC THREAD17: k8s found rule matching 'l5d.ingress.k8s.dev.redacted.com' '/v4/redacted/ping': 'IngressPath(None,Some(/v4/redacted(/?.*)),Some(true),identity,redacted,80)'\n[l5d-1jlmb l5d] E 0529 10:29:18.613 UTC THREAD17: service failure\n[l5d-1jlmb l5d] com.twitter.finagle.NoBrokersAvailableException: No hosts are available for /svc/identity/80/redacted, Dtab.base=[/svc=>/#/io.l5d.k8s], Dtab.local=[]. Remote Info: Not Available\n[l5d-1jlmb l5d] \n[l5d-1jlmb kubectl] I0529 10:29:25.369975       1 proxy_server.go:91] /apis/extensions/v1beta1/ingresses matched ^.*\n[l5d-1jlmb kubectl] I0529 10:29:25.370009       1 proxy_server.go:91] localhost matched ^localhost$\n[l5d-1jlmb kubectl] I0529 10:29:25.370016       1 proxy_server.go:130] Filter accepting GET /apis/extensions/v1beta1/ingresses localhost\n[l5d-1jlmb kubectl] I0529 10:29:25.370115       1 round_trippers.go:398] curl -k -v -XGET  -H \"Content-Type: application/json\" -H \"X-Forwarded-For: 127.0.0.1\" -H \"X-B3-Traceid: efa680e3723dcfe5\" -H \"X-B3-Spanid: efa680e3723dcfe5\" -H \"X-B3-Flags: 0\" -H \"Authorization: Bearer eyJhbGciOiJS...N0j2A\" -H \"Finagle-Ctx-Com.twitter.finagle.retries: 0\" -H \"User-Agent: kubectl/v1.6.2 (linux/amd64) kubernetes/477efc3\" https://10.3.0.1:443/apis/extensions/v1beta1/ingresses\n[l5d-1jlmb kubectl] I0529 10:29:25.454060       1 round_trippers.go:417] GET https://10.3.0.1:443/apis/extensions/v1beta1/ingresses 200 OK in 83 milliseconds\n[l5d-1jlmb kubectl] I0529 10:29:25.454091       1 round_trippers.go:423] Response Headers:\n[l5d-1jlmb kubectl] I0529 10:29:25.454099       1 round_trippers.go:426]     Content-Type: application/json\n[l5d-1jlmb kubectl] I0529 10:29:25.454104       1 round_trippers.go:426]     Content-Length: 1785\n[l5d-1jlmb kubectl] I0529 10:29:25.454109       1 round_trippers.go:426]     Date: Mon, 29 May 2017 10:29:25 GMT\n[l5d-1jlmb kubectl] I0529 10:29:25.465046       1 proxy_server.go:91] /apis/extensions/v1beta1/ingresses matched ^.*\n[l5d-1jlmb kubectl] I0529 10:29:25.465071       1 proxy_server.go:91] localhost matched ^localhost$\n[l5d-1jlmb kubectl] I0529 10:29:25.465079       1 proxy_server.go:130] Filter accepting GET /apis/extensions/v1beta1/ingresses localhost\n[l5d-1jlmb kubectl] I0529 10:29:25.465177       1 round_trippers.go:398] curl -k -v -XGET  -H \"X-B3-Traceid: 77d358e268fb0970\" -H \"User-Agent: kubectl/v1.6.2 (linux/amd64) kubernetes/477efc3\" -H \"X-B3-Spanid: 77d358e268fb0970\" -H \"X-B3-Flags: 0\" -H \"Authorization: Bearer eyJhbGciOiJ...teN0j2A\" -H \"Finagle-Ctx-Com.twitter.finagle.retries: 0\" -H \"Content-Type: application/json\" -H \"X-Forwarded-For: 127.0.0.1\" https://10.3.0.1:443/apis/extensions/v1beta1/ingresses?watch=true&resourceVersion=2789186\n[l5d-1jlmb kubectl] I0529 10:29:25.469344       1 round_trippers.go:417] GET https://10.3.0.1:443/apis/extensions/v1beta1/ingresses?watch=true&resourceVersion=2789186 200 OK in 4 milliseconds\n[l5d-1jlmb kubectl] I0529 10:29:25.469382       1 round_trippers.go:423] Response Headers:\n[l5d-1jlmb kubectl] I0529 10:29:25.469406       1 round_trippers.go:426]     Content-Type: application/json\n[l5d-1jlmb kubectl] I0529 10:29:25.469460       1 round_trippers.go:426]     Date: Mon, 29 May 2017 10:29:25 GMT\nIt might be something related to the usage of ELB between etcd and apiserver or something else, but doesn't look related to this PR.. How do I update docs?. All done, waiting for the final review.. Replacement is a different feature (and I can't see how you could make it work with k8s Ingress), this was a PR required for our POC, we finished that and concluded that linkerd is not a good fit for us, so I'd appreciate if we could close this PR quickly and avoid scope creep.. good catch, dough I didn't expect an immutable type to have a mutable field\nhow do you propose to fix that?. default intellij, will fix that. it means that the annotation is not present and it's necessary to support this requirement:\n\nIf useCapturingGroupsGlobal is true but useCapturingGroups is false, the whole thing evaluates to true which seems potentially confusing. I'd either default to useCapturingGroups if defined (since it's a more local scope), or remove the capture group settings from the config altogether.\n\nI think the code is self explanatory also haven't seen any comments in way more supprising parts of code. that looks wrong on so many levels... but if you insist I could do that\na mutable field of an immutable object is asking for trouble if you ask me. ",
    "valdemon": "As this has been closed - does it mean the feature is perceived as unimportant and disappears from the Linkerd roadmap?. @klingerf Our use case leverages the automatic TLS certificate lifecycle management for services exposed via k8s Ingress using LetsEncrypt and the kube-lego mentioned before by @esbie.\nCurrently I'm using NGINX Ingress Controller and it works for me well enough. We might be interested in switching to Linkerd Ingress Controller though, as soon as it provides this feature.\nExcept of having a single Ingress class (less of artifacts to deploy) I guess this would likely also give us better traceability (metrics) of network communication flow (provided by Linkerd-viz, Zipkin etc), starting already from the Ingress controller associated k8s service.\nBelow is our current Ingress example:\nyaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: my-service\n  namespace: my-system\n  annotations:\n    kubernetes.io/tls-acme: \"true\"\n    ingress.kubernetes.io/secure-backends: \"true\"\n    kubernetes.io/ingress.class: \"nginx\"\n    external-dns.alpha.kubernetes.io/hostname: \"my-service.example.org.\"\n    external-dns.alpha.kubernetes.io/ttl: 60\nspec:\n  tls:\n    - hosts:\n      - my-service.example.org\n      secretName: my-service-ingress-tls\n  rules:\n  - host: my-service.example.org\n    http:\n      paths:\n      - backend:\n          serviceName: my-service\n          servicePort: 4140\n. ",
    "captjt": "No problem @wmorgan. I think the releases link is broken as well. I think it needs to reference (/releases/latest) rather than just release. \nhttps://help.github.com/articles/linking-to-releases/\nI can fix that as well if you'd like. . No problem @adleong!. ",
    "hhtpcd": "Thanks both for the feedback, glad to hear this could be worthwhile. Agree about the env var however I don't know enough about how DCOS authentication is set up to know if it can be configured with HTTP auth at the same time, I'll look into that and try to justify exclusivity, or lack of.\nWill rework adding the auth header as well, into the existing initializer.. I agree with the suggestions and pulling the basic auth out into a separate object as it looks like it will keep the auth code tidier. My understanding is that DCOS is more of a focus for Mesosphere than standalone Mesos/Marathon atm so the Basic auth option may go away or become a lot less relevant/supported. Keeping it separate will help further down the line if it needs to be removed.\nI expect the match block after MarathonSecret.load() is not ideal, but conceptually feels much nicer than forcing everything into AuthRequest. Thanks for the continued feedback, it's much appreciated. . Following your comments, I've removed all the added logic from the Authenticator object for basic auth and implemented the BasicAuthenticatorFilter as suggested, which I now realise is very similar to the Consul API namer.. I've updated the docs with the env var details, but it looks like the tests failed with an inotify error. . I'm going to close this as support was added in https://github.com/linkerd/linkerd/pull/1148\nDocs: https://github.com/linkerd/linkerd/blob/master/linkerd/docs/namer.md#marathon-authentication. ",
    "nm-harry": "I've moved the basic auth into the existing Authenticator flow and set up an environment variable, on the assumption that you can use one or the other. I haven't included any checks for providing both. \nLet me know if this is the sort of implementation you were thinking of. As before, I expect this is not ideal Scala so keen to get feedback. If this is the right direction, I can amend tests etc after. \nedit: I've logged into Github as the wrong user, but I'll leave this message and go and get a coffee. . ",
    "chrisboulton": "Hey @esbie, yep - I agree with that WRT to namerd for dtabs etc. I was more thinking for the things that are less dynamic, such as the router definitions, telemetry settings, etc themselves.. ",
    "rmichela": "Envoy has implemented this using the forwarded-client-cert header. It would be wonderful if linkerd also implemented this issue in the same way. This request is also needed for implementing SPIFFE - see #1570. \nhttps://www.envoyproxy.io/docs/envoy/latest/configuration/http_conn_man/headers.html#config-http-conn-man-headers-x-forwarded-client-cert. @cponomaryov What's the status of your x-forwarded-client-cert work?. As an alternative, you could update the /admin/ping handler to return an empty response when a HEAD request is made.. The SPIFFE SVID client certificate specification can be found at https://github.com/spiffe/svid/blob/master/SPECIFICATION.md. In addition to validating SVID client certs, it would be great if Linkerd could populate the x-forwarded-client-cert header when proxying an incoming request to the supporting service. x-forwarded-client-cert would allow services to make detailed authorization decisions using the SPIFFE ID from the client certificate.\nhttps://lyft.github.io/envoy/docs/configuration/http_conn_man/headers.html#x-forwarded-client-cert. What's the status of this PR? Will it be in the next version of Linkerd?. Some(arg.status) works.. ",
    "cponomaryov": "@rmichela Opened a PR #1656 . Done. I made a fix according to code review, but it seems that there is an unrelated test failure. Could you please rerun the build because I have no permission to do this.. Thanks!. Done. @adleong Done. I fixed that test.. @adleong, fixed according to your code review. @adleong fixed. ",
    "briansmith": "1802 is related to this.. @shakti-das Thanks for reporting this. I agree with you that it would be good to expose the DNS Name entries too.\nBasically, the X-Forwarded-Client-Cert feature was implemented as described in https://www.envoyproxy.io/docs/envoy/latest/configuration/http_conn_man/headers.html#x-forwarded-client-cert, which also doesn't consider the DNS name entries.\nIn order to extend it to support DNS names, I suggest that we include one DNS:<dns name> entry for each DNS name. If there are three DNS names, a.example.com, b.example.com, c.example.com, then the header field value would include \"DNS:a.example.com;DNS:b.example.com;DNS:c.example.com\" in addition to the other information.\nWDYT?. In the case where the certificate has both URI names, we have to use \"SAN:\" instead of \"URI:\" for backward compatibility, e.g. SAN:https://example.com;DNS:a.example.com;DNS:b.example.com.. > after x-forwarded-client-cert: Hash=468ed33be74eee6556d90c0149c1309e9ba61d6425303443c0748a02dd8de688;SAN=\"SAN:https://example.com;DNS:example.com\"\nNo, I'm thinking it should be like this:\nHash=468ed33be74eee6556d90c0149c1309e9ba61d6425303443c0748a02dd8de688;SAN=https://example.com;DNS=example.com.\nBasically \"SAN=\" would prefix each URI and \"DNS=\" would prefix each DNS name. Presumably we could have \"Email=\" for email addresses too.\n[Edit: fix typo s/,/;/]. > But that would make it part of the XFCC value and not SAN as SAN itself is an element of the XFCC value.\nI understand that. Basically, we never should have named \"SAN\" as \"SAN\" but instead should have named it \"URI\". I'm suggesting we take \"SAN\" to mean \"URI\" for backward compatibility only, and use \"DNS\" for DNS names (and \"Email\" for email names). It's inconsistent but I think this is the clearest design given backward compatibility concerns. If you have an alternate suggestion, I am open to it.. The advantage of implementing the Envoy approach is that any code that works with Envoy would automatically work with Linkerd. The disadvantage of the Envoy approach is that every application that tries to parse the SAN fields has to implement a check \"is this a DNS name or a URI or an email address or what?\"\nThe advantage of the way I propose is that such parsing is avoided. The disadvantage of the approach I suggested is that any app transitioning from Envoy -> Linkerd would have  to be modified to do do it a slightly different way.\n@adleong @wmorgan What do you think?\n. The name of the setting was found by looking at the source code: https://github.com/linkerd/linkerd/blob/5e74bcc8ea1617af5a058a936d4e5157a63d3700/admin/src/main/scala/io/buoyant/admin/AdminConfig.scala#L10. I changed the summary of this issue to say \"new identifier\" instead of \"new namer\" to better reflect what seems to be needed.\nI have to admit that I don't understand the aversion to identifiers mutating the request URI. I think there are probably two mental models that are common, the linkerd model and another model that is what the people requesting this feature have. I can understand that if one has fully internalized the linkerd model then the other model might lead to confusing results. However, there's no denying that at least some people (probably many people) have a need for this functionality. Like Alex said, linkerd has already introduced the concept of mutating identifiers with the Path identifier, so a new one that mutates the request URI more drastically doesn't seem to actually introduce anything new conceptually.. > @briansmith I could use some SSL expertise in understanding what the default_md is used for when generating pkcs#8 keys and\nIn the openssl.cfg file, there are two places where sha256 is mentioned, as there are two default_md settings. The first time I experimented with this, I only saw & changed one of them, in the [ CA_default ] section, but the one that seems to matter here is the one in the [ req ] section.\nAfter I changed both instances of sha256 to sha512, then I re-ran the openssl req command that was linked to, and then I ran openssl x509 -in certificates/cacertificate.pem -text which showed that the certificate is using Signature Algorithm: sha512WithRSAEncryption\n\nhow to configure the Netty SSLContext to be able to handle such keys. Is the message digest the same as the cipher?\n\nAre you able to reproduce the failure in this bug report? If so, it would be helpful if you could tell me the exact steps to reproduce, preferably without Kubernetes, starting from \"I just downloaded Linkerd and I don't even know how to start it or how to given the configuration file to it.\" That would help me figure out what Netty stuff I need to learn to answer this question.. If you can, email the certificate chain that doesn't work, without the private keys, to brian@buoyant.io.. @Capitrium That's right, I got them! Thanks! I don't see anything out of the ordinary with those certificates. I actually just filed #1876 to make our example certificates look more like what you sent!\nI'm now going to attempt to create my own root CA and end-entity certificate with CFSSL and see if I can reproduce the issue with CFSSL-generated certificates in linkerd.. > I don't see anything out of the ordinary with those certificates.\nI was wrong! openssl x509 -in linkerd.pem -text shows:\nX509v3 Subject Alternative Name:\n                DNS:\nYour certificate has a subjectAltName field with one dNSName entry that is blank (length == 0)! There are two problems here:\n\nA dNSName field cannot be blank! It must be a valid DNS name.\nIf your certificate includes a subjectAltName extension at all then the CN of the subject will be ignored.\n\nTo fix this, when you use CFSSL, you need to pass the hostnames that you want your certificate to be valid for on the command line as explained in the CFSSL documentation. In particular, CFSSL seems to always create a subjectAltName field, which is good practice, so trying to use the Subject CN field will not work.. That is, either use cfssl gencert -ca cert -ca-key key -hostname=linkerd csr.json or use the hosts field in your JSON config file. Probably using the -hostname command line option is better because then you can use the same configuration file for all certificates.. Great! I'm glad I could help and I'm glad there's no bug to fix here!. > @briansmith Turns out we jumped the gun a little here, we're still having TLS issues - should we open a new bug report or reopen this one?\nIt's fine to keep this open. Please email me the new certificates (without the private keys). Thanks!. I emailed the contact that emailed me the certificate. Please provide the exact steps to reproduce here, starting from the kubectl apply step(s) that add the things being tested, along with the exact YAML input file for kubectl apply, and the commands you used to trigger the failing requests (curl?). Thanks!. openssl, up until the very latest, is very annoying with respect to generating certificates that have subjectAltName set. We should consider providing examples using a different tool, like CFSSL, in addition to or instead of OpenSSL.. > Dechunk response and send to the client in HTTP 1.0 compatible form.\nThe problem with dechunking is that it requires buffering the response entire body to calculate the Content-Length. That means either implementing a maximum response body size cutoff that is small or having linkerd potentially consume huge amounts of memory buffering response bodies.\nAn alternate solution is to avoid upgrading HTTP/1.0 requests; that is, if the original request was HTTP/1.0 then send the request upstream using HTTP/1.0 too. Then the upstream won't chunk the response. This has downsides too, since the benefits of upgrading to HTTP/1.1 wouldn't apply. However, IMO it is still better than having a response body size cut-off or unbounded buffering.\n(We should get some agreement on what's best to do between Conduit and Linkerd. The above comments summarize the discussion around the issue when it was brought up for Conduit.). RE: rfc6202, it's definitely OK as far as the HTTP spec is concerned to do the de-chunking by buffering. The question is really just the quality-of-implementation issue regarding memory usage.. I think avoiding upgrading HTTP/1.0 requests to HTTP/1.1 by default is the safest and most compatible thing to do. The vast majority of applications will be sending HTTP/1.1 requests already so most things wouldn't be affected. For the applications that would be negatively affected, the negative effect would probably be diminished performance.\nIt would be good to hear from @olix0r and @seanmonstar because they both had concerns about that approach last time I suggested this (for Conduit).. The other major problem with close-delimited responses is that they don't allow the connection to be reused for multiple transactions (requests/responses).\n[edit: s/connections/transactions (requests/responses)/]. > @briansmith, I update my PR based on your advice. Please check if it is an acceptable solution.\nIMO, this is the best idea for solving this kind of problem. The only question is whether @olix0r or @seanmonstar or @adleong object to the idea. In particular, would we do the same thing in Conduit?\nAs far as the implementation is concerned, @adleong and @olix0r can review this much better than me.. This is a very reasonable approach. In the past other products (e.g. Apache) have found that having separate files for the end-entity certificate and the certificate chain have pros and cons. The pro is that you can easily replace the end-entity certificate from the same CA, as long as the certificate chain never changes, just by replacing one file. The con is that \"as long as the certificate chain never changes\" is a very error-prone condition. Even if you request a new certificate from the same CA, they might use a different intermediate certificate.\nAnother con is that people may just ignore the clientCaChainPath and often things will \"just work\" because of AIA fetching. Hoewver, the AIA fetching causes the client to have to do extra networking to fetch the intermediate certificates.\nIMO, a less error prone design would be to change the processing of certPath to allow the intermediates to be extracted from it, if there is more than one certificate in the file.\nThe other error that is common is for the user to include the entire certificate chain in the file, including the root, and then the web server serves the root certificate in the TLS handshake too. It is worth documenting, at least, that the cert chain should omit the root certificate, for efficiency reasons.\nAlternatively, you could document that the cert chain should contain at least one root certificate. Then you could, after loading the cert chain, verify that the certificate is valid and issue a warning (at least) if it isn't valid, and then strip any root (self-signed) certificates before sending the cert chain in the TLS handshake. This is basically what I'm planning to do in Conduit, but it is more work because it requires dealing with certificate verification APIs.. OK, keep in mind that if you have two files and you watch both of them, then you must deal with race conditions where one file is updated and is out of sync with the other file. For example, the user might replace the end-entity certificate file and then some time might pass before they replace the intermediate certificate chain file. If the configuration is reloaded between those two times then the end-entity certificate won't match the intermediate and the configuration will be broken until the second file is reloaded. Again, not a show-stopper but something to be aware of.. > OK, keep in mind that if you have two files and you watch both of them, then you must deal with race conditions where one file is updated and is out of sync with the other file. For example, the user might replace the end-entity certificate file and then some time might pass before they replace the intermediate certificate chain file. If the configuration is reloaded between those two times then the end-entity certificate won't match the intermediate and the configuration will be broken until the second file is reloaded. Again, not a show-stopper but something to be aware of.\nI guess we must already handle this, because we have the same issue with the private key file being updated out-of-sync with the end-entity certificate file.. true should be the default value. If I understand your comment below, some tests will fail if true is used. I think that instead we should modify those tests to explicitly pass in false.\nIn particular, we should move away from using the subject CN for anything except as a fallback when the DNS names aren't provided, following RFC 6125.\n. NIT: I would put this function right under addDnsAltNamesInConfig so that all this directly-related code is together.. Please add a comment here that says something like Use \"SAN:\" instead of \"URI:\" for backward compatibility with previous releases.. If you make true the default for addDnsAltNames then you can just remove this comment. Instead, add a comment at each withCerts(false, ...) call site explaining that the test is explicitly testing Subject CN behavior and not SAN behavior.. Let's add a comment here explaining that we forward HTTP/1.0 requests using HTTP/1.0 to minimize the chance that the server will send a response that uses HTTP/1.1 features, in particular Transfer-Encoding: chunked, that the HTTP/1.0 application can't handle.. Why does just this setting have the word \"client\" in its name and in its description, but the other settings don't? It seems like \"client\" is redundant here and could/should be removed.. Same thing here. It seems like client in the name is inconsistent with the other parts of the clientAuth setting.. Should this have client in its name like requireClientAuth does?. Shouldn't we have a way of setting the intermediate certificate chain above for the server cert too, like we do for the client cert? In other words, (cert, intermediates, key) should always go together.. I don't have strong feelings about this. Since both already have \"keyPath\" and \"certPath\" it makes sense to me that both would use the same name for this third thing too. Perhaps \"intermediateCertsPath\" might be a reasonable name, since that implies that (1) these aren't the trusted roots, and (2) the roots shouldn't be included in the chain.. It would be good to have a test where the value is syntactically valid to ensure that it is removed even when it looks good, and not only when it is \"totally bugus.\". It would also be good to have a test that shows that if there are multiple instances of the header field that all of them are removed.. ",
    "Oded-B": "Hmm, not sure, the responses are relatively large, at about 1MB.\nAfter the weekend I can try to see if I can send you a memory dump somehow.\n. ",
    "jeniag": "I did some digging into this and it doesn't seem to be telemetry related. Even with telemetry off, Netty4ServerDispatcher keeps a huge amount of streams open. \nIt happens on linkerd 0.8.6 and 0.9.0. The client is java grpc and the server is c++ grpc (Tensorflow Serving).\n\n. This is a trace from the same machine @Oded-B used, but with tracing disabled. \nI can open a separate issue, but I added here because it might have been the underlying cause.. We managed to find a workaround for this issue, by recycling ManagedChannel objects on the java grpc client side. \nWe could reliably reproduce this by running ~100 grpc requests per second reusing the same single ManagedChannel from a java process via localhost to linkerd running on the same host. The heap starts climbing immediately and continues at the same rate until linkerd crashes.\nIt seems like the grpc client opens new streams frequently and while I see end_stream messages sent from linkerd to the client, the stream objects remain in the hashmap that maps streamId to the streams.\n(see Wireshark screen shots)\n\n\nAll of the objects in the heap dump are reachable.\nUnfortunately I can't provide a heap dump or the Wireshark recording - they contain actual traffic and therefore sensitive information.\nMetrics and a config file are attached. They are not exactly from the same time as the recording but its from a machine in a similar state.  However, the problem remains even if we change timeouts or provide different maxConcurrentRequests settings. \nconfig.json.txt\nmetrics.json.txt\n. ",
    "macalinao": "^Hmm looks like CircleCI failed, but not because my test didn't work. I didn't run the test locally-- does anyone have any tips on how to do this?. Looks like the end-to-end tests are still OOMing... \ud83e\udd14 Could my change be the reason?. @adleong Sorry for getting back to you so late. I see you've already merged things.. Let me know if there's a more idiomatic way to do this.. I'm not sure why path.show is unreliable for equality.. ",
    "KarolisL": "Hello,\nEven with clearContext: true, we get verbose errors from linkerd:\n```\n$ curl -v localhost:8081/ ; echo\n About to connect() to localhost port 8081 (#0)\n   Trying 127.0.0.1...\n* Connected to localhost (127.0.0.1) port 8081 (#0)\n\nGET / HTTP/1.1\nUser-Agent: curl/7.29.0\nHost: localhost:8081\nAccept: /\n< HTTP/1.1 400 Bad Request\n< l5d-err: Unknown+destination%3A+Request%28%22GET+%2F%22%2C+from+%2F10.0.0.15%3A33672%29+%2F+not+enough+segments+in+path\n< Content-Type: text/plain\n< Content-Length: 93\n<\n* Connection #0 to host localhost left intact\nUnknown destination: Request(\"GET /\", from /10.0.0.15:33672) / not enough segments in path\n```\n\nOur config includes clearContext option. Is this the expected behaviour? According to the commited tests, linkerd should return empty body on such errors.\nOur config:\n```\n- protocol: http\n  label: my-router\n  identifier:\n    kind: io.l5d.path\n    segments: 2\n  dtab: |-\n    /payments_srv => /$/inet/my-payments/8080;\n   <...snip...>\n    /svc/v1/operations => /payments_srv;\n   <...snip...>\n  servers:\n  - ip: 0.0.0.0\n    port: 8081\n    clearContext: true\n  service:\n    responseClassifier:\n      kind: io.l5d.http.nonRetryable5XX\n  client:\n    kind: io.l5d.global\n    failFast: false\n    failureAccrual:\n      kind: none\n\n```. Yes, @adleong, I'm interested in doing a PR, I'd appreciate some pointers since I'm mostly unfamiliar with the codebase. . @adleong Currntly I am busy with other stuff. The help was sufficient, I was able to fix the code, but left without a proper test.\nWe do have a plan for making a PR, but don't have a timeline yet.. ",
    "agunnerson-ibm": "I don't know if it's related, but I found that if you delete a kubernetes service, but keep the deployment/pods around, then linkerd still routes to those pods (instead of returning some sort of error).. > @agunnerson-ibm that behavior is intentional. we've found that treating service discovery as advisory allows us to be more resilient to service discovery outages. The behavior can be configured with the enableProbation flag: https://linkerd.io/config/0.9.1/linkerd/index.html#load-balancer\n\n@agunnerson-ibm thanks for the report with detailed repro instructions! Very helpful. Namespace thing is probably a bug. We'll look into it. Probation thing, like Alex says, is working as intended.\n\nUnderstood. Thanks!. It looks like this bug was finally fixed in linkerd 1.2.1! I was able reproduce it in 1.2.0, but not 1.2.1.. Just ran into this again. Logs: https://gist.github.com/agunnerson-ibm/344cd7f508bec2e5032e065d4d55f6cc\nRequests were failing at around 16:15 UTC (line 26656 in the logs) and recovered at around 16:22 UTC. This is for redacted-service-1 in the master namespace.. We had to downgrade to 1.1.0 in the meantime. Let me know if there's any more information I can provide to help debug this.. Thank you! I'll give a try in our QA environments on Monday.. ",
    "manishrajkarnikar": "@adleong I set enableProbation to true. Then deleted my service and recreated it. It could not route to the new service . ",
    "nik786": "linkerd conatiners are not getting created on kubernetes 1.5. Is Linkerd compatible with kubernetes 1.5 ??I have executed below mentioned command to install it with kubernetes\nkubectl apply -f https://raw.githubusercontent.com/linkerd/linkerd-examples/master/k8s-daemonset/k8s/linkerd.yml...Containers are in creating state or crash loopback state. ",
    "taer": "Attaching the whole metrics.json. I sanitized it. The app with a problem is named \nrt/cgp/dst/id/#/consul/us-east-1-vpc-XXXX/live/appWithProblem\nmetrics.txt\nroot@ip-10-122-17-136:~# netstat -atpn | grep 3887|grep CLOSE_WAIT|wc\n    166    1162   16102\n. Managed to get a node.js driver to reproduce this.\n```\nconst http = require('http')\nconst port = 3000\nconst requestHandler = (request, response) => {\n  console.log(Date.now())\n  response.writeHead(500, {'Content-Type': 'text/plain'})\n  response.write(\"Help Me\")\n  response.end()\n}\nconst server = http.createServer(requestHandler)\nserver.listen(port, (err) => {\n  if (err) {\n    return console.log('something bad happened', err)\n  }\nconsole.log(server is listening on ${port})\n})\n```\nIf you hit this service via linkerd with the \nresponseClassifier:\n    kind: io.l5d.retryableRead5XX \nenabled, you'll get 20 seconds of retry as expected, but then you'll have about 100 connections orphaned. Eventually, the node server gives up and closes the socket. But linkerd never finishes it's read, and the sockets will go into CLOSE_WAIT as above.\nIf we remove the retryableRead5XX and hit linkerd via a loop\nfor i in `seq 1 100` ; do curl   localhost:4140/hello & done\nYou'll get < 100 connections, but they are drained properly.. @adleong that was just an artifact on how I scrubbed the files for our security team.\nThe service on the bad namerd local/live/serviceName-production is present in the goodNamerds, but got randomly named something app-numbery\nFor the same service on the good ones, the lastUpdate times were\n\"lastUpdatedAt\": \"2018-09-07 13:16:25 +0000\",\n      \"lastUpdatedAt\": \"2018-09-07 13:16:25 +0000\",\n      \"lastUpdatedAt\": \"2018-09-07 13:16:25 +0000\",\n      \"lastUpdatedAt\": \"2018-09-07 13:16:51 +0000\",\n      \"lastUpdatedAt\": \"2018-09-07 13:16:51 +0000\",\nthe same one in the badNamerd state had this\n\"/.local/live/serviceName-production\": {\n    \"state\": {\n      \"running\": true,\n      \"lastStartedAt\": \"2018-08-25 12:05:01 +0000\",\n      \"lastStoppedAt\": \"2018-08-25 06:45:39 +0000\",\n      \"lastUpdatedAt\": \"2018-09-05 21:08:52 +0000\",\n. That workaround works. ",
    "Disturbing": "Awesome.\nI will test this out and let you know if it works. Didn't know auth session\nhandling worked across two separate grpc instances yet. Was thinking\nLinkerd had to broadcast the auth to both services so that it works\nflawlessly since essentially multiple http2 connections and grpc sessions\nare under the hood.\nWill give it a run and report the results in the next week.\nOn Thu, Apr 13, 2017 at 2:26 AM Kevin Lingerfelt notifications@github.com\nwrote:\n\nHey @Disturbing https://github.com/Disturbing, thanks for the detailed\nwrite up! If I understand correctly, you want gRPC clients to connect to\nLinkerd, and Linkerd to route some endpoints to MicroserviceA, and other\nendpoints to MicroserviceB. This should already be possible using Linkerd's\nDtab functionality. You would just need the client to use a combined\nservice definition, and then break out the endpoints in your Dtab.\nSomething like:\n/svc/combinedapp/Login => /srv/serviceA/Login\n/svc/combinedapp/LoginWithFB => /srv/serviceB/LoginWithFB\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/linkerd/linkerd/issues/1186#issuecomment-293666349,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACQev_2W4ndug0kD_3AFAMzEtLzSw27Oks5rvRdtgaJpZM4M3q2x\n.\n. \n",
    "jgensler8": "@adleong,\nThanks for the clarification! It sounds like this is the expected behavior from Linkerd's part.\nThe fallback identifier is a good idea. I am in the process of understanding consumption and organizational patterns of Linkerd's Ingress and there will have to be a fallback (default) somewhere. I suppose my confusion would be the blog post's use of one ingress controller for the whole cluster (meaning, v1 is the fallback for all teams). I'll be writing some blog posts in the near future about less trivial use cases to clarify my thoughts.\nThanks, again!. I think some of the earlier examples of Ingress Controllers gave the responsibility of a \"default backend\" to the Controller, not to the Ingress Objects. link\nargs:\n- /nginx-ingress-controller\n- --default-backend-service=$(POD_NAMESPACE)/default-http-backend\nBased on these lines, the \"default backend\" serves 404 pages.\nOne use case I was thinking of is an organization with multiple applications (say shoes.com and books.org) that are hosted on the same entry Ingress Controller but eventually end up with different \"default\" page based on the website. The first Ingress Controller wouldn't have a default backend. Instead, it would have separate backends based on the Host. Each separate backend (shoes.com and books.org) would have its own organizational Ingress Controller with their branded 404 page. Alternatively, they could use Ingress's .spec.backend. Similar to what @adleong has stated earlier, it is unclear which option (multiple Ingress Controllers w/ default backend set to .spec.ingress vs Single Controller w/ multiple Ingress Objects) would be the best for such an organization.. ",
    "conanconstantine": "hey guys, do you know how to change the links in the payload (JSON API) based on the X-Forwarded-Host?. ",
    "magician2017": "Any idea, when you could resolve this?. ",
    "lkysow": "Also failing for us after downgrading to 1.6.0:\nkubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"5\", GitVersion:\"v1.5.3\", GitCommit:\"029c3a408176b55c30846f0faedf56aae5992e9b\", GitTreeState:\"clean\", BuildDate:\"2017-02-15T06:40:50Z\", GoVersion:\"go1.7.4\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"6\", GitVersion:\"v1.6.0\", GitCommit:\"fff5156092b56e6bd60fff75aad4dc9de6b6ef37\", GitTreeState:\"clean\", BuildDate:\"2017-03-28T16:24:30Z\", GoVersion:\"go1.7.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n\u279c  load-test-endpoint\n\u279c  load-test-endpoint k get ds\nNAME         DESIRED   CURRENT   READY     NODE-SELECTOR                   AGE\nflannel-ds   5         5         4         beta.kubernetes.io/arch=amd64   2d\nlinkerd-ds   5         5         4         <none>                          2d\n\u279c  load-test-endpoint k get pods |grep linkerd\nlinkerd-ds-7vzzv                        2/2       Running   0          12m\nlinkerd-ds-7xvkk                        2/2       Running   1          12m\nlinkerd-ds-90svd                        2/2       Running   0          12m\nlinkerd-ds-m1h7b                        2/2       Running   5          4m\nlinkerd-ds-rnlkv                        2/2       Running   0          1m\nlinkerd-ds-v45pf                        2/2       Running   0          1m\n\u279c  load-test-endpoint k logs linkerd-ds-90svd linkerd --since=1m\nI 0417 23:58:44.656 UTC THREAD10: k8s initializing kube-system\nE 0417 23:58:44.667 UTC THREAD26: k8s failed to list endpoints\njava.lang.NullPointerException\n    at io.buoyant.k8s.EndpointsNamer$.io$buoyant$k8s$EndpointsNamer$$getEndpoints(EndpointsNamer.scala:126)\n    at io.buoyant.k8s.EndpointsNamer$NsCache$$anonfun$io$buoyant$k8s$EndpointsNamer$NsCache$$mkSvc$1.apply(EndpointsNamer.scala:221)\n    at io.buoyant.k8s.EndpointsNamer$NsCache$$anonfun$io$buoyant$k8s$EndpointsNamer$NsCache$$mkSvc$1.apply(EndpointsNamer.scala:219)\n    at scala.Option.map(Option.scala:146)\n    at io.buoyant.k8s.EndpointsNamer$NsCache.io$buoyant$k8s$EndpointsNamer$NsCache$$mkSvc(EndpointsNamer.scala:219)\n    at io.buoyant.k8s.EndpointsNamer$NsCache$$anonfun$13.apply(EndpointsNamer.scala:200)\n    at io.buoyant.k8s.EndpointsNamer$NsCache$$anonfun$13.apply(EndpointsNamer.scala:199)\n    at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:252)\n    at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:252). I think these v1.scala and v1beta1.scala (https://github.com/linkerd/linkerd/blob/07f34ba01d894319c661922eb003dce1db787e77/k8s/src/main/scala/io/buoyant/k8s/v1beta1.scala#L115) need to be modified:\ndiff\n  case class LoadBalancerIngress(\n-    ip: String\n+    ip: Option[String] = None,\n+    hostname: Option[String] = None\n  )\nAnd then ServiceNamer https://github.com/linkerd/linkerd/blob/master/k8s/src/main/scala/io/buoyant/k8s/ServiceNamer.scala#L203 can be modified\ndiff\n    for {\n      meta <- service.metadata.toSeq\n      name <- meta.name.toSeq\n      status <- service.status.toSeq\n      lb <- status.loadBalancer.toSeq\n      ingress <- lb.ingress.toSeq.flatten\n+     hostname <- ingress.hostname.orElse(ingress.ip)\n      spec <- service.spec.toSeq\n      port <- spec.ports\n    } {\n-    ports += port.name -> Address(new InetSocketAddress(ingress.ip, port.port))\n+    ports += port.name -> Address(new InetSocketAddress(hostname, port.port))\n      portMap += (port.targetPort match {\n        case Some(targetPort) => port.port -> targetPort\n        case None => port.port -> port.port\n      })\n    }. @adleong done (#1323) wasn't sure how best to modify the tests. Feel free to work off that PR or just take the changes into your own.. Yeah I think the streaming parameter is super dangerous and shouldn't be listed. If you turn it off and try to proxy chunked encoded requests something happens with Netty and you get responses from other requests that are happening concurrently.\nThen I'd suggest renaming the maxChunkKB to maxResponseKB. ",
    "anubhavmishra": "@siggy I see this behaviour for 1.6.0 as well:\n```bash\n$ kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"6\", GitVersion:\"v1.6.1\", GitCommit:\"b0b7a323cc5a4a2019b2e9520c21c7830b7f708e\", GitTreeState:\"clean\", BuildDate:\"2017-04-03T23:37:30Z\", GoVersion:\"go1.8\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"6\", GitVersion:\"v1.6.0\", GitCommit:\"fff5156092b56e6bd60fff75aad4dc9de6b6ef37\", GitTreeState:\"clean\", BuildDate:\"2017-03-28T16:24:30Z\", GoVersion:\"go1.7.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n$ kubectl -n=kube-system get ep kube-controller-manager -o jsonpath='{.subsets}'\n[]\n$ kubectl -n=kube-system get ep -o jsonpath='{.items[?(@.metadata.name==\"kube-controller-manager\")].subsets}'\n\n```\n. ",
    "ahmetb": "FWIW adding some context, this incompatibility has apparently caused an outage at a bank\u2019s infrastructure. https://community.monzo.com/t/current-account-payments-may-fail-major-outage/26296/95. ",
    "kenkouot": "Just wanted to chime in and say that we have been seeing the same issue. After happily serving gRPC requests for some amount of time (also saw from 20 min to hours), all of our gRPC traffic flowing through linkerd starts deadlining. We're only running in a limited amount of our services and can confirm this only happens when routing through linkerd. \nEnvironment info:\n linkerd 1.0 running in k8s as daemonset\n namerd 1.0\n grpc over h2 routers\n all default configuration for failure accrual.. Awesome @klingerf. I've swapped out the image and will monitor for a few days and will report back.. ",
    "rclayton-the-terrible": "@olix0r looks like it's timing out:\n```\n[root@services-i-0353413672ac1bf49 ~]# curl -v 'namerd.example.com:4180/api/1/bind/microservices?path=/actual/logstash&watch=1'\n   Trying 10.26.4.114...\n TCP_NODELAY set\n* Connected to namerd.example.com (10.26.4.114) port 4180 (#0)\n\nGET /api/1/bind/microservices?path=/actual/logstash&watch=1 HTTP/1.1\nHost: namerd.example.com:4180\nUser-Agent: curl/7.51.0\nAccept: /\n< HTTP/1.1 200 OK\n< Date: Wed, 26 Apr 2017 23:45:44 GMT\n< Transfer-Encoding: chunked\n< Connection: keep-alive\n<\n{\"type\":\"leaf\",\"bound\":{\"addr\":{\"type\":\"bound\",\"addrs\":[{\"ip\":\"10.26.5.246\",\"port\":5000,\"meta\":{}},{\"ip\":\"10.26.3.7\",\"port\":5000,\"meta\":{}},{\"ip\":\"10.26.2.6\",\"port\":5000,\"meta\":{}},{\"ip\":\"10.26.2.5\",\"port\":5000,\"meta\":{}},{\"ip\":\"10.26.4.174\",\"port\":5000,\"meta\":{}},{\"ip\":\"10.26.4.48\",\"port\":5000,\"meta\":{}}],\"meta\":{}},\"id\":\"/#/consul/eu-central-1/logstash\",\"path\":\"/\"}}\n transfer closed with outstanding read data remaining\n Curl_http_done: called premature == 1\n* Closing connection 0\ncurl: (18) transfer closed with outstanding read data remaining\n```. @olix0r do you want me to pull that state?. @olix0r no worries (or rush needed).  We've been deploying namerd as a \"sidecar\" container with linkerd to get around only having one node and more complex service discovery logic for the namerd instance.  Admit it's not ideal, but it's working well for us at the moment.. @hawkw  Awesome - if my Scala was up to par I would help.  Any interest in a Node.js version of Namerd?. @hawkw you guys are the best!  Thank you for the quick turnaround.. @wmorgan I will try as soon as possible to verify, but I doubt I will have time before vacation (which starts in 3 hours!).  Therefore, I won't be able to get to it until early January.  Fortunately, discovering that having an invalid DTab was causing my issue has unblocked me on my infrastructure project (so this is not such a high priority at the moment).. @hawkw if you do that, I will take the time to test tonight -- prefer Docker image.. @hawkw OK, success:\n\nThe malformed Dtab was:\n/virtual/qa/limesurvey => /actual/{outage};\n/dns/com/intelli-zoom/feature-something-something-survey-qa => /virtual/qa/limesurvey;\n/virtual/qa/gateway => /actual/outage;\n/dns/com/intelli-zoom/feature-something-something => /virtual/qa/gateway;\n/svc => /$/io.buoyant.http.domainToPathPfx/dns;\n/actual => /#/consul/qa-feature-something-something/ecs-qa;\nAnd the logs show:\n```\nE 1220 23:01:36.149 UTC THREAD36 TraceId:bc70c71e388327ee: consul ns ecs-qa-entrypoint dtab parsing failed: java.lang.IllegalArgumentException: label char expected but '{' found at '/virtual/qa/limesurvey => /actual/[{]outage};\n/dns/com/intelli-zoom/feature-something-something-survey-qa => /virtual/qa/limesurvey;\n/virtual/qa/gateway => /actual/outage;\n/dns/com/intelli-zoom/feature-something-something => /virtual/qa/gateway;\n/svc => /$/io.buoyant.http.domainToPathPfx/dns;\n/actual => /#/consul/qa-feature-something-something/ecs-qa;'; dtab: '/virtual/qa/limesurvey => /actual/{outage};\n/dns/com/intelli-zoom/feature-something-something-survey-qa => /virtual/qa/limesurvey;\n/virtual/qa/gateway => /actual/outage;\n/dns/com/intelli-zoom/feature-something-something => /virtual/qa/gateway;\n/svc => /$/io.buoyant.http.domainToPathPfx/dns;\n/actual => /#/consul/qa-feature-something-something/ecs-qa;'\nE 1220 23:01:36.152 UTC THREAD36: adminhttp\njava.lang.IllegalArgumentException: label char expected but '{' found at '/virtual/qa/limesurvey => /actual/[{]outage};\n/dns/com/intelli-zoom/feature-something-something-survey-qa => /virtual/qa/limesurvey;\n/virtual/qa/gateway => /actual/outage;\n/dns/com/intelli-zoom/feature-something-something => /virtual/qa/gateway;\n/svc => /$/io.buoyant.http.domainToPathPfx/dns;\n/actual => /#/consul/qa-feature-something-something/ecs-qa;'\n    at com.twitter.finagle.NameTreeParsers.illegal(NameTreeParsers.scala:29)\n    at com.twitter.finagle.NameTreeParsers.illegal(NameTreeParsers.scala:36)\n    at com.twitter.finagle.NameTreeParsers.parseLabel(NameTreeParsers.scala:111)\n    at com.twitter.finagle.NameTreeParsers.parsePath(NameTreeParsers.scala:173)\n    at com.twitter.finagle.NameTreeParsers.parseSimple(NameTreeParsers.scala:220)\n    at com.twitter.finagle.NameTreeParsers.parseWeighted(NameTreeParsers.scala:250)\n    at com.twitter.finagle.NameTreeParsers.parseTree1(NameTreeParsers.scala:198)\n    at com.twitter.finagle.NameTreeParsers.parseTree(NameTreeParsers.scala:184)\n    at com.twitter.finagle.NameTreeParsers.parseDentry(NameTreeParsers.scala:258)\n    at com.twitter.finagle.NameTreeParsers.parseDtab(NameTreeParsers.scala:268)\n    at com.twitter.finagle.NameTreeParsers.parseAllDtab(NameTreeParsers.scala:305)\n    at com.twitter.finagle.NameTreeParsers$.parseDtab(NameTreeParsers.scala:12)\n    at com.twitter.finagle.Dtab$.read(Dtab.scala:356)\n    at io.buoyant.namerd.storage.consul.ConsulDtabStore.$anonfun$_observe$3(ConsulDtabStore.scala:177)\n    at com.twitter.util.Try$.apply(Try.scala:15)\n    at io.buoyant.namerd.storage.consul.ConsulDtabStore.$anonfun$_observe$2(ConsulDtabStore.scala:177)\n    at com.twitter.util.Promise$Transformer.liftedTree1$1(Promise.scala:228)\n    at com.twitter.util.Promise$Transformer.k(Promise.scala:228)\n    at com.twitter.util.Promise$Transformer.apply(Promise.scala:239)\n    at com.twitter.util.Promise$Transformer.apply(Promise.scala:220)\n    at com.twitter.util.Promise$$anon$7.run(Promise.scala:532)\n    at com.twitter.concurrent.LocalScheduler$Activation.run(Scheduler.scala:198)\n    at com.twitter.concurrent.LocalScheduler$Activation.submit(Scheduler.scala:157)\n    at com.twitter.concurrent.LocalScheduler.submit(Scheduler.scala:274)\n    at com.twitter.concurrent.Scheduler$.submit(Scheduler.scala:109)\n    at com.twitter.util.Promise.runq(Promise.scala:522)\n    at com.twitter.util.Promise.updateIfEmpty(Promise.scala:887)\n    at com.twitter.util.Promise.update(Promise.scala:859)\n    at com.twitter.util.Promise.setValue(Promise.scala:835)\n    at com.twitter.concurrent.AsyncQueue.offer(AsyncQueue.scala:122)\n    at com.twitter.finagle.netty4.transport.ChannelTransport$$anon$1.channelRead(ChannelTransport.scala:183)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at com.twitter.finagle.netty4.http.handler.UnpoolHttpHandler$.channelRead(UnpoolHttpHandler.scala:32)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at com.twitter.finagle.netty4.http.handler.ClientExceptionMapper$.channelRead(ClientExceptionMapper.scala:33)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:438)\n    at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:310)\n    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:284)\n    at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:253)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1342)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:934)\n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:134)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run(BlockingTimeTrackingThreadFactory.scala:23)\n    at java.lang.Thread.run(Thread.java:748)\nW 1220 23:01:36.153 UTC THREAD36: Exception propagated to the default monitor (upstream address: /172.16.33.17:51684, downstream address: n/a, label: adminhttp).\njava.lang.IllegalArgumentException: label char expected but '{' found at '/virtual/qa/limesurvey => /actual/[{]outage};\n/dns/com/intelli-zoom/feature-something-something-survey-qa => /virtual/qa/limesurvey;\n/virtual/qa/gateway => /actual/outage;\n/dns/com/intelli-zoom/feature-something-something => /virtual/qa/gateway;\n/svc => /$/io.buoyant.http.domainToPathPfx/dns;\n/actual => /#/consul/qa-feature-something-something/ecs-qa;'\n    at com.twitter.finagle.NameTreeParsers.illegal(NameTreeParsers.scala:29)\n    at com.twitter.finagle.NameTreeParsers.illegal(NameTreeParsers.scala:36)\n    at com.twitter.finagle.NameTreeParsers.parseLabel(NameTreeParsers.scala:111)\n    at com.twitter.finagle.NameTreeParsers.parsePath(NameTreeParsers.scala:173)\n    at com.twitter.finagle.NameTreeParsers.parseSimple(NameTreeParsers.scala:220)\n    at com.twitter.finagle.NameTreeParsers.parseWeighted(NameTreeParsers.scala:250)\n    at com.twitter.finagle.NameTreeParsers.parseTree1(NameTreeParsers.scala:198)\n    at com.twitter.finagle.NameTreeParsers.parseTree(NameTreeParsers.scala:184)\n    at com.twitter.finagle.NameTreeParsers.parseDentry(NameTreeParsers.scala:258)\n    at com.twitter.finagle.NameTreeParsers.parseDtab(NameTreeParsers.scala:268)\n    at com.twitter.finagle.NameTreeParsers.parseAllDtab(NameTreeParsers.scala:305)\n    at com.twitter.finagle.NameTreeParsers$.parseDtab(NameTreeParsers.scala:12)\n    at com.twitter.finagle.Dtab$.read(Dtab.scala:356)\n    at io.buoyant.namerd.storage.consul.ConsulDtabStore.$anonfun$_observe$3(ConsulDtabStore.scala:177)\n    at com.twitter.util.Try$.apply(Try.scala:15)\n    at io.buoyant.namerd.storage.consul.ConsulDtabStore.$anonfun$_observe$2(ConsulDtabStore.scala:177)\n    at com.twitter.util.Promise$Transformer.liftedTree1$1(Promise.scala:228)\n    at com.twitter.util.Promise$Transformer.k(Promise.scala:228)\n    at com.twitter.util.Promise$Transformer.apply(Promise.scala:239)\n    at com.twitter.util.Promise$Transformer.apply(Promise.scala:220)\n    at com.twitter.util.Promise$$anon$7.run(Promise.scala:532)\n    at com.twitter.concurrent.LocalScheduler$Activation.run(Scheduler.scala:198)\n    at com.twitter.concurrent.LocalScheduler$Activation.submit(Scheduler.scala:157)\n    at com.twitter.concurrent.LocalScheduler.submit(Scheduler.scala:274)\n    at com.twitter.concurrent.Scheduler$.submit(Scheduler.scala:109)\n    at com.twitter.util.Promise.runq(Promise.scala:522)\n    at com.twitter.util.Promise.updateIfEmpty(Promise.scala:887)\n    at com.twitter.util.Promise.update(Promise.scala:859)\n    at com.twitter.util.Promise.setValue(Promise.scala:835)\n    at com.twitter.concurrent.AsyncQueue.offer(AsyncQueue.scala:122)\n    at com.twitter.finagle.netty4.transport.ChannelTransport$$anon$1.channelRead(ChannelTransport.scala:183)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at com.twitter.finagle.netty4.http.handler.UnpoolHttpHandler$.channelRead(UnpoolHttpHandler.scala:32)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at com.twitter.finagle.netty4.http.handler.ClientExceptionMapper$.channelRead(ClientExceptionMapper.scala:33)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:438)\n    at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:310)\n    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:284)\n    at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:253)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1342)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:934)\n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:134)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run(BlockingTimeTrackingThreadFactory.scala:23)\n    at java.lang.Thread.run(Thread.java:748)\n```\nAnd the UI request failed (instead of hung waiting forever):\n\n. Yes, we don't use namerd's API to write DTabs.  We sidecar namerd on our ECS hosts, so it exists more as an ephemeral service to containers on the host than something we interact with as a dedicated Ops service.  Consul's role in our architecture is the exact opposite - it's an essential part of our control plane.  I guess we could theoretically write DTabs through the namerd API, but we never got the sense that that was mandatory.\n-- oh and the cause of the malformed DTabs was literally me missing a dollar sign ($) used by a Terraform template which left something like /foo/bar/live => /foo/bar/{environment}. ",
    "danielBreitlauch": "It is running as a jvm process.\nThe requests are http get requests generated with wrt. The connection is kept alive. Tls is off. And the server responds with a simple 200 no data.\nMachine resource wise:\nThe CPU was under medium load and the jvm footprint was minimal ~100mb. The resources were certainly enough to run a second linkerd instance and get nearly the same throughout in addition to the first instance.\n\nAm 27.04.2017 um 19:24 schrieb William Morgan notifications@github.com:\nHi @danielBreitlauch. There are many reasons this might happen. We'll need many more details to be helpful.\nHow are you running Linkerd? In a Docker container, or as a JVM process? What are the nature of these requests? HTTP, Thrift, something else? Are they concurrent? If they're HTTP, are you doing keepalives or restarting the connection every time? Are they big requests? Small? Chunked? Are you using TLS? What's the CPU usage and memory footprint of Linkerd at this point?\nWe've been able to get a single instance to ~40k HTTP RPS under ideal conditions.\nYou might find this doc useful: https://github.com/linkerd/linkerd/wiki/Debugging-network-performance\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Linkerd version: 0.9.1\njvm: HotSpot 1.8.0_111\nThe CPU usage was 50% equally over 32 cores. Soft irqs where a bit higher but not too much.\nThe traffic definitely goes to another host through a gigabit network.\n\nConcerning the metrics. I did the test already a while ago. I will atach them as soon as I do the test again.. Sorry for not responding for some time. Asking namerd directly is indeed the solution I was not seeing.\nI will try this soon. Thx for the hint.. Sorry to reopen this but is there a way to ask the \"namerd\" bundled into linkerd via http too? Our setup consists of linkerds asking local consul agents. There is no need to have separate namerd instances.. Yeah I am doing that now. It just does not feel like an official api. It is not documented and could go away/ change any time.\nThx for the example, I hope you consider adding it to the api.\nOn a side note, how is it supposed to be done to take routes out? Just assuming it happens instant? Or like we did before with sleeps? How often does linkerd check the consul agent btw?. ",
    "owensk": "I am not particularly familiar with Linkerd architecture so I can't comment on where it would best fit. \nWhat I'd like to see is the ability to write plugins that can return data instead of routing to an endpoint downstream. In this example, I'd like to write a plugin that has a keygen function, a caching instance FQDN, and can read into that caching instance. It would take information about the request being made, check cache, if it's present, return, if it's not present, route to endpoint. It would basically act to short circuit potentially large amounts of requests while not having to do any writing into caching, etc. though it might be nice to have that ability.\nCurrently, there isn't a layer in the plugin architecture(As discussed in the slack channel) that fits this roll because plugins, as far as I know, aren't for returning data but to dynamically modify routing.\nTo your question of \"What interface\", I'm not well versed enough to speculate. To you question of \"where should this be added\", I'd imagine having a plugin like this would be best suited to making decisions as early as possible in the process but, again, I don't have the expertise to comment on where that would actually happen.\nLet me know if I can explain more or help in anyway.. @adleong I'm glad to see something like this was added but I can't seem to find any documentation about the feature you're talking about. Can you please link to something about it?. So this request was specifically about the Thrift protocol. Also, I'm not sure how the loggers allow bypassing an end point and hitting a cache instead. I might be missing something.. That sounds reasonable. Thanks for the clarification.. I've changed the name but I can't re-open.. I have no idea why but I can't reproduce this anymore. I'll close and re-open if it arises again. Thanks.. ",
    "aschepis": "when you get to surveying real world data, i can provide some.. ",
    "SamFleming": "Just wondering if there's been any more thoughts on this? \nFor the time being I've set both my liveness and readiness checks to the same endpoint and am hoping that'll be okay.\nyaml\nlivenessProbe:\n  httpGet:\n    path: /admin/ping\n    port: 9990\n  initialDelaySeconds: 40\n  periodSeconds: 3\nreadinessProbe:\n  httpGet:\n    path: /admin/ping\n    port: 9990\n  initialDelaySeconds: 40\n  periodSeconds: 5. Seemingly okay, no problem so far. \ud83d\udc4d . ",
    "betson-nuovo": "In our l5d configuration (k8s daemonset with configmap), we've found that the /admin/ping endpoint alone is not sufficient in checking the health of l5d pods and have encountered situations where that endpoint returns successfully, while the l5d pod fails to proxy requests.  Initially, we have attempted to route through the local proxy port to the /admin/ping endpoint and that did provide better stability, but have found that pods would fail for other types of requests. In these scenarios, a pod redeploy resolves the issue.\nOver multiple iterations, we've found that a single endpoint is insufficient for testing the health of an l5d pod as we've discovered different l5d pods in the same k8s cluster failing in different ways. This was confirmed by executing a custom troubleshooting script, which ran a series of tests against every l5d pod in a cluster, modeling various requests that could potentially route through the pod in our environment and resulted in a different set of test failures for different pods ( sampling of tests executed: HTTP 1.1 GET/POST, one-way SSL GET/POST, two-way ssl GET/POST, including self-signed CA certs, to services within the cluster and third-party services outside the cluster, ie google).\nThe current iteration of our health check for l5d consists of using the k8s liveness/readiness ExecAction command to run a shell script consisting of a similar series of tests against the l5d pod.\nA reduced version of our current liveness check (similar for readiness):\nyaml\nlivenessProbe:\n    exec:\n      command:\n      - bash\n      - -c\n      - export http_proxy=$POD_IP:4140; declare -a httpUrl; httpUrl[0]=\"curl -v -w '%{http_code}' -s  -o /dev/null --max-time 2 http://$POD_IP:9990/admin/ping\"; httpUrl[1]=\"curl -v -w '%{http_code}' -s  -o /dev/null --max-time 2 http://redactedurl\" for i in \"${httpUrl[@]}\"; do resp=\"$(eval $i)\"; if [ \"$resp\" -eq 200 ]; then echo \"success\"; else  echo \"failure\"; export http_proxy=; exit 1; fi; done; export http_proxy=; exit 0;\n    initialDelaySeconds: 90\n    periodSeconds: 25\n    timeoutSeconds: 25\nHope this helps\n. Unfortunately, we've not been able to repro this in a targeted way so far. The situation arises after a period of days in our various clusters which are exercised through normal developer and QA testing load. I'll post an issue with container logs and deliver heapdumps privately from our latest few occasions.. Yes, that's correct. An independent curl from within the l5d pod, direct to the target endpoint without proxying through l5d.. Not that I'm aware of after initial configuration. It is possible when we are first configuring a new endpoint that DNS record changes are in-flight, but I believe those remain mostly static for a particular endpoint after initial configuration. These particular endpoints are owned by a different group, so I can't say for certain. I can try to do a bit of digging there. Good idea. We'll incorporate into our troubleshooting script to be rigorous about fetching that data. My colleague has been working on this and I'll ask to provide an update. Any luck with the heapdumps?. ",
    "smparekh": "Not sure why this was closed, the documentation explicitly states you can have multiple identifiers and linkerd should try each one in succession.. Ok, not sure if this is expected or not either (edit: using the suggested config):\nIf I include the path kind first the request fails but if I include the header.token first in the list the request works for both identifier techniques path and header.\nOnce we verify this is expected I can create a PR for the docs.. If I have the path first, it definitely does not like routing it with only the header included:\nIncluding my tests and config:\n```\nsparekh@linux-sparekh:~$ http GET linkerd:4140/hello-world/api/v1/hello Custom-Header:hello-world\nHTTP/1.1 200 OK\nContent-Encoding: gzip\nContent-Length: 106\nVia: 1.1 linkerd\ncontent-type: application/json\n{\n    \"hello\": \"world\"\n}\nsparekh@linux-sparekh:~$ http GET linkerd:4140/api/v1/hello Custom-Header:hello-world\nHTTP/1.1 502 Bad Gateway\nContent-Encoding: gzip\nContent-Length: 106\nContent-Type: text/plain\nl5d-err: No+hosts+are+available+for+%2Fsvc%2Fapi%2C+Dtab.base%3D%5B%5D%2C+Dtab.local%3D%5B%5D.+Remote+Info%3A+Not+Available\nNo hosts are available for /svc/api, Dtab.base=[], Dtab.local=[]. Remote Info: Not Available\nLinkerd config:\nadmin:\n  port: 9990\ntelemetry:\n- kind: io.l5d.prometheus\nrouters:\n- protocol: http\n  identifier:\n- kind: io.l5d.header.token\nheader: Custom-Header\n- kind: io.l5d.path\n  segments: 1\n  consume: true\n- kind: io.l5d.header.token\n  header: Custom-Header\n\nclient:\n    loadBalancer:\n      kind: ewma\n      maxEffort: 10\n      decayTimeMs: 15000\n  servers:\n  - port: 4140\n    ip: 0.0.0.0\n  label: linkerd_proxy\n  interpreter:\n    kind: io.l5d.namerd\n    dst: /$/inet/x.x.x.x/4100\n```\nCould be something to do with namerd?. So to have both header and path identifier work, if I have header defined first and then path it should be the recommended config to use?. Cool, thanks for the help!. ",
    "yoshi-taka": "No problem. I have signed the CLA, so please check it.. ",
    "nmurthy": "Hey @adleong I'm in the midst of implementing this and would love to give back to the community. Here's how I was thinking about doing it:\nDesignate a new HTTP header x-dark-route whose value is a dtab entry representing where the response should be multicast to. Our use case definitely involves analyzing the response differences, so I'm also thinking about having a config variable optionally point to a kafka topic/http endpoint where both reqs/responses will be sent. (We could def just wire it up to diffy this way too) \nThoughts?. ",
    "prdoyle": "Can I ask, why do you guys use HTTP headers for things like this?  Doesn't that mean that anyone using a REST client can play your application like a fiddle and make it do all kinds of unintended things?. Tried with linkerd+namerd 1.1.0, same problem.. Thanks @hawkw, tried it and still got the failure.\nI 0826 20:28:31.207 UTC THREAD1: linkerd 1.2.0-rc1 (rev=a044149e51936068a4b255f0acf30f582bb077bf) built at 20170825-135459\n...\ncom.twitter.io.Reader$ReaderDiscarded: This writer's reader has been discarded\n   at com.twitter.finagle.netty4.http.StreamTransports$$anon$1.discard(StreamTransports.scala:70)\n   at com.twitter.finagle.http.DelayedReleaseService$$anon$2.discard(DelayedReleaseService.scala:56)\n   at com.twitter.finagle.http.codec.HttpServerDispatcher$$anonfun$handle$2.applyOrElse(HttpServerDispatcher.scala:61)\n   at com.twitter.finagle.http.codec.HttpServerDispatcher$$anonfun$handle$2.applyOrElse(HttpServerDispatcher.scala:60)\n   at com.twitter.util.Promise.raise(Promise.scala:681)\n   at com.twitter.util.Future$JoinPromise$$anonfun$2.applyOrElse(Future.scala:254)\n   at com.twitter.util.Future$JoinPromise$$anonfun$2.applyOrElse(Future.scala:251)\n   at com.twitter.util.Promise.raise(Promise.scala:681)\n   at com.twitter.util.Promise.raise(Promise.scala:686)\n   at com.twitter.finagle.http.exp.GenSerialServerDispatcher.$anonfun$new$1(ServerDispatcher.scala:125)\n   at com.twitter.util.Future.$anonfun$ensure$1(Future.scala:941)\n   at com.twitter.util.Future.$anonfun$ensure$1$adapted(Future.scala:941)\n   at com.twitter.util.Promise$Monitored.apply(Promise.scala:202)\n   at com.twitter.util.Promise$Monitored.apply(Promise.scala:193)\n   at com.twitter.util.Promise$$anon$7.run(Promise.scala:530)\n   at com.twitter.concurrent.LocalScheduler$Activation.run(Scheduler.scala:200)\n   at com.twitter.concurrent.LocalScheduler$Activation.submit(Scheduler.scala:158)\n   at com.twitter.concurrent.LocalScheduler.submit(Scheduler.scala:272)\n   at com.twitter.concurrent.Scheduler$.submit(Scheduler.scala:108)\n   at com.twitter.util.Promise.runq(Promise.scala:520)\n   at com.twitter.util.Promise.updateIfEmpty(Promise.scala:877)\n   at com.twitter.finagle.netty4.transport.ChannelTransport.com$twitter$finagle$netty4$transport$ChannelTransport$$fail(ChannelTransport.scala:95)\n   at com.twitter.finagle.netty4.transport.ChannelTransport$$anon$1.channelInactive(ChannelTransport.scala:186)\n   at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\n   at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\n   at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\n   at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75). @hawkw - Yep, still 502 Bad Gateway.. When I get some time, I'm hoping to reduce the testcase to a few Dockerfiles and a docker-compose.yml for you.  So, the more you procrastinate, the easier this may become to reproduce.... @hawkw sounds good.  I'll reopen if I see it again.  Thanks!. ",
    "mattolson": "\ud83d\ude4f . Awesome. ",
    "albttx": "Same issue, any solution ?. @wmorgan yes, running with docker 1.1.0. @hawkw Thanks ! When it will be on docker ?. ",
    "urtrcc": "The  csharp client have same issue . how to solve it ?. @wmorgan  did you solve this issue? I'm so confused with this issue\u3002. @hawkw  Thank you so much!\n. @klingerf  I'm not sure csharp client have same issue ,but i got  same exception message log from Linkerd .\nand  Linkerd version is 1.1.1.\nThis is  Linkerd  error log : \n\n. @hawkw  Thanks to fixed it! and what time is next release?. ",
    "christtrc": "Sorry! I had a mistake with universe repo, i did not notice 10 in https://github.com/mesosphere/universe/tree/version-3.x/repo/packages/L\uff0c and my dc/os universe use a local repo which was not updated.. ",
    "arbarlow": "I added a similar issue to Go gRPC https://github.com/grpc/grpc-go/issues/1243\nThey have a similar opinion in that, their http_proxy variable is for tcp proxys that have a HTTP CONNECT \nI guess it's fine to point gRPC things strait to linkerd, it just means that Go doesn't work like others and that's a little annoying!\nFeel free to close this if you like.. @wmorgan unfortunately, it's a code change in every service. However, I guess I can just point all my services to localhost and use path routing. Which is a shame, but do-able!. ",
    "zhangyou1010": "get it . ",
    "samek": "depends on what you want to achieve .. \nhttps://github.com/linkerd/linkerd-examples/tree/master/plugins/header-classifier\nThere are 2 examples of plugins you probably might want to look into the identifier plugin.  . @kalyan02 When I started using linkerd I went through the documentation and I saw it. \nI also had a look what exactly is sent out when it calls  home .. which is not something that you should really worry about. \nAnyway we all have a great piece of software here for free and If guys from linkerd decided that this information is important to them this is the least what we can do - to keep it on. \nThis is my opinion. \nbtw. you can always disable it. \n. @adleong thanks for the info (so that I can setup a check). But I would swear that it always worked in admin but never when actually using it. \nI'll try to replicate it since I think I know what caused it to go that high and let you know if it does.  . I would definitely have it per service :) \nSent from my iPhone \n\nOn 29 Oct 2018, at 08:31, Robert Panzer notifications@github.com wrote:\nHi @adleong\nThat's very good questions :)\nFor our use case the limit can be per instance.\nRegarding server side or client side limits:\nIs there a difference if we have a Linkerd instance in front of the service?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \n",
    "magg": "@samek I was looking to do something similar to what envoy does: https://lyft.github.io/envoy/docs/configuration/http_filters/fault_filter.html\nBut I want to do this in linkerd. ",
    "franziskagoltz": "@wmorgan - Going through a backlog of linkerd issues, this came up. Do you still feel like the button look needs to be removed? The fields do have button-like functionality, as they highlight the associated resolution steps when clicked. Image below. \nI spoke with risha and siggy about it, they both refer to asking you if this is still something we want to change.\n\n. per discussion in slack, @LukaszMarchewka will take on this ticket. Hey @msiebeneicher, thanks for filling this feature request.\nTransformers would not work for this feature implementation, rather a custom RequestAuthorization plugin would be the way to go. This does not exist in Linkerd yet, but you can write your own and if you're so inclined, you're welcome to submit a PR so others can benefit from your change, too. \nWe will keep it as in open issue for future milestone planning, though we can't make any promises of when it will be released. \nThanks again!\n. Thanks for your input @perrymanuk and @frvge!. Thanks for filing this feature request @zillani!. Thank you for submitting this bug report @jftz!. current view of access log:\n\n. Thank you for submitting this issue @pavel-mikhalchuk. We will look into it and let you know if we have additional questions.\nHappy Friday!. yes, after discussing this with kevin he said to not change the Request object for now, so i'll leave this as is.. happy to change it to this. kevin and i  did loop over all headers and referrer was not a key in that sequence: here's the output for you review, in case you see the referer being associated to a different key?\n:method\nPOST\n:scheme\nhttp\n:path\n/helloworld.Hello/Greeting\n:authority\nlocalhost:4142\ncontent-type\napplication/grpc\nuser-agent\ngrpc-go/1.8.0\nte\ntrailers\nl5d-dst-service\n/svc/localhost:4142\nvia\nh2 linkerd\nl5d-dst-client\n/$/inet/127.1/8888\nl5d-ctx-trace\nBFa50jQ7Na/DuK+B+vMbkMO4r4H68xuQAAAAAAAAAAA=\nl5d-reqid\nc3b8af81faf31b90\nthe current output after changing the value of referer to reqHeaders.get(\"referer\").getOrElse(\"-\") is still \"-\". awesome! i'll give this a shot - thanks!. awesome, thanks @adleong! i had to import InetSocketAddress and hope I choose the correct one?! . ",
    "marccardinal": "When do you think this will be released? I ran into this problem when I upgraded k8s to 1.6 this morning.. When do you think this might get released? I am evaluating linkerd for production and we seem to be stuck without this fix.. Thanks for the info @klingerf, I will plug the nightly build in my k8s configuration for now.. ",
    "kumudt": "Will this PR fix the issue, If we use NodePort service instead of a loadbalancer service. Because, loadbalancer object itself will be null if the service type is NodePort. There won't be any ingress IP or hostname.. @adleong Thanks for helping me Identify the issue.. Any workaround for this until this PR gets merged??. Today, we have found another issue along with this. It is not just for a single service. We have other services as well which are failing. Also, this time we didn't even do any deployments on those services which are failing. Later, we deleted all the Linkerd and Namerd pods which fixed it. This is also happening randomly.\nNot sure, if your issues #1626 and #1635 explains this behavior.. I deployed Linkerd and Namerd 1.2.1 and made a deployment to my application. It failed just like before with 504 and I see the exact same logs in the linkerd pods.\n[l5d-cmlms l5d] E 0915 08:47:15.837 UTC THREAD44 TraceId:22097486f9419347: service failure: Failure(connection timed out: /100.96.3.244:3000 at remote address: /100.96.3.244:3000. Remote Info: Not Available, flags=0x09) with RemoteInfo -> Upstream Address: /100.96.3.241:37030, Upstream Client Id: Not Available, Downstream Address: /100.96.3.244:3000, Downstream Client Id: %/io.l5d.k8s.localnode/100.96.3.241/#/io.l5d.k8s/webapp/webapp-server/webapp-service, Trace Id: 22097486f9419347.e6fab0ca0856059c<:db5b9130907f941d \n[l5d-cmlms l5d] E 0915 08:47:16.478 UTC THREAD37 TraceId:22097486f9419347: service failure: Failure(request cancelled. Remote Info: Upstream Address: /10.11.10.139:21963, Upstream Client Id: Not Available, Downstream Address: /100.96.3.241:4145, Downstream Client Id: %/io.l5d.k8s.daemonset/linkerd/webapp-ingress/linkerd-internal/#/io.l5d.k8s/webapp/webapp-server/webapp-service, Trace Id: 22097486f9419347.37618bd902da6cc1<:22097486f9419347, flags=0x03) with RemoteInfo -> Upstream Address: /100.96.3.241:37030, Upstream Client Id: Not Available, Downstream Address: /100.96.3.244:3000, Downstream Client Id: %/io.l5d.k8s.localnode/100.96.3.241/#/io.l5d.k8s/webapp/webapp-server/webapp-service, Trace Id: 22097486f9419347.760c2ba7b873dab5<:37618bd902da6cc1 \n[l5d-cmlms l5d] E 0915 08:47:16.479 UTC THREAD42 TraceId:22097486f9419347: service failure: com.twitter.finagle.CancelledRequestException: request cancelled. Remote Info: Upstream Address: /10.11.10.139:21963, Upstream Client Id: Not Available, Downstream Address: /100.96.3.241:4145, Downstream Client Id: %/io.l5d.k8s.daemonset/linkerd/webapp-ingress/linkerd-internal/#/io.l5d.k8s/webapp/webapp-server/webapp-service, Trace Id: 22097486f9419347.37618bd902da6cc1<:22097486f9419347\n[l5d-cmlms l5d] E 0915 08:21:37.888 UTC THREAD41 TraceId:8e66c24793f4581e: service failure: Failure(No route to host: /100.96.3.244:3000 at remote address: /100.96.3.244:3000. Remote Info: Not Available, flags=0x09) with RemoteInfo -> Upstream Address: /100.96.14.161:52262, Upstream Client Id: Not Available, Downstream Address: /100.96.3.244:3000, Downstream Client Id: %/io.l5d.k8s.localnode/100.96.3.241/#/io.l5d.k8s/webapp/webapp-server/webapp-service, Trace Id: 8e66c24793f4581e.53b07a9e52e508da<:976733ca46ae590b. ```\n    telemetry:\n    - kind: io.l5d.influxdb\nnamers:\n- kind: io.l5d.k8s\n  host: localhost\n  port: 8001\n\nstorage:\n  kind: io.l5d.k8s\n  host: localhost\n  port: 8001\n  namespace: linkerd\n\ninterfaces:\n- kind: io.l5d.thriftNameInterpreter\n  ip: 0.0.0.0\n  port: 4100\n- kind: io.l5d.mesh\n  ip: 0.0.0.0\n  port: 4321\n- kind: io.l5d.httpController\n  ip: 0.0.0.0\n  port: 4180\n\n. I am using\nkubectl apply -f .yaml\n```\nThis will create new pods and delete the old ones. Also, at the end of the deployment we are deleting the old replicaset.. From 1.1.2. We skipped 1.1.3. Currently, we have downgraded it to 1.1.3.. No. Haven't checked much after downgrading to 1.1.3. But, don't think so. We have seen it significantly in 1.2.0 and 1.2.1\nThere are a lot of these errors in 1.1.2 and 1.1.3 though\n[l5d-8sksv l5d] E 0915 18:19:01.053 UTC THREAD25: [S L:/100.96.14.188:4150 R:/100.96.14.200:43078] dispatcher failed \n[l5d-8sksv l5d] com.twitter.finagle.ChannelClosedException: ChannelException at remote address: /100.96.14.200:43078. Remote Info: Not Available\nBut these errors haven't taken down the service.. Configuration looks similar to ours. Even for us, sometimes it was routing correctly to the new endpoints and sometimes it breaks with those error logs.\nAlso, In a couple of cases, it worked fine just after the deployment but started to fail after some time with the same error logs (And there were no pod terminations / reschedule operations).. Not sure. I will do one thing. I will redeploy 1.2.1. Are there any specific metrics that you are interested in? We are pushing Linkerd metrics to influxdb.. I upgraded to 1.2.1 and all services started to fail with error No hosts are available for /svc/<hostname>, Dtab.base=[], Dtab.local=[]. Remote Info: Not Available.\nNo deployments happened before and after the upgrade.\nThis is the error stack in Linkerd logs.\n[l5d-ng1p1 l5d] E 0918 06:21:18.857 UTC THREAD43: [S L:/100.96.3.71:4150 R:/100.96.3.74:33350] dispatcher failed \n[l5d-ng1p1 l5d] com.twitter.finagle.ChannelClosedException: ChannelException at remote address: /100.96.3.74:33350. Remote Info: Not Available \n[l5d-ng1p1 l5d] at com.twitter.finagle.netty4.transport.ChannelTransport$$anon$1.channelInactive(ChannelTransport.scala:186) \n[l5d-ng1p1 l5d] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245) \n[l5d-ng1p1 l5d] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231) \n[l5d-ng1p1 l5d] at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224) \n[l5d-ng1p1 l5d] at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75) \n[l5d-ng1p1 l5d] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245) \n[l5d-ng1p1 l5d] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231) \n[l5d-ng1p1 l5d] at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224) \n[l5d-ng1p1 l5d] at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75) \n[l5d-ng1p1 l5d] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245) \n[l5d-ng1p1 l5d] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231) \n[l5d-ng1p1 l5d] at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224) \n[l5d-ng1p1 l5d] at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75) \n[l5d-ng1p1 l5d] at com.twitter.finagle.netty4.channel.ChannelRequestStatsHandler.channelInactive(ChannelRequestStatsHandler.scala:36) \n[l5d-ng1p1 l5d] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245) \n[l5d-ng1p1 l5d] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231) \n[l5d-ng1p1 l5d] at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224) \n[l5d-ng1p1 l5d] at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75) \n[l5d-ng1p1 l5d] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245) \n[l5d-ng1p1 l5d] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231) \n[l5d-ng1p1 l5d] at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224) \n[l5d-ng1p1 l5d] at io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:360) \n[l5d-ng1p1 l5d] at io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:325) \n[l5d-ng1p1 l5d] at io.netty.handler.codec.http2.Http2ConnectionHandler.channelInactive(Http2ConnectionHandler.java:390) \n[l5d-ng1p1 l5d] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245) \n[l5d-ng1p1 l5d] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231) \n[l5d-ng1p1 l5d] at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224) \n[l5d-ng1p1 l5d] at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75) \n[l5d-ng1p1 l5d] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245) \n[l5d-ng1p1 l5d] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231) \n[l5d-ng1p1 l5d] at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224) \n[l5d-ng1p1 l5d] at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75) \n[l5d-ng1p1 l5d] at com.twitter.finagle.netty4.channel.ChannelStatsHandler.channelInactive(ChannelStatsHandler.scala:115) \n[l5d-ng1p1 l5d] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245) \n[l5d-ng1p1 l5d] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231) \n[l5d-ng1p1 l5d] at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224) \n[l5d-ng1p1 l5d] at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1329) \n[l5d-ng1p1 l5d] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245) \n[l5d-ng1p1 l5d] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231) \n[l5d-ng1p1 l5d] at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:908) \n[l5d-ng1p1 l5d] at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:744) \n[l5d-ng1p1 l5d] at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163) \n[l5d-ng1p1 l5d] at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403) \n[l5d-ng1p1 l5d] at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:462) \n[l5d-ng1p1 l5d] at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) \n[l5d-ng1p1 l5d] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) \n[l5d-ng1p1 l5d] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) \n[l5d-ng1p1 l5d] at com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run(BlockingTimeTrackingThreadFactory.scala:24) \n[l5d-ng1p1 l5d] at java.lang.Thread.run(Thread.java:748) \n[l5d-ng1p1 l5d]\nAlso, for some services I randomly see this error log even though there were no deployments on them for a very long time.\n[l5d-ng1p1 l5d] E 0918 07:06:33.207 UTC THREAD34 TraceId:0a6af464d790518c: service failure: com.twitter.finagle.NoBrokersAvailableException: No hosts are available for /svc/<hostname>, Dtab.base=[], Dtab.local=[]. Remote Info: Not Available. @hawkw Our Kubernetes version was 1.7.3 and the linkerd sidecar kubectl which we were using was 1.7.3 as well.. Should I change my sidecar kubectl to 1.6.2 as well and test??. @adleong I think scenario two is fixed now (after 1.2.1)\nI don't think scenario one is a dtab config issue, because we were using the same dtabs for a long time and number of errors increased after 1.2.0. It won't happen all the time. \nThis is the snapshot of the dtab playground.\nSnapshot\n. @adleong I will try to reproduce it. If not we can close this issue (except for the one service rest of them are fine from the last 24 hours).\nAlso, a specific case when we are seeing this log\n[l5d-ng1p1 l5d] E 0919 09:48:54.612 UTC THREAD39 TraceId:ca275f25b938a41e: service failure: com.twitter.finagle.ChannelClosedException: ChannelException at remote address: /100.96.3.173:3000 from service: %/io.l5d.k8s.localnode/100.96.3.71/#/io.l5d.k8s/webapp/webapp-server/webapp-service. Remote Info: Upstream Address: /100.96.15.248:35286, Upstream Client Id: Not Available, Downstream Address: /100.96.3.173:3000, Downstream Client Id: %/io.l5d.k8s.localnode/100.96.3.71/#/io.l5d.k8s/webapp/webapp-server/webapp-service, Trace Id: ca275f25b938a41e.d5de0d6d57a2490c<:3347c074369404ad\nis when the server is returning 403. Not sure if this is a config issue. If not, why isn't it returning the true response from the server. Linkerd is sending 200 in this case with an error message.. Even I have observed the endpoint watch errors and restarting all l5d pods fixed that.  . Kubernetes master we are able to reach. It was not down. May be something was changed in the way the communication is happening with the K8s master, because I am not seeing these watch error logs with Linkerd 1.1.2 and the same K8s version 1.7.3. Also, as I mentioned before restarting Linkerd pods is fixing this issue.. ",
    "superwen0001": "I'm not sure whether my writing has a problem, what should be correctly spelled.\nEnglish is not very good, please forgive me. ",
    "jamessharp": "Just to let you know - I ran into what looks like a memory leak in production with linkerd 1.0.2 (kubernetes 1.6.2, GKE, daemonsets)\nHere's the l5d memory usage for the last week - I upgraded from 1.0.0 to 1.0.2 on Monday afternoon. Performed various restarts in between, downgraded to 1.0.0 again today (Thursday), and everything is looking OK again.\n\n. I've sent @siggy a slack message with the config I've been using. Let me know if there's anything else you need. Hi Everyone - I've been away so haven't been able to look into this yet.\nI've only managed to get it happening in production, which is less than ideal for testing, but with a bit of close monitoring I reckon I can get a memory dump for you. I've had this running all day and got to about 600MB of mem usage. Due to time constraints I'm going to have to take it down. I'm sending the dump over to @siggy now (via slack). Please keep it confidential. Here's the memory usage of the pod over time:\n\nIgnoring the final peak (when I took the dump), it's interesting that kubectl is also consuming more and more memory. @siggy - Glad to see you're making progress!\nThe setup is our production GKE cluster running roughly 10 nodes, with linkerd on all of them as a service mesh. All traffic was real world traffic - we get ~1m HTTP requests in a day. Requests come in via 2 external facing endpoints that run https://github.com/ortoo/linkerd-proxy (a very simple, but very possibly dodgy, proxy that terminates SSL and strips out l5d headers). The proxies then forward the traffic onto their hosts linkerd and it routes it from there. \nI've now moved those proxies over to use nginx rather than the custom go app, and upgraded to linkerd 1.1.0 and there seems to be no sign of the leak. We've just run into this as well - can probably get a repro script up and running if it would be helpful?. Repro setup here: https://github.com/ortoo/linkerd-1605. Hopefully the README gives you enough info, but let me know if not.\nInitial findings from having a quick play around is that adding in an explicit initialStreamWindowBytes: 65536 (supposedly the default) to the server config when in client -> linkerd -> server mode does the trick and things work. However in client -> linkerd -> linkerd -> server everything over 64KB hangs, even with initialStreamWindowBytes set.. ",
    "turchanov": "Yes, indeed, the issue is triggered by RBAC. I posted a bug report for that.. ",
    "ethanrubio": "I'm also running into issues with namerd trying to get the dtabs.l5d.io from the thirdpartyresource found here. \nio.buoyant.k8s.Api$UnexpectedResponse: Response(\"HTTP/1.1 Status(403)\"): User \"system:serviceaccount:default:default\" cannot list dtabs.l5d.io in the namespace \"default\".\n    at io.buoyant.k8s.Api$.parse(Api.scala:72)\n    at io.buoyant.k8s.ListResource.$anonfun$get$2(resources.scala:136)\n    at com.twitter.util.Future.$anonfun$flatMap$1(Future.scala:1089)\n    at com.twitter.util.Promise$Transformer.liftedTree1$1(Promise.scala:107)\n    at com.twitter.util.Promise$Transformer.k(Promise.scala:107)\n    at com.twitter.util.Promise$Transformer.apply(Promise.scala:117)\n    at com.twitter.util.Promise$Transformer.apply(Promise.scala:98)\nI've allowed linkerd and namerd to have all permissions as mentioned above:\n```\n\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: RoleBinding\nmetadata:\n  name: linkerd\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: linkerd-allow-all\nsubjects:\n- kind: ServiceAccount\n  name: default\n  namespace: default\n\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: linkerd-allow-all\nrules:\n  - apiGroups: [\"\"]\n    resources:\n      - ''\n    verbs:\n      - ''\n\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: RoleBinding\nmetadata:\n  name: namerd\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: namerd-allow-all\nsubjects:\n- kind: ServiceAccount\n  name: default\n  namespace: default\n\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: namerd-allow-all\nrules:\n  - apiGroups: [\"\"]\n    resources:\n      - ''\n    verbs:\n      - ''\n```\nHow can I let namerd have permissions to get the dtabs from the third party resource?. A quick update I solved it by doing the following:\n```\napiVersion: rbac.authorization.k8s.io/v1alpha1\nkind: ClusterRoleBinding\nmetadata:\n  name: namerd\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: default\n  namespace: default\n\napiVersion: rbac.authorization.k8s.io/v1alpha1\nkind: ClusterRoleBinding\nmetadata:\n  name: namerctl\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: default\n  namespace: default\n\napiVersion: rbac.authorization.k8s.io/v1alpha1\nkind: ClusterRoleBinding\nmetadata:\n  name: namerctl-script\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: default\n  namespace: default\n``\nIs there a better way to do this than granting cluster-admin access?. Is there anything that I can provide to get a better diagnosis? I'd like to see if I can do anything to push this forward since theio.l5d.mesh` API becomes the preferred method soon. . @siggy is there a corresponding namerd config that goes with this linkerd config?. ",
    "kalyan02": "@wmorgan While I understand the flag turns it off, it is not OK on principle that it is enabled by default and set to aggressively post metrics hourly. . I see few possibilities:\n1. Disable this completely. Remove all code.\n2. Disable by default. Enable it optionally.\nAdditional improvements:\n- Make it very very obvious in documentation that this is enabled by default.\n- Make it less aggressive and configurable.\nAm also unsure how security auditors will take to it in its current form (we were planning on having a fairly large deployment at booking.com). ",
    "mkavanagh": "I appreciate that you are trying to be transparent as possible with this but I still think this is a surprising thing for server software to be doing. The data being sent is identifying because it necessarily includes the source IP address, it's an unnecessary risk from a user perspective to expose the OS version, and the fact this has been done means I feel I have to carefully audit every linkerd release I deploy to confirm that nothing else \"surprising\" has been added.\nI don't know any other examples of commercially backed F/OSS server software that have opt-out phone home functionality - I'd be glad to be enlightened with examples I'm unaware of though, because I'd always want to disable that functionality in any production environment.. ",
    "aguilbau": "sure, I will update the main post with these informations.. So, I added -XX:+HeapDumpOnOutOfMemoryError to the namerd jvm, and got a dump of the heap\nYou can find it here (it is from v1.1.0) :\nhttp://dl.free.fr/r5hHafwSy\nMaybe that can help tracking the cause of the crash. Hi @stevej , I'm using v1.6.4. \nServer Version: version.Info{Major:\"1\", Minor:\"6\", GitVersion:\"v1.6.4+coreos.0\", GitCommit:\"8996efde382d88f0baef1f015ae801488fcad8c4\", GitTreeState:\"clean\", BuildDate:\"2017-05-19T21:11:20Z\", GoVersion:\"go1.7.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}. Also, I don't know if this is related, but some instances of namerd are outputing this error, in a loop:\nhttps://pastebin.com/y7ucrDiE\nI will try to see if these instances are subject to the bug.\nEdit: so yes, there is a correlation between the memory usage increase and these logs.\nThe instances outputing these errors in a loop see their memory usage grow until they reach the limit and crash after a while, while the others stay still.\nHere is an other error that i see in a loop :\nhttps://pastebin.com/ksZQWkmp\nAlso, on these pods, namerctl loops on this line :\nGET https://10.3.0.1:443/api/v1/namespaces/kube-system/services 200 OK in 3 milliseconds. @adleong you might be right ;)\n$> kubectl get svc -o yaml --all-namespaces | grep 'targetPort: [^0-9]'\n      targetPort: tiller. @adleong yes, even without named ports.\nAlso, I can confirm that the problem is gone when I stop routing to services in kube-system.. ",
    "mejran": "I got hit by this memory leak recently and I'm wondering if this could be the cause: \nhttps://github.com/linkerd/linkerd/blob/master/config/src/main/scala/io/buoyant/config/JsonStreamParser.scala#L116\nThere seems to be a recursive call done on an very long reader input which seems to lead to very deep call stacks and memory usage.\nedit: Not call stack I think but references are kept to previous promises.. Yes, copy mistake, fixed now, thanks.. I've dug further into this. I think the issue is that if one of the ingress definitions has a default rule in it then the evaluation of potential routes will stop at that rule. \nhttps://github.com/linkerd/linkerd/blob/master/k8s/src/main/scala/io/buoyant/k8s/IngressCache.scala#L65\nhttps://github.com/linkerd/linkerd/blob/master/k8s/src/main/scala/io/buoyant/k8s/IngressCache.scala#L21. Yeah, exactly right.. Does anyone know a better way to get AsyncStream.scanLeft to not be behind by one element? I'm currently adding two elements to the input stream (one real, one fake) and then ignoring the second one. It works but it's definitely not the cleanest.. Thanks Alex. \nI've been able to create a more minimal example of the scanLeft failing:\nval p = Promise[Int]\n    val p2 = Promise[Int]\n    def s: AsyncStream[Int] = AsyncStream.fromFuture(p)\n      .flatMap(_ +:: AsyncStream.fromFuture(p2)\n        .flatMap(_ +:: AsyncStream.empty[Int]))\n    val ss = s.scanLeft(0)(_ + _).filter(_ >= 1)\n    println(ss.uncons.isDefined)\n    p.setValue(1)\n    //p2.setValue(1)\n    println(ss.uncons.isDefined)\nThe second println should be true however it is false. If you uncomment the p2.setValue it becomes true. If you change almost anything else in the example it also becomes true (for example remove the flatMap or remove the  AsyncStream.empty). So basically we're hitting an edge case of scanLeft which doesn't behave correctly.\nThe best I can figure is that the flatMap causes the AsyncStream to be wrapped in an AsyncStream.Embed which triggers the first case of the scanLeft case match which blocks. The first case however doesn't output the previous result but rather indirectly calls the Cons case to return it (which would happen as soon as the Embed future is defined). Still not fully sure of the flow or how to fix it however.. Comment added.. My boss I believe has already accepted the CLA on behalf of the company, Iown.. We hit this issue as well, however out of two linkerd ingress controllers one updated correctly to the new endpoint and one still pointed to the old endpoint. Something to watch out for since the issue may be random or configuration dependent.. ",
    "ldemailly": "thanks ! it does work after git clone + git checkout 0.1.2 which builds with a minor issue #1372. ",
    "thedebugger": "@adleong ahh, that makes sense. I thought we are setting the logger level to the given level. Actually, we are trying to log the traces with the given level. I'll update the docs to make it more clear. Thanks!. hi @klingerf, sorry, somehow this skipped my mind. I think it makes sense to have trace level default to INFO, instead of setting it to TRACE -- https://github.com/linkerd/linkerd/blob/master/linkerd/examples/tracelog.yaml#L16\nIt is not intutive when someone runs examples/tracelog.yaml, and not see any traces logged when the purpose of examples/tracelog.yaml is primarily to log traces.. @dadjeibaah so it does retry depending on where namerd receives 404. If it receives 404 during watchable, it tries to restart the watch. But restarting a watch requires a GET on the resource which if it fails with 404, namerd stops watching. That is what we are seeing. I can't reproduce the same scenario. The closest i can come up with is above steps. Let me know if you more questions.. i gave it another try on namerd 1.5.2 but i didn't see above log line. Here are the namerd logs\nI just follow the same steps as above\n1. bootup namerd with some dtabs. let it run for few mins.\n2. kubectl delete crd/dtabs.l5d.io. Here is crd spec \n3. namerd logs the following line\nW 0128 23:49:17.122 UTC THREAD45 TraceId:f4ee3042e6688020: k8s failed to watch resource /apis/l5d.io/v1alpha1/namespaces/ck-system/dtabs: 404 Not Found\nD 0128 23:49:17.139 UTC THREAD11 TraceId:f4ee3042e6688020: k8s restarting watch on /apis/l5d.io/v1alpha1/watch/namespaces/ck-system/dtabs, resource version Some(1268327) was too old\n4. But no logs after that indicates the watch has been stopped\nLet me know if you folks need more information. Thanks!. @dadjeibaah hey so i think this won't fix our problem. Let me try to capture the state transitions\n1) namerd makes a watch dtab storage call to the API server \n2) if the call succeeds watch is established, namerd listens for events. This PR logs the watch events if i'm reading the code correctly. Right?\n3) if the first fails for any reason (this call fails with 404s), namerd tries to restart the watch by making a GET call (to get updated version) on the resource. We see 404s in this call too. So i think we should add a WARN for this case.\nLet me know if this makes sense. Feel free to ping me on slack if you want to discuss more.. I just want to add that both client (linkerd) and server (namerd) should support enabling the ping frame. @dadjeibaah I'll try if i can go over this tonight. Thanks for the PR. @dadjeibaah @adleong we are trying to test this today. Hopefully, I will have an update on it later today. @adleong @dadjeibaah this looks great. Let me verify it. Is there a build that i can use? Or should i build it myself?. @dadjeibaah We build our own images. So let me try building it and create a new docker image. @dadjeibaah thank for you for being patient. I was able to verify the fix - namerd is retrying in the case of 404 and recovers when I create the new CRD. Thanks!. Here is the snapshot of namerd metrics for k8s namer client. It has connections, request metrics, and latency \n\n. @klingerf if the resource version is actually \"65078108\" and we are seeing above error, that to me sounds like a k8s bug. Based on my experience working with zk/consul, the client should always send a last known version because the client could miss the events when the client is not watching. So if the client has an old version, k8s should return what has changed and latest resource version. I hope we tested that linkerd doesn't miss a watch change b/w the retries. But again, i'm speaking based on my past experience and may not be true for k8s.. ",
    "chrisgoffinet": "I have confirmed this is a real leak. This exists in the latest build as well. I've taken a heap dump and working on a patch to fix this.. Can you try adding this to LOCAL_JVM_OPTIONS:\n-Djdk.nio.maxCachedBufferSize=262144\nAnother user was having similar leaks in the JVM, and after providing this suggestion his memory growth stabilized. . @mebe I'd be happy to take a look as well, feel free to email me at chris@threecomma.io. Logging into the machine would be super helpful. @mebe can you confirm you are using the same AMI image as production? I want to rule out any glibc differences. I too see it settle around 3.5GB. Also you should be running with-XX:+AlwaysPreTouch that way the JVM heap memory gets touched and the RSS will show the actual. It will be much easier to watch.. @mebe i was able to reproduce your issue, before I do a full write up on what's going on, could you try applying these changes below and report back the results. When running linkerd, do not use taskset.\n```\nLOCAL_JVM_OPTIONS=\"-XX:MaxDirectMemorySize=2G -XX:+AlwaysPreTouch\"\nLOCAL_JVM_OPTIONS=\"$LOCAL_JVM_OPTIONS -Dio.netty.threadLocalDirectBufferSize=0 -Djdk.nio.maxCachedBufferSize=262144\"\nLOCAL_JVM_OPTIONS=\"$LOCAL_JVM_OPTIONS -Dio.netty.recycler.maxDelayedQueuesPerThread=4\"\nLOCAL_JVM_OPTIONS=\"$LOCAL_JVM_OPTIONS -Dio.netty.recycler.maxCapacity=4096\"\nLOCAL_JVM_OPTIONS=\"$LOCAL_JVM_OPTIONS -Dio.netty.allocator.numHeapArenas=4\"\nLOCAL_JVM_OPTIONS=\"$LOCAL_JVM_OPTIONS -Dio.netty.allocator.numDirectArenas=4\"\nexport JVM_HEAP_MIN=4G\nexport JVM_HEAP_MAX=4G\nexport MALLOC_ARENA_MAX=2\nexport MALLOC_MMAP_THRESHOLD_=131072\nexport MALLOC_TRIM_THRESHOLD_=131072\nexport MALLOC_TOP_PAD_=131072\nexport MALLOC_MMAP_MAX_=65536\n``. Also please paste the results forps aux | grep linkerdso we can confirm the options I put above ended up getting passed properly, for sanity sake. Thank you. @mebe Could you add this as addition to theLOCAL_JVM_OPTIONS` and report back the results.\n-Dcom.twitter.finagle.netty4.numWorkers=4, I missed that Finagle was lazily using numCores() * 2\nThank you. I will open a new PR with optimal settings.\ntl; dr:\n\nmemory fragmentation due to glibc\nnetty and finagle tuneables required to bound memory growth, especially gets bad on systems with many cpu cores due to defaults in finagle and netty for their pool allocator.\n\nWe have a couple issues going on that I think I've been able to nail down. To rule out a memory leak I used jemalloc + jeprof to trace where memory was being allocated outside the JVM. I was able confirm there was no leaks occurring, and suspected what we have been seeing was malloc heap fragmentation. But it became a bit more complicated since netty implements their own allocator, and the underlying malloc() still is using glibc (more on that later).\nThe first issue is finagle by default will use numCores() * 2. So in the case for @mebe they had 72 cores * 2 = 144 finagle threads running due to their large machine.  -Dcom.twitter.finagle.netty4.numWorkers is a way to override this. The tricky part was finagle does not just create those threads on startup. They are lazily created, and it took me awhile even after most of these settings to track down why I was still seeing some growth, even slowly. Which was due to Finagle's lazy thread creation.\nIn netty 4x, they implemented the pool allocator, which is based on a variant of jemalloc using buddy and slab allocation (https://netty.io/wiki/new-and-noteworthy-in-4.0.html)\nOut of the box, netty and finagle will set io.netty.allocator.numHeapArenas and io.netty.allocator.numDirectArenas to numCores() * 2. This is to reduce per-thread contention when accessing memory. Whatever value we set numWorkers to above, we need to set those values for the heapArenas.\nglibc fragmentation:\nLlinkerd with docker (uses Debian Jessie w/ glibc 2.24) and most linux distros are using glibc >=2.10, glibc implemented per-thread arenas. Feel free to read the malloc scalability section at https://udrepper.livejournal.com/20948.html\nYou can adjust this behavior by setting MALLOC_ARENA_MAX, which will set a max # of arenas for the JVM to use, instead of it being calculated based on # of threads running (which is the default). I've found MALLOC_ARENA_MAX=2 to be acceptable, and found various other projects utilizing this value without major performance issues.\nNetty 4.x changes:\nOn Feb 12, an issue was opened and fixed in Netty 4.1.22 where they were caching DirectByteBuffer objects (up to 64k) along with the recycler which may cache up to (32k) objects. The below PRs have this fixed in 4.1.22, and thankfully they are already tunable so we can adjust this by setting a defined property.\n-Dio.netty.threadLocalDirectBufferSize=0\n-Dio.netty.recycler.maxCapacity=4096\nhttps://github.com/netty/netty/pull/7704\nhttps://github.com/netty/netty/pull/7701\nHere are some graphs I put together tracking RSS with before and after.\nDefault Values\n\nTuned values\n\n. @yennifehrrr The memory usage graph, is that JVM heap usage or RSS usage?. Two issues I discovered with this ticket, one I believe is an actual memory leak regarding consul.\nnon-existing service\nWhenever any service is passed to Linkerd, existent or non-existent, we create stats for tracking purposes for that service. Because the service doesn't exist, it's going to keep generating stats for each new service. Making it look like memory leak due to the decision of creating stats for any service that gets looked up. After the default 10 minutes of a service being idle, the stats for the services will be torn down.\n@adleong please chime in, i think it might of been a design decision to do this -- but if not we could just simply decide not to create a stats for every svc that we can't lookup.\nconsul memory leak\nI thought this was fixed in #1877 but it looks like when investigating this bug, we found another leak. I am happy to submit a patch for this, it looks like it just keeps the client around when a service didn't exist in consul.\n```tcp    ESTAB      0      0      172.17.0.3:39968                172.17.0.5:8500\ntcp    ESTAB      0      0      172.17.0.3:39756                172.17.0.5:8500\ntcp    ESTAB      0      0      172.17.0.3:40066                172.17.0.5:8500\ntcp    ESTAB      0      0      172.17.0.3:39902                172.17.0.5:8500\ntcp    ESTAB      0      0      172.17.0.3:40014                172.17.0.5:8500\ntcp    ESTAB      0      0      172.17.0.3:39936                172.17.0.5:8500\ntcp    ESTAB      0      0      172.17.0.3:39946                172.17.0.5:8500\ntcp    ESTAB      0      0      172.17.0.3:39916                172.17.0.5:8500\ntcp    ESTAB      0      0      172.17.0.3:39874                172.17.0.5:8500\ntcp    ESTAB      0      0      172.17.0.3:39972                172.17.0.5:8500\ntcp    ESTAB      0      0      172.17.0.3:39848                172.17.0.5:8500\ntcp    ESTAB      0      0      172.17.0.3:39888                172.17.0.5:8500\ntcp    ESTAB      0      0      172.17.0.3:39804                172.17.0.5:8500\ntcp    ESTAB      0      0      172.17.0.3:40026                172.17.0.5:8500\ntcp    ESTAB      0      0      172.17.0.3:39796                172.17.0.5:8500\n$ docker exec -it linkerd ss | grep 8500  | wc -l\n     156\n``\n. Confirmed the consul memory leak looks to be due toLookupCachehaving its ownconsulApi` being passed to. As soon as the non-existent svcs  get torn down after idleTimeout of 10m, the connections are released. /cc @adleong \nWe should make the code use a pool of consul agents for requests.. @mirosval the memory growth is coming from the fact that they create a ton of stats for every new service that tries to be requested. So for example if you are sending services with the following:\nhelloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4 it creates stats:\n/grpc-consul/service/svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/failures\" : 1,\n  \"rt//grpc-consul/service/svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/failures/com.twitter.finagle.NoBrokersAvailableException\" : 1,\n  \"rt//grpc-consul/service/svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/request/stream/data_bytes.count\" : 0,\n  \"rt//grpc-consul/service/svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/request/stream/stream_duration_ms.count\" : 0,\n  \"rt//grpc-consul/service/svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/request/stream/stream_failures\" : 0,\n  \"rt//grpc-consul/service/svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/request/stream/stream_success\" : 0,\n  \"rt//grpc-consul/service/svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/request_latency_ms.count\" : 0,\n  \"rt//grpc-consul/service/svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/requests\" : 1,\n  \"rt//grpc-consul/service/svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/response/stream/data_bytes.count\" : 0,\n  \"rt//grpc-consul/service/svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/response/stream/stream_duration_ms.count\" : 0,\n  \"rt//grpc-consul/service/svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/response/stream/stream_failures\" : 0,\n  \"rt//grpc-consul/service/svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/response/stream/stream_success\" : 0,\n  \"rt//grpc-consul/service/svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/retries/backoffs_exhausted\" : 0,\n  \"rt//grpc-consul/service/svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/retries/budget\" : 0.0,\n  \"rt//grpc-consul/service/svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/retries/budget_exhausted\" : 0,\n  \"rt//grpc-consul/service/svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/retries/classification_timeout\" : 0,\n  \"rt//grpc-consul/service/svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/retries/per_request.count\" : 0,\n  \"rt//grpc-consul/service/svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/retries/request_stream_too_long\" : 0,\n  \"rt//grpc-consul/service/svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/retries/response_stream_too_long\" : 0,\n  \"rt//grpc-consul/service/svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/retries/total\" : 0,\n  \"rt//grpc-consul/service/svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/sourcedfailures//svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4\" : 1,\n  \"rt//grpc-consul/service/svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/sourcedfailures//svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/com.twitter.finagle.NoBrokersAvailableException\" : 1,\n  \"rt//grpc-consul/service/svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/stream/data_bytes.count\" : 0,\n  \"rt//grpc-consul/service/svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/stream/stream_failures\" : 0,\n  \"rt//grpc-consul/service/svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/stream/stream_success\" : 0,\n  \"rt//grpc-consul/service/svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/stream/total_latency_ms.count\" : 0,\n  \"rt//grpc-consul/service/svc/helloworld.Greeter-44c56e49-b192-4272-9148-7a2fc698b0b4/success\" : 0\nAnd on top of the new stats for every service that is requested, it also makes connections to Consul.\nWhen you run your unit tests, make a curl call to the admin port for /admin/metrics.json?pretty=1 and you can see the tons of stats that end up getting created for services that do not exist. This adds to the memory growth that is seen.. I would say yes after all the clarifications and design decisions being made here.\nBest,\nChris On Wed, May 23, 2018 at 9:18pm, William Morgan < notifications@github.com [notifications@github.com] > wrote:\nSo is this effectively \"working as intended\", or is there an underlying bug that has been uncovered?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub [https://github.com/linkerd/linkerd/issues/1824#issuecomment-391556654] , or mute the thread [https://github.com/notifications/unsubscribe-auth/AZOaFRYnypWrWX-6Wn2qm-X1-t22zaXcks5t1gpUgaJpZM4SKTXp] .. Haven't been able to reproduce this just yet. So far I've setup the environment where I randomly send SIGSTOP and SIGCONT to the compute (linkerd) process, and I've also setup chaos monkey style approach of randomly killing that process. So far none have yielded the result. My next step is to try simulating packet corruption.. Commit updated. Yes so far HomeAway tried it out and their memory usage has been stable for 5 days. \nSee the following thread:\nhttps://discourse.linkerd.io/t/possible-memory-leak/572. ### tl; dr:\n * memory fragmentation due to glibc\n * netty and finagle tuneables required to bound memory growth, especially gets bad on systems with many cpu cores due to defaults in finagle and netty for their pool allocator.\nWe have a couple issues going on that I think I've been able to nail down. To rule out a memory leak I used jemalloc + jeprof to trace where memory was being allocated outside the JVM. I was able confirm there was no leaks occurring, and suspected what we have been seeing was malloc heap fragmentation. But it became a bit more complicated since netty implements their own allocator, and the underlying malloc() still is using glibc (more on that later).\nThe first issue is finagle by default will use numCores() * 2. So in the case for @mebe they had 72 cores * 2 = 144 finagle threads running due to their large machine.  -Dcom.twitter.finagle.netty4.numWorkers is a way to override this. The tricky part was finagle does not just create those threads on startup. They are lazily created, and it took me awhile even after most of these settings to track down why I was still seeing some growth, even slowly. Which was due to Finagle's lazy thread creation.\nIn netty 4x, they implemented the pool allocator, which is based on a variant of jemalloc using buddy and slab allocation (https://netty.io/wiki/new-and-noteworthy-in-4.0.html)\nOut of the box, netty and finagle will set io.netty.allocator.numHeapArenas and io.netty.allocator.numDirectArenas to numCores() * 2. This is to reduce per-thread contention when accessing memory. Whatever value we set numWorkers to above, we need to set those values for the heapArenas.\nglibc fragmentation:\nLlinkerd with docker (uses Debian Jessie w/ glibc 2.24) and most linux distros are using glibc >=2.10, glibc implemented per-thread arenas. Feel free to read the malloc scalability section at https://udrepper.livejournal.com/20948.html\nYou can adjust this behavior by setting MALLOC_ARENA_MAX, which will set a max # of arenas for the JVM to use, instead of it being calculated based on # of threads running (which is the default). I've found MALLOC_ARENA_MAX=2 to be acceptable, and found various other projects utilizing this value without major performance issues.\nNetty 4.x changes:\nOn Feb 12, an issue was opened and fixed in Netty 4.1.22 where they were caching DirectByteBuffer objects (up to 64k) along with the recycler which may cache up to (32k) objects. The below PRs have this fixed in 4.1.22, and thankfully they are already tunable so we can adjust this by setting a defined property.\n-Dio.netty.threadLocalDirectBufferSize=0\n-Dio.netty.recycler.maxCapacity=4096\nhttps://github.com/netty/netty/pull/7704\nhttps://github.com/netty/netty/pull/7701\nHere are some graphs I put together tracking RSS with before and after.\nDefault Values\n\nTuned values\n\n. @milesbxf trying to share the love, check out #2010. I know of a large user of Linkerd using Prom + many services, and we discovered that the /admin interface wasnt setup with its own thread pool. Our tail latencies have been impacted since it was in critical path of requests.\n. \n. PR #1997. Yes I'll get that ready\nBest,\nChris On Mon, Jun 18, 2018 at 1:48pm, Alex Leong < notifications@github.com [notifications@github.com] > wrote:\n@adleong commented on this pull request.\nAwesome! Should we have a separate variable for this so that it can be disabled when features like jmap are desired?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub [https://github.com/linkerd/linkerd/pull/1997#pullrequestreview-129668857] , or mute the thread [https://github.com/notifications/unsubscribe-auth/AZOaFQnWpZ4p4blD0CGVzqMvaTB9VEaqks5t9-f3gaJpZM4Uq92Q] .. @adleong Sorry for the delay on this, got busy. If you want to enable the old behavior you can set ENABLE_HSPREF to true or any value. When it's not set it will disable this feature (recommended) for production use.. @adleong interesting point. I am using the defaults which sets Xms to 32MB and grows to 1gb. I re-ran the tests and forced Xms == Xmx and latency for /admin/metrics.json went from 70ms to <30ms.\nI took your PR #2006, latency went down there too. So that helped. . I think what's going on here is that /admin runs in the same Finagle/netty pool as the critical path of requests. Since we are looking at tail latencies -- when running those endpoints it makes sense that if a request tried to come in it would be blocked until the handlers in Admin finished processing the stats output which currently do take anywhere from 70ms to 300ms depending what's being run (/admin/metrics vs /admin/metrics/prometheus). @adleong Thanks for the quick turn around on this! Here is a graph after applying the PR #2010.\nWe can close this out after this gets merged. Latency dropped to <10ms as expected!\n\n. Yes go ahead.. Update as I started looking into this issue. I can easily reproduce this now. I've confirmed the K8s endpoint notifications do get sent to Linkerd on destroy and create. It's the client_state.json that's showing the staleness. Now let me also explain why it's kind of hard to catch this running on say your laptop. I noticed that if you just start destroying pods, they will start back up really quickly, and reuse the same IP address, so if you're trying to hit this case it looks like no bug. The client_state.json is technically stale.\nIt wasn't until I modified my deployment in K8s to inject an init container that would add a 30s sleep on pod creation, that we can see this bug surface easily. I noticed that it never seems to recover, unless you stop all traffic and let the 10m idle timeout kick in, which destroys all the state. \nNow that this is easily repo now, I should be able to track down where in the code we're missing this. . I see you are running linkerd in Kubernetes. Be aware about the cgroup cfs scheduling you can adjust to give higher priority.\nSee this example for in-detail on the issue and how to tune it.\nhttps://github.com/golang/go/issues/19378#issuecomment-304892994\nI found this on the golang github issues but it applies to everything.. #2108 should resolve this. Please help test and verify. Forgot to force push that, fixed.. @perrymanuk What is your prometheus scape interval currently set to? . @adleong this is working from my testing. If you would like to validate the backlog queue size is adjusted, exec into the container and run:\n$ ss -ln\nIf you look at the LISTEN of the port you set backlog to, the Send-Q should match the value.\nNOTE: In Linux if your net.core.somaxconn value is say 128, you can't adjust the backlog queue above what the host level value is. Linux will silently truncate it if you try to go above. So you can only set backlog to <=$CURRENT_VALUE. Here is the test case for the kubectl proxy issue. If you build a copy of kubectl from master branch of Kubernetes, you will see a new http2 connection will open once you run curl and everything will work as expected.\nproxy-streams.zip\n. @adleong I think using http2 and doing our own stream management is the right approach. Relying on http1/.1 upgrading and version of kubectl can be problematic as you can see. What happens if Go makes further changes?. Not a solution. Cloud providers like GKE and AWS Don't allow you to make\nthese configurations\nOn Fri, Mar 15, 2019, 4:56 PM Alex Leong notifications@github.com wrote:\n\nAnother potential solution is to simply increase the value of\nmaxConcurrentStreams on the Kubernetes api server (which I believe can be\nset by a flag). If Linkerd needs to watch more than 250 services, I think\nit would be better to raise this limit and have all of those watches on a\nsingle connection, rather than splitting them across multiple connections.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/linkerd/linkerd/issues/2251#issuecomment-473438334,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AZOaFQRlodM8AvoP0qWRK6P4uosqQROFks5vXAj9gaJpZM4b1Ux0\n.\n. Unfortunately there is no default for SOMAXCONN, it depends on the kernel :), by setting it to None, it tells Finagle to not set it, and let the OS handle it. \n",
    "UnwashedMeme": "I came across https://linkerd.io/getting-started/istio/ yesterday found it interesting and trying to find more got to here. The big question I'm trying to figure out is \"Why?\" or \"What does this setup offer that is better than running istio with envoy or linkerd standalone?\"\nI'm seeing a whole bunch of issues open on improving integration and from what I've found linkerd is a bit closer to envoy and istio is providing a larger control plane?. ",
    "nitishm": "Why is the doc no longer available at https://linkerd.io/getting-started/istio/ ?. ",
    "cchatfield": "In my case, traces can start from the UI. With these traces, it's important to capture the full correlated trace including the spans from linkerd. I think your suggestion of creating a plain text header might be a little easier than the encoded serialized one for linkerd.\nI did get the tracing to pass correctly by serializing in the form needed by linkerd.\nHere is a snippet that works for node.js services.\nconst ByteBuffer = require('bytebuffer');\n\npublic traceToLinkerdPackedHeader():string{\n  let flagBytes = [0, 0, 0, 0, 0, 0, 0, 6];\n  let serialized = ByteBuffer.concat([this.getByteArray(this.traceId.spanId),this.getByteArray(this.traceId.parentId), this.getByteArray(this.traceId.traceId), flagBytes]);\n\n  return serialized.toBase64();\n}\nprivate getByteArray(id: string ){\n  let array = Buffer.from(id, 'utf8');\n  let byteArray  = [];\n  for (let i = 0; i < 16; i = i + 2) {\n    let parsed = parseInt(id.slice(i, i + 2), 16);\n    parsed = parsed & 255;\n\n    if (parsed > 127) {\n      parsed = -((parsed & 127) ^ 127) - 1;\n    }\n\n    byteArray.push(parsed);\n  }\n\n  return byteArray;\n}\n\n. @hawkw\nWith 1.3.6 (docker pull buoyantio/namerd) I am seeing the observation error spam in the log.\n```\nW 0317 03:14:07.864 UTC THREAD28 TraceId:83b2534e4e7d2f12: consul datacenter 'dc1' service 'service-payments' observation error Failure(service observation released, flags=0x02) with RemoteInfo -> Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: consul.service.consul/10.10.1.4:8500, Downstream label: client, Trace Id: 27d1a3600be2b20b.27d1a3600be2b20b<:27d1a3600be2b20b. Last known state is Bound(Set(Inet(/10.12.1.5:31562,Map(endpoint_addr_weight -> 1.0))),Map(authority -> service-payments.service.dc1.consul))\nW 0317 03:14:12.869 UTC THREAD27 TraceId:83b2534e4e7d2f12: consul datacenter 'dc1' service 'service-payments' observation error Failure(service observation released, flags=0x02) with RemoteInfo -> Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: consul.service.consul/10.10.1.4:8500, Downstream label: client, Trace Id: b700ed57ab93668d.b700ed57ab93668d<:b700ed57ab93668d. Last known state is Bound(Set(Inet(/10.12.1.5:31562,Map(endpoint_addr_weight -> 1.0))),Map(authority -> service-payments.service.dc1.consul))\nW 0317 03:14:18.709 UTC THREAD27 TraceId:83b2534e4e7d2f12: consul datacenter 'dc1' service 'service-payments' observation error Failure(service observation released, flags=0x02) with RemoteInfo -> Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: consul.service.consul/10.10.1.4:8500, Downstream label: client, Trace Id: fa573f54f71166af.fa573f54f71166af<:fa573f54f71166af. Last known state is Bound(Set(Inet(/10.12.1.5:31562,Map(endpoint_addr_weight -> 1.0))),Map(authority -> service-payments.service.dc1.consul))\nW 0317 03:14:19.711 UTC THREAD27 TraceId:83b2534e4e7d2f12: consul datacenter 'dc1' service 'service-payments' observation error Failure(service observation released, flags=0x02) with RemoteInfo -> Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: consul.service.consul/10.10.1.4:8500, Downstream label: client, Trace Id: 5f11725cfcb18726.5f11725cfcb18726<:5f11725cfcb18726. Last known state is Bound(Set(Inet(/10.12.1.5:31562,Map(endpoint_addr_weight -> 1.0))),Map(authority -> service-payments.service.dc1.consul))\nW 0317 03:14:37.093 UTC THREAD27 TraceId:83b2534e4e7d2f12: consul datacenter 'dc1' service 'service-payments' observation error Failure(service observation released, flags=0x02) with RemoteInfo -> Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: consul.service.consul/10.10.1.4:8500, Downstream label: client, Trace Id: 38ab7ee46a211cd5.38ab7ee46a211cd5<:38ab7ee46a211cd5. Last known state is Bound(Set(Inet(/10.12.1.5:31562,Map(endpoint_addr_weight -> 1.0))),Map(authority -> service-payments.service.dc1.consul))\n```\nI am also seeing namerd is creating thousands of established connections over time to consul ultimately leading consul to run out of file descriptors. It seemed with 1.3.5 to create about ~200 a day. With 1.3.6 it creates about 500 a day. This ultimately brings down consul and vault rendering an environment unroutable.\nSample lsof -p [consul pid]\nconsul  51242 root  525u     IPv6          264999301      0t0       TCP non-prod-one-instance-consul-0.node.dc1.consul:fmtp->non-prod-two-instance-nomad-client-0.node.dc1.consul:47658 (ESTABLISHED)\nconsul  51242 root  526u     IPv6          264999302      0t0       TCP non-prod-one-instance-consul-0.node.dc1.consul:fmtp->non-prod-two-instance-nomad-client-0.node.dc1.consul:47652 (ESTABLISHED)\nconsul  51242 root  528u     IPv6          264998500      0t0       TCP non-prod-one-instance-consul-0.node.dc1.consul:fmtp->non-prod-two-instance-nomad-client-0.node.dc1.consul:47666 (ESTABLISHED)\nconsul  51242 root  529u     IPv6          264998501      0t0       TCP non-prod-one-instance-consul-0.node.dc1.consul:fmtp->non-prod-two-instance-nomad-client-0.node.dc1.consul:47670 (ESTABLISHED)\nconsul  51242 root  530u     IPv6          264998502      0t0       TCP non-prod-one-instance-consul-0.node.dc1.consul:fmtp->non-prod-two-instance-nomad-client-0.node.dc1.consul:47674 (ESTABLISHED)\nconsul  51242 root  532u     IPv6          264971322      0t0       TCP non-prod-one-instance-consul-0.node.dc1.consul:fmtp->non-prod-two-instance-nomad-client-0.node.dc1.consul:41378 (ESTABLISHED)\nconsul  51242 root  533u     IPv6          264998503      0t0       TCP non-prod-one-instance-consul-0.node.dc1.consul:fmtp->non-prod-two-instance-nomad-client-0.node.dc1.consul:47676 (ESTABLISHED)\nconsul  51242 root  535u     IPv6          264988129      0t0       TCP non-prod-one-instance-consul-0.node.dc1.consul:fmtp->non-prod-two-instance-nomad-client-0.node.dc1.consul:52014 (ESTABLISHED)\nconsul  51242 root  536u     IPv6          264998504      0t0       TCP non-prod-one-instance-consul-0.node.dc1.consul:fmtp->non-prod-two-instance-nomad-client-0.node.dc1.consul:47672 (ESTABLISHED)\nconsul  51242 root  537u     IPv6          264998505      0t0       TCP non-prod-one-instance-consul-0.node.dc1.consul:fmtp->non-prod-two-instance-nomad-client-0.node.dc1.consul:47668 (ESTABLISHED)\nconsul  51242 root  538u     IPv6          264998507      0t0       TCP non-prod-one-instance-consul-0.node.dc1.consul:fmtp->non-prod-two-instance-nomad-client-0.node.dc1.consul:47678 (ESTABLISHED)\nconsul  51242 root  539u     IPv6          264998508      0t0       TCP non-prod-one-instance-consul-0.node.dc1.consul:fmtp->non-prod-two-instance-nomad-client-0.node.dc1.consul:47680 (ESTABLISHED)\nconsul  51242 root  540u     IPv6          264998510      0t0       TCP non-prod-one-instance-consul-0.node.dc1.consul:fmtp->non-prod-two-instance-nomad-client-0.node.dc1.consul:47686 (ESTABLISHED)\nI am temporarily resolving this by restarting the namerd container on a daily basis.\n1873 . For most cases, the current environment is only generating sporadic traffic. Namerd can just sit there with connections from linkerd and create all of the extra connections.\nFor my environment, just starting up namerd consistently creates the connections. If there are certain captures or steps you would like me to do in the environment, I can. It's a non production environment.\n. Above mentioned pull request has resolved the issue. Look forward to the full release. @wmorgan  I can definitely try it. Do you have a docker container that I can pull?. @adleong Were you able to build a docker container that I can test?\nThanks. Great.  I'll start it up.  Should be able to have solid results in 24 hrs.\nOn Wed, Mar 28, 2018, 11:18 AM Alex Leong notifications@github.com wrote:\n\n@cchatfield https://github.com/cchatfield I've pushed\nbuoyantio/namerd:consul_observations_leak and\nbuoyantio/linkerd:consul_observations_leak\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/linkerd/linkerd/pull/1877#issuecomment-376965880, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAiguem8jxuihKd8t_4yeU-LmxUJYAmuks5ti8XZgaJpZM4S8KVv\n.\n. 7 hours and constant at 16 connections. Seems like a solid fix.. Connections are still stable with a total of 17. This fix works for me. Thanks for figuring it out.. \n",
    "sozuuuuu": "I want to reopen this issue because linkerd ingress controller class is configurable in code but not exposed to configuration file.\nlinkerd slack channel conversation:\n- https://linkerd.slack.com/archives/C0JV5E7BR/p1500870468909826\n- https://linkerd.slack.com/archives/C0JV5E7BR/p1500918310662653. ",
    "pcalcado": "@sozuuuuu could you look at #1584 and let me know what you think of it? Maybe this is the most relevant file: \nhttps://github.com/linkerd/linkerd/blob/221d343a6d24cc82299e0ba018b627fd42540731/linkerd/docs/protocol-http.md#ingress-identifier. Fixed on  #1584 . Testing has shown that this initial approach won't work as we dont have an istioPath set at that stage yet. I'm working on an alternative approach.. This should be merge-able, waiting for the final \ud83d\udc4d  to merge . @hawkw DO you think 3a843ebb2b25f56a923d841ca99c1b7250cade89 addresses those comments? Caveat: I haven't spent any time on interpretee or namer so I am not sure if these changes make sense, especially around the initialising config files.. @hawkw I was thinking \"how the hell is this working on my box...\" and then realised I hadn't reloaded SBT -_-\nShould be fixed now, including the initializers. Some prior art here: https://github.com/linkerd/linkerd/issues/1410#issuecomment-309193732. @adleong cool! In that case, I'd recommend we also review/merge and test #1606 , as the only difference between that PR and this one is this commit: https://github.com/linkerd/linkerd/pull/1606/commits/b9dd7dfd1f2cd4853c1ba53f39bb90437911ad4e\n. @yangzhares thanks! Upon investigation, this looks like a different issue. I've opened #1766 to track it and would need some info about your setup to help me reproduce it here.. From @bourquep at the Discourse thread:\n\nI believe they are the same issue, or at least have the same root cause. What they have in common:\n\nThe io.l5d.k8s namer stops being able to resolve a k8s service/endpoint.\nThe problem is intermittent, with no obvious trigger (although the OP of the #1730 issue mentions that deploying a new version of a service sometimes triggers the bug, on my end it starts occurring out of the blue - nothing was updated on my test cluster during the Holidays and the problem manifested itself).\nWe both see k8s resource too old in the linkerd logs, accompanied by k8s restarting watch messages.\nWe both see context canceled errors in the kubectl container logs from the l5d pods\nRestarting l5d (killing the pods and letting the DaemonSet recreate them) fixes the issue.\n. @yennifehrrr \nThanks! We're waiting for the heap dump, but I think that on (3) @adleong was suggesting that you have a small app that reproduces the issue that we can run in our test lab. Maybe if you stip out all of your business logic from this particular applications and make them just blindly passthrough data?. Thanks for the patch, @fantayeneh! While we review it,  would you ming adding a DCO signature to this commit?\n\n\nIn practical terms, you should only have to do:\nshell\n$ git commit --amend -s -a --no-edit\nAnd the force push the branch. This will add a signature to the commit message. For more info you can check this doc:\nhttps://github.com/linkerd/linkerd/blob/master/CONTRIBUTING.md#certificate-of-origin. Indeed. I'm happy with keeping the name, but something for consideration: Because there are so many types called things like Request, Response, Logger, and Service in the classpath, at SoundCloud we had a convention that every time we imported one of those we'd rename it so that we could identify at a glance where it came from. e.g. we'd always do\nscala\nimport com.twitter.finagle.http.{Request => FinagleRequest, Response => FinagleResponse}\nimport com.soundcloud.ourinternalstuff.http.{Request => ScRequest, Response => ScResponse}\nBut if we don't follow anything like this convention I'm happy to simplify.. Uh... good catch. I'll add the test.. My understanding is that the way this should work is:\n\nIf there is only one ingress resource, we use it\nIf there are more than one ingress resources, we only use the one annotated with 'linkerd' (or whatever we configure it to be)\n\nDo you think that on (2) we should also add and any other ingress resources without annotations?. Yeah, I've flipped these by mistake :P. Aha! Now I understand what you meant earlier, thanks! I've changed it to match specific routes and to be more liberal on the shared one, given which Controller would be picked is undefined. wdyt?. @adleong In the original code, this would send the values of these things as part of the dictionary. My understanding is that the dictionary should only contain the keys to be resolved by Mixer. Is this intentional?. Should we consider the case where no default backend exists?. I think I might be missing something, but is this basically the same as Iterator#find()?. Oh yeah, great point. I didn't want to change too much, hence the k8s->istio dep, but this looks like an easy one and maybe we'll be able to break it completely in one go.. This makes sense. Because this branch is a dependency for the mixer work I'll make an issue for it and tackle once that work is done. This was copied verbatim from https://github.com/googleapis/googleapis/blob/master/google/rpc/README.md#google-rpc so I think we should keep it but... it had actually never occurred to me that the g stood for \"Google\", I always thought it was something like \"generalised\" :P. Thanks so much, Zack!\n\nUhm.. the comment was about mixerclient, I probably misread something on the code. This iteration doesn't have a cache yet, but I will make sure to follow the same logic when we add it \ud83d\udc4d \nThis is brilliant, thank you. As I understand it, our first integration code was tested against an early alpha release and as everything was moving so fast we decided to send a lot of stuff just to make sure we didn't miss on anything that would become important or mandatory. I'll change it to the list you provided!\nOur current integration runs against an older schema that didn't have the negative index logic. I'll update the schemas as we move our integration to 0.2.x. Intellij being silly with protobuf imports -- fixed. Interesting, the idea behind the use of an interface was to increase ergonomics. My thinking here is that the usual pattern of implementing a fake Service[Req,Resp] on tests have a few drawbacks:\n1) I think that it leaks a lot of \"infrastructure\" types into higher level tests. These tests shouldn't have any reason to even know about Finagle or any other piece of infrastructure\n2) Maybe more important, I also think that it also creates coupling between the consumer of a type and its implementation. For example, consider https://github.com/linkerd/linkerd/pull/1650/files#diff-bcd0445a689106c21b39b7304f665801R36 . If we follow the usual pattern of implementing a fake Service[Req,Resp] there then the test for InternalTrafficIdentifier would make assumptions about the API used by ClusterCacheBackedByApi. If the next version of the API used by this class changes the URL or any other implementation detail we would have to change not only the code and tests for ClusterCacheBackedByApi but also the tests for InternalTrafficIdentifier. This feels like accidental coupling.\n3) I am not sure if I agree that this would exercise the codebase more, at least in any useful manner. One rule of thumb that I like to follow when it comes to testing is \"don't mock what you don't own\"_, and in this case I think that we might be trying to use unit tests for what should be integration tests. Instead, I would suggest that we keep as much as possible of our own logic independent from these dependencies (which drives back to the 1st point) and use integration tests for those instead of mocking APIs that aren't under our control.\n4) The concept of ergonomics or complex code that we are discussing here are subjective, but I have the impression that one of the reasons for what I perceive to be low test coverage around these boundary classes is because it is actually a lot of work to have those fake Service[Req,Resp], so much so that we tend to have a single \"fixture\" reused across multiple tests, which requires tests to use special requests to know which part of the fixture will be hit.\n\nBut with all that said, Linkerd is an existing code base and I am happy to comply with the existing testing strategy if we collectively think that's the way to go.. I tend to prefer more descriptive names, but I am happy with the suggestion. The only thing I'd suggest is that we keep the same naming pattern around things, do we usually go with Qualifier + Thing, as in Api ClusterCache or do we go the other way around? Looking around the code, it seems like so far we have relied on packages to namespace different implementations but keep the same name, should I do this here?. :P I actually find the cake pattern super cute, but had way too many problems debugging badly wired software \ud83d\ude24. Hm.. that's a good point. I think this had some template pattern logic at first but was gradually removed. Let me explore it, brb. Done!. You are absolutely right, and I had felt some of this already. Unfortunately this branch didn't pick the changes, I've added them: https://github.com/linkerd/linkerd/pull/1650/files#diff-042aed18959e3a7caabf44f4294034ef\nI am still not happy with CurrentIstioPath, but I ran out of ideas here.. Yup, fixed with the same merge as the one above. Fixed on the same as above. Waiting for resolution on similar comment. Fixed. Interestingly enough, this seems to be a default on IntelliJ. Maybe we should upload a common IntelliJ formatting configuration file?. Yup, this was fixed when we moved it to where Logger  was. Current Identifier has no DstBoundCtx: https://github.com/linkerd/linkerd/pull/1650/files#diff-64056934f2ceb2827c6a08c5026e0ddeR20. Can you let me know what you think of: https://github.com/linkerd/linkerd/pull/1650/files#diff-0a76be1349b81581e237715901125ebcR35 ?\nI don't have a particular preference\u2014I usually go with the Editor's default.n Like I mentioned before, it would probably be a good idea if we all shared a single formatter configuration file.. This one is similar to the one above, but different in one important way. As I mentioned there, I don't really have a personal preference, all I would lobby for is for us to share a single formatter configuration\u2014my general philosophy is that I usually don't care about these things as long as there's one way to do it and we all follow, whatever this way might be.\nAll that said, my editor right now wraps at 120 columns and the Effective Scala guide mentions 100  columns as the standard: \n\nIndent by two spaces. Try to avoid lines greater than 100 columns in length. Use one blank line between method, class, and object definitions.\n\nWhat rule should we follow, wrap at 100, always wrap...?\n. Yup, the reason is that I am still not used to the new mac's keyboard and keep fat fingering everything -_-\nFixed it, thanks :)\nOn the second comment, I think that most of this stuff is only relevant in a Mixer API context, and I would not like to create any coupling between the request object and Mixer. What did you have in mind?\n. I think the current version might be a little bit better? https://github.com/linkerd/linkerd/pull/1650/files#diff-00e114bad2c9fdd832ebe5854d4a1a4fR50. I looked at this for a bit and couldn't think of anything, due to attributes: Seq[IstioAttribute[_]]. Did I miss something?. It's definitely fire-and-forget, and because nobody cares about when it's completed I decided to just return a constant. On the return type, I generally like methods from a single type to be symmetric, i.e. given a set A it always map from A to S[A], but no strong preference. . Fixed, thanks!. Fixed, thanks!. On the chaining, I am not sure I understand: each option can be present or not, and if not we're using the default value. How would we deal with the default values on a flatMap chain?. Done!. Aye. \ud83d\ude3b . Absolutely agree, this is a leftover from the previous shape of this type. Let me fix it.. I mean that I would expect all functions in a given type/namespace to return values of the same container, or at least the same kind. In this particular case, it means that I would expect the published interface of MixerClient to be either this:\nscala\nclass MixerClient(client: Mixer) {\n  def report(/* ... */): Future[Unit]\n  def checkPreconditions(/* ... */): Future[MixerCheckStatus]\n}\nOr:\nscala\nclass MixerClient(client: Mixer) {\n  def report(/* ... */): Unit\n  def checkPreconditions(/* ... */): MixerCheckStatus\n}\nBut not:\nscala\nclass MixerClient(client: Mixer) {\n  def report(/* ... */): Unit\n  def checkPreconditions(/* ... */): Future[MixerCheckStatus]\n}\nBut, again, this is just a preference, I'm happy to make it Unit if it makes it more aligned with our internal philosophy.\n. Done!. Oh, and on the release() comment, do we have to manually release each stream item in our gRPC implementation? The current implementation seems to be a noop and I couldn't find an example of something else consuming gRPC like this, so I'm not super sure what's the usual idiom. . I've made the changes @adleong mentioned. @hawkw that's a valid concern and interesting solution, but I would argue that we should rather get through with istio 0.1.x features and address performance on our 0.2 work. Istio 0.1.x doesn't focus on performance as much and I find it unlikely that this would be an actual bottleneck as opposed to, for e.g. the lack of caching in this release.. Me too, will fix it. I think Intellij has a setting for this somewhere?. This is pretty much a panic situation so I'm not sure if it matters, but I will add the =. Will do. How are we testing this class and its initialisers below?. ",
    "d10i": "Seeing the exact same. Thanks @hawkw, great work!. I understand there are ways around this so that this transformer it's not essential for deployments on ECS, but it would be a nice-to-have. I had to do a similar hack in order to get it working on AWS ECS and I would have liked this transformer to exist in order to make my life a bit easier and not having to go through that mess.. It's in this very repo @kevholditch, see https://github.com/linkerd/linkerd/blob/master/linkerd/docs/config.md. I'm also seeing this on both 1.1.2 and 1.2.1. Tried to reproduce but haven't been able to.... Yeah I know, just wanted to point out https://github.com/linkerd/linkerd/pull/1609 definitely didn't fix the problem as it's still happening on 1.2.1.\nIt seems to only happen when we fire > 5 requests in parallel to the same service and same endpoint (with different payload). We get HTTP 502 with This writer's reader has been discarded in the response body. Only one of the requests that are fired in parallel fails and it doesn't happen all the time (happened 12 times out of 68 in the past 48 hours). I suspect race condition...\nIf I have time I'll try to setup a docker-compose environment that reproduces the issue.. Hi @adleong and @hawkw, I finally managed to create a project to reproduce this issue. You can find it here: https://github.com/Attest/writers-reader-discarded. The README explains how it works and how to run it but let me know if you need help!. FYI I've just tried with linkerd 1.3.2 and it doesn't fail. It must have been fixed as a side effect of another change?. 1.3.0 fixes it too and by looking at https://github.com/linkerd/linkerd/compare/1.2.1...1.3.0 I think it's the upgrade to finagle 7 that might have fixed it?. ",
    "ryadav88": "@siggy ^^ . ",
    "ashahan": "Hi @siggy, on the same team as @daftclouds. Following the documentation you linked, the installation and functionality do work with enterprise DC/OS clusters that are in permissive or disabled security modes. And in fact you can install linkerd in a strict Mesosphere Enterprise DC/OS 1.9 cluster - but linkerd cannot talk to marathon in strict mode no matter how I've tried to configure linkerd.\nFrom what I understand about how the linkerd package works with the optional config file (default optional config in the code block at bottom) is that it forces all connections to use adminrouter which would be port 443 instead of 80 in a strict cluster. So I changed the marathon-port to 443, still linkerd would not work. I attempted to use port 8443 which is the marathon port in strict mode, but also that fails. I even tried 8443 with just the marathon-uri-prefix of / to no avail. The point here is that marathon uses https in strict mode but I am not sure if linkerd is set up to work with https in this context.\nI looked at all the normal logging locations and potential debugging steps (including the ones in the linkerd UI help page) but was unable to get any information about why the connections were failing with the log level details set to all. Any assistance would be useful here - additional logging locations to check, or point me to some other configurations I can test to make this work.\nWhat it looks like when I run linkerd in a permissive Enterprise DC/OS Cluster\n\nWhat it looks like when I run linkerd in a strict Enterprise DC/OS Cluster with port 8443\n\nWhat it looks like when I run linkerd in a strict Enterprise DC/OS Cluster with port 443\n\nDefault optional config file\nOPTIONS=$(cat <<EOF\n{\n  \"linkerd\": {\n    \"instances\": <node count>,\n    \"marathon-host\": \"leader.mesos\",\n    \"marathon-port\": 80,\n    \"marathon-uri-prefix\": \"/marathon\",\n    \"secret_name\": \"linkerd-secret\"\n  }\n}\nEOF\n)\n. ",
    "vadimi": "We've been experiencing this issue with linkerd and DCOS in strict mode and figured this is happening due to self signed certificates. Linkerd connects to marathon over https, but cannot validate certificates DCOS generates. I even tried curling marathon endpoints from the machine (and container) directly and it fails on ssl verification step. Basically linkerd cannot even perform authentication.. Yes, I use token identifier. Should I use path instead?\nHere is my config:\nadmin:\n  port: 9990\nnamers:\n- kind: io.l5d.marathon\n  host: leader.mesos\n  port: 443\n  uriPrefix: \"/marathon\"\n  prefix: \"/io.l5d.marathon\"\n  tls:\n    commonName: master.mesos\n    trustCerts:\n    - \"/mnt/mesos/sandbox/.ssl/ca.crt\"\nrouters:\n- protocol: h2\n  experimental: true\n  identifier:\n    kind: io.l5d.header.token\n  dtab: >-\n    /ph=>/$/io.buoyant.rinet;\n    /srv=>/$/io.buoyant.porthostPfx/ph;\n    /svc=>/srv;/marathonId=>/#/io.l5d.marathon\n        /svc=>/$/io.buoyant.http.domainToPathPfx/marathonId\n  servers:\n  - port: 4140\n    ip: 0.0.0.0. So I tried to use path identifier with gRPC clients, but I wasn't able to override :path header to add additional elements.\nIt appears to me that there is something wrong with the encoding of the service name and linkerd cannot resolve it by that name:\n/svc/\\6d\\x79\\x67\\x72\\x6f\\x75\\x70\\x2f\\x6d\\x79\\x2d\\x73\\x65\\x72\\x76\\x69\\x63\\x65 \nthis looks like some sort of hex encoded string. Does linkerd do encoding/decoding of services names?. Actually this configuration did the trick:\nidentifier:\n    kind: io.l5d.header.path\n    header: :authority\nThanks for your help. We run linkerd in DC/OS in production without namerd. But I was able to reproduce it in my local dev environment by just running linkerd docker container. Here is the config I used:\n```\nadmin:\n  port: 9990\nrouters:\n- protocol: h2\n  experimental: true\n  identifier:\n    kind: io.l5d.header.token\n  dtab: |\n    /ph        => /$/io.buoyant.rinet ;\n    /svc       => /ph/80 ;\n    /srv       => /$/io.buoyant.porthostPfx/ph ;\n    /svc       => /srv ;\n  servers:\n  - port: 4140\n    ip: 0.0.0.0\n```\nand docker run command:\ndocker run --rm --name linkerd -p 9990:9990 -p 4140:4140 --network=host -v `pwd`/config.yaml:/config.yaml buoyantio/linkerd:1.1.3 -log.level=DEBUG /config.yaml\nIt's pretty easy to reproduce the issue - just start sending lots of messages into gRPC stream.. Just tried 1.3.2 in DCOS and this issue is not reproducible. But I can reliably reproduce it in 1.3.3. I tried to reproduce it in my local cluster, but unfortunately no luck. But I can steadily reproduce it two AWS DCOS clusters, one is 1.9, another is 1.10.\nConsidering the issue started happening between 1.3.2 and 1.3.3, I tried to find a specific commit. Here is what I've got. I've build two linkerd docker images, one from commit https://github.com/linkerd/linkerd/commit/c6f0d2eaeecca80c60314e6f6cb852a31870877a, another from commit https://github.com/linkerd/linkerd/commit/0bd8a91ed51fecc34b86f110382e2077a7b88600. The issue is happening with the first one, but not the second one. It looks like https://github.com/linkerd/linkerd/commit/c6f0d2eaeecca80c60314e6f6cb852a31870877a contributes to the issue somehow.. @wmorgan I was able to troubleshoot it a bit further. I added some logs to gRPC client to get http2 framing info:\n2017/12/13 15:24:23 [FrameHeader HEADERS flags=END_HEADERS|PRIORITY stream=59 len=10]\n2017/12/13 15:24:23 [FrameHeader DATA stream=59 len=16384]\n2017/12/13 15:24:23 [FrameHeader DATA stream=59 len=16384]\n2017/12/13 15:24:23 [FrameHeader DATA stream=59 len=16384]\n2017/12/13 15:24:23 [FrameHeader DATA stream=59 len=1761]\n2017/12/13 15:24:23 [FrameHeader HEADERS flags=END_STREAM|END_HEADERS|PRIORITY stream=59 len=8]\n2017/12/13 15:24:23 [FrameHeader HEADERS flags=END_HEADERS|PRIORITY stream=61 len=10]\n2017/12/13 15:24:23 [FrameHeader DATA stream=61 len=16384]\n2017/12/13 15:24:23 rpc error: code = ResourceExhausted desc = grpc: received message larger than max (845559858 vs. 4194304)\nI call the same endpoint with the same parameters several times (unary call). Stream 59 is a successful one. Stream 61 is when the error happens. Basically when it reads http2 data frame it should be length-prefixed (at least this is what gRPC expects), so gRPC reads first several bytes to get the length of the message, but this message from stream 61 doesn't have length header, so gRPC reads a few bytes of the message instead and throws an error.\nAre there any logs I can get you from linkerd? I still cannot reproduce the issue locally.. thanks @hawkw, I just tried this docker image, but unfortunately it's still failing with the same error.\nI made another attempt - modified your code a bit to call f.content.release() after updateWindow(sz), built my own image and it worked, no errors since. This is basically what I've got (sorry, I'm not a scala guy, I'm sure there is a better way to do this):\nscala\ndef apply(f: Http2DataFrame, updateWindow: Int => Future[Unit]): Frame.Data = {\n    val sz = f.content.readableBytes + f.padding\n    val buf = ByteBufAsBuf(f.content.retain())\n    val releaser: () => Future[Unit] = {\n      if (sz > 0) () => {\n        val res = updateWindow(sz)\n        f.content.release()\n        res\n      }\n      else () => {\n        f.content.release()\n        Future.Unit\n      }\n    }\n    Frame.Data(buf, f.isEndStream, releaser)\n}\n. thanks @hawkw, since it got merged into your local branch (not master) do you need me to do anything else with it?. @hawkw, no, that's fine, I'm already pretty happy that the issue got finally fixed, it was driving me crazy. :). I tested GOAWAY sending a bit more and it looks like linkerd doesn't send it at all to clients. Here are a couple of tests I did:\n\nGracefully terminated gRPC server which sends GOAWAY frame to all clients. Without linkerd it works fine, with linkerd clients never receive it.\nConfigured grpc.max_connection_idle_ms on gRPC server side which terminates connection after some period of time and sends GOAWAY frame to all clients. Without linkerd all clients receive GOAWAY , with linkerd they don't.\n\nBasically without GOAWAY frame clients don't know they got disconnected.\nAnd another test - when I gracefully terminate linkerd it seems all clients receive GOAWAY.. thanks for checking @dadjeibaah. The thing is that behavior is different from what gRPC expects - https://github.com/grpc/proposal/blob/master/A9-server-side-conn-mgt.md (please see MAX_CONNECTION_IDLE section).\nBasically if I route requests through linkerd my grpc clients don't know that grpc server sent them GOAWAY and they would never try to reconnect. This is useful in case I want clients to reconnect to linkerd cluster if I add one more node. Another example is if I have AWS network load balancer in front of linkerd - it has 350 seconds idle timeout after which it terminates connections. If I would configure max idle timeout in linkerd to let's say 300 seconds it will force grpc clients to reconnect and mitigate connection termination (since linkerd doesn't have any \"keepalive\" type of functionality).\nIf I run grpc client and server without linkerd in the middle all scenarios work as expected. I also compared linkerd behavior with envoy and envoy does send GOAWAY in all cases.. Ok, thank you both for explanation.. ",
    "mtweten": "I was running into this same issue - I was able to make a pretty small change which allows the user to specify TLS config for the marathon namer which allows the namer to hit the TLS protected DC/OS endpoints.\nThe change makes use of the existing TlsClientConfig class, which seems to be a pretty good fit for this use case. The only thing that will go unused in this workflow from that class is the ClientAuth, since strict mode DC/OS doesn't use mutual TLS.\nHere's an example configuration:\nnamers:\n- kind: io.l5d.marathon\n  host: leader.mesos\n  port: 443\n  prefix: \"/io.l5d.marathon\"\n  uriPrefix: \"/marathon\"\n  tls:\n    disableValidation: false\n    commonName: master.mesos\n    trustCerts:\n      - /opt/linkerd/config/dcos-ca.crt\nwhere dcos-ca.crt is the self-signed root certificate for the DC/OS CA (as described in Securing communication with TLS and Downloading the Root Certificate)\nIf this is an acceptable fix I could go ahead and open a PR for this if you guys want.. ",
    "adriancole": "hmm is there an existing issue out to synchronize and/or link the linkerd trace header with another format like X-B3? I might have misspoken about compatibility, which is cool, but better to at least have a tracking issue to refer to.. Hmm so I suppose I might be oversimplifying, but what is actually preventing a trace join plugin? Like a bijection. For example, what would be the risk of continuing a user-created B3 trace and also B3'ing on the other side (even if l5 headers are also sent). For example, this is likely how folks I've talked to will support switching formats. Ex double-propagating. The cost would be a coordinated plugin, but at least B3 is extremely common and understood in finagle world.. yeah I think of these contexts as different, for example, a user ID context\nbeing different than a routing rule context being different than tracing\ncontext (although they are indeed inter-related). Happy to help think this\nthrough together, even pow-wow at the next tracing workshop\nhttps://docs.google.com/document/d/19SQOio1z7mHqb79MzLQCiKdkq5tdirOW8OCcTQNQdUg/edit#\n. cheers!\n. yep can do if no one else in a timeout :P. this has nothing to do with opentracing by the way. OpenTracing includes no library support for B3 unless something changed recently. census does however. B3 is defined here https://github.com/openzipkin/b3-propagation. ",
    "xiaoerlyl": "Hello, is there any plan about passing existing trace context through linkerd? \n  . @klingerf thanks for your reply. I want that linkerd to be a part of my entire end to end trace both for http and other protocol. For http, I just followed the approach from issue #1428 . and i further modified linkerd internal implementation to pass the trace info generated in linkerd to downstream server. \nHere is the modification in LinkerdHeaders.scala \njava\n def set(headers: HeaderMap, id: TraceId): Unit = {\n        val bytes = TraceId.serialize(id)\n        val b64 = Base64.getEncoder.encodeToString(bytes)\n        val _ = headers.set(Key, b64)\n        val map = headers.set(\"X-B3-SpanId\", id.spanId.toString)\n    }\n  . @klingerf  I also want this patch. two questions.\n1. when will this patch be released?\n2. Does this patch work for ttwiter protocol?\nthanks.. hello, I want to know how can i add service name to twitter thrift 'dest' request header for finagle thrift client? . @adleong  thanks. This really helps. . ",
    "jmalvarezf": "Hi!\nI tried to find this in the Roadmap but didn't. Is it still something that you will have in the medium term? We are planning to use linkerd but not being able to participate in an existing tracing context is a big drawback for us.\nThanks. Hi @adleong,\nYes, I think it makes total sense! \nI will rewrite this when you have the interface ready. Do you want me to close this? Or you prefer to leave it open? \nThank you! . I'll try to adapt this to the interface you propose during the following month when I have enough time and I'll let you know. Thank you!. You are right. This is not opentracing but zipkin. Ill close this and start working on the interface already merged.\nThanks. Hi @adleong,\nYes, that was the intention. If you think this is more confusing than useful, I can redo that part to only propagate via Zipkin headers.\nThank you,. I have also a commit that would address a problem with some tracing libraries that need all b3 headers (there are two missing when forwarding the request) so if you confirm that you don't want that behavior I would address both.\nThanks! . Hi @adleong,\nThanks for your comments!! I've added them all, along with the other corrections (no fallback behaviour, and b3 headers correction).\nBest regards,\nJose Maria.. Hi @adleong,\nThank you for your patience! I've edited the commits with the sign-off. Is that enough?\nThank you,. Ok, now it is properly done. \nThanks!. We discovered a bug in the flags/sampled headers. The code now includes the patch.. ",
    "MrTravisB": "Any chance this can get another look? As long as linkerd config contains router and telemetry info and namerd config contains namer info this will be a problem. Namerd can at least be solved with a rolling update. Linkerd DaemonSet however result in actual downtime during a rolling update.. I've recently run into this as well. @siggy example shows pretty much exactly what I'm trying to. With the current implementation I have to create a differently named service per version of the app.\nI actually don't even see the value in filtering services by labels since service names are unique and the path always contains a service name. Unless there is some old behavior of Kubernetes this worked with that I'm not aware of this seems like a bug to me. Ideally the labelSelector option would work just like Consul tags and filter pods that the service points to. . Ahh! So this is a shortcoming of the Kubernetes API. Seems the only current way around this would be watch all of the pods returned in the endpoints which doesn't seem like a good idea. If the endpoints watcher returned pod labels that would be best case scenario.. ",
    "kevholditch": "If you see my blog post that is how I've done it except using envsubst.  William tweeted saying it sounded like a reasonable request to have it built in natively.\nAt the moment the only solution when needing env vars in your config is to derive from the official container and do some config manipulation yourself which is a bit cumbersome. . @adleong thanks! The scenario is you are running linkerd on a docker container on AWS ECS.  The linkerd docker container gets its own IP address different from the host EC2 instance.  You have a consul container also running on the same host and a container with your service in.  Your service will be registered in consul with the IP address of the EC2 host.  The ec2Host filter allows you to filter only services running in containers on the same machine in an automated way.\nUnfortunately none of the other filters fit the bill as we don't know the hostname or IP of the EC2 host until runtime.  I came up with a workaround for now as detailed on my blog.. @adleong thanks for getting back to me. Yep the shortcomings of this solution is that currently to filter to a specific host when running on AWS ECS you have to jump through the hoops mentioned above. The solution may be ok but every person who wants to run linkerd on AWS ECS and filter on the host is going to run into this problem and have to work out a solution (like I did). It also means that it is necessary to create your own docker container (even if the official one included gettext) as you have to template the linkerd config before starting Linkerd. \n@dario-simonetti (the author) of the specific host filter ran into the same issue and had to solve it using a similar technique. \n. @adleong another way of approaching this would be to build in environment variable substition into linkerd itself. Linkerd could parse its config file and replace environment variables before it parses the config file. I have raised an issue for this. @klingerf thanks. The problem is when running on AWS ECS this means having the linkerd config file on each ec2 ECS host machine that is running the containers. This means that to change the config you have to coordinate updating the config on every ec2 host and then restart the containers. Due to this constraint practically you have to run your own linkerd container. So when time comes you want to make a change you can just roll out the new container using AWS ECS by updating the task and not having somehow change files on all of the host machines . @adleong Hey Alex, I've made the changes to the filter to use an activity so it no longer blocks waiting for the IP address back from the AWS meta endpoint.. @adleong where is your documentation repo?  I'd like to update it if possible to include my change as part of the PR. hey @adleong, that's ok timezones are tough as you said.\nYou are correct with everything you say.  In that we would have to run our own custom container anyway as it is tough to manage a config file on the ECS host itself.  \nThe workaround using envsubst works but feels quite awkward.  It puts up an obstacle that both I and @dario-simonetti encountered (and probably many more people) when trying to run linkerd on a container on ECS. As my envsubst solution is included in this PR I think it seems quite obvious but it took us a few days to come up with it and during that time we actually was starting to think it wasn't possible to run linkerd on a container on ECS using a local filter (which we needed to for our configuration).  I'm sure other people will also come up against this issue and may come to the conclusion it isn't possible causing them potentially to look at service mesh alternatives (as we nearly did).\nWhen we can write a solution that makes linkerd play nicely with ECS I'm not sure why we wouldn't include that?  It would give ECS users a much better experience and would save them all having to come up with the same workaround as I did.\n. @adleong thanks.  I agree with both of those points.  We could take it one stage further and as well as building in gettext build in envsubst too.  So we could allow the user of the container to pass in the config as an environment variable and we could have a flag in the container that causes it to run env substitution over the config. e.g. a sample entrypoint.sh would be:\n```\noptionally allow passing of config via env variable\nif [ -n \"$LINKERD_CONFIG\" ]; then\n    echo \"$LINKERD_CONFIG\" > \"/opt/linkerd/conf/linkerd.conf\"\nfi\noptionally replace env vars in config file\nif [ \"$ENV_SUBST\" = \"true\" ]; then\n    envsubst < /opt/linkerd/conf/linkerd.conf > /opt/linkerd/conf/linkerd.conf\nfi\n./bundle-exec /opt/linkerd/conf/linkerd.conf \n```\nThis would make the container a lot more flexible.  In our case we would still have to derive our own container to set the ip variable but I think a lot of people would find the above useful.\nI'm happy to help with this in any way I can.\n. @siggy no worries.  Understand your decision to cover this with examples.  Have looked through your example and looks good to me.. The AWS API always lives at this address.  It is a special endpoint that you can hit on any AWS EC2 box.  I don't believe this is possible to change so I'm not sure it makes sense to make this configurable?\nFor more information on the AWS meta endpoint see here.. I'm going to try and attempt to do this... (wish me luck :) ). It is always on port 80 so I do not see why we should make this configurable?  It adds more configuration overhead, surely it is better to favour convention over configuration?  This filter is specifically for use on AWS EC2 instances.. ",
    "rickardrosen": "@wmorgan thanks!\nIf I understand things I should be able to fire up a few docker instances of namerd with consul as a backend and these containers should essentially be stateless?\n. ",
    "mnnit-geek": "Yes in fact I'm running an infinite while loop to push the data to my simple python web-server. There's nothing in LinkerD server logs. @adleong Thanks for reply. Actually, I have pretty much followed your talk to do all of my setup. And I have waited for more than half-an-hour to make sure there's enough time for linkerD to detect the change, however, unfortunately it doesn't detect the changes at all. But if I restart the server it start redirecting traffic to fourth node as well.\nAny idea what could be wrong here?. can you suggest any checks/verifications that will help single out the possible cause of the problem?. Thanks for swift response. Consul is our ultimate requirement in terms of the actual architectural implement. I have been reading on different blogs how to integrate LinkerD and Consul, but things are not very clear to me just yet. By any chance your'e aware about any good blogs or articles that can help me achieve the integration relatively easily?. Well, A few of my colleagues also tried and faced the same problem. Can you suggest a way we can deduce what's going on?. ",
    "jyothidat": "@esbie  Thank you - Here are some results.\n1 On minikube (linkerd.yml + hello-world-legacy.yml) does work and we can curl hello and see hello-world communication.\n2. On Kubernetes v 1.6 running internally (non GCE, non AWS environment) we have CNI and as such\n(linkerd-cni.yml + hello-world-legacy.yml) works - implying we can curl hello and see hello-world communication. \n3. On Kubernetes v 1.6 running internally (non GCE, non AWS environment) we have CNI and as such\n(linkerd-cni.yml + hello-world) does not work - implying  we can curl hello and see hello-world communication failing.\nWhat are factors that we take into account to determine whether we need legacy way or non-legacy way to determine proxy hostname?\nThanks\nJyothi\n. ",
    "tjquinno": "@esbie It turns out that our k8s installation does not report spec.nodeName as l5d expects, and so the proxy configuration for services calling out to other services did not actually let the service talk to the proxy. We have worked around that, at least for the moment, and have seen s2s traffic working.. ",
    "SurajMenon": "@adleong : Thanks for the quick response. One more observation that I had was, the Accept header does not get honoured in both the cases (502 Bad Gateway &  400 Bad Request). I get a plain text response when I am expecting a json response. Let me know if this is not the expected behaviour. If it is, can this be also added as a part of https://github.com/linkerd/linkerd/issues/1466. ",
    "serhii-samoilenko": "Maybe will be useful for someone ended up here. You can bypass this limitation by throwing io.buoyant.linkerd.protocol.http.ErrorResponder.HttpResponseException\nScala Example:\n```\nimport com.twitter.finagle.buoyant.linkerd.Headers\nimport com.twitter.finagle.http.{Request, Status}\nimport com.twitter.util.Future\nimport io.buoyant.router.RoutingFactory\nimport io.buoyant.router.RoutingFactory.{RequestIdentification, UnidentifiedRequest}\nimport io.buoyant.linkerd.protocol.http.ErrorResponder\nclass AuthIdentifier extends RoutingFactory.Identifier[Request] {\ndef apply(req: Request): Future[RequestIdentification[Request]] = {\n    val auth = req.headerMap.get(\"Authorization\")\n    if (auth.isEmpty) {\n      val response = Headers.Err.respond(\"Unauthorized\", Status.fromCode(401))\n      throw ErrorResponder.HttpResponseException(response)\n    }\n    Future.value(new UnidentifiedRequestRequest)\n  }\n}\n```. ",
    "splittingred": "Seeing this as well, with no defined telemetry block:\nI 1019 04:23:46.479 UTC THREAD40 TraceId:e16323182cce719d: FailureAccrualFactory marking connection to \"zipkin-tracer\" as dead. Remote Address: Inet(localhost/127.0.0.1:1463,Map())\nNot sure of impact on performance due to this.. ",
    "stvndall": "I'm really looking forward to the mux streams :champagne: . ",
    "eduponte": "@adleong I managed to get a heapdump instead, hope it helps:\nOOM heapdump\n. @hawkw Not from the same heapdump, sorry, metrics weren't enabled at that point.. @hawkw We somehow changed our docker container to crash before reaching Java Heap Size OOM. We will need some time to create a controlled environment where we can get those metrics.. @hawkw In case it helps, we are hitting the following endpoints for healtch check purposes:\n{\n                    \"name\": \"SERVICE_4100_NAME\",\n                    \"value\": \"namerd-thrift\"\n                },\n                {\n                    \"name\": \"SERVICE_4180_CHECK_INTERVAL\",\n                    \"value\": \"15s\"\n                },\n                {\n                    \"name\": \"SERVICE_4100_CHECK_INTERVAL\",\n                    \"value\": \"15s\"\n                },\n                {\n                    \"name\": \"SERVICE_9991_NAME\",\n                    \"value\": \"namerd-admin\"\n                },\n                {\n                    \"name\": \"SERVICE_4180_CHECK_HTTP\",\n                    \"value\": \"/api/1/dtabs\"\n                },\n                {\n                    \"name\": \"SERVICE_4180_CHECK_TIMEOUT\",\n                    \"value\": \"2s\"\n                },\n                {\n                    \"name\": \"SERVICE_4100_CHECK_TCP\",\n                    \"value\": \"true\"\n                },\n                {\n                    \"name\": \"SERVICE_9991_CHECK_INTERVAL\",\n                    \"value\": \"15s\"\n                },\n                {\n                    \"name\": \"SERVICE_4180_NAME\",\n                    \"value\": \"namerd-http\"\n                },\n                {\n                    \"name\": \"SERVICE_9991_CHECK_HTTP\",\n                    \"value\": \"/\"\n                },\n                {\n                    \"name\": \"SERVICE_9991_CHECK_TIMEOUT\",\n                    \"value\": \"2s\"\n                },\n                {\n                    \"name\": \"SERVICE_4100_CHECK_TIMEOUT\",\n                    \"value\": \"2s\"\n                }\n            ]\n/api/1/dtabs could be the cause for the heap growing periodically, since we are experiencing problems even during low-traffic periods.. @hawkw @adleong Do you think https://github.com/linkerd/linkerd/pull/1579 might be fixing this issue as well, since changes are on JsonStreamParser?. @hawkw Upgrading version didn't seem to solve the problem, but switching to thrift from http as linkerd-namerd communication protocol did. Does that make sense?. @hawkw Sorry, I didn't notice you were referring to a snapshot, we are currently using 1.2.1 version.. ",
    "Taik": "Hey guys, thanks for the quick turnaround! Any ETA when this will be released?. @hawkw testing the above tag out (buoyantio/linkerd:1.2.0.fix4-SNAPSHOT) and we're seeing a few of these warnings; not sure if its benign:\nl5d-xcw7g l5d W 0914 13:21:49.739 UTC THREAD35 TraceId:5062f359f2920fbc: k8s ns prod service conversation-srv endpoints watch error Status(Some(Status),Some(v1),Some(ObjectMeta(None,None,None,None,None,None,None,None,None,None,None)),Some(Failure),Some(too old resource version: 77004884 (77815101)),Some(Gone),None,Some(410))\nl5d-9hxgb l5d W 0914 13:21:58.461 UTC THREAD41 TraceId:aa44c3d8f2b20d20: k8s ns prod service guest-srv endpoints watch error Status(Some(Status),Some(v1),Some(ObjectMeta(None,None,None,None,None,None,None,None,None,None,None)),Some(Failure),Some(too old resource version: 77007165 (77815142)),Some(Gone),None,Some(410))\nl5d-1kfz8 l5d W 0914 13:22:47.102 UTC THREAD37 TraceId:bbff0e2e41bd128a: k8s ns prod service shift-srv endpoints watch error Status(Some(Status),Some(v1),Some(ObjectMeta(None,None,None,None,None,None,None,None,None,None,None)),Some(Failure),Some(too old resource version: 77008615 (77815359)),Some(Gone),None,Some(410))\n...\nl5d-xcw7g l5d I 0914 13:23:04.961 UTC THREAD10: Reaping /svc/room-srv\nl5d-xcw7g l5d I 0914 13:23:05.037 UTC THREAD10: Reaping %/io.l5d.k8s.localnode/10.76.5.82/#/io.l5d.k8s.ns/http/room-srv\n...\nl5d-9hxgb l5d W 0914 13:23:19.801 UTC THREAD43 TraceId:e10a9374cc05bd70: k8s ns prod service table-srv endpoints watch error Status(Some(Status),Some(v1),Some(ObjectMeta(None,None,None,None,None,None,None,None,None,None,None)),Some(Failure),Some(too old resource version: 77008313 (77815500)),Some(Gone),None,Some(410))\nl5d-9hxgb l5d W 0914 13:23:26.171 UTC THREAD43 TraceId:cf2bf18db35c9133: k8s ns prod service conversation-srv endpoints watch error Status(Some(Status),Some(v1),Some(ObjectMeta(None,None,None,None,None,None,None,None,None,None,None)),Some(Failure),Some(too old resource version: 77004884 (77815528)),Some(Gone),None,Some(410))\nl5d-9hxgb l5d W 0914 13:23:49.836 UTC THREAD37 TraceId:79898d79a98e89a5: k8s ns prod service auth-srv endpoints watch error Status(Some(Status),Some(v1),Some(ObjectMeta(None,None,None,None,None,None,None,None,None,None,None)),Some(Failure),Some(too old resource version: 77004053 (77815642)),Some(Gone),None,Some(410))\nl5d-xcw7g l5d W 0914 13:23:53.073 UTC THREAD22: k8s ns prod service l5d endpoints watch error Status(Some(Status),Some(v1),Some(ObjectMeta(None,None,None,None,None,None,None,None,None,None,None)),Some(Failure),Some(too old resource version: 77783381 (77815657)),Some(Gone),None,Some(410))\nl5d-1kfz8 l5d W 0914 13:23:57.514 UTC THREAD25 TraceId:c0e8e62f015e1ab6: k8s ns prod service user-srv endpoints watch error Status(Some(Status),Some(v1),Some(ObjectMeta(None,None,None,None,None,None,None,None,None,None,None)),Some(Failure),Some(too old resource version: 77008477 (77815675)),Some(Gone),None,Some(410)). Thanks for linking to that issue @hawkw, that's helpful. It sounds like it; we'll run a test on the latest nighty and see if the issue still exists.. @kumudt there's no real workaround AFAIK; doing a rolling delete of linkerd pod does it for us, but its not ideal.. @hawkw we can no longer reproduce this issues in our synthetic tests (scaling up and down replica sets). Our kubernetes version is 1.7.3, linkerd 1.2.1, kubectl 1.6.2.. Following up on this, it might or might not be related. If it is not then I can create a new issue. I'm sporadically seeing the same error noted by @kumudt after doing deploys.\nHere's some interesting logs entries that I found: https://gist.github.com/Taik/4879143d6146bab6cfaed7dda00d6aa6\nIf I'm understanding the log message correctly, sometimes it marks the connection to kubectl as down (when fetching namespace data)? If that's the case then that would explain why only sometimes new pods don't get picked up instantly (and old pods removed). If kubectl loses its connection with the master, would that do it?. @hawkw some more logs snippets from our dev cluster: https://gist.github.com/Taik/4879143d6146bab6cfaed7dda00d6aa6#file-service-endpoints-watch-error-log\nFull log of the past few days here: https://gist.githubusercontent.com/Taik/87358bb9fa05b1fcaa3658e2fad9850d/raw/e7aa416f08a6bcd3ae17f87fa68b0d0e0c33a8a3/dev.log)\nIt looks like when there are watch errors (related to kubectl?), no pods gets updated. Let me know if you need more info.. @adleong that makes a lot of sense, thanks for clarifying! Do you have any suggestions as to how to mitigate the issue? We're running on GKE so pretty vanilla in terms of setup. With the sporadic network issues, it makes it really hard for us to get things out to production without any downtime.\nI do want to mention that the sidecar proxy (buouyantio/kubectl) we're using is the one that is suggested by one of the guides (1.6.2), and it seems like there are no newer versions here.. After reverting to 1.1.2 for the last 24 hours, I haven't seen any of the errors mentioned above, even after doing re-deploys. I'll let it bake for another few days and report back if anything crops up.. Just piggybacking this issue. We're seeing the same behavior after deploying linkerd/namerd 1.3.0 to our staging env. Thanks for looking into it @klingerf.. @klingerf Looks good for me so far. I'll let it bake in our staging for a bit to see how things go.\nThanks for the quick turnaround!. ",
    "b-hoyt": "@olix0r I haven't been able to brainstorm a scenario when it is good behavior yet. \nMaking the single surgical change (default inet resolver = not fixed) would solve my immediate problem, extra configurability would be a bonus, although it could also be too much rope.. Config does include the fs namer\nnamers:\n  - kind: io.l5d.fs\n    rootDir: /private/tmp/output/disco. Sorry for total lack of repro info here. We're still digging on our side. The fs namer is enabled but we aren't actually using it (rootDir is empty), I just mentioned it as it was the only thing that stood out in my mind around inotify, but then we ran another test and saw that inotify instances ++ when traffic transits linkerd, so wondering now if that is a factor. Sorry for lack of updates, its been slow to debug on our side as the minority of environments we saw this in intermittently have very restricted access and lack of tools like lsof.\nAnyway right now it looks like:\n 1. separate competing process on the same host is exhausting inotify watches - the directory it monitors has lots of stuff in it (legit use but happened to creep over the watch limit)\n 2. when inotify watches are exhausted and linkerd's fs namer is exercised (first request that might hit that namer traverses it), linkerd via filesystem.newwatchservice tries to acquire an inotify instance to then set a watch on it\n 3. that fails because of watch exhaustion. the instance is retained by the process though\n 4. something happens to retrigger #2-3, inotify instance--, repeat until inotify instances are also exhausted\nNot clear this is a linkerd bug around usage of watchservice or something a bit lower level.\n. ",
    "edio": "While looking through sources, I found, that it is possible to effectively fix the issue just by setting org.apache.thrift.readLength environment variable\nSomething like\nexport JVM_OPTIONS=\"$JVM_OPTIONS -Dorg.apache.thrift.readLength=$RD_LENGTH_BYTES\". Exemplary request from bug report crashes namerd with this commit.\nI still do not understand linkerd/finagle well, but the issue might be in code generated by Scrooge.\nGenerated classes use special subclass of TBinaryProtocol \u2014 com.twitter.finagle.thrift.Protocols.TFinagleBinaryProtocol. Instances of this subclass created directly, without use of TBinaryProtocol.Factory and thus readLimit is never set, but is read from environment variable by default.\nI will try to dig into that more as soon as I have time (unless you fix it before that time ;) ).. @adleong Just one thing I wanted to ask. I made this PR at home, where I had no access to metrics.\nNow I checked, and I see that the largest request payload ever recorded in our environment is 166 bytes (I'm talking about interface/io.l5d.thriftNameInterpreter/thrift/request_payload_bytes.max metric)\nProbably 10MiB is way too much. Do you have any info, on how large normal request may be?. @olix0r I've tested it with the request from bug-report - no OOMs.\nIf you asked about any regressions: we limited read line to 4K on our dev environment.\nSo far I can only say, that namerds are operational. If we see no issues after couple of days (and tbh I don't expect any), we'll roll-out that further.. @adleong done. Although it's benign, it produces a lot of noise, over 6500 messages in one hour of uptime of a single instance for us.. I'm sorry, but I think we need to reopen this one.\nWe are on 1.3.1, but we still see these warnings. Overnight each of our instances generated ~40k warnings.\ncurl localhost:9001/admin/server_info\n{\n  \"build_branch_name\" : \"unknown\",\n  \"name\" : \"namerd-main\",\n  \"start_time\" : \"Thu Nov 16 12:35:44 EST 2017\",\n  \"build_revision\" : \"fba06b305b28dca17fb1ae37be14774c70db98d3\",\n  \"build\" : \"20171024-164313\",\n  \"version\" : \"1.3.1\",\n  \"build_last_few_commits\" : [\n    \"unknown\"\n  ],\n  \"scm_repository\" : \"unknown\",\n  \"merge_base\" : \"unknown\",\n  \"uptime\" : 1590906,\n  \"merge_base_commit_date\" : \"unknown\"\n}. @hawkw \nnamerd 1.3.2\nW 1207 04:58:59.418 UTC THREAD26: consul datacenter '********' service '********' observation error Failure(service observation released, flags=0x02) with RemoteInfo -> Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /127.0.0.1:8500, Downstream label: client, Trace Id: 6fff57cbfb47d850.6fff57cbfb47d850<:6fff57cbfb47d850 with Service -> client, falling back to last good state\nWe also have similar messages in linkerd log, but these are very rare (several per day, linkerd talks only to namerd via consul). Maybe this additional info will give you some insite.\nW 1207 13:59:15.804 UTC THREAD17 TraceId:d57687bbf177a8f0: consul datacenter '**********' service 'namerd-grpc' observation error Failure(service observation released, flags=0x02) with RemoteInfo -> Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: localhost/127.0.0.1:8500, Downstream label: client, Trace Id: a47d0d545b3d3da6.a47d0d545b3d3da6<:a47d0d545b3d3da6 with Service -> client, falling back to last good state\n. It seems, that once the feature is implemented in finagle, linkerd should have support for it almost automatically.\nI made a PoC patch for finagle https://github.com/twitter/finagle/pull/740\nAfter that I built latest linkerd against patched finagle.\nAs a result I was able to route 1000 MiB unchunked request using only 64 MiB heap.. @adleong thanks for explaining big picture and for pointint out conceptual difference between Activity and Future (this now obvious thing didn't occur to me).\nI do see, that Delegator signature change will cause a lot of changes across codebase and more important, will change behavior of other parts of namerd.\nAnd this doesn't look like something, that can be included in the nearest bugfix release. And the issue is real for us (although, I do understand, that without STR this is not very convincing).\nDo you think we could implement some part of this big change asap? Namely, we'll try to return Future up to the point, where it conflicts with traits signatures. This, I hope, will be aligned with the bigger picture, while still being isolated.\nPlease, see the update to the code.\nIf you're not willing to include this, would reproduce scenario of the issue we have convince you? (i would really like to avoid that, because it appears to be much more effortful, than the fix itself, but if there are no other options... :))\n. I guess we could block delegator.json for users and expose that new endpoint instead which would partially solve the issue for us.. @olix0r, oh sorry for not making it clear. It's only delegator UI, so we can check (mostly during troubleshooting), how a name is bound and resolved.. @adleong I'd be willing to take this opportunity to dig into code deeper, but I can do this not earlier than the next week too. And probably even only the week after the next one. So feel free to work on that if you have time. \nI'll post here, when I'm ready to start work, and if it appears that you haven't started yet, I will work on the PR.. @hawkw do you happen to know, what happens to linkerd if all namerds are down. Are there any differences between thrift and mesh interpreters when it comes to caching and ability of linkerd to survive through namerd downtimes?\n. I updated plugins to use the new method.\nCurrently, if no method is implemented by a plugin, namerd will fail in runtime. I wasn't sure what is the decision on the old method, so made it as simple as possible for now.\nI see 3 options:\n1) going your way and removing old method completely; Pro - simple code, Con - breaks compatibility with plugins, that linkerd users may have\n2) introducing more complicated class hierarchy (particularly, smthng like BaseDtabStoreConfig and ParameterizedDtabStoreConfig. DtabStoreConfig is marked as deprecated and implements mkDtabStore(Stack.Params), ParameterziedDtabStoreConfig is used in all bundled plugins and has mkDtabStore() implemented. Pro - no plugins should be broken on linkerd upgrade, Con - complication.\n3) Leaving things as they are (throwing NotImplementedException in Runtime). Pro - compatibility and simple code, Con - no compile-time checking and somewhat hacky.\nLet me know, what do you think please.. @adleong I removed the old method.\nCI now fails due to NetworkedInteropTest, which appears to be flaky (different cases have failed for me before).. This is related to #1670, but is way more bizarre. Simple test-case works as expected and without any issues: as soon as last observer to the var is closed, Closeable is invoked, Failure is raised and loop is stopped.\nval svc = SvcAddr(new TestApi(), \".local\", SvcKey(\"foo\", None), None, None, None, Map.empty, SvcAddr.Stats(stats))\n    val closable = svc.changes.respond(println)\n    closable.close()\n. Is it possible, that there are some class loading issues and ServiceRelease val is created multiple times, so the condition if cause == ServiceRelease doesn't hold true?. @adleong, I prepared a PR, that uses the same backoff stream that is used inside ConsulApi.\nThere's a still an edge case, if system flaps between \"consul returned 500\", and \"cannot allocate socket for outbound connection\", then we'll be stuck in the first values of the backoffs Stream (and by default it's 1ms --- effectively, no backoff).\nIdeal solution, I believe, would be to handle all retries in a single place. However, this means exception being thrown (means expensive stack unwinding) on every bad response from consul, which may become a problem on its own if we observe many names and consul goes down. So with that, ConsulApi should be changed to return Either of parsed response or status code if response is different from expected (without attempting to parse the resposne in this case), so that client could decide whether to retry or not.. @adleong that's what I implied, by saying \"handling in single place\".\nBut, as I mentioned, this means exception being thrown on every 5xx from consul because json parsing will be attempted. And I'd prefer to avoid that.\nSo some ConsulApi redesign is also required.\nOr am I missing something?. I'm working on a different PR, that will not rely on retry logic in consul api but will handle everything in SvcAddr. Closing this one.. All errors are handled in the same fashion: if previous state exists, we treat error as intermittent and continue watching the service. If no previous state exists, we terminate immediately.\nMain differences in behavior\n1) On connection exception we no longer try observing with the same blocking index. Assumption is that while consul was unreachable, service catalog state may have changed anyway, so it is safe to retry without index. And the cost is just one single request\n2) On request to non-existent DC we no longer monitor address infinitely. We do this only if  we succeeded for the name in the past.\n3) All errors are retried with back-off\n4) Only first error in a row for each name is logged as WARNING, rest are logged as DEBUG to reduce noise in logs.. @Ashald I addressed your comments in ammended commit.\nAnd since we started working on code readability, I also took the liberty to change state() = ... to state.update(...) as the latter is much more readable imo. I hope that's ok.. @adleong I can confirm, that in our environment namerd built from this branch doesn't hang during resolution of a non-existent address and there's no StackOverflowError in cases where it appeared before.\nLooking forward for 1.3.7 with this fix. Thanks!. This was tested in prod NCBI environment both before and after #1877.\nWithout #1877  it reduced noise in logs indeed. We never tested #1877  in isolation, so hard to say what difference this PR makes after #1877 . And this is my concern.\nInitially, we removed retry filter to simplify consul namer. Having only one single loop and handling all errors in a single place made code easier to understand and reason about. But it made high-level consul code to deal with low-level exceptions.\nAnd now we are introducing back another level retries. It is already better than it was in the beginning, as those levels of retries handle different errors, and also Requeue filter has budget, it is not infinite retry.\nSo I still think we should have this fix in the code, but frankly we never tested its effect on the code after #1877 . We can only say, that #1877 and this PR work perfectly together.. @adleong :smile: I see, it is already merged. It happened, while I was composing my message.\nSorry I didn't react to your comment earlier, had a busy week. Anyway, I believe, merging it is a right decision.. Fixes #1873 . @wmorgan would be great. The more, the better.\nSorry, I don't have time to write tests atm. Probably could do this only next week.. @adleong I'm sorry, I don't have a docker image. @adleong I addressed your comments, also renamed few vars to state more clearly what are they for.\nI noticed, there are squashed commits in git log, so I assume this is the usual way of doing things in this repo. Hence separate commit, but let me know if I should ammend the previous one instead.. @adleong \nI tested with the same test I used to reproduce the issue.\nYour change fixes the leak, memory consumption is stable. Thanks!\n\nSmall note (if anyone is interested): this change affects both server and client. If there are any applications/libraries that communicate to namerd via buoyant mesh client, those are to be updated too to fix the leak.. If the intention is to see stats under admin/metrics.json, then it is already possible with linkerd.\nUsually (I'd say, always, but maybe some type of plugins miss that still) plugin factory method receives Stack.Params as an argument, and one can obtain StatsReceiver from it:\nval stats = params[param.Stats].statsReceiver. I'm not 100% certain, but I believe, consul responds with the same message, when DC exists, but network paritioning happens, so a server receiving an initial request for resolution can't communicate with a server from existent but not accessible DC.\nIt would be great if namerd could survive such intermittent failures. I do see the necessity of returning Neg right away if DC does not exist, though...\nMaybe we should consider previous state when making a decision whether to update to Neg or not? If an address was ever resolved to a non-neg, it is very unlikely, I believe, that No path to datacenter is a truthful message. If you still think, current behavior is more correct, maybe we could agree on some toggle in config to let multiple behaviors?. @dadjeibaah \n\nmodify this so that it only returns Neg when the previous state is Pending\n\nI think this is the right solution. I created an issue for this.. @dadjeibaah Thanks for this change, happy to see it in linkerd!\n@adleong absolutely! I'm back on Tuesday however, if you think it is worth waiting till then I'd be happy to look at the change and give it a try. But I believe the only thing to discuss here is the default value of the parameter (I don't really like finagle's default 5 MiB), and it is not that important anyway.\nBtw we just started using similar patch  (based on linkerd 1.5.2) at NCBI.\nAccording to my benchmarks the change also improves latency for fixed length requests. On my scenarios the difference is about 10-15% for requests below 1 MiB but it gets much more pronounced as request size grows larger or link to linkerd gets slower (I think everyone has linkerds on the same host with their apps, so it's all about copying data in memory anyway, hence only request size is a significant factor).\nI have only single result on me right now, but if there's interest I could post more when I'm back\n```\nbaseline\nClient.invoke:invoke\u00b7p0.95               4096  sample           5.677           ms/op\nvanilla l5d\nClient.invoke:invoke\u00b7p0.95               4096  sample          10.879          ms/op\npatched l5d\nClient.invoke:invoke\u00b7p0.95               4096  sample           8.282          ms/op\n```\nWhere baseline is single threaded direct invocation of a service on localhost which just counts bytes in request. Linkerd is executed on Java 8 with -Xms256m -Xmx256m. This case is 4MiB request.. @adleong \nI do agree, that logic becomes more stateful and (a little bit) more complex.\nThe patch, however, is trivial\n-          case Throw(e: UnexpectedResponse) if e.rsp.contentString == DatacenterErrorMessage =>\n+          case Throw(e: UnexpectedResponse) if e.rsp.contentString == DatacenterErrorMessage && currentValueToLog != Addr.Pending  =>\n\nIn particular, the proposed solution does not allow for DCs to be deleted. This may be rare in practice, but it maps logically to the error message.\n\nUsually mapping to DC happens in Dtabs:\n/svc/foo=>/dc/a/foo | /dc/b/foo | /dc/c/foo | !\nNow, if we want to delete dc b, we'll want to cleanup our dtab:\n/svc/foo=>/dc/a/foo | /dc/c/foo | !\nRemoval of DC in consul is a huge (and yes, unusual) event, and preparation to it is required. Cleaning up dtabs is not a big deal, and, ideally, should be performed as a part of DC removal procedure, ideally before removing DC in consul. I'd say, such order of events is not even a workaround, but, rather, a normal clean way of removing DC.\nNetwork partitions, however, happen without our knowledge and happen quite often, we can't workaround them in any way. So I strongly believe, that namerd should handle such situations in the namer code.. FWIW, at NCBI we use different approach. Our consul store plugin combines pieces of dtabs stored in different keys into single dtab (originally the idea belongs to @Ashald)\nThat's how it may look in KV with our approach:\nkv/namerd/dtabs contains the index, what keys should be read and in which order should be combined\nkv/namerd/dtabs/base\nkv/namerd/dtabs/cool-thing\nkv/namerd/dtabs/lame-thing\nThen, for example\nkv/namerd/dtabs/base:\n/http=>/svc\nkv/namerd/dtabs/cool-thing:\n/svc/cool-thing => /tagged/ver-1.3/cool-thing;\nkv/namerd/dtabs/lame-thing:\n/svc/lame-thing => 0.1 * /tagged/ver-2.0/lame-thing & 0.9 * /tagged/ver-1.8/lame-thing;\nThis approach also lets you having fine-grained ACLs for your dtabs. For example, you may issue a restricted token specifically for lame-thing, and let your deployment pipeline for lame-thing to update only that single key in consul KV without affecting anything else.\n. @chrismikehogan , we have custom plugin for that. We haven't thought about publishing it, but if there's interest, we can do this.. interfaces:\n- kind: io.l5d.mesh\n  ip: 0.0.0.0\n  port: 4321\n- kind: io.l5d.httpController\n  ip: 0.0.0.0\n  port: 4180\n@adw12382 , which of these interfaces you use the most?\nWe also observe namerd memory leak in our environment, but our namerd config is very different and even includes custom plugins. Only the interfaces secion is the same. So I wonder, how your usage is similar to ours.. Won't this break all custom plugins that linked users might have?\nI prepared this in haste, and I do see now, that fix is not optimal, as new plugin would need to implement both methods.\nI can fix that and update all plugins that come with linked, but I believe it's better to keep the old method anyway.. I won't insist though if you decide to remove the old method :). I was having a specific case in mind: we have security scanner running periodically (I recall seeing someone else among linkerd users with similar setup). Every now and then scanner queries for some random addresses. I'd prefer those to not be accumulated over time causing unnecessary load.\nAlso requests to nonexistent DCs, I believe, shouldn't be retried (although, they were previously --- in infinite retry filter). It seems unlikely, that there are envs where DC in consul may suddenly appear after request came and thus transitioning to pending with retries will likely give no benefit ever.. @adleong ugh, I didn't think of that. No I do not know what is the linkerd behavior.\nI'll check this later.. I looked, how Addr.Failed will be handled by Linkerd. Here what happens to Addr.Failed in case if mesh interpreter is used:\n\nSvcAddr: returns Addr.Failed\nLookupCache: transforms it into Activity.Failed[NameTree[Name]]. Value gets cached through Memoize (sic!)\nConsulNamer and farther: same\nmesh.InterpreterService.toBoundTreeRspEv: transform to VarEventStream.End(Throw(e)) (terminal event, containing either value or error)\n\nwire\n\nmesh.Client.streamActivity: transforms End event into Activity.Failed[NameTree[Name]]\nDst: transforms to Activity.Failed[BoundTree]\nDynBoundFactory: internal state transitions from Pending -> Failed. On apply() factory will return Future.exception (DunBoundFactory:81)\nServiceFactoryCache: wraps DynBoundFactory decorated with NoBrokersAvailableException handling into IdlingFactory\nIdlingFactory: returns same Future.exception. Value remains in cache for TTL if idle\nRoutingService: invokes ServiceFactoryCache and returns Future.exception as response. Every invocation resets idle timer, so as long as there are requests, ServiceFactory remains \"stuck\" in cache.\n\nSo it seems that\n1. Even implementation of consul namer in master can put linkerd in \"stuck\" state (no previous state in namer, no index in response from consul). Obviously implementation from this PR will do this on any error. To get out of the \"stuck\" state linkerd has to be restarted or failing activity should be evicted from cache.\n2. Considering that activities are memoized in namer, we also put namerd in \"stuck\" state. Looks like we should never interrupt a loop in activity (except for cases of explicit interruption). Theoretically[*], it can recover on its own, if all observes to activity are gone and activity is closed. Then the next observer will start the loop again.\nPlease, let me know if I missed something.\n[*] during my experiments, I noticed that activity is not always closed as soon as all supposed observers are gone. I'd guess, there's another cache somewhere (maybe in http interface?). I had no time to dig into this however.\nI'm going to change code to retry with back-off in case of exception.\nI believe I should also change the case, when consul returns no index, but let me know if I misunderstood something. (Btw, why such case even exists? I understand, we can't do long poll on consul w/o index, but can't we just continue polling with some constant delay?)\n. In general I agree, that storing same state in 2 places isn't very pretty.\nHowever I don't see how can we get rid of lastGood if we want to log last state. We handle errors inside the loop and there we only have access to Updatable. Even conceptually it's tricky: closure with this loop is an argument to Var's \"constructor\", so to get current value of Var one would need to populate a reference to yet unconstructed Var to the closure that is used to construct the Var instance.\nWe still can omit the state while logging. Imo it may be usefull during troubleshooting if any sort of outage happens, but it's not something super important.. This is a change in semantics and I'm not sure, it's something we want. Last observation will then survive through close() and the first thing the next observer sees would be this last observed value. Unless I'm missing something.. Anyway, I read further and I agree that we should avoid storing state in volatile var, especially considering that it's only for logging. Will push update shortly. @Ashald  we currently count closes (see Closable).\nIdeally, this should correspond  to number of entrances into this branch (however, there's #1820 so it's not that simple). Having counter still may be a good idea, I'm not sure, whether we need to add this counter as part of this PR.. Now I think, do we really need to parse the error message? Don't we want to have the same behavior as we have on this line for all errors? Wouldn't it be correct to return Negon any error if we haven't resolved anything yet instead of just stucking in Pending?\nThis would simplify code as well.. @adleong , if service is not known to consul it responds with 200\n```\n[~]$ curl -i http://127.0.0.1:8500/v1/catalog/service/my-service                            \nHTTP/1.1 200 OK\nContent-Type: application/json\nX-Consul-Index: 6\nX-Consul-Knownleader: true\nX-Consul-Lastcontact: 0\nDate: Thu, 17 Jan 2019 19:25:36 GMT\nContent-Length: 3\n[]\n```\nNamer returns Addr.Neg if returned set of addresses is empty.. > The difference in behavior from today would be that if we get a 5XX which is not a \"No path to DC\" error while the Addr is pending, we would immediately update it to Neg (and then retry) instead of leaving it as Pending and retrying. But I think that's okay.\nPending is generally bad, because, I believe, it will make linkerd to timeout eventually without letting namerd exploring other branches of dtab.\nLet's describe all possible kinds of errors we can expect from consul\n1. Consul agent can't be contacted\nalready handled in code partially by Throw(e: IndividualRequestTimeoutException), partially by Throw(e)\nNamer should retain last known state :heavy_check_mark:\nIf it is an initial request, Namer should return Addr.Neg :question: Currently it is not the case, and I think it is not entirely correct. Addr.Pending will be returned right now and if it remains like that for more than 10 seconds, linkerd will return 502. If it was Addr.Neg, namerd could try other discovery systems in the meantime (for example, try dns, or try contacting consul server directly instead of local agent (given proper namerd configuration and dtab of course)). \nBasically it is a choice between \"let requests hang for 10 seconds and then fail\" and \"let requests fail right away\". But with the latter option we can try other discovery systems before failing (calls to other consul datacenters will likely fail too).\n\n\nConsul agent can be contacted but its response is not a response we expect\nalready handled in code by Throw(e)\nBasically same as case 1. In the code the only difference is the message logged.\n\n\nConsul agent can be contacted but, but not all servers can be contacted (No path to datacenter error)\nHandled in code by Throw(e: UnexpectedResponse) if e.rsp.contentString == DatacenterErrorMessage\nNamerd should retain last known state :heavy_check_mark:  (fixed with this commit)\nIf it is an initial request, Namer should return Addr.Neg.\nHere, again, it is a choice between \"let requests hang for 10 seconds and then fail\" and \"let requests fail right away\". However, with the second option we can try other datacenters as well.\n\n\nWould you agree, that behavior in those 3 cases looks very similar, and in code it is actually the same, so 3 cases can be merged into one? Only logged messages should be different.. @adleong I'll fix the code shortly. Thanks!. I just checked 2 months of our logs (14 namerd instances talking to 5 datacenters in a pretty flaky consul cluster) and this message hasn't happenned even once.\n@adleong , do you happen to recall why was this branch added in the first place? As of today, it looks to me, as if this was effectively dead code.. @dadjeibaah , my point is that other cases should behave the same way \"No path to datacenter\" case behaves after this PR.\nIf a name was never resolved \u2014 return Addr.Neg and let fallback to other parts of dtab\nIf a name was ever resolved before \u2014 return result of previous resolution\nThe code that handles all cases will look like this (still working on tests, so can't push) (and logged message is still wrong, yeah)\n```\n          case Throw(e) =>\n            // Update state only if it is Pending to not let linkerd hang on name resolution\n            // Otherwise retain last known state to allow namerd survive intermittent failures\n            val effectiveState = if (currentState == Addr.Pending) {\n              state.update(Addr.Neg)\n              Addr.Neg\n            } else {\n              currentState\n            }\n        log.log(\n          failureLogLevel,\n          \"consul datacenter %s service %s lookup request received invalid dc error %s. Current state is %s\",\n          datacenter, key.name, e, effectiveState\n        )\n        val backoff #:: nextBackoffs = backoffs\n        // subsequent errors are logged as DEBUG\n        Future.sleep(backoff).before(loop(None, nextBackoffs, Level.DEBUG, effectiveState))\n\n```\nLet me know please if I'm still missing something with this chane.. @adleong , I believe, though, you replied to a wrong thread ;). I see. I personally think, this code does more harm than good. I concluded from the code, that there's some case, where consul behaves like this. And, as I said, in the last 2 months we haven't seen this branch of code executed even once, so I'm almost certain, it is indeed never executed.\nI don't have strong opinion on this, but I'd prefer to remove this branch of code. Would you mind if I did that?\n. I reverted my changes in that place in the code to keep this PR as relevant to the issue as possible.. ",
    "dinabogdan": "Hello,\nI have the following LinkerD configuration:\n\nprotocol: h2\n  experimental: true    \n  identifier:\n    kind: io.l5d.header.path\n    header: authority\n    segments: 2\n  dtab: |\n    /svc => /#/io.l5d.fs ;\n  label: grpc\n  servers:\nport: 6565\n    ip: 0.0.0.0\n\nWhat should I put in the disco folder in order to perform requests to grpc service methods? Currently I have created 4 files (because I have 4 grpc packages) and I'm receiving the \"No hosts are available\" error.\nThank you!. ",
    "ievgen-kolomiiets": "Hi @klingerf.\nLet me describe my case. service-a and service-b are services in different availability zones.\nFor zone a routes look like /service => /service-a | /service-b and for b - /service => /service-b | /service-a. This allows us to avoid cross-zone requests as much as possible.\nSo if something bad happens in zone a and all pods are getting restarted or unavailable I want traffic to go to zone b. But what I saw in namerd code that fallback starts working only if there is a Neg resolution but instead empty address list is returned for service with no pods.\n. ",
    "dvulpe": "This looks like an inconsistency in the way finagle is used for client/server auth. \nTlsServerConfig uses a file with multiple ca certs as a truststore collection, whereas TlsClientConfig takes from a list of files with certs and concatenates all of them into a temporary file.\nWith the downside of a backwards incompatible change I am happy to align client TLS truststore configuration with the TLS server configuration and use a ca bundle file instead of a list of files.. Thanks @adleong! I just rebased from master, hopefully for a green ci.. Hi, \nIs there anything else needed for this PR or is it ok to be merged into master?\nThanks,\nDan. I'm going to close this one off and attempt an approach with deprecating trustCerts and introducing something like trustCertsBundle.. Nice one, thanks! I look forward to my next contribution to this great project!. Thanks for the feedback.\nI think this test boundary is around the Finagle integration rather than L5D. \nIn my opinion this test should only fail when something in Finagle (newer version?) would change. \nIf you think this kind of test would add value, we could look at introducing it in another part of the project, I tend to add as fewer as possible as they can fail for multiple reasons which makes debugging really hard and slow down the feedback loop. . Fixing. Fixing. Cool, I will update the code to reflect the above.. ",
    "perspectivus1": "Hi @esbie,\nkubectl get ingress -o wide returns no external IP either:\nNAME                HOSTS     ADDRESS         PORTS     AGE\nnginx-ingress       *         35.189.224.x    80        1d\nlinkerd-ingress     *                         80        1d\nI then used the l5d service's external address (kubectl get svc l5d -n l5d-system) on port 80. It works like a charm and forwards requests to the service configured in my ingress. Thanks for your help!\nWhat I don't understand is how I can manage multiple Linkerd ingresses if a single IP address and a single port are assigned to all. Do you join the rules from all ingresses? What if there are conflicting rules? Do you do this even when the ingresses reside in different namespaces?\nThanks again. Everything is clear @adleong. Thanks.. ",
    "oscar-stripe": "any update here? What about a module inside this repo so you don't need to move anything and still build normally (just a separate subproject)?. actually, maybe you already did this?. ",
    "maruina": "Hello @adleong, can we re-open this?\nWe found the same issue: when we send gRPC payload bigger than 64k, linkerd hangs. What do you need to troubleshoot this issue?\n//cc @davibo. Hey @rmars, unfortunately, I'm using production data so I need to generate some redacted or sample. I'll see what I can do.. ",
    "coleca": "The workaround hinted at in the original post works like this:\nOn each HTTP2 router (inbound and outbound) add in initialStreamWindowsBytes: XXX where XXX is the bytes to increase over the default of 65536.\nAs in:\nservers:\n      - port: 4143\n        ip: 0.0.0.0\n        initialStreamWindowBytes: 524288\nI have tested this with messages over 64k up to the 4Gb limit of gRPC and it works fine w/the value set to 512k.  It may also work with a lower value over 64k, but I haven't tested that.  Not sure what this does to the network efficiency.\n. ",
    "aywrite": "Yeah, that makes sense to me. I have update the diff accordingly, should be good to go.. No worries, thanks @hawkw and @adleong, my first time writing Scala (or Java for that matter) so I especially appreciate the help and feedback.. @dadjeibaah thanks for looking in to it. The intention with the second namer is to act as a fallback pool, if any of our backends are in the warning state we only want to route requests to them if no host are in the healthy state. As we should only be using the fallback pool when there are no healthy hosts I did not put passing in the list of statuses. Based on my understanding the configuration attached should do exactly that.. Its possible something more complicated is going on to trigger this issue but I know for any of our hosts if linkerd comes up before consul, or if I stop consul and then restart linkerd before starting consul again then it gets into this state where all requests get the 'Failure(Connection refused: localhost/127.0.0.1:8500 at remote address: localhost/127.0.0.1:8500. Remote Info: Not Available, flags=0x100000018) with RemoteInfo -> Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: localhost/127.0.0.1:8500, Downstream label: client, Trace Id: 72eba5a6f101baef.72eba5a6f101baef<:72eba5a6f101baef with Service -> 0.0.0.0/11010' error, which is different to the error I normally get if linkerd can talk to consul but there are no backends.. So with linkerd and consul both running and working correctly, if I stop consul the following errors appear in the linkerd logs:\nJun 24 20:17:55 <ip-addr> linkerd[11224]: I 0624 20:17:55.623 EDT THREAD32 TraceId:132f01f661cfd099: #/default/.local/abtest_app: name resolution is negative (local dtab: Dtab())\nJun 24 20:17:55 <ip-addr> linkerd[11224]: I 0624 20:17:55.723 EDT THREAD35 TraceId:977969bad75c7174: #/default/.local/contacts_midlayer: name resolution is negative (local dtab: Dtab())\nJun 24 20:17:55 <ip-addr> linkerd[11224]: I 0624 20:17:55.784 EDT THREAD38 TraceId:d0dc4f849cf66794: #/default/.local/projects_midlayer: name resolution is negative (local dtab: Dtab())\nJun 24 20:17:55 <ip-addr> linkerd[11224]: I 0624 20:17:55.804 EDT THREAD35 TraceId:67b28f0fa8f47d9f: #/default/.local/users_midlayer: name resolution is negative (local dtab: Dtab())\nJun 24 20:17:55 <ip-addr> linkerd[11224]: I 0624 20:17:55.824 EDT THREAD31 TraceId:12bf9cf63406328b: #/default/.local/memberships_midlayer: name resolution is negative (local dtab: Dtab())\nFor applications which are sending requests to linkerd I see:\n2018/06/24 20:17:50 Check memberships_midlayer failed: Internal error processing ping: 'com.twitter.finagle.NoBrokersAvailableException: No hosts are available for /svc/ping, Dtab.base=[/svc=>/#/fallback/.local/memberships_midlayer;/svc=>/#/default/.local/memberships_midlayer], Dtab.local=[]. Remote Info: Not Available'\n2018/06/24 20:17:50 Check users_midlayer failed: Internal error processing ping: 'com.twitter.finagle.NoBrokersAvailableException: No hosts are available for /svc/ping, Dtab.base=[/svc=>/#/fallback/.local/users_midlayer;/svc=>/#/default/.local/users_midlayer], Dtab.local=[]. Remote Info: Not Available'\n2018/06/24 20:17:50 Check abtest_app failed: Internal error processing ping: 'com.twitter.finagle NoBrokersAvailableException: No hosts are available for /svc/ping, Dtab.base=[/svc=>/#/fallback/.local/abtest_app;/svc=>/#/default/.local/abtest_app], Dtab.local=[]. Remote Info: Not Available'\n2018/06/24 20:17:50 Check messages_midlayer failed: Internal error processing ping: 'com.twitter.finagle.NoBrokersAvailableException: No hosts are available for /svc/ping, Dtab.base=[/svc=>/#/fallback/.local/messages_midlayer;/svc=>/#/default/.local/messages_midlayer], Dtab.local=[]. Remote Info: Not Available'\n2018/06/24 20:17:50 Check projects_midlayer failed: Internal error processing ping: 'com.twitter.finagle.NoBrokersAvailableException: No hosts are available for /svc/ping, Dtab.base=[/svc=>/#/fallback/.local/projects_midlayer;/svc=>/#/default/.local/projects_midlayer], Dtab.local=[]. Remote Info: Not Available'\n2018/06/24 20:17:50 Check realemail_restricted_app failed: Internal error processing ping: 'com.twitter.finagle.NoBrokersAvailableException: No hosts are available for /svc/ping, Dtab.base=[/svc=>/#/fallback/.local/realemail_restricted_app;/svc=>/#/default/.local/realemail_restricted_app], Dtab.local=[]. Remote Info: Not Available'\n2018/06/24 20:17:50 Check contacts_midlayer failed: Internal error processing ping: 'com.twitter.finagle.NoBrokersAvailableException: No hosts are available for /svc/ping, Dtab.base=[/svc=>/#/fallback/.local/contacts_midlayer;/svc=>/#/default/.local/contacts_midlayer], Dtab.local=[]. Remote Info: Not Available'\nIf I start consul again at this point everything recovers and works as expected, but if instead I restart linkerd (without starting consul back up) this is all that shows up in the linkerd logs:\nJun 24 20:17:59 <ip-addr> linkerd[11451]: -XX:+AggressiveOpts -XX:+AlwaysPreTouch -XX:+CMSClassUnloadingEnabled -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled -XX:+CMSScavengeBeforeRemark -XX:InitialHeapSize=33554432 -XX:MaxHeapSize=1073741824 -XX:MaxNewSize=174485504 -XX:MaxTenuringThreshold=6 -XX:OldPLABSize=16 -XX:+PrintCommandLineFlags -XX:+ScavengeBeforeFullGC -XX:-TieredCompilation -XX:+UseCMSInitiatingOccupancyOnly -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+UseStringDeduplication\nJun 24 20:18:01 <ip-addr> linkerd[11451]: Jun 24, 2018 8:18:01 PM com.twitter.finagle.http.HttpMuxer$ $anonfun$new$1\nJun 24 20:18:01 <ip-addr> linkerd[11451]: INFO: HttpMuxer[/admin/metrics.json] = com.twitter.finagle.stats.MetricsExporter(<function1>)\nJun 24 20:18:01 <ip-addr> linkerd[11451]: Jun 24, 2018 8:18:01 PM com.twitter.finagle.http.HttpMuxer$ $anonfun$new$1\nJun 24 20:18:01 <ip-addr> linkerd[11451]: INFO: HttpMuxer[/admin/per_host_metrics.json] = com.twitter.finagle.stats.HostMetricsExporter(<function1>)\nJun 24 20:18:02 <ip-addr> linkerd[11451]: I 0624 20:18:02.038 EDT THREAD1: linkerd 1.4.0 (rev=a59bdc01bcb4b0d716001259305d766afede5f5c) built at 20180430-154029\nJun 24 20:18:02 <ip-addr> linkerd[11451]: I 0624 20:18:02.795 EDT THREAD1: Finagle version 18.4.0 (rev=82da7ddebf5de885f2adaf556d91ef39c2ffe937) built at 20180410-150636\nJun 24 20:18:05 <ip-addr> linkerd[11451]: I 0624 20:18:05.698 EDT THREAD1: Tracer: com.twitter.finagle.zipkin.thrift.ScribeZipkinTracer\nJun 24 20:18:05 <ip-addr> linkerd[11451]: I 0624 20:18:05.760 EDT THREAD1: connecting to usageData proxy at Set(Inet(stats.buoyant.io/104.28.22.233:443,Map()))\nJun 24 20:18:05 <ip-addr> linkerd[11451]: I 0624 20:18:05.935 EDT THREAD1: Resolver[inet] = com.twitter.finagle.InetResolver(com.twitter.finagle.InetResolver@3206174f)\nJun 24 20:18:05 <ip-addr> linkerd[11451]: I 0624 20:18:05.935 EDT THREAD1: Resolver[fixedinet] = com.twitter.finagle.FixedInetResolver(com.twitter.finagle.FixedInetResolver@42fb8c87)\nJun 24 20:18:05 <ip-addr> linkerd[11451]: I 0624 20:18:05.936 EDT THREAD1: Resolver[neg] = com.twitter.finagle.NegResolver$(com.twitter.finagle.NegResolver$@65e0b505)\nJun 24 20:18:05 <ip-addr> linkerd[11451]: I 0624 20:18:05.938 EDT THREAD1: Resolver[nil] = com.twitter.finagle.NilResolver$(com.twitter.finagle.NilResolver$@795f5d51)\nJun 24 20:18:05 <ip-addr> linkerd[11451]: I 0624 20:18:05.938 EDT THREAD1: Resolver[fail] = com.twitter.finagle.FailResolver$(com.twitter.finagle.FailResolver$@34aeacd1)\nJun 24 20:18:05 <ip-addr> linkerd[11451]: I 0624 20:18:05.938 EDT THREAD1: Resolver[flag] = com.twitter.server.FlagResolver(com.twitter.server.FlagResolver@54067fdc)\nJun 24 20:18:05 <ip-addr> linkerd[11451]: I 0624 20:18:05.938 EDT THREAD1: Resolver[zk] = com.twitter.finagle.zookeeper.ZkResolver(com.twitter.finagle.zookeeper.ZkResolver@4098dd77)\nJun 24 20:18:05 <ip-addr> linkerd[11451]: I 0624 20:18:05.939 EDT THREAD1: Resolver[zk2] = com.twitter.finagle.serverset2.Zk2Resolver(com.twitter.finagle.serverset2.Zk2Resolver@43aeb5e0)\nJun 24 20:18:06 <ip-addr> linkerd[11451]: I 0624 20:18:06.459 EDT THREAD1: serving http admin on /0.0.0.0:9990\nJun 24 20:18:06 <ip-addr> linkerd[11451]: I 0624 20:18:06.489 EDT THREAD1: serving abtest_app on /0.0.0.0:11010\nJun 24 20:18:06 <ip-addr> linkerd[11451]: I 0624 20:18:06.516 EDT THREAD1: serving contacts_midlayer on /0.0.0.0:11003\nJun 24 20:18:06 <ip-addr> linkerd[11451]: I 0624 20:18:06.533 EDT THREAD1: serving language_app on /0.0.0.0:12000\nJun 24 20:18:06 <ip-addr> linkerd[11451]: I 0624 20:18:06.548 EDT THREAD1: serving memberships_midlayer on /0.0.0.0:11008\nJun 24 20:18:06 <ip-addr> linkerd[11451]: I 0624 20:18:06.563 EDT THREAD1: serving messages_midlayer on /0.0.0.0:11002\nJun 24 20:18:06 <ip-addr> linkerd[11451]: I 0624 20:18:06.590 EDT THREAD1: serving projects_midlayer on /0.0.0.0:11001\nJun 24 20:18:06 <ip-addr> linkerd[11451]: I 0624 20:18:06.602 EDT THREAD1: serving realemail_restricted_app on /0.0.0.0:11100\nJun 24 20:18:06 <ip-addr> linkerd[11451]: I 0624 20:18:06.615 EDT THREAD1: serving users_midlayer on /0.0.0.0:11000\nJun 24 20:18:06 <ip-addr> linkerd[11451]: I 0624 20:18:06.628 EDT THREAD1: initialized\nThis time for applications which are sending requests to linkerd I see:\n2018/06/24 20:31:22 Check abtest_app failed: Internal error processing ping: 'Failure(Connection refused: /127.0.0.1:8500 at remote address: /127.0.0.1:8500. Remote Info: Not Available, flags=0x100000018) with RemoteInfo -> Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /127.0.0.1:8500, Downstream label: client, Trace Id: 282639513409e935.282639513409e935<:282639513409e935 with Service -> 0.0.0.0/11010'\n2018/06/24 20:31:32 Check abtest_app failed: Internal error processing ping: 'Failure(Connection refused: /127.0.0.1:8500 at remote address: /127.0.0.1:8500. Remote Info: Not Available, flags=0x100000018) with RemoteInfo -> Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /127.0.0.1:8500, Downstream label: client, Trace Id: 282639513409e935.282639513409e935<:282639513409e935 with Service -> 0.0.0.0/11010'\n2018/06/24 20:31:32 Check projects_midlayer failed: Internal error processing ping: 'Failure(Connection refused: /127.0.0.1:8500 at remote address: /127.0.0.1:8500. Remote Info: Not Available, flags=0x100000018) with RemoteInfo -> Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /127.0.0.1:8500, Downstream label: client, Trace Id: 282639513409e935.282639513409e935<:282639513409e935 with Service -> 0.0.0.0/11001'\n2018/06/24 20:31:32 Check contacts_midlayer failed: Internal error processing ping: 'Failure(Connection refused: /127.0.0.1:8500 at remote address: /127.0.0.1:8500. Remote Info: Not Available, flags=0x100000018) with RemoteInfo -> Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /127.0.0.1:8500, Downstream label: client, Trace Id: 282639513409e935.282639513409e935<:282639513409e935 with Service -> 0.0.0.0/11003'\n2018/06/24 20:31:32 Check messages_midlayer failed: Internal error processing ping: 'Failure(Connection refused: /127.0.0.1:8500 at remote address: /127.0.0.1:8500. Remote Info: Not Available, flags=0x100000018) with RemoteInfo -> Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /127.0.0.1:8500, Downstream label: client, Trace Id: 282639513409e935.282639513409e935<:282639513409e935 with Service -> 0.0.0.0/11002'\n2018/06/24 20:31:32 Check memberships_midlayer failed: Internal error processing ping: 'Failure(Connection refused: /127.0.0.1:8500 at remote address: /127.0.0.1:8500. Remote Info: Not Available, flags=0x100000018) with RemoteInfo -> Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /127.0.0.1:8500, Downstream label: client, Trace Id: 282639513409e935.282639513409e935<:282639513409e935 with Service -> 0.0.0.0/11008'\n2018/06/24 20:31:32 Check users_midlayer failed: Internal error processing ping: 'Failure(Connection refused: /127.0.0.1:8500 at remote address: /127.0.0.1:8500. Remote Info: Not Available, flags=0x100000018) with RemoteInfo -> Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /127.0.0.1:8500, Downstream label: client, Trace Id: 282639513409e935.282639513409e935<:282639513409e935 with Service -> 0.0.0.0/11000'\n2018/06/24 20:31:32 Check realemail_restricted_app failed: Internal error processing ping: 'Failure(Connection refused: /127.0.0.1:8500 at remote address: /127.0.0.1:8500. Remote Info: Not Available, flags=0x100000018) with RemoteInfo -> Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /127.0.0.1:8500, Downstream label: client, Trace Id: 282639513409e935.282639513409e935<:282639513409e935 with Service -> 0.0.0.0/11100'\nNothing additional shows up in the linkerd logs after I start consul again, the application continues to show the same connection refused error. To be clear, I never see connection refused in the linkerd logs, only in the application which is calling linkerd. There is also no sign of linkerd attempting to communicate with consul in the consul logs. \nAs soon as I restart linkerd from this state everything starts working again. \nAt this stage while there may be a more complicated set of circumstances required to reproduce this issue I am fairly sure it is a problem at least in part with linkerd but I will work on replicating this inside a docker image or something similar which I can share.  . I haven't packaged this into a docker image but I was able to replicate this by installing linkerd and consul locally with a minimal config. I have created a repo with the files I used and the steps to reproduce.\nThis issue will only show up when running consul locally alongside linkerd, if the linkerd config points to a remote consul (even one which does not exist) it will retry the connection to consul until it becomes available. Without looking at the code it looks to me like the connection refused error which appears is classified as non-retryable. . @dadjeibaah thanks!. I was mostly copying from the consistency mode enum also in consul, only I used enum instead of a bunch of case classes so that they get an implicit ordering for the worst case method. It seems like there are also a bunch of custom deserialisers in the config types package. Happy to change to a case class with a value or a JsonSubType etc if it's preferable though.\nhttps://github.com/linkerd/linkerd/blob/a9c1627d389fab206faac44d1f674695207ff5cd/consul/src/main/scala/io/buoyant/consul/v1/ConsistencyMode.scala#L9-L29. Fixed and passing. passing=false means skip filtering. From the (somewhat unclear) Consul API docs:\n\npassing (bool: false) - Specifies that the server should return only nodes with all checks in the passing state. This can be used to avoid additional filtering on the client side.. Whoops, should be fixed now. \n",
    "wjwinner": "Hi, @klingerf  I have just  tested  1.2.0-32b, 1.2.0 and nightly. The problem still exists\u3002\nE 0908 02:45:44.001 UTC THREAD28: service failure: Failure(Invalid argument: /192.168.10.149:7778 at remote address: /192.168.10.149:7778. Remote Info: Not Available, flags=0x09) with RemoteInfo -> Upstream Address: /192.168.12.176:50206, Upstream Client Id: Not Available, Downstream Address: /192.168.10.149:7778, Downstream Client Id: #/io.l5d.k8s/default/http/world-v2-service, Trace Id: 083849135a36689c.083849135a36689c<:083849135a36689c. @hawkw  The endpoints IP had not changed. All two times are \"172.17.0.2:7778,172.17.0.3:7778,172.17.0.5:7778\"\nI use calico, so the endpoints IP change every time when I redeploy my app.. @hawkw I just tested buoyantio/linkerd:1.2.0-SNAPSHOT. The problem seems to be still there.\n```\nkubectl get endpoints\nNAME               ENDPOINTS                                 AGE\nworld-v2-service   192.168.10.151:7778                       4m\n```\n```\nkubectl logs l5d-g6l8t l5d -n linkerd\nE 0912 02:42:29.003 UTC THREAD35: service failure: Failure(Invalid argument: /192.168.10.150:7778 at remote address: /192.168.10.150:7778. Remote Info: Not Available, flags=0x09) with RemoteInfo -> Upstream Address: /192.168.13.97:49542, Upstream Client Id: Not Available, Downstream Address: /192.168.10.150:7778, Downstream Client Id: #/io.l5d.k8s/default/http/world-v2-service, Trace Id: 8f47b4eeec8dd19f.8f47b4eeec8dd19f<:8f47b4eeec8dd19f\n```. It's working perfect.thanks all. ",
    "weitzj": "I am using kubernetes 1.7.5 on AWS with flannel. RBAC is enabled. Service has ClusterIP: None\nKubernetes was deployed as multi AZ master, multi AZ node using kops. @hawkw  Sorry. This happened to me again on 1.2.1\n\nThe service stayed the same as before\nI have updated the deployment/pod inside the service and ran kubectl apply -f mypoddeployment.yaml\n\nSee:\n\nbadtrace.txt (https://gist.github.com/weitzj/3a57820cf4ca0a37d38ad43387d87154)\ngoodtrace.txt (afer linkerd restart)\n (https://gist.github.com/weitzj/41b0065708c714015a11e0a6e3d93909\n\nbadtrace.txt contains several retries\n. ",
    "ihac": "Same issue here.\n$ kubectl get endpoints hello\nNAME      ENDPOINTS                                                      AGE\nhello     192.168.168.233:7777,192.168.172.40:7777,192.168.175.24:7777   4m\n$ http_proxy=localhost:4140 curl http://hello\nHello (192.168.172.40) world (192.168.172.42)!!\n$\n$\n$ kubectl delete -f hello-world.yml && kubectl apply -f hello-world.yml\n...\n$\n$ kubectl get endpoints hello\nNAME      ENDPOINTS                                                       AGE\nhello     192.168.168.235:7777,192.168.168.236:7777,192.168.172.43:7777   9s\n$\n$ http_proxy=localhost:4140 curl http://hello\nInvalid argument: /192.168.172.40:7777 at remote address: /192.168.172.40:7777. Remote Info: Not Available\nFYI, k8s version:\n$ kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"7\", GitVersion:\"v1.7.4\", GitCommit:\"793658f2d7ca7f064d2bdf606519f9fe1229c381\", GitTreeState:\"clean\", BuildDate:\"2017-08-17T08:48:23Z\", GoVersion:\"go1.8.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"7\", GitVersion:\"v1.7.5\", GitCommit:\"17d7182a7ccbb167074be7a87f0a68bd00d58d97\", GitTreeState:\"clean\", BuildDate:\"2017-08-31T08:56:23Z\", GoVersion:\"go1.8.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nimage tag:\n$ grep 'image:' linkerd-zipkin-cni.yml\n        image: buoyantio/linkerd:nightly\n        image: buoyantio/kubectl:v1.4.0\npod network: Calico. @hawkw I tried linkerd:1.2.0-SNAPSHOT(not using namerd) but bug still existed.\nHowever, what surprised me most was that, sometimes, only 1(or some?) Linkerd instance could not forward request to the rightful endpoint.\n$ http_proxy=http://node1:4140 curl http://hello\nInvalid argument: /192.168.168.236:7777 at remote address: /192.168.168.236:7777. Remote Info: Not Available\n$\n$ http_proxy=http://node2:4140 curl http://hello\nHello (192.168.175.25) world (192.168.172.46)!!\n$\n$ http_proxy=http://node3:4140 curl http://hello\nHello (192.168.175.25) world (192.168.168.240)!!\nIt's quite random so I could not reproduce it.\n. Nice! glad to hear it.\nBTW, I tried buoyantio/linkerd:1.2.0.fix4-SNAPSHOT and it did work properly.. ",
    "jsurdilla": "@hawkw , I'm able to reproduce this on the nightly build.. @hawkw , tested it on both buoyantio/linkerd:1.2.0 and buoyantio/linkerd:nightly. Tried it quickly, can confirm that I'm able to reproduce on 1.2.0-SNAPSHOT. Thanks for looking into it!. That's amazing, looking forward to checking it out. Thank you!. ",
    "bseibel": "We're seeing this same problem in GKE with the same watch errors that @Taik is seeing. Also using linkerd/namerd 1.2.1, though to be fair we have seen this problem crop up in prior 1.1.x releases too. \nIt is definitely intermittent and doesn't affect all of our running namerds at the same time. It does eventually get the correct endpoints if left alone, but it will still miss new updates. Usually restarting our linkerd and namerd pods will allow it to resume working without further issue for a time.\nI havent looked too deeply yet but wondering if it could be related to error handling and https://github.com/kubernetes/kubernetes/issues/35068\nA quick update here: It appears that namerd in this same state also affects its ability to update dtabs stored in thirdparty resources. \n. So unfortunately we're still seeing this issue even with the fix here, we now see in debug logs \nD 1003 19:20:25.158 UTC THREAD51 TraceId:9921f7129139749d: k8s returned 'too old resource version' error with incorrect HTTP status code, restarting watch\nhowever where we do see lines like (and pardon my slightly filtered log lines without the endpoints):\n```\nE  D 1003 19:51:13.636 UTC THREAD65 TraceId:8b3991f3b0f04b0d: k8s ns default svc yarisgrmn constructed new ServiceEndpoints with:\nE  D 1003 19:51:13.636 UTC THREAD65 TraceId:8b3991f3b0f04b0d: k8s ns default service yarisgrmn added port mappings\nE  D 1003 19:51:13.636 UTC THREAD65 TraceId:8b3991f3b0f04b0d: k8s ns default service yarisgrmn added endpoints\n``` \nfor most pre-existing endpoints which is fine, and expected, but a service that was added after the restarting watch line doesn't appear in the logs at all, and linkerd ends up giving us \"No hosts are available\".\nSo far we've only seen this happen in production, but it happens pretty frequently, sometimes minutes after we kick our namerd pods. Linkerd isnt logging any issues about connectivity to namerd. I'm out of town at the moment and I'm going to try to narrow down the issue further when I'm back later this week, but if theres anything specific you would like me to poke at to help narrow down the issue please let me know.\n. Thanks for looking into this @hawkw \nBoth linkerd/namerd were the nightly at rev: b5cd1c168873d1d668b5f4fdbd729a44122135d2\nKubernetes is running out of Google Container Engine, with the master currently at 1.6.10-gke.1\n. ",
    "sokoow": "FYI i'm referring to this article: https://buoyant.io/2016/11/04/a-service-mesh-for-kubernetes-part-iv-continuous-deployment-via-traffic-shifting/?__hstc=249056664.9082b41d8a52a00a19ebd6f40131c9a2.1507023894299.1507023894299.1507023894299.1&__hssc=249056664.5.1507023894299&__hsfp=584158062. ",
    "zsojma": "Hello, is there any update on this issue? Thank you.. I followed kubernetes instructions how to migrate a ThirdPartyResource on this link: https://kubernetes.io/docs/tasks/access-kubernetes-api/migrate-third-party-resource/ and it looks like it is working.\nHere is my result:\nkind: CustomResourceDefinition\napiVersion: apiextensions.k8s.io/v1beta1\nmetadata:\n  name: dtabs.l5d.io\nspec:\n  scope: Namespaced\n  group: l5d.io\n  version: v1alpha1\n  names:\n    kind: DTab\n    plural: dtabs\n    singular: dtab. ",
    "dlaidlaw": "Today I downloaded the latest minikube using Kubernetes 1.8 to test this out.\n$ minikube version\nminikube version: v0.23.0\n$ kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"8\", GitVersion:\"v1.8.1\", GitCommit:\"f38e43b221d08850172a9a4ea785a86a3ffa3b3a\", GitTreeState:\"clean\", BuildDate:\"2017-10-11T23:27:35Z\", GoVersion:\"go1.8.3\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"8\", GitVersion:\"v1.8.0\", GitCommit:\"0b9efaeb34a2fc51ff8e4d34ad9bc6375459c4a4\", GitTreeState:\"dirty\", BuildDate:\"2017-10-17T15:09:55Z\", GoVersion:\"go1.8.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nThe only change I needed to make was to change the definition of the ThirdPartyResource into a CustomResourceDefinition like the one above. Thanks @zsojma! \nYou just change the ThirdPartyResource definition in the namerd.yml file to the CustomResourceDefinition above. No other changes were required.. ",
    "mirosval": "@amitsaha There is no particular reason, other than I have tried all possible combinations including the io.buoyant.http.domainToPathPfx rewrite namer, which seems to only work for HTTP traffic. So for those experiments it made sense to have 2 rules.. When you do it that way, you need one configuration entry per service. \nI would like to multiplex my Thrift services. I want to have one port for Thrift, same as I have 80 for HTTP and then I want all my Thrift clients to talk to that one port and linkerd would discover & forward my requests to the appropriate one based on the dest field of the Thrift message. This should be the Thrift equivalent of HTTP Host header.. @olix0r My understanding after reading the documentation was that it is indeed the case that Thrift does not include the service name in the message, but TThrift (the Twitter extensions to Thrift) does. . So after further digging, this seems to be fixable with ThriftMux, but you have to explicitly enable muxing protocol like so:\nscala\nval client = new ImageService.FinagledClient(\n    service = ThriftMux.client.newClient(thriftAddress, \"image-service-client\").toService,\n    protocolFactory = Protocols.multiplex(\"image-thrift\", Protocols.binaryFactory()),\n    serviceName = \"image-thrift\",\n    stats = NullStatsReceiver\n). I'm sorry I have to reopen this, it seems I was overly excited to see it work, but now I have some additional insights:\nSo now my error message looks like this:\norg.apache.thrift.TApplicationException: Internal error processing image-thrift:deleteImage: 'com.twitter.finagle.NoBrokersAvailableException: \nNo hosts are available for /svc/image-thrift:deleteImage, \nDtab.base=[\n/thrift/*=>/#/io.l5d.consul/small-virtualbox;\n/svc=>/$/io.buoyant.porthostPfx/thrift\n], Dtab.local=[]. Remote Info: Not Available'\n Good news is that the image-thrift is the name of my service, so this is now definitely coming to Linkerd\n Not-so-good news is that it appears that Linkerd is not handling it correctly:\n  1. When I set thriftMethodInDst: true then both the service name and the method name are appearing like this: /svc/<service-name>:<method-name>, I think Linkerd should correctly split it like this: /svc/<service-name>/<method-name> and the method-name is controlled by the thriftMethodInDst parameter.\n  2. When I set thriftMethodInDst: false then neither the service name nor method name appears\n. @klingerf I just submitted the CLA form, thanks!. ",
    "hoswey": "I go through the Linkerd-examples, but find nothing about the thrift part, does linkerd support thrift?. ",
    "jippi": "Hi @hawkw \nI'm probably the reason why this telmeter was implemented, as I requested NewRelic integration from @wmorgan \nLooking at the code, it looks like something different from what I asked was possible.\nMy request was to support request queue in linkerd, which probably is a way simpler thing to implement - or maybe an additional thing on top of this.\nWhat we would like to see Linkerd do, is simply adding a X-Request-Start header to outgoing requests, with the value (in msec) of when linkerd received the request. There is examples on this on their configuration page.\nThis setup do not require any kind of newrelic license key or anything else, as the NewRelic SDK's automatically process the header if present. \nThe benefit of the X-Request-Start header is that the latency it measures, will show up per-app next to $LANGUAGE, Redis, Database etc in the performance charts, rather than being a distinct metric like this PR have.\n\n. @hawkw really sorry I haven't been clear in what i wanted to see. What's done here is excellent too, and something that can be super useful as well. My request was a simple header though :) I hope to see that being added as a config (that doesn't require NR license key) . amazing @hawkw ! thank you so much! . @wmorgan thanks! we currently use linkerd-zipkin to get tracing data into jaeger, it seem to work pretty nice :). ",
    "christophetd": "Should be good now. Done . Note that the memory usage doesn't seem to grow bigger if I hit LinkerD with a higher request rate (I tested 10-20rps).. It seems to me that the memory usage is just high in general, but doesn't particularly grow unbounded. I will try some of the remediations suggested and let you know if it lowers the memory footprint of Linkerd. . Yes, definitely. I'll update that tomorrow. . ",
    "jsenon": "I don't have historical payload. does namerd metrics or heapster screenshot can help you?. @klingerf Unfortunately env. is freeze for demo. I will check on another cluster.. @klingerf thanks for investigation and error investigation on your side. @klingerf sure, I ll test it tomorrow and give you quick feedback, thanks. Same for me, update with curl command line on namerd dtab is automatically take into account. Thanks a lot for this quick fix: @klingerf @wmorgan and all. . ",
    "activeshadow": "@klingerf any update on when this might get fixed?. Great, thank you for the update!\nOn Wed, Oct 18, 2017 at 02:43:03PM -0700, William Morgan wrote:\n\nUpdate: we have identified a possible root cause and are working to verify.\nThis is a high priority issue.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.*\n\n\n-- \n//SIGNED//\nBryan T. Richardson\nActive Shadow LLC\n505.382.2077\n. @klingerf so far the fix looks to be working! I can restart pods, causing them to get a new endpoint IP, and linkerd picks it up and continues to send them data. w00t!\nThanks!. ",
    "yangzhares": "On my environment i got this issue again, every 5 mins namerd will check service health status info from Consul, but in fact those services are not existed.\nNamerd and linkerd: 1.3.4\nNamerd logs:\nE 0103 01:32:44.061 UTC THREAD30: Retrying Consul request 'GET /v1/health/service/phpnuke73?index=1082854&dc=hf01&tag=active&passing=true' on NonFatal error: com.twitter.finagle.ChannelWriteException: com.twitter.finagle.ChannelClosedException: null at remote address: /127.0.0.1:8500. Remote Info: Not Available from service: client. Remote Info: Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /127.0.0.1:8500, Downstream label: client, Trace Id: 46d8dd952ddb6102.46d8dd952ddb6102<:46d8dd952ddb6102\nE 0103 01:32:44.064 UTC THREAD33: Retrying Consul request 'GET /v1/health/service/webboard?index=1082854&dc=hf01&tag=active&passing=true' on NonFatal error: com.twitter.finagle.ChannelWriteException: com.twitter.finagle.ChannelClosedException: null at remote address: /127.0.0.1:8500. Remote Info: Not Available from service: client. Remote Info: Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /127.0.0.1:8500, Downstream label: client, Trace Id: e3bad8c27cd3b046.e3bad8c27cd3b046<:e3bad8c27cd3b046\nE 0103 01:32:44.069 UTC THREAD32: Retrying Consul request 'GET /v1/health/service/owls?index=1082854&dc=hf01&tag=active&passing=true' on NonFatal error: com.twitter.finagle.ChannelWriteException: com.twitter.finagle.ChannelClosedException: null at remote address: /127.0.0.1:8500. Remote Info: Not Available from service: client. Remote Info: Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /127.0.0.1:8500, Downstream label: client, Trace Id: 871b1a26d5186be5.871b1a26d5186be5<:871b1a26d5186be5\nE 0103 01:32:44.077 UTC THREAD35: Retrying Consul request 'GET /v1/health/service/mt?index=1082854&dc=hf01&tag=active&passing=true' on NonFatal error: com.twitter.finagle.ChannelWriteException: com.twitter.finagle.ChannelClosedException: null at remote address: /127.0.0.1:8500. Remote Info: Not Available from service: client. Remote Info: Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /127.0.0.1:8500, Downstream label: client, Trace Id: b2b7b1825a8091c1.b2b7b1825a8091c1<:b2b7b1825a8091c1\nW 0103 01:32:44.078 UTC THREAD35 TraceId:e2e304cd6c60297f: consul datacenter 'hf01' service 'versatilebulletinboard' observation error Failure(service observation released, flags=0x02) with RemoteInfo -> Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /127.0.0.1:8500, Downstream label: client, Trace Id: c1e8fde74c62d5b7.c1e8fde74c62d5b7<:c1e8fde74c62d5b7 with Service -> client, falling back to last good state\nE 0103 01:32:44.081 UTC THREAD34: Retrying Consul request 'GET /v1/health/service/mt?index=1082854&dc=hf01&tag=active&passing=true' on NonFatal error: com.twitter.finagle.ChannelWriteException: com.twitter.finagle.ChannelClosedException: null at remote address: /127.0.0.1:8500. Remote Info: Not Available from service: client. Remote Info: Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /127.0.0.1:8500, Downstream label: client, Trace Id: aeea482ab304669a.aeea482ab304669a<:aeea482ab304669a\nW 0103 01:32:44.091 UTC THREAD35 TraceId:e2e304cd6c60297f: consul datacenter 'hf01' service 'thatware_0.4.6' observation error Failure(service observation released, flags=0x02) with RemoteInfo -> Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /127.0.0.1:8500, Downstream label: client, Trace Id: 8efa41401a627d36.8efa41401a627d36<:8efa41401a627d36 with Service -> client, falling back to last good state\nE 0103 01:32:44.099 UTC THREAD29: Retrying Consul request 'GET /v1/health/service/mt?index=1082854&dc=hf01&tag=active&passing=true' on NonFatal error: com.twitter.finagle.ChannelWriteException: com.twitter.finagle.ChannelClosedException: null at remote address: /127.0.0.1:8500. Remote Info: Not Available from service: client. Remote Info: Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /127.0.0.1:8500, Downstream label: client, Trace Id: a250cc973acfbfeb.a250cc973acfbfeb<:a250cc973acfbfeb\nE 0103 01:32:44.103 UTC THREAD21: Retrying Consul request 'GET /v1/health/service/mt?index=1082854&dc=hf01&tag=active&passing=true' on NonFatal error: com.twitter.finagle.ChannelWriteException: com.twitter.finagle.ChannelClosedException: null at remote address: /127.0.0.1:8500. Remote Info: Not Available from service: client. Remote Info: Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /127.0.0.1:8500, Downstream label: client, Trace Id: e650cff757b92f83.e650cff757b92f83<:e650cff757b92f83\nE 0103 01:32:44.142 UTC THREAD33: Retrying Consul request 'GET /v1/health/service/mt?index=1082854&dc=hf01&tag=active&passing=true' on NonFatal error: com.twitter.finagle.ChannelWriteException: com.twitter.finagle.ChannelClosedException: null at remote address: /127.0.0.1:8500. Remote Info: Not Available from service: client. Remote Info: Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /127.0.0.1:8500, Downstream label: client, Trace Id: 370a10407a3c32f5.370a10407a3c32f5<:370a10407a3c32f5\nW 0103 01:32:44.145 UTC THREAD33 TraceId:e2e304cd6c60297f: consul datacenter 'hf01' service 'phpware' observation error Failure(service observation released, flags=0x02) with RemoteInfo -> Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /127.0.0.1:8500, Downstream label: client, Trace Id: 52f79738bc990a36.52f79738bc990a36<:52f79738bc990a36 with Service -> client, falling back to last good state\nE 0103 01:32:44.148 UTC THREAD30: Retrying Consul request 'GET /v1/health/service/backofficelite?index=1083373&dc=hf01&tag=active&passing=true' on NonFatal error: com.twitter.finagle.ChannelWriteException: com.twitter.finagle.ChannelClosedException: null at remote address: /127.0.0.1:8500. Remote Info: Not Available from service: client. Remote Info: Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /127.0.0.1:8500, Downstream label: client, Trace Id: 64259cb64ae21831.64259cb64ae21831<:64259cb64ae21831\nW 0103 01:32:44.162 UTC THREAD27 TraceId:e2e304cd6c60297f: consul datacenter 'hf01' service 'phpnuke73' observation error Failure(service observation released, flags=0x02) with RemoteInfo -> Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /127.0.0.1:8500, Downstream label: client, Trace Id: d04a4f922611f366.d04a4f922611f366<:d04a4f922611f366 with Service -> client, falling back to last good state\nW 0103 01:32:44.180 UTC THREAD33 TraceId:e2e304cd6c60297f: consul datacenter 'hf01' service 'espocrm' observation error Failure(service observation released, flags=0x02) with RemoteInfo -> Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /127.0.0.1:8500, Downstream label: client, Trace Id: 44d1b83ca1fb3901.44d1b83ca1fb3901<:44d1b83ca1fb3901 with Service -> client, falling back to last good state\nW 0103 01:32:44.210 UTC THREAD21 TraceId:e2e304cd6c60297f: consul datacenter 'hf01' service 'owls' observation error Failure(service observation released, flags=0x02) with RemoteInfo -> Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /127.0.0.1:8500, Downstream label: client, Trace Id: d9c3d2738d7fb40b.d9c3d2738d7fb40b<:d9c3d2738d7fb40b with Service -> client, falling back to last good state\nGrafana graph for RPC reqeust number received by Consul server:\n. I also want to file an issue for this, good to see guys have fixed it, we also hit it under namerd 1.3.0, thanks guys.. @pcalcado , thank you for checking this, if you need more info, i can help you\n  . linkerd config:\n```\nadmin:\n  port: 9990\n  ip: 0.0.0.0\nrouters:\n- protocol: http\n  identifier:\n    kind: io.l5d.custIdentifier\n  label: cust_outgoing\n  interpreter:\n    kind: io.l5d.namerd\n    dst: /$/inet/namerd.service.consul/4100\n    namespace: cust\n  servers:\n  - port: 8080\n    ip: 0.0.0.0\n- protocol: http\n  label: outgoing\n  interpreter:\n    kind: io.l5d.namerd\n    dst: /$/inet/namerd.service.consul/4100\n    namespace: default\n  #httpAccessLog: /alloc/logs/access_outgoing.log\n  servers:\n  - port: 8081\n    ip: 0.0.0.0\ntelemetry:\n- kind: io.l5d.recentRequests\n  sampleRate: 1.0\n- kind: io.l5d.prometheus\nusage:\n  enabled: false\n```\nnamerd config:\n```\nadmin:\n  ip: 0.0.0.0\n  port: 9991\nstorage:\n  kind: io.l5d.consul\n  host: 127.0.0.1\n  port: 8500\n  pathPrefix: /namerd/dtabs\n  datacenter: hf01\nnamers:\n- kind: io.l5d.consul\n  prefix: /io.l5d.consul\n  host: 127.0.0.1\n  port: 8500\n  includeTag: false\n  setHost: false\n  useHealthCheck: true\n- kind: io.l5d.consul\n  prefix: /io.l5d.mbs.consul\n  host: 127.0.0.1\n  port: 8500\n  includeTag: true\n  setHost: false\n  useHealthCheck: true\ninterfaces:\n- kind: io.l5d.thriftNameInterpreter\n  ip: 0.0.0.0\n  port: 4100\n- kind: io.l5d.httpController\n  ip: 0.0.0.0\n  port: 4180\n```\ndtab for namespace cust:\n/active      => /#/io.l5d.mbs.consul/hf01/active;\n/baking      => /#/io.l5d.mbs.consul/hf01/baking;\n/svc/active  => /active;\n/svc/baking  => /baking;\nSome background for io.l5d.custIdentifier:\nIdentifier io.l5d.custIdentifier transforms request URL's first segment as service name, also reads HEADER from request to indicate a service is active or baking, be default, active , so,  GET http://example.com/app, will be transformed as /svc/active/app. . @deebo91 Due to our Security Team scanned linkerd's router port with extremely lots of URLs that could make security issues, and when those requests go through linkerd, linkerd picks up the URL's first segment as service name, then query it from Consul, almost those services are not existed in Consul. Often Security Team scanned linkerd every few days, not 5 minutes, from Namerd log, it did queries every 5 minutes, although no Security Team scanning at that time, as graph showed, lots of queries(almost service health status checking queries) happened on Consul.. @deebo91 Here you are:\n What version of consul are you using?\nConsul v0.8.4\n What environment are you running linkerd, namerd and consul? Is it in k8s? AWS ECS etc?\nWe use Hashicorp Nomad as container orchestration engine, linkerd and namerd running with Docker container are managed by Nomad, Consul is running as dedicated standalone process on each VM.\n* Can you reproduce the issue in a docker environment?\nAlthough i can't trigger the security scan action happen, but i can share you the latest Namerd logs, because if \nsecurity scan happen, Namerd will continuously check those nonexistent services's health status with Consul API, until restart Namerd. you can the log file.\nNamerd log:\nnamerd.stderr.6.txt\nThanks. @deebo91, thanks for your updates. we got all environments from DEV to Prod were impacted, because Security Team scanned all environments. Usually each several days will happen, and we must restart Namerd. At last, we added service wthitelist in our customized Identifier to avoid unknown service query to Consul through Namerd.\nTwo questions i want you guys help:\n1. What's difference between thriftNameInterpreter, mesh and httpController?\n2. I found mesh don't have cache mechanism, instead thriftNameInterpreter has, that means if use mesh, when service discovery backend becomes down, linkerd doesn't cache routing entries received service discovery backend, right?. @briansmith  thanks for opening this. . thanks @dennis.ab, follow your advice, update client tls configuration like:\nclient:\n        kind: io.l5d.static\n        configs:\n        - prefix: /#/io.l5d.k8s\n          tls:\n            commonName: linkerd\n            trustCerts:\n            - /io.buoyant/linkerd/tls/ca.crt\n        - prefix: \"%/io.l5d.k8s.daemonset\"\n          tls:\n            commonName: linkerd\n            trustCerts:\n            - /io.buoyant/linkerd/tls/ca.crt\nit definitely works. As you said, this should be explicitly included in docs, what\u2019s more, i think if Transformer is applied on outgoing router and TLS is enabled, when configure client tls as kind io.l5d.static, prefix should be started with \"%/$transformer_kind\".. ",
    "blakebarnett": "Bump!\nThis is preventing us from using linkerd for ingress at all (we need this for the DNS record automation to work).. ",
    "ismaelka": "@siggy Just to confirm, we've seen a similar issue when using Linkerd as a sidecar for TLS, below is the gc log of Linkerd process running over 4 days.\n\nPurple:  Tenured Generation\nGreen: GC times Line\nThe initial heap dump analysis suggest it might be similar to the issue reported in  #1696:\n\nI will post more details once we have further findings. \n. ",
    "shinofara": "+1. ",
    "chrismikehogan": "FWIW, we were seeing similar issues to @DukeyToo. We deployed 1.3.5 and the leak we were seeing was resolved. . Were names like foo.bar actually causing problems? We use namespace names following this pattern, and have not seen them cause any identifiable problems.\nBut since 1.3.4 these namespace names are invalid and 500 when queried via namerd, which is a breaking change from 1.3.3.\n. I saw that flag in the configuration docs, but it wasn't obvious to me that it would do what I wanted, in addition to what the docs said it does:\n\nif true, all headers that set Linkerd contexts are removed from inbound requests. Useful for servers exposed on untrusted networks.\n\nWe definitely don't wan't the behavior described in the docs, even if it got us the suppressed error responses.. Truncating would work perfectly for us as well, even more so if the level of truncation could be controlled.\nWe could take or leave the concept debug header adding additional info. . > Thanks for filling this! I think this totally makes sense as a feature and seems pretty well scoped. @chrismikehogan, are you interested in working on this? If so, I can point you in the right direction.\nI'm interested! And I could definitely use some directions haha. @edio That sounds really promising as well. How do the separate k/v entries get combined in the final dtab?\nOr did you write custom code that knows how to parse these?. ",
    "mebe": "We've run into something that looks identical. We are running Linkerd in front of an object storage system, and are blocked by this issue. We get io.netty.util.internal.OutOfDirectMemoryError with an identical stacktrace after we've uploaded around 400-500 GB of data. We're running the Linkerd with -Xms and -Xmx of 4 GB and -XX:MaxDirectMemorySize of 2GB.\nThis is fairly straightforward to reproduce by running Minio in Docker and using the AWS CLI to repeatedly upload and delete stuff.\nLet's start by bringing up the Minio container:\ndocker pull minio/minio\ndocker run -p 9000:9000 minio/minio server /data\nMake a note of the AccessKey and SecretKey printed by this. You'll need those later.\nInstall AWS CLI, e.g. with the bundled installer:\ncurl \"https://s3.amazonaws.com/aws-cli/awscli-bundle.zip\" -o \"awscli-bundle.zip\"\nunzip awscli-bundle.zip\n./awscli-bundle/install -b ~/bin/aws\nConfigure AWS CLI to use Minio:\n```\nmkdir ~/.aws\ncat > ~/.aws/config <<EOF\n[default]\ns3 =\n    max_concurrent_request = 100\n    endpoint_url = http://minio\n    multipart_threshold = 50MB\n    multipart_chunksize = 10MB\nEOF\ncat > ~/.aws/credentials <\naws_secret_access_key = \nEOF\n```\nConfigure Linkerd to proxy to Minio:\n```\ncat > linkerd-minio.yaml <<EOF\nadmin:\n  port: 9990\nrouters:\n- protocol: http\n  dtab: |\n    /svc/minio => /$/inet/127.0.0.1/6060;\n  httpAccessLog: logs/access.log\n  maxRequestKB: 51200\n  maxResponseKB: 51200\n  servers:\n  - port: 6007\n    ip: 127.0.0.1\nEOF\n```\nStart Linkerd:\nLOCAL_JVM_OPTIONS=\"-XX:MaxDirectMemorySize=2G\" JVM_HEAP_MIN=4G JVM_HEAP_MAX=4G linkerd linkerd-minio.yaml\nMake AWS CLI use Linkerd as a proxy:\nexport HTTP_PROXY=http://127.0.0.1:6007\nCreate a bucket in Minio:\n~/bin/aws s3 mb s3://five-gigs\nCreate some files to upload:\nmkdir five-gigs; touch five-gigs/5gb-{1..10} && shred -n 1 -s 5G five-gigs/5gb-{1..10}\nLoop upload and delete until it fails:\nuntil [ $? -gt 0 ]; do ~/bin/aws --endpoint-url http://minio s3 rm --recursive s3://five-gigs/ && ~/bin/aws --endpoint-url http://minio s3 sync five-gigs s3://five-gigs; done\nAfter about 400-500 GB (which takes a bit over an half an hour for me), you should start seeing OutOfDirectMemoryErrors from Linkerd and the sync will fail. Linkerd won't recover from this until it's restarted.\nI imagine the reproduction time will be shorter with less -XX:MaxDirectMemorySize. With 4 GB heap, the heap usage doesn't seem to be a problem and total GC for the whole run stays in a couple of seconds.\nI hope this helps! Let me know if you need any more information.. @chris-goffinet-ck, 1.3.5.. I added -Djdk.nio.maxCachedBufferSize=262144 and removed the -XX:MaxDirectMemorySize. Result is that Linkerd still crashed with the same error message after a bit shy of 3 TB had been uploaded. This took me about 3.5 hours with constant transfer rate of around 225-230 MB/s.\nW 0307 19:18:57.801 UTC THREAD55: Unhandled exception in connection with /127.0.0.1:51350, shutting down connection\nio.netty.util.internal.OutOfDirectMemoryError: failed to allocate 1048576 byte(s) of direct memory (used: 2146435079, max: 2147483648)\nThe RSS reached at the time of the crash was about 6972 MB.\n\nI left Linkerd running with no traffic after the OOM. The RSS kept on creeping up for about 3.5 hours when it suddenly spiked up by about 760 MB over the course of an hour. The RSS stayed at that level for an hour before finally making another 40 MB jump to 7787 MB. The RSS has stayed at this level ever since.\nThe RSS didn't decrease at any point.\n\n. @adleong,\nI could provide you SSH access to an EC2 instance where I've already successfully reproduced the problem. If you think that's useful, let me know, and I'll set it up.\nThanks!. I've been trying to reproduce this on a clean Packet.net c1.small.x86 (aka Type 1), but there it seems to work fine. The RSS quickly settles around 3.5 GB and doesn't really budge one way or the other.\nThere is a big difference between the HW of c1.small.x86 and what we're using internally:\n c1.small.x86: 4 physical cores, 32 GB of RAM and 2 x SSD in Raid 1\n Internal: 72 physical cores, 1.5 TB of RAM and 24 x SAS HD in Raid 6\nI'm currently running the test on the internal server with CPU count limited to 4 with taskset. Initial results are promising.. @chrisgoffinet these are both bare metal instances from different provides, so the images aren't identical. The 4-core is running Centos 7.4.1708 and the 72-core is running Centos 7.2.1511. Both have glibc 2.17-196.\nFWIW, the restricted-to-4-cores instance on the 72-core machine has now been running continuously for 8 hours when it previously crashed in about 3.5 hours.. @chrisgoffinet here's a chart of the RSS after about 7 hours with these settings:\n\nAfter about 6 hours, the RSS seems to have stablized at around 5080 MB.\nI started Linkerd with a script like this:\n```\nLOCAL_JVM_OPTIONS=\"-XX:MaxDirectMemorySize=2G -XX:+AlwaysPreTouch\"\nLOCAL_JVM_OPTIONS=\"$LOCAL_JVM_OPTIONS -Dio.netty.threadLocalDirectBufferSize=0 -Djdk.nio.maxCachedBufferSize=262144\"\nLOCAL_JVM_OPTIONS=\"$LOCAL_JVM_OPTIONS -Dio.netty.recycler.maxDelayedQueuesPerThread=4\"\nLOCAL_JVM_OPTIONS=\"$LOCAL_JVM_OPTIONS -Dio.netty.recycler.maxCapacity=4096\"\nLOCAL_JVM_OPTIONS=\"$LOCAL_JVM_OPTIONS -Dio.netty.allocator.numHeapArenas=4\"\nLOCAL_JVM_OPTIONS=\"$LOCAL_JVM_OPTIONS -Dio.netty.allocator.numDirectArenas=4\"\nexport LOCAL_JVM_OPTIONS\nexport JVM_HEAP_MIN=4G\nexport JVM_HEAP_MAX=4G\nexport MALLOC_ARENA_MAX=2\nexport MALLOC_MMAP_THRESHOLD_=131072\nexport MALLOC_TRIM_THRESHOLD_=131072\nexport MALLOC_TOP_PAD_=131072\nexport MALLOC_MMAP_MAX_=65536\n./linkerd-1.3.6-exec -log.append='true' -log.output='logs/linkerd.log' -log.rollPolicy='Daily' config/linkerd.yaml\n```\nps aux | grep linkerd output:\n/usr/bin/java -XX:+PrintCommandLineFlags -Djava.net.preferIPv4Stack=true -Dsun.net.inetaddr.ttl=60 -Xms4G -Xmx4G -XX:+AggressiveOpts -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+CMSClassUnloadingEnabled -XX:+ScavengeBeforeFullGC -XX:+CMSScavengeBeforeRemark -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=70 -XX:-TieredCompilation -XX:+UseStringDeduplication -Dcom.twitter.util.events.sinkEnabled=false -Dorg.apache.thrift.readLength=10485760 -XX:MaxDirectMemorySize=2G -XX:+AlwaysPreTouch -Dio.netty.threadLocalDirectBufferSize=0 -Djdk.nio.maxCachedBufferSize=262144 -Dio.netty.recycler.maxDelayedQueuesPerThread=4 -Dio.netty.recycler.maxCapacity=4096 -Dio.netty.allocator.numHeapArenas=4 -Dio.netty.allocator.numDirectArenas=4 -cp ./linkerd-1.3.6-exec -server io.buoyant.linkerd.Main -log.append=true -log.output=logs/linkerd.log -log.rollPolicy=Daily config/linkerd.yaml. I ran my S3 stress test over the weekend for a bit shy of two days with these latest switches (including netty4.numWorkers=4). The results look very promising:\n\nThere's a sudden spike (literally between two data points in my measurements) of 150MB at 15 hours in to the test, but otherwise the memory usage looks practically stable. I think we can consider the issue solved.\nGreat work, @chrisgoffinet!. @adleong,\nWe are being blocked by #1690 for the object storage use case. After that issue has been resolved, we'll dive deeper into this one.\nThanks!\n. ",
    "chris-goffinet-ck": "@mebe which version of linkerd is this repo case for?. Let's reopen this, I am finding this in Credit Karma logs too. I'm gonna try reproducing this more.. Don't we need to allow the user to set threshold? It looks to be missing. @olix0r weigh in here, but as discussed I think this value should be higher. closer to 60.seconds or 300.seconds\n  . ",
    "sgrankin": "@hawkw emailed to you @buoyant.io.  Thanks!. @hawkw I ended up trying out some fixes; the AsyncStream seems to have been the leaked object, but the leak I theorized was not the (only?) one.  I couldn't track down how it was leaking though, so I hacked up a non-AsyncStream based variant of the watch code.  It's been running for 2 days on our dev deployment with steady heap usage and no stale endpoint issues (so far).\nThe change is here: https://github.com/sgrankin/linkerd/commit/a454fa0dc6edfec3781e1237343e9a1ad5802c42\nIf this looks like a good approach to you, I can clean it up and submit a PR.. Sorry for the delay\u2014just finally got some time this weekend to clean up the diff.  PR opened at #1714. Updated with CR fixes and rebased on latest master.\nNote that io.buoyant.grpc.interop.NetworkedInteropTest seems to be flaky (failed, then passed on re-run).. I just got home :) \nI\u2019ll do the last round of changes sometime in the next few hours \u2014 since these are all effectively cosmetic change stability should be unaffected (in case you are concerned about re-running the stress tests).\n\nOn Nov 29, 2017, at 21:14, Andrew Seigner notifications@github.com wrote:\nHi @sgrankin https://github.com/sgrankin, just checking in to see how things are going. We'd love to get this merged for our 1.3.3 release this week.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub https://github.com/linkerd/linkerd/pull/1714#issuecomment-348060898, or mute the thread https://github.com/notifications/unsubscribe-auth/AA9_Qef1LhzVyqNKALm8_lELx1Pmrippks5s7g-HgaJpZM4QrFLl.\n\n\n. @siggy: I'm done with all the changes. Thanks!. (Also rephrased the comments slightly for clarity). Good catch!. IIRC I actually started this change with watch returning a Event[].  It was indeed cleaner, and worked in deployment, but the tests failed:\n- some of the existing tests write multiple events into the response to be processed before actually testing what came out on the other end.\n- the only way to consume a stream of events from a Var or Activity seems to be Var.changes or Activity.states (which then calls Var.changes)\n- in the observe method of the Var created in Var.async, I think multiple updates were managing to sneak in between the synchronized creation of the nested Var and the call to .observe which would register the Observer, so the first update would not necessarily be seen\n- and so the tests would not necessarily see the first event and would fail.\nI thought this behavior of missing some initial events should be ok in the wild (we'd get the latest event eventually), but wanted to make sure the tests (that test the watch function directly) saw the complete stream of events, as they did previously with AsyncStream.  I was also trying to avoid changing the logic of the tests.  So I ended up with this function  thus this function and the concurrent-queue machinery in the tests. \nI don't see a way of extracting the complete stream of events out of a Var (otherwise we'd likely run into the same issue as with AsyncStream, right?).  Anything I may be missing about Vars/Activities?\nActually... another option to improve the readability without changing the signature much (obvious in retrospect):\ndef watch(\n    labelSelector: Option[String] = None,\n    fieldSelector: Option[String] = None,\n    resourceVersion: Option[String] = None,\n    state: Activity.State[W] => Unit\n  )\nThe watchable call-site doesn't actually benefit from being able to partially watch, and the ApiTest callsite is easily changed to work with this signature as well.\nThoughts?   It'd still keep the weird state type but make the watch signature plainer, and the tests can still avoid dealing with Var quirks.. It was in some other incarnation of this lambda, but it seems no longer (or was possibly an IntelliJ-only error).  Will remove.. updated to 'largestVersion' and 'largestEvent'.  \nAside: a good cleanup to do afterwards may be to get rid of the event argument here.  I think the resource version comparison from ResourceVersionOrdering can be factored out and used directly with the resource version here, avoiding the need to keep a whole event around.  (I think this may be the only use case of that Ordering type class ...  I didn't do the change wanting to limit the diff size.). per discussion, went with the l updated signature... and also changed it from Activity.State[W] => Unit to Updatable[Activity.State[W]].  It makes the intent clear and simplifies the non-test callsite.. ",
    "vielmetti": "Based on https://github.com/google/protobuf/issues/3844#issuecomment-343325559 this issue will be resolved with the release of protobuf 3.5.0. There is an arm64 binary linked to in that issue, which when is used to replace the existing protoc shell script things start to build just fine.. protobuf 3.5.0 is available, so this needs to be tested.. ",
    "draveness": "Hi, we met the same problem in production, and how is this issue going on?. We removed Linkerd from production because of this problem. \ud83d\ude13 . @adleong Sorry, I can't provide the approach to reproduce the issue in 1.4.6, since it's several months ago and we moved forward istio after running into this problem. But it may be fixed in the 1.5.1 release. Thanks for your reply anyway.\n\nHTTP/2\nFixes an HTTP/2 issue that causes Linkerd to stop processing incoming frames on an HTTP/2\nconnection after Linkerd sends a RST_STREAM frame to its remote peer. This was causing gRPC\nclients to experience timeout errors intermittently because connections between Linkerd and its\nremote peers weren't being closed properly.\nSets the maxConcurrentStreamsPerConnection config value for the h2 router to 1000 by default\nto prevent Linkerd from running out of memory when HTTP/2 clients leak connection streams.\n. \n",
    "krak3n": "Also see the discord thread: https://discourse.linkerd.io/t/namerd-etcd-400-bad-request-on-get-v2-keys-namerd-dtabs-recursive-true/368. I don't know if this is related at all but I have been noticing problems using the HTTP API to update the dtab namesapce using namerd.\nOccasionally PUT requests return the expected 204 No Content but do not update the dtab namespace (at least visibile in the admin), restarting the pods then reveals changes to namerd.\nSo I am guessing this is a problem with watching the CRD, here is my stack trace:\n```\nW 0129 14:18:51.382 UTC THREAD26 TraceId:dee913496b808c02: Exception propagated to the default monitor (upstream address: /10.32.0.1:46404, downstream address: localhost/127.0.0.1:8001, label: /$/inet/localhost/8001).\nio.buoyant.k8s.Watchable$Closed$\n        at io.buoyant.k8s.Watchable$Closed$.(Watchable.scala)\n        at io.buoyant.k8s.Watchable.$anonfun$activity$10(Watchable.scala:284)\n        at com.twitter.util.Closable$$anon$5.close(Closable.scala:144)\n        at com.twitter.util.Var$$anon$4.$anonfun$closable$1(Var.scala:399)\n        at com.twitter.util.Closable$$anon$5.close(Closable.scala:144)\n        at com.twitter.util.Closable$.com$twitter$util$Closable$$safeClose(Closable.scala:67)\n        at com.twitter.util.Closable$$anon$3.closeSeq(Closable.scala:125)\n        at com.twitter.util.Closable$$anon$3.onTry$1(Closable.scala:122)\n        at com.twitter.util.Closable$$anon$3.closeSeq(Closable.scala:128)\n        at com.twitter.util.Closable$$anon$3.close(Closable.scala:134)\n        at com.twitter.util.Closable$.com$twitter$util$Closable$$safeClose(Closable.scala:67)\n        at com.twitter.util.Closable$$anon$3.closeSeq(Closable.scala:125)\n        at com.twitter.util.Closable$$anon$3.close(Closable.scala:134)\n        at com.twitter.util.Closable.close(Closable.scala:20)\n        at com.twitter.util.Closable.close$(Closable.scala:20)\n        at com.twitter.util.Closable$$anon$3.close(Closable.scala:101)\n        at com.twitter.util.Event.$anonfun$toFuture$1(Event.scala:286)\n        at com.twitter.util.Future.$anonfun$ensure$1(Future.scala:1549)\n        at com.twitter.util.Future.$anonfun$ensure$1$adapted(Future.scala:1548)\n        at com.twitter.util.Promise$Monitored.apply(Promise.scala:205)\n        at com.twitter.util.Promise$Monitored.apply(Promise.scala:200)\n        at com.twitter.util.Promise$$anon$7.run(Promise.scala:532)\n        at com.twitter.concurrent.LocalScheduler$Activation.run(Scheduler.scala:198)\n        at com.twitter.concurrent.LocalScheduler$Activation.submit(Scheduler.scala:157)\n        at com.twitter.concurrent.LocalScheduler.submit(Scheduler.scala:274)\n        at com.twitter.concurrent.Scheduler$.submit(Scheduler.scala:109)\n        at com.twitter.util.Promise.runq(Promise.scala:522)\n        at com.twitter.util.Promise.updateIfEmpty(Promise.scala:880)\n        at com.twitter.finagle.netty4.transport.ChannelTransport.com$twitter$finagle$netty4$transport$ChannelTransport$$fail(ChannelTransport.scala:95)\n        at com.twitter.finagle.netty4.transport.ChannelTransport$$anon$1.channelInactive(ChannelTransport.scala:188)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\n        at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\n        at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\n        at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)\n        at com.twitter.finagle.netty4.channel.ChannelRequestStatsHandler.channelInactive(ChannelRequestStatsHandler.scala:35)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\n        at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\n        at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\n        at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)\n        at io.netty.handler.codec.MessageAggregator.channelInactive(MessageAggregator.java:417)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\n        at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\n        at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\n        at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)\n        at io.netty.handler.codec.http.HttpContentDecoder.channelInactive(HttpContentDecoder.java:205)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\n        at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)\n        at io.netty.handler.codec.http.HttpContentEncoder.channelInactive(HttpContentEncoder.java:299)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\n        at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelInactive(CombinedChannelDuplexHandler.java:420)\n        at io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:377)\n        at io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:342)\n        at io.netty.channel.CombinedChannelDuplexHandler.channelInactive(CombinedChannelDuplexHandler.java:223)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\n        at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)\n        at com.twitter.finagle.netty4.channel.ChannelStatsHandler.channelInactive(ChannelStatsHandler.scala:131)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\n        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1337)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\n        at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:916)\n        at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:744)\n        at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\n        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\n        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)\n        at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n        at com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run(BlockingTimeTrackingThreadFactory.scala:23)\n        at java.lang.Thread.run(Thread.java:748)\nE 0129 14:18:51.427 UTC THREAD26: retrying k8s request to /apis/l5d.io/v1alpha1/namespaces/my-namespace/dtabs on error io.buoyant.k8s.Watchable$Closed$\nW 130 14:18:51.516 UTC THREAD14 TraceId:44f1d79477bfaaa5: k8s failed to get resource at /apis/l5d.io/v1alpha1/namespaces/my-namespace/dtabs: Failure(io.buoyant.k8s.Watchable$Closed$, flags=0x02) with RemoteInfo -> Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: localhost/127.0.0.1:8001, Downstream label: /$/inet/localhost/8001, Trace Id: 7f9a5b33c94d0c63.7f9a5b33c94d0c63<:7f9a5b33c94d0c63\n``. @deebo91 yup I am running in Kubernetets1.8.5in Google Kubernetes Engine and running namerd1.3.5with a kubectl sidecar at version1.8.5`.\nHere is the configuration for namerd I am using:\nyaml\n    # Admin UI\n    admin:\n      ip: 0.0.0.0\n      port: 9991\n    # DTab Storage location (k8s custom resource definition)\n    storage:\n      kind: io.l5d.k8s\n      namespace: my-namespace\n      experimental: true\n    # Namers\n    namers:\n      # Kubernetes Namer\n      - kind: io.l5d.k8s\n        host: 127.0.0.1\n        port: 8001\n    # External Interfaces to namerd\n    interfaces:\n      - kind: io.l5d.thriftNameInterpreter\n        ip: 0.0.0.0\n        port: 4100\n      - kind: io.l5d.httpController\n        ip: 0.0.0.0\n        port: 4180\n      - kind: io.l5d.mesh\n        ip: 0.0.0.0\n        port: 4321\nHere is the full Deployment for namerd (via helm):\nyaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: \"1\"\n  creationTimestamp: 2018-01-29T15:41:39Z\n  generation: 1\n  labels:\n    app: namerd-namerd\n    chart: namerd-2018.1.29\n    heritage: Tiller\n    release: namerd\n  name: namerd-namerd\n  namespace: my-namespace\n  resourceVersion: \"4079447\"\n  selfLink: /apis/extensions/v1beta1/namespaces/my-namespace/deployments/namerd-namerd\n  uid: e5a93b3d-050a-11e8-bddc-42010a9a000c\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: namerd-namerd\n      release: namerd\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/config: d856cc693b32c2fd5cec2f6be7329b817f13f018d16083cd951ebda26778e72c\n      creationTimestamp: null\n      labels:\n        app: namerd-namerd\n        release: namerd\n    spec:\n      containers:\n      - args:\n        - /io.buoyant/namerd/config/config.yaml\n        image: buoyantio/namerd:1.3.5\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /api/1/dtabs\n            port: 4180\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: namerd-namerd\n        ports:\n        - containerPort: 4100\n          name: sync\n          protocol: TCP\n        - containerPort: 4180\n          name: api\n          protocol: TCP\n        - containerPort: 4321\n          name: mesh\n          protocol: TCP\n        - containerPort: 9991\n          name: admin\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /api/1/dtabs\n            port: 4180\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: 500m\n            memory: 512Mi\n          requests:\n            cpu: \"0\"\n            memory: 512Mi\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n        volumeMounts:\n        - mountPath: /io.buoyant/namerd/config\n          name: namerd-namerd-config\n          readOnly: true\n      - args:\n        - proxy\n        - -p\n        - \"8001\"\n        image: buoyantio/kubectl:v1.8.5\n        imagePullPolicy: IfNotPresent\n        name: kubectl\n        resources:\n          requests:\n            cpu: \"0\"\n            memory: 32Mi\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n      dnsPolicy: ClusterFirst\n      restartPolicy: Always\n      schedulerName: default-scheduler\n      securityContext: {}\n      serviceAccount: namerd\n      serviceAccountName: namerd\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: namerd-namerd-config\n        name: namerd-namerd-config\nstatus:\n  availableReplicas: 3\n  conditions:\n  - lastTransitionTime: 2018-01-29T15:42:51Z\n    lastUpdateTime: 2018-01-29T15:42:51Z\n    message: Deployment has minimum availability.\n    reason: MinimumReplicasAvailable\n    status: \"True\"\n    type: Available\n  observedGeneration: 1\n  readyReplicas: 3\n  replicas: 3\n  updatedReplicas: 3. Hi @deebo91 sorry for lack of responses, busy @ work.\nI haven't yet come across this again, but I will post here if I do.. ",
    "perrymanuk": "I would also +1 this, would make our development/stage environments a lot easier to integrate with our production.. @adleong pm'd it to you on slack ;-P. ",
    "frvge": "I've got a similar request where I want to pass through a static Host header to the service running on Nomad. It'd be nice if we can override headers, maybe with regex support, with a possibility to reference the original incoming headers.. ",
    "carloszuluaga": "@ccmtaylor fixed according to your code review.. I think the CI failure is not related to this changes, @siggy can you please take a look?\n. @siggy I don't have access either, the button is disabled with this message \"You need write permissions to trigger builds.\" . @hawkw thanks for the review. I agree. Defined a custom exception with the right meaning of the error. Let me know if this is ok.. @siggy wow! thank you so much :smiley:. my t-shirt size is M. @hawkw I think Excepcion is the most correct because in this case the error is not produced by failed or interrupted I/O operations.. ",
    "sawyerzhu": "We encounter same issue with version 1.3.3. @hawkw Thank you. Can you give me the direct download link for linkerd-1.3.3-SNAPSHOT-grpc? I will try. We do not use docker in test environment.. Let me build from that branch first.. ",
    "hollinwilkins": "Hey,\nI am seeing an issue similar to this, or perhaps it is different. For some reason, it is unclear why, certain linkerd instances will not see any endpoints are available for a service.\nHere is the error I get:\ncom.twitter.finagle.NoBrokersAvailableException: No hosts are available for /svc/aai-account.default, Dtab.base=[/ph=>/$/io.buoyant.rinet;/svc=>/ph/80;/svc=>/$/io.buoyant.porthostPfx/ph;/k8s=>/#/io.l5d.k8s.h2;/portNsSvc=>/#/portNsSvcToK8s;/host=>/portNsSvc/h2/default;/host=>/portNsSvc/h2;/svc=>/$/io.buoyant.http.domainToPathPfx/host], Dtab.local=[]. Remote Info: Not Available\nI only get this error on a single instance of linkerd from the daemonset, all other instances are able to communicate with aai-account.default.\nThe aai-account definitely has endpoints:\nkubectl get endpoints -l app=account\nNAME          ENDPOINTS          AGE\naai-account   10.52.5.217:5560   9m\nAfter deleting the pod, it comes back up, and is able to see the endpoints.\nservicemesh.yaml comes straight from the tutorial, except I enable zipkin and upgrade image to 1.3.4\n```\n\nLinkerd Service Mesh\n\nThis is a basic Kubernetes config file to deploy a service mesh of Linkerd\ninstances onto your Kubernetes cluster that is capable of handling HTTP,\nHTTP/2 and gRPC calls with some reasonable defaults.\n\nTo configure your applications to use Linkerd for HTTP traffic you can set the\nhttp_proxy environment variable to $(NODE_NAME):4140 where NODE_NAME is\nthe name of node on which the application instance is running.  The\nNODE_NAME environment variable can be set with the downward API.\n\nIf your application does not support the http_proxy environment variable or\nif you want to configure your application to use Linkerd for HTTP/2 or gRPC\ntraffic, you must configure your application to send traffic directly to\nLinkerd:\n\n* $(NODE_NAME):4140 for HTTP\n* $(NODE_NAME):4240 for HTTP/2\n* $(NODE_NAME):4340 for gRPC\n\nIf you are sending HTTP or HTTP/2 traffic directly to Linkerd, you must set\nthe Host/Authority header to <service> or <service>.<namespace> where\n<service> and <namespace> are the names of the service and namespace\nthat you want to proxy to.  If unspecified, <namespace> defaults to\ndefault.\n\nIf your application receives HTTP, HTTP/2, and/or gRPC traffic it must have a\nKubernetes Service object with ports named http, h2, and/or grpc\nrespectively.\n\nYou can deploy this to your Kubernetes cluster by running:\nkubectl create ns linkerd\nkubectl apply -n linkerd -f servicemesh.yml\n\nThere are sections of this config that can be uncommented to enable:\n* CNI compatibility\n* Automatic retries\n* Zipkin tracing\n\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: l5d-config\n  namespace: linkerd\ndata:\n  config.yaml: |-\n    admin:\n      ip: 0.0.0.0\n      port: 9990\n# Namers provide Linkerd with service discovery information.  To use a\n# namer, you reference it in the dtab by its prefix.  We define 4 namers:\n# * /io.l5d.k8s gets the address of the target app\n# * /io.l5d.k8s.http gets the address of the http-incoming Linkerd router on the target app's node\n# * /io.l5d.k8s.h2 gets the address of the h2-incoming Linkerd router on the target app's node\n# * /io.l5d.k8s.grpc gets the address of the grpc-incoming Linkerd router on the target app's node\nnamers:\n- kind: io.l5d.k8s\n- kind: io.l5d.k8s\n  prefix: /io.l5d.k8s.http\n  transformers:\n    # The daemonset transformer replaces the address of the target app with\n    # the address of the http-incoming router of the Linkerd daemonset pod\n    # on the target app's node.\n  - kind: io.l5d.k8s.daemonset\n    namespace: linkerd\n    port: http-incoming\n    service: l5d\n    # hostNetwork: true # Uncomment if using host networking (eg for CNI)\n- kind: io.l5d.k8s\n  prefix: /io.l5d.k8s.h2\n  transformers:\n    # The daemonset transformer replaces the address of the target app with\n    # the address of the h2-incoming router of the Linkerd daemonset pod\n    # on the target app's node.\n  - kind: io.l5d.k8s.daemonset\n    namespace: linkerd\n    port: h2-incoming\n    service: l5d\n    # hostNetwork: true # Uncomment if using host networking (eg for CNI)\n- kind: io.l5d.k8s\n  prefix: /io.l5d.k8s.grpc\n  transformers:\n    # The daemonset transformer replaces the address of the target app with\n    # the address of the grpc-incoming router of the Linkerd daemonset pod\n    # on the target app's node.\n  - kind: io.l5d.k8s.daemonset\n    namespace: linkerd\n    port: grpc-incoming\n    service: l5d\n    # hostNetwork: true # Uncomment if using host networking (eg for CNI)\n- kind: io.l5d.rewrite\n  prefix: /portNsSvcToK8s\n  pattern: \"/{port}/{ns}/{svc}\"\n  name: \"/k8s/{ns}/{port}/{svc}\"\n\n# Telemeters export metrics and tracing data about Linkerd, the services it\n# connects to, and the requests it processes.\ntelemetry:\n- kind: io.l5d.prometheus # Expose Prometheus style metrics on :9990/admin/metrics/prometheus\n- kind: io.l5d.recentRequests\n  sampleRate: 0.25 # Tune this sample rate before going to production\n- kind: io.l5d.zipkin # Uncomment to enable exporting of zipkin traces\n  host: zipkin # Zipkin collector address\n  port: 9410\n  sampleRate: 1.0 # Set to a lower sample rate depending on your traffic volume\n\n# Usage is used for anonymized usage reporting.  You can set the orgId to\n# identify your organization or set `enabled: false` to disable entirely.\nusage:\n  orgId: linkerd-examples-servicemesh\n\n# Routers define how Linkerd actually handles traffic.  Each router listens\n# for requests, applies routing rules to those requests, and proxies them\n# to the appropriate destinations.  Each router is protocol specific.\n# For each protocol (HTTP, HTTP/2, gRPC) we define an outgoing router and\n# an incoming router.  The application is expected to send traffic to the\n# outgoing router which proxies it to the incoming router of the Linkerd\n# running on the target service's node.  The incoming router then proxies\n# the request to the target application itself.  We also define HTTP and\n# HTTP/2 ingress routers which act as Ingress Controllers and route based\n# on the Ingress resource.\nrouters:\n- label: http-outgoing\n  protocol: http\n  servers:\n  - port: 4140\n    ip: 0.0.0.0\n  # This dtab looks up service names in k8s and falls back to DNS if they're\n  # not found (e.g. for external services). It accepts names of the form\n  # \"service\" and \"service.namespace\", defaulting the namespace to\n  # \"default\". For DNS lookups, it uses port 80 if unspecified. Note that\n  # dtab rules are read bottom to top. To see this in action, on the Linkerd\n  # administrative dashboard, click on the \"dtab\" tab, select \"http-outgoing\"\n  # from the dropdown, and enter a service name like \"a.b\". (Or click on the\n  # \"requests\" tab to see recent traffic through the system and how it was\n  # resolved.)\n  dtab: |\n    /ph  => /$/io.buoyant.rinet ;                     # /ph/80/google.com -> /$/io.buoyant.rinet/80/google.com\n    /svc => /ph/80 ;                                  # /svc/google.com -> /ph/80/google.com\n    /svc => /$/io.buoyant.porthostPfx/ph ;            # /svc/google.com:80 -> /ph/80/google.com\n    /k8s => /#/io.l5d.k8s.http ;                      # /k8s/default/http/foo -> /#/io.l5d.k8s.http/default/http/foo\n    /portNsSvc => /#/portNsSvcToK8s ;                 # /portNsSvc/http/default/foo -> /k8s/default/http/foo\n    /host => /portNsSvc/http/default ;                # /host/foo -> /portNsSvc/http/default/foo\n    /host => /portNsSvc/http ;                        # /host/default/foo -> /portNsSvc/http/default/foo\n    /svc => /$/io.buoyant.http.domainToPathPfx/host ; # /svc/foo.default -> /host/default/foo\n  client:\n    kind: io.l5d.static\n    configs:\n    # Use HTTPS if sending to port 443\n    - prefix: \"/$/io.buoyant.rinet/443/{service}\"\n      tls:\n        commonName: \"{service}\"\n\n- label: http-incoming\n  protocol: http\n  servers:\n  - port: 4141\n    ip: 0.0.0.0\n  interpreter:\n    kind: default\n    transformers:\n    - kind: io.l5d.k8s.localnode\n      # hostNetwork: true # Uncomment if using host networking (eg for CNI)\n  dtab: |\n    /k8s => /#/io.l5d.k8s ;                           # /k8s/default/http/foo -> /#/io.l5d.k8s/default/http/foo\n    /portNsSvc => /#/portNsSvcToK8s ;                 # /portNsSvc/http/default/foo -> /k8s/default/http/foo\n    /host => /portNsSvc/http/default ;                # /host/foo -> /portNsSvc/http/default/foo\n    /host => /portNsSvc/http ;                        # /host/default/foo -> /portNsSvc/http/default/foo\n    /svc => /$/io.buoyant.http.domainToPathPfx/host ; # /svc/foo.default -> /host/default/foo\n\n- label: h2-outgoing\n  protocol: h2\n  experimental: true\n  servers:\n  - port: 4240\n    ip: 0.0.0.0\n  dtab: |\n    /ph  => /$/io.buoyant.rinet ;                       # /ph/80/google.com -> /$/io.buoyant.rinet/80/google.com\n    /svc => /ph/80 ;                                    # /svc/google.com -> /ph/80/google.com\n    /svc => /$/io.buoyant.porthostPfx/ph ;              # /svc/google.com:80 -> /ph/80/google.com\n    /k8s => /#/io.l5d.k8s.h2 ;                          # /k8s/default/h2/foo -> /#/io.l5d.k8s.h2/default/h2/foo\n    /portNsSvc => /#/portNsSvcToK8s ;                   # /portNsSvc/h2/default/foo -> /k8s/default/h2/foo\n    /host => /portNsSvc/h2/default ;                    # /host/foo -> /portNsSvc/h2/default/foo\n    /host => /portNsSvc/h2 ;                            # /host/default/foo -> /portNsSvc/h2/default/foo\n    /svc => /$/io.buoyant.http.domainToPathPfx/host ;   # /svc/foo.default -> /host/default/foo\n  client:\n    kind: io.l5d.static\n    configs:\n    # Use HTTPS if sending to port 443\n    - prefix: \"/$/io.buoyant.rinet/443/{service}\"\n      tls:\n        commonName: \"{service}\"\n\n- label: h2-incoming\n  protocol: h2\n  experimental: true\n  servers:\n  - port: 4241\n    ip: 0.0.0.0\n  interpreter:\n    kind: default\n    transformers:\n    - kind: io.l5d.k8s.localnode\n      # hostNetwork: true # Uncomment if using host networking (eg for CNI)\n  dtab: |\n    /k8s => /#/io.l5d.k8s ;                             # /k8s/default/h2/foo -> /#/io.l5d.k8s/default/h2/foo\n    /portNsSvc => /#/portNsSvcToK8s ;                   # /portNsSvc/h2/default/foo -> /k8s/default/h2/foo\n    /host => /portNsSvc/h2/default ;                    # /host/foo -> /portNsSvc/h2/default/foo\n    /host => /portNsSvc/h2 ;                            # /host/default/foo -> /portNsSvc/h2/default/foo\n    /svc => /$/io.buoyant.http.domainToPathPfx/host ;   # /svc/foo.default -> /host/default/foo\n\n- label: grpc-outgoing\n  protocol: h2\n  experimental: true\n  servers:\n  - port: 4340\n    ip: 0.0.0.0\n  identifier:\n    kind: io.l5d.header.path\n    segments: 1\n  dtab: |\n    /hp  => /$/inet ;                                # /hp/linkerd.io/8888 -> /$/inet/linkerd.io/8888\n    /svc => /$/io.buoyant.hostportPfx/hp ;           # /svc/linkerd.io:8888 -> /hp/linkerd.io/8888\n    /srv => /#/io.l5d.k8s.grpc/default/grpc;         # /srv/service/package -> /#/io.l5d.k8s.grpc/default/grpc/service/package\n    /svc => /$/io.buoyant.http.domainToPathPfx/srv ; # /svc/package.service -> /srv/service/package\n  client:\n    kind: io.l5d.static\n    configs:\n    # Always use TLS when sending to external grpc servers\n    - prefix: \"/$/inet/{service}\"\n      tls:\n        commonName: \"{service}\"\n\n- label: gprc-incoming\n  protocol: h2\n  experimental: true\n  servers:\n  - port: 4341\n    ip: 0.0.0.0\n  identifier:\n    kind: io.l5d.header.path\n    segments: 1\n  interpreter:\n    kind: default\n    transformers:\n    - kind: io.l5d.k8s.localnode\n      # hostNetwork: true # Uncomment if using host networking (eg for CNI)\n  dtab: |\n    /srv => /#/io.l5d.k8s/default/grpc ;             # /srv/service/package -> /#/io.l5d.k8s/default/grpc/service/package\n    /svc => /$/io.buoyant.http.domainToPathPfx/srv ; # /svc/package.service -> /srv/service/package\n\n# HTTP Ingress Controller listening on port 80\n- protocol: http\n  label: http-ingress\n  servers:\n    - port: 80\n      ip: 0.0.0.0\n      clearContext: true\n  identifier:\n    kind: io.l5d.ingress\n  dtab: /svc => /#/io.l5d.k8s\n\n# HTTP/2 Ingress Controller listening on port 8080\n- protocol: h2\n  experimental: true\n  label: h2-ingress\n  servers:\n    - port: 8080\n      ip: 0.0.0.0\n      clearContext: true\n  identifier:\n    kind: io.l5d.ingress\n  dtab: /svc => /#/io.l5d.k8s\n\n\napiVersion: extensions/v1beta1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: l5d\n  name: l5d\n  namespace: linkerd\nspec:\n  template:\n    metadata:\n      labels:\n        app: l5d\n    spec:\n      # hostNetwork: true # Uncomment to use host networking (eg for CNI)\n      volumes:\n      - name: l5d-config\n        configMap:\n          name: \"l5d-config\"\n      containers:\n      - name: l5d\n        image: buoyantio/linkerd:1.3.4\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        args:\n        - /io.buoyant/linkerd/config/config.yaml\n        ports:\n        - name: http-outgoing\n          containerPort: 4140\n          hostPort: 4140\n        - name: http-incoming\n          containerPort: 4141\n        - name: h2-outgoing\n          containerPort: 4240\n          hostPort: 4240\n        - name: h2-incoming\n          containerPort: 4241\n        - name: grpc-outgoing\n          containerPort: 4340\n          hostPort: 4340\n        - name: grpc-incoming\n          containerPort: 4341\n        - name: http-ingress\n          containerPort: 80\n        - name: h2-ingress\n          containerPort: 8080\n        volumeMounts:\n        - name: \"l5d-config\"\n          mountPath: \"/io.buoyant/linkerd/config\"\n          readOnly: true\n  # Run `kubectl proxy` as a sidecar to give us authenticated access to the\n  # Kubernetes API.\n  - name: kubectl\n    image: buoyantio/kubectl:v1.8.5\n    args:\n    - \"proxy\"\n    - \"-p\"\n    - \"8001\"\n\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: l5d\n  namespace: linkerd\nspec:\n  selector:\n    app: l5d\n  type: LoadBalancer\n  ports:\n  - name: http-ingress\n    port: 80\n  - name: h2-ingress\n    port: 8080\n```\nKubernetes version: 1.8.4-gke.0. ",
    "bourquep": "For the record, I am experiencing a similar issue, described in this thread on Discourse. Some observations:\n\n\nI see quite a few k8s restarting watch on /api/v1/watch/namespaces/studyo/endpoints/generator, resource version None was too old in the logs, but it seems like these show up even during periods where my service is accessible, so it may not be related to the issue. However,  resource version None seems suspicious...\n\n\nWhen the issue happens, I see a ton of these in the logs:\nretrying k8s request to /api/v1/namespaces/studyo/endpoints/generator on error Failure(Connection refused: localhost/127.0.0.1:8001 at remote address: localhost/127.0.0.1:8001. Remote Info: Not Available, flags=0x08) with RemoteInfo -> Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: localhost/127.0.0.1:8001, Downstream label: client\nAnd some (not as many) of these:\nException propagated to the default monitor (upstream address: /10.8.0.168:42958, downstream address: localhost/127.0.0.1:8001, label: client).\nIf I am not mistaken, 127.0.0.1:8001 is the address of the kubectl proxy, so it looks like linkerd is unable to communicate with kubectl, therefore unable to resolve services/endpoints\n\n\nI also see a few of these in the kubectl container logs: httputil: ReverseProxy read error during body copy: context canceled. But the timestamps don't correlate to the aforementioned connection errors to localhost:8001. \n\n\nAccording to https://github.com/golang/go/issues/20071, this context canceled error happens when a client prematurely abandon a proxied HTTP request. Not sure what to make of this with regards to how linkerd interacts with the k8s proxy, but thought it might be worth posting here for your consideration.\n. I've been running Linkerd on my production cluster for the past 2 weeks, and so far I had to restart the l5d pods only once (the issue described in this thread - l5d not being able to resolve gRPC endpoints - happened only once). The issue is still happening every few hours/days on my test cluster.\nBoth clusters are running the same k8s version on GKE, 3 nodes, same Linkerd version, same k8s deployment/service yaml files, same docker images... The only difference is that most of my deployments on test have a single pod, whereas I have dozens of pods per service on prod.\nCould this issue have something to do with l5d on a specific node not being able to resolve a service which doesn't have any pods running on the l5d node? . @siggy Status quo here. Our dev/test cluster exhibits this issue almost daily, while our prod cluster is much more immune to it (See my last comment). @hawkw Oh, that is interesting. We delete a lot of resources on our test cluster, because we have \"review\" deployments triggered by CI for each branch of each service, and once the branch is merged we delete the relevant k8s resources (service, deployment).\nSo this might explain why I am experiencing this issue almost daily on my test cluster, but almost never on my prod cluster, where k8s resources are updated but seldom deleted.. @hawkw Unfortunately, I have replaced Linkerd with Conduit on my test cluster, which is where I was experiencing this issue most of the time. If you feel like it would really help if I tested a new Linkerd version, I could probably bring it back into that cluster, but I'm kinda in love with Conduit now and I'd rather stick with it... ;). ",
    "pavel-mikhalchuk": "Hey!\nJust wanted to share some details on issue I had couple minutes ago. Unfortunately I didn't grab logs :(\nAfter another deployment of one of my apps in some namespace linkerd stopped serving requests. \nI have restarted linkerd and after some time I got this exception:\nE 0207 09:36:27.703 UTC THREAD16: retrying k8s request to /api/v1/namespaces/dev-dev/endpoints/s-portal-m-server on error com.twitter.io.Reader$ReaderDiscarded: This writer's reader has been discarded\nW 0207 09:36:27.718 UTC THREAD16 TraceId:adb9457471620bb5: Exception propagated to the default monitor (upstream address: /10.100.20.21:59600, downstream address: localhost/127.0.0.1:7001, label: client).\ncom.twitter.io.Reader$ReaderDiscarded: This writer's reader has been discarded\n    at com.twitter.finagle.netty4.http.StreamTransports$$anon$1.discard(StreamTransports.scala:71)\n    at com.twitter.finagle.http.DelayedReleaseService$$anon$2$$anon$3.discard(DelayedReleaseService.scala:58)\n    at com.twitter.finagle.http.DelayedReleaseService$$anon$2$$anon$3.discard(DelayedReleaseService.scala:58)\n    at io.buoyant.k8s.Watchable.$anonfun$watch$5(Watchable.scala:131)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)\n    at com.twitter.util.Try$.apply(Try.scala:15)\n    at com.twitter.util.Future$.apply(Future.scala:166)\n    at io.buoyant.k8s.Watchable.$anonfun$watch$4(Watchable.scala:131)\nBut if I curl localhost:7001/api/v1/namespaces/dev-dev/endpoints/s-portal-m-server manually on the server where linkerd is running (hostNetwork is set to true) I get:\n{\n  \"kind\": \"Endpoints\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {\n    \"name\": \"s-portal-m-server\",\n    \"namespace\": \"dev-dev\",\n    \"selfLink\": \"/api/v1/namespaces/dev-dev/endpoints/s-portal-m-server\",\n    \"uid\": \"e3a0e4dc-c156-11e7-bbdc-005056b554ea\",\n    \"resourceVersion\": \"17002723\",\n    \"creationTimestamp\": \"2017-11-04T11:54:18Z\",\n    \"labels\": {\n      \"env\": \"dev-dev\",\n      \"app\": \"portal-m\"\n    }\n  },\n  \"subsets\": [\n    {\n      \"addresses\": [\n        {\n          \"ip\": \"10.44.0.18\",\n          \"nodeName\": \"asvc-tst-pool1\",\n          \"targetRef\": {\n            \"kind\": \"Pod\",\n            \"namespace\": \"dev-dev\",\n            \"name\": \"d-portal-m-server-5b764dfb67-nphbf\",\n            \"uid\": \"839dcc96-0b56-11e8-bbdc-005056b554ea\",\n            \"resourceVersion\": \"17002722\"\n          }\n        }\n      ],\n      \"ports\": [\n        {\n          \"name\": \"http\",\n          \"port\": 8030,\n          \"protocol\": \"TCP\"\n        }\n      ]\n    }\n  ]\n}\nLinkerd kubectl sidecar is running on 7001. Could it be kubectl proxy an issue? I remember port forwarding is not working very stable and I'm getting 404 from time to time. \nThanks!. ",
    "yoitsro": "I'm still seeing this on 1.3.7. Can anyone recommend a short term fix/workaround in the meantime?\nIs it likely this could be fixed in 1.4.0?. Strangely enough, the disk usage seems to creep up and get to a certain threshold when eventually, it just stops serving all traffic:\n\n\n\nAre these just logs being stored to disk somewhere, which is what's causing the increased disk usage?. Anyone? :). @ejwood79 I'm about to jump ship as well. What are you using now?. Hey @dadjeibaah. Thanks for this. Out of curiosity, have you managed to test this against a remote IP too? Also, does this work with IPv6 and what would the header look like for IPv6 addresses?. @dadjeibaah I think that would suffice as an appropriate test. \nAnd yeah - I guess it's always good to have tests for something where people may depend on the format of this string. I know I'm gonna have to parse it in Node.js. Again, thank you :). I have the logs, but I'll send them to you via Slack.. Hey @adleong. I'm not sure where the disk usage is coming from. Next time an instance starts acting funny, I'll dig into it.. It looks like the issue is surfacing again, albeit with the requests eventually getting through. It seems as though the connections to the gRPC services die, and then upon the first subsequent request, they eventually get routed again. I think that over time, reconnections stop happening altogether.\nI found it interesting that the CPU usage of the kubectl pod increases, though not by much as far as actual CPU shares go!\n. I can't see anything. The disk usage increases rarely enough to make it difficult for me to track down.\nI've just seen this error surface though:\nI 0522 21:24:16.614 UTC THREAD10: Reaping /svc/hops.Organisations\nI 0522 21:24:16.643 UTC THREAD10: Reaping %/io.l5d.k8s.localnode/10.8.2.177/#/io.l5d.k8s/production-public/grpc/organisations\nW 0522 21:24:34.426 UTC THREAD17 TraceId:4aeff0294f9d06d6: Exception propagated to the default monitor (upstream address: /10.154.0.2:51653, downstream address: localhost/127.0.0.1:8001, label: client).\ncom.twitter.io.Reader$ReaderDiscarded: This writer's reader has been discarded\n    at com.twitter.finagle.netty4.http.StreamTransports$$anon$1.discard(StreamTransports.scala:71)\n    at com.twitter.finagle.http.DelayedReleaseService$$anon$2$$anon$3.discard(DelayedReleaseService.scala:58)\n    at com.twitter.finagle.http.DelayedReleaseService$$anon$2$$anon$3.discard(DelayedReleaseService.scala:58)\n    at io.buoyant.k8s.Watchable.$anonfun$watch$5(Watchable.scala:133)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)\n    at com.twitter.util.Try$.apply(Try.scala:15)\n    at com.twitter.util.Future$.apply(Future.scala:166)\n    at io.buoyant.k8s.Watchable.$anonfun$watch$4(Watchable.scala:133)\n    at com.twitter.util.Closable$$anon$5.close(Closable.scala:144)\n    at com.twitter.util.Closable$$anon$6.close(Closable.scala:148)\n    at com.twitter.util.Var$$anon$4.$anonfun$closable$1(Var.scala:399)\n    at com.twitter.util.Closable$$anon$5.close(Closable.scala:144)\n    at com.twitter.util.Closable$.com$twitter$util$Closable$$safeClose(Closable.scala:67)\n    at com.twitter.util.Closable$$anon$3.closeSeq(Closable.scala:125)\n    at com.twitter.util.Closable$$anon$3.onTry$1(Closable.scala:122)\n    at com.twitter.util.Closable$$anon$3.closeSeq(Closable.scala:128)\n    at com.twitter.util.Closable$$anon$3.close(Closable.scala:134)\n    at com.twitter.util.Closable$$anon$6.close(Closable.scala:148)\n    at io.buoyant.k8s.Watchable.$anonfun$activity$10(Watchable.scala:299)\n    at com.twitter.util.Closable$$anon$5.close(Closable.scala:144)\n    at com.twitter.util.Var$$anon$4.$anonfun$closable$1(Var.scala:399)\n    at com.twitter.util.Closable$$anon$5.close(Closable.scala:144)\n    at com.twitter.util.Closable$.com$twitter$util$Closable$$safeClose(Closable.scala:67)\n    at com.twitter.util.Closable$$anon$3.closeSeq(Closable.scala:125)\n    at com.twitter.util.Closable$$anon$3.onTry$1(Closable.scala:122)\n    at com.twitter.util.Closable$$anon$3.closeSeq(Closable.scala:128)\n    at com.twitter.util.Closable$$anon$3.close(Closable.scala:134)\n    at com.twitter.util.Closable$.com$twitter$util$Closable$$safeClose(Closable.scala:67)\n    at com.twitter.util.Closable$$anon$3.closeSeq(Closable.scala:125)\n    at com.twitter.util.Closable$$anon$3.close(Closable.scala:134)\n    at com.twitter.util.Var$$anon$4.$anonfun$closable$1(Var.scala:399)\n    at com.twitter.util.Closable$$anon$5.close(Closable.scala:144)\n    at com.twitter.util.Closable$.com$twitter$util$Closable$$safeClose(Closable.scala:67)\n    at com.twitter.util.Closable$$anon$3.closeSeq(Closable.scala:125)\n    at com.twitter.util.Closable$$anon$3.onTry$1(Closable.scala:122)\n    at com.twitter.util.Closable$$anon$3.closeSeq(Closable.scala:128)\n    at com.twitter.util.Closable$$anon$3.close(Closable.scala:134)\n    at com.twitter.util.Closable$.com$twitter$util$Closable$$safeClose(Closable.scala:67)\n    at com.twitter.util.Closable$$anon$3.closeSeq(Closable.scala:125)\n    at com.twitter.util.Closable$$anon$3.close(Closable.scala:134)\n    at com.twitter.util.Closable$.com$twitter$util$Closable$$safeClose(Closable.scala:67)\n    at com.twitter.util.Closable$$anon$3.closeSeq(Closable.scala:125)\n    at com.twitter.util.Closable$$anon$3.close(Closable.scala:134)\n    at com.twitter.util.Closable$.com$twitter$util$Closable$$safeClose(Closable.scala:67)\n    at com.twitter.util.Closable$$anon$3.closeSeq(Closable.scala:125)\n    at com.twitter.util.Closable$$anon$3.close(Closable.scala:134)\n    at com.twitter.util.Closable$$anon$6.close(Closable.scala:148)\n    at com.twitter.util.Closable$.com$twitter$util$Closable$$safeClose(Closable.scala:67)\n    at com.twitter.util.Closable$$anon$3.closeSeq(Closable.scala:125)\n    at com.twitter.util.Closable$$anon$3.onTry$1(Closable.scala:122)\n    at com.twitter.util.Closable$$anon$3.closeSeq(Closable.scala:128)\n    at com.twitter.util.Closable$$anon$3.close(Closable.scala:134)\n    at com.twitter.util.Closable$$anon$6.close(Closable.scala:148)\n    at com.twitter.util.Closable$.com$twitter$util$Closable$$safeClose(Closable.scala:67)\n    at com.twitter.util.Closable$$anon$3.closeSeq(Closable.scala:125)\n    at com.twitter.util.Closable$$anon$3.onTry$1(Closable.scala:122)\n    at com.twitter.util.Closable$$anon$3.closeSeq(Closable.scala:128)\n    at com.twitter.util.Closable$$anon$3.close(Closable.scala:134)\n    at com.twitter.util.Closable$$anon$6.close(Closable.scala:148)\n    at com.twitter.util.Closable$.com$twitter$util$Closable$$safeClose(Closable.scala:67)\n    at com.twitter.util.Closable$$anon$3.closeSeq(Closable.scala:125)\n    at com.twitter.util.Closable$$anon$3.onTry$1(Closable.scala:122)\n    at com.twitter.util.Closable$$anon$3.closeSeq(Closable.scala:128)\n    at com.twitter.util.Closable$$anon$3.close(Closable.scala:134)\n    at com.twitter.util.Closable$$anon$6.close(Closable.scala:148)\n    at com.twitter.util.Closable$.com$twitter$util$Closable$$safeClose(Closable.scala:67)\n    at com.twitter.util.Closable$$anon$3.closeSeq(Closable.scala:125)\n    at com.twitter.util.Closable$$anon$3.onTry$1(Closable.scala:122)\n    at com.twitter.util.Closable$$anon$3.closeSeq(Closable.scala:128)\n    at com.twitter.util.Closable$$anon$3.close(Closable.scala:134)\n    at com.twitter.util.Closable.close(Closable.scala:20)\n    at com.twitter.util.Closable.close$(Closable.scala:20)\n    at com.twitter.util.Closable$$anon$3.close(Closable.scala:101)\n    at com.twitter.util.Closable$$anon$1.run(Closable.scala:161)\nW 0522 21:24:34.426 UTC THREAD17: retrying k8s request to /api/v1/namespaces/production-public/endpoints/organisations on error com.twitter.io.Reader$ReaderDiscarded: This writer's reader has been discarded\nhttputil: ReverseProxy read error during body copy: context canceled\nDoes that mean anything to you?. Fair enough. At the moment, I'm struggling to track down the source of the increasing disk usage. If anyone else is experiencing a similar issue, I wonder if they've seen any increased disk usage.. Linkerd has stopped routing requests again. I'm happy to leave my application in a broken state for a while if it means someone can take a look. Who's the lucky winner? \ud83d\ude04. @adleong I've tried but there's nothing significant enough in size for me to track it down. Where does linkerd write its logs?. That makes sense! So the disk usage can't be coming from that. It's there anywhere else linkerd writes to disk that I could check?. I'm not. And I guess if I add it now, it'll require a restart and the problem will go away, right?. @wmorgan that option is off. I'll check out that thread in just a moment.\nAs far as the disk usage goes, it looks like linkerd's failures are not correlated to a particular amount of disk usage. For instance, these are the graphs for the current failing daemonset (comprised of 3 instances of linkerd):\n\nThe drops in all usage is when I've restarted the daemonset.. root@l5d-linkerd-5c6k2:/io.buoyant/linkerd/1.3.7# df\nFilesystem     1K-blocks     Used Available Use% Mounted on\noverlay         47259264 26238264  21004616  56% /\ntmpfs            7702980        0   7702980   0% /dev\ntmpfs            7702980        0   7702980   0% /sys/fs/cgroup\n/dev/sda1       47259264 26238264  21004616  56% /etc/hosts\nshm                65536        0     65536   0% /dev/shm\ntmpfs            7702980        8   7702972   1% /io.buoyant/linkerd/certs\ntmpfs            7702980       12   7702968   1% /run/secrets/kubernetes.io/serviceaccount\ntmpfs            7702980        0   7702980   0% /sys/firmware\nroot@l5d-linkerd-5c6k2:/# du | sort -n -r\ndu: cannot access './proc/3246/task/3246/fd/4': No such file or directory\ndu: cannot access './proc/3246/task/3246/fdinfo/4': No such file or directory\ndu: cannot access './proc/3246/fd/3': No such file or directory\ndu: cannot access './proc/3246/fdinfo/3': No such file or directory\n734128  .\n630440  ./usr\n338152  ./usr/lib\n262828  ./usr/share\n167900  ./usr/lib/x86_64-linux-gnu\n116780  ./usr/lib/jvm\n116772  ./usr/lib/jvm/java-8-openjdk-amd64\n116744  ./usr/lib/jvm/java-8-openjdk-amd64/jre\n116168  ./usr/lib/jvm/java-8-openjdk-amd64/jre/lib\n101236  ./usr/share/locale\n59124   ./usr/share/icons\n47692   ./io.buoyant\n47688   ./io.buoyant/linkerd\n47664   ./io.buoyant/linkerd/1.3.7\n41588   ./usr/share/icons/Adwaita\n37300   ./usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64\n33076   ./usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server\n32968   ./usr/share/doc\n30888   ./usr/share/vim\n30868   ./usr/share/vim/vim80\n30580   ./var\n28392   ./var/lib\n27260   ./usr/bin\n24516   ./usr/lib/python3.5\n20568   ./usr/lib/python2.7\n16844   ./usr/lib/x86_64-linux-gnu/dri\n15948   ./usr/share/icons/gnome\n15828   ./var/lib/apt\n15792   ./var/lib/apt/lists\n13160   ./lib\n12852   ./lib/x86_64-linux-gnu\n12456   ./var/lib/dpkg\n11780   ./usr/share/icons/Adwaita/cursors\n11688   ./var/lib/dpkg/info\n10028   ./usr/share/fonts\n10024   ./usr/share/fonts/truetype\n10020   ./usr/share/fonts/truetype/dejavu\n.\n.\n.. I would agree. At the moment, the daemonset is still broken. I'm more than happy to grant temporary access to our set up if it'd help someone investigate. . Here are the logs from the pod:\nkubectl.log.zip\nHere are the metrics:\nmetrics.json.zip\nHere's a trace from the iOS app:\nError Domain=io.grpc Code=16 \"Received RST_STREAM with error code 2\" UserInfo={io.grpc.HeadersKey={\n}, NSLocalizedDescription=Received RST_STREAM with error code 2, io.grpc.TrailersKey={\n    \"l5d-success-class\" = \"1.0\";\n    via = \"h2 linkerd, h2 linkerd\";\n}}. That is the address of my Kubernetes API server. That explains a lot, but here it is with timestamps:\nkubectl.log.zip\nThey seem to occur frequently, but then disappear quickly. Would it be helpful to see more recent logs?. I'm not certain with these specific logs, but I've just been back to a time when I can remember linkerd failing, and there are none of the logs complaining about not being able to reach the API server.. So I'm taking a look at the dashboard now and I can see that requests are being successfully received by linkerd, they're being received by the application, but then the application calls to linkerd again to reach the internal microservices contained in a different namespace. It seems to be at this point that the failure occurs.\nThe first request returns a 14 UNAVAILABLE: TCP Read failed error.\nSubsequent requests return a Received RST_STREAM with error code 2 error.\nSo just to clarify, a typical request lifecycle would be as follows:\n| external world | public linkerd ingress | public namespace | internal linkerd ingress | internal namespace |\n| --- | --- | --- | --- | --- |\n| give me data from service A | | | | |\n| | route this request to service A in public namespace | | | |\n| | | get data from internal service X | | |\n| | | | route this request to service X in internal namespace | |\n| | | | | retrieve data from database, manipulate it and return it |\n| | | | <--- | |\n| | | manipulate retrieved data and combine with other sources if necessary | | |\n| | <--- | | | |\n| handle data | | | | |\n. I can also see that the first failed request manages to reach the internal linkerd service, but subsequent requests don't seem to reach the internal service.\nIs this an application level thing, or is this behaviour a result of linkerd deciding that because of the first failure, it's not going to try to route subsequent requests?. I've just been continually hitting the services and eventually, each service recovers. However, the services which have not been touched are still inaccessible.\nHere are the logs:\nl5d.log.zip\n . Here are the logs from the kubectl instances:\nkubectl.log.zip\n. And now requests to the services which were working are now failing again. Repeatedly requesting the services seems to eventually restore connectivity.. After chatting with @adleong, I've upgraded linkerd to version 1.4.1 and upgraded the kubectl sidecar to at least 1.8.5. That should allow for the new diagnostics metrics and perhaps some stability improvements.\nI'll keep you updated. Thanks for your support and help so far.. An update:\nThings seem much more stable. 5 days in and I've not had any problems yet. The high level metrics look good too:\n\nMemory usage seems more stable and disk usage seems more stable (though I suspect this is down to Linkerd 1.4.1's reduced logging).\nI can't explain the CPU 'spikes', but to be honest, the size of the spikes are relatively insignificant so I'm not worrying about it. Once it gets to 7 days with no problems, I'm happy to close this issue.. It's looking good so far. Happy to close this but will let you know if we experience anything else strange. Thank you all!. An update - this kept happening. I've migrated to linkerd2 now, but it's really strange behaviour. Really wish I could have gotten to the bottom of it, but never mind!. ",
    "ejwood79": "We actually just stopped using linkerd altogether due to this issue.. Starting to look at istio.. It was the issue with linkerd talking to pods that are no longer scheduled. We've since removed linkerd from our cluster, though. It was on kubernetes 1.7.. ",
    "jackkleeman": "What would you prefer @adleong? The first seems simpler but it's hard to estimate how many requests we need to accommodate. OK, I can write a PR now setting it to 1000?. ",
    "fangel": "Please note that this is the first time I've ever touched Scala, so there are likely tons of non-idiomatic Scala-code in there. But I welcome any hints on what is considered better Scala-code!. So a little point for discussion: Right now the namer will route to the Docker-local ip's of the services (and the internal port of the service, e.g. 80) \u2013 so each instance has it's own ip. This enables services to have a higher scale than the number of Rancher-hosts in your cluster.\nThe other alternative is to route to the ip of the Rancher-host, and the port that the internal port got mapped to (e.g. 80 in the container maps to 60001 externally on the host).\nThe reason I haven't done it this way, is because Rancher has a (weird?) idea of always wanting the external port of all instances to be the same across all insatnces- and since two instances on the same host can't share an external port, this limits the number of instances of your service to the number of physical hosts in your cluster.\nHowever, I was just looking through some of the examples of using Linkerd-to-Linkerd, and they rely on a localhost-transformation to only send traffic to the instances running on the same machine. My understanding would be that if the routes are to the virtual Docker-local ip's, then the localhost transformation wouldn't be able to find the services, since their ip's aren't the same as the linkerd-instance, even if run on the same host. So for a similar setup to work in Rancher, I think the routing should go to the external port on the Rancher-hosts - no?\nThe question is, which mode is the more desirable. An alternative could be to offer a switch in the configuration between the two modes. Then if you need to scale to more instances than hosts (if your application can't utilise more than one core, and your hosts are multi-cored), then you just can't use the same setup for doing linkerd-to-linkerd.. Thanks @adleong - I'll take a look at it when I get a bit of free time.. Thank you so much for your suggestions \u2013 they've been great! I'm slowly working my way through all of them \u2013 right now I still have to address the input about the long-polling loop in the RancherClient-class (i.e., the stuff about modifying it into a @volatile var v: Option[String] etc).\nBut I've successfully added a custom deserialiser which can verify the format of the port-binding info before constructing an instance of the case-class. I've also added a unit-test to ensure that the parser actually understands the JSON returned from the API.\nOnce I've addressed the last few comments, and run the corrected code on our Rancher-cluster, I'll push all the changes up for you to see.. Okay, I've pushed my changes. Please take a look - I've tried to address all of your comments (except making the api-endpoint configurable - see the line-comment above for more info).. A belated Merry Christmas / Happy Holidays to you guys and girls! I've addressed the latest comments now, so please take another look at this now.\nI also found and removed a rather unfortunate equivalent-to-infinite-loop situation stemming from the String to Option[String] refactoring in the long-polling. Basically it sent the wrong version name (it sent Some([version]) leading it to instantly return with the same version. This would just make it repeat itself over and over again. Our ops-team was a little unhappy with the amount of log-data that generated. Oops. But that's fixed now.\nThe CI tests failed on one of the two builds on an error in the NetworkedInteropTest.cancel_after_begin which I don't quite understand, and I haven't been anywhere near that. So if anyone has any hints as to why that could be, that would be much appreciated. . I can totally make it configurable if you would prefer that - but according to the documentation for the API, it will always be available at the magical host of \"rancher-metadata\" on port 80: http://rancher.com/docs/rancher/latest/en/rancher-services/metadata-service/. Agreed. I wouldn't wish it upon anyone to work on the Rancher Metadata-API. It's wrong in so many ways, and this long-polling API isn't even mentioned in their documentation.\nBut yes, they use dates as their API-versions, and the current API actually also mentions a version (eh, date) that isn't mentioned in the documentation so there is no way of knowing what you would get if you were to use that version). Ah, thanks for the hint about val being unnecessary. I'll definitely clean that up, then.. I'll look into doing a custom deserialiser first, but if that doesn't work out I'll definitely look into improving the constructor this way.\nI had a lot of issues trying to overload the constructor of a case-class so I could use a regex, so to make progress I added the very ugly kludge to make some progress. My recollection was that the constructor for case-classes had to start with a call to this(...), but I might be mistaken.. Probably a good idea. Since the long-polling API isn't even documented from Rancher's side, I have no idea how long of a wait it actually supports - so making it configurable makes a lot of sense. I'll do that.. Are there any rules for how many imports you can do from each package before switching to xyz._?\nRight now I explicitly name 8 different classes from com.twitter.utils because I wanted to actually know where each class came from as I was figuring out how stuff worked. But I guess I could just as well import com.twitter.utils._. Yes - that .transform seems to do exactly the same as .liftToTry.flatMap, just in a way more concise way. Thanks!. Clearly, I need to get better about fully understanding flatMap, because I think you are right.\nRight now I use the unlift-method to turn toAddr into a partial function that only returns something when it returns Some(address), and then collect to do a filter/map-combo which maps the partial function and filters out all the ones where it wasn't defined (i.e. the ones that would have had a None returned from toAddr.\nI thought my way was fairly elegant \u2013 but I agree that flatMap aught to do the same and in a even shorter way. I just hadn't quite gotten that Option is traversable too, and returns either no or a single element, which can then be flattened into desired list too.. Just leaving this here until I have the time to fix it: ${v} needs to be updated after v changed from String to a Option[String] - so probably something like ${v.getOrElse('initial')}.. You are very right. While trying to figure out how stuff worked, I found it useful to add in extra typo annotations to make sure my assumptions about types held up.... ",
    "dadadom": "Finally managed to do that, @siggy . Thanks for the hint!. ",
    "nikolay-pshenichny": "nit: this PR adds '/client_state.json', but in the 1.3.5 release info the endpoint is '/admin/client_state.json`. Maybe a little bit late for this, but here is a Vagrantfile & a script that allow to reproduce the issue - https://github.com/nikolay-pshenichny/linkerd-issue-1781\nThose \"scripts\" set the fs.inotify.max_user_watches to zero in order to emulate the scenario when OS is low on the \"inotify watches\"\n\nIt would also be good to know if this issue occurs when the fs namer is removed from the config entirely.\n\nWe didn't see the same behavior without the fs namer. Everything worked fine without it in our tests.\n. @adleong , thank you for the fix!\nI did some testing with the vagrant setup that I shared and, separately, with docker-compose.\nDon't see any leaking inotify instances anymore \ud83d\udc4d \nA few observations/notes:\n\nThe error message appears twice in the stdout (saw this in Vagrant):\n\nE 0123 18:57:31.390 UTC THREAD26 TraceId:39888a114f71b5b5: service failure: Failure(User limit of inotify watches reached, flags=0x100000000) with NoSources\nE 0123 18:57:31.393 UTC THREAD26 TraceId:c571cb9d077a0c70: service failure: Failure(User limit of inotify watches reached, flags=0x100000000) with NoSources\n\nEvaluation of the alternative paths in dtabs will be terminated if the error will occur (ie.: when /svc => /fs# | /zk#;, then zk will never be reached).\n   Afaik, this is an expected behaviour from the DTAB resolution perspective (pls. correct me if i am wrong). . @adleong , thx for the update!\nI haven't done much testing around Http1 and won't be able to confirm that part.\nBut with Http2 - yes, we are seeing a single connection being present after a single request went through... That connection is never closed.\n\n\nMy recommendation for a fix would be to change the singleton connection pool that is used by HTTP/2 to close the singleton connection after it has been idle for idleTimeMs. What do you think?\n\nFrom your description, it sounded like Http1 pool is doing a similar job - ie. the pool will only close the excess connections after the timeout and will hold all other connections. So the Singleton Pool appears to be working as designed. And since it is defined in Finagle-Core, I am not sure I completely understand the idea of fixing it.\nFrom the design perspective of Linkerd, should the connection pools be responsible for tracking and closing idle connections, or is it ok for the connections to expire on their own?\nI did some digging and found that com.twitter.finagle.buoyant.H2.Client supports ClientSession params underneath. A change in the io.buoyant.linkerd.protocol.H2ClientConfig to support ExpiringService params appears to be working properly for our use case. But i haven't had a chance to look at the Http1 side yet.\n```\ntrait H2ClientConfig extends ClientConfig with H2EndpointConfig {\n  var forwardClientCert: Option[Boolean] = None\n  var lifeTimeMs: Option[Int] = None // ExpiringService properties for client connections\n  var idleTimeMs: Option[Int] = None // ExpiringService properties for client connections\n@JsonIgnore\n  override def params(vars: Map[String, String]): Stack.Params = {\n    val result = withEndpointParams(super.params(vars))\n      .maybeWith(forwardClientCert.map(ForwardClientCertFilter.Enabled))\n      .maybeWith(Some(new ExpiringService.Param(\n        lifeTime = lifeTimeMs.map(x => Duration.fromMilliseconds(x)).orElse(Some(Duration.Top)).get,\n        idleTime = idleTimeMs.map(x => Duration.fromMilliseconds(x)).orElse(Some(Duration.Top)).get\n      )))\nresult\n\n}\n}\n```\nI can prepare a PR with the ^ change, to give you more context on the scope of the change; or I can try looking at the connection pool changes, but will need more context on that.. @adleong , preliminary PR - https://github.com/nikolay-pshenichny/linkerd/pull/1/files\nI want to clean the things up a little and expand the support to H1. After that I will create a PR against the official repository.\nPls let me know if I am moving in the right direction. . @adleong , I cleaned up the PR that I had. Here is the latest version - https://github.com/linkerd/linkerd/pull/1903\nPlease take a look when you have a chance. Thx!. @adleong , could you please tell me when is the next scheduled release date for Linkerd (1.3.8) ? \nWe are trying to decide if we need to apply a temporary workaround or wait till the next release  (assuming that all review comments are addressed and there are not issues with the PR).\nThank you.. > My goal is to cut a release candidate this week, get some folks to test it, and hopefully do a full release sometime next week. If you're able to help us test the RC once it's ready, that would be super helpful!\n@adleong , what is required for testing ?\nI am going to be away for a few days, but other people from my team can help with the tests.\n. @adleong , done.. @adleong , I will look into that a little later today.. @adleong , resolved. @dadjeibaah , I can look into adding more tests for the scenarios when both configurations are used. But I will be able to do that only starting Tuesday (24th) next week.. ",
    "mmrozek": "@adleong, Why do you want to use reference counting? I am not sure what you have in mind. I thought about that and in my opinion solution similar like in inactive cache will be enough. I could change the implementation of active cache and add a timeout. What do you think? . @adleong, what should be the default value of the TTL? I could take care of this issue.. @adleong I am not sure if it is the best option. Users rely on default values (especially when they don't know which value should be used). It could cause hard to find performance issues (users may don't realise that they have the problem in this place). Perhaps better idea is to set default to some long (but not infinite) time. Ex. if ttl unit will be seconds, we could set default to 604800 - 1 week. What do you think?. I made a PR - https://github.com/linkerd/linkerd/pull/1923. @adleong if you decide that you prefer different behaviour by default, I will change that.. The problem is caused by fact that - is treated as a flag.\nIn my opinion mixing flags with the parameters without context is not the best idea (at the moment is possible to run sth like:  ./linkerd-1.3.5-32b-exec -log.append=true linkerd/examples/http.yaml -log.level=INFO - and it is not clear what linkerd/examples/http.yaml means).\nMaybe better solution will be to treat config path as a flag also? E.g. ./linkerd-1.3.5-32b-exec -config=linkerd/examples/http.yaml or ./linkerd-1.3.5-32b-exec -config=stdin. @rmars, what do you think about that?. @rmars, @adleong, I could take care of this issue but I need some guidance. How should it look like?. I could create a PR against twitter-util repo but I am still not sure if it is the best idea. In that case, we will have a - between other flags and it could be unreadable. E.g. ./linkerd-1.3.5-32b-exec -log.append=true - -log.level=INFO. Maybe the better solution is to use an alphanumeric alias? If we use stdin instead of - it will be possible to reasoning about this argument and we will preserve backwards compatibility.\nBtw, your workaround is working.. Thanks @adleong. Sorry for late response. I tested locally \"Hourly\" and variations of \"1.kilobyte\".. @adleong, this PR fix a simple bug about autoloading tracers. Could you check why CI tests failed? There is some problem with dependencies but I don't think it is connected to my changes.. @briansmith thank you for a comment. I agree with you that it is probably not the best solution. On the other hand, I am not sure how to interpret rfc7230 \n\nIntermediaries that process HTTP messages (i.e., all intermediaries\n   other than those acting as tunnels) MUST send their own HTTP-version\n   in forwarded messages.\n\nor rfc6202\n\nNetwork Intermediaries:  The HTTP protocol allows for intermediaries\n      (proxies, transparent proxies, gateways, etc.) to be involved in\n      the transmission of a response from the server to the client.\n      There is no requirement for an intermediary to immediately forward\n      a partial response, and it is legal for the intermediary to buffer\n      the entire response before sending any data to the client (e.g.,\n      caching transparent proxies).. Ok @briansmith, I understand your point of view. Do you think that I should change my solution to avoid upgrading HTTP 1.0 requests? How did you solve this issue in Conduit?. @briansmith, I update my PR based on your advice. Please check if it is an acceptable solution.\n@adleong there is a problem with CI server. Could you look at this issue? I don't have a permission to re-run build.. You're right. I didn't check that - I started only \"router-http/test\". Now the broken test is corrected.. @adleong, could you please give the final decision? I could revert my changes to the previous version but I would like to have clear info how it should work.. @adleong, I updated PR.. @dadjeibaah & @adleong, I updated my PR according to your suggestions.. @adleong, I addressed all of your comments.. You have a right. It is my mistake.. I wasn't sure what should be the default value. I tried to find the value that could be treated as \"infinity\" (similar to the current behaviour) but I wanted to avoid the situation when cache elements live forever. Do you have any suggestion what should be the default value?. I will add the test.. I thought about that but I wanted to be consistent with the rest of the config - all time-related parameters are defined as a numeric value ex. \"retryJitterSecs\" or \"ttlSecs\" or \"decayTimeMs\". Yes, but in our case, it requires exposing Ticker interface in the ObserverCache constructor. Do you want me to do that?. You're right.. Ok. @adleong, sorry for that. I didn't know this interface and I didn't understand you correctly. I updated my PR. Please check if everything is as you wanted.. \n",
    "msiebeneicher": "@adleong - thx for your response! we will take a look.. ",
    "negz": "Other bugs where default backends have been gremlins:\n https://github.com/linkerd/linkerd/issues/1566\n https://github.com/linkerd/linkerd/issues/1187. To play devil's advocate on my own feature request: you could argue that this would be better enforced using an admission control webhook that prevents Ingress resources with default backends from being created than via a linkerd knob.. Thanks @deebo91! Any idea what I should do about CircleCI? As best I can tell it was the cancel_after_begin gRPC interop test that failed. That seems unlikely to be related to my PR.. Adding some detail from my tests here.\nCluster layout and version\nNAME                                                   STATUS    ROLES     AGE       VERSION\ntfk-negz-ctrl-fsqd.c.REDACTED.internal       Ready     control   1d        v1.9.2\ntfk-negz-ctrl-kbkb.c.REDACTED.internal       Ready     control   1d        v1.9.2\ntfk-negz-ctrl-wb98.c.REDACTED.internal       Ready     control   1d        v1.9.2\ntfk-negz-ing-hkfz.c.REDACTED.internal        Ready     ingress   1d        v1.9.2\ntfk-negz-wrk-udtd-m17n.c.REDACTED.internal   Ready     worker    6h        v1.9.2\nlinkerd deployment (running with strict mode off):\n```yaml\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: l5d-config\n  namespace: kube-system\ndata:\n  config.yaml: |-\n    admin:\n      ip: 0.0.0.0\n      port: 9990\nnamers:\n- kind: io.l5d.k8s\n- kind: io.l5d.k8s\n  prefix: /io.l5d.k8s.http\n  transformers:\n  - kind: io.l5d.k8s.daemonset\n    namespace: kube-system\n    port: http-incoming\n    service: l5d\n    hostNetwork: true\n- kind: io.l5d.k8s\n  prefix: /io.l5d.k8s.h2\n  transformers:\n  - kind: io.l5d.k8s.daemonset\n    namespace: kube-system\n    port: h2-incoming\n    service: l5d\n    hostNetwork: true\n- kind: io.l5d.k8s\n  prefix: /io.l5d.k8s.grpc\n  transformers:\n  - kind: io.l5d.k8s.daemonset\n    namespace: kube-system\n    port: grpc-incoming\n    service: l5d\n    hostNetwork: true\n- kind: io.l5d.rewrite\n  prefix: /portNsSvcToK8s\n  pattern: \"/{port}/{ns}/{svc}\"\n  name: \"/k8s/{ns}/{port}/{svc}\"\n\ntelemetry:\n- kind: io.l5d.prometheus\n- kind: io.l5d.recentRequests\n  sampleRate: 0.05\n\nusage:\n  enabled: false\n\nrouters:\n- label: http-outgoing\n  originator: true\n  protocol: http\n  servers:\n  - port: 4140\n    ip: 0.0.0.0\n    maxConcurrentRequests: 5000\n  dtab: |\n    /ph  => /$/io.buoyant.rinet ;                     # /ph/80/google.com -> /$/io.buoyant.rinet/80/google.com\n    /svc => /ph/80 ;                                  # /svc/google.com -> /ph/80/google.com\n    /svc => /$/io.buoyant.porthostPfx/ph ;            # /svc/google.com:80 -> /ph/80/google.com\n    /k8s => /#/io.l5d.k8s.http ;                      # /k8s/default/http/foo -> /#/io.l5d.k8s.http/default/http/foo\n    /portNsSvc => /#/portNsSvcToK8s ;                 # /portNsSvc/http/default/foo -> /k8s/default/http/foo\n    /host => /portNsSvc/http/default ;                # /host/foo -> /portNsSvc/http/default/foo\n    /host => /portNsSvc/http ;                        # /host/default/foo -> /portNsSvc/http/default/foo\n    /svc => /$/io.buoyant.http.domainToPathPfx/host ; # /svc/foo.default -> /host/default/foo\n  client:\n    kind: io.l5d.static\n    configs:\n    - prefix: \"/$/io.buoyant.rinet/443/{service}\"\n      tls:\n        commonName: \"{service}\"\n\n- label: http-incoming\n  protocol: http\n  servers:\n  - port: 4141\n    ip: 0.0.0.0\n    maxConcurrentRequests: 5000\n  identifier:\n    - kind: io.l5d.ingress\n    - kind: io.l5d.header.token\n  interpreter:\n    kind: default\n    transformers:\n    - kind: io.l5d.k8s.localnode\n      hostNetwork: true\n  dtab: |\n    /svc => /#/io.l5d.k8s ;                           # /svc/default/http/foo -> /#/io.l5d.k8s/default/http/foo\n    /k8s => /#/io.l5d.k8s ;                           # /k8s/default/http/foo -> /#/io.l5d.k8s/default/http/foo\n    /portNsSvc => /#/portNsSvcToK8s ;                 # /portNsSvc/http/default/foo -> /k8s/default/http/foo\n    /host => /portNsSvc/http/default ;                # /host/foo -> /portNsSvc/http/default/foo\n    /host => /portNsSvc/http ;                        # /host/default/foo -> /portNsSvc/http/default/foo\n    /svc => /$/io.buoyant.http.domainToPathPfx/host ; # /svc/foo.default -> /host/default/foo\n\n- label: h2-outgoing\n  originator: true\n  protocol: h2\n  servers:\n  - port: 4240\n    ip: 0.0.0.0\n    maxConcurrentRequests: 5000\n  dtab: |\n    /ph  => /$/io.buoyant.rinet ;                       # /ph/80/google.com -> /$/io.buoyant.rinet/80/google.com\n    /svc => /ph/80 ;                                    # /svc/google.com -> /ph/80/google.com\n    /svc => /$/io.buoyant.porthostPfx/ph ;              # /svc/google.com:80 -> /ph/80/google.com\n    /k8s => /#/io.l5d.k8s.h2 ;                          # /k8s/default/h2/foo -> /#/io.l5d.k8s.h2/default/h2/foo\n    /portNsSvc => /#/portNsSvcToK8s ;                   # /portNsSvc/h2/default/foo -> /k8s/default/h2/foo\n    /host => /portNsSvc/h2/default ;                    # /host/foo -> /portNsSvc/h2/default/foo\n    /host => /portNsSvc/h2 ;                            # /host/default/foo -> /portNsSvc/h2/default/foo\n    /svc => /$/io.buoyant.http.domainToPathPfx/host ;   # /svc/foo.default -> /host/default/foo\n  client:\n    kind: io.l5d.static\n    configs:\n    - prefix: \"/$/io.buoyant.rinet/443/{service}\"\n      tls:\n        commonName: \"{service}\"\n\n- label: h2-incoming\n  protocol: h2\n  servers:\n  - port: 4241\n    ip: 0.0.0.0\n    maxConcurrentRequests: 5000\n  identifier:\n    - kind: io.l5d.ingress\n    - kind: io.l5d.header.token\n  interpreter:\n    kind: default\n    transformers:\n    - kind: io.l5d.k8s.localnode\n      hostNetwork: true\n  dtab: |\n    /svc => /#/io.l5d.k8s ;                             # /svc/default/h2/foo -> /#/io.l5d.k8s/default/h2/foo\n    /k8s => /#/io.l5d.k8s ;                             # /k8s/default/h2/foo -> /#/io.l5d.k8s/default/h2/foo\n    /portNsSvc => /#/portNsSvcToK8s ;                   # /portNsSvc/h2/default/foo -> /k8s/default/h2/foo\n    /host => /portNsSvc/h2/default ;                    # /host/foo -> /portNsSvc/h2/default/foo\n    /host => /portNsSvc/h2 ;                            # /host/default/foo -> /portNsSvc/h2/default/foo\n    /svc => /$/io.buoyant.http.domainToPathPfx/host ;   # /svc/foo.default -> /host/default/foo\n\n- label: grpc-outgoing\n  originator: true\n  protocol: h2\n  servers:\n  - port: 4340\n    ip: 0.0.0.0\n    maxConcurrentRequests: 5000\n  identifier:\n    kind: io.l5d.header.path\n    segments: 1\n  dtab: |\n    /hp  => /$/inet ;                                # /hp/linkerd.io/8888 -> /$/inet/linkerd.io/8888\n    /svc => /$/io.buoyant.hostportPfx/hp ;           # /svc/linkerd.io:8888 -> /hp/linkerd.io/8888\n    /srv => /#/io.l5d.k8s.grpc/default/grpc;         # /srv/service/package -> /#/io.l5d.k8s.grpc/default/grpc/service/package\n    /svc => /$/io.buoyant.http.domainToPathPfx/srv ; # /svc/package.service -> /srv/service/package\n  client:\n    kind: io.l5d.static\n    configs:\n    - prefix: \"/$/inet/{service}\"\n      tls:\n        commonName: \"{service}\"\n\n- label: gprc-incoming\n  protocol: h2\n  servers:\n  - port: 4341\n    ip: 0.0.0.0\n    maxConcurrentRequests: 5000\n  identifier:\n    kind: io.l5d.header.path\n    segments: 1\n  interpreter:\n    kind: default\n    transformers:\n    - kind: io.l5d.k8s.localnode\n      hostNetwork: true\n  dtab: |\n    /srv => /#/io.l5d.k8s/default/grpc ;             # /srv/service/package -> /#/io.l5d.k8s/default/grpc/service/package\n    /svc => /$/io.buoyant.http.domainToPathPfx/srv ; # /svc/package.service -> /srv/service/package\n\n- protocol: http\n  label: http-ingress\n  originator: true\n  servers:\n    - port: 80\n      ip: 0.0.0.0\n      maxConcurrentRequests: 5000\n  identifier:\n    kind: io.l5d.ingress\n  dtab: /svc => /#/io.l5d.k8s.http ;                 # /svc/default/http/foo -> /#/io.l5d.k8s/default/http/foo\n\n- protocol: h2\n  originator: true\n  label: h2-ingress\n  servers:\n    - port: 81\n      ip: 0.0.0.0\n      maxConcurrentRequests: 5000\n  identifier:\n    kind: io.l5d.ingress\n  dtab: /svc => /#/io.l5d.k8s.h2 ;                   # /svc/default/h2/foo -> /#/io.l5d.k8s/default/h2/foo\n\n\napiVersion: apps/v1beta2\nkind: DaemonSet\nmetadata:\n  labels:\n    component: l5d\n  name: l5d\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      component: l5d\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        component: l5d\n    spec:\n      volumes:\n      - name: l5d-config\n        configMap:\n          name: \"l5d-config\"\n      hostNetwork: true\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n      - name: l5d\n        # This is an sbt docker build of this PR\n        image: us.gcr.io/REDACTED/negz/linkerd-strict:e131b9c2\n        args:\n        - /io.buoyant/linkerd/config/config.yaml\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: JVM_HEAP_MIN\n          value: 1024M\n        - name: JVM_HEAP_MAX\n          value: 1024M\n        ports:\n        - name: http-outgoing\n          containerPort: 4140\n          hostPort: 4140\n        - name: http-incoming\n          containerPort: 4141\n        - name: h2-outgoing\n          containerPort: 4240\n          hostPort: 4240\n        - name: h2-incoming\n          containerPort: 4241\n        - name: grpc-outgoing\n          containerPort: 4340\n          hostPort: 4340\n        - name: grpc-incoming\n          containerPort: 4341\n        - name: http-ingress\n          containerPort: 80\n        - name: h2-ingress\n          containerPort: 81\n        - name: admin\n          containerPort: 9990\n        volumeMounts:\n        - name: \"l5d-config\"\n          mountPath: \"/io.buoyant/linkerd/config\"\n          readOnly: true\n        readinessProbe:\n          httpGet:\n            path: /admin/ping\n            port: 9990\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 5\n  - name: kubectl\n    image: buoyantio/kubectl:v1.4.0\n    args:\n    - \"proxy\"\n    - \"-p\"\n    - \"8001\"\n  tolerations:\n  - key: \"terraflop.REDACTED/pool_type\"\n    operator: \"Exists\"\n    effect: \"NoSchedule\"\n\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: l5d\n  namespace: kube-system\nspec:\n  selector:\n    component: l5d\n  type: ClusterIP\n  ports:\n  - name: http-outgoing\n    port: 4140\n  - name: http-incoming\n    port: 4141\n  - name: h2-outgoing\n    port: 4240\n  - name: h2-incoming\n    port: 4241\n  - name: grpc-outgoing\n    port: 4340\n  - name: grpc-incoming\n    port: 4341\n  - name: admin\n    port: 9990\n```\nTest deployment:\n```yaml\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: explicit\n  labels:\n    component: explicit\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: explicit\n  template:\n    metadata:\n      name: explicit\n      labels:\n        component: explicit\n    spec:\n      containers:\n      - name: explicit\n        image: alpine:3.7\n        args:\n          - 'sh'\n          - '-c'\n          - 'echo -e \"#!/bin/sh\\necho \\\"HTTP/1.1 200 OK\\n\\n explicit\\\"\" > /srv.sh && chmod +x /srv.sh && nc -lk -p 80 -e /srv.sh'\n        ports:         \n        - name: explicit\n          containerPort: 80\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: fallback\n  labels:\n    component: fallback\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: fallback\n  template:\n    metadata:\n      name: fallback\n      labels:\n        component: fallback\n    spec:\n      containers:\n      - name: fallback\n        image: alpine:3.7\n        args:\n          - 'sh'\n          - '-c'\n          - 'echo -e \"#!/bin/sh\\necho \\\"HTTP/1.1 200 OK\\n\\n fallback\\\"\" > /srv.sh && chmod +x /srv.sh && nc -lk -p 80 -e /srv.sh'\n        ports:         \n        - name: fallback\n          containerPort: 80\n\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    name: explicit\n  name: explicit\nspec:\n  selector:\n    component: explicit\n  ports:\n  - name: explicit\n    protocol: TCP\n    port: 80\n\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    name: fallback\n  name: fallback\nspec:\n  selector:\n    component: fallback\n  ports:\n  - name: fallback\n    protocol: TCP\n    port: 80\n\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n  labels:\n    component: ingresses\n  name: ingresses\nspec:\n  backend:\n    serviceName: fallback\n    servicePort: 80\n  rules:\n  - host: explicit.ingress.tfk-negz.staging.k8s.REDACTED\n    http:\n      paths:\n      - backend:\n          serviceName: explicit\n          servicePort: 80\n```\nConfirming the expected existing behaviour with strict mode off and a single default backend. Explicitly configured rules work, anything else falls back to the default backend.\n```bash\ncurl -v http://explicit.ingress.tfk-negz.staging.k8s.REDACTED\n Rebuilt URL to: http://explicit.ingress.tfk-negz.staging.k8s.REDACTED/\n   Trying REDACTED.0.167...\n TCP_NODELAY set\n Connected to explicit.ingress.tfk-negz.staging.k8s.REDACTED (REDACTED.0.167) port 80 (#0)\n\nGET / HTTP/1.1\nHost: explicit.ingress.tfk-negz.staging.k8s.REDACTED\nUser-Agent: curl/7.54.0\nAccept: /\n< HTTP/1.1 200 OK\n< l5d-success-class: 1.0\n< Via: 1.1 linkerd, 1.1 linkerd\n< Transfer-Encoding: chunked\n< \n explicit\n* Connection #0 to host explicit.ingress.tfk-negz.staging.k8s.REDACTED left intact\n\nThis ingress rule does not exist\ncurl -v http://nonexistent.ingress.tfk-negz.staging.k8s.REDACTED\n Rebuilt URL to: http://nonexistent.ingress.tfk-negz.staging.k8s.REDACTED/\n   Trying REDACTED.0.167...\n TCP_NODELAY set\n Connected to nonexistent.ingress.tfk-negz.staging.k8s.REDACTED (REDACTED.0.167) port 80 (#0)\n\nGET / HTTP/1.1\nHost: nonexistent.ingress.tfk-negz.staging.k8s.REDACTED\nUser-Agent: curl/7.54.0\nAccept: /\n< HTTP/1.1 200 OK\n< l5d-success-class: 1.0\n< Via: 1.1 linkerd, 1.1 linkerd\n< Transfer-Encoding: chunked\n< \n fallback\n* Connection #0 to host nonexistent.ingress.tfk-negz.staging.k8s.REDACTED left intact\n```\n\nNow to add a competing default backend:\n```yaml\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: anotherfallback\n  labels:\n    component: anotherfallback\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: anotherfallback\n  template:\n    metadata:\n      name: anotherfallback\n      labels:\n        component: anotherfallback\n    spec:\n      containers:\n      - name: anotherfallback\n        image: alpine:3.7\n        args:\n          - 'sh'\n          - '-c'\n          - 'echo -e \"#!/bin/sh\\necho \\\"HTTP/1.1 200 OK\\n\\n anotherfallback\\\"\" > /srv.sh && chmod +x /srv.sh && nc -lk -p 80 -e /srv.sh'\n        ports:         \n        - name: anotherfallback\n          containerPort: 80\n\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    name: anotherfallback\n  name: anotherfallback\nspec:\n  selector:\n    component: anotherfallback\n  ports:\n  - name: anotherfallback\n    protocol: TCP\n    port: 80\n\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n  labels:\n    component: anotherfallback\n  name: anotherfallback\nspec:\n  backend:\n    serviceName: anotherfallback\n    servicePort: 80\n```\nThe explicit rule still works, and the new anotherfallback is ignored:\n```bash\ncurl -v http://explicit.ingress.tfk-negz.staging.k8s.REDACTED\n Rebuilt URL to: http://explicit.ingress.tfk-negz.staging.k8s.REDACTED/\n   Trying REDACTED.0.167...\n TCP_NODELAY set\n Connected to explicit.ingress.tfk-negz.staging.k8s.REDACTED (REDACTED.0.167) port 80 (#0)\n\nGET / HTTP/1.1\nHost: explicit.ingress.tfk-negz.staging.k8s.REDACTED\nUser-Agent: curl/7.54.0\nAccept: /\n< HTTP/1.1 200 OK\n< l5d-success-class: 1.0\n< Via: 1.1 linkerd, 1.1 linkerd\n< Transfer-Encoding: chunked\n< \n explicit\n* Connection #0 to host explicit.ingress.tfk-negz.staging.k8s.REDACTED left intact\n\nThis ingress rule does not exist\ncurl -v http://boop.ingress.tfk-negz.staging.k8s.REDACTED\n Rebuilt URL to: http://boop.ingress.tfk-negz.staging.k8s.REDACTED/\n   Trying REDACTED.0.167...\n TCP_NODELAY set\n Connected to boop.ingress.tfk-negz.staging.k8s.REDACTED (REDACTED.0.167) port 80 (#0)\n\nGET / HTTP/1.1\nHost: boop.ingress.tfk-negz.staging.k8s.REDACTED\nUser-Agent: curl/7.54.0\nAccept: /\n< HTTP/1.1 200 OK\n< l5d-success-class: 1.0\n< Via: 1.1 linkerd, 1.1 linkerd\n< Transfer-Encoding: chunked\n< \n fallback\n* Connection #0 to host boop.ingress.tfk-negz.staging.k8s.REDACTED left intact\n```\n\nAfter setting strict: true neither of the default backends serve the request.\n```\ncurl -v http://explicit.ingress.tfk-negz.staging.k8s.REDACTED\n Rebuilt URL to: http://explicit.ingress.tfk-negz.staging.k8s.REDACTED/\n   Trying REDACTED.0.167...\n TCP_NODELAY set\n Connected to explicit.ingress.tfk-negz.staging.k8s.REDACTED (REDACTED.0.167) port 80 (#0)\n\nGET / HTTP/1.1\nHost: explicit.ingress.tfk-negz.staging.k8s.REDACTED\nUser-Agent: curl/7.54.0\nAccept: /\n< HTTP/1.1 200 OK\n< l5d-success-class: 1.0\n< Via: 1.1 linkerd, 1.1 linkerd\n< Transfer-Encoding: chunked\n< \n explicit\n* Connection #0 to host explicit.ingress.tfk-negz.staging.k8s.REDACTED left intact\n\ncurl -v http://boop.ingress.tfk-negz.staging.k8s.REDACTED\n Rebuilt URL to: http://boop.ingress.tfk-negz.staging.k8s.REDACTED/\n   Trying REDACTED.0.167...\n TCP_NODELAY set\n Connected to boop.ingress.tfk-negz.staging.k8s.REDACTED (REDACTED.0.167) port 80 (#0)\n\nGET / HTTP/1.1\nHost: boop.ingress.tfk-negz.staging.k8s.REDACTED\nUser-Agent: curl/7.54.0\nAccept: /\n< HTTP/1.1 400 Bad Request\n< l5d-err: Unknown+destination%3A+Request%28%22GET+%2F%22%2C+from+%2FREDACTED.20.27%3A54362%29+%2F+no+ingress+rule+matches\n< Content-Type: text/plain\n< Content-Length: 89\n< \n Connection #0 to host boop.ingress.tfk-negz.staging.k8s.REDACTED left intact\nUnknown destination: Request(\"GET /\", from /REDACTED.20.27:54362) / no ingress rule matches\n```\n. nginx Ingress Controller\n Allows setting a fallback explicitly configured as a flag: [1] [2]\n I'm not super familiar with nginx but I think it treats default backends as a set of endpoints that should be considered upstreams for all* explicit rules in the ingress.\n\n```yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n  labels:\n    component: ingresses\n  name: ingresses\nspec:\n  backend:\n    serviceName: fallback\n    servicePort: 80\n  rules:\n  - host: explicit.ingress.tfk-negz.staging.k8s.REDACTED\n    http:\n      paths:\n      - backend:\n          serviceName: explicit\n          servicePort: 80\n```\nI believe the above would result in requests for host header explicit.ingress.tfk-negz.staging.k8s.REDACTED being distributed across the endpoints of both the explicit and fallback services.\nThat seems a little different from the purpose of the default backend:\ngo\n    // A default backend capable of servicing requests that don't match any\n    // rule. At least one of 'backend' or 'rules' must be specified. This field\n    // is optional to allow the loadbalancer controller or defaulting logic to\n    // specify a global default.\n    // +optional. traefik\n Seems to just not support default backends [1] [2]. Voyager (HAProxy)\n Use their own custom resource definition version of an ingress.\n Supports default backends [1]\n Seems to create a 'controller' managing a HAProxy config per (custom) ingress resource, so each HAProxy config can only ever have one default backend. I'm completely unfamiliar with HAProxy so I'm not entirely sure how this all works for actual requests.. > Another approach would be to allow setting a \"priority\" value as an annotation on the ingress \nresource. Not sure if this is a good idea or not, just brainstorming.\nI like that approach. I noticed Traefik supports that pattern when surveying Ingress controllers. I can't find any evidence that the nginx or Voyager controllers support explicitly setting a priority.\n. Interestingly client_state.json still contains a binding for the service even after linkerd seems to notice the service is gone.\nThe requests get \njson\n{\n  \"/$/inet/localhost/8001\": {\n    \"state\": \"bound\",\n    \"addresses\": [\n      \"localhost:8001\"\n    ]\n  },\n  \"/%/io.l5d.k8s.daemonset/kube-system/http-incoming/l5d/#/io.l5d.k8s.http/default/80/nginx\": {\n    \"state\": \"bound\",\n    \"addresses\": [\n      \"10.REDACTED:4141\"\n    ]\n  }\n}\nDespite the requests now being met with:\n```\ncurl -v http://nginx.ingress.tfk-drp5.REDACTED\n Rebuilt URL to: http://nginx.ingress.tfk-drp5.REDACTED/\n   Trying 10.REDACTED...\n TCP_NODELAY set\n Connected to nginx.ingress.tfk-drp5.REDACTED (10.REDACTED) port 80 (#0)\n\nGET / HTTP/1.1\nHost: nginx.ingress.tfk-drp5.REDACTED\nUser-Agent: curl/7.54.0\nAccept: /\n< HTTP/1.1 502 Bad Gateway\n< l5d-err: No+hosts+are+available+for+%2Fsvc%2Fdefault%2F80%2Fnginx%2C+Dtab.base%3D%5B%2Fsvc%3D%3E%2F%23%2Fio.l5d.k8s.http%5D%2C+Dtab.local%3D%5B%5D.+Remote+Info%3A+Not+Available\n< Content-Type: text/plain\n< Content-Length: 129\n< \n* Connection #0 to host nginx.ingress.tfk-drp5.REDACTED left intact\nNo hosts are available for /svc/default/80/nginx, Dtab.base=[/svc=>/#/io.l5d.k8s.http], Dtab.local=[]. Remote Info: Not Available\n```. On the plus side linkerd noticed immediately when I recreated the Service resource.\n\nThe approximate timeline:\n ~12:00 - 12:05 - Delete the Kubernetes service resource.\n 12:22 - Requests begin to fail.\n* 12:39 - Recreate the Kubernetes service resource. Requests begin to succeed again.\n\n. @deebo91 Interesting. Note that I deleted the Service resource and left the Ingress in place. Sounds like it could well still be the same bug - just want to be clear.. @deebo91 A little more (unfortunately anecdotal and unreproducible) context - we've also seen the reverse of this issue at least once. i.e. We created a Service resource and linkerd did not notice.\nSpecifically we:\n1. Setup a Kubernetes cluster and deployed a dummy nginx Service and Ingress resource.\n1. Confirmed it successfully served requests via the Ingress.\n1. Deleted the Service resource.\n1. Confirmed that linkerd could not route requests for the now Service-less Ingress.\n1. Reinstated the Service resource ~30 mins later.\n1. Observed that linkerd continued to not route requests for the Ingress.\nIt's possible given #1810's bug that:\n1. linkerd sees a DELETE watch event for the Service. DELETE events do not increment the resourceVersion because they return \"the state of the object immediately before deletion\".\n1. linkerd sees an ADDED watch event for the same Service. The ADDED resource has a resourceVersion lower than the previously DELETED Service, and thus get swallowed?\nThough in that case I'd have expected linkerd to never have noticed the deletion in the first place? \ud83e\udd14\nFrom the Kubernetes API WatchEvent docs:\n\nIf Type is Deleted: the state of the object immediately before deletion. We might be able to help testing this if there's anything in particular you're interested in. Anecdotally we're seeing issues that feel like this one a lot recently - i.e. linkerd not noticing updates to service and ingress resources.. @deebo91 We rolled buoyantio/linkerd:1.3.6-watch-change today to test. I've found one bug that I can consistently reproduce, though it's unclear whether it was caused by this PR or already in linkerd. In short, deleting any Kubernetes Ingress will break all ingress rules. i.e. all other ingresses will report Unknown destination: Request(\"GET /graph\", from /x.x.x.x:64296) / no ingress rule matches.\n\nI can provide some more detail later - got to run for the moment.. > I've found one bug that I can consistently reproduce, though it's unclear whether it was caused by this PR or already in linkerd. In short, deleting any Kubernetes Ingress will break all ingress rules.\nLooks like it was a pre-existing bug that we just happened to repro while running this build. I've raised https://github.com/linkerd/linkerd/pull/1817 to fix it.. > The resourceVersion is supposedly a number that increases after a change event occurs on a resource. For a particular ingress resource, its resourceVersion increases after Add, Modify and Change events. Sometimes k8s returns a resourceVersion that is unchanged. \n\nI'd like to have a better understanding of the Kubernetes API's resource versioning and what the actual semantics are.\n\nI've been trying to wrap my head around this today too. I'm probably just paraphrasing @deebo91 here,  but based on the WatchEvent docs my understanding is that MODIFIED objects will have a higher resourceVersion, while DELETED objects will have an unchanged resourceVersion because they return \"the state of the object immediately before deletion\".. @adleong I don't expect #1817 to fix the issues described in #1805. Unfortunately we haven't been able to reliably reproduce those issues, so I don't have an actual test case I can use.\nI should be able to bake an image for this and push it out to test today.. Thanks! We're now running a build of this PR on a lightly trafficked cluster. A quick spot check shows historically problematic things (deleting and recreating ingresses, deleting and recreating services) working as expected. I'll leave this running and report back with any issues we see.. @siggy No issues thus far! That said, the class of issues we've seen are mostly caused by cluster customers \"doing stuff\" i.e. creating/deleting service and ingress resources, so soaking overnight doesn't build a huge amount of confidence for us yet.\nI can say that the main service currently taking traffic in the cluster autoscales fairly frequently and our dashboards show no errors due to the changing set of backends.. @adleong I think we've been hit by (at least) two classes of bug:\n\nThis one.\nThe one described in https://github.com/linkerd/linkerd/issues/1805\n\nI'm pretty sure we can consistently reproduce this one. I'm going to do so now with linkerd 1.3.5 vs a build of this PR to be doubly sure.\nSo far we can't reliably reproduce https://github.com/linkerd/linkerd/issues/1805, but anecdotally we've seen linkerd take a long time to catch up both when adding and deleting Kubernetes Service resources that are backing Ingress rules.\nEdit: to answer your question, I'm quite sure the issues I mentioned in https://github.com/linkerd/linkerd/pull/1810#issuecomment-365379424 are unrelated to this one.. Thanks for the review! I'm going to comment here with the process I followed to reproduce this bug on my test cluster with linkerd 1.3.5 (well, actually the build of linkerd from #1810). I'll then address the review comments and follow these steps again to confirm the bug is fixed on my test cluster.\nMy linkerd config:\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: l5d-config\n  namespace: kube-system\ndata:\n  config.yaml: |-\n    admin:\n      ip: 0.0.0.0\n      port: 9990\nnamers:\n- kind: io.l5d.k8s\n- kind: io.l5d.k8s\n  prefix: /io.l5d.k8s.http\n  transformers:\n  - kind: io.l5d.k8s.daemonset\n    namespace: kube-system\n    port: http-incoming\n    service: l5d\n    hostNetwork: true\n- kind: io.l5d.k8s\n  prefix: /io.l5d.k8s.h2\n  transformers:\n  - kind: io.l5d.k8s.daemonset\n    namespace: kube-system\n    port: h2-incoming\n    service: l5d\n    hostNetwork: true\n- kind: io.l5d.k8s\n  prefix: /io.l5d.k8s.grpc\n  transformers:\n  - kind: io.l5d.k8s.daemonset\n    namespace: kube-system\n    port: grpc-incoming\n    service: l5d\n    hostNetwork: true\n- kind: io.l5d.rewrite\n  prefix: /portNsSvcToK8s\n  pattern: \"/{port}/{ns}/{svc}\"\n  name: \"/k8s/{ns}/{port}/{svc}\"\n\ntelemetry:\n- kind: io.l5d.prometheus\n- kind: io.l5d.recentRequests\n  sampleRate: 0.05\n- kind: io.zipkin.http\n  host: zipkin:9411\n  initialSampleRate: 0.05\n\nusage:\n  enabled: false\n\nrouters:\n- label: http-outgoing\n  originator: true\n  protocol: http\n  servers:\n  - port: 4140\n    ip: 0.0.0.0\n    maxConcurrentRequests: 5000\n  dtab: |\n    /ph  => /$/io.buoyant.rinet ;                     # /ph/80/google.com -> /$/io.buoyant.rinet/80/google.com\n    /svc => /ph/80 ;                                  # /svc/google.com -> /ph/80/google.com\n    /svc => /$/io.buoyant.porthostPfx/ph ;            # /svc/google.com:80 -> /ph/80/google.com\n    /k8s => /#/io.l5d.k8s.http ;                      # /k8s/default/http/foo -> /#/io.l5d.k8s.http/default/http/foo\n    /portNsSvc => /#/portNsSvcToK8s ;                 # /portNsSvc/http/default/foo -> /k8s/default/http/foo\n    /host => /portNsSvc/http/default ;                # /host/foo -> /portNsSvc/http/default/foo\n    /host => /portNsSvc/http ;                        # /host/default/foo -> /portNsSvc/http/default/foo\n    /svc => /$/io.buoyant.http.domainToPathPfx/host ; # /svc/foo.default -> /host/default/foo\n  client:\n    kind: io.l5d.static\n    configs:\n    - prefix: \"/$/io.buoyant.rinet/443/{service}\"\n      tls:\n        commonName: \"{service}\"\n\n- label: http-incoming\n  protocol: http\n  servers:\n  - port: 4141\n    ip: 0.0.0.0\n    maxConcurrentRequests: 5000\n  identifier:\n    - kind: io.l5d.ingress\n    - kind: io.l5d.header.token\n  interpreter:\n    kind: default\n    transformers:\n    - kind: io.l5d.k8s.localnode\n      hostNetwork: true\n  dtab: |\n    /svc => /#/io.l5d.k8s ;                           # /svc/default/http/foo -> /#/io.l5d.k8s/default/http/foo\n    /k8s => /#/io.l5d.k8s ;                           # /k8s/default/http/foo -> /#/io.l5d.k8s/default/http/foo\n    /portNsSvc => /#/portNsSvcToK8s ;                 # /portNsSvc/http/default/foo -> /k8s/default/http/foo\n    /host => /portNsSvc/http/default ;                # /host/foo -> /portNsSvc/http/default/foo\n    /host => /portNsSvc/http ;                        # /host/default/foo -> /portNsSvc/http/default/foo\n    /svc => /$/io.buoyant.http.domainToPathPfx/host ; # /svc/foo.default -> /host/default/foo\n\n- label: h2-outgoing\n  originator: true\n  protocol: h2\n  servers:\n  - port: 4240\n    ip: 0.0.0.0\n    maxConcurrentRequests: 5000\n  dtab: |\n    /ph  => /$/io.buoyant.rinet ;                       # /ph/80/google.com -> /$/io.buoyant.rinet/80/google.com\n    /svc => /ph/80 ;                                    # /svc/google.com -> /ph/80/google.com\n    /svc => /$/io.buoyant.porthostPfx/ph ;              # /svc/google.com:80 -> /ph/80/google.com\n    /k8s => /#/io.l5d.k8s.h2 ;                          # /k8s/default/h2/foo -> /#/io.l5d.k8s.h2/default/h2/foo\n    /portNsSvc => /#/portNsSvcToK8s ;                   # /portNsSvc/h2/default/foo -> /k8s/default/h2/foo\n    /host => /portNsSvc/h2/default ;                    # /host/foo -> /portNsSvc/h2/default/foo\n    /host => /portNsSvc/h2 ;                            # /host/default/foo -> /portNsSvc/h2/default/foo\n    /svc => /$/io.buoyant.http.domainToPathPfx/host ;   # /svc/foo.default -> /host/default/foo\n  client:\n    kind: io.l5d.static\n    configs:\n    - prefix: \"/$/io.buoyant.rinet/443/{service}\"\n      tls:\n        commonName: \"{service}\"\n\n- label: h2-incoming\n  protocol: h2\n  servers:\n  - port: 4241\n    ip: 0.0.0.0\n    maxConcurrentRequests: 5000\n  identifier:\n    - kind: io.l5d.ingress\n    - kind: io.l5d.header.token\n  interpreter:\n    kind: default\n    transformers:\n    - kind: io.l5d.k8s.localnode\n      hostNetwork: true\n  dtab: |\n    /svc => /#/io.l5d.k8s ;                             # /svc/default/h2/foo -> /#/io.l5d.k8s/default/h2/foo\n    /k8s => /#/io.l5d.k8s ;                             # /k8s/default/h2/foo -> /#/io.l5d.k8s/default/h2/foo\n    /portNsSvc => /#/portNsSvcToK8s ;                   # /portNsSvc/h2/default/foo -> /k8s/default/h2/foo\n    /host => /portNsSvc/h2/default ;                    # /host/foo -> /portNsSvc/h2/default/foo\n    /host => /portNsSvc/h2 ;                            # /host/default/foo -> /portNsSvc/h2/default/foo\n    /svc => /$/io.buoyant.http.domainToPathPfx/host ;   # /svc/foo.default -> /host/default/foo\n\n- label: grpc-outgoing\n  originator: true\n  protocol: h2\n  servers:\n  - port: 4340\n    ip: 0.0.0.0\n    maxConcurrentRequests: 5000\n  identifier:\n    kind: io.l5d.header.path\n    segments: 1\n  dtab: |\n    /hp  => /$/inet ;                                # /hp/linkerd.io/8888 -> /$/inet/linkerd.io/8888\n    /svc => /$/io.buoyant.hostportPfx/hp ;           # /svc/linkerd.io:8888 -> /hp/linkerd.io/8888\n    /srv => /#/io.l5d.k8s.grpc/default/grpc;         # /srv/service/package -> /#/io.l5d.k8s.grpc/default/grpc/service/package\n    /svc => /$/io.buoyant.http.domainToPathPfx/srv ; # /svc/package.service -> /srv/service/package\n  client:\n    kind: io.l5d.static\n    configs:\n    - prefix: \"/$/inet/{service}\"\n      tls:\n        commonName: \"{service}\"\n\n- label: gprc-incoming\n  protocol: h2\n  servers:\n  - port: 4341\n    ip: 0.0.0.0\n    maxConcurrentRequests: 5000\n  identifier:\n    kind: io.l5d.header.path\n    segments: 1\n  interpreter:\n    kind: default\n    transformers:\n    - kind: io.l5d.k8s.localnode\n      hostNetwork: true\n  dtab: |\n    /srv => /#/io.l5d.k8s/default/grpc ;             # /srv/service/package -> /#/io.l5d.k8s/default/grpc/service/package\n    /svc => /$/io.buoyant.http.domainToPathPfx/srv ; # /svc/package.service -> /srv/service/package\n\n- protocol: http\n  label: http-ingress\n  originator: true\n  servers:\n    - port: 80\n      ip: 0.0.0.0\n      maxConcurrentRequests: 5000\n  identifier:\n    kind: io.l5d.ingress\n  dtab: /svc => /#/io.l5d.k8s.http ;                 # /svc/default/http/foo -> /#/io.l5d.k8s/default/http/foo\n\n- protocol: h2\n  originator: true\n  label: h2-ingress\n  servers:\n    - port: 81\n      ip: 0.0.0.0\n      maxConcurrentRequests: 5000\n  identifier:\n    kind: io.l5d.ingress\n  dtab: /svc => /#/io.l5d.k8s.h2 ;                   # /svc/default/h2/foo -> /#/io.l5d.k8s/default/h2/foo\n\n\n```\nMy test Ingresses:\n```yaml\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nginx\ndata:\n  index.html: |-\n    Hi!\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    component: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: nginx\n  template:\n    metadata:\n      name: nginx\n      labels:\n        component: nginx\n    spec:\n      volumes:\n      - name: nginx\n        configMap:\n          name: \"nginx\"\n      containers:\n      - name: nginx\n        image: nginx:alpine\n        ports:         \n        - name: nginx\n          containerPort: 80\n        volumeMounts:\n        - name: \"nginx\"\n          mountPath: \"/usr/share/nginx/html\"\n          readOnly: true\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 500m\n            memory: 500Mi\n\napiVersion: policy/v1beta1\nkind: PodDisruptionBudget\nmetadata:\n  name: nginx\nspec:\n  minAvailable: 80%\n  selector:\n    matchLabels:\n      component: nginx\n\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    name: nginx\n  name: nginx\nspec:\n  selector:\n    component: nginx\n  ports:\n  - name: nginx\n    protocol: TCP\n    port: 80\n\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n  labels:\n    component: nginx\n  name: nginx\nspec:\n  rules:\n  - host: nginx.ingress.tfk-negz.staging.k8s.REDACTED\n    http:\n      paths:\n      - backend:\n          serviceName: nginx\n          servicePort: 80\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nginy\ndata:\n  index.html: |-\n    Hej!\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: nginy\n  labels:\n    component: nginy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: nginy\n  template:\n    metadata:\n      name: nginy\n      labels:\n        component: nginy\n    spec:\n      volumes:\n      - name: nginy\n        configMap:\n          name: \"nginy\"\n      containers:\n      - name: nginy\n        image: nginx:alpine\n        ports:         \n        - name: nginy\n          containerPort: 80\n        volumeMounts:\n        - name: \"nginy\"\n          mountPath: \"/usr/share/nginx/html\"\n          readOnly: true\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 500m\n            memory: 500Mi\n\napiVersion: policy/v1beta1\nkind: PodDisruptionBudget\nmetadata:\n  name: nginy\nspec:\n  minAvailable: 80%\n  selector:\n    matchLabels:\n      component: nginy\n\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    name: nginy\n  name: nginy\nspec:\n  selector:\n    component: nginy\n  ports:\n  - name: nginy\n    protocol: TCP\n    port: 80\n\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n  labels:\n    component: nginy\n  name: nginy\nspec:\n  rules:\n  - host: nginy.ingress.tfk-negz.staging.k8s.REDACTED\n    http:\n      paths:\n      - backend:\n          serviceName: nginy\n          servicePort: 80\n\n```\n```bash\n$ curl -v http://nginx.ingress.tfk-negz.staging.k8s.REDACTED && echo\n Rebuilt URL to: http://nginx.ingress.tfk-negz.staging.k8s.REDACTED/\n   Trying REDACTED...\n TCP_NODELAY set\n Connected to nginx.ingress.tfk-negz.staging.k8s.REDACTED (REDACTED) port 80 (#0)\n\nGET / HTTP/1.1\nHost: nginx.ingress.tfk-negz.staging.k8s.REDACTED\nUser-Agent: curl/7.54.0\nAccept: /\n< HTTP/1.1 200 OK\n< Server: nginx/1.13.8\n< Date: Thu, 15 Feb 2018 19:30:08 GMT\n< Content-Type: text/html\n< Content-Length: 3\n< Last-Modified: Thu, 15 Feb 2018 19:25:31 GMT\n< ETag: \"5a85deab-3\"\n< Accept-Ranges: bytes\n< l5d-success-class: 1.0\n< Via: 1.1 linkerd, 1.1 linkerd\n< \n* Connection #0 to host nginx.ingress.tfk-negz.staging.k8s.REDACTED left intact\nHi!\n\n$ curl -v http://nginy.ingress.tfk-negz.staging.k8s.REDACTED && echo\n Rebuilt URL to: http://nginy.ingress.tfk-negz.staging.k8s.REDACTED/\n   Trying REDACTED...\n TCP_NODELAY set\n Connected to nginy.ingress.tfk-negz.staging.k8s.REDACTED (REDACTED) port 80 (#0)\n\nGET / HTTP/1.1\nHost: nginy.ingress.tfk-negz.staging.k8s.REDACTED\nUser-Agent: curl/7.54.0\nAccept: /\n< HTTP/1.1 200 OK\n< Server: nginx/1.13.8\n< Date: Thu, 15 Feb 2018 19:30:15 GMT\n< Content-Type: text/html\n< Content-Length: 4\n< Last-Modified: Thu, 15 Feb 2018 19:29:58 GMT\n< ETag: \"5a85dfb6-4\"\n< Accept-Ranges: bytes\n< l5d-success-class: 1.0\n< Via: 1.1 linkerd, 1.1 linkerd\n< \n* Connection #0 to host nginy.ingress.tfk-negz.staging.k8s.REDACTED left intact\nHej!\n```\n\nDelete an ingress...\nbash\n$ kubectl delete ing nginx\ningress \"nginx\" deleted\nIt's gone\n```bash\n$ curl -v http://nginx.ingress.tfk-negz.staging.k8s.REDACTED && echo\n Rebuilt URL to: http://nginx.ingress.tfk-negz.staging.k8s.REDACTED/\n   Trying REDACTED...\n TCP_NODELAY set\n Connected to nginx.ingress.tfk-negz.staging.k8s.REDACTED (REDACTED) port 80 (#0)\n\nGET / HTTP/1.1\nHost: nginx.ingress.tfk-negz.staging.k8s.REDACTED\nUser-Agent: curl/7.54.0\nAccept: /\n< HTTP/1.1 400 Bad Request\n< l5d-err: Unknown+destination%3A+Request%28%22GET+%2F%22%2C+from+%2FREDACTED%3A54666%29+%2F+no+ingress+rule+matches\n< Content-Type: text/plain\n< Content-Length: 89\n< \n* Connection #0 to host nginx.ingress.tfk-negz.staging.k8s.REDACTED left intact\nUnknown destination: Request(\"GET /\", from /REDACTED:54666) / no ingress rule matches\n```\n\n...but so is the other ingress\n```bash\n$ kubectl get ing\nNAME      HOSTS                                                ADDRESS   PORTS     AGE\nnginy     nginy.ingress.tfk-negz.staging.k8s.REDACTED             80        10m\n$ curl -v http://nginy.ingress.tfk-negz.staging.k8s.REDACTED && echo\n Rebuilt URL to: http://nginy.ingress.tfk-negz.staging.k8s.REDACTED/\n   Trying REDACTED...\n TCP_NODELAY set\n Connected to nginy.ingress.tfk-negz.staging.k8s.REDACTED (REDACTED) port 80 (#0)\n\nGET / HTTP/1.1\nHost: nginy.ingress.tfk-negz.staging.k8s.REDACTED\nUser-Agent: curl/7.54.0\nAccept: /\n< HTTP/1.1 400 Bad Request\n< l5d-err: Unknown+destination%3A+Request%28%22GET+%2F%22%2C+from+%2FREDACTED%3A54691%29+%2F+no+ingress+rule+matches\n< Content-Type: text/plain\n< Content-Length: 89\n< \n* Connection #0 to host nginy.ingress.tfk-negz.staging.k8s.REDACTED left intact\nUnknown destination: Request(\"GET /\", from /REDACTED:54691) / no ingress rule matches\n```. Just got around to running my tests against a linkerd built from this PR. linkerd config and test Ingresses remain the same.\n\nBoth ingresses working fine:\n```bash\n$ curl -v http://nginx.ingress.tfk-negz.staging.k8s.REDACTED && echo                                \n Rebuilt URL to: http://nginx.ingress.tfk-negz.staging.k8s.REDACTED/\n   Trying REDACTED...                                                                                                                              \n TCP_NODELAY set                                                                            \n Connected to nginx.ingress.tfk-negz.staging.k8s.REDACTED (REDACTED) port 80 (#0)\n\nGET / HTTP/1.1               \nHost: nginx.ingress.tfk-negz.staging.k8s.REDACTED\nUser-Agent: curl/7.54.0                                                                 \nAccept: / \n< HTTP/1.1 200 OK                                                                                                                                         \n< Server: nginx/1.13.8\n< Date: Thu, 15 Feb 2018 22:41:59 GMT\n< Content-Type: text/html \n< Content-Length: 3                                                                                                   \n< Last-Modified: Thu, 15 Feb 2018 22:33:20 GMT\n< ETag: \"5a860ab0-3\" \n< Accept-Ranges: bytes                                                                                                                               \n< l5d-success-class: 1.0                                                            \n< Via: 1.1 linkerd, 1.1 linkerd                                                        \n<                                                                                            \n* Connection #0 to host nginx.ingress.tfk-negz.staging.k8s.REDACTED left intact\nHi!\n\n$ curl -v http://nginy.ingress.tfk-negz.staging.k8s.REDACTED && echo\n Rebuilt URL to: http://nginy.ingress.tfk-negz.staging.k8s.REDACTED/              \n   Trying REDACTED...\n TCP_NODELAY set                                                                                                                                  \n Connected to nginy.ingress.tfk-negz.staging.k8s.REDACTED (REDACTED) port 80 (#0)\n\nGET / HTTP/1.1   \nHost: nginy.ingress.tfk-negz.staging.k8s.REDACTED                                                                                                 \nUser-Agent: curl/7.54.0 \nAccept: / \n< HTTP/1.1 200 OK      \n< Server: nginx/1.13.8 \n< Date: Thu, 15 Feb 2018 22:42:03 GMT       \n< Content-Type: text/html \n< Content-Length: 4                 \n< Last-Modified: Thu, 15 Feb 2018 22:34:21 GMT\n< ETag: \"5a860aed-4\"         \n< Accept-Ranges: bytes   \n< l5d-success-class: 1.0                                                                                              \n< Via: 1.1 linkerd, 1.1 linkerd                                     \n<                                                                  \n* Connection #0 to host nginy.ingress.tfk-negz.staging.k8s.REDACTED left intact\nHej!\n```\n\nDelete one ingress...\nbash\n$ kubectl delete ing nginx\ningress \"nginx\" deleted\nThe deleted ingress is gone\n```bash\n$ curl -v http://nginx.ingress.tfk-negz.staging.k8s.REDACTED && echo\n Rebuilt URL to: http://nginx.ingress.tfk-negz.staging.k8s.REDACTED/\n   Trying REDACTED...\n TCP_NODELAY set\n Connected to nginx.ingress.tfk-negz.staging.k8s.REDACTED (REDACTED) port 80 (#0)\n\nGET / HTTP/1.1\nHost: nginx.ingress.tfk-negz.staging.k8s.REDACTED\nUser-Agent: curl/7.54.0\nAccept: /\n< HTTP/1.1 400 Bad Request\n< l5d-err: Unknown+destination%3A+Request%28%22GET+%2F%22%2C+from+%2FREDACTED%3A56751%29+%2F+no+ingress+rule+matches\n< Content-Type: text/plain\n< Content-Length: 89\n<\n* Connection #0 to host nginx.ingress.tfk-negz.staging.k8s.REDACTED left intact\nUnknown destination: Request(\"GET /\", from /REDACTED:56751) / no ingress rule matches\n```\n\nBut the untouched one remains!\n```bash\n$ curl -v http://nginy.ingress.tfk-negz.staging.k8s.REDACTED && echo\n Rebuilt URL to: http://nginy.ingress.tfk-negz.staging.k8s.REDACTED/\n   Trying REDACTED...\n TCP_NODELAY set\n Connected to nginy.ingress.tfk-negz.staging.k8s.REDACTED (REDACTED) port 80 (#0)\n\nGET / HTTP/1.1\nHost: nginy.ingress.tfk-negz.staging.k8s.REDACTED\nUser-Agent: curl/7.54.0\nAccept: /\n< HTTP/1.1 200 OK\n< Server: nginx/1.13.8\n< Date: Thu, 15 Feb 2018 22:42:39 GMT\n< Content-Type: text/html\n< Content-Length: 4\n< Last-Modified: Thu, 15 Feb 2018 22:34:21 GMT\n< ETag: \"5a860aed-4\"\n< Accept-Ranges: bytes\n< l5d-success-class: 1.0\n< Via: 1.1 linkerd, 1.1 linkerd\n<\n* Connection #0 to host nginy.ingress.tfk-negz.staging.k8s.REDACTED left intact\nHej!\n```. Thanks for working on this! FWIW I'm happy to switch our clusters over to running a nightly build once this is in to soak all the new changes before 1.3.6.. I suspect I may have just observed a variant of this issue. Running linkerd 1.4.6 and Kubernetes 1.11.2 on GCE (not GKE).\n\nhttps://gist.github.com/negz/54f00238405ad90aff84aa7ceda34b42\nAs you can see from the abridged config in the above gist we're running linkerd in linker to linker mode, including an ingress router. I was attempting to confirm my expected linkerd behaviour in the case where an http-ingress linkerd is unable to reach the Kubernetes API server and its downstream http-incoming routers become unavailable. To do so I:\n1. Deployed a staging cluster with three 'control' (i.e. API server) nodes, one dedicated 'ingress' (i.e. http-ingress) node and two 'worker' (i.e. http-incoming) nodes.\n1. Deployed 100 tiny nginx pods serving a small static file to the two worker nodes.\n1. Began sending 1,000qps to the http-ingress router via Slow Cooker.\n1. Deleted the control nodes to make the Kubernetes API servers unavailable.\n1. Deleted one of the two worker nodes. \n1. Confirmed the ingress node linkerd noticed the failed worker linkerd and removed it.\nAfter the final step I added three new API server instances with the intention of running another variant of the test. Our etcd clusters are offboard, so the three new instances simply pick up where the old ones left off. The cluster autoscaler kicked in shortly after I spun up the new control nodes, turning up a new worker to rehome the nginx pods that were running on the worker I previously deleted.\nAround this time two potentially interesting things happened; the Kubelet killed the ingress linkerd pod for failing its liveness probe (for reasons I have not determined), and the pod that came up to replace it was missing one of the worker nodes upon which nginx pods were now running, despite the Kubernetes API servers being back online and functioning correctly. In the attached namer state you can see 100 nginx pods running across two nodes, yet in the client state only one of those node's http-incoming linkerd router is listed. I polled the client state for ~15 minutes without observing any change.\nI (kind of) confirmed via the Kubernetes API that both worker nodes were listed as endpoints of the kube-system/l5d service. I'm not aware of any way to inspect whether linkerd is in sync with Kubernetes here.\n```\nThree control nodes, one ingress node, and two worker nodes makes six linkerds.\nPort:              http-incoming  4141/TCP                                                                                                                       \nTargetPort:        4141/TCP                                                                                                                                               \nEndpoints:         REDACTED.0.39:4141,REDACTED.0.82:4141,REDACTED.1.16:4141 + 3 more...                                                                                         \n```\nFWIW here's that ingress node failing its liveness probe for reasons I couldn't determine.\nWarning  Unhealthy  3m (x6 over 4m)  kubelet, tfk-negz-ing-mspx.c.REDACTED-staging.internal  Liveness probe failed: Get http://REDACTED.1.16:9990/admin/ping: net/http: request canceled (Client.Timeout exceeded while awaiting headers). https://gist.github.com/negz/953e8eaff6fce13bc5d46e9cf82b3e2f\nI'm seeing this or something like this again in production per the above gist. You can see the client state:\nREDACTED.0.52:4141 is tfk-uc13-wrk-69xs-zh0p\nREDACTED.2.173:4141 is tfk-uc13-wrk-6meb-fc8n\nREDACTED.0.232:4141 is tfk-uc13-wrk-wt7n-wcqd\nAt the time we observed this there was no live/tasking-api pod running on tfk-uc13-wrk-wt7n-wcqd. I see a pod on that node shows up in the lastStreamData, but not the response, which shows an accurate picture of the cluster.\nAs I write this things are even more weird. Here's where the pods really are:\n$ kubectl --context tfk-uc13 -n live get po -l component=api,service=tasking-api,team=honeybadgers -o wide\nNAME                           READY     STATUS    RESTARTS   AGE       IP               NODE\ntasking-api-75ddcc6f6c-7lsbq   1/1       Running   0          51m       REDACTED.122.222   tfk-uc13-wrk-6meb-fc8n.c.REDACTED-prod.internal\ntasking-api-75ddcc6f6c-fvlx5   1/1       Running   0          50m       REDACTED.31.174    tfk-uc13-wrk-6meb-m1ll.c.REDACTED-prod.internal\ntasking-api-75ddcc6f6c-nvrtd   1/1       Running   0          51m       REDACTED.1.41      tfk-uc13-wrk-69xs-zh0p.c.REDACTED-prod.internal\nAnd here's where the wonky linkerd thinks they are:\njson\n      \"watch\": {\n        \"request\": \"GET /api/v1/watch/namespaces/live/endpoints/tasking-api?resourceVersion=113658655\",\n        \"lastRequestAt\": \"2018-10-06 15:31:29 +0000\",\n        \"response\": {\n          \"subsets\": [\n            {\n              \"addresses\": [\n                {\n                  \"ip\": \"REDACTED.1.14\",\n                  \"nodeName\": \"tfk-uc13-wrk-69xs-zh0p.c.REDACTED-prod.internal\",\n                  \"targetRef\": {\n                    \"kind\": \"Pod\",\n                    \"namespace\": \"live\",\n                    \"name\": \"tasking-api-6bfbf9b65d-srkf8\",\n                    \"uid\": \"2538808c-c826-11e8-93e0-42010ab00159\",\n                    \"resourceVersion\": \"112706812\"\n                  }\n                },\n                {\n                  \"ip\": \"REDACTED.122.152\",\n                  \"nodeName\": \"tfk-uc13-wrk-6meb-fc8n.c.REDACTED-prod.internal\",\n                  \"targetRef\": {\n                    \"kind\": \"Pod\",\n                    \"namespace\": \"live\",\n                    \"name\": \"tasking-api-6bfbf9b65d-klqmk\",\n                    \"uid\": \"25466027-c826-11e8-93e0-42010ab00159\",\n                    \"resourceVersion\": \"112706203\"\n                  }\n                },\n                {\n                  \"ip\": \"REDACTED.92.22\",\n                  \"nodeName\": \"tfk-uc13-wrk-wt7n-qbj0.c.REDACTED-prod.internal\",\n                  \"targetRef\": {\n                    \"kind\": \"Pod\",\n                    \"namespace\": \"live\",\n                    \"name\": \"tasking-api-6bfbf9b65d-tzk2z\",\n                    \"uid\": \"3733f6f0-c826-11e8-93e0-42010ab00159\",\n                    \"resourceVersion\": \"112707520\"\n                  }\n                }\n              ],\n              \"ports\": [\n                {\n                  \"port\": 8080,\n                  \"protocol\": \"TCP\"\n                }\n              ]\n            }\n          ],\n          \"kind\": \"Endpoints\",\n          \"metadata\": {\n            \"name\": \"tasking-api\",\n            \"namespace\": \"live\",\n            \"selfLink\": \"/api/v1/namespaces/live/endpoints/tasking-api\",\n            \"uid\": \"89540f85-43ff-11e8-9f2e-42010ab0018e\",\n            \"resourceVersion\": \"112707521\",\n            \"creationTimestamp\": \"2018-04-19T18:29:03Z\",\n            \"labels\": {\n              \"component\": \"api\",\n              \"service\": \"tasking-api\",\n              \"team\": \"honeybadgers\"\n            }\n          },\n          \"apiVersion\": \"v1\"\n        },\n        \"lastResponseAt\": \"2018-10-05 09:42:45 +0000\",\n        \"lastStreamStartAt\": \"2018-10-06 14:33:05 +0000\",\n        \"lastStreamData\": {\n          \"type\": \"ADDED\",\n          \"object\": {\n            \"subsets\": [\n              {\n                \"addresses\": [\n                  {\n                    \"ip\": \"REDACTED.1.14\",\n                    \"nodeName\": \"tfk-uc13-wrk-69xs-zh0p.c.REDACTED-prod.internal\",\n                    \"targetRef\": {\n                      \"kind\": \"Pod\",\n                      \"namespace\": \"live\",\n                      \"name\": \"tasking-api-6bfbf9b65d-srkf8\",\n                      \"uid\": \"2538808c-c826-11e8-93e0-42010ab00159\",\n                      \"resourceVersion\": \"112706812\"\n                    }\n                  },\n                  {\n                    \"ip\": \"REDACTED.122.152\",\n                    \"nodeName\": \"tfk-uc13-wrk-6meb-fc8n.c.REDACTED-prod.internal\",\n                    \"targetRef\": {\n                      \"kind\": \"Pod\",\n                      \"namespace\": \"live\",\n                      \"name\": \"tasking-api-6bfbf9b65d-klqmk\",\n                      \"uid\": \"25466027-c826-11e8-93e0-42010ab00159\",\n                      \"resourceVersion\": \"112706203\"\n                    }\n                  },\n                  {\n                    \"ip\": \"REDACTED.35.84\",\n                    \"nodeName\": \"tfk-uc13-wrk-wt7n-wcqd.c.REDACTED-prod.internal\",\n                    \"targetRef\": {\n                      \"kind\": \"Pod\",\n                      \"namespace\": \"live\",\n                      \"name\": \"tasking-api-6bfbf9b65d-27hdw\",\n                      \"uid\": \"09779fe8-c8b1-11e8-93e0-42010ab00159\",\n                      \"resourceVersion\": \"113658654\"\n                    }\n                  }\n                ],\n                \"ports\": [\n                  {\n                    \"port\": 8080,\n                    \"protocol\": \"TCP\"\n                  }\n                ]\n              }\n            ],\n            \"kind\": \"Endpoints\",\n            \"metadata\": {\n              \"name\": \"tasking-api\",\n              \"namespace\": \"live\",\n              \"selfLink\": \"/api/v1/namespaces/live/endpoints/tasking-api\",\n              \"uid\": \"89540f85-43ff-11e8-9f2e-42010ab0018e\",\n              \"resourceVersion\": \"113658655\",\n              \"creationTimestamp\": \"2018-04-19T18:29:03Z\",\n              \"labels\": {\n                \"component\": \"api\",\n                \"service\": \"tasking-api\",\n                \"team\": \"honeybadgers\"\n              }\n            },\n            \"apiVersion\": \"v1\"\n          }\n        },\n        \"lastStreamDataAt\": \"2018-10-06 14:33:05 +0000\",\n        \"lastStreamEndAt\": \"2018-10-06 14:33:05 +0000\",\n        \"streaming\": true\n      }\n    }\n  },\nNote that neither the response nor lastStreamData are accurate now.. @adleong This was a separate incident from #2148 (this was a prod cluster, #2148 was a staging cluster). Unfortunately I have no way of confirming whether this was an instance of this bug or an instance of the issue I saw in #2148 in which the linkerd DaemonSet service had stale endpoints.. https://gist.github.com/negz/d8589423736702d5d17ef92ed809a887\nThe above gist contains all the artifacts I could gather.. ```\nps -c 23956\nPID CLS PRI TTY      STAT   TIME COMMAND\n23956 TS   19 ?        Ssl    5:23 /docker-java-home/jre/bin/java -XX:+PrintCommandLineFlags -Djava.net.preferIPv4Stack=true -Dsun.net.inetaddr.ttl=60 -Xms1024M -Xmx1024M -XX:+AggressiveOpts -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+CMSClassUnloadingEnabled -XX:+ScavengeBeforeFullGC -XX:+CMSScavengeBeforeRemark -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=70 -XX:-TieredCompilation -XX:+UseStringDeduplication -XX:+AlwaysPreTouch -Dcom.twitter.util.events.sinkEnabled=false -Dorg.apache.thrift.readLength=10485760 -Djdk.nio.maxCachedBufferSize=262144 -Dio.netty.threadLocalDirectBufferSize=0 -Dio.netty.recycler.maxCapacity=4096 -Dio.netty.allocator.numHeapArenas=8 -Dio.netty.allocator.numDirectArenas=8 -Dcom.twitter.finagle.netty4.numWorkers=8 -XX:+PerfDisableSharedMem -cp /io.buoyant/linkerd/1.4.6/bundle-exec:/io.buoyant/linkerd/1.4.6/plugins/linkerd-zipkin-1.4.3.jar -server io.buoyant.linkerd.Main /io.buoyant/linkerd/config/config.yml -log.level=WARNING\nlsof -p 23956|grep 8001\njava    23956 root  201u     IPv4            1038193      0t0     TCP localhost:50804->localhost:8001 (ESTABLISHED)\njava    23956 root  202u     IPv4            1044815      0t0     TCP localhost:50808->localhost:8001 (ESTABLISHED)\njava    23956 root  203u     IPv4            1043146      0t0     TCP localhost:50814->localhost:8001 (ESTABLISHED)\njava    23956 root  204u     IPv4            1047981      0t0     TCP localhost:50816->localhost:8001 (ESTABLISHED)\njava    23956 root  208u     IPv4            1046251      0t0     TCP localhost:52308->localhost:8001 (ESTABLISHED)\njava    23956 root  209u     IPv4            1048863      0t0     TCP localhost:52326->localhost:8001 (ESTABLISHED)\n```\nRunning a strace on the JVM while sending requests, it seems like every incoming HTTP request is causing linkerd to send an HTTP reques to something (kubectl proxy?) that results in a 404:\n```\nstrace -e read,write -f -p 23956 2>&1|grep 404\n[pid  5934] <... read resumed> \"HTTP/1.1 404 Not Found\\r\\nContent-\"..., 1024) = 298\n[pid  5935] <... read resumed> \"HTTP/1.1 404 Not Found\\r\\nContent-\"..., 1024) = 302\n[pid  5934] <... read resumed> \"HTTP/1.1 404 Not Found\\r\\nContent-\"..., 512) = 296\n[pid  5935] read(155, \"HTTP/1.1 404 Not Found\\r\\nContent-\"..., 1024) = 300\n[pid  5934] read(197, \"HTTP/1.1 404 Not Found\\r\\nContent-\"..., 512) = 298\n[pid  5934] <... read resumed> \"HTTP/1.1 404 Not Found\\r\\nContent-\"..., 496) = 296\n[pid  5935] <... read resumed> \"HTTP/1.1 404 Not Found\\r\\nContent-\"..., 512) = 300\n[pid  5935] <... read resumed> \"HTTP/1.1 404 Not Found\\r\\nContent-\"..., 512) = 298\n[pid  5934] read(197, \"HTTP/1.1 404 Not Found\\r\\nContent-\"..., 496) = 302\n[pid  5935] <... read resumed> \"HTTP/1.1 404 Not Found\\r\\nContent-\"..., 496) = 296\n[pid  5935] read(155, \"HTTP/1.1 404 Not Found\\r\\nContent-\"..., 496) = 300\n[pid  5940] <... read resumed> \"HTTP/1.1 404 Not Found\\r\\nContent-\"..., 1024) = 302\n[pid  5940] read(196, \"HTTP/1.1 404 Not Found\\r\\nContent-\"..., 1024) = 300\n[pid  5935] read(155, \"HTTP/1.1 404 Not Found\\r\\nContent-\"..., 480) = 298\n[pid  5935] <... read resumed> \"HTTP/1.1 404 Not Found\\r\\nContent-\"..., 480) = 296\n[pid  5934] read(200, \"HTTP/1.1 404 Not Found\\r\\nContent-\"..., 1024) = 300\n[pid  5935] read(155, \"HTTP/1.1 404 Not Found\\r\\nContent-\"..., 464) = 302\n[pid  5935] <... read resumed> \"HTTP/1.1 404 Not Found\\r\\nContent-\"..., 464) = 300\n[pid  5934] read(200, \"HTTP/1.1 404 Not Found\\r\\nContent-\"..., 1024) = 298\n[pid  5934] <... read resumed> \"HTTP/1.1 404 Not Found\\r\\nContent-\"..., 512) = 296\n[pid  5934] read(200, \"HTTP/1.1 404 Not Found\\r\\nContent-\"..., 512) = 298\n[pid  5936] read(155, \"HTTP/1.1 404 Not Found\\r\\nContent-\"..., 1024) = 302\n[pid  5936] <... read resumed> \"HTTP/1.1 404 Not Found\\r\\nContent-\"..., 1024) = 296\n```. (At this point you're just getting my live blog of the issue, sorry!) \nI tried hitting the kubectl proxy sidecar with a docker kill to restart it. I notice now that I'm seeing a whole bunch of the I1011 07:33:39.040508       1 logs.go:49] http: proxy error: context canceled errors on the kubectl proxy container whenever I send the linkerd pod requests. The kubectl proxy log lines stop when I stop sending linkerd requests. I can still successfully curl the Kubernetes API via the kubectl proxy sidecar both from the linkerd container and the underlying node despite the continuingcontext canceled log lines from the linkerd requests.. This is very strange. It seems like the Kubernetes API server is responding to something linkerd is asking for with HTTP 404, yet the linkerd namer state for the live/nginx service I'm requesting looks fine to me. \nThis screenshot shows the HTTP requests to the API server; the yellow is HTTP 404 responses to linkerd.\n\nWhen I curl the desired service and its endpoints via the kubectl proxy sidecar (or just kubectl get/describe them) they look as expected. I notice that when I run curl -v -H 'Host: nginx.live.ingress.tfk-negz.staging.k8s.REDACTED' http://localhost:4840 (the http-ingress router) on a worker node I see the error, yet when I run curl -v -H 'Host: nginx.live.ingress.tfk-negz.staging.k8s.REDACTED' http://localhost:4141 (the http-incoming router) I do not. This seems like perhaps the http-ingress router is somehow broken. linkerd responds to the error with the below message, which tells me it's able to resolve my Host header to the appropriate name path.\n```\nUnable to route request!\nservice name: /svc/live/80/nginx\nresolutions considered:\n  /#/io.l5d.k8s.http/live/80/nginx (neg)\n  /$/io.buoyant.porthostPfx/portNsSvc/live/80/nginx (neg)\n  /#/io.l5d.k8s.http/live/http/80 (neg)\n  /#/io.l5d.k8s.http/live/http/live (neg)\n  /#/io.l5d.k8s.http/live/80/80 (neg)\n  /#/io.l5d.k8s.http/live/80/live (neg)\ndtab:\nbase dtab:\n  /k8s => /#/io.l5d.k8s.http\n  /portNsSvc => /#/portNsSvcToK8s\n  /host => /portNsSvc/80/live\n  /host => /portNsSvc/80\n  /host => /portNsSvc/http/live\n  /host => /portNsSvc/http\n  /host => /$/io.buoyant.porthostPfx/portNsSvc\n  /svc => /$/io.buoyant.http.domainToPathPfx/host\n  /svc => /#/io.l5d.k8s.http\noverride dtab:\n```\nThe /#/io.l5d.k8s.http namer is configured to use the DaemonSet transformer, so I wonder if there's some issue with linkerd looking up where its DaemonSets are in the API server? As far as I can tell the service linkerd uses to find its DaemonSets is normal and working properly.\nAt this point I've restarted the offending pod completely, yet the problem persists, and I see the problem on the http-ingress router of a completely different pod, so it seems like the issue may be some kind of misconfiguration with this particular cluster I'm experimenting with rather than a linkerd bug. That being said, it's not at all obvious to me what is going wrong here so I'm going to leave the issue open until I get a chance to take another look in the morning.. Ah - the API server does not have access logs (at least not enabled by default, and I'm hesitant to restart the API servers in case they're causing the bug here). I did turn up the verbosity of kubectl proxy and it became obvious where the 404s are coming from; we're trying to watch a bunch of strange nonexistent paths due to our dtab rules:\n1 21:43:30.186736       1 proxy_server.go:138] Filter accepting GET /api/v1/namespaces/live/services/80 localhost\nI1011 21:43:30.187579       1 proxy_server.go:138] Filter accepting GET /api/v1/watch/namespaces/live/endpoints/80 localhost\nI1011 21:43:30.188432       1 proxy_server.go:138] Filter accepting GET /api/v1/namespaces/live/services/live localhost\nI1011 21:43:30.188945       1 proxy_server.go:138] Filter accepting GET /api/v1/watch/namespaces/live/endpoints/live localhost\n```\nUnable to route request!\nservice name: /svc/live/80/nginx\nresolutions considered:\n  /#/io.l5d.k8s.http/live/80/nginx (neg)\n  /$/io.buoyant.porthostPfx/portNsSvc/live/80/nginx (neg)\n  /#/io.l5d.k8s.http/live/http/80 (neg)\n  /#/io.l5d.k8s.http/live/http/live (neg)\n  /#/io.l5d.k8s.http/live/80/80 (neg)\n  /#/io.l5d.k8s.http/live/80/live (neg)\ndtab:\nbase dtab:\n  /k8s => /#/io.l5d.k8s.http\n  /portNsSvc => /#/portNsSvcToK8s\n  /host => /portNsSvc/80/live\n  /host => /portNsSvc/80\n  /host => /portNsSvc/http/live\n  /host => /portNsSvc/http\n  /host => /$/io.buoyant.porthostPfx/portNsSvc\n  /svc => /$/io.buoyant.http.domainToPathPfx/host\n  /svc => /#/io.l5d.k8s.http\noverride dtab:\n``. It seems like the broken service isn't actually binding to a set ofhttp-incomingdaemonsets for some reason. Here's a working service on the same cluster compared to the offendingnginx` service:\n\n\nMeanwhile I can't see anything obviously different between the service that does work and the one that doesn't. I do note that the nginx service is deployed to a different set of nodes than the dashboard service. Possibly the http-ingress router is unable to reach the downstream http-incoming routers?\n```bash\n$ kubectl --kubeconfig=$HOME/tfk-negz.kubecfg -n kube-system describe svc dashboard\nName:              dashboard\nNamespace:         kube-system\nLabels:            component=kube-dashboard\nAnnotations:       autoingresser.planet.com: true\nSelector:          component=kube-dashboard\nType:              ClusterIP\nIP:                192.168.171.154\nPort:              kube-dashboard  80/TCP\nTargetPort:        80/TCP\nEndpoints:         REDACTED.25.5:80\nSession Affinity:  None\nEvents:            \n$ kubectl --kubeconfig=$HOME/tfk-negz.kubecfg -n live describe svc nginx\nName:              nginx\nNamespace:         live\nLabels:            name=nginx\nAnnotations:       autoingresser.planet.com: true\nSelector:          component=nginx\nType:              ClusterIP\nIP:                192.168.162.167\nPort:              nginx  80/TCP\nTargetPort:        80/TCP\nEndpoints:         REDACTED.4.100:80,REDACTED.4.101:80,REDACTED.4.102:80 + 97 more...\nSession Affinity:  None\nEvents:            \n``. I've confirmed the container where the offendinghttp-ingressrouter runs can curl thehttp-incoming` router of both downstream linkers:\n```bash\ncurl -v -H 'Host: nginx.live.ingress.tfk-negz.staging.k8s.REDACTED'  http://REDACTED.1.35:4141\n Rebuilt URL to: http://REDACTED.1.35:4141/\n   Trying REDACTED.1.35...\n TCP_NODELAY set\n Connected to REDACTED.1.35 (REDACTED.1.35) port 4141 (#0)\n\nGET / HTTP/1.1\nHost: nginx.live.ingress.tfk-negz.staging.k8s.REDACTED\nUser-Agent: curl/7.52.1\nAccept: /\n< HTTP/1.1 200 OK\n< Via: 1.1 linkerd\n< l5d-success-class: 1.0\n< Accept-Ranges: bytes\n< Date: Fri, 12 Oct 2018 01:17:01 GMT\n< ETag: \"5bbff1f9-3\"\n< Server: nginx/1.15.5\n< Content-Length: 3\n< Last-Modified: Fri, 12 Oct 2018 00:59:37 GMT\n< Content-Type: text/html\n< \n Curl_http_done: called premature == 0\n Connection #0 to host REDACTED.1.35 left intact\nHi!\n```\n\nInterestingly one of the two downstream linkers is showing as a 'NotReadyAddress':\n```bash\n$ kubectl --kubeconfig=$HOME/tfk-negz.kubecfg -n kube-system describe endpoints l5d                                                                       \nName:         l5d\nNamespace:    kube-system\nLabels:       component=l5d\nAnnotations:  \nSubsets:\n  Addresses:          REDACTED.0.124,REDACTED.0.139,REDACTED.0.168,REDACTED.0.178,REDACTED.0.206,REDACTED.0.209,REDACTED.0.214,REDACTED.0.61,REDACTED.0.82,REDACTED.1.11,REDACTED.1.15,REDACTED.1.20   \n  NotReadyAddresses:  REDACTED.0.118,REDACTED.0.63,REDACTED.1.35\n  Ports:\n    Name           Port  Protocol\n    ----           ----  --------\n    admin          9990  TCP\n    h2-incoming    4241  TCP\n    grpc-outgoing  4340  TCP\n    grpc-incoming  4341  TCP\n    h2-outgoing    4240  TCP\n    http-incoming  4141  TCP\n    http-outgoing  4140  TCP\nEvents:  \n```\nThe nginx pods are running on the two tfk-negz-wrk nodes.\n$ kubectl --kubeconfig=$HOME/tfk-negz.kubecfg -n kube-system get po -l component=l5d -o wide\nNAME        READY   STATUS    RESTARTS   AGE   IP             NODE                                                       NOMINATED NODE\nl5d-bwxc2   2/2     Running   0          3h    REDACTED.0.179   tfk-negz-ing-8gmr.c.REDACTED-staging.internal            <none>\nl5d-m6pvh   2/2     Running   0          15h   REDACTED.0.239   tfk-negz-ctrl-l7d8.c.REDACTED-staging.internal           <none>\nl5d-rxlh5   2/2     Running   0          19h   REDACTED.1.35    tfk-negz-wrk-pre-hgrk-gwf5.c.REDACTED-staging.internal   <none>\nl5d-tw4gq   2/2     Running   0          3h    REDACTED.1.12    tfk-negz-wrk-pre-zc54-vkct.c.REDACTED-staging.internal   <none>\nl5d-wqjps   2/2     Running   0          7h    REDACTED.0.63    tfk-negz-ctrl-wltz.c.REDACTED-staging.internal           <none>\nl5d-z2977   2/2     Running   0          13h   REDACTED.1.15    tfk-negz-ctrl-gd2g.c.REDACTED-staging.internal           <none>. This seems like a likely culprit:\n```\n$ kubectl --kubeconfig=$HOME/tfk-negz.kubecfg -n kube-system get -o yaml endpoints l5d | grep 'tfk-negz-wrk-pre'\n    nodeName: tfk-negz-wrk-pre-ce7p-sv22.c.REDACTED-staging.internal\n    nodeName: tfk-negz-wrk-pre-zc54-ntpp.c.REDACTED-staging.internal\n    nodeName: tfk-negz-wrk-pre-e2jw-311h.c.REDACTED-staging.internal\n$ kubectl --kubeconfig=$HOME/tfk-negz.kubecfg -n kube-system get po -l component=l5d -o wide|grep 'tfk-negz-wrk-pre'\nl5d-gx9zn   2/2     Running   0          18m   REDACTED.1.82    tfk-negz-wrk-pre-hgrk-r3k2.c.REDACTED-staging.internal   \nl5d-rxlh5   2/2     Running   0          20h   REDACTED.1.35    tfk-negz-wrk-pre-hgrk-gwf5.c.REDACTED-staging.internal   \nl5d-ssgwd   2/2     Running   0          18m   REDACTED.1.81    tfk-negz-wrk-pre-zc54-9sl5.c.REDACTED-staging.internal   \nl5d-tw4gq   2/2     Running   0          4h    REDACTED.1.12    tfk-negz-wrk-pre-zc54-vkct.c.REDACTED-staging.internal   \n```\nNone of the active workers are actually showing up as endpoints of the l5d service for some reason.. I deleted and recreated the kube-system/l5d service that linkerd uses to discover the pods within its daemonset and everything started working as expected!\nWhatever was going wrong, it seems like it was at the Kubernetes end. That said, I feel like there's some linkerd debugging feedback to be had amongst my live blogging ;).\n\nThis isn't the first time I've suspected something was awry with the DaemonSet transformer while attempting to debug a linkerd issue. I wonder whether it might be worth exposing the DaemonSet transformer state in a similar fashion to the k8s namer state. Perhaps I should raise a bug for this?\nI don't understand why I was seeing ChannelClosedException from linkerd and the http: proxy error: context canceled, assuming the problem was that linkerd was unable to find any downstream http-incoming routers due to the busted l5d service. Is there a chance these are symptoms of linkerd trying to resolve fallback variants of the service and hitting 404s while trying to take a watch on API server endpoints that don't exist?. I've raised the above two issues to track some ideas that came out of this one. Thanks for humoring me while I debugged!. https://github.com/kubernetes/kubernetes/issues/66720\n\nFWIW it turns out our endpoints have been becoming stale due to the above Kubernetes issue.. > Do you have repro steps for how to trigger this error message?\nSadly no. I've seen it happen at least twice; once in #2148 and another time during a similar issue we saw in production, but I'm not able to reproduce it.. Not terribly useful, but we're seeing this again in production.\nW 1023 01:12:07.071 UTC THREAD34 TraceId:da817311cab52ccf: Exception propagated to the default monitor (upstream address: /127.0.0.1:53730, downstream address: localhost/127.0.0.1:8001, label: client).\nFailure(k8s observation released, flags=0x02) with RemoteInfo -> Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: localhost/127.0.0.1:8001, Downstream label: client, Trace Id: da817311cab52ccf.da817311cab52ccf<:da817311cab52ccf\nI see correlating log lines in the kubectl sidecar:\nI1023 01:09:56.468980       1 logs.go:49] http: proxy error: context canceled\nI1023 01:11:14.271047       1 logs.go:49] http: proxy error: context canceled\nI1023 01:13:19.468649       1 logs.go:49] http: proxy error: context canceled\nHonestly I'm not sure how to repro this or what more information to give you.. Sounds good to me on both counts. Will change.. I'm afraid I don't quite follow the suggestion - I'm very new to Scala. Are you suggesting something like this?\nscala\nval fallback =\n  if (strict) None\n  else spec.backend.map(b => ...)\nThat's how I had originally implemented this. I changed to the current implementation because I wanted to log in the None case and assumed (probably incorrectly) that I couldn't call log.warning from inside the assignment if/else.. I was in the process of changing this when I recalled another reason I ended up with the current implementation: determining the potential fallback path before making the decision whether to include it lets us log more specifically what default backend was omitted.\nWARNING: ingress the-ingress.default: ignoreDefaultBackends mode enabled, ignoring default backend: IngressPath(None,None,default,fallback,2022)\nAdmittedly the log line would already mention the ingress resource name and namespace, so including the stringified IngressPath is arguably superfluous.\nDoes this change the verdict on the current implementation? . Ack. Fixed!. This case block could probably use some close review. I mostly blindly copied and pasted from EndpointsNamerTest until it worked, so I'm sure it could be done better. \ud83e\udd13. Would it be worth making this a log.warn()? It seems like this should always be indicative of something weird/out of sync between the watch events and linkerd, and thus would provide a good signal to triage that kind of thing.. No luck with the first method:\nscala\n[error] /Users/negz/control/linkerd/k8s/src/test/scala/io/buoyant/k8s/IngressCacheTest.scala:478: missing parameter type for expanded function ((x$1: <error>) => x$1.s\nvc.$eq$eq(\"echo2\"))                                                  \n[error]     assert(await(cache.matchPath(host, \"/linkerd-2\")).contains(_.svc == \"echo2\"))\nSo I tried...\nscala\n[error] /Users/negz/control/linkerd/k8s/src/test/scala/io/buoyant/k8s/IngressCacheTest.scala:478: missing parameter type\n[error]     assert(await(cache.matchPath(host, \"/linkerd-2\")).contains(p => p.svc == \"echo2\"))\nSo I tried...\nscala\nassert(await(cache.matchPath(host, \"/linkerd-2\")).contains((p: IngressPath) => p.svc == \"echo2\"))\nAnd got...\n[info] - deleting an ingress does not reset the ingress cache *** FAILED ***\n[info]   Some(IngressPath(None,Some(/linkerd-2),default,echo2,7070)) did not contain io.buoyant.k8s.IngressCacheTest$$Lambda$1620/1333133483@4c5d52cd (IngressCacheTest.scala:478)\nThe org.scalatest.OptionValues worked though! I've switched over all the tests.. I could have sworn this test only started working as expected when I added the Promise, but it works totally fine without them as you say. Thanks!. Shouldn't we normally see an ADDED after a DELETED before any MODIFIED, assuming we had an uninterrupted watch? I don't think it matters for the purposes of this test, just curious if I misunderstand how things work.. Friendly reminder about potentially making this a log.warn  (or info?), assuming this PR will supercede #1810. :). ",
    "peterfroehlich": "Linkerd should provide a command line parameter to trigger the validation\nof the config file and depending files and then exit.\nReturn code should reflect the status.\nThat would be very neat to have in an automated configuration /\norchestration scenario to check the generated config inside the environment\nbefore restarting linkerd to take it live.\nAs said, this is standard practice for other proxies such as haproxy,\nnginx, etc...\nI would love to submit a PR... but I'm not fluent in Scala. ^^;;\n2018-02-14 3:13 GMT+01:00 Alex Leong notifications@github.com:\n\nBetter config validation is always a good idea!\nLinkerd will already exit immediately if the config file is absent or\nfails to parse. There are a few things I can see where we can improve:\n\nFor client tls trustCerts we could use the File config type instead\n   of string which will validate that the file exists\nDitto for server tls certPath, keyPath, caCertPath\n\nAre there other validations that you had in mind? Are you interested in\nsubmitting a PR for this?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/linkerd/linkerd/issues/1796#issuecomment-365473063,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACJjgkNRd3oq3eG9M5S4_oxlxi7ZmkZdks5tUkE_gaJpZM4R0CvM\n.\n. We are running in the same problems, but with serverset namers. Sometimes a namerd node stays responsive but does not recieve updates from zookeeper anymore. Logs are clean. . Not yet, but I will build something that writes everything down when the error happens again. \nAh, wait a second, there is no /namer_state/serverset endpoint in linkerd/namerd. :/. We ran into this problem again, and this time I looked through all documented admin endpoints. I found a Deadlock in the Contention output on the broken node: \n\n```\nDEADLOCKS:\n\"finagle/netty4-1-EventThread\" Id=52 BLOCKED on com.twitter.finagle.serverset2.Zk2Resolver@1fc72d68 owned by \"finagle/netty4-1-EventThread\" Id=28\n    at com.twitter.finagle.serverset2.Zk2Resolver.$anonfun$addrOf_$9(Zk2Resolver.scala:225)\n    -  blocked on com.twitter.finagle.serverset2.Zk2Resolver@1fc72d68\n    at com.twitter.finagle.serverset2.Zk2Resolver.$anonfun$addrOf_$9$adapted(Zk2Resolver.scala:213)\n    at com.twitter.finagle.serverset2.Zk2Resolver$$Lambda$834/781748775.apply(Unknown Source)\n    at com.twitter.util.Witness$$anon$17.notify(Event.scala:454)\n    at com.twitter.util.Event$$anon$8.$anonfun$register$13(Event.scala:220)\n    at com.twitter.util.Event$$anon$8.$anonfun$register$13$adapted(Event.scala:211)\n    at com.twitter.util.Event$$anon$8$$Lambda$847/1705443860.apply(Unknown Source)\n    at com.twitter.util.Function$.$anonfun$synchronizeWith$1(Function.scala:41)\n    -  locked java.lang.Object@5b5c78e7\n    ...\n\"finagle/netty4-1-EventThread\" Id=28 BLOCKED on com.twitter.util.Var$$anon$4@7bbdbc52 owned by \"finagle/netty4-7\" Id=45\n    at com.twitter.util.Var$$anon$4.observe(Var.scala:409)\n    -  blocked on com.twitter.util.Var$$anon$4@7bbdbc52\n    at com.twitter.util.Var$$anon$2.observe(Var.scala:68)\n    at com.twitter.util.Var$$anon$2.observe(Var.scala:68)\n    at com.twitter.util.Var$$anon$2.observe(Var.scala:68)\n    at com.twitter.util.Var$$anon$2.observe(Var.scala:68)\n    at com.twitter.util.Var$$anon$2.observe(Var.scala:68)\n    at com.twitter.util.Var$$anon$2.$anonfun$observe$1(Var.scala:80)\n    at com.twitter.util.Var$$anon$2.$anonfun$observe$1$adapted(Var.scala:68)\n    ...\n\"finagle/netty4-7\" Id=45 BLOCKED on com.twitter.finagle.serverset2.Zk2Resolver@1fc72d68 owned by \"finagle/netty4-1-EventThread\" Id=28\n    at com.twitter.finagle.serverset2.Zk2Resolver.$anonfun$addrOf_$9(Zk2Resolver.scala:225)\n    -  blocked on com.twitter.finagle.serverset2.Zk2Resolver@1fc72d68\n    at com.twitter.finagle.serverset2.Zk2Resolver.$anonfun$addrOf_$9$adapted(Zk2Resolver.scala:213)\n    at com.twitter.finagle.serverset2.Zk2Resolver$$Lambda$834/781748775.apply(Unknown Source)\n    at com.twitter.util.Witness$$anon$17.notify(Event.scala:454)\n    at com.twitter.util.Event$$anon$8.$anonfun$register$13(Event.scala:217)\n    at com.twitter.util.Event$$anon$8.$anonfun$register$13$adapted(Event.scala:211)\n    at com.twitter.util.Event$$anon$8$$Lambda$847/1705443860.apply(Unknown Source)\n    at com.twitter.util.Function$.$anonfun$synchronizeWith$1(Function.scala:41)\n    -  locked java.lang.Object@6dbffb33\n    ...\nNumber of locked synchronizers = 1\n- java.util.concurrent.ThreadPoolExecutor$Worker@2e05833c\n\n```\ncontention-mm6.txt\ncontention-mm6.pprof.zip\nstacks-mm6.txt\n. Arg, sorry, mixing notebooks and private / professional github accounts made a mess of things. ^^; \nI'll fix it tomorrow at work.. Superseeded by proper PR #2124 . Ah, sorry thats a typo. ^^;;; \nThe \"Id\" part was added by mistake, its TraceId.serialize.\nSee the Finagle doc: \nhttps://twitter.github.io/finagle/docs/com/twitter/finagle/tracing/TraceId$.html\n. ",
    "LukaszMarchewka": "I think an optional parameter \"-validate\" can solve the request. When the flag is set then Linkerd will exit immediately after loading a config with a proper status and a message.\nI can implement it, if you agree.. @Ashald I'm doing it now :). @Ashald I have added TLS option to a Consul Dtab Store. I have tested it with a Consul.. I have added validation only for a linkerd. If it is ok then i can implement it for a namer too.. @adleong I have changed the commit according your suggestions. Could you verify it ? . @adleong I have just move some part of a code from the main method to the new run method (the code is exactly the same). The main method is responsible for parameters \"parsing\" and then starting a linkerd via the run method. In my opinion this code is more readable than adding if statement into the main method. But if you want I can change it.. I have thought about it. I can use io.buoyant.linkerd.TlsClientConfig if you prefer.. ",
    "xiaobin": "I read code,at io.buoyant.admin.TimeZoneLogFormat,method:\n\nformat()\nline 59.\n\n\nthis use UTC time.. @olix0r sorry\uff0cThe past few days are Chinese New Year\uff0cI did not come and reply\u3002Thank you\uff01\n. Sorry, the problem is with me.\nI modified the configuration\uff1a\n\n```\nnamers:\n- kind: io.l5d.fs\n  rootDir: linkerd/examples/io.l5d.fs\n\nrouters:\n- protocol: http\n  identifier:\n    kind: io.l5d.path\n    segments: 1\n    consume: true\n  interpreter:\n    kind: io.l5d.fs\n    dtabFile: linkerd/examples/example.dtab\n  dtab: |\n    /svc/service-media=>/$/inet/LHSJ-slave/9999;\n    /http//=>/svc\n  servers:\n  - port: 4140\ntelemetry:\n- kind: io.l5d.prometheus\n- kind: io.l5d.recentRequests\n  sampleRate: 1.0\n  capacity: 10\n- kind: io.l5d.tracelog\n  sampleRate: 1.0\n  level: TRACE\n```\nthis work ok\uff01\n\n. \n",
    "shakti-das": "I was working on a PR for the fix but had a basic doubt on what value is expected when there are multiple GenericName type values in the SAN. Like:\nSubject Alternative Name:\nURI: https://xyz.com DNS: xyz.com DNS: www.xyz.com \nRight now the XFCC SAN values only picks up \"https://xyz.com\" as it's a single name type. But if we want to add multiple types here, then should it be something like:\nXFCC: Hash:....;SAN:https://xyz.com,xyz.com,www.xyz.com;....\nor\nXFCC: Hash:....;SAN:\"URI:https://xyz.com,DNS:xyz.com,DNS:www.xyz.com\". Agree, NameType:NameValue combination provides better details than just NameValue list. . Like this?\nbefore x-forwarded-client-cert: Hash=468ed33be74eee6556d90c0149c1309e9ba61d6425303443c0748a02dd8de688;SAN=https://example.com  \nafter x-forwarded-client-cert: Hash=468ed33be74eee6556d90c0149c1309e9ba61d6425303443c0748a02dd8de688;SAN=\"SAN:https://example.com;DNS:example.com\"\nIt still won't be backward compatible as earlier anyone using the SAN field from XFCC would have just parsed a single string assuming one URI value. Now, extracting the same SAN string from XFCC would return a delimiter separated string which needs to be parsed again.\nThe SAN in the actual certificate file looks like:\n            X509v3 Subject Alternative Name: \n                URI:https://buoyant.io, DNS:buoyant.io\nWill it be better for the future to keep it in the same format in XFCC SAN value as well?. Since this was replicated from envoy behavior, raised an issue for envoy as well to get some more clarity:  https://github.com/envoyproxy/envoy/issues/2535. But that would make it part of the XFCC value and not SAN as SAN itself is an element of the XFCC value.. From a backward compatibility point of view, it's definitely the cleanest way. And considering XFCC is a non-standard header we should be able to capture this information in doc and comments.\nMy thought was if we can specify the value to intuitively convey that the DNS entries are part of Subject Alternate Name in the actual certificate, that would have been great. \nDid a quick search in Envoy on config attribute verify_subject_alt_name, it seems envoy is considering all the SAN entires as a single list irrespective of type: https://github.com/envoyproxy/envoy/search?p=1&q=verify_subject_alt_name&type=&utf8=%E2%9C%93. @briansmith The backward compatible approach that you proposed looks best, should I send out a PR for this which has DNS entries as part of the XFCC? Like:\nHash=468ed33be74eee6556d90c0149c1309e9ba61d6425303443c0748a02dd8de688;SAN=https://example.com;DNS=example.com. Sorry for the late response, was out on vacation. I will send out a PR for it soon. Thanks.. @briansmith @wmorgan @adleong please review the changes.. @briansmith updated the PR to add DNS SAN entries by default.. @briansmith @wmorgan @adleong sorry to pester you guys, but am currently in the flow with this one so some new review comments would help me proceed quickly. Thanks.. Updated PR with the Scala code style suggestions.. Thanks reviewers for your time.. @drichelson thanks for catching this issue early.. @briansmith @wmorgan @adleong this is a follow-up PR to my original one https://github.com/linkerd/linkerd/pull/1826\nIt fixes the default case for SAN entries other than URI and DNS in ForwardClientCert. Modified the test config as well to add an IP SAN in order to catch this and similar errors in future. Please review. Thanks.. @wmorgan @adleong @briansmith Hi, I was wondering if this would be merged before releasing 1.3.7 as we are waiting on this fix for our internal implementation.. We faced this issue recently and have a solution for it. Creating a PR with the same changes that fixed the internal issue.. Sounds good, will make changes accordingly.. ",
    "yennifehrrr": "NOTE: This may be TMI, but coupling this linkerd version upgrade with com.twitter.finagle.netty4.trackReferenceLeaks set to true (to track reference leaks in metrics), resulted in 0 memory leaks; but corrupted the incoming protobuf intermittently, resulting in various InvalidProtocolBufferExceptions, so perhaps there is some correlation between whatever is causing the invalid protobufs when the flag is turned on, and the memory leak when it is turned off.. Hi @adleong , sorry this dropped off my list for a bit, but yes.\n1. I upgraded directly from 1.1.2 to 1.3.3/4/5, and all of them suffered from memory leaks, but did not try the versions in between 1.1.2 and 1.3.3.\n1. I can capture a heap dump, will do when I get a spare moment.\n1. A standalone app will be a little difficult, as I'd also need a gRPC endpoint that serves protobuf out in the open for you to test against. We generally don't provide this kind of data to third parties.. @pcalcado yup working on that heapdump. Once again, tricky because it may have confidential info in there... \ud83d\ude15 but I'll analyze first and see if it's safe to pass over. For the standalone app, this particular issue is when we use linkerd to make gRPC calls for protobuf payloads. So to reproduce, I think we'd need both an app making the calls and gRPC service open to the public serving these requests.. Hi @pcalcado @adleong I've taken a heap dump, but realized that all of our client secrets are in there and I don't have clearance to provide to 3rd parties. Is there anything else I can do to help?. @adleong oh great! The past load test looks promising; I'm going to keep one going for about half a day to a day, to make sure. Will keep you posted!. Hi @adleong It's looking a lot better, but running the load test for ~24 hrs, I'm still seeing a subtle climb:\n\nI'm going to keep the load test running overnight to see if the growth is continuous. This looks pretty different from prod where memory is flat over the same time interval.\n\n. Confirmed continuous memory increase over a ~two-day period. It does seem somewhat better than before.. Sorry, I forgot to update! I tried with the flags for about a day, and didn't see much difference, but this issue is hard to spot because memory creeps up slowly. I'll try running the load test for the whole weekend this weekend.. Hi! Sorry I've been MIA, this was not on my radar for awhile. I had to retry a few times due to some errors on our end. I finally re-ran for a whole wknd with the flags, and the results look promising tho not conclusive. When we get a chance I'm going to release this upgrade with the flags to a canary server and let it run for a day to see how things are. Will update then, thank you for your hard work on this!. Hi @adleong et al, just to put the final nail in this, we just canaried this over the weekend with the flags mentioned, and it looks great! We're pushing a full deployment today. Thanks to everyone for their help!!. ",
    "hynek": "JFTR, just tested with consul 1.0.6 and the problem persists.. I can confirm that deleting everything and creating namerd/dtabs/default in one step fixes the problem for me. Thanks everyone and good luck fixing. :). Sweet!. I\u2019m travelling and on my phone right now. I can send you the pem file and config later. I\u2019m fairly confident it\u2019s created using HashiCorp\u2019s Vault CA capabilities.. Here\u2019s our config template for now. Both admin and servers have the same problem.\n``` yaml\nadmin:\n  port: 9990\n  ip: {{ linkerd.admin.address }}\n  tls:\n    certPath: /etc/ssl/certs/.host.vrmd.de_all.crt\n    keyPath: /etc/ssl/private/.host.vrmd.de.pkcs8\nnamers:\n  - kind: io.l5d.consul\n    includeTag: false\n    useHealthCheck: true\n    token: XXX\nrouters:\n  - protocol: http\n    label: /http-consul\ninterpreter:\n  kind: io.l5d.mesh\n  dst: /#/io.l5d.consul/.local/namerd\n  experimental: true\n  root: /default\n\nservers:\n  - port: 4140\n    ip: {{ linkerd.router.address }}\n    addForwardedHeader:\n      by:\n        kind: ip\n      for:\n        kind: ip\n    tls:\n      certPath: /etc/ssl/certs/*.host.vrmd.de_all.crt\n      keyPath: /etc/ssl/private/*.host.vrmd.de.pkcs8\n    clearContext: {{ \"true\" if linkerd.edge else \"false\" }}\n\nclient:\n  kind: io.l5d.static\n  configs:\n    - prefix: /\n      requeueBudget:\n        percentCanRetry: 5.0\n      tls:\n        disableValidation: true\n\nservice:\n  kind: io.l5d.global\n  responseClassifier:\n    kind: io.l5d.http.retryableIdempotent5XX\n\ntelemetry:\n  - kind: io.l5d.prometheus\n    path: /admin/metrics/prometheus\n    prefix: linkerd_\nusage:\n  enabled: false # sorry :)\n```\nI double checked the logs if linkerd complains about cert loading but no dice.\nI suspect the reason nobody ran into it yet is that we use an intermediate certificate to be able to revoke everything at once without compromising our root and that\u2019s not that common? I suspect that\u2019s a path for you to reproduce.. (so yeah totally just http). It feels weird to paste it here but I guess the point of certificates is to make them public. :)\n\n\n```pem\n-----BEGIN CERTIFICATE-----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\nTA==\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----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\nxjNQPNcW5LR26j1qdTR9hucnDA==\n-----END CERTIFICATE-----\n```\n\n\n\nSo I have two theories for you:\n\nmaybe even your 1.3.7 accidentally uses a newer Finagle by accident?\nMy client configuration says disableValidation: true because I just couldn\u2019t find any good docs on how to get it working (my commonName is predictable but changing\u2026and I couldn\u2019t piece myself the information together from k8n-heavy blog posts\u2026). Anyhow, it says\n\n\nSetting disableValidation: true will force the use of the JDK SSL provider which does not support client auth.\n\nCould that be related and the reasons why you can\u2019t reproduce under 1.3.7? Different TLS engines are certainly a plausible reasons for a change in behavior\u2026. ",
    "MirzaMerdovic": "@adleong Thanks for detailed information and @chrisgoffinet thanks for investigating this one.\n@adleong I have one question you said here: \n\nThe result is that Linkerd maintains a connection to Consul per name it is watching.\n\nSo if I have thousands of requests towards the non-existing service Linkerd will cache 1 entry not thousands? In our load test we invoked 1-6 gRPC services, sorry if I got this all wrong.. @adleong Number of connection is consistent to the number of channels created, so there wasn't a problem there and there weren't many different service names in the play. \nWhat I don't understand is why ins scenario where linkerd invokes the same service and than when we change the service name while the linkerd still invokes the service by the old name (name that doesn't exist anymore) memory consumption grows. @chrisgoffinet explanation made sense to me, so can I asume the Consul is a culprit here?\nThis is not something that is blocking us in our work, and I don't find it as a serious issue since it probably won'r happen again and if it does it's easy to mitigate, but I am just curios to understand why is it happening.. @chrisgoffinet thanks for the clarification!. ",
    "pbagchi": "Literally just ran into this myself.. @adleong - This was the first time that we saw it yesterday. \nI'll check with io.l5d.mesh (it might be some time before I get around to doing it though).. We encountered this again last week and had to restart all Linkerd nodes. \nOn io.l5d.namerd.\nDetails:\nSeemed like some linkerd nodes got the change and some didn't. Checked the CRD - that definitely had the information.\nHere's a sampling of client_state.json from some of the nodes.\nWe were moving traffic from 100% search-1-0-4 to 100% search-1-0-6. \n{\u00a0\u00a0\n\u00a0\u00a0\u00a0\"/#/io.l5d.k8s/some-namespace/grpc/search-v1-0-4\":{\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"state\":\"bound\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"addresses\":[\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"<ip>:8080\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"<ip>:8080\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"<ip>:8080\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"<ip>:8080\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"<ip>:8080\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"<ip>:8080\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0]\n\u00a0\u00a0\u00a0},\n\u00a0\u00a0\u00a0\"/#/io.l5d.k8s/some-namespace/grpc/another-service-v1-0-3\":{\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"state\":\"bound\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"addresses\":[\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"<ip>:8080\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"<ip>:8080\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0]\n\u00a0\u00a0\u00a0},\n\u00a0\u00a0\u00a0\"/$/inet/namerd.service-mesh.svc.cluster.local/4100\":{\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"state\":\"bound\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"addresses\":[\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"namerd.service-mesh.svc.cluster.local:4100\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0]\n\u00a0\u00a0\u00a0},\n\u00a0\u00a0\u00a0\"/#/io.l5d.k8s/some-namespace/grpc/search-edge\":{\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"state\":\"bound\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"addresses\":[\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"<ip>:8080\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0]\n\u00a0\u00a0\u00a0}\n}\nAnother node had search-1-0-6.\n```\n{\u00a0\u00a0\n\u00a0\u00a0\u00a0\"/#/io.l5d.k8s/some-namespace/grpc/search-v1-0-5\":{\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"state\":\"Neg\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"addresses\":[\u00a0\u00a0\n]\n\u00a0\u00a0\u00a0},\n\u00a0\u00a0\u00a0\"/#/io.l5d.k8s/some-namespace/grpc/search-v1-0-4\":{\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"state\":\"bound\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"addresses\":[\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\":8080\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\":8080\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\":8080\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\":8080\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\":8080\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\":8080\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0]\n\u00a0\u00a0\u00a0},\n\u00a0\u00a0\u00a0\"/#/io.l5d.k8s/some-namespace/grpc/search-edge\":{\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"state\":\"bound\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"addresses\":[\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\":8080\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0]\n\u00a0\u00a0\u00a0},\n\u00a0\u00a0\u00a0\"/$/inet/namerd.service-mesh.svc.cluster.local/4100\":{\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"state\":\"bound\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"addresses\":[\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"namerd.service-mesh.svc.cluster.local:4100\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0]\n\u00a0\u00a0\u00a0},\n\u00a0\u00a0\u00a0\"/#/io.l5d.k8s/some-namespace/grpc/another-service-v1-0-3\":{\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"state\":\"bound\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"addresses\":[\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\":8080\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\":8080\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0]\n\u00a0\u00a0\u00a0},\n\u00a0\u00a0\u00a0\"/#/io.l5d.k8s/some-namespace/grpc/search-v1-0-6\":{\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"state\":\"bound\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"addresses\":[\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\":8080\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\":8080\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\":8080\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\":8080\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\":8080\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\":8080\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0]\n\u00a0\u00a0\u00a0}\n}\n```\n. Sure, will update to 1.4.1 and if we see this again, I'll get the diagnostics.. We did. \nCo-incidentally we haven't encountered this problem for the last few traffic-shapings that we did even for 1.3.7. \nI held off on updating to io.l5d.mesh from io.l5d.namerd so I can get diagnostics for this but I haven't seen it either in 1.3.7 or 1.4.1.. Sounds good to me. I can re-open it if it comes up again. \nBtw, I just noticed I mis-stated the version in my previous comment, we upgraded from 1.3.7 to 1.4.1 - I updated the comment.. @adleong - I think we are seeing this again. There is definitely mismatch. Can you point me to what should I be looking for in the interpreter_state? . @adleong - Sure, I can do that.. Opened a new one, closing this.. ",
    "jjshanks": "Did a little poking on this and I think it will require updating finagle which doesn't seem to be doable in place without code changes.. ",
    "voki": "@adleong To be honest I have no idea, how to reproduce this one. Happens randomly from my point of view. We are running 4 instances of namerd on unique public agents. This is mostly happening on our production environment where we have 5 masters. On staging environment, where we have 3 masters it happend maybe once over past few months. I've checked and it looks like namerd on stg had a little different config. I have updated storage section of namerd config on prd. I will let it roll for some time and let you know if this helped.\nstorage:\n  kind: io.l5d.zk\n  zkAddrs:\n  - host: <MASTER1_IP>\n    port: 2181\n  - host: <MASTER2_IP>\n    port: 2181\n  - host: <MASTER3_IP>\n    port: 2181\n  - host: <MASTER4_IP>\n    port: 2181\n  - host: <MASTER5_IP>\n    port: 2181\n  pathPrefix: /dtabs\n  sessionTimeoutMs: 30000. @adleong Sure, I can help with testing it.. Docker image would be much appreciated. I am quite busy this week, travelling and attending  workshops, but I will try to test it by Friday.  . @adleong Thanks, did you also fix namerd or linkerd only ?. Deployed on our STG env. I have lowered ZK timeout to 5 seconds for testing purposes.\nSo far looks like it works.\nShould I worry about this binding name errors in namerd log? Plenty of them being logged constantly.\n```I0228 06:35:56.955044 24189 fetcher.cpp:533] Fetcher Info: {\"cache_directory\":\"\\/tmp\\/mesos\\/fetch\\/root\",\"items\":[{\"action\":\"BYPASS_CACHE\",\"uri\":{\"cache\":false,\"executable\":false,\"extract\":true,\"value\":\"file:\\/\\/\\/etc\\/docker.tar.gz\"}},{\"action\":\"BYPASS_CACHE\",\"uri\":{\"cache\":false,\"executable\":false,\"extract\":false,\"value\":\"http:\\/\\/nginx.marathon.mesos:10777\\/namerd.yml\"}}],\"sandbox_directory\":\"\\/var\\/lib\\/mesos\\/slave\\/slaves\\/996deeba-f2d5-4ccd-9bee-39d1b2c735e9-S0\\/frameworks\\/6b2a572f-3dc1-4459-a696-6b4b4ee86bbd-0000\\/executors\\/namerd.a1e10f9f-1c51-11e8-a9e9-70b3d5800008\\/runs\\/69b78464-eae6-484d-9b1e-716661806c25\",\"user\":\"root\"}\nI0228 06:35:56.960553 24189 fetcher.cpp:444] Fetching URI 'file:///etc/docker.tar.gz'\nI0228 06:35:56.960580 24189 fetcher.cpp:285] Fetching directly into the sandbox directory\nI0228 06:35:56.960608 24189 fetcher.cpp:222] Fetching URI 'file:///etc/docker.tar.gz'\nI0228 06:35:56.963500 24189 fetcher.cpp:207] Copied resource '/etc/docker.tar.gz' to '/var/lib/mesos/slave/slaves/996deeba-f2d5-4ccd-9bee-39d1b2c735e9-S0/frameworks/6b2a572f-3dc1-4459-a696-6b4b4ee86bbd-0000/executors/namerd.a1e10f9f-1c51-11e8-a9e9-70b3d5800008/runs/69b78464-eae6-484d-9b1e-716661806c25/docker.tar.gz'\nI0228 06:35:57.066776 24189 fetcher.cpp:123] Extracted '/var/lib/mesos/slave/slaves/996deeba-f2d5-4ccd-9bee-39d1b2c735e9-S0/frameworks/6b2a572f-3dc1-4459-a696-6b4b4ee86bbd-0000/executors/namerd.a1e10f9f-1c51-11e8-a9e9-70b3d5800008/runs/69b78464-eae6-484d-9b1e-716661806c25/docker.tar.gz' into '/var/lib/mesos/slave/slaves/996deeba-f2d5-4ccd-9bee-39d1b2c735e9-S0/frameworks/6b2a572f-3dc1-4459-a696-6b4b4ee86bbd-0000/executors/namerd.a1e10f9f-1c51-11e8-a9e9-70b3d5800008/runs/69b78464-eae6-484d-9b1e-716661806c25'\nI0228 06:35:57.066835 24189 fetcher.cpp:582] Fetched 'file:///etc/docker.tar.gz' to '/var/lib/mesos/slave/slaves/996deeba-f2d5-4ccd-9bee-39d1b2c735e9-S0/frameworks/6b2a572f-3dc1-4459-a696-6b4b4ee86bbd-0000/executors/namerd.a1e10f9f-1c51-11e8-a9e9-70b3d5800008/runs/69b78464-eae6-484d-9b1e-716661806c25/docker.tar.gz'\nI0228 06:35:57.066846 24189 fetcher.cpp:444] Fetching URI 'http://nginx.marathon.mesos:10777/namerd.yml'\nI0228 06:35:57.066854 24189 fetcher.cpp:285] Fetching directly into the sandbox directory\nI0228 06:35:57.066874 24189 fetcher.cpp:222] Fetching URI 'http://nginx.marathon.mesos:10777/namerd.yml'\nI0228 06:35:57.066895 24189 fetcher.cpp:165] Downloading resource from 'http://nginx.marathon.mesos:10777/namerd.yml' to '/var/lib/mesos/slave/slaves/996deeba-f2d5-4ccd-9bee-39d1b2c735e9-S0/frameworks/6b2a572f-3dc1-4459-a696-6b4b4ee86bbd-0000/executors/namerd.a1e10f9f-1c51-11e8-a9e9-70b3d5800008/runs/69b78464-eae6-484d-9b1e-716661806c25/namerd.yml'\nI0228 06:35:57.079149 24189 fetcher.cpp:582] Fetched 'http://nginx.marathon.mesos:10777/namerd.yml' to '/var/lib/mesos/slave/slaves/996deeba-f2d5-4ccd-9bee-39d1b2c735e9-S0/frameworks/6b2a572f-3dc1-4459-a696-6b4b4ee86bbd-0000/executors/namerd.a1e10f9f-1c51-11e8-a9e9-70b3d5800008/runs/69b78464-eae6-484d-9b1e-716661806c25/namerd.yml'\nI0228 06:35:57.428531 24220 exec.cpp:162] Version: 1.4.2\nI0228 06:35:57.434134 24225 exec.cpp:236] Executor registered on agent 996deeba-f2d5-4ccd-9bee-39d1b2c735e9-S0\nI0228 06:35:57.435317 24225 executor.cpp:120] Registered docker executor on IP\nI0228 06:35:57.435379 24225 executor.cpp:160] Starting task namerd.a1e10f9f-1c51-11e8-a9e9-70b3d5800008\nFeb 28, 2018 6:36:02 AM com.twitter.finagle.http.HttpMuxer$ $anonfun$new$1\nINFO: HttpMuxer[/admin/metrics.json] = com.twitter.finagle.stats.MetricsExporter()\nFeb 28, 2018 6:36:02 AM com.twitter.finagle.http.HttpMuxer$ $anonfun$new$1\nINFO: HttpMuxer[/admin/per_host_metrics.json] = com.twitter.finagle.stats.HostMetricsExporter()\nI 0228 06:36:03.433 UTC THREAD1: namerd 1.3.5-SNAPSHOT (rev=207e230a5416fbb7907f4a0de3856d1b650063fb) built at 20180227-143720\nI 0228 06:36:07.004 UTC THREAD1: Closing zk session ffffffffffffffff\nI 0228 06:36:07.118 UTC THREAD1: Starting new zk session 0\nI 0228 06:36:07.496 UTC THREAD1: New ZKSession is connected. Session ID: 1618a6e09130280\nI 0228 06:36:08.118 UTC THREAD1: Finagle version 7.1.0 (rev=37212517b530319f4ba08cc7473c8cd8c4b83479) built at 20170906-132024\nI 0228 06:36:08.700 UTC THREAD1: Resolver[inet] = com.twitter.finagle.InetResolver(com.twitter.finagle.InetResolver@37191ef0)\nI 0228 06:36:08.701 UTC THREAD1: Resolver[fixedinet] = com.twitter.finagle.FixedInetResolver(com.twitter.finagle.FixedInetResolver@f1a45f8)\nI 0228 06:36:08.702 UTC THREAD1: Resolver[nil] = com.twitter.finagle.NilResolver$(com.twitter.finagle.NilResolver$@26a262d6)\nI 0228 06:36:08.702 UTC THREAD1: Resolver[fail] = com.twitter.finagle.FailResolver$(com.twitter.finagle.FailResolver$@11bd803)\nI 0228 06:36:08.702 UTC THREAD1: Resolver[neg] = com.twitter.finagle.NegResolver$(com.twitter.finagle.NegResolver$@4dbad37)\nI 0228 06:36:08.703 UTC THREAD1: Resolver[flag] = com.twitter.server.FlagResolver(com.twitter.server.FlagResolver@58f07f02)\nI 0228 06:36:08.705 UTC THREAD1: Resolver[zk2] = com.twitter.finagle.serverset2.Zk2Resolver(com.twitter.finagle.serverset2.Zk2Resolver@1ffcf674)\nI 0228 06:36:08.703 UTC THREAD1: Resolver[zk] = com.twitter.finagle.zookeeper.ZkResolver(com.twitter.finagle.zookeeper.ZkResolver@40f8f5a8)\nI 0228 06:36:10.117 UTC THREAD1: serving http on /0.0.0.0:9001\nI 0228 06:36:10.749 UTC THREAD1: Tracer: com.twitter.finagle.zipkin.thrift.ScribeZipkinTracer\nI 0228 06:36:10.918 UTC THREAD1: serving io.l5d.thriftNameInterpreter interface on /0.0.0.0:4100\nI 0228 06:36:11.005 UTC THREAD1: serving io.l5d.httpController interface on /0.0.0.0:4180\nI 0228 06:42:27.551 UTC THREAD38: Drainer is disabled; bypassing\nI 0228 06:42:31.246 UTC THREAD35 TraceId:57edf069cbefca88: Attempting to observe /dtabs/default\nI 0228 06:43:17.446 UTC THREAD35 TraceId:364b8cc5b07c80c4: Attempting to observe /dtabs/default\nI 0228 06:43:19.328 UTC THREAD37 TraceId:eb1409269f36debe: Attempting to observe /dtabs/default\nI 0228 06:43:19.346 UTC THREAD17 TraceId:1b66dcdca94ff198: FailureAccrualFactory marking connection to \"zipkin-tracer\" as dead. Remote Address: Inet(localhost/127.0.0.1:1463,Map())\nI 0228 06:43:20.329 UTC THREAD38 TraceId:7d3557596138e4b7: Attempting to observe /dtabs/default\nI 0228 06:43:20.664 UTC THREAD43 TraceId:fa2a553af9f593c8: Attempting to observe /dtabs/default\nI 0228 06:43:21.479 UTC THREAD31 TraceId:1386c7e4eddf05f1: Attempting to observe /dtabs/default\nI 0228 06:43:22.107 UTC THREAD37 TraceId:70e341738376002c: Attempting to observe /dtabs/default\nI 0228 06:43:22.639 UTC THREAD43 TraceId:d622c8575325449c: Attempting to observe /dtabs/default\nI 0228 06:43:24.042 UTC THREAD38 TraceId:d57d65c0364496d7: Attempting to observe /dtabs/default\nI 0228 06:43:33.814 UTC THREAD34 TraceId:5e061b034d749d29: FailureAccrualFactory marking connection to \"zipkin-tracer\" as dead. Remote Address: Inet(localhost/127.0.0.1:1463,Map())\nI 0228 06:44:09.086 UTC THREAD43 TraceId:dd21f4912c970405: Attempting to observe /dtabs/default\nI 0228 06:44:09.098 UTC THREAD17 TraceId:85e5dafe9e25c94e: FailureAccrualFactory marking connection to \"zipkin-tracer\" as dead. Remote Address: Inet(localhost/127.0.0.1:1463,Map())\nI 0228 06:44:10.392 UTC THREAD35 TraceId:e4f292625d396072: Attempting to observe /dtabs/default\nI 0228 06:44:12.238 UTC THREAD43 TraceId:f59c14e417b609a3: Attempting to observe /dtabs/default\nI 0228 06:44:12.452 UTC THREAD35 TraceId:dcf40817db695cd2: Attempting to observe /dtabs/default\nI 0228 06:44:19.032 UTC THREAD35 TraceId:36e32ac51ddfcf6b: Attempting to observe /dtabs/default\nI 0228 06:44:25.602 UTC THREAD36 TraceId:966c2f07d421d639: Attempting to observe /dtabs/default\nI 0228 06:44:25.612 UTC THREAD17 TraceId:0fb8cf2e235a612e: FailureAccrualFactory marking connection to \"zipkin-tracer\" as dead. Remote Address: Inet(localhost/127.0.0.1:1463,Map())\nI 0228 06:44:29.327 UTC THREAD43 TraceId:3ea7108c9f9492db: Attempting to observe /dtabs/default\nI 0228 06:44:29.645 UTC THREAD35 TraceId:8f098446cc29b023: Attempting to observe /dtabs/default\nI 0228 06:45:10.172 UTC THREAD31 TraceId:8185e25ef229042d: FailureAccrualFactory marking connection to \"zipkin-tracer\" as dead. Remote Address: Inet(localhost/127.0.0.1:1463,Map())\nI 0228 06:45:15.325 UTC THREAD35 TraceId:bbfd40de1274cac2: Attempting to observe /dtabs/default\nI 0228 06:45:18.076 UTC THREAD38 TraceId:389bd00556c40e02: Attempting to observe /dtabs/default\nI 0228 06:45:18.453 UTC THREAD38 TraceId:7aed57486113ea83: Attempting to observe /dtabs/default\nI 0228 06:45:18.532 UTC THREAD35 TraceId:b4f8312f39929a79: Attempting to observe /dtabs/default\nI 0228 06:45:18.778 UTC THREAD35 TraceId:167997d768319451: Attempting to observe /dtabs/default\nI 0228 06:45:18.981 UTC THREAD31 TraceId:e1b66cf7749fc9d6: Attempting to observe /dtabs/default\nI 0228 06:45:23.652 UTC THREAD36 TraceId:a8c504a2e8037acf: Attempting to observe /dtabs/default\nI 0228 06:45:24.341 UTC THREAD43 TraceId:ed1e9a0f2a8f2b5b: Attempting to observe /dtabs/default\nI 0228 06:45:24.873 UTC THREAD31 TraceId:754bc8f437c0ab62: Attempting to observe /dtabs/default\nI 0228 06:46:26.944 UTC THREAD43 TraceId:46afbdb3cecaab61: FailureAccrualFactory marking connection to \"zipkin-tracer\" as dead. Remote Address: Inet(localhost/127.0.0.1:1463,Map())\nI 0228 06:47:24.360 UTC THREAD38 TraceId:7456b4d9fcde1e36: Attempting to observe /dtabs/default\nI 0228 06:48:05.220 UTC THREAD36 TraceId:04185cf1c45f90c0: FailureAccrualFactory marking connection to \"zipkin-tracer\" as dead. Remote Address: Inet(localhost/127.0.0.1:1463,Map())\nI 0228 06:50:15.135 UTC THREAD35 TraceId:d1c8747152aa0b58: Attempting to observe /dtabs/default\nI 0228 06:50:15.326 UTC THREAD31 TraceId:2f32c40c72b32f7a: Attempting to observe /dtabs/default\nI 0228 06:50:32.898 UTC THREAD34 TraceId:5a6dbec3fca1b5e2: Attempting to observe /dtabs/default\nI 0228 06:52:11.128 UTC THREAD10 TraceId:29d4d96aac4aaf1b: FailureAccrualFactory marking connection to \"zipkin-tracer\" as dead. Remote Address: Inet(localhost/127.0.0.1:1463,Map())\nI 0228 06:57:11.256 UTC THREAD10 TraceId:fc1a22c0f6e4d7ef: FailureAccrualFactory marking connection to \"zipkin-tracer\" as dead. Remote Address: Inet(localhost/127.0.0.1:1463,Map())\nI 0228 07:00:27.328 UTC THREAD35 TraceId:135772bd8eb03315: Attempting to observe /dtabs/default\nE 0228 07:01:14.268 UTC THREAD38 TraceId:47f86b8d9ddd5065: binding name /svc/changecontrol.stg.crow\ncom.twitter.finagle.mux.ClientDiscardedRequestException: Failure(Released, flags=0x02) with NoSources\n    at com.twitter.finagle.mux.ServerDispatcher.process(Server.scala:275)\n    at com.twitter.finagle.mux.ServerDispatcher.$anonfun$loop$2(Server.scala:292)\n    at com.twitter.finagle.mux.ServerDispatcher.$anonfun$loop$2$adapted(Server.scala:290)\n    at com.twitter.util.Future$.$anonfun$each$1(Future.scala:1343)\n    at com.twitter.util.Future.$anonfun$flatMap$1(Future.scala:1740)\n    at com.twitter.util.Promise$Transformer.liftedTree1$1(Promise.scala:228)\n    at com.twitter.util.Promise$Transformer.k(Promise.scala:228)\n    at com.twitter.util.Promise$Transformer.apply(Promise.scala:239)\n    at com.twitter.util.Promise$Transformer.apply(Promise.scala:220)\n    at com.twitter.util.Promise$$anon$7.run(Promise.scala:547)\n    at com.twitter.concurrent.LocalScheduler$Activation.run(Scheduler.scala:198)\n    at com.twitter.concurrent.LocalScheduler$Activation.submit(Scheduler.scala:157)\n    at com.twitter.concurrent.LocalScheduler.submit(Scheduler.scala:274)\n    at com.twitter.concurrent.Scheduler$.submit(Scheduler.scala:109)\n    at com.twitter.util.Promise.runq(Promise.scala:522)\n    at com.twitter.util.Promise.updateIfEmpty(Promise.scala:887)\n    at com.twitter.util.Promise.update(Promise.scala:859)\n    at com.twitter.util.Promise.setValue(Promise.scala:835)\n    at com.twitter.concurrent.AsyncQueue.offer(AsyncQueue.scala:122)\n    at com.twitter.finagle.netty4.transport.ChannelTransport$$anon$1.channelRead(ChannelTransport.scala:183)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)\n    at com.twitter.finagle.netty4.channel.ChannelRequestStatsHandler.channelRead(ChannelRequestStatsHandler.scala:41)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at com.twitter.finagle.netty4.codec.BufCodec$.channelRead(BufCodec.scala:69)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:310)\n    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:284)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)\n    at com.twitter.finagle.netty4.channel.ChannelStatsHandler.channelRead(ChannelStatsHandler.scala:106)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1342)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:934)\n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:134)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run(BlockingTimeTrackingThreadFactory.scala:23)\n    at java.lang.Thread.run(Thread.java:748)\nE 0228 07:01:14.342 UTC THREAD38 TraceId:9ef40006908a53b3: binding name /svc/crowvictorops.stg.crow\ncom.twitter.finagle.mux.ClientDiscardedRequestException: Failure(Released, flags=0x02) with NoSources\n    at com.twitter.finagle.mux.ServerDispatcher.process(Server.scala:275)\n    at com.twitter.finagle.mux.ServerDispatcher.$anonfun$loop$2(Server.scala:292)\n    at com.twitter.finagle.mux.ServerDispatcher.$anonfun$loop$2$adapted(Server.scala:290)\n    at com.twitter.util.Future$.$anonfun$each$1(Future.scala:1343)\n    at com.twitter.util.Future.$anonfun$flatMap$1(Future.scala:1740)\n    at com.twitter.util.Promise$Transformer.liftedTree1$1(Promise.scala:228)\n    at com.twitter.util.Promise$Transformer.k(Promise.scala:228)\n    at com.twitter.util.Promise$Transformer.apply(Promise.scala:239)\n    at com.twitter.util.Promise$Transformer.apply(Promise.scala:220)\n    at com.twitter.util.Promise$$anon$7.run(Promise.scala:547)\n    at com.twitter.concurrent.LocalScheduler$Activation.run(Scheduler.scala:198)\n    at com.twitter.concurrent.LocalScheduler$Activation.submit(Scheduler.scala:157)\n    at com.twitter.concurrent.LocalScheduler.submit(Scheduler.scala:274)\n    at com.twitter.concurrent.Scheduler$.submit(Scheduler.scala:109)\n    at com.twitter.util.Promise.runq(Promise.scala:522)\n    at com.twitter.util.Promise.updateIfEmpty(Promise.scala:887)\n    at com.twitter.util.Promise.update(Promise.scala:859)\n    at com.twitter.util.Promise.setValue(Promise.scala:835)\n    at com.twitter.concurrent.AsyncQueue.offer(AsyncQueue.scala:122)\n    at com.twitter.finagle.netty4.transport.ChannelTransport$$anon$1.channelRead(ChannelTransport.scala:183)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)\n    at com.twitter.finagle.netty4.channel.ChannelRequestStatsHandler.channelRead(ChannelRequestStatsHandler.scala:41)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at com.twitter.finagle.netty4.codec.BufCodec$.channelRead(BufCodec.scala:69)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:310)\n    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:284)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)\n    at com.twitter.finagle.netty4.channel.ChannelStatsHandler.channelRead(ChannelStatsHandler.scala:106)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1342)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:934)\n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:134)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run(BlockingTimeTrackingThreadFactory.scala:23)\n    at java.lang.Thread.run(Thread.java:748)\nE 0228 07:01:14.567 UTC THREAD38 TraceId:f681133d5bdce6c6: resolving addr /#/io.l5d.marathon/crow/stg/calendar\ncom.twitter.finagle.mux.ClientDiscardedRequestException: Failure(Released, flags=0x02) with NoSources\n    at com.twitter.finagle.mux.ServerDispatcher.process(Server.scala:275)\n    at com.twitter.finagle.mux.ServerDispatcher.$anonfun$loop$2(Server.scala:292)\n    at com.twitter.finagle.mux.ServerDispatcher.$anonfun$loop$2$adapted(Server.scala:290)\n    at com.twitter.util.Future$.$anonfun$each$1(Future.scala:1343)\n    at com.twitter.util.Future.$anonfun$flatMap$1(Future.scala:1740)\n    at com.twitter.util.Promise$Transformer.liftedTree1$1(Promise.scala:228)\n    at com.twitter.util.Promise$Transformer.k(Promise.scala:228)\n    at com.twitter.util.Promise$Transformer.apply(Promise.scala:239)\n    at com.twitter.util.Promise$Transformer.apply(Promise.scala:220)\n    at com.twitter.util.Promise$$anon$7.run(Promise.scala:547)\n    at com.twitter.concurrent.LocalScheduler$Activation.run(Scheduler.scala:198)\n    at com.twitter.concurrent.LocalScheduler$Activation.submit(Scheduler.scala:157)\n    at com.twitter.concurrent.LocalScheduler.submit(Scheduler.scala:274)\n    at com.twitter.concurrent.Scheduler$.submit(Scheduler.scala:109)\n    at com.twitter.util.Promise.runq(Promise.scala:522)\n    at com.twitter.util.Promise.updateIfEmpty(Promise.scala:887)\n    at com.twitter.util.Promise.update(Promise.scala:859)\n    at com.twitter.util.Promise.setValue(Promise.scala:835)\n    at com.twitter.concurrent.AsyncQueue.offer(AsyncQueue.scala:122)\n    at com.twitter.finagle.netty4.transport.ChannelTransport$$anon$1.channelRead(ChannelTransport.scala:183)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)\n    at com.twitter.finagle.netty4.channel.ChannelRequestStatsHandler.channelRead(ChannelRequestStatsHandler.scala:41)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at com.twitter.finagle.netty4.codec.BufCodec$.channelRead(BufCodec.scala:69)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:310)\n    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:284)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)\n    at com.twitter.finagle.netty4.channel.ChannelStatsHandler.channelRead(ChannelStatsHandler.scala:106)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1342)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:934)\n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:134)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadP.oolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run(BlockingTimeTrackingThreadFactory.scala:23)\n    at java.lang.Thread.run(Thread.java:748)\n.\n.\n.\n```. And linkerd log looks like this:\n```\nI0228 06:40:31.612766 27210 fetcher.cpp:533] Fetcher Info: {\"cache_directory\":\"\\/tmp\\/mesos\\/fetch\\/root\",\"items\":[{\"action\":\"BYPASS_CACHE\",\"uri\":{\"cache\":false,\"executable\":false,\"extract\":true,\"value\":\"file:\\/\\/\\/etc\\/docker.tar.gz\"}},{\"action\":\"BYPASS_CACHE\",\"uri\":{\"cache\":false,\"executable\":false,\"extract\":false,\"value\":\"http:\\/\\/nginx.marathon.mesos:10777\\/linkerd.yml\"}}],\"sandbox_directory\":\"\\/var\\/lib\\/mesos\\/slave\\/slaves\\/996deeba-f2d5-4ccd-9bee-39d1b2c735e9-S9\\/frameworks\\/6b2a572f-3dc1-4459-a696-6b4b4ee86bbd-0000\\/executors\\/linkerd.458bb657-1c52-11e8-a9e9-70b3d5800008\\/runs\\/2febfd1c-6469-499d-b80e-69f1ca36c2bc\",\"user\":\"root\"}\nI0228 06:40:31.618230 27210 fetcher.cpp:444] Fetching URI 'file:///etc/docker.tar.gz'\nI0228 06:40:31.618257 27210 fetcher.cpp:285] Fetching directly into the sandbox directory\nI0228 06:40:31.618284 27210 fetcher.cpp:222] Fetching URI 'file:///etc/docker.tar.gz'\nI0228 06:40:31.620911 27210 fetcher.cpp:207] Copied resource '/etc/docker.tar.gz' to '/var/lib/mesos/slave/slaves/996deeba-f2d5-4ccd-9bee-39d1b2c735e9-S9/frameworks/6b2a572f-3dc1-4459-a696-6b4b4ee86bbd-0000/executors/linkerd.458bb657-1c52-11e8-a9e9-70b3d5800008/runs/2febfd1c-6469-499d-b80e-69f1ca36c2bc/docker.tar.gz'\nI0228 06:40:31.723994 27210 fetcher.cpp:123] Extracted '/var/lib/mesos/slave/slaves/996deeba-f2d5-4ccd-9bee-39d1b2c735e9-S9/frameworks/6b2a572f-3dc1-4459-a696-6b4b4ee86bbd-0000/executors/linkerd.458bb657-1c52-11e8-a9e9-70b3d5800008/runs/2febfd1c-6469-499d-b80e-69f1ca36c2bc/docker.tar.gz' into '/var/lib/mesos/slave/slaves/996deeba-f2d5-4ccd-9bee-39d1b2c735e9-S9/frameworks/6b2a572f-3dc1-4459-a696-6b4b4ee86bbd-0000/executors/linkerd.458bb657-1c52-11e8-a9e9-70b3d5800008/runs/2febfd1c-6469-499d-b80e-69f1ca36c2bc'\nI0228 06:40:31.724058 27210 fetcher.cpp:582] Fetched 'file:///etc/docker.tar.gz' to '/var/lib/mesos/slave/slaves/996deeba-f2d5-4ccd-9bee-39d1b2c735e9-S9/frameworks/6b2a572f-3dc1-4459-a696-6b4b4ee86bbd-0000/executors/linkerd.458bb657-1c52-11e8-a9e9-70b3d5800008/runs/2febfd1c-6469-499d-b80e-69f1ca36c2bc/docker.tar.gz'\nI0228 06:40:31.724069 27210 fetcher.cpp:444] Fetching URI 'http://nginx.marathon.mesos:10777/linkerd.yml'\nI0228 06:40:31.724076 27210 fetcher.cpp:285] Fetching directly into the sandbox directory\nI0228 06:40:31.724097 27210 fetcher.cpp:222] Fetching URI 'http://nginx.marathon.mesos:10777/linkerd.yml'\nI0228 06:40:31.724117 27210 fetcher.cpp:165] Downloading resource from 'http://nginx.marathon.mesos:10777/linkerd.yml' to '/var/lib/mesos/slave/slaves/996deeba-f2d5-4ccd-9bee-39d1b2c735e9-S9/frameworks/6b2a572f-3dc1-4459-a696-6b4b4ee86bbd-0000/executors/linkerd.458bb657-1c52-11e8-a9e9-70b3d5800008/runs/2febfd1c-6469-499d-b80e-69f1ca36c2bc/linkerd.yml'\nI0228 06:40:31.778082 27210 fetcher.cpp:582] Fetched 'http://nginx.marathon.mesos:10777/linkerd.yml' to '/var/lib/mesos/slave/slaves/996deeba-f2d5-4ccd-9bee-39d1b2c735e9-S9/frameworks/6b2a572f-3dc1-4459-a696-6b4b4ee86bbd-0000/executors/linkerd.458bb657-1c52-11e8-a9e9-70b3d5800008/runs/2febfd1c-6469-499d-b80e-69f1ca36c2bc/linkerd.yml'\nI0228 06:41:19.353930 27449 exec.cpp:162] Version: 1.4.2\nI0228 06:41:19.370368 27459 exec.cpp:236] Executor registered on agent 996deeba-f2d5-4ccd-9bee-39d1b2c735e9-S9\nI0228 06:41:19.371492 27453 executor.cpp:120] Registered docker executor on IP\nI0228 06:41:19.371582 27453 executor.cpp:160] Starting task linkerd.458bb657-1c52-11e8-a9e9-70b3d5800008\nFeb 28, 2018 6:41:29 AM com.twitter.finagle.http.HttpMuxer$ $anonfun$new$1\nINFO: HttpMuxer[/admin/metrics.json] = com.twitter.finagle.stats.MetricsExporter()\nFeb 28, 2018 6:41:29 AM com.twitter.finagle.http.HttpMuxer$ $anonfun$new$1\nINFO: HttpMuxer[/admin/per_host_metrics.json] = com.twitter.finagle.stats.HostMetricsExporter()\nI 0228 06:41:30.433 UTC THREAD1: linkerd 1.3.5-SNAPSHOT (rev=207e230a5416fbb7907f4a0de3856d1b650063fb) built at 20180226-173215\nI 0228 06:41:33.521 UTC THREAD1: Finagle version 7.1.0 (rev=37212517b530319f4ba08cc7473c8cd8c4b83479) built at 20170906-132024\nI 0228 06:41:43.442 UTC THREAD1: Tracer: com.twitter.finagle.zipkin.thrift.ScribeZipkinTracer\nI 0228 06:41:43.626 UTC THREAD1: connecting to usageData proxy at Set(Inet(stats.buoyant.io/104.28.23.233:443,Map()))\nI 0228 06:41:44.041 UTC THREAD1: tracer: io.buoyant.telemetry.ZipkinConfig$$anon$1@66ac89d9\nI 0228 06:41:44.041 UTC THREAD1: tracer: io.buoyant.telemetry.recentRequests.RecentRequetsTracer@252fa00\nI 0228 06:41:44.343 UTC THREAD1: Resolver[inet] = com.twitter.finagle.InetResolver(com.twitter.finagle.InetResolver@33f2df51)\nI 0228 06:41:44.344 UTC THREAD1: Resolver[fixedinet] = com.twitter.finagle.FixedInetResolver(com.twitter.finagle.FixedInetResolver@107bfcb2)\nI 0228 06:41:44.344 UTC THREAD1: Resolver[neg] = com.twitter.finagle.NegResolver$(com.twitter.finagle.NegResolver$@7bac686b)\nI 0228 06:41:44.344 UTC THREAD1: Resolver[nil] = com.twitter.finagle.NilResolver$(com.twitter.finagle.NilResolver$@2ab26378)\nI 0228 06:41:44.345 UTC THREAD1: Resolver[fail] = com.twitter.finagle.FailResolver$(com.twitter.finagle.FailResolver$@f9f3928)\nI 0228 06:41:44.345 UTC THREAD1: Resolver[flag] = com.twitter.server.FlagResolver(com.twitter.server.FlagResolver@404eca05)\nI 0228 06:41:44.345 UTC THREAD1: Resolver[zk] = com.twitter.finagle.zookeeper.ZkResolver(com.twitter.finagle.zookeeper.ZkResolver@58b91d57)\nI 0228 06:41:44.346 UTC THREAD1: Resolver[zk2] = com.twitter.finagle.serverset2.Zk2Resolver(com.twitter.finagle.serverset2.Zk2Resolver@61a91c9b)\nI 0228 06:41:46.140 UTC THREAD1: serving http admin on /0.0.0.0:9990\nI 0228 06:41:46.236 UTC THREAD1: serving outgoing on /0.0.0.0:4140\nI 0228 06:41:46.428 UTC THREAD1: serving incoming on /0.0.0.0:4141\nI 0228 06:41:46.533 UTC THREAD1: initialized\nI 0228 06:44:18.886 UTC THREAD26: Response with a status code of 204 must not have a Content-Length header field thus the field has been removed. Content-Length: 0\nI 0228 06:44:20.624 UTC THREAD26: Response with a status code of 204 must not have a Content-Length header field thus the field has been removed. Content-Length: 0\nI 0228 06:44:21.523 UTC THREAD26: Response with a status code of 204 must not have a Content-Length header field thus the field has been removed. Content-Length: 0\nI 0228 06:44:23.231 UTC THREAD26: Response with a status code of 204 must not have a Content-Length header field thus the field has been removed. Content-Length: 0\nI 0228 06:49:19.156 UTC THREAD26: Response with a status code of 204 must not have a Content-Length header field thus the field has been removed. Content-Length: 0\nI 0228 06:53:37.403 UTC THREAD26: Response with a status code of 204 must not have a Content-Length header field thus the field has been removed. Content-Length: 0\nI 0228 07:00:28.199 UTC THREAD26: Response with a status code of 204 must not have a Content-Length header field thus the field has been removed. Content-Length: 0\nE 0228 07:01:46.311 UTC THREAD10 TraceId:533cb9855389ff2a: bind /svc/calendar.stg.crow\nFailure(Released, flags=0x02) with RemoteInfo -> Upstream Address: /IP:34938, Upstream id: Not Available, Downstream Address: namerd.marathon.mesos/IP:4100, Downstream label: interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig, Trace Id: 636e5fab1cc4f321.636e5fab1cc4f321<:636e5fab1cc4f321 with Service -> interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig\nI 0228 07:01:46.341 UTC THREAD10: Reaping /svc/calendar.stg.crow\nE 0228 07:01:46.352 UTC THREAD10 TraceId:1293b7b91b630180: bind /svc/crowjira.stg.crow\nFailure(Released, flags=0x02) with RemoteInfo -> Upstream Address: /172.17.0.8:39144, Upstream id: Not Available, Downstream Address: namerd.marathon.mesos/IP:4100, Downstream label: interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig, Trace Id: e6cab47c0d38db60.e6cab47c0d38db60<:e6cab47c0d38db60 with Service -> interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig\nI 0228 07:01:46.354 UTC THREAD10: Reaping /svc/crowjira.stg.crow\nE 0228 07:01:46.464 UTC THREAD10 TraceId:1293b7b91b630180: addr on /#/io.l5d.marathon/crow/stg/crowjira\nFailure(Released, flags=0x02) with RemoteInfo -> Upstream Address: /172.17.0.8:39144, Upstream id: Not Available, Downstream Address: namerd.marathon.mesos/IP:4100, Downstream label: interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig, Trace Id: b28916dc131e3fc8.b28916dc131e3fc8<:b28916dc131e3fc8 with Service -> interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig\nI 0228 07:01:46.528 UTC THREAD10: Reaping %/io.l5d.port/4141/#/io.l5d.marathon/crow/stg/crowjira\nE 0228 07:01:46.625 UTC THREAD10 TraceId:533cb9855389ff2a: addr on /#/io.l5d.marathon/crow/stg/calendar\nFailure(Released, flags=0x02) with RemoteInfo -> Upstream Address: /IP:34938, Upstream id: Not Available, Downstream Address: namerd.marathon.mesos/IP:4100, Downstream label: interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig, Trace Id: 3f8b1696897ac31c.3f8b1696897ac31c<:3f8b1696897ac31c with Service -> interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig\nI 0228 07:01:46.628 UTC THREAD10: Reaping %/io.l5d.port/4141/#/io.l5d.marathon/crow/stg/calendar\nE 0228 07:01:46.642 UTC THREAD10 TraceId:b531fac72663fbd3: bind /svc/%/io.l5d.port/4141/#/io.l5d.marathon/crow/stg/bphcspinoff\nFailure(Released, flags=0x02) with RemoteInfo -> Upstream Address: /IP:34130, Upstream id: Not Available, Downstream Address: namerd.marathon.mesos/IP:4100, Downstream label: interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig, Trace Id: 4d7630b7af502203.4d7630b7af502203<:4d7630b7af502203 with Service -> interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig\nI 0228 07:01:46.644 UTC THREAD10: Reaping /svc/%/io.l5d.port/4141/#/io.l5d.marathon/crow/stg/bphcspinoff\nE 0228 07:01:46.748 UTC THREAD10 TraceId:b531fac72663fbd3: addr on /#/io.l5d.marathon/crow/stg/bphcspinoff\nFailure(Released, flags=0x02) with RemoteInfo -> Upstream Address: /IP:34130, Upstream id: Not Available, Downstream Address: namerd.marathon.mesos/IP:4100, Downstream label: interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig, Trace Id: 5c4ca5e544e1d7bf.5c4ca5e544e1d7bf<:5c4ca5e544e1d7bf with Service -> interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig\nI 0228 07:01:46.752 UTC THREAD10: Reaping %/io.l5d.localhost/#/io.l5d.marathon/crow/stg/bphcspinoff\nE 0228 07:11:46.668 UTC THREAD10 TraceId:8aa5bd0516b80eb1: bind /svc/%/io.l5d.port/4141/#/io.l5d.marathon/crow/stg/crowmailer\nFailure(Released, flags=0x02) with RemoteInfo -> Upstream Address: /IP:52744, Upstream id: Not Available, Downstream Address: namerd.marathon.mesos/IP:4100, Downstream label: interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig, Trace Id: a88c1e5356718ea2.a88c1e5356718ea2<:a88c1e5356718ea2 with Service -> interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig\nI 0228 07:11:46.673 UTC THREAD10: Reaping /svc/%/io.l5d.port/4141/#/io.l5d.marathon/crow/stg/crowmailer\nE 0228 07:11:46.861 UTC THREAD10 TraceId:8aa5bd0516b80eb1: addr on /#/io.l5d.marathon/crow/stg/crowmailer\nFailure(Released, flags=0x02) with RemoteInfo -> Upstream Address: /43.194.55.92:52744, Upstream id: Not Available, Downstream Address: namerd.marathon.mesos/43.194.55.83:4100, Downstream label: interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig, Trace Id: 1c0fc5d428ded900.1c0fc5d428ded900<:1c0fc5d428ded900 with Service -> interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig\nI 0228 07:11:46.918 UTC THREAD10: Reaping %/io.l5d.localhost/#/io.l5d.marathon/crow/stg/crowmailer\nI 0228 07:13:27.097 UTC THREAD27: Response with a status code of 204 must not have a Content-Length header field thus the field has been removed. Content-Length: 0\nI 0228 07:20:17.541 UTC THREAD39: Response with a status code of 204 must not have a Content-Length header field thus the field has been removed. Content-Length: 0\nE 0228 07:21:46.383 UTC THREAD10 TraceId:a766fa10d8bec469: bind /svc/healthcheck.stg.crow\nFailure(Released, flags=0x02) with RemoteInfo -> Upstream Address: /IP:36068, Upstream id: Not Available, Downstream Address: namerd.marathon.mesos/IP:4100, Downstream label: interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig, Trace Id: eef81561a435b3c3.eef81561a435b3c3<:eef81561a435b3c3 with Service -> interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig\nI 0228 07:21:46.385 UTC THREAD10: Reaping /svc/healthcheck.stg.crow\nE 0228 07:21:46.396 UTC THREAD10 TraceId:6ebe2b007d14221d: bind /svc/bpsnc.stg.crow\nFailure(Released, flags=0x02) with RemoteInfo -> Upstream Address: /IP:34634, Upstream id: Not Available, Downstream Address: namerd.marathon.mesos/IP:4100, Downstream label: interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig, Trace Id: fa17adf0f8cb2aa4.fa17adf0f8cb2aa4<:fa17adf0f8cb2aa4 with Service -> interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig\nI 0228 07:21:46.399 UTC THREAD10: Reaping /svc/bpsnc.stg.crow\nE 0228 07:21:46.677 UTC THREAD10 TraceId:a766fa10d8bec469: addr on /#/io.l5d.marathon/crow/stg/healthcheck\nFailure(Released, flags=0x02) with RemoteInfo -> Upstream Address: /IP:36068, Upstream id: Not Available, Downstream Address: namerd.marathon.mesos/IP:4100, Downstream label: interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig, Trace Id: 9380c0a9afdf13bd.9380c0a9afdf13bd<:9380c0a9afdf13bd with Service -> interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig\nI 0228 07:21:46.680 UTC THREAD10: Reaping %/io.l5d.port/4141/#/io.l5d.marathon/crow/stg/healthcheck\nE 0228 07:21:46.828 UTC THREAD10 TraceId:6ebe2b007d14221d: addr on /#/io.l5d.marathon/crow/stg/bpsnc\nFailure(Released, flags=0x02) with RemoteInfo -> Upstream Address: /IP:34634, Upstream id: Not Available, Downstream Address: namerd.marathon.mesos/IP:4100, Downstream label: interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig, Trace Id: aaecdf7995596305.aaecdf7995596305<:aaecdf7995596305 with Service -> interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig\nI 0228 07:21:46.831 UTC THREAD10: Reaping %/io.l5d.port/4141/#/io.l5d.marathon/crow/stg/bpsnc\nI 0228 07:23:31.929 UTC THREAD27: Response with a status code of 204 must not have a Content-Length header field thus the field has been removed. Content-Length: 0\nI 0228 07:30:23.082 UTC THREAD28: Response with a status code of 204 must not have a Content-Length header field thus the field has been removed. Content-Length: 0\nE 0228 07:31:46.413 UTC THREAD10 TraceId:154f9a4264f9c8d2: bind /svc/wmback.stg.crow\nFailure(Released, flags=0x02) with RemoteInfo -> Upstream Address: /IP:38094, Upstream id: Not Available, Downstream Address: namerd.marathon.mesos/IP:4100, Downstream label: interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig, Trace Id: 0e195c55bdde56b2.0e195c55bdde56b2<:0e195c55bdde56b2 with Service -> interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig\nI 0228 07:31:46.416 UTC THREAD10: Reaping /svc/wmback.stg.crow\nE 0228 07:31:46.425 UTC THREAD10 TraceId:c8856bd703d18105: bind /svc/crowjira.stg.crow\nFailure(Released, flags=0x02) with RemoteInfo -> Upstream Address: /172.17.0.8:42426, Upstream id: Not Available, Downstream Address: namerd.marathon.mesos/IP:4100, Downstream label: interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig, Trace Id: 3ee24d86cb1830f9.3ee24d86cb1830f9<:3ee24d86cb1830f9 with Service -> interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig\nI 0228 07:31:46.428 UTC THREAD10: Reaping /svc/crowjira.stg.crow\nE 0228 07:31:46.437 UTC THREAD10 TraceId:8cd439ab9a14deb9: bind /svc/bpavscan.stg.crow\nFailure(Released, flags=0x02) with RemoteInfo -> Upstream Address: /IP:38742, Upstream id: Not Available, Downstream Address: namerd.marathon.mesos/IP:4100, Downstream label: interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig, Trace Id: 6e30c9dfcaf3b2eb.6e30c9dfcaf3b2eb<:6e30c9dfcaf3b2eb with Service -> interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig\nI 0228 07:31:46.439 UTC THREAD10: Reaping /svc/bpavscan.stg.crow\nE 0228 07:31:46.873 UTC THREAD10 TraceId:c8856bd703d18105: addr on /#/io.l5d.marathon/crow/stg/crowjira\nFailure(Released, flags=0x02) with RemoteInfo -> Upstream Address: /172.17.0.8:42426, Upstream id: Not Available, Downstream Address: namerd.marathon.mesos/IP:4100, Downstream label: interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig, Trace Id: de98e6d1e8af5e18.de98e6d1e8af5e18<:de98e6d1e8af5e18 with Service -> interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig\nI 0228 07:31:46.877 UTC THREAD10: Reaping %/io.l5d.port/4141/#/io.l5d.marathon/crow/stg/crowjira\nE 0228 07:31:46.943 UTC THREAD10 TraceId:8cd439ab9a14deb9: addr on /#/io.l5d.marathon/crow/stg/bpavscan\nFailure(Released, flags=0x02) with RemoteInfo -> Upstream Address: /IP:38742, Upstream id: Not Available, Downstream Address: namerd.marathon.mesos/IP:4100, Downstream label: interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig, Trace Id: fcc884016402f7e3.fcc884016402f7e3<:fcc884016402f7e3 with Service -> interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig\nI 0228 07:31:46.946 UTC THREAD10: Reaping %/io.l5d.port/4141/#/io.l5d.marathon/crow/stg/bpavscan\nE 0228 07:31:47.045 UTC THREAD10 TraceId:154f9a4264f9c8d2: addr on /#/io.l5d.marathon/crow/stg/wmback\nFailure(Released, flags=0x02) with RemoteInfo -> Upstream Address: /IP:38094, Upstream id: Not Available, Downstream Address: namerd.marathon.mesos/IP:4100, Downstream label: interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig, Trace Id: ad308677fb31ad28.ad308677fb31ad28<:ad308677fb31ad28 with Service -> interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig\nI 0228 07:31:47.048 UTC THREAD10: Reaping %/io.l5d.port/4141/#/io.l5d.marathon/crow/stg/wmback\nE 0228 07:31:47.118 UTC THREAD10 TraceId:d7f87d6a0f63a8ec: bind /svc/%/io.l5d.port/4141/#/io.l5d.marathon/crow/stg/bphcspinoff\nFailure(Released, flags=0x02) with RemoteInfo -> Upstream Address: /IP:35266, Upstream id: Not Available, Downstream Address: namerd.marathon.mesos/IP:4100, Downstream label: interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig, Trace Id: db24292704c6a286.db24292704c6a286<:db24292704c6a286 with Service -> interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig\nI 0228 07:31:47.120 UTC THREAD10: Reaping /svc/%/io.l5d.port/4141/#/io.l5d.marathon/crow/stg/bphcspinoff\nE 0228 07:31:47.154 UTC THREAD10 TraceId:d7f87d6a0f63a8ec: addr on /#/io.l5d.marathon/crow/stg/bphcspinoff\nFailure(Released, flags=0x02) with RemoteInfo -> Upstream Address: /IP:35266, Upstream id: Not Available, Downstream Address: namerd.marathon.mesos/IP:4100, Downstream label: interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig, Trace Id: f91afd8a853890d7.f91afd8a853890d7<:f91afd8a853890d7 with Service -> interpreter/io.buoyant.namerd.iface.NamerdInterpreterConfig\nI 0228 07:31:47.216 UTC THREAD10: Reaping %/io.l5d.localhost/#/io.l5d.marathon/crow/stg/bphcspinoff\n```. Despite all these \"failures\" being logged, looks like it works just fine.. No problem, thanks for the fix!. ",
    "jacob-go": "Sent you via mail (the one on your profile here) all the details @siggy . Sorry I didn't updated, we were aggressively testing all of the layers to see what's really going on. \nFirst of all, we did manage to find a bug in our test services that well may contributed to the false 200 status being returned. After we started getting 5XX statuses we could configure retries and requeues. We continued with our scenario: non-stop requests and restarts of the proxy from time to time. Indeed, retries/requeues worked, but didn't save all the failed requests during a restart. \nIn that point we suddenly thought that maybe we've missed the real problem. We wondered what was happening after the SIGTERM was sent and we noticed an unclear behaviour of the container. Tough it supposed to wait for a fixed period of time or better, wait for all the requests to drain from queue/memory - it just killed itself after 2 seconds or less, no matter the load. We tried configuring explicitly via the admin block (shutdownGraceMs), but with no success. \nAfter confirmation from Linkerd's Slack, I've opened a new issue about graceful shutdowns: https://github.com/linkerd/linkerd/issues/1887. I want to make the same restart tests with the proper graceful timeout\nconfig, I'll let you know in the next 48 hours\nOn Tue, Apr 24, 2018, 11:40 PM Alex Leong notifications@github.com wrote:\n\n@jacob-go https://github.com/jacob-go does #1887\nhttps://github.com/linkerd/linkerd/issues/1887 capture the issues the\nyou're running into? can this issue be closed?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/linkerd/linkerd/issues/1834#issuecomment-384072631,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AcVEgmcasjFa7-9epykoNmf2_IKXyRf-ks5tr43UgaJpZM4STh2V\n.\n. @adleong After running my tests again, I can gladly confirm that indeed, enabling graceful shutdown correctly in Linkerd on top of DC/OS solved our problem (#1887 and #1910). Hi there @dadjeibaah, how are you? \n\nDid you managed to take a look? Do you see a similar behaviour? . Hello, @dadjeibaah, thanks for investigating. Unfortunately, we see something different. \nNo in-flight requests with 10 second graceful shutdown window\nI 0423 13:23:14.725 UTC THREAD36 TraceId:390ad22c14dedea3: [/admin/shutdown] from 127.0.0.1 quitting\nI0423 13:23:16.155283 18896 health_checker.cpp:165] Health checking stopped\nW0423 13:23:16.155283 18889 logging.cpp:91] RAW: Received signal SIGTERM from process 3921 of user 0; exiting \nRequest with a 8 second latency and linkerd with a 10 second graceful shutdown window\nI 0423 13:25:10.461 UTC THREAD38 TraceId:3c02c1ff15ea5a99: [/admin/shutdown] from 127.0.0.1 quitting\nI 0423 13:25:11.536 UTC THREAD33 TraceId:3c02c1ff15ea5a99: Reaping /svc/check-linkerd-deployments.<CLUSTER.COM>\nI 0423 13:25:11.539 UTC THREAD33 TraceId:3c02c1ff15ea5a99: Reaping #/io.l5d.marathon/check-linkerd-deployments\nE 0423 13:25:11.541 UTC THREAD33 TraceId:c1050292e20dffdf: service failure: com.twitter.finagle.CancelledRequestException: request cancelled. Remote Info: Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /<IP>:7606, Downstream label: #/io.l5d.marathon/check-linkerd-deployments, Trace Id: c1050292e20dffdf.bd83b8fbbae6750e<:c1050292e20dffdf\nI0423 13:25:12.947687 20386 health_checker.cpp:165] Health checking stopped\nW0423 13:25:12.947687 20382 logging.cpp:91] RAW: Received signal SIGTERM from process 3921 of user 0; exiting\nRequest with a 15 second latency with linkerd shutdown grace period of 10 seconds\nI 0423 13:27:10.218 UTC THREAD38 TraceId:ae1cffdecca59d9d: [/admin/shutdown] from 127.0.0.1 quitting\nI 0423 13:27:11.278 UTC THREAD23 TraceId:ae1cffdecca59d9d: Reaping /svc/check-linkerd-deployments.<CLUSTER.COM>\nI 0423 13:27:11.282 UTC THREAD23 TraceId:ae1cffdecca59d9d: Reaping #/io.l5d.marathon/check-linkerd-deployments\nE 0423 13:27:11.284 UTC THREAD23 TraceId:5901fbfd51cee73d: service failure: com.twitter.finagle.CancelledRequestException: request cancelled. Remote Info: Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /<IP>:7606, Downstream label: #/io.l5d.marathon/check-linkerd-deployments, Trace Id: 5901fbfd51cee73d.95057738731a166e<:5901fbfd51cee73d\nI0423 13:27:12.717963 24235 health_checker.cpp:165] Health checking stopped\nW0423 13:27:12.717963 24233 logging.cpp:91] RAW: Received signal SIGTERM from process 3921 of user 0; exiting\nI wondered wether it's because the version, I'm working on Linekrd 1.3.0. But Linkerd 1.3.7 shows the same results:\nNo in-flight requests with 10 second graceful shutdown window\nI 0423 13:29:52.533 UTC THREAD1: initialized\nI 0423 13:30:20.145 UTC THREAD37 TraceId:2300803fdb35abf6: [/admin/shutdown] from 127.0.0.1 quitting\nI0423 13:30:21.590337 28055 health_checker.cpp:165] Health checking stopped\nW0423 13:30:21.590337 28045 logging.cpp:91] RAW: Received signal SIGTERM from process 3921 of user 0; exiting\nRequest with a 8 second latency and linkerd with a 10 second graceful shutdown window\nI 0423 13:30:32.854 UTC THREAD1: initialized\nI 0423 13:31:15.368 UTC THREAD29 TraceId:a325ba805411f1e6: [/admin/shutdown] from 127.0.0.1 quitting\nI 0423 13:31:16.434 UTC THREAD24 TraceId:a325ba805411f1e6: Reaping /svc/check-linkerd-deployments.<CLUSTER.COM>\nI 0423 13:31:16.437 UTC THREAD24 TraceId:a325ba805411f1e6: Reaping #/io.l5d.marathon/check-linkerd-deployments\nE 0423 13:31:16.440 UTC THREAD24 TraceId:0d4cf1d5f9aca468: service failure: com.twitter.finagle.CancelledRequestException: request cancelled. Remote Info: Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /<IP>:7606, Downstream label: #/io.l5d.marathon/check-linkerd-deployments, Trace Id: 0d4cf1d5f9aca468.c5aae1fb78ec8dd0<:0d4cf1d5f9aca468\nI0423 13:31:17.870793 28864 health_checker.cpp:165] Health checking stopped\nW0423 13:31:17.870793 28860 logging.cpp:91] RAW: Received signal SIGTERM from process 3921 of user 0; exiting\nRequest with a 15 second latency with linkerd shutdown grace period of 10 seconds\nI 0423 13:32:05.127 UTC THREAD1: initialized\nI 0423 13:32:31.389 UTC THREAD39 TraceId:ed7cd5fee810c8ef: [/admin/shutdown] from 127.0.0.1 quitting\nI 0423 13:32:32.456 UTC THREAD23 TraceId:ed7cd5fee810c8ef: Reaping /svc/check-linkerd-deployments.<CLUSTER.COM>\nI 0423 13:32:32.460 UTC THREAD23 TraceId:ed7cd5fee810c8ef: Reaping #/io.l5d.marathon/check-linkerd-deployments\nE 0423 13:32:32.462 UTC THREAD23 TraceId:aa0dd291f1db2156: service failure: com.twitter.finagle.CancelledRequestException: request cancelled. Remote Info: Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /<IP>:7606, Downstream label: #/io.l5d.marathon/check-linkerd-deployments, Trace Id: aa0dd291f1db2156.4a60e844d1900f81<:aa0dd291f1db2156\nI0423 13:32:33.847641 31006 health_checker.cpp:165] Health checking stopped\nW0423 13:32:33.847641 31006 logging.cpp:91] RAW: Received signal SIGTERM from process 3921 of user 0; exiting\nWe're working on top of DC/OS 1.9.6, but the SIGTEM is sent from local machine via http://127.0.0.1/admin/shutdown endpoint. \nCan you elaborate more on your experiments?. 3921 is the mesos-agent process on that instance:\n\nI'll try to send a simple SIGTERM directly to the container, but why do you think this influences the results? Should a SIGTERM behave differently than /admin/shutdown, which in turn behaves differently than sending a restart command via Marathon API? . Please let me know if I can help with something . That's awesome.\nI'll test it as well today.\nThank you.\nThe thing is, since we're running on DC/OS, we're probably going to\nshutdown/restart Linkerd via Marathon API. I'll test that as well to see\nhow Linkerd picks up the SIGTERM.. It seems that restarting via Marathon has an effect: \nI 0424 04:56:47.687 UTC THREAD1: initialized\nI0424 04:58:05.672359 16581 health_checker.cpp:165] Health checking stopped\nI0424 04:58:25.814765 16583 health_checker.cpp:165] Health checking stopped\nW0424 04:58:25.814765 16580 logging.cpp:91] RAW: Received signal SIGTERM from process 3486 of user 0; exiting\nThat double Health checking stopped message appeared only once when sending commands via /admin/shutdown. I wonder what's actually going on. Though it does saves my requests (both short-lived and with large delays; with or without shutdownGraceMs: 10000 in the config file), while Linkerd serves as a LB, not as a local proxy on an agent. . So I've further tested. It seems that if running in DC/OS from a buoyantio/linkerd:1.3.0 image, one has to pass a command to be run from /bin/sh -c after the container is up. This is part of the Marathon App Definition. When you pass this command to Linkerd as a parameter in Marathon's JSON, you have to specify the location of bundle-exec (/io.buoyant/linkerd/1.3.0/bundle-exec) and a path to the configuration file. This is the only way, currently, it can run.\nThis JSON objects looks like, for instance:\n{\n    \"id\": \"basic-0\", \n    \"cmd\": \"while [ true ] ; do echo 'Hello Marathon' ; sleep 5 ; done\",\n    \"cpus\": 0.1,\n    \"mem\": 10.0,\n    \"instances\": 1\n}\nIt's a pure Marathon structure.\nThis causes the last process to be run by Linkerd container on DC/OS to be bundle-exec - not \"pure\" linkerd. Any signal I'll send will be caught by bundle-exec or even the mesos-executor, hence those lines you've asked us about:\nlogging.cpp:91] RAW: Received signal SIGTERM from process 3486 of user 0; exiting\nI've build a private image from this docker file:\n```\nFROM buoyantio/linkerd:1.3.0\nCOPY ./config/finalConfig.yml ./config\nENTRYPOINT [\"/io.buoyant/linkerd/1.3.0/bundle-exec\", \"-log.level=DEBUG\", \"./config\"]\n```\nThe configuration inside is just a basic linkerd config, without any shutdownGraceMs setting. Now, when I restart via Marathon api, I see the line:\nI 0424 09:38:53.431 UTC THREAD32: Received SIGTERM. Shutting down ...\nMeaning - Linkerd is the process catching the signal. I sure that when I'll build an image with a configuration like shutdownGraceMs: 10000 I'll see the exact behaviour you've suggested here. . Also, If you'll use some of Marathon's features you can still use the official buoyantIO image. \n\n\nuse Fetch to download the configuration file from a given bucket/repo, for instance:\n\"fetch\": [\n    {\n      \"uri\": \"https://s3.eu-central-1.amazonaws.com/<BUCKET>/config.yml\",\n      \"extract\": false,\n      \"executable\": false,\n      \"cache\": false\n    },\n\n\nInside the container object of the app definition there's a volumes field, there you can mount the config file from MESOS_SANDBOX :\n\"container\": {\n    \"type\": \"DOCKER\",\n    \"volumes\": [\n      {\n        \"containerPath\": \"/io.buoyant/linkerd/1.3.0/config.yml\",\n        \"hostPath\": \"./config.yml\",\n        \"mode\": \"RW\"\n      }\n    ],\n    \"docker\": {\n      \"image\": \"buoyantio/linkerd:1.3.0\",\n      \"network\": \"HOST\",\n      \"portMappings\": [],\n      \"privileged\": true,\n      \"parameters\": [],\n      \"forcePullImage\": false\n    }\n  },\n\n\nUsing the args object, you can pass the config file path to Linkerd's executable, as needed in any image:\n\"args\": [\n    \"./config.yml\"\n  ],\n\n\nMake sure cmd object is empty or even not written in the App Definition. \n\n\nThat way, Linkerd is the process catching signals, and the config file controls it fully. Indeed, if Linkerd is run with these settings on top of DC/OS, it behaves exactly as you've noticed.\nRequest with a 15 second latency and linkerd with a 10 second graceful shutdown window\nI 0424 11:51:25.045 UTC THREAD1: initialized\nI0424 11:52:53.611531  5862 health_checker.cpp:165] Health checking stopped\nI 0424 11:52:53.623 UTC THREAD41: Received SIGTERM. Shutting down ...\nI 0424 11:53:03.684 UTC THREAD21: Reaping /svc/check-linkerd-deployments.<CLUSTER.COM>\nI 0424 11:53:03.687 UTC THREAD21: Reaping #/io.l5d.marathon/check-linkerd-deployments\nE 0424 11:53:03.689 UTC THREAD21 TraceId:92757a9801f48e79: service failure: com.twitter.finagle.CancelledRequestException: request cancelled. Remote Info: Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /<IP>:7606, Downstream label: #/io.l5d.marathon/check-linkerd-deployments, Trace Id: 92757a9801f48e79.eac07dc610108cfd<:92757a9801f48e79\nI0424 11:53:04.181509  5858 health_checker.cpp:165] Health checking stopped\nW0424 11:53:04.181509  5857 logging.cpp:91] RAW: Received signal SIGTERM from process 3874 of user 0; exiting\nRequest with a 8 second latency and linkerd with a 10 second graceful shutdown window\nI 0424 11:52:59.902 UTC THREAD1: initialized\nI0424 11:54:27.230944  3635 health_checker.cpp:165] Health checking stopped\nI 0424 11:54:27.243 UTC THREAD41: Received SIGTERM. Shutting down ...\nI 0424 11:54:34.651 UTC THREAD35: Reaping /svc/check-linkerd-deployments.<CLUSTER.COM>\nI 0424 11:54:34.653 UTC THREAD35: Reaping #/io.l5d.marathon/check-linkerd-deployments\nI0424 11:54:35.144332  3636 health_checker.cpp:165] Health checking stopped\nW0424 11:54:35.144332  3622 logging.cpp:91] RAW: Received signal SIGTERM from process 3722 of user 0; exiting. I'll open a merge request in the next few days.\nCan you explain why all the DC/OS examples are based on \"echoing\" a JSON\nstruct over a pipeline to \"bundle-exec\"?\nWon't it be cleaner to somehow switch to working with yaml files and the\nfetch feature?\nOn Tue, Apr 24, 2018 at 8:20 PM, William Morgan notifications@github.com\nwrote:\n\nGreat debugging, @jacob-go https://github.com/jacob-go! At a minimum,\nwe should add this to our DC/OS docs and examples.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/linkerd/linkerd/issues/1887#issuecomment-384012544,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AcVEgndWAklYmTcpHNosZ5OI_YzjTSNsks5tr173gaJpZM4TEtBd\n.\n. @siggy Well I'm glad you've opened #1910 - because DC/OS users should understand in the future how not to block some features because of the platform . @dadjeibaah which section exactly do you want to change? \nYou have the the main README in the DC/OS folder of linkerd-examples (https://github.com/linkerd/linkerd-examples/tree/master/dcos). Should it be already highlighted there? . \n",
    "etotten": "Howdy @adleong. Yes, it worked. Using the namers config for the zk hosts and removing the host from the interpreter dst did allow us to use all the zk boxes w/o making a round robin DNS entry.\nThanks!. ",
    "krjensen": "Let me know if you need more information from me. . Why did you close this issue. I still think it's 100% relevant and a feature we really need.. ",
    "sean-brandt": "We have this same exact use case. \nFinagle supports this ( see here ), it just needs to be 'exposed'/'lifted' to linkerd http level configs. Perhaps as static client config options to allow per router config.\nI can offer some time to look at this, however, my scala is rather rusty - any pointers/starting locations would be very much appreciated.\n. No, I'm looking to configure linkerd to, on a per 'service' basis use an http 'forward proxy' to reach external services. This may be either over http, or https, so the protocol requires supporting http proxy specific communication.\nGenerally I'm looking to support this:\n[client]  --> [linkerd] --> [http proxy (squid forward proxy, non-transparent) ] --> [ ultimate destination service ]\nNo need, nor desire, for SOCKS at all on my end.\n. @adleong Ok. How, then, is linkerd supposed to know where to tell the proxy to go to? The 'proxy' in this case is a forward proxy ( squid, or similar ) and it is expecting requests to it to follow the http  proxy protocol. \nBasically requests would require that GET/HEAD/.. use the fully qualified host/url and the Host header be set to the destination. \nFor https it's basically the same, but there's only one method ( and no URL ) - CONNECT. Host header is still required.\nWhere would I configure that /svc/foo requests should go to foobar.nowhere.com/bar VIA :  ?. Additionally - as I noted above, Finagle supports this. It really just needs to be exposed somewhere/how in the config.\n```\nimport com.twitter.finagle.{Service, Http}\nimport com.twitter.finagle.http.{Request, Response}\nimport com.twitter.finagle.client.Transporter\nimport java.net.SocketAddress\nval twitter: Service[Request, Response] = Http.client\n  .withTransport.httpProxyTo(\n    host = \"twitter.com:443\",\n    credentials = Transporter.Credentials(\"user\", \"password\")\n  )\n  .newService(\"inet!my-proxy-server.com:3128\") // using local DNS to resolve proxy\n```. That makes sense, at least in the plaintext case - I think. :) I'll put together a test, since I'm not certain that it will work with the TLS case ( where the ultimate target service is https ) and a proxy 'tunnel' is required using the CONNECT method.\n. TLS doesn't work via the routes above, or at least I wasn't able to get it to do so. It's the switch to the CONNECT method and related work that's the issue.\nThere is code in Finagle to support this - unfortunately, however I lack the time and familiarity to be able to help out much more than that.. Hmm, additionally - a 'test' should be easy enough to replicate self contained, presuming a containerized environment.\n\nContainer A : Attempts to connect to https://someapi.somewhere.com via container B by connecting to it on port 8080: HTTP GET /someapi\nContainer B: Linkerd listens on 8080, routes requests for /someapi/* to https://someapi.somewhere.com via squid running on container C listening on 3128\nContainer C: squid listening on 3128 operating in a traditional caching proxy role\n. Absent CONNECT support, I've got no opinion one way or the other on this particular issue. #1982 covers my use-case/concern, I believe.. \n",
    "rasmus": "Any progress on this?. Any reason why this was closed?. ",
    "ccbeloy": "@adleong - even for the HTTP case, the suggested dtab doesn't work. We're also using Squid proxy server as Sean describe.  Basically I needs to follow http proxy protocol for Squid to successfully process the request.\n@sean-brandt - if you don't mind sharing, what was your workaround for running Linkerd behind corporate proxy? Thanks.. ",
    "sankate": "Is it fine if we update the Build.md to add a section which describes Building docker images locally\nIs it as simple as replacing OpenJDK in the circleci config file and LinkerdBuild.scala? Or do we have to do something else?\n https://github.com/linkerd/linkerd/blob/243db9b10dfe49e6127350d4367318c6ac6fedab/.circleci/config.yml#L9\nhttps://github.com/linkerd/linkerd/blob/3f6e89580027386e37caea0ce3f66bcede41a8a1/project/LinkerdBuild.scala#L627\n. ",
    "eroji": "Interestingly, if I test the route using dtab, it seems to point to the pod IPs and port correctly however fails to complete.\n\nHowever, looking at kubectl, it shows that the services has no ClusterIP at all.\nNAME             TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE\nsvc/hello        ClusterIP   None         <none>        7777/TCP   1h\nsvc/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP    24d\nsvc/world-v1     ClusterIP   None         <none>        7778/TCP   1h. Thank you!. That did the trick. Thank you!. Sorry for the confusion. I was not entirely sure if I was clear in my explanation as well.\nSo I will be running multiple applications each has a API with its own respective paths. For instance:\napp1 -> /app1/api/v1/...\napp2 -> /app2/api/v1/...\napp3 -> /app3/api/v1/...\nI want them all to sit behind linkerd service mesh and also gather useful metrics of each. I have a ingress rule using nginx ingress to point all requests to https://services.example.com to linkerd service, port 4140. My expectation is that then if I try to navigate to https://services.example.com/app1/api/v1/... it takes me to app1 and respectively for all other applications.. The config is currently default.\n```\napiVersion: v1\ndata:\n  config.yaml: |-\n    admin:\n      ip: 0.0.0.0\n      port: 9990\nnamers:\n- kind: io.l5d.k8s\n  host: localhost\n  port: 8001\n\ntelemetry:\n- kind: io.l5d.prometheus\n- kind: io.l5d.recentRequests\n  sampleRate: 0.25\n\nusage:\n  orgId: linkerd-example-daemonset\n\nrouters:\n- protocol: http\n  label: outgoing\n  dtab: |\n    /srv        => /#/io.l5d.k8s/default/http;\n    /host       => /srv;\n    /svc        => /host;\n    /host/world => /srv/world-v1;\n  interpreter:\n    kind: default\n    transformers:\n    - kind: io.l5d.k8s.daemonset\n      namespace: linkerd\n      port: incoming\n      service: l5d\n      hostNetwork: true\n  servers:\n  - port: 4140\n    ip: 0.0.0.0\n  service:\n    responseClassifier:\n      kind: io.l5d.http.retryableRead5XX\n\n- protocol: http\n  label: incoming\n  dtab: |\n    /srv        => /#/io.l5d.k8s/default/http;\n    /host       => /srv;\n    /svc        => /host;\n    /host/world => /srv/world-v1;\n  interpreter:\n    kind: default\n    transformers:\n    - kind: io.l5d.k8s.localnode\n      hostNetwork: true\n  servers:\n  - port: 4141\n    ip: 0.0.0.0\n\nkind: ConfigMap\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"v1\",\"data\":{\"config.yaml\":\"admin:\\n  ip: 0.0.0.0\\n  port: 9990\\n\\nnamers:\\n- kind: io.l5d.k8s\\n  host: localhost\\n  port: 8001\\n\\ntelemetry:\\n- kind: io.l5d.prometheus\\n- kind: io.l5d.recentRequests\\n  sampleRate: 0.25\\n\\nusage:\\n  orgId: linkerd-example-daemonset\\n\\nrouters:\\n- protocol: http\\n  label: outgoing\\n  dtab: |\\n    /srv        =\\u003e /#/io.l5d.k8s/default/http;\\n    /host       =\\u003e /srv;\\n    /svc        =\\u003e /host;\\n    /host/world =\\u003e /srv/world-v1;\\n  interpreter:\\n    kind: default\\n    transformers:\\n    - kind: io.l5d.k8s.daemonset\\n      namespace: linkerd\\n      port: incoming\\n      service: l5d\\n      hostNetwork: true\\n  servers:\\n  - port: 4140\\n    ip: 0.0.0.0\\n  service:\\n    responseClassifier:\\n      kind: io.l5d.http.retryableRead5XX\\n\\n- protocol: http\\n  label: incoming\\n  dtab: |\\n    /srv        =\\u003e /#/io.l5d.k8s/default/http;\\n    /host       =\\u003e /srv;\\n    /svc        =\\u003e /host;\\n    /host/world =\\u003e /srv/world-v1;\\n  interpreter:\\n    kind: default\\n    transformers:\\n    - kind: io.l5d.k8s.localnode\\n      hostNetwork: true\\n  servers:\\n  - port: 4141\\n    ip: 0.0.0.0\"},\"kind\":\"ConfigMap\",\"metadata\":{\"annotations\":{},\"creationTimestamp\":\"2018-03-09T01:39:30Z\",\"name\":\"l5d-config\",\"namespace\":\"linkerd\",\"resourceVersion\":\"4330504\",\"selfLink\":\"/api/v1/namespaces/linkerd/configmaps/l5d-config\",\"uid\":\"b6194b2a-233a-11e8-9633-0050568f4635\"}}\n  creationTimestamp: 2018-03-13T09:00:16Z\n  name: l5d-config\n  namespace: linkerd\n  resourceVersion: \"4975581\"\n  selfLink: /api/v1/namespaces/linkerd/configmaps/l5d-config\n  uid: f2d9e807-269c-11e8-92ed-0050568f0a75\n```\nI tried a number of different dtab rules but none of them seemed to have worked and all resulted in the similar error\nNo hosts are available for /svc/services.example.com, Dtab.base=[/srv=>/#/io.l5d.k8s/default/http;/host=>/srv;/svc=>/host;/host/world=>/srv/world-v1], Dtab.local=[]. Remote Info: Not Available. I'll give it a try. Does this also mean that the outgoing dtab needs to match?. To configure services outgoing request through linkerd, is that a configuration in linkerd or the application?. Thank you. I think this answers all my questions.. ",
    "yurishkuro": "\nThe underlying issue is that the client span name corresponds to the id used by linkerd to label the clients that it builds dynamically. If we strip out parts of the id, then they become ambiguous.\n\n@klingerf  This sounds like you're deciding on a user-facing feature based on the internal implementation detail. You can still keep the unique IDs internally, and associate them with a normalized structure like\n{\n    service: \"myservice\",\n    cluster: \"k8-development\",\n    transport: \"grpc\",\n    // etc\n}\ntracing systems can easily handle high dimension metadata, and can actually provide better / more interesting aggregations when metadata is provided in a normalized form instead of a single denormalized string.. Haven't looked at finagle in a couple of years, but fairly certain span names are plain strings. I'm not suggesting that they should be structs, rather that the rest of the dimensions can be recorded as span tags (finagle might call them binary annotations, following zipkin notation).. ",
    "Capitrium": "@adleong Thanks for taking the time to look into this, we'll try and reproduce it again ourselves shortly and report back - in the meantime, can you share the version of openssl you're using just so we can make sure we're doing everything in exactly the same way?. @adleong we're not able to reproduce this anymore either. This issue originally came up since we're having trouble using certs that are generated using our pre-existing TLS pipeline powered by cfssl and Vault, which we've used to generate SHA512 certs that are currently used by components in our Kubernetes cluster (etcd, apiservers, etc.).\nAre there any other config options (when generating certs, or in linkerd itself) that you can think of which would cause the errors we're seeing on linkerd using our cfssl-generated certs? We've been trying to get linkerd running for a few weeks now, and we're running out of ideas.\nE 0309 15:23:41.199 UTC THREAD120 TraceId:0ea20f703404c448: service failure: Failure(null at remote address: /10.0.24.7:4141. Remote Info: Not Available at remote address: /10.0.24.7:4141. Remote Info: Not Available, flags=0x09) with RemoteInfo -> Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /10.0.24.7:4141, Downstream label: %/io.l5d.k8s.daemonset/linkerd/incoming/l5d/#/io.l5d.k8s/default/http/consumer, Trace Id: 0ea20f703404c448.0ea20f703404c448<:0ea20f703404c448\nI 0309 15:28:50.204 UTC THREAD68 TraceId:9023d4a952049423: FailureAccrualFactory marking connection to \"%/io.l5d.k8s.daemonset/linkerd/incoming/l5d/#/io.l5d.k8s/default/http/consumer\" as dead. Remote Address: Inet(/10.0.24.7:4141,Map(nodeName -> ip-10-0-24-7.ec2.internal)). @briansmith My coworker emailed you the non-working cert chain the other day; let us know if you're able to find anything!. @briansmith Thanks for the info! I'll regenerate our test certs when I'm back in the office on Monday and report back. Getting X509v3 certs working with linkerd is one of the last remaining blockers we have for deploying it to prod (I had some concerns with using X509v1... :stuck_out_tongue:) \nOn a side note, I did just check another cert that we created for linkerd using Vault that gave us similar problems, and there are no blank dNSName entries in the subjectAltName field; however I'm still sorting out some other issues with our Vault configuration, so I'm hoping those are related.. Looks like the empty dNSName field was the issue with our CFSSL certs - thanks again @briansmith for looking into that!. @briansmith Turns out we jumped the gun a little here, we're still having TLS issues - should we open a new bug report or reopen this one?. @briansmith Any updates on this? Hate to keep bugging you, but I need something to bring back to my project manager so we can start working on a backup plan in case linkerd won't be compatible with cfssl or vault certs anytime soon.\nAs a side note, we've validated that we can use both our cfssl-generated certs and our vault-generated certs with openssl's s_client/s_server tools to setup a working TLS connection.. ",
    "a-kinder": "hey @briansmith just wondering if you had any luck with finding the issue with those certs?. Just wanted to make note here: we've implemented linkerd-tls with cfssl certs that work fine.. coworker of @Capitrium here, issue is resolved. \nTo reiterate publicly from my e-mail yesterday with Brian:\nI realized that the common name was being overridden by the subject alternative names, so although we had \u201clinkerd\u201d as the CN in both the linkerd config file as well as the certificates, it was being overridden by our DNS name entries. Both our cfssl certificates and vault certificates worked once I changed the commonName to \u201cl5d\u201d in our linkerd config. I think the openssl ones worked because they didn\u2019t have a subject alternative name therefore it wasn\u2019t overriding the \u201clinkerd\u201d common name, and the cfssl certs worked in linkerd-tcp because it doesn\u2019t specify common name in it\u2019s configuration at all. \n. ",
    "mrezaei00": "@siggy good eye \ud83d\udcaf \n@wmorgan it's good to be here :). @adleong the wildcard-proxycert is a single file/certificate with a wildcard Common Name, e.g. *.domain.inc.com, and it is at the root of its certificate chain, i.e. not part of a chain. Does the intermediateCertsPath still apply to my situation?. @adleong structure looks quite different (also, not using a self-signed cert):\n```\nopenssl s_client -connect localhost:4141\nCONNECTED(00000003)\ndepth=2 C = US, O = DigiCert Inc, OU = www.digicert.com, CN = DigiCert High Assurance EV Root CA\nverify return:1\ndepth=1 C = US, O = DigiCert Inc, OU = www.digicert.com, CN = DigiCert SHA2 High Assurance Server CA\nverify return:1\ndepth=0 C = US, ST = California, L = Palo Alto, O = \"WePay, Inc.\", CN = *.domain.inc.com\nverify return:1\n\nCertificate chain\n 0 s:/C=US/ST=California/L=Palo Alto/O=WePay, Inc./CN=*.domain.inc.com\n   i:/C=US/O=DigiCert Inc/OU=www.digicert.com/CN=DigiCert SHA2 High Assurance Server CA\n 1 s:/C=US/O=DigiCert Inc/OU=www.digicert.com/CN=DigiCert SHA2 High Assurance Server CA\n   i:/C=US/O=DigiCert Inc/OU=www.digicert.com/CN=DigiCert High Assurance EV Root CA\n\nServer certificate\n-----BEGIN CERTIFICATE-----\nMIIFZDCCBEygAwIBAgIQA00dEv5Kh2CNuX+vdbgWWjANBgkqhkiG9w0BAQsFADBw\n...\nyawYd01LF15VVQlIGGM2TMpWfUOZBpTh62Ux2iKq/SjVbUILdJKvWw==\n-----END CERTIFICATE-----\nsubject=/C=US/ST=California/L=Palo Alto/O=WePay, Inc./CN=*.domain.inc.com\nissuer=/C=US/O=DigiCert Inc/OU=www.digicert.com/CN=DigiCert SHA2 High Assurance Server CA\n---\nNo client certificate CA names sent\nPeer signing digest: SHA256\nServer Temp Key: ECDH, P-256, 256 bits\n---\nSSL handshake has read 3101 bytes and written 302 bytes\nVerification: OK\n---\nNew, TLSv1.2, Cipher is ECDHE-RSA-AES128-GCM-SHA256\nServer public key is 2048 bit\nSecure Renegotiation IS supported\nCompression: NONE\nExpansion: NONE\nNo ALPN negotiated\nSSL-Session:\n    Protocol  : TLSv1.2\n    Cipher    : ECDHE-RSA-AES128-GCM-SHA256\n    Session-ID: F4160F3B...D46C2\n    Session-ID-ctx: \n    Master-Key: 2DDE231...F45B6\n    PSK identity: None\n    PSK identity hint: None\n    SRP username: None\n    Start Time: 1529292904\n    Timeout   : 7200 (sec)\n    Verify return code: 0 (ok)\n    Extended master secret: yes\n---\n. Is the TLS config you mentioned still related or the issue is entirely due to the Finagle issue?. @adleong it looks like that exception did show up around the time we saw the issue. Full trace:\nio.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: General OpenSslEngine problem \n    at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:459) \n    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265) \n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) \n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) \n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) \n    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) \n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) \n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) \n    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) \n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:134) \n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645) \n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) \n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) \n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459) \n    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) \n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \n    at com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run(BlockingTimeTrackingThreadFactory.scala:23) \n    at java.lang.Thread.run(Thread.java:748) Caused by: javax.net.ssl.SSLHandshakeException: General OpenSslEngine problem \n    at io.netty.handler.ssl.ReferenceCountedOpenSslContext$AbstractCertificateVerifier.verify(ReferenceCountedOpenSslContext.java:648) \n    at io.netty.internal.tcnative.SSL.readFromSSL(Native Method) \n    at io.netty.handler.ssl.ReferenceCountedOpenSslEngine.readPlaintextData(ReferenceCountedOpenSslEngine.java:482) \n    at io.netty.handler.ssl.ReferenceCountedOpenSslEngine.unwrap(ReferenceCountedOpenSslEngine.java:1020) \n    at io.netty.handler.ssl.ReferenceCountedOpenSslEngine.unwrap(ReferenceCountedOpenSslEngine.java:1127) \n    at io.netty.handler.ssl.SslHandler$SslEngineType$1.unwrap(SslHandler.java:210) \n    at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1215) \n    at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1127) \n    at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1162) \n    at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:489) \n    at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:428) \n    ... 18 more Caused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target \n    at sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:397) \n    at sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:302) \n    at sun.security.validator.Validator.validate(Validator.java:260) \n    at sun.security.ssl.X509TrustManagerImpl.validate(X509TrustManagerImpl.java:324) \n    at sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:281) \n    at sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:136) \n    at io.netty.handler.ssl.ReferenceCountedOpenSslClientContext$ExtendedTrustManagerVerifyCallback.verify(ReferenceCountedOpenSslClientContext.java:221) \n    at io.netty.handler.ssl.ReferenceCountedOpenSslContext$AbstractCertificateVerifier.verify(ReferenceCountedOpenSslContext.java:644) \n    ... 28 more Caused by: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target \n    at sun.security.provider.certpath.SunCertPathBuilder.build(SunCertPathBuilder.java:141) \n    at sun.security.provider.certpath.SunCertPathBuilder.engineBuild(SunCertPathBuilder.java:126) \n    at java.security.cert.CertPathBuilder.build(CertPathBuilder.java:280) \n    at sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:392) \n    ... 35 more\nDigiCert in the keystore on the proxy:\nkeytool -list -rfc -storepass changeit -keystore $JAVA_HOME/lib/security/cacerts | grep -i digicert\nAlias name: debian:digicert_trusted_root_g4.pem\nAlias name: debian:digicert_global_root_g2.pem\nAlias name: debian:digicert_assured_id_root_ca.pem\nAlias name: debian:digicert_assured_id_root_g3.pem\nAlias name: debian:digicert_high_assurance_ev_root_ca.pem\nAlias name: debian:digicert_assured_id_root_g2.pem\nAlias name: debian:digicert_global_root_ca.pem\nAlias name: debian:digicert_global_root_g3.pem\n``. @adleong So I setup a slightly different setup, than my original setup, with two versions of Linkerd running in different clusters:\n1. Cluster 1 running Linkerd version1.4.31. Cluster 2 running Linkerd version1.3.6`\n...and it looks like calls to older versions of Linkerd succeeds and not with the latest version.\nCall A:\nSVC A --> Linkerd 1 --> Linkerd 2 --> SVC B --> 200 OK\nCall B:\nSVC A --> Linkerd 1a --> Linkerd 1b --> SVC B --> General OpenSslEngine problem at remote address: /10.84.2.53:4141. Remote Info: Not Available\nwith stack trace:\n```\nW 0705 20:14:29.322 UTC THREAD21: ChannelStatsHandler caught an exception\nio.netty.handler.codec.DecoderException: java.lang.NullPointerException: ssl\n    at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:459)\n    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:134)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run(BlockingTimeTrackingThreadFactory.scala:23)\n    at java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException: ssl\n    at io.netty.internal.tcnative.SSL.getHandshakeCount(Native Method)\n    at io.netty.handler.ssl.ReferenceCountedOpenSslEngine.rejectRemoteInitiatedRenegotiation(ReferenceCountedOpenSslEngine.java:1118)\n    at io.netty.handler.ssl.ReferenceCountedOpenSslEngine.unwrap(ReferenceCountedOpenSslEngine.java:1081)\n    at io.netty.handler.ssl.ReferenceCountedOpenSslEngine.unwrap(ReferenceCountedOpenSslEngine.java:1127)\n    at io.netty.handler.ssl.ReferenceCountedOpenSslEngine.unwrap(ReferenceCountedOpenSslEngine.java:1170)\n    at io.netty.handler.ssl.SslHandler$SslEngineType$1.unwrap(SslHandler.java:215)\n    at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1215)\n    at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1127)\n    at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1162)\n    at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:489)\n    at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:428)\n    ... 18 more\nWARN 0705 20:14:29.322 UTC finagle/netty4-1: An exceptionCaught() event was fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did not handle the exception.\nio.netty.handler.codec.DecoderException: java.lang.NullPointerException: ssl\n    at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:459)\n    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:134)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run(BlockingTimeTrackingThreadFactory.scala:23)\n    at java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException: ssl\n    at io.netty.internal.tcnative.SSL.getHandshakeCount(Native Method)\n    at io.netty.handler.ssl.ReferenceCountedOpenSslEngine.rejectRemoteInitiatedRenegotiation(ReferenceCountedOpenSslEngine.java:1118)\n    at io.netty.handler.ssl.ReferenceCountedOpenSslEngine.unwrap(ReferenceCountedOpenSslEngine.java:1081)\n    at io.netty.handler.ssl.ReferenceCountedOpenSslEngine.unwrap(ReferenceCountedOpenSslEngine.java:1127)\n    at io.netty.handler.ssl.ReferenceCountedOpenSslEngine.unwrap(ReferenceCountedOpenSslEngine.java:1170)\n    at io.netty.handler.ssl.SslHandler$SslEngineType$1.unwrap(SslHandler.java:215)\n    at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1215)\n    at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1127)\n    at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1162)\n    at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:489)\n    at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:428)\n    ... 18 more\nW 0705 20:14:29.322 UTC THREAD21: Unhandled exception in connection with /10.84.0.54:47666, shutting down connection\nio.netty.handler.codec.DecoderException: java.lang.NullPointerException: ssl\n    at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:459)\n    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:134)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run(BlockingTimeTrackingThreadFactory.scala:23)\n    at java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException: ssl\n    at io.netty.internal.tcnative.SSL.getHandshakeCount(Native Method)\n    at io.netty.handler.ssl.ReferenceCountedOpenSslEngine.rejectRemoteInitiatedRenegotiation(ReferenceCountedOpenSslEngine.java:1118)\n    at io.netty.handler.ssl.ReferenceCountedOpenSslEngine.unwrap(ReferenceCountedOpenSslEngine.java:1081)\n    at io.netty.handler.ssl.ReferenceCountedOpenSslEngine.unwrap(ReferenceCountedOpenSslEngine.java:1127)\n    at io.netty.handler.ssl.ReferenceCountedOpenSslEngine.unwrap(ReferenceCountedOpenSslEngine.java:1170)\n    at io.netty.handler.ssl.SslHandler$SslEngineType$1.unwrap(SslHandler.java:215)\n    at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1215)\n    at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1127)\n    at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1162)\n    at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:489)\n    at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:428)\n    ... 18 more\n``\nSomehow the config or logic difference in1.3.6vs.1.4.3causes problems on the receiving end.. @adleong are there any workarounds/short term fix, or we're stuck until the update comes through from Finagle?. I'd like to pick up the recent versions' performance improvements for our prod environment, and get the better logging for debugging. Right now debugging is almost as hard as solving a murder mystery \ud83d\ude04 . @adleong I did some more debugging on the Netty side when we last spoke about this, and it looked to me that Netty was not happy about unpacking the private key I was using on theserver`.\nSo, today I created some internal CA and certificates to test this, and I think I was able to fix the issue!!\nEnvironment:\n Linkerd 1.4.4\n Internal CA to trust on the client side\n Internal public and private key for the server side\n Namerd untouched on 1.3.6\nI still see lots of javax.net.ssl.SSLHandshakeException: General OpenSslEngine problem in the logs, but it might be from my testing from this new setup to another cluster with the older setup, which uses the wildcard certificates with a different CA. I'm going to cleanup the logs and see if this is actually fixing all the SSL issues in our environment, but this is looking up.. Well, I can't break it anymore on 1.4.4:\n$ kubectl logs po/l5d-bhg2h -n mesh\n-XX:+AggressiveOpts -XX:+AlwaysPreTouch -XX:+CMSClassUnloadingEnabled -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled -XX:+CMSScavengeBeforeRemark -XX:InitialHeapSize=33554432 -XX:MaxHeapSize=1073741824 -XX:MaxNewSize=357916672 -XX:MaxTenuringThreshold=6 -XX:OldPLABSize=16 -XX:+PerfDisableSharedMem -XX:+PrintCommandLineFlags -XX:+ScavengeBeforeFullGC -XX:-TieredCompilation -XX:+UseCMSInitiatingOccupancyOnly -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+UseStringDeduplication \nAug 01, 2018 3:24:15 AM com.twitter.finagle.http.HttpMuxer$ $anonfun$new$1\nINFO: HttpMuxer[/admin/metrics.json] = com.twitter.finagle.stats.MetricsExporter(<function1>)\nAug 01, 2018 3:24:15 AM com.twitter.finagle.http.HttpMuxer$ $anonfun$new$1\nINFO: HttpMuxer[/admin/per_host_metrics.json] = com.twitter.finagle.stats.HostMetricsExporter(<function1>)\nI 0801 03:24:16.114 UTC THREAD1: linkerd 1.4.4 (rev=4fafcadc2eca0c4a2c227150c4cce64ecfed87a7) built at 20180714-171546\nI 0801 03:24:16.800 UTC THREAD1: Finagle version 18.5.0 (rev=225a244e96935278721ea40a54aa2c2e53412f21) built at 20180508-105457\n...\ncross-cluster routing with both Linkerds on 1.4.4\n$ kubectl exec -it l5d-ghsr7 bash -n mesh\n[root@l5d-ghsr7 1.4.4]# curl localhost:4140/default/apilogger/build\n{\"service_name\":\"apilogger\",...}\nso I think we can call this fixed! On to Linkerd 1.4+!. @adleong I think moving the logic to the server is a great idea, but I still think a health check can use some boost from the return code, and my main reason is to avoid implementing a \"health check wrapper\" just to get the service health checked. I feel like that pattern may quickly diverge as the service is used more and more. If the health of the service is decided on the server, then everyone uses the same core health check.\nAnother reason for including a corresponding response code would be the limitations that come with the monitoring systems out there. We use K8S in this case to do health checking through livenessProbe, and without a custom health script, we can't achieve proper liveness.\nI also agree that 5xx is not the right choice, and 404(4xx) or new codes may be better choice(s).. Tests failed \ud83d\ude1e and it looks like it might be intermittent:\n```\n!/bin/bash -eo pipefail\nset -x\ncd admin/src/main/resources/io/buoyant/admin\nnpm install\n+ cd admin/src/main/resources/io/buoyant/admin\n+ npm install\nToo long with no output (exceeded 10m0s)\n``\nHow do I re-run it?. I see, a simple search on the issue list didn't show anything, so I created this one. It looks liketrustCertsis mentioned astrustCert` in the PRs and the search didn't pick that up.\nI'm curious though, when would the changes in #2080 and #2082 be live?. suggestion\n  sets the duration before a connection is marked as \"unhealthy\". ",
    "jayeve": "Looking to take a stab at this one. There are certainly engineers more familiar with this specific codebase; specifically @siggy and @adleong. I'm fairly proficient with scala code and can certainly adopt the patterns I've seen used in the linkerd repo; however, I would benefit tremendously if either of you could provide some insight as to where the relevant changes need to be made to address this issue. Thank you in advance for any guidance you may provide. It would be great to contribute to an open source project like this one!. As Activity.Pending, there are quite a few failures\n[info] - Namer handles looking up /app/id *** FAILED ***              \n[info]   com.twitter.util.Activity$Pending$@294151d6 did not equal Ok(Leaf(Path(io.l5d.marathon,service,name))) (AppIdNamerTest.scala:68)    \n[info] - Namer handles looking up /app/id case-insensitive *** FAILED ***                                                                    \n[info]   com.twitter.util.Activity$Pending$@294151d6 did not equal Ok(Leaf(Path(io.l5d.marathon,service,name))) (AppIdNamerTest.scala:80)    \n[info] - Namer updates when blocking call from getAppIds returns *** FAILED ***                                                              \n[info]   com.twitter.util.Activity$Pending$@294151d6 did not equal Ok(Leaf(Path(io.l5d.marathon,servicename))) (AppIdNamerTest.scala:93)                \n[info] - Addrs update when blocking call for getAddrs returns *** FAILED ***                                                                 \n[info]   com.twitter.util.Activity$Pending$@294151d6 is not a NameTree.Leaf[Name.Bound] (AppIdNamerTest.scala:140)                           \n[info] - Namer recovers if marathon api fails initially *** FAILED ***                                                                       \n[info]   com.twitter.util.Activity$Pending$@294151d6 did not equal Ok(Leaf(Path(io.l5d.marathon,foo,bar))) (AppIdNamerTest.scala:171)        \n[info] - Namer returns a cached address when the marathon api goes down *** FAILED ***                                                       \n[info]   com.twitter.util.Activity$Pending$@294151d6 did not equal Ok(Leaf(Path(io.l5d.marathon,foo,bar))) (AppIdNamerTest.scala:192)        \n[info] - Namer returns failure if getAddrs() fails due to marathon api being down *** FAILED ***                                             \n[info]   Failed(com.twitter.finagle.ChannelWriteException) is not a NameTree.Leaf[Name] (AppIdNamerTest.scala:111)                           \n[info] - Namer recovers if getAddrs() fails initially *** FAILED ***  \n[info]   Failed(com.twitter.finagle.ChannelWriteException) is not a NameTree.Leaf[Name] (AppIdNamerTest.scala:111)\nAs Activity.Ok(NameTree.Leaf(Name.Bound(...))) there are still failures, but fewer\n[info] - Namer returns failure if getAddrs() fails due to marathon api being down *** FAILED ***                                             \n[info]   Failed(com.twitter.finagle.ChannelWriteException) is not a NameTree.Leaf[Name] (AppIdNamerTest.scala:111)                           \n[info] - Namer recovers if getAddrs() fails initially *** FAILED ***  \n[info]   Failed(com.twitter.finagle.ChannelWriteException) is not a NameTree.Leaf[Name] (AppIdNamerTest.scala:111)\nStill investigating why that is. It may require changing the tests. Afterall, previously all found appIds would be bound regardless of the addr value. ",
    "xcorpio": "Thanks. That's ok , just the dtab in that namespace may be a little big.. ",
    "Adiqq": "Same behaviour with one change to https://raw.githubusercontent.com/linkerd/linkerd-examples/master/k8s-daemonset/k8s/servicemesh.yml\nclient:\n        kind: io.l5d.static\n        configs:\n        # Use HTTPS if sending to port 443\n        - prefix: \"/$/io.buoyant.rinet/*/{service}\"\n          tls:\n            commonName: \"{service}\". And example for some other service\nhttp_proxy=http://dmse02lx0680c:4140 curl -s mongodb-api.dev-ram:4321\nnull at remote address: mongodb-api.dev-ram/10.233.15.234:4321. Remote Info: Not Available%\nW 0321 22:12:48.807 UTC THREAD40 TraceId:d7ca3e0d52f3ae5d: k8s ns dev-ram:4321 service mongodb-api endpoints resource does not exist, assuming it has yet to be created\nW 0321 22:12:48.886 UTC THREAD40 TraceId:d7ca3e0d52f3ae5d: k8s ns default service dev-ram:4321 endpoints resource does not exist, assuming it has yet to be created\nI 0321 22:12:49.354 UTC THREAD37 TraceId:d7ca3e0d52f3ae5d: FailureAccrualFactory marking connection to \"$/io.buoyant.rinet/4321/mongodb-api.dev-ram\" as dead. Remote Address: Inet(mongodb-api.dev-ram/10.233.15.234:4321,Map())\nE 0321 22:12:49.397 UTC THREAD37 TraceId:d7ca3e0d52f3ae5d: service failure: Failure(null at remote address: mongodb-api.dev-ram/10.233.15.234:4321. Remote Info: Not Available, flags=0x09) with RemoteInfo -&gt; Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: mongodb-api.dev-ram/10.233.15.234:4321, Downstream label: $/io.buoyant.rinet/4321/mongodb-api.dev-ram, Trace Id: d7ca3e0d52f3ae5d.d7ca3e0d52f3ae5d&lt;:d7ca3e0d52f3ae5d. Thanks, indeed my service name was empty, after adding \"name: http\",\nhttp_proxy=http://dmse02lx0680c:4140 curl -s mongodb-api.dev-ram works correctly. ",
    "flatmap13": "@dadjeibaah Thanks for conducting these experiments, your findings are in line with our results :). This feature would be great for us, we need a way to block any external interference with the linkerd sidecars for security purposes. PR #2034 looks like a good solution.. ",
    "joeyb": "@adleong - I\u2019m working with @nikolay-pshenichny to test this on our end. Do you have an ETA on that release candidate for 1.3.8? We\u2019re ready to give it a shot. . @adleong - Did the release candidate get a full push to maven central? I'm not seeing any artifacts for 1.4.0-rc1: http://central.maven.org/maven2/io/buoyant/linkerd-core_2.12/. @adleong - FYI, our smoke tests of this have been baking for a few days and we haven't seen any issues.. @adleong - I think I'm seeing the same issue and these changes don't appear to resolve it. My certs are setup with a single root CA in the {{serverCa}}/{{clientCa}} file and the actual cert files have the cert itself along with 2 intermediaries. I'm not really familiar with the linkerd/finagle TLS code, but I suspect it's missing the intermediaries in the actual cert file. These certs were all working properly back in 1.3.7.\nFor inbound requests, I have the following config:\nyaml\ntls:\n    caCertPath: {{serverCa}}\n    serverCaChainPath: {{serverCa}}\n    certPath: {{serverCert}}\n    keyPath: {{serverKey}}\n    requireClientAuth: true\nAnd I'm seeing the typical exception related to not understanding the cert chain from linkerd:\nCaused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n    at sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:397)\n    at sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:240)\n    at sun.security.validator.Validator.validate(Validator.java:260)\n    at sun.security.ssl.X509TrustManagerImpl.validate(X509TrustManagerImpl.java:324)\n    at sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:281)\n    at sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:136)\n    at io.netty.handler.ssl.ReferenceCountedOpenSslClientContext$ExtendedTrustManagerVerifyCallback.verify(ReferenceCountedOpenSslClientContext.java:221)\n    at io.netty.handler.ssl.ReferenceCountedOpenSslContext$AbstractCertificateVerifier.verify(ReferenceCountedOpenSslContext.java:644)\n    ... 26 more\nCaused by: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n    at sun.security.provider.certpath.SunCertPathBuilder.build(SunCertPathBuilder.java:141)\n    at sun.security.provider.certpath.SunCertPathBuilder.engineBuild(SunCertPathBuilder.java:126)\n    at java.security.cert.CertPathBuilder.build(CertPathBuilder.java:280)\n    at sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:392)\n    ... 33 more\nFor outbound requests, I have the following config:\nyaml\ntls:\n    commonName: \"{service}\"\n    trustCerts:\n        - {{clientCa}}\n    clientAuth:\n        certPath: {{clientCert}}\n        keyPath: {{clientKey}}\nAnd I'm seeing the following error on the linkerd side:\nio.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: error:10000410:SSL routines:OPENSSL_internal:SSLV3_ALERT_HANDSHAKE_FAILURE\n    at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:459)\n    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:134)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run(BlockingTimeTrackingThreadFactory.scala:23)\n    at java.lang.Thread.run(Thread.java:748)\nCaused by: javax.net.ssl.SSLHandshakeException: error:10000410:SSL routines:OPENSSL_internal:SSLV3_ALERT_HANDSHAKE_FAILURE\n    at io.netty.handler.ssl.ReferenceCountedOpenSslEngine.shutdownWithError(ReferenceCountedOpenSslEngine.java:869)\n    at io.netty.handler.ssl.ReferenceCountedOpenSslEngine.sslReadErrorResult(ReferenceCountedOpenSslEngine.java:1108)\n    at io.netty.handler.ssl.ReferenceCountedOpenSslEngine.unwrap(ReferenceCountedOpenSslEngine.java:1064)\n    at io.netty.handler.ssl.ReferenceCountedOpenSslEngine.unwrap(ReferenceCountedOpenSslEngine.java:1127)\n    at io.netty.handler.ssl.ReferenceCountedOpenSslEngine.unwrap(ReferenceCountedOpenSslEngine.java:1170)\n    at io.netty.handler.ssl.SslHandler$SslEngineType$1.unwrap(SslHandler.java:215)\n    at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1215)\n    at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1127)\n    at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1162)\n    at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:489)\n    at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:428)\n    ... 18 more. @adleong - That's actually a pretty huge breaking change for us. I'm not familiar enough with how linkerd previously handled certs to know exactly which change caused the breakage, but separating the cert files like that would require fairly large changes to our cert infrastructure. Our real-world, production certs have intermediates in both the main ca.pem and the server's actual certificate PEM file.\n1.3.7 was able to follow the cert chain with the same certs.. @wmorgan - Nope, the repro is using the nightly image, which already includes the changes from #1926.\nI suspect this does at least tie back to the same finagle API changes that lead to #1926, but they are separate issues.. @adleong - It looks like client auth is required to trigger the issue. I modified my repro to use the following linkerd.yaml and now curl localhost:4140 returns successfully.\n```yaml\nadmin:\n  ip: 0.0.0.0\n  port: 9990\nrouters:\n- protocol: http\n  label: server\n  dtab: |\n    /svc => /$/inet/hello/80\n  servers:\n  - ip: 0.0.0.0\n    port: 4141\n    tls:\n      caCertPath: /io.buoyant/certs/ca/certs/ca.cert.pem\n      certPath: /io.buoyant/certs/ca/intermediate/certs/hello-with-intermediate.cert.pem\n      keyPath: /io.buoyant/certs/ca/intermediate/private/hello.key.pkcs8.pem\n      serverCaChainPath: /io.buoyant/certs/ca/intermediate/certs/hello-with-intermediate.cert.pem\n\nprotocol: http\n  label: client\n  dtab: |\n    /svc => /$/inet/localhost/4141\n  servers:\nip: 0.0.0.0\n    port: 4140\n  client:\n    tls:\n      commonName: \"hello\"\n      trustCerts:\n/io.buoyant/certs/ca/certs/ca.cert.pem\n``. @adleong - I think that's exactly it. From what I tell based on the traffic in wireshark, it looks like the server-side linkerd router presents its cert as it should, but the client-side fails to validate it against itstrustCerts` collection, so it never sends the client cert back to the server.\n\n\n\nI haven't dug into the bowels of the related finagle code, by my guess at this point is that the client is not properly following the cert chain when it tries to validate against its trustCerts collection.\nMore concretely, in the repro it should follow the leaf cert to its issuer, which is the intermediate sent along with the server cert, then the intermediate's issuer is the root in trustCerts, so it should be able to validate/trust that cert (and therefore trust the leaf cert).. I haven't created the certs to test this yet, but would that fix work for scenarios where the client and server have different intermediates in their cert PEMs? For example, take the following cert chains:\nclient: ca -> ca_int -> client_int -> client_cert\nserver: ca -> ca_int -> server_int -> server_cert\nAnd cert files setup with ca.pem containing ca -> ca_int, client-cert.pem containing client_int -> client_cert, and server-cert.pem containing server_int -> server_cert.\nDuring the TLS handshake, the server-side sends over server_int -> server_cert. The client should still be able to validate that chain since server_int is issued by the trusted ca_int cert. With the fix from #1960, I suspect it may still fail since the client-side doesn't include server_int in its clientCaChainPath.. ",
    "seanmonstar": "Translating an HTTP/1.1 chunked response to an HTTP/1.0 compatible response indeed has two solutions:\n\nBuffer the full response in the proxy, and once complete, send the HTTP/1.0 response with a Content-Length header and no chunked encoding.\nSimply strip the chunked encoding and serve the response as a close-delimited response.\n\nBoth are unfortunate. (1) leads to more memory usage in the proxy, and more latency in the response, where (2) can lead to accidentally assuming a response body is fully received when there is a network interruption.. ",
    "xinyuliu-glu": "It seems our namerd resolves the dtab to an IP address.\n/http/ou_route_v11/us-east-1/integration/bulkupload\n/srv#/us-east-1/integration/bulkupload\n/#/io.l5d.consul/us-east-1/integration/bulkupload\n52.84.128.91:443. I tried a dedicated \"io.l5d.fs\" interpreter, with a single dtab rule https/ou_route_v11/us-east-1/integration/bulkupload => /$/io.buoyant.rinet/443/bulkupload-regional.integration.abcd.services, API Gateway returns 403 {\"message\":\"Forbidden\"}, which suggests SNI servername is not set by L5D. \nAfter inspecting the tcpdump, I believe TLS handshake works just fine without namerd in place. Still not clear on the cause of 403.\n. @wmorgan thank you for looking into this issue. \nA slice of the Linkerd config.\ninterpreter:\n    kind: io.l5d.fs\n    dtabFile: /opt/linkerd/namerfs/bulkupload\n  servers:\n  - port: 4142\n    ip: 0.0.0.0\n  client:\n    tls:\n      commonName: \"integration.abcd.services\"\n      disableValidation: true\nContent of dtabFile: /opt/linkerd/namerfs/bulkupload,\n/https/ou_route_v11/us-east-1/integration/bulkupload => /$/inet/bulkupload-regional.integration.abcd.services/443;\nWhen I do \ncurl -i https://bulkupload.integration.abcd.services:443\n200 Response is returned from the endpoint hosted by AWS API Gateway.\nWhile, curl -i  http://localhost:4142/ou_route_v11/us-east-1/integration/bulkupload\n403 forbidden message is returned.\nAWS API Gateway requires SNI for TLS connections.\n. The tcpdump file loaded in Wireshark doesn't show any clear text on request and response messages, because of HTTPS/TLS?. We have both API Gateway log and access log turned on. But nothing was recorded for those 403 requests from L5D.. Understood the 403 issue. Thanks for working on SNI support. . ",
    "peterfich": "Hi Alex,\nCan I sign off my own PR?\nWill do if I find more to fix. Thanks for accepting it anyway.\nPeter\n\nOn 4 May 2018, at 21:12, Alex Leong notifications@github.com wrote:\n@peterfich I know this is silly for a docs fix, but our DCO robot will be much happier if you could amend your commit to include the DCO signoff (the -s option in git will do this).\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Hi @dadjeibaah,\n\nI Think you have the wrong Peter.. ",
    "Hugh-ifly": "I peeked the content of hostIP.sh:\n```\n!/bin/sh\nset -e\nsleep 10\ncurl -s \"${K8S_API:-localhost:8001}/api/v1/namespaces/$NS/pods/$POD_NAME\" | jq '.status.hostIP' | sed 's/\"//g'\n``\nand theK8S_APIwas not set, so the master of kubernetes was set tolocalhost:8001`.\nI add K8S_API=10.1.86.22:8080 into the env of docker container via hello-world-legacy.yml and it works for me.\n. ",
    "robertpanzer": "Awesome!\nTo check I also downloaded the artifacts of  1.3.6 and 1.3.7 of admin_2.12 and their size differs significantly. (And 1.4.0 has the same size as 1.3.7). Sure! \nThat means I have to change the commit message to add the \u201eSigned off by...\u201c line?. Just updated the commit message.. Thanks! I just retrieved the artifact from Maven central.. These are the network captures of these calls with 1.4.0 and 1.4.1.\nWith 1.4.1 Linkerd seems to no longer act as a HTTP 1.1. server if the backend talks 1.0.\nlinkerd1.4.0.log\nlinkerd1.4.1.log\n. I could try to create a PR for this feature in case it is considered worth to be added.\nWould it make sense to simply have a parameter enableShutdown or so with default true that allows to disable this endpoint?\nyaml\nadmin:\n  port: 9990\n  ip: 0.0.0.0\n  shutdownEnabled: false\nOr should there be a more general approach that would allow to disable any admin endpoint by defining a blacklist?\nyaml\nadmin:\n  port: 9990\n  ip: 0.0.0.0\n  endpointBlackList:\n  - /admin/shutdown\n  - /admin/threads\nWhat do you think?. Do you think that it would even make sense to introduce a whitelist that makes admin endpoints accessible?\nLike, if the parameter is missing everything is accessible so that the current behavior remains. \nBut if the parameter exists, it would define what is allowed. \nI could imagine this to be a list of uri prefixes. \nWdyt?. @adleong I created https://github.com/linkerd/linkerd/pull/2034 that allows to enable the UI as a whole and additionally whitelist selected endpoints using regular expressions.\nDo you think this makes sense? . That sounds great.\nI would just like to also have a whitelist in addition.\nWhen we add own pages to the admin ui we should be able to whitelist them explicitly without having to open everything.. As suggested in #2033 I changed the PR to allow explicit whitelisting of any admin URLs.\nIf the admin config remains unchanged, all URLs are accessible to preserve the current behavior.\nBut it is also possible to configure the allowed URLs:\nyaml\nadmin:\n  security:\n    uiEnabled: true\n    pathWhitelist:\n    - ^/threads.json$\nThis way all endpoints for the UI are allowed plus the endpoint /threads.json is whitelisted explicitly.\nIf a URL is not on the whitelist, the new SecurityFilter will respond with 404.\nThe default for the UI endpoints is true in case the security option is configured. (If the security option isn't given, everything is accessible anyway)\n. As suggested on #2033 I split the defined multiple whitelists for ui, control and diagnostics and also added a blacklist parameter.. @adleong I think I implemented according to your proposal.\nIt would be great if you could have another look.. Thanks for helping out @adleong! \ud83c\udf89 \nI merged your branch and squashed my commits.. Hi @adleong \nThat's very good questions :)\nFor our use case the limit can be per instance. \nRegarding server side or client side limits:\nIs there a difference if we have a Linkerd instance in front of the service?\n. This looks great!\nWould it be possible to have the counter per client name?\nIf I read the code above correctly the counter would limit per prefix?\nAlso I am wondering if it's already possible to configure requestAuthorizers for the clients, that would be new, right?. Thanks @dadjeibaah\nI'll check as soon as possible!. I had a look at this plugin and it looks great!\nIt seems though that clients are allowed twice the limit in the first interval or if the previous interval didn't see any requests.\nDo you know the impact of the scheduling of the decrementToZero updater for very high rate limits like 100 requests per second? That would also run the timer every 10 msecs if I'm not wrong.. Used a pattern match instead of a simple direct boolean expression because it might be necessary to also filter out other files across all modules.. Oh yes, that\u2019s a good point. \nWhat do you think about adding the ui and control endpoints to the blacklist if the corresponding parameter is set to false?\nI am unsure atm how to find out the category. . I\u2019ll try to implement this tomorrow when I have a fresh mind :). Updated the PR according to your suggestion.. ",
    "utrack": "Nope, nothing there @adleong .\nLast logs:\nMay 31, 2018 8:16:23 AM com.twitter.finagle.http.HttpMuxer$ $anonfun$new$1\nINFO: HttpMuxer[/admin/metrics.json] = com.twitter.finagle.stats.MetricsExporter(<function1>)\nMay 31, 2018 8:16:23 AM com.twitter.finagle.http.HttpMuxer$ $anonfun$new$1\nINFO: HttpMuxer[/admin/per_host_metrics.json] = com.twitter.finagle.stats.HostMetricsExporter(<function1>)\nI 0531 08:16:23.997 UTC THREAD1: namerd 1.3.6 (rev=48a2a63d47fd0f6713c74ec03b8588bbc067e1de) built at 20180302-132449\nI 0531 08:16:31.747 UTC THREAD1: Finagle version 7.1.0 (rev=37212517b530319f4ba08cc7473c8cd8c4b83479) built at 20170906-132024\nI 0531 08:16:33.419 UTC THREAD1: Resolver[inet] = com.twitter.finagle.InetResolver(com.twitter.finagle.InetResolver@50d3bf39)\nI 0531 08:16:33.420 UTC THREAD1: Resolver[fixedinet] = com.twitter.finagle.FixedInetResolver(com.twitter.finagle.FixedInetResolver@25a73de1)\nI 0531 08:16:33.420 UTC THREAD1: Resolver[neg] = com.twitter.finagle.NegResolver$(com.twitter.finagle.NegResolver$@29852487)\nI 0531 08:16:33.421 UTC THREAD1: Resolver[nil] = com.twitter.finagle.NilResolver$(com.twitter.finagle.NilResolver$@3afae281)\nI 0531 08:16:33.421 UTC THREAD1: Resolver[fail] = com.twitter.finagle.FailResolver$(com.twitter.finagle.FailResolver$@26ae880a)\nI 0531 08:16:33.422 UTC THREAD1: Resolver[flag] = com.twitter.server.FlagResolver(com.twitter.server.FlagResolver@260f2144)\nI 0531 08:16:33.422 UTC THREAD1: Resolver[zk] = com.twitter.finagle.zookeeper.ZkResolver(com.twitter.finagle.zookeeper.ZkResolver@3c017078)\nI 0531 08:16:33.423 UTC THREAD1: Resolver[zk2] = com.twitter.finagle.serverset2.Zk2Resolver(com.twitter.finagle.serverset2.Zk2Resolver@51827393)\nI 0531 08:16:51.369 UTC THREAD1: serving http on /0.0.0.0:9991\nI 0531 08:16:52.337 UTC THREAD1: Tracer: com.twitter.finagle.zipkin.thrift.ScribeZipkinTracer\nI 0531 08:16:52.939 UTC THREAD1: serving io.l5d.thriftNameInterpreter interface on /0.0.0.0:4100\nI 0531 08:16:52.959 UTC THREAD1: serving io.l5d.httpController interface on /0.0.0.0:4180\nI 0531 08:16:57.192 UTC THREAD1: serving io.l5d.mesh interface on /0.0.0.0:4321\nI 0531 08:17:09.918 UTC THREAD31: Drainer is disabled; bypassing. Aha, yep, I've got some null dentries.\nI've been hacking namerctl (https://github.com/linkerd/namerctl). It has an array of []*namer.Dentry and I've sent some nils to the server - didn't think it would cause a problem though.\nI've removed null entries via update to test it and DELETE still returns 500.. @adleong I've split this thing to the separate issue - see https://github.com/linkerd/linkerd/issues/1974. I'm not 100% sure that those are related since rewriting null dentries have no impact on this issue @adleong. I'll experiment and update or close this issue in a couple of days. . Cannot reproduce the issue since we've recreated the cluster. :(\nLet's just reopen the issue if someone else encounters the problem.. Can you point me to the right direction please @dadjeibaah ?\nAll I have for TLS specifically is this part:\nrouters:\n    - label: http-outgoing\n      protocol: http\n      interpreter:\n        kind: io.l5d.namerd\n        dst: /#/io.l5d.k8s/linkerd/thrift/namerd\n        namespace: outgoing\n      servers:\n      - port: 4140\n        ip: 0.0.0.0\n      service:\n        responseClassifier:\n          kind: io.l5d.http.retryableRead5XX\n      client:\n        kind: io.l5d.static\n        configs:\n        # Use HTTPS if sending to port 443\n        - prefix: \"/$/io.buoyant.rinet/443/{service}\"\n          tls:\n            commonName: \"{service}\". @Aavon, golang gRPC uses HTTP/1.1 tunneling when using HTTP proxies instead of HTTP2 AFAIK - see https://github.com/grpc/grpc-go/issues/1243\nYou'd be better off using iptables forwarding every gRPC call to the right destination or forwarding your service name patterns to the proxy via ad-hoc DNS server. Also, try pointing HTTP_PROXY to the HTTP1 proxy, but it may not work.. You may try doing curl -H \"l5d-add-context:1\" -XTRACE yourservice:port to see how exactly the resolution works. Usually resolvers enter the dtab table with /svc/<your-hostport> instead of /<hostport>, but that depends on the config.. Welcome :)\nI suspect the problem may lie in the retryer - theoretically, l5d could report that error if one pod did drop the connection and another one is unavailable (?). Dunno what's the current error reporting rule is\nCould you point me to any info on how to test l5d locally w/o any k8s setup? Like, is there any way to run l5d+namerd on my machine and let it eat IPs/addresses from a flat file instead of k8s API?. Nono, the limit is not on linkerd side, but on k8s side: see --http2-max-streams-per-connection @ https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/ .\nLinkerd exhausts apiserver's limits and stops resolving k8s addresses.\nWe don't have any customized H2 routers, HTTP and vanilla H2-for-gRPC only - but the resolver fails for any router, both HTTP and gRPC.. Yeah, on a second thought, aux connections feel like a hack. Logging would definitely help tho :) \nWhy l5d uses a separate stream for every service/deployment/hostname? It seems that it brings an unnecessary cap on the service count. . It is a default setting in Go, see https://github.com/golang/net/blob/master/http2/server.go#L57\nk8s docs say:\n```\n--http2-max-streams-per-connection int\n| The  limit that the server gives to clients for the maximum number of  streams in an HTTP/2 connection. Zero means to use golang's default.\n```\nSo if a flag wasn't set then k8s-apiserver sets max of 250 concurrent streams per connection.. ",
    "zillani-nuovo": "@adleong I am working on reproducing the issue under same conditions where this was reported, I want to load test the setup up for few days. I will update you soon, thanks for the support!. Hey, @adleong thanks for checking, we noticed the issue again today, and I have sent you the dig logs.\ncurrently, we are restarting the pods for fixing the issue & the dnspolicy is ClusterFirstWithHostNet. ",
    "dunjut": "dtab result for /svc/hello\n\n. ",
    "mikebz": "@dadjeibaah I'd love to contribute, I am not a Scala developer though - if it was Python/Plain Java/C#/Node - I'd be of more help.\n@adleong can you help me with a pointer on how to achieve that?  That'd be much appreciated.. Yes that makes sense.  I think after digging into this for a minute I figured that for me to contribute I need to get up to speed on Scala, the configuration system and also the logging system.  I don't know if I am the best candidate for implementing this feature, but all your guys' comments are spot on.. ",
    "Pveasey": "@adleong a single ZK node in the cluster of 5 hit full disk. In my understanding this shouldnt cause an interruption but it caused a lot of problems for linkerd and namerd. Will try and reproduce in our dev environment, and get back to you. . @adleong I've tried to reproduce this but was unable, the only differences in config would be there are far more linkerd clients in prod (150+ vs 6). \nId expect it is still a problem with production cluster as there was clear causation, but without hard evidence i'm good with closing.. ",
    "milesbxf": "Thanks @adleong ! I can see immediately that the Prometheus response is 1.8M and the json one 853k, which is interesting.\nWhat's the best way to share these with you? I'm on the linkerd Slack if that helps. We'll give it a go - thanks :). ",
    "mstoskus": "Hi, @dadjeibaah! I signed the commits.. Hi @dadjeibaah ! I added \"Signed Off By:\" portion to my commits.. Hi, @adleong! Yes, I'll add the comment.. ",
    "MikeRoetgers": "Whoops, my bad. Mixed up the targets in centralized logging. The exceptions were coming from a linkerd runnig version 1.3.7, the 1.4.3 instances are indeed not logging this anymore. All good. . ",
    "ivanopagano": "This is what I get from the endpoint after my latest changes\njson\n{\n    \"/dc1/consul\": {\n        \"poll\": {\n            \"lastRequestAt\": \"2018-06-27 18:31:40 +0000\",\n            \"lastResponseAt\": \"2018-06-27 18:31:40 +0000\",\n            \"request\": \"GET /v1/catalog/service/consul?index=153&dc=dc1\",\n            \"response\": {\n                \"index\": \"153\",\n                \"value\": [\n                    {\n                        \"Address\": \"127.0.0.1\",\n                        \"Node\": \"l5d-test\",\n                        \"ServiceAddress\": \"127.0.0.1\",\n                        \"ServiceID\": \"consul\",\n                        \"ServiceName\": \"consul\",\n                        \"ServicePort\": 9999,\n                        \"ServiceTags\": [\n                            \"primary\",\n                            \"v1\"\n                        ]\n                    }\n                ]\n            }\n        },\n        \"state\": {\n            \"lastStartedAt\": \"2018-06-27 18:26:47 +0000\",\n            \"lastUpdatedAt\": \"2018-06-27 18:31:40 +0000\",\n            \"running\": true,\n            \"value\": {\n                \"addrs\": [\n                    {\n                        \"addr\": \"127.0.0.1:9999\",\n                        \"metadata\": {\n                            \"endpoint_addr_weight\": 1.0\n                        }\n                    }\n                ],\n                \"metadata\": {}\n            }\n        }\n    },\n    \"/dc1/play\": {\n        \"poll\": {\n            \"lastRequestAt\": \"2018-06-27 18:30:53 +0000\",\n            \"lastResponseAt\": \"2018-06-27 18:30:53 +0000\",\n            \"request\": \"GET /v1/catalog/service/play?index=66&dc=dc1\",\n            \"response\": {\n                \"index\": \"66\",\n                \"value\": [\n                    {\n                        \"Address\": \"192.168.0.148\",\n                        \"Node\": \"l5d-test\",\n                        \"ServiceAddress\": \"127.0.0.1\",\n                        \"ServiceID\": \"play\",\n                        \"ServiceName\": \"play\",\n                        \"ServicePort\": 9000,\n                        \"ServiceTags\": [\n                            \"primary\",\n                            \"v1\"\n                        ]\n                    }\n                ]\n            }\n        },\n        \"state\": {\n            \"lastStartedAt\": \"2018-06-27 18:30:53 +0000\",\n            \"lastUpdatedAt\": \"2018-06-27 18:30:53 +0000\",\n            \"running\": true,\n            \"value\": {\n                \"addrs\": [\n                    {\n                        \"addr\": \"127.0.0.1:9000\",\n                        \"metadata\": {\n                            \"endpoint_addr_weight\": 1.0\n                        }\n                    }\n                ],\n                \"metadata\": {}\n            }\n        }\n    }\n}. Thanks for being patient and for the careful review. I'd need a hand to figure out how to add the new Admin Handlers exposed now by the ConsulDtabStore to the standard admin service\nLooking at the codebase I seem to understand that Namers endpoints are added by default, but it seems instead that it doesn't work the same for the DtabStore. I'm also wondering if it's useful for the ConsulDtabStore.list calls to be recorded and shown to the admin endpoints.\nFor what I understand the registered namespaces are already available as keys to the dtabs map, and I don't see any particular value in a separate endpoint. Indeed it would be possible, but only if the actual \"parser\" is passed to the LookupCache.apply call each time by the corresponding subclass.\nThis is subjective, but this way the essential difference between the two Namers is actually captured by that function and nothing more.\nEven more, the function is passed to build the specific instance of LookupCache because it's there that the Path is now parsed, to avoid passing both the parsed Path elements and the whole Path (needed as a cache key) to the cache lookup call.\nI'm not totally against the different subclasses, I'm just pointing out things to consider to decide what's the best solution.\nLet me know what you think about this. I must've been overly-cautious to start with, considering it would be called from different places. Removing now. Cool then, I'll see to it \ud83d\udc4d . Now that I think of it, after the latest changes it doesn't make sense anymore, I'll fix. indeed, that's why the function doing that is memoized (i.e. convertToName)\nTo be honest, I'm not entirely sure if memoization is a good fit for an input parameter of type Future[InstrumentVar[Addr]] actually.\nWould you like it better if we make 2 actual caches, one for the namer call (holding the Activity[...]) and another for the admin endpoint calls (holding the InstrumentedBind, and thus the original InstrumentedVar[Addr])?. this would happen only if there's a race condition to compute the address, and never again once the cache is filled\nare you suggesting to synchronize the whole method with the mutex for the cache and double check at the beginning of the method?. I've rewritten it to use a Var[...] to be able to make the result visible outside the synchronized thunk\nI'm not familiar enough with twitter-util to evaluate if there could be issues related to Var lazy semantics. wdyt?\nDid you had anything else in mind?. Now that I think of it, are you suggesting to instead memoize the \"whole LookupCache\" call instead of simply convertToName?\nThat wouldn't sound so bad in fact. I didn't actually trust enough Memoize to avoid race conditions\nI've reviewed the source code and I'll remove this redundant check. will do. I'll put myself to it after dinner. will this correction need to be done on io.buoyant.namerd.iface.PollState, too?\nthe original code was there. fine for me. I shamelessly stole the one for k8s from @dadjeibaah (thanks). done and fixed the previous comment on the concurrent map definition. the idea is that the api call function is now reified as an object, so I made a link with my idea of a thunk, i.e. () => Something. I'll see to it. ",
    "Aavon": "I modified the configuration according to the advice you gave.\n(I used to use \u201cminikube\u201d)\nbut,I get the error in the \"l5d\"\nW 0703 01:51:25.642 UTC THREAD33: [S L:/172.17.0.2:4340 R:/172.17.0.1:50946] HTTP/2 router could not handle non-HTTP/2 request!\nW 0703 01:51:49.742 UTC THREAD35: [S L:/172.17.0.2:4340 R:/172.17.0.1:51016] HTTP/2 router could not handle non-HTTP/2 request!\nW 0703 01:52:50.645 UTC THREAD38: [S L:/172.17.0.2:4340 R:/172.17.0.1:51170] HTTP/2 router could not handle non-HTTP/2 request!\nW 0703 01:59:47.537 UTC THREAD33: [S L:/172.17.0.2:4340 R:/172.17.0.1:52244] HTTP/2 router could not handle non-HTTP/2 request!\nI remembered there is some way to exclude some ports (I have not noted...)\nOur microservice build by a API Gateway. It handler the network request and Distribute the request to some service instance:\nprotocol\n network - API Gateway: http 1.1\n API Gateway - Service,Service - Service:gRPC\nCould you give me some advice about the Linkerd integration?\nThank for your reply!\n. I make a clean Minikube & Linkerd.\n1.kubectl error\nI kubectl`s error log no longer appears.\n2.multi port support\nI change the proxy to HTTP/1.1\nThe following error no longer appears.\nHTTP/2 router could not handle non-HTTP/2 request!\nAnd\nI folow your advice in the slack\n\n@AaronZz you should route to /#/io.l5d.k8s/<namespace>/<portname>/<deployment|service> in the end\nI debuged the dtab,they matched.But  it dose not work when actual service calling\n\nD 0705 12:38:36.706 UTC THREAD33 TraceId:14338bbb1619535e: k8s lookup: /kube-zcloudsoft/grpc-srv/v1-user /kube-zcloudsoft/grpc-srv/v1-user\nD 0705 12:38:36.717 UTC THREAD33 TraceId:14338bbb1619535e: k8s watch cancelled\nD 0705 12:38:39.801 UTC THREAD626 TraceId:8789c55deba1a49f: Failed to resolve v1-user. Error java.net.UnknownHostException: v1-user\nkube-zcloudsoft: k8s namespace\ngrpc-srv: port name\nv1-user: service name\n\"To do \"\nI am not sure the router work well. So I could not sure the HTTP/1.1 works.\nBy the way,iptables may be works but I dont thinks its a convenient way to config all service request router...\n. Thanks!\nI just used this config.But it not work,So i change the ConfigMap\u2018s dtab spec.\nSuch as,my addr is /srv-user:9090 by gRPC/h2\nThen,I work right on the dtab debug UI,but it alse not work for the internal call.\nI think It may still be a proxy problem.\nI`ll close the issue,since I have some useful infomation.\nThank you all for your help.\n. ",
    "flodiebold": "Hi @adleong, I think that fixed the analogous problem for interpreters, but it still exists for namers. I was testing with master.\nI think the difference is that namers use wrap and not delegatingWrap, and only the latter was fixed?. ",
    "a4chet": "Thank you for the response. . ",
    "lifubang": "It can't work when use scale button. If you use update yml config, it can work.. ",
    "AMCR": "Hi @adleong,\nWe have updated the code with all the missing tasks and renamed the classifier to retryableAll5XX. We have also extended the feature to Http2 protocol.\nThanks. Shall I merge #2053 with mine?. ",
    "evhfla": "Will do\nOn July 13, 2018, at 9:26 PM, Dennis Adjei-Baah notifications@github.com wrote:\n@evhflahttps://github.com/evhfla thanks for filing this. Could you try again and see if the issue is fixed? I just rebuilt the images and pushed them to docker.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://github.com/linkerd/linkerd/issues/2069#issuecomment-404988950, or mute the threadhttps://github.com/notifications/unsubscribe-auth/Abr7dGgsStNvikJPnSOFJKoHBfSZGqg_ks5uGUiygaJpZM4VPpuc.\n. That took care of the issue....Thanks.. Is this same issue present in the OpenJ9JDK version?  If it is not, is that version good enough to run in production?. Thanks for the quick reply @zackangelo ...hoping that Alex can have a fix soon.. Looks good to me @adleong.  Thank you for your quick action on this matter.. ",
    "dhruvmphatek": "@dadjeibaah Yes, we are getting error in namerd logs \nAttempting to observe /icx/arpit/default\nE 0720 11:47:08.715 IST THREAD15 TraceId:d589104d0a12e23e: Operation failed with com.twitter.finagle.serverset2.client.KeeperException$NoNode. Session 164ae098394001d. @dadjeibaah Named configuration\n```\nadmin:\n ip: 127.0.0.1\n port: 7770\ninterfaces:\n- kind: io.l5d.httpController\n  ip: 0.0.0.0\n  port: 4180\nstorage:\n kind: io.l5d.zk\n experimental: true\n pathPrefix: /icx/arpit\n zkAddrs:\n - host: 192.168.0.90\n   port: 2181\nnamers:\n - kind: io.l5d.serversets\n   zkAddrs:\n   - host: 192.168.0.90\n     port: 2181\n```. @dadjeibaah Namerd host is running on 0.165 server and zookeeper is running in 0.90 server, but can communicate each other over the network.\n. @dadjeibaah namerd log shows \nTraceId:6398a87a0c7ec677: Attempting to observe /arpit/default. we didn't create any node under /arpit/default, how to change this path in namerd configuration.. @adleong We don't need to create the node manually in zookeeper. we want linkerd to create a node in zookeeper whenever we create a new instance of application and linkerd should announce and register the same to zookeeper. Also, when we create node under /arpit/default, no error message is displayed in namerd logs but it shows attempting to observe /arpit/default and it is taking more than 10 seconds to observe due to which time out occurred.. @adleong Actually we have updated /icx to /arpit and kept /#/io.l5d.serversets/arpit, so in linkerd log we are getting announcing /192.168.0.165:4140 as /#/io.l5d.serversets to zk-serversets and Set group member ID to member_0000000000 message. And we have one more linkerd and application instance running on different machine with same configuration.\nApplication instance running on different servers has different context paths. So, when we are making a request from one server and calling the application instance running on different machine all the requests are not returning the data. and namerd printing error message continuously TraceId:6398a87a0c7ec677: Attempting to observe /arpit/default. we didn't create any node under /arpit/default, how to change this path in namerd configuration .\nwhen we had changed the prefix to /arpit/default then in namerd Attempting to observe /arpit/default is printed and after a while timeout occurred with no data returned.\n. @adleong Hi,now we are facing com.twitter.finagle.naming.buoyant.DynBoundTimeoutException: Exceeded 10.seconds binding timeout while resolving name: /svc/192.168.0.165:4140 issue, when we are running 4 instances of linkerd. We have increased the timeout, but still not getting the response.. @adleong actually that's the host n port where my linkerd is running. Please help me to resolve this issue. What value should I put into it, my application instance is running on port 8080.. @adleong Hi, Below Error occurred in namerd logs \nException propagated to the default monitor (upstream address: /192.168.0.108:51209, downstream address: n/a, label: io.l5d.httpController).\njava.lang.IllegalArgumentException: '/' expected but '{' found at '[{]\"serviceEndpoint\":{\"host\":\"192.168.0.165\",\"port\":4041},\"additionalEndpoints\":{},\"status\":\"ALIVE\",\"shard\":0}'\nwe are trying to get the details of nodes after creating a node in linkerd.  . @dadjeibaah and @adleong thank you guys for helping us in setting up the environment. We have successfully implemented our system using linkerd and zookeeper. \nThanks a lot.. ",
    "ylopatin-earnin": "@adleong \nLog says\nE 0802 12:29:17.319 UTC THREAD29: service failure: Failure(No route to host: /100.96.7.223:80 at remote address: /100.96.7.223:80. Remote Info: Not Available, flags=0x09) with RemoteInfo -&gt; Upstream Address: Not Available, Upstream id: Not Available, Downstream Address: /100.96.7.223:80, Downstream label: #/io.l5d.k8s/default/http/{{MY_SERVICENAME}}, Trace Id: 941d1dae33b973cf.941d1dae33b973cf&lt;:941d1dae33b973cf\nwhere 100.96.7.223 - ip of previous pod\nAfter deployment l5d still connects to old ip\nAlso i see the old IP in /namer_state/io.l5d.k8s.json output\n /namer_state/io.l5d.k8s.json is updated only with pod restart. @adleong thank you for quick update\nFollowing info is attached:\n- linkerd /namer_state/io.l5d.k8s.json  responses  before and after deployment (before_deployment_io.l5d.k8s.json.log and after_deployment_io.l5d.k8s.json.log)\n- kubernetes /api/v1/namespaces/default/endpoints/gateway responses  before and after deployment (before_deployment_k8s_api.json.log and after_deployment_k8s_api.json.log)\nafter_deployment_k8s_api.json.log\nbefore_deployment_io.l5d.k8s.json.log\nbefore_deployment_k8s_api.json.log\nafter_deployment_io.l5d.k8s.json.log\n. @adleong,  '100.96.7.223' was in initial logs, logs files attached are 1 day after.\n1 day - several l5d restarts, so it is expected that first IP is not found in logs\nPlease look only to 4 files attached, they describe behavior right before and after deployment\nThing i want to show is screenshoted (endpoints number changed coz of replicas # change):\n\n. @adleong  thanks for answers\nLastStreamData is MODIFIED with new values, but why does linkerd pod bind to old IP as it is seen in logs?\nwhich information could i share to help in understing the case?\nThanks. /client_state.json is not updated after deployment, staying same as before.\nOnly pod restart wipes old data from gateway client. @lgmyrek can you share more details?\nk8s cluster version?  what tool or cloud used?\nis it in AWS?\nthanks. @adleong  as my provider is AWS, other guy's is bare metal, my linkerd  is also 1.4.5, but i saw that problem even on 1.3.7. So, i do not think it is k5d version or provider specific error\nMay be wrong configuration,\nI can share my configs, may be there is an issue:\nl5d-configmap-main-yaml.txt\n. 1. configure router like this\n```\nadmin:\n  ip: 0.0.0.0\n  port: 9990\nusage:\n  orgId: linkerd-daemonset\nnamers:\n- kind: io.l5d.k8s\n  host: localhost\n  port: 8001\n- kind: io.l5d.rewrite\n  prefix: /portNsSvcToK8s\n  pattern: \"/{port}/{ns}/{svc}\"\n  name: \"/k8s/{ns}/{port}/{svc}\"   \nrouters:\n- protocol: http\n  label: http-ingress\n  client:\n    failureAccrual:\n      kind: none\n  identifier:\n    kind: io.l5d.path\n    consume: true      \n  interpreter:\n    kind: io.l5d.k8s.configMap\n    experimental: true\n    name: l5d-dtabs-config\n    filename: http-ingress\n    namespace: default\n  servers:\n  - port: 4142\n    ip: 0.0.0.0\n    clearContext: true\n  bindingTimeoutMs: 30000\n  bindingCache:\n    idleTtlSecs: 1\nand dtab for it\n/svc       => /#/io.l5d.k8s/default/http/gateway;\n```\nDeploy any service to default namespace, port http and name 'gateway'\ntry redeploy it\nand access via linkerd. ",
    "lgmyrek": "@adleong  I'm suffering from similar issue, ~~observed for now only with replicas >= 2~~\nedit: now it failed for for replica: 1. @ylopatin-earnin\nbare metal, 1.10\nlinkerd: 1.4.5. ",
    "sahilbadla27": "I am seeing this issue as well in production. Was able to create a test cluster and easily reproduce the issue. Happens more frequently on delete(pod) events than upgrade(deployment) events. Config:\nkubernetes in AWS (not EKS)\nlinkerd 1.5.0\n. @adleong's fix(v1.5.1-stab) looks promising. Its deployed in dev cluster for a day now and haven't seen any issue.. ",
    "fixed-point": "Hi @adleong, thanks for responding!\nThis is good to know -- it does appear that this resolves the issue for HTTP/1.1 requests, thank you!\nIs there an equivalent for HTTP/2 requests? I tried the same principle with io.l5d.header.token (since io.l5d.header is not an identifier for h2), but couldn't get it to work.. I can confirm this resolves the issue. Thank you very much!. ",
    "bdlk": "@adleong Nice catch! Thank you :). ",
    "krancour": "@wmorgan can you comment at all on why Istio integration would be deprecated? There's no judgement implied in the question-- just curiosity. A Bouyant blog post from a year ago proposed that Istio (its control plane) and Linkerd complemented one another like peanut butter and jelly. That seemed to make sense, so I'm just trying to wrap my head around what changed.. It's partly academic. I have seen a lot of Istio v Linkerd comparison, but based on the blog post I referenced, I'd believed them to be complementary.\nLinkerd does not seem to expose its own control plane via k8s CRDs, which makes it harder to configure the mesh through a manifest or a Helm chart, for instance. (Although, I guess, maybe this can be done via ConfigMaps?) To the extent that I understood Istio to be providing a universal service mesh control plane (or attempting to), I quite enjoyed the notion of policies being decoupled from the underlying mesh via such an abstraction. I was looking forward to someday using those CRDs (or similar) with a mesh of my choosing, or even better, allowing me to configure policies without having to even know that the underlying mesh is.\nLinkerd's integration with Istio had seemed to speak to that vision.\n. ",
    "grampelberg": "Well that's embarrassing. Should be back to working now. Thank you for the report!. @vguhesan this isn't my day apparently. It should be working for reals now and I believe I've got a permanent solution to make sure this never happens in the future.\n(tldr. is that I'm fighting with sass locally, the *.css is cached, CDN isn't flushing the cache because the name isn't changing). ",
    "vguhesan": "Verified - thanks for the speedy turnaround! ;-). @grampelberg Sorry to bring this up - but now I'm seeing SCSS errors on other parts of the site.\nExample links:\nhttps://linkerd.io/\nhttps://linkerd.io/1/advanced/dtabs/\n\nAm I seeing this because the content has not yet fully synced up to all the CDN nodes?. ",
    "mjschmidt": "Thanks will take a look!. ",
    "rogoman": "One possible fix would be for the marathon.v2.Api object to also look at the app/healthChecks property in the JSON returned from v2/apps/[appId] to determine if an app has any health checks configured at all. If so, a task should be excluded from the load-balancing pool if the healthCheckResults property is an empty array.. @dadjeibaah PR ready: https://github.com/linkerd/linkerd/pull/2099. Please note that I only learned the basics of Scala for the purpose of this PR, so any suggestions to my coding style or hints on how to implement things easier are highly appreciated.. @dadjeibaah not introducing this additional class was my first idea but I worried that if I added a new attribute to the original Task case class, it wouldn't match what Marathon API returns anymore. @dadjeibaah, @adleong it's your call. I can implement any case you choose.. @adleong as for manual testing, I have built a Docker image using the instructions from BUILD.md and was planning to deploy such image to one of our test clusters to perform the test that led me to filing the original issue. I apologize for not doing it upfront, but was hoping to get a general \"thumbs-up\" from you guys before performing any extensive testing.. The original test that I performed was deploying a service that I can mark certain instances of to be unhealthy. See the chart below (logs of incoming HTTP requests grouped by app instance):\n\nOriginally it was running 3 instances (2 healthy). I scaled it up to 5 instances (3 should be healthy) but during the health check grace period one of the additional instances (that was failing the health check but still in the grace period) received traffic as well.\nNow this is the situation after my fix:\n\nInitially I had 3 healthy instances. I scaled them up to 5, out of which 4 were supposed to be healthy. During the grace period none of the requests were routed to any of the new instances. Once the fourth instance showed up as healthy, it started receiving requests. Same thing happened after scaling up to 8 instances (7 were supposed to be healthy). During the health-check grace period none of the new instances received any traffic. Once Marathon decided they're healthy, they started receiving traffic.\nThis is just visual data, to help understand the procedure. Of course I checked the logs of all the instances just to make sure there were no requests routed when there shouldn't be. None of the unhealthy instances received any requests, ever.. Thanks for the approval, @adleong . Is there anything else I need to do before this is merged to master? . @adleong yes. Adding the number of health checks to the original Task case class was the easiest to implement, but I worried that it's going to be confusing for anyone reading the code and trying to find a match with what can be found in the Marathon API.. ",
    "hsmade": "Np, you can ping me on slack if you need more info.\nThx,\nWim Fournier\nOn Mon, Aug 20, 2018, 19:26 Dennis Adjei-Baah notifications@github.com\nwrote:\n\nThanks for filing this issue @hsmade https://github.com/hsmade and\nthanks for the detail repro steps! We will take a look.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/linkerd/linkerd/issues/2104#issuecomment-414397031,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABFW9bCAix7-sR22O3LDIV4TlqalaDxeks5uSvEogaJpZM4V_Uoh\n.\n. Sorry, haven't had the time yet. It's on my todo list though.\n\nOn Tue, Sep 11, 2018, 00:20 Alex Leong notifications@github.com wrote:\n\n@hsmade https://github.com/hsmade any luck investigating this?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/linkerd/linkerd/issues/2104#issuecomment-420080476,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABFW9QPdn8nkvQ1eKQKfELmI1pEwMcMsks5uZuWegaJpZM4V_Uoh\n.\n. I like the axe idea :) The issue we actually had was that the consul server got overloaded, so connections started to time out and drop. I guess you could simulate that by ddossing consul.. I think the answer may lie in the difference of state in time. If your DC\ndoesn't exist, you'll find that out on the first call. A DC doesn't usually\ncease to exist during runtime. Where as the other reasons for 500 will.\n\nOn Wed, Oct 10, 2018, 22:48 Dennis Adjei-Baah notifications@github.com\nwrote:\n\nYea this is tricky. I also do not like the idea of checking the\nhuman-readable error from the 500 response and using that info to determine\nwhat state the namer should be in. I am in favor of reverting this change\nback so that all 500 responses always use the last known good state and\nalways retry.\n1863 https://github.com/linkerd/linkerd/pull/1863 was meant to fix a\ncase where nonexistent dcs would never resolve the namer state. It feels as\nif that is an edge case scenario and that more often than not a dc should\nalways exist.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/linkerd/linkerd/issues/2145#issuecomment-428726111,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABFW9YhOs_aOTGirGIeyB_OKso9bjCQdks5ujl0OgaJpZM4XSKjy\n.\n. \n",
    "muuki88": "Muki to the rescue \ud83e\udd84. ",
    "zoltrain": "So we've run into the problem in the wild.\nWe're currently running this version of the docker image. buoyantio/linkerd:1.4.5 and we have it deployed into google kubernetes engine.\nRequest will start to fail after an indeterminate amount of time, a pod that has a gRPC client connected to another gRPC service.\nThis tends to happen on services with a high release cadence, so our staging services fail around the week mark, but I think it really just depends on how often the pod is moved. The service which is being developed at the moment has had this happen twice in the last week.\nI've tested connectivity from the running pods network, as in shell in and run grpcurl, hitting the service that's unreachable from our go processes, and the call happens without error. So new connections are not a problem. This steered us towards hung client gRPC connections in the go processes. I managed to run tcpdump on the pods in question, and from what I could tell the connections the client were sending to linkerd were being rejected.\nUnfortunately I can't post the pcap files I exported as there's sensitive data in there.\nI'm going to try repeat the above test, but add in keep alives on the clients to see if the subsequent ping will sent in the keep alive check can reset the connection.\n. @siggy so gRPC client keep alives won't force a reconnect. They still fail, just without timeouts.\nI tried this is bb\n```func NewGrpcClientsIfConfigured(config *service.Config) ([]service.Client, error) {\n    clients := make([]service.Client, 0)\nfor _, serverURL := range config.GRPCDownstreamServers {\n    target := serverURL\n    authority := \"\"\n    clientID := serverURL\n    if config.GRPCProxy != \"\" {\n        target = config.GRPCProxy\n        authority = serverURL\n        clientID = config.GRPCProxy + \" / \" + serverURL\n    }\n    options := []grpc.DialOption{\n        grpc.WithAuthority(authority),\n        grpc.WithBlock(),\n        grpc.WithInsecure(),\n    }\n\n    if config.EnableKeepAlive {\n        keepAliveParams := keepalive.ClientParameters{\n            Time:                config.KeepAliveInterval,\n            Timeout:             config.KeepAliveTimeout,\n            PermitWithoutStream: true,\n        }\n\n        options = append(options,\n            grpc.WithKeepaliveParams(keepAliveParams),\n        )\n    }\n\n    ctx, cancel := context.WithTimeout(context.Background(), config.DownstreamTimeout)\n    defer cancel()\n\n    conn, err := grpc.DialContext(\n        ctx,\n        target,\n        options...,\n    )\n    if err != nil {\n        return nil, err\n    }\n\n    client := pb.NewTheServiceClient(conn)\n    clients = append(clients,\n        &theGrpcClient{\n            id:         clientID,\n            conn:       conn,\n            grpcClient: client,\n            timeout:    config.DownstreamTimeout,\n        },\n    )\n}\n\nreturn clients, nil\n\n}\n```\nSomething interesting I found today, I figured, well if I can't get the clients to reconnect, maybe I can just get the server to force them to refresh their connections periodically.\ngRPC ServerOptions also have keep alive settings so I did this in bb\n```func NewGrpcServerIfConfigured(config service.Config, serviceHandler service.RequestHandler) (service.Server, error) {\n    if config.GRPCServerPort == -1 {\n        return nil, nil\n    }\ngrpcServerPort := config.GRPCServerPort\nlis, err := net.Listen(\"tcp\", fmt.Sprintf(\":%d\", grpcServerPort))\nif err != nil {\n    return nil, err\n}\n\noptions := grpc.KeepaliveParams(keepalive.ServerParameters{\n    MaxConnectionAge: 5 * time.Minute,\n})\ngrpcServer := grpc.NewServer(options)\ntheGrpcServer := &theGrpcServer{\n    grpcServer:     grpcServer,\n    port:           grpcServerPort,\n    serviceHandler: serviceHandler,\n}\n\npb.RegisterTheServiceServer(grpcServer, theGrpcServer)\nlog.Infof(\"gRPC server listening on port [%d]\", grpcServerPort)\ngo func() { grpcServer.Serve(lis) }()\nreturn theGrpcServer, nil\n\n}\n```\nWhat was interesting about the above is the server forcibly closes the connections after 5 minutes, by sending a reset I think. Now the clients are supposed to reconnect, with a backoff. But by doing the above they ALL go into a failure state like the issue we're seeing after the indeterminate length of time without any of the settings. This leads me to believe the reset isn't being propagated to the client.\nThis is the slow cooker output.\nslow-cooker-6c449c8d54-kpj57 slow-cooker # sending 1 GET req/s with concurrency=1 to http://bb-p2p.lifecycle1 ...\nslow-cooker-6c449c8d54-kpj57 slow-cooker #                      good/b/f t   goal%   min [p50 p95 p99  p999]  max bhash change\nslow-cooker-6c449c8d54-kpj57 slow-cooker 2018-09-28T14:27:39Z      9/0/0 10  90% 10s 123 [140 271 271  271 ]  271      0 +\nslow-cooker-6c449c8d54-kpj57 slow-cooker 2018-09-28T14:27:49Z     10/0/0 10 100% 10s 166 [207 254 254  254 ]  254      0\nslow-cooker-6c449c8d54-kpj57 slow-cooker 2018-09-28T14:27:59Z     10/0/0 10 100% 10s 127 [194 404 404  404 ]  404      0\nslow-cooker-6c449c8d54-kpj57 slow-cooker 2018-09-28T14:28:09Z      5/5/0 10 100% 10s  63 [234 544 544  544 ]  544      0\nslow-cooker-6c449c8d54-kpj57 slow-cooker 2018-09-28T14:28:19Z     10/0/0 10 100% 10s  97 [184 501 501  501 ]  501      0\nslow-cooker-6c449c8d54-kpj57 slow-cooker 2018-09-28T14:28:29Z     10/0/0 10 100% 10s 106 [139 475 475  475 ]  475      0\nslow-cooker-6c449c8d54-kpj57 slow-cooker 2018-09-28T14:28:39Z     10/0/0 10 100% 10s 105 [164 408 408  408 ]  408      0\nslow-cooker-6c449c8d54-kpj57 slow-cooker 2018-09-28T14:28:49Z     10/0/0 10 100% 10s  89 [133 606 606  606 ]  606      0\nslow-cooker-6c449c8d54-kpj57 slow-cooker 2018-09-28T14:28:59Z     10/0/0 10 100% 10s 102 [123 218 218  218 ]  218      0\nslow-cooker-6c449c8d54-kpj57 slow-cooker 2018-09-28T14:29:09Z      6/1/0 10  70% 10s 108 [167 4503 4503 4503 ] 4503      0 +\nslow-cooker-6c449c8d54-kpj57 slow-cooker 2018-09-28T14:29:19Z     10/0/0 10 100% 10s  85 [125 174 174  174 ]  174      0\nslow-cooker-6c449c8d54-kpj57 slow-cooker 2018-09-28T14:29:29Z     10/0/0 10 100% 10s  91 [104 287 287  287 ]  287      0\nslow-cooker-6c449c8d54-kpj57 slow-cooker 2018-09-28T14:29:39Z     10/0/0 10 100% 10s  91 [126 258 258  258 ]  258      0\nslow-cooker-6c449c8d54-kpj57 slow-cooker 2018-09-28T14:29:49Z     10/0/0 10 100% 10s  92 [120 212 212  212 ]  212      0\nslow-cooker-6c449c8d54-kpj57 slow-cooker 2018-09-28T14:29:59Z     10/0/0 10 100% 10s  77 [104 131 131  131 ]  131      0\nslow-cooker-6c449c8d54-kpj57 slow-cooker 2018-09-28T14:30:09Z      5/5/0 10 100% 10s  86 [120 243 243  243 ]  243      0\nslow-cooker-6c449c8d54-kpj57 slow-cooker 2018-09-28T14:30:19Z      8/2/0 10 100% 10s  93 [116 373 373  373 ]  373      0\nslow-cooker-6c449c8d54-kpj57 slow-cooker 2018-09-28T14:30:29Z     10/0/0 10 100% 10s  89 [111 243 243  243 ]  243      0\nslow-cooker-6c449c8d54-kpj57 slow-cooker 2018-09-28T14:30:39Z     10/0/0 10 100% 10s  76 [104 187 187  187 ]  187      0\nslow-cooker-6c449c8d54-kpj57 slow-cooker 2018-09-28T14:30:49Z     10/0/0 10 100% 10s  63 [ 94 138 138  138 ]  138      0\nslow-cooker-6c449c8d54-kpj57 slow-cooker 2018-09-28T14:30:59Z     10/0/0 10 100% 10s  72 [ 87 119 119  119 ]  119      0\nslow-cooker-6c449c8d54-kpj57 slow-cooker 2018-09-28T14:31:09Z     10/0/0 10 100% 10s  81 [110 180 180  180 ]  180      0\nslow-cooker-6c449c8d54-kpj57 slow-cooker 2018-09-28T14:31:19Z      2/1/0 10  30% 10s  81 [103 3619 3619 3619 ] 3618      0 +\nslow-cooker-6c449c8d54-kpj57 slow-cooker Get http://bb-p2p.lifecycle1: net/http: request canceled (Client.Timeout exceeded while awaiting headers)\nslow-cooker-6c449c8d54-kpj57 slow-cooker 2018-09-28T14:31:29Z      0/0/1 10   0% 10s   0 [  0   0   0    0 ]    0      0 -\nslow-cooker-6c449c8d54-kpj57 slow-cooker Get http://bb-p2p.lifecycle1: net/http: request canceled (Client.Timeout exceeded while awaiting headers)\nslow-cooker-6c449c8d54-kpj57 slow-cooker 2018-09-28T14:31:39Z      0/0/1 10   0% 10s   0 [  0   0   0    0 ]    0      0 -\nslow-cooker-6c449c8d54-kpj57 slow-cooker Get http://bb-p2p.lifecycle1: net/http: request canceled (Client.Timeout exceeded while awaiting headers). One last thing to add, I also tried a pseudo control setup. A gRPC server and client where I generate a load of requests, but also set MaxConnectionAge on the server. I tested a request every 100ms, for 10 seconds, with an expectation of 100 requests completing without error. The clients successfully reconnect and there were no failures.. No problem.\n@siggy I've done some more digging today. I figured \"maybe this has been introduced recently\". So I've been retroactively testing the minor release changes, using the MaxConnectionAge setup I had on Friday. Good news, it fails consistently, and I also found that something may have been introduced between version 1.4.3 and 1.4.4 that causes this issue. I tested 1.4.4 and it has the failures after around the 10 min mark, I'm currently running 1.4.3, it's been running for 30 minutes without failure. I dug into the commit history between those releases and there was a lot of work done on H2 streams/connections. Finagle was upgraded, a stream buffer was replaced, and a status code guard for resets was removed.\nIf I had to guess, I'd say one of these might be causing this issue.\nhttps://github.com/linkerd/linkerd/commit/9876e3da13f1ab29365926e8455c334106397256\nhttps://github.com/linkerd/linkerd/commit/ac64c5991df2d008c4e6855982273eca4e63f51c\nhttps://github.com/linkerd/linkerd/commit/62aa66e5d0c6abec77f6289d5d9249a102928397\nI'm no Scala expert, so can't really comment much on what's going on in the above. I'll leave that to the experts.\nI'm going to run 1.4.3 continuously this afternoon to make sure it doesn't hit that same barriers eventually. If not we'll be looking at downgrading the release to hopefully stop this while the source of it is tracked down.. @siggy do we know when the next scheduled release will be, and if this will be in it?. ",
    "corhere": "Those are excellent questions! It took me a while to find the answers as I have not used Scala nor Finagle before, but the client id of all outgoing requests is set to the configured value. That being said, it seems that an unmodified linkerd 1.4.6 (downloaded straight from the GitHub release) does not copy the client id from an incoming TTwitter request into the outgoing client request either. Is there an underlying linkerd bug or am I doing something wrong?\nI tested the behaviour by modifying ThriftClient and ThriftServer from finagle-example to make a request with a client id and respond with the contents of the request's client id, respectively. Then I put linkerd in the middle.\n\nThriftClient.scala\nThriftServer.scala\nlinkerd.yaml\n\nWhen I connected them directly, the ThriftClient client id was echoed back, as expected. But when I put linkerd 1.4.6 in the middle, the server responded with \"no clientId\".. ",
    "Bachmann1234": "Just out of curiosity what was left to do in this PR? . Not 100% sure I have the bandwidth right now so if the original person comes back awesome. But I wanted to make a note in case I end up having a strong desire to pick something new up. . ",
    "richard-pounder-ck": "Should/Does this apply to watches used for the K8S? Although not 100% confirmed we have seen issues getting updates that look similar when K8S is used as the store.. ",
    "stamm": "@adleong done. ",
    "kleimkuhler": "@robertpanzer Thank you for the review! If you are able to take take a look at #2, you'll find these issues are addressed.\nThe first window, as well as windows that had no requests sent in the previous window, now reset the request count so that the limit cannot be doubled.\nThe scheduler has also been removed. It now works with timestamps so that a short period will not lead to any type of performance degradation.. Hi @robertpanzer -I am looping back around to this and I did not address your final bullet point in the original issue report:\n\nExceeded rate limits should be reported via the stats.\n\nI'm wondering if you would like the rate limiter example provided to have that functionality added? I realize it has been a few weeks since the last activity on this, but if you would still find that helpful I can work on adding that. Let me know either way, thanks!. I like the suggestion on adding these counters for services as well. I'll get that change up!. No problem! Yes I see what you mean. Thinking about it, I think a low-hanging fruit before looking at a complete refactor would be changing the type of labels to Map. It could avoid a lot of the member checking and string manipulation in addException. I could swing back around with some general changes for that after a merge.. @adleong New test is up and PR comment has an example of a line that was being incorrectly formatted.. > Running a linkerd docker image with this change displays this error:\n\nshell\n$ docker run -p 9990:9990 -v `pwd`/basic.config.yml:/config.yaml -v `pwd`/disco:/disco buoyantio/linkerd:1.5.2-SNAPSHOT /config.yaml\n/io.buoyant/linkerd/1.5.2-SNAPSHOT/bundle-exec: 15: /io.buoyant/linkerd/1.5.2-SNAPSHOT/bundle-exec: [[: not found\nIt might the fact that you the docker image runs the script with sh and not bash.\nA quick search on the interwebs shows that you can do logical regex shell expressions with grep but I'm not sure if its the best way.\n\nIt looks like case is the best performance wise for just a quick regex check. I made the change.. @adleong @dadjeibaah Ok my last commit introduced some significant changes from the first go at this.\nThe script still checks the version, but it enables GC logging no matter what Java version is being used (much better - thanks Alex). How this happens is GC_LOG_OPTIONS is now set the way that the specific Java version requires. For Java 9+, this is with a single -Xlog. The selectors, file, and decorators are equivalent to the old tags we used. I translated them via the article linked above: https://dzone.com/articles/disruptive-changes-to-gc-logging-in-java-9. I also moved -XX:+UseConcMarkSweepGC so that it is only set in a Java environment using a version than 9. This cleaned up a warning that was displayed about the flag being deprecated.\n@adleong I have tested this on 8, 9, 10, and 11.. Hi @chenhaibocode - We closed #2199 with #2208 recently.\nIt has been difficult to reproduce this specific issue. If this is still an issue for you, could you try reproducing it with the new 1.6.0-SNAPSHOT binaries found in this Google Drive folder? Apologies for providing the binaries this way; we have not released and uploaded the new version yet.\nIf that fixes the issue - great. If it does not, we ask that you try and provide some more concrete steps to reproducing this specific issue. If it requires certain configurations, please also try and provide those as text instead of images. The ability to copy and paste makes the process a lot quicker!\n. @adleong Thank you for the helpful review comments. I've resolved everything you addressed; let me know your thoughts!. @adw12382 Glad to hear you are seeing the expected fix from this change.\nThe io.netty.buffer.PoolCheck suspect is something you can keep an eye on as your environment continues to run. During my testing, it consistently showed up as a suspect, but the heap percentage did not scale similarly to the ZooKeeper HashMap. I do not think it is an issue, but if you observe differently, then we can address that in a separate issue.\nI will merge this as it currently is so that the ZooKeeper issue is fixed!. @adw12382 Glad to hear! We plan on releasing a new version soon.. @adleong Thanks for the question. I have confirmed that socketOptions is set correctly on clients.\nAfter running Linkerd in docker and establishing a connection to localhost:9990, I can see the following difference in netstat -to when I specify the following socketOptions for clients:\nclient:\n    socketOptions:\n      noDelay: true\n      reuseAddr: true\n      reusePort: true\n      readTimeoutMs: 61000\n      writeTimeoutMs: 61000\n      keepAlive: true\n      backlog: 128\nWithout socketOptions:\n$ netstat -to\n$ tcp        0      0 localhost:34354         localhost:9990          ESTABLISHED off (0.00/0/0)\nWith socketOptions:\n$ netstat -to\n$ tcp        0      0 localhost:47580         localhost:9990          ESTABLISHED keepalive (7189.26/0/0). Okay I found the change: https://github.com/twitter/finagle/commit/25e5551b0853395f0e742a43ef6b5d60f5521bfc\nWireTracingFilter was extended to the server. We are asserting the annotation keys for the server. Up until Finagle 19.1.0 the server would not generate these traces, but now it does.\nSo these additional traces are expected and updating the tests is the correct fix.. I agree it would be more descriptive. The reason I kept it as exn is because that's the naming convention of the metric tree. I'm not sure if that would introduce any confusion when trying to debug and realizing that the node exn is in fact labeled as exceptions in the output.. Yes exceptions will be nested, but the stack trace prints out each layer of the nested structure. In the case of an exception that is nested foo:bar:baz, it will be printed across three lines like this: foo, foo:bar, foo:bar:baz. The counter will count each layer of the nested structure.. Also related to my answer above, this is how I've seen them recorded. If you have seen differently then I would be interested in seeing that!. @dadjeibaah Okay thanks for the link - I understand what you mean. I can work on adding a catch for multiple exceptions within one counter.. That's what I was originally working with, but a nested exception is built incrementally - not all at once. In the case of .counter(\"bar\", \"qux\"), the first pass through writeMetrics will only see bar, and the second pass will only see qux.\nWe want to append qux to the already existing exception bar, but we can't do that in one go.. @adleong Hey I updated the issue to link back to finagle. @adleong Good call, thanks. I made the changes.. I decided to drop the longer name once we got into an area where there is no user interaction. User's specify nonAccruableStatusCodes in the optional parameter, but it then gets passed around as that name.\nIt's the same naming convention that retryableStatusCodes vs retryableCodes has.\nI can change both if you'd like!. Sure. It's not completely necessary, but it allows uses like this: https://github.com/linkerd/linkerd/blob/1880c232bf7483abd62c5307738d8887be665326/linkerd/protocol/h2/src/test/scala/io/buoyant/linkerd/protocol/h2/CompliantGrpcClassifierTest.scala#L44\nto not worry about passing parameters.\nIf there is a convention to stick with I can make a change if necessary.. Good call - this is fixed.. Okay that makes sense. I'll change the name.. Good call. It looks like we'll still need to duplicate def fromZKData since it is private, but that is it.. Comments have been added to each stage of the chain of operations!. I like that right now, the result of each operation is changed together with the addition of comments for each stage. I think that introducing an intermediate variable could possibly reduce readability as a reader may get hung up on the naming of that variable. I can separate the chain if you feel strongly about it, but otherwise I inclined to leave it. Let me know your thoughts!. ",
    "yarosman": "I am sorry, what is DCO?. @adleong  I added Signed-off-by. ",
    "ganasubrgit": "jcmd output from my docker linkerd\n```\nroot@ce01267112ae:/io.buoyant/linkerd/1.5.0# jcmd 1 VM.native_memory summary\n1:\nNative Memory Tracking:\nTotal: reserved=2745121KB, committed=1655117KB\n-                 Java Heap (reserved=1048576KB, committed=1048576KB)\n                            (mmap: reserved=1048576KB, committed=1048576KB)\n\n\nClass (reserved=1088396KB, committed=45776KB)\n                            (classes #7364)\n                            (malloc=908KB #7016)\n                            (mmap: reserved=1087488KB, committed=44868KB)\n\n\nThread (reserved=24777KB, committed=24777KB)\n                            (thread #25)\n                            (stack: reserved=24672KB, committed=24672KB)\n                            (malloc=76KB #127)\n                            (arena=28KB #48)\n\n\nCode (reserved=50156KB, committed=2772KB)\n                            (malloc=236KB #1162)\n                            (mmap: reserved=49920KB, committed=2536KB)\n\n\nGC (reserved=12840KB, committed=12840KB)\n                            (malloc=9404KB #207)\n                            (mmap: reserved=3436KB, committed=3436KB)\n\n\nCompiler (reserved=163KB, committed=163KB)\n                            (malloc=33KB #116)\n                            (arena=130KB #2)\n\n\nInternal (reserved=463267KB, committed=463267KB)\n                            (malloc=463267KB #40876)\n\n\nSymbol (reserved=10767KB, committed=10767KB)\n                            (malloc=9544KB #74950)\n                            (arena=1223KB #1)\n\n\nNative Memory Tracking (reserved=1954KB, committed=1954KB)\n                            (malloc=6KB #78)\n                            (tracking overhead=1947KB)\n\n\nArena Chunk (reserved=186KB, committed=186KB)\n                            (malloc=186KB)\n\n\nUnknown (reserved=44040KB, committed=44040KB)\n                            (mmap: reserved=44040KB, committed=44040KB)\n```. \n\n",
    "chenhaibocode": "\n. \n. \n. @adleong download it from the releases page. 1.5.2 version. @dadjeibaah ok, thank you.. @dadjeibaah  Is it related to the zookeeper version?. ",
    "menya84": "It's a great idea to truncate a response body in case of a mesh error (request timeout or bad routing, etc) or just unify all errors to have a generic error body for all cases. It will much simplify client side because now it's a headache to handle all this different structures of a response body for each error.\nLooking forward to this important fix.. ",
    "adw12382": "@edio We use the io.l5.mesh the most. The environment we observed having memory leak has 5 requests per second to Namerd in average. The number of Server Connections is around 150 ~ 170 in average. Let me know if you want to hear more specific details, thanks!. I investigated a bit and was able to generate the heap dump logs using jmap from the live n4d pod in our experimental environment. \nHere are steps I used to generate the heap dump logs\n  - Deploy Namerd into one of our clusters with replica number equals to 3.\n  - Get into the pod and execute jmap -dump:format=b,file=namerdump.hprof {Java PID}. Thanks @dadjeibaah for sharing the command.\n  - Copy the file out and analyze it using Eclipse - Memory Analyzer.\nThen in the Leak Suspects section it shows the following -\n\nIt seems related to connections to zookeeper where we are storing our dtabs. \nBesides, according to the logs populated by n4d, every 15 minutes we receive around 2 thousands logs with message Attempting to observe dtab/*. I checked the source code . It seems checking whether the dtab exists or is valid, but since I am not familiar with Scala so do not know much detail regarding it.\nThe attachments are reports from Eclipse - Memory Analyzer, and also let me know if there is any other details I can provide.\ndominatorTreeReport.zip\nThreadDetailsReport.zip\nnamerdumpLeakHunterReport.zip\n. > @adw12382: are you able to test if this change fixes the Namerd memory issue for you? Would it be helpful for us to provide a docker image for you to test?\n@adleong : I just come back from vacation today, and thanks for the awesome work regarding the issue! I will test it today or tomorrow and I think I should be able to generate a docker image from the branch. I will post my updates here. . I just got the following Leak Suspects report from an image built from the issue-2212 branch. \n\nIt seems to me that the zookeeper memory leak is fixed according to the report, but not sure what's the issue, which seems in-line with what is observed by @kleimkuhler , showing in the report though. Also, the namerd pod has been up and running for 1.5 hours in environment where live traffic goes through, and the namerd pods' memory usage stays around 10% line, which is much lower than what we observed previously.\nI will let it run in our environment for several days and see if it works as expected, and will post updates here if any.\nThanks to all of you for the awesome work!. @adleong Just curious that is there any garbage collection specific config can be set for n4d container?. Hi there, just a quick update, the image has been up for 4 days and the memory usage climbs much more slower than how it was previously. Looking forward to the new release :octocat: . \nJust an update, it looks like the memory usage keeps increasing over time. I feel a potential leak is still there. I am curious to see see what happens after the usage bumps up to 100% again . Hi @dadjeibaah ,\nThe lines that dropped and then came up again in same color are caused by GC. The lines that suddenly disappeared are caused by pod restarting. Besides, the lines appeared around 2/22 are reflecting status of the image built from the bugfix branch. All the previous lines are reflecting the namerd-1.6.0.\nThanks. ",
    "longkb": "Thank you very much ^_^. > Failed is a member of Activity so I think this was correct without the space: Activity.Failed.\nThanks you for your comment. I have push a change to revert it :). ",
    "yeyincai": "@JustinVenus      thank you  , found  for delegation tables. @dadjeibaah yes , I have solved it\uff0cbut it is done by changing the Linkerd  loadbalancer code.. @dadjeibaah  Linkerd has a variety of load balancing algorithms, using roundrobin as an example\uff0cI   changing   RoundRobinBalancer.pick() ,add   cross-DC failover logic . Create address with metadata\n```java\n    private[this] val (localDC: Vector[Node], remoteDC: Vector[Node]) =\n      vector.partition(item => item.factory.address.asInstanceOf[Inet].metadata.get(CrossLabel).isEmpty)\nprivate[this] val (remoteUp: Vector[Node], remoteDown: Vector[Node]) = remoteDC.partition(\n  _.isAvailable\n)\n\nprivate[this] val (localUp: Vector[Node], localDown: Vector[Node]) = localDC.partition(\n      _.isAvailable\n    )\ndef pick(): Node = {\n      var selections = Vector.empty[Node]\n      if (localUp.nonEmpty) {\n        selections = localUp\n      } else {\n        if (remoteUp.nonEmpty) {\n          selections = remoteUp\n        }\n      }\n      ..........\n   }\n```\nChanged  RoundRobinBalancer.class  is need  Balancers.class  reconfigure.\nHope it can help you!\n. ",
    "tuanvcw": "Hi @dadjeibaah and @adleong \nThank you very much, wish you a great day ahead :smile:. Thank you, @adleong\nHave a great day :smile:. Thanks a lot, @dadjeibaah :smile:. Thank you, @dadjeibaah \nHave a great day :). @adleong, thank you very much for approving this PR :). Waiting on code owner review from @olix0r and/or @wmorgan . ",
    "truongnh1992": "\nThanks for adding this! I left a few comments but other than that this should be good to go.\n\nThank you for your review. I've just updated it.. @adleong Could you help me to review this PR, please.. /retest. ",
    "huynq0911": "\nThanks for submitting this, this is a great idea @huynq0911. We have a PR template described in CONTRIBUTING.md that describes how PRs should be formatted. I think it would be good to follow that format.\n\nI see. But as from other GitHub projects, this template will be shown when contributor create new pull request, they have not to open CONTRIBUTING.md again to copy that.. ",
    "fengzixi": "It work now.\nchange the config, add some item to client:\n  servers:\n  - port: 15003\n    ip: 0.0.0.0\n    thriftFramed: false\n  client:\n    thriftFramed: false\n    hostConnectionPool:\n       minSize: 0\n       maxSize: 3000\n       idleTimeMs: 0  . ",
    "angrylogic": "FWIW this patch follows a suggested convention from the apache httpd: https://httpd.apache.org/docs/2.4/logs.html#virtualhost\nThere are existing tools like split-logfile which should be able to parse this format:\nhttps://httpd.apache.org/docs/2.4/programs/split-logfile.html\n. ",
    "jgehrcke": "Just noting: as an optimization, you could also inspect the authentication token, look at its expiration time (encoded in the exp claim of the JSON Web Token), and schedule to asynchronously retrieve a new one early enough, effectively preventing the 401 responses as of token expiration. Might keep latency for certain things at a minimum (I don't know if that matters or not).\n. ",
    "tsandall": "Oops! Should be Open Policy Agent instead of Oracle Policy Automation.. ",
    "nvartolomei": "What do you mean? This is here to break previous tests, which are fixed by adding backticks.\nCLA signed.. ",
    "ZackButcher": "There's no need to repeat the well-known attribute names in your words list: they already exist in the global list, so you can index into that list for their names. Then, you only need to include your own custom words in your per-request list. That should cut down on a lot of data over the wire.\nThe global list is defined here: https://github.com/istio/api/blob/master/mixer/v1/global_dictionary.yaml\nYou'll want to include the global word count with each of your requests anyway (IIRC Mixer will reject the call if it can't agree on a dictionary), so you need either a copy of the global list or to hard-code the number of entries anyway. We do not anticipate changing the global list frequently, and any change made to it will be a breaking API change, so there will be plenty of warning. We embed it directly into the Mixer.. Since you're sharing the same word list for each of these Attributes messages, you can specify it once in the default_words field of the ReportRequest and not have to repeat it per Attributes (even though there's only one Attributes right now).. I'm ignorant about Scala: when you retrieve indices in dictionary with indexOf, where're you flipping the index around to be negative? You want them to be negative since you're indexing into the local dictionary instead of the global one.. ",
    "natemurthy": "super minor misspelling: ClassifiedRetyFilter => ClassifiedRetryFilter. got an extra article here: ...of a the request.... ",
    "sterlingwhite": "nice catch \ud83d\udc4f. another nice catch \u26be. ",
    "jreichhold": "done. ",
    "JoeWrightss": "Thank you for reminding me, @adleong.\ud83d\ude38. "
}